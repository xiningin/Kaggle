{"cell_type":{"9f607cae":"code","0a5f2786":"code","bd655404":"code","b94fae92":"code","e3d217ae":"code","a0044769":"code","5283b9f1":"code","e2bfb9a3":"code","454baef3":"code","5713cc8e":"code","0284b1cc":"code","53a6d11f":"code","9b7fdf36":"code","11101990":"code","2f336470":"code","0923567d":"code","3e2e849d":"code","18da746f":"code","b2f17e0a":"code","760ef6e9":"code","744870b5":"code","2f8daa71":"code","e69fc0cb":"code","bc251ade":"code","05527ec6":"code","6a720f6f":"code","056b0b33":"code","a4502f6e":"code","73e59c33":"code","a30f6e17":"code","74787c7f":"code","2dedd558":"code","6c185c94":"code","c47cc457":"code","f889b686":"code","033660ff":"code","73384309":"code","45188015":"code","4b634187":"code","c32dd55f":"code","c101782a":"code","60fc0290":"code","4f53066c":"code","986bfcdb":"code","be56eb7f":"code","67a0e183":"code","bfe757dd":"code","2b60813d":"code","41d55b40":"code","de4f2dc7":"code","c28c3cfb":"code","fe90ad59":"code","eb6522df":"code","88abc3b5":"code","91912082":"code","c76aea55":"code","11aa6560":"code","75e9338e":"code","5e81f329":"code","5cadd966":"code","553f2fda":"code","7ae07d61":"code","36a9dabb":"code","3d401462":"code","3f32dc51":"code","4791f728":"code","8b60f40f":"code","b1a307a8":"code","4001cd33":"code","4127d0d9":"code","69978b1d":"code","a431d1f9":"code","2b0d58e1":"code","62f095bf":"code","3b697036":"code","7d2fedce":"code","7728a645":"code","ae7d869b":"code","96ebc2b3":"code","a7587308":"code","7c30a2a2":"code","f16e57b8":"code","2701081c":"code","51f4ba87":"code","056ad894":"code","573f2d34":"code","601a42a7":"code","fddd831f":"code","72701f94":"code","ec9b59cc":"code","b74d3851":"code","85aa031b":"code","765da9a6":"code","b9489433":"code","61f68930":"code","8710e67d":"code","7cf9e771":"code","cf0c3114":"code","8ab43631":"code","ec2d0618":"code","d3619053":"code","256c2491":"code","845d5013":"code","ef3e1c67":"code","7afc54e9":"markdown","339c4ad1":"markdown","dda209f1":"markdown","e810d1bd":"markdown","69899366":"markdown","ad2af264":"markdown","97a8af28":"markdown","b3154454":"markdown","21be5210":"markdown","403a524c":"markdown","29c79e4d":"markdown","b5ffcbb2":"markdown","a9edb0be":"markdown","4c0c3a71":"markdown","94359314":"markdown","399f897e":"markdown","73968b3a":"markdown","58470512":"markdown","0c647506":"markdown","f2d4ba33":"markdown","229d2090":"markdown","e4ed7300":"markdown","ba93ec06":"markdown","3b8be659":"markdown","db01971b":"markdown"},"source":{"9f607cae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport torch #pytorch\nimport torch.nn as nn #for our model class\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# general NLP preprocessing and basic tools\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nimport spacy\n\nfrom sklearn.metrics import confusion_matrix\n# train\/test split\nfrom sklearn.model_selection import train_test_split\n# basic machine learning models\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n# our evaluation metric for sentiment classification\nfrom sklearn.metrics import fbeta_score\n\nimport re","0a5f2786":"!pip install pycld2\n!pip install spacy_langdetect\nimport pycld2 as cld2\nfrom spacy_langdetect import LanguageDetector","bd655404":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b94fae92":"train_df1 = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-3\/train.csv')\ntest_df1 = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-3\/test.csv')","e3d217ae":"len(train_df1)+len(test_df1)","a0044769":"train_df1.head()","5283b9f1":"test_df1.head()","e2bfb9a3":"pos=train_df1[train_df1.sentiment == \"positive\"].count()[1]\nneu=train_df1[train_df1.sentiment == \"neutral\"].count()[1]\nneg=train_df1[train_df1.sentiment == \"negative\"].count()[1]\nprint(\"Number of positive tweets: \"+str(pos)+\", ratio of positive tweets: \"+str(pos\/(neg+neu+pos)))\nprint(\"Number of neutral tweets: \"+str(neu)+\", ratio of neutral tweets: \"+str(neu\/(neg+neu+pos)))\nprint(\"Number of negative tweets: \"+str(neg)+\", ratio of negative tweets: \"+str(neg\/(neg+neu+pos)))\nprint(\"Total number of tweets: \"+str(len(train_df1)))","454baef3":"# language detection 1\nsum = 0\n# good to find sentences with several languages\nfor i in range (len(train_df1)):\n    text=preprocessing_normalization(train_df1.iloc[i]['text'])\n    _, _, _, detected_language = cld2.detect(text,  returnVectors=True)\n    for j in range (len(detected_language)):\n        found = detected_language[j][3]\n        print(found)\n        if (found != \"en\"):\n            #if (found !=\"un\"): # a lot of unknown here are in fact english\n            print(found+\", \"+str(i)+\", \"+text)\n            sum = sum + 1\nprint(sum)\n# sum = 4349 for initial data (un + !en)\n# sum = 17 for initial data (!en)\n# sum = 9940 for preprocessed data (un + !en)\n# sum =  34 for preprocessed data (!en)","5713cc8e":"# language detection 2\nsum2=0\nnlp = spacy.load('en')\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n# good to find unknown language: which are very often punctuation, symbols or link\n# unknown symbols as :) ;) <3 can help classify the data: add them to vocabulary?\nfor i in range (len(train_df)):\n    text=preprocessing_normalization(train_df.iloc[i]['text'])\n    doc = nlp(text)\n    for sent in doc.sents:\n        if (sent._.language['language'] == 'UNKNOWN'):\n            # print(doc._.language) # to print the majority language of the sentence\n            print(i, sent, sent._.language['language'], sent._.language['score'])\n            sum2 = sum2 + 1\nprint(sum2)\n# before preprocessing\n## unknown: 1645\n\n#after preprocessing\n##unknown: 226","0284b1cc":"# we create a validation dataset from the training data\ntrain_df1, val_df1 = train_test_split(train_df1, test_size=0.1, random_state=0)","53a6d11f":"target_conversion = {\n    'neutral': 1,\n    'positive': 2,\n    'negative': 0\n}","9b7fdf36":"train_df1['target'] = train_df1['sentiment'].map(target_conversion)\nval_df1['target'] = val_df1['sentiment'].map(target_conversion)","11101990":"nlp = spacy.load('en', disable=['parser', 'ner']) #put at the correct place: load the english dictionnary in spacy\npuncts=\".?!,*)'`\"\nabbrevs={'<3':'love', ':)':'smile', '(:': 'smile', \"they're\": 'they are', ';)':'winking', ':>':'', ':\/':'disappointed',\n         '*-*': 'admiration', '^__^':'very amused', '^___^':'very amused', ':|':'indifferent', \"can`t\": 'can not', \"can't\": \"can not\",\n         ':S': 'confused', \"'ll\": \" will\", \"`ll\": \" will\", \"it's\": 'it is', \"it`s\": 'it is', \"lol\": \"laughing a lot\"}\n\n\n# can add spelling correction\ndef preprocessing_normalization(text):\n    #print(text)\n    filtered_sentence = text\n    # abbreviation and smileys\n    for abbrev in abbrevs:\n        filtered_sentence = filtered_sentence.replace(abbrev,abbrevs[abbrev])\n    # lowercase\n    filtered_sentence = filtered_sentence.lower()\n    #numbers removal\n    filtered_sentence = re.sub(r'[0-9]', '', filtered_sentence)\n    #link removal\n    filtered_sentence=re.sub(r'http\\S+', '', filtered_sentence)\n    # stop words removal\n    text_tokens = word_tokenize(filtered_sentence)\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n    filtered_sentence = (\" \").join(tokens_without_sw)\n    # lemmatization\n    doc = nlp(filtered_sentence)\n    filtered_sentence = \" \".join([token.lemma_ for token in doc])\n    # punctuation removal\n    for sym in puncts:\n        filtered_sentence = filtered_sentence.replace(sym,' ')\n    # multiple whitespaces removal\n    filtered_sentence=re.sub(' +', ' ', filtered_sentence)\n    \n    return filtered_sentence","2f336470":"# train normalization of the text column\ntrain_df = train_df1.copy()\ntrain_df['text'] = train_df['text'].transform(func = lambda text : preprocessing_normalization(text))\ntrain_df.head()","0923567d":"# val normalization of the text column\nval_df = val_df1.copy()\nval_df['text'] = val_df['text'].transform(func = lambda text : preprocessing_normalization(text))\nval_df.head()","3e2e849d":"# test normalization of the text column\ntest_df = test_df1.copy()\ntest_df['text'] = test_df['text'].transform(func = lambda text : preprocessing_normalization(text))\ntest_df.head()","18da746f":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nPRE_TRAINED_MODEL_NAME = 'bert-base-cased'","b2f17e0a":"tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","760ef6e9":"sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'\ntokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","744870b5":"encoding = tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  truncation=True,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\nencoding.keys()","2f8daa71":"print(len(encoding['input_ids'][0]))\nencoding['input_ids'][0]","e69fc0cb":"print(len(encoding['attention_mask'][0]))\nencoding['attention_mask']","bc251ade":"token_lens = []\nfor txt in train_df['text']:\n  tokens = tokenizer.encode(txt, max_length=512)\n  token_lens.append(len(tokens))","05527ec6":"sns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count');","6a720f6f":"MAX_LEN = 100","056b0b33":"from torch.utils.data import Dataset, DataLoader\nclass GPReviewDataset(Dataset):\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.reviews)\n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        encoding = self.tokenizer.encode_plus(\n          review,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          #pad_to_max_length=True,\n          padding='max_length',\n          return_attention_mask=True,\n          return_tensors='pt',\n        )\n        return {\n          'review_text': review,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }","a4502f6e":"train_df.shape, val_df.shape, test_df.shape","73e59c33":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = GPReviewDataset(\n        reviews=df['text'].to_numpy(),\n        targets=df['target'].to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n      )\n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=4\n      )\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)","a30f6e17":"data = next(iter(train_data_loader))\ndata.keys()","74787c7f":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","2dedd558":"bert_model = BertModel.from_pretrained('bert-base-uncased')","6c185c94":"last_hidden_state, pooled_output = bert_model(\n  input_ids=encoding['input_ids'],\n  attention_mask=encoding['attention_mask'],\n  return_dict=False\n)\nlast_hidden_state.shape","c47cc457":"bert_model.config.hidden_size","f889b686":"pooled_output.shape","033660ff":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=False\n        )\n        output = self.drop(pooled_output)\n        return self.out(output)","73384309":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\") ","45188015":"class_names = ['negative', 'neutral', 'positive']\nmodel = SentimentClassifier(len(class_names))\nmodel = model.to(device)","4b634187":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","c32dd55f":"F.softmax(model(input_ids, attention_mask), dim=1)","c101782a":"EPOCHS = 10\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","60fc0290":"def train_epoch(\n  model,\n  data_loader,\n  loss_fn,\n  optimizer,\n  device,\n  scheduler,\n  n_examples\n):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","4f53066c":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","986bfcdb":"from collections import defaultdict","be56eb7f":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(train_df)\n      )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n        model,\n        val_data_loader,\n        loss_fn,\n        device,\n        len(val_df)\n      )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","67a0e183":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","bfe757dd":"test_predictions_nb = np.zeros(len(test_df), dtype=int)\ncompt=0\nfor i in range (len(test_df)):\n    encoded_review = tokenizer.encode_plus(\n      test_df.iloc[i]['text'],\n      max_length=MAX_LEN,\n      add_special_tokens=True,\n      return_token_type_ids=False,\n      #pad_to_max_length=True,\n      padding='max_length',\n      return_attention_mask=True,\n      return_tensors='pt',\n    )\n    input_ids = encoded_review['input_ids'].to(device)\n    attention_mask = encoded_review['attention_mask'].to(device)\n    output = model(input_ids, attention_mask)\n    _, prediction = torch.max(output, dim=1)\n    if (class_names[prediction] == 'positive'):\n        test_predictions_nb[i]=1\n    if (class_names[prediction] == 'negative'):\n        test_predictions_nb[i]=-1\n    if (class_names[prediction] == 'neutral'):\n        test_predictions_nb[i]=0\n    print('Prediction {}\/{}'.format(compt+1, len(test_df)), end='\\r')\n    compt = compt+1","2b60813d":"#make the prediction in the correct format\ntest = pd.DataFrame(test_predictions_nb)\nsubmission_df = pd.DataFrame()\nsubmission_df['textID'] = test_df['textID']\nsubmission_df['sentiment'] = test\nsubmission_df.to_csv('submissionb.csv', index=False)","41d55b40":"count_vect = CountVectorizer()","de4f2dc7":"train_df['text']\ntrain_df.iloc[7025]['text']","c28c3cfb":"# train normalization of the text column\nnorm_train_df = train_df.copy()\nnorm_train_df['text'] = norm_train_df['text'].transform(func = lambda text : preprocessing_normalization(text))\nnorm_train_df.head()","fe90ad59":"# val normalization of the text column\nnorm_val_df = val_df.copy()\nnorm_val_df['text'] = norm_val_df['text'].transform(func = lambda text : preprocessing_normalization(text))\nnorm_val_df.head()","eb6522df":"# test normalization of the text column\nnorm_test_df = test_df.copy()\nnorm_test_df['text'] = norm_test_df['text'].transform(func = lambda text : preprocessing_normalization(text))\nnorm_test_df.head()","88abc3b5":"# here we are obtaining the vocabulary from the training data minus validation data\n# you may want to change this to the full training data for the final submission\nX_train_counts = count_vect.fit_transform(list(norm_train_df['text'].values))\nX_val_counts = count_vect.transform(list(norm_val_df['text'].values))\nX_test_counts = count_vect.transform(list(norm_test_df['text'].values))","91912082":"print('Train feature shape:', X_train_counts.shape)\nprint('Train feature shape:', X_val_counts.shape)\nprint('Test feature shape:', X_test_counts.shape)","c76aea55":"# Now we quickly analyze the matrix of word counts:\n# Only 255125 of the 22258x23162 values => 0.049487% are non-zero.\n# The sparse encoding only needs to store these.\nX_train_counts","11aa6560":"# Yet, we can ask to convert a part of the matrix into the traditional dense format.\n# It's quite challenging to find any non-zeros here!\nX_train_counts[:10,:10].toarray()","75e9338e":"# The other way around is easier. We can ask to find the ID (index) of a specific word.\ncount_vect.vocabulary_.get('sleep')","5e81f329":"count_vect.items()","5cadd966":"# So the first tweet should have a one at this position:\nprint('Tweet:\\n', train_df.iloc[0]['text'])\nprint('Number of times the word \"sleep\" appeared:\\n', X_train_counts[0, 14676])","553f2fda":"nlp = spacy.load('en', disable=['parser', 'ner']) #put at the correct place: load the english dictionnary in spacy\npuncts=\".?!,*)'`\"\nabbrevs={'<3':'love', ':)':'smile', '(:': 'smile', \"they're\": 'they are', ';)':'winking', ':>':'', ':\/':'disappointed',\n         '*-*': 'admiration', '^__^':'very amused', '^___^':'very amused', ':|':'indifferent', \"can`t\": 'can not', \"can't\": \"can not\",\n         ':S': 'confused', \"'ll\": \" will\", \"`ll\": \" will\", \"it's\": 'it is', \"it`s\": 'it is', \"lol\": \"laughing a lot\"}\n\n\n# can add spelling correction\ndef preprocessing_normalization(text):\n    #print(text)\n    filtered_sentence = text\n    # abbreviation and smileys\n    for abbrev in abbrevs:\n        filtered_sentence = filtered_sentence.replace(abbrev,abbrevs[abbrev])\n    # lowercase\n    filtered_sentence = filtered_sentence.lower()\n    #numbers removal\n    filtered_sentence = re.sub(r'[0-9]', '', filtered_sentence)\n    #link removal\n    filtered_sentence=re.sub(r'http\\S+', '', filtered_sentence)\n    # stop words removal\n    text_tokens = word_tokenize(filtered_sentence)\n    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n    filtered_sentence = (\" \").join(tokens_without_sw)\n    # lemmatization\n    doc = nlp(filtered_sentence)\n    filtered_sentence = \" \".join([token.lemma_ for token in doc])\n    # punctuation removal\n    for sym in puncts:\n        filtered_sentence = filtered_sentence.replace(sym,' ')\n    # multiple whitespaces removal\n    filtered_sentence=re.sub(' +', ' ', filtered_sentence)\n    \n    return filtered_sentence","7ae07d61":"nb_t = 24621\nprint(preprocessing_normalization(train_df.iloc[nb_t]['text']))\nprint(train_df.iloc[nb_t]['sentiment'])","36a9dabb":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\") ","3d401462":"new_train = norm_train_df['text'].to_numpy()\nnew_train","3f32dc51":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch #pytorch\nimport torch.nn as nn #for our model class\nimport torch.nn.functional as F\nfrom nltk.corpus import stopwords #removing stop words\nfrom collections import Counter #counting the unique numbers\nimport string \nimport re #regex\nimport seaborn as sns #plotting\nimport matplotlib.pyplot as plt #plotting\nfrom torch.utils.data import TensorDataset, DataLoader #data prep\nfrom sklearn.model_selection import train_test_split","4791f728":"#Tokenize the sentences\ntokenizer = Tokenizer()\n\n#preparing vocabulary\ntokenizer.fit_on_texts(list(norm_train_df['text']))\n\n#converting text into integer sequences\nx_train  = tokenizer.texts_to_sequences(norm_train_df['text']) \nx_test = tokenizer.texts_to_sequences(norm_val_df['text'])","8b60f40f":"size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\nprint(size_of_vocabulary)\nrev_len = [len(i) for i in x_train]\npd.Series(rev_len).hist()\nplt.show()\npd.Series(rev_len).describe() ","b1a307a8":"def padding_(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features ","4001cd33":"#we have very less number of reviews with length > 500.\nx_train_pad = padding_(x_train,30)\nx_test_pad = padding_(x_test,30)","4127d0d9":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(norm_train_df['target'].to_numpy()))\nvalid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(norm_val_df['target'].to_numpy()))\n\n# batch size\nbatch_size = 50\n\n#shuffling\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)","69978b1d":"class SentimentRNN(nn.Module):\n    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5): \n        super(SentimentRNN,self).__init__()\n \n        self.output_dim = output_dim #output dimensions\n        self.hidden_dim = hidden_dim #hidden dimensions\n  \n        self.no_layers = no_layers #number of layers\n        self.vocab_size = vocab_size #vocabulary size\n    \n        # embedding \n        self.embedding = nn.Embedding(vocab_size, embedding_dim) #embedding of vocabulary size and embedding dimensions        \n        #lstm\n        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n                           num_layers=no_layers, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.4)\n    \n        # linear and sigmoid layer\n        self.fc0 = nn.Linear(self.hidden_dim, 512) #first drop out\n        self.fc1 = nn.Linear(512, 256) #1st fc layer\n        self.dropout1 = nn.Dropout(0.2) #2nd drop out\n        self.fc = nn.Linear(256, output_dim) #2nd fully connected layer\n        self.sig = nn.Sigmoid() #for last layer\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        # embeddings and lstm_out\n        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n        #print(embeds.shape)  #[50, 500, 1000]\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        \n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n        \n        # dropout and fully connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc0(out)\n        out = self.dropout1(out)\n        out = self.fc1(out)\n        out = self.fc(out)\n        \n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n        \n        \n        \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n        hidden = (h0,c0)\n        return hidden","a431d1f9":"no_layers = 4 #4 hidden LSTM stacked layers\nvocab_size = size_of_vocabulary + 1 #extra 1 for padding\nembedding_dim = 64 # embedding dimensions\noutput_dim = 1 #single output 1 or 0\nhidden_dim = 256 # hidden dimensions\nmodel = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n#moving to gpu if available\nmodel.to(device)\nprint(model)","2b0d58e1":"def acc(pred,label):\n    pred = torch.round(pred.squeeze()) #remove extra dimensions\n    return torch.sum(pred == label.squeeze()).item()","62f095bf":"lr=0.001 #learning rate\ncriterion = nn.BCELoss() #binary cross entropy for binary classification\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","3b697036":"clip = 5 #gradient clipping for exploding gradients\nepochs = 10 #number of epochs\nvalid_loss_min = np.Inf #setting loss for minimum i.e infinity epochs\n# train for some number of epochs\nepoch_tr_loss,epoch_vl_loss = [],[]  #lists for appending per epochs statistics to visualize\nepoch_tr_acc,epoch_v_acc = [],[]","7d2fedce":"for epoch in range(epochs):\n    train_losses = []\n    train_acc = 0.0\n    model.train()\n    # initialize hidden state \n    h = model.init_hidden(batch_size)\n    for inputs, labels in train_loader: #training data\n        \n        inputs, labels = inputs.to(device), labels.to(device)   \n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h]) \n        \n        model.zero_grad() #zero gradients before starting the training\n        output,h = model(inputs,h) #forward pass\n        \n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float()) #criterion loss\n        loss.backward() # computes loss\n        train_losses.append(loss.item()) #appending the loss\n        # calculating accuracy\n        accuracy = acc(output,labels) \n        train_acc += accuracy \n\n        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step() #improving the loss via optimizer using back prop (ADAM optimizer)\n \n    \n        \n    val_h = model.init_hidden(batch_size) #initialize the hidden state\n    val_losses = []\n    val_acc = 0.0\n    model.eval()\n    for inputs, labels in valid_loader: #validation data\n            val_h = tuple([each.data for each in val_h]) #separate variable for hidden state\n\n            inputs, labels = inputs.to(device), labels.to(device) #checking if gpu or not\n\n            output, val_h = model(inputs, val_h) #forward pass\n            val_loss = criterion(output.squeeze(), labels.float()) # loss\n\n            val_losses.append(val_loss.item()) #appending loss\n            \n            accuracy = acc(output,labels)\n            val_acc += accuracy\n            \n    epoch_train_loss = np.mean(train_losses)  #mean of the loss for calculating epoch loss\n    epoch_val_loss = np.mean(val_losses) #same for validation loss\n    epoch_train_acc = train_acc\/len(train_loader.dataset) #acc for epoch\n    epoch_val_acc = val_acc\/len(valid_loader.dataset) #vali acc for epoch\n    epoch_tr_loss.append(epoch_train_loss)\n    epoch_vl_loss.append(epoch_val_loss)\n    epoch_tr_acc.append(epoch_train_acc)\n    epoch_vl_acc.append(epoch_val_acc)\n    print(f'Epoch {epoch+1}') \n    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n    if epoch_val_loss <= valid_loss_min:\n        torch.save(model.state_dict(), '..\/working\/state_dict.pt') #saving the model in dictionary in .pt format\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n        valid_loss_min = epoch_val_loss\n    print(25*'==') #for epoch ending symbol","7728a645":"ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(2, 3, 4))\nngram_vectorizer.fit(reviews_train_clean)\nX = ngram_vectorizer.transform(reviews_train_clean)\nX_test = ngram_vectorizer.transform(reviews_test_clean)\n\nfinal_ngram = LogisticRegression(C=0.5)\nfinal_ngram.fit(X, target)\nprint (\"Final Accuracy: %s\" \n       % accuracy_score(target, final_ngram.predict(X_test)))","ae7d869b":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n#Tokenize the sentences\ntokenizer = Tokenizer()\n\n#preparing vocabulary\ntokenizer.fit_on_texts(list(norm_train_df['text']))\n\n#converting text into integer sequences\nx_tr_seq  = tokenizer.texts_to_sequences(norm_train_df['text']) \nx_val_seq = tokenizer.texts_to_sequences(norm_val_df['text'])\n\ngl_lenght=0\nfor i in range (len(x_tr_seq)):\n    if (len(x_tr_seq[0])>gl_lenght):\n        gl_lenght = len(x_tr_seq[0])\nprint(gl_lenght)\n\n#padding to prepare sequences of same length\nx_tr_seq  = pad_sequences(x_tr_seq, maxlen=100)\nx_val_seq = pad_sequences(x_val_seq, maxlen=100)","96ebc2b3":"size_of_vocabulary=len(tokenizer.word_index) + 1 #+1 for padding\nprint(size_of_vocabulary)","a7587308":"#deep learning library\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\n\nmodel=Sequential()\n\n#embedding layer\nmodel.add(Embedding(size_of_vocabulary,100,input_length=100,trainable=True)) \n\n#lstm layer\nmodel.add(LSTM(128,return_sequences=True,dropout=0.2))\n\n#Global Maxpooling\nmodel.add(GlobalMaxPooling1D())\n\n#Dense Layer\nmodel.add(Dense(64,activation='relu')) \nmodel.add(Dense(1,activation='sigmoid')) \n\n#Add loss function, metrics, optimizer\nmodel.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"acc\"]) \n\n#Adding callbacks\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=3)  \nmc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True,verbose=1)  \n\n#Print summary of model\nprint(model.summary())","7c30a2a2":"history = model.fit(np.array(x_tr_seq),np.array(norm_train_df['target']),batch_size=32,epochs=10,validation_data=(np.array(x_val_seq),np.array(norm_val_df['target'])),verbose=1,callbacks=[es,mc])","f16e57b8":"#loading best model\nfrom keras.models import load_model\nmodel = load_model('best_model.h5')\n\n#evaluation \n_,val_acc = model.evaluate(x_val_seq,norm_val_df['target'], batch_size=128)\nprint(val_acc)","2701081c":"%%time\nclf = MultinomialNB(alpha=0.94).fit(X_train_counts, norm_train_df['target'])","51f4ba87":"# here we are obtaining the vocabulary from the training data minus validation data\n# you may want to change this to the full training data for the final submission\nX_train_counts = count_vect.fit_transform(list(norm_train_df['text'].values))\nX_val_counts = count_vect.transform(list(norm_val_df['text'].values))\nX_test_counts = count_vect.transform(list(norm_test_df['text'].values))","056ad894":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier(n_estimators=700, random_state=0).fit(X_train_counts, norm_train_df['target'])","573f2d34":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=800, learning_rate=1, max_depth=1, random_state=0).fit(X_train_counts, norm_train_df['target'])","601a42a7":"val_predictions_nb = clf.predict(X_val_counts)","fddd831f":"# confusion matrix\nc_matrix = confusion_matrix(val_df['target'].values, val_predictions_nb)\ncolumns = [\"pred -1\", \"pred 0\", \"pred 1\"]\nindex = [\"actual -1\", \"actual 0\", \"actual 1\"]\nc_matrix_table = pd.DataFrame(data=c_matrix,index=index,columns=columns)\nc_matrix_table ","72701f94":"accuracy = (val_predictions_nb == val_df['target'].values).mean()\nprint('The accuracy of our multinomial Naive Bayes classifier is: {:.2f}%'.format(accuracy*100))","ec9b59cc":"fbeta = fbeta_score(val_df['target'].values, val_predictions_nb, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)","b74d3851":"# Creating a submission\n\nX_train_counts = count_vect.fit_transform(list(norm_train_df['text'].values) + list(norm_val_df['text'].values))\nX_test_counts = count_vect.transform(list(norm_test_df['text'].values))\n\n# clf = MultinomialNB().fit(X_train_counts, np.hstack([train_df['target'].values, val_df['target'].values]))\n# clf = AdaBoostClassifier(n_estimators=700, random_state=0).fit(X_train_counts, np.hstack([train_df['target'].values, val_df['target'].values]))\nclf = GradientBoostingClassifier(n_estimators=800, learning_rate=1, max_depth=1, random_state=0).fit(X_train_counts, np.hstack([train_df['target'].values, val_df['target'].values]))\n\ntest_predictions_nb = clf.predict(X_test_counts)\n\nsubmission_df = pd.DataFrame()\nsubmission_df['textID'] = norm_test_df['textID']\nsubmission_df['sentiment'] = test_predictions_nb\nsubmission_df.to_csv('submissiong.csv', index=False)","85aa031b":"nltk.download('vader_lexicon')","765da9a6":"sid = SentimentIntensityAnalyzer()","b9489433":"# We show a few prediction examples:\nfor doc in norm_val_df['text'].iloc[:5].values:\n    print(doc)\n    print(sid.polarity_scores(doc))","61f68930":"def vader_predict(x):\n    prediction = sid.polarity_scores(x)\n    prediction_list = [\n        (1, prediction['pos']),\n        (-1, prediction['neg']),\n        (0, prediction['neu'])\n    ]\n    label = sorted(prediction_list, key=lambda x: x[1], reverse=True)[0][0]\n    return label","8710e67d":"predictions_vader = norm_val_df['text'].apply(vader_predict)","7cf9e771":"accuracy = (predictions_vader == norm_val_df['target'].values).mean()\nprint('The accuracy of VADER is: {:.2f}%'.format(accuracy*100))","cf0c3114":"fbeta = fbeta_score(norm_val_df['target'].values, predictions_vader, average='macro', beta=1.0)\nprint('The fbeta score is:', fbeta)","8ab43631":"# selected_text shows the words selected from text to lead to the classification stored in sentiment\ntrain_df[['text', 'selected_text', 'sentiment']].iloc[:5]","ec2d0618":"import numpy as np\nimport _pickle as cPickle\nimport csv\n!pip3 install stanza\nimport stanza\ncorenlp_dir = '\/kaggle\/working\/corenlp'\nstanza.install_corenlp(dir=corenlp_dir)\nos.environ[\"CORENLP_HOME\"] = corenlp_dir\nfrom stanza.server import CoreNLPClient\nfrom nltk.tree import Tree","d3619053":"def data_to_banktrees(data_df, name):\n    fichier = open(\"\/kaggle\/working\/\"+ name +\"_tree.txt\", \"a\")\n    doc = data_df.text\n    entire_text = ''\n    for line in doc:\n        text = line.replace('.', ' ') + '.'\n        entire_text += text\n    with CoreNLPClient(annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse','coref'], use_gpu=True) as client:\n        ann = client.annotate(entire_text)\n    for i in range(len(doc)):\n        sentence = ann.sentence[i]\n        constituency_parse = sentence.parseTree\n        tree = convert_parse_tree_to_nltk_tree(constituency_parse)\n        flat_tree = tree._pformat_flat(\"\", \"()\", False)\n        fichier.write(flat_tree+\"\\n\")\n    fichier.close()","256c2491":"def convert_parse_tree_to_nltk_tree(parse_tree):\n    return Tree(parse_tree.value, [convert_parse_tree_to_nltk_tree(child) for child in parse_tree.child]) if parse_tree.child else parse_tree.value","845d5013":"train_df = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-3\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-3\/test.csv')\n\ndata_to_banktrees(train_df, 'train')\ndata_to_banktrees(test_df,'test_test')","ef3e1c67":"class RNTNModel:\n\n    def __init__(self, vocab_file=None, wvec_dim=None, num_classes=None, l=None):\n        ## Building vocabulary index\n        with open(vocab_file, 'r') as f:\n            reader = csv.reader(f)\n            self.word_index_map = dict((row[0], int(row[1])) for row in reader)\n\n        ## Initializing the parameters that will be learnt\n        d = wvec_dim\n        C = num_classes\n        self.params = {}                         # Parameter hash table\n        self.params['L'] = np.random.uniform(    # Word vector matrix (dx|vocabulary|)\n            low=-0.0001, high=0.0001, size=(\n                d,len(self.word_index_map)))\n        self.params['V'] = np.random.uniform(    # Tensor (2dx2dxd)\n            low=-0.0001, high=0.0001, size=(2*d,2*d,d))\n        self.params['W'] = np.random.normal(     # Weight matrix (dx2d)\n            loc=0.0, scale=0.01, size=(d,2*d))\n        self.params['b'] = np.random.normal(     # Bias (dx1)\n            loc=0.0, scale=0.01, size=(d,1))\n        self.params['Ws'] = np.random.normal(    # Weights for the softmax classifier (Cxd)\n            loc=0.0, scale=0.01, size=(C,d))\n        self.params['bs'] = np.random.normal(    # Bias for the softmax classifier (Cx1)\n            loc=0.0, scale=0.01, size=(C,1))\n\n        ## Initializing the gradient accumulators of the parameters\n        self.params['dL'] = np.zeros(shape=(d,len(self.word_index_map)))\n        self.params['dV'] = np.zeros(shape=(2*d, 2*d, d))\n        self.params['dW'] = np.zeros(shape=(d, 2*d))\n        self.params['db'] = np.zeros(shape=(d, 1))\n        self.params['dWs'] = np.zeros(shape=(C, d))\n        self.params['dbs'] = np.zeros(shape=(C, 1))\n\n        ## Initializing the historic gradient accumulators for AdaGrad\n        self.params['hdL'] = np.zeros(shape=(d,len(self.word_index_map)))\n        self.params['hdV'] = np.zeros(shape=(2*d, 2*d, d))\n        self.params['hdW'] = np.zeros(shape=(d, 2*d))\n        self.params['hdb'] = np.zeros(shape=(d, 1))\n        self.params['hdWs'] = np.zeros(shape=(C, d))\n        self.params['hdbs'] = np.zeros(shape=(C, 1))\n\n        self.params['C'] = C # Number of classes\n        self.params['d'] = d # Dimensionality of the word vectors\n\n    def make_rntn_tree(self, current, node):\n        node.label = int(current.label)\n        if not current.subtrees:\n            node.leaf = True\n            node.word = current.word\n            node.word_idx = self.word_index_map[current.word]\n        else:\n            left = current.subtrees[0]\n            node.left = self.make_rntn_tree(left, RNTNTree(self.params))\n            right = current.subtrees[1]\n            node.right = self.make_rntn_tree(right, RNTNTree(self.params))\n        return node\n\n       def __get_rntn_trees(self, filename):\n        trees = parser.read_trees(filename)\n        trees = [self.make_rntn_tree(tree, RNTNTree(self.params)) for tree in trees]\n        return trees\n\n    def get_accuracy(self, trees):\n        \"\"\" Computes the average fine grained accuracy\n            for all the trees provided in the list\"\"\"\n\n        def compute_accuracy(tree):\n            acc = float(np.argmax(tree.probs) == tree.label)\n            if tree.leaf:\n                return acc, 1.0\n            lacc, lc = compute_accuracy(tree.left)\n            racc, rc = compute_accuracy(tree.right)\n            return acc + lacc + racc, lc + rc + 1.0\n        if type(trees) is list:\n            accuracy = 0.0\n            for tree in trees:\n                a, b = compute_accuracy(tree)\n                accuracy += a \/ b\n            accuracy \/= len(trees)\n            return accuracy\n        else:\n            return compute_accuracy(trees)\n\n    def train(self, train_file=None, step_size=0.001, lamda=0.01,\n                epsilon=1e-12, num_epoch=3, batch_size=None):\n        trees = self.__get_rntn_trees(train_file)\n        if not batch_size:\n            batch_size = 1\n        for epoch in xrange(num_epoch):\n            for i, tree in enumerate(trees):\n                tree.forward_propagate(tree)\n                tree.back_propagate(tree, lamda=lamda)\n                if (i+1) % batch_size == 0:\n                    tree.adagrad_update(step_size, epsilon, batch_size)\n            if not (len(trees) % batch_size == 0):\n                trees[0].adagrad_update(step_size, epsilon, batch_size)\n            print 'Epoch', epoch + 1, 'Accuracy:', self.get_accuracy(trees)\n            \n    def test(self, test_file=None):\n        trees = self.__get_rntn_trees(test_file)\n        for tree in trees:\n            tree.forward_propagate(tree)\n        accuracy = self.get_accuracy(trees)\n        print 'Accuracy:', accuracy\n\n    def save(self, save_file):\n        with open(save_file, 'wb') as f:\n            cPickle.dump(self.params, f, protocol=cPickle.HIGHEST_PROTOCOL)\n\n    def load(self, save_file):\n        with open(save_file, 'r') as f:\n            self.params = cPickle.load(f)","7afc54e9":"We are training a naive Bayes classifier on the Bag-of-Words features of the training data:\n\nhttps:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/naive-bayes-text-classification-1.html\n\nIt is already built into the sklearn library:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html\n\nKeep in mind that not only storing the features is challenging but also processing them. A simple SVM may be quite slow on such high-dimensional features. Naive Bayes works well with Bag-of-Words.","339c4ad1":"## Training a simple classifier","dda209f1":"## Where to go from here?\n\nWe can improve our Machine Learning pipeline on multiple aspects:","e810d1bd":"### Neural Net","69899366":"Early approaches in NLP used rule-based classifiers for sentiment analysis. A popular baseline is VADER which was published in 2014:\n\nhttps:\/\/www.aaai.org\/ocs\/index.php\/ICWSM\/ICWSM14\/paper\/viewPaper\/8109\n\nVADER does not use any machine learning but is purely handcrafted by humans. It uses text preprocessing and lexica to determine the sentiment of a text.","ad2af264":"VADER performs worse! That is a good sign that our classifier learned useful generalizations from the training data (better than standard handcrafted rules).","97a8af28":"## How good is this score?","b3154454":"## Data preprocessing","21be5210":"# RNTN : implementation ","403a524c":"### Simple models","29c79e4d":"### Model evaluation:\n\nMake sure to select potential model hyperparameters using cross-validation or similar. Our evaluation metric of choice is the F1-score:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score\n\nWe choose beta=1 and average=macro","b5ffcbb2":"# Example solution for tweet sentiment analysis","a9edb0be":"### Data analysis:\n\nHow is the data distributed? Can we analyze our data to find patterns associated with the classes? Which kinds of words are useful, which aren't?","4c0c3a71":"## Loading the data","94359314":"### Pre-trained NN","399f897e":"_________________________________________________________","73968b3a":"### Language detection","58470512":"### Bonus:\n\nApart from classifying the sentiment of tweets, we can also try to determine which words are the reason for the classifier to determine the classification. Ground-truth labels for these words are contained in our training data. The evaluation will not take place on the Kaggle platform.\nYou need to do it yourself. Use the Jaccard coefficient to evaluate the overlap between the selected words and the ground truth:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#jaccard-similarity-coefficient-score","0c647506":"The Bag-of-Words representation assigns a unique ID to each word that appears in the training data. 23239 unique words have been extracted. Each input data point (tweet) is then represented by a vector of the size of the vocabulary. Each of its elements are the counts of the respective word appearing in the tweet.\n\nTherefore, the features have a *huge* dimension! Storing the feature matrix directly would require (n_datapoints x vocabulary size) * 32 bits $\\approx$ 2 GB CPU\/GPU RAM! Imagine we were not analyzing tweets (limited vocabulary) but Wikipedia! Or imagine we had a larger corpus of documents. Then we could not store the features!\n\nInstead, the Bag-of-Words features are usually stored using a *sparse* representation. Imagine this like a dictionary of ID-count tuples assigned to each tweet.","f2d4ba33":"### Model selection:\n\nThe model of choice highly depends on the previously extracted features. Depending on whether you obtain a sparse or dense feature representation, you have to choose an appropriate model!","229d2090":"We start off by converting the labels to numbers. This is a requirement for the submission and numerical inputs are generally more compatible with machine learning libraries.","e4ed7300":"Now we need to find a numerical representation for our input data. Extracting features from text is one of the major building blocks of any Natural Language Processing (NLP) pipeline.\n\nThere have been huge developments in the field during the last decade. A very traditional approach is to extract Bag-of-Words features. See here for an explanation:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction\n\nWe will stick to this technique for the purpose of this example notebook. However, be aware that much more powerful feature extraction techniques exist. The most recent ones use neural network based language models. See e.g.:\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2019\/06\/understanding-transformers-nlp-state-of-the-art-models\/","ba93ec06":"### Feature extraction:\n- Can we make our Bag-of-Words representation more compact or richer? There are many things you could try to implement. Here are some buzzwords: tokenization, stop words removal, lemmatization, n-gram extraction, ...\n- A useful Python library to address these issues is: NLTK (https:\/\/www.nltk.org\/)\n- The sklearn CountVectorizer we used can be combined with NLTK preprocessing:\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#customizing-the-vectorizer-classes\n- Is there also a dense (as opposed to sparse) representation of documents (tweets in our case)? Buzzwords: word2vec, LDA, LSI\n- A useful Python library for this purpose (with pretrained models) is: gensim https:\/\/radimrehurek.com\/gensim\/\n- The state-of-the-art: ... are neural network language models, so-called Transformers. There are pretrained models available. If you feel comfortable with neural networks, fine-tuning and GPUs, have a look here: https:\/\/huggingface.co\/transformers\/","3b8be659":"## Quick data inspection","db01971b":"### Text normalization: putting all together in a function"}}