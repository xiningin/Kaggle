{"cell_type":{"11ebcb68":"code","6f99d5df":"code","6fc318a4":"code","3c2694d0":"code","1940976f":"code","0f37c2e2":"code","9c10b463":"code","90d6113d":"code","a5617749":"code","1328069d":"code","5eb68fc4":"code","37a0a651":"code","04239a9c":"code","93c1928a":"markdown","3782e3f6":"markdown","b5458f1b":"markdown","f4add9f6":"markdown","2773cac1":"markdown","d229916a":"markdown","c6ce9b8d":"markdown","7468f35f":"markdown","3bf93e03":"markdown","e6085367":"markdown","43c403be":"markdown","d44ca042":"markdown","07aa6893":"markdown","1ed169ee":"markdown"},"source":{"11ebcb68":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6f99d5df":"diabetes = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndiabetes.head()","6fc318a4":"# Separate features and labels\nfeatures = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\nlabel = 'Outcome'\nX, y = diabetes[features].values, diabetes[label].values\n\nfor n in range(0,4):\n    print(\"Patient\", str(n+1), \"\\n  Features:\",list(X[n]), \"\\n  Label:\", y[n])","3c2694d0":"from matplotlib import pyplot as plt\n%matplotlib inline\n\nfeatures = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']\nfor col in features:\n    diabetes.boxplot(column=col, by='Outcome', figsize=(6,6))\n    plt.title(col)\nplt.show()","1940976f":"from sklearn.model_selection import train_test_split\n\n# Split data 70%-30% into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\nprint ('Training cases: %d\\nTest cases: %d' % (X_train.shape[0], X_test.shape[0]))","0f37c2e2":"# Train the model\nfrom sklearn.linear_model import LogisticRegression\n\n# Set regularization rate\nreg = 0.01\n\n# train a logistic regression model on the training set\nmodel = LogisticRegression(C=1\/reg, solver=\"liblinear\").fit(X_train, y_train)\nprint (model)","9c10b463":"predictions = model.predict(X_test)\nprint('Predicted labels: ', predictions)\nprint('Actual labels:    ' ,y_test)","90d6113d":"from sklearn.metrics import accuracy_score\n\nprint('Accuracy: ', accuracy_score(y_test, predictions))","a5617749":"from sklearn. metrics import classification_report\n\nprint(classification_report(y_test, predictions))","1328069d":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Overall Precision:\",precision_score(y_test, predictions))\nprint(\"Overall Recall:\",recall_score(y_test, predictions))","5eb68fc4":"y_scores = model.predict_proba(X_test)\nprint(y_scores[:10,])","37a0a651":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# calculate ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n\n# plot ROC curve\nfig = plt.figure(figsize=(6, 6))\n# Plot the diagonal 50% line\nplt.plot([0, 1], [0, 1], 'k--')\n# Plot the FPR and TPR achieved by our model\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","04239a9c":"from sklearn.metrics import roc_auc_score\n\nauc = roc_auc_score(y_test,y_scores[:,1])\nprint('AUC: ' + str(auc))","93c1928a":"This data consists of diagnostic information about some patients who have been tested for diabetes. Scroll to the right if necessary, and note that the final column in the dataset (**Diabetic**) contains the value ***0*** for patients who tested negative for diabetes, and ***1*** for patients who tested positive. This is the label that we will train our model to predict; most of the other columns (**Pregnancies**,**PlasmaGlucose**,**DiastolicBloodPressure**, and so on) are the features we will use to predict the **Diabetic** label.\n\nLet's separate the features from the labels - we'll call the features ***X*** and the label ***y***:","3782e3f6":"## Conclusion\nIn this notebook, we looked at the basics of binary classification by obtaining logistic regression algorithm to diabetes dataset.","b5458f1b":"The ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50\/50 random prediction; so you obviously want the curve to be higher than that (or your model is no better than simply guessing!).\n\nThe area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. The closer to 1 this value is, the better the model. Once again, scikit-Learn includes a function to calculate this metric.","f4add9f6":"The precision and recall metrics are derived from four possible prediction outcomes:\n* *True Positives*: The predicted label and the actual label are both 1.\n* *False Positives*: The predicted label is 1, but the actual label is 0.\n* *False Negatives*: The predicted label is 0, but the actual label is 1.\n* *True Negatives*: The predicted label and the actual label are both 0.\n\nThese metrics are generally tabulated for the test set and shown together as a *confusion matrix*, which takes the following form:\n\n<table style=\"border: 1px solid black;\">\n    <tr style=\"border: 1px solid black;\">\n        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TN<\/td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FP<\/td>\n    <\/tr>\n    <tr style=\"border: 1px solid black;\">\n        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FN<\/td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TP<\/td>\n    <\/tr>\n<\/table>\n\nNote that the correct (*true*) predictions form a diagonal line from top left to bottom right - these figures should be significantly higher than the *false* predictions if the model is any good.\n\nIn Python, you can use the **sklearn.metrics.confusion_matrix** function to find these values for a trained classifier:","2773cac1":"# Classification\n\n*Supervised* machine learning techniques involve training a model to operate on a set of *features* and predict a *label* using a dataset that includes some already-known label values. You can think of this function like this, in which ***y*** represents the label we want to predict and ***X*** represents the vector of features the model uses to predict it.\n\n$$y = f([x_1, x_2, x_3, ...])$$\n\n\n*Classification* is a form of supervised machine learning in which you train a model to use the features (the ***x*** values in our function) to predict a label (***y***) that calculates the probability of the observed case belonging to each of a number of possible classes, and predicting an appropriate label. The simplest form of classification is *binary* classification, in which the label is 0 or 1, representing one of two classes; for example, \"True\" or \"False\"; \"Internal\" or \"External\"; \"Profitable\" or \"Non-Profitable\"; and so on. \n\n## Binary Classification\n\nIn this notebook, we will focus on an example of *binary classification*, where the model must predict a label that belongs to one of two classes. In this exercise, we'll train a binary classifier to predict whether or not a patient should be tested for diabetes based on some medical data.","d229916a":"The decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilities are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the *true positive rate* (which is another name for recall) and the *false positive rate* for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a *received operator characteristic (ROC) chart*, like this:","c6ce9b8d":"Now we've trained the model using the training data, we can use the test data we held back to evaluate how well it predicts. Again, **scikit-learn** can help us do this. Let's start by using the model to predict labels for our test set, and compare the predicted labels to the known labels:","7468f35f":"For some of the features, there's a noticeable difference in the distribution for each label value. In particular, **Pregnancies** and **Age** show markedly different distributions for diabetic patients than for non-diabetic patients. These features may help predict whether or not a patient is diabetic.\n\n### Split the data\n\nOur dataset includes known values for the label, so we can use this to train a classifier so that it finds a statistical relationship between the features and the label value; but how will we know if our model is any good? How do we know it will predict correctly when we use it with new data that it wasn't trained with? Well, we can take advantage of the fact we have a large dataset with known label values, use only some of it to train the model, and hold back some to test the trained model - enabling us to compare the predicted labels with the already known labels in the test set.\n\nIn Python, the **scikit-learn** package contains a large number of functions we can use to build a machine learning model - including a **train_test_split** function that ensures we get a statistically random split of training and test data. We'll use that to split the data into 70% for training and hold back 30% for testing.","3bf93e03":"### Explore the data\n\nRun the following cell to load a CSV file of patent data into a **Pandas** dataframe:","e6085367":"### Train and Evaluate a Binary Classification Model\nOK, now we're ready to train our model by fitting the training features (**X_train**) to the training labels (**y_train**). There are various algorithms we can use to train the model. In this example, we'll use *Logistic Regression*, which (despite its name) is a well-established algorithm for classification. In addition to the training features and labels, we'll need to set a *regularization* parameter. This is used to counteract any bias in the sample, and help the model generalize well by avoiding *overfitting* the model to the training data.\n\n> **Note**: Parameters for machine learning algorithms are generally referred to as *hyperparameters* (to a data scientist, *parameters* are values in the data itself - *hyperparameters* are defined externally from the data!)","43c403be":"The accuracy is returned as a decimal value - a value of 1.0 would mean that the model got 100% of the predictions right; while an accuracy of 0.0 is, well, pretty useless! \n\n\n## Summary\n\nHere we prepared our data by splitting it into test and train datasets, and applied logistic regression - a way of applying binary labels to our data. Our model was able to predict whether patients had diabetes with what appears like reasonable accuracy. But is this good enough? Further, we will look at alternatives to accuracy that can be much more useful in machine learning.","d44ca042":"Now let's compare the feature distributions for each label value.","07aa6893":"The arrays of labels are too long to be displayed in the notebook output, so we can only compare a few values. Even if we printed out all of the predicted and actual labels, there are too many of them to make this a sensible way to evaluate the model. Fortunately, scikit-learn has a few more tricks up its sleeve, and it provides some metrics that we can use to evaluate the model.\n\nThe most obvious thing you might want to do is to check the accuracy of the predictions - in simple terms, what proportion of the labels did the model predict correctly?","1ed169ee":"The classification report includes the following metrics for each class  (0 and 1)\n\n> note that the header row may not line up with the values!\n\n* *Precision*: Of the predictions the model made for this class, what proportion were correct?\n* *Recall*: Out of all of the instances of this class in the test dataset, how many did the model identify?\n* *F1-Score*: An average metric that takes both precision and recall into account.\n* *Support*: How many instances of this class are there in the test dataset?\n\nThe classification report also includes averages for these metrics, including a weighted average that allows for the imbalance in the number of cases of each class.\n\nBecause this is a *binary* classification problem, the ***1*** class is considered *positive* and its precision and recall are particularly interesting - these in effect answer the questions:\n\n- Of all the patients the model predicted are diabetic, how many are actually diabetic?\n- Of all the patients that are actually diabetic, how many did the model identify?\n\nYou can retrieve these values on their own by using the **precision_score** and **recall_score** metrics in scikit-learn (which by default assume a binary classification model)."}}