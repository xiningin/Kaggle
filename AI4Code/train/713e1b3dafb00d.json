{"cell_type":{"f14d8a70":"code","084dc5e7":"code","f33df47b":"code","0488dfc1":"code","fb1918b3":"code","f037f840":"code","44442039":"code","a29463b6":"code","66e56029":"code","70cfee60":"code","c4217475":"code","f0735225":"code","886dca52":"code","ff23787f":"markdown","e5810149":"markdown","d1989780":"markdown","d9700e80":"markdown","7e2b1f18":"markdown","eb637573":"markdown","ea5bb53d":"markdown","d73744d8":"markdown","824639f8":"markdown"},"source":{"f14d8a70":"\nfrom absl import logging\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\n\nfrom textblob import TextBlob\n\n\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.optimizers import Adam,RMSprop,Adagrad\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Concatenate\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import layers","084dc5e7":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","f33df47b":"%%time\nmodule_url = 'https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5'\nembed = hub.KerasLayer(module_url, trainable=False, name='USE_embedding')","0488dfc1":"# Import the Universal Sentence Encoder's TF Hub module\ndef get_sentiment(message):\n    result = TextBlob(message).sentiment.subjectivity\n    return result\ndef get_sentiment2(message):\n    result = TextBlob(message).sentiment.polarity\n    return result+1\n# Reduce logging output.\ndef get_sentese_embedding(messages):\n    return embed(messages)\n\ndef process_text(text):\n    text = text.encode('ascii', errors='ignore').decode()\n    text = text.lower()\n    text = re.sub(r'http\\S+', ' ', text)\n    text = re.sub(r'#+', ' ', text )\n    text = re.sub(r'@[A-Za-z0-9]+', ' ', text)\n    text = re.sub(r\"([A-Za-z]+)'s\", r\"\\1 is\", text)\n    text = re.sub(r\"\\'s\", \" \", text)     \n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"won't\", \"will not \", text)\n    text = re.sub(r\"isn't\", \"is not \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub(r'\\d+', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip()\n    return text","fb1918b3":"train['text'] = train.text.apply(lambda x: process_text(x))\ntest['text'] = test.text.apply(lambda x: process_text(x))\n\ntrain['count'] = train.text.apply(lambda x: len(x.split()))\ntest['count'] = test.text.apply(lambda x: len(x.split()))","f037f840":"sentiments_list = np.vectorize(get_sentiment)(train.text.values).reshape(-1,1)\nsentiments_list2 = np.vectorize(get_sentiment2)(train.text.values).reshape(-1,1)\nprint(sentiments_list.shape)\n\ntest_sentiments_list = np.vectorize(get_sentiment)(test.text.values).reshape(-1,1)\ntest_sentiments_list2 = np.vectorize(get_sentiment2)(test.text.values).reshape(-1,1)\ntest_sentiments_list.shape","44442039":"embeddings = get_sentese_embedding(train.text.values).numpy()\nprint(embeddings.shape)\n\ntest_embeddings = get_sentese_embedding(test.text.values).numpy()\nprint(test_embeddings.shape)","a29463b6":"final = pd.concat([pd.DataFrame(embeddings),pd.DataFrame(sentiments_list),pd.DataFrame(sentiments_list2)],axis=1)\nprint(final.values.shape)\n\ntest_final = pd.concat([pd.DataFrame(test_embeddings),pd.DataFrame(test_sentiments_list),pd.DataFrame(test_sentiments_list2)],axis=1)\ntest_final.values.shape","66e56029":"train_data = final.values\ntrain_labels = train.target.values\n\n\ntest_data = test_final.values\ntrain_data.shape,train_labels.shape,test_data.shape","70cfee60":"def build_model():\n    model = Sequential([\n        Input(shape=(514,)),\n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n","c4217475":"model = build_model()\nmodel.summary()\n\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntrain_history = model.fit(\n    train_data, train_labels,\n    validation_split=0.3,\n    epochs=100,\n    callbacks=[checkpoint,callback],\n    batch_size=128\n)","f0735225":"model.load_weights('model.h5')\ntest_pred = model.predict(test_data)","886dca52":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","ff23787f":"Read Files","e5810149":"lets get the sentiments and put them in array","d1989780":"lets cleanup the text","d9700e80":"lets get the embeddings","7e2b1f18":"concatinate all features we have so far to make a training data","eb637573":"**Building The NN Model**","ea5bb53d":"**Test and Submit **","d73744d8":"Setup our Sentense embedder ","824639f8":"Add Some Helper functions to :\n1. get the sentiment \/ subjectivity\n2. get the sentiment \/ polarity\n3. get the sentense embedding\n4. preprocess text"}}