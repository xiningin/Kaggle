{"cell_type":{"90248a59":"code","71afed37":"code","8b96af7d":"code","b49a0353":"code","7b923304":"code","1946c684":"code","3314dacb":"code","f64aa852":"code","c3191a5c":"code","0e15db4e":"code","2959750a":"code","1e6c54aa":"code","fcc16a93":"code","08621656":"code","87fda411":"code","f1badf1b":"code","5a228df0":"code","0696752e":"code","d4b611bf":"code","00d6abd0":"code","0e1736ba":"code","c4c641c0":"code","482fea62":"code","13b9ce05":"code","3e17fd56":"code","4716931e":"code","d20472ae":"code","144046c9":"code","e5bc223c":"code","5c17ea26":"code","d22d9667":"code","9a3db13e":"code","014c3f7b":"code","6bdf191f":"code","2ded36c9":"code","3489d419":"code","8c54c1cc":"code","8f71e807":"code","774a62c2":"code","f11afe58":"code","fa6bf935":"code","5ebb4bc4":"code","edf900e6":"code","1084297e":"code","b2d4e03b":"code","87d48a63":"code","fe8a1ba5":"code","5f87085a":"code","bce2d5c5":"code","7bdde2a6":"code","459dea76":"code","88eff793":"code","a83ed2fe":"code","b49f1fbe":"code","10de237a":"code","3a218f94":"code","6ae1dfc1":"code","aa26aef8":"code","ae35d222":"code","b7b234b9":"code","a9ba01b1":"code","7155472d":"code","3909e5bf":"code","2c82d9dc":"code","878b8094":"code","dd3ad516":"code","d55a40ac":"code","4e5df4b9":"code","fa017ef0":"code","a2bc98af":"code","0f735846":"code","f85a104d":"code","3c13652a":"code","e6846fe0":"code","cf35dc8d":"code","00b0a381":"code","2685b4ef":"code","c20db5c9":"code","c3106e33":"code","531a833e":"code","c39607d4":"code","e2852b4b":"code","ebdebbf5":"code","f69da4e1":"code","1d4c8c36":"markdown","75e2d88f":"markdown","dc6e4808":"markdown","2b3252c1":"markdown","81f3f304":"markdown","18e39a70":"markdown","bf51b6dd":"markdown","547963c9":"markdown","1facd3f9":"markdown","a9454b66":"markdown","667c4af0":"markdown","9eacb77c":"markdown"},"source":{"90248a59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","71afed37":"import zipfile\nwith zipfile.ZipFile('\/kaggle\/input\/platesv2\/plates.zip', 'r') as zip_obj:\n   # Extract all the contents of zip file in current directory\n   zip_obj.extractall('\/kaggle\/working\/')","8b96af7d":"print('After zip extraction:')\nprint(os.listdir(\"\/kaggle\/working\/\"))","b49a0353":"data_root = '\/kaggle\/working\/plates\/'\nprint(os.listdir(data_root))","7b923304":"import shutil \nfrom tqdm import tqdm\n\ntrain_dir = 'train'\nval_dir = 'val'\n\nclass_names = ['cleaned', 'dirty']\n\nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n\nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","1946c684":"!ls train","3314dacb":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread","f64aa852":"test_path = data_root+'test'\ntrain_path = data_root+'train'","c3191a5c":"test_path","0e15db4e":"# os.listdir(test_path)","2959750a":"os.listdir(train_path)","1e6c54aa":"os.listdir(train_path+'\/dirty')[0]","fcc16a93":"one_dirty_plate = train_path+'\/dirty'","08621656":"one_dirty_plate = one_dirty_plate+'\/0005.jpg'","87fda411":"one_dirty_plate","f1badf1b":"one_dirty_plate= imread(one_dirty_plate)","5a228df0":"plt.imshow(one_dirty_plate)","0696752e":"train_path","d4b611bf":"os.listdir(train_path+'\/cleaned')[0]","00d6abd0":"one_clean_plate = train_path+'\/cleaned'","0e1736ba":"one_clean_plate = one_clean_plate+'\/0005.jpg'","c4c641c0":"one_clean_plate= imread(one_clean_plate)","482fea62":"plt.imshow(one_clean_plate)","13b9ce05":"dirty = '\/kaggle\/working\/train\/dirty'\ncleaned = '\/kaggle\/working\/train\/cleaned'","3e17fd56":"len(os.listdir(dirty))","4716931e":"len(os.listdir(cleaned))","d20472ae":"one_clean_plate.shape, one_dirty_plate.shape","144046c9":"image_shape = one_clean_plate.shape","e5bc223c":"image_shape","5c17ea26":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","d22d9667":"image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.10, # Shift the pic width by a max of 5%\n                               height_shift_range=0.10, # Shift the pic height by a max of 5%\n                               rescale=1\/255, # Rescale the image by normalzing it.\n                               shear_range=0.1, # Shear means cutting away part of the image (max 10%)\n                               zoom_range=0.1, # Zoom in by 10% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n                              )","9a3db13e":"plt.imshow(one_dirty_plate)","014c3f7b":"plt.imshow(image_gen.random_transform(one_dirty_plate))","6bdf191f":"plt.imshow(image_gen.random_transform(one_dirty_plate))","2ded36c9":"train_path = '\/kaggle\/working\/train\/'\ntest_path = '\/kaggle\/working\/val\/'","3489d419":"image_gen.flow_from_directory(train_path);","8c54c1cc":"image_gen.flow_from_directory(test_path)","8f71e807":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D","774a62c2":"#https:\/\/stats.stackexchange.com\/questions\/148139\/rules-for-selecting-convolutional-neural-network-hyperparameters\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu',))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Flatten())\n\n\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\n\n# Dropouts help reduce overfitting by randomly turning neurons off during training.\n# Here we say randomly turn off 50% of neurons.\nmodel.add(Dropout(0.5))\n\n# Last layer, remember its binary so we use sigmoid\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","f11afe58":"model.summary()","fa6bf935":"from tensorflow.keras.callbacks import EarlyStopping","5ebb4bc4":"early_stop = EarlyStopping(monitor='val_loss',patience=2)","edf900e6":"help(image_gen.flow_from_directory)","1084297e":"batch_size = 8","b2d4e03b":"train_image_gen = image_gen.flow_from_directory(train_path,\n                                               target_size=image_shape[:2],\n                                                color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary')","87d48a63":"test_image_gen = image_gen.flow_from_directory(test_path,\n                                               target_size=image_shape[:2],\n                                               color_mode='rgb',\n                                               batch_size=batch_size,\n                                               class_mode='binary',shuffle=False)","fe8a1ba5":"train_image_gen.class_indices","5f87085a":"import warnings\nwarnings.filterwarnings('ignore')","bce2d5c5":"results = model.fit_generator(train_image_gen,epochs=40,\n                              validation_data=test_image_gen,\n                             callbacks=[early_stop])","7bdde2a6":"losses = pd.DataFrame(model.history.history)","459dea76":"losses[['loss','val_loss']].plot()","88eff793":"model.metrics_names","a83ed2fe":"model.evaluate_generator(test_image_gen)","b49f1fbe":"from tensorflow.keras.preprocessing import image","10de237a":"pred_probabilities = model.predict_generator(test_image_gen)","3a218f94":"pred_probabilities","6ae1dfc1":"test_image_gen.classes","aa26aef8":"predictions = pred_probabilities > 0.5","ae35d222":"from sklearn.metrics import classification_report,confusion_matrix","b7b234b9":"print(classification_report(test_image_gen.classes,predictions))","a9ba01b1":"confusion_matrix(test_image_gen.classes,predictions)","7155472d":"results = model.fit_generator(train_image_gen,epochs=40,\n                              validation_data=test_image_gen)","3909e5bf":"losses = pd.DataFrame(model.history.history)","2c82d9dc":"losses[['loss','val_loss']].plot()","878b8094":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=256, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=512, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n'''\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n'''\n\nmodel.add(Flatten())\n\n\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\n\n# Dropouts help reduce overfitting by randomly turning neurons off during training.\n# Here we say randomly turn off 50% of neurons.\nmodel.add(Dropout(0.5))\n\n# Last layer, remember its binary so we use sigmoid\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","dd3ad516":"results = model.fit_generator(train_image_gen,epochs=40,\n                              validation_data=test_image_gen)","d55a40ac":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot()","4e5df4b9":"model.evaluate_generator(test_image_gen)","fa017ef0":"pred_probabilities = model.predict_generator(test_image_gen)","a2bc98af":"pred_probabilities","0f735846":"predictions = pred_probabilities > 0.5","f85a104d":"confusion_matrix(test_image_gen.classes,predictions)","3c13652a":"one_clean_plate = '\/kaggle\/working\/plates\/train\/cleaned\/0000.jpg'","e6846fe0":"one_clean_plate = image.load_img(one_clean_plate,target_size=image_shape)","cf35dc8d":"one_clean_plate","00b0a381":"type(one_clean_plate)","2685b4ef":"one_clean_plate = image.img_to_array(one_clean_plate)","c20db5c9":"type(one_clean_plate)","c3106e33":"one_clean_plate.shape","531a833e":"one_clean_plate = np.expand_dims(one_clean_plate, axis=0)","c39607d4":"one_clean_plate.shape","e2852b4b":"model.predict(one_clean_plate)","ebdebbf5":"train_image_gen.class_indices","f69da4e1":"test_image_gen.class_indices","1d4c8c36":"# Predicting on a image","75e2d88f":"## Very Low accuracy. Lets train without early stopping","dc6e4808":"# Creating the model","2b3252c1":"### Our model is tottally not confident.","81f3f304":"### A little bit better this time","18e39a70":"## Early Stopping","bf51b6dd":"## Awful. Lets tune our model","547963c9":"## Lets check out the number of images in the training set","1facd3f9":"## Training the model","a9454b66":"# Preparing the data for model","667c4af0":"## Generating many manipulated images from a directory","9eacb77c":"## I will work on improving this model later. "}}