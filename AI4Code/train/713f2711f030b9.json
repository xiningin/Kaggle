{"cell_type":{"62158d8c":"code","d536f710":"code","8a6e294f":"code","e15cec56":"code","8711d610":"code","a65432a5":"code","07cd9db4":"code","22802d14":"code","56035356":"code","ae89891c":"code","5b1e631b":"code","01c4391d":"code","e607bc82":"code","848bd64b":"code","99d5c59d":"code","73259552":"code","f3c303f5":"code","e012fc87":"code","d790ddae":"code","c4c25ed6":"code","13b7d472":"markdown","260e53e7":"markdown","ead193a1":"markdown","0f768925":"markdown","c00bb4f1":"markdown","73de763b":"markdown","3664db85":"markdown","92fee22c":"markdown","9d129660":"markdown","b7865e18":"markdown","899bca51":"markdown"},"source":{"62158d8c":"#@title Check if GPU (driver) is avaiiable (you do not want to run this on CPU, trust me)\n!nvcc --version\n!nvidia-smi","d536f710":"#@title Clone\/Install all dependencies\n!git clone https:\/\/github.com\/asigalov61\/midi-neural-processor\n!git clone https:\/\/github.com\/asigalov61\/MusicTransformer-Pytorch\n!pip install tqdm\n!pip install progress\n!pip install pretty-midi\n!pip install pypianoroll\n!pip install matplotlib\n!pip install librosa\n!pip install scipy\n!pip install pillow\n!apt install fluidsynth #Pip does not work for some reason. Only apt works\n!pip install midi2audio\n!pip install mir_eval\n!cp \/usr\/share\/sounds\/sf2\/FluidR3_GM.sf2 \/content\/font.sf2","8a6e294f":"#@title Import all needed modules\nimport numpy as np\nimport pickle\nimport os\nimport sys\nimport math\nimport random\n# For plotting\nimport pypianoroll\nfrom pypianoroll import Multitrack, Track\nimport matplotlib\nimport matplotlib.pyplot as plt\n#matplotlib.use('SVG')\n#%matplotlib inline\n#matplotlib.get_backend()\nimport mir_eval.display\nimport librosa\nimport librosa.display\n# For rendering output audio\nimport pretty_midi\nfrom midi2audio import FluidSynth\nfrom google.colab import output\nfrom IPython.display import display, Javascript, HTML, Audio","e15cec56":"#@title (Optional) Pre-trained models download (2 models trained for 100 epochs to 1.968 FLoss and 0.420 acc)\n!mkdir \/content\/MusicTransformer-Pytorch\/rpr\n!mkdir \/content\/MusicTransformer-Pytorch\/rpr\/results\n%cd \/content\/MusicTransformer-Pytorch\/rpr\/results\n!wget 'https:\/\/superpiano.s3-us-west-1.amazonaws.com\/SuperPiano3models.zip'\n!unzip SuperPiano3models.zip\n%cd \/content\/MusicTransformer-Pytorch\/","8711d610":"#@title Download Google Magenta MAESTRO v.2.0.0 Piano MIDI Dataset (~1300 MIDIs)\n%cd \/content\/MusicTransformer-Pytorch\/dataset\/\n!wget 'https:\/\/storage.googleapis.com\/magentadata\/datasets\/maestro\/v2.0.0\/maestro-v2.0.0-midi.zip'\n!unzip maestro-v2.0.0-midi.zip\n%cd \/content\/MusicTransformer-Pytorch\/","a65432a5":"#@title Prepare directory sctructure and MIDI processor\n%cd \/content\/\n!mv midi-neural-processor midi_processor\n%cd \/content\/MusicTransformer-Pytorch\/","07cd9db4":"#@title Process MAESTRO MIDI DataSet\n!python3 preprocess_midi.py '\/content\/MusicTransformer-Pytorch\/dataset\/maestro-v2.0.0'","22802d14":"#@title Create directory structure for the DataSet and prep MIDI processor\n\n!mkdir '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/'\n!mkdir '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/train'\n!mkdir '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/test'\n!mkdir '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/val'\n!mkdir '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/custom_midis'\n\n%cd \/content\/\n!mv midi-neural-processor midi_processor\n%cd \/content\/MusicTransformer-Pytorch\/","56035356":"#@title Upload your custom MIDI DataSet to created \"dataset\/e_piano\/custom_midis\" folder through this cell or manually through any other means. You can also use ready-to-use DataSets below\nfrom google.colab import files\n%cd '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/custom_midis'\nuploaded = files.upload()\n\nfor fn in uploaded.keys():\n  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n      name=fn, length=len(uploaded[fn])))","ae89891c":"#@title (The Best Choice\/Works best stand-alone) Super Piano 2 Original 2500 MIDIs of Piano Music\n%cd \/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/custom_midis\n!wget 'https:\/\/github.com\/asigalov61\/SuperPiano\/raw\/master\/Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'\n!unzip -j 'Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip'\n!rm Super_Piano_2_MIDI_DataSet_CC_BY_NC_SA.zip","5b1e631b":"#@title (Second Best Choice\/Works best stand-alone) Alex Piano Only Drafts Original 1500 MIDIs \n%cd \/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/custom_midis\n!wget 'https:\/\/github.com\/asigalov61\/AlexMIDIDataSet\/raw\/master\/AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip'\n!unzip -j 'AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip'\n!rm AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip","01c4391d":"#@title Process your custom MIDI DataSet :)\n%cd \/content\/MusicTransformer-Pytorch\nfrom processor import encode_midi\n\nimport os\nimport random\n\n\n\n%cd '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/custom_midis'\n\ncustom_MIDI_DataSet_dir = '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/custom_midis'\n\ntrain_dir = '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/train' # split_type = 0\ntest_dir = '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/test' # split_type = 1  \nval_dir = '\/content\/MusicTransformer-Pytorch\/dataset\/e_piano\/val' # split_type = 2\n\ntotal_count = 0\ntrain_count = 0\nval_count   = 0\ntest_count  = 0\n\nf_ext = '.pickle'\nfileList = os.listdir(custom_MIDI_DataSet_dir)\nfor file in fileList:\n     # we gonna split by a random selection for now\n    \n    split = random.randint(1, 2)\n    if (split == 0):\n         o_file = os.path.join(train_dir, file+f_ext)\n         train_count += 1\n\n    elif (split == 2):\n         o_file0 = os.path.join(train_dir, file+f_ext)\n         train_count += 1\n         o_file = os.path.join(val_dir, file+f_ext)\n         val_count += 1\n\n    elif (split == 1):\n         o_file0 = os.path.join(train_dir, file+f_ext)\n         train_count += 1\n         o_file = os.path.join(test_dir, file+f_ext)\n         test_count += 1\n\n    prepped = encode_midi(file)\n    o_stream = open(o_file0, \"wb\")\n    pickle.dump(prepped, o_stream)\n    o_stream.close()\n\n    prepped = encode_midi(file)\n    o_stream = open(o_file, \"wb\")\n    pickle.dump(prepped, o_stream)\n    o_stream.close()\n\n    print(file)\n    print(o_file)\n    print('Coverted!')\n\nprint('Done')\nprint(\"Num Train:\", train_count)\nprint(\"Num Val:\", val_count)\nprint(\"Num Test:\", test_count)\nprint(\"Total Count:\", train_count)\n\n%cd \/content\/MusicTransformer-Pytorch","e607bc82":"#@title Activate Tensorboard Graphs\/Stats to monitor\/evaluate model perfomance during and after training runs\n# Load the TensorBoard notebook extension\n%reload_ext tensorboard\nimport tensorflow as tf\nimport datetime, os\n%tensorboard --logdir \/content\/MusicTransformer-Pytorch\/rpr","848bd64b":"#@title Start to Train the Model\nbatch_size = 4 #@param {type:\"slider\", min:0, max:8, step:1}\nnumber_of_training_epochs = 100 #@param {type:\"slider\", min:0, max:200, step:1}\nmaximum_output_MIDI_sequence = 2048 #@param {type:\"slider\", min:0, max:8192, step:128}\n!python3 train.py -output_dir rpr --rpr -batch_size=$batch_size -epochs=$number_of_training_epochs -max_sequence=$maximum_output_MIDI_sequence #-n_layers -num_heads -d_model -dim_feedforward","99d5c59d":"#@title Evaluate Best Resulting Accuracy Model (best_acc_weights.pickle)\n!python3 evaluate.py -model_weights rpr\/results\/best_acc_weights.pickle --rpr","73259552":"#@title Evaluate Best Resulting Loss Model (best_loss_weights.pickle)\n!python3 evaluate.py -model_weights rpr\/results\/best_loss_weights.pickle --rpr","f3c303f5":"#@title Graph the results\nimport argparse\nimport os\nimport csv\nimport math\nimport matplotlib.pyplot as plt\n\nRESULTS_FILE = \"results.csv\"\nEPOCH_IDX = 0\nLR_IDX = 1\nEVAL_LOSS_IDX = 4\nEVAL_ACC_IDX = 5\n\nSPLITTER = '?'\n\n\ndef graph_results(input_dirs=\"\/content\/MusicTransformer-Pytorch\/rpr\/results\", output_dir=None, model_names=None, epoch_start=0, epoch_end=None):\n    \"\"\"\n    ----------\n    Author: Damon Gwinn\n    ----------\n    Graphs model training and evaluation data\n    ----------\n    \"\"\"\n\n    input_dirs = input_dirs.split(SPLITTER)\n\n    if(model_names is not None):\n        model_names = model_names.split(SPLITTER)\n        if(len(model_names) != len(input_dirs)):\n            print(\"Error: len(model_names) != len(input_dirs)\")\n            return\n\n    #Initialize Loss and Accuracy arrays\n    loss_arrs = []\n    accuracy_arrs = []\n    epoch_counts = []\n    lrs = []\n\n    for input_dir in input_dirs:\n        loss_arr = []\n        accuracy_arr = []\n        epoch_count = []\n        lr_arr = []\n\n        f = os.path.join(input_dir, RESULTS_FILE)\n        with open(f, \"r\") as i_stream:\n            reader = csv.reader(i_stream)\n            next(reader)\n\n            lines = [line for line in reader]\n\n        if(epoch_end is None):\n            epoch_end = math.inf\n\n        epoch_start = max(epoch_start, 0)\n        epoch_start = min(epoch_start, epoch_end)\n\n        for line in lines:\n            epoch = line[EPOCH_IDX]\n            lr = line[LR_IDX]\n            accuracy = line[EVAL_ACC_IDX]\n            loss = line[EVAL_LOSS_IDX]\n\n            if(int(epoch) >= epoch_start and int(epoch) < epoch_end):\n                accuracy_arr.append(float(accuracy))\n                loss_arr.append(float(loss))\n                epoch_count.append(int(epoch))\n                lr_arr.append(float(lr))\n\n        loss_arrs.append(loss_arr)\n        accuracy_arrs.append(accuracy_arr)\n        epoch_counts.append(epoch_count)\n        lrs.append(lr_arr)\n\n    if(output_dir is not None):\n        try:\n            os.mkdir(output_dir)\n        except OSError:\n            print (\"Creation of the directory %s failed\" % output_dir)\n        else:\n            print (\"Successfully created the directory %s\" % output_dir)\n\n    ##### LOSS #####\n    for i in range(len(loss_arrs)):\n        if(model_names is None):\n            name = None\n        else:\n            name = model_names[i]\n\n        #Create and save plots to output folder\n        plt.plot(epoch_counts[i], loss_arrs[i], label=name)\n        plt.title(\"Loss Results\")\n        plt.ylabel('Loss (Cross Entropy)')\n        plt.xlabel('Epochs')\n        fig1 = plt.gcf()\n\n    plt.legend(loc=\"upper left\")\n\n    if(output_dir is not None):\n        fig1.savefig(os.path.join(output_dir, 'loss_graph.png'))\n\n    plt.show()\n\n    ##### ACCURACY #####\n    for i in range(len(accuracy_arrs)):\n        if(model_names is None):\n            name = None\n        else:\n            name = model_names[i]\n\n        #Create and save plots to output folder\n        plt.plot(epoch_counts[i], accuracy_arrs[i], label=name)\n        plt.title(\"Accuracy Results\")\n        plt.ylabel('Accuracy')\n        plt.xlabel('Epochs')\n        fig2 = plt.gcf()\n\n    plt.legend(loc=\"upper left\")\n\n    if(output_dir is not None):\n        fig2.savefig(os.path.join(output_dir, 'accuracy_graph.png'))\n\n    plt.show()\n\n    ##### LR #####\n    for i in range(len(lrs)):\n        if(model_names is None):\n            name = None\n        else:\n            name = model_names[i]\n\n        #Create and save plots to output folder\n        plt.plot(epoch_counts[i], lrs[i], label=name)\n        plt.title(\"Learn Rate Results\")\n        plt.ylabel('Learn Rate')\n        plt.xlabel('Epochs')\n        fig2 = plt.gcf()\n\n    plt.legend(loc=\"upper left\")\n\n    if(output_dir is not None):\n        fig2.savefig(os.path.join(output_dir, 'lr_graph.png'))\n\n    plt.show()\ngraph_results('\/content\/MusicTransformer-Pytorch\/rpr\/results', model_names='rpr')","e012fc87":"#@title Generate, Plot, Graph, Save, Download, and Render the resulting output\nnumber_of_tokens_to_generate = 2048 #@param {type:\"slider\", min:1, max:2048, step:1}\npriming_sequence_length = 17 #@param {type:\"slider\", min:1, max:2048, step:8}\nmaximum_possible_output_sequence = 2048 #@param {type:\"slider\", min:0, max:2048, step:8}\nselect_model = \"\/content\/MusicTransformer-Pytorch\/rpr\/results\/best_loss_weights.pickle\" #@param [\"\/content\/MusicTransformer-Pytorch\/rpr\/results\/best_acc_weights.pickle\", \"\/content\/MusicTransformer-Pytorch\/rpr\/results\/best_loss_weights.pickle\"]\ncustom_MIDI = \"\" #@param {type:\"string\"}\n\nimport processor\nfrom processor import encode_midi, decode_midi\n\n!python generate.py -output_dir output -model_weights=$select_model --rpr -target_seq_length=$number_of_tokens_to_generate -num_prime=$priming_sequence_length -max_sequence=$maximum_possible_output_sequence $custom_MIDI #\n\nprint('Successfully exported the output to output folder. To primer.mid and rand.mid')\n\n# set the src and play\nFluidSynth(\"\/content\/font.sf2\").midi_to_audio('\/content\/MusicTransformer-Pytorch\/output\/rand.mid', '\/content\/MusicTransformer-Pytorch\/output\/output.wav')\n\nfrom google.colab import files\nfiles.download('\/content\/MusicTransformer-Pytorch\/output\/rand.mid')\nfiles.download('\/content\/MusicTransformer-Pytorch\/output\/primer.mid')\n\nAudio('\/content\/MusicTransformer-Pytorch\/output\/output.wav')\n","d790ddae":"#@title Plot and Graph the Output :)\ngraphs_length_inches = 18 #@param {type:\"slider\", min:0, max:20, step:1}\nnotes_graph_height = 6 #@param {type:\"slider\", min:0, max:20, step:1}\nhighest_displayed_pitch = 120 #@param {type:\"slider\", min:1, max:128, step:1}\nlowest_displayed_pitch = 10 #@param {type:\"slider\", min:1, max:128, step:1}\npiano_roll_color_map = \"Blues\"\n\nimport librosa\nimport numpy as np\nimport pretty_midi\nimport pypianoroll\nfrom pypianoroll import Multitrack, Track\nimport matplotlib\nimport matplotlib.pyplot as plt\n#matplotlib.use('SVG')\n# For plotting\nimport mir_eval.display\nimport librosa.display\n%matplotlib inline\n\n\nmidi_data = pretty_midi.PrettyMIDI('\/content\/MusicTransformer-Pytorch\/output\/rand.mid')\n\ndef plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n    # Use librosa's specshow function for displaying the piano roll\n    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n\n\n\nroll = np.zeros([int(graphs_length_inches), 128])\n# Plot the output\n\ntrack = Multitrack('\/content\/MusicTransformer-Pytorch\/output\/rand.mid', name='track')\nplt.figure(figsize=[graphs_length_inches, notes_graph_height])\nfig, ax = track.plot()\nfig.set_size_inches(graphs_length_inches, notes_graph_height)\nplt.figure(figsize=[graphs_length_inches, notes_graph_height])\nax2 = plot_piano_roll(midi_data, int(lowest_displayed_pitch), int(highest_displayed_pitch))\nplt.show(block=False)","c4c25ed6":"from google.colab import drive\ndrive.mount('\/content\/drive')","13b7d472":"#Please note that you MUST DOWNLOAD AND PROCESS ONE OF THE DATASETS TO TRAIN OR TO USE PRE-TRAINED MODEL as it primes the model from DATASET files.","260e53e7":"# Generate and Explore the output :)","ead193a1":"To have the model continue your custom MIDI enter the following into the custom_MIDI field below:\n\n-primer_file '\/content\/some_dir\/some_seed_midi.mid'\n\nFor example: -primer_file '\/content\/MusicTransformer-Pytorch\/seed.mid'","0f768925":"For now, we are going to split the dataset by random into \"test\"\/\"val\" dirs which is not ideal. So feel free to modify the code to your liking to achieve better training results with this implementation.","c00bb4f1":"<a href=\"https:\/\/colab.research.google.com\/github\/asigalov61\/SuperPiano\/blob\/master\/Super_Piano_3.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","73de763b":"#Train the Model","3664db85":"###Evaluate the resulted models","92fee22c":"#Option 2: Your own Custom MIDI DataSet","9d129660":"# Super Piano 3: Google Music Transformer\n## Generating Music with Long-Term structure\n### Based on 2019 ICLR paper by Cheng-Zhi Anna Huang, Google Brain and Damon Gwinn's code\/repo https:\/\/github.com\/gwinndr\/MusicTransformer-Pytorch\n\nHuge thanks go out to the following people who contributed the code\/repos used in this colab. Additional contributors are listed in the code as well.\n\n1) Kevin-Yang https:\/\/github.com\/jason9693\/midi-neural-processor\n\n2) gudgud96 for fixing Kevin's MIDI Encoder properly https:\/\/github.com\/gudgud96\n\n2) jinyi12, Zac Koh, Akamight, Zhang https:\/\/github.com\/COMP6248-Reproducability-Challenge\/music-transformer-comp6248\n\nThank you so much for your hard work and for sharing it with the world :)\n","b7865e18":"#Option 1: MAESTRO DataSet","899bca51":"###Setup Environment and Dependencies. Check GPU."}}