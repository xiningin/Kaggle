{"cell_type":{"da926746":"code","e4840ae2":"code","079629ab":"code","bef3e8ba":"code","94b0e1ab":"code","466217f2":"code","960c4492":"code","3923564b":"code","96b58849":"code","4d1bb9cf":"code","9a42a3fa":"code","a227b7cc":"code","e3238ad4":"code","5e946fef":"code","64bd8816":"code","3b42a973":"code","cfeeae78":"code","d9b069a3":"code","98b7d79b":"code","7f396f2b":"code","61b0ecb1":"code","311eb71a":"code","b34e3280":"code","8fd03805":"code","dc918dce":"code","297d08c9":"code","b0cc4542":"code","dcac1c73":"code","0b81e982":"code","3a2bea7e":"code","bfda239d":"code","478b337c":"code","62c81af9":"code","ed2d5a17":"code","86175a26":"code","e9a45a68":"code","160d97f8":"code","6c3a70fb":"code","ce7236ac":"code","a80dd8eb":"code","cbca36d8":"code","6e36f389":"code","715f7c05":"markdown","72c5b791":"markdown","f21fe02c":"markdown","b492bb6c":"markdown","608f6c3d":"markdown","b2b73277":"markdown","9b1a6526":"markdown","093a9741":"markdown","3cbed21b":"markdown","7e97f73c":"markdown","bdf86600":"markdown","f6ef1168":"markdown","f35dcd92":"markdown","d9feaf79":"markdown","649c860d":"markdown","7f53a52c":"markdown","ae6d3de0":"markdown","6e7fc686":"markdown","49aa55c4":"markdown","830011dc":"markdown"},"source":{"da926746":"import re # Regular Expressions\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom gensim.models import Word2Vec\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","e4840ae2":"# Importing the dataset\ndf = pd.read_csv(\"..\/input\/amazon-music-reviews\/Musical_instruments_reviews.csv\")","079629ab":"df.shape","bef3e8ba":"df.head()","94b0e1ab":"df.dtypes","466217f2":"# I will modify overall column of the dataset to start with\n# In the overall column the ratings (given by the user) is given.\n# I will modify the column - 4 and 5 as positive rating, 1 and 2 as negative rating and 3 as neutral \n# Function to modify overall column\n\ndef change_score(rating):\n    if rating < 3:\n        return 0\n    elif rating > 3:\n        return 2\n    else:\n        return 1\n\ndf_score = df[\"overall\"]\ndf_score = df_score.map(change_score)\ndf[\"overall\"] = df_score","960c4492":"df.tail()","3923564b":"# Counting occurences of positive, negative and neutral reviews\ndf[\"overall\"].value_counts()","96b58849":"# The helpful column of the dataframe gives value - [x,y]\n# out of 'y' people 'x' found the corresponding review helpful\n# so people who found the review helful = x, and people who found the review \"not helpful\" = y-x (total people voted on the review - people who voted helful)\n# I will seprate the x and y values in different columns\n\n# Since helpful columns have values in object form, I will have to convert all the values to python list\ndef convert_to_list(str_lst):\n    str_ = str_lst.strip(\"[]\").replace(\",\",\" \")\n    lst = str_.split()\n    lst_to_int = list(map(int, lst))\n    return lst_to_int\n        \ndef total_rating(lst_rating):\n    return lst_rating[1] # y\n\ndef helpful_rating(lst_rating):\n    return lst_rating[0] # x\n\ndf[\"helpful\"] = df[\"helpful\"].map(convert_to_list) # \"[x,y]\" -> [x,y]\ndf[\"total_ratings\"] = df[\"helpful\"].map(total_rating) # y\ndf[\"helpful\"] = df[\"helpful\"].map(helpful_rating) # x\n\ndf.head()","4d1bb9cf":"# Checking for duplicate rows in columns - [\"reviewerName\", \"reviewText\", \"unixReviewTime\"]\n\nprint(\"Number rows having common values of [reviewerName, reviewText, unixReviewTime] =\", df[df.duplicated(subset=[\"reviewerName\", \"reviewText\", \"unixReviewTime\"])].shape[0])","9a42a3fa":"# Checking in columns helpful and total_ratings if helpful > total_ratings\n# If any row follows above condition, I will remove it\n\nprint(\"Number of rows, in which helpful > total_ratings =\",df[df[\"helpful\"] > df[\"total_ratings\"]].shape[0])","a227b7cc":"# Checking for null values\ndf.isnull().sum()","e3238ad4":"# Review text is the most important column for classification\n# I will remove columns having null review text\n# Missing values of reviewerName column doesn't matter, because reveiwerName doesn't contribute in determining polarity of the review\ndf.drop(df[df[\"reviewText\"].isnull()].index, axis=0, inplace=True)\ndf.reset_index(drop=True, inplace=True)","5e946fef":"# Removing HTML tags\ndef remove_html(text):\n    html_pattern = re.compile(\"<.*?>\")\n    text = re.sub(html_pattern, \" \", text) # Substitute HTML tag with space\n    return text\n\n# Removing special characters\ndef remove_spl_char(text):\n    text = re.sub(r\"[?|!|.|,|)|(|\\|\/|#|\\'|\\\"]\", r\"\", text) # All the special characters removed\n    return text\n\n# Converting to lowercase\ndef in_lowercase(text):\n    text = text.lower()\n    return text\n\n# Removing stop words\nstop_words = set(stopwords.words(\"english\")) # List of all the stop words\n\ndef remove_stopwords(text):\n    filtered_text_lst = []\n    text_lst = text.split()\n    for word in text_lst:\n        if word not in stop_words:\n            filtered_text_lst.append(word)\n        else:\n            continue\n    filtered_word = \" \".join(filtered_text_lst)\n    return filtered_word\n\nstem = PorterStemmer()\ndef stemming(text):\n    stemmed_txt_lst = []\n    text_lst = text.split()\n    for word in text_lst:\n        stemmed_word = stem.stem(word)\n        stemmed_txt_lst.append(stemmed_word)\n    stemmed_txt_lst = \" \".join(stemmed_txt_lst)\n    return stemmed_txt_lst\n    ","64bd8816":"def text_preprocessing(text):\n    rem_html_txt = remove_html(text) # Remove HTML\n    rem_spl_char_txt = remove_spl_char(rem_html_txt) # Remove Special Characters\n    lowercase_txt = in_lowercase(rem_spl_char_txt) # Conversion in lowercase\n    rem_stopwords_txt = remove_stopwords(lowercase_txt) # Remove stopwords\n    stemmed_txt = stemming(text)\n    final_txt = stemmed_txt\n    return final_txt\n\ndf[\"final_review\"] = df[\"reviewText\"].map(text_preprocessing)","3b42a973":"print(\"Before Text Preprocessing- \", \"\\n\")\nprint(df[\"reviewText\"][0], \"\\n\")\nprint(\"After Text Preprocessing- \", \"\\n\")\nprint(df[\"final_review\"][0])","cfeeae78":"reviews = []\ndef construct_reviews_lst(review):\n    review_split = review.split()\n    reviews.append(review_split)\ndf[\"final_review\"].map(construct_reviews_lst)   \n\nprint(df[\"final_review\"].iloc[0]) # Before\nreviews[0] # After","d9b069a3":"# Constructing W2V model\nw2v_model = Word2Vec(reviews, vector_size=50, min_count=5)","98b7d79b":"# Using Avg W2V for each review\ndef avg_w2v(reviews):\n    text_vector = []\n    for review in reviews:\n        review_vec_sum = np.zeros(50)\n        num_words = 0\n        for word in review:\n            try:\n                word_vec = w2v_model.wv[word]\n                review_vec_sum += word_vec\n                num_words += 1\n            except:\n                pass\n        avg_review_vector = review_vec_sum \/ num_words\n        text_vector.append(avg_review_vector)\n    return text_vector\n\ntext_vector = np.array(avg_w2v(reviews)) # Text Vector of all the reviews","7f396f2b":"print(df[\"final_review\"][0])\n\nprint(\"\\n\\nVector Representation of above text - \")\ntext_vector[0] # Text Vector of review","61b0ecb1":"# Feature name for text vector\ndef create_feature_names():\n    text_features = []\n    for index in range(1,51):\n        feature_name = \"text-feature-\"+ str(index)\n        text_features.append(feature_name)\n    return text_features\ntext_features = create_feature_names()","311eb71a":"# Constructing dataframe from 'text_vector' variable\n\ndef create_df_txt_vec(text_vector):\n    df_text_lst = []\n    for vector in text_vector:\n        vector_reshape = np.reshape(vector ,(50, 1)).T\n        df_vector = pd.DataFrame(vector_reshape, columns=text_features)\n        df_text_lst.append(df_vector)\n    df_text = pd.concat(df_text_lst, ignore_index=True)\n    return df_text\n\ndf_text = create_df_txt_vec(text_vector)","b34e3280":"# Selecting featrures 'helpful' and 'total_ratings' from the original dataframe, because they might affect the prediction\n# Selecting 'overall' attribute for output\n\ndf_final = pd.concat([df_text, df[\"helpful\"], df[\"total_ratings\"], df[\"overall\"]], axis=1)","8fd03805":"df_final.head()","dc918dce":"df_features = df_final.drop(\"overall\", axis=1)\ndf_target = df_final[\"overall\"]\n\ndf_features_columns = df_features.columns\ndf_features_scaled = StandardScaler().fit_transform(df_features)\ndf_features_scaled = pd.DataFrame(df_features_scaled, columns=df_features_columns)","297d08c9":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(df_features_scaled, df_target, test_size=0.25, train_size=0.75)","b0cc4542":"# Checking for imbalanced dataset\ndf_final[\"overall\"].value_counts()","dcac1c73":"# Here, The positive reviews are 9015, neutral reviews are 772 and negative reviews are 467\n# I will use combination of undersampling and oversampling to match the classes\n\n# Creating Pipeline to perform SMOTE and UnderSampling techniques\noversampling_smote = SMOTE(sampling_strategy={1:5000, 0:5000})\nundersampling = RandomUnderSampler(sampling_strategy={2:5000})\npipeline = Pipeline([('under', undersampling), ('smote', oversampling_smote)])\ndf_train_resampled = pipeline.fit_resample(X_train, y_train)","0b81e982":"X_train = df_train_resampled[0] # Resampled X_train \ny_train = df_train_resampled[1] # Resampled y_train\n\nX_train, y_train = shuffle(X_train, y_train)","3a2bea7e":"C = [0.001 ,0.01 ,0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # Values of hyperparameter C\n\ncv_f1_mean= []\n\nfor value in C:\n    model = LogisticRegression(C = value, solver=\"sag\", max_iter=5000)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1_micro\")\n    cv_f1_mean.append(np.mean(scores))\n    \nmax_f1 = max(cv_f1_mean)\nindex_max_f1 = cv_f1_mean.index(max_f1)\n\nprint(\"Optimal value of hyperparameter C: \" + str(C[index_max_f1]))\nprint(\"F1 score at optimal C: \" + str(max_f1))","bfda239d":"model = LogisticRegression(C = 6, solver=\"sag\", max_iter=5000) # Calculating test accuracy of model\nmodel.fit(X_train, y_train)","478b337c":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of LogisticRegression model on test set: \" + str(acc_test) + \"%\")","62c81af9":"K = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # Values of hyperparameter K\n\ncv_f1_mean= []\n\nfor value in K:\n    model = KNeighborsClassifier(n_neighbors=value)\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1_micro\")\n    cv_f1_mean.append(np.mean(scores))\n    \nmax_f1 = max(cv_f1_mean)\nindex_max_f1 = cv_f1_mean.index(max_f1)\n\nprint(\"Optimal value of hyperparameter K: \" + str(K[index_max_f1]))\nprint(\"F1 score at optimal K: \" + str(max_f1))","ed2d5a17":"model = KNeighborsClassifier(n_neighbors=1) # Calculating test accuracy of model\nmodel.fit(X_train, y_train)","86175a26":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of 1-NN model on test set: \" + str(acc_test) + \"%\")","e9a45a68":"C = [0.001 ,0.01 ,0.1, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] # Values of hyperparameter C\n\ncv_f1_mean= []\n\nfor value in C:\n    model = SVC(C = value,kernel=\"rbf\")\n    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"f1_micro\")\n    cv_f1_mean.append(np.mean(scores))\n    \nmax_f1 = max(cv_f1_mean)\nindex_max_f1 = cv_f1_mean.index(max_f1)\n\nprint(\"Optimal value of hyperparameter C: \" + str(C[index_max_f1]))\nprint(\"F1 score at optimal C: \" + str(max_f1))","160d97f8":"model = SVC(C = 15,kernel=\"rbf\") # Calculating test accuracy of model\nmodel.fit(X_train, y_train)","6c3a70fb":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of SVM model on test set: \" + str(acc_test) + \"%\")","ce7236ac":"model = GaussianNB()\nmodel.fit(X_train, y_train)","a80dd8eb":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of GaussianNB model on test set: \" + str(acc_test) + \"%\")","cbca36d8":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)","6e36f389":"acc_test = model.score(X_test, y_test)*100\nacc_test = round(acc_test, 2)\nprint(\"Accuracy of Random Forest model on test set: \" + str(acc_test) + \"%\")","715f7c05":"## Text Preprocessing\n\nIn Text Preprcessing, I will-\n* Remove HTML tags in the review text\n* Remove special characters from the text (#, ! etc.)\n* Convert the text in lowercase\n* Removal of stop words\n* Applying Stemming to the text","72c5b791":"### Logistic Regression","f21fe02c":"**Attributes in the dataset**\n\n1. reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n2. asin - ID of the product, e.g. 0000013714\n3. reviewerName - name of the reviewer\n4. helpful - helpfulness rating of the review, e.g. 2\/3\n5. reviewText - text of the review\n6. overall - rating of the product\n7. summary - summary of the review\n8. unixReviewTime - time of the review (unix time)\n9. reviewTime - time of the review (raw)","b492bb6c":"### Gaussian Naive Nayes","608f6c3d":"## Model Fitting","b2b73277":"* I will use word2vec to convert text values to vector\n* I have sufficient training examples to construct W2V","9b1a6526":"### SVM","093a9741":"Webportals like Bhuvan get vast amount of feedback from the users. To go through all the feedback's can be a tedious job. You have to categorize opinions expressed in feedback forums. This can be utilized for feedback management system. We Classification of individual comments\/reviews.and we also determining overall rating based on individual comments\/reviews. So that company can get a complete idea on feedback's provided by customers and can take care on those particular fields. This makes more loyal Customers to the company, increase in business , fame ,brand value ,profits.","3cbed21b":"## Avg Word2Vec","7e97f73c":"* There are no duplicates in the dataset","bdf86600":"## Data Cleaning","f6ef1168":"**Libraries Required**","f35dcd92":"### KNN","d9feaf79":"* There are no rows which satisfy helpful > total_ratings ","649c860d":"* The dataset has imbalanced classes, which I will fix later","7f53a52c":"**Importing and analysis the dataset**","ae6d3de0":"**Task**\n\nClassify reviews as positive, negative, and neutral based on the attributes above.","6e7fc686":"* Now, I will create final dataset which will contain all features, which will be used for classification","49aa55c4":"### Random Forest","830011dc":"# Amazon Musical Instruments Reviews"}}