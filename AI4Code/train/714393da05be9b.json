{"cell_type":{"d24be006":"code","ac6e472f":"code","21588062":"code","77278105":"code","a831b9d9":"code","d7665162":"code","3e1db51a":"code","617347bf":"code","a02584e3":"code","1f965468":"code","0b2b8afb":"code","b09c4dc1":"code","3458deaa":"code","8dbda23e":"code","762ba310":"code","1532e5f6":"code","12ec1501":"code","3f7a35c3":"code","189698af":"code","c7002d20":"code","f5fa98e9":"code","0c96264d":"code","e8e13250":"code","c25c7e95":"code","1a6a0d38":"code","8393b99d":"code","92b23dfe":"code","c8e9e851":"code","5d8a816e":"code","83488e44":"code","03679016":"code","5bca3186":"code","720f59d4":"code","b617ffc1":"code","ebe0225c":"code","f6306b8d":"code","e4beabeb":"code","15426dfd":"code","6f22e1cb":"code","b8dcfe1e":"code","e6ab43dc":"code","2001d316":"code","0000b176":"code","32a479d6":"code","91316f13":"code","482bfe64":"code","2210fc22":"code","e4f90916":"code","d6185a94":"code","4fe0dcc4":"code","41602694":"code","5c41ac9e":"code","228eecf7":"code","b62015e5":"code","2af7b2df":"code","170ce02e":"code","783712db":"code","b64bc9af":"code","70203e07":"code","fdafd54b":"code","4fdf5bea":"code","524614fc":"code","62e3da5f":"code","5945621b":"code","d5edf062":"code","09c476de":"code","73c60429":"code","f179b14e":"code","fcb29f54":"code","08dcd0dd":"code","acbe28a9":"code","f861cbff":"code","ca422fef":"code","798339ef":"code","4727328d":"code","2882320e":"markdown","7d7f4915":"markdown","a0ae0c28":"markdown","8ddf9c9c":"markdown","d3924bb3":"markdown","ed693f09":"markdown","c8151f49":"markdown","aab6fed8":"markdown","3e2e8c66":"markdown","7a2e9b4f":"markdown","2810e40e":"markdown","5ff5d082":"markdown","465dd3dc":"markdown","edf596e1":"markdown","64a06de0":"markdown","05f32ff3":"markdown","84ecfa9c":"markdown","e46ba16f":"markdown","6318c27c":"markdown","2958ee12":"markdown","e8eb1089":"markdown","373f585d":"markdown","98b5b849":"markdown","7e20689b":"markdown","16f45e2c":"markdown","b4654076":"markdown","51f3a047":"markdown","e44895a4":"markdown","04cd77db":"markdown","5d9ea2dd":"markdown","db58e499":"markdown","ef6f7a8a":"markdown","e088d773":"markdown","a662e80e":"markdown","85a4b551":"markdown","bee26fb9":"markdown","5ceade0f":"markdown","d688b5d1":"markdown","a495df44":"markdown","c28393b1":"markdown","8a6508c7":"markdown","b0d773d4":"markdown","d7bfde19":"markdown","6a06e6e6":"markdown","0c641fbc":"markdown","fcb3a41d":"markdown"},"source":{"d24be006":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('ggplot')","ac6e472f":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import Imputer","21588062":"from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor","77278105":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","a831b9d9":"plt.figure(figsize=(15,8))\nsns.boxplot(train.YearBuilt, train.SalePrice)","d7665162":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","3e1db51a":"train.drop(train[(train[\"GrLivArea\"]>4000)&(train[\"SalePrice\"]<300000)].index,inplace=True)","617347bf":"full=pd.concat([train,test], ignore_index=True)","a02584e3":"full.drop(['Id'],axis=1, inplace=True)\nfull.shape","1f965468":"aa = full.isnull().sum()\naa[aa>0].sort_values(ascending=False)","0b2b8afb":"full.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])","b09c4dc1":"full[\"LotAreaCut\"] = pd.qcut(full.LotArea,10)","3458deaa":"full.groupby(['LotAreaCut'])[['LotFrontage']].agg(['mean','median','count'])","8dbda23e":"full['LotFrontage']=full.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","762ba310":"# Since some combinations of LotArea and Neighborhood are not available, so we just LotAreaCut alone.\nfull['LotFrontage']=full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))","1532e5f6":"cols=[\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\nfor col in cols:\n    full[col].fillna(0, inplace=True)","12ec1501":"cols1 = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\nfor col in cols1:\n    full[col].fillna(\"None\", inplace=True)","3f7a35c3":"# fill in with mode\ncols2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\", \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\nfor col in cols2:\n    full[col].fillna(full[col].mode()[0], inplace=True)","189698af":"full.isnull().sum()[full.isnull().sum()>0]","c7002d20":"NumStr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\nfor col in NumStr:\n    full[col]=full[col].astype(str)","f5fa98e9":"full.groupby(['MSSubClass'])[['SalePrice']].agg(['mean','median','count'])","0c96264d":"def map_values():\n    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n                                        '30':2, '45':2, \n                                        '190':3, '50':3, '90':3, \n                                        '85':4, '40':4, '160':4, \n                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n                                        '120': 6, '60':6})\n    \n    full[\"oMSZoning\"] = full.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n    \n    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n                                               'IDOTRR':2, 'BrDale':2,\n                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n                                               'NPkVill':5, 'Mitchel':5,\n                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n                                               'StoneBr':9,\n                                               'NoRidge':10, 'NridgHt':10})\n    \n    full[\"oCondition1\"] = full.Condition1.map({'Artery':1,\n                                           'Feedr':2, 'RRAe':2,\n                                           'Norm':3, 'RRAn':3,\n                                           'PosN':4, 'RRNe':4,\n                                           'PosA':5 ,'RRNn':5})\n    \n    full[\"oBldgType\"] = full.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n    \n    full[\"oHouseStyle\"] = full.HouseStyle.map({'1.5Unf':1, \n                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n                                           '1Story':3, 'SLvl':3,\n                                           '2Story':4, '2.5Fin':4})\n    \n    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n                                             'BrkFace':4, 'Plywood':4,\n                                             'VinylSd':5,\n                                             'CemntBd':6,\n                                             'Stone':7, 'ImStucc':7})\n    \n    full[\"oMasVnrType\"] = full.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n    \n    full[\"oExterQual\"] = full.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n                                           'Wood':3, 'PConc':4})\n    \n    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oBsmtExposure\"] = full.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n    \n    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n    \n    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oKitchenQual\"] = full.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n    \n    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n    \n    full[\"oFireplaceQu\"] = full.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n    \n    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n                                           'Detchd':2,\n                                           '2Types':3, 'Basment':3,\n                                           'Attchd':4, 'BuiltIn':5})\n    \n    full[\"oGarageFinish\"] = full.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n    \n    full[\"oPavedDrive\"] = full.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n    \n    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n                                       'CWD':2, 'Con':3, 'New':3})\n    \n    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n                \n                        \n                        \n    \n    return \"Done!\"","e8e13250":"map_values()","c25c7e95":"# drop two unwanted columns\nfull.drop(\"LotAreaCut\",axis=1,inplace=True)\nfull.drop(['SalePrice'],axis=1,inplace=True)","1a6a0d38":"class labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        lab=LabelEncoder()\n        X[\"YearBuilt\"] = lab.fit_transform(X[\"YearBuilt\"])\n        X[\"YearRemodAdd\"] = lab.fit_transform(X[\"YearRemodAdd\"])\n        X[\"GarageYrBlt\"] = lab.fit_transform(X[\"GarageYrBlt\"])\n        return X","8393b99d":"class skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.5):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        return X","92b23dfe":"# build pipeline\npipe = Pipeline([\n    ('labenc', labelenc()),\n    ('skew_dummies', skew_dummies(skew=1)),\n    ])","c8e9e851":"# save the original data for later use\nfull2 = full.copy()","5d8a816e":"data_pipe = pipe.fit_transform(full2)","83488e44":"data_pipe.shape","03679016":"data_pipe.head()","5bca3186":"scaler = RobustScaler()","720f59d4":"n_train=train.shape[0]\n\nX = data_pipe[:n_train]\ntest_X = data_pipe[n_train:]\ny= train.SalePrice\n\nX_scaled = scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = scaler.transform(test_X)","b617ffc1":"lasso=Lasso(alpha=0.001)\nlasso.fit(X_scaled,y_log)","ebe0225c":"FI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=data_pipe.columns)","f6306b8d":"FI_lasso.sort_values(\"Feature Importance\",ascending=False)","e4beabeb":"FI_lasso[FI_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()","15426dfd":"class add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n        else:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            \n            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n            \n            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n            \n           \n            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n\n    \n            return X","6f22e1cb":"pipe = Pipeline([\n    ('labenc', labelenc()),\n    ('add_feature', add_feature(additional=2)),\n    ('skew_dummies', skew_dummies(skew=1)),\n    ])","b8dcfe1e":"full_pipe = pipe.fit_transform(full)","e6ab43dc":"full_pipe.shape","2001d316":"n_train=train.shape[0]\nX = full_pipe[:n_train]\ntest_X = full_pipe[n_train:]\ny= train.SalePrice\n\nX_scaled = scaler.fit(X).transform(X)\ny_log = np.log(train.SalePrice)\ntest_X_scaled = scaler.transform(test_X)","0000b176":"pca = PCA(n_components=410)","32a479d6":"X_scaled=pca.fit_transform(X_scaled)\ntest_X_scaled = pca.transform(test_X_scaled)","91316f13":"X_scaled.shape, test_X_scaled.shape","482bfe64":"# define cross validation strategy\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","2210fc22":"models = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n          ExtraTreesRegressor(),XGBRegressor()]","e4f90916":"names = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\nfor name, model in zip(names, models):\n    score = rmse_cv(model, X_scaled, y_log)\n    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))","d6185a94":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])","4fe0dcc4":"grid(Lasso()).grid_get(X_scaled,y_log,{'alpha': [0.0004,0.0005,0.0007,0.0009],'max_iter':[10000]})","41602694":"grid(Ridge()).grid_get(X_scaled,y_log,{'alpha':[35,40,45,50,55,60,65,70,80,90]})","5c41ac9e":"grid(SVR()).grid_get(X_scaled,y_log,{'C':[11,13,15],'kernel':[\"rbf\"],\"gamma\":[0.0003,0.0004],\"epsilon\":[0.008,0.009]})","228eecf7":"param_grid={'alpha':[0.2,0.3,0.4], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[0.8,1]}\ngrid(KernelRidge()).grid_get(X_scaled,y_log,param_grid)","b62015e5":"grid(ElasticNet()).grid_get(X_scaled,y_log,{'alpha':[0.0008,0.004,0.005],'l1_ratio':[0.08,0.1,0.3],'max_iter':[10000]})","2af7b2df":"class AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","170ce02e":"lasso = Lasso(alpha=0.0005,max_iter=10000)\nridge = Ridge(alpha=60)\nsvr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)\nker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8)\nela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)\nbay = BayesianRidge()","783712db":"# assign weights based on their gridsearch score\nw1 = 0.02\nw2 = 0.2\nw3 = 0.25\nw4 = 0.3\nw5 = 0.03\nw6 = 0.2","b64bc9af":"weight_avg = AverageWeight(mod = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])","70203e07":"score = rmse_cv(weight_avg,X_scaled,y_log)\nprint(score.mean())","fdafd54b":"weight_avg = AverageWeight(mod = [svr,ker],weight=[0.5,0.5])","4fdf5bea":"score = rmse_cv(weight_avg,X_scaled,y_log)\nprint(score.mean())","524614fc":"class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X,y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean","62e3da5f":"# must do imputer first, otherwise stacking won't work, and i don't know why.\na = Imputer().fit_transform(X_scaled)\nb = Imputer().fit_transform(y_log.values.reshape(-1,1)).ravel()","5945621b":"stack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)","d5edf062":"score = rmse_cv(stack_model,a,b)\nprint(score.mean())","09c476de":"X_train_stack, X_test_stack = stack_model.get_oof(a,b,test_X_scaled)","73c60429":"X_train_stack.shape, a.shape","f179b14e":"X_train_add = np.hstack((a,X_train_stack))","fcb29f54":"X_test_add = np.hstack((test_X_scaled,X_test_stack))","08dcd0dd":"X_train_add.shape, X_test_add.shape","acbe28a9":"score = rmse_cv(stack_model,X_train_add,b)\nprint(score.mean())","f861cbff":"# This is the final model I use\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)","ca422fef":"stack_model.fit(a,b)","798339ef":"pred = np.exp(stack_model.predict(test_X_scaled))","4727328d":"result=pd.DataFrame({'Id':test.Id, 'SalePrice':pred})\nresult.to_csv(\"submission.csv\",index=False)","2882320e":"### SVR","7d7f4915":"+ __Im my case, doing PCA is very important. It lets me gain a relatively big boost on leaderboard. At first I don't believe PCA can help me, but \nin retrospect, maybe the reason is that the features I built are highly correlated, and it leads to multicollinearity. PCA can decorrelate these features.__","a0ae0c28":"## Feature Selection","8ddf9c9c":"+ __Apply log1p to the skewed features, then get_dummies.__","d3924bb3":"+ __By using a pipeline, you can quickily experiment different feature combinations.__","ed693f09":"+ __So basically I'll do__  \n                '180' : 1\n                '30' : 2   '45' : 2\n                '190' : 3, '50' : 3, '90' : 3,\n                '85' : 4, '40' : 4, '160' : 4\n                '70' : 5, '20' : 5, '75' : 5, '80' : 5, '150' : 5\n                '120': 6, '60' : 6","c8151f49":"+ __Then we filling in other missing values according to data_description.__","aab6fed8":"### Lasso","3e2e8c66":"# Data Cleaning","7a2e9b4f":"+ __Next we do some hyperparameters tuning. First define a gridsearch method.__","2810e40e":"+ __Label Encoding three \"Year\" features.__","5ff5d082":"+ __Convert some numerical features into categorical features. It's better to use LabelEncoder and get_dummies for these features.__","465dd3dc":"# Ensemble Methods ","edf596e1":"### Kernel Ridge","64a06de0":"+ __You can even do parameter tuning for your meta model after you get \"X_train_stack\", or do it after combining with the original features. but that's a lot of work too !__","05f32ff3":"+ __Let's first try it out ! It's a bit slow to run this method, since the process is quite compliated. __","84ecfa9c":"+ __But if we average only two best models, we gain better cross-validation score.__","e46ba16f":"# Exploratory Visualization","6318c27c":"+ __Based on the \"Feature Importance\" plot and other try-and-error, I decided to add some features to the pipeline.__","2958ee12":"### Ridge","e8eb1089":"+ __It seems that the price of recent-built houses are higher. So later I 'll use labelencoder for three \"Year\" feature.__","373f585d":"+ __I have to confess, the feature engineering above is not enough, so we need more.__   \n+ __Combining different features is usually a good way, but we have no idea what features should we choose. Luckily there are some models that can provide feature selection, here I use Lasso, but you are free to choose Ridge, RandomForest or GradientBoostingTree.__","98b5b849":"+ __Aside from normal stacking, I also add the \"get_oof\" method, because later I'll combine features generated from stacking and original features.__","7e20689b":"+ __As is discussed in other kernels, the bottom right two two points with extremely large GrLivArea are likely to be outliers. So we delete them.__","16f45e2c":"# Modeling & Evaluation","b4654076":"## Stacking","51f3a047":"+ __Next we extract the features generated from stacking, then combine them with original features.__","e44895a4":"# Feature Engineering","04cd77db":"+ __And there is no missing data except for the value we want to predict !__","5d9ea2dd":"+ __Now I want to do a long list of value-mapping. __\n+ __I was influenced by the insight that we should build as many features as possible and trust the model to choose the right features. So I decided to groupby SalePrice according to one feature and sort it based on mean and median. Here is an example:__","db58e499":"+ __We choose 13 models and use 5-folds cross-calidation to evaluate these models.__","ef6f7a8a":"+ __So I'll use approximately the same dimension in PCA as  in the original data. Since the aim here is not deminsion reduction.__","e088d773":"# Content","a662e80e":"### Submission","85a4b551":"## PCA","bee26fb9":"## Pipeline","5ceade0f":"+ __Next we can build a pipeline. It's convenient to experiment different feature combinations once you've got a pipeline.__","d688b5d1":"### ElasticNet","a495df44":"__1. Exploratory Visualization__  \n__2. Data Cleaning__  \n__3. Feature Engineering__  \n__4. Modeling & Evaluation__  \n__5. Ensemble Methods__  ","c28393b1":"+ __Different people may have different views on how to map these values, so just follow your instinct =^_^=__  \n__Below I also add a small \"o\" in front of the features so as to keep the original features to use get_dummies in a moment.__","8a6508c7":"### Missing Data","b0d773d4":"### Weight Average","d7bfde19":"+ __use robustscaler since maybe there are other outliers.__","6a06e6e6":"Models include:\n\n+ LinearRegression\n+ Ridge\n+ Lasso\n+ Random Forrest\n+ Gradient Boosting Tree\n+ Support Vector Regression\n+ Linear Support Vector Regression\n+ ElasticNet\n+ Stochastic Gradient Descent\n+ BayesianRidge\n+ KernelRidge\n+ ExtraTreesRegressor\n+ XgBoost","0c641fbc":"+ __Average base models according to their weights.__","fcb3a41d":"+ __Let's first imput the missing values of LotFrontage based on the median of LotArea and Neighborhood. Since LotArea is a continuous feature, We use qcut to divide it into 10 parts.__"}}