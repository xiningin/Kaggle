{"cell_type":{"ffef2b0a":"code","ead4b0f7":"code","87de75f8":"code","547ab67f":"code","1d49bb2f":"code","f1e0d988":"code","41478941":"code","23f0b2e5":"code","e6eb209e":"code","67c714ad":"code","dabb57ee":"code","687a9d5c":"code","ae3320f3":"code","28ca215d":"code","edf07501":"code","00148172":"code","652a21f1":"code","b635a378":"code","09a1842a":"code","8a6e92c5":"markdown","b266a6b9":"markdown","f18ac31b":"markdown","d82dacc9":"markdown","9b0f78df":"markdown","b68ba540":"markdown","dd67e33b":"markdown","50c014ac":"markdown"},"source":{"ffef2b0a":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import ExtraTreeRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings('ignore')","ead4b0f7":"# setting paths\nBASE_DIR = \"..\/input\/datathonvk2021\"\ntrain_path = os.path.join(BASE_DIR, \"VK_Train_Final.csv\")\ntest_path = os.path.join(BASE_DIR, \"VK_Test_Final.csv\")\nsample_submission = os.path.join(BASE_DIR, \"VK_Sample_Submission.csv\")","87de75f8":"# reading data\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nsample_submission = pd.read_csv(sample_submission)\ntrain.shape, test.shape","547ab67f":"# converting object to numerical\nobj_to_num_cols = train.columns[:-2]\n\ntrain[obj_to_num_cols] = train[obj_to_num_cols].apply(pd.to_numeric, errors='coerce', axis=1)\ntest[obj_to_num_cols] = test[obj_to_num_cols].apply(pd.to_numeric, errors='coerce', axis=1)","1d49bb2f":"train_id = train['ID']\ntest_id = test['ID']\ntrain.drop(['ID'], axis=1, inplace=True)\ntest.drop(['ID'], axis=1, inplace=True)\ntrain.shape, test.shape","f1e0d988":"target = train['Wind Speed']\ntrain_feats = train[train.columns[:-1]]\ntest_feats = test[test.columns]","41478941":"# ==============================================================================\n\n# FEATURE SELECTION\n\ndef feature_selection(combined):\n    cols = ['hour', 'Particulate Matter (10)', 'Nitrogen Dioxide (conc.)', 'Carbon Monoxide (conc.)', 'Ozone (conc.)', 'Temperature (Celsius)', 'Pressure', 'Dew Point Temperature', 'Wind Direction']\n    combined = combined[cols]\n    \n    return combined\n\n# ==============================================================================\n\n# REPLACING OUTLIERS\n\ndef replace_outliers():\n    \n    combined = pd.concat([train_feats, test_feats]).reset_index(drop=True)\n    \n    #columns which can't have values less than 0\n    non_negative_cols = ['year', 'month', 'date', 'hour', 'Particulate Matter (2.5)',\n       'Particulate Matter (10)', 'Sulphur Dioxide (conc.)',\n       'Nitrogen Dioxide (conc.)', 'Carbon Monoxide (conc.)', 'Ozone (conc.)',\n       'Pressure', 'Precipitation']\n    \n    for col in non_negative_cols:\n        combined[col] = combined[col].mask(combined[col] < 0.0, np.nan)\n\n    eliminate_greater_than_10000 = ['hour', 'Carbon Monoxide (conc.)', 'Pressure',\n                               'Particulate Matter (2.5)', 'Particulate Matter (10)',\n                               'Sulphur Dioxide (conc.)', 'Ozone (conc.)']\n    \n    for col in eliminate_greater_than_10000:\n        combined[col] = combined[col].mask(combined[col] >= 10000.0, np.nan)\n    \n    combined[['year', 'date', 'hour']] = combined[['year', 'date', 'hour']].mask(combined[['year', 'date', 'hour']] == 0.0, np.nan)\n    combined['Nitrogen Dioxide (conc.)'] = combined['Nitrogen Dioxide (conc.)'].mask(combined['Nitrogen Dioxide (conc.)'] >= 1000.0, np.nan)\n    combined['Temperature (Celsius)'] = combined['Temperature (Celsius)'].mask(combined['Temperature (Celsius)'] > 100.0, np.nan)\n    combined['Temperature (Celsius)'] = combined['Temperature (Celsius)'].mask(combined['Temperature (Celsius)'] < -100.0, np.nan)\n    combined['Dew Point Temperature'] = combined['Dew Point Temperature'].mask(combined['Dew Point Temperature'] > 100.0, np.nan)\n    combined['Wind Direction'] = combined['Wind Direction'].mask(combined['Wind Direction'] == '-', np.nan)\n    combined['Wind Direction'] = combined['Wind Direction'].mask(combined['Wind Direction'] == '0', np.nan)\n    \n    return combined\n\n# ===========================================================================\n\n# SCALE DATA\n\ndef scale_data(combined, target):\n    \"\"\"\n    Scale the data with StandardScaler\n    :param: combined dataset \n    :param: target\n    :returns : combined_scaled, target_scaled, sc_target(fitted params for target)\n    \"\"\"\n    # scale features\n    sc_feats = StandardScaler()\n    combined_scaled = pd.DataFrame(sc_feats.fit_transform(combined), columns = combined.columns)\n    \n    # scale target\n    target = np.array(target).reshape(-1,1)\n    sc_target = StandardScaler()\n    target_scaled = sc_target.fit_transform(target)\n    target_scaled = target_scaled.flatten()\n    \n    return combined_scaled, target_scaled, sc_target\n    \n    \n# ===========================================================================\n\n# To impute by most common value => year, month, date, hour, Wind Direction\n# To impute by KNN => all numerical columns and target column\n\ndef preprocess_data(combined, train_feats, test_feats, target, scale=False, feat_select=False):\n    \"\"\"\n    Preprocess the data (fill missing, scale, split into train and test)\n    \"\"\"\n    \n    # imputing target col\n    target = target.fillna(target.mean())\n\n    # columns to be filled by median of the column\n    cols_fill_median = ['year', 'month', 'date', 'hour']\n\n    for col in cols_fill_median:\n        combined[col] = combined[col].fillna(combined[col].median())\n\n    # Imputing categorical feature\n    combined['Wind Direction'] = combined['Wind Direction'].fillna('NNW')\n    \n    # label encoding the categorical feature\n    \n    le = LabelEncoder()\n    combined['Wind Direction'] = le.fit_transform(combined['Wind Direction'])\n    \n    # feature selection (9\/15)\n    if feat_select == True:\n        combined = feature_selection(combined)\n        \n    # imputing numerical variable\n    knn_imputer = KNNImputer(n_neighbors = 3)\n    combined = pd.DataFrame(knn_imputer.fit_transform(combined), columns = combined.columns)\n    \n    sc_target = None\n    # scaling the data\n    if scale == True:\n        combined, target, sc_target = scale_data(combined, target)\n    \n    # split back to train and test set\n    train_feats = combined.iloc[:len(train_feats), :]\n    test_feats = combined.iloc[len(train_feats):, :]\n    \n    return train_feats, test_feats, target, sc_target","23f0b2e5":"combined = replace_outliers()\ntrain_feats, test_feats, target, sc_target = preprocess_data(combined, train_feats, test_feats, target, scale=True, feat_select=False)\n# sc_target => fitted standard scaler object for further transforming target back to original form\ntrain_feats.shape, test_feats.shape, target.shape","e6eb209e":"## script used for feature selection\n# # trying boruta\n# from boruta import BorutaPy\n# from sklearn.metrics import r2_score\n# from sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(train_feats, target, test_size=0.2)\n\n# rf = RandomForestRegressor()\n# rf.fit(X_train, y_train)\n\n# base_pred = rf.predict(X_test)\n# base_score = r2_score(y_test, base_pred)\n\n# print(f'Base score: {base_score}')\n# boruta_selector = BorutaPy(rf, n_estimators='auto', verbose=2)\n# boruta_selector.fit(np.array(X_train), np.array(y_train))\n# green_area = X_train.columns[boruta_selector.support_].to_list()\n# blue_area = X_train.columns[boruta_selector.support_weak_].to_list()\n# print('features in the green area:', green_area)\n# print('features in the blue area:', blue_area)\n# # X_train = boruta_selector.transform(np.array(X_train))\n# # X_test = boruta_selector.transform(np.array(X_test))\n\n# # rf = RandomForestRegressor()\n# # rf.fit(X_train, y_train)\n\n# # pred_new = rf.predict(X_test)\n# # new_score = r2_score(y_test, pred_new)\n# # print(f\"New score: {new_score}\")\n# # change = new_score - base_score\n# # print(change)","67c714ad":"def train_models(train_feats, target, n_fold):\n    \"\"\"\n    Train several models with specified n_fold CV\n    \"\"\"\n    model_dispatcher = {\"linear_regression\" : LinearRegression(),\n        \"randomforest\" : RandomForestRegressor(),\n        \"xgboost\" : XGBRegressor(),\n        \"catboost\" : CatBoostRegressor(silent=True),\n        \"lightbgm\" : lgb.LGBMRegressor(),\n        \"knn\" : KNeighborsRegressor(),\n        \"svr\" : SVR(),\n        \"extratrees\" : ExtraTreeRegressor(),\n        \"GBDT\" : GradientBoostingRegressor()}\n    \n    for name, model in model_dispatcher.items():\n        # perform cv and calculate score\n        cv = model_selection.cross_val_score(model, train_feats, target, cv=n_fold, scoring=\"r2\")\n        print(f\"{name}: {cv} \\t Mean CV: {cv.mean()}\")","dabb57ee":"# print(\"Without scaling: \")\n# train_models(train_feats, target,n_fold=3)\n\n# =========================================================================================\n\n# Without scaling:\n# linear_regression: [0.38230427 0.36561475 0.39321417] \t Mean CV: 0.380377732184954\n# randomforest: [0.59728251 0.57122159 0.5930847 ] \t Mean CV: 0.587196262634929\n# xgboost: [0.59218131 0.56706607 0.59038936] \t Mean CV: 0.5832122469515998\n# catboost: [0.61793047 0.59447413 0.61510089] \t Mean CV: 0.6091684948670609\n# lightbgm: [0.60022175 0.57595573 0.59600011] \t Mean CV: 0.5907258626597992\n# knn: [0.40375759 0.37360531 0.42557983] \t Mean CV: 0.4009809081321678\n# svr: [0.22991319 0.20758859 0.22330364] \t Mean CV: 0.22026847504592625\n# extratrees: [0.14278441 0.14261478 0.12789525] \t Mean CV: 0.13776481232883311\n# GBDT: [0.53057715 0.5101892  0.5302294 ] \t Mean CV: 0.5236652509130891\n\n# =========================================================================================","687a9d5c":"# print(\"With scaling: \")\n# train_models(train_feats, target,n_fold=3)\n\n# =========================================================================================\n\n# With scaling: \n# linear_regression: [0.38230427 0.36561475 0.39321417] \t Mean CV: 0.3803777321849536\n# randomforest: [0.60012462 0.57059516 0.59219569] \t Mean CV: 0.5876384893958928\n# xgboost: [0.58861526 0.55693619 0.58414396] \t Mean CV: 0.5765651330826875\n# catboost: [0.61818227 0.59446568 0.61510278] \t Mean CV: 0.609250244286426\n# lightbgm: [0.59792297 0.58228571 0.59377276] \t Mean CV: 0.5913271429161338\n# knn: [0.49849146 0.45738166 0.49758764] \t Mean CV: 0.48448692075627897\n# svr: [0.49577574 0.46653181 0.5081583 ] \t Mean CV: 0.4901552820957826\n# extratrees: [0.13570558 0.0515228  0.12103374] \t Mean CV: 0.10275403914259547\n# GBDT: [0.53058459 0.51009303 0.52994702] \t Mean CV: 0.523541548444241\n\n# =========================================================================================","ae3320f3":"# selecting best performing models\nmodel_dispatcher_best = {\n    \"rf\": RandomForestRegressor(),\n    \"xgb\": XGBRegressor(),\n    \"cb\": CatBoostRegressor(silent=True),\n    \"lgbm\": lgb.LGBMRegressor(),\n}\n\n\ndef train_best_models(train_feats, test_feats, target, models):\n    \"\"\"\n    Trains the best model obtained from CV\n    :param train_feats: processed train data (pandas Dataframe)\n    :param test_feats: processed test data (pandas DataFrame)\n    :param target: target variable (numpy array)\n    :param models: dictionary of model's definitions\n    :return: stacked_model_preds, base_preds \n            (stacked model predictions(numpy array), base models prediction (numpy array))\n    \"\"\"\n    trained_models = {}\n    \n    print(\"Training base models ...\")\n    \n    base_preds = {}\n    \n    for name, model in models.items():\n        trained_models[name] = model.fit(train_feats, target)\n        base_preds[name] = sc_target.inverse_transform(model.predict(test_feats))\n    \n    \n    print(\"Finished training base models\")\n    print(\"Training stacked model ...\")\n    stack = StackingCVRegressor(regressors=(trained_models.get(\"rf\"),\n                                                trained_models.get(\"xgb\"),\n                                                trained_models.get(\"lgbm\"), \n                                                trained_models.get(\"cb\")),\n                                meta_regressor=trained_models.get(\"cb\"),\n                                use_features_in_secondary=True)\n\n\n    stack_model = stack.fit(np.array(train_feats), np.array(target))\n    \n    stack_model_pred = sc_target.inverse_transform(stack_model.predict(test_feats))\n    \n    print(\"Finished Training.\")\n    return stack_model_pred, base_preds","28ca215d":"stack_model_pred, base_preds = train_best_models(train_feats, test_feats, target, model_dispatcher_best)","edf07501":"# stack, cb, lgbm, rf, xgb (arranged in ascending order by their scores)\n\nfinal_pred = stack_model_pred * 0.6 + base_preds.get('cb') * 0.2 + base_preds.get('lgbm') * 0.1 + base_preds.get('rf') * 0.05 + base_preds.get('xgb') * 0.05\n# final_pred = stack_model_pred\nfinal_pred","00148172":"final_pred = np.round(final_pred, decimals=1)\nfinal_pred","652a21f1":"sub = pd.DataFrame()\nsub['ID'] = test_id.astype(int)\nsub['Wind Speed'] = final_pred\nsub.to_csv('submission.csv', index=False)","b635a378":"import pandas as pd\nsubmit_path = '.\/submission.csv'\nsubmit = pd.read_csv(submit_path)\nsubmit.shape\nsubmit.dtypes","09a1842a":"submit.head()","8a6e92c5":"## Importing stuff","b266a6b9":"# No Hyperparameter Tuning, simple stacked and blended vanilla models \ud83d\udd25\ud83d\udd25","f18ac31b":"## Vanilla Modelling","d82dacc9":"## Stacking \n","9b0f78df":"## Data Cleaning (fill missing, scaling, outlier removal)","b68ba540":"## Blending","dd67e33b":"## Data Preparation","50c014ac":"## Submission"}}