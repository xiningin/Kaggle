{"cell_type":{"cdd1cead":"code","06497600":"code","552ece1e":"code","45f6221c":"code","68ade3e4":"code","dc854c64":"code","0c4f3435":"code","a2c3740f":"code","bad87610":"code","95e84492":"code","97560983":"markdown","05a8564d":"markdown","8c3c35ea":"markdown","98c0f885":"markdown","fde0d976":"markdown","dbd1ec45":"markdown","439fab76":"markdown"},"source":{"cdd1cead":"import numpy as np\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport pandas as pd\nimport os\nimport pickle\nimport cv2\nimport ast\nimport glob\n\nfrom shutil import copyfile\nimport sys\nfrom sklearn.model_selection import StratifiedKFold\nsys.path.append('..\/input\/tensorflow-great-barrier-reef')","06497600":"ROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nHOME_DIR = '\/kaggle\/working\/'\nDATASET_PATH = 'COTS-YOLOv5-StratifiedKFold'\nLABEL_DIR = '\/kaggle\/labels' # to save yolo converted labels\nFOLD = 4 # number of folds to train","552ece1e":"!mkdir {HOME_DIR}{DATASET_PATH}\n!mkdir {HOME_DIR}{DATASET_PATH}\/images\n!mkdir {HOME_DIR}{DATASET_PATH}\/images\/train\n!mkdir {HOME_DIR}{DATASET_PATH}\/images\/val\n!mkdir {HOME_DIR}{DATASET_PATH}\/labels\n!mkdir {HOME_DIR}{DATASET_PATH}\/labels\/train\n!mkdir {HOME_DIR}{DATASET_PATH}\/labels\/val\n!mkdir {LABEL_DIR}","45f6221c":"def get_path(row):\n    row['image_path'] = f'{ROOT_DIR}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    row['label_path'] = f'{LABEL_DIR}\/video_{row.video_id}_{row.video_frame}.txt'\n    return row\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row","68ade3e4":"def coco2yolo(image_height, image_width, bboxes):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]\/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]\/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\/2\n    \n    return bboxes","dc854c64":"# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}\/train.csv')\ndf = df.progress_apply(get_path, axis=1) # add image path to dataframe\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x)) # str to list\ndf['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndf = df.query(\"num_bbox>0\") # take only rows which contains bounding boxes\ndf['bboxes'] = df.annotations.progress_apply(get_bbox) # add number of bboxes column\n# image resolution\ndf['width']  = 1280\ndf['height'] = 720\ndisplay(df.head(5))","0c4f3435":"kf = StratifiedKFold(n_splits = 5)\ndf = df.reset_index(drop=True)\ndf['fold'] = -1\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df, y=df.video_id.tolist(), groups=df.sequence)):\n    df.loc[val_idx, 'fold'] = fold\n    \ndisplay(df.fold.value_counts())   \ndf.head(5)","a2c3740f":"for i in tqdm(range(len(df))):\n    \n    row = df.loc[i]\n    \n    if row.fold != FOLD: # train\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/images\/train\/{row.image_id}.jpg')\n    \n    elif row.fold == FOLD: # val\n        copyfile(f'{row.image_path}', f'{HOME_DIR}{DATASET_PATH}\/images\/val\/{row.image_id}.jpg')","bad87610":"cnt = 0\nall_bboxes = []\nfor row_idx in tqdm(range(df.shape[0])):\n    row = df.iloc[row_idx]\n    image_height = row.height\n    image_width  = row.width\n    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n    num_bbox     = len(bboxes_coco)\n    names        = ['cots']*num_bbox\n    labels       = [0]*num_bbox\n    ## Create Annotation(YOLO)\n    with open(row.label_path, 'w') as f:\n        if num_bbox<1:\n            annot = ''\n            f.write(annot)\n            cnt+=1\n            continue\n        bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n        bboxes_yolo  = np.clip(bboxes_yolo, 0, 1)\n        all_bboxes.extend(bboxes_yolo)\n        for bbox_idx in range(len(bboxes_yolo)):\n            annot = [str(labels[bbox_idx])]+ list(bboxes_yolo[bbox_idx].astype(str))+(['\\n'] if num_bbox!=(bbox_idx+1) else [''])\n            annot = ' '.join(annot)\n            annot = annot.strip(' ')\n            f.write(annot)\nprint('Missing:',cnt)","95e84492":"for i in tqdm(range(len(df))):\n    \n    row = df.loc[i]\n    \n    if row.fold != FOLD: # train\n        copyfile(f'{row.label_path}', f'{HOME_DIR}{DATASET_PATH}\/labels\/train\/{row.image_id}.txt')\n    \n    elif row.fold == FOLD: # val\n        copyfile(f'{row.label_path}', f'{HOME_DIR}{DATASET_PATH}\/labels\/val\/{row.image_id}.txt')","97560983":"# Helper Functions","05a8564d":"# Convert YOLO annotation format","8c3c35ea":"# Modify train dataframe","98c0f885":"# Save annotations","fde0d976":"# StratifiedKFold","dbd1ec45":"# Save images","439fab76":"# Set up paths"}}