{"cell_type":{"19d53161":"code","3feca03f":"code","ce59c7c6":"code","546cf5a8":"code","fae2ec65":"code","79e588de":"code","6460a21c":"code","fdabfa89":"code","dff1a171":"code","f63caecb":"code","33e9ec50":"code","5725b047":"code","c4c80093":"code","5e96520e":"code","119a3346":"code","db9f2cd5":"code","f6634fa5":"code","3b66762d":"code","daf875a2":"code","5f71fe1c":"code","d7fa4274":"code","3422d7ef":"code","a0f006e3":"code","79c7e50a":"code","da4e4163":"code","0ed81d74":"code","8476f142":"code","c66dc340":"code","e3c93b6d":"code","a1d0244e":"code","c847229f":"markdown","43bca574":"markdown","b280927f":"markdown","2bf7cc5e":"markdown","36fa0e13":"markdown","df904281":"markdown","214b1aa5":"markdown","04bbbcd0":"markdown","54d32b40":"markdown","5faa0c81":"markdown","37961c11":"markdown","7a170baf":"markdown","c267f839":"markdown","1896abf1":"markdown","4585429f":"markdown","7c3da331":"markdown","1f2deb7c":"markdown","6bc11f29":"markdown","d927ace1":"markdown","4ff9b7b8":"markdown","183e407d":"markdown","3135e8db":"markdown","efbc738e":"markdown","327f42c9":"markdown","5d7d7aea":"markdown","df3b826b":"markdown","7c3c715b":"markdown","e5d54445":"markdown","6af73f0e":"markdown","667e847f":"markdown","baa857db":"markdown","0c3bc00a":"markdown","96fe562f":"markdown","1d808d6f":"markdown","14a194a1":"markdown","67e2962c":"markdown","acd6ca7a":"markdown","4732e15b":"markdown","1ea2c0d5":"markdown","f12e528c":"markdown","ff4cf011":"markdown","575f67dd":"markdown","2b070801":"markdown","a29d853b":"markdown","8c1afacd":"markdown","1360005e":"markdown","6613397d":"markdown","48e1854e":"markdown","8ad77bab":"markdown","4b06ebcb":"markdown","d08efdda":"markdown","9d7a6428":"markdown","1a11c55c":"markdown","e4a44bdb":"markdown","10f6aad5":"markdown","f09237e3":"markdown"},"source":{"19d53161":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport itertools\nimport math\nimport category_encoders as ce","3feca03f":"# Leitura dos dados\nX_train = pd.read_csv('..\/input\/titanic\/train.csv')\n\nX_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#Visualiza\u00e7\u00e3o\nX_train.head(10)","ce59c7c6":"print(\"Formato dos dados de treino: {}\".format(X_train.shape))\nprint(\"Formato dos dados de teste: {}\".format(X_test.shape))\n\nprint(\"Features iniciais contidas no dataset: {}\".format(X_test.columns))\nprint(\"Label: Survived\")","546cf5a8":"X_train.describe()","fae2ec65":"# Criando a representa\u00e7\u00e3o, \u00e1rea de plot\nfig1, ax1 = plt.subplots(figsize = (4,4))\n\n# Conjunto de dados a ser representado\nsns.set(style=\"darkgrid\")\nsizes = X_train.Survived.value_counts()\nlabels = ['N\u00e3o sobreviveu', 'Sobreviveu']\n\n# Criando o g\u0155afico\nax1.pie(sizes, labels = labels, autopct = '%1.1f%%', shadow = True, startangle = 90, colors = [(0.8,0.4,0.4), 'skyblue'])\n\n# Op\u00e7\u00f5es Adicionais\nplt.title('Taxa de sobreviv\u00eancia no Titanic')\nax1.axis('equal')\n\n# Mostrando o g\u0155afico\nplt.show()","79e588de":"sizes = X_train.Sex.value_counts()\nlabels = ['Homens', 'Mulheres']\n\n# Criando a representa\u00e7\u00e3o, \u00e1rea de plot\nfig, (ax2,ax1) = plt.subplots(figsize = [12,12], nrows = 1, ncols = 2)\nplt.subplots_adjust(left=0, bottom=None, right=1, top=0.5, wspace = 0.2, hspace=None)\n# Gr\u00e1fico de pizza (porcentagem de homens e de mulheres)\nsns.set(style=\"darkgrid\")\n\n\nax2.bar(labels,sizes,0.2,0.2, color = ['skyblue', 'pink'])\nax2.set_title('Quantidade Absoluta de Homens e de Mulheres', fontsize = 10)\nax2.set_ylabel('Quantidade',fontsize = 10)\n\n\nsns.set(style=\"darkgrid\")\nsns.countplot(x = 'Survived', hue = 'Sex', data = X_train, ax = ax1)\nax1.set_title('Rela\u00e7\u00e3o do Sexo do Passageiro com sua Sobreviv\u00eancia')\nax1.set_xlabel('Sobreviveu', fontsize = 10)\nax1.set_ylabel('Quantidade', fontsize = 10)\n\n\n# Mostrando o g\u0155afico\nplt.show()","6460a21c":"def age_groups(row):\n    if row.Age < 20:\n        return 1\n    elif row.Age < 30:\n        return 2\n    elif row.Age < 40:\n        return 3\n    elif row.Age < 60:\n        return 4\n    elif row.Age < 100:\n        return 5\n    \nX_train['Age_group'] = X_train.apply(lambda row: age_groups(row), axis = 1)\n","fdabfa89":"# Criando a representa\u00e7\u00e3o, \u00e1rea de plot\nfig, (ax2,ax1) = plt.subplots(figsize = [12,12], nrows = 1, ncols = 2)\nplt.subplots_adjust(left=0, bottom=None, right=1, top=0.5, wspace = 0.2, hspace=None)\n# Gr\u00e1fico de pizza (porcentagem de homens e de mulheres)\nsns.set(style=\"darkgrid\")\n\n# Distribui\u00e7\u00e3o de Idade dos Passageiros\nsns.kdeplot(data=X_train['Age'], shade=True, ax = ax2)\nax2.set_title('Distribui\u00e7\u00e3o de Idade dos Passageiros', fontsize = 10)\nax2.set_ylabel('Fun\u00e7\u00e3o distribui\u00e7\u00e3o',fontsize = 10)\nax2.set_xlabel('Idade',fontsize = 10)\n\n\n# Rela\u00e7\u00e3o da Faixa Et\u00e1ria com a Sobreviv\u00eancia\nsns.countplot(x = 'Survived', hue = 'Age_group', data = X_train, ax = ax1)\n\nax1.set_title('Rela\u00e7\u00e3o da Faixa Et\u00e1ria do Passageiro com sua Sobreviv\u00eancia')\nax1.set_xlabel('Sobreviveu', fontsize = 10)\nax1.set_ylabel('Quantidade', fontsize = 10)\n\n\n\n# Mostrando o g\u0155afico\nplt.show()","dff1a171":"sizes = X_train.Pclass.value_counts().reindex(index = [1,2,3])\nlabels = ['1','2','3']\n\n# Criando a representa\u00e7\u00e3o, \u00e1rea de plot\nfig, (ax2,ax1) = plt.subplots(figsize = [12,12], nrows = 1, ncols = 2)\nplt.subplots_adjust(left=0, bottom=None, right=1, top=0.5, wspace = 0.2, hspace=None)\n# Gr\u00e1fico de pizza (porcentagem de homens e de mulheres)\nsns.set(style=\"darkgrid\")\n\n\nax2.bar(labels,sizes,0.2,0.2, color = ['skyblue', 'pink','orange'])\nax2.set_title('Quantidade de Passageiros por Classe de Passagem', fontsize = 10)\nax2.set_ylabel('Quantidade',fontsize = 10)\nax2.set_xlabel('Classe de Passagem',fontsize = 10)\n\n\n\nsns.set(style=\"darkgrid\")\nsns.countplot(x = 'Survived', hue = 'Pclass', data = X_train, ax = ax1)\nplt.title('Rela\u00e7\u00e3o da Classe da Passagem do Passageiro e sua Sobreviv\u00eancia')\nax1.set_xlabel('Sobreviveu', fontsize = 10)\nax1.set_ylabel('Quantidade',fontsize = 10)\n\n\n# Mostrando o g\u0155afico\nplt.show()","f63caecb":"correlation = X_train.corr()\n\nplt.figure(figsize=(8,8))\nsns.heatmap(correlation, annot = True)\nplt.title('Correla\u00e7\u00e3o das Features')\n\nplt.show()","33e9ec50":"missing_val_count_by_column = (X_train.isnull().sum())\nprint('Features com dados faltantes no DataSet:')\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","5725b047":"# Substitui\u00e7\u00e3o dos valores faltantes na coluna de idade pela mediana dos valores\nX_train.loc[X_train.Age.isnull(),'Age']=X_train.Age.median()\n\n# Substitui\u00e7\u00e3o dos valores faltantes na coluna Embarked pelo valor mais comum\nX_train.loc[X_train.Embarked.isnull(), 'Embarked'] = X_train.Embarked.describe().top\n\n# Atualiza\u00e7\u00e3o da coluna Age_group\nX_train['Age_group'] = X_train.apply(lambda row: age_groups(row), axis = 1)\n\n# Retiramos a vari\u00e1vel Cabin, j\u00e1 que possui muitos valores faltantes\nX_train.drop(['Cabin'], axis = 1, inplace = True)\n\n# Realizamos uma c\u00f3pia do Dataset para podermos construir um tratamento e um modelo base\nX_train_baseline = X_train.copy()","c4c80093":"X_train_baseline.head()","5e96520e":"s = (X_train_baseline.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Vari\u00e1veis categ\u00f3ricas presentes no conjunto de dados: {}\".format(object_cols))","119a3346":"X_train_baseline.describe(include=[np.object])","db9f2cd5":"# Retirada das vari\u00e1veis Name e Ticket\nX_train_baseline.drop(['Name', 'Ticket'], axis = 1, inplace = True)\n","f6634fa5":"# Colunas a sofrerem o OH Encoding\nobject_cols = ['Sex', 'Embarked']\n\n# Aplica\u00e7\u00e3o do OH Encoding\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_baseline[object_cols]))\n\n# Recolocando o index\nOH_cols_train.index = X_train_baseline.index\nX_train_baseline.drop(['Sex', 'Embarked'], axis = 1, inplace=  True)\n\n# Concatenando de volta o dataset\nX_train_baseline = pd.concat([X_train_baseline,OH_cols_train ], axis = 1)","3b66762d":"def fit_predict_evaluate_model(clf, X, y):\n    # Predicting the data\n    my_pipeline = clf\n    accuracy = 1 * cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n    precision = 1 * cross_val_score(my_pipeline, X, y,\n                          cv=5,\n                          scoring='precision')\n    recall = 1 * cross_val_score(my_pipeline, X, y,\n                          cv=5,\n                          scoring='recall')\n    \n    auc = 1 * cross_val_score(my_pipeline, X, y,\n                          cv=5,\n                          scoring='roc_auc')\n    return accuracy.mean(), precision.mean(), recall.mean(), auc.mean()","daf875a2":"X = X_train_baseline.drop(['Survived', 'PassengerId'], axis = 1)\ny = X_train_baseline['Survived']","5f71fe1c":"# Defini\u00e7\u00e3o do modelo\nrandomforest = RandomForestClassifier(random_state = 5, criterion = 'gini', max_depth = 10, max_features = 'auto', n_estimators = 500)\n\n# M\u00e9tricas de avalia\u00e7\u00e3o\nacc_RF, precision_RF, recall_RF, auc_RF = fit_predict_evaluate_model(randomforest,X,y)\n\nprint(\"Acur\u00e1cia:\\t {}\".format(acc_RF))\nprint(\"Precis\u00e3o:\\t {}\".format(precision_RF))\nprint(\"Recall:\\t\\t {}\".format(recall_RF))\n\nprint(\"AUC:\\t\\t {}\".format(auc_RF))","d7fa4274":"# Defini\u00e7\u00e3o do modelo\ngbk = GradientBoostingClassifier()\nacc_gbk, precision_gbk, recall_gbk, auc_gbk = fit_predict_evaluate_model(gbk,X,y)\n\n# M\u00e9tricas de avalia\u00e7\u00e3o\nprint(\"Acur\u00e1cia:\\t {}\".format(acc_gbk))\nprint(\"Precis\u00e3o:\\t {}\".format(precision_gbk))\nprint(\"Recall:\\t\\t {}\".format(recall_gbk))\n\nprint(\"AUC:\\t\\t {}\".format(auc_gbk))\n","3422d7ef":"# Defini\u00e7\u00e3o do modelo\nlightbm = lgb.LGBMClassifier(num_leaves = 64, objective= 'binary', seed = 7)\n\n\n# M\u00e9tricas de avalia\u00e7\u00e3o\nacc_lgb, precision_lgb, recall_lgb, auc_lgb = fit_predict_evaluate_model(lightbm,X,y)\n\nprint(\"Acur\u00e1cia:\\t {}\".format(acc_lgb))\nprint(\"Precis\u00e3o:\\t {}\".format(precision_lgb))\nprint(\"Recall:\\t\\t {}\".format(recall_lgb))\nprint(\"AUC:\\t\\t {}\".format(auc_lgb))\n","a0f006e3":"# Criando DataFrame\n\nmodelos = ['RandomForests', 'XGBoost', 'LGBM']\nacuracia = [acc_RF, acc_gbk, acc_lgb]\nprecision = [precision_RF, precision_gbk, precision_lgb]\nrecall = [recall_RF, recall_gbk, recall_lgb]\nAUC = [auc_RF, auc_gbk, auc_lgb]\ninitial_results = pd.DataFrame({'Modelo': modelos, 'Acur\u00e1cia': acuracia, 'Precis\u00e3o': precision, 'Recall': recall,\n                               'AUC': AUC})\n\ninitial_results","79c7e50a":"features = ['Sex', 'Age', 'Pclass']\n\n# Criando intera\u00e7\u00f5es entre features\ninteractions = pd.DataFrame(index = X_train.index)\n\nfor col1, col2 in itertools.combinations(features,2):\n    new_col_name = '_'.join([col1,col2])\n    \n    new_values = X_train[col1].map(str) + \"_\" + X_train[col2].map(str)\n    interactions[new_col_name] = new_values\n\n# Adicionando intera\u00e7\u00f5es ao dataframe\nX_optimization = X_train.join(interactions)\n\nX_optimization.head()","da4e4163":"# Colunas a sofrerem o encoding\nobject_cols = ['Sex', 'Embarked', 'Sex_Age', 'Sex_Pclass', 'Age_Pclass']\n\n# Colunas retiradas do dataset\nX_optimization.drop(['Name', 'Ticket'], axis = 1, inplace = True)\n\n\n\n# Criando o count encoder\ncount_enc = ce.CountEncoder(cols=object_cols)\n\n# Aprendendo e aplicando o encoding\ncount_enc.fit(X_optimization[object_cols])\nX_encoded = count_enc.transform(X_optimization[object_cols])\nX_optimization = X_optimization.join(X_encoded.add_suffix('_count'))\nX_optimization.drop(object_cols, axis = 1, inplace = True)\n","0ed81d74":"X_optm = X_optimization.drop(['Survived', 'PassengerId'], axis = 1)\ny_optm = X_optimization['Survived']","8476f142":"randomforest = RandomForestClassifier(random_state = 5, criterion = 'gini', max_depth = 10, max_features = 'auto', n_estimators = 500)\nrandomforest = randomforest.fit(X, y)\n\nacc_RF_opt, precision_RF_opt, recall_RF_opt, auc_RF_opt = fit_predict_evaluate_model(randomforest,X_optm,y_optm)\n\nprint(\"Acur\u00e1cia:\\t {}\".format(acc_RF_opt))\nprint(\"Precis\u00e3o:\\t {}\".format(precision_RF_opt))\nprint(\"Recall:\\t\\t {}\".format(recall_RF_opt))\n\nprint(\"AUC:\\t\\t {}\".format(auc_RF_opt))","c66dc340":"gbk_opt = GradientBoostingClassifier()\nacc_gbk_opt, precision_gbk_opt, recall_gbk_opt, auc_gbk_opt = fit_predict_evaluate_model(gbk_opt,X_optm,y_optm)\n\nprint(\"Acur\u00e1cia:\\t {}\".format(acc_gbk_opt))\nprint(\"Precis\u00e3o:\\t {}\".format(precision_gbk_opt))\nprint(\"Recall:\\t\\t {}\".format(recall_gbk_opt))\n\nprint(\"AUC:\\t\\t {}\".format(auc_gbk_opt))\nprint(\"AUC:\\t\\t {}\".format(auc_gbk))","e3c93b6d":"lightbm = lgb.LGBMClassifier(num_leaves = 64, objective= 'binary', seed = 7)\n\n\n\nacc_lgb_opt, precision_lgb_opt, recall_lgb_opt, auc_lgb_opt = fit_predict_evaluate_model(lightbm,X_optm,y_optm)\n\nprint(\"Acur\u00e1cia:\\t {}\".format(acc_lgb_opt))\nprint(\"Precis\u00e3o:\\t {}\".format(precision_lgb_opt))\nprint(\"Recall:\\t\\t {}\".format(recall_lgb_opt))\nprint(\"AUC:\\t\\t {}\".format(auc_lgb_opt))\n","a1d0244e":"modelos_opt = ['RandomForests_opt', 'XGBoost_opt', 'LGBM_opt']\nacuracia_opt = [acc_RF_opt, acc_gbk_opt, acc_lgb_opt]\nprecision_opt = [precision_RF_opt, precision_gbk_opt, precision_lgb_opt]\nrecall_opt = [recall_RF_opt, recall_gbk_opt, recall_lgb_opt]\nAUC_opt = [auc_RF_opt, auc_gbk_opt, auc_lgb_opt]\nfinal_results = pd.DataFrame({'Modelo': modelos_opt, 'Acur\u00e1cia': acuracia_opt, 'Precis\u00e3o': precision_opt, 'Recall': recall_opt,\n                               'AUC': AUC_opt})\n\n\npd.concat([initial_results, final_results])","c847229f":"# **7.1 RandomForests**","43bca574":"Uma das formas de melhorar a capacidade de previs\u00e3o dos modelos \u00e9 adicionar *features* que aumentam a quantidade de informa\u00e7\u00e3o da base da dados. Umas das formas de fazer isso \u00e9 por meio da combina\u00e7\u00e3o de vari\u00e1veis. Esse tipo de feature geralmente \u00e9 chamada de **intera\u00e7\u00e3o**.\n\nNo caso do conjunto de dados do Titanic, geraremos 3 novas features por meio desse processo:\n* Sex_Age: Intera\u00e7\u00e3o da idade do passageiro e seu g\u00eanero\n* Pclass_Age: Intera\u00e7\u00e3o da classe de passagem e a idade do passageiro\n* Pclass_Sex: Intera\u00e7\u00e3o da classe de passagem e a idade do passageiro\n\nPrimeiramente, devemos novamente tratar os valores faltantes","b280927f":"A seguir, podemos ver o levantamento estat\u00edstico das vari\u00e1veis num\u00e9ricas","2bf7cc5e":"# **4.5 Rela\u00e7\u00e3o da Classe de Passagem e a Sobreviv\u00eancia**","36fa0e13":"Os resultados inciais dos 3 modelos adotados para o caso b\u00e1sico podem ser observados no DataFrame a seguir","df904281":"Realizando um levantamento estat\u00edstico das vari\u00e1veis categ\u00f3ricas, podemos perceber que as vari\u00e1veis Name e Ticket possuem uma quantidade de valores \u00fanicos muito grande, como pode ser esperado. \n\nTamanha variedade n\u00e3o acrescenta informa\u00e7\u00f5es significativas nos modelos de aprendizagem. Desse modo, tanto para acrescentar simplicidade ao modelo quanto para evitar overfitting, retiramos tais features do conjunto de dados.","214b1aa5":"# **5.1 Valores Faltantes**","04bbbcd0":"Neste conjunto de dados, a porcentagem de homens \u00e9 de 65,7% e de mulheres \u00e9 de 35,2%.","54d32b40":"* Desenvolver e comparar modelos para classificar se um determinado passageiro sobreviver\u00e1 ou n\u00e3o ao desastre do Titanic\n","5faa0c81":"As t\u00e9cnicas associadas ao Machine Learning, tais como a regress\u00e3o linear e a regress\u00e3o log\u00edstica s\u00e3o largamente utilizadas para a resolu\u00e7\u00e3o de problemas de estima\u00e7\u00e3o e de classifica\u00e7\u00e3o. Dentro desse contexto, este notebook utiliza fun\u00e7\u00f5es contidas na biblioteca do sklearn para resolver um problema de classifica\u00e7\u00e3o: determinar se um passageiro a bordo do Titanic sobreviver\u00e1 ou n\u00e3o ao t\u00e3o famoso acidente. \n\nPrimeiramente realizamos uma breve an\u00e1lise explorat\u00f3ria dos dados importados pelo Kaggle. Em seguida, realizamos o tratamento dos dados de entrada, lidando com os valores faltantes e com as vari\u00e1veis categ\u00f3ricas, al\u00e9m de adicionar algumas features de modo a tentar melhorar a ferramenta de previs\u00e3o. Al\u00e9m, disso, implementamos fun\u00e7\u00f5es de cross-validation para medir a precis\u00e3o, a acur\u00e1cia, o recall e a AUC (*area under the curve*) para os modelos propostos. Por fim, 3 modelos de aprendizagem s\u00e3o propostos: RandomForests, XGBoost e o LGBoost","37961c11":"**Forma de busca usada pelo LGB**","7a170baf":"# **4.2 Taxa de Sobreviv\u00eancia Geral**","c267f839":"# **7.3 Light GBM **","1896abf1":"# **2. Objetivos**","4585429f":"**Forma de busca usada pelo XGB**","7c3da331":"# **9. Resultados Finais**","1f2deb7c":"Em seguida, definimos o modelo e calculamos as m\u00e9tricas de avalia\u00e7\u00e3o","6bc11f29":"# **7.2 Extreme Gradient Boosting**","d927ace1":"![leaf-wise.png](attachment:leaf-wise.png)","4ff9b7b8":"Nesta se\u00e7\u00e3o, lidamos com os valores faltantes na base de dados, al\u00e9m de realizar o tratamento das vari\u00e1veis categ\u00f3ricas para que possam serem utilizadas pelo modelo de aprendizagem.\nO processamento descrito aqui ser\u00e1 utilizado para constru\u00e7\u00e3o de um modelo base. Em seguida, outras tentativas ser\u00e3o realizadas de modo que se possa otimizar o modelo obtido.","183e407d":"# **8. Propostas de Otimiza\u00e7\u00e3o**","3135e8db":"J\u00e1 que a taxa de sobreviv\u00eancia \u00e9 de 38,4%, podemos inferir que a acur\u00e1cia do nosso modelo deve ser no m\u00ednimo melhor do que 61,6%, j\u00e1 que um modelo que apenas aponta que toda entrada \u00e9 dada como n\u00e3o sobrevivente deve atingir essa taxa de sucesso.","efbc738e":"# **8.1 Gera\u00e7\u00e3o de *Features***\n\n","327f42c9":"Inicialmente, separamos o conjunto de dados tratados em features e na vari\u00e1vel alvo (label).","5d7d7aea":"# **4.3 Rela\u00e7\u00e3o do G\u00eanero dos Passageiros com a sua Sobreviv\u00eancia**\n","df3b826b":"# **1. Introdu\u00e7\u00e3o**","7c3c715b":"# **7. Cria\u00e7\u00e3o e Teste de Modelos**","e5d54445":"# **5.2 Tratamento das Vari\u00e1veis Categ\u00f3ricas**","6af73f0e":"Por fim, devemos novamente criar modelos RandomForests, XGBoost e LGBM ao novo conjunto de dados, com as mudan\u00e7as aplicadas. As c\u00e9lulas de c\u00f3digo a seguir repetem o procedimento da se\u00e7\u00e3o 7.","667e847f":"# **7.2 Resultados Iniciais**","baa857db":"O One-Hot encoding, embora bastante utilizado para realizar o encoding de vari\u00e1veis categ\u00f3rias n\u00e3o \u00e9 o \u00fanico dispon\u00edvel. Como alternativa, utilizaremos o *count encoder*, que substitui cada valor categ\u00f3rico pelo n\u00famero de vezes que o mesmo aparece no conjunto de dados. Nesse tipo de encoding, valores raros tendem a ter contagens semelhantes,ent\u00e3o, \u00e9 poss\u00edvel classifica-los juntos no momento da previs\u00e3o. \u00c9 improv\u00e1vel que valores comuns com contagens grandes tenham a mesma contagem exata que outros valores. Portanto, os valores comuns e importantes obt\u00eam seu pr\u00f3prio agrupamento.","0c3bc00a":"O LightGBM \u00e9 outro framework que utiliza o gradiente, tamb\u00e9m baseado nos algoritmos de busca em \u00e1rvores.\n\nA diferen\u00e7a b\u00e1sica do LGBM para o XGB \u00e9 que o LGBM expande as \u00e1rvores de maior profundidade primeiro, enquanto o XGB expande os n\u00f3s de um mesmo n\u00edvel primeiro. A diferen\u00e7a \u00e9 basicamente a mesma entre algoritmos de busca em profundidade e algoritmos de busca em largura. A seguinte imagem denota tal diferen\u00e7a.","96fe562f":"O gr\u00e1fico em formato de pizza a seguir mostra a porcentagem de pessoas que sobreviveram e que n\u00e3o sobreviveram ao desastre.","1d808d6f":"O histograma a esquerda nos mostra a fun\u00e7\u00e3o de distribui\u00e7\u00e3o da idade dos passageiros. J\u00e1 o gr\u00e1fico de barras a direita indica a rela\u00e7\u00e3o da idade dos passageiros com sua sobreviv\u00eancia.\n\nOs grupos de idade foram divididos da seguinte maneira:\n\n* Grupo 1: Passageiros com idade menor de 20 anos\n* Grupo 2: Passageiros entre 20 e 30 anos\n* Grupo 3: Passageiros entre 30 e 40 anos\n* Grupo 4: Passageiros entre 40 e 60 anos\n* Grupo 5: Passageiros acima de 60 anos","14a194a1":"O XGBoost, por sua vez, \u00e9 um m\u00e9todo que passa por ciclos para adicionar iterativamente modelos a um conjunto.\n\nCome\u00e7a-se inicializando o conjunto com um \u00fanico modelo, cujas previs\u00f5es podem ser bem ing\u00eanuas. (Mesmo que suas previs\u00f5es sejam muito imprecisas, as adi\u00e7\u00f5es subsequentes ao conjunto abordar\u00e3o esses erros.)\n\nEnt\u00e3o, come\u00e7amos o ciclo:\n\n* Primeiro, usamos o conjunto atual para gerar previs\u00f5es para cada observa\u00e7\u00e3o no conjunto de dados. Para fazer uma previs\u00e3o, adicionamos as previs\u00f5es de todos os modelos no conjunto.\n* Essas previs\u00f5es s\u00e3o usadas para calcular uma fun\u00e7\u00e3o de perda (como erro m\u00e9dio quadr\u00e1tico, por exemplo).\n* Em seguida, usamos a fun\u00e7\u00e3o de perda para ajustar um novo modelo que ser\u00e1 adicionado ao conjunto. Especificamente, determinamos os par\u00e2metros do modelo para que a adi\u00e7\u00e3o desse novo modelo ao conjunto reduza a perda. (Observa\u00e7\u00e3o: o \"gradiente\" em \"aumento de gradiente\" refere-se ao fato de que usaremos a descida do gradiente na fun\u00e7\u00e3o de perda para determinar os par\u00e2metros neste novo modelo.)\n* Por fim, adicionamos o novo modelo ao conjunto e repetimos o ciclo","67e2962c":"![level-wise.png](attachment:level-wise.png)","acd6ca7a":"# **4.4 Rela\u00e7\u00e3o da Idade dos Passageiros com sua Sobreviv\u00eancia**","4732e15b":"Em problemas de classifica\u00e7\u00e3o de uma vari\u00e1vel alvo, tal como o do conjunto de dados do Titanic, as m\u00e9tricas mais comuns de avalia\u00e7\u00e3o s\u00e3o a acur\u00e1cia, a precis\u00e3o, o recall e a AUC (*area under the curve*).\n\n* **Acur\u00e1cia:** A acur\u00e1cia se refere a quantidade de predi\u00e7\u00f5es que o modelo de fato acertou, ou seja, a raz\u00e3o entre a quantidade de predi\u00e7\u00f5es corretas e a quantidade total de predi\u00e7\u00f5es\n\n* **Precis\u00e3o:** A precis\u00e3o \u00e9 a raz\u00e3o entre a quantidade de predi\u00e7\u00f5es verdadeiramente positivas pela quantidade de predi\u00e7\u00f5es que foram dadas como positivas\n\n* **Recall:** Trata-se da raz\u00e3o entre a quantidade de predi\u00e7\u00f5es verdadeiramente positivas pela quantidade de predi\u00e7\u00f5es verdadeiramente positivas somadas com as falsas negativas\n\n* **AUC:** \u00c9 uma m\u00e9trica de avalia\u00e7\u00e3o que representa a probabilidade de um resultado positivo ser dado como positivo enquanto um resultado negativo \u00e9 dado como negativo\n\nAliado \u00e0 tais m\u00e9tricas, utilizaremos uma t\u00e9nica de avalia\u00e7\u00e3o de Cross-Validation. Na Cross Validation, executamos nosso processo de modelagem em diferentes subconjuntos de dados para obter v\u00e1rias medidas de qualidade do modelo\n\nA seguinte fun\u00e7\u00e3o (fit_predict_evaluate_model) permite que avaliemos um modelo utilizando essa t\u00e9cnica e as m\u00e9tricas definidas na se\u00e7\u00e3o 6.2. Temos os seguintes par\u00e2metros como entrada:\n\nclf: modelo de ML a ser validado\nX: features de entrada\ny: vari\u00e1vel alvo","1ea2c0d5":"Primeiramente, visualizamos as vari\u00e1veis categ\u00f3ricas presentes no dataset.","f12e528c":"O conjunto de dados em quest\u00e3o pode ser observado em [Titanic](https:\/\/www.kaggle.com\/c\/titanic\/data). Possui 9 vari\u00e1veis descritivas, conforme mostradas a seguir:\n\n* pclass:\tClasse do ticket do passageiro\t\n* sex:\tSexo do passageiro\t\n* Age:\tIdade em anos\t\n* sibsp:\tN\u00famero de irm\u00e3os ou c\u00f4njuges a bordo\t\n* parch: N\u00famero de pais ou de filhos a bordo\t\n* ticket:\tN\u00famero do ticket\t\n* fare:\tValor da passagem\t\n* cabin:\tN\u00famero da cabine\t\n* embarked: Porto de embarque\n\nA vari\u00e1vel alvo do dataset \u00e9 a 'survived', que indica se o passageiro sobreviveu ou n\u00e3o ao embarque.\n\nA c\u00e9lula a seguir importa todos os m\u00f3dulos necess\u00e1rios ao desenvolvimento deste modelo.","ff4cf011":"# **4.1 Descri\u00e7\u00e3o Estat\u00edstica dos Dados**","575f67dd":"As RandomForest s\u00e3o uma alternativa ao uso das \u00e1rvores de decis\u00e3o, j\u00e1 que resolvem muitos problemas relacionados ao underfitting e ao overfitting. Esse modelo usa muitas \u00e1rvores e faz uma previs\u00e3o calculando a m\u00e9dia das previs\u00f5es de cada \u00e1rvore componente. Geralmente, ele tem uma precis\u00e3o preditiva muito melhor do que uma \u00fanica \u00e1rvore de decis\u00e3o e funciona bem com par\u00e2metros padr\u00e3o. ","2b070801":"Uma das etapas mais importantes no desenvolvimento de bons modelos de Machine Learning \u00e9 o conhecimento dos dados com os quais se est\u00e1 lidando. Esta se\u00e7\u00e3o tem como objetivo entender quais s\u00e3o as principais vari\u00e1veis que afetam a sobreviv\u00eancia dos passageiros e de que maneira isso ocorre.\n","a29d853b":"Podemos perceber que as colunas relativas \u00e0 idade (Age) e ao n\u00famero da cabine do passageiro s\u00e3o as que mais possuem valores faltando no conjunto de dados. \n\nDentre essas, o n\u00famero da cabine possui um valor muito significativo e, portanto, ser\u00e1 retirada da an\u00e1lise. A idade dos passageiros, por sua vez, traz informa\u00e7\u00f5es pertinentes que podem ser utilizadas pelo modelo de aprendizagem. \n\nDesse modo, utilizaremos o valor mediano da idade para preencher o lugar dos dados faltantes. Para a vari\u00e1vel Embarked, utilizaremos o valor mais frequente para substituir os dados que faltam.","8c1afacd":"![Titanic.jpg](attachment:Titanic.jpg)","1360005e":"Inicialmente, identificamos as colunas que possuem valores faltantes no conjunto de dados","6613397d":"# **4. An\u00e1lise Explorat\u00f3ria dos dados**","48e1854e":"# **4.6 Correla\u00e7\u00e3o das vari\u00e1veis**\n","8ad77bab":"J\u00e1 para as vari\u00e1veis Sex e Embarked, que possuem correla\u00e7\u00e3o com a vari\u00e1vel alvo, conforme mostrado na se\u00e7\u00e3o 4 deste documento, aplicamos, inicialmente, o One-Hot encoding.","4b06ebcb":"* De acordo com as m\u00e9tricas de avalia\u00e7\u00e3o implementadas, podemos perceber que todos os modelos criados apresentam resultados bastante semelhantes. O modelo de RandomForests aplicado ao conjunto de dados com modifica\u00e7\u00f5es levantadas na se\u00e7\u00e3o 8 apresentou um resultado levemente superior aos demais\n\n**Algumas outras formas de mudan\u00e7a poderiam ser citadas para poss\u00edvel melhoria do modelo**: \n\n* Tratamento da vari\u00e1vel 'name' para selecionar apenas o prefixo de cada nome (retornando o t\u00edtulo atribu\u00eddo a cada passageiro e, consequentemente, informa\u00e7\u00f5es acerca da sua posi\u00e7\u00e3o social)\n* Mudan\u00e7a no imput da vari\u00e1vel 'Age' (utilizar a m\u00e9dia ou mediana da idade de cada grupo 'Pclass', desse modo, os valores faltantes seriam substitu\u00eddos pela m\u00e9dia ou mediana de idade do grupo 'Pclass' em que o determinado passageiro se encontra);\n\n* Normaliza\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas (alguns modelos respondem melhor \u00e0 coeficientes pequenos e normais das features). Nesse caso, poder\u00edamos aplicar a regulariza\u00e7\u00e3o L1 ou L2.","d08efdda":"# **3. Descri\u00e7\u00e3o e Importa\u00e7\u00e3o do Conjunto de Dados**","9d7a6428":"# **6. Defini\u00e7\u00e3o das M\u00e9tricas de Avalia\u00e7\u00e3o**","1a11c55c":"# **8.2 Utiliza\u00e7\u00e3o de Encoder Diferente**","e4a44bdb":"Em seguida, realizamos a importa\u00e7\u00e3o dos dados de treino e de teste, fornecidos pelo pr\u00f3prio notebook Kaggle.\nUma vis\u00e3o ger\u00e3o do conjunto de dados pode ser observada a seguir","10f6aad5":"# **5. Preprocessamento da Base de Dados**","f09237e3":"# **10. Conclus\u00f5es**"}}