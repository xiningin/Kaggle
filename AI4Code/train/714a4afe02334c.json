{"cell_type":{"c6af0e84":"code","6bbdcae6":"code","a7dcef89":"code","11b2b7b1":"code","1ba88836":"code","3da1e2f8":"code","186cc6c6":"code","a17d1920":"code","4a95891e":"code","b940cd8e":"code","11a2b4df":"code","78e0d9de":"code","39a1795a":"code","bd89796c":"code","2ef643aa":"code","4752a0d6":"code","6cc656ac":"code","bcc8e125":"code","bd8bab55":"code","0ee8a950":"code","eed61263":"code","1931832b":"code","bba1c898":"code","1a0d90aa":"code","b66a2419":"code","3b137ee1":"code","7f364f68":"code","5f41b703":"code","7bd55319":"code","f9cda678":"code","1695bb1f":"code","cc2a7064":"code","d15121e7":"code","f76f0f48":"code","4c2eadbe":"code","1f620c1b":"code","4e73034a":"code","2703205d":"code","ed3ecea3":"code","dc1da036":"code","4ddda967":"code","9f3f68da":"code","cbfa05fa":"code","44febd27":"code","47e13fdb":"code","6d9cbe52":"code","db83b9ba":"code","fd22697e":"code","cf85b986":"code","bf46b757":"code","84b2e222":"code","2e15f53e":"code","909fc3dc":"code","a107e22d":"code","c223dd0a":"code","b0f17533":"code","009fd623":"code","39b8df27":"code","facfd7ce":"code","ad488812":"code","745fa1fb":"code","521fa816":"code","26d3d854":"code","c8298067":"code","5007b2db":"code","c5d3fb4b":"code","ca9f9b1c":"code","9276ec3a":"code","525177c8":"code","a26c839d":"code","bb9b20d7":"code","42f60861":"code","09dbc780":"code","362dd236":"code","5d3f6501":"code","e5c21e68":"code","d41f86ec":"code","72b1fbe9":"code","5030ab3c":"code","8198b605":"code","24ecfff4":"code","ac4e1bcb":"code","563fc5c1":"code","9a44c758":"code","cea65734":"code","2ef4d0e4":"code","71647210":"code","8b33c057":"code","e7e190f1":"code","c11e6e51":"code","db815efd":"code","54d4d328":"code","25788c57":"code","6ee65ee1":"code","5f875043":"code","7faf38f6":"code","ffa920af":"code","db02780d":"code","3a9b8412":"code","cd99ef09":"code","31fd6101":"markdown","cfd3cfa5":"markdown","d9ab5006":"markdown","8aefd5a1":"markdown","988fe367":"markdown","d3d19a57":"markdown","ee47824a":"markdown","058504ce":"markdown","973a2d5b":"markdown","e0bb94cc":"markdown","9f8dbf36":"markdown","aebbbe3b":"markdown"},"source":{"c6af0e84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bbdcae6":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sc\nimport pandas_profiling   #need to install using anaconda prompt (pip install pandas_profiling)\n%pylab inline","a7dcef89":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport sklearn.metrics as metrics\nimport statsmodels.formula.api as sm","11b2b7b1":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math","1ba88836":"import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression","3da1e2f8":"from collections import defaultdict\nimport time\nimport gc\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets.samples_generator import make_regression\nfrom sklearn.ensemble.forest import RandomForestRegressor\nfrom sklearn.linear_model.ridge import Ridge\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\nfrom sklearn.svm.classes import SVR\nfrom sklearn.utils import shuffle","186cc6c6":"train = pd.read_csv (\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain","a17d1920":"train.info()","4a95891e":"train.describe()","b940cd8e":"train.dtypes","11a2b4df":"test = pd.read_csv (\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest","78e0d9de":"test.info()","39a1795a":"test.describe()","bd89796c":"test.dtypes","2ef643aa":"pandas_profiling.ProfileReport(train)","4752a0d6":"pandas_profiling.ProfileReport(test)","6cc656ac":"numeric_var_names=[key for key in dict(train.dtypes) if dict(train.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\ncat_var_names=[key for key in dict(train.dtypes) if dict(train.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","bcc8e125":"train_num = train[numeric_var_names]","bd8bab55":"train_cat = train[cat_var_names]","0ee8a950":"# Use a general function that returns multiple values\ndef var_summary(x):\n    return pd.Series([x.count(), x.isnull().sum(), x.sum(), x.mean(), x.median(),  x.std(), x.var(), x.min(), x.dropna().quantile(0.01), x.dropna().quantile(0.05),x.dropna().quantile(0.10),x.dropna().quantile(0.25),x.dropna().quantile(0.50),x.dropna().quantile(0.75), x.dropna().quantile(0.90),x.dropna().quantile(0.95), x.dropna().quantile(0.99),x.max()], \n                  index=['N', 'NMISS', 'SUM', 'MEAN','MEDIAN', 'STD', 'VAR', 'MIN', 'P1' , 'P5' ,'P10' ,'P25' ,'P50' ,'P75' ,'P90' ,'P95' ,'P99' ,'MAX'])\n\nnum_summary = train_num.apply(lambda x: var_summary(x)).T","eed61263":"num_summary","1931832b":"import numpy as np\nfor col in train_num.columns:\n    percentiles = train_num[col].quantile([0.01,0.99]).values\n    train_num[col] = np.clip(train_num[col], percentiles[0], percentiles[1])","bba1c898":"#Handling missings - Method2\ndef Missing_imputation(x):\n    x = x.fillna(x.median())\n    return x\n\ntrain_num=train_num.apply(lambda x: Missing_imputation(x))","1a0d90aa":"#Handling missings - Method2\ndef Cat_Missing_imputation(x):\n    x = x.fillna(x.mode())\n    return x\n\ntrain_cat=train_cat.apply(lambda x: Cat_Missing_imputation(x))","b66a2419":"# An utility function to create dummy variable\ndef create_dummies( df, colname ):\n    col_dummies = pd.get_dummies(df[colname], prefix=colname, drop_first=True)\n    df = pd.concat([df, col_dummies], axis=1)\n    df.drop( colname, axis = 1, inplace = True )\n    return df\n\nfor c_feature in train_cat.columns:\n    train_cat[c_feature] = train_cat[c_feature].astype('category')\n    train_cat = create_dummies(train_cat , c_feature )","3b137ee1":"train_cat.head().T","7f364f68":"train_num.head().T","5f41b703":"train_new = pd.concat([train_num, train_cat], axis=1)\ntrain_new.head()","7bd55319":"pandas_profiling.ProfileReport(train_new)","f9cda678":"numeric_var_names=[key for key in dict(train_new.dtypes) if dict(train_new.dtypes)[key] in ['float64', 'int64', 'float32', 'int32','uint8']]\ncat_var_names=[key for key in dict(train_new.dtypes) if dict(train_new.dtypes)[key] in ['object']]\nprint(numeric_var_names)\nprint(cat_var_names)","1695bb1f":"train_new_num = train_new[numeric_var_names]","cc2a7064":"train_new_cat = train_new[cat_var_names]","d15121e7":"train_new_num.info()","f76f0f48":"np.log(train_new.SalePrice).hist()","4c2eadbe":"# Distribution of variables\nimport seaborn as sns\nsns.distplot(np.log(train_new.SalePrice))","1f620c1b":"# correlation matrix (ranges from 1 to -1)\ncorrm=train_new.corr()\ncorrm.to_csv('corrm.csv')","4e73034a":"# visualize correlation matrix in Seaborn using a heatmap\nsns.heatmap(train_new.corr())","2703205d":"features = train_new[train_new.columns.difference( ['SalePrice'] )]\ntarget = train_new['SalePrice']","ed3ecea3":"features.columns","dc1da036":"features.shape","4ddda967":"target","9f3f68da":"from sklearn.feature_selection import RFE","cbfa05fa":"from sklearn.linear_model import LinearRegression","44febd27":"import itertools","47e13fdb":"features.shape","6d9cbe52":"lm = LinearRegression()","db83b9ba":"# create the RFE model and select 10 attributes\nrfe = RFE(lm, n_features_to_select=30)\nrfe = rfe.fit(features,target)","fd22697e":"rfe.get_support()","cf85b986":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nimport itertools\n\nlm = LinearRegression()\n\n# create the RFE model and select 10 attributes\nrfe = RFE(lm, n_features_to_select=30)\nrfe = rfe.fit(features,target)","bf46b757":"rfe.get_support()","84b2e222":"# summarize the selection of the attributes\nfeature_map = [(i, v) for i, v in itertools.zip_longest(features.columns, rfe.get_support())]","2e15f53e":"feature_map","909fc3dc":"#Alternative of capturing the important variables\nRFE_features=features.columns[rfe.get_support()]","a107e22d":"RFE_features","c223dd0a":"features1 = features[RFE_features]","b0f17533":"features1.head()","009fd623":"# Feature Selection based on importance\nfrom sklearn.feature_selection import f_regression\nF_values, p_values  = f_regression(  features1, target )","39b8df27":"import itertools\nf_reg_results = [(i, v, z) for i, v, z in itertools.zip_longest(features1.columns, F_values,  ['%.3f' % p for p in p_values])]","facfd7ce":"f_reg_results=pd.DataFrame(f_reg_results, columns=['Variable','F_Value', 'P_Value'])","ad488812":"f_reg_results.sort_values(by=['F_Value'],ascending = False)","745fa1fb":"f_reg_results.P_Value = pd.to_numeric(f_reg_results.P_Value)","521fa816":"f_reg_results_new=f_reg_results[f_reg_results.P_Value<=0.2]","26d3d854":"f_reg_results_new","c8298067":"f_reg_results_new.to_csv(\"f_reg_results_new.csv\")","5007b2db":"f_reg_results_new.info()","c5d3fb4b":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif","ca9f9b1c":"features = train_new[train_new.columns.difference( ['SalePrice'] )]\ntarget = train_new['SalePrice']\nfeatures_new = SelectKBest(f_classif, k=15).fit(features, target )","9276ec3a":"features_new.get_support()","525177c8":"features_new.scores_","a26c839d":"# summarize the selection of the attributes\nimport itertools\nfeature_map = [(i, v) for i, v in itertools.zip_longest(features.columns, features_new.get_support())]\nfeature_map\n#Alternative of capturing the important variables\nKBest_features=features.columns[features_new.get_support()]\n\nselected_features_from_KBest = features[KBest_features]","bb9b20d7":"KBest_features","42f60861":"list_vars1 = list(f_reg_results_new.Variable)\nlist_vars1","09dbc780":"all_columns = \"+\".join(list_vars1)\nmy_formula = \"SalePrice~\" + all_columns\n\nprint(my_formula)","362dd236":"import statsmodels.formula.api as sm","5d3f6501":"model = sm.ols('SalePrice~BldgType_Twnhs+BsmtExposure_Gd +Condition2_PosN +ExterQual_Fa +ExterQual_Gd +ExterQual_TA +GarageCond_Fa +GarageCond_Po +GarageCond_TA +GarageQual_Fa +GarageQual_Gd+GarageQual_Po +GarageQual_TA+KitchenQual_Fa +KitchenQual_Gd +KitchenQual_TA +Neighborhood_Crawfor +Neighborhood_NoRidge +Neighborhood_NridgHt +Neighborhood_StoneBr +RoofMatl_CompShg +RoofMatl_WdShake +RoofMatl_WdShngl+Condition2_RRAn+ExterCond_Po+ExterQual_TA+Exterior1st_ImStucc+Exterior2nd_Other+GarageCars+GrLivArea+Neighborhood_NridgHt+OverallQual+SaleCondition_Alloca+SaleCondition_Partial+SaleType_Con+SaleType_New+TotalBsmtSF',data = train_new)","e5c21e68":"model = model.fit()","d41f86ec":"model.summary()","72b1fbe9":"print(model.summary())","5030ab3c":"my_formula = 'SalePrice~BldgType_Twnhs+BsmtExposure_Gd+Condition2_PosN+ExterQual_Fa+GarageCond_Po+GarageQual_Fa +GarageQual_Gd+GarageQual_Po+KitchenQual_Fa+Neighborhood_Crawfor +Neighborhood_NoRidge +Neighborhood_NridgHt +Neighborhood_StoneBr +RoofMatl_CompShg +RoofMatl_WdShake +RoofMatl_WdShngl+Condition2_RRAn+ExterCond_Po+Exterior1st_ImStucc+Exterior2nd_Other+GarageCars+GrLivArea+Neighborhood_NridgHt+SaleCondition_Alloca+SaleType_Con+SaleType_New+TotalBsmtSF'\nmy_formula","8198b605":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom patsy import dmatrices","24ecfff4":"# get y and X dataframes based on this regression\ny, X = dmatrices(my_formula,train_new,return_type='dataframe')","ac4e1bcb":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.round(1)","563fc5c1":"Train, Test = train_test_split( train_new, test_size = 0.3, random_state = 123456 )","9a44c758":"print(len(Train))\nprint(len(Test))","cea65734":"import statsmodels.formula.api as smf","2ef4d0e4":"my_formula = 'SalePrice~BldgType_Twnhs+BsmtExposure_Gd+Condition2_PosN+ExterQual_Fa+GarageCond_Po+GarageQual_Fa+GarageQual_Gd+GarageQual_Po+KitchenQual_Fa+Neighborhood_Crawfor+Neighborhood_NoRidge+Neighborhood_NridgHt+Neighborhood_StoneBr+RoofMatl_CompShg+RoofMatl_WdShake+RoofMatl_WdShngl+Condition2_RRAn+ExterCond_Po+Exterior1st_ImStucc+Exterior2nd_Other+GarageCars+GrLivArea+SaleCondition_Alloca+SaleType_Con+SaleType_New+TotalBsmtSF'\nmy_formula","71647210":"model = smf.ols(my_formula, data=Train).fit()\nprint(model.summary())","8b33c057":"Train['pred'] = pd.DataFrame(model.predict(Train))","e7e190f1":"Train.head()","c11e6e51":"Test['pred'] = pd.DataFrame(model.predict(Test))\nTest.head()","db815efd":"# calculate these metrics by hand!\nfrom sklearn import metrics\nimport numpy as np\nimport scipy.stats as stats","54d4d328":"#Train Data\nMAPE_Train = np.mean(np.abs(Train.SalePrice - Train.pred)\/Train.SalePrice)\nprint(MAPE_Train)\n\n\nRMSE_Train = metrics.mean_squared_error(Train.SalePrice ,Train.pred)\nprint(RMSE_Train)\n\nCorr_Train = stats.stats.pearsonr(Train.SalePrice,Train.pred)\nprint(Corr_Train)\n\n\n#Test Data\nMAPE_Test = np.mean(np.abs(Test.SalePrice - Test.pred)\/Test.SalePrice)\nprint(MAPE_Test)\n\nRMSE_Test = metrics.mean_squared_error(Test.SalePrice, Test.pred)\nprint(RMSE_Test)\n\nCorr_Test = stats.stats.pearsonr(Test.SalePrice, Test.pred)\nprint(Corr_Test)","25788c57":"model.resid.hist(bins=100)","6ee65ee1":"#Decile analysis - Train\n\nTrain['Deciles']=pd.qcut(Train['pred'],10, labels=False)\n\navg_actual = Train[['Deciles','SalePrice']].groupby(Train.Deciles).mean().sort_index(ascending=False)['SalePrice']\navg_pred = Train[['Deciles','pred']].groupby(Train.Deciles).mean().sort_index(ascending=False)['pred']\n\nDecile_analysis_Train = pd.concat([avg_actual, avg_pred], axis=1)\n\nDecile_analysis_Train","5f875043":"#Decile analysis - Train\nTest['Deciles']=pd.qcut(Test['pred'],10, labels=False)\n\navg_actual_Test = Test[['Deciles','SalePrice']].groupby(Test.Deciles).mean().sort_index(ascending=False)['SalePrice']\navg_pred_Test = Test[['Deciles','pred']].groupby(Test.Deciles).mean().sort_index(ascending=False)['pred']\n\nDecile_analysis_Test = pd.concat([avg_actual_Test, avg_pred_Test], axis=1)\n\nDecile_analysis_Test","7faf38f6":"train_new","ffa920af":"from sklearn import model_selection\ndef split(df):\n    train_new = df\n    train_new[\"kfold\"] = -1\n    train_new = train_new.sample(frac=1).reset_index(drop=True)\n    kf = model_selection.KFold(n_splits=5, shuffle=False, random_state=42)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_new)):\n            print(len(train_idx), len(val_idx))\n            train_new.loc[val_idx, 'kfold'] = fold\n    return train_new","db02780d":"train_new","3a9b8412":"train_folds = split(train_new)","cd99ef09":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge,ElasticNet,Lasso\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfor FOLD in [0,1,2,3,4]:\n    print(\"The Fold is :\",FOLD)\n    FOLD_MAPPPING = {\n        0: [1, 2, 3, 4],\n        1: [0, 2, 3, 4],\n        2: [0, 1, 3, 4],\n        3: [0, 1, 2, 4],\n        4: [0, 1, 2, 3]\n    }\n\n    train_new = train_folds[train_folds.kfold.isin(FOLD_MAPPPING.get(FOLD))].reset_index(drop=True)\n    valid_new = train_folds[train_folds.kfold==FOLD].reset_index(drop=True)\n\n    ytrain = train_new.SalePrice.values\n    yvalid = valid_new.SalePrice.values\n    train_new = train_new.drop([\"kfold\"], axis=1)\n    valid_new = valid_new.drop([\"kfold\"], axis=1)\n\n    valid_new = valid_new[train_new.columns]\n\n\n    def rmse_cv(model):\n        rmse= np.sqrt(-cross_val_score(model, train_new, ytrain, scoring=\"neg_mean_squared_error\", cv = 5))\n        return(rmse)\n\n    lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n    model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                                 learning_rate=0.05, max_depth=3, \n                                 min_child_weight=1.7817, n_estimators=2200,\n                                 reg_alpha=0.4640, reg_lambda=0.8571,\n                                 subsample=0.5213, silent=1,\n                                 random_state =7, nthread = -1)\n    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n    GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n    model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n    KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n    score = rmse_cv(lasso)\n    print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    score = rmse_cv(ENet)\n    print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    score = rmse_cv(KRR)\n    print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    score = rmse_cv(GBoost)\n    print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    score = rmse_cv(model_xgb)\n    print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n    score = rmse_cv(model_lgb)\n    print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n    \n    \n    print(train_new.shape)","31fd6101":"Variable reduction using Select K-Best technique","cfd3cfa5":"Variance Inflation Factor (VIF)","d9ab5006":"Conclusion\n\nWe reached the end of our exercise.\n\nWe learned how to Approach a Regression Problem and also how to approach any machine learning problem in general.\n\nIf you liked the Kernal then don't forget to hit the Upvote! :) I will post more kernals soon!\n\n","8aefd5a1":"Split the data into train & test","988fe367":"Assumptions of Regression","d3d19a57":"Variable Reduction\n\nRecursive Feature Elimination","ee47824a":"Let's Define the Models and Do a Scoring","058504ce":"Outlier treatment","973a2d5b":"F-Regression","e0bb94cc":"In this notebook, let us try and explore the data given for House Prices: Advanced Regression Techniques.Let us know a little more about the competition.\n\nWhat decides a House Price?\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But the ral world experiments proves that price negotiations are much more dependent on other Factors rather than the number of bedrooms or a white-picket fence.\n\nObjective:\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","9f8dbf36":"Recursive Feature Elimination","aebbbe3b":"Modeling"}}