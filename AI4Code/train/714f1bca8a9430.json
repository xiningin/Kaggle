{"cell_type":{"5c180305":"code","7a6edc38":"code","4ae70e65":"code","5c282950":"code","7a409326":"code","11bd2cce":"code","06309f49":"code","6742cdb1":"code","5fd3dc62":"code","f88f333a":"code","712b760a":"code","9ee5ed0c":"code","08a86705":"code","e9d79dfc":"code","923b064c":"code","10dda263":"code","c7908b27":"code","e62eae90":"markdown","0fa6c91c":"markdown","1f2a0988":"markdown","8fb03215":"markdown","3f9adfa6":"markdown","e9d076fe":"markdown","a0f0ec45":"markdown","41d81dd1":"markdown","2c917a09":"markdown","2c32ce80":"markdown","a4ef508b":"markdown","db198355":"markdown","e05e5ab2":"markdown","a882d7ce":"markdown","d5a47015":"markdown","ac2382f1":"markdown","56774788":"markdown","f63c21fc":"markdown","269f43e7":"markdown","c729eaf7":"markdown","1fdd2ded":"markdown","27c6305f":"markdown","89a21b09":"markdown","b5da7329":"markdown","2bb58103":"markdown","37367a59":"markdown","f06b51fb":"markdown","6e9cf299":"markdown","4743cb4b":"markdown"},"source":{"5c180305":"# Import the dependencies\nimport numpy as np\nfrom scipy.linalg import toeplitz, cholesky, sqrtm, inv\n# import scipy.linalg as la\nfrom scipy import signal\nfrom scipy.integrate import odeint\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")\nprint(\"Imports done\")","7a6edc38":"def g_gp(x,v):\n    \"\"\"\n    Generative process, equation of sensory mapping: g(x) at point x    \n   \n    INPUTS:\n        x       - Hidden state, depth position in centimetres\n        v       - Hidden causal state, in this example not used\n        \n    OUTPUT:\n        y       - Temperature in degrees celsius\n        \n    \"\"\"\n    #t0= 20\n    #y=(t0-8)\/(0.2*x**2+1)+8\n    t0=25\n    return t0 -16 \/ (1 + np.exp(5-x\/5))\n\ndef dg_gp(x):\n    \"\"\"\n    Partial derivative of generative process towards x, equation of sensory mapping: g'(x) at point x    \n   \n    INPUTS:\n        x       - Position in centimetres    \n        \n    OUTPUT:\n        y       - Temperature in degrees celsius\n        \n    \"\"\"\n    #t0= 20\n    #y=-2*0.2*x*(t0-8)\/(0.2*x**2+1)**2\n    \n    return -16\/5* np.exp(5-x\/5) \/ (np.exp(5-x\/5)+1)**2\n\n# Show the temperature curve\nx_show = np.arange (-0,50,0.01)\ny_show = g_gp(x_show,0)\ndy_show = dg_gp(x_show)\nplt.plot(y_show, x_show)\n#plt.plot(dy_show, x_show)\nplt.ylabel('Depth (centimeters)')\nplt.xlabel('Temperature (\u00b0 C)')\nplt.gca().invert_yaxis()\nplt.vlines(17, 50, 25, colors='r', linestyles='dashed')\nplt.hlines(25, 10,17, colors='r', linestyles='dashed')\nplt.text(17.3,27,\"Optimal temparature 17\u00b0 C\")\nplt.show;\n\nprint('Temperature at 25 centimetres is: ', g_gp(25,0), ' degrees celsius')","4ae70e65":"# Setting up the time data:\ndt = 0.005; # integration step, average neuron resets 200 times per second\nT = 15+dt; # maximum time considered\nt = np.arange(0,T,dt)\nN= t.size #Amount of data points\nprint ('Amount of data points: ', N)\nprint ('Starting with', t[0:5])\nprint ('Ending with', t[N-5:N])\nprint ('Data elements', np.size(t))","5c282950":"class ai_capsule():\n    \"\"\"\n        Class that constructs a group of neurons that perform Active Inference for one hidden state, one sensory input, one prior\n        In neurology it could eg represent a (micro) column\n        \n        Version 0.3 Alternative approach where the prediction error is cast as a motion \n    \"\"\"\n    def __init__(self,dt, mu_v, Sigma_w, Sigma_z, a_mu):   \n        self.dt = dt    # integration step\n        self.mu_x = mu_v   # initializing the best guess of hidden state by the hierarchical prior\n        self.F = 0      # Free Energy\n        self.eps_x = 0  # delta epsilon_x, prediction error on hidden state\n        self.eps_y = 0  # delta epsilon_y, prediction error on sensory measurement\n        self.Sigma_w = Sigma_w #Estimated variance of the hidden state \n        self.Sigma_z = Sigma_z # Estimated variance of the sensory observation \n        self.alpha_mu = a_mu # Learning rate of the gradient descent mu (hidden state)\n    \n    def g(self,x,v):\n        \"\"\"\n            equation of sensory mapping of the generative model: g(x) at point x \n            Given as input for this example equal to the true generative process g_gp(x)\n        \"\"\"\n        return g_gp(x,v)\n    \n    def dg(self, x):\n        \"\"\"\n            Partial derivative of the equation of sensory mapping of the generative model towards x: g'(x) at point x \n            Given as input for this example equal to the true derivative of generative process dg_gp(x)\n        \"\"\"\n        return dg_gp(x)\n    \n    def f(self,x,v):\n        \"\"\"\n            equation of motion of the generative model: f(x) at point x \n            Given as input for this example equal to the prior belief v\n        \"\"\"\n        return v\n    \n    # def df(self,x): Derivative of the equation of motion of the generative model: f'(x) at point x\n    # not needed in this example \n\n    \n    def inference_step (self, i, mu_v, y):\n        \"\"\"\n        Perceptual inference    \n\n        INPUTS:\n            i       - tic, timestamp\n            mu_v    - Hierarchical prior input signal (mean) at timestamp\n            y       - sensory input signal at timestamp\n\n        INTERNAL:\n            mu      - Belief or hidden state estimation\n\n        \"\"\"\n       \n        # Calculate the prediction errors\n        e_x = self.mu_x - self.f(self.mu_x, mu_v)  # prediction error hidden state\n        e_y = y - self.g(self.mu_x, mu_v) #prediction error sensory observation\n        # motion of prediction error hidden state\n        self.eps_x = self.eps_x + dt * self.alpha_mu * (e_x - self.Sigma_w * self.eps_x)\n        # motion of prediction error sensory observation\n        self.eps_y = self.eps_y + dt * self.alpha_mu * (e_y - self.Sigma_z * self.eps_y)\n        # motion of hidden state mu_x \n        self.mu_x = self.mu_x + dt * - self.alpha_mu * (self.eps_x - self.dg(self.mu_x) * self.eps_y)\n        \n        # Calculate Free Energy to report out\n        # Recalculate the prediction errors because hidden state has been updated, could leave it out for performance reasons\n        e_x = self.mu_x - self.f(self.mu_x, mu_v)  # prediction error hidden state\n        e_y = y - self.g(self.mu_x, mu_v) #prediction error sensory observation\n        # Calculate Free Energy\n        self.F = 0.5 * (e_x**2 \/ self.Sigma_w + e_y**2 \/ self.Sigma_z + np.log(self.Sigma_w * self.Sigma_z))\n        \n        return self.F, self.mu_x , self.g(self.mu_x,0)\n","7a409326":"class ai_capsule_v1():\n    \"\"\"\n        Class that constructs a group of neurons that perform Active Inference for one hidden state, one sensory input, one prior\n        In neurology it could eg represent a (micro) column\n        \n        Version 0.1, same as the first notebook for base reference \n    \"\"\"\n    def __init__(self,dt, mu_v, Sigma_w, Sigma_z, a_mu):   \n        self.dt = dt    # integration step\n        self.mu_x = mu_v   # initializing the best guess of hidden state by the hierarchical prior\n        self.F = 0      # Free Energy\n        self.eps_x = 0  # epsilon_x, prediction error on hidden state\n        self.eps_y = 0  # epsilon_y, prediction error on sensory measurement\n        self.Sigma_w = Sigma_w #Estimated variance of the hidden state \n        self.Sigma_z = Sigma_z # Estimated variance of the sensory observation \n        self.alpha_mu = a_mu # Learning rate of the gradient descent mu (hidden state)\n    \n    def g(self,x,v):\n        \"\"\"\n            equation of sensory mapping of the generative model: g(x,v) at point x \n            Given as input for this example equal to the true generative process g_gp(x)\n        \"\"\"\n        return g_gp(x,v)\n    \n    def dg(self, x):\n        \"\"\"\n            Partial derivative of the equation of sensory mapping of the generative model towards x: g'(x) at point x \n            Given as input for this example equal to the true derivative of generative process dg_gp(x)\n        \"\"\"\n        return dg_gp(x)\n    \n    def f(self,x,v):\n        \"\"\"\n            equation of motion of the generative model: f(x,v) at point x \n            Given as input for this example equal to the prior belief v\n        \"\"\"\n        return v\n    \n    # def df(self,x): Derivative of the equation of motion of the generative model: f'(x) at point x\n    # not needed in this example \n  \n    def inference_step (self, i, mu_v, y):\n        \"\"\"\n        Perceptual inference    \n\n        INPUTS:\n            i       - tic, timestamp\n            mu_v    - Hierarchical prior input signal (mean) at timestamp\n            y       - sensory input signal at timestamp\n\n        INTERNAL:\n            mu      - Belief or hidden state estimation\n\n        \"\"\"\n\n        # Calculate prediction errors\n        self.eps_x = self.mu_x - self.f(self.mu_x, mu_v)  # prediction error hidden state\n        self.eps_y = y - self.g(self.mu_x, mu_v) #prediction error sensory observation\n        # Free energy gradient\n        dFdmu_x = self.eps_x\/self.Sigma_w - self.dg(self.mu_x) * self.eps_y\/self.Sigma_z\n        # Perception dynamics\n        dmu_x   = 0 - self.alpha_mu*dFdmu_x  # Note that this is an example without generalised coordinates of motion hence dmu_x'=0\n        # motion of mu_x \n        self.mu_x = self.mu_x + self.dt * dmu_x\n        \n        # Calculate Free Energy to report out\n        self.F = 0.5 * (self.eps_x**2 \/ self.Sigma_w + self.eps_y**2 \/ self.Sigma_z + np.log(self.Sigma_w * self.Sigma_z))\n        \n        return self.F, self.mu_x , self.g(self.mu_x,0)","11bd2cce":"def makeNoise(C,s2,t):\n    \"\"\"\n    Generate coloured noise \n    Code by Sherin Grimbergen (V1 2019) and Charel van Hoof (V2 2020)\n    \n    INPUTS:\n        C       - variance of the required coloured noise expressed as desired covariance matrix\n        s2      - temporal smoothness of the required coloured noise, expressed as variance of the filter\n        t       - timeline \n        \n    OUTPUT:\n        ws      - coloured noise, noise sequence with temporal smoothness\n    \"\"\"\n    \n    if np.size(C)== 1:\n        n = 1\n    else:\n        n = C.shape[1]  # dimension of noise\n        \n    # Create the white noise with correct covariance\n    N = np.size(t)      # number of elements\n    L =cholesky(C, lower=True)  #Cholesky method\n    w = np.dot(L,np.random.randn(n,N))\n    \n    if s2 <= 1e-5: # return white noise\n        return w\n    else: \n        # Create the noise with temporal smoothness\n        P = toeplitz(np.exp(-t**2\/(2*s2)))\n        F = np.diag(1.\/np.sqrt(np.diag(np.dot(P.T,P))))\n        K = np.dot(P,F)\n        ws = np.dot(w,K)\n        return ws","06309f49":"def simulation (v, mu_v, Sigma_w, Sigma_z, noise, a_mu, version):\n    \"\"\"\n    Basic simplist example perceptual inference    \n   \n    INPUTS:\n        v        - Hydars actual depth, used in generative model, since it is a stationary example hidden state x = v + random fluctuation\n        mu_v     - Hydar prior belief\/hypotheses of the hidden state\n        Sigma_w  - Estimated variance of the hidden state \n        Sigma_z  - Estimated variance of the sensory observation  \n        noise    - white, smooth or none\n        a_mu     - Learning rate for mu\n        version  - 1=original solution 1; 3=alternative solution\n\n    \"\"\"\n\n\n    \n    # Init tracking\n    mu_x = np.zeros(N) # Belief or estimation of hidden state \n    F = np.zeros(N) # Free Energy of AI neuron\n    mu_y = np.zeros(N) # Belief or prediction of sensory signal \n    x = np.zeros(N) # True hidden state\n    y = np.zeros(N) # Sensory signal as input to AI neuron\n\n    # Create active inference neuron\n    if version==1:\n        capsule = ai_capsule_v1(dt, mu_v, Sigma_w, Sigma_z, a_mu) \n    elif version == 3:\n        capsule = ai_capsule(dt, mu_v, Sigma_w, Sigma_z, a_mu)\n\n    # Construct noise signals with emporal smoothness:\n    np.random.seed(1234)\n    sigma = 1\/2000 # smoothness of the noise parameter, variance of the filter\n    w = makeNoise(Sigma_w,sigma,t)\n    z = makeNoise(Sigma_z,sigma,t)\n\n    ssim = time.time() # start sim\n    \n    # Simulation\n    for i in np.arange(1,N):\n        # Generative process\n        if noise == 'white':\n            x[i] = v + np.random.randn(1)* Sigma_w\n            y[i] = g_gp(x[i],v) + np.random.randn(1)* Sigma_z\n        elif noise == 'smooth':\n            x[i]= v + w[0,i]\n            y[i] = g_gp(x[i],v) + z[0,i]\n        else: #no noise\n            x[i]= v \n            y[i] = g_gp(x[i],v)\n        #Active inference\n        F[i], mu_x[i], mu_y[i] =capsule.inference_step(i,mu_v,y[i])\n\n    # Print the results\n    tsim = time.time() - ssim\n    #print('Simulation time: ' + \"%.2f\" % tsim + ' sec' )\n\n    return F, mu_x, mu_y, x, y\n\n# Test case\n\nv = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1,3) # alternative solution\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'no noise',1,1) # original version (1.01)\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief enhanced active inference');\naxes[0].plot(t[1:],mu_x2[1:],label='Belief basic active inference');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief enhanced active inference');\naxes[1].plot(t[1:],mu_y2[1:],label='Belief basic active inference');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='Belief enhanced active inference');\naxes[2].semilogy(t[1:],F2[1:],label='Belief basic active inference');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","6742cdb1":"v = 25 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1,3) # prior and observation balanced\n\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief enhanced active inference');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief enhanced active inference');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].plot(t[1:],F1[1:],label='Belief enhanced active inference');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","5fd3dc62":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1,3) # prior and observation balanced\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,0.1,'no noise',1,3) # underctain about prior\nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,0.1,1,'no noise',1,3) # Trust generative model, belief high variance in sensor\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='balanced');\naxes[0].plot(t[1:],mu_x2[1:],label='Trust sensor');\naxes[0].plot(t[1:],mu_x3[1:],label='Trust Genmodel');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='balanced');\naxes[1].plot(t[1:],mu_y2[1:],label='Trust sensor');\naxes[1].plot(t[1:],mu_y3[1:],label='Trust Genmodel');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\n#axes[1].legend(loc='upper right');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='balanced');\naxes[2].semilogy(t[1:],F2[1:],label='Trust sensor');\naxes[2].semilogy(t[1:],F3[1:],label='Trust Genmodel');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel;\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","f88f333a":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,0.1,0.1,'white',1,3) # alternative version\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,0.1,0.1,'white',1,1) # original version (1.01)\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief enhanced active inference');\naxes[0].plot(t[1:],mu_x2[1:],label='Belief basic active inference');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief enhanced active inference');\naxes[1].plot(t[1:],mu_y2[1:],label='Belief basic active inference');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='Belief enhanced active inference');\naxes[2].semilogy(t[1:],F2[1:],label='Belief basic active inference');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","712b760a":"# showcase the coloured noise of the experiment, use same random seed to reproduce exact same noise\nnp.random.seed(42)\nz = makeNoise(1,1\/2000,t)\nplt.plot(t,z[0,:]);","9ee5ed0c":"\nv = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'smooth',1,3) # alternative version\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'smooth',1,1) # original version (1.01)\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief enhanced active inference');\naxes[0].plot(t[1:],mu_x2[1:],label='Belief basic active inference');\naxes[0].plot(t[1:],x1[1:],label='Generative process');\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[0].grid(1);\naxes[1].plot(t[1:],mu_y1[1:],label='Belief enhanced active inference');\naxes[1].plot(t[1:],mu_y2[1:],label='Belief basic active inference');\naxes[1].plot(t[1:],y1[1:],label='Generative process');\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='Belief enhanced active inference');\naxes[2].semilogy(t[1:],F2[1:],label='Belief basic active inference');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","08a86705":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,10,10,'no noise',1,3) # high variance = high uncertainty =low precision\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'no noise',1,3) \nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,0.1,0.1,'no noise',1,3) # low variance =  low uncertainty = high precision\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='$\\sigma^2$=10 low precision');\naxes[0].plot(t[1:],mu_x2[1:],label='$\\sigma^2$=1');\naxes[0].plot(t[1:],mu_x3[1:],label='$\\sigma^2$=0.1 high precision');\naxes[0].plot(t[1:],x1[1:],label='Generative process'); #note x1=x2=x3 because no noise\naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='$\\sigma^2$=10 low precision');\naxes[1].plot(t[1:],mu_y2[1:],label='$\\sigma^2$=1');\naxes[1].plot(t[1:],mu_y3[1:],label='\\sigma^2$=0.1 high precision');\naxes[1].plot(t[1:],y1[1:],label='Generative process'); #note y1=y2=y3 because no noise\naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='$\\sigma^2$=10 low precision');\naxes[2].semilogy(t[1:],F2[1:],label='$\\sigma^2$=1');\naxes[2].semilogy(t[1:],F3[1:],label='\\sigma^2$=0.1 high precision');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);\n","e9d79dfc":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,10,10,'smooth',1,3) # high variance = high uncertainty =low precision\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'smooth',1,3) \nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,0.1,0.1,'smooth',1,3) # low variance =  low uncertainty = high precision\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='$\\sigma^2$=10 low precision');\naxes[0].plot(t[1:],mu_x2[1:],label='$\\sigma^2$=1');\naxes[0].plot(t[1:],mu_x3[1:],label='$\\sigma^2$=0.1 high precision');\naxes[0].plot(t[1:],x1[1:],label='Generative process (low precision)'); \naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='$\\sigma^2$=10 low precision');\naxes[1].plot(t[1:],mu_y2[1:],label='$\\sigma^2$=1');\naxes[1].plot(t[1:],mu_y3[1:],label='\\sigma^2$=0.1 high precision');\naxes[1].plot(t[1:],y1[1:],label='Generative process (low precision)'); \naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='$\\sigma^2$=10 low precision');\naxes[2].semilogy(t[1:],F2[1:],label='$\\sigma^2$=1');\naxes[2].semilogy(t[1:],F3[1:],label='\\sigma^2$=0.1 high precision');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","923b064c":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'no noise',1,3) # \nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'no noise',2,3) \nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,1,1,'no noise',4,3) # \n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='LR=1');\naxes[0].plot(t[1:],mu_x2[1:],label='LR=2');\naxes[0].plot(t[1:],mu_x3[1:],label='LR=4');\naxes[0].plot(t[1:],x1[1:],label='Generative process'); \naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='LR=1');\naxes[1].plot(t[1:],mu_y2[1:],label='LR=2');\naxes[1].plot(t[1:],mu_y3[1:],label='LR=4');\naxes[1].plot(t[1:],y1[1:],label='Generative process'); \naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='LR=1');\naxes[2].semilogy(t[1:],F2[1:],label='LR=2');\naxes[2].semilogy(t[1:],F3[1:],label='LR=4');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","10dda263":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'smooth',1,3) # \nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'smooth',2,3) \nF3, mu_x3, mu_y3, x3, y3 = simulation(v,mu_v,1,1,'smooth',4,3) # \n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference');\naxes[0].plot(t[1:],mu_x1[1:],label='LR=1');\naxes[0].plot(t[1:],mu_x2[1:],label='LR=2');\naxes[0].plot(t[1:],mu_x3[1:],label='LR=4');\naxes[0].plot(t[1:],x1[1:],label='Generative process'); \naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='LR=1');\naxes[1].plot(t[1:],mu_y2[1:],label='LR=2');\naxes[1].plot(t[1:],mu_y3[1:],label='LR=4');\naxes[1].plot(t[1:],y1[1:],label='Generative process'); \naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='LR=1');\naxes[2].semilogy(t[1:],F2[1:],label='LR=2');\naxes[2].semilogy(t[1:],F3[1:],label='LR=4');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","c7908b27":"v = 30 # actual depth Hydar\nmu_v = 25 # Hydars belief of the depth\nF1, mu_x1, mu_y1, x1, y1 = simulation(v,mu_v,1,1,'smooth',10,1) # Belief original active inference\nF2, mu_x2, mu_y2, x2, y2 = simulation(v,mu_v,1,1,'smooth',10,3) # Belief alternative version\n\n\n# Plot results:\nfig, axes = plt.subplots(3, 1, sharex='col');\nfig.suptitle('Active Inference with motion of errors, inference part');\naxes[0].plot(t[1:],mu_x1[1:],label='Belief basic active inference');\naxes[0].plot(t[1:],mu_x2[1:],label='Belief enhanced active inference');\naxes[0].plot(t[1:],x1[1:],label='Generative process'); \naxes[0].hlines(mu_v, 0,T, label='Prior belief')\naxes[0].set_ylabel('$\\mu_x$ position');\naxes[0].grid(1);\nfig.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\naxes[1].plot(t[1:],mu_y1[1:],label='Belief baisc active inference');\naxes[1].plot(t[1:],mu_y2[1:],label='Belief enhanced active inference');\naxes[1].plot(t[1:],y1[1:],label='Generative process'); \naxes[1].hlines(g_gp(mu_v,0), 0,T, label='Prior belief')\naxes[1].set_ylabel('$\\mu_y$ temperature');\naxes[1].grid(1);\naxes[2].semilogy(t[1:],F1[1:],label='LR=1');\naxes[2].semilogy(t[1:],F2[1:],label='LR=2');\naxes[2].set_xlabel('time [s]');\naxes[2].set_ylabel('Free energy');\naxes[2].grid(1);","e62eae90":"<a id='sec2'><\/a>\n# Active inference code version 0.3\nThe heart of the predictive coding for perceptual inference you will find in the inference_step function, where 1 iteration of perceptual inference is executed every time it is called.  \n1. First the motion of the 2 prediction errors are calculated\/executed: \n    * $\\dot\\varepsilon_x = \\alpha_x (\\mu_x-f(\\mu_x,\\mu_v) - \\sigma_w^2\\varepsilon_x) = \\alpha_x (\\mu_x-\\mu_v - \\sigma_w^2\\varepsilon_x)$ \n    * $\\dot\\varepsilon_y = \\alpha_x (y-g(\\mu_x,\\mu_v) - \\sigma_z^2\\varepsilon_y) $  \n1. Next the motion of the hidden state is calculated\/executed: $\\dot{\\mu}_x = - \\alpha_x ( \\varepsilon_x - g'(\\mu_x) \\varepsilon_y) $.  \n\nIf you wonder how these equations relate to the equations of code example 1: once the predictions erros converge then $\\dot\\varepsilon$=0. By setting $\\dot\\varepsilon$=0 the prediction errors can be calculated (e.g. $\\varepsilon_x= \\frac{\\mu_x-\\mu_v}{\\sigma_w^2}$ and substituting those back into the motion of the hidden state you get exactly the same equation as code example 1: $\\dot{\\mu}_x = - \\alpha_x (\\frac{\\mu_x-\\mu_v}{\\sigma_w^2} - g'(\\mu_x) \\frac{y-g(\\mu_x,\\mu_v)}{\\sigma_z^2} )$. \n\nWhere the [forward Euler method](https:\/\/en.wikipedia.org\/wiki\/Euler_method) is used to execute the motions of $\\dot\\varepsilon_x, \\dot\\varepsilon_y, \\dot\\mu_x$.  \nTo summarize it in a picture:\n<img src=\"https:\/\/i.imgur.com\/16MSLoi.jpg\" width=800>\n\n","0fa6c91c":"## Notes\n* Higher learning rates deliver faster convergence (and faster oscilations) to the result. So, in this solution a higher precision (lower variance) does not equal faster convergence but a higher learning rate does.\n* A large learning rate of e.g. 100 does not make the convergence unstable but e.g. 1000 creates an overflow error in python calculating an exponential","1f2a0988":"## Experiment 3.05 - Coloured noise\nSame set-up as experiment 1.05 but with prediction errors cast as a motion. It is experiment 3.01 but now with actual coloured noise in the generative process (couloured noise as explained in the [second notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-2)). ","8fb03215":"## Experiment 3.01 - The first\nSame experiment as 1.01 but now with the solution where prediction errors are cast as motion.  \nHydar's hypotheses or prior $\\mu_v$ is that it's depth is 25 meters, so it is expecting a temperature of 17 degrees. Its sensors give somehow a reading of approx 13 degrees, so around 30 meters depth. What to believe? Should it trust it's sensor or its internal hypothesis\/prior expectation? \n","3f9adfa6":"## Notes\n+ Both solutions 3.01 and 1.01 come to the same end-result, as expected. Hydars belief about the depth start with 25 meters as it's prior and the more the sensory observations keep giving a persistent signal that the temperature is lower is starts shifting it's belief about the depth $p(x \\mid y)$ to around 27 meters.\n* The motion of $\\dot\\varepsilon_x$ and $\\dot\\varepsilon_y$ makes the resulting belief motion (in the enhanced solution) like a kind of damped oscillation, also resulting in an oscilation in the Free energy.","e9d076fe":"# Experiments\nLet's see the theory in action and showcase perceptual inference in our simulation environment using this extended version.\nIn this example, Hydar has to predict its body temperature and therefor needs to infer the hidden state (its depth). It will do so by minimizing the surprise associated with the occurrence of atypical events in it's sensory observations by minimizing the Free Energy.  ","a0f0ec45":"## Generative model\nThe generative model (model in the Hydra brain to encode a probabilistic model of the environment\/world in which it is immersed), it is the same generative model (by design) as the first example.\n* $x = f(x,v) + w $ \n* $y = g(x,v) + z $    \n\nwhere\n* The function of motion $f(x,v)$  is given as input and Hydar believes it's depth is simply equal to it's prior belief $f(\\mu_x,\\,u_v) = \\mu_v $. Note that the scope of this example is a static case (e.g. still water) so $x=f(x,v) + w$ instead of $\\dot x=f(x,v) + w$\n* The function of sensory mapping $g(x,v)$ is given as input and is equal to the true generative process. Same for and it's derivative $g'(x)$.\n\n## Free energy\nThe Free Energy for one active inference capsule is (the simplest univariate case we used in the [second notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-2\/#sec5))\n\n$$\\mathcal{F}(y,\\mu) = \\frac{1}{2\\sigma_w^2}\\varepsilon_x^2 + \\frac{1}{2\\sigma_z^2}\\varepsilon_y^2  + \\frac{1}{2} ln\\: ({\\sigma_z^2\\sigma_w^2) } $$\nWhere\n* $\\varepsilon_x = \\mu_x-f(\\mu_x,\\mu_v) = \\mu_x-\\mu_v $ is the the motion prediction error.\n* $\\varepsilon_y = y-g(\\mu_x,\\mu_v) = y-g_{gp}(\\mu_x,\\mu_v) $ is the sensory prediction error. \n\n\n## Inference by gradient descent\n\nPerceptual inference by iterativly taking steps proportional to the negative of the partial derivative of the Free Energy with respect to $\\mu_x$. Following the steps of the [fourth notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-4), the gradient descent can be written as a differential equation:\n\n$$\\dot{\\mu}_x = \\mathcal{D}{\\mu}_x - \\alpha_x \\frac{\\partial \\mathcal{F}( y,\\mu)}{\\partial \\mu_x} $$ \n\nwher $\\mathcal{D}{\\mu}_x$ is 0 because this example is without generalised coordinates of motion.  \nThe derevative of the Free Energy can be calculated straightforward\n$$ \\frac{\\partial \\mathcal{F}(y,\\mu)}{\\partial {\\mu}_x}=  \\frac{\\partial \\varepsilon_x}{\\partial \\mu_x} \\frac{1}{\\sigma_w^2}\\varepsilon_x + \\frac{\\partial \\varepsilon_y}{\\partial \\mu_x} \\frac{1}{\\sigma_z^2}\\varepsilon_y = \\frac{1}{\\sigma_w^2}\\varepsilon_x - g'(\\mu_x) \\frac{1}{\\sigma_z^2}\\varepsilon_y $$ \n","41d81dd1":"# Scope code example 3\nThe scope of the third example is exactly the same as the [first example](https:\/\/www.kaggle.com\/charel\/active-inference-code-by-example-1) but this time following the approach of the [tutorial on the free-energy framework for modelling perception and learning ](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0022249615000759)from Rafal Bogacz where the prediction errors are also cast as motions with the argument it is more biologocal plausible, meaning \"an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons\".\n\nScope of the example: \n* One active inference capsule (single hidden state x, single sensory channel y and single hierarchical prior v) \n* No generalised coordinates of motion  \n* Static environment (no differential equation for equation of motion but f(x) expresses an a priori static reference point) \n* The generative model $\\mu_\\theta$ and expected precision of uncertainty $\\mu_\\lambda$ are invariant during the process and for this example given as input. Remember ${\\mu}_x$ is estimated by fast neuronal states while $\\mu_\\theta$ and $\\mu_\\lambda$ adapt on slower timescales (e.g. synaptic efficacy) so invariant during perceptual inference. We simply assume the generative model \/ noise estimation has already been learned and is known (so, function of motion and sensory mapping are given as input).   \n* The brain estimate for the hierarchical prior ($\\mu_v$) is given as input\n* Perceptual inference by minimizing the Free Energy for one $\\mu_x$:   \n\n$$\\mu_x=\\underset{\\mu_x }{Argmin}\\:  \\mathcal{F}( y,\\mu)$$ \n* by iterativly taking steps proportional to the negative of the partial derivative of the Free Energy with respect to $\\mu_x$.\n* This third code example is the same use-case as the first code example but following the approach of [tutorial on the free-energy framework for modelling perception and learning](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0022249615000759) from Rafal Bogacz where the prediction errors are also cast as motions with the argument it is more biologocal plausible, meaning \"an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons\".\n\n \n ","2c917a09":"# How the brain might function - code example 3\n### Free Energy Principle tutorial without a PhD\n  \n<img src=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/08\/08\/44\/artificial-intelligence-3382507_960_720.jpg\" width=500>\n<center>Image by <a href=\"https:\/\/pixabay.com\/users\/geralt-9301\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3382507\">Gerd Altmann<\/a> from <a href=\"https:\/\/pixabay.com\/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3382507\">Pixabay<\/a> <\/center>   \n<br>\n\nWelcome to my notebook on Kaggle. I did record my notes with examples so it might help others in their journey to understand **Active Inference** minimizing the underlying **Free Energy**. Neuroscience has produced a candidate which suggests that several global brain theories might be unified within a free-energy framework: [Free Energy Principle](https:\/\/en.wikipedia.org\/wiki\/Free_energy_principle) (FEP) by [Karl Fristion](https:\/\/en.wikipedia.org\/wiki\/Karl_J._Friston): The free-energy principle is an attempt to explain the structure and function of the brain, starting from the very fact that we exist.\n\nThis is a code example notebook and it belongs to a series of notebooks on the Free Energy Principle tutorial without a PhD. If you are interested to have a deep understanding of this code please read the \"Free Energy Principle tutorial without a PhD\" series ([part 1](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1), [part 2](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-2), [part 3](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-3), [part 4](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-4)). There is a lot to explain but I promise you an excellent journey into something that might hold the key to true AI.","2c32ce80":"## Notes\n* It is becomming hard to spot the difference both in speed of convergence as well as predictions. Seems a solution with prediction errors cast as a motion is benefitting more from a higher learning rate.","a4ef508b":"## Experiment 3.04 - White noise\nSame set-up as experiment 1.04 but with prediction errors cast as a motion. It is experiment 3.01 but now with some actual white noise in the generative process.","db198355":"## Experiment 3.03 - Trust your sensors or your prior belief?\nSame set-up as experiment 1.03 but with prediction errors cast as a motion. It is experiment 3.01 but now showcase the delta between\n1.  Balanced variances : $\\sigma_z^2$ equals $\\sigma_w^2$ \n1.  Confidence in own sensory observations (bottom-up): low variance $\\sigma_z^2$ compared $\\sigma_w^2$\n1.  Confidence in the internal model including prior (top-down): low variance $\\sigma_w^2$ compared $\\sigma_z^2$","e05e5ab2":"## Experiment 3.07 - Noise to signal ratio coloured noise\nSame set-up as experiment 1.07  but with prediction errors cast as a motion. It is experiment 3.06 but now with the actual coloured noise in the generative process.\n","a882d7ce":"## Experiment 3.02 - Observe what is expected\nQuick check that what happens if the prior expectation and actual sensor readings do match. Hydar's prior and its actual depth is 25 meters, so it is expecting and receiving a temperature of 17 degrees.","d5a47015":"## Notes\n* as expected, same results as experiment 1.03 but this time with damped oscilation as seen in experiment 3.01\n* At first sight it looks like higher thrust in the generative model creates more and longers oscilations for this specific case.\n* if Hydar trust the sensor more, it's belief about the depth $p(x \\mid y)$ is closer to the depth associated with the sensor input, so it also predicts a temperature closer to the sensor input.\n* if Hydar trust the prior expectation more, it's belief about the depth $p(x \\mid y)$ is far closer to the depth associated with the prior, so it also predicts a temperature much closer to the prior input.\n\n","ac2382f1":"## Notes\n* The higher the learning rate the faster the convergence but also a more erratic the line, I guess more reacting to the input of the generative process\n* A large learning rate of e.g. 100 does not make the convergence unstable but e.g. 1000 creates an overflow error in python calculating the exponential","56774788":"## Long live Hydar!\n\nThe example is set in the simulation environment where Hydar needs to infer its depth based on temperature sensor readings, even when the sensor readings are not what is expected.  \n\nAs introduced in the 4th notebook, [Hydar](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-4\/#sec4) is an early evolutionary imaginary aquatic ancestor that must preserve its physical integrity to survive. For example, it needs to keep in a certain depth range.\n\nRemember from the [first notebook](https:\/\/www.kaggle.com\/charel\/learn-by-example-active-inference-in-the-brain-1) that in essence the brain is a prediction mechanism, one that attempts to minimize the error of its hypothesis about the world and the sensory input it receives:\n* The skull-bound brain needs to infer the world.\n* The brain builds an internal model of the world using Bayesian inference (the generative model).\n* Discrepancies between the internal model (the prediction) and the sensory observations result in prediction error.\n* The brain aims to minimize the Free Energy which is equivalent to minimizing prediction errors.\n* By either Improving perception, Acting on the environment, Learning the generative model, Optimizing expected precision of uncertainty. \n\nThe scope of this notebook is improving perception. In this example Hydar's brain is equipped with one active inference capsule to encode the generative model. With one single sensor (y), in this case to measure the temperature in degrees Celsius, one hidden state it tries to infer (x), in this case the depth, and one prior (v) representing its top-down hypotheses of the hidden state\/depth. \n\nIn this example the water is still \/ not moving.\n\n<img src=\"https:\/\/i.imgur.com\/wTiEZK9.jpg\" width=500>\n","f63c21fc":"## Experiment 3.09 - Tuning learning rates coloured noise\nSame set-up as experiment 3.08 with different learning rates (with balanced noise variances) with coloured noise in the generative process.","269f43e7":"## Generative process\n   \nThe generative process (the process in the external environment\/world generating the sensory states) is simulated by:\n* $x = f_{gp}(x,v) + w $ \n* $y = g_{gp}(x,v) + z $    \n\nit is by design the same as the first code example to compare results, where:\n* x is the position (depth in centimetres)\n* y is the temperature (degrees Celsius)\n* The function of motion  $f_{gp}(x,v)$ of this example is a static scope (e.g. still water) so $x= v + w$ (where $w$ is the random fluctuation) thus Hydar basically stays on it's initial position (plus\/minus some noise)\n* The function of sensory mapping  $g_{gp}(x,v)$  of this example is defined as $g_{gp}(x,v) = t_0 -\\frac{16}{1+e^{5-\\frac{x}{5}}}$ with $t_0=25$ ,  see picture and code below.\n* The derivative of the function of sensory mapping with respect to $x$ is $g'_{gp}(x) = -\\frac{16e^{5-\\frac{x}{5}}}{5\\left(1+e^{5-\\frac{x}{5}}\\right)^2}$ (calculated upfront with some good old high-school math)\n","c729eaf7":"## Summary code example 3\nTo recap all you need to perform active inference in the table below:\n\nWhat| Symbol | Description |\n--- | --- | --- |\nposition | x  | single external hidden environment state the Hydar brain tries to infer, in this case the position. <br> Hydar lives in a one-dimensional plane, so it is the depth in centimetres | \nposition | $\\mu_x$  | Hydars belief or estimation of the depth, the hidden state x it's tries to infer |\nbody temperature sensor | y  | single sensory observation, in this case the temperature.  | \nprior | v  | The initial depth of Hydar in the generative process | \nprior | $\\mu_v$  | Hydars prior belief of the depth |\nfunction of sensory mapping| $g(x,v)$  | Given as input and is equal to the true generative process $g(x,v) = g_{gp}(x,v) = t_0 -\\frac{16}{1+e^{5-\\frac{x}{5}}}$  | \nDerivative function of sensory mapping| $g'(x)$  | Given as input and is equal to the derivative of the true generative process towards $x$ $g'(x) = g_{gp}'(x) = -\\frac{16e^{5-\\frac{x}{5}}}{5\\left(1+e^{5-\\frac{x}{5}}\\right)^2}$  | \nfunction of motion | $f(x,v)$  | Given as input and Hydar believes it's depth is simply equal to it's prior belief $f(x) = \\mu_v $ with no motion |\nFree Energy | $\\mathcal{F}(y,\\mu)$  | $\\mathcal{F}(y,\\mu) = \\frac{1}{2\\sigma_w^2}\\varepsilon_x^2 + \\frac{1}{2\\sigma_z^2}\\varepsilon_y^2  + \\frac{1}{2} ln\\: ({\\sigma_z^2\\sigma_w^2) } $ |\nsensory prediction error | $\\varepsilon_y$  | $\\varepsilon_y = y-g(\\mu_x,\\mu_v) = y-g_{gp}(\\mu_x,\\mu_v) $ |\nmotion prediction error | $\\varepsilon_x$  | $\\varepsilon_x = \\mu_x-f(\\mu_x,\\mu_v) = \\mu_x-\\mu_v $ |\nDerevative Free Energy | $\\frac{\\partial \\mathcal{F}(y,\\mu)}{\\partial {\\mu}_x}$  | $ \\frac{\\partial \\mathcal{F}(y,\\mu)}{\\partial {\\mu}_x}=  \\frac{\\partial \\varepsilon_x}{\\partial \\mu_x} \\frac{1}{\\sigma_w^2}\\varepsilon_x + \\frac{\\partial \\varepsilon_y}{\\partial \\mu_x} \\frac{1}{\\sigma_z^2}\\varepsilon_y = \\frac{1}{\\sigma_w^2}\\varepsilon_x - g'(\\mu_x) \\frac{1}{\\sigma_z^2}\\varepsilon_y $ |\n\n","1fdd2ded":"## Notes\n* The red line shows the temperature and depth resulting from the low precision sensory input (a variance of 10 with a mean of 17). The corresponding enhanced active inference estimation is the blue (low precision) line. Interesting to see that despite the red line goes all over the place the blue line is relative straight. Good for Hydar else it would be very dizzy. ","27c6305f":"## Experiment 3.06 - Noise to signal ratio\nSame set-up as experiment 1.06  but with prediction errors cast as a motion. It is experiment 3.01 but with different estimations of the noise (different variances 0.1 \/ 1 \/ 10) with balanced variances:  $\\sigma^2_z$  equals  $\\sigma^2_w$\nNote: for this example there is no noise in the generative process (although the generative model expects noise to be present) to best see how the belief develops.","89a21b09":"## Notes\n* I tuned the noise down to a variance of 0.1 to still see the curve of the Free energy coming down. If you run the same code with higher variances (eg 1 higher) it still works nicely, see also one of the later experiments.\n* This solution does converge to the same results as the previous solution but takes (far) more time, eyeballing in the graph I would argue 12-14 seconds","b5da7329":"## Thank you\nPlease do copy this kernel and try out for yourself to better understand active inference and the free energy principle. My intent is to help catalyse knowledge and research on Active Inference in an engineering\/robotics\/data sciences\/machine learning context. Hope you liked my notebook (upvote top right), my way to contribute back to this fantastic Kaggle platform and community.","2bb58103":"## Experiment 3.08 - Tuning learning rates\nSame set-up as experiment 1.08 but with prediction errors cast as a motion. It is experiment 3.01 but with different learning rates (with balanced noise variances).  \nNote: for this example there is no noise in the generative process (although the generative model expects noise to be present) to best see how the belief develops.","37367a59":"## Notes\n* This solution does converge to the same results and faster compared to white noise (eyeballing in the graph I would argue 4 seconds). Interesting to observe that in this experiment under coloured noise it improves.  I observe in the Free Energy that the lines even coincide for quite some time.","f06b51fb":"## Notes\n* As expected, the sensor readings and prior confirm immediately, only 1 observation needed for Hydar to know it's depth. A good sign to showcase the strength of a model with top-down predictions and bottom-up sensory observations to perceive the world fast. \n* The Free Energy is zero, optimal ","6e9cf299":"## Experiment 3.10 - Compare solutions\nCompare solution of the first notebook vs this proposed enhanced solution under coloured noise with learning rate of 10.","4743cb4b":"## Notes\nIn is interesting to compare the results of this experiment with the results of [experiment 1.06](https:\/\/www.kaggle.com\/charel\/active-inference-code-by-example-1\/).\nWhere in the original solution  \n> the higher the precision, the \u00a8bigger steps\u00a8 to converge because Hydar can trust the signals\/beliefs, the faster the conversion  \n\nthe results in this solution shows the higher the precision, the higher amplitude in the oscilations. Higher precision is not equal to faster conversion, high precision or low precision take as long. The amplitude of the oscilations differ.\n"}}