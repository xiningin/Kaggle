{"cell_type":{"651eb32d":"code","05187fb0":"code","5285082d":"code","d0463f15":"code","f7e524df":"code","49d01a26":"code","4921ffbd":"code","8f6f3615":"code","bae4dcf3":"code","133a99f1":"code","73d9f341":"code","1604b406":"code","0f81c13b":"code","a1cdc773":"code","35c69b17":"code","1fe098a3":"code","6a51b309":"code","c83fa565":"code","e20c8d0a":"code","0a52790b":"code","ffec79fb":"code","3315b80a":"code","79dd36b2":"code","7bcb3a94":"code","f2a07ee8":"code","6b4d79d9":"code","bfec1d93":"code","f9088989":"code","5cd483d9":"code","d0cb7d20":"code","3911277e":"code","12a29960":"markdown","78e417cc":"markdown","4a01a973":"markdown","3836db83":"markdown","f965b5ee":"markdown","7f3025e6":"markdown","1f08c77d":"markdown","79ff9951":"markdown","03d394e9":"markdown","f6c29d02":"markdown","97bea0f6":"markdown","239cb5e5":"markdown","9f5557ef":"markdown","12102f1d":"markdown","8c2caf52":"markdown","16a51a47":"markdown","a9711c57":"markdown","f98e888e":"markdown"},"source":{"651eb32d":"# import standard libraries for linear algebra, handling data and plotting\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('precision', 2)\nsns.set(style=\"white\", color_codes=True)\n\n%matplotlib inline","05187fb0":"# read the data into a Pandas DataFrame\ndata = pd.read_csv('..\/input\/data.csv')\n\n# show some samples\ndata.head()","5285082d":"# remove unnamed 32th column\ndata.drop(data.columns[32], axis=1, inplace=True)\n\n# save identifications\nids = data['id']\n\n# remove unnecessary id column\ndata.drop(['id'], axis=1, inplace=True)","d0463f15":"# how many rows and columns are there in the data?\ndata.shape","f7e524df":"# which are the names of the columns and their datatypes?\ndata.info()","49d01a26":"# is there any column with null values?\ndata.isnull().sum()","4921ffbd":"# describing numerical features\ndata.describe()","8f6f3615":"# describing categorical features\ndata.describe(include=['O'])","bae4dcf3":"# which are the possible values for the categorical attribute?\ndata['diagnosis'].value_counts()","133a99f1":"# normalizing numeric values in order to avoid distortions\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata.iloc[:,1:] = scaler.fit_transform(data.iloc[:,1:])\ndata.head()","73d9f341":"# analyzing correlation between features\ndata.corr()","1604b406":"# plotting this correlation between features\nfig, ax = plt.subplots(figsize=(17, 17))\nsns.heatmap(data=data.corr().round(2), annot=True, cmap=\"PiYG\", ax=ax)","0f81c13b":"# pair plot diagnosis + 10 features (mean)\nsns.pairplot(data=data.iloc[:,0:11], hue='diagnosis', diag_kind='kde')","a1cdc773":"# pair plot diagnosis + 10 features (std error)\nsns.pairplot(data=data.iloc[:,np.append([0],np.arange(11,21))], hue='diagnosis', diag_kind='kde')","35c69b17":"# pair plot diagnosis + 10 features (worst\/largest)\nsns.pairplot(data=data.iloc[:,np.append([0],np.arange(21,31))], hue='diagnosis', diag_kind='kde')","1fe098a3":"# scatter plotting radius (mean) versus concave points (mean)\nsns.FacetGrid(data, hue='diagnosis', height=5) \\\n   .map(plt.scatter, 'radius_mean', 'concave points_mean') \\\n   .add_legend()","6a51b309":"# box plotting diagnosis against radius (mean)\nsns.boxplot(x='diagnosis', y='radius_mean', data=data)","c83fa565":"# create an histogram on radius (mean)\nsns.FacetGrid(data, hue='diagnosis')\\\n   .map(plt.hist, 'radius_mean', alpha=.5, bins=20)\\\n   .add_legend()","e20c8d0a":"# create an histogram on concave points (mean)\nsns.FacetGrid(data, hue='diagnosis')\\\n   .map(plt.hist, 'concave points_mean', alpha=.5, bins=20)\\\n   .add_legend()","0a52790b":"# select the features\n#X = data[data.columns[[1, 3, 4, 7, 8]]] # use only selected 5 features\nX = data.iloc[:,1:] # use all numeric features\nX.head()","ffec79fb":"# select the class column\ny = data.diagnosis\ny.tail()","3315b80a":"# importing packages used in model selection and metrics evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# importing all the necessary packages to use the various classification algorithms\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier","79dd36b2":"# separate data for training (70%) and testing (30%)\n\nprint('original data shapes:', X.shape, y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint('splitted data shapes:', X_train.shape, X_test.shape, y_train.shape, y_test.shape)","7bcb3a94":"# instantiate checking algorithms\nmodels = []\nmodels.append(('Support Vector Machines (SVM)', SVC()))\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('Decision Tree', DecisionTreeClassifier()))\nmodels.append(('K-Nearest Neighbours (3)', KNeighborsClassifier(n_neighbors=3)))\nmodels.append(('K-Nearest Neighbours (7)', KNeighborsClassifier(n_neighbors=7)))\nmodels.append(('K-Nearest Neighbours (11)', KNeighborsClassifier(n_neighbors=11)))\nmodels.append(('Random Forest', RandomForestClassifier()))\nmodels.append(('Random Forest (10)', RandomForestClassifier(n_estimators=10)))\nmodels.append(('Random Forest (100)', RandomForestClassifier(n_estimators=100)))\nmodels.append(('Gaussian Na\u00efve Bayes', GaussianNB()))\nmodels.append(('Perceptron (5)', Perceptron(max_iter=5)))\nmodels.append(('Perceptron (10)', Perceptron(max_iter=10)))\nmodels.append(('Perceptron (50)', Perceptron(max_iter=50)))\nmodels.append(('Stochastic Gradient Decent (SGD)', SGDClassifier(max_iter=50)))\nmodels.append(('Linear SVC', LinearSVC()))","f2a07ee8":"names = []\nscores = []\nfalnegs = []\n\nbest_model = None\nhighest_score = 0.0\nfalse_negatives = None\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    \n    y_pred = model.predict(X_test)\n\n    cm = confusion_matrix(y_pred, y_test)\n    tn, fp, fn, tp = cm.ravel()\n    print(name, '\\n', cm, '\\n')\n\n    names.append(name)\n    scores.append(score)\n    falnegs.append(fn)\n\n    if ((score > highest_score) or (score == highest_score and fn < false_negatives)):\n        best_model = model\n        highest_score = score\n        false_negatives = fn\n        \nprint('Best model:', best_model, '\\n[Score: %.3f, False Negatives: %d]' % (highest_score, false_negatives))","6b4d79d9":"results = pd.DataFrame({'Model': names, 'Score': scores, 'FN': falnegs})\nresults.sort_values(by=['Score', 'FN'], ascending=[False, True])","bfec1d93":"# consider the best algorithm found\nmodel = best_model\n\n# train the model with the training dataset\nmodel.fit(X_train, y_train)\n\n# calculate the score against the whole dataset\nscore = model.score(X, y)\nprint('Final score:', score)\n\n# produce the confusion matrix\ny_pred = model.predict(X)\nprint('Confusion matrix:\\n', confusion_matrix(y_pred, y), '\\n')","f9088989":"submission = pd.DataFrame({\n  \"ID\": ids,\n  \"Diagnosis\": y,\n  \"Predicted\": y_pred,\n  \"Correct\": (y == y_pred).map({True: 1, False: 0})\n})\nsubmission.head(10)","5cd483d9":"# show the incorrectly classified cases\n\nincorrectly = submission[submission[\"Correct\"] == False]\n\nincorrect = len(incorrectly.index)\ntotal_cases = len(submission)\nprint('Incorrectly classified cases:', incorrect, \\\n      'of', total_cases, '(%.3f%%)' % (incorrect \/ total_cases))\n\nincorrectly","d0cb7d20":"unforgiven_incorrectly = submission.query(\"Diagnosis == 'M' & Predicted == 'B'\")\nunforgiven_incorrectly.head()","3911277e":"submission.to_csv(\"predicted.csv\", index=False)","12a29960":"With the best model found, let's predict the values for the entire dataset and then print the score and the confusion matrix.","78e417cc":"We still cannot define the distinction. Let's try box plotting another feature.","4a01a973":"There's a region in the middle where we can't define perfectly the class. Let's try it with another feature.","3836db83":"For each algorithm, let's perform the training, try predicting values, and then measure the model accuracy. A confusion matrix is to be calculated, as well as the number of False Negatives found.","f965b5ee":"## Breast Cancer Diagnostic - Automatically choosing the best algorithm\n\nThe goal here is to predict the diagnosis of a breast cancer, whether it is malignant or benign, depending on the values of several observations performed in cells.","7f3025e6":"False Negatives are unforgiven incorrect classifications for the given study. Which are they?","1f08c77d":"In the end, this section will give us the best model found. We consider the higher accuracy and the fewer number of false negatives to chosse this model.","79ff9951":"There are so many features. Therefore, pair plotting all of them isn't a good idea. Let's do it by each 10 sets.","03d394e9":"## Understanding the dataset\n\n### Attribute information\n\n#### 1) ID number\n\n#### 2) Diagnosis\n\n- M = malignant\n- B = benign\n\n#### 3-32) Features (Mean, Std Err, Worst\/Largest)\n\nTen real-valued features are computed for each cell nucleus:\n\n- a) **radius** (mean of distances from center to points on the perimeter)\n- b) **texture** (standard deviation of gray-scale values)\n- c) **perimeter**\n- d) **area**\n- e) **smoothness** (local variation in radius lengths)\n- f) **compactness** (perimeter^2 \/ area - 1.0)\n- g) **concavity** (severity of concave portions of the contour)\n- h) **concave points** (number of concave portions of the contour)\n- i) **symmetry**\n- j) **fractal dimension** (\"coastline approximation\" - 1)\n\nThe **mean**, **standard error** and **\"worst\" or largest** (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n### Observations\n\n- All feature values are recoded with four significant digits.\n- Missing attribute values: none.\n- Class distribution: 357 benign, 212 malignant.","f6c29d02":"Finally, let's create a DataFrame containing a possible submission to the competition.","97bea0f6":"The features alone definitely can't split the class (i.e., give a clue to the diagnosis). Therefore, we'll include every numeric feature in the model. ","239cb5e5":"First of all, let's split the data we handled so far in two sets: training and testing. The latter must contain fewer rows.","9f5557ef":"From these 3 last plottings, we can't realize a clear distinction in the class (diagnosis) against a given pair of numerical attributes. Let's try scatter plotting a given pair of attributes.","12102f1d":"Next, we'll instantiate each algorithm to be checked. We'll insert them in a single list.","8c2caf52":"If there is any incorrect classification, which were them?","16a51a47":"Still confusing. Let's try an histogram based on these same attributes.","a9711c57":"Let's start creating a model for the problem based on the data!","f98e888e":"The last thing: submitting the final file."}}