{"cell_type":{"fdb12bb6":"code","22d55ed1":"code","673be3bf":"code","674ed6c4":"code","7e589108":"code","ccb9830f":"code","7cc00674":"code","fec6a3a0":"code","ecef96f3":"code","d6c046e0":"code","ddc9d887":"code","f7bcbbec":"code","f613e54a":"code","c3359158":"code","6c2db9a1":"code","cd8d4960":"code","7d2ebb36":"code","b9cd38c1":"code","17673568":"code","dd9be89e":"code","ec00d8e6":"code","7deb934b":"code","2f868a49":"code","bf4052c3":"code","060ee617":"code","90e8b6e4":"code","7e4e18f3":"code","deb4bf55":"code","f2c6dbdc":"code","cc0f6603":"code","18fe3de0":"code","a55774f9":"code","2e96e0cd":"code","c555d2c3":"code","daa44c8a":"code","b9bb0074":"code","ced1ec8e":"markdown","b958f42d":"markdown","1bc6e463":"markdown","8a9f8466":"markdown","df7f274c":"markdown","6b2d1f9a":"markdown","0b830494":"markdown"},"source":{"fdb12bb6":"import warnings\nwarnings.filterwarnings('ignore')","22d55ed1":"import pandas as pd\ndf=pd.read_csv('..\/input\/diabetes.csv')\ndf.head()","673be3bf":"import numpy as np\ndf['Glucose']=np.where(df['Glucose']==0,df['Glucose'].median(),df['Glucose'])\ndf.head()","674ed6c4":"#### Independent And Dependent features\nX=df.drop('Outcome',axis=1)\ny=df['Outcome']","7e589108":"pd.DataFrame(X,columns=df.columns[:-1])","ccb9830f":"#### Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)","7cc00674":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier=RandomForestClassifier(n_estimators=10).fit(X_train,y_train)\nprediction=rf_classifier.predict(X_test)","fec6a3a0":"y.value_counts()","ecef96f3":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nprint(confusion_matrix(y_test,prediction))\nprint(accuracy_score(y_test,prediction))\nprint(classification_report(y_test,prediction))","d6c046e0":"### Manual Hyperparameter Tuning\nmodel=RandomForestClassifier(n_estimators=300,criterion='entropy',\n                             max_features='sqrt',min_samples_leaf=10,random_state=100).fit(X_train,y_train)\npredictions=model.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(accuracy_score(y_test,predictions))\nprint(classification_report(y_test,predictions))","ddc9d887":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(random_grid)","f7bcbbec":"rf=RandomForestClassifier()\nrf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n                               random_state=100,n_jobs=-1)\n### fit the randomized model\nrf_randomcv.fit(X_train,y_train)","f613e54a":"rf_randomcv.best_params_","c3359158":"rf_randomcv","6c2db9a1":"best_random_grid=rf_randomcv.best_estimator_","cd8d4960":"from sklearn.metrics import accuracy_score\ny_pred=best_random_grid.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report: {}\".format(classification_report(y_test,y_pred)))","7d2ebb36":"rf_randomcv.best_params_","b9cd38c1":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'criterion': [rf_randomcv.best_params_['criterion']],\n    'max_depth': [rf_randomcv.best_params_['max_depth']],\n    'max_features': [rf_randomcv.best_params_['max_features']],\n    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'], \n                         rf_randomcv.best_params_['min_samples_leaf']+2, \n                         rf_randomcv.best_params_['min_samples_leaf'] + 4],\n    'min_samples_split': [rf_randomcv.best_params_['min_samples_split'] - 2,\n                          rf_randomcv.best_params_['min_samples_split'] - 1,\n                          rf_randomcv.best_params_['min_samples_split'], \n                          rf_randomcv.best_params_['min_samples_split'] +1,\n                          rf_randomcv.best_params_['min_samples_split'] + 2],\n    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100, \n                     rf_randomcv.best_params_['n_estimators'], \n                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]\n}\n\nprint(param_grid)","17673568":"#### Fit the grid_search to the data\nrf=RandomForestClassifier()\ngrid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n","dd9be89e":"grid_search.best_estimator_","ec00d8e6":"best_grid=grid_search.best_estimator_","7deb934b":"best_grid","2f868a49":"y_pred=best_grid.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report: {}\".format(classification_report(y_test,y_pred)))","bf4052c3":"!pip install hyperopt","060ee617":"from hyperopt import hp,fmin,tpe,STATUS_OK,Trials","90e8b6e4":"space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\n    }","7e4e18f3":"space","deb4bf55":"\ndef objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_train, y_train, cv = 5).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }","f2c6dbdc":"from sklearn.model_selection import cross_val_score\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 80,\n            trials= trials)\nbest","cc0f6603":"crit = {0: 'entropy', 1: 'gini'}\nfeat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}\n\n\nprint(crit[best['criterion']])\nprint(feat[best['max_features']])\nprint(est[best['n_estimators']])","18fe3de0":"best['min_samples_leaf']","a55774f9":"trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'], \n                                       max_features = feat[best['max_features']], \n                                       min_samples_leaf = best['min_samples_leaf'], \n                                       min_samples_split = best['min_samples_split'], \n                                       n_estimators = est[best['n_estimators']]).fit(X_train,y_train)\npredictionforest = trainedforest.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(accuracy_score(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))\nacc5 = accuracy_score(y_test,predictionforest)","2e96e0cd":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nparam = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(param)","c555d2c3":"param","daa44c8a":"\nfrom tpot import TPOTClassifier\n\n\ntpot_classifier = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12,\n                                 verbosity= 2, early_stop= 12,\n                                 config_dict={'sklearn.ensemble.RandomForestClassifier': param}, \n                                 cv = 4, scoring = 'accuracy')\ntpot_classifier.fit(X_train,y_train)","b9bb0074":"\naccuracy = tpot_classifier.score(X_test, y_test)\nprint(accuracy)","ced1ec8e":"The main parameters used by a Random Forest Classifier are:\n\n- criterion = the function used to evaluate the quality of a split.\n- max_depth = maximum number of levels allowed in each tree.\n- max_features = maximum number of features considered when splitting a node.\n- min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n- min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n- n_estimators = number of trees in the ensamble.","b958f42d":"#### Genetic Algorithms\nGenetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts.\n\nLet's immagine we create a population of N Machine Learning models with some predifined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again caltulate the accuracy of each model and repeate the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.","1bc6e463":"### All Techniques Of Hyper Parameter Optimization\n\n1. GridSearchCV\n2. RandomizedSearchCV\n3. Bayesian Optimization -Automate Hyperparameter Tuning (Hyperopt)\n4. Sequential Model Based Optimization(Tuning a scikit-learn estimator with skopt)\n4. Optuna- Automate Hyperparameter Tuning\n5. Genetic Algorithms (TPOT Classifier)\n\n###### References\n- https:\/\/github.com\/fmfn\/BayesianOptimization\n- https:\/\/github.com\/hyperopt\/hyperopt\n- https:\/\/www.jeremyjordan.me\/hyperparameter-tuning\/\n- https:\/\/optuna.org\/\n- https:\/\/towardsdatascience.com\/hyperparameters-optimization-526348bb8e2d(By Pier Paolo Ippolito )\n- https:\/\/scikit-optimize.github.io\/stable\/auto_examples\/hyperparameter-optimization.html\n- https:\/\/github.com\/krishnaik06\/All-Hyperparamter-Optimization","8a9f8466":"#### Bayesian Optimization\nBayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can gives us the lowest possible output value.It usually performs better than random,grid and manual search providing better performance in the testing phase and reduced optimization time.\nIn Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin.\n\n- Objective Function = defines the loss function to minimize.\n- Domain Space = defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).\n- Optimization Algorithm = defines the search algorithm to use to select the best input values to use in each new iteration.","df7f274c":"### Automated Hyperparameter Tuning\nAutomated Hyperparameter Tuning can be done by using techniques such as \n- Bayesian Optimization\n- Gradient Descent\n- Evolutionary Algorithms","6b2d1f9a":"#### GridSearch CV","0b830494":"##### Randomized Search Cv"}}