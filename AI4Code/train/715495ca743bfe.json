{"cell_type":{"22b19b6c":"code","27f10090":"code","cf1ff969":"code","54b94f57":"code","aeea8ff5":"code","18a221ec":"code","275b8869":"code","1dc79095":"code","2de1da3d":"code","8cf3b20a":"code","626e4629":"code","cf5e6077":"code","558c4fcb":"code","63483512":"code","ec8325af":"code","8344ed6e":"code","e72fa3c2":"code","ca3e40cf":"code","f084836b":"code","fdf40a3b":"code","900181b5":"code","1f2fcf19":"markdown","9d684f34":"markdown","9db1b0e0":"markdown","e1c81941":"markdown","4905c5c0":"markdown","2c363615":"markdown","2cae7597":"markdown","0bfecbb3":"markdown","badb3435":"markdown","7e5fd35f":"markdown","a6f5de1b":"markdown"},"source":{"22b19b6c":"import matplotlib.pyplot as plt\nimport os \nimport seaborn as sns\nimport pandas as pd\nimport numpy as np","27f10090":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                shear_range = 0.2,\n                zoom_range = 0.2,\n                width_shift_range = 0.2,\n                height_shift_range = 0.2,\n                fill_mode=\"nearest\",\n                validation_split=0.15)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","cf1ff969":"train_data = train_datagen.flow_from_directory(\"..\/input\/gender-classification-dataset\/Training\",\n                                      target_size=(96,96),\n                                       seed=123,\n                                       batch_size=32,\n                                       subset=\"training\"\n                                      )\nval_data = train_datagen.flow_from_directory(\"..\/input\/gender-classification-dataset\/Training\",\n                                      target_size=(96,96),\n                                       seed=123,\n                                       batch_size=32,\n                                       subset=\"validation\"\n                                      )\ntest_data = test_datagen.flow_from_directory(\"..\/input\/gender-classification-dataset\/Validation\",\n                                    target_size=(96,96),\n                                    seed=123,\n                                    batch_size=32,\n                                    shuffle=False)","54b94f57":"plt.figure(figsize=(20,10))\nsns.countplot(train_data.classes)","aeea8ff5":"from tensorflow.keras.applications import VGG19\n\nvggModel = VGG19(weights=\"imagenet\", input_shape=(96,96,3), include_top=False)","18a221ec":"for layer in vggModel.layers:\n    layer.trainable = False","275b8869":"from tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(vggModel)\nmodel.add(Flatten())\nmodel.add(Dense(units=2, activation=\"sigmoid\"))","1dc79095":"import keras\nmodel.compile(optimizer=keras.optimizers.Adam(lr=0.01), loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","2de1da3d":"from keras.callbacks import EarlyStopping,ModelCheckpoint\n\ncallbacks = [EarlyStopping(monitor=\"val_accuracy\",patience=3),\n            ModelCheckpoint(\"Model.h5\",verbose= 1 ,save_best_only=True)]","8cf3b20a":"hist = model.fit_generator(generator=train_data, \n                          steps_per_epoch=len(train_data)\/\/128,\n                          epochs=20, validation_data=val_data,\n                          validation_steps=len(val_data)\/\/128,\n                          callbacks=callbacks)","626e4629":"plt.figure(figsize=(15,6))\n\nplt.subplot(1,2,1)\nplt.plot(hist.epoch,hist.history['accuracy'],label = 'Training')\nplt.plot(hist.epoch,hist.history['val_accuracy'],label = 'validation')\n\nplt.title(\"Accuracy\")\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(hist.epoch,hist.history['loss'],label = 'Training')\nplt.plot(hist.epoch,hist.history['val_loss'],label = 'validation')\n\nplt.title(\"Loss\")\nplt.legend()\nplt.show()","cf5e6077":"modelEnd = keras.models.load_model(\".\/Model.h5\")","558c4fcb":"pred = modelEnd.predict(test_data, verbose=1)","63483512":"from sklearn.metrics import confusion_matrix,classification_report\n\npredicted = [np.argmax(i) for i in pred]\nconfidence = [np.round((max(i)*100),2) for i in pred]","ec8325af":"y_test = test_data.classes","8344ed6e":"classification_report(predicted,y_test)","e72fa3c2":"print(classification_report(predicted,y_test))","ca3e40cf":"sns.heatmap(confusion_matrix(predicted,y_test),annot=True,fmt=\"d\",cmap=\"Blues\");","f084836b":"import cv2\n\nimg = cv2.imread(\"..\/input\/ronaldo\/4.jpg\")\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)","fdf40a3b":"face_cascade = cv2.CascadeClassifier(\"..\/input\/haarcascade\/haarcascade_frontalface_default.xml\")","900181b5":"results= {0:\"Female\",\n         1:\"Male\"}\n\nface = face_cascade.detectMultiScale(img, 1.1, 7)\n#face = x, y, w, h\nx = face[0][0]\ny = face[0][1]\nw = face[0][2]\nh = face[0][3]\ncv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 5)\n\nimg = cv2.resize(img, (96,96))\nimg = np.resize(img, (1,96,96,3))\nresult = np.argmax(model.predict(img))\nresults[result]","1f2fcf19":"![2.png](attachment:807142a9-793b-49ce-89fc-389f9d93b437.png)","9d684f34":"![1.png](attachment:57e31dbf-8392-4c86-9192-db7eaa4c7236.png)","9db1b0e0":"**Let's try on example**","e1c81941":"**Look Data Distrubition**","4905c5c0":"***Let's look at VGG-16 Model Representation***","2c363615":"**Compile and Train VGG-16 Model**","2cae7597":"# Prepare Model","0bfecbb3":"**We'll use VGG-16 model for classification and we'll try model on some images**","badb3435":"![3.png](attachment:d1e38382-a251-42d2-901b-f728a0e51daa.png)","7e5fd35f":"**If you are wondering how \u0131 can implement in code , I left VGG-16 Model Structure as an image**","a6f5de1b":" # Prepare Data With ImageDataGenerator"}}