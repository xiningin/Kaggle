{"cell_type":{"3d111c7e":"code","8a78b7a6":"code","ae96cc96":"code","b0ea53fc":"code","d1d69992":"code","054793ca":"code","44aaf96d":"code","8e14bd90":"code","e0f2d4a2":"code","188aa8ae":"code","30b0f737":"code","55e10101":"code","9cfd95d3":"code","f0323149":"code","d54bd16a":"code","f7e632ae":"code","682a5c8a":"code","db7a75cf":"code","08a5dde1":"code","86fe705d":"code","4dc02f73":"code","fdae0d97":"code","716506bc":"code","6c66bd77":"code","9e9a97be":"code","4a032390":"code","0c9d4a47":"code","74595812":"code","1a086859":"code","a226033a":"code","b17ae84e":"code","02aa828c":"code","c711de57":"code","7b380ef9":"code","f8adeb6e":"code","78fa4d0c":"code","d03690cb":"code","b530fdc7":"code","317e9ffd":"code","17628c2a":"code","fdd3dc34":"markdown","22a0445c":"markdown","ecb3e505":"markdown","fdef6f6d":"markdown","c59e6d38":"markdown","61b8176e":"markdown","0884cde0":"markdown","93e384b4":"markdown","808ff175":"markdown","f1a14204":"markdown","f4d3c809":"markdown","6718a261":"markdown","f7997ad3":"markdown","8e055cab":"markdown","a64bf5f1":"markdown"},"source":{"3d111c7e":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\nimport lightgbm as lgb \nimport xgboost as xgb\nfrom sklearn.linear_model import LinearRegression,HuberRegressor,SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport optuna\nimport math","8a78b7a6":"LINEAR_DATE_AUG = False\n\nOPTUNA = True\nNUM_TRIALS = 400\n\n##Add DATA\nADD_2014 = False \n\n#Holidays\nHOLIDAYS = True   #good for all except catboost?\nNEXT_HOLIDAY = True  # good for lightgbm\n\nPOST_PROCESSING = False\nMODEL_TYPE = \"lightgbm\" #lightgbm catboost\n\nVAL_SPLIT = \"2017-12-31\" #\"2018-05-31\"","ae96cc96":"EPOCHS = 10000     #Catboost best is 100 epochs - lightgbm is 1000\nEARLY_STOPPING = 30\n\nDEVICE = \"cpu\"\nBOOSTING =  'gbdt'  # \"goss\" 'dart'  'gbdt'\n\nSCALER_NAME = \"MinMax\"  #None MinMax\nSCALER = MinMaxScaler() ","b0ea53fc":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col = 0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\n\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\nif HOLIDAYS:\n    holidays = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\",usecols = [\"Date\",\"Country\",\"Name\"]                      )\n    holidays.rename(columns = {\"Date\":\"date\",\"Country\":\"country\",\"Name\":\"holiday\"},inplace= True)\n    holidays[\"holiday\"]= 1\n    holidays[\"holiday\"]= holidays[\"holiday\"].astype(\"int32\")\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])","d1d69992":"#Make date\ntrain[\"date\"] = pd.to_datetime(train[\"date\"])\ntest[\"date\"] = pd.to_datetime(test[\"date\"])","054793ca":"if ADD_2014:\n    train_2014= train [train[\"date\"]<\"2016-01-01\"]\n    train_2014[\"date\"] = train_2014[\"date\"] - pd.DateOffset(years=1)\n    train_2014[\"num_sold\"] = train_2014[\"num_sold\"]*0.98\n    train = pd.concat([train_2014,train],axis=0,ignore_index=True)","44aaf96d":"train.head()","8e14bd90":"def public_hols(df):\n    df = pd.merge(df, holidays, how='left', on=['date', 'country'])\n    df.fillna(value = 0,inplace=True)\n    \n    return df\n\nif HOLIDAYS:\n    train = public_hols(train)\n    test = public_hols(test)","e0f2d4a2":"def get_gdp(row):\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]","188aa8ae":"def engineer(df):\n    #get GDP from file \n    df[\"gdp\"] = df.apply(get_gdp, axis=1)   #improves Huber & Tweedie & catboost\n    \n    df[\"day\"] = df[\"date\"].dt.day\n    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"year\"] = df[\"date\"].dt.year\n    \n    #play around with if Tree model - each varies \n    #df['dayofyear'] = df['date'].dt.dayofyear                ### This can cause noise \n    df['inverse_dayofyear'] = 365 - df['date'].dt.dayofyear    # good for all  except catboost\n    df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"] = df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"]+1     #Leap year in 2016\n    df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)      # Good for lightgbm & Huber, bad for Tweedie & catboost\n    df['daysinmonth'] = df['date'].dt.days_in_month           ## Bad for all except Lightgbm\n     \n    # catboost and lightgbm dont like this   \n    #df[\"Friday\"] = df[\"dayofweek\"] ==4\n    #df[\"Sat_sun\"] = (df[\"dayofweek\"] ==5) |(df[\"dayofweek\"] ==6)\n    \n    if LINEAR_DATE_AUG:\n        for country in ['Finland', 'Norway']:\n            df[country] = df.country == country\n        df['KaggleRama'] = df.store == 'KaggleRama'\n        for product in ['Kaggle Mug', 'Kaggle Sticker']:\n            df[product] = df['product'] == product\n            \n        df.drop([\"country\",\"store\",\"product\"], axis =1, inplace = True)\n\n        # Seasonal variations (Fourier series)\n        dayofyear = df.date.dt.dayofyear\n        for k in range(1, 20):\n            df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n            df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n            df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n            df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n\n    return df","30b0f737":"train = engineer(train)\ntest = engineer(test)\n\ncategorical_feats = [\n    \"country\",\"store\",\"product\",\n                     \"quarter\", \n                    ]","55e10101":"def next_holiday(x):\n    i=1\n    while sum(holidays[\"date\"] == pd.Timestamp(x) + pd.DateOffset(days=i)) ==0:\n        i+=1\n        if i >200:\n            i=0\n            break\n            break\n    return i\n\nif NEXT_HOLIDAY:\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])\n    train[\"to_holiday\"] = train[\"date\"].apply(lambda x : next_holiday(x))\n    test[\"to_holiday\"] = test[\"date\"].apply(lambda x : next_holiday(x))","9cfd95d3":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","f0323149":"def scale_data(X_train, X_test, test):\n    scaler= SCALER\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    test = scaler.transform(test)\n    \n    return X_train, X_test, test","d54bd16a":"train.head()","f7e632ae":"train = pd.get_dummies(train,columns= categorical_feats)\ntest = pd.get_dummies(test,columns= categorical_feats)","682a5c8a":"train.head()","db7a75cf":"prior_2017 = train[train[\"date\"]<=VAL_SPLIT].index\nafter_2017 = train[train[\"date\"]>VAL_SPLIT].index","08a5dde1":"train.index = train[\"date\"]\ntrain.drop(\"date\",axis=1,inplace=True)\n\ntest.index = test[\"date\"]\ntest.drop(\"date\",axis=1,inplace=True)","86fe705d":"X = train.drop(\"num_sold\", axis=1)\ny= train[\"num_sold\"]","4dc02f73":"X_train = train.iloc[prior_2017,:].drop(\"num_sold\", axis=1)\nX_test = train.iloc[after_2017,:].drop(\"num_sold\", axis=1)\ny_train= train.iloc[prior_2017,:][\"num_sold\"]\ny_test= train.iloc[after_2017,:][\"num_sold\"]","fdae0d97":"X_train.head(2)","716506bc":"X_train,X_test,test =scale_data(X_train,X_test,test)","6c66bd77":"X_train[0]","9e9a97be":"def objective_lgb(trial):\n    # 2. Suggest values of the hyperparameters using a trial object.\n    params = {\n        #'objective': OBJECTIVE,\n        #'metric': METRIC,\n        \"num_threads\": -1,\n        \"verbose\" : -1,\n        \"boosting_type\":BOOSTING,\n        \"objective\":trial.suggest_categorical(\"objective\", ['poisson', 'mape', 'rmse',\"mae\"]),\n        \"learning_rate\": trial.suggest_uniform('learning_rate', 0.001, 0.10),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),  #usually less than 2**max depth\n        'max_bin': trial.suggest_int('max_bin', 10, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n        \"min_data_in_leaf\":trial.suggest_int('min_data_in_leaf', 10,200)\n    }\n    \n    metric_optuna = trial.suggest_categorical(\"metric\", ['poisson', 'mape', 'rmse',\"mae\"])\n    params[\"metric\"] = metric_optuna\n    \n    train_data = lgb.Dataset(X_train, label=y_train,)\n    test_data =lgb.Dataset(X_test,label=y_test )\n\n    #pruning_callback = optuna.integration.LightGBMPruningCallback(trial, metric_optuna)\n    \n    ## CREATE lightgbm model\n    model = lgb.train(params=params,\n                      train_set= train_data, \n                      num_boost_round= EPOCHS,\n                      valid_sets= [test_data], \n                      callbacks=[lgb.early_stopping(EARLY_STOPPING),\n                                 #pruning_callback\n                                ]\n                     )\n\n    test_predictions = model.predict(X_test)\n    smape = SMAPE(y_test,test_predictions)\n    \n    print(\"SMAPE:\",smape)\n    \n    return smape","4a032390":"if OPTUNA:\n    print(\"RUNNINING OPTUNA LIGHTGBM\")\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective_lgb, n_trials=NUM_TRIALS)\n    trial = study.best_trial","0c9d4a47":"if OPTUNA:\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial num :\",trial.number)\n    print(\" SMAPE Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","74595812":"#poisson', 'mape', 'rmse',\"tweedie\"\nif OPTUNA: \n    print(\"Using Optuna params\")\n    params = trial.params\n\nelse:\n    #OPTUNA 400 7.79556964946429\n    params= {\n    \"objective\": \"poisson\",\n    \"learning_rate\": 0.007502236406176916,\n    \"lambda_l1\": 4.901678633449036e-08,\n    \"lambda_l2\": 0.04542006655651054,\n    \"num_leaves\": 176,\n    \"max_bin\": 748,\n    \"max_depth\": 10,\n    \"metric\": \"mape\"\n    }\n\"\"\"    \n    # optuna score of : 7.886738068386228\n    params ={\n        \"objective\": \"poisson\",\n        \"metric\": \"mape\",\n        \"learning_rate\": 0.0856871611722979,\n        \"lambda_l1\": 6.006877273786205e-07,\n        \"lambda_l2\": 4.784086439520516e-06,\n        \"num_leaves\": 178,\n        \"max_bin\": 542,\n        \"max_depth\": 10\n    }\"\"\"","1a086859":"def fit_model(X_train,y_train,X_test,y_test):\n    \n    train_data = lgb.Dataset(X_train, label=y_train,)\n    test_data =lgb.Dataset(X_test,label=y_test )\n\n    ## CREATE lightgbm model\n    model = lgb.train(params=params,\n                      train_set= train_data, \n                      num_boost_round= EPOCHS,\n                      valid_sets= [test_data], \n                      callbacks=[lgb.early_stopping(EARLY_STOPPING)],\n                     )\n\n    test_predictions = model.predict(X_test)\n    print(\"SMAPE:\", SMAPE(y_test,test_predictions))\n    \n    return test_predictions, model","a226033a":"test_predictions, model = fit_model(X_train,y_train,X_test,y_test)","b17ae84e":"print(\"SMAPE :\",SMAPE(y_test,test_predictions) )\nprint(f\"\\n EPOCHS: {EPOCHS}\")\nprint(f\"\\n SCALER: {SCALER_NAME}\")\nprint(f\"\\n PARAMS: { params}\")\nprint(f\"\\n Holidays : {HOLIDAYS}\")\nprint(f\"\\n Next Holiday : {NEXT_HOLIDAY}\")\nprint(f\"\\n Linear Date Augmentation : {LINEAR_DATE_AUG}\")\nprint(f\"\\n POST_PROCESSING: {POST_PROCESSING}\")\n\n## 8.10587538123248  - no linear - holidays + next holiday  ","02aa828c":"final_predictions = model.predict(test)","c711de57":"final_predictions","7b380ef9":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)","f8adeb6e":"if POST_PROCESSING:\n    # from previous run we are under predicting, lets scale the values upwards\n    print(\"Scaling predictions \")\n    print(\"preds_prior:\", final_predictions)\n    \n    sub[\"num_sold\"] = final_predictions*1.143\n    \n    print(\"preds after:\", np.array(sub[\"num_sold\"]))\nelse:\n    sub[\"num_sold\"] = final_predictions","78fa4d0c":"sub.to_csv(\"submission.csv\")","d03690cb":"sub.head()","b530fdc7":"sub.head()","317e9ffd":"#for visual only\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ntest[\"date\"] = pd.to_datetime(test[\"date\"])\n\nfig,ax = plt.subplots(2,1, figsize=(25,20),sharey= True)\n\ndiff = y_test - test_predictions\nsns.lineplot(ax=ax[0], data= y_test, label=\"Train Actual\",ci=None)\nsns.lineplot(ax=ax[0], data = y_test,x = y_test.index , y = test_predictions, label =\"Validation Prediction\" ,ci=None)\nsns.lineplot(ax=ax[0],data =sub, x= test[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \n\nax[0].set_title(f\"Actual and Predicted Sales for {MODEL_TYPE}\")\n\nsns.lineplot(ax=ax[1], data = diff, label =\"Residuals\" )\nax[1].set_title(f\"Residuals for {MODEL_TYPE} for 2018\")\n\nplt.show()","17628c2a":"plt.figure(figsize=(25,10))\n\nsns.lineplot(data= train[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data =sub, x= test[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \nplt.title(\"Actual and Predicted Sales\")\n\nplt.show()","fdd3dc34":"# Post Processing & Submission ","22a0445c":"# Training Visualization","ecb3e505":"# Run model","fdef6f6d":"# Model Creation","c59e6d38":"# Functions ","61b8176e":"# Split and Scale","0884cde0":"obj is the objective function of the algorithm, i.e. what it's trying to maximize or minimize, e.g. \"regression\" means it's minimizing squared residuals.\n\nMetric and eval are essentially the same. They are used for Early stopping ","93e384b4":"### One HotEncoder","808ff175":"# Optuna","f1a14204":"# To DO\n1. Add fake 2014 data with gDP\n1. Run a model for each product\n1. Recursive model\n1. Add Further Feature engineering - ambrosm https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/\n1. SARIMAX model\n\n* **Outliers**? - check for linear and Trees\n\n* Catboost minmax scaling ","f4d3c809":"# Libraries","6718a261":"# Final Train","f7997ad3":"# Load Data","8e055cab":"## Previous  Runs \n\n#### Best - 8.10587538123248  - no linear - holidays + next holiday  \n\nSMAPE : 7.883328735411218\n\n EPOCHS: 10000\n\n SCALER: MinMax\n\n PARAMS: {'objective': 'poisson', 'learning_rate': 0.007502236406176916, 'lambda_l1': 4.901678633449036e-08, 'lambda_l2': 0.04542006655651054, 'num_leaves': 176, 'max_bin': 748, 'max_depth': 10, 'metric': 'mape'}\n\n Holidays : True\n\n Next Holiday : True\n\n Linear Date Augmentation : False\n\n POST_PROCESSING: False\n \n \n## Date Aug\n\ndef engineer(df):\n    #get GDP from file \n    df[\"gdp\"] = df.apply(get_gdp, axis=1)   #improves Huber & Tweedie & catboost\n    \n    df[\"day\"] = df[\"date\"].dt.day\n    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"year\"] = df[\"date\"].dt.year\n    \n    #play around with if Tree model - each varies \n    #df['dayofyear'] = df['date'].dt.dayofyear                ### This can cause noise \n    df['inverse_dayofyear'] = 365 - df['date'].dt.dayofyear    # good for all  except catboost\n    df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"] = df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"]+1     #Leap year in 2016\n    df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)      # Good for lightgbm & Huber, bad for Tweedie & catboost\n    df['daysinmonth'] = df['date'].dt.days_in_month           ## Bad for all except Lightgbm\n     \n    # catboost and lightgbm dont like this   \n    #df[\"Friday\"] = df[\"dayofweek\"] ==4\n    #df[\"Sat_sun\"] = (df[\"dayofweek\"] ==5) |(df[\"dayofweek\"] ==6)\n    \n    if LINEAR_DATE_AUG:\n        for country in ['Finland', 'Norway']:\n            df[country] = df.country == country\n        df['KaggleRama'] = df.store == 'KaggleRama'\n        for product in ['Kaggle Mug', 'Kaggle Sticker']:\n            df[product] = df['product'] == product\n            \n        df.drop([\"country\",\"store\",\"product\"], axis =1, inplace = True)\n\n        # Seasonal variations (Fourier series)\n        dayofyear = df.date.dt.dayofyear\n        for k in range(1, 20):\n            df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n            df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n            df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n            df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n            df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n\n    return df","a64bf5f1":"Lightgbm doesnt like linear changes"}}