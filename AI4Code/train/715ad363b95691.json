{"cell_type":{"6d227807":"code","08e82275":"code","4d92dac1":"code","64761390":"code","6c639c29":"code","e37e4238":"code","30befb5c":"code","5adda965":"code","67c59c2c":"code","a7bfdcf8":"code","ac3f85eb":"code","b1db801a":"code","1d449856":"code","2145b25e":"code","2f87cbc2":"code","e0ecb0d0":"code","7d441a63":"code","97e432d9":"code","34883310":"code","f915495f":"code","f6464de9":"code","3913ac8d":"code","a803aaad":"code","f692cd7a":"code","4e1748c0":"code","9098df33":"code","c572c0d0":"code","a6495b75":"code","6e0f6275":"code","da79eb61":"code","c1c5a529":"code","3b1c3a62":"code","05b624b2":"code","b6175608":"code","c9e3d0bd":"code","be25fca5":"code","064eaab8":"code","528f33c1":"code","b463510d":"code","a6192482":"code","9fe08f9f":"markdown","9bc91083":"markdown","2a6bce5b":"markdown","e9c5debc":"markdown","ce4b87b6":"markdown","a8751d5e":"markdown","9a31756c":"markdown","26587742":"markdown","16bdb72d":"markdown","bc625cf5":"markdown","162cc995":"markdown","cd5d224a":"markdown","cfa751c9":"markdown","97972fb4":"markdown","c1d428e5":"markdown","63361955":"markdown","4df1fbe8":"markdown","2e51efa6":"markdown","9d64ff32":"markdown","825f1fee":"markdown","775e8a95":"markdown","c970cf3b":"markdown","d224af13":"markdown"},"source":{"6d227807":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n#from empiricaldist import Pmf, Cdf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08e82275":"def LogRegModel(X, y):\n    X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y) \n    logreg = LogisticRegression()\n    logreg.fit(X_train, y_train)\n    y_pred = logreg.predict(X_test)\n    score = logreg.score(X_test, y_test)\n    matrix = confusion_matrix(y_test, y_pred)\n    #print('The score of the model is {}'.format(score))\n    #print('The confusion matrix is: ', matrix)\n    return score, matrix","4d92dac1":"def KNeighbors(X, y):\n    X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n    k = KNeighborsClassifier(n_neighbors=3)\n    k.fit(X_train, y_train)\n    y_pred = k.predict(X_test)\n    score = k.score(X_test, y_test)\n    matrix = confusion_matrix(y_test, y_pred)\n    #print('The score of the model is {}'.format(score))\n    #print('The confusion matrix is: ', matrix)\n    return score, matrix","64761390":"def TreeClass(X, y):\n    X_train, X_test,y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n    tree = DecisionTreeClassifier()\n    tree.fit(X_train, y_train)\n    y_pred = tree.predict(X_test)\n    score = tree.score(X_test, y_test)\n    matrix = confusion_matrix(y_test, y_pred)\n    #print('The score of the model is {}'.format(score))\n    #print('The confusion matrix is: ', matrix)\n    return score, matrix","6c639c29":"def ecdf(dataseries):\n    ### Compute Empirical CDF for 1D dataseries ###\n    n = len(dataseries)\n    x = np.sort(dataseries)\n    y = np.arange(1, n+1)\/n\n    return x, y","e37e4238":"#read in train and test data sets\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv') \ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","30befb5c":"#check shape of  datasets\nprint('train: ',train.shape)\nprint('test: ',test.shape)","5adda965":"display(train.head())","67c59c2c":"print(train.info())\nprint(test.info())","a7bfdcf8":"data_list = [train, test] #create list of both datasets to easily replicate manipulation\n\n#drop passenger id, name, cabin,and ticket\nfor dataset in data_list:\n    dataset.drop(['Name', 'Cabin', 'Ticket'], axis=1, inplace=True)\n    \n#Handle missing data in both datasets\nfor dataset in data_list:\n    dataset['Age'].fillna(dataset.Age.mean(), inplace=True)\n    dataset['Embarked'].fillna(dataset.Embarked.mode()[0], inplace=True)\n    \n#fill in missing value for missing fare in test set     \ntest['Fare'].fillna(test['Fare'].median(), inplace=True)","ac3f85eb":"#check discrete object columns for incorrect or redundanct values\n#print(train.Sex.dtype)\ndisplay(train['Sex'].unique())\n#print(train.Embarked.dtype)\ndisplay(train['Embarked'].unique())","b1db801a":"#convert object classes to category classes and then convert to numerical features\nfor dataset in data_list:\n    for column in dataset:\n        if dataset[column].dtype == 'object':\n            dataset[column] = dataset[column].astype('category')\n            dataset[column] = LabelEncoder().fit_transform(dataset[column])\n            \n#check to see that there are no missing\nprint(train.info())\nprint(test.info())","1d449856":"train.head()","2145b25e":"#initialize random seed\nnp.random.seed(0)","2f87cbc2":"#create list for training features and labels\nfeatures = list(train.columns.drop('Survived'))\nX = train[features]\ny = train['Survived']","e0ecb0d0":"model1 = LogRegModel(X, y)","7d441a63":"print('Number of Survivors: {}'.format(train.Survived.sum())) #total number of survivors in training set\nprint('Number of Deaths: {}'.format(train[train['Survived'] == 0]['Survived'].count())) #total number of deaths in training set","97e432d9":"print('Total number of males onboard: {}'.format(train[train['Sex'] == 0]['Sex'].count()))\nprint('Total number of females onboard: {}'.format(train[train['Sex'] == 1]['Sex'].count()))\nsns.countplot(data=train, x='Survived', hue='Sex')\nplt.title('Number of Survivors Male vs. Female')\nplt.show()","34883310":"age_dist = sns.FacetGrid(train, col='Survived')\nage_dist.map(sns.distplot, 'Age')\nplt.show()\n\nsns.distplot(train['Age'])\nplt.ylabel\nplt.show()","f915495f":"for dataset in data_list:\n    dataset['Age'] = pd.cut(dataset['Age'], bins=[0,4,9,14,19,24,35,50,65,120], labels=['Toddler', 'Young Child', 'Child', 'Teenage', 'Young Adult', 'Adult','Older Adult', 'Middle Age', 'Senior'])","f6464de9":"Age_Group_Count = train.groupby('Age')['Survived'].sum()\nAge_Group_Proportion = train.groupby('Age')['Survived'].mean()\nAge_Group_Count\nAge_Group_Proportion","3913ac8d":"print(train['Age'].value_counts())\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\n\n#fig.subplots_adjust(wspace=1)\n\nAge_Group_Count.plot(kind='bar', ax=ax[0])\nax[0].set_title('Total Survived per Age Group')\n\nAge_Group_Proportion.plot(kind='bar', ax=ax[1])\nax[1].set_title('Percent survived per Age Group')\n\nplt.show()","a803aaad":"for dataset in data_list:\n    dataset['Age'] = LabelEncoder().fit_transform(dataset['Age'])","f692cd7a":"#see distribution of survived based of embarked location\nfig, ax = plt.subplots(1, 3, figsize=(15,10))\nfig.subplots_adjust(wspace=0.5)\nsns.countplot(data=train, x='Survived', hue='Embarked', ax=ax[0])\nax[0].set_title('Suvival and Death numbers per Embarked')\n\nsns.countplot(data=train, x='Embarked', ax=ax[1])\nax[1].set_title('Total Passengers Embarked from each location')\n\nax[2] = embarked_mean = train.groupby('Embarked')['Survived'].mean().plot(kind='bar', ax=ax[2])\nax[2].set_title('Survival % from each location')\nplt.show()\n\n\n","4e1748c0":"for dataset in data_list:\n    dataset.drop('Embarked', axis=1, inplace=True)","9098df33":"#see distribution of survived based of embarked location\nfig, ax = plt.subplots(1, 3, figsize=(15,10))\nfig.subplots_adjust(wspace=0.5)\nsns.countplot(data=train, x='Survived', hue='Pclass', ax=ax[0])\nax[0].set_title('Suvival and Death numbers per Economic Class')\n\nsns.countplot(data=train, x='Pclass', ax=ax[1])\nax[1].set_title('Total Passengers Embarked from each Economic Class')\n\nax[2] = embarked_mean = train.groupby('Pclass')['Survived'].mean().plot(kind='bar', ax=ax[2])\nax[2].set_title('Survival % from each economic class')\nplt.show()","c572c0d0":"print(train['SibSp'].value_counts())\nprint(train['Parch'].value_counts())","a6495b75":"fig, ax = plt.subplots(1,2, figsize = (8, 10))\nsns.countplot(data=train, x='Parch', hue='Survived', ax=ax[0])\nsns.countplot(data=train, x='SibSp', hue='Survived', ax=ax[1])\nplt.show()","6e0f6275":"# for dataset in data_list:\n# #     dataset['Total Family'] = pd.cut(dataset['Total Family'], bins=[-1,3,6,16], labels=['Small', 'Medium', 'Large'])\n# #     dataset['Total Family'] = LabelEncoder().fit_transform(dataset['Total Family'])\n#     dataset.drop(['Parch', 'SibSp'], axis=1, inplace=True)","da79eb61":"fare_x_ecdf, fare_y_ecdf = ecdf(train['Fare'])\nsns.lineplot(x=fare_x_ecdf, y=fare_y_ecdf)\nplt.xlabel('Ticket Fare')\nplt.ylabel('Pecentile of Ticket Fare')\nplt.title('ECDF of Ticket Fare ')\nplt.show()","c1c5a529":"#create groups for price paid for ticket\npercentiles = [20, 40, 60, 80]\nprint(train['Fare'].min())\nprint(np.percentile(train['Fare'], percentiles))\nprint(train['Fare'].max())","3b1c3a62":"fare_bins = np.percentile(train['Fare'], percentiles)\nfare_bins = np.append(fare_bins, np.max(train['Fare'])+1)\nfare_bins = np.insert(fare_bins, 4, 100)\nfare_bins = np.insert(fare_bins, 0, np.min(train['Fare'])-1)\n#print(fare_bins)\nfor dataset in data_list:\n    dataset['Fare'] = pd.cut(dataset['Fare'], bins=fare_bins, labels=['VeryLow', 'Low', 'LowMed', 'Medium','MedHigh', 'High'])\n","05b624b2":"sns.countplot(x='Fare', hue='Survived', data=train)\nplt.show()","b6175608":"for dataset in data_list:\n    dataset['Fare'] = LabelEncoder().fit_transform(dataset['Fare'])\n    ","c9e3d0bd":"features = list(train.columns.drop(['PassengerId','Survived']))\nX = train[features]\ny = train['Survived']\nX_test_data = test[features]","be25fca5":"log_model2 = LogRegModel(X, y)\nk_model = KNeighbors(X, y)\ntree_model = TreeClass(X, y)","064eaab8":"print(model1)\nprint(log_model2)\nprint(tree_model)\nprint(k_model)","528f33c1":"tree = DecisionTreeClassifier()\ntree.fit(X, y)\ny_pred_test = tree.predict(X_test_data)","b463510d":"test_id = test['PassengerId']\ntest_results = y_pred_test","a6192482":"results = pd.DataFrame({'PassengerId': test_id, 'Survived': test_results})\nresults.to_csv('survival_predictions.csv', index=False)","9fe08f9f":"Unlike Embarked location, the number survivors and deaths from the different economic classes doesn't look proportionate to the total number of passengers from each class","9bc91083":"three columns with missing values (Age, Cabin, Embarked)\nCabin missing many values so will delete column\nMissing Age will be imputed\n\nPassengerId,Name, and Ticket can also be removed as they are unique to every passenger so will not help in classification","2a6bce5b":"Most data comes from only 1 embarked location and plots don't indicate enough of a correlation to outcome based so it can be removed as feature","e9c5debc":"There does not look to be any issues with the categorical data. But for Scikit-Learn Modelling features need to be numerical so each categorical feature will be encoded to numbers","ce4b87b6":"Decision Tree Model has highest accuracy and confusion matrix reflects that so will be used for submission. Update model to use full set of training data for final submission.","a8751d5e":"# **Visualization of Family Members**","9a31756c":"The strictly alphabetical characters are good for visualization but need to be converted to back to a numerical value corresponding to each category","26587742":"Test data sets has one less column since survival of person is not listed","16bdb72d":"The countplot shows that a disproportionate amount of males died compared to females and in toal more females survived compared to men","bc625cf5":"# **Visualization of Age**","162cc995":"parch = # of parents\/children on boards\nsibsp = # of siblings\/spouse on board","cd5d224a":"# **Visualization of Passengers by Sex**","cfa751c9":"# **Logistic Regression w\/o Feature Engineering**","97972fb4":"The age group with the most survivors is the adult age group, although it could be telling to see whether that age group was the most represented on the titanic.  The group with the survival mortality rate is Toddler which is at about 2\/3 of the group","c1d428e5":"# **Logistic Regression with Basic Feature Engineering**","63361955":"# <center>Titanic Survival Prediction using Machine Learning<\/center>","4df1fbe8":"<b> Function Definitions  <\/b>","2e51efa6":"# **Visualization of Fare Prices**","9d64ff32":"# **Feature Engineering and EDA**","825f1fee":"There are significantly more deaths in the training data set than survivors","775e8a95":"Distribution of age for survival vs deceased pretty consistent with overal distriubtion of age","c970cf3b":"# **Visualization of Embarked Location**","d224af13":"# **Visualization of Economic Class**"}}