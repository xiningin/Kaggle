{"cell_type":{"235aa07f":"code","983e3e03":"code","7db12ab7":"code","817c077a":"code","e13fb41e":"code","71568ede":"code","54667723":"code","707360f1":"code","d05a28a3":"code","ebdf6768":"code","4f0192b5":"code","7e90b504":"code","2b029f64":"code","57015a88":"code","a59070e1":"code","06a4e1fd":"code","3eec0345":"code","62410ee8":"code","98b6f443":"code","a16678ec":"code","815d8202":"code","b5c07d92":"code","619f8115":"code","acaf77b3":"code","b68f7bec":"code","e463038f":"code","558037f9":"code","6fbd274a":"code","407d3413":"code","a6a01590":"code","2dc36d56":"code","1556bbef":"code","096443df":"code","d507fb80":"code","6c3a8758":"code","9bf5406a":"markdown","2fd2f501":"markdown","a95f861b":"markdown","1a7d3132":"markdown","16781862":"markdown","23eacf63":"markdown","d017241c":"markdown","5da049b3":"markdown","7655b01a":"markdown","901c4fdd":"markdown","2105e70e":"markdown","b6821573":"markdown","31db0d41":"markdown","c07fa938":"markdown","11348560":"markdown","7c81fde1":"markdown","3f13c304":"markdown","a169c3a4":"markdown","b251ed7c":"markdown","588e3ba9":"markdown","047aa3cc":"markdown","a8b5a7b7":"markdown","b5ed8bcc":"markdown","4d5737db":"markdown","6d8a8d1e":"markdown","2b07bcc1":"markdown","4dbc7dea":"markdown","a6af9e10":"markdown","8f08b5ef":"markdown","c3c9dea3":"markdown","a552b55e":"markdown","e52f88a4":"markdown","a33026f2":"markdown","bc8e23f2":"markdown","09eeea03":"markdown","20906110":"markdown","31bb2f07":"markdown","015d8670":"markdown","9b2084fc":"markdown","fdd00fc2":"markdown","3306ecc8":"markdown","114cf960":"markdown","4d2c3014":"markdown","159f870b":"markdown","46fc4036":"markdown","e5320e50":"markdown","33fbeb9c":"markdown","c9d1ad75":"markdown","c7fab9c7":"markdown","2a937884":"markdown","d86703ee":"markdown","22c59c62":"markdown","63157ff4":"markdown","0e460a56":"markdown","33e4f706":"markdown","27141865":"markdown","86820b60":"markdown","c55a1fc2":"markdown","2a1fd7f4":"markdown","95cb2a2a":"markdown","925ebc3c":"markdown","eaf90713":"markdown","520341ef":"markdown","82ad8274":"markdown"},"source":{"235aa07f":"# import libraries for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# import libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n# import libraries for building linear regression model\nfrom statsmodels.formula.api import ols\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n\n# import library for preparing data\nfrom sklearn.model_selection import train_test_split\n\n# import library for data preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","983e3e03":"df = pd.read_csv(\"..\/input\/boston-house-prices\/housing.csv\",  header=None, delim_whitespace=True)\ndf.head()","7db12ab7":"df.columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B', 'LSTAT','MEDV']\ndf.head()","817c077a":"df.info()","e13fb41e":"df.describe().T","71568ede":"# let's plot all the columns to look at their distributions\nfor i in df.columns:\n    plt.figure(figsize=(7, 4))\n    sns.histplot(data=df, x=i, kde = True)\n    plt.show()","54667723":"df['MEDV_log'] = np.log(df['MEDV'])","707360f1":"sns.histplot(data=df, x='MEDV_log', kde = True)","d05a28a3":"plt.figure(figsize=(12,8))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(df.corr(),annot=True,fmt='.2f',cmap=cmap ) \nplt.show()","ebdf6768":"# scatterplot to visualize the relationship between NOX and INDUS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x=df['NOX'], y=df['INDUS'], data=df)\nplt.show()","4f0192b5":"# scatterplot to visualize the relationship between AGE and NOX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x=df['AGE'], y=df['NOX'], data=df)\n\nplt.show()","7e90b504":"# scatterplot to visualize the relationship between DIS and NOX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(y=df['NOX'], x=df['DIS'], data=df)\n\nplt.show()","2b029f64":"# scatterplot to visualize the relationship between AGE and DIS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'AGE', y = 'DIS', data = df)\nplt.show()","57015a88":"# scatterplot to visualize the relationship between AGE and INDUS\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'AGE', y = 'INDUS', data = df)\nplt.show()","a59070e1":"# scatterplot to visulaize the relationship between RAD and TAX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'RAD', y = 'TAX', data = df)\nplt.show()","06a4e1fd":"# remove the data corresponding to high tax rate\ndf1 = df[df['TAX'] < 600]\n# import the required function\nfrom scipy.stats import pearsonr\n# calculate the correlation\nprint('The correlation between TAX and RAD is', pearsonr(df1['TAX'], df1['RAD'])[0])","3eec0345":"# scatterplot to visualize the relationship between INDUS and TAX\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'INDUS', y = 'TAX', data = df)\nplt.show()","62410ee8":"# scatterplot to visulaize the relationship between RM and MEDV\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'RM', y = 'MEDV', data = df)\nplt.show()","98b6f443":"# scatterplot to visulaize the relationship between LSTAT and MEDV\nplt.figure(figsize=(6, 6))\nsns.scatterplot(x = 'LSTAT', y = 'MEDV', data = df)\nplt.show()","a16678ec":"# separate the dependent and indepedent variable\nY = df['MEDV_log']\nX = df.drop(columns = {'MEDV', 'MEDV_log'})\n\n# add the intercept term\nX = sm.add_constant(X)","815d8202":"# splitting the data in 70:30 ratio of train to test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30 , random_state=1)","b5c07d92":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# function to check VIF\ndef checking_vif(train):\n    vif = pd.DataFrame()\n    vif[\"feature\"] = train.columns\n\n    # calculating VIF for each feature\n    vif[\"VIF\"] = [\n        variance_inflation_factor(train.values, i) for i in range(len(train.columns))\n    ]\n    return vif\n\n\nprint(checking_vif(X_train))","619f8115":"# create the model after dropping TAX\nX_train = X_train.drop(['TAX'],1)\n\n# check for VIF\nprint(checking_vif(X_train))","acaf77b3":"# create the model\nmodel1 = sm.OLS(y_train, X_train).fit()\n\n# get the model summary\nmodel1.summary()","b68f7bec":"# create the model after dropping TAX\nY = df['MEDV_log']\n\n#write your code here\nX = df.drop(columns = {'MEDV', 'MEDV_log', 'ZN', 'AGE', 'INDUS', 'TAX'}) \nX = sm.add_constant(X)\n\n#splitting the data in 70:30 ratio of train to test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30 , random_state=1)\n\n# create the model\nmodel2 = sm.OLS(y_train, X_train).fit() \n\n# get the model summary\nmodel2.summary()","e463038f":"residuals = model2.resid\nresiduals.mean()","558037f9":"from statsmodels.stats.diagnostic import het_white\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms","6fbd274a":"name = [\"F statistic\", \"p-value\"]\ntest = sms.het_goldfeldquandt(y_train, X_train)\nlzip(name, test)","407d3413":"# predicted values\nfitted = model2.fittedvalues\n\n#sns.set_style(\"whitegrid\")\nsns.residplot(x = y_train, y = residuals , color=\"lightblue\", lowess=True) \nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residual\")\nplt.title(\"Residual PLOT\")\nplt.show()","a6a01590":"# Plot histogram of residuals\nsns.histplot(residuals, kde=True)","2dc36d56":"# Plot q-q plot of residuals\nimport pylab\nimport scipy.stats as stats\n\nstats.probplot(residuals, dist=\"norm\", plot=pylab)\nplt.show()","1556bbef":"# RMSE\ndef rmse(predictions, targets):\n    return np.sqrt(((targets - predictions) ** 2).mean())\n\n\n# MAPE\ndef mape(predictions, targets):\n    return np.mean(np.abs((targets - predictions)) \/ targets) * 100\n\n\n# MAE\ndef mae(predictions, targets):\n    return np.mean(np.abs((targets - predictions)))\n\n\n# Model Performance on test and train data\ndef model_pref(olsmodel, x_train, x_test):\n\n    # Insample Prediction\n    y_pred_train = olsmodel.predict(x_train)\n    y_observed_train = y_train\n\n    # Prediction on test data\n    y_pred_test = olsmodel.predict(x_test)\n    y_observed_test = y_test\n\n    print(\n        pd.DataFrame(\n            {\n                \"Data\": [\"Train\", \"Test\"],\n                \"RMSE\": [\n                    rmse(y_pred_train, y_observed_train),\n                    rmse(y_pred_test, y_observed_test),\n                ],\n                \"MAE\": [\n                    mae(y_pred_train, y_observed_train),\n                    mae(y_pred_test, y_observed_test),\n                ],\n                \"MAPE\": [\n                    mape(y_pred_train, y_observed_train),\n                    mape(y_pred_test, y_observed_test),\n                ],\n            }\n        )\n    )\n\n\n# Checking model performance\nmodel_pref(model2, X_train, X_test)","096443df":"# import the required function\n\nfrom sklearn.model_selection import cross_val_score\n\n# build the regression model and \nlinearregression = LinearRegression()                                    \n\ncv_Score11 = cross_val_score(linearregression, X_train, y_train, cv = 10)\ncv_Score12 = cross_val_score(linearregression, X_train, y_train, cv = 10, scoring = 'neg_mean_squared_error')                                \n\n\nprint(\"RSquared: %0.3f (+\/- %0.3f)\" % (cv_Score11.mean(), cv_Score11.std() * 2))\nprint(\"Mean Squared Error: %0.3f (+\/- %0.3f)\" % (-1*cv_Score12.mean(), cv_Score12.std() * 2))","d507fb80":"coef = pd.Series(index = X_train.columns, data = model2.params.values)\n\ncoef_df = pd.DataFrame(data = {'Coefs': model2.params.values }, index =  X_train.columns)\ncoef_df","6c3a8758":"# Let us write the equation of the fit\nEquation = \"log (Price) =\"\nprint(Equation, end='\\t')\nfor i in range(len(coef)):\n    print('(', coef[i], ') * ', coef.index[i], '+', end = ' ')","9bf5406a":"#### Apply cross validation to improve the model and evaluate it using different evaluation metrics","2fd2f501":"**Observations:____**\n- In general, the low errors above give a positive impression about the model's accuracy.\n- We can see that the errors have increased slightly on the test data. This suggested further investigation to improve the performance on general data.","a95f861b":"---------------------------\n## Bivariate Analysis\n---------------------------","1a7d3132":"### summary statistics","16781862":"* We can see that the **R-squared value has decreased by 0.002**, since we have removed variables from the model, whereas the **adjusted R-squared value has increased by 0.001**, since we removed statistically insignificant variables only.","23eacf63":"- The distance of the houses to the Boston employment centers appears to decrease moderately as the the proportion of the old houses increase in the town. It is possible that the Boston employment centers are located in the established towns where proportion of owner-occupied units built prior to 1940 is comparatively high.\n\n- This, along with the previous observations, support the assumption that old owner-occupied houses are closer to employeement centers and employeement centers are emitting oxides.","d017241c":"- The R-squared on the cross validation is 0.729, whereas on the training dataset it was 0.769\n- And the MSE on cross validation is 0.041, whereas on the training dataset it was 0.038","5da049b3":"Now, we will check the linear regression assumptions.","7655b01a":"### Examining the significance of the model\n\nIt is not enough to fit a multiple regression model to the data, it is necessary to check whether all the regression coefficients are significant or not. Significance here means whether the population regression parameters are significantly different from zero. \n\nFrom the above it may be noted that the regression coefficients corresponding to ZN, AGE, and INDUS are not statistically significant at level \u03b1 = 0.05. In other words, the regression coefficients corresponding to these three are not significantly different from 0 in the population. Hence, we will eliminate the three features and create a new model.","901c4fdd":"#### Let's check the correlation using the heatmap ","2105e70e":"### Get model Coefficients in a pandas dataframe with column 'Feature' having all the features and column 'Coefs' with all the corresponding Coefs. Write the regression equation.","b6821573":"### Check for Multicollinearity\n\nWe will use the Variance Inflation Factor (VIF), to check if there is multicollinearity in the data.\n\nFeatures having a VIF score > 5 will be dropped\/treated till all the features have a VIF score < 5","31db0d41":"**Observations:____** It appears that there is no pattern in this correlation between the two features.","c07fa938":"### Conclusions and business recommendations derived from the model","11348560":"Before creating the linear regression model, it is important to check the bivariate relationship between the variables. Let's check the same using the heatmap and scatterplot.","7c81fde1":"* There are two variables with a high VIF - RAD and TAX. Let's remove TAX as it has the highest VIF values and check the multicollinearity again.","3f13c304":"**Observations:_____**\n- R square and adjusted R square values are large which gives a good level of confidence about the model.\n- Independent variables (ZN, AGE, and INDUS) have a high p-value and low t, which implies a minimum significance.","a169c3a4":"Let's check the correlation after removing the outliers.","b251ed7c":"Next, we will check the multicollinearity in the train dataset.","588e3ba9":"### Get information about the dataset using the info() method","047aa3cc":"## Problem Statement\n---------------------------\n\nThe problem on hand is to predict the housing prices of a town or a suburb based on the features of the locality provided to us. In the process, we need to identify the most important features in the dataset. We need to employ techniques of data preprocessing and build a linear regression model that predicts the prices for us. \n\n----------------------------\n## Data Information\n---------------------------\n\nEach record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. Detailed attribute information can be found below-\n\nAttribute Information (in order):\n- **CRIM:**     per capita crime rate by town\n- **ZN:**       proportion of residential land zoned for lots over 25,000 sq.ft.\n- **INDUS:**    proportion of non-retail business acres per town\n- **CHAS:**     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n- **NOX:**      nitric oxides concentration (parts per 10 million)\n- **RM:**       average number of rooms per dwelling\n- **AGE:**     proportion of owner-occupied units built prior to 1940\n- **DIS:**      weighted distances to five Boston employment centres\n- **RAD:**      index of accessibility to radial highways\n- **TAX:**      full-value property-tax rate per 10,000 dollars\n- **PTRATIO:**  pupil-teacher ratio by town\n- **B1000:**  (Bk - 0.63)^2 where Bk is the proportion of blacks by town\n- **LSTAT:**    %lower status of the population\n- **MEDV:**     Median value of owner-occupied homes in 1000 dollars.\n\n--------------------------------------------","a8b5a7b7":"### Visualizing the relationship between the features having significant correlations (> 0.7) ","b5ed8bcc":"- We started by exploring the data and performing EDA including univariate and bivariate analysis.\n- We checked for Multicollinearity which is found between RAD and TAX, based on that TAX was droped.\n- We analyzed and drop insignificant variables (ZN, AGE, and INDUS).\n- Then we checked the linear regression assumptions which held true.\n- Then built the model, checked its performance and applied cross validation and concluded results.\n- At least 50% of Boston towns have no zoned lands for large lot and most of the houses are not on Charles riverside.\n- Most of the old owner-occupied houses are located closer employeement center where oxide concentration is high.\n- The house prices increase as the rooms increases.","4d5737db":"* The tax rate appears to increase with an increase in the proportion of non-retail business acres per town. This might be due to the reason that the variables TAX and INDUS are related with a third variable.","6d8a8d1e":"* The price of the house seems to increase as the value of RM increases. This is expected as the price is generally higher for more rooms.\n\n* There are a few outliers in a horizotal line as the MEDV value seems to be capped at 50.","2b07bcc1":"We may want to reiterate the model building process again with new features or better feature engineering to increase the R-squared and decrease the MSE on cross validation.","4dbc7dea":"Now, we will visualize the relationship between the pairs of features having significant correlations.","a6af9e10":"* The price of the house indicated by the variable MEDV is the target variable and the rest are the independent variables based on which we will predict house price.","8f08b5ef":"---------------------------\n## Univariate Analysis\n---------------------------","c3c9dea3":"Now, we will create the linear regression model as the VIF is less than 5 for all the independent variables, and we can assume that multicollinearity has been removed between the variables.","a552b55e":"**Observations:____**\n\n- Crime rates average is 3.6 with very low crime rates in 50% of towns and extreme high rates in other towns.\n- At least 50% of Boston towns have no zoned lands for large lot.\n- The mean of CHAS is 0.07, which means that most of the houses are not on riverside.","e52f88a4":"### Read the dataset","a33026f2":"Before performing the modeling, it is important to check the univariate distribution of the variables.","bc8e23f2":"**Observations:____**\n- The more proportion of owner-occupied units built prior to 1940 exist, the more oxides concentration. This implies that old owner-occupied houses are located in a geographical location closer to the oxide source than more recently-built houses.","09eeea03":"### Check the distribution of the variables","20906110":"#### Check for mean residuals","31bb2f07":"* The price of the house tends to decrease with an increase in LSTAT. This is also possible as the house price is lower in areas where lower status people live.\n\n* There are few outliers and the data seems to be capped at 50.","015d8670":"# Linear Regression: Boston House Price Prediction\n","9b2084fc":"### Let us start by importing the required libraries","fdd00fc2":"**Observations:____** P-value is greater than 0.05, so we fail to reject the null hypothesis. Meaning, the residuals have heteroscedasticity.","3306ecc8":"### Check the performance of the model on the train and test data set","114cf960":"* There are a total of 506 non-null observations in each of the columns. This indicates that there are no missing values in the data.\n\n* Every column in this dataset is numeric in nature.","4d2c3014":"**Observations:___**\n- The distant the house is from employeement centers, the less oxide concentration is there. This implies that empolyeement centers location is where the oxid source lies","159f870b":"### Split the dataset\nLet's split the data into the dependent and independent variables and further split it into train and test set in a ratio of 70:30 for train and test set.","46fc4036":"The correlation between RAD and TAX is very high. But, no trend is visible between the two variables. \nThis might be due to outliers. ","e5320e50":"#### Linearity of variables\n\nIt states that the predictor variables must have a linear relation with the dependent variable.\n\nTo test the assumption, we'll plot residuals and fitted values on a plot and ensure that residuals do not form a strong pattern. They should be randomly and uniformly scattered on the x-axis.","33fbeb9c":"As the dependent variable is sightly skewed, we will apply a **log transformation on the 'MEDV' column** and check the distribution of the transformed column.","c9d1ad75":"**Observations:_____** We can see normality of distribution.","c7fab9c7":"### Check the below linear regression assumptions\n\n1. **Mean of residuals should be 0**\n2. **No Heteroscedasticity**\n3. **Linearity of variables**\n4. **Normality of error terms**","2a937884":"So the high correlation between TAX and RAD is due to the outliers. The tax rate for some properties might be higher due to some other reason.","d86703ee":"### Drop the column 'TAX' from the training data and check if multicollinearity is removed","22c59c62":"The log-transformed variable (**MEDV_log**) appears to have a **nearly normal distribution without skew**, and hence we can proceeed.","63157ff4":"### Create the linear regression model using statsmodels OLS and print the model summary.","0e460a56":"**Observations:______**\n\n- Proportion of non-retail business acres is positively correlated with oxide concentration. This implies town with high non-retail business areas have higher acids emissions. Could be a causation as well.\n- Also, proportion of non-retail business acres is positively correlated with taxes. This implies that higher taxes are imposed on houses in town with high non-retail business areas.\n- Number of rooms is positively correlated with house value, which makes sense.\n- Oxide concentration is positively correlated with old owner-occupied buildings. This could transitively imply that old houses are more centered around non-retail business areas (which is positively correlated with oxid emissions).\n- Distance from employeement centers is negatively correlated with propotion of non-retail business acres, oxide concentration, and proportion of owner-occupied buildings. This says towns away from employeement centers have more recent houses and less oxide concentration.\n- Prices of houses is negatively correlated with low status of population which makes much sense.","33e4f706":"### Drop insignificant variables from the above model and create the regression model again","27141865":"\n\n* Homoscedasticity - If the residuals are symmetrically distributed across the regression line, then the data is said to homoscedastic.\n\n* Heteroscedasticity- - If the residuals are not symmetrically distributed across the regression line, then the data is said to be heteroscedastic. In this case, the residuals can form a funnel shape or any other non-symmetrical shape.\n\n* We'll use `Goldfeldquandt Test` to test the following hypothesis with alpha = 0.05:\n\n    - Null hypothesis: Residuals are homoscedastic\n    - Alternate hypothesis: Residuals have heteroscedasticity","86820b60":"We have seen that the variables LSTAT and RM have a linear relationship with the dependent variable MEDV. Also, there are significant relationships among a few independent variables, which is not desirable for a linear regression model. Let's first split the dataset.","c55a1fc2":"**Observatioins:_____** We can see that residuals are randomly and uniformly scattered.","2a1fd7f4":"#### Normality of error terms\nThe residuals should be normally distributed.","95cb2a2a":"**Observations:____** Mean of residuals is close to zero, which supports the linear regression assumption.","925ebc3c":"* **The variables CRIM and ZN are positively skewed.** This suggests that most of the areas have lower crime rates and most residential plots are under the area of 25,000 sq. ft.\n* **The variable CHAS, with only 2 possible values 0 and 1, follows a binomial distribution**, and the majority of the houses are away from Charles river (CHAS = 0).\n* The distribution of the variable AGE suggests that many of the owner-occupied houses were built before 1940. \n* **The variable DIS** (average distances to five Boston employment centers) **has a nearly exponential distribution**, which indicates that most of the houses are closer to these employment centers.\n* **The variables TAX and RAD have a bimodal distribution.**, indicating that the tax rate is possibly higher for some properties which have a high index of accessibility to radial highways.  \n* The dependent variable MEDV seems to be slightly right skewed.","eaf90713":"#### Check for homoscedasticity","520341ef":"**Model Equation:** log (Price) = ( 4.649385823266652 ) const + ( -0.012500455079103941 ) CRIM + ( 0.11977319077019594 ) CHAS + ( -1.0562253516683235 ) NOX + ( 0.058906575109279144 ) RM + ( -0.044068890799406124 ) DIS + ( 0.007848474606244051 ) RAD + ( -0.048503620794999036 ) PTRATIO + ( -0.029277040479797338 ) * LSTAT\n\n* From the equation we can derive that, the prices go high for houses located near Charles river and housed with many rooms.\n* The prices are negatively affected mostly by the oxide concentration which is mostly high in employeement centers. Crime rates and residents profile also negatively affects prices.","82ad8274":"* No trend between the two variables is visible in the above plot."}}