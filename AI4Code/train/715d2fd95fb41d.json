{"cell_type":{"49eb1ff9":"code","87dd9ac8":"code","109475ac":"code","bcd0022b":"code","8209d50f":"code","cce746b9":"code","7c2ef0c3":"code","033860bc":"code","efd0f2e4":"code","eaae0f51":"code","4b29a63d":"code","eb838b8b":"code","9dd6644c":"code","fd65a58b":"code","0e068b9c":"code","c69450f7":"code","12c75f31":"code","b744d56d":"code","dc05693d":"code","d539eb25":"code","5c2a1572":"code","13c0ef17":"code","52b1c270":"code","280d24fc":"code","ba117862":"code","df7793b3":"markdown","1794ab58":"markdown","fca153f8":"markdown","e681166a":"markdown","5b3fa357":"markdown","5a36b55c":"markdown"},"source":{"49eb1ff9":"#Genel komutlar\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas import read_csv\nfrom sklearn.preprocessing import MinMaxScaler\nimport os\nimport math\n#RMSE ile tahmin hatalar\u0131m\u0131 belirlemek i\u00e7in sqrt \u00e7a\u011f\u0131rd\u0131m.(evaluate forecast)\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\n#Verisetini ay\u0131klamak i\u00e7in \u00e7a\u011f\u0131rd\u0131m\nfrom numpy import split\nfrom numpy import array\n#TimeSerieslerde kullan\u0131lan k\u00fct\u00fcphaneler\nfrom datetime import timedelta\n#Tensorflow k\u00fct\u00fcphaneleri\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import Sequence\n","87dd9ac8":"path = \"..\/input\/solar-radiation-dataset\/solar_angles_dataset.csv\"\ndf = pd.read_csv(path)\ndf.head()","109475ac":"df.info()","bcd0022b":"df=df.drop(['Julian day',\"Top. azimuth angle (eastward from N)\",\"Topocentric zenith angle\"], axis = 1) \ndf=df","8209d50f":"cols = [\"Date (M\/D\/YYYY)\",\"Time (H:MM:SS)\"]\ndf[\"date_time\"] = df[cols].apply(lambda row: \"\".join(row.values.astype(str)), axis=1)","cce746b9":"df['date_time'] = pd.to_datetime(df['date_time'], format='%m\/%d\/%Y%H:%M:%S')","7c2ef0c3":"df=df.drop([\"Date (M\/D\/YYYY)\",\"Time (H:MM:SS)\" ,\"Unnamed: 0\"], axis = 1)","033860bc":"# Split into training, validation and test datasets.\n# Since it's timeseries we should do it by date.\ntest_cutoff_date = df['date_time'].max() - timedelta(days=1)\nval_cutoff_date = test_cutoff_date - timedelta(days=14)\n\ndf_test = df[df['date_time'] > test_cutoff_date]\ndf_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\ndf_train = df[df['date_time'] <= val_cutoff_date]\n\n#check out the datasets\nprint('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max()))\nprint('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max()))\nprint('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max()))","efd0f2e4":"df_test=df_test.set_index('date_time') #Columnu index yapmak i\u00e7in\ndf_val=df_val.set_index('date_time') #Columnu index yapmak i\u00e7in\ndf_train=df_train.set_index('date_time') #Columnu index yapmak i\u00e7in","eaae0f51":"df_train.plot(figsize=(16,8))","4b29a63d":"len(df_train)","eb838b8b":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(df_train)","9dd6644c":"scaled_train=scaler.transform(df_train)\nscaled_test =scaler.transform(df_test)\nscaled_val=scaler.transform(df_val)","fd65a58b":"from keras.preprocessing.sequence import TimeseriesGenerator","0e068b9c":"n_input =4\nn_features =1\ntrain_generator = TimeseriesGenerator(scaled_train,scaled_train,length=n_input,batch_size = 1)\n","c69450f7":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","12c75f31":"model =Sequential()\nmodel.add(LSTM(64,activation =\"relu\",input_shape=(n_input,n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer =\"adam\",loss=\"mse\")\nmodel.summary()\n","b744d56d":"model.fit_generator(train_generator,epochs=10)","dc05693d":"model.history.history.keys()  ","d539eb25":"myloss = model.history.history[\"loss\"]\nplt.plot(range(len(myloss)),myloss)","5c2a1572":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-n_input:]\ncurrent_batch = first_eval_batch.reshape((1, n_input, n_features))\n\nfor i in range(len(df_test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","13c0ef17":"true_predictions = scaler.inverse_transform(test_predictions)","52b1c270":"true_predictions","280d24fc":"df_test['Predictions'] = true_predictions","ba117862":"df_test.plot(figsize=(20,8))","df7793b3":"-Zaman s\u00fctunlar\u0131n\u0131 birle\u015ftirip datetime'a \u00e7evirdim, sonra di\u011fer s\u00fctunlar\u0131 sildim.\n\n-Datetime s\u00fctununu index yapt\u0131m .","1794ab58":"Verisetini olu\u015fturuken olu\u015fan bo\u015f s\u00fctunu sildim","fca153f8":"Forecast Visualisation","e681166a":"\u00d6\u011frenme setini \u00e7a\u011f\u0131rd\u0131m.Verisetimi google drive'a kaydedip \u00e7a\u011f\u0131rd\u0131m.\n\n","5b3fa357":"Gerekli k\u00fct\u00fcphaneleri \u00e7a\u011f\u0131ral\u0131m ","5a36b55c":"1-VER\u0130 HAZIRLAMA A\u015eAMASI\n"}}