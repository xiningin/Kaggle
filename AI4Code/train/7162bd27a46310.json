{"cell_type":{"5057c1c6":"code","a269efda":"code","00fc9cd6":"code","4d1e33cb":"code","10226323":"code","db3737cf":"code","4c253572":"code","cd590494":"code","c00777b2":"code","acaf8f94":"code","3a2d247b":"code","840c5e10":"code","298600b6":"code","ad62a232":"code","74fba22b":"code","8343d902":"code","9e5a7b97":"code","9dc94ac3":"code","fb6d2fc0":"code","35e657f6":"code","a1c4c1d7":"code","135d2e84":"code","bb2bbe9a":"code","9bca04cb":"code","d87dd000":"code","9a2bcfad":"code","fcaeff09":"code","18584f86":"code","1afd2e40":"code","3ce9c48b":"code","b781f4ec":"code","0ac7c9a7":"code","dc451f1c":"code","7e6dada2":"code","374ba776":"code","8d572612":"code","70c5c8e1":"code","c1f57be3":"code","bab503e2":"code","ca4711a8":"code","c1ca5c0d":"code","b073daad":"code","44c2ea2e":"code","94ff2df3":"code","bda5a189":"code","0910c7c4":"code","ba364db0":"code","ec4c7ba8":"code","b1b539d0":"code","ded780e3":"code","65b2f781":"code","788e94b5":"code","9ac1e67f":"code","1c6dcb05":"code","146da005":"code","3e454144":"code","0b27bdce":"code","3e117536":"code","85b5477f":"code","bc78d46d":"code","8460e0dc":"code","068bbe30":"code","fba56d67":"code","9176fd00":"markdown","ba0310f4":"markdown"},"source":{"5057c1c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a269efda":"description = pd.read_csv('\/kaggle\/input\/ds-nigeria-2019-challenge-insurance-prediction\/VariableDescription.csv')\ndescription","00fc9cd6":"train = pd.read_csv('\/kaggle\/input\/ds-nigeria-2019-challenge-insurance-prediction\/train_data.csv')\ntest = pd.read_csv('\/kaggle\/input\/ds-nigeria-2019-challenge-insurance-prediction\/test_data.csv')","4d1e33cb":"train.info()","10226323":"test.info()","db3737cf":"train.head()","4c253572":"train['Date_of_Occupancy'] = train['Date_of_Occupancy'].fillna(train['Date_of_Occupancy'].median())\ntrain['age_on_observation'] = train['YearOfObservation'] - train['Date_of_Occupancy']\ntrain = train.drop(['Customer Id', 'Geo_Code', 'YearOfObservation', 'Date_of_Occupancy', 'NumberOfWindows'], axis=1)\ntrain = pd.get_dummies(train, columns=['Building_Painted','Building_Fenced','Garden','Settlement'])\ntrain.dropna(inplace=True)\n\ntest['Date_of_Occupancy'] = test['Date_of_Occupancy'].fillna(test['Date_of_Occupancy'].median())\ntest['Building Dimension'] = test['Building Dimension'].fillna(test['Building Dimension'].median())\ntest['Garden'] = test['Garden'].fillna(test['Garden'].mode())\ntest['age_on_observation'] = test['YearOfObservation'] - test['Date_of_Occupancy']\ntest = test.drop(['Geo_Code', 'YearOfObservation', 'Date_of_Occupancy', 'NumberOfWindows'], axis=1)\ntest = pd.get_dummies(test, columns=['Building_Painted','Building_Fenced','Garden','Settlement'])\ntest.dropna(inplace=True)\n\ntrain.head()","cd590494":"import seaborn as sns\nsns.heatmap(train.corr())","c00777b2":"# \u0420\u0430\u0437\u043e\u0431\u044c\u0435\u043c \u043d\u0430 train, valid\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(train.drop(['Claim'], axis=1), train['Claim'].copy(), test_size=0.25, random_state=2021)","acaf8f94":"# \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0441 \u043c\u043e\u0434\u0435\u043b\u0438 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439\u0440\u0435\u0433\u0440\u0435\u0441\u0438\u0438\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\ny_pred = LR.predict(X_valid)","3a2d247b":"from sklearn.metrics import precision_score, recall_score, f1_score\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))\nprint('F1 score:', f1_score(y_valid, y_pred))","840c5e10":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nprint(confusion_matrix(y_valid, y_pred))","298600b6":"plot_confusion_matrix(LR, X_valid, y_valid, values_format='5g')","ad62a232":"# \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043a\u0440\u043e\u0441\u0441\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e\nfrom sklearn.model_selection import GridSearchCV\n\nlog_reg = LogisticRegression(solver='liblinear')\n\nC_values = {'C': np.logspace(-3, 3, 10)}\nlogreg_grid = GridSearchCV(log_reg, C_values, cv=5, scoring='f1')\nlogreg_grid.fit(X_train, y_train)","74fba22b":"print(logreg_grid.best_params_)\nprint(logreg_grid.best_score_)","8343d902":"import matplotlib.pyplot as plt\nresults_df = pd.DataFrame(logreg_grid.cv_results_)\nplt.plot(results_df['param_C'], results_df['mean_test_score'])\n\n# \u041f\u043e\u0434\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0441\u0438 \u0438 \u0433\u0440\u0430\u0444\u0438\u043a\nplt.xlabel('C')\nplt.ylabel('Test accuracy')\nplt.title('Validation curve')\nplt.show()","9e5a7b97":"# \u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0432\u043e\u0437\u044c\u043c\u0435\u043c F1 score valid\nlog_reg = LogisticRegression(solver='liblinear', penalty='l1')\n\nC_values = {'C': np.logspace(-3, 3, 10)}\nlogreg_grid = GridSearchCV(log_reg, C_values, cv=5, scoring='f1')\nlogreg_grid.fit(X_train, y_train)","9dc94ac3":"print(logreg_grid.best_params_)\nprint(logreg_grid.best_score_)","fb6d2fc0":"results_df = pd.DataFrame(logreg_grid.cv_results_)\nplt.plot(results_df['param_C'], results_df['mean_test_score'])\n\n# \u041f\u043e\u0434\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0441\u0438 \u0438 \u0433\u0440\u0430\u0444\u0438\u043a\nplt.xlabel('C')\nplt.ylabel('Test accuracy')\nplt.title('Validation curve')\nplt.show()","35e657f6":"y_pred = logreg_grid.best_estimator_.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))","a1c4c1d7":"# \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c kNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))","135d2e84":"# \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043a\u0440\u043e\u0441\u0441\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u043e\u0439 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043a\u043e\u043b-\u0432\u0430 \u0441\u043e\u0441\u0435\u0434\u0435\u0439 \u0434\u043e 50\nknn_params = {'n_neighbors': np.arange(1, 50, 2)}\nknn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='f1')\nknn_grid.fit(X_train, y_train)\n\ny_pred = knn_grid.best_estimator_.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))","bb2bbe9a":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=300)\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))","9bca04cb":"# \u0418\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043a\u043b\u0430\u0441\u0441\u0430 1\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X_train, y_train)","d87dd000":"# \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0431\u0430\u043b\u0430\u043d\u0441\ny_ros.value_counts()","9a2bcfad":"# \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u0441 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435\u043c \u043a\u043b\u0430\u0441\u0441\u0430 1\nlogreg_ros = LogisticRegression(solver='liblinear')\nlogreg_ros.fit(X_ros, y_ros)\ny_pred = logreg_ros.predict(X_valid)\n\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))","fcaeff09":"plot_confusion_matrix(logreg_ros, X_valid, y_valid, values_format='5g')","18584f86":"# \u041f\u043e\u0434\u0431\u043e\u0440 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432\nlogreg_params = {'C': np.logspace(-3, 3, 10), 'penalty': ['l2', 'l1']}\nlogreg_grid = GridSearchCV(logreg_ros, logreg_params, cv=5, scoring='f1')\nlogreg_grid.fit(X_ros, y_ros)\n\ny_pred = logreg_grid.best_estimator_.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1 score valid:', f1_score(y_valid, y_pred))\nprint('Precision:', precision_score(y_valid, y_pred))\nprint('Recall:', recall_score(y_valid, y_pred))","1afd2e40":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import RandomOverSampler\nimport matplotlib.pyplot as plt\n\nnp.random.seed(1)","3ce9c48b":"def print_results(y_true, y_pred):\n    print(confusion_matrix(y_true, y_pred))\n    print('F1-score:', f1_score(y_true, y_pred))","b781f4ec":"def plot_validation_curve(model_grid, param_name, params=None):\n    # \u0420\u0438\u0441\u0443\u0435\u043c \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u043a\u0440\u0438\u0432\u0443\u044e\n    # \u041f\u043e \u043e\u0441\u0438 \u0445 --- \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 (param_***)\n    # \u041f\u043e \u043e\u0441\u0438 y --- \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 (mean_test_score)\n\n    results_df = pd.DataFrame(model_grid.cv_results_)\n    \n    if params == None:\n        plt.plot(results_df['param_'+param_name], results_df['mean_test_score'])\n    else:\n        plt.plot(params, results_df['mean_test_score'])\n\n    # \u041f\u043e\u0434\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0441\u0438 \u0438 \u0433\u0440\u0430\u0444\u0438\u043a\n    plt.xlabel(param_name)\n    plt.ylabel('Test F1 score')\n    plt.title('Validation curve')\n    plt.show()","0ac7c9a7":"# \u041c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)","dc451f1c":"mlp = MLPClassifier(solver='lbfgs')\nmlp.fit(X_train, y_train)\n\ny_pred = mlp.predict(X_valid)\nprint_results(y_valid, y_pred) # log_reg: ~0.2","7e6dada2":"mlp_2 = MLPClassifier(hidden_layer_sizes=(200,), solver='lbfgs', max_iter=400, alpha=0.1)\nmlp_2.fit(X_train, y_train)\n\ny_pred = mlp.predict(X_valid)\nprint_results(y_pred, y_valid)","374ba776":"ros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X_train, y_train)","8d572612":"mlp_3 = MLPClassifier(hidden_layer_sizes=(200,), solver='lbfgs', max_iter=400, alpha=0.1)\nmlp_3.fit(X_ros, y_ros)\n\ny_pred = mlp_3.predict(X_valid)\nprint_results(y_pred, y_valid)","70c5c8e1":"mlp_4 = MLPClassifier(hidden_layer_sizes=(100, 50, 20), solver='lbfgs', alpha=0.001)\nmlp_4.fit(X_ros, y_ros)\n\ny_pred = mlp_4.predict(X_valid)\nprint(confusion_matrix(y_valid, y_pred))\nprint('F1-score:', f1_score(y_valid, y_pred))","c1f57be3":"scaler = StandardScaler()\nmlp = MLPClassifier(solver='lbfgs')\nmodel = Pipeline([('scaler', scaler), ('mlp', mlp)])\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(columns=['Claim']), train.Claim, test_size=0.25)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint('F1-score:', f1_score(y_test, y_pred))","bab503e2":"param_grid = {'mlp__alpha': np.logspace(-4, 4, 10)}\nmodel_grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', \n                          n_jobs=-1) # n_jobs=-1 \u0437\u0430\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432\nmodel_grid.fit(X_train, y_train)","ca4711a8":"plot_validation_curve(model_grid, 'mlp__alpha')","c1ca5c0d":"print('Best (hyper)parameters:', model_grid.best_params_)\nprint('Best score:', model_grid.best_score_)","b073daad":"y_pred = model_grid.best_estimator_.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint('F1-score:', f1_score(y_test, y_pred))","44c2ea2e":"X_ros, y_ros = ros.fit_sample(X_train, y_train)\nmodel_grid.fit(X_ros, y_ros)","94ff2df3":"print('Best (hyper)parameters:', model_grid.best_params_)\nprint('Best score:', model_grid.best_score_)","bda5a189":"y_pred = model_grid.best_estimator_.predict(X_test)\nprint_results(y_test, y_pred)","0910c7c4":"end = test.drop(['Customer Id'], axis=1)","ba364db0":"test['Claim'] = model_grid.best_estimator_.predict(end)","ec4c7ba8":"test[['Customer Id', 'Claim']].to_csv('completed.csv')","b1b539d0":"param_grid = {'mlp__activation': ['logistic', 'tanh', 'relu']}\nmodel_grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', \n                          n_jobs=-1) # n_jobs=-1 \u0437\u0430\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432\nmodel_grid.fit(X_train, y_train)","ded780e3":"plot_validation_curve(model_grid, 'mlp__activation')","65b2f781":"print('Best (hyper)parameters:', model_grid.best_params_)\nprint('Best score:', model_grid.best_score_)","788e94b5":"y_pred = model_grid.best_estimator_.predict(X_test)\nprint_results(y_test, y_pred)","9ac1e67f":"param_grid = {'mlp__hidden_layer_sizes': [(i, ) for i in range(200, 1500, 200)]}\nmodel_grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', \n                          n_jobs=-1) # n_jobs=-1 \u0437\u0430\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432\nmodel_grid.fit(X_train, y_train)","1c6dcb05":"plot_validation_curve(model_grid, 'mlp__hidden_layer_sizes', \n                      [i for i in range(200, 1500, 200)])","146da005":"print('Best (hyper)parameters:', model_grid.best_params_)\nprint('Best score:', model_grid.best_score_)","3e454144":"y_pred = model_grid.best_estimator_.predict(X_test)\nprint_results(y_test, y_pred)","0b27bdce":"param_grid = {'mlp__warm_start': [True, False]}\nmodel_grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', \n                          n_jobs=-1) # n_jobs=-1 \u0437\u0430\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432\nmodel_grid.fit(X_train, y_train)","3e117536":"print('Best (hyper)parameters:', model_grid.best_params_)\nprint('Best score:', model_grid.best_score_)","85b5477f":"y_pred = model_grid.best_estimator_.predict(X_test)\nprint_results(y_test, y_pred)","bc78d46d":"hidden = [(100,), (100, 50), (100, 50, 20), (50, 50), (50, 50, 50), (50, 30, 30, 20)]\nparam_grid = {'mlp__hidden_layer_sizes': hidden}\nmodel_grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', \n                          n_jobs=-1) # n_jobs=-1 \u0437\u0430\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u0435\u0442 \u0431\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u043e\u0432\nmodel_grid.fit(X_train, y_train)","8460e0dc":"plot_validation_curve(model_grid, 'mlp__hidden_layer_sizes', \n                      [str(x) for x in hidden])","068bbe30":"print('Best (hyper)parameters:', model_grid.best_params_)\nprint('Best score:', model_grid.best_score_)","fba56d67":"y_pred = model_grid.best_estimator_.predict(X_test)\nprint_results(y_test, y_pred)","9176fd00":"\u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u0447\u0438\u0441\u0442\u043a\u0443 \u043d\u0430\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445:\n- \u0443\u0434\u0430\u043b\u0438\u043c \u0438\u0434\u0435\u043d\u0442\u0435\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0438 Geo_Code\n- \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u043c \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u044e\u0449\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432 Date_of_Occupancy\n- \u043f\u043e\u043c\u0435\u043d\u044f\u0435\u043c \u0433\u043e\u0434 \u0437\u0430\u0441\u0435\u043b\u0435\u043d\u0438\u044f \u0438 \u0433\u043e\u0434 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0437\u0430 \u0437\u0430\u0441\u0442\u0440\u0430\u0445\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u043f\u043e\u043b\u0438\u0441\u043e\u043c \u043d\u0430 \u0432\u043e\u0437\u0440\u0430\u0441\u0442 \u0437\u0434\u0430\u043d\u0438\u044f\n- Building_Painted, Building_Fenced, Garden, Settlement \u0440\u0430\u0437\u043e\u0431\u044c\u0435\u043c \u043d\u0430 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e get_dummies()\n- \u0443\u0434\u0430\u043b\u0438\u043c \u043f\u043e\u043b\u0435 NumberOfWindows, \u0442\u0430\u043a \u043a\u0430\u043a \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u043d\u043e\u0433\u043e \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n- \u0434\u0440\u043e\u0431\u043d\u0435\u043c \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u044e\u0449\u0438\u0435 \u0437\u043d\u0430-\u044f, \u0442\u0430\u043a \u043a\u0430\u043a \u0438\u0445 \u043d\u0435\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b-\u0432\u043e","ba0310f4":"## \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043d\u0430\u043f\u043e\u0441\u043b\u0435\u0434\u043e\u043a \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0435 \u0441\u0435\u0442\u0438\n### \u042d\u0442\u0443 \u0447\u0430\u0441\u0442\u044c \u044f \u0434\u043e\u0431\u0430\u0432\u0438\u043b \u0443\u0436\u0435 \u043f\u043e\u0441\u043b\u0435 \u0442\u043e\u0433\u043e, \u043a\u0430\u043a \u0441\u0434\u0430\u043b \u0440\u0430\u0431\u043e\u0442\u0443\n#### \u041b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432\u044b\u0434\u0430\u043b\u0430 \u043b\u043e\u0433. \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f \u043f\u043e\u0441\u043b\u0435 \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u043a\u043b\u0430\u0441\u0441\u0430 1, \u0445\u043e\u0442\u044f \u0443 \u043d\u0435\u0439\u0440\u043e\u043d\u043a\u0438 F1 \u0431\u044b\u043b \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0432\u044b\u0448\u0435\n\u041d\u0430 \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u0437\u0430\u043d\u044f\u043b 175 \u043c\u0435\u0441\u0442\u043e"}}