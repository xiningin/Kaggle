{"cell_type":{"48b2aba2":"code","579bd17e":"code","763f6370":"code","9bcf9ba1":"code","72da5785":"code","11dba37e":"code","63ac7e7e":"code","42eb1d1a":"code","852f0008":"code","bce2bb35":"code","8759dff6":"code","19ea2365":"code","5ec5e47e":"code","fa286e95":"code","c6132b4e":"code","0e8378a4":"code","ad18c2f3":"markdown","4bc9aa6d":"markdown","88a68d78":"markdown","4e48e58e":"markdown","d5f8b5ad":"markdown","48d7dc32":"markdown","7ace4ac7":"markdown","e5bc5f11":"markdown"},"source":{"48b2aba2":"#Importing libraries for sentiment analysis \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk #Natural Language Processing Package \nimport os #functions for interacting with the operating system\nimport spacy #Models for NLP\nimport torch #also for NLP\nfrom tqdm.notebook import tqdm \nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\nimport transformers #contains pretrained models to perform tasks on texts\nfrom transformers import BertForSequenceClassification\nfrom wordcloud import WordCloud #For nice wordclouds\nimport tensorflow as tf #Package to develop train models \nfrom tensorflow.keras.preprocessing import text \nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem import WordNetLemmatizer\nimport time #for handling dates and times\nimport re \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.metrics import AUC\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom nltk.tokenize import TweetTokenizer\n","579bd17e":"#Loading and cleaning data\ntrain_data = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin-1')\ntest_data = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin-1')","763f6370":"#Train data \n#preview\ntrain_data.head(5)\n","9bcf9ba1":"#descriptive statistics\ntrain_data.describe()","72da5785":"#Test data\n#preview\ntest_data.head(5)\n\n\n","11dba37e":"#descriptive statistics\ntest_data.describe()","63ac7e7e":"\n\n#Create histogram --> Distribution TEST DATA\n#Can we also make this a function?\nplt.figure(figsize=(12,6)) #specifying the size of the figure\nsns.set_palette(\"Spectral\") #color palette\nsns.countplot(x='Sentiment', data=test_data, order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], )\nplt.xlabel('Sentiment(tag)')\nplt.ylabel('Count of tweets')\nplt.suptitle('Histogram of tweet distribution per sentiment classification (Test data)')\n","42eb1d1a":"#Create histogram --> Distribution TRAIN DATA\nplt.figure(figsize=(12,6))\nsns.set_palette(\"Spectral\")\nsns.countplot(x='Sentiment', data=train_data, order=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'], )\nplt.xlabel('Sentiment (tag)')\nplt.ylabel('Count of tweets')\nplt.suptitle('Histogram of tweet distribution per sentiment classification (Train data)')","852f0008":"#Distribution of tweet counts --> TEST DATA\ntest_data.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='line', figsize=(12, 6))\nplt.title('Tweets on Coronavirus March 2020 (Test data)')\nplt.ylabel('Tweet Count')","bce2bb35":"#Distribution of tweet counts --> TRAIN DATA \ntrain_data.groupby(['TweetAt', 'Sentiment'])['OriginalTweet'].count().unstack().plot(kind='line', figsize=(12, 6))\nplt.title('Tweets on Coronavirus, March 2020, (Train data)')\nplt.ylabel('Tweet Count')","8759dff6":"# Import nltk \/ stopwords\nimport nltk\nnltk.download('stopwords')\n\n# Define stopwords \nstop_words = stopwords.words('english') #defining var to remove stopwords in the process_tweet function \n\n# Define function for cleaning tweets \ndef clean_tweet(tweet):\n    tweet = re.sub(r'http\\S+', ' ', tweet) #removing urls\n    tweet = re.sub(r'<.*?>', ' ', tweet)  # removing html tags    \n    tweet = re.sub(r'\\d+', ' ', tweet) #removing digits\n    tweet = re.sub(r'#\\w+', ' ', tweet)    #removing hashtags\n    tweet = re.sub(r'@\\w+', ' ', tweet) #removing mentions\n    tweet = tweet.split() #removing stop words\n    tweet = \" \".join([word for word in tweet if not word in stop_words])\n    return tweet\n\ntrain_data['CleanTweet'] = train_data['OriginalTweet'].apply(lambda x: clean_tweet(x))\ntrain_data.head(10)\n\n# Clean tweets from test data by creating a new column in the test_data df\ntest_data['CleanTweet'] = test_data['OriginalTweet'].apply(lambda x: clean_tweet(x))\ntest_data.head(10)","19ea2365":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n#Attempt to create wordcloud\ndef wordcloud1(training_data):\n    stopwords = set(STOPWORDS)\n    stopwords.add(\"https\")\n    stopwords.add(\"00A0\")\n    stopwords.add(\"00BD\")\n    stopwords.add(\"00B8\")\n    stopwords.add(\"ed\")\n    wordcloud1 = WordCloud(background_color=\"white\",stopwords=stopwords).generate(\" \".join([i for i in train_data['OriginalTweet'].str.upper()]))\n    plt.imshow(wordcloud1)\n    plt.axis(\"off\")\n    plt.title(\"Most common words, training data\")\n    figsize=(12, 6)\n\nwordcloud1(train_data)  ","5ec5e47e":"#Attempt to create wordcloud - test data\ndef wordcloud2(test_data):\n    stopwords = set(STOPWORDS)\n    stopwords.add(\"https\")\n    stopwords.add(\"00A0\")\n    stopwords.add(\"00BD\")\n    stopwords.add(\"00B8\")\n    stopwords.add(\"ed\")\n    wordcloud2 = WordCloud(background_color=\"white\",stopwords=stopwords).generate(\" \".join([i for i in test_data['OriginalTweet'].str.upper()]))\n    plt.imshow(wordcloud2)\n    plt.axis(\"off\")\n    plt.title(\"Most common words, training data\")\n    figsize=(12, 6)\n\nwordcloud2(test_data)  ","fa286e95":"## tweet tokenizer \n\nimport nltk\nnltk.download('punkt')\n\nfrom nltk.tokenize import TweetTokenizer \n\ncompare_list = train_data['CleanTweet'].head(10)\n\n## need to add code to clean test_data\n\ntweet_tokenizer = TweetTokenizer()\n\ntweet_tokens = []\nfor sent in compare_list:\n    print(tweet_tokenizer.tokenize(sent))\n    tweet_tokens.append(tweet_tokenizer.tokenize(sent))\n","c6132b4e":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer \nimport pandas as pd\n\n#instantiate CountVectorizer() \nvectoriser = CountVectorizer()\n\n#Generate vectors\n\nX_test = vectoriser.transform(test_data[\"CleanTweet\"])\ny_test = encoder.transform(test_data[\"Sentiment\"])\n\n \n# this steps generates word counts for the words in your docs \nXtest_wcount = cv.fit_transform(X_test)\nytest_wcount = cv.fit_transform(y_test)","0e8378a4":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import words\n\ntv = TfidfVectorizer(\n                    ngram_range = (1,3),\n                    sublinear_tf = True,\n                    max_features = 40000)\n\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_data(train_data, train_data['CleanTweet'], random_state=42, test_size=0.3)\n\ntrain_tv= tv.fit_transform(train_data['CleanTweet'])\ntest_tv= tv.fit_transform(test_data['CleanTweet'])\n\ntrain_tfidf = train_tv[:31962,:]\ntest_tfidf = test_tv[31962:,:]\n\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain)\n\nprediction = lreg.predict_proba(xvalid_tfidf)\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)\n\n ","ad18c2f3":"Data pre-processing","4bc9aa6d":"### Tokenization","88a68d78":"### References \n* Matplotlib.org. 2020. Pyplot Tutorial \u2014 Matplotlib 3.3.2 Documentation. [online] Available at: <https:\/\/matplotlib.org\/tutorials\/introductory\/pyplot.html> [Accessed 20 October 2020].\n* Kaggle.com. 2020. Sentiment Prediction. [online] Available at: <https:\/\/www.kaggle.com\/shahraizanwar\/covid19-tweets-sentiment-prediction-rnn-85-acc> [Accessed 18 October 2020].\n* ","4e48e58e":"**TF-IDF Model - Ji Yoon**","d5f8b5ad":"# GRAD-E1326: Python Programming for Data Scientists\n## Ph.D. Hannah B\u00e9chara\n### Ji Yoon Han & Mariana G. Carrillo \n\n**Initial Project report: Tweet Sentiment Analysis**\n","48d7dc32":"### Data cleaning","7ace4ac7":"### Exploratory Data analysis","e5bc5f11":"Loading data"}}