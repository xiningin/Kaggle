{"cell_type":{"64246125":"code","cb2a2c1f":"code","d5f82835":"code","17463017":"code","6b0b8c64":"code","7ab27820":"code","27e00e4f":"code","23c1b449":"code","a5404b82":"markdown","9c7b1a95":"markdown","5f7c57eb":"markdown","1aca3638":"markdown","136e432a":"markdown","f77784d7":"markdown","54794bcf":"markdown"},"source":{"64246125":"!pip install ..\/input\/shopee-external-models\/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ..\/input\/shopee-external-models\/efficientnet-1.1.0-py3-none-any.whl\nimport numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml import PCA\nfrom cuml.neighbors import NearestNeighbors\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tqdm.notebook import tqdm\nimport math\nfrom shutil import copyfile\ncopyfile(src = \"..\/input\/bert-baseline\/tokenization.py\", dst = \"..\/working\/tokenization.py\")\nimport tokenization\nimport tensorflow_hub as hub","cb2a2c1f":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nBATCH_SIZE = 16\nIMAGE_SIZE = [512,512]\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes\nN_CLASSES = 11011","d5f82835":"# RESTRICT TENSORFLOW TO 2GB OF GPU RAM\n# SO THAT WE HAVE 14GB RAM FOR RAPIDS\nLIMIT = 2.0\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_virtual_device_configuration(\n            gpus[0],\n            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\nprint('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\nprint('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))","17463017":"# Flag to get cv score\nGET_CV = True\n# Flag to check ram allocations (debug)\nCHECK_SUB = False\n\ndf = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\ndel df\n\n# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1\n\n# Function to combine predictions\ndef combine_predictions(row):\n    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n    return ' '.join( np.unique(x) )\n\n# Function to read out dataset\ndef read_dataset():\n    if GET_CV:\n        df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n        df['matches'] = df['label_group'].map(tmp)\n        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        df_cu = cudf.DataFrame(df)\n        image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\n    else:\n        df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n        df_cu = cudf.DataFrame(df)\n        image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n        \n    return df, df_cu, image_paths\n\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https:\/\/arxiv.org\/pdf\/1801.07698.pdf\n        https:\/\/github.com\/lyakaap\/Landmark2019-1st-and-3rd-Place-Solution\/\n            blob\/master\/src\/modeling\/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n\n\n# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths):\n    embeds = []\n    \n    margin = ArcMarginProduct(\n            n_classes = N_CLASSES, \n            s = 30, \n            m = 0.5, \n            name='head\/arc_margin', \n            dtype='float32'\n            )\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    x = efn.EfficientNetB3(weights = None, include_top = False)(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n        \n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    model.load_weights('..\/input\/shopee-efficientnetb3-arcmarginproduct\/EfficientNetB3_512_42.h5')\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) \/ chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings\n\n# Return tokens, masks and segments from a text array or series\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n# Function to get our text title embeddings using a pre-trained bert model\ndef get_text_embeddings(df, max_len = 70):\n    embeds = []\n    module_url = \"..\/input\/shopee-external-models\/bert_en_uncased_L-24_H-1024_A-16_1\"\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.5, \n            name='head\/arc_margin', \n            dtype='float32'\n            )\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    x = margin([clf_output, label])\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n    \n    model.load_weights('..\/input\/bert-baseline\/Bert_123.h5')\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) \/ chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n        embeds.append(text_embeddings)\n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings\n    \n# Function to get 50 nearest neighbors of each image and apply a distance threshold to maximize cv\ndef get_neighbors(df, embeddings, KNN = 50, image = True):\n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        if image:\n            thresholds = list(np.arange(3.0, 5.0, 0.1))\n        else:\n            thresholds = list(np.arange(15, 35, 1))\n        scores = []\n        for threshold in thresholds:\n            predictions = []\n            for k in range(embeddings.shape[0]):\n                idx = np.where(distances[k,] < threshold)[0]\n                ids = indices[k,idx]\n                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n                predictions.append(posting_ids)\n            df['pred_matches'] = predictions\n            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n            score = df['f1'].mean()\n            print(f'Our f1 score for threshold {threshold} is {score}')\n            scores.append(score)\n        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n        best_threshold = max_score['thresholds'].values[0]\n        best_score = max_score['scores'].values[0]\n        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n        \n        # Use threshold\n        predictions = []\n        for k in range(embeddings.shape[0]):\n            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n            if image:\n                idx = np.where(distances[k,] < 3.6)[0]\n            else:\n                idx = np.where(distances[k,] < 20.0)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n    \n    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n    else:\n        predictions = []\n        for k in tqdm(range(embeddings.shape[0])):\n            if image:\n                idx = np.where(distances[k,] < 3.8)[0]\n            else:\n                idx = np.where(distances[k,] < 20.0)[0]\n            ids = indices[k,idx]\n            posting_ids = df['posting_id'].iloc[ids].values\n            predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return df, predictions\n    \n\ndf, df_cu, image_paths = read_dataset()\nimage_embeddings = get_image_embeddings(image_paths)\ntext_embeddings = get_text_embeddings(df)\ngc.collect()","6b0b8c64":"# Get neighbors for image_embeddings\ndf, image_predictions = get_neighbors(df, image_embeddings, KNN = 50, image = True)\n#v1 KNN = 100","7ab27820":"# Get neighbors for text_embeddings\ndf, text_predictions = get_neighbors(df, text_embeddings, KNN = 50, image = False)\n# v1 KNN = 100","27e00e4f":"# Concatenate image predctions with text predictions\nif GET_CV:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n    score = df['f1'].mean()\n    print(f'Our final f1 cv score is {score}')\n    df['matches'] = df['pred_matches']\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\nelse:\n    df['image_predictions'] = image_predictions\n    df['text_predictions'] = text_predictions\n    df['matches'] = df.apply(combine_predictions, axis = 1)\n    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","23c1b449":"sub = pd.read_csv('submission.csv')\nsub.head()","a5404b82":"I increased the neighborhood to maximize the chances of getting actual neigbors","9c7b1a95":"# Imports","5f7c57eb":"<b>Please Note!<\/b> as stated in competition's evaluation page:\n> Group sizes were capped at 50, so there is no benefit to predict more than 50 matches.\n\nI predicted 100 matches in hope of finding the actual neighbors and it did improve the score on LB a very little (3rd digits maybe). ","1aca3638":"# Configurations","136e432a":"# Getting Embeddings","f77784d7":"# Finding K Nearest Neighbors among embeddings","54794bcf":"This Notebook is a little expansion of <a href=\"https:\/\/www.kaggle.com\/ragnar123\/unsupervised-baseline-arcface\">this notebook <\/a> So please don't forget to support original creator."}}