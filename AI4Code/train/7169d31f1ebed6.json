{"cell_type":{"0f122518":"code","bd8012a6":"code","9e441459":"code","3593e8de":"code","87a05f57":"code","a4b3eca3":"code","4b443655":"code","726d6a24":"code","12edc59f":"code","22e579d1":"code","f6078bdc":"code","26fab55b":"code","825c6b6e":"code","db7baf53":"code","72acb1bb":"code","c341f9a5":"code","4ff90cb7":"code","43ce010e":"code","fbc0e34b":"code","9bd1ee1d":"code","c5c3e0c4":"code","afcbfc9c":"code","d0f54862":"markdown","90562537":"markdown","bf0dc629":"markdown","5e23c55a":"markdown","10b6f561":"markdown","75af2255":"markdown","1da2bc4f":"markdown","6c4db3b0":"markdown","c810755f":"markdown","05d98848":"markdown"},"source":{"0f122518":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.svm import SVC\n\nimport keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.utils import to_categorical\n\nimport random","bd8012a6":"# set up dataset\nnumber_classes = 7\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\n# lets take a look...\ntrain_df.head()","9e441459":"# create train datasets\nX_train = train_df.drop(['Id', 'Cover_Type'], axis=1)\nY_train = train_df[['Cover_Type']].values\nY_train = Y_train.reshape(len(Y_train))\n\n# create test dataset and ID's\nX_test = test_df.drop(['Id'], axis=1)\nID_test = test_df['Id'].values\nID_test = ID_test.reshape(len(ID_test))\n\n# concatenate both together for feature engineering and normalisation\nX_all = pd.concat([X_train, X_test], axis=0)","3593e8de":"# do some EDA\ncols_non_onehot = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                   'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                   'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                   'Horizontal_Distance_To_Fire_Points', 'Cover_Type']\n\ncols_onehot = [col_name for col_name in train_df.columns if col_name not in cols_non_onehot]\ncols_onehot.append('Cover_Type')","87a05f57":"sns.pairplot(train_df[cols_non_onehot], hue='Cover_Type')","a4b3eca3":"fig, ax = plt.subplots()\n\n# Sort the dataframe by target\ntarget_1 = train_df.loc[train_df['Cover_Type'] == 1]\ntarget_2 = train_df.loc[train_df['Cover_Type'] == 2]\ntarget_3 = train_df.loc[train_df['Cover_Type'] == 3]\ntarget_4 = train_df.loc[train_df['Cover_Type'] == 4]\ntarget_5 = train_df.loc[train_df['Cover_Type'] == 5]\ntarget_6 = train_df.loc[train_df['Cover_Type'] == 6]\ntarget_7 = train_df.loc[train_df['Cover_Type'] == 7]\n\nsns.distplot(target_1[['Elevation']], ax=ax)\nsns.distplot(target_2[['Elevation']], ax=ax)\nsns.distplot(target_3[['Elevation']], ax=ax)\nsns.distplot(target_4[['Elevation']], ax=ax)\nsns.distplot(target_5[['Elevation']], ax=ax)\nsns.distplot(target_6[['Elevation']], ax=ax)\nsns.distplot(target_7[['Elevation']], ax=ax)\n\nplt.show()","4b443655":"interesting_cols = ['Elevation', 'Horizontal_Distance_To_Fire_Points',\n                    'Horizontal_Distance_To_Roadways', 'Hillshade_9am',\n                    'Horizontal_Distance_To_Hydrology', 'Cover_Type']\nsns.pairplot(train_df[interesting_cols], hue='Cover_Type', height=5)","726d6a24":"# mean hillshade\ndef mean_hillshade(df):\n    df['mean_hillshade'] = (df['Hillshade_9am'] + df['Hillshade_Noon'] + df['Hillshade_3pm']) \/ 3\n    return df\n\n# calculate the distance to hydrology using pythagoras theorem\ndef distance_to_hydrology(df):\n    df['distance_to_hydrology'] = np.sqrt(np.power(df['Horizontal_Distance_To_Hydrology'], 2) + \\\n                                          np.power(df['Vertical_Distance_To_Hydrology'], 2))\n    return df\n\n# calculate diagnial distance down to sea level?\ndef diag_to_sealevl(df):\n    df['diag_to_sealevel'] = np.divide(df['Elevation'], np.cos(180-df['Slope']))\n    return df\n\n# calculate mean distance to features\ndef mean_dist_to_feature(df):\n    df['mean_dist_to_feature'] = (df['Horizontal_Distance_To_Hydrology'] + \\\n                                  df['Horizontal_Distance_To_Roadways'] + \\\n                                  df['Horizontal_Distance_To_Fire_Points']) \/ 3\n    return df\n\ndef mean_shade(df):\n    df['Shadiness_morn_noon'] = df.Hillshade_9am\/(df.Hillshade_Noon+1)\n    df['Shadiness_noon_3pm'] = df.Hillshade_Noon\/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_3'] = df.Hillshade_9am\/(df.Hillshade_3pm+1)\n    df['Shadiness_morn_avg'] = (df.Hillshade_9am+df.Hillshade_Noon)\/2\n    df['Shadiness_afernoon'] = (df.Hillshade_Noon+df.Hillshade_3pm)\/2\n    df['Shadiness_total_mean'] = (df.Hillshade_9am+df.Hillshade_Noon+df.Hillshade_3pm)\/3\n    return df\n \ndef hydro_fire_combinations(df):\n    df['HF1'] = df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Fire_Points']\n    df['HF2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Fire_Points'])\n    df['Mean_HF1'] = df.HF1\/2\n    df['Mean_HF2'] = df.HF2\/2\n    return df\n\ndef hydro_road_combinations(df):\n    df['HR1'] = (df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Roadways'])\n    df['HR2'] = (df['Horizontal_Distance_To_Hydrology']-df['Horizontal_Distance_To_Roadways'])\n    df['Mean_HR1'] = df.HR1\/2\n    df['Mean_HR2'] = df.HR2\/2\n    return df\n\ndef fire_road_combinations(df):\n    df['FR1'] = (df['Horizontal_Distance_To_Fire_Points']+df['Horizontal_Distance_To_Roadways'])\n    df['FR2'] = (df['Horizontal_Distance_To_Fire_Points']-df['Horizontal_Distance_To_Roadways'])\n    df['Mean_FR1'] = df.FR1\/2\n    df['Mean_FR2'] = df.FR2\/2\n    return df\n\ndef elevation_hydro_combinations(df):\n    df['EV1'] = df.Elevation+df.Vertical_Distance_To_Hydrology\n    df['EV2'] = df.Elevation-df.Vertical_Distance_To_Hydrology\n    df['Mean_EV1'] = df.EV1\/2\n    df['Mean_EV2'] = df.EV2\/2\n    return df\n\ndef binned_columns(df):\n    bin_defs = [\n        # col name, bin size, new name\n#         ('Elevation', 200, 'Binned_Elevation'),\n        ('Aspect', 45, 'Binned_Aspect'),\n        ('Slope', 6, 'Binned_Slope'),\n        ('Horizontal_Distance_To_Hydrology', 140, 'Binned_Horizontal_Distance_To_Hydrology'),\n        ('Horizontal_Distance_To_Roadways', 712, 'Binned_Horizontal_Distance_To_Roadways'),\n        ('Hillshade_9am', 32, 'Binned_Hillshade_9am'),\n        ('Hillshade_Noon', 32, 'Binned_Hillshade_Noon'),\n        ('Hillshade_3pm', 32, 'Binned_Hillshade_3pm'),\n        ('Horizontal_Distance_To_Fire_Points', 717, 'Binned_Horizontal_Distance_To_Fire_Points')\n    ]\n    \n    for col_name, bin_size, new_name in bin_defs:\n        df[new_name] = np.floor(df[col_name]\/bin_size)\n    \n    return df\n\nX_all = mean_hillshade(X_all)\nX_all = distance_to_hydrology(X_all)\n# X_all = diag_to_sealevl(X_all)\nX_all = mean_dist_to_feature(X_all)\nX_all = mean_shade(X_all)\nX_all = hydro_fire_combinations(X_all)\nX_all = hydro_road_combinations(X_all)\nX_all = fire_road_combinations(X_all)\n# X_all = elevation_hydro_combinations(X_all)\nX_all = binned_columns(X_all)\n\nX_all.head()","12edc59f":"# normalise dataset\ndef normalise_df(df):\n    df_mean = df.mean()\n    df_std = df.std()    \n    df_norm = (df - df_mean) \/ (df_std)\n    return df_norm, df_mean, df_std\n\n# define columsn to normalise\ncols_non_onehot = [#'Elevation', \n                   'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n                   'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n                   'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', \n                   'Horizontal_Distance_To_Fire_Points', 'mean_hillshade',\n                   'Shadiness_morn_noon', 'Shadiness_noon_3pm', 'Shadiness_morn_3',\n                   'Shadiness_morn_avg', 'Shadiness_afernoon', 'Shadiness_total_mean',\n                   'HF1', 'HF2', 'Mean_HF1', 'Mean_HF2',\n                   'HR1', 'HR2', 'Mean_HR1', 'Mean_HR2',\n                   'FR1', 'FR2', 'Mean_FR1', 'Mean_FR2',\n                   #'EV1', 'EV2', 'Mean_EV1', 'Mean_EV2',\n                   'distance_to_hydrology', \n                   #'diag_to_sealevel', \n                   'mean_dist_to_feature']\n\nX_all_norm, df_mean, df_std = normalise_df(X_all[cols_non_onehot])\n\n# replace columns with normalised versions\nX_all = X_all.drop(cols_non_onehot, axis=1)\nX_all = pd.concat([X_all_norm, X_all], axis=1)","22e579d1":"# elevation was found to have very different distributions on test and training sets\n# lets just drop it for now to see if we can implememnt a more robust classifier!\nX_all = X_all.drop('Elevation', axis=1)","f6078bdc":"# split back into test and train sets\nX_train = np.array(X_all[:len(X_train)])\nX_test = np.array(X_all[len(X_train):])","26fab55b":"from sklearn.feature_selection import RFECV\n\n# rf = RandomForestClassifier(n_estimators=150, n_jobs=-1, criterion='entropy',\n#                             min_samples_split=3, class_weight='balanced')\n\nlgb = LGBMClassifier(n_estimators=100, max_depth=3)\n\nrfecv = RFECV(estimator=lgb, step=1, cv=StratifiedKFold(3),\n              scoring='accuracy', verbose=1)\n\nrfecv.fit(np.array(X_train), np.array(Y_train))\n\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","825c6b6e":"# reduce features to the best subset found with rfecv\nX_train = rfecv.transform(np.array(X_train))","db7baf53":"def create_blender(X, Y):\n    blender = Sequential()\n\n    blender.add(Dense(512, activation='elu', input_shape=(len(X[0]),), \n                      kernel_regularizer='l2'))\n    blender.add(BatchNormalization())\n    blender.add(Dropout(0.3))\n\n    blender.add(Dense(256, activation='elu', kernel_regularizer='l2'))\n    blender.add(BatchNormalization())\n    blender.add(Dropout(0.3))\n    \n    blender.add(Dense(128, activation='elu', kernel_regularizer='l2'))\n    blender.add(BatchNormalization())\n    blender.add(Dropout(0.3))\n    \n    blender.add(Dense(32, activation='elu', kernel_regularizer='l2'))\n    blender.add(BatchNormalization())\n    blender.add(Dropout(0.3))\n    \n    blender.add(Dense(len(Y[0]), activation='softmax'))\n    sgd = keras.optimizers.SGD(lr=0.0001, momentum=0.85, nesterov=True)\n    blender.compile(loss='categorical_crossentropy', optimizer='sgd')\n    return blender","72acb1bb":"class BlenderClassifier(object):\n    def __init__(self, learners, create_blender_func, n_folds=5, max_epoch=50):\n        self.learners = learners\n        self.n_folds = n_folds\n        self.max_epoch = max_epoch\n        self.history = {}\n        self.create_blender = create_blender_func\n        \n    def fit(self, X, Y, verbose=True):\n        kfolds = StratifiedKFold(self.n_folds, shuffle=True)\n        predictions = [[] for x in self.learners]\n        targets = []\n        for fold_idx, (t, v) in enumerate(kfolds.split(X, Y)):\n            if verbose:\n                print('  commencing fold {}\/{}...'.format(fold_idx + 1, \n                                                          self.n_folds))\n            targets.extend(Y[v])\n            for l_idx, learner in enumerate(self.learners):\n                if verbose:\n                    print('    fitting {}...'.format(type(learner)))\n                learner.fit(X[t], Y[t])\n                if verbose:\n                    print('      predicting {}...'.format(type(learner)))                \n                predictions[l_idx].extend(learner.predict(X[v]))\n        \n        if verbose:\n            print('  creating blender...')\n        \n        targets = to_categorical(targets)\n        predictions = np.swapaxes(predictions, 0, -1)\n        x_new = np.concatenate([X, predictions], axis=-1)\n        self.blender = self.create_blender(x_new, targets)\n        \n        if verbose:\n            print('  fitting blender...')\n        self.blender.fit(x_new, targets, epochs=self.max_epoch, \n                         shuffle=True, validation_split=0.1)\n        if verbose:\n            print('done!')\n        \n        \n    def predict(self, X, verbose=True):\n        if verbose:\n            print('predicting X...')\n        predictions = [[] for x in self.learners]\n        for l_idx, learner in enumerate(self.learners):\n            if verbose:\n                print('  predicting using {}...'.format(type(learner)))                \n            predictions[l_idx].extend(learner.predict(X))\n        \n        predictions = np.swapaxes(predictions, 0, -1)\n        \n        if verbose:\n            print('  blending predictions...')\n\n        x_new = np.concatenate([X, predictions], axis=-1)\n            \n        return self.blender.predict(x_new)","c341f9a5":"logreg = LogisticRegression()\n\ngnb = GaussianNB()\n\nrfc1 = RandomForestClassifier(n_estimators=100, \n                              min_samples_split=2, \n                              class_weight='balanced',\n                              criterion='entropy',\n                              n_jobs=-1)\n\nrfc2 = RandomForestClassifier(n_estimators=100, \n                              min_samples_split=3, \n                              class_weight='balanced',\n                              criterion='entropy',\n                              n_jobs=-1)\n\nrfc3 = RandomForestClassifier(n_estimators=100, \n                              min_samples_split=5, \n                              class_weight='balanced',\n                              criterion='entropy',\n                              n_jobs=-1)\n\netc1 = ExtraTreesClassifier(n_estimators=100, \n                            min_samples_split=2,\n                            class_weight='balanced',\n                            criterion='entropy',\n                            n_jobs=-1)\n\netc2 = ExtraTreesClassifier(n_estimators=100, \n                            min_samples_split=3,\n                            class_weight='balanced',\n                            criterion='entropy',\n                            n_jobs=-1)\n\netc3 = ExtraTreesClassifier(n_estimators=100, \n                            min_samples_split=5,\n                            class_weight='balanced',\n                            criterion='entropy',\n                            n_jobs=-1)\n\nlbc1 = LGBMClassifier(n_estimators=500,\n                     learning_rate=0.001)\n\nlbc2 = LGBMClassifier(n_estimators=500,\n                     learning_rate=0.01)\n\nlbc3 = LGBMClassifier(n_estimators=500,\n                     learning_rate=0.1)","4ff90cb7":"default_learners = [logreg, gnb, \n                    rfc1, rfc2, rfc3, \n                    etc1, etc2, etc3, \n                    lbc1, lbc2, lbc3]","43ce010e":"Xt, Xv, Yt, Yv = train_test_split(np.array(X_train), np.array(Y_train), shuffle=True, stratify=np.array(Y_train))\n\nbc = BlenderClassifier(learners=default_learners, \n                       create_blender_func=create_blender)\n    \nbc.fit(Xt, Yt)","fbc0e34b":"y_pred = bc.predict(Xv)\n\n# for stupid sklearn metric\ny_pred = np.argmax(y_pred, axis=-1)\ny_pred = to_categorical(y_pred)\nYv = to_categorical(Yv)\n\nfrom sklearn.metrics import accuracy_score\n\nscore = accuracy_score(Yv, y_pred)\n\nprint('final validation score: {}'.format(score))","9bd1ee1d":"print('producing test data predictions...')\nX_test = rfecv.transform(X_test)\ny_pred = bc.predict(X_test)","c5c3e0c4":"y_pred = np.argmax(y_pred, axis=-1)\n\nprint(max(y_pred))\nprint(min(y_pred))","afcbfc9c":"sub = pd.DataFrame()\nsub['Id'] = ID_test\nsub['Cover_Type'] = y_pred\nsub.to_csv('my_submission.csv', index=False)\nprint('good luck!')","d0f54862":"Let's load the data using pandas and have a quick look at the first 5 rows...","90562537":"Now we will:\n  * remove the ID field\n  * split the data into the features and targets\/classes\n  * concatenate all the feature data together for analysis and feature engineering","bf0dc629":"Now its time to visualise some of the features and how they are related!","5e23c55a":"Now we have a generous set of features lets normalise them while being careful to leave the one hot encoded variables untouched.","10b6f561":"Lets look into the distribution of elevation a little bit further...","75af2255":"From the above chart we can see that elevation is one of the most important variables, see how each class has an almost distinguishable distribution? Cover type's 3 and 6 are the hardest to discriminate using elevation alone, however some of the other variables may help our models distinguish this!","1da2bc4f":"Now lets visualise some of the other interesting relationships in a larger pairplot","6c4db3b0":"In this notebook we are going to:\n  * load the data\n  * visualise it briefly\n  * engineer some features\n  * use RFE to select the best subset of features\n  * train some simple base classifiers\n  * blend the results using a neural network\n  * produce a submission with the trained blending ensemble classifier","c810755f":"Now we've got an idea about how the data set is distributed and some of the relations between the data, lets put that knowlege into practice by creating some new features of the data. Since we are using RFE in the next step we can be generous in our engineering of features.","05d98848":"Lets start by using seaborn to view the distributions and relations between the non onehot encoded features (view in full screen!)"}}