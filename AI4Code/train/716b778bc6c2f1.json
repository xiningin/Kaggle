{"cell_type":{"a0c30ca1":"code","2c21a0c0":"code","d72a52de":"code","c4ab0402":"code","c3527ad7":"code","d1519ce4":"code","35e5b1a3":"code","df2c69c7":"code","2eb964eb":"code","dcb8d3ac":"code","5890cee0":"code","52492ec9":"code","e7fb3390":"code","c05a43b7":"code","c1d1b4ad":"code","2f39bfdf":"code","0e918ffd":"code","26bd52a9":"code","0f389398":"code","034f9f2e":"code","59102231":"code","5d0ac1c6":"code","5eeddc1a":"code","3cf46317":"code","d31653d5":"code","4d8712cb":"code","e33b2e2b":"code","49e06167":"code","bb25ec09":"code","e80b939f":"code","44ce7b68":"code","d2054dff":"code","04c6a1fb":"code","1f48865a":"code","3516f120":"code","4b44f7c7":"code","c09ccfdc":"code","b4a5af1a":"code","61e31984":"code","7c48c525":"code","8fdd3816":"code","b6a75d4e":"code","609980d3":"code","75b04780":"code","0c6466c5":"code","bf2f82b1":"code","17a1d69e":"code","a1d4502b":"markdown","0cc8b9fe":"markdown","a97c014a":"markdown","cef16f16":"markdown","7614c973":"markdown","c4453327":"markdown","50d4c981":"markdown","3e2a5ecd":"markdown","5dcfe6ac":"markdown","f052ad14":"markdown","dac25c06":"markdown","ac236d3d":"markdown","c735e716":"markdown","3559bd20":"markdown","9b112cc4":"markdown","26941df9":"markdown","3e25a0be":"markdown","ed13d5fb":"markdown","bd8afa56":"markdown","3996c7cd":"markdown","aae8f490":"markdown","2c4d7ee6":"markdown","7fcf8eed":"markdown","8e0d9064":"markdown","b9c70885":"markdown","ee02eacb":"markdown","2bfcbdf0":"markdown","62957e5d":"markdown","88b6a959":"markdown","28c845dd":"markdown","a340d59f":"markdown"},"source":{"a0c30ca1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c21a0c0":"FILE_PATH = \"\/kaggle\/input\/tabular-playground-series-sep-2021\/\"\n\ntrain_file_path = os.path.join(FILE_PATH, \"train.csv\")\ntest_file_path  = os.path.join(FILE_PATH, \"test.csv\")","d72a52de":"!pip install tensorflow_decision_forests","c4ab0402":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf\n\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow Version: {}\".format(tf.__version__))\nprint(\"TensorFlow Decision Forests: {}\".format(tfdf.__version__))","c3527ad7":"np.random.seed(1337)\ntf.random.set_seed(1337)\n\nVALID_RATIO = 0.1","d1519ce4":"train_full_data = pd.read_csv(train_file_path)\nprint(\"Full train dataset shape is {}\".format(train_full_data.shape))","35e5b1a3":"train_full_data.head()","df2c69c7":"train_full_data = train_full_data.drop('id', axis=1)\nfeatures = [f'f{i}' for i in range(1, 119)]\nlabel = 'claim'","2eb964eb":"train_full_data[features].isna().sum()","dcb8d3ac":"def split_dataset(dataset, test_ratio=0.1):\n    test_indices = np.random.rand(len(dataset)) < test_ratio\n    return dataset[~test_indices], dataset[test_indices]","5890cee0":"train_full_data['nan'] = train_full_data[features].isnull().sum(axis=1)\ntrain_full_data['std'] = train_full_data[features].std(axis=1)\ntrain_full_data['var'] = train_full_data[features].var(axis=1)","52492ec9":"train_ds_pd, valid_ds_pd = split_dataset(train_full_data, test_ratio=VALID_RATIO)\nprint(\"{} samples in training and {} in validation\".format(train_ds_pd.shape[0], valid_ds_pd.shape[0]))","e7fb3390":"train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)","c05a43b7":"model_1 = tfdf.keras.GradientBoostedTreesModel(\n    growing_strategy = 'BEST_FIRST_GLOBAL',\n    l1_regularization = 0.8\n)\n\nmodel_1.compile(metrics=[keras.metrics.AUC()])","c1d1b4ad":"%%time\nmodel_1.fit(train_ds, verbose=0)","2f39bfdf":"model_1.summary()","0e918ffd":"inspector = model_1.make_inspector()","26bd52a9":"print(\"Model contains {} trees\".format(inspector.num_trees()))","0f389398":"inspector.features()","034f9f2e":"inspector.variable_importances()","59102231":"logs = inspector.training_logs()\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy (out-of-bag)')\nplt.subplot(1, 2, 2)\nplt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Logloss (out-of-bag)')\nplt.show()","5d0ac1c6":"evaluation = model_1.evaluate(valid_ds, return_dict = True)\nfor name, value in evaluation.items():\n    print(\"{}: {}\".format(name, value))","5eeddc1a":"train_full_data = pd.read_csv(train_file_path)","3cf46317":"train_full_data = train_full_data.drop('id', axis=1)\nfeatures = [f'f{i}' for i in range(1, 119)]\nlabel = 'claim'","d31653d5":"train_ds_pd, valid_ds_pd = split_dataset(train_full_data, test_ratio=VALID_RATIO)\nprint(\"{} samples in training and {} in validation\".format(train_ds_pd.shape[0], valid_ds_pd.shape[0]))","4d8712cb":"train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\nvalid_ds = tfdf.keras.pd_dataframe_to_tf_dataset(valid_ds_pd, label=label)","e33b2e2b":"def reduce_mean_without_nan(input_tensor):\n    return tf.experimental.numpy.nanmean(input_tensor, axis=1, keepdims=True)\n\ndef get_nan_std_var(input_tensor):\n    # nan tensor\n    is_nan = tf.math.is_nan(input_tensor)\n    \n    # Get the number of nans available in each sample\n    nan_number_per_sample = tf.cast(\n        tf.math.reduce_sum(\n            tf.where(is_nan, [1], [0]),\n            axis=1,\n            keepdims=True\n        ),\n        tf.float32\n    )\n    \n    # Calculate mean excluding nan\n    mean_excluding_nan = keras.layers.Lambda(reduce_mean_without_nan)(input_tensor)\n    \n    # input tensor replacing nan with the mean for each row (i.e. for each sample)\n    input_tensor_with_nan_replaced_by_mean = tf.where(\n        is_nan,\n        mean_excluding_nan,\n        input_tensor\n    )\n    \n    squared_distance_from_mean = tf.math.reduce_sum(\n        tf.math.square(\n            input_tensor_with_nan_replaced_by_mean - mean_excluding_nan,\n        ),\n        axis=1,\n        keepdims=True\n    )\n    \n    # Calculate std\n    std = tf.math.sqrt(\n        tf.math.divide(\n            squared_distance_from_mean,\n            input_tensor.shape[1] - nan_number_per_sample,\n        )\n    )\n    \n    # Calculate var\n    var = tf.math.divide(\n        squared_distance_from_mean,\n        input_tensor.shape[1] - nan_number_per_sample - 1,\n    )\n    \n    stack =tf.stack([nan_number_per_sample, std, var], axis=1)\n    \n    return tf.squeeze(stack, axis=-1)","49e06167":"def build_preprocessing_model(features):\n    # Create inputs\n    input_layers = []\n\n    # Each feature will be one input\n    for feature in features:\n        input_layers.append(keras.layers.Input(shape=(1,), name=feature))\n    \n    # Concatenate all inputs\n    inputs = keras.layers.concatenate(input_layers, name=\"inputs\")\n        \n    \n    # Add 3 additional features:\n    # - How many nan are they in each sample\n    # - std accross the features of each sample\n    # - var accross the features of each sample\n    additional_features = get_nan_std_var(inputs)\n    \n    outputs = keras.layers.concatenate([inputs, additional_features])\n    \n    return keras.Model(input_layers, outputs)\n\npreprocessing_model = build_preprocessing_model(features)\npreprocessing_model.summary()","bb25ec09":"tfdf_features = []\n\nfor feature in features:\n    print(\"Creating FeatureUsage for {}\".format(feature))\n    tfdf_features.append(tfdf.keras.FeatureUsage(name=feature))","e80b939f":"model_2 = tfdf.keras.GradientBoostedTreesModel(\n    growing_strategy = 'BEST_FIRST_GLOBAL',\n    l1_regularization = 0.6,\n    preprocessing = preprocessing_model\n)\n\nmodel_2.compile(metrics=[keras.metrics.AUC()])","44ce7b68":"%%time\nmodel_2.fit(train_ds, verbose=0)","d2054dff":"model_2.summary()","04c6a1fb":"inspector = model_2.make_inspector()","1f48865a":"print(\"Model contains {} trees\".format(inspector.num_trees()))","3516f120":"inspector.features()","4b44f7c7":"inspector.variable_importances()","c09ccfdc":"logs = inspector.training_logs()\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Accuracy (out-of-bag)')\nplt.subplot(1, 2, 2)\nplt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\nplt.xlabel('Number of trees')\nplt.ylabel('Logloss (out-of-bag)')\nplt.show()","b4a5af1a":"inspector.export_to_tensorboard(\".\/model_2_logs\")","61e31984":"%tensorboard --logdir \".\/model_2_logs\"","7c48c525":"evaluation = model_2.evaluate(valid_ds, return_dict = True)\nfor name, value in evaluation.items():\n    print(\"{}: {}\".format(name, value))","8fdd3816":"model = model_2","b6a75d4e":"test_data = pd.read_csv(test_file_path)\nids = test_data.pop('id')","609980d3":"if model == model_1:\n    test_data['nan'] = test_data[features].isnull().sum(axis=1)\n    test_data['std'] = test_data[features].std(axis=1)\n    test_data['var'] = test_data[features].var(axis=1)","75b04780":"test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_data)","0c6466c5":"preds = model.predict(test_ds)","bf2f82b1":"output = pd.DataFrame({'id': ids,\n                       'claim': preds.squeeze()})\n\noutput.head()","17a1d69e":"output_filename = \"test_prediction_output.csv\"\noutput.to_csv(output_filename, index=False)","a1d4502b":"## Preprocessing","0cc8b9fe":"Create the training and validation datasets using TensorFlow Decision Forests `pd_dataframe_to_tf_dataset`","a97c014a":"### Training Logs","cef16f16":"You can now run the below cell and go prepare a cup of \u2615 ... it will take around 30 minutes to finish.","7614c973":"# Test Set Prediction\nWe will use the 2nd approach for our prediction","c4453327":"### Tensorboard","50d4c981":"## Evaluation\n\nLet's evaluate the model using the validation dataset","3e2a5ecd":"`model.summary()` shows us the overall structure of the model","5dcfe6ac":"# Data Loading","f052ad14":"### Training Logs","dac25c06":"## Datasets","ac236d3d":"# Second Approach: TensorFlow based preprocessing\n\nIn this second approach, we will add the 3 columns that we added using pandas but rather through TensorFlow \/ Keras preprocessing that we will pass to the model via the preprocessing parameter.\n\nYou will notice that this approach is more difficult, more complicated, however it has the advantage of having the preprocessing within the model itself.","c735e716":"## GradientBoostedTreesModel Training","3559bd20":"## GradientBoostedTreesModel Training","9b112cc4":"Let's check if we have missing data","26941df9":"## Post Training Analysis","3e25a0be":"# References\n\n\n*   [KerasTuner + TF Decision Forest](https:\/\/www.kaggle.com\/ekaterinadranitsyna\/kerastuner-tf-decision-forest?linkId=133421702) by [Ekaterina Dranitsyna](https:\/\/www.kaggle.com\/ekaterinadranitsyna)\n*   [TensorFlow Decision Forests tutorials](https:\/\/www.tensorflow.org\/decision_forests\/tutorials) which are a set of 3 very interesting (beginner, intermediate and advanced levels) tutorials.\n*   The [TensorFlow Forum](https:\/\/discuss.tensorflow.org\/) where one can get in touch with the TensorFlow community. Check it out if you haven't yet.\n","ed13d5fb":"The data is composed of 120 columns all of which are numerical:\n* 118 feature columns named `f1, f2, ... f118`\n* label column named `claim`\n* An `id` column that we will drop","bd8afa56":"## Evaluation","3996c7cd":"Let's first define the features that we will be using ","aae8f490":"We can see that the data contains a lot of missing values. Approximately 15000 for each feature. That's around 1.5%\n\nIn the approach that we will use in this notebook, we will keep the missing values but will add 3 additional features:\n* `Number of missing values` in each sample. So for each sample out of the 957919, we will see how many values are missing across all features\n* `Standard deviation` over axis=1 which gives us the standard deviation for each sample\n* `Unbiased Variance` over axis=1 which gives us the variance for each sample\n\nWe will be implementing this preprocessing using 2 methods as I said before.\nLet's start with the first one","2c4d7ee6":"For hyperparameter tuning, we did the following:\n* Tried the predefined hyperparameters which did not give good results especially `benchmark_rank1` which gave very bad results. `better_default` on the other hand gave acceptable results\n* Used Keras tuner in order to search for hyperparameters that maximise AUC. If you want to check how to do this, check out the following [notebook](https:\/\/www.kaggle.com\/ekaterinadranitsyna\/kerastuner-tf-decision-forest?linkId=133421702) by Ekaterina Dranitsyna\n\n**Note:** The idea of adding 3 new columns is taken also from the above mentioned notebook by Ekaterina Dranitsyna\n\nFinally the hyperparameters that gave me the best results where the below which are `better_default` with `l1_regularization`","7fcf8eed":"# Imports & Configuration","8e0d9064":"# First Approach: Preprocessing using pandas","b9c70885":"## Preprocessing\n\nLet's add the 3 additional features using pandas","ee02eacb":"## Datasets\n\nSplit the dataframe into training and validation sets","2bfcbdf0":"The model achieved `AUC = 0.8160` on the validation dataset\n\nLet's now look at our second approach","62957e5d":"In this notebook, we will use TensorFlow Decision Forests `GradientBoostedTreesModel` in order to create a classification model that can achieve good results.\n\nWe will use 2 preprocessing methods:\n* On the training and validation dataframes directly\n* Creating a Keras model that we will pass to the GradientBoostedTreesModel preprocessing parameter\n\nBoth methods achieved almost the same result on the validation set\n\nSo, let's start ... first let's install `tensorflow_decision_forests`","88b6a959":"## Post Training Analysis","28c845dd":"This model achieved a lower `AUC = 0.8111` on the validation dataset than the previous one which achieved `0.8160`\n\nThis model as you noticed created the additional features differently. While the pandas based method counts the nan values and calculates std and var on the whole training dataset, this model calculates and counts them for each batch.","a340d59f":"We can access all this information using the model inspector"}}