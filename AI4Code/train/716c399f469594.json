{"cell_type":{"67de0f5b":"code","860c4c4e":"code","8bbe9aee":"code","0bf8ffda":"code","9c346c4c":"code","633e73a4":"code","8dff801d":"code","f479313e":"code","2c5accb8":"code","5fb92de5":"code","46628dcd":"code","6ecd13ef":"code","c5a9f940":"code","dad25952":"code","a5d9e3bd":"code","606f2ec3":"code","fd39eeff":"code","f574b790":"code","051bbf36":"code","33bfe091":"code","f6ecffc3":"code","5e5269c9":"code","40fcff9b":"code","7e4a7592":"code","73aee32a":"code","4c79b1a3":"code","a660f3c4":"code","57d1b082":"code","6c8a1859":"code","bd5962b5":"code","bffb6465":"markdown","59ec0651":"markdown","974467e8":"markdown","e6b9463a":"markdown","d10d9f63":"markdown","79e0acf5":"markdown","955eef15":"markdown","58290e3f":"markdown","95ed58df":"markdown","b96d0d1d":"markdown","4014e7b7":"markdown","3140c2a6":"markdown","e29aefab":"markdown","6f915b75":"markdown","b5d036d8":"markdown","712350c0":"markdown","5f61064e":"markdown","65f308e1":"markdown","347469fc":"markdown","268ef465":"markdown"},"source":{"67de0f5b":"from glob import glob\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport urllib\nimport numpy as np \nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.layers import BatchNormalization,Conv2D,MaxPool2D,Dense,Flatten,Input,GlobalMaxPooling2D,Dropout\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom sklearn.metrics import classification_report\nfrom keras.preprocessing.image import ImageDataGenerator\n! pip install visualkeras\nimport visualkeras\nimport warnings\nwarnings.filterwarnings('ignore')","860c4c4e":"label=0\ndf=pd.DataFrame()\nfor file in glob('..\/input\/eyes-rtte\/*'):\n    for images in tqdm(glob(file+'\/*')):\n#         print(images)\n        image=cv2.imread(images)\n        image=cv2.resize(image,(64,64))    #As images are of different shape so resizing all to (224,224,3)\n        data=pd.DataFrame({'image':[image],'label':[label]})\n        df=df.append(data)\n    label+=1","8bbe9aee":"train_data,test_data=train_test_split(df,test_size=0.2,random_state=23)","0bf8ffda":"def change_to_input_dimension(data):\n    data=np.reshape(data.to_list(),(len(data),64,64,3))\n    data=data\/255.0\n    return data\ntrain_image=change_to_input_dimension(train_data.image)\ntest_image=change_to_input_dimension(test_data.image)","9c346c4c":"plt.figure(figsize=(18,15))\nfor i in range(12):\n    plt.subplot(4,3,(i%12)+1)\n    rnd_idx=np.random.randint(1000)\n    if train_data.label.values[rnd_idx]==1:\n        plt.title('Female eyes',fontdict={'size':13})\n    else:\n        plt.title('Male eyes',fontdict={'size':13})\n    \n    plt.axis('off')\n    plt.imshow(train_image[rnd_idx])","633e73a4":"early_stop=EarlyStopping(patience=4)\nreduce_lr=ReduceLROnPlateau(patience=2)","8dff801d":"model=Sequential()\nmodel.add(Input(shape=(64,64,3)))\nmodel.add(Conv2D(128,(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64,(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(32,(3,3),padding='same',activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.25))\n# model.add(keras.layers.Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","f479313e":"visualkeras.layered_view(model)","2c5accb8":"r=model.fit(train_image,train_data.label.values,validation_data=(test_image,test_data.label.values),\n            epochs=20,\n            callbacks=[early_stop,reduce_lr]\n           )","5fb92de5":"plt.figure(figsize=(15,12))\nplt.plot(r.history['loss'])\nplt.plot(r.history['val_loss'])\nplt.title('Loss curve',fontdict={'size':20})\nplt.show()","46628dcd":"plt.figure(figsize=(10,8))\nplt.plot(r.history['accuracy'])\nplt.plot(r.history['val_accuracy'])\nplt.title('Loss curve',fontdict={'size':20})\nplt.show()","6ecd13ef":"\nprint('CLASSIFICATION REPORT ON TRAIN DATASET \\n\\n')\nprint(classification_report(train_data.label,model.predict_classes(train_image),target_names=['male','female']))\n\nprint('CLASSIFICATION REPORT ON TEST DATASET \\n\\n')\nprint(classification_report(test_data.label,model.predict_classes(test_image),target_names=['male','female']))","c5a9f940":"plt.figure(figsize=(18,15))\nfor i in range(12):\n    plt.subplot(4,3,(i%12)+1)\n    rnd_idx=np.random.randint(1000)\n    pred=model.predict_classes(test_image[rnd_idx:rnd_idx+1])\n    if test_data.label.values[rnd_idx]==1:\n        if pred==1:\n            plt.title('Predicted :-female \\n actual:- female',fontdict={'size':13})\n        else:\n            plt.title('Predicted :-male \\n actual:- female',fontdict={'size':13})\n    else:\n        if pred==1:\n            plt.title('Predicted :-female \\n actual:- male',fontdict={'size':13})\n        else:\n            plt.title('Predicted :-male \\n actual:- male',fontdict={'size':13})\n    \n    plt.axis('off')\n    plt.imshow(test_image[rnd_idx])","dad25952":"# function to plot images from urls\n\n\ndef image_prediction(urls):\n    plt.figure(figsize=(20,18))\n    for i,img_url in enumerate(urls):\n        resp = urllib.request.urlopen(img_url)\n        img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n        img=cv2.resize(img,(64,64))\n        img_data=np.reshape(img,(1,64,64,3))\n        img_data=img_data\/255.0\n        pred=model.predict_classes(img_data)\n        if pred==0:\n            out='male eyes'\n        else:\n            out='female eyes'\n        plt.subplot(4,3,(i+1)%12)\n        plt.title('Its predicted as {0}'.format(out))\n        plt.axis('off')\n        plt.imshow(img)","a5d9e3bd":"female_urls=['https:\/\/image.shutterstock.com\/image-photo\/female-eye-extreme-long-false-260nw-1224470146.jpg',\n             'https:\/\/s3.envato.com\/files\/256875710\/2018_MG_1318.jpg',\n             'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTpM75WmCMoDKh7MKJukBDrG1M8ZS3SDEJHTIUal3OjNThQ8OKQkIhXkDAuEbXJ5rH7H0s&usqp=CAU',\n            'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/b0\/Iris_-_right_eye_of_a_girl.jpg',\n            'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRAcYU1pNbnCGgXHBoT3YTY_NwwCG-DOxnClQ&usqp=CAU',\n            'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/d\/d5\/Human_eye_-_blue_-_without_watermark.jpg',\n            'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQm8zIrrEs2RUJOOK8V7tPMdpGqjkxBuUFcpg&usqp=CAU',\n             'https:\/\/st3.depositphotos.com\/6783846\/13684\/i\/1600\/depositphotos_136844984-stock-photo-perfect-shape-of-eyebrows-brown.jpg',\n            'https:\/\/st3.depositphotos.com\/6783846\/13870\/i\/1600\/depositphotos_138702250-stock-photo-perfect-shape-of-eyebrows-brown.jpg',]","606f2ec3":"# testing on random women eyes urls from google images\n\nimage_prediction(female_urls)","fd39eeff":"male_urls=['https:\/\/d3i6fh83elv35t.cloudfront.net\/newshour\/app\/uploads\/2014\/04\/Eyeball_Blue_male.jpg',\n          'https:\/\/img4.goodfon.com\/wallpaper\/big\/6\/d9\/eye-eyes-men-face-brown-gold-soft-faces-nice-chshm.jpg',\n           'https:\/\/t4.ftcdn.net\/jpg\/02\/49\/12\/67\/360_F_249126788_hRkiuKCzs1zLrPoo4kODXf8CAW5U3Mna.jpg',\n           'https:\/\/s3.envato.com\/files\/25761728\/95.jpg',\n           'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRllhnr8yoh56C7qmNqKjit1GbQp0ExtMPDKQ&usqp=CAU',\n           'https:\/\/thumbs.dreamstime.com\/z\/male-eye-menacing-looking-menacingly-raised-eyebrow-31506103.jpg',\n           'https:\/\/image.shutterstock.com\/image-photo\/all-eyes-260nw-761411689.jpg',\n           'https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSPO32XndQp8_jIUAJ2CUx1dQW79njDbRNq74rE7v97EFHdWJ2GhZC5zyG_xEfbNXESRuc&usqp=CAU',\n           'https:\/\/i.pinimg.com\/originals\/c5\/f5\/f8\/c5f5f863e632766fae2808fba19f15c6.jpg'\n          ]","f574b790":"# testing on random men eyes urls from google images\n\nimage_prediction(male_urls)","051bbf36":"# train_image","33bfe091":"\ntrain_img_generator=ImageDataGenerator(rescale=1.0\/255.0,\n                                 horizontal_flip=True,\n                                 height_shift_range=0.5,\n                                 width_shift_range=0.5,\n                                 brightness_range=[0.8,1.0],\n                                zoom_range=[0.5,1.0])","f6ecffc3":"model_arg=Sequential()\nmodel_arg.add(Input(shape=(64,64,3)))\n# model_arg.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n# model_arg.add(MaxPool2D(2,2))\n# model_arg.add(BatchNormalization())\n# model_arg.add(Dropout(0.5))\nmodel_arg.add(Conv2D(128,(3,3),padding='same',activation='relu'))\nmodel_arg.add(MaxPool2D(2,2))\nmodel_arg.add(BatchNormalization())\nmodel_arg.add(Dropout(0.3))\nmodel_arg.add(Conv2D(64,(3,3),padding='same',activation='relu'))\nmodel_arg.add(MaxPool2D(2,2))\nmodel_arg.add(BatchNormalization())\nmodel_arg.add(Dropout(0.3))\nmodel_arg.add(Conv2D(32,(3,3),padding='same',activation='relu'))\nmodel_arg.add(MaxPool2D(2,2))\nmodel_arg.add(BatchNormalization())\nmodel_arg.add(Dropout(0.3))\nmodel_arg.add(Flatten())\nmodel_arg.add(Dense(256,activation='relu'))\nmodel_arg.add(Dropout(0.2))\n# model_arg.add(Dense(64,activation='relu'))\n# model_arg.add(Dropout(0.3))\nmodel_arg.add(Dense(1,activation='sigmoid'))\nmodel_arg.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","5e5269c9":"visualkeras.layered_view(model_arg)","40fcff9b":"batch_size=64\nr1=model_arg.fit_generator(train_img_generator.flow(train_image, train_data.label.values), \n          validation_data=(test_image, test_data.label.values), steps_per_epoch=len(train_image) \/ batch_size,\n                           epochs=30)","7e4a7592":"plt.figure(figsize=(10,8))\nplt.plot(r1.history['loss'])\nplt.plot(r1.history['val_loss'])\nplt.title('Loss curve',fontdict={'size':20})\nplt.show()","73aee32a":"plt.figure(figsize=(10,8))\nplt.plot(r1.history['accuracy'])\nplt.plot(r1.history['val_accuracy'])\nplt.title('Accuracy curve',fontdict={'size':20})\nplt.show()","4c79b1a3":"print('CLASSIFICATION REPORT ON TEST DATASET \\n\\n')\nprint(classification_report(test_data.label,model_arg.predict_classes(test_image),target_names=['male','female']))","a660f3c4":"plt.figure(figsize=(18,15))\nfor i in range(12):\n    plt.subplot(4,3,(i%12)+1)\n    rnd_idx=np.random.randint(1000)\n    pred=model_arg.predict_classes(test_image[rnd_idx:rnd_idx+1])\n    if test_data.label.values[rnd_idx]==1:\n        if pred==1:\n            plt.title('Predicted :-female \\n actual:- female',fontdict={'size':13})\n        else:\n            plt.title('Predicted :-male \\n actual:- female',fontdict={'size':13})\n    else:\n        if pred==1:\n            plt.title('Predicted :-female \\n actual:- male',fontdict={'size':13})\n        else:\n            plt.title('Predicted :-male \\n actual:- male',fontdict={'size':13})\n    \n    plt.axis('off')\n    plt.imshow(test_image[rnd_idx])","57d1b082":"# function to plot images from urls\n\n\ndef image_gen_prediction(urls):\n    plt.figure(figsize=(20,18))\n    for i,img_url in enumerate(urls):\n        resp = urllib.request.urlopen(img_url)\n        img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n        img=cv2.resize(img,(64,64))\n        img_data=np.reshape(img,(1,64,64,3))\n        img_data=img_data\/255.0\n        pred=model_arg.predict_classes(img_data)\n        if pred==0:\n            out='male eyes'\n        else:\n            out='female eyes'\n        plt.subplot(4,3,(i+1)%12)\n        plt.title('Its predicted as {0}'.format(out))\n        plt.axis('off')\n        plt.imshow(img)","6c8a1859":"image_gen_prediction(male_urls)","bd5962b5":"image_gen_prediction(female_urls)","bffb6465":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">MODEL ON VALIDATION DATASET<\/h1>\n","59ec0651":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">PREDICTION USING AURGUMENTED MODEL<\/h1>\n","974467e8":"## Label : 0 represents Male eyes\n## Label : 1 represents Female eyes","e6b9463a":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">PREDICTING ON RANDOM URLS USING MODEL<\/h1>\n","d10d9f63":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">FUNCTION TO PREDICT CLASS FROM URLS<\/h1>\n","79e0acf5":"\n<h1 style=\"font-family:ALGERIAN;font-size:200%;padding: 15px\">This is a dataset consisting of about 11k images of male eyes and female eyes. Our main objective here is to determine the gender of person by training a model on there eyes. This model can make gender prediction a easier task. We could predict gender of person even if we don't have complete access to the face of the person.<\/h1>\n","955eef15":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">CONCLUSION<\/h1>\n","58290e3f":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">OVERVIEW<\/h1>\n","95ed58df":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">PREDICTION ON RANDOM URLS<\/h1>\n","b96d0d1d":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">MODEL AFTER APPLYING DATA AURGUMENTATION<\/h1>\n","4014e7b7":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">VIEWING TRAINING DATA<\/h1>\n","3140c2a6":"<h1 style=\"color:black;font-family:ALGERIAN;font-size:130%;border-radius: 10px 10px;padding: 15px\">Here we could notice that validation accuracy of our first simple model is high than the model trained on argumented image, but on increasing the epoch accuracy of our second model can increase. So the question arises which model to choose. In my opinion using 2nd model i.e, argumented model can perform better on real world data as it is trained on images of different brightness,different orientation and many others parameters, which makes it efficient on real world data.\n<br><br>What's your thought on this?<\/h1>","e29aefab":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">CALLBACKS FUNCTION<\/h1>\n","6f915b75":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">DATA-SPLIT<\/h1>\n","b5d036d8":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">IMPORTING LIBRARIES<\/h1>\n","712350c0":"<h1 style=\"color:RED;font-family:ALGERIAN;font-size:150%;text-align:center;border-radius: 10px 10px;padding: 15px\">Hope you liked the notebook, if yes please upvote it!! if having any queries or suggestion feel free to ask in comment section.<\/h1>\n","5f61064e":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">LOADING IMAGES<\/h1>\n","65f308e1":"<h1 style=\"background-color:yellow;font-family:ALGERIAN;font-size:350%;text-align:center;border-radius: 15px 50px;padding: 5px\">EYES CLASSIFICATION<\/h1>\n","347469fc":"<h1 style=\"background-color:#3BF9F5;font-family:ALGERIAN;font-size:250%;text-align:center;border-radius: 10px 10px;padding: 5px\">MODEL BUILDING<\/h1>\n","268ef465":"<center><img src=\"https:\/\/i.pinimg.com\/originals\/60\/01\/53\/600153d0abfef397bdb260e23620a47b.jpg\",height='400',width='500'><\/center>\n"}}