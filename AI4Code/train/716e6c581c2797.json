{"cell_type":{"a7295b32":"code","14d22cb1":"code","703e67c7":"code","482f5079":"code","9314f076":"code","eb1e4b0b":"code","a5383907":"code","d163ee3b":"code","0e7ad333":"code","3d475d77":"code","92accdbc":"code","626b58d6":"code","69bfe89b":"code","e5e79520":"code","84983163":"code","2ba737db":"code","93a24359":"code","97d03b85":"code","63a7a46d":"code","e96bd00d":"code","d50368a4":"code","48dd36b7":"code","2409177d":"code","600c304b":"code","7b319d8a":"code","59c08698":"code","b21dc830":"code","54d2d756":"code","7d5311a2":"code","6b48b841":"code","663c143f":"code","d3c6d0a9":"code","b2df4867":"code","6b5a68f5":"code","8e4bda0a":"code","e3ed2cb1":"code","7d79e13b":"code","80838930":"code","234ab4b7":"code","d95079ee":"code","537d3330":"code","094e38a4":"code","b963cf64":"code","622317de":"code","6b9a43b4":"code","5d42e585":"code","83f4117b":"code","15a75ce9":"code","eb5d9fa3":"code","2efc5ba8":"code","82681f27":"code","3725987b":"code","82f6338e":"code","eed87811":"code","0598c726":"code","3a5fcdde":"code","2c144e86":"code","8afcc59b":"code","486d9688":"code","dc30f668":"code","5cd6ac00":"code","2677e36e":"code","68bdb76b":"code","19e52d32":"code","8eb07566":"code","49093e31":"code","150fa7d5":"markdown","aca76fbc":"markdown","ba8242ec":"markdown","39510a6f":"markdown","2fe0a867":"markdown","f0199f79":"markdown","8e01d9a9":"markdown","5433ac72":"markdown","d9fd798a":"markdown","abc3944d":"markdown","b498057c":"markdown","7f1f95da":"markdown","a2668920":"markdown","eed85a53":"markdown","b667de1f":"markdown","f94a7be1":"markdown","e97b2d74":"markdown","6b47e97c":"markdown","f4e2d6cf":"markdown","21beb654":"markdown","f22403ed":"markdown","d476abd4":"markdown","3467f454":"markdown","8c8d9538":"markdown","ce765e7d":"markdown","4cc846f4":"markdown","43b948df":"markdown","63434461":"markdown","7decfe13":"markdown","36d6d64f":"markdown","e2675e87":"markdown","aae5a3bd":"markdown","5f522e95":"markdown","f2905a32":"markdown","9e0d355f":"markdown","27c24da8":"markdown","8500ac71":"markdown","b97f6877":"markdown","2e51fb6b":"markdown","af17a67d":"markdown","f9d6fbe4":"markdown","373dbcce":"markdown","32951e81":"markdown","f2f572f3":"markdown","20b66e4b":"markdown"},"source":{"a7295b32":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nfrom sklearn.decomposition import PCA\n\nfrom nltk.stem.snowball import SnowballStemmer\n\nplt.style.use('ggplot')\n\npd.set_option('display.max_columns', None)\npd.set_option('max_colwidth', None)\npd.set_option('max_rows', None)","14d22cb1":"df = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\nprint(f'The training set has {df.shape[0]} rows and {df.shape[1]} columns')","703e67c7":"df.info()","482f5079":"df.head()","9314f076":"print(f\"We have {df['id'].nunique()} essays\")","eb1e4b0b":"df['full_text'] = df['discourse_text'].groupby(df['id']).transform(lambda x: ' '.join(x)) # obviously we will have duplicates","a5383907":"df.full_text.iloc[0]","d163ee3b":"text_length = df['full_text'].drop_duplicates().apply(len)\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = text_length.plot(kind='hist', color = \"#120f7a\", bins=100)\nax1.set_title('Essay Length Distribution')\nax1.set_xlabel(\"Essay Length\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","0e7ad333":"word_count = df['full_text'].drop_duplicates().apply(lambda x: len(str(x).split()))\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = word_count.plot(kind='hist', color = \"#120f7a\", bins=100)\nax1.set_title('Word Count Distribution')\nax1.set_xlabel(\"Word Count\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","3d475d77":"def get_top_n_words(corpus, n=None, remove_stop_words=False, n_words=1): # if n_words=1 -> unigrams, if n_words=2 -> bigrams..\n    if remove_stop_words:\n        vec = CountVectorizer(stop_words = 'english', ngram_range=(n_words, n_words)).fit(corpus)\n    else:\n        vec = CountVectorizer(ngram_range=(n_words, n_words)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","92accdbc":"common_words = get_top_n_words(df['full_text'].drop_duplicates(), 20, remove_stop_words=True, n_words=1)\nfor word, freq in common_words:\n    print(word, freq)","626b58d6":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","69bfe89b":"common_words = get_top_n_words(df['full_text'].drop_duplicates(), 20, remove_stop_words=True, n_words=2)\nfor word, freq in common_words:\n    print(word, freq)","e5e79520":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","84983163":"common_words = get_top_n_words(df['full_text'].drop_duplicates(), 20, remove_stop_words=True, n_words=3)\nfor word, freq in common_words:\n    print(word, freq)","2ba737db":"df_tmp = pd.DataFrame(common_words, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(10,8))\n\nax1 = df_tmp.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","93a24359":"text_Lead = df[df.discourse_type == 'Lead'].discourse_text.values\ntext_Position = df[df.discourse_type == 'Position'].discourse_text.values\ntext_Evidence = df[df.discourse_type == 'Evidence'].discourse_text.values\ntext_Claim = df[df.discourse_type == 'Claim'].discourse_text.values\ntext_Concluding_Statement = df[df.discourse_type == 'Concluding Statement'].discourse_text.values\ntext_Counterclaim = df[df.discourse_type == 'Counterclaim'].discourse_text.values\ntext_Rebuttal = df[df.discourse_type == 'Rebuttal'].discourse_text.values\n\ncommon_words_Lead = get_top_n_words(text_Lead, 20, remove_stop_words=True, n_words=1)\ncommon_words_Position = get_top_n_words(text_Position, 20, remove_stop_words=True, n_words=1)\ncommon_words_Evidence = get_top_n_words(text_Evidence, 20, remove_stop_words=True, n_words=1)\ncommon_words_Claim = get_top_n_words(text_Claim, 20, remove_stop_words=True, n_words=1)\ncommon_words_Concluding_Statement = get_top_n_words(text_Concluding_Statement, 20, remove_stop_words=True, n_words=1)\ncommon_words_Counterclaim = get_top_n_words(text_Counterclaim, 20, remove_stop_words=True, n_words=1)\ncommon_words_Rebuttal = get_top_n_words(text_Rebuttal, 20, remove_stop_words=True, n_words=1)","97d03b85":"df_tmp_Lead = pd.DataFrame(common_words_Lead, columns = ['text' , 'count'])\ndf_tmp_Position = pd.DataFrame(common_words_Position, columns = ['text' , 'count'])\ndf_tmp_Evidence = pd.DataFrame(common_words_Evidence, columns = ['text' , 'count'])\ndf_tmp_Claim = pd.DataFrame(common_words_Claim, columns = ['text' , 'count'])\ndf_tmp_Concluding_Statement = pd.DataFrame(common_words_Concluding_Statement, columns = ['text' , 'count'])\ndf_tmp_Counterclaim = pd.DataFrame(common_words_Counterclaim, columns = ['text' , 'count'])\ndf_tmp_Rebuttal = pd.DataFrame(common_words_Rebuttal, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Lead.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Lead Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Position.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Position Unigram Distribution')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Evidence.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Evidence Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Claim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Claim Unigram Distribution')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Concluding_Statement.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Concluding Statement Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Counterclaim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Counterclaim Unigram Distribution')\nax2.set_xlabel(\"Unigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Rebuttal.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Rebuttal Unigram Distribution')\nax1.set_xlabel(\"Unigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","63a7a46d":"text_Lead = df[df.discourse_type == 'Lead'].discourse_text.values\ntext_Position = df[df.discourse_type == 'Position'].discourse_text.values\ntext_Evidence = df[df.discourse_type == 'Evidence'].discourse_text.values\ntext_Claim = df[df.discourse_type == 'Claim'].discourse_text.values\ntext_Concluding_Statement = df[df.discourse_type == 'Concluding Statement'].discourse_text.values\ntext_Counterclaim = df[df.discourse_type == 'Counterclaim'].discourse_text.values\ntext_Rebuttal = df[df.discourse_type == 'Rebuttal'].discourse_text.values\n\ncommon_words_Lead = get_top_n_words(text_Lead, 20, remove_stop_words=True, n_words=2)\ncommon_words_Position = get_top_n_words(text_Position, 20, remove_stop_words=True, n_words=2)\ncommon_words_Evidence = get_top_n_words(text_Evidence, 20, remove_stop_words=True, n_words=2)\ncommon_words_Claim = get_top_n_words(text_Claim, 20, remove_stop_words=True, n_words=2)\ncommon_words_Concluding_Statement = get_top_n_words(text_Concluding_Statement, 20, remove_stop_words=True, n_words=2)\ncommon_words_Counterclaim = get_top_n_words(text_Counterclaim, 20, remove_stop_words=True, n_words=2)\ncommon_words_Rebuttal = get_top_n_words(text_Rebuttal, 20, remove_stop_words=True, n_words=2)","e96bd00d":"df_tmp_Lead = pd.DataFrame(common_words_Lead, columns = ['text' , 'count'])\ndf_tmp_Position = pd.DataFrame(common_words_Position, columns = ['text' , 'count'])\ndf_tmp_Evidence = pd.DataFrame(common_words_Evidence, columns = ['text' , 'count'])\ndf_tmp_Claim = pd.DataFrame(common_words_Claim, columns = ['text' , 'count'])\ndf_tmp_Concluding_Statement = pd.DataFrame(common_words_Concluding_Statement, columns = ['text' , 'count'])\ndf_tmp_Counterclaim = pd.DataFrame(common_words_Counterclaim, columns = ['text' , 'count'])\ndf_tmp_Rebuttal = pd.DataFrame(common_words_Rebuttal, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Lead.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Lead Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Position.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Position Bigram Distribution')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Evidence.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Evidence Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Claim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Claim Bigram Distribution')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Concluding_Statement.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Concluding Statement Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Counterclaim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Counterclaim Bigram Distribution')\nax2.set_xlabel(\"Bigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Rebuttal.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Rebuttal Bigram Distribution')\nax1.set_xlabel(\"Bigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","d50368a4":"text_Lead = df[df.discourse_type == 'Lead'].discourse_text.values\ntext_Position = df[df.discourse_type == 'Position'].discourse_text.values\ntext_Evidence = df[df.discourse_type == 'Evidence'].discourse_text.values\ntext_Claim = df[df.discourse_type == 'Claim'].discourse_text.values\ntext_Concluding_Statement = df[df.discourse_type == 'Concluding Statement'].discourse_text.values\ntext_Counterclaim = df[df.discourse_type == 'Counterclaim'].discourse_text.values\ntext_Rebuttal = df[df.discourse_type == 'Rebuttal'].discourse_text.values\n\ncommon_words_Lead = get_top_n_words(text_Lead, 20, remove_stop_words=True, n_words=3)\ncommon_words_Position = get_top_n_words(text_Position, 20, remove_stop_words=True, n_words=3)\ncommon_words_Evidence = get_top_n_words(text_Evidence, 20, remove_stop_words=True, n_words=3)\ncommon_words_Claim = get_top_n_words(text_Claim, 20, remove_stop_words=True, n_words=3)\ncommon_words_Concluding_Statement = get_top_n_words(text_Concluding_Statement, 20, remove_stop_words=True, n_words=3)\ncommon_words_Counterclaim = get_top_n_words(text_Counterclaim, 20, remove_stop_words=True, n_words=3)\ncommon_words_Rebuttal = get_top_n_words(text_Rebuttal, 20, remove_stop_words=True, n_words=3)","48dd36b7":"df_tmp_Lead = pd.DataFrame(common_words_Lead, columns = ['text' , 'count'])\ndf_tmp_Position = pd.DataFrame(common_words_Position, columns = ['text' , 'count'])\ndf_tmp_Evidence = pd.DataFrame(common_words_Evidence, columns = ['text' , 'count'])\ndf_tmp_Claim = pd.DataFrame(common_words_Claim, columns = ['text' , 'count'])\ndf_tmp_Concluding_Statement = pd.DataFrame(common_words_Concluding_Statement, columns = ['text' , 'count'])\ndf_tmp_Counterclaim = pd.DataFrame(common_words_Counterclaim, columns = ['text' , 'count'])\ndf_tmp_Rebuttal = pd.DataFrame(common_words_Rebuttal, columns = ['text' , 'count'])\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Lead.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Lead Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Position.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Position Trigram Distribution')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Evidence.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Evidence Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Claim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Claim Trigram Distribution')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Concluding_Statement.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Concluding Statement Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nax2 = fig.add_subplot(122)\nax2 = df_tmp_Counterclaim.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n   kind='bar', color = \"#120f7a\")\nax2.set_title('Counterclaim Trigram Distribution')\nax2.set_xlabel(\"Trigrams\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(15,6))\n\nax1 = fig.add_subplot(121)\nax1 = df_tmp_Rebuttal.groupby('text').sum()['count'].sort_values(ascending=False).plot(\n    kind='bar', color = \"#120f7a\")\nax1.set_title('Rebuttal Trigram Distribution')\nax1.set_xlabel(\"Trigrams\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","2409177d":"del df_tmp_Lead, df_tmp_Position, df_tmp_Evidence, df_tmp_Claim, df_tmp_Concluding_Statement, df_tmp_Counterclaim, df_tmp_Rebuttal, common_words_Lead, common_words_Position, common_words_Evidence, common_words_Claim, common_words_Concluding_Statement, common_words_Counterclaim,common_words_Rebuttal ","600c304b":"df_grouped = df.groupby('id').size().to_frame('size')\ndf_grouped.reset_index(inplace=True)","7b319d8a":"fig = plt.figure(figsize=(10,8))\n\nax1 = df_grouped['size'].plot(kind='hist', color = \"#120f7a\", bins=50)\nax1.set_title('Distribution of Different Part of Text for Each Essay')\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","59c08698":"fig = plt.figure(figsize=(10,8))\n\nax1 = df['discourse_type'].value_counts().plot(kind='bar', color = '#120f7a')\nax1.set_title('Distribution of Different Part of Text for Each Essay')\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","b21dc830":"def color_func(word, font_size, position, orientation, random_state=None, hsl=[242, 70, 20],\n                    **kwargs):\n    return f\"hsl({hsl[0]}, {random.randint(hsl[1]-10, hsl[1]+10)}%, {random.randint(hsl[2]-10, hsl[1]+10)}%)\"","54d2d756":"wc_text_Lead = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_Position = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_Evidence = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_Claim = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_Concluding_Statement = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_Counterclaim = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\nwc_text_Rebuttal = WordCloud(background_color=\"#fff\",max_words=1000,stopwords=set(STOPWORDS))\n\n\nwc_text_Lead.generate(\" \".join(text_Lead))\nwc_text_Position.generate(\" \".join(text_Position))\nwc_text_Evidence.generate(\" \".join(text_Evidence))\nwc_text_Claim.generate(\" \".join(text_Claim))\nwc_text_Concluding_Statement.generate(\" \".join(text_Concluding_Statement))\nwc_text_Counterclaim.generate(\" \".join(text_Counterclaim))\nwc_text_Rebuttal.generate(\" \".join(text_Rebuttal))\n\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_text_Lead.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Lead\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_text_Position.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Position\", fontsize=20)\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_text_Evidence.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Evidence\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_text_Claim.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Claim\", fontsize=20)\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_text_Concluding_Statement.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Concluding Statement\", fontsize=20)\n\nax2 = fig.add_subplot(122)\nax2 = plt.imshow(wc_text_Counterclaim.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax2 = plt.title(\"Counterclaim\", fontsize=20)\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = plt.imshow(wc_text_Rebuttal.recolor(color_func=color_func, random_state=42),\n           interpolation=\"bilinear\")\nax1 = plt.title(\"Concluding Rebuttal\", fontsize=20)\n\nplt.show()","7d5311a2":"df_f = os.listdir('..\/input\/feedback-prize-2021\/train')\n\nfor file in range(len(df_f)):\n    df_f[file] = str('..\/input\/feedback-prize-2021\/train') + \"\/\" +  str(df_f[file])","6b48b841":"j = 40\nents = []\nfor i, row in df[df['id'] == df_f[j][35:-4]].iterrows():\n    ents.append({\n                    'start': int(row['discourse_start']), \n                     'end': int(row['discourse_end']), \n                     'label': row['discourse_type']\n                })\nwith open(df_f[j], 'r') as file: data = file.read()\n\ndoc2 = {\n    \"text\": data,\n    \"ents\": ents,\n}\ncols = {'Lead': '#dad1f6','Position': '#f9d5de','Claim': '#adcfad','Evidence': '#fbbf9a','Counterclaim': '#bdf2fa','Concluding Statement': '#eea69e','Rebuttal': '#d1f8f4'}\noptions = {\"ents\": df.discourse_type.unique().tolist(), \"colors\": cols}\ndisplacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);","663c143f":"text_Lead = ' '.join(df[df.discourse_type == 'Lead'].discourse_text.sample(2000, random_state=42))\ntext_Position = ' '.join(df[df.discourse_type == 'Position'].discourse_text.sample(2000, random_state=42))\ntext_Evidence = ' '.join(df[df.discourse_type == 'Evidence'].discourse_text.sample(2000, random_state=42))\ntext_Claim = ' '.join(df[df.discourse_type == 'Claim'].discourse_text.sample(2000, random_state=42))\ntext_Concluding_Statement = ' '.join(df[df.discourse_type == 'Concluding Statement'].discourse_text.sample(2000, random_state=42))\ntext_Counterclaim = ' '.join(df[df.discourse_type == 'Counterclaim'].discourse_text.sample(2000, random_state=42))\ntext_Rebuttal = ' '.join(df[df.discourse_type == 'Rebuttal'].discourse_text.sample(2000, random_state=42))","d3c6d0a9":"doc_Lead = nlp(text_Lead)\ndoc_Position = nlp(text_Position)\ndoc_Evidence = nlp(text_Evidence)\ndoc_Claim = nlp(text_Claim)\ndoc_Concluding_Statement = nlp(text_Concluding_Statement)\ndoc_Counterclaim = nlp(text_Counterclaim)\ndoc_Rebuttal = nlp(text_Rebuttal)","b2df4867":"items_Lead = [X.label_ for X in doc_Lead.ents]\nitems_Position = [X.label_ for X in doc_Position.ents]\nitems_Evidence = [X.label_ for X in doc_Evidence.ents]\nitems_Claim = [X.label_ for X in doc_Claim.ents]\nitems_Concluding_Statement = [X.label_ for X in doc_Concluding_Statement.ents]\nitems_Counterclaim = [X.label_ for X in doc_Counterclaim.ents]\nitems_Rebuttal = [X.label_ for X in doc_Rebuttal.ents]","6b5a68f5":"Counter(items_Lead).most_common(5)","8e4bda0a":"Counter(items_Position).most_common(5)","e3ed2cb1":"Counter(items_Evidence).most_common(5)","7d79e13b":"Counter(items_Claim).most_common(5)","80838930":"Counter(items_Concluding_Statement).most_common(5)","234ab4b7":"Counter(items_Counterclaim).most_common(5)","d95079ee":"Counter(items_Rebuttal).most_common(5)","537d3330":"displacy.render(doc_Lead[500:600], jupyter=True, style='ent')","094e38a4":"displacy.render(doc_Position[500:600], jupyter=True, style='ent')","b963cf64":"displacy.render(doc_Evidence[500:600], jupyter=True, style='ent')","622317de":"displacy.render(doc_Claim[200:300], jupyter=True, style='ent')","6b9a43b4":"displacy.render(doc_Concluding_Statement[500:600], jupyter=True, style='ent')","5d42e585":"displacy.render(doc_Counterclaim[500:600], jupyter=True, style='ent')","83f4117b":"displacy.render(doc_Rebuttal[200:300], jupyter=True, style='ent')","15a75ce9":"df.columns","eb5d9fa3":"labels = df.discourse_type\ntrue_k = np.unique(labels).shape[0]","2efc5ba8":"logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n\nop = OptionParser()\nop.add_option(\n    \"--lsa\",\n    dest=\"n_components\",\n    type=\"int\",\n    help=\"Preprocess documents with latent semantic analysis.\",\n)\nop.add_option(\n    \"--no-minibatch\",\n    action=\"store_false\",\n    dest=\"minibatch\",\n    default=True,\n    help=\"Use ordinary k-means algorithm (in batch mode).\",\n)\nop.add_option(\n    \"--no-idf\",\n    action=\"store_false\",\n    dest=\"use_idf\",\n    default=True,\n    help=\"Disable Inverse Document Frequency feature weighting.\",\n)\nop.add_option(\n    \"--use-hashing\",\n    action=\"store_true\",\n    default=False,\n    help=\"Use a hashing feature vectorizer\",\n)\nop.add_option(\n    \"--n-features\",\n    type=int,\n    default=10000,\n    help=\"Maximum number of features (dimensions) to extract from text.\",\n)\nop.add_option(\n    \"--verbose\",\n    action=\"store_true\",\n    dest=\"verbose\",\n    default=False,\n    help=\"Print progress reports inside k-means algorithm.\",\n)\n\nprint(__doc__)\nop.print_help()\nprint()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules[\"__main__\"], \"__file__\")\n\n\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(\"this script takes no arguments.\")\n    sys.exit(1)\n","82681f27":"print(\"Extracting features from the training dataset using a sparse vectorizer\")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        hasher = HashingVectorizer(\n            n_features=opts.n_features,\n            stop_words=\"english\",\n            alternate_sign=False,\n            norm=None,\n        )\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(\n            n_features=opts.n_features,\n            stop_words=\"english\",\n            alternate_sign=False,\n            norm=\"l2\",\n        )\nelse:\n    vectorizer = TfidfVectorizer(\n        max_df=0.5,\n        max_features=opts.n_features,\n        min_df=2,\n        stop_words=\"english\",\n        use_idf=opts.use_idf,\n    )\nX = vectorizer.fit_transform(df.discourse_text)\n\nprint(\"done in %fs\" % (time() - t0))\nprint(\"n_samples: %d, n_features: %d\" % X.shape)\nprint()","3725987b":"if opts.n_components:\n    print(\"Performing dimensionality reduction using LSA\")\n    t0 = time()\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(\"done in %fs\" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(\n        \"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100))\n    )\n\n    print()\n\n\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(\n        n_clusters=true_k,\n        init=\"k-means++\",\n        n_init=1,\n        init_size=1000,\n        batch_size=1000,\n        verbose=opts.verbose,\n    )\nelse:\n    km = KMeans(\n        n_clusters=true_k,\n        init=\"k-means++\",\n        max_iter=100,\n        n_init=1,\n        verbose=opts.verbose,\n    )\n\nprint(\"Clustering sparse data with %s\" % km)\nt0 = time()\nkm.fit(X)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\nprint(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\n    \"Silhouette Coefficient: %0.3f\"\n    % metrics.silhouette_score(X, km.labels_, sample_size=1000)\n)\n\nprint()\n\n\nif not opts.use_hashing:\n    print(\"Top terms per cluster:\")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(\"Cluster %d:\" % i, end=\"\")\n        for ind in order_centroids[i, :10]:\n            print(\" %s\" % terms[ind], end=\"\")\n        print()\n","82f6338e":"result={'cluster':km.labels_,'text':df.discourse_text}\nresult=pd.DataFrame(result)","eed87811":"for k in range(0,true_k):\n    s=result[result.cluster==k]\n    text=s['text'].str.cat(sep=' ')\n    text=text.lower()\n    text=' '.join([word for word in text.split()])\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n    print('Cluster: {}'.format(k))\n    plt.figure()\n    plt.imshow(wordcloud.recolor(color_func=color_func, random_state=42), interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\n","0598c726":"result={'cluster':km.labels_,'discourse_type':df.discourse_type}\nresult=pd.DataFrame(result)","3a5fcdde":"fig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = result[result.cluster == 0]['discourse_type'].value_counts().sort_index().plot(kind='bar', color = '#120f7a')\nax1.set_title('Distribution of Different Discourse Type for Cluster 0')\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"Frequency\")\n\nax1 = fig.add_subplot(122)\nax2 = result[result.cluster == 1]['discourse_type'].value_counts().sort_index().plot(kind='bar', color = '#120f7a')\nax2.set_title('Distribution of Different Discourse Type for Cluster 1')\nax2.set_xlabel(\"\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = result[result.cluster == 2]['discourse_type'].value_counts().sort_index().plot(kind='bar', color = '#120f7a')\nax1.set_title('Distribution of Different Discourse Type for Cluster 2')\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"Frequency\")\n\nax1 = fig.add_subplot(122)\nax2 = result[result.cluster == 3]['discourse_type'].value_counts().sort_index().plot(kind='bar', color = '#120f7a')\nax2.set_title('Distribution of Different Discourse Type for Cluster 3')\nax2.set_xlabel(\"\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = result[result.cluster == 4]['discourse_type'].value_counts().sort_index().plot(kind='bar', color = '#120f7a')\nax1.set_title('Distribution of Different Discourse Type for Cluster 4')\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"Frequency\")\n\nax1 = fig.add_subplot(122)\nax2 = result[result.cluster == 5]['discourse_type'].value_counts().sort_index().plot(kind='bar', color = '#120f7a')\nax2.set_title('Distribution of Different Discourse Type for Cluster 5')\nax2.set_xlabel(\"\")\nax2.set_ylabel(\"Frequency\")\n\nfig = plt.figure(figsize=(20,8))\n\nax1 = fig.add_subplot(121)\nax1 = result[result.cluster == 6]['discourse_type'].value_counts().sort_index().plot(kind='bar', color = '#120f7a')\nax1.set_title('Distribution of Different Discourse Type for Cluster 6')\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"Frequency\")\n\nplt.show()","2c144e86":"discourse_type_list = df.discourse_type.unique().tolist()","8afcc59b":"for d_type in discourse_type_list:\n    total_text = result[result.discourse_type == d_type].shape[0]\n    total_text_cluster_0 = result[(result.cluster == 0)&(result.discourse_type == d_type)].shape[0]\n    total_text_cluster_1 = result[(result.cluster == 1)&(result.discourse_type == d_type)].shape[0]\n    total_text_cluster_2 = result[(result.cluster == 2)&(result.discourse_type == d_type)].shape[0]\n    total_text_cluster_3 = result[(result.cluster == 3)&(result.discourse_type == d_type)].shape[0]\n    total_text_cluster_4 = result[(result.cluster == 4)&(result.discourse_type == d_type)].shape[0]\n    total_text_cluster_5 = result[(result.cluster == 5)&(result.discourse_type == d_type)].shape[0]\n    total_text_cluster_6 = result[(result.cluster == 6)&(result.discourse_type == d_type)].shape[0]\n    print(f' -Total text of type {d_type}: {total_text} \\n'\n         f'- Percentage in cluster 0: {round(total_text_cluster_0\/total_text*100,2)} \\n'\n         f'- Percentage in cluster 1: {round(total_text_cluster_1\/total_text*100,2)} \\n'\n         f'- Percentage in cluster 2: {round(total_text_cluster_2\/total_text*100,2)} \\n'\n         f'- Percentage in cluster 3: {round(total_text_cluster_3\/total_text*100,2)} \\n'\n         f'- Percentage in cluster 4: {round(total_text_cluster_4\/total_text*100,2)} \\n'\n         f'- Percentage in cluster 5: {round(total_text_cluster_5\/total_text*100,2)} \\n'\n         f'- Percentage in cluster 6: {round(total_text_cluster_6\/total_text*100,2)} \\n'\n         )","486d9688":"model = KeyedVectors.load_word2vec_format('..\/input\/word2vecglove6b300d\/word2vec-glove.6B.300d.txt')","dc30f668":"def display_pca_scatterplot(model, words=None, title_text=None, sample=0):\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n        \n    word_vectors = np.array([model[w] for w in words])\n\n    twodim = PCA().fit_transform(word_vectors)[:,:2]\n    \n    plt.figure(figsize=(6,6))\n    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='#120f7a', c='#120f7a')\n    plt.title(title_text)\n    for word, (x,y) in zip(words, twodim):\n        plt.text(x+0.05, y+0.05, word)","5cd6ac00":"common_words = get_top_n_words(df['full_text'].drop_duplicates(), 50, remove_stop_words=True, n_words=1)\nunigrams = []\nfor word, _ in common_words:\n    unigrams.append(word)","2677e36e":"display_pca_scatterplot(model, unigrams, title_text='Unigrams Distribution')","68bdb76b":"number_of_words = 20","19e52d32":"text_Lead = df[df.discourse_type == 'Lead'].discourse_text.values\ntext_Position = df[df.discourse_type == 'Position'].discourse_text.values\ntext_Evidence = df[df.discourse_type == 'Evidence'].discourse_text.values\ntext_Claim = df[df.discourse_type == 'Claim'].discourse_text.values\ntext_Concluding_Statement = df[df.discourse_type == 'Concluding Statement'].discourse_text.values\ntext_Counterclaim = df[df.discourse_type == 'Counterclaim'].discourse_text.values\ntext_Rebuttal = df[df.discourse_type == 'Rebuttal'].discourse_text.values","8eb07566":"common_words_Lead = pd.DataFrame(get_top_n_words(text_Lead, number_of_words, remove_stop_words=True, n_words=1)).iloc[:, 0] \ncommon_words_Position = pd.DataFrame(get_top_n_words(text_Position, number_of_words, remove_stop_words=True, n_words=1)).iloc[:, 0] \ncommon_words_Evidence = pd.DataFrame(get_top_n_words(text_Evidence, number_of_words, remove_stop_words=True, n_words=1)).iloc[:, 0] \ncommon_words_Claim = pd.DataFrame(get_top_n_words(text_Claim, number_of_words, remove_stop_words=True, n_words=1)).iloc[:, 0] \ncommon_words_Concluding_Statement = pd.DataFrame(get_top_n_words(text_Concluding_Statement, number_of_words, remove_stop_words=True, n_words=1)).iloc[:, 0] \ncommon_words_Counterclaim = pd.DataFrame(get_top_n_words(text_Counterclaim, number_of_words, remove_stop_words=True, n_words=1)).iloc[:, 0] \ncommon_words_Rebuttal = pd.DataFrame(get_top_n_words(text_Rebuttal, number_of_words, remove_stop_words=True, n_words=1)).iloc[:, 0] ","49093e31":"display_pca_scatterplot(model, list(common_words_Lead), title_text='Lead Unigram Vectors')\ndisplay_pca_scatterplot(model, list(common_words_Position), title_text='Position Unigram Vectors')\ndisplay_pca_scatterplot(model, list(common_words_Evidence), title_text='Evidence Unigram Vectors')\ndisplay_pca_scatterplot(model, list(common_words_Claim), title_text='Claim Unigram Vectors')\ndisplay_pca_scatterplot(model, list(common_words_Concluding_Statement), title_text='Concluding Statement Unigram Vectors')\ndisplay_pca_scatterplot(model, list(common_words_Counterclaim), title_text='Counterclaim Unigram Vectors')\ndisplay_pca_scatterplot(model, list(common_words_Rebuttal), title_text='Rebuttal Unigram Vectors')\n\nplt.show()","150fa7d5":"Let's create the mapping between the acronyms that spacy assigns and their meaning:","aca76fbc":"#### k-means","ba8242ec":"Let's see how many text for each category there are:","39510a6f":"<a id='distribution_of_top_n_grams_for_each_discourse_type'><\/a>\n## Distribution of top n-grams for each discourse type","2fe0a867":"Let's see for each cluster which discourse types are part of it","f0199f79":"Let's see the length distribution of all essay:","8e01d9a9":"### Spacy abbreviation legend:\n**PERSON** = People, including fictional, **NORP** = Nationalities or religious or political groups, **FAC** = Buildings, airports, highways, bridges etc., \n**ORG** = Companies, agencies, institutions, etc., **GPE** = Countries, cities, states, **LOC** = Non-GPE locations, mountain ranges, bodies of water, \n**PRODUCT** = Objects, vehicles, foods, etc. (Not services), **EVENT** = Named hurricanes, battles, wars, sports events, etc., **WORK_OF_ART** = Titles of books, songs, etc., \n**LAW** = Named documents made into laws, **LANGUAGE** = Any named language, **DATE** = Absolute or relative dates or periods, **TIME** = Times smaller than a day, \n**PERCENT** = Percentage, including \"%\", **MONEY** = Monetary values, including unit, **QUANTITY** = Measurements as of weight or distance, **ORDINAL** = \"firts\", \"second\", etc., \n**CARDINAL** = Numerals that do not fell under another type.","5433ac72":"Find 5 most frequent entities for each discourse type:","d9fd798a":"Let's represent each text as a numeric vector:","abc3944d":"<a id='install_and_import_libraries'><\/a>\n## Install and Import Libraries","b498057c":"#### Bigrams","7f1f95da":"And word count distribution:","a2668920":"<a id='word_count'><\/a>\n## Word Count","eed85a53":"Let's look at the distribution of the number of parts for which an essay has been divided within the dataset:","b667de1f":"Let's see for example the first essay:","f94a7be1":"<a id='preliminary_eda'><\/a>\n# Preliminary Data Exploration","e97b2d74":"The column descriptions are:\n- <span style=\"background-color:#e1e6e3;\">id<\/span> - ID code for essay response\n- <span style=\"background-color:#e1e6e3;\">discourse_id<\/span> - ID code for discourse element\n- <span style=\"background-color:#e1e6e3;\">discourse_start<\/span> - character position where discourse element begins in the essay response\n- <span style=\"background-color:#e1e6e3;\">discourse_end<\/span> - character position where discourse element ends in the essay response\n- <span style=\"background-color:#e1e6e3;\">discourse_text<\/span> - text of discourse element\n- <span style=\"background-color:#e1e6e3;\">discourse_type<\/span> - classification of discourse element\n- <span style=\"background-color:#e1e6e3;\">discourse_type_num<\/span> - enumerated class label of discourse element\n- <span style=\"background-color:#e1e6e3;\">predictionstring<\/span> - the word indices of the training sample, as required for predictions","6b47e97c":"<a id='eda'><\/a>\n# Exploratory Data Analysis","f4e2d6cf":"<a id='text_clustering'><\/a>\n## Text Clustering","21beb654":"<a id='text_distribution'><\/a>\n## Text Distribution","f22403ed":"<a id='ner'><\/a>\n## NER: Name Entity Recognition","d476abd4":"<a id='general_dataset_information'><\/a>\n## General Dataset Information","3467f454":"#### Trigrams","8c8d9538":"Merge 2000 random texts from each discourse type:","ce765e7d":"#### Bigrams","4cc846f4":"<b>Competition URL<\/b>:\nhttps:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\n\n<b style='background-color:#fbffb3'>I will try to keep this notebook updated as much as possible<\/b>\n\n## Table of Contents\n* [Dataset Information](#data_information)\n    - [Data description](#data_description)\n    - [Files](#files)\n* [Preliminary Data Exploration](#preliminary_eda)\n    - [Install and Import Libraries](#install_and_import_libraries)\n    - [Load Data](#load_data)\n    - [General Dataset Information](#general_dataset_information)\n* [Exploratory Data Analysis](#eda)\n    - [Text Length](#text_length)\n    - [Word Count](#word_count)\n    - [Distribution of top n-grams](#distribution_of_top_n_grams)\n    - [Distribution of top n-grams for each discourse type](#distribution_of_top_n_grams_for_each_discourse_type)\n    - [Text Distribution](#text_distribution)\n    - [Word Clouds](#word_clouds)\n    - [Displaying Labels](#displaying_labels)\n    - [NER: Name Entity Recognition](#ner)\n    - [Text Clustering](#text_clustering)\n    - [Word Embeddings Visualization](#w2vec)","43b948df":"Let's show the word vectors representation for each class in the principal components space","63434461":"<a id='load_data'><\/a>\n## Load Data","7decfe13":"Let's display most common unigrams:","36d6d64f":"<a id='data_information'><\/a>\n# Dataset Information","e2675e87":"#### Unigrams","aae5a3bd":"![cover.png](attachment:bc3705e5-7e7a-439b-a342-da3a92ca2fa0.png)","5f522e95":"<div>The dataset contains argumentative essays written by U.S students in grades 6-12. The essays were annotated by expert raters for elements commonly found in argumentative writing.<\/br>\nThe task is to predict the human annotations. You will first need to segment each essay into discrete rhetorical and argumentative elements (i.e., discourse elements) and then classify each element as one of the following:\n\n<span style=\"background-color:#e1e6e3;\">Lead<\/span> - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader\u2019s attention and point toward the thesis<\/br>\n<span style=\"background-color:#e1e6e3;\">Position<\/span> - an opinion or conclusion on the main question<\/br>\n<span style=\"background-color:#e1e6e3;\">Claim<\/span> - a claim that supports the position<\/br>\n<span style=\"background-color:#e1e6e3;\">Counterclaim<\/span> - a claim that refutes another claim or gives an opposing reason to the position<\/br>\n<span style=\"background-color:#e1e6e3;\">Rebuttal<\/span> - a claim that refutes a counterclaim<\/br>\n<span style=\"background-color:#e1e6e3;\">Evidence<\/span> - ideas or examples that support claims, counterclaims, or rebuttals.<\/br>\n<span style=\"background-color:#e1e6e3;\">Concluding Statement<\/span> - a concluding statement that restates the claims<\/br>\n<\/br>\n<b>Important<\/b>:\nNote that some parts of the essays will be unannotated (i.e., they do not fit into one of the classifications above).\n\n<a id='files'><\/a>\n<h3>Files<\/h3>\n<span style=\"background-color:#e1e6e3;\">train.zip<\/span> - folder of individual .txt files, with each file containing the full text of an essay response in the training set <\/br>\n<span style=\"background-color:#e1e6e3;\">train.csv<\/span> - file containing the annotated version of all essays in the training set <\/br>\n<span style=\"background-color:#e1e6e3;\">test.zip<\/span> - folder of individual .txt files, with each file containing the full text of an essay response in the test set <\/br>\n<span style=\"background-color:#e1e6e3;\">sample_submission.csv<\/span> - file in the required format for making predictions - note that if you are making multiple predictions for a document, submit multiple rows <\/br><\/div>","f2905a32":"For this part (Displaying labels) I took inspiration from: https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda, upvote for this notebook too! :) ","9e0d355f":"<a id='word_clouds'><\/a>\n## Word Clouds","27c24da8":"<a id='displaying_labels'><\/a>\n## Displaying Labels","8500ac71":"<a id=\"w2vec\"><\/a>\n## Word Embeddings Visualization","b97f6877":"<a id='text_length'><\/a>\n## Text Length","2e51fb6b":"Percentage of each discourse type by cluster:","af17a67d":"#### Unigrams","f9d6fbe4":"#### Trigrams","373dbcce":"Let's visualize the entities for some parts of text for each discourse type:","32951e81":"<a id='data_description'><\/a>\n## Data Description","f2f572f3":"We can see that there are some parts of the text that are not classified.","20b66e4b":"<a id='distribution_of_top_n_grams'><\/a>\n## Distribution of top n-grams for full-text essays"}}