{"cell_type":{"abd78913":"code","6d616dd3":"code","00ba3e6d":"code","c527821f":"code","77236fea":"code","0ba735c0":"code","fbf53f46":"code","b7060aa4":"code","796ac851":"code","07176769":"code","4caf3ea4":"code","20807e36":"code","7614642a":"code","253ee625":"code","2537e881":"code","819c18d3":"code","abcec70b":"code","3fc7c3ed":"code","c68f84ed":"code","a4a6fa4f":"code","68cb4ac3":"code","f04031eb":"code","908c3ddb":"code","63de01fd":"code","138da281":"code","107050a4":"code","d03c1bd0":"code","64b67044":"code","2325b562":"code","e9ce71b7":"code","cd665096":"code","2944549a":"code","6619d859":"code","e1170578":"markdown","6585cddd":"markdown","b0c3db21":"markdown"},"source":{"abd78913":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d616dd3":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nprint(tf.__version__)","00ba3e6d":"csv_data=pd.read_csv('\/kaggle\/input\/image-captioning2040-images\/file.csv')\nprint(csv_data.shape)\ncsv_data.head()","c527821f":"csv_data.drop(['image'],axis=1,inplace=True)\ncsv_data.head()","77236fea":"print('Image Count : ',len(os.listdir('\/kaggle\/input\/image-captioning2040-images\/file (1)\/content\/images')))\nprint((np.unique(csv_data.index)).shape)","0ba735c0":"csv_data['index']='image'+csv_data['index'].apply(str)+'.jpg'\ncsv_data.head()","fbf53f46":"import re","b7060aa4":"short_forms = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","796ac851":"def preprocessText(text):\n    \n    txt=text.lower()\n    txt=re.sub(r'[<>\\:;?@#$%&\\&\\^*\\(\\)\\\\!\\+\/\\[\\]]','',txt)\n    txt=' '.join([short_forms[i] if i in short_forms.keys() else i for i in txt.split()])\n    txt=re.sub(r\"'s'\",'',txt)\n    txt=re.sub('\"','',txt)\n    txt=re.sub(r'[\\.,_-]',' ',txt)\n    txt=' '.join(i for i in txt.split() if i.isalpha())\n    txt=re.sub(r'[^a-zA-Z ]','',txt)\n    \n    return txt","07176769":"csv_data['caption']='<start> '+csv_data['caption'].apply(preprocessText)+' <end>'\ncsv_data['caption'][0]","4caf3ea4":"Images=os.listdir('\/kaggle\/input\/image-captioning2040-images\/file (1)\/content\/images')\nImages[:5]","20807e36":"train_size=0.7\ntrain_images=Images[:round(0.7*len(Images))]\ntest_images=Images[round(0.7*len(Images)):]\nprint(len(train_images),len(test_images))\n","7614642a":"train_captions=[csv_data[csv_data['index']==i]['caption'].values[0] for i in train_images]\ntest_captions=[csv_data[csv_data['index']==i]['caption'].values[0] for i in test_images]\nprint(len(train_captions),len(test_captions))","253ee625":"import cv2","2537e881":"\nimg=cv2.imread('\/kaggle\/input\/image-captioning2040-images\/file (1)\/content\/images\/'+train_images[5])\ncvt_img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\ncv2.putText(cvt_img,train_captions[5],(10,40),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),1,cv2.LINE_AA)\nplt.imshow(cvt_img)\n    ","819c18d3":"heights=[]\nwidths=[]\nfor img in train_images:\n    h,w=(cv2.imread('\/kaggle\/input\/image-captioning2040-images\/file (1)\/content\/images\/'+img,0)).shape\n    heights.append(h)\n    widths.append(w)\n\nprint('Median height : ',np.median(np.array(heights)),'widths : ',np.median(np.array(widths)))","abcec70b":"median_height=400\nmedian_width=600","3fc7c3ed":"max_cap_len=max([len(i.split()) for i in csv_data['caption']])\nprint(max_cap_len)","c68f84ed":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv2D,Dense,MaxPooling2D,BatchNormalization,Dropout,Flatten,LSTM,Embedding,Bidirectional,Input,TimeDistributed\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.utils import plot_model,to_categorical","a4a6fa4f":"vgg16_model=VGG16()\nvgg16_model.summary()","68cb4ac3":"train_images_array=np.zeros((len(train_images),224,224,3),dtype=np.float32)\ntest_images_array=np.zeros((len(test_images),224,224,3),dtype=np.float32)\n\nfor i in range(len(train_images)):\n    \n    tr_image=cv2.imread('\/kaggle\/input\/image-captioning2040-images\/file (1)\/content\/images\/'+train_images[i])\n    cvt_tr_image=cv2.cvtColor(tr_image,cv2.COLOR_BGR2RGB)\n    image=cv2.resize(cvt_tr_image,(224,224))\n    train_images_array[i,:]=image\/255.0\n    \nfor i in range(len(test_images)):\n    \n    te_image=cv2.imread('\/kaggle\/input\/image-captioning2040-images\/file (1)\/content\/images\/'+test_images[i])\n    cvt_te_image=cv2.cvtColor(te_image,cv2.COLOR_BGR2RGB)\n    image=cv2.resize(cvt_te_image,(224,224))\n    test_images_array[i,:]=image\/255.0\n    \nprint(train_images_array.shape,test_images_array.shape)\n     ","f04031eb":"conv_model=Model(vgg16_model.inputs,vgg16_model.layers[-2].output)\nconv_model.summary()","908c3ddb":"from tqdm import tqdm\n\nX_train_img=np.zeros((len(train_images),(conv_model.output.shape)[1]),dtype=np.float32)\nX_val_img=np.zeros((len(test_images),(conv_model.output.shape)[1]),dtype=np.float32)\n\nfor i in tqdm(range(len(train_images))):\n    \n    img_feat=conv_model(train_images_array[i,:].reshape(1,224,224,3))\n    img_feat=img_feat.numpy()\n    X_train_img[i,:]=img_feat\n\nfor i in tqdm(range(len(test_images))):\n    \n    img_feat=conv_model(test_images_array[i,:].reshape(1,224,224,3))\n    img_feat=img_feat.numpy()\n    X_val_img[i,:]=img_feat\n    ","63de01fd":"print(X_train.shape,X_val.shape)","138da281":"tokenizer=Tokenizer(oov_token='<UNK>',)\ntokenizer.fit_on_texts(train_captions)\n\nX_train_cap=tokenizer.texts_to_sequences(train_captions)\nX_val_cap=tokenizer.texts_to_sequences(test_captions)\n\ncap_vocab_size=len(tokenizer.word_index)+1\nprint(cap_vocab_size)\nprint(len(X_train_cap),len(X_val_cap))","107050a4":"\nInputs1,Inputs2,Outputs=[],[],[]\n\nfor i in tqdm(range(0,len(X_train_cap))):\n    \n    seq=X_train_cap[i]\n    #print(len(seq))\n    for j in range(1,len(X_train_cap[i])):\n        #print(j)\n        Inputs1.append(X_train_img[i])\n        inp_seq,out_seq=seq[:j],seq[j]\n        pad_inp_seq=pad_sequences([inp_seq],maxlen=max_cap_len)[0]\n        out_seq=to_categorical(out_seq,num_classes=cap_vocab_size)\n        \n        Inputs2.append(pad_inp_seq)\n        Outputs.append(out_seq)\n        \nInputs1,Inputs2,Outputs=np.array(Inputs1),np.array(Inputs2),np.array(Outputs)\n\nprint(Inputs1.shape,Inputs2.shape,Outputs.shape)\n        \n    ","d03c1bd0":"\nInputs3,Inputs4,Outputs2=[],[],[]\n\nfor i in tqdm(range(0,len(X_val_cap))):\n    \n    seq=X_val_cap[i]\n    #print(len(seq))\n    for j in range(1,len(X_val_cap[i])):\n        #print(j)\n        Inputs3.append(X_val_img[i])\n        inp_seq,out_seq=seq[:j],seq[j]\n        pad_inp_seq=pad_sequences([inp_seq],maxlen=max_cap_len)[0]\n        out_seq=to_categorical(out_seq,num_classes=cap_vocab_size)\n        \n        Inputs4.append(pad_inp_seq)\n        Outputs2.append(out_seq)\n        \nInputs3,Inputs4,Outputs2=np.array(Inputs3),np.array(Inputs4),np.array(Outputs2)\n\nprint(Inputs3.shape,Inputs4.shape,Outputs2.shape)\n        \n    ","64b67044":"\nenc_inputs=Input(shape=(4096,),name='Enc_Inputs')\ndense1=Dense(300,activation='relu',name='Dense1')\nenc_out=dense1(enc_inputs)\n#enc_output=tf.keras.layers.RepeatVector(1,name='Enc_Output')(dense1_out)\n\ndec_inputs=Input(shape=(max_cap_len,),name='Dec_Inputs')\nembedding=Embedding(cap_vocab_size,300,trainable=True,name='Dec_Embedding')\ndec_embedding=embedding(dec_inputs)\n\nlstm=LSTM(300,name='Dec_LSTM')\nlstm_out=lstm(dec_embedding)\n\ncombined_outputs=tf.add(enc_out,lstm_out)\n\ndense2=Dense(256,activation='relu',name='Dense2')\ndesnse2_out=dense2(combined_outputs)\n\noutput_layer=Dense(cap_vocab_size,activation='softmax')\noutput=output_layer(desnse2_out)\nmodel=Model([enc_inputs,dec_inputs],output)\n\nmodel.summary()","2325b562":"plot_model(model,show_shapes=True)","e9ce71b7":"model.compile(loss='categorical_crossentropy',optimizer='Adam')","cd665096":"history=model.fit([Inputs1,Inputs2],Outputs,epochs=10,batch_size=64,\n          validation_data=([Inputs3,Inputs4],Outputs2))","2944549a":"def Image_caption(image):\n    \n    caption='start'\n    \n    plt.imshow(image)\n    img=cv2.resize(image,(224,224))\n    img=img\/255.0\n    \n    img_feat=conv_model(img.reshape(1,224,224,3))\n    word='start'\n    \n    while word!='end':\n    \n        start_token=tokenizer.texts_to_sequences([caption])[0]\n\n        pad_seq=pad_sequences([start_token],maxlen=max_cap_len)\n\n        pred_word=model.predict([img_feat,pad_seq])[0]\n        word=tokenizer.index_word[np.argmax(pred_word)]\n        \n        caption+=' '+word\n    \n    return ' '.join([i for i in caption.split() if i not in ['start','end']])\n\n","6619d859":"Image_caption(test_images_array[0])","e1170578":"# Prepare Data","6585cddd":"# Model","b0c3db21":"# Train Test Split"}}