{"cell_type":{"6c9508b3":"code","4c5777e0":"code","2802d84e":"code","a16a165e":"code","72012a36":"code","b21cdd97":"code","52a38d05":"code","ab76b3e3":"code","10dbeb72":"code","76443368":"code","283b869d":"code","702f7d62":"code","c4cf67de":"code","fde1c4b7":"markdown","f9ecf001":"markdown","ab8b57eb":"markdown","d6b0368f":"markdown"},"source":{"6c9508b3":"!pip uninstall typing -y\n!pip install flair","4c5777e0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nsns.set_style('darkgrid')","2802d84e":"data = pd.read_csv(\n    \"..\/input\/entity-annotated-corpus\/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False, \n    usecols=['sentence_idx', 'word', 'tag']\n)\ndata  = data[data['sentence_idx'] != 'prev-lemma'].dropna(subset=['sentence_idx']).reset_index(drop=True)\ndata['sentence_idx'] = data['sentence_idx'].apply(int)\nprint(data.shape)\ndata.head()","a16a165e":"fig, ax = plt.subplots(figsize=(20, 6))\nax.hist(data['sentence_idx'].value_counts().values, bins=50)\nax.set_title('Number of words in each Sentence')\n\nmaxlen = np.max(data['sentence_idx'].value_counts().values)\nprint('Number of Sentences:', data['sentence_idx'].nunique())\nprint ('Maximum sequence length:', maxlen)\n\nwords = list(set(data[\"word\"].values))\nn_words = len(words)\nprint('Number of unique words:', n_words)","72012a36":"fig, ax = plt.subplots(2, 1, figsize=(20, 12))\ndata.tag.value_counts().plot.bar(ax=ax[0], title='Distribution of Tags')\ndata[data.tag != 'O'].tag.value_counts().plot.bar(ax=ax[1], title='Distribution of non-O Tags')\n\ntags = list(set(data[\"tag\"].values))\nn_tags = len(tags)\nprint('Number of unique Tags:', n_tags)","b21cdd97":"mask = data['sentence_idx'].ne(data['sentence_idx'].shift(-1))\ndata1 = pd.DataFrame('',index=mask.index[mask] + .5, columns=data.columns)\n\ndata = pd.concat([data, data1]).sort_index().reset_index(drop=True).iloc[:-1]\ndata[['word', 'tag']].to_csv('data.txt', sep=' ', index=False, header=False)\ndata.head(30)","52a38d05":"from flair.data import Corpus, Sentence\nfrom flair.datasets import ColumnCorpus\nfrom flair.embeddings import WordEmbeddings, StackedEmbeddings, TokenEmbeddings\nfrom flair.models import SequenceTagger\nfrom flair.trainers import ModelTrainer\nfrom typing import List","ab76b3e3":"columns = {0 : 'text', 1 : 'tag'}\ndata_folder = '\/kaggle\/working'\n\ncorpus: Corpus = ColumnCorpus(data_folder, columns, train_file = 'data.txt')","10dbeb72":"tag_type = 'tag'\ntag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)","76443368":"embedding_types : List[TokenEmbeddings] = [WordEmbeddings('glove')]\nembeddings : StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)","283b869d":"tagger : SequenceTagger = SequenceTagger(\n    hidden_size=256, embeddings=embeddings, tag_dictionary=tag_dictionary, \n    tag_type=tag_type, use_crf=True\n)\nprint(tagger)","702f7d62":"trainer : ModelTrainer = ModelTrainer(tagger, corpus)\ntrainer.train('model', learning_rate=0.1, mini_batch_size=32, max_epochs=15)","c4cf67de":"model = SequenceTagger.load('model\/final-model.pt')\n\nsentence = Sentence('I love India')\n\nmodel.predict(sentence)\nprint(sentence.to_tagged_string())","fde1c4b7":"# Preprocessing","f9ecf001":"# Named Entity Recognition (NER)\n\nNER is an information extraction technique to identify and classify named entities in text. These entities can be pre-defined and generic like location names, organizations, time and etc, or they can be very specific like the example with the resume.\n\nThe goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities. This can be broken down into two sub-tasks: identifying the boundaries of the NE, and identifying its type.\n\nNamed entity recognition is a task that is well-suited to the type of classifier-based approach. In particular, a tagger can be built that labels each word in a sentence using the IOB format, where chunks are labelled by their appropriate type.\n\nThe IOB Tagging system contains tags of the form:\n\n* B - {CHUNK_TYPE} \u2013 for the word in the Beginning chunk\n* I - {CHUNK_TYPE} \u2013 for words Inside the chunk\n* O \u2013 Outside any chunk\n\nThe IOB tags are further classified into the following classes \u2013\n\n* geo = Geographical Entity\n* org = Organization\n* per = Person\n* gpe = Geopolitical Entity\n* tim = Time indicator\n* art = Artifact\n* eve = Event\n* nat = Natural Phenomenon\n\n## Approaches to NER\n* **Classical Approaches:** mostly rule-based.\n* **Machine Learning Approaches:** there are two main methods in this category: \n    * Treat the problem as a multi-class classification where named entities are our labels so we can apply different classification algorithms. The problem here is that identifying and labeling named entities require thorough understanding of the context of a sentence and sequence of the word labels in it, which this method ignores that.\n    * Conditional Random Field (CRF) model. It is a probabilistic graphical model that can be used to model sequential data such as labels of words in a sentence. The CRF model is able to capture the features of the current and previous labels in a sequence but it cannot understand the context of the forward labels; this shortcoming plus the extra feature engineering involved with training a CRF model, makes it less appealing to be adapted by the industry.\n* **Deep Learning Approaches:** Bidirectional RNNs","ab8b57eb":"# Modelling","d6b0368f":"# EDA"}}