{"cell_type":{"833f39b6":"code","b2ff8e24":"code","14a4f2c8":"code","b59754f2":"code","51881391":"code","c23aa7f1":"code","2c3ebc89":"code","75f1832d":"code","1ebf8b50":"code","2a23415d":"code","7d8dea77":"code","b413912c":"code","10a7591e":"code","f4e16815":"code","1cb89ddc":"code","5cd20c6f":"code","55d92e13":"code","085b8e54":"code","584c73cf":"code","dcb3741d":"markdown","49e79550":"markdown","dbebd44c":"markdown","c69b76c4":"markdown","8bb5c0a3":"markdown","08810ee0":"markdown","1fcac92c":"markdown","f978e728":"markdown","b218a5f5":"markdown","7ee5752a":"markdown"},"source":{"833f39b6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport os\nimport glob\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)","b2ff8e24":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","14a4f2c8":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef calc_wap3(df):\n    a = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1']+ df['ask_size1'])\n    b = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2']+ df['ask_size2'])\n    wap = (a + b) \/ 2\n    return wap\ndef calc_wap4(df):\n    a1 = df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']\n    a2 = df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']\n    b = df['bid_size1'] + df['ask_size1'] + df['bid_size2']+ df['ask_size2']    \n    wap = (a1 + a2)\/ b\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","b59754f2":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap3'] = calc_wap3(df)\n    df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n    \n    df['wap4'] = calc_wap4(df)\n    df['log_return4'] = df.groupby('time_id')['wap4'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    df['wap_balance2'] = abs(df['wap3'] - df['wap4'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'log_return3':[realized_volatility],\n        'log_return4':[realized_volatility],\n        'wap_balance':[np.mean],\n        'wap_balance2':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature","51881391":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","c23aa7f1":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","2c3ebc89":"train = pd.read_csv(data_dir + 'train.csv')\ntrain_ids = train.stock_id.unique()\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","75f1832d":"test = pd.read_csv(data_dir + 'test.csv')\ntest_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')","1ebf8b50":"from sklearn.model_selection import KFold\n\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","2a23415d":"import lightgbm as lgbm","7d8dea77":"df_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)\nX = df_train.drop(['row_id','target'],axis=1)\ny = df_train['target']","b413912c":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False","10a7591e":"import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\nimport optuna\n\ndef objective(trial):\n    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid = lgb.Dataset(valid_x, label=valid_y)\n\n    param = {\n        \"objective\": \"rmse\", \n        \"metric\": \"rmse\", \n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    result = rmspe(valid_y, pred_labels)\n    return result\n\n\"\"\"\nstudy = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\"\n    )\nstudy.optimize(objective, n_trials=500)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\"\"\"","f4e16815":"params = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      \"early_stopping_rounds\": 30,\n      \"lambda_l1\": 4.2101866218814175e-07,\n      \"lambda_l2\": 0.0038091134937949717,\n      \"num_leaves\": 106,\n      \"feature_fraction\": 0.9870522726482065,\n      \"bagging_fraction\": 0.6229667658481408,\n      \"bagging_freq\": 3,\n      \"min_child_samples\": 72\n  }","1cb89ddc":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=20, random_state=98,shuffle=True)\noof = pd.DataFrame()                \nmodels = []                       \nscores = 0.0                         ","5cd20c6f":"%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold :\", fold+1)\n    \n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    weights = 1\/np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n\n    weights = 1\/np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n    \n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100,\n                      categorical_feature = ['stock_id']                \n                     )\n   \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n\n    scores += RMSPE \/ 5\n    models.append(model)\n    print(\"*\" * 100)","55d92e13":"scores","085b8e54":"y_pred = df_test[['row_id']]\nX_test = df_test.drop(['time_id', 'row_id'], axis = 1)\n\ntarget = np.zeros(len(X_test))\n\nfor model in models:\n    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)\n    target += pred \/ len(models)","584c73cf":"y_pred = y_pred.assign(target = target)\ny_pred.to_csv('submission.csv',index = False)\ny_pred","dcb3741d":"## Main function for preprocessing book data","49e79550":"## Combined preprocessor function","dbebd44c":"## Functions for preprocess","c69b76c4":"## LightGBM","8bb5c0a3":"## Target encoding by stock_id","08810ee0":"# Test set","1fcac92c":"## Training set","f978e728":"### Cross Validation","b218a5f5":"## Main function for preprocessing trade data","7ee5752a":"## Test set"}}