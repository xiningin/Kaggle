{"cell_type":{"934fcd04":"code","f6c52efb":"code","39002a3f":"code","83429231":"code","0a437a75":"code","f6cda34c":"code","6dcdb0db":"code","dacd33d9":"code","da40ac89":"code","c88cddce":"code","cbd568dd":"code","b605bd72":"code","8a543b8e":"code","4d597584":"code","7d17012b":"code","5fec2d1f":"code","e6991896":"code","00dd67d5":"code","ff066498":"markdown","8bb06c6c":"markdown","7fa9e65f":"markdown","a4b876ac":"markdown","ed7f1376":"markdown","cfa2e3a5":"markdown","d86e2022":"markdown"},"source":{"934fcd04":"import os\nimport random\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.notebook import tqdm\nimport torchvision.models as models\nfrom PIL import *\n\n%matplotlib inline","f6c52efb":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","39002a3f":"INPUT_DIR = '..\/input\/sugiura-lab-first-competition\/'\n\nPATH = {\n    'train': os.path.join(INPUT_DIR, 'train.csv'),\n    'sample_submission': os.path.join(INPUT_DIR, 'sample_submission.csv'),\n    'train_image_dir': os.path.join(INPUT_DIR, 'train_images\/train_images'),\n    'test_image_dir': os.path.join(INPUT_DIR, 'test_images\/test_images'),\n}\n\nID = 'fname'\nTARGET = 'label'\n\nSEED = 42\nseed_everything(SEED)\n\n# GPU settings for PyTorch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Parameters for neural network.\nPARAMS = {\n    'valid_size': 0.2,\n    'batch_size': 128,\n    'epochs': 10,\n    'lr': 0.0005,\n    'valid_batch_size': 256,\n    'test_batch_size': 256,\n}","83429231":"train_df = pd.read_csv(PATH['train'])\nsample_submission_df = pd.read_csv(PATH['sample_submission'])","0a437a75":"class KMNISTDataset(Dataset):\n    def __init__(self, fname_list, label_list, image_dir, transform=None):\n        super().__init__()\n        self.fname_list = fname_list\n        self.label_list = label_list\n        self.image_dir = image_dir\n        self.transform = transform\n    \n    # \u30c7\u30fc\u30bf\u6570\u3092\u8fd4\u3059\u3088\u3046\u306b\u8a2d\u5b9a\u3059\u308b\n    def __len__(self):\n        return len(self.fname_list)\n    \n    # \u30c7\u30fc\u30bf\u3068\u305d\u308c\u306b\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eb\u3092\u8fd4\u3059\u95a2\u6570\n    def __getitem__(self, idx):\n        fname = self.fname_list[idx]\n        label = self.label_list[idx]\n        \n        image = cv2.imread(os.path.join(self.image_dir, fname))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        if self.transform is not None:\n            image = self.transform(image)\n        \n        return image, label","f6cda34c":"def accuracy_score_torch(y_pred, y):\n    y_pred = torch.argmax(y_pred, axis=1).cpu().numpy()\n    y = y.cpu().numpy()\n\n    return accuracy_score(y_pred, y)","6dcdb0db":" \nHalf_width =128\nlayer_width =128\n    \nclass SpinalVGG(nn.Module):  \n    \"\"\"\n    Based on - https:\/\/github.com\/kkweon\/mnist-competition\n    from: https:\/\/github.com\/ranihorev\/Kuzushiji_MNIST\/blob\/master\/KujuMNIST.ipynb\n    \"\"\"\n    def two_conv_pool(self, in_channels, f1, f2):\n        s = nn.Sequential(\n            nn.Conv2d(in_channels, f1, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(f1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(f1, f2, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(f2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        for m in s.children():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        return s\n    \n    def three_conv_pool(self,in_channels, f1, f2, f3):\n        s = nn.Sequential(\n            nn.Conv2d(in_channels, f1, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(f1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(f1, f2, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(f2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(f2, f3, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(f3),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n        for m in s.children():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        return s\n        \n    \n    def __init__(self, num_classes=10):\n        super(SpinalVGG, self).__init__()\n        self.l1 = self.two_conv_pool(1, 64, 64)\n        self.l2 = self.two_conv_pool(64, 128, 128)\n        self.l3 = self.three_conv_pool(128, 256, 256, 256)\n        self.l4 = self.three_conv_pool(256, 256, 256, 256)\n        \n        \n        self.fc_spinal_layer1 = nn.Sequential(\n            nn.Dropout(p = 0.5), nn.Linear(Half_width, layer_width),\n            nn.BatchNorm1d(layer_width), nn.ReLU(inplace=True),)\n        self.fc_spinal_layer2 = nn.Sequential(\n            nn.Dropout(p = 0.5), nn.Linear(Half_width+layer_width, layer_width),\n            nn.BatchNorm1d(layer_width), nn.ReLU(inplace=True),)\n        self.fc_spinal_layer3 = nn.Sequential(\n            nn.Dropout(p = 0.5), nn.Linear(Half_width+layer_width, layer_width),\n            nn.BatchNorm1d(layer_width), nn.ReLU(inplace=True),)\n        self.fc_spinal_layer4 = nn.Sequential(\n            nn.Dropout(p = 0.5), nn.Linear(Half_width+layer_width, layer_width),\n            nn.BatchNorm1d(layer_width), nn.ReLU(inplace=True),)\n        self.fc_out = nn.Sequential(\n            nn.Dropout(p = 0.5), nn.Linear(layer_width*4, num_classes),)\n        \n    \n    def forward(self, x):\n        x = self.l1(x)\n        x = self.l2(x)\n        x = self.l3(x)\n        x = self.l4(x)\n        x = x.view(x.size(0), -1)\n        \n        x1 = self.fc_spinal_layer1(x[:, 0:Half_width])\n        x2 = self.fc_spinal_layer2(torch.cat([ x[:,Half_width:2*Half_width], x1], dim=1))\n        x3 = self.fc_spinal_layer3(torch.cat([ x[:,0:Half_width], x2], dim=1))\n        x4 = self.fc_spinal_layer4(torch.cat([ x[:,Half_width:2*Half_width], x3], dim=1))\n        \n        x = torch.cat([x1, x2], dim=1)\n        x = torch.cat([x, x3], dim=1)\n        x = torch.cat([x, x4], dim=1)\n        \n        x = self.fc_out(x)\n\n        return F.log_softmax(x, dim=1)\n\n    \ndevice = 'cuda' ","dacd33d9":"train_df, valid_df = train_test_split(\n    train_df, test_size=PARAMS['valid_size'], random_state=SEED, shuffle=True\n)\ntrain_df = train_df.reset_index(drop=True) # train\u30c7\u30fc\u30bf\nvalid_df = valid_df.reset_index(drop=True) # validation\u30c7\u30fc\u30bf","da40ac89":"import torchvision\n\ntmp_dataset  =  KMNISTDataset(train_df[ID], train_df[TARGET], PATH['train_image_dir'],\n                            transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                             ])\n                             )\ntmp_dataloader = DataLoader(tmp_dataset, batch_size=PARAMS['batch_size'], shuffle=True, num_workers=2)\n\nall_tensor = torch.tensor([])\n\nfor x,y in tmp_dataloader:\n    all_tensor = torch.cat([all_tensor, x],dim = 0)\n    \n\nprint(all_tensor.mean())\nprint(all_tensor.std())","c88cddce":"tmp_dataset2  =  KMNISTDataset(train_df[ID], train_df[TARGET], PATH['train_image_dir'],\n                            transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize((0.1918,), (0.3483,))\n                             ])\n                             )\ntmp_dataloader2 = DataLoader(tmp_dataset2, batch_size=PARAMS['batch_size'], shuffle=True, num_workers=2)\nall_ten2 = torch.tensor([])\n\n\nfor x,y in tmp_dataloader2:\n    all_ten2 = torch.cat([all_ten2, x],dim = 0)\n    \nprint(all_ten2.std(),all_ten2.mean())","cbd568dd":"train_df = pd.read_csv(PATH['train'])\nsample_submission_df = pd.read_csv(PATH['sample_submission'])\n\ntrain_dataset = train_dataset = KMNISTDataset(train_df[ID], train_df[TARGET], PATH['train_image_dir'],\n                            transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.RandomPerspective(), \n                               torchvision.transforms.RandomRotation(10, fill=(0,)), \n                               torchvision.transforms.Normalize((0.1918,), (0.3483,))\n                             ])\n                             )\n\n\n\nvalid_dataset = KMNISTDataset(valid_df[ID], valid_df[TARGET], PATH['train_image_dir'], \n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize((0.1918,), (0.3483,))\n                             ])\n                             )\n\n# \u4f5c\u3063\u305fdataset\u304b\u3089\u30c7\u30fc\u30bf\u3092\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u306b\u307e\u3068\u3081\u3066\u8fd4\u3057\u3066\u304f\u308c\u308bDataloader\u3092\u7528\u610f\u3057\u307e\u3059\ntrain_loader = DataLoader(train_dataset, batch_size=PARAMS['batch_size'], shuffle=True, num_workers=2)\nvalid_loader = DataLoader(valid_dataset, batch_size=PARAMS['valid_batch_size'], shuffle=False, num_workers=2)\n\n","b605bd72":"\ntotal_step = len(train_loader)","8a543b8e":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = SpinalVGG().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),lr=1e-3, weight_decay=1e-5) \n\nbest_accuracy = 0","4d597584":"import warnings\n\nwarnings.simplefilter('ignore')\n\nnum_epochs = 50\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 100 == 99:\n            print (\"Spinal Epoch [{}\/{}], Step [{}\/{}] Loss: {:.4f}\"\n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n        \n    # Test the model\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in valid_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n                        \n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n        if best_accuracy>= correct \/ total:\n            print('Test Accuracy of SpinalNet: {} % Best: {} %'.format(100 * correct \/ total, 100*best_accuracy))\n        else:\n            best_accuracy = correct \/ total\n            net_opt = model\n            print('Test Accuracy of SpinalNet: {} % (improvement)'.format(100 * correct \/ total))\n\n        model.train()","7d17012b":"\ntest_dataset = KMNISTDataset(\n    sample_submission_df[ID],\n    sample_submission_df[TARGET],\n    PATH['test_image_dir'],\n     transform=torchvision.transforms.Compose([\n       torchvision.transforms.ToTensor(),\n       torchvision.transforms.Normalize((0.1918,), (0.3483,))\n     ])\n)\ntest_dataloader = DataLoader(test_dataset, batch_size=PARAMS['test_batch_size'], shuffle=False, num_workers=2)\n","5fec2d1f":"model.eval()\npredictions = []\n\nfor x, _ in test_dataloader:\n    x = x.to(dtype=torch.float32, device=DEVICE)\n    #x = x.repeat(1,3,1,1)\n    \n    with torch.no_grad():\n        y_pred = model(x)\n        y_pred = torch.argmax(y_pred, axis=1).cpu().numpy()\n        y_pred = y_pred.tolist()\n        \n    predictions += y_pred","e6991896":"sample_submission_df[TARGET] = predictions","00dd67d5":"sample_submission_df.to_csv('submission.csv', index=False)\nfrom IPython.display import FileLink\nFileLink('submission.csv')","ff066498":"# \u5b66\u7fd2","8bb06c6c":"# \u4e88\u6e2c","7fa9e65f":"# Dataset,Dataloader\u306e\u5b9a\u7fa9","a4b876ac":"# mean,std\u3092\u8a08\u7b97","ed7f1376":"# \u30c7\u30fc\u30bf\u306e\u30ed\u30fc\u30c9","cfa2e3a5":"# \u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9","d86e2022":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8"}}