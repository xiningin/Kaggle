{"cell_type":{"eba54a31":"code","dcb89138":"code","b3933d35":"code","408142c7":"code","c1cf729c":"code","f8504cbd":"code","84e1e749":"code","cad3b21f":"code","c3bb3e7a":"code","196c8895":"code","363e98ed":"code","41999983":"code","62f6d63a":"code","1ab161f2":"code","da8257e0":"code","5ed86330":"code","73b3141b":"code","3da43969":"code","cade1177":"code","ab072232":"code","27c20160":"code","34d83fa9":"code","92853153":"code","4da9ad7e":"code","8635a8af":"code","cb60f857":"code","588390d7":"markdown","ce617e4a":"markdown","75e337f4":"markdown","e5b10c5b":"markdown","d11a8de3":"markdown","343bf723":"markdown","41e8ea18":"markdown","4ecd79eb":"markdown"},"source":{"eba54a31":"import torchvision\nimport torch\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import IncrementalPCA\nimport numpy as np\nfrom torch.optim import lr_scheduler\nimport time\nfrom sklearn.preprocessing import StandardScaler","dcb89138":"from glob import glob\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torch import optim\nfrom torchvision import models\nfrom torch import nn\nimport torch\nfrom torch.optim import lr_scheduler\nimport time\nimport copy\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.metrics import normalized_mutual_info_score\nimport seaborn as sns ","b3933d35":"n_epochs = 3\nbatch_size_train = 1024\nbatch_size_test = 1024\nlearning_rate = 0.01\nmomentum = 0.5\nlog_interval = 10","408142c7":"train_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('.\/', train=True, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=batch_size_train, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('.\/', train=False, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=batch_size_test, shuffle=True)","c1cf729c":"examples = enumerate(test_loader)\nbatch_idx, (example_data, example_targets) = next(examples)","f8504cbd":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nfor i in range(6):\n  plt.subplot(2,3,i+1)\n  plt.tight_layout()\n  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n  plt.xticks([])\n  plt.yticks([])\nfig","84e1e749":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","cad3b21f":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Sequential(         \n            nn.Conv2d(\n                in_channels=1,              \n                out_channels=16,            \n                kernel_size=5,              \n                stride=1,                   \n                padding=2,                  \n            ),                              \n            nn.ReLU(),                      \n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(16, 32, 5, 1, 2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n        )\n        # fully connected layer, output 10 classes\n        self.out = nn.Sequential(\n            nn.Linear(32*7*7, 10),)\n    def forward(self, x):\n        x = self.conv1(x)\n        \n        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n        x = x.view(x.size(0), -1) \n        output = self.out(x)\n        return output, x    # return x for clustering","c3bb3e7a":"net = Net()","196c8895":"dataloaders = {\n    'train':train_loader, \n    'val': test_loader\n}\ndataset_sizes = {\n    'train':60000,\n    'val':10000\n}","363e98ed":"criterion_class = nn.CrossEntropyLoss()\noptimizer_ft = optim.Adam(net.parameters(), lr=0.002)","41999983":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncpu = torch.device('cpu')","62f6d63a":"net = net.to(device)","1ab161f2":"pca = IncrementalPCA(n_components=1024, batch_size=512, whiten=True)\nkmeans = MiniBatchKMeans(n_clusters=10, batch_size=512, max_iter=1000, init_size=3*100)","da8257e0":"def train_model(model, criterion_class, optimizer, num_epochs=25, c=1000):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = 5\n    first_time = True\n    # \u0444\u043b\u0430\u0433 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0431\u0430\u0442\u0447\u0430\n\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = []\n            train_acc = []\n            val_acc = []\n            cluster_acc = []\n\n            batch_num = 0\n            # Iterate over data.\n            for inputs, class_labels in dataloaders[phase]:\n\n                inputs = inputs.to(device)\n                class_labels = class_labels.to(device)\n                \n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n\n                with torch.set_grad_enabled(phase == 'train'):\n              \n                    class_out, vectors = model(inputs)\n                    _, preds = torch.max(class_out, 1)\n                    vectors = vectors.to(cpu)\n                    vectors_np = vectors.detach().numpy()\n                    vectors_np = StandardScaler().fit_transform(vectors_np)\n                    cluster_labels = None\n\n                    # \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u043f\u0435\u0440\u0432\u043e\u043c \u0431\u0430\u0442\u0447\u0435\n                    if first_time:\n                      cluster_labels = kmeans.fit_predict(vectors_np)\n                      first_time = False\n                    else:\n                      cluster_labels = kmeans.predict(vectors_np)\n\n                    \n                    clust_sam = cluster_labels.copy()\n                    cluster_labels = torch.from_numpy(cluster_labels).long().to(device)\n                    loss = criterion_class(class_out,cluster_labels)\n                    \n                    if (phase == 'train'):\n                        loss.backward()\n                        optimizer.step()\n                        cluster = True\n\n                    class_labels_clone = class_labels.clone().to(cpu).detach().numpy()\n                    preds_clone = preds.clone().to(cpu).detach().numpy()\n\n                    my_score = normalized_mutual_info_score(class_labels_clone,preds_clone )\n                    cluster_acc.append(normalized_mutual_info_score(class_labels_clone,clust_sam))\n\n\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects.append(normalized_mutual_info_score(class_labels_clone,preds_clone ))\n\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = np.mean(running_corrects)\n            plt.plot(running_corrects)\n            plt.xlabel(f'number of batch fro phase {phase}')\n            plt.grid(True)\n            plt.show()\n            print(f'{phase} Loss: {epoch_loss:.4f} acc: {epoch_acc:.4f} cluster_acc: {np.mean(cluster_acc)}')\n\n            if phase == 'val' and epoch_loss < best_epoch_loss:\n                best_epoch_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n\n\n\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best : {:4f}'.format( best_epoch_loss))\n\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n#     torch.save(model.state_dict(), '.\/models\/model')\n    return model","5ed86330":"model_ft_tf = train_model(net, criterion_class, optimizer_ft, num_epochs=4)","73b3141b":"def train_reconstructor(model, criterion, optimizer, epoch, lr):\n  since = time.time()\n  best_model_wts = copy.deepcopy(model.state_dict())\n  best_epoch_loss = 5\n  best_acc = 0.0\n  mni_scores = []\n  new_layer = nn.Sequential(\n            nn.Linear(32*7*7, 20),\n            nn.Sigmoid(),\n            nn.Linear(20, 10))\n  kmean = MiniBatchKMeans(n_clusters=10, batch_size=512, max_iter=1000, init_size=3*100)\n  need_new_cluster = True\n  psevdo_labels = None\n\n  criterion_in = criterion\n  optimizer_in = optimizer.Adam(model.parameters(), lr=lr)\n\n\n  for i in range(epoch):\n\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      if phase == 'val':\n        model.eval()\n\n      running_loss = 0.0\n      running_corrects = 0\n      batch_num = 1\n\n      for inputs, labels in dataloaders[phase]:\n        optimizer_in.zero_grad()\n        \n\n\n\n        if  batch_num % 10 == 0 and phase == 'train':\n\n          model.out = new_layer\n          model = model.to(device)\n          criterion_in = criterion\n          optimizer_in = optimizer.Adam(model.parameters(), lr=lr)\n          model.train()\n          print('new model')\n\n\n\n\n        with torch.set_grad_enabled(phase == 'train'):\n          inputs = inputs.to(device)\n          labels = labels.to(device)\n          \n          probs, vectors = model(inputs)\n          vectors_cp = vectors.clone().to(cpu).detach().numpy()\n          vectors_cp = StandardScaler().fit_transform(vectors_cp)\n          _, preds = torch.max(probs, 1)\n\n\n\n          if need_new_cluster:\n            psevdo_labels = kmean.fit_predict(vectors_cp)\n            need_new_cluster = False\n          else:\n            psevdo_labels = kmean.predict(vectors_cp)\n          \n          labels_cp = labels.clone().to(cpu).detach().numpy()\n\n          preds_cp = preds.clone().to(cpu).detach().numpy()\n          nmi_score = normalized_mutual_info_score(labels_cp ,preds_cp)\n          mni_scores.append(nmi_score)\n          if batch_num % 10 == 0:\n            plt.plot(mni_scores)\n            plt.xlabel(f'number of batch fro phase {phase}')\n            plt.grid(True)\n            plt.title('changes of MNI score for each 10 batches')\n            plt.show()\n            mni_scores = []\n          # print(nmi_score)\n\n\n         \n\n          psevdo_labels_gpu = torch.from_numpy(psevdo_labels).long().to(device)\n\n\n\n          loss = criterion_in(probs,psevdo_labels_gpu)\n\n\n\n          if (phase == 'train'):\n              loss.backward()\n              optimizer_in.step()\n\n        running_loss += loss*inputs.size(0)\n        running_corrects += nmi_score*inputs.size(0)\n        batch_num += 1\n\n      epoch_loss = running_loss \/ dataset_sizes[phase]\n      epoch_acc = running_corrects \/ dataset_sizes[phase]\n\n      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n          phase, epoch_loss, epoch_acc))\n\n      # deep copy the model\n      if phase == 'val' and epoch_acc > best_acc:\n          best_acc = epoch_acc\n          best_model_wts = copy.deepcopy(model.state_dict())\n\n  model.load_state_dict(best_model_wts)\n  return model","3da43969":"criterion = nn.CrossEntropyLoss()\noptimizer = optim\nnet_drop = Net()\nnet_drop.to(device)","cade1177":"model_drop = train_reconstructor(net_drop, criterion, optimizer, epoch = 6, lr=0.002)","ab072232":"def train_model_model(model, criterion, optimizer, epoch=25):\n  since = time.time()\n  best_model_wts = copy.deepcopy(model.state_dict())\n  best_epoch_loss = 5\n  best_acc = 0.0\n\n  for i in range(epoch):\n    corrects = []\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      if phase == 'val':\n        model.eval()\n\n      running_loss = 0.0\n      running_corrects = 0\n\n      for inputs, labels in dataloaders[phase]:\n        optimizer.zero_grad()\n        with torch.set_grad_enabled(phase == 'train'):\n          inputs = inputs.to(device)\n          labels = labels.to(device)\n          \n          probs, vectors = model(inputs)\n          _, preds = torch.max(probs, 1)\n          \n          loss = criterion(probs,labels)\n\n\n          if (phase == 'train'):\n              loss.backward()\n              optimizer.step()\n\n          corrects.append(torch.sum(preds == labels.data).double().to(cpu).detach().numpy() \/ inputs.size(0))\n\n          \n\n        running_loss += loss*inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n\n      epoch_loss = running_loss \/ dataset_sizes[phase]\n      epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n      plt.plot(corrects)\n      plt.xlabel(f'number of batch fro phase {phase}')\n      plt.grid(True)\n      plt.title(f'changes of accuracy score for phase {phase}')\n      plt.show()\n      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n          phase, epoch_loss, epoch_acc))\n\n      # deep copy the model\n      if phase == 'val' and epoch_acc > best_acc:\n          best_acc = epoch_acc\n          best_model_wts = copy.deepcopy(model.state_dict())\n\n  model.load_state_dict(best_model_wts)\n  return model","27c20160":"net2 = Net()\nnet2.conv1 = model_drop.conv1\nfor param in net2.parameters():\n  param.requires_grad = False\nnet2.out = nn.Sequential(\n            nn.Linear(32*7*7, 150),\n            nn.Sigmoid(),\n            nn.Linear(150, 10))\nnet2 = net2.to(device)\ncriterion2 = nn.CrossEntropyLoss()\noptimizer2 = optim.Adam(net2.parameters(), lr=0.001)","34d83fa9":"train_model_model(net2, criterion2, optimizer2, epoch=1)","92853153":"net3 = Net()\nnet3 = net3.to(device)\n# for param in net3.parameters():\n#   param.requires_grad = False\n# net3.out = nn.Sequential(\n#             nn.Linear(32*7*7, 150),\n#             nn.Sigmoid(),\n#             nn.Linear(150, 10))\ncriterion3 = nn.CrossEntropyLoss()\noptimizer3 = optim.Adam(net3.parameters(), lr=0.0015)","4da9ad7e":"train_model_model(net3, criterion3, optimizer3, epoch=1)","8635a8af":"net4 = Net()\nfor param in net4.parameters():\n  param.requires_grad = False\nnet4.out = nn.Sequential(\n            nn.Linear(32*7*7, 150),\n            nn.Sigmoid(),\n            nn.Linear(150, 10))\nnet4 = net4.to(device)\ncriterion4 = nn.CrossEntropyLoss()\noptimizer4 = optim.Adam(net4.parameters(), lr=0.0015)","cb60f857":"train_model_model(net4, criterion4, optimizer4, epoch=1)","588390d7":"To solve this problem, let's drop the fully connected layers and reinitialize them and retrain clustering model every 10 batches . ","ce617e4a":"Let's use the pretrained convolutional layers from deep clustering for transfer learing and compare the results with model, witch will not use the pretrained convolutional layers","75e337f4":"for conclusion we can say, that the this type of deep clustering can reach not bad score without so mutch labels compared with classic learing. It could be used if the labeling process is pretty long, also this method is mutch better then the method with randomly initialized weights","e5b10c5b":"Let's compare with model with frozen randomly initialized convolutional weights","d11a8de3":"we can notice that the first version have increased learing seep in first steps, also we must not forget that in first version we trained only fully connected layers","343bf723":"The problem of the method described above is that we initialized the clustering model only one time, so witch means, that we have limited traning space","41e8ea18":"with pretrained frozen layers","4ecd79eb":"simple training"}}