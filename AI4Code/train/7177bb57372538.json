{"cell_type":{"94fa8204":"code","ed0f7de2":"code","a257aada":"code","f14d9b4f":"code","3094795b":"code","2b81e090":"code","4d65f2bc":"code","bd9969f6":"code","91ee44ac":"code","986b0e2e":"code","279cf141":"code","203a0c3e":"code","5d7ef945":"code","e950638b":"code","b3931bd6":"code","d1b215a0":"code","40153033":"code","3cf671f1":"code","9b8b455e":"code","5062a073":"code","cfac51f1":"code","5dbcaddd":"code","7646de7e":"markdown"},"source":{"94fa8204":"ROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nDATASET_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/'\nCKPT_PATH_1 = '\/kaggle\/input\/cotsy5m\/best0.pt'\nCKPT_PATH_2 = '\/kaggle\/input\/cotsy5m\/best1.pt'\nCKPT_PATH_3 = '\/kaggle\/input\/cotsy5m\/best2.pt'\n\n\nIMG_SIZE_1  = 2900\nCONF_1      = 0.1\nIOU_1       = 0.90\n\nIMG_SIZE_2  = 2900\nCONF_2      = 0.1\nIOU_2       = 0.90\n\nIMG_SIZE_3  = 2900\nCONF_3      = 0.1\nIOU_3       = 0.90\n\n\nDEBUG = True\n\nimport numpy as np\nimport pandas as pd\nimport sys\nimport cv2\nimport torch\nfrom PIL import Image\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport torch\nimport importlib\n\n\nfrom PIL import Image\nfrom IPython.display import display\n\nsys.path.append('..\/input\/tensorflow-great-barrier-reef')","ed0f7de2":"%pwd","a257aada":"!mkdir -p \/root\/.config\/Ultralytics\n!cp \/kaggle\/input\/yolo5data\/Arial.ttf \/root\/.config\/Ultralytics\/","f14d9b4f":"%cd \/kaggle\/input\/norfair031py3\/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f .\/ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir \/kaggle\/working\/tmp\n!cp -r \/kaggle\/input\/norfair031py3\/filterpy-1.4.5\/filterpy-1.4.5\/ \/kaggle\/working\/tmp\/\n%cd \/kaggle\/working\/tmp\/filterpy-1.4.5\/\n!pip install .\n!rm -rf \/kaggle\/working\/tmp\n\n# norfair\n%cd \/kaggle\/input\/norfair031py3\/\n!pip install norfair-0.3.1-py3-none-any.whl -f .\/ --no-index","3094795b":"import warnings\nimport numpy as np\nfrom numba import jit\nimport time\n\n@jit(nopython=True)\ndef bb_intersection_over_union(A, B) -> float:\n    xA = max(A[0], B[0])\n    yA = max(A[1], B[1])\n    xB = min(A[2], B[2])\n    yB = min(A[3], B[3])\n\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n\n    if interArea == 0:\n        return 0.0\n\n    # compute the area of both the prediction and ground-truth rectangles\n    boxAArea = (A[2] - A[0]) * (A[3] - A[1])\n    boxBArea = (B[2] - B[0]) * (B[3] - B[1])\n\n    iou = interArea \/ float(boxAArea + boxBArea - interArea)\n    return iou\n\n\ndef prefilter_boxes(boxes, scores, labels, weights, thr):\n\n    new_boxes = dict()\n\n    for t in range(len(boxes)):\n\n        if len(boxes[t]) != len(scores[t]):\n            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))\n            exit()\n\n        if len(boxes[t]) != len(labels[t]):\n            print('Error. Length of boxes arrays not equal to length of labels array: {} != {}'.format(len(boxes[t]), len(labels[t])))\n            exit()\n\n        for j in range(len(boxes[t])):\n            score = scores[t][j]\n            if score < thr:\n                continue\n            label = int(labels[t][j])\n            box_part = boxes[t][j]\n            x1 = float(box_part[0])\n            y1 = float(box_part[1])\n            x2 = float(box_part[2])\n            y2 = float(box_part[3])\n\n            # Box data checks\n            if x2 < x1:\n                warnings.warn('X2 < X1 value in box. Swap them.')\n                x1, x2 = x2, x1\n            if y2 < y1:\n                warnings.warn('Y2 < Y1 value in box. Swap them.')\n                y1, y2 = y2, y1\n            if x1 < 0:\n                warnings.warn('X1 < 0 in box. Set it to 0.')\n                x1 = 0\n            if x1 > 1:\n                warnings.warn('X1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x1 = 1\n            if x2 < 0:\n                warnings.warn('X2 < 0 in box. Set it to 0.')\n                x2 = 0\n            if x2 > 1:\n                warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                x2 = 1\n            if y1 < 0:\n                warnings.warn('Y1 < 0 in box. Set it to 0.')\n                y1 = 0\n            if y1 > 1:\n                warnings.warn('Y1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y1 = 1\n            if y2 < 0:\n                warnings.warn('Y2 < 0 in box. Set it to 0.')\n                y2 = 0\n            if y2 > 1:\n                warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')\n                y2 = 1\n            if (x2 - x1) * (y2 - y1) == 0.0:\n                warnings.warn(\"Zero area box skipped: {}.\".format(box_part))\n                continue\n\n            # [label, score, weight, model index, x1, y1, x2, y2]\n            b = [int(label), float(score) * weights[t], weights[t], t, x1, y1, x2, y2]\n            if label not in new_boxes:\n                new_boxes[label] = []\n            new_boxes[label].append(b)\n\n    # Sort each list in dict by score and transform it to numpy array\n    for k in new_boxes:\n        current_boxes = np.array(new_boxes[k])\n        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]\n\n    return new_boxes\n\n\ndef get_weighted_box(boxes, conf_type='avg'):\n    \n    box = np.zeros(8, dtype=np.float32)\n    conf = 0\n    conf_list = []\n    w = 0\n    for b in boxes:\n        box[4:] += (b[1] * b[4:])\n        conf += b[1]\n        conf_list.append(b[1])\n        w += b[2]\n    box[0] = boxes[0][0]\n    if conf_type == 'avg':\n        box[1] = conf \/ len(boxes)\n    elif conf_type == 'max':\n        box[1] = np.array(conf_list).max()\n    elif conf_type in ['box_and_model_avg', 'absent_model_aware_avg']:\n        box[1] = conf \/ len(boxes)\n    box[2] = w\n    box[3] = -1 \n    box[4:] \/= conf\n    return box\n\n\ndef find_matching_box(boxes_list, new_box, match_iou):\n    best_iou = match_iou\n    best_index = -1\n    for i in range(len(boxes_list)):\n        box = boxes_list[i]\n        if box[0] != new_box[0]:\n            continue\n        iou = bb_intersection_over_union(box[4:], new_box[4:])\n        if iou > best_iou:\n            best_index = i\n            best_iou = iou\n\n    return best_index, best_iou\n\n\ndef find_matching_box_quickly(boxes_list, new_box, match_iou):\n    \n    def bb_iou_array(boxes, new_box):\n        # bb interesection over union\n        xA = np.maximum(boxes[:, 0], new_box[0])\n        yA = np.maximum(boxes[:, 1], new_box[1])\n        xB = np.minimum(boxes[:, 2], new_box[2])\n        yB = np.minimum(boxes[:, 3], new_box[3])\n\n        interArea = np.maximum(xB - xA, 0) * np.maximum(yB - yA, 0)\n\n        # compute the area of both the prediction and ground-truth rectangles\n        boxAArea = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n        boxBArea = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])\n\n        iou = interArea \/ (boxAArea + boxBArea - interArea)\n\n        return iou\n\n    if boxes_list.shape[0] == 0:\n        return -1, match_iou\n\n    # boxes = np.array(boxes_list)\n    boxes = boxes_list\n\n    ious = bb_iou_array(boxes[:, 4:], new_box[4:])\n\n    ious[boxes[:, 0] != new_box[0]] = -1\n\n    best_idx = np.argmax(ious)\n    best_iou = ious[best_idx]\n\n    if best_iou <= match_iou:\n        best_iou = match_iou\n        best_idx = -1\n\n    return best_idx, best_iou\n\n\ndef weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights=None, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n\n    if weights is None:\n        weights = np.ones(len(boxes_list))\n    if len(weights) != len(boxes_list):\n        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))\n        weights = np.ones(len(boxes_list))\n    weights = np.array(weights)\n\n    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:\n        print('Unknown conf_type: {}. Must be \"avg\", \"max\" or \"box_and_model_avg\", or \"absent_model_aware_avg\"'.format(conf_type))\n        exit()\n\n    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)\n    if len(filtered_boxes) == 0:\n        return np.zeros((0, 4)), np.zeros((0,)), np.zeros((0,))\n\n    overall_boxes = []\n    for label in filtered_boxes:\n        boxes = filtered_boxes[label]\n        new_boxes = []\n        weighted_boxes = np.empty((0,8))\n        # Clusterize boxes\n        for j in range(0, len(boxes)):\n            index, best_iou = find_matching_box_quickly(weighted_boxes, boxes[j], iou_thr)\n\n            if index != -1:\n                new_boxes[index].append(boxes[j])\n                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)\n            else:\n                new_boxes.append([boxes[j].copy()])\n                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))\n        # Rescale confidence based on number of models and boxes\n        for i in range(len(new_boxes)):\n            clustered_boxes = np.array(new_boxes[i])\n            if conf_type == 'box_and_model_avg':\n                # weighted average for boxes\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) \/ weighted_boxes[i, 2]\n                # identify unique model index by model index column\n                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)\n                # rescale by unique model weights\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() \/ weights.sum()\n            elif conf_type == 'absent_model_aware_avg':\n                # get unique model index in the cluster\n                models = np.unique(clustered_boxes[:, 3]).astype(int)\n                # create a mask to get unused model weights\n                mask = np.ones(len(weights), dtype=bool)\n                mask[models] = False\n                # absent model aware weighted average\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) \/ (weighted_boxes[i, 2] + weights[mask].sum())\n            elif conf_type == 'max':\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] \/ weights.max()\n            elif not allows_overflow:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) \/ weights.sum()\n            else:\n                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) \/ weights.sum()\n        overall_boxes.append(weighted_boxes)\n    overall_boxes = np.concatenate(overall_boxes, axis=0)\n    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]\n    boxes = overall_boxes[:, 4:]\n    scores = overall_boxes[:, 1]\n    labels = overall_boxes[:, 0]\n    return boxes, scores, labels\n\ndef weighted_boxes_fusion_tresh(boxes_list, scores_list, labels_list, weights=None, conf_tresh = 0.2, iou_thr=0.55, skip_box_thr=0.0, conf_type='avg', allows_overflow=False):\n    boxes_fin = []\n    scores_fin = []\n    labels_fin = []\n    \n    boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights, iou_thr, skip_box_thr, conf_type, allows_overflow)\n    \n    if len(boxes) >0 :\n        for score, box, label in zip(scores, boxes, labels):\n            if score < conf_tresh:\n                break\n\n            boxes_fin.append(box)\n            scores_fin.append(score)\n            labels_fin.append(label)\n    \n    \n    return np.array(boxes_fin), np.array(scores_fin), np.array(labels_fin)","2b81e090":"def RecoverCLAHE(sceneRadiance):\n    clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(4, 4))\n    for i in range(3):\n        sceneRadiance[:, :, i] = clahe.apply((sceneRadiance[:, :, i]))\n\n    return sceneRadiance","4d65f2bc":"def load_model(ckpt_path, conf=0.15, iou=0.30):\n    model = torch.hub.load('\/kaggle\/input\/yolo-lib',\n                           'custom',\n                           path=ckpt_path,\n                           source='local',\n                           force_reload = True)  # local repo\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou  # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 100  # maximum number of detections per image\n    return model","bd9969f6":"def predict(model, img, size = 1280, augment = False):\n    bboxes = []\n    scores = []\n    classes = []\n    results = model(img, size=size, augment = augment)  # custom inference size\n    preds   = results.pandas().xyxy[0].values\n    for bbox in preds:\n        xmin = bbox[0]\n        ymin = bbox[1]\n        xmax = bbox[2]\n        ymax = bbox[3]\n        \n        bboxes.append([xmin , ymin , xmax, ymax]) \n    \n    scores = preds[: , 4]\n    classes = preds[: , 5]\n    return bboxes, scores, classes","91ee44ac":"from norfair import Detection, Tracker\n\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) \/ 2, (y_min + y_max) \/ 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)","986b0e2e":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nframe_id = 0","279cf141":"def show_prediction(img, bboxes):\n    colors = [(0, 0, 255)]\n\n    obj_names = [\"s\"]\n\n    for box in bboxes:\n        cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255,0,0), 2)\n    \n    img = Image.fromarray(img).resize((1280, 720))\n    return img","203a0c3e":"model_y5_1 = load_model(CKPT_PATH_1, CONF_1, IOU_1)\nmodel_y5_2 = load_model(CKPT_PATH_2, CONF_2, IOU_2)\nmodel_y5_3 = load_model(CKPT_PATH_3, CONF_3, IOU_3)","5d7ef945":"df = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")","e950638b":"from ast import literal_eval\n\n\ndef decode_annotations(annotaitons_str):\n    \"\"\"decode annotations in string to list of dict\"\"\"\n    return literal_eval(annotaitons_str)\n\ndef load_image_with_annotations(img, annotaitons_str):\n    annotations = decode_annotations(annotaitons_str)\n    if len(annotations) > 0:\n        for ann in annotations:\n            cv2.rectangle(img, (ann['x'], ann['y']),\n                (ann['x'] + ann['width'], ann['y'] + ann['height']),\n                (255, 255, 255), thickness=2,)\n            cv2.rectangle(img, (ann['x'], ann['y'] - 15),\n                (ann['x'] + ann['width'], ann['y']),\n                (255, 255, 255), -1 ,)\n            cv2.putText(img, f'GT', (int(ann['x']), int(ann['y'] -3)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n    return img","b3931bd6":"dir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_0\/9674.jpg',\n                          'video_0\/20.jpg', \n                          'video_0\/17.jpg', \n                          'video_0\/100.jpg',\n                          'video_0\/246.jpg',\n                          'video_0\/337.jpg',\n                          'video_0\/358.jpg',\n                          'video_0\/1566.jpg',\n                          'video_0\/1854.jpg',\n                          'video_0\/1884.jpg',\n                          'video_0\/4224.jpg',\n                          'video_0\/4540.jpg',\n                          'video_0\/4582.jpg',\n                          'video_0\/4664.jpg',\n                          'video_0\/8215.jpg',\n                          'video_0\/8897.jpg',\n                          'video_0\/8928.jpg',\n                          'video_0\/9441.jpg',\n                          'video_0\/9859.jpg',\n                          'video_0\/12267.jpg',\n                          'video_2\/5748.jpg',\n                          'video_2\/5772.jpg',\n                          'video_2\/5820.jpg',\n                          'video_2\/5409.jpg',\n                          'video_2\/5482.jpg',\n                          'video_2\/5679.jpg',\n                          'video_2\/5712.jpg',\n                          'video_2\/5730.jpg',\n                          'video_2\/5751.jpg',\n                          'video_2\/5817.jpg',\n                          'video_2\/5868.jpg',\n                          'video_2\/6254.jpg',\n                          'video_2\/6339.jpg',\n                          'video_2\/10622.jpg',\n                          'video_1\/4159.jpg', \n                          'video_1\/4183.jpg', \n                          'video_1\/4501.jpg',\n                          'video_1\/5474.jpg',\n                          'video_1\/625.jpg',\n                          'video_1\/616.jpg',\n                          'video_1\/672.jpg',\n                          'video_1\/684.jpg',\n                          'video_1\/850.jpg',\n                          'video_1\/1927.jpg',\n                          'video_1\/2000.jpg',\n                          'video_1\/3903.jpg',\n                          'video_1\/3945.jpg',\n                          'video_1\/4051.jpg',\n                          'video_1\/4078.jpg',\n                          'video_1\/4096.jpg',\n                          'video_1\/4126.jpg',\n                          'video_1\/4456.jpg',\n                          'video_1\/4525.jpg',\n                          'video_1\/5267.jpg',\n                          'video_1\/5366.jpg',\n                          'video_1\/5411.jpg',\n                          'video_1\/5429.jpg',\n                          'video_1\/5492.jpg',\n                          'video_1\/5661.jpg',\n                          'video_1\/5892.jpg',\n                          'video_1\/6747.jpg',\n                          'video_1\/9082.jpg',)]\n\nids = ids = ['0-9674', '0-20', '0-17', '0-100', '0-246', '0-337', '0-358', '0-1566',\n             '0-1854', '0-1884', '0-4224', '0-4540', '0-4582', '0-4664', '0-8215',\n             '0-8897', '0-8928', '0-9441', '0-9859', '0-12267',\n             '2-5748', '2-5772', '2-5820', '2-5409', '2-5482',\n             '2-5679', '2-5712', '2-5730', '2-5751',  '2-5817',\n             '2-5868', '2-6254', '2-6339', '2-10622',\n             '1-4159', '1-4183', '1-4501', '1-5474', '1-625', '1-616', '1-672', '1-684',\n             '1-850', '1-1927', '1-2000', '1-3903', '1-3945',\n             '1-4051', '1-4078', '1-4096', '1-4126', '1-4456',\n             '1-4525', '1-5267', '1-5366', '1-5411', '1-5429',\n             '1-5492', '1-5661', '1-5892',  '1-6747', '1-9082',]\n\n\n# for img, idx in zip(imgs, ids):\n#     im = cv2.imread(img)\n#     #im = RecoverCLAHE(im)\n#     im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n#     bboxes, scores, classes = predict(model_y5_1, im, size = IMG_SIZE_1, augment = True)\n#     annot = df.query(\"image_id == @idx\").annotations.item()\n#     im = load_image_with_annotations(im, annot)\n#     display(show_prediction(im, bboxes))","d1b215a0":"def final_pred(img_in, w_conf = 0.1, w_nms = 0.3):\n        bboxes_y5_1 = []\n        scores_y5_1 = []\n        classes_y5_1 = []\n        \n        bboxes_y5_2 = []\n        scores_y5_2 = []\n        classes_y5_2 = []\n        \n        bboxes_y5_3 = []\n        scores_y5_3 = []\n        classes_y5_3 = []\n        \n        bboxes_fin = [] \n        scores_fin = [] \n        bbclasses_fin = []\n\n        # predykcja Y5\n        bboxes_y5_1, scores_y5_1, classes_y5_1 = predict(model_y5_1, img_in, size = IMG_SIZE_1, augment = False)\n        bboxes_y5_1 = np.array(bboxes_y5_1)\n        \n        if DEBUG:\n            print(\"*********************\")\n            print(\"BBOXES MODEL - 1\")\n            print(bboxes_y5_1)\n            print(\"SCORES\")\n            print(scores_y5_1)\n            print(\"\\n\")\n        \n        if len(bboxes_y5_1)>0:\n            bboxes_y5_1[:, [0, 2]] \/= 1280\n            bboxes_y5_1[:, [1, 3]] \/= 720\n        \n        \n        \n        bboxes_y5_2, scores_y5_2, classes_y5_2 = predict(model_y5_2, img_in, size = IMG_SIZE_2, augment = False)\n        bboxes_y5_2 = np.array(bboxes_y5_2)\n        \n        if DEBUG:\n            print(\"BBOXES MODEL - 2\")\n            print(bboxes_y5_2)\n            print(\"SCORES\")\n            print(scores_y5_2)\n            print(\"\\n\")\n        \n        if len(bboxes_y5_2)>0:\n            bboxes_y5_2[:, [0, 2]] \/= 1280\n            bboxes_y5_2[:, [1, 3]] \/= 720\n            \n        \n        bboxes_y5_3, scores_y5_3, classes_y5_3 = predict(model_y5_3, img_in, size = IMG_SIZE_3, augment = False)\n        bboxes_y5_3 = np.array(bboxes_y5_3)\n        \n        if DEBUG:\n            print(\"*********************\")\n            print(\"BBOXES MODEL - 1\")\n            print(bboxes_y5_3)\n            print(\"SCORES\")\n            print(scores_y5_3)\n            print(\"\\n\")\n        \n        if len(bboxes_y5_3)>0:\n            bboxes_y5_3[:, [0, 2]] \/= 1280\n            bboxes_y5_3[:, [1, 3]] \/= 720\n\n        \n        \n        bboxes_fin, scores_fin, bbclasses_fin = weighted_boxes_fusion_tresh([bboxes_y5_1, bboxes_y5_2, bboxes_y5_3],\n                                                                      [scores_y5_1, scores_y5_2, scores_y5_3],\n                                                                      [classes_y5_1, classes_y5_2, classes_y5_3],\n                                                                      weights=[1,1, 1],\n                                                                      conf_tresh = 0.1,\n                                                                      iou_thr= 0.25,\n                                                                      skip_box_thr=0.01,\n                                                                      conf_type='avg')\n\n        #print(bboxes_fin.shape)\n        if len(bboxes_fin) > 0:\n            bboxes_fin[:, [0, 2]] *= 1280\n            bboxes_fin[:, [1, 3]] *= 720\n        else:\n            bboxes_fin = bboxes_fin.reshape(0,4)\n        \n        if DEBUG:\n            print(\"BBOXES FINAL\")\n            print(bboxes_fin)\n            print(\"SCORES\")\n            print(scores_fin)\n            print(\"\\n\\n\")\n\n        return bboxes_fin, scores_fin, bbclasses_fin\n","40153033":"DEBUG = True\ndir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_0\/9674.jpg',\n                          'video_0\/20.jpg', \n                          'video_0\/17.jpg', \n                          'video_0\/100.jpg',\n                          'video_0\/246.jpg',\n                          'video_0\/337.jpg',\n                          'video_0\/358.jpg',\n                          'video_0\/1566.jpg',\n                          'video_0\/1854.jpg',\n                          'video_0\/1884.jpg',\n                          'video_0\/4224.jpg',\n                          'video_0\/4540.jpg',\n                          'video_0\/4582.jpg',\n                          'video_0\/4664.jpg',\n                          'video_0\/8215.jpg',\n                          'video_0\/8897.jpg',\n                          'video_0\/8928.jpg',\n                          'video_0\/9441.jpg',\n                          'video_0\/9859.jpg',\n                          'video_0\/12267.jpg',\n                          'video_2\/5748.jpg',\n                          'video_2\/5772.jpg',\n                          'video_2\/5820.jpg',\n                          'video_1\/4159.jpg', \n                          'video_1\/4183.jpg', \n                          'video_1\/4501.jpg',\n                          'video_1\/5474.jpg',\n                          'video_1\/625.jpg',\n                          'video_1\/850.jpg',\n                          'video_1\/1927.jpg',\n                          'video_1\/2000.jpg',\n                          'video_1\/3903.jpg',\n                          'video_1\/3945.jpg',\n                          'video_1\/4051.jpg',\n                          'video_1\/4078.jpg',\n                          'video_1\/4096.jpg',\n                          'video_1\/4126.jpg',\n                          'video_1\/4456.jpg',\n                          'video_1\/4525.jpg',\n                          'video_1\/5267.jpg',\n                          'video_1\/5366.jpg',\n                          'video_1\/5411.jpg',\n                          'video_1\/5429.jpg',\n                          'video_1\/5492.jpg',\n                          'video_1\/5661.jpg',\n                          'video_1\/5892.jpg',\n                          'video_1\/6747.jpg',\n                          'video_1\/9082.jpg',)]\n\nids = ids = ['0-9674', '0-20', '0-17', '0-100', '0-246', '0-337', '0-358', '0-1566',\n             '0-1854', '0-1884', '0-4224', '0-4540', '0-4582', '0-4664', '0-8215',\n             '0-8897', '0-8928', '0-9441', '0-9859', '0-12267',\n             '2-5748', '2-5772', '2-5820',\n             '1-4159', '1-4183', '1-4501', '1-5474', '1-625',\n             '1-850', '1-1927', '1-2000', '1-3903', '1-3945',\n             '1-4051', '1-4078', '1-4096', '1-4126', '1-4456',\n             '1-4525', '1-5267', '1-5366', '1-5411', '1-5429',\n             '1-5492', '1-5661', '1-5892',  '1-6747', '1-9082',]\n\n\nfor img, idx in zip(imgs, ids):\n    im = cv2.imread(img)\n    imy = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    print(f\"{img} -> {idx}\")\n    bboxes, scores, _ = final_pred(imy, w_conf = 0.2, w_nms = 0.1)\n    annot = df.query(\"image_id == @idx\").annotations.item()\n    imy = load_image_with_annotations(imy, annot)\n    display(show_prediction(imy, bboxes))\n    ","3cf671f1":"%cd \/kaggle\/working\/","9b8b455e":"DEBUG = False","5062a073":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","cfac51f1":"#model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n\nfor (image_np, sample_prediction_df) in iter_test:\n    \n    #bboxes, scores = predict(model, image_np, size = IMG_SIZE, augment = True)\n    \n    bboxes, scores, _ = final_pred(image_np, w_conf = 0.4, w_nms = 0.1)\n    \n    predictions = []\n    detects = []\n    \n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        score = scores[i]\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        detects.append([x_min, y_min, x_max, y_max, score])\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    \n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n            \n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width \/ 2)), int(round(yc - bbox_height \/ 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n    frame_id += 1","5dbcaddd":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","7646de7e":"## ENSEMBLE\n"}}