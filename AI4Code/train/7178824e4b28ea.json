{"cell_type":{"8143d73c":"code","cdee3b62":"code","c696de2a":"code","23caba65":"code","18797887":"code","16b382ba":"code","4bfe20e7":"code","b6bf52b3":"code","08ebc3c4":"code","a344b3c4":"code","a270c6e0":"code","c72ee92f":"code","f3344e36":"code","973be293":"code","912b04f8":"code","92c1fcc5":"code","c47a1f86":"code","3de1db9a":"code","173c2d94":"code","0ba5f94f":"code","3b68bd1f":"code","94ba38bd":"code","243b418e":"code","252459f7":"code","3d9d2237":"code","713b9b40":"code","babf47c9":"code","71726dfc":"code","8cf45bb4":"code","59ef9182":"code","a79f3dd3":"code","d42c8235":"code","cbcad2d0":"code","cbb90269":"code","11b26f44":"code","a4c6e09b":"code","c0eab713":"code","b5602b4d":"code","f32ccc27":"code","86d09c45":"code","ae0da336":"code","16a3f9a8":"code","bf8df0a9":"code","a5320951":"code","3d60aec2":"code","d97c715e":"code","04e34569":"code","2912db48":"code","b0ab57bd":"code","a6fd9515":"code","dfa15211":"code","7e029694":"code","46640462":"code","2723d184":"code","05c6b831":"code","826a6b23":"code","5312d636":"code","98db1a0f":"code","af2a1dcf":"code","a885a409":"code","57a217b8":"code","772c3cf2":"code","c2f864cc":"code","f19ecc44":"code","3b9cae8e":"code","a197f5e9":"code","158b4f09":"code","e71b6a92":"code","00d66e59":"markdown","05e21f3e":"markdown","ce7be6b9":"markdown","1fe021f8":"markdown","48425461":"markdown","3c10d490":"markdown","bc5c369b":"markdown","867e7e3e":"markdown","ab342696":"markdown","ccd68cca":"markdown","a2332dbc":"markdown","1e3c4686":"markdown","590b2e2f":"markdown","e22942d4":"markdown","49feb41c":"markdown","301d6c5b":"markdown","c8fa70f9":"markdown","0f1be14e":"markdown","71ba6240":"markdown","d1827ca7":"markdown","0cc0a7ec":"markdown","c871fae6":"markdown","9ef5d653":"markdown","7f833aff":"markdown","ac08f908":"markdown","60c7633e":"markdown"},"source":{"8143d73c":"#!pip install xgboost\n#!pip install lightgbm\n#!pip install catboost","cdee3b62":"import warnings\nwarnings.simplefilter(action='ignore')\n\nimport pandas as pd\nimport numpy as np\nimport xgboost\nfrom sklearn.impute import KNNImputer\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import LocalOutlierFactor\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV \nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, ElasticNet, Lasso, LassoCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","c696de2a":"df = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf_copy = df.copy()\ndf.head()","23caba65":"#veri setinin yap\u0131sal bilgileri\ndf.info()","18797887":"#veri setindeki de\u011fi\u015fkenlerin bilgisi\ndf.dtypes","16b382ba":"#object tipini kategorik tipe d\u00f6n\u00fc\u015ft\u00fcrme i\u015flemi\ndf.League = pd.Categorical(df.League)\ndf.Division = pd.Categorical(df.Division)\ndf.NewLeague = pd.Categorical(df.NewLeague)","4bfe20e7":"df.dtypes","b6bf52b3":"#veri setinde boyut bilgisi\ndf.shape","08ebc3c4":"#veri setinde hangi de\u011fi\u015fkenden ka\u00e7 tane eksik g\u00f6zlem var?\ndf.isnull().sum()","a344b3c4":"#Betimsel \u0130statistikler\ndf.describe().T","a270c6e0":"kat_df = df.select_dtypes(include=[\"category\"])\nkat_df.head()","c72ee92f":"#League kategorik de\u011fi\u015fkeni i\u00e7in s\u0131n\u0131flara, s\u0131n\u0131f say\u0131s\u0131na ve s\u0131n\u0131f frekans\u0131na eri\u015fim ve g\u00f6rselle\u015ftirme\nprint(kat_df.League.unique())\nprint(kat_df[\"League\"].value_counts().count())\nprint(kat_df[\"League\"].value_counts())\nprint(df[\"League\"].value_counts().plot.barh())","f3344e36":"#Division kategorik de\u011fi\u015fkeni i\u00e7in s\u0131n\u0131flara, s\u0131n\u0131f say\u0131s\u0131na ve s\u0131n\u0131f frekans\u0131na eri\u015fim ve g\u00f6rselle\u015ftirme\nprint(kat_df.Division.unique())\nprint(kat_df[\"Division\"].value_counts().count())\nprint(kat_df[\"Division\"].value_counts())\nprint(df[\"Division\"].value_counts().plot.barh())","973be293":"#New League kategorik de\u011fi\u015fkeni i\u00e7in s\u0131n\u0131flara,s\u0131n\u0131f say\u0131s\u0131na ve s\u0131n\u0131f frekans\u0131na eri\u015fim ve g\u00f6rselle\u015ftirme\nprint(kat_df.NewLeague.unique())\nprint(kat_df[\"NewLeague\"].value_counts().count())\nprint(kat_df[\"NewLeague\"].value_counts())\nprint(df[\"NewLeague\"].value_counts().plot.barh())","912b04f8":"num_df = df.select_dtypes(include = [\"float64\",\"int64\"])\nnum_df","92c1fcc5":"#Betimsel \u0130statistikler\nnum_df.describe().T","c47a1f86":"num_df.isnull().sum()","3de1db9a":"#Sadece \"Salary\" de\u011fi\u015fkeninde eksik de\u011ferler var. KNNImputer ile eksik de\u011fer dolduraca\u011f\u0131m\ncols = num_df.columns\nimputer = KNNImputer(n_neighbors=8)\nnum_df = imputer.fit_transform(num_df)\nnum_df=pd.DataFrame(num_df,columns=cols)\nnum_df.head","173c2d94":"num_df.isnull().sum()","0ba5f94f":"kat_df","3b68bd1f":"le = LabelEncoder()\nkat_df['League'] = le.fit_transform(kat_df['League'])\nkat_df['Division'] = le.fit_transform(kat_df['Division'])\nkat_df['NewLeague'] = le.fit_transform(kat_df['NewLeague'])","94ba38bd":"kat_df","243b418e":"clf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nclf.fit_predict(num_df)\ndf_scores = clf.negative_outlier_factor_\nnp.sort(df_scores)[0:30]\n","252459f7":"sns.boxplot(df_scores);","3d9d2237":"esik_deger = np.sort(df_scores)[7]\nprint(esik_deger)","713b9b40":"aykiri_tf = df_scores>esik_deger\naykirilar = num_df[~aykiri_tf]","babf47c9":"#aykirilar\naykirilar","71726dfc":"aykirilar.index","8cf45bb4":"num_df","59ef9182":"#aykiri olmayanlar\nnum_df = num_df[aykiri_tf]\n\n# Kategorik de\u011fi\u015fkenlerden, indexi ayk\u0131r\u0131 olanlar\u0131 u\u00e7uruyorum\nkat_df = kat_df.drop(aykirilar.index)","a79f3dd3":"num_df","d42c8235":"kat_df","cbcad2d0":"#reset index i\u015flemleri\nnum_df = num_df.reset_index(drop=True)\nkat_df = kat_df.reset_index(drop=True)","cbb90269":"num_df","11b26f44":"kat_df","a4c6e09b":"y = num_df[\"Salary\"]\nX = num_df.drop('Salary', axis=1)\nX","c0eab713":"scaled_cols=StandardScaler().fit_transform(X)\nscaled_cols=pd.DataFrame(scaled_cols, columns=X.columns)","b5602b4d":"#De\u011fi\u015fken standardizasyonundan sonra, model kurmadan \u00f6nce kategorik de\u011fi\u015fkenin birle\u015ftirilmesi\nX = pd.concat([scaled_cols,kat_df], axis=1)\nX","f32ccc27":"dogrusal_sonuc_ilkel = []\ndogrusal_sonuc = []","86d09c45":"y","ae0da336":"X","16a3f9a8":"#y = num_df[\"Salary\"]\n#X = num_df.drop('Salary', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.20, \n                                                    random_state=46)\n","bf8df0a9":"linreg = LinearRegression()\nlinreg_model = linreg.fit(X_train,y_train)\nlinreg_y_pred = linreg_model.predict(X_test)\nlinreg_rmse = np.sqrt(mean_squared_error(y_test,linreg_y_pred))\ndogrusal_sonuc_ilkel.append((\"LinReg\", linreg_rmse))\nlinreg_rmse","a5320951":"ridreg = Ridge()\nridreg_model = ridreg.fit(X_train, y_train)\nridreg_y_pred = ridreg_model.predict(X_test)\nridreg_rmse = np.sqrt(mean_squared_error(y_test,ridreg_y_pred))\ndogrusal_sonuc_ilkel.append((\"RidgeReg\", ridreg_rmse))\nridreg_rmse","3d60aec2":"ridreg_model.alpha","d97c715e":"alpha = [0,2,1]\nridreg_cv = RidgeCV(alphas = alpha, scoring = \"neg_mean_squared_error\", cv = 10, normalize = True)\nridreg_cv.fit(X_train, y_train)\nprint(ridreg_cv.alpha_)\n#Final Model \nridreg_tuned = Ridge(alpha = ridreg_cv.alpha_).fit(X_train,y_train)\nridreg_tuned_y_pred = ridreg_tuned.predict(X_test)\nridreg_tuned_rmse = np.sqrt(mean_squared_error(y_test,ridreg_tuned_y_pred))\nprint(ridreg_tuned_rmse)\n\ndogrusal_sonuc.append((\"RidgeReg\",ridreg_cv.alpha_,ridreg_tuned_rmse))","04e34569":"lasreg = Lasso()\nlasreg_model = lasreg.fit(X_train,y_train)\nlasreg_y_pred = lasreg_model.predict(X_test)\nlasreg_rmse = np.sqrt(mean_squared_error(y_test,lasreg_y_pred))\ndogrusal_sonuc_ilkel.append((\"Lasso\", lasreg_rmse))\nlasreg_rmse","2912db48":"alpha = [0,5,1]\nlasso_cv = LassoCV(alphas = alpha, cv = 10, normalize = True)\nlasso_cv.fit(X_train, y_train)\nprint(lasso_cv.alpha_)\n#Final Model \nlasso_tuned = Lasso(alpha = lasso_cv.alpha_).fit(X_train,y_train)\nlasso_tuned_y_pred = lasso_tuned.predict(X_test)\nlasso_tuned_rmse = np.sqrt(mean_squared_error(y_test,lasso_tuned_y_pred))\nprint(lasso_tuned_rmse)\n\ndogrusal_sonuc.append((\"Lasso\",lasso_cv.alpha_,lasso_tuned_rmse))","b0ab57bd":"enet = ElasticNet()\nenet_model = enet.fit(X_train,y_train)\nenet_y_pred = enet_model.predict(X_test)\nenet_rmse = np.sqrt(mean_squared_error(y_test,enet_y_pred))\ndogrusal_sonuc_ilkel.append((\"ENet\", enet_rmse))\nenet_rmse","a6fd9515":"enet_params = {\"l1_ratio\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],\n              \"alpha\":[0.1,0.01,0.001,0.2,0.3,0.5,0.8,0.9,1]}\n\nenet_model = ElasticNet().fit(X_train,y_train)\nenet_cv = GridSearchCV(enet_model, enet_params, cv = 10).fit(X, y)\nprint(enet_cv.best_params_)\n\n#Final Model \nenet_tuned = ElasticNet(**enet_cv.best_params_).fit(X_train,y_train)\nenet_tuned_y_pred = enet_tuned.predict(X_test)\nenet_tuned_rmse = np.sqrt(mean_squared_error(y_test,enet_tuned_y_pred))\nprint(enet_tuned_rmse)\n\ndogrusal_sonuc.append((\"ENet\",enet_cv.best_params_,enet_tuned_rmse))","dfa15211":"dogrusal_sonuc = pd.DataFrame(dogrusal_sonuc, columns = [\"Model \u0130smi\",\"Optimum Parametreler\", \"Test Hatas\u0131\"])\ndogrusal_sonuc = dogrusal_sonuc.set_index(\"Model \u0130smi\")\nprint(dogrusal_sonuc)\n\ndogrusal_sonuc_ilkel = pd.DataFrame(dogrusal_sonuc_ilkel,columns = [\"Model \u0130smi\",\"\u0130lkel Test Hatas\u0131\"])\ndogrusal_sonuc_ilkel = dogrusal_sonuc_ilkel.set_index(\"Model \u0130smi\")\nprint(dogrusal_sonuc_ilkel)\n\nresult_linear = pd.concat([dogrusal_sonuc_ilkel,dogrusal_sonuc,],axis = 1)\nresult_linear.sort_values(\"Test Hatas\u0131\", ascending = True,inplace = True)\nprint(result_linear)\n\n\n","7e029694":"models = []\nsonuc_ilkel = []\nsonuc = []","46640462":"#Model Nesneleri Olu\u015ftur\nknn = KNeighborsRegressor().fit(X_train,y_train)\nsvr = SVR().fit(X_train,y_train)\nysa = MLPRegressor().fit(X_train,y_train)\ncart = DecisionTreeRegressor().fit(X_train,y_train)\nrf = RandomForestRegressor(random_state = 42).fit(X_train,y_train)\ngbm = GradientBoostingRegressor().fit(X_train,y_train)\nxgb = XGBRegressor().fit(X_train,y_train)\nlgbm = LGBMRegressor().fit(X_train,y_train)\ncatb = CatBoostRegressor(verbose = False).fit(X_train,y_train)","2723d184":"models.append(('KNN', knn))\nmodels.append(('SVR', svr))\nmodels.append(('YSA',ysa))\nmodels.append(('CART', cart))\nmodels.append(('RF', rf))\nmodels.append(('GBM', gbm))\nmodels.append((\"XGBoost\", xgb))\nmodels.append((\"LightGBM\", lgbm))\nmodels.append((\"CatBoost\", catb))","05c6b831":"for name, modelobj in models:\n        y_pred = modelobj.predict(X_test)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        sonuc_ilkel.append((name,rmse))","826a6b23":"sonuc_ilkel_df = pd.DataFrame(sonuc_ilkel,columns = [\"Model \u0130smi\",\"\u0130lkel Test Hatas\u0131\"])\nsonuc_ilkel_df = sonuc_ilkel_df.set_index(\"Model \u0130smi\")\nprint(sonuc_ilkel_df)\n\n","5312d636":"knn_params = {'n_neighbors': np.arange(1,10,1)}\n\nsvr_params = {\"C\": [0.01,0.001, 0.2, 0.1,0.5,0.8,0.9,1, 10, 100, 500,1000]}\n\nmlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100), (1000,100,10)]}\n\ncart_params = {\"max_depth\": [2,3,4,5,10,20, 100, 1000],\n              \"min_samples_split\": [2,10,5,30,50,10]}\n\nrf_params = {\"max_depth\": [5,8,10,None],\n             \"max_features\": [2,5,10,15,17],\n             \"n_estimators\": [100,200, 500, 1000],\n             \"min_samples_split\": [2,5,10,20,30]}\n\ngbm_params = {\"learning_rate\": [0.1,0.2,0.3],\n             \"max_depth\": [2,3,4],\n             \"n_estimators\": [100,200,300,400,500],\n             \"subsample\": [0.5,1.0,2.0,3.0],\n             \"loss\": [\"ls\",\"lad\",\"quantile\"]}\n\nxgb_params = {\"learning_rate\": [0.1,0.01,0.5],\n             \"max_depth\": [2,3,4,5,8],\n             \"n_estimators\": [100,200,500,1000],\n             \"colsample_bytree\": [0.4,0.7,1]}\n\nlgbm_params = {\"learning_rate\": [0.1,0.2,0.3],\n              \"n_estimators\": [100,200,300,400,500],\n              \"max_depth\": [-1,0,2,4,5,7,10],\n              \"colsample_bytree\": [1,0.8,0.5,0.4]}\n\ncatb_params = {\"iterations\": [200,500,100],\n              \"learning_rate\": [0.01,0.1],\n              \"depth\": [3,6,8]}","98db1a0f":"#KNN\nknn_cv_model = GridSearchCV(knn,knn_params,cv=10,n_jobs = -1).fit(X_train,y_train)\nprint(knn_cv_model.best_params_)\n\n#Final Model \nknn_tuned = KNeighborsRegressor(**knn_cv_model.best_params_).fit(X_train,y_train)\nknn_tuned_y_pred = knn_tuned.predict(X_test)\nknn_tuned_rmse = np.sqrt(mean_squared_error(y_test,knn_tuned_y_pred))\nprint(knn_tuned_rmse)\n\nsonuc.append(('KNN',knn_cv_model.best_params_,knn_tuned_rmse))\n\n","af2a1dcf":"#SVR\nsvr_cv_model = GridSearchCV(svr, svr_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(svr_cv_model.best_params_)\n\n#Final Model\nsvr_tuned = SVR(**svr_cv_model.best_params_).fit(X_train, y_train)\nsvr_tuned_y_pred = svr_tuned.predict(X_test)\nsvr_tuned_rmse = np.sqrt(mean_squared_error(y_test, svr_tuned_y_pred))\nprint(svr_tuned_rmse)\n\nsonuc.append(('SVR',svr_cv_model.best_params_,svr_tuned_rmse))\n\n","a885a409":"#YSA\nmlp_cv_model = GridSearchCV(ysa, mlp_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(mlp_cv_model.best_params_)\n\n#Final Model\nmlp_tuned = MLPRegressor(**mlp_cv_model.best_params_).fit(X_train,y_train)\nmlp_tuned_y_pred = mlp_tuned.predict(X_test)\nmlp_tuned_rmse = np.sqrt(mean_squared_error(y_test, mlp_tuned_y_pred))\nprint(mlp_tuned_rmse)\n\nsonuc.append(('YSA',mlp_cv_model.best_params_,mlp_tuned_rmse))\n\n","57a217b8":"#CART\ncart_cv_model = GridSearchCV(cart, cart_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(cart_cv_model.best_params_)\n\n#Final Model\ncart_tuned = DecisionTreeRegressor(**cart_cv_model.best_params_).fit(X_train, y_train)\ncart_tuned_y_pred = cart_tuned.predict(X_test)\ncart_tuned_rmse = np.sqrt(mean_squared_error(y_test, cart_tuned_y_pred))\nprint(cart_tuned_rmse)\n\nsonuc.append(('CART',cart_cv_model.best_params_,cart_tuned_rmse))\n","772c3cf2":"#Random Forests\nrf_cv_model = GridSearchCV(rf, rf_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(rf_cv_model.best_params_)\n\n#Final Model\nrf_tuned = RandomForestRegressor(**rf_cv_model.best_params_).fit(X_train, y_train)\nrf_tuned_y_pred = rf_tuned.predict(X_test)\nrf_tuned_rmse = np.sqrt(mean_squared_error(y_test, rf_tuned_y_pred))\nprint(rf_tuned_rmse)\n\nsonuc.append(('RF',rf_cv_model.best_params_,rf_tuned_rmse))","c2f864cc":"#XGBoost\nxgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(xgb_cv_model.best_params_)\n\n#Final Model\nxgb_tuned = XGBRegressor(**xgb_cv_model.best_params_).fit(X_train,y_train)\nxgb_tuned_y_pred = xgb_tuned.predict(X_test)\nxgb_tuned_rmse = np.sqrt(mean_squared_error(y_test, xgb_tuned_y_pred))\nprint(xgb_tuned_rmse)\n\nsonuc.append(('XGBoost',xgb_cv_model.best_params_,xgb_tuned_rmse))","f19ecc44":"#LightGBM\nlgbm_cv_model = GridSearchCV(lgbm,lgbm_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(lgbm_cv_model.best_params_)\n\n#Final Model\nlgbm_tuned = LGBMRegressor(**lgbm_cv_model.best_params_).fit(X_train,y_train)\nlgbm_tuned_y_pred = lgbm_tuned.predict(X_test)\nlgbm_tuned_rmse = np.sqrt(mean_squared_error(y_test, lgbm_tuned_y_pred))\nprint(lgbm_tuned_rmse)\n\nsonuc.append(('LightGBM',lgbm_cv_model.best_params_,lgbm_tuned_rmse))","3b9cae8e":"#CatBoost\ncatb_cv_model = GridSearchCV(catb,catb_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(catb_cv_model.best_params_)\n\n#Final Model\ncatb_tuned = CatBoostRegressor(**catb_cv_model.best_params_).fit(X_train,y_train)\ncatb_tuned_y_pred = catb_tuned.predict(X_test)\ncatb_tuned_rmse = np.sqrt(mean_squared_error(y_test, catb_tuned_y_pred))\nprint(catb_tuned_rmse)\n\nsonuc.append(('CatBoost',catb_cv_model.best_params_,catb_tuned_rmse))\n","a197f5e9":"#GBM\ngbm_cv_model = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1).fit(X_train, y_train)\nprint(gbm_cv_model.best_params_)\n\n#Final Model\ngbm_tuned = GradientBoostingRegressor(**gbm_cv_model.best_params_).fit(X_train, y_train)\ngbm_tuned_y_pred = gbm_tuned.predict(X_test)\ngbm_tuned_rmse = np.sqrt(mean_squared_error(y_test, gbm_tuned_y_pred))\nprint(gbm_tuned_rmse)\n\nsonuc.append(('GBM',gbm_cv_model.best_params_,gbm_tuned_rmse))","158b4f09":"sonuc_df = pd.DataFrame(sonuc, columns = [\"Model \u0130smi\",\"Optimum Parametreler\", \"Test Hatas\u0131\"])\nsonuc_df = sonuc_df.set_index(\"Model \u0130smi\")\nprint(sonuc_df)\n\nresult_nonlinear = pd.concat([sonuc_ilkel_df,sonuc_df],axis = 1)\nresult_nonlinear.sort_values('Test Hatas\u0131', ascending=True, inplace = True)\nprint(result_nonlinear)\n\nresult = pd.concat([result_linear,result_nonlinear])\nresult.sort_values(\"Test Hatas\u0131\", ascending = True, inplace = True)\nprint(result)","e71b6a92":"result","00d66e59":"#### Optimizasyonsuz","05e21f3e":"### 2). Veriyi Okuma","ce7be6b9":"#### Lasso Regresyon","1fe021f8":"### Veri setindeki kategorik de\u011fi\u015fkenlere eri\u015fim","48425461":"#### De\u011fi\u015fken Standardizayonu","3c10d490":"#### Ridge Regresyon - HiperParametre Optimizasyonu","bc5c369b":"#### LabelEncoder","867e7e3e":"#### B).Do\u011frusal Olmayan Regresyon Modelleri","ab342696":"#### Lasso Regresyon - HiperParametre Optimizasyonu","ccd68cca":"### Veri setindeki s\u00fcrekli de\u011fi\u015fkenlere eri\u015fim","a2332dbc":"## Hitters Veri Seti ile Maa\u015f Tahmin Modelleri K\u0131yaslama","1e3c4686":"### 1). K\u00fct\u00fcphane Import \u0130\u015flemleri","590b2e2f":"## Model Tuning","e22942d4":"#### \u00c7\u0131kt\u0131 Haz\u0131rlama","49feb41c":"#### \u00c7\u0131kt\u0131 Haz\u0131rlama","301d6c5b":"### 4). Veri \u00d6n \u0130\u015fleme","c8fa70f9":"### 5). Model Kurma ","0f1be14e":"#### Do\u011frusal Regresyon","71ba6240":"#### ElasticNet Regresyon - HiperParametre Optimizasyonu","d1827ca7":"#### ElasticNet Regresyon","0cc0a7ec":"### 3). Ke\u015fif\u00e7i Veri Analizi","c871fae6":"#### \u00c7ok De\u011fi\u015fkenli Ayk\u0131r\u0131 G\u00f6zlem Analizi (LOF)","9ef5d653":"#### Ridge Regresyon","7f833aff":"#### Eksik De\u011fer","ac08f908":"#### AMA\u00c7\n\nBu \u00e7al\u0131\u015fmadaki amac\u0131m, Hitters veri seti i\u00e7in makine \u00f6\u011frenmesi modellerini kurup, hata skorlar\u0131n\u0131 minimize etmektir. Bu ama\u00e7la yapt\u0131\u011f\u0131m \u00e7al\u0131\u015fmalar a\u015fa\u011f\u0131dad\u0131r :\n\n#### **1).** Hitters Veri Seti okundu.\n#### **2).** Ke\u015fif\u00e7i Veri Analizi ile;\n* veri setinin yap\u0131sal bilgilerine bak\u0131ld\u0131.\n* veri setindeki de\u011fi\u015fkenlerin tiplerine bak\u0131ld\u0131.\n* object tipini kategorik tipe d\u00f6n\u00fc\u015ft\u00fcrme i\u015flemi yap\u0131ld\u0131.\n* veri setinin boyut bilgisine eri\u015fildi.\n* veri setindeki hangi de\u011fi\u015fkenden ka\u00e7 tane eksik g\u00f6zlem oldu\u011fu bilgisine eri\u015fildi. Sadece ba\u011f\u0131ml\u0131 de\u011fi\u015fkenimiz olan \"Salary\" de 59 adet eksik g\u00f6zlem oldu\u011fu g\u00f6zlendi.\n* veri setinin betimsel istatistiklerine bak\u0131ld\u0131.\n* veri setindeki kategorik de\u011fi\u015fkenlere eri\u015filip, bu de\u011fi\u015fkenlerin s\u0131n\u0131flar\u0131 g\u00f6rselle\u015ftirildi.\n* veri setindeki s\u00fcrekli de\u011fi\u015fkenlere eri\u015filip, betimsel istatistiklerine bak\u0131ld\u0131.\n\n#### **3).** Veri \u00d6n \u0130\u015fleme k\u0131sm\u0131nda; \nVeri setindeki s\u00fcrekli de\u011fi\u015fken ve kategorik de\u011fi\u015fkenlere ayr\u0131 ayr\u0131 i\u015flemler yap\u0131ld\u0131. \u00c7\u00fcnk\u00fc kategorik de\u011fi\u015fkenlere Label Encoding y\u00f6ntemi uyguland\u0131ktan sonra, ayk\u0131r\u0131 g\u00f6zlem analizine kategorik de\u011fi\u015fkenler dahil edilmek istenmedi.\n* s\u00fcrekli de\u011fi\u015fkenlerde, hangi de\u011fi\u015fkenden ka\u00e7 tane eksik g\u00f6zlem oldu\u011funa bak\u0131ld\u0131.\n* sadece \"Salary\" de\u011fi\u015fkeninde olan eksik g\u00f6zlemler, KNNImputer yakla\u015f\u0131m\u0131 ile dolduruldu.\n* kategorik de\u011fi\u015fkenlere Label Encoding d\u00f6n\u00fc\u015f\u00fcm\u00fc uyguland\u0131.\n* s\u00fcrekli de\u011fi\u015fkenlere ayk\u0131r\u0131 g\u00f6zlemler i\u00e7in LOF y\u00f6ntemi ile \u00e7ok de\u011fi\u015fkenli ayk\u0131r\u0131 g\u00f6zlem analizi yap\u0131ld\u0131 ve ayk\u0131r\u0131 olan de\u011ferler veri setinden uzakla\u015ft\u0131r\u0131ld\u0131.\n* s\u00fcrekli de\u011fi\u015fkenlerden uzakla\u015ft\u0131r\u0131lan indexlerin kategorik de\u011fi\u015fken dataframeinden de silinmesi sa\u011fland\u0131. \n* s\u00fcreki ve kategorik de\u011fi\u015fken dataframelerinde ayr\u0131 ayr\u0131 index resetleme i\u015flemi yap\u0131ld\u0131.\n* kategorik de\u011fi\u015fkeni i\u00e7ermeyen ve ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin(Salary) at\u0131lm\u0131\u015f hali \u00fczerinden standardizasyon i\u015flemi uyguland\u0131.\n* de\u011fi\u015fken standardizasyonundan sonra, model kurulmadan \u00f6nce, normalize edilmi\u015f s\u00fcrekli de\u011fi\u015fken ve kategorik de\u011fi\u015fkenler birle\u015ftirilerek, veri setinin son halini almas\u0131 sa\u011fland\u0131.\n\n#### **4).** Model Kurma a\u015famas\u0131nda;\n\n\u00d6\u011frendi\u011fimiz makine \u00f6\u011frenmesi modelleri ile ilk \u00f6nce herhangi bir hiperparametre optimizasyonu uygulanmadan, modellerin ilkel test hatas\u0131 bulundu.\n\u00d6\u011frendi\u011fimiz makine \u00f6\u011frenmesi modelleri ile kurulan modeller, hiperparametre optimizasyonu ile tune edilerek, bulunan test hatas\u0131 d\u00fc\u015f\u00fcr\u00fclmeye \u00e7al\u0131\u015f\u0131larak ger\u00e7ek test hatalar\u0131 bulundu.","60c7633e":"#### A).Do\u011frusal Regresyon Modelleri"}}