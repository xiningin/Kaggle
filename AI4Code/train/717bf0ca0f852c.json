{"cell_type":{"0ea76ab1":"code","8d7399e8":"code","13894a7a":"code","9539a2ab":"code","b78779f8":"code","4d430dc5":"code","7df4fd9f":"code","fb22e2e9":"code","e6c4d9e1":"code","258cb29e":"code","acd2b2d5":"code","a5bfdaf1":"code","b998045b":"code","1d83bd27":"code","3b24af6e":"code","410f5be8":"code","df86565a":"code","3524119d":"code","e2d0b31c":"code","a7894b62":"code","f5b7e815":"code","3a51638e":"code","9fe8653c":"markdown","1e3f7fc1":"markdown","7d74cfa8":"markdown","46f16371":"markdown","d62b63fe":"markdown","aa7ab6d6":"markdown","dd7e950b":"markdown","95d79825":"markdown","c93c0229":"markdown","aea04feb":"markdown","d4641591":"markdown","709971e2":"markdown","6c42f314":"markdown","6eb2a909":"markdown","04e65294":"markdown","55d5aebc":"markdown","dd9226e8":"markdown","a51e5b92":"markdown","8b4b3bc7":"markdown"},"source":{"0ea76ab1":"# Import de l'environnement classique en Data Science\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import de la fonction permettant de s\u00e9parer notre jeu de donn\u00e9es en jeux d'entra\u00eenement et de test\nfrom sklearn.model_selection import train_test_split\n\n# Import de la fonction permettant d'impl\u00e9menter une r\u00e9gression lin\u00e9aire\nfrom sklearn.linear_model import LinearRegression\n\n# Import de la fonction permettant d'impl\u00e9menter un mod\u00e8le de r\u00e9gression se basant sur les for\u00eats al\u00e9atoires\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Import de m\u00e9triques permettant de quantifier la performance de notre mod\u00e8le de r\u00e9gression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Commande permettant d'afficher des graphiques directement dans ce notebook\n%matplotlib inline","8d7399e8":"# Import du fichier CSV 'housing' dans un DataFrame nomm\u00e9 df\ndf = pd.read_csv('..\/input\/housing\/housing.csv')\n\n# Affichage des 10 premi\u00e8res lignes de df\ndf.head(10)","13894a7a":"# Affichage d'informations sur les colonnes de notre jeu de donn\u00e9es\ndf.info()","9539a2ab":"# Comptage du nombre de doublons\ndf.duplicated().sum()","b78779f8":"# Suppression des valeurs manquantes\ndf = df.dropna()","4d430dc5":"# Affichage des nouvelles dimensions de df apr\u00e8s suppression des valeurs manquantes\ndf.shape","7df4fd9f":"# Affichage des districts en Californie selon leurs coordonn\u00e9es g\u00e9ographiques, color\u00e9s par prix\ndf.plot(kind = 'scatter', x = 'longitude', y = 'latitude', c = 'median_house_value', cmap = 'rainbow', s = 3,figsize = (12, 12))\nplt.show()","fb22e2e9":"# Affichage des districts en Californie selon leurs coordonn\u00e9es g\u00e9ographiques, color\u00e9s par proximit\u00e9 \u00e0 l'oc\u00e9an\nsns.lmplot(x = 'longitude', y = 'latitude', data = df, markers = '.', hue = 'ocean_proximity', fit_reg = False, height = 9)\nplt.show()","e6c4d9e1":"# Affichage des modalit\u00e9s de la variable 'ocean_proximity'\ndf['ocean_proximity'].value_counts()","258cb29e":"# Affichage de bo\u00eetes \u00e0 moustaches en fonction des modalit\u00e9s de la variable 'ocean_proximity'\nplt.figure(figsize = (12, 12))\nsns.boxplot(x = 'ocean_proximity', y = 'median_house_value', data = df)\nplt.show()","acd2b2d5":"# Renommage des modalit\u00e9s de la variable 'ocean_proximity' afin de pouvoir avoir des noms d'indicatrices propres et sans espaces\ndf['ocean_proximity'] = df['ocean_proximity'].replace(to_replace = ['INLAND', '<1H OCEAN', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND'],\n                                                      value = ['inland', '<1H', 'nearOcean', 'nearBay', 'island'])\n\n# Transformation de la variable 'ocean_proximity' en variables indicatrices\ndf = df.join(pd.get_dummies(df['ocean_proximity'], prefix = 'op_')).drop(['ocean_proximity'], axis = 1)\n\n# Affichage d'une heatmap des corr\u00e9lations entre les variables de df selon le crit\u00e8re de Pearson, regroup\u00e9es par clusters class\u00e9s par proximit\u00e9\nsns.clustermap(abs(df.drop(['longitude', 'latitude'], axis = 1).corr(method = 'pearson')), cmap = \"coolwarm\")\nplt.show()","a5bfdaf1":"# Affichage des valeurs de corr\u00e9lations de Pearson entre la variable cible 'median_house_value' et les autres\ncorrelations_house_value = df.drop(['longitude', 'latitude'], axis = 1).corr(method = 'pearson')['median_house_value'].drop('median_house_value', axis = 0)\nprint(abs(correlations_house_value).sort_values(ascending = False))","b998045b":"# Stockage de la variable cible 'median_house_value' dans un vecteur nomm\u00e9 'target'\ntarget = df['median_house_value']\n\n# Stockage des variables explicatives dans un DataFrame nomm\u00e9 'explanatory_vars'\nexplanatory_vars = df.drop(['median_house_value'], axis = 1)\n\n# S\u00e9paration en un jeu d'apprentissage contenant 80% des donn\u00e9es et un jeu de test avec les donn\u00e9es restantes\nX_train, X_test, y_train, y_test = train_test_split(explanatory_vars, target, test_size = 0.2, random_state = 42)","1d83bd27":"# Instanciation d'un mod\u00e8le de r\u00e9gression lin\u00e9aire nomm\u00e9 \"lin_reg\" \u00e0 l'aide de la classe LinearRegression\nlin_reg = LinearRegression()\n\n# Entra\u00eenement de notre mod\u00e8le lin_reg\nlin_reg.fit(X_train, y_train)\n\n# Stockage des pr\u00e9dictions effectu\u00e9es par notre mod\u00e8le lin_reg dans un vecteur y_lin_reg\ny_lin_reg = lin_reg.predict(X_test)","3b24af6e":"# Comparaison entre la pr\u00e9diction de notre mod\u00e8le lin_reg et les r\u00e9sultats attendus\nplt.figure(figsize = (12, 12))\nplt.scatter(y_test, y_lin_reg)\nplt.plot([y_test.min(), y_test.max()],[y_test.min(), y_test.max()], color = 'red', linewidth = 3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Pr\u00e9diction de prix\")\nplt.title(\"Prix r\u00e9els vs pr\u00e9dictions\")\nplt.show()","410f5be8":"# Calcul de la racine carr\u00e9 de l'erreur quadratique moyenne (rmse)\nnp.sqrt(mean_squared_error(y_test, y_lin_reg))","df86565a":"# Filtre sur les maisons qui font moins de 500 000 USD\ndf_inliers = df.loc[df['median_house_value'] < 500000]\n\ntarget_inliers = df_inliers['median_house_value']\nexplanatory_vars_inliers = df_inliers.drop(['median_house_value'], axis = 1)\nX_train_inliers, X_test_inliers, y_train_inliers, y_test_inliers = train_test_split(explanatory_vars_inliers, target_inliers, test_size = 0.2, random_state = 42)\n\nlin_reg_inliers = LinearRegression()\nlin_reg_inliers.fit(X_train_inliers, y_train_inliers)\ny_lin_reg_inliers = lin_reg_inliers.predict(X_test_inliers)\n\nplt.figure(figsize = (12, 12))\nplt.scatter(y_test_inliers, y_lin_reg_inliers)\nplt.plot([y_test_inliers.min(), y_test_inliers.max()],[y_test_inliers.min(), y_test_inliers.max()], color = 'red', linewidth = 3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Pr\u00e9diction de prix\")\nplt.title(\"Prix r\u00e9els vs pr\u00e9dictions\")\nplt.show()","3524119d":"# Calcul de la racine carr\u00e9 de l'erreur quadratique moyenne (rmse)\nnp.sqrt(mean_squared_error(y_test_inliers, y_lin_reg_inliers))","e2d0b31c":"# Calcul du coefficient de d\u00e9termination\n# (= score R\u00b2 = rapport des variances estim\u00e9es\/r\u00e9elles)\nr2_lin_reg = r2_score(y_test_inliers, y_lin_reg_inliers)\n#\u00e9quivalent \u00e0 : lin_reg_inliers.score(X_test_inliers, y_test_inliers)\n\nr2_lin_reg","a7894b62":"# Instanciation d'un mod\u00e8le de r\u00e9gression RandomForestRegressor\nrf_reg = RandomForestRegressor()\n\n# Entra\u00eenement de notre mod\u00e8le rf_reg\nrf_reg.fit(X_train, y_train)\n\n# Stockage des pr\u00e9dictions effectu\u00e9es par notre mod\u00e8le rf_reg dans un vecteur y_rf_reg\ny_rf_reg = rf_reg.predict(X_test)\n\n# Calcul du coefficient de d\u00e9termination du mod\u00e8le\nr2_rf_reg = rf_reg.score(X_test, y_test)\n\nr2_rf_reg","f5b7e815":"# Comparaison entre la pr\u00e9diction de notre mod\u00e8le rf_reg et les r\u00e9sultats attendus\nplt.figure(figsize = (12, 12))\nplt.scatter(y_test, y_rf_reg)\nplt.plot([y_test.min(), y_test.max()],[y_test.min(), y_test.max()], color = 'red', linewidth = 3)\nplt.xlabel(\"Prix\")\nplt.ylabel(\"Pr\u00e9diction de prix\")\nplt.title(\"Prix r\u00e9els vs pr\u00e9dictions\")\nplt.show()","3a51638e":"# Calcul du RMSE de notre mod\u00e8le rf_reg\nnp.sqrt(mean_squared_error(y_test, y_rf_reg))","9fe8653c":"### R\u00e9gression bas\u00e9e sur des for\u00eats al\u00e9atoires","1e3f7fc1":"On voit ainsi que les variables explicatives $X_{i}$ arrive \u00e0 expliquer \u00e0 travers notre mod\u00e8le de r\u00e9gression lin\u00e9aire environ **62%** des variations du prix des districts en Californie.","7d74cfa8":"### Informations sur les donn\u00e9es :\nSource : https:\/\/github.com\/ageron\/handson-ml\/tree\/master\/datasets\/housing \\\nLivre : https:\/\/www.amazon.fr\/Hands-Machine-Learning-Scikit-learn-Tensorflow\/dp\/1492032646\/ref=pd_lpo_1?pd_rd_i=1492032646&psc=1\n\n\"*This dataset is a modified version of the California Housing dataset available from [Lu\u00eds Torgo's page](http:\/\/www.dcc.fc.up.pt\/~ltorgo\/Regression\/cal_housing.html) (University of Porto). Lu\u00eds Torgo obtained it from the StatLib repository (which is closed now). The dataset may also be downloaded from StatLib mirrors.*\n\n*This dataset appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. They built it using the 1990 California census data. It contains one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).*\n\n*The dataset in this directory is almost identical to the original, with two differences:*\n\n- *207 values were randomly removed from the `total_bedrooms` column, so we can discuss what to do with missing data.*\n- *An additional categorical attribute called `ocean_proximity` was added, indicating (very roughly) whether each block group is near the ocean, near the Bay area, inland or on an island. This allows discussing what to do with categorical data.*\n\n*Note that the block groups are called \"districts\" in the Jupyter notebooks, simply because in some contexts the name \"block group\" was confusing.*\"","46f16371":"## Import des donn\u00e9es","d62b63fe":"D'apr\u00e8s ces deux graphiques, on voit que la proximit\u00e9 \u00e0 l'oc\u00e9an d'un district en Californie a l'air d'impacter positivement son prix.","aa7ab6d6":"### R\u00e9gression lin\u00e9aire","dd7e950b":"## Analyse exploratoire de donn\u00e9es","95d79825":"On peut voir que la variable cible `median_house_value` est tr\u00e8s corr\u00e9l\u00e9e \u00e0 la variable **median_income**. \\\nDe plus, les variables **total_rooms**, **population**, **total_bedrooms**, **households** sont tr\u00e8s corr\u00e9l\u00e9es entre elles. Il faudrait donc faire de la s\u00e9lection de variables car en th\u00e9orie, un mod\u00e8le de Machine Learning fonctionne mieux avec des variables ind\u00e9pendantes.","c93c0229":"On voit que beaucoup de maisons font exactement 500 000 USD mais qui sont tr\u00e8s mal pr\u00e9dites par le mod\u00e8le, ce que nous pouvons consid\u00e9rer comme des outliers. Essayons donc de ne pas prendre en compte ces maisons-l\u00e0 et relancer notre mod\u00e8le :","aea04feb":"On peut voir notamment que la seule variable contenant des valeurs manquantes est `total_bedrooms`.","d4641591":"Les bo\u00eetes \u00e0 moustaches des modalit\u00e9s de la variable 'ocean_proximity' confirment notre hypoth\u00e8se : les maisons loin de l'oc\u00e9an sont significativement moins chers, m\u00eame si un grand nombre d'outliers existent et montrent que plusieurs maisons peuvent \u00eatre tr\u00e8s chers tout en \u00e9tant \u00e9loign\u00e9s de l'oc\u00e9an.\n\nIl faut donc pousser notre analyse sur d'autres caract\u00e9ristiques afin de mieux comprendre ce qui impacte r\u00e9ellement le prix d'une maison, en commen\u00e7ant d'abord par transformer les modalit\u00e9s de la variable 'ocean_proximity' en \"dummies\" (variables indicatrices) afin de pouvoir calculer leurs corr\u00e9lations avec les autres.","709971e2":"Les performances du mod\u00e8le de r\u00e9gression par for\u00eats al\u00e9atoires sont bien meilleures que celles obtenues par le mod\u00e8le de r\u00e9gression lin\u00e9aire, nous pouvons effectivement voir que la racine carr\u00e9 de l'erreur quadratique moyenne a baiss\u00e9 de 12 000 USD et le coefficient de d\u00e9termination a augment\u00e9 de 20 points.","6c42f314":"Calculons \u00e9galement le coefficient de d\u00e9termination ($R^2$) qui permet de d\u00e9terminer quel pourcentage des variations de la variable cible $y$ est expliqu\u00e9e par les variations des variables explicatives $X_{i}$ :","6eb2a909":"Pour le mod\u00e8le de r\u00e9gression par for\u00eats al\u00e9atoires, nous allons laisser dans notre jeu d'entra\u00eenement les maisons dont la valeur est de 500 000 USD.","04e65294":"La diff\u00e9rence entre les valeurs de pr\u00e9dictions et les valeurs r\u00e9elles a l'air moindre, quantifions-l\u00e0 afin d'en avoir le coeur net :","55d5aebc":"Le jeu de donn\u00e9es contient 20640 lignes et 9 colonnes :\n- **longitude**\n- **latitude**\n- **housing_median_age**: \u00c2ge median d'une maison dans un p\u00e2t\u00e9 dans un district --> plus la valeur est forte, plus le district est ancien\n- **total_rooms**: Nombre total de pi\u00e8ces dans un district\n- **total_bedrooms**: Nombre total de chambre dans un district\n- **population**: Nombre total de personnes r\u00e9sidant dans un district\n- **households**: Nombre total de m\u00e9nages r\u00e9sidant dans un district\n- **median_income**: Revenu m\u00e9dian des m\u00e9nages au sein d'un district (en dizaines de milliers de dollars am\u00e9ricains)\n- **ocean_proximity**: Emplacement de la maison par rapport \u00e0 l'oc\u00e9an\/la mer\n\n\\+ la cible : `median_house_value`: Valeur m\u00e9diane des maisons d'un district (en dollars am\u00e9ricains)","dd9226e8":"## Machine Learning","a51e5b92":"# Exercice California Housing Data (1990)","8b4b3bc7":"## Nettoyage de donn\u00e9es"}}