{"cell_type":{"19612e9e":"code","481d953f":"code","36787dc9":"code","0d7d368e":"code","677941be":"code","5ca84a3a":"code","8ff68b4e":"code","92d5d530":"code","3d99ff22":"code","65f61ecb":"code","abc83d80":"code","7f64bbb2":"code","6e3076cf":"code","d20eab5f":"code","1a691f96":"code","a36cc885":"code","3737dd82":"code","8356e2d6":"code","e742a4bd":"code","b065532d":"code","1ae7a683":"code","21955a90":"code","2834c65f":"code","915e538a":"code","a5d7dfc5":"code","1948f92d":"code","b3d0f512":"code","d506f137":"code","251d9498":"code","ea0d4a2a":"code","e122f393":"code","eeed8178":"code","94064f74":"code","42d2747a":"code","c72e5f93":"code","aff1728a":"code","c0a0c6e5":"code","b32161a5":"code","608b055b":"code","e66295cf":"code","7ac47c75":"code","1f930d38":"code","59ab4c89":"code","72a4a285":"code","6065e7d0":"code","e4154e04":"code","d110a467":"code","9ca57561":"code","077db132":"code","1d10d58a":"code","efddd248":"code","f3531830":"code","72cb73cd":"code","25ee8517":"code","0460c05f":"code","16fec78b":"code","6952684c":"code","48036f40":"markdown","510532ed":"markdown","bdaae840":"markdown","164446a6":"markdown","3d9999fc":"markdown","20f5136e":"markdown","7152d172":"markdown","51a37bc4":"markdown","939a4b84":"markdown","e8d17202":"markdown","42304b3f":"markdown","8042bc63":"markdown","5d392544":"markdown"},"source":{"19612e9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os # im,porting os\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","481d953f":"\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n#Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use('ggplot')\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# File system manangement\nimport os\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#importing tensorflow libraries\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","36787dc9":"train=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","0d7d368e":"train.head()","677941be":"test.head()","5ca84a3a":"display(train.info())","8ff68b4e":"display(test.info())","92d5d530":"#!pip install pycomp","3d99ff22":"from pycomp.viz.insights import *\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","65f61ecb":"display(train['target'].value_counts())\nplt.figure(figsize=(8,6))\ncarrier_count = train[\"target\"].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(carrier_count.index, carrier_count.values, alpha=1,edgecolor='k',palette='rocket')\nplt.title('Frequency Distribution of target')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('target', fontsize=12)\nplt.xticks((0,1),('Fake', 'Real'))\nplt.show()","abc83d80":"mapping = {1: 'Real', 0: 'Fake'}\nplot_donut_chart(df=train, col='target', label_names=mapping, colors=[\"#ff7f51\",\"#ff9b54\"],\n                 title='Target Value Distribution')","7f64bbb2":"import plotly.graph_objects as go\ntrain['length'] = train['text'].apply(len)\ndata = [go.Box(y=train[train['target']==0]['length'],name='Fake'),\n        go.Box(y=train[train['target']==1]['length'],name='Real')]\nlayout = go.Layout(title = 'Comparison of text length in Tweets')\nfig = go.Figure(data=data, layout=layout)\nfig.show()","6e3076cf":"train.describe()","d20eab5f":"train=train.drop(['id','keyword','location','length'],axis=1,inplace=False)","1a691f96":"train_feature=train.drop('target',axis=1)\ntrain_target=train.target","a36cc885":"test_feature=test.drop(['id','keyword','location'],axis=1,inplace=False)","3737dd82":"display(train_feature.isna().sum())\ndisplay(train_target.isna().sum())","8356e2d6":"display(train_feature.shape,train_feature.dtypes)\ndisplay(train_target.shape,train_target.dtypes)","e742a4bd":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() # Convert to lower\n    text = re.sub('\\[.*?\\]', '', text) #remove texts in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)#remove links\n    text = re.sub('<.*?>+', '', text)#remove special characters\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)#remove punctuation\n    text = re.sub('\\n', '', text)#remove words containing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain_feature['text'] = train_feature['text'].apply(lambda x: clean_text(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain_feature['text'].head()","b065532d":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_feature['text'] = train_feature['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain_feature['text'].head()","1ae7a683":"def remove_stopwords(text):\n    \"\"\"Removing stopwords belonging to english language\n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\ntrain_feature['text'] = train_feature['text'].apply(lambda x : remove_stopwords(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x : remove_stopwords(x))\ntrain_feature.head()","21955a90":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain_feature['text'] = train_feature['text'].apply(lambda x : combine_text(x))\ntest_feature['text'] = test_feature['text'].apply(lambda x : combine_text(x))\ntrain_feature['text']\ntrain_feature.head()","2834c65f":"import cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()","915e538a":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(train_feature['text'], 30)\ndf2 = pd.DataFrame (common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', \n                        linecolor='black',title='Top 30 unigrams used in Tweets',color='#48cae4')","a5d7dfc5":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_bigram(train_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#720026')","1948f92d":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_trigram(train_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#e9b827')","b3d0f512":"def get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_words(test_feature['text'], 30)\ndf2 = pd.DataFrame (common_words,columns=['word','count'])\ndf2.groupby('word').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', \n                        linecolor='black',title='Top 30 unigrams used in Tweets',color='#48cae4')","d506f137":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_bigram(test_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#720026')","251d9498":"def get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ncommon_words = get_top_n_trigram(test_feature['text'], 30)\ndf3 = pd.DataFrame(common_words, columns = ['words' ,'count'])\ndf3.groupby('words').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 30 bigrams used in Tweets', color='#e9b827')","ea0d4a2a":"from wordcloud import WordCloud\nplt.figure(figsize=(16,8))\nwc = WordCloud(background_color=\"black\", max_words=150,max_font_size=150,random_state=42)\nwc.generate(' '.join(train_feature['text']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","e122f393":"from wordcloud import WordCloud\nplt.figure(figsize=(16,8))\nwc = WordCloud(background_color=\"black\", max_words=150,max_font_size=150,random_state=42)\nwc.generate(' '.join(test_feature['text']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","eeed8178":"tf_idf= TfidfVectorizer()\nX=tf_idf.fit_transform(train_feature.text)\nX=X.toarray()","94064f74":"\nX_train, X_val, y_train, y_val = train_test_split(X,train['target'], test_size=0.33, random_state=42)","42d2747a":"parameters = {'alpha': (1, 0.1, 0.01, 0.001, 0.0001, 0.00001)} \nmodel_nv=MultinomialNB()\nclf = GridSearchCV(model_nv,parameters,cv=10, scoring='accuracy')\nclf.fit(X_train, y_train)\nprint(\"The best Score\",clf.best_score_)\nprint(\"-------\")\nprint(\"The best Estimator\",clf.best_estimator_)","c72e5f93":"y_pred=clf.predict(X_val)\naccuracy=accuracy_score(y_val,y_pred)\naccuracy","aff1728a":"Y_Pred=clf.predict(X_val)\ncnf_mat=confusion_matrix(y_val, Y_Pred)\nfig, ax = plot_confusion_matrix(conf_mat=cnf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","c0a0c6e5":"print(classification_report(y_val,Y_Pred))","b32161a5":"from sklearn.ensemble import  AdaBoostClassifier\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8),random_state = 42)\nparameters = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"random\"],#\"algorithm\" : [\"SAMME\",\"SAMME.R\"]\n              \"n_estimators\" :[100],\n              \"learning_rate\":  [0.05, 0.5, 1]}\nada_clf = GridSearchCV(ada_clf, parameters, cv=3, scoring=\"accuracy\")\nada_clf.fit(X_train, y_train)\nprint(f'Best parameters {ada_clf.best_params_}')\nprint('-----')\nprint(f'Mean cross-validated accuracy score of the best_estimator: '+f'{ada_clf.best_score_:.3f}')\n# Ada_clf = AdaBoostClassifier(DecisionTreeClassifier,n_estimators=5,random_state=1)\n# Ada_clf.fit(X_train, y_train)\n# y_pred = Ada_clf.predict(X_val)\n# score = metrics.accuracy_score(y_val,y_pred)\n# print(\"accuracy: %0.3f\" %score)","608b055b":"print(\"Test Accuracy:\",ada_clf.score(X_val, y_val))","e66295cf":"Y_Pred=ada_clf.predict(X_val)\ncnf_mat=confusion_matrix(y_val, Y_Pred)\nfig, ax = plot_confusion_matrix(conf_mat=cnf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","7ac47c75":"print(classification_report(y_val,Y_Pred))","1f930d38":"## hyperparameter tuning example grid for catboost : \nimport catboost as cb\nfrom catboost import CatBoostClassifier\n\nparameters = {'depth': [4, 7, 10],\n              'learning_rate' : [0.03, 0.1, 0.15],\n              'l2_leaf_reg': [1,9],\n              'iterations': [100]}\ncb_clf = cb.CatBoostClassifier()\ncb_clf = GridSearchCV(cb_clf, parameters, scoring=\"roc_auc\", cv = 5)\ncb_clf.fit(X_train,y_train)\nprint(f'Best parameters {cb_clf.best_params_}')\nprint('-----')\nprint(f'Mean cross-validated accuracy score of the best_estimator: '+f'{cb_clf.best_score_:.3f}')","59ab4c89":"print(\"Test Accuracy:\",cb_clf.score(X_val, y_val))","72a4a285":"Y_Pred=cb_clf.predict(X_val)\ncnf_mat=confusion_matrix(y_val, Y_Pred)\nfig, ax = plot_confusion_matrix(conf_mat=cnf_mat,figsize=(8, 8),\n                                show_absolute=True,\n                                show_normed=True,\n                                colorbar=True)\nplt.show()","6065e7d0":"voc_size=len(train_feature['text'])+1 #deciding My Vocabulary Size","e4154e04":"onehot_representation=[one_hot(words,voc_size) for words in train_feature['text']]\nonehot_representation","d110a467":"sent_length=120 #Since to make each sentence of equal length we are padding. \nembedding=pad_sequences(onehot_representation,padding='post',maxlen=sent_length)\nprint(embedding)\ndisplay(len(embedding))","9ca57561":"## Creating model\nembedding_vector_features=200\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length,trainable=True)) #Embedding Layer\nmodel.add(LSTM(100)) # 1LSTM Layer with 128 Neurons\n#model.add(LSTM(output_nodes, dropout = dropout, recurrent_dropout = recurrent_dropout))\nmodel.add(Dense(1,activation='sigmoid'))#Since Classification type problem so Dense Layer\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","077db132":"X_final=np.array(embedding)\ny_final=np.array(train['target'])","1d10d58a":"X_final.shape,y_final.shape","efddd248":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)","f3531830":"history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=20,batch_size=100)","72cb73cd":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))","25ee8517":"voc_size=len(test_feature['text'])+1 #deciding My Vocabulary Size\ntest_onehot=[one_hot(words,voc_size) for words in test_feature['text']]\nsent_length=120 #Since to make each sentence of equal length we are padding. \ntest_embedding=pad_sequences(test_onehot,padding='post',maxlen=sent_length)\nprint(test_embedding)\ndisplay(len(test_embedding))","0460c05f":"test_final=np.array(test_embedding)","16fec78b":"y_pred=model.predict_classes(test_final)","6952684c":"submission=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target']=y_pred\nsubmission.to_csv('new_submission.csv') \nsubmission.head(5)","48036f40":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Importing Necessary Packages <\/h1>","510532ed":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Neural Net<\/h1>","bdaae840":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Text Preprocessing<\/h1>","164446a6":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">AdaBoost-GridSearch<\/h1>","3d9999fc":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Importing Data<\/h1>","20f5136e":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">WordCloud Test Set<\/h1>","7152d172":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Uni-gram,Bi-gram,Tri-Gram for Test Set<\/h1>","51a37bc4":"**Embedding Representation**","939a4b84":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">WordCloud Train Set<\/h1>","e8d17202":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Exploratory Text-Data Analysis<\/h1>","42304b3f":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">CatBoost-GridSearch<\/h1>","8042bc63":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Uni-gram,Bi-gram,Tri-Gram for Train Set<\/h1>","5d392544":"<h1 style=\"background-color:DodgerBlue;text-align:center;color:white;\">Tf-idf & Multinomial Naive-Bayes<\/h1>"}}