{"cell_type":{"60aa1301":"code","696a712b":"code","4efafec0":"code","277eb99a":"code","45121160":"code","ec25fa0f":"code","4faaf7d3":"code","adb09fad":"code","15162fa5":"code","2d332ccb":"code","bc2acee1":"code","fc3eaa6e":"code","238a71ef":"code","6d5dc13e":"code","01318c01":"code","85e6f314":"code","eb61a374":"code","54dd46e9":"code","994be5b2":"code","898e74fe":"code","687fb6fb":"code","2d4778c3":"code","57f04eaa":"code","ab4168e0":"code","e5c2533e":"code","1bf654a1":"code","d864dbd0":"code","e379e7d8":"code","34a470f1":"code","afe7b503":"code","8d5730da":"code","6c43d24c":"code","112ac805":"code","cbc888cf":"code","a7b59c47":"code","6d4132e7":"code","6ff26902":"code","2463c973":"code","ebe33f5b":"code","98d2468b":"code","d657a56c":"code","9a108192":"code","fab392b9":"code","4c86e1dd":"code","8e5d4ae7":"code","37a07812":"code","bab62768":"code","00282086":"code","c296c6aa":"code","8cf1b6cb":"code","51dc010f":"code","1d91039f":"code","a9175fa0":"markdown","7e4299a1":"markdown","a3b85ef0":"markdown","3aae960a":"markdown","d169acb1":"markdown","82c1d9f1":"markdown","c8e137d3":"markdown","1605d7b8":"markdown","3a15ed10":"markdown","37b15f13":"markdown","62d64867":"markdown","b94e2239":"markdown","5f6a123b":"markdown","6ac6e114":"markdown","585807d5":"markdown","5a020180":"markdown","d06d2118":"markdown","ae51279b":"markdown","d6a5b035":"markdown","f29d6989":"markdown","68c6a629":"markdown","1b2846db":"markdown","068df323":"markdown","e1c35343":"markdown","d7f01420":"markdown","5a29340d":"markdown","c40900ae":"markdown","c0ebd53e":"markdown","9923c614":"markdown","0c1cb855":"markdown","a25bf30f":"markdown","bfb035e8":"markdown","36883301":"markdown","38690fbd":"markdown","096b58d5":"markdown","0179bb23":"markdown","f9a819b1":"markdown","7278f504":"markdown","9debbd5d":"markdown","5ca42819":"markdown","36944a71":"markdown"},"source":{"60aa1301":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","696a712b":"#import data \ndiabetes_df=pd.read_csv(\"..\/input\/diabetes-dataset\/diabetes.csv\")\ndiabetes_df.head()\n\n","4efafec0":"diabetes_df.info()","277eb99a":"# Reaarange Outcome columns \nl1=[col  for col in diabetes_df.columns if col!=\"Outcome\"]\nl1.append(\"Outcome\")\ndiabetes_df=diabetes_df[l1]\ndiabetes_df.head()","45121160":"def data_clean(df=None):\n        df[\"Exercise\"]=df[\"Exercise\"].replace({\"No\":1,\"Evening\":2,\"Morning\":3,\"Both\":4}).astype(int)\n        df[\"Gender\"]=df[\"Gender\"].replace({\"M\":1,\"F\":0}).astype(int)\n        df[\"CalorieIntake\"].fillna(df[\"CalorieIntake\"].median(),inplace=True)\n        return df\n    \n    \ndiabetes_df_clean=data_clean(diabetes_df)","ec25fa0f":"diabetes_df_clean.head()","4faaf7d3":"sns.pairplot(diabetes_df_clean,hue=\"Outcome\")","adb09fad":"#lda_df use for Without affecting clean dataframe\nLda_df=diabetes_df_clean.copy()","15162fa5":"X=Lda_df.iloc[:,:-1]\ny=Lda_df.iloc[:,-1]\n","2d332ccb":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nclf1=LinearDiscriminantAnalysis(\"eigen\",n_components=1)\n\nX_r1=clf1.fit(X,y).transform(X)\nl2=np.ones(len(X_r1))  ## dummy variable use for X_train,X_test because they not allow 1-D array\n\nlda_df=pd.DataFrame(data=np.column_stack([X_r1,l2,diabetes_df_clean[\"Outcome\"]]),columns=[\"X_r1\",\"l2\",\"Outcome\"])","bc2acee1":"lda_df","fc3eaa6e":"sns.displot(x=lda_df[\"X_r1\"],hue=lda_df[\"Outcome\"])","238a71ef":"from sklearn.model_selection import train_test_split \nX_train,X_test,y_train,y_test=train_test_split(lda_df.iloc[:,:-1],lda_df[\"Outcome\"],test_size=0.33)","6d5dc13e":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nclf=LinearDiscriminantAnalysis()\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)\ny_pre=clf.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pre))","01318c01":"from sklearn.decomposition import PCA\npca=PCA(n_components=2)\nX_embedded=pca.fit_transform(X)\n\npca_df=pd.DataFrame(data=X_embedded,columns=[\"x1\",\"x2\"])\npca_df[\"Outcome\"]=y","85e6f314":"pca_df.head()","eb61a374":"sns.scatterplot(data=pca_df,x='x1',y=\"x2\",hue=\"Outcome\")","54dd46e9":"from sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2,perplexity=30.0,n_iter=5000).fit_transform(X)\nX_embedded.shape","994be5b2":"t_sne_df=pd.DataFrame(data=X_embedded,columns=[\"x1\",\"x2\"])\nt_sne_df[\"Outcome\"]=y","898e74fe":"t_sne_df","687fb6fb":"sns.scatterplot(data=t_sne_df,x='x1',y=\"x2\",hue='Outcome')","2d4778c3":"from matplotlib import pyplot\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nmap_dict={}\nfor i,v in zip(X.columns,importance):\n#          print('Feature: %0d, Score: %.5f' % (i,v))\/abs\n         map_dict[i]=v\n    \n    \n# plot feature importance\npyplot.bar(X.columns, importance)\npyplot.xticks(rotation=90)\npyplot.show()","57f04eaa":"map_dict_list=dict(sorted(map_dict.items(),key=lambda kv: kv[1],reverse=True))","ab4168e0":"map_dict_list","e5c2533e":"X=X[list(map_dict_list.keys())[:5]]\ny=y","1bf654a1":"##","d864dbd0":"class Ml_Model(object):\n    '''\n        Here in this class we will implements methods like fitting,classification report,\n        validation, visualization.\n    '''\n\n    def __init__(self, *arg, scaling=False, balance=False, Oversampling=False):\n        \n        ''' Here in this constructor we will perform the basic preprocessing steps\n            like scaling and balance dataset.\n          \n        '''\n        import numpy as np\n        self.X = arg[0]\n        self.y = arg[1]\n       \n        self.score_dict=None\n        \n        self.scaling=scaling\n   \n        if balance:\n\n            if not Oversampling:\n                from imblearn.under_sampling import RandomUnderSampler\n                b = RandomUnderSampler(sampling_strategy=1)\n                self.X, self.y = b.fit_resample(self.X, self.y)\n\n            else:\n                from imblearn.over_sampling import RandomOverSampler\n                b = RandomOverSampler(sampling_strategy=1)\n                self.X, self.y = b.fit_resample(self.X, self.y)\n\n        if not scaling:\n            from sklearn.model_selection import train_test_split\n            from sklearn.preprocessing import StandardScaler\n            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n                self.X, self.y)\n\n        else:\n            from sklearn.model_selection import train_test_split\n            from sklearn.preprocessing import StandardScaler\n            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n                self.X, self.y,random_state=34)\n\n            self.sc = StandardScaler()\n            self.X_train = self.sc.fit_transform(self.X_train)\n            self.X_test = self.sc.transform(self.X_test)\n            \n            \n    def train_test_split_obj(self):\n        ''' this function useful for return scaling train,test data\n        '''\n        \n        \n        return (self.X_train,self.X_test,self.y_train,self.y_test)\n\n    def pass_classifier(self, clf=None):\n        self.clf = clf\n        print(clf)\n\n        return self.clf\n\n    def fit_method_and_score(self):\n\n        clf.fit(self.X_train, self.y_train)\n\n        return clf.predict(self.X_test)\n\n    def classification_report(self):\n        self.y_pred = self.clf.predict(self.X_test)\n        from sklearn.metrics import classification_report\n        return print(classification_report(self.y_test, self.y_pred))\n\n    def plot_boundary(self, X, y, fitted_model):\n        '''helping function for visualization below method(decision boundry of algo)'''\n\n        plt.figure(figsize=(9.8, 5), dpi=100)\n        X = X\n        y = y\n        for i, plot_type in enumerate(['Decision Boundary', 'Decision Probabilities']):\n            plt.subplot(1, 2, i+1)\n\n            mesh_step_size = 0.01  # step size in the mesh\n            x_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\n            y_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\n            xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size), np.arange(\n                y_min, y_max, mesh_step_size))\n            if i == 0:\n                Z = fitted_model.predict(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                try:\n                    Z = fitted_model.predict_proba(\n                        np.c_[xx.ravel(), yy.ravel()])[:, 1]\n                except:\n                    plt.text(0.4, 0.5, 'Probabilities Unavailable', horizontalalignment='center',\n                             verticalalignment='center', transform=plt.gca().transAxes, fontsize=12)\n                    plt.axis('off')\n                    break\n            Z = Z.reshape(xx.shape)\n            plt.scatter(X[y.values == 0, 0], X[y.values == 0, 1],\n                        alpha=0.4, label=\"0\", s=5)\n            plt.scatter(X[y.values == 1, 0], X[y.values == 1, 1],\n                        alpha=0.4, label=\"1\", s=5)\n            plt.imshow(Z, interpolation='nearest', cmap='RdYlBu_r', alpha=0.15,\n                       extent=(x_min, x_max, y_min, y_max), origin='lower')\n            plt.title(plot_type + '\\n' +\n                      str(fitted_model).split('(')[0] + ' Test Accuracy: ' + str(np.round(fitted_model.score(X, y), 5)))\n            plt.gca().set_aspect('equal')\n\n        plt.tight_layout()\n        plt.subplots_adjust(top=0.9, bottom=0.08, wspace=0.02)\n\n    def validation(self):\n        '''this function is used for validate our model'''\n\n        from sklearn.model_selection import StratifiedKFold\n        from sklearn.metrics import f1_score\n        from sklearn.metrics import recall_score\n        from sklearn.metrics import accuracy_score\n        from sklearn.metrics import precision_score\n        from colorama import Fore, Back, Style\n\n        lst_accu_stratified = []\n        lst_recall_stratified = []\n        lst_precision_stratified = []\n\n        lst_f1_score_stratified = []\n        x = 0\n        skf = StratifiedKFold(n_splits=18, shuffle=True, random_state=1)\n      \n        for train_index, test_index in skf.split(self.X, self.y):\n            x_train_fold, x_test_fold = self.X.loc[train_index,\n                                                   :], self.X.loc[test_index, :]\n            y_train_fold, y_test_fold = self.y[train_index], self.y[test_index]\n\n            if not self.scaling :\n                         self.clf.fit(x_train_fold, y_train_fold)\n                         y_pre = clf.predict(self.X_test)\n            else:\n               \n                from sklearn.preprocessing import StandardScaler\n                sc=StandardScaler()\n                x_train_fold = sc.fit_transform(x_train_fold)\n                x_test_fold = sc.transform(x_test_fold)\n                clf.fit(x_train_fold, y_train_fold)\n                y_pre = clf.predict(self.X_test)\n\n\n#                 print(classification_report(self.y_test,y_pre))\n\n            lst_accu_stratified.append(\n                round(accuracy_score(self.y_test, y_pre), 4))\n            lst_recall_stratified.append(\n                round(recall_score(self.y_test, y_pre), 4))\n            lst_f1_score_stratified.append(\n                round(f1_score(self.y_test, y_pre), 4))\n            lst_precision_stratified.append(\n                round(precision_score(self.y_test, y_pre), 4))\n\n            \n        self.score_dict=dict(zip([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\"],\n                            [lst_accu_stratified, lst_recall_stratified, lst_precision_stratified, lst_f1_score_stratified]))\n        \n        for i, j in zip([\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\"], [lst_accu_stratified, lst_recall_stratified, lst_precision_stratified, lst_f1_score_stratified]):\n\n            print(Fore.BLUE+f\"\\n\\n{i} \")\n            print(Style.RESET_ALL)\n\n            print(f'List of possible {i} Score:\\n', j)\n\n            print(f'\\nMaximum {i} Score That can be obtained from this model is:', max(\n                j)*100, '%')\n            print(f'\\nMinimum {i} Score:', min(j)*100, '%')\n            print(\n                f'\\nAverage {i} Score That can be obtained from this model is::', np.mean(j))\n            print(\n                f'\\nMedian {i} Score That can be obtained from this model is::', np.median(j))\n            print('\\nStandard Deviation is:', np.std(j))\n            \n        return None\n            \n    def score_return(self,score=\"F1-Score\"):\n            '''for returning the evalution score return'''\n            \n            try :\n                 return (score,self.score_dict[score])\n\n            except KeyError :\n                    print()\n                    print(Fore.RED+\"KeyError : please follow  given list score format for gaining score list that created by validation function\\n\"+\n                          \":[Accuracy,Recall,Precision,F1-Score]\")\n\n\n                    print()\n                \n       \n            \n            \n\n    def visualization(self):\n        '''this function helps us to visualize for how our alogrithms seperate our classes\n           by decision boundry'''\n        from colorama import Fore, Back, Style\n        print()\n        print(Fore.BLUE+\"\\n Below Test Acurracy Based On PCA\")\n        from sklearn.model_selection import train_test_split\n        from imblearn.under_sampling import RandomUnderSampler\n        from sklearn.preprocessing import StandardScaler\n        from sklearn.decomposition import PCA\n        pca = PCA(n_components=2,)\n        X_embedded = pca.fit_transform(self.X)\n        sc = StandardScaler()\n\n        y_pca = self.y\n        X_pca = X_embedded\n        X_pca = sc.fit_transform(X_pca)\n        b = RandomUnderSampler(sampling_strategy=1)\n        X_pca_balance, y_pca_balance = b.fit_resample(X_pca, y_pca)\n\n        X_pca_balance_train, X_pca_balance_test, y_pca_balance_train, y_pca_balance_test = train_test_split(\n            X_pca_balance, y_pca_balance)\n\n        model = self.clf\n        model.fit(X_pca_balance_train, y_pca_balance_train)\n        model.score(X_pca_balance_test, y_pca_balance_test)\n        self.plot_boundary(X_pca_balance_test, y_pca_balance_test, model)","e379e7d8":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nm1=Ml_Model(X,y,scaling=True,balance=True,Oversampling=False)\nclf=LinearDiscriminantAnalysis()\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\n\nm1.visualization()\nm1.validation()","34a470f1":"recall=m1.score_return(score=\"Recall\")[1]\nrecall_list=[recall]\nnames=[\"LDA\"]\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(recall_list)\nax.set_xticklabels(names)\nplt.title(\"Recall distribution comparision\")\nplt.ylabel(\"Recall\")\nplt.xticks(rotation=75)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([0.88,np.max(recall_list)+0.020])\n\n# plt.yticks(np.arange(0.90, np.max(recall_list)+0.02, 0.01))\n\n\nplt.show()","afe7b503":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nm1=Ml_Model(X,y,scaling=True,balance=True)\nclf=QuadraticDiscriminantAnalysis()\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()","8d5730da":"recall=m1.score_return(\"Recall\")[1]\nrecall_list.append(recall)\n\nnames.append(\"QDA\")\n\nax = fig.add_subplot(111)\nplt.boxplot(recall_list,labels=names)\n# ax.xticks(labels=names)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([np.min(recall_list),np.max(recall_list)+0.001])\n\n# plt.yticks(np.arange(np.min(recall_list), np.max(recall_list), 0.0001))\n\n\nplt.show()","6c43d24c":"from sklearn.svm import SVC\nm1=Ml_Model(X,y,scaling=True,balance=True)\nclf=SVC()\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()","112ac805":"recall=m1.score_return(\"Recall\")[1]\nrecall_list.append(recall)\n\nnames.append(\"SVM\")\n\nax = fig.add_subplot(111)\nplt.boxplot(recall_list,labels=names)\n# ax.xticks(labels=names)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([np.min(recall_list),np.max(recall_list)+0.001])\n\n# plt.yticks(np.arange(np.min(recall_list), np.max(recall_list), 0.0001))\n\n\nplt.show()","cbc888cf":"from sklearn.linear_model import LogisticRegression\n\nm1=Ml_Model(X,y,scaling=True,balance=True)\nclf=LogisticRegression(max_iter=1000,C=1)\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()","a7b59c47":"recall=m1.score_return(\"Recall\")[1]\nrecall_list.append(recall)\n\nnames.append(\"Lg\")\n\nax = fig.add_subplot(111)\nplt.boxplot(recall_list,labels=names)\n# ax.xticks(labels=names)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([np.min(recall_list),np.max(recall_list)+0.001])\n\n# plt.yticks(np.arange(np.min(recall_list), np.max(recall_list), 0.0001))\n\n\nplt.show()","6d4132e7":"from sklearn.neural_network import MLPClassifier\n\nm1=Ml_Model(X,y,scaling=True,balance=True)\nclf=MLPClassifier()\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()","6ff26902":"recall=m1.score_return(\"Recall\")[1]\nrecall_list.append(recall)\n\nnames.append(\"MLP\")\n\nax = fig.add_subplot(111)\nplt.boxplot(recall_list,labels=names)\n# ax.xticks(labels=names)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([np.min(recall_list),np.max(recall_list)+0.001])\n\n# plt.yticks(np.arange(np.min(recall_list), np.max(recall_list), 0.0001))\n\n\nplt.show()","2463c973":"from sklearn.neighbors import KNeighborsClassifier\nm1=Ml_Model(X,y,scaling=True,balance=True)\nclf=KNeighborsClassifier(p=1.5,n_jobs=-1,n_neighbors=5,weights=\"distance\") \nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()","ebe33f5b":"recall=m1.score_return(\"Recall\")[1]\nrecall_list.append(recall)\n\nnames.append(\"KNN\")\n\nax = fig.add_subplot(111)\nplt.boxplot(recall_list,labels=names)\n# ax.xticks(labels=names)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([np.min(recall_list),np.max(recall_list)+0.001])\n\n# plt.yticks(np.arange(np.min(recall_list), np.max(recall_list), 0.0001))\n\n\nplt.show()","98d2468b":"from sklearn.tree import DecisionTreeClassifier\nm1=Ml_Model(X,y,scaling=True,balance=True)\nclf=DecisionTreeClassifier(max_depth=4,max_features=2,min_samples_leaf=20,min_samples_split=8)\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()","d657a56c":"X_train_dc=m1.train_test_split_obj()[0]\nX_test_dc=m1.train_test_split_obj()[1]\ny_train_dc=m1.train_test_split_obj()[2]\ny_test_dc=m1.train_test_split_obj()[3]\n","9a108192":"Dc=DecisionTreeClassifier()\nDc.fit(X_train_dc,y_train_dc)","fab392b9":"from sklearn import tree\nplt.figure(figsize=(10,15))\ntree.plot_tree(Dc,filled=True)\nplt.show()","4c86e1dd":"path=Dc.cost_complexity_pruning_path(X_train_dc,y_train_dc)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","8e5d4ae7":"ccp_alphas","37a07812":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha,)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","bab62768":"train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","00282086":"m1=Ml_Model(X,y,balance=True)\nclf=DecisionTreeClassifier(ccp_alpha=0.02)\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()","c296c6aa":"from sklearn import tree\nplt.figure(figsize=(10,15))\ntest=DecisionTreeClassifier(ccp_alpha=0.02)\ntest.fit(X_train_dc,y_train_dc)\ntree.plot_tree(test,filled=True)\nplt.show()","8cf1b6cb":"recall=m1.score_return(\"Recall\")[1]\nrecall_list.append(recall)\n\nnames.append(\"Decision Tree\")\n\nax = fig.add_subplot(111)\nplt.boxplot(recall_list,labels=names)\n# ax.xticks(labels=names)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([np.min(recall_list),np.max(recall_list)+0.001])\n\n# plt.yticks(np.arange(np.min(recall_list), np.max(recall_list), 0.0001))\n\n\nplt.show()","51dc010f":"\nfrom sklearn.ensemble import RandomForestClassifier\nm1=Ml_Model(X,y)\nclf=RandomForestClassifier(n_estimators=10,max_leaf_nodes=5,max_depth=3,min_samples_split=20,min_samples_leaf=15)\n\nm1.pass_classifier(clf)\nm1.fit_method_and_score()\nm1.classification_report()\nm1.validation()\nm1.visualization()\n","1d91039f":"recall=m1.score_return(\"Recall\")[1]\nrecall_list.append(recall)\n\nnames.append(\"RandomF\")\n\nax = fig.add_subplot(111)\nplt.boxplot(recall_list,labels=names)\n# ax.xticks(labels=names)\n# plt.show()\n# # plt.boxplot(recall_list)\n# plt.ylim([np.min(recall_list),np.max(recall_list)+0.001])\n\n# plt.yticks(np.arange(np.min(recall_list), np.max(recall_list), 0.0001))\n\n\nplt.show()","a9175fa0":"\n\n<h2> <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\"> Logistics Regression<\/a> <\/h2>","7e4299a1":"<p> Below plot capture Overall Data shape By PCA  <\/p> ","a3b85ef0":"<b> Conclusion : <\/b> Svm doing great job on Recall(median) with standard deviation\n    ,So Both QDA and SVM give us almost same results\n<br>\n**In The Race Now Winner is**: QDA is still winner","3aae960a":"## <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html\">PCA<\/a>","d169acb1":"## LDA ","82c1d9f1":"<b> Nomination List<\/b>: [\"QDA\",\"SVM\",\"LDA\",\"Lg\",\"MLP\",\"KNN\",\"Decision Tree\",\"RF] ","c8e137d3":"<b> Conclusion : <\/b> LDA is do pretty good job on recall Median=0.9 and f1-score 0.94.\n\n**Nomination Start**\n\nNomination List: [\"LDA\"] \n\n**In The Race Now Winner is**: LDA\n","1605d7b8":"<b>Conclusion:<\/b> Here We can see LDA  transformation gives best Results  <strong>0.85 +\/0.4recall<\/strong> is the best because Here  We don't Want to Consider Diabetes Patient as normal person\n<b>Note:<\/b> We don't perform <b>scaling <\/b> and <\/b>balanced<\/b> dataset operation we will do ahead.","3a15ed10":"## Random Forest","37b15f13":"### Extra Experiments By Post-Prunings","62d64867":"## Feature Selection","b94e2239":"<b>Note<\/b>\n- Glucose :- Glucose Level increase then Diabetes chance Increase\n- CalorieIntake :- per day how many calorieIntake? if people take more calories than fat and sugar level increase                    so Diabetes chances Increase \n- Excercise :- In Every disease excercise will perform major role\n- sleepDuration :- If someone take less sleep it's also increase chance of diabetes, our body needs good sleep \n- BMI :- Overweighted person have more probability with context of diabetes.\n\nOther Factor Also play important role in Diabetes but we needs higher important feature so we would select above  5 Feature. ","5f6a123b":"<b style=\"color:blue\"> We Build Box Plot For Comaparision Our <\/b>","6ac6e114":"\n<b> Nomination List<\/b>: [\"QDA\",\"SVM\",\"LDA\",\"Lg\",\"MLP\",\"KNN\",\"Decision Tree\"] ","585807d5":"<b style=\"font-size:16px\">Aim:<\/b><b style=\"font-size:16px; color:green\"> Don't Forget Our Main is Aim to identify the diabetes person and it's represented by 1\nso our main focus on       Recall <\/b>","5a020180":"<h2> <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\">KNN Algorithms<\/a> <\/h2>","d06d2118":"## <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html\"> T-SNE <\/a>","ae51279b":"<b> Final Winner <\/b>:  Because **Random forest** have good diversity for futuristics data we can also select the Decision tree but Random Forest have randomness and bootstrap sampling so that point makes random forest more generalized algo. ","d6a5b035":"\n<b> Nomination List<\/b>: [\"QDA\",\"SVM\",\"LDA\",\"Lg\",\"MLP\",\"KNN\"] ","f29d6989":"Another Unsupervised Machine learning Algorithms","68c6a629":"<b> Conclusion : <\/b> MLP is doing great job without good parameter so in competition we have two algo\n(QDA,MLP) Note: neural network has capability of non-linear functionality\n<br>\n**In The Race Now Winner is**: [\"QDA\",\"MLP\"] ","1b2846db":"\n<b> Nomination List<\/b>: [\"QDA\",\"SVM\",\"LDA\",\"Lg\",\"MLP\"] ","068df323":"<b> Conclusion : <\/b> Decision Tree is have less variance so DT is good competeitor but Still Arbitrary Decision boundry win\n<br>\n**In The Race Now Winners Are**: [\"QDA\",\"MLP\",\"DT\"] ","e1c35343":"<b> Conclusion : <\/b> If LDA perform well then QDA Definetly perform well and QDA get simliar recall \n\nQDA is do pretty good job.\n-Nomination List: [\"QDA\",LDA\"] \n\n\n\n","d7f01420":"## SVM","5a29340d":"<b> seaborn Through visualization <\/b>","c40900ae":"<b> Conclusion : <\/b> QDA is clearly won\n<br>\n**In The Race Now Winner is**: QDA","c0ebd53e":"### with prepruning","9923c614":"\n<b> Nomination List<\/b>: [\"QDA\",\"SVM\",LDA\",\"Lg\"] ","0c1cb855":"### Below Class have ability to make our data scale,handle imbalance dataset,fitting,validation and visualization.","a25bf30f":"<b> Conclusion : <\/b> Decision Tree is have less variance so DT is good competeitor but Still Arbitrary Decision boundry win\n<br>\n**In The Race Now Winners Are**: [\"QDA\",\"MLP\",\"DT\",\"RF\"] ","bfb035e8":"## Data Clean\n","36883301":"\n    \n\n\n\n<b> Nomination List<\/b>: [\"QDA\",\"SVM\",LDA\"] \n\n\n\n","38690fbd":"<b> Conclusion : <\/b> Still QDA is  Winner\n<br>\n**In The Race Now Winner is**: QDA ","096b58d5":"## Visuliazation Of Data by LDA\n","0179bb23":"## General Class For ML Algorithms","f9a819b1":"## MLP Classifier","7278f504":"## QDA","9debbd5d":"## Decision Tree ","5ca42819":"## Here is Decision Tree overcome their overfiting","36944a71":"<b> Conclusion : <\/b>KNN Algorithms wins But KNN is Not Learning somethings from data so it's  highly unstable for prediction.\n<br>\n**In The Race Now Winner is**: [\"QDA\",\"MLP\"] "}}