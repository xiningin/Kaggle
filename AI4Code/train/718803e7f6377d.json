{"cell_type":{"cd93adfb":"code","21a1ed79":"code","bcefc1e2":"code","9af4074d":"code","a99ab19a":"code","1a51ca92":"code","36fec694":"code","c8627168":"code","021424da":"code","34babadf":"code","a94e8c94":"code","2a7f1a34":"code","2fd6cd79":"code","c31a73d8":"code","d7321b1d":"code","39cc9825":"code","8b2cd940":"code","78d8e0ce":"code","02946803":"code","c248ff8c":"code","423f8093":"code","e7d6e520":"code","7d015f16":"code","ad8635bd":"code","ec690f61":"code","adb81c7b":"code","dddf8082":"code","7821e453":"code","5ed77423":"code","0f4d55db":"code","1c5a4da0":"code","46ccd8c6":"code","e9092d21":"code","ec5bb549":"code","799fc989":"code","0f4175cf":"code","227707b4":"code","04cee090":"code","e1e8da48":"code","e28f7c35":"code","23f20dac":"markdown","c78302b7":"markdown","0333adaf":"markdown","09cc222f":"markdown","6d8ae874":"markdown","f4798c3e":"markdown","3835c70b":"markdown","704a4c7c":"markdown","23ffee83":"markdown","3f066d6e":"markdown","4d95eef5":"markdown","a86da909":"markdown","08eca4ac":"markdown","5293dfe7":"markdown","194ac819":"markdown","e2ff4a39":"markdown","f6f9d484":"markdown"},"source":{"cd93adfb":"import os\nimport numpy as np\n# set the seed for to make results reproducable\nnp.random.seed(2)\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n\nfrom tensorflow.random import set_seed\n# set the seed for to make results reproducable\nset_seed(2)\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","21a1ed79":"!find '..\/input\/fruit-recognition\/' -type d -print","bcefc1e2":"def load_images_from_folder(folder, only_path = False, label = \"\"):\n# Load the paths to the images in a directory\n# or load the images\n    if only_path == False:\n        images = []\n        for filename in os.listdir(folder):\n            img = plt.imread(os.path.join(folder,filename))\n            if img is not None:\n                images.append(img)\n        return images\n    else:\n        path = []\n        for filename in os.listdir(folder):\n            img_path = os.path.join(folder,filename)\n            if img_path is not None:\n                path.append([label,img_path])\n        return path","9af4074d":"# Load the paths on the images\nimages = []\n\ndirp = \"..\/input\/fruit-recognition\/\"\n# Load the paths on the images\nimages = []\n\nfor f in os.listdir(dirp):\n    if \"png\" in os.listdir(dirp+f)[0]:\n        images += load_images_from_folder(dirp+f,True,label = f)\n    else: \n        for d in os.listdir(dirp+f):\n            images += load_images_from_folder(dirp+f+\"\/\"+d,True,label = f)\n            \n# Create a dataframe with the paths and the label for each fruit\ndf = pd.DataFrame(images, columns = [\"fruit\", \"path\"])\ndf.head()","a99ab19a":"df.loc[:,'fruit'].nunique()","1a51ca92":"# Assign to each fruit a specific number\nfruit_names = sorted(df.fruit.unique())\nmapped_fruit_names = dict(zip([t for t in range(len(fruit_names))], fruit_names))\ndf[\"label\"] = df[\"fruit\"].map(mapped_fruit_names)\nprint(mapped_fruit_names)","36fec694":"fruit_values = df.loc[:,'fruit'].value_counts","c8627168":"sns.set(rc={'figure.figsize':(10, 5)})\nplt.xticks(rotation=45)\nsns.barplot(data=pd.DataFrame(fruit_values(normalize=False)).reset_index(),\n            x='index', y='fruit', dodge=False)","021424da":"sns.set(rc={'figure.figsize':(10, 5)})\nplt.xticks(rotation=45)\nsns.barplot(data=pd.DataFrame(fruit_values(normalize=True)).reset_index(), x='index', y='fruit')","34babadf":"fruit_values(normalize=True)","a94e8c94":"df.shape","2a7f1a34":"0.05*70_549","2fd6cd79":"from sklearn.model_selection import StratifiedShuffleSplit","c31a73d8":"strat_split = StratifiedShuffleSplit(n_splits=5, test_size=0.05, random_state=2)","d7321b1d":"for tr_index, val_index in strat_split.split(df, df.loc[:,'fruit']):\n    strat_train_df = df.loc[tr_index]\n    strat_valid_df = df.loc[val_index]","39cc9825":"strat_train_df.shape,strat_valid_df.shape","8b2cd940":"strat_train_df.loc[:,'fruit'].value_counts()","78d8e0ce":"strat_valid_df.loc[:,'fruit'].value_counts()","02946803":"img_width=300\nimg_height=300\n\n\ntrain_data_IDG = ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2,\n                                    zoom_range=0.2, fill_mode='nearest', horizontal_flip=True, rescale=1.0\/255)\n\nvalid_data_IDG = ImageDataGenerator(rescale=1.0\/255, validation_split=0.05)\n\n\ntraining_data = train_data_IDG.flow_from_dataframe(dataframe=strat_train_df,\n                                                   y_col='fruit', x_col='path',\n                                                   target_size=(img_width,img_height), batch_size=8, shuffle=True,\n                                                   class_mode='categorical')\n\nvalidation_data = valid_data_IDG.flow_from_dataframe(dataframe=strat_valid_df,\n                                                     y_col='fruit', x_col='path',\n                                                     target_size=(img_width,img_height), shuffle=True,\n                                                     class_mode='categorical')\n\nprint()\nprint(set(training_data.classes))\nprint()\nprint(set(validation_data.classes))","c248ff8c":"training_data.target_size, training_data.n, validation_data.target_size, validation_data.n","423f8093":"training_data.image_shape, training_data.batch_size, training_data.class_mode, validation_data.image_shape, validation_data.batch_size, validation_data.class_mode","e7d6e520":"tr_set_images, tr_set_labels = next(training_data)\nval_set_images, val_set_labels = next(validation_data)\ntr_set_images.shape, tr_set_labels.shape, val_set_images.shape, val_set_labels.shape","7d015f16":"i = 2\nprint(mapped_fruit_names[np.where(tr_set_labels[i] == 1)[0][0]])\nplt.imshow(X=tr_set_images[i])","ad8635bd":"i = 6\nprint(mapped_fruit_names[np.where(tr_set_labels[i] == 1)[0][0]])\nplt.imshow(X=tr_set_images[i])","ec690f61":"i = 7\nprint(mapped_fruit_names[np.where(val_set_labels[i] == 1)[0][0]])\nplt.imshow(X=val_set_images[i])","adb81c7b":"from tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model","dddf8082":"base_xception = Xception(include_top=False, weights='imagenet', input_shape=(299,299,3), pooling='max')\n\ninner = base_xception.output\ninner = layers.Dense(units=512, activation='relu')(inner)\ninner = layers.Dropout(rate=0.5)(inner)\ninner = layers.Dense(units=15, activation='softmax')(inner)\n\nmodel_1 = Model(inputs=base_xception.input, outputs=inner)\nmodel_1.summary()","7821e453":"from tensorflow_addons.metrics import FBetaScore\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import CategoricalAccuracy, Precision, Recall\nfrom tensorflow.keras.losses import CategoricalCrossentropy\n\nmodel_1.compile(loss = CategoricalCrossentropy(),\n                optimizer = Adam(),\n                metrics = [FBetaScore(num_classes=15, average='macro', name='fbeta_score'),\n                           CategoricalAccuracy(name='cat_acc'),\n                           # GeometricMeanScore(average='weighted'),\n                           Precision(name='precision'), Recall(name='recall')])","5ed77423":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nearly_stop_cb = EarlyStopping(monitor='val_fbeta_score', min_delta=0, patience=4, verbose=1, mode='max')\nreduce_learning_rate_cb = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, cooldown=2, min_lr=0.00001, verbose=1)","0f4d55db":"new_maps_2 = pd.Series(training_data.classes).value_counts()\nnew_maps_2 = new_maps_2.rename(\"fruit\")\nnew_maps_2","1c5a4da0":"new_maps = new_maps_2.to_dict()\nnew_maps","46ccd8c6":"sum(new_maps.values())","e9092d21":"set(training_data.classes)","ec5bb549":"# Scaling by total\/15 helps keep the loss to a similar magnitude.\n# The sum of the weights of all examples stays the same.\nfruit_weights = {i:(1\/new_maps[i])*(67_021)\/15.0 for i in range(15)}\nfruit_weights","799fc989":"history_model_1 = model_1.fit(training_data,\n                              validation_data=validation_data,\n                              class_weight=fruit_weights,\n                              verbose=1, epochs=30,\n                              callbacks=[early_stop_cb, reduce_learning_rate_cb])","0f4175cf":"# save the final model with all the weights\nmodel_1.save(filepath='\/kaggle\/working', include_optimizer=True, save_format='.tf')","227707b4":"plt.plot(history_model_1.history['loss'])\nplt.plot(history_model_1.history['val_loss'])\nplt.title('loss by epochs')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='best')\nplt.show()","04cee090":"plt.plot(history_model_1.history['fbeta_score'])\nplt.plot(history_model_1.history['val_fbeta_score'])\nplt.title('f-beta score by epochs(beta=15)')\nplt.ylabel('F-Score')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='best')\nplt.show()","e1e8da48":"plt.plot(history_model_1.history['cat_acc'])\nplt.plot(history_model_1.history['val_cat_acc'])\nplt.title('Categorical Accuracy by epochs')\nplt.ylabel('Categorical Accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='best')\nplt.show()","e28f7c35":"plt.plot(history_model_1.history['precision'])\nplt.plot(history_model_1.history['val_precision'])\nplt.plot(history_model_1.history['recall'])\nplt.plot(history_model_1.history['val_recall'])\nplt.title('Precision and Recall by epochs')\nplt.ylabel('Precision and Recall')\nplt.xlabel('epoch')\nplt.legend(['train_precision', 'val_precision','train_recall', 'val_recall'], loc='best')\nplt.show()","23f20dac":"## callbacks","c78302b7":"clean all the above cells for them to be able to run without issues on kaggle when committed","0333adaf":"## plotting model statistics","09cc222f":"## making stratified splits for train and validation","6d8ae874":"## making model","f4798c3e":"## fitting the model","3835c70b":"## compiling model","704a4c7c":"## Data Exploration","23ffee83":"if code fails this time, after first epoch, include geometric mean epoch","3f066d6e":"since this is going to be imbalanced classification problem, class weights for undersampled classes has to be defined.","4d95eef5":"So, class weights has to be supplied to handle this imablance.","a86da909":"# about all the code below\n\n1. Data was imabalanced, so splits with equal proportions of all classes has to be included in train and test.\n2. Data was imabalanced, so multiple weighted evaluation metrics has to be chosen. I have calculated:  \n    a. <u>Primary evaluation metric<\/u>: Weighted harmonic mean using <a href='https:\/\/www.tensorflow.org\/addons\/api_docs\/python\/tfa\/metrics\/FBetaScore'><i>F-beta score<\/i><\/a>, that gives equal weights to all classes(with average='macro') has been used.  \n    b. Accuracy using <a href='https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/metrics\/CategoricalAccuracy'><i>CategoricalAccuracy<\/i><\/a>  \n    c. Average <b>Precision<\/b> and <b>Recall<\/b> has also been calculated.  \n3. Becuase of the imbalance in the data, additonal class weights have been supplied to let the algorithm give extra weight to under-sampled classes.  \n4. Xception with weights of 'imagenet' has been adopted without including top layers. At final layers, two additional fully connected layers with dropout of 0.5 has been added.","08eca4ac":"so, 3,528 samples have to be included in validation set.","5293dfe7":"## preparing class weights for undersampled classes","194ac819":"# Modelling","e2ff4a39":"checking the directory structure","f6f9d484":"# getting and exploring the data"}}