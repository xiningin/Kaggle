{"cell_type":{"91cbd516":"code","3dc9cf7c":"code","c23b73eb":"code","c8f3fc1d":"code","9ede98bd":"code","3c7d04a1":"code","92b16947":"code","82f76e71":"code","0b2a5051":"code","d9b86979":"code","590fc05a":"code","0f163c69":"code","9a46e084":"code","0fd616b9":"code","c7a3ed0b":"code","b3728f56":"code","0cdff1f6":"code","76d7662e":"code","4c6b8da7":"code","7564301e":"code","0b918b40":"code","660e93b3":"code","7920f5d9":"code","fad74690":"code","9c441482":"code","c0e45673":"markdown","b67a446b":"markdown","90e6433f":"markdown","4b440795":"markdown","7a99b4a5":"markdown","153a63a7":"markdown","03c4ef3e":"markdown","8d90fd07":"markdown","ebb38b47":"markdown","1fca57b8":"markdown","2741133f":"markdown","a6fee52f":"markdown","6fc06ed0":"markdown","724b3809":"markdown","10929db4":"markdown","6bba9857":"markdown","8658697f":"markdown"},"source":{"91cbd516":"import sys\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport matplotlib as plt\nimport sklearn","3dc9cf7c":"# To turn off warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c23b73eb":"# Some common model algorithms -- M\u1ed9t s\u1ed1 thu\u1eadt to\u00e1n th\u00f4ng d\u1ee5ng\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n# Some Model Helpers -- M\u1ed9t s\u1ed1 class\/ function tr\u1ee3 gi\u00fap vi\u1ec7c ph\u00e2n t\u00edch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection\n\n# Visualization -- \u0110\u1ed3 th\u1ecb h\u00f3a\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot configuration\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')","c8f3fc1d":"# Load data\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\n# Create a deep copy from train data to wrangle\ndata1 = train_data.copy(deep=True)\n\n# Gather 2 datas into a list to clean faster with loop\n# \u0110\u01b0a 2 data v\u00e0o 1 list \u0111\u1ec3 l\u00e0m s\u1ea1ch nhanh h\u01a1n v\u1edbi loop\ndata_cleaner = [data1, test_data]","9ede98bd":"data1.info()","3c7d04a1":"data1.head()","92b16947":"print('Train cols with null -- Ki\u1ec3m tra null c\u1ee7a train_data:\\n' +\n      str(data1.isnull().sum())) # Count all the null values each column\nprint('-' * 10)\n\nprint('Test cols with null -- Ki\u1ec3m tra null c\u1ee7a test_data:\\n' +\n     str(test_data.isnull().sum()))\nprint('-' * 10)\n\ndata1.describe(include='all')","82f76e71":"for dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace=True)\n    \ndrop_columns = ['PassengerId', 'Cabin', 'Ticket']\ndata1.drop(columns=drop_columns, inplace=True)\n    \nprint('Train cols with null -- Ki\u1ec3m tra null c\u1ee7a train_data:\\n' +\n      str(data1.isnull().sum()))\nprint('-' * 10)\n\nprint('Test cols with null -- Ki\u1ec3m tra null c\u1ee7a test_data:\\n' +\n     str(test_data.isnull().sum()))\nprint('-' * 10)","0b2a5051":"for dataset in data_cleaner:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n    \n    dataset['Title'] = dataset['Name'].str.split(', ', expand=True)[1] \\\n    .str.split('.', expand=True)[0]\n    \n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    \n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \ntitle_names = (data1['Title'].value_counts() < 10)\ndata1['Title'] = data1['Title'].apply(lambda x: 'Other' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint('-' * 10)\n\ndata1.info()\nprint('-' * 10)\ntest_data.info()\ndata1.head()","d9b86979":"label = LabelEncoder()\nfor dataset in data_cleaner:\n    # Create bin features\n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n# Define y variable - Easier for later call\nTarget = ['Survived']\n\n# Define x variables for original features\ndata1_x = ['Sex', 'Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age',\n           'Fare', 'FamilySize', 'IsAlone'] # We will use these columns to create crosstabulation tables.\n\n# Define x variables for original with bin features\ndata1_x_bin = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n# Make dummy variables - Not used in this pj\ndata1_dummy = pd.get_dummies(data1[data1_x]) # Auto get dummies of object type\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')","590fc05a":"print('Train cols with null -- Ki\u1ec3m tra null c\u1ee7a train_data:\\n' +\n      str(data1.isnull().sum()))\nprint('-' * 10)\nprint(data1.info())\nprint(('-' * 10), ('-' * 10), sep='\\n')\n\nprint('Test cols with null -- Ki\u1ec3m tra null c\u1ee7a test_data:\\n' +\n     str(test_data.isnull().sum()))\nprint('-' * 10)\nprint(test_data.info())","0f163c69":"# Run crosstabs to determine the correlation between target and other variables\ncontinuous_variables = ['Age', 'Fare']\n\nfor x in data1_x:\n    if x not in continuous_variables: # Not using continuous data to run crosstab\n        print(pd.crosstab(data1[x],data1[Target[0]], normalize='index')) # Normalize each row\n        print('-' * 10)","9a46e084":"# Graph distribution of quantitative data\nplt.figure(figsize=[16,12])\n\n# Boxplot charts\nplt.subplot(231)\nplt.boxplot(x=data1['Fare'], showmeans=True, meanline=True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(x=data1['Age'], showmeans=True, meanline=True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(x=data1['FamilySize'], showmeans=True, meanline=True)\nplt.title('FamilySize Boxplot')\nplt.ylabel('People')\n\n# Histogram charts\nplt.subplot(234)\nplt.hist(x=[data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']],\n         stacked=True, color=['b', 'r'], label=['Survived', 'Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x=[data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']],\n         stacked=True, color=['b', 'r'], label=['Survived', 'Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x=[data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']],\n         stacked=True, color=['b', 'r'], label=['Survived', 'Dead'])\nplt.title('FamilySize Histogram by Survival')\nplt.xlabel('FamilySize ($)')\nplt.ylabel('# of Passengers')\nplt.legend()","0fd616b9":"# Graph distribution of qualitative data\nfig, ax = plt.subplots(2, 3, figsize=(16,12))\n\n# Barplot charts\nsns.barplot(x='Embarked', y='Survived', data=data1, ax=ax[0,0])\nsns.barplot(x='Pclass', y='Survived', data=data1, ax=ax[0,1])\nsns.barplot(x='IsAlone', y='Survived', data=data1, ax=ax[0,2])\n\n# Pointplot charts\nsns.pointplot(x='FareBin', y='Survived', data=data1, ax=ax[1,0])\nsns.pointplot(x='AgeBin', y='Survived', data=data1, ax=ax[1,1])\nsns.pointplot(x='FamilySize', y='Survived', data=data1, ax=ax[1,2])","c7a3ed0b":"# Distribution of Pclass and another feature by survival\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,12))\n\nsns.boxplot(x='Pclass', y='Fare', hue='Survived', data=data1, ax=ax1)\nax1.set_title('Pclass-Fare Survival Comparison')\n\nsns.boxplot(x='Pclass', y='Age', hue='Survived', data=data1, ax=ax2)\nax2.set_title('Pclass-Age Survival Comparison')\n\nsns.boxplot(x='Pclass', y='FamilySize', hue='Survived', data=data1, ax=ax3)\nax3.set_title('Pclass-FamilySize Survival Comparison')","b3728f56":"# Distribution of Sex and another feature by survival\nfig, ax = plt.subplots(1, 3, figsize=(16, 12))\n\nsns.barplot(x='Sex', y='Survived', hue='Embarked', data=data1, ax=ax[0])\nax[0].set_title('Sex-Embarked Survival Comparison')\n\nsns.barplot(x='Sex', y='Survived', hue='Pclass', data=data1, ax=ax[1])\nax[1].set_title('Sex-Pclass Survival Comparison')\n\nsns.barplot(x='Sex', y='Survived', hue='IsAlone', data=data1, ax=ax[2])\nax[2].set_title('Sex-IsAlone Survival Comparison')","0cdff1f6":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,12))\n\n# Distribution of Pclass-Sex by survival\nsns.pointplot(x='Pclass', y='Survived', hue='Sex', data=data1,\n              palette={'male': 'blue', 'female': 'pink'},\n              markers=['*', 'o'], linestyles=['-', '--'], ax=ax1)\nplt.title('Pclass-Sex Survival Comparison')\n\n# Distribution of FamilySize-Sex by survival\nsns.pointplot(x='FamilySize', y='Survived', hue='Sex', data=data1,\n              palette={'male': 'blue', 'female': 'pink'},\n              markers=['*', 'o'], linestyles=['-', '--'], ax=ax2)\nplt.title('FamilySize-Sex Survival Comparison')","76d7662e":"# Distribution of Embarked-Pclass-Sex by survival\ne = sns.FacetGrid(data1, col='Embarked', hue_order=['male', 'female'])\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep').add_legend()","4c6b8da7":"# Distribution of Age by survival\na = sns.FacetGrid(data1, hue='Survived', aspect=4)\na.map(sns.kdeplot, 'Age', shade=True)\na.set(xlim=(0, data1['Age'].max()))\na.add_legend()","7564301e":"# Histogram of Sex-Pclass-Age by survival\nh = sns.FacetGrid(data1, row='Sex', col='Pclass', hue='Survived')\nh.map(plt.hist, 'Age', alpha=.60).add_legend()","0b918b40":"# Correlation heatmap\n_, ax = plt.subplots(figsize=(16,12))\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.set(font_scale=1.5) # Change label font size\n\n_ = sns.heatmap(data1.corr(), cmap=colormap, square=True, cbar_kws={'shrink': 1},\n                ax=ax, annot=True, linewidths=0, vmax=1.0, linecolor='white',\n                annot_kws={'fontsize': 15})\n\nplt.title('Pearson Correlation of Features')","660e93b3":"MLA = [\n    # Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    # Guassian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    # GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    # Naive Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    # Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    # SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    # Trees\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    # Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    # Xgboost\n    XGBClassifier()\n]","7920f5d9":"cv_results","fad74690":"%%time\ncv_split = model_selection.ShuffleSplit(n_splits=10, test_size=.3, train_size=.7, random_state=0)\n\nMLA_columns = ['MLA_name', 'MLA_params', 'MLA_train_acc_mean', 'MLA_test_acc_mean',\n               'MLA_test_acc_3std', 'MLA_time']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n\nMLA_predict=data1[Target]\n\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA_name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA_params'] = str(alg.get_params())\n    \n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv=cv_split, return_train_score=True)\n    \n    MLA_compare.loc[row_index, 'MLA_time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA_train_acc_mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA_test_acc_mean'] = cv_results['test_score'].mean()\n    MLA_compare.loc[row_index, 'MLA_test_acc_3std'] = cv_results['test_score'].std()*3\n    \n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index += 1\n    \nMLA_compare.sort_values(by=['MLA_test_acc_mean'], ascending=False, inplace=True)\nMLA_compare","9c441482":"# Using XGBClassifier to predict\nxgb = XGBClassifier()\nxgb.fit(data1[data1_x_bin], data1[Target])\ntest_data['Survived'] = xgb.predict(test_data[data1_x_bin])\nprint(test_data['Survived'].value_counts(normalize=True))\nsubmission = test_data[['PassengerId', 'Survived']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission.sample(10)","c0e45673":"### 7. Using the best model to make prediction\nAfter running a bunch of Classifier models with default parameters, we will use XGBClassifier model to predict our survival status of passengers in the dataset test_data.\n\n*Sau khi ch\u1ea1y th\u1eed c\u00e1c Classifier models v\u1edbi c\u1ea5u h\u00ecnh m\u1eb7c \u0111\u1ecbnh, ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng XGBClassifier \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n t\u00ecnh tr\u1ea1ng s\u1ed1ng\/ch\u1ebft c\u1ee7a h\u00e0nh kh\u00e1ch trong b\u1ed9 d\u1eef li\u1ec7u test_data.*","b67a446b":"### 5. Perform exploratory analysis\n# *Add explaination*\n#### 5.1. Crosstabulations","90e6433f":"##### b. Plotting correlation of multi-variable with survival -- \u0110\u1ed3 th\u1ecb t\u01b0\u01a1ng quan \u0111a bi\u1ebfn","4b440795":"### 3. Examine the data\n# *Add description about datasets*","7a99b4a5":"#### a. Completing\nIn this case, we will fill null in **Age** and **Fare** with median, **Embarked** with mode, drop **PassengerId, Cabin, Ticket** since they have no impact on analyzing.\n\n*Trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y, ch\u00fang ta s\u1ebd thay th\u1ebf gi\u00e1 tr\u1ecb null trong **Age** v\u00e0 **Fare** v\u1edbi trung v\u1ecb, **Embarked** v\u1edbi mode, b\u1ecf c\u1ed9t **PassengerId, Cabin, Ticket** v\u00ec ch\u00fang kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb nghi\u00ean c\u1ee9u.*","153a63a7":"1. The **Survived** variable is our outcome\/dependent\/target variable. It is a binary nominal datatype of 1 for survived and 0 for didnot survive.\n2. **PassengerID** and **Ticket** varibles are unique identifiers, thus have no impact on target variable.\n3. **Pclass** is an ordinal datatype for ticket class, representing 1=upper class, 2=middle class, 3=lower class.\n4. **Name** is a nominal datatype. It could use to find out their gender, family size, SES... In this case, since those variables already exist, we will use it to see if title, like *master*, makes a difference.\n5. **Sex** and **Embarked** are nominal datatypes. They will be converted to *[dummy variable](https:\/\/voer.edu.vn\/c\/bien-phan-loai\/2d2e6a46\/df65058a)* for mathematical calculations. In fact, we are not using dummies for this pj, so just know that it exists.\n6. **Age** and **Fare** are continuous quantitative datatypes.\n7. **SibSp** represents number of related siblings\/spouse aboard, **Parch** is number of related parents\/children aboard. Both are discrete quantitative datatypes and can be use for features.\n8. **Cabin** has too many NULL values, thus this will be excluded from analysis.\n---\n1. Bi\u1ebfn **Survived** l\u00e0 bi\u1ebfn \u0111\u1ed1i t\u01b0\u1ee3ng\/ bi\u1ebfn ph\u1ee5 thu\u1ed9c (K\u1ebft qu\u1ea3 c\u1ea7n d\u1ef1 \u0111o\u00e1n). L\u00e0 ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh danh v\u1edbi 1-Survived v\u00e0 0-Didnot survive.\n2. **PassengerID** v\u00e0 **Ticket** l\u00e0 2 bi\u1ebfn nh\u1eadn d\u1ea1ng, do \u0111\u00f3 n\u00f3 kh\u00f4ng t\u00e1c \u0111\u1ed9ng t\u1edbi bi\u1ebfn \u0111\u1ed1i t\u01b0\u1ee3ng.\n3. Bi\u1ebfn **Pclass** l\u00e0 bi\u1ebfn th\u1ee9 b\u1eadc cho h\u1ea1ng v\u00e9, v\u1edbi 1-upper class, 2-middle class, 3-lower class\n4. **Name** l\u00e0 bi\u1ebfn \u0111\u1ecbnh danh. C\u00f3 th\u1ec3 s\u1eed d\u1ee5ng bi\u1ebfn n\u00e0y \u0111\u1ec3 tr\u00edch xu\u1ea5t gi\u1edbi t\u00ednh, s\u1ed1 ng\u01b0\u1eddi trong gia \u0111\u00ecnh, SES... Trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y, c\u00e1c bi\u1ebfn v\u1eeba n\u00eau tr\u00ean v\u1ed1n \u0111\u00e3 t\u1ed3n t\u00e0i, ch\u00fang ta s\u1ebd ch\u1ec9 s\u1eed d\u1ee5ng **Name** \u0111\u1ec3 t\u00ecm ra s\u1ef1 kh\u00e1c bi\u1ec7t gi\u1eefa c\u00e1c SES nh\u01b0 h\u1ecdc h\u00e0m *Master* v\u00e0 nh\u1eefng nh\u00f3m kh\u00e1c.\n5. **Sex** v\u00e0 **Embarked** l\u00e0 ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh danh. 2 bi\u1ebfn n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c bi\u1ebfn \u0111\u1ed5i th\u00e0nh *[Bi\u1ebfn gi\u1ea3](https:\/\/voer.edu.vn\/c\/bien-phan-loai\/2d2e6a46\/df65058a)* \u0111\u1ec3 ph\u1ee5c v\u1ee5 vi\u1ec7c t\u00ednh to\u00e1n. Tuy nhi\u00ean, ch\u00fang ta s\u1ebd kh\u00f4ng s\u1eed d\u1ee5ng bi\u1ebfn gi\u1ea3 trong pj n\u00e0y.\n6. **Age** v\u00e0 **Fare** thu\u1ed9c ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh l\u01b0\u1ee3ng li\u00ean ti\u1ebfp.\n7. **SibSp** cho bi\u1ebft s\u1ed1 l\u01b0\u1ee3ng anh\/ch\u1ecb\/em\/v\u1ee3\/ch\u1ed3ng c\u00f9ng l\u00ean t\u00e0u, **Parch** cho bi\u1ebft s\u1ed1 l\u01b0\u1ee3ng cha m\u1eb9\/ con c\u00e1i c\u00f9ng l\u00ean t\u00e0u. C\u1ea3 2 \u0111\u1ec1u ki\u1ebfn ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ecbnh l\u01b0\u1ee3ng r\u1eddi r\u1ea1c v\u00e0 c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng l\u00e0m bi\u1ebfn \u0111\u1ed9c l\u1eadp.\n8. Bi\u1ebfn **Cabin** thi\u1ebfu qu\u00e1 nhi\u1ec1u gi\u00e1 tr\u1ecb n\u00ean s\u1ebd kh\u00f4ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 ph\u00e2n t\u00edch","03c4ef3e":"### 6. Model Data","8d90fd07":"To discrete outcome, binary outcome with 0-Dead and 1-Survived in this case, Classification algorithms are usually suitable. Therefore, we will run 9 algorithm groups in Classification category to find the most suitable model for our problem.\n\n*\u0110\u1ed1i v\u1edbi c\u00e1c k\u1ebft qu\u1ea3 l\u00e0 ki\u1ec3u r\u1eddi r\u1ea1c, c\u1ee5 th\u1ec3 trong tr\u01b0\u1eddng h\u1ee3p n\u00e0y l\u00e0 binary outcome v\u1edbi 0-Dead v\u00e0 1-Survived, c\u00e1c m\u00f4 h\u00ecnh\/ c\u00f4ng th\u1ee9c Classification th\u01b0\u1eddng s\u1ebd ph\u00f9 h\u1ee3p. Do \u0111\u00f3, ch\u00fang ta s\u1ebd ch\u1ea1y th\u1eed 9 nh\u00f3m c\u00f4ng th\u1ee9c thu\u1ed9c ph\u00e2n lo\u1ea1i Classification \u0111\u1ec3 t\u00ecm ra c\u00f4ng th\u1ee9c t\u1ed1i \u01b0u nh\u1ea5t.*","ebb38b47":"#### d. Double check cleaned data","1fca57b8":"**This notebook is based on a great Kernel of [LD Freeman](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/notebook).**","2741133f":"### 4. Cleaning data\nRemember the 4 C's of Data Cleaning, which is: Correcting, Completing, Creating and Converting.\n1. **Correcting**: Correct the unreasonable values, for example age = 800 instead of 80. ***This should be done with caution because it maybe necessary to create an accurate model***.\n2. **Completing**: Replace null data with reasonable input such as: mean, median (for quantitative data) or mode (for qualitative data). These are just basic methods for completing data.\n3. **Creating**: Using existing features to create new features to determine if they provide new signals to predict outcome.\n4. **Converting**: Changing datatype formats to use.\n\nWe will start with checking for null values.\n\n---\n1. **Ch\u1ec9nh s\u1eeda d\u1eef li\u1ec7u**: S\u1eeda l\u1ea1i nh\u1eefng gi\u00e1 tr\u1ecb v\u00f4 l\u00fd, v\u00ed d\u1ee5 nh\u01b0 tu\u1ed5i = 800 thay v\u00ec 80. ***Vi\u1ec7c ch\u1ec9nh s\u1eeda d\u1eef li\u1ec7u g\u1ed1c n\u00ean \u0111\u01b0\u1ee3c c\u00e2n nh\u1eafc k\u1ef9 v\u00ec c\u00f3 th\u1ec3 l\u00e0m \u1ea3nh h\u01b0\u1edfng t\u1edbi \u00fd ngh\u0129a d\u1eef li\u1ec7u***.\n2. **B\u1ed5 sung d\u1eef li\u1ec7u**: Thay nh\u1eefng gi\u00e1 tr\u1ecb null b\u1eb1ng nh\u1eefng gi\u00e1 tr\u1ecb h\u1ee3p l\u00fd nh\u01b0: mean, median (cho d\u1eef li\u1ec7u \u0111\u1ecbnh l\u01b0\u1ee3ng) ho\u1eb7c mode (cho d\u1eef li\u1ec7u \u0111\u1ecbnh t\u00ednh). Nh\u1eefng c\u00e1ch n\u00e0y m\u1edbi ch\u1ec9 l\u00e0 nh\u1eefng c\u00e1ch \u0111\u01a1n gi\u1ea3n nh\u1ea5t.\n3. **T\u1ea1o m\u1edbi**: S\u1eed d\u1ee5ng nh\u1eefng bi\u1ebfn \u0111\u1ed9c l\u1eadp c\u00f3 s\u1eb5n \u0111\u1ec3 t\u1ea1o ra nh\u1eefng bi\u1ebfn \u0111\u1ed9c l\u1eadp m\u1edbi c\u00f3 \u00fd ngh\u0129a cho nghi\u00ean c\u1ee9u v\u00e0 d\u1ef1 \u0111o\u00e1n.\n4. **Thay \u0111\u1ed5i format**: Thay \u0111\u1ed5i format c\u1ee7a d\u1eef li\u1ec7u \u0111\u1ec3 ph\u1ee5c v\u1ee5 cho ph\u00e2n t\u00edch.\n\nCh\u00fang ta s\u1ebd b\u1eaft \u0111\u1ea7u b\u1eb1ng vi\u1ec7c ki\u1ec3m tra c\u00e1c gi\u00e1 tr\u1ecb null.","a6fee52f":"#### 5.2. Plotting\n##### a. Plotting correlation of single variable with survival -- \u0110\u1ed3 th\u1ecb t\u01b0\u01a1ng quan \u0111\u01a1n bi\u1ebfn","6fc06ed0":"#### b. Creating\nWe will create some new features to support our analyzing and prediction.\n1. Create **FamilySize** base on **SibSp** and **Parch**.\n2. Create **IsAlone** base on **FamilySize** (Assign every row value of 1, then look up for rows that have **FamilySize** > 1 to assign to 0).\n3. Create **Title** base on **Name**.\n4. Create **FareBin** base on quartile cut of **Fare**.\n5. Create **AgeBin** base on cut of **Age**.\n6. Change all the title in **Title** which appears < 10 (Based on statistical rules) into 'Other'.\n    * Use `data1['Title'].value_counts() < 10` to create a series of Boolean with titles as index.\n    * Use `lambda` to apply on **Title** column.\n---\nCh\u00fang ta s\u1ebd t\u1ea1o m\u1ed9t s\u1ed1 features m\u1edbi \u0111\u1ec3 ph\u1ee5c v\u1ee5 cho vi\u1ec7c ph\u00e2n t\u00edch v\u00e0 d\u1ef1 \u0111o\u00e1n.\n1. T\u1ea1o bi\u1ebfn **FamilySize** d\u1ef1a v\u00e0o bi\u1ebfn **SibSp** v\u00e0 **Parch**.\n2. T\u1ea1o bi\u1ebfn **IsAlone** d\u1ef1a v\u00e0o **FamilySize** (\u0110\u1ea7u ti\u00ean g\u00e1n t\u1ea5t c\u1ea3 c\u00e1c d\u00f2ng v\u1edbi gi\u00e1 tr\u1ecb 1, sau \u0111\u00f3 t\u00ecm nh\u1eefng d\u00f2ng c\u00f3 **FamilySize** > 1 v\u00e0 g\u00e1n l\u1ea1i th\u00e0nh 0).\n3. T\u1ea1o bi\u1ebfn **Title** d\u1ef1a v\u00e0o bi\u1ebfn **Name**.\n4. T\u1ea1o bi\u1ebfn **FareBin** d\u1ef1a v\u00e0o kho\u1ea3ng ph\u00e2n v\u1ecb c\u1ee7a **Fare**.\n5. T\u1ea1o bi\u1ebfn **AgeBin** d\u1ef1a v\u00e0o chia kho\u1ea3ng gi\u00e1 tr\u1ecb c\u1ee7a **Age**.\n6. \u0110\u1ed5i t\u1ea5t c\u1ea3 c\u00e1c title xu\u1ea5t hi\u1ec7n d\u01b0\u1edbi 10 l\u1ea7n trong c\u1ed9t **Title** th\u00e0nh 'Other' (D\u1ef1a tr\u00ean nguy\u00ean t\u1eafc th\u1ed1ng k\u00ea).\n    * D\u00f9ng `data1['Title'].value_counts() < 10` \u0111\u1ec3 t\u1ea1o m\u1ed9t Panda Series c\u00e1c gi\u00e1 tr\u1ecb Boolean v\u1edbi title l\u00e0 index.\n    * D\u00f9ng `lambda` \u0111\u1ec3 \u00e1p d\u1ee5ng cho c\u1ed9t **Title**.","724b3809":"# Apply Machine Learning in Titanic data\n> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this project, we will try to analyze what kind of passengers were more likely to survive the tragedy, also applying machine learning in order to predict which passengers survived.\n\n*Trong pj n\u00e0y ch\u00fang ta s\u1ebd th\u1eed ph\u00e2n t\u00edch nh\u1eefng ki\u1ec3u h\u00e0nh kh\u00e1ch c\u00f3 t\u1ef7 l\u1ec7 s\u1ed1ng s\u00f3t cao h\u01a1n, \u0111\u1ed3ng th\u1eddi \u00e1p d\u1ee5ng machine learning \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n h\u00e0nh kh\u00e1ch s\u1ed1ng s\u00f3t*\n\n### 1. Import libraries","10929db4":"After importing, now we take a quick look at data.\n\n*Sau khi \u0111\u00e3 nh\u1eadp d\u1eef li\u1ec7u, ch\u00fang ta xem qua 1 s\u1ed1 th\u00f4ng s\u1ed1 v\u1ec1 d\u1eef li\u1ec7u.*","6bba9857":"### 2. Load data modelling libraries\nWe will use library **scikit-learn** to apply machine learning algorithms, **matplotlib** and **seaborn** to visualize.\n\n*Ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n **scikit-learn** \u0111\u1ec3 \u00e1p d\u1ee5ng c\u00e1c thu\u1eadt to\u00e1n machine learning, **matplotlib** v\u00e0 **seaborn** \u0111\u1ec3 bi\u1ec3u di\u1ec5n \u0111\u1ed3 th\u1ecb.*","8658697f":"#### c. Convert Formats\n# *Add explaination*"}}