{"cell_type":{"7e94d087":"code","9cb790d9":"code","194f0a60":"code","05f114db":"code","ca2e36bb":"code","bf686c4b":"code","6990ef0c":"code","fad74fd5":"code","34f5b1b4":"code","0e534ad1":"code","287ced25":"code","62600586":"code","af40d47a":"code","87151018":"markdown","00cf1b23":"markdown","15b4b7ef":"markdown","1f85e3e8":"markdown","c1806b10":"markdown"},"source":{"7e94d087":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nimport plotly.graph_objects as go\n\nfrom joblib import Parallel, delayed\nfrom sklearn import preprocessing, model_selection\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport umap\n\n\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'","9cb790d9":"!mkdir -p \/tmp\/pip\/cache\/\n!cp ..\/input\/hdbscan0827-whl\/hdbscan-0.8.27-cp37-cp37m-linux_x86_64.whl \/tmp\/pip\/cache\/\n!pip install --no-index --find-links \/tmp\/pip\/cache\/ hdbscan\n\n!pip install umap-learn\n\nimport hdbscan\nimport umap\nfrom sklearn.manifold import TSNE","194f0a60":"def read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train\n\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    stock_id = file_path.split('=')[1]\n    df['row_id'] = df['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    df = df.drop('time_id',axis=1)\n    return df\n\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        df_tmp = trade_preprocessor(file_path_trade)\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df","05f114db":"# Read train and test\ntrain = read_train_test()\n\n# preprocess stock-id = 0\ntrain_ = preprocessor([0], is_train = True)\ntrain = train.merge(train_, on = ['row_id'], how = 'left')","ca2e36bb":"train[train['stock_id']==0]","bf686c4b":"train = train.groupby(['stock_id','time_id'])['price','size','order_count'].agg(['mean', 'std', 'max', 'min', ]).reset_index()\ntrain","6990ef0c":"train = train.dropna()\ntrain = train[train['stock_id']==0]\nX = np.array(train)","fad74fd5":"np.array(train)","34f5b1b4":"train","0e534ad1":"mapper = umap.UMAP(random_state=0)\nembedding = mapper.fit_transform(X)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x = embedding[:,0], y = embedding[:,1], mode = 'markers'\n))\nfig.show()\n","287ced25":"clusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True).fit(X)\ncolor_palette = sns.color_palette('Paired', 12)\ncluster_colors = [color_palette[x] if x >= 0\n                  else (0.5, 0.5, 0.5)\n                  for x in clusterer.labels_]\ncluster_member_colors = [sns.desaturate(x, p) for x, p in\n                         zip(cluster_colors, clusterer.probabilities_)]\nplt.scatter(*embedding .T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n","62600586":"u, counts = np.unique(clusterer.labels_, return_counts=True)\nprint(u)\nprint(counts)","af40d47a":"len(clusterer.labels_)","87151018":"## UMAP\n","00cf1b23":"## About this notebook\n\nDimensionality reduction by UMAP and clustering by HDBSCAN.\nIn order to prevent the data from becoming too large, only cases with stock_id =0 will be handled.\n\nAlso, the installation and implementation of HDBSCAN was done by referring to [\nOptiver HDBSCAN](https:\/\/www.kaggle.com\/something4kag\/optiver-hdbscan), and on top of that, this notebook was created for this competition.\n\nThank you for @something4kag\n\nI wrote about t-SNE and HDBSCAN in separate notebooks. [\nt-SNE & HDBSCAN (stock_id=0)](https:\/\/www.kaggle.com\/takemi\/t-sne-hdbscan-stock-id-0)\n","15b4b7ef":"Group by stock_id and time_id, and calculate mean, max, min, and std values for price, size, and orde_count, respectively, and use them as features.","1f85e3e8":"## Simple Preprocess\n\nConcat the elements of train.csv and train_trade.parquet to create train data.\n","c1806b10":"## HDBSCAN\n\nClustering with HDBSCAN and mapping the result to the t-SNE plot generated in the figure above."}}