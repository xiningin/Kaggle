{"cell_type":{"aa8e2f8b":"code","0d8d045c":"code","9755216b":"code","69dd7bae":"code","f3ab5afe":"code","20b453d4":"code","2062ef6b":"code","5ce81963":"code","c5e5e008":"code","0e73b597":"code","81a63ab8":"code","9954fdd1":"code","2421f901":"code","a2f3ed60":"code","2ae011a1":"code","4bc2c51e":"code","fabed542":"code","952f9852":"code","37fa7ddd":"code","1d48703d":"code","cd043416":"code","cdb7294c":"code","dea2717d":"code","477c366b":"code","52831426":"code","14dc7a6a":"code","ec7e932c":"code","241eb1be":"code","694ca792":"code","53bbc18a":"markdown","14e3b03f":"markdown","cb12a12b":"markdown","0fb536a5":"markdown","d1e67193":"markdown","9050e328":"markdown","abbbad7b":"markdown","36afa7c7":"markdown","7fbed3f1":"markdown","c4fec455":"markdown","4b28c8b4":"markdown"},"source":{"aa8e2f8b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score","0d8d045c":"def plot_formatting():\n    '''\n    Set up the default plotting settings.\n    '''\n    \n    plt.rc(\n        'figure',\n        figsize=(12,6),\n        titleweight='bold',\n        titlesize=25\n    )\n    plt.rc(\n        'axes',\n        labelweight='ultralight',\n        titleweight='ultralight',\n        titlelocation='left',\n        titlecolor='k',\n        titley=1.03,\n        titlesize=16,\n        grid=True\n    )\n    plt.rc(\n        'axes.spines',\n        right=False,\n        left=False,\n        top=False   \n    )\n    plt.rc(\n        'grid',\n        color='k',\n        linestyle=(0,15,2,0),\n        alpha=0.5\n    )\n    plt.rc('axes.grid', axis='y')\n    plt.rc('ytick.major', width=0)\n    plt.rc('font', family='monospace')\n    \nplot_formatting() # Setting our default settings","9755216b":"train = pd.read_csv('..\/input\/cleaned-datasets-give-me-some-credit\/Cleaned_train.csv')\ntest = pd.read_csv('..\/input\/cleaned-datasets-give-me-some-credit\/Cleaned_test.csv')","69dd7bae":"y = train['SeriousDlqin2yrs']\ntrain = train.drop('SeriousDlqin2yrs', axis=1)","f3ab5afe":"train.head()","20b453d4":"X_stand = StandardScaler().fit_transform(train)\npca = PCA(n_components=2).fit(X_stand)\nX_pca = pca.transform(X_stand)\n\nsns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, alpha=0.3)\nplt.tick_params(bottom=False, labelbottom=False, labelleft=False)\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.xlim(-6, 13)\nplt.ylim(-5, 20)\nplt.suptitle('Training set visualisation using PCA features');","2062ef6b":"def score_dataset(X, y, fold_scores=False, model = DecisionTreeClassifier(max_depth=7, min_samples_leaf=25,\n                                                       min_samples_split = 4, random_state=0)):\n    \n    folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n    oof = np.zeros(len(X))\n\n    for fold_, (train_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n        \n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        clf = model.fit(X_train, y_train)\n        \n        oof[val_idx] = clf.predict_proba(X_val)[:, 1]\n        \n        if fold_scores:\n            print('Fold score: ', roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1]))\n\n    print(\"CV score: {:.8f}\".format(roc_auc_score(y, oof)))\n    return","5ce81963":"sns.heatmap(train.corr(), annot=True, fmt='.3f')","c5e5e008":"score_dataset(train, y)","0e73b597":"X = train.copy()\n\nX['Weighted_Sum_PastDue'] = 2 * X['NumberOfTime30-59DaysPastDueNotWorse'] + 3 * X['NumberOfTime60-89DaysPastDueNotWorse'] + 6 * X['NumberOfTimes90DaysLate']\n\nX['90days_out_of_TotalPastDue'] = X['NumberOfTimes90DaysLate'] \/ (X['Weighted_Sum_PastDue'])\nX.loc[X['Weighted_Sum_PastDue']==0, '90days_out_of_TotalPastDue'] = 0\n\nscore_dataset(X, y)","81a63ab8":"X = train.copy()\n\nX['RevolvingUtilization_per_age'] = X['RevolvingUtilizationOfUnsecuredLines'] \/ X['age']\n\nscore_dataset(X, y)","9954fdd1":"X = train.copy()\n\nX['Disposable_Income_per_person'] = (X['MonthlyIncome'] - X['Debt']) \/ (X['NumberOfDependents'] + 1)\nX.loc[X['NumberOfDependents']==0, 'Disposable_Income_per_person'] = 0\n\nX.drop('MonthlyIncome', axis=1, inplace=True)\nscore_dataset(X, y)","2421f901":"X = train.copy()\n\nX['Debt_per_Real_Estate_Loan'] = X['Debt'] \/ X['NumberRealEstateLoansOrLines']\nX.loc[X['NumberRealEstateLoansOrLines']==0, 'Debt_per_Real_Estate_Loan'] = 0\n\nX.drop('Debt', axis=1, inplace=True)\nscore_dataset(X, y)","a2f3ed60":"X = train.copy()\n\nX['RemainingLines_per_person'] = (X['NumberOfOpenCreditLinesAndLoans'] - X['NumberRealEstateLoansOrLines']) \/ (X['NumberOfDependents'] + 1)\n\nscore_dataset(X, y)","2ae011a1":"X = train.copy()\n\nX['Ratio_age_per_number_of_dependents'] = X['age'] \/ (X['NumberOfDependents'])\nX.loc[X['NumberOfDependents']==0, 'Ratio_age_per_number_of_dependents'] = 0\n\nscore_dataset(X, y)","4bc2c51e":"X = train.copy()\n\nX['Never3089'] = ((X['NumberOfTime30-59DaysPastDueNotWorse'] * X['NumberOfTime60-89DaysPastDueNotWorse']) > 0).astype('int64')\n\nscore_dataset(X, y)","fabed542":"X = train.copy()\n\nX['RelvovingLines'] = X['NumberOfOpenCreditLinesAndLoans'] - X['NumberRealEstateLoansOrLines']\n\nX['HasRevolvingLines'] = (X['RelvovingLines']>0).astype('int64')\n\nscore_dataset(X, y)","952f9852":"X = train.copy()\n\nX['Minor_PastDue_per_Line'] = X['NumberOfTime30-59DaysPastDueNotWorse'] \/ X['NumberOfOpenCreditLinesAndLoans']\nX.loc[X['NumberOfOpenCreditLinesAndLoans']==0, 'Minor_PastDue_per_Line'] = 0\n\nscore_dataset(X, y)","37fa7ddd":"X = train.copy()\n\nX['Weighted_Sum_PastDue'] = 2 * X['NumberOfTime30-59DaysPastDueNotWorse'] + 3 * X['NumberOfTime60-89DaysPastDueNotWorse'] + 6 * X['NumberOfTimes90DaysLate']\n\nX['90days_out_of_TotalPastDue'] = X['NumberOfTimes90DaysLate'] \/ (X['Weighted_Sum_PastDue'])\nX.loc[X['Weighted_Sum_PastDue']==0, '90days_out_of_TotalPastDue'] = 0\n\nX['RemainingLines'] = X['NumberOfOpenCreditLinesAndLoans'] - X['NumberRealEstateLoansOrLines']\n\nX['Loans_vs_Other_Lines'] = X['RemainingLines'] \/ (1 + X['NumberRealEstateLoansOrLines'])\n\nX['Debt_per_Real_Estate_Loan'] = X['Debt'] \/ X['NumberRealEstateLoansOrLines']\nX.loc[X['NumberRealEstateLoansOrLines']==0, 'Debt_per_Real_Estate_Loan'] = 0\n\nX['Disposable_Income_per_person'] = (X['MonthlyIncome'] - X['Debt']) \/ (X['NumberOfDependents'] + 1)\nX.loc[X['NumberOfDependents']==0, 'Disposable_Income_per_person'] = 0\n\nX['RemainingLines_per_person'] = X['RemainingLines'] \/ (X['NumberOfDependents'] + 1)\n\nX['NumberRE_X_DebtRatio_X_age'] = X['NumberRealEstateLoansOrLines'] * X['DebtRatio'] \/ X['age']\n\nX['RevolvingUtilization_per_age'] = X['RevolvingUtilizationOfUnsecuredLines'] \/ X['age']\n\nX.drop(['MonthlyIncome', 'NumberRealEstateLoansOrLines'], axis=1, inplace=True)\nscore_dataset(X, y)","1d48703d":"def PCA_features():\n    \n    df = pd.concat([train, test.drop(['SeriousDlqin2yrs'], axis=1)], axis=0)\n\n    X_stand = StandardScaler().fit_transform(df)\n    pca = PCA().fit(X_stand)\n    X_pca = pca.transform(X_stand)\n\n    Components_names = ['PC{}'.format(i) for i in range(1, pca.n_components_ + 1)]\n    PC_df_train = pd.DataFrame(X_pca, columns = Components_names)[:len(train)]\n    PC_df_test = pd.DataFrame(X_pca, columns = Components_names)[len(train):len(train) + len(test)].reset_index()\n    \n    return PC_df_train, PC_df_test","cd043416":"def feature_eng(df):\n    \n    df['Weighted_Sum_PastDue'] = 2 * df['NumberOfTime30-59DaysPastDueNotWorse'] + 3 * df['NumberOfTime60-89DaysPastDueNotWorse'] + 6 * df['NumberOfTimes90DaysLate']\n\n    df['90days_out_of_TotalPastDue'] = df['NumberOfTimes90DaysLate'] \/ (df['Weighted_Sum_PastDue'])\n    df.loc[df['Weighted_Sum_PastDue']==0, '90days_out_of_TotalPastDue'] = 0\n\n    df['RemainingLines'] = df['NumberOfOpenCreditLinesAndLoans'] - df['NumberRealEstateLoansOrLines']\n\n    df['Loans_vs_Other_Lines'] = df['RemainingLines'] \/ (1 + df['NumberRealEstateLoansOrLines'])\n\n    df['Debt_per_Real_Estate_Loan'] = df['Debt'] \/ df['NumberRealEstateLoansOrLines']\n    df.loc[df['NumberRealEstateLoansOrLines']==0, 'Debt_per_Real_Estate_Loan'] = 0\n\n    df['Disposable_Income_per_person'] = (df['MonthlyIncome'] - df['Debt']) \/ (df['NumberOfDependents'] + 1)\n    df.loc[df['NumberOfDependents']==0, 'Disposable_Income_per_person'] = 0\n\n    df['RemainingLines_per_person'] = df['RemainingLines'] \/ (df['NumberOfDependents'] + 1)\n\n    df['NumberRE_X_DebtRatio_X_age'] = df['NumberRealEstateLoansOrLines'] * df['DebtRatio'] \/ df['age']\n\n    df['RevolvingUtilization_per_age'] = df['RevolvingUtilizationOfUnsecuredLines'] \/ df['age']\n\n    df.drop(['MonthlyIncome', 'NumberRealEstateLoansOrLines'], axis=1, inplace=True)\n    \n    return df","cdb7294c":"PCA_features = PCA_features()\ntrain = train.join(PCA_features[0][['PC1','PC5','PC9']])\ntest = test.join(PCA_features[1][['PC1','PC5','PC9']])\ntrain = feature_eng(train)\ntest = feature_eng(test)","dea2717d":"score_dataset(train, y)","477c366b":"def score_model(X, y, model, print_fold_scores=False):\n    \n    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    oof = np.zeros(len(X))\n    predictions = np.zeros(len(test))\n\n    for fold_, (train_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n        print('Fold {}'.format(fold_))\n        \n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        clf = model.fit(X_train, y_train)\n        \n        oof[val_idx] = clf.predict_proba(X_val)[:, 1]\n        \n        if print_fold_scores:\n            print('Fold train score: ', roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1]))\n            print('Fold val score: ', roc_auc_score(y_val, clf.predict_proba(X_val)[:, 1]))\n            \n            \n        predictions += clf.predict_proba(test.drop('SeriousDlqin2yrs', axis=1))[:, 1] \/ folds.n_splits\n        \n    print(\"CV score: {:.8f}\".format(roc_auc_score(y, oof)))\n    \n    return predictions","52831426":"# Found after a Bayesian Optimization process (code below)\n\nGBM_params = {'learning_rate': 0.0018069834369607075,\n             'max_depth': 8,\n             'max_features': 4,\n             'min_samples_leaf': 47,\n             'min_samples_split': 389,\n             'subsample': 0.8573598985000007,\n             'n_iter_no_change': 300,\n             'n_estimators': 5000,\n             'verbose': 1,\n             'random_state': 144}","14dc7a6a":"predictions = score_model(train, y, GradientBoostingClassifier(**GBM_params), True)","ec7e932c":"# bayes_cv_tuner = BayesSearchCV(estimator = GradientBoostingClassifier(n_iter_no_change = 300,\n#                                                                       n_estimators = 5000,\n#                                                                       verbose=1,\n#                                                                       random_state=144),\n                               \n#                                search_spaces = {'learning_rate': (0.001, 0.04),\n#                                                 'max_depth': (1, 8),\n#                                                 'subsample': (0.01, 1.0, 'uniform'),\n#                                                 'min_samples_split': (10, 500, 'log-uniform'),\n#                                                 'min_samples_leaf': (5, 50, 'log-uniform'),\n#                                                 'max_features': (2, 15, 'uniform')},\n                               \n#                                scoring = 'roc_auc',\n#                                cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n#                                n_jobs = -1,\n#                                n_iter = 20,\n#                                n_points=3,\n#                                verbose = 500,\n#                                refit = True,\n#                                random_state = 786)\n\n# result = bayes_cv_tuner.fit(train.values, y.values)","241eb1be":"sample_submission = pd.read_csv('..\/input\/GiveMeSomeCredit\/sampleEntry.csv')\nsample_submission['Probability'] = predictions\nsample_submission.to_csv('sample_sub6.csv', index=False)","694ca792":"import warnings\n\ntry:\n    from collections.abc import Sized\nexcept ImportError:\n    from collections import Sized\n\nimport numpy as np\nfrom scipy.stats import rankdata\n\nfrom sklearn.model_selection._search import BaseSearchCV\nfrom sklearn.utils import check_random_state\n\nfrom sklearn.utils.validation import check_is_fitted\ntry:\n    from sklearn.metrics import check_scoring\nexcept ImportError:\n    from sklearn.metrics.scorer import check_scoring\n\nfrom skopt import Optimizer\nfrom skopt.utils import point_asdict, dimensions_aslist, eval_callbacks\nfrom skopt.space import check_dimension\nfrom skopt.callbacks import check_callback\n\n\nclass BayesSearchCV(BaseSearchCV):\n\n    def __init__(self, estimator, search_spaces, optimizer_kwargs=None,\n                 n_iter=50, scoring=None, fit_params=None, n_jobs=1,\n                 n_points=1, iid='deprecated', refit=True, cv=None, verbose=0,\n                 pre_dispatch='2*n_jobs', random_state=None,\n                 error_score='raise', return_train_score=False):\n\n        self.search_spaces = search_spaces\n        self.n_iter = n_iter\n        self.n_points = n_points\n        self.random_state = random_state\n        self.optimizer_kwargs = optimizer_kwargs\n        self._check_search_space(self.search_spaces)\n        # Temporary fix for compatibility with sklearn 0.20 and 0.21\n        # See scikit-optimize#762\n        # To be consistent with sklearn 0.21+, fit_params should be deprecated\n        # in the constructor and be passed in ``fit``.\n        self.fit_params = fit_params\n\n        if iid != \"deprecated\":\n            warnings.warn(\"The `iid` parameter has been deprecated \"\n                          \"and will be ignored.\")\n        self.iid = iid  # For sklearn repr pprint\n\n        super(BayesSearchCV, self).__init__(\n             estimator=estimator, scoring=scoring,\n             n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,\n             pre_dispatch=pre_dispatch, error_score=error_score,\n             return_train_score=return_train_score)\n\n    def _check_search_space(self, search_space):\n        \"\"\"Checks whether the search space argument is correct\"\"\"\n\n        if len(search_space) == 0:\n            raise ValueError(\n                \"The search_spaces parameter should contain at least one\"\n                \"non-empty search space, got %s\" % search_space\n            )\n\n        # check if space is a single dict, convert to list if so\n        if isinstance(search_space, dict):\n            search_space = [search_space]\n\n        # check if the structure of the space is proper\n        if isinstance(search_space, list):\n            # convert to just a list of dicts\n            dicts_only = []\n\n            # 1. check the case when a tuple of space, n_iter is provided\n            for elem in search_space:\n                if isinstance(elem, tuple):\n                    if len(elem) != 2:\n                        raise ValueError(\n                            \"All tuples in list of search spaces should have\"\n                            \"length 2, and contain (dict, int), got %s\" % elem\n                        )\n                    subspace, n_iter = elem\n\n                    if (not isinstance(n_iter, int)) or n_iter < 0:\n                        raise ValueError(\n                            \"Number of iterations in search space should be\"\n                            \"positive integer, got %s in tuple %s \" %\n                            (n_iter, elem)\n                        )\n\n                    # save subspaces here for further checking\n                    dicts_only.append(subspace)\n                elif isinstance(elem, dict):\n                    dicts_only.append(elem)\n                else:\n                    raise TypeError(\n                        \"A search space should be provided as a dict or\"\n                        \"tuple (dict, int), got %s\" % elem)\n\n            # 2. check all the dicts for correctness of contents\n            for subspace in dicts_only:\n                for k, v in subspace.items():\n                    check_dimension(v)\n        else:\n            raise TypeError(\n                \"Search space should be provided as a dict or list of dict,\"\n                \"got %s\" % search_space)\n\n    @property\n    def optimizer_results_(self):\n        check_is_fitted(self, '_optim_results')\n        return self._optim_results\n\n    def _make_optimizer(self, params_space):\n        \"\"\"Instantiate skopt Optimizer class.\n        Parameters\n        ----------\n        params_space : dict\n            Represents parameter search space. The keys are parameter\n            names (strings) and values are skopt.space.Dimension instances,\n            one of Real, Integer or Categorical.\n        Returns\n        -------\n        optimizer: Instance of the `Optimizer` class used for for search\n            in some parameter space.\n        \"\"\"\n\n        kwargs = self.optimizer_kwargs_.copy()\n        kwargs['dimensions'] = dimensions_aslist(params_space)\n        optimizer = Optimizer(**kwargs)\n        for i in range(len(optimizer.space.dimensions)):\n            if optimizer.space.dimensions[i].name is not None:\n                continue\n            optimizer.space.dimensions[i].name = list(sorted(\n                params_space.keys()))[i]\n\n        return optimizer\n\n    def _step(self, search_space, optimizer, evaluate_candidates, n_points=1):\n        \"\"\"Generate n_jobs parameters and evaluate them in parallel.\n        \"\"\"\n        # get parameter values to evaluate\n        params = optimizer.ask(n_points=n_points)\n\n        # convert parameters to python native types\n        params = [[np.array(v).item() for v in p] for p in params]\n\n        # make lists into dictionaries\n        params_dict = [point_asdict(search_space, p) for p in params]\n\n        all_results = evaluate_candidates(params_dict)\n        # Feed the point and objective value back into optimizer\n        # Optimizer minimizes objective, hence provide negative score\n        local_results = all_results[\"mean_test_score\"][-len(params):]\n        return optimizer.tell(params, [-score for score in local_results])\n\n    @property\n    def total_iterations(self):\n        \"\"\"\n        Count total iterations that will be taken to explore\n        all subspaces with `fit` method.\n        Returns\n        -------\n        max_iter: int, total number of iterations to explore\n        \"\"\"\n        total_iter = 0\n\n        for elem in self.search_spaces:\n\n            if isinstance(elem, tuple):\n                space, n_iter = elem\n            else:\n                n_iter = self.n_iter\n\n            total_iter += n_iter\n\n        return total_iter\n\n    def fit(self, X, y=None, *, groups=None, callback=None, **fit_params):\n        \"\"\"Run fit on the estimator with randomly drawn parameters.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like, shape = [n_samples] or [n_samples, n_output]\n            Target relative to X for classification or regression (class\n            labels should be integers or strings).\n        groups : array-like, with shape (n_samples,), optional\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        callback: [callable, list of callables, optional]\n            If callable then `callback(res)` is called after each parameter\n            combination tested. If list of callables, then each callable in\n            the list is called.\n        \"\"\"\n        self._callbacks = check_callback(callback)\n\n        if self.optimizer_kwargs is None:\n            self.optimizer_kwargs_ = {}\n        else:\n            self.optimizer_kwargs_ = dict(self.optimizer_kwargs)\n\n        super().fit(X=X, y=y, groups=groups, **fit_params)\n\n        # BaseSearchCV never ranked train scores,\n        # but apparently we used to ship this (back-compat)\n        if self.return_train_score:\n            self.cv_results_[\"rank_train_score\"] = \\\n                rankdata(-np.array(self.cv_results_[\"mean_train_score\"]),\n                         method='min').astype(int)\n        return self\n\n    def _run_search(self, evaluate_candidates):\n        # check if space is a single dict, convert to list if so\n        search_spaces = self.search_spaces\n        if isinstance(search_spaces, dict):\n            search_spaces = [search_spaces]\n\n        callbacks = self._callbacks\n\n        random_state = check_random_state(self.random_state)\n        self.optimizer_kwargs_['random_state'] = random_state\n\n        # Instantiate optimizers for all the search spaces.\n        optimizers = []\n        for search_space in search_spaces:\n            if isinstance(search_space, tuple):\n                search_space = search_space[0]\n            optimizers.append(self._make_optimizer(search_space))\n        self.optimizers_ = optimizers  # will save the states of the optimizers\n\n        self._optim_results = []\n\n        n_points = self.n_points\n\n        for search_space, optimizer in zip(search_spaces, optimizers):\n            # if not provided with search subspace, n_iter is taken as\n            # self.n_iter\n            if isinstance(search_space, tuple):\n                search_space, n_iter = search_space\n            else:\n                n_iter = self.n_iter\n\n            # do the optimization for particular search space\n            while n_iter > 0:\n                # when n_iter < n_points points left for evaluation\n                n_points_adjusted = min(n_iter, n_points)\n\n                optim_result = self._step(\n                    search_space, optimizer,\n                    evaluate_candidates, n_points=n_points_adjusted\n                )\n                n_iter -= n_points\n\n                if eval_callbacks(callbacks, optim_result):\n                    break\n            self._optim_results.append(optim_result)","53bbc18a":"### Additional features to improve final score: PCA features","14e3b03f":"Baseline score was 85.13404","cb12a12b":"### Bayesian Optimization code to find GBDT hyperparameters","0fb536a5":"# Modeling","d1e67193":"### Transformation of raw train and test datasets","9050e328":"### Baseline Score","abbbad7b":"### Score when new features are added","36afa7c7":"# [Give me some credit competition](https:\/\/www.kaggle.com\/c\/GiveMeSomeCredit\/overview) - Feature Engineering & Prediction\n---\n\nLink to the EDA & Data Cleaning part : [EDA-Cleaning](https:\/\/www.kaggle.com\/jamesngoa\/credit-scoring-challenge-eda-data-cleaning)\n### Competition Intro : \n*Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit.* \n\n*Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.*\n\n*The goal of this competition is to build a model that borrowers can use to help make the best financial decisions.*\n\n*Historical data are provided on 250,000 borrowers.*\n\n---\n### Data Dictionary\n\n`SeriousDlqin2yrs` : Person experienced 90 days past due delinquency or worse.\n\n`RevolvingUtilizationOfUnsecuredLines` : Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits.\n\n`age` : Age of borrower in years.\n\n`NumberOfTime30-59DaysPastDueNotWorse` : Number of times borrower has been 30-59 days past due but no worse in the last 2 years.\n\n`DebtRatio` : Monthly debt payments, alimony,living costs divided by monthy gross income.\n\n`MonthlyIncome` : Monthly income.\n\n`NumberOfOpenCreditLinesAndLoans` : Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards).\n\n`NumberOfTimes90DaysLate` : Number of times borrower has been 90 days or more past due.\n\n`NumberRealEstateLoansOrLines` : Number of mortgage and real estate loans including home equity lines of credit.\n\n`NumberOfTime60-89DaysPastDueNotWorse` : Number of times borrower has been 60-89 days past due but no worse in the last 2 years.\n\n`NumberOfDependents` : Number of dependents in family excluding themselves (spouse, children etc.).","7fbed3f1":"##### Score of this new train dataset with our simply tuned Decision Tree Classifier","c4fec455":"## Feature Engineering","4b28c8b4":"### Assessment of the relevance of new features"}}