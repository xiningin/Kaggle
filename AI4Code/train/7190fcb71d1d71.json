{"cell_type":{"3807dd3a":"code","93756820":"code","e9a80a59":"code","cfafc2eb":"code","cf1eef10":"code","786fc089":"code","70a12c17":"code","b2e3c49b":"code","b8a809a9":"code","67d10ab2":"code","ecc9629b":"code","e209c825":"code","debf85ea":"code","7e3415f4":"code","4b077b96":"code","525bc4e4":"code","e2faa706":"code","42127a49":"code","e3a45d14":"code","e79c93a8":"code","89573505":"code","e4fd6fa2":"code","6ee64009":"code","56f3f250":"code","1d5d4314":"code","40649a46":"code","ac836adb":"code","6900c909":"code","2c379d75":"code","7802fba6":"code","0dda05a5":"code","78f591fd":"code","b1d62634":"code","39195f1e":"code","cabcd67a":"code","90b55fc6":"code","4a0ac627":"code","8a72292b":"code","0925d7b0":"code","0bb00ca8":"code","db17529b":"code","d0f5d847":"code","481d989e":"code","288c06f3":"code","2332e863":"code","6cc704b5":"code","56663862":"code","1bed78e3":"code","38050676":"code","f8be9a21":"code","378c5593":"code","ead54b95":"code","0dc4b772":"code","bf0ae53d":"code","42c33bbb":"code","fe0e2b14":"code","0467efbf":"code","1cc7b9c3":"code","aeab2a73":"code","8aa6ed2f":"code","2a0c31a8":"code","9bebc99a":"code","f420e131":"code","efc70785":"code","c81ac294":"code","a8559929":"code","9cfbd317":"code","ea870ee5":"code","fa936ff0":"code","c58b52e7":"code","37faa2e9":"code","0ffcfe3d":"code","b83b78d2":"code","a7c24275":"code","ebc3be48":"code","ca0e5934":"code","b88f861d":"code","9070a285":"code","6731c836":"code","72b095ac":"code","d47ad106":"code","63dace5f":"code","7db2620a":"code","f490757f":"code","d3d5528d":"code","7c631cd3":"code","05438c9e":"code","993be35e":"markdown","0b0c5464":"markdown","ca372466":"markdown","65064f23":"markdown","648ae32e":"markdown","3c9a78a5":"markdown","9e92ac23":"markdown","f1cd9ada":"markdown","38dd90b5":"markdown","ad836d4c":"markdown","66537f78":"markdown","a9efb0df":"markdown","4abcff2f":"markdown","6142ba16":"markdown","28512573":"markdown","c0f058f7":"markdown","a7afb809":"markdown","7360f931":"markdown","843b8fc5":"markdown","f2e8f2c0":"markdown","0b3d5c02":"markdown","035f947b":"markdown","19dbdb34":"markdown","038319f7":"markdown","c226097f":"markdown"},"source":{"3807dd3a":"#Import all the required libraries\n\n# System Libraries \nimport os, glob\nfrom glob import glob\nimport pickle\nfrom sys import getsizeof\n\n# Date and Time \nimport datetime,time\n\n# Data manipulation\nimport numpy as np\nimport pandas as pd\nimport collections, random, re\nfrom collections import Counter\n\n\n# Model building \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\n\n#Read\/Display  images\nfrom skimage import io\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","93756820":"# tensorflow Libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import plot_model","e9a80a59":"## Global Variables\n\nINPUT_PATH = \"..\/input\/flickr8k\/\"\nIMAGE_PATH = INPUT_PATH+'Images\/'\nCAPTIONS_FILE = INPUT_PATH+'captions.txt'\nOUTPUT_IMAGE_PATH = \"..\/working\/Image\/\"","cfafc2eb":"#1.Import the dataset and read image & captions into two seperate variables\n#2.Visualise both the images & text present in the dataset\n\nall_imgs = glob(IMAGE_PATH + '*.jpg')\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs)))\nprint(all_imgs[0])","cf1eef10":"def plot_image(images, captions=None, cmap=None ):\n    f, axes = plt.subplots(1, len(images), sharey=True)\n    f.set_figwidth(15)\n   \n    for ax,image in zip(axes, images):\n        ax.imshow(io.imread(image), cmap)","786fc089":"#Plotting last 10 images \nplot_image(all_imgs[8081:])","70a12c17":"# Creating dataframe column of captions file for visulaizing\n# Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples.\nimport pandas as pd\ncaptions_df = pd.read_csv(CAPTIONS_FILE)\n\npd.set_option('display.max_colwidth',-1) # Set the max column width to see the complete caption\nprint(captions_df.shape)\ncaptions_df.head()","b2e3c49b":"captions_df.nunique()","b8a809a9":"plot_image(all_imgs[:10])\ncaptions_df.head(10)","67d10ab2":"# Reading captions file\nfile = open(CAPTIONS_FILE,'rb')\ncaptions_txt = file.read().decode('utf-8')\nfile.close()\nimg_cap_corpus=captions_txt.split('\\n')\nimg_cap_corpus.pop(0)## poping first line names of columns (image,caption)","ecc9629b":"datatxt = []\nfor line in img_cap_corpus:\n    col = line.split(',')# Seperates columns image and caption\n   \n    if len(col)==1:\n        continue\n    w = col[0].split(\"_\") # seperating image filename to extract the id \n   \n    w[1] = IMAGE_PATH + col[0] # saving complete path of image file for building model\n    datatxt.append(w + [col[1].lower()])\n\ndf= pd.DataFrame(datatxt,columns=['ID','Path','Captions'])\n#df = df.reindex(columns =['ID','Path','Captions'])\nuni_filenames= np.unique(df.ID.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df.ID.values).values())","e209c825":"df","debf85ea":"all_img_id= df.ID#store all the image id \nall_img_vector= df.Path#store all the image path here\nannotations= df.Captions#store all the captions here\n\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(set(all_img_vector))))\nprint(annotations[:10])","7e3415f4":"uni_filenames = np.unique(df.ID.values)\nprint(\"The number of unique file names : {}\".format(len(uni_filenames)))\nprint(\"The distribution of the number of captions for each image:\")\nCounter(Counter(df.ID.values).values())","4b077b96":"\n## Creating vocabulary of all words present in captions\n\ndef split_sentence(sentence):\n    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n\ndef generate_vocabulary(captions):\n      \n    words = []\n\n    for sentence in captions:\n        sent_words = split_sentence(sentence)\n        for word in sent_words: \n            words.append(word)\n    return sorted(words)\n    \nvocab = generate_vocabulary(df.Captions)","525bc4e4":"vocabulary =  Counter(vocab)\n\ndf_word = pd.DataFrame.from_dict(vocabulary, orient='index')\n\ndf_word = df_word.sort_values(by=[0],ascending=False).reset_index()\ndf_word =df_word.rename(columns={'index':'word', 0:'count'})\n","e2faa706":"#Visualize the top 30 occuring words in the captions\n\ndef plthist(index,words,count, title=\"The top 50 most frequently appearing words\"):\n    plt.figure(figsize=(20,3))\n    plt.bar(words,count,color='maroon', width =0.4)\n    plt.xlabel(\"Words\",  fontsize=20) \n    plt.ylabel(\"Word Count\",rotation=90,fontsize=20) \n   # plt.yticks(fontsize=20)\n    plt.xticks(index,words,rotation=90,fontsize=20)\n    plt.title(title,fontsize=20)\n    plt.show()\n    \nwords = list(df_word[:30].word)\n\ncount =list(df_word['count'][:30])\nplthist(list(range(0,30)),words,count)","42127a49":"#Create a list which contains all the captions\n\n#adding  the <start> & <end> token to all the captions \ndf['Captions']=df.Captions.apply(lambda x : f\"<start> {x} <end>\")\nannotations = df.Captions\n# Find max length of sequence\nmax_length = max(df.Captions.apply(lambda x : len(x.split())))\n\n#Create a list which contains all the path to the images\nunique_img_path= sorted(set(all_img_vector))#write your code here\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(all_img_vector)))\nprint(\"Unique images present in the dataset: \" + str(len(unique_img_path)))","e3a45d14":"def plot_image_captions(Pathlist,captionsList,fig,count=2,npix=299,nimg=2):\n        image_load = load_img(Path,target_size=(npix,npix,3))\n        ax = fig.add_subplot(nimg,2,count,xticks=[],yticks=[])\n        ax.imshow(image_load)\n        \n        count +=1\n        ax = fig.add_subplot(nimg,2,count)\n        plt.axis('off')\n        ax.plot()\n        ax.set_xlim(0,1)\n        ax.set_ylim(0,len(captions))\n        for i, caption in enumerate(captions):\n            ax.text(0,i,caption,fontsize=20)\n        ","e79c93a8":"# Images \nfig = plt.figure(figsize=(10,20))\ncount = 1\n    \nfor Path in df[:20].Path.unique():\n    captions = list(df[\"Captions\"].loc[df.Path== Path].values)\n    plot_image_captions(Path,captions,fig,count,299,5)\n    count +=2\nplt.show()","89573505":"vocabulary =  Counter(vocab)\n\n","e4fd6fa2":"def data_limiter(all_captions,all_img_vector):\n    img_captions, img_name_vector = shuffle(all_captions,all_img_vector,random_state=42)\n   # img_captions = img_captions[:num]\n   # img_name_vector = img_name_vector[:num]\n    return img_captions,img_name_vector\n\nimg_captions,all_img_vector = data_limiter(annotations,all_img_vector)","6ee64009":"def tokenize_captions(top_cap,captions):\n    special_chars = '!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ '\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_freq_words,\n                                                  oov_token=\"UNK\",\n                                                  filters=special_chars)\n    tokenizer.fit_on_texts(captions)\n    \n    # Adding PAD to tokenizer list\n    tokenizer.word_index['PAD'] = 0\n    tokenizer.index_word[0] = 'PAD'   \n   \n    return tokenizer  \n    ","56f3f250":"top_freq_words = 5000\ntokenizer = tokenize_captions(top_freq_words,img_captions)\n# Pad each vector to the max_length of the captions ^ store it to a vairable\n\n# Create the tokenized vectors\ncap_seqs = tokenizer.texts_to_sequences(img_captions)\n# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(cap_seqs, padding='post')\n\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))\nprint(cap_vector[:5])","1d5d4314":"# Maximum length of sequence \nmax_length = max([len(cap) for cap in cap_seqs])\nprint(\"Shape of caption vector :\", cap_vector.shape, )\nprint(\"Maximium length of sequence = \", max_length)","40649a46":"# Create word-to-index and index-to-word mappings.\ndef print_word_2_index(word):\n    print(\"Word = {}, index = {}\".format(word, tokenizer.word_index[word]))\n\n          \nprint( \"Word 2 index mapping\")\nprint_word_2_index(\"<start>\")\nprint_word_2_index(\"PAD\")","ac836adb":"# Create word-to-index and index-to-word mappings.\ndef print_index_2_word(index):\n    print(\"Index = {}, Word = {}\".format(index, tokenizer.index_word[index]))\n\n          \nprint( \"Index 2 word mapping\")\nprint_index_2_word(5)\nprint_index_2_word(4999)","6900c909":"# Word count of your tokenizer to see the Top 30 occuring words after text processing\nword_count = tokenizer.word_counts\nimport operator\nword_count = sorted(word_count.items(), key=operator.itemgetter(1),reverse=True)\nfor k,v in word_count[:30]:\n    print (k,v)\n    ","2c379d75":"#3.1 To save the memory(RAM) from getting exhausted, extract the features of the image using the last layer of pre-trained model InceptionV3\n\n\nimage_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\n\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","7802fba6":"#3.2.Resize them into the shape of (299, 299) for InceptionV3\n#3.3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3.\n\ndef preprocess_image(image_path):\n    shape = (299, 299)\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image,channels=3)\n    image = tf.image.resize(image,shape)\n    image = tf.keras.applications.inception_v3.preprocess_input(image)\n    return image, image_path","0dda05a5":"#### Installing progrss bar\n#!pip install --upgrade pip\n#pip install -q tqdm","78f591fd":"# Creating Image dataset of preprocessed images \n\nBATCH_SIZE = 64\nshape = (299, 299)\nencode = sorted (set(all_img_vector))\n\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode)\nimage_dataset = image_dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)","b1d62634":"sample_img_batch, sample_cap_batch = next(iter(image_dataset))\nprint(sample_img_batch.shape) #(batch_size, 299,299,3)\nprint(sample_cap_batch.shape) #(batch_size, max_len)","39195f1e":"image_train, image_test, captions_train, captions_test = train_test_split(all_img_vector,cap_vector, test_size=0.2, random_state=42)","cabcd67a":"# Deleting previously created  npy files\nall_output_imgs = glob(OUTPUT_IMAGE_PATH + '*.*')\nprint(\"The total images present in the dataset: {}\".format(len(all_output_imgs)))\nif len(all_output_imgs)> 0:\n    print(all_output_imgs[0])\n    for path in (all_output_imgs):\n        os.remove(path)\nall_output_imgs = glob(OUTPUT_IMAGE_PATH + '*.npy')\nprint(\"The total images after deleting in the dataset: {}\".format(len(all_output_imgs)))","90b55fc6":"from tqdm import tqdm","4a0ac627":"\n\nos.mkdir(\"Image\")\nall_imgs_npy = []\nfor img,path in tqdm(image_dataset):\n    batch_features = image_features_extract_model(img)\n    batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n    \n    for bf, p in zip(batch_features, path):\n        file_name = p.numpy().decode(\"utf-8\").split('\/')[-1]\n        all_imgs_npy.append(file_name)\n        np.save( OUTPUT_IMAGE_PATH +file_name, bf.numpy())","8a72292b":"#all_imgs_npy = glob(OUTPUT_IMAGE_PATH +\"*.*\")\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs_npy)))\nprint(all_imgs_npy[0])","0925d7b0":"img_to_cap_vector = collections.defaultdict(list)\n\nfor img,cap in zip(all_img_vector,cap_vector):\n    #print(img,cap)\n    img_to_cap_vector[img].append(cap)","0bb00ca8":"#print(img_to_cap_vector[IMAGE_PATH+\"2330062180_355ccbceb5.jpg\"])","db17529b":"def vector_to_sentence(caplist):\n    captions_word =[]\n    #print(caplist)\n    for captions in caplist:\n        #print(captions)\n        list_caption = list(captions)\n\n        captions_word.append(' '.join(tokenizer.index_word[i] for i in list_caption))\n    return captions_word\n        ","d0f5d847":"## 4. Create the train & test data \n4.1.Combine both images & captions to create the train & test dataset using tf.data.Dataset API. Create the train-test spliit using 80-20 ratio & random state = 42\n\n4.2.Make sure you have done Shuffle and batch while building the dataset\n\n4.3.The shape of each image in the dataset after building should be (batch_size, 299, 299, 3)\n\n4.4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n","481d989e":"#4.1 Combine both images & captions to create the train & test dataset using tf.data.Dataset API. \n#Create the train-test spliit using 80-20 ratio & random state = 42\ntf.random.set_seed(42)\n# Create training and validation sets using an 80-20 split randomly.\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\n\nslice_index = int(len(img_keys)*0.8)\nimg_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n#print(img_name_train_keys, img_name_val_keys)\nimage_train = []\ncaptions_train = []\nfor imgt in img_name_train_keys:\n    captions_len = len(img_to_cap_vector[imgt])\n    #print(captions_len)\n    image_train.extend([imgt] * captions_len)\n    captions_train.extend(img_to_cap_vector[imgt])\n\nimage_test = []\ncaptions_test = []\nfor imgtest in img_name_val_keys:\n    captions_len = len(img_to_cap_vector[imgtest])\n    image_test.extend([imgtest] * captions_len)\n    captions_test.extend(img_to_cap_vector[imgtest])","288c06f3":"#image_train,image_test,captions_train,captions_test = train_test_split(all_imgs_npy,cap_vector)","2332e863":"print(\"Image_train = {}, Captions_train = {}\".format(len(image_train), len(captions_train)))\n\nprint(\"Image_test = {}, Captions_test = {}\".format(len(image_test), len(captions_test)))","6cc704b5":"def get_file_name(filename):\n    return (IMAGE_PATH + filename)\n\ndef vector_to_sentence(caplist):\n    captions_word =[]\n    #print(caplist)\n    for captions in caplist:\n        #print(captions)\n        list_caption = list(captions)\n\n        captions_word.append(' '.join(tokenizer.index_word[i] for i in list_caption))\n    return captions_word","56663862":"caplist_train = []\n#for i in range(1,10):\ncaptions_word = vector_to_sentence(list(captions_train[0:30]))\nprint(captions_word[28:])\n","1bed78e3":"#print(image_train[28],image_train[29])\nfilename = (IMAGE_PATH + image_train[28].split('\/')[-1]).replace(\".npy\",\"\")\nfilename1 = (IMAGE_PATH + image_train[29].split('\/')[-1]).replace(\".npy\",\"\")\n\nplot_image([filename,filename1],caplist_train)\n","38050676":"# Load the numpy files\ndef map_func(img_name, cap):\n    filename = OUTPUT_IMAGE_PATH + img_name.decode('utf-8').split('\/')[-1] + \".npy\"\n    img_tensor = np.load(filename)\n    return img_tensor, cap","f8be9a21":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\ntrain_dataset = tf.data.Dataset.from_tensor_slices((image_train, captions_train))\n# Use map to load the numpy files in parallel\ntrain_dataset = train_dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)","378c5593":"\ntest_dataset = tf.data.Dataset.from_tensor_slices((image_test, captions_test))\n# Use map to load the numpy files in parallel\ntest_dataset = test_dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.AUTOTUNE)\n\n# Shuffle and batch\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)","ead54b95":"train_img_batch, train_cap_batch = next(iter(train_dataset))\nprint(train_img_batch.shape) #(batch_size, 8*8,2048)\nprint(train_cap_batch.shape) #(batch_size, max_len)","0dc4b772":"test_img_batch, test_cap_batch = next(iter(test_dataset))\nprint(test_img_batch.shape) #(batch_size, 8*8,2048)\nprint(test_cap_batch.shape) #(batch_size, max_len)","bf0ae53d":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\n\n\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64\n\nembedding_dim = 256 \nunits = 512\nvocab_size = top_freq_words + 1 #top 5,000 words +1\ntrain_num_steps = len(image_train) \/\/ BATCH_SIZE\ntest_num_steps = len(image_test) \/\/ BATCH_SIZE","42c33bbb":"def padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\ndef look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\ndef scaled_dot_product_attention(W1, W2, v, mask):\n    matmul_W1W2 = tf.matmul(W1, W2, transpose_b=True)  # (..., seq_len_W1, seq_len_W2)\n    dimW2 = tf.cast(tf.shape(W2)[-1], tf.float32)\n    scaled_attention_logits = matmul_W1W2\/ tf.math.sqrt(dimW2)\n\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9) \n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n    output = tf.matmul(attention_weights, v)  # (..., seq_len_W1, depth_v)\n\n    return output, attention_weights","fe0e2b14":"# Subclassing tensor layer to create multilayer Attention layer to be later  used in Attention Layer\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % self.num_heads == 0\n        self.depth = d_model \/\/ self.num_heads\n        self.W1 = tf.keras.layers.Dense(d_model)\n        self.W2 = tf.keras.layers.Dense(d_model)\n        self.Wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, W1, W2, mask=None):\n        batch_size = tf.shape(q)[0]\n        W1 = self.W1(W1)  # (batch_size, seq_len, d_model)\n        W2 = self.W2(W2)  # (batch_size, seq_len, d_model)\n        Wv = self.Wv(v)  # (batch_size, seq_len, d_model)\n\n        W1 = self.split_heads(W1, batch_size)  # (batch_size, num_heads, seq_len_W1, depth)\n        W2 = self.split_heads(W2, batch_size)  # (batch_size, num_heads, seq_len_W2, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(W1, W2, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q,      num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,\n                                 (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        return output, attention_weights\n\n    def point_wise_feed_forward_network(d_model, dff):\n        return tf.keras.Sequential([\n                tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n                tf.keras.layers.Dense(d_model)])  # (batch_size, seq_len, d_model)\n","0467efbf":"#### Creating Encoder layer subclass keras.layers\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, training, mask=None):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n        return out2","1cc7b9c3":"\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2","aeab2a73":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, row_size,col_size,rate=0.1):\n        super(Encoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n        self.pos_encoding = positional_encoding_2d(row_size,col_size,self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask=None):\n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)  # (batch_size, input_seq_len(H*W), d_model)\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)","8aa6ed2f":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers,d_model,num_heads,dff, target_vocab_size, maximum_position_encoding,   rate=0.1):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                         for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                            look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n\n        return x, attention_weights","2a0c31a8":"class Encoder(Model):\n    def __init__(self,embed_dim):\n        super(Encoder, self).__init__()\n        #Dense layer with relu activation\n        self.dense = tf.keras.layers.Dense(embedding_dim)\n        # shape after fc == (batch_size, 64, embedding_dim)\n        #self.fc = tf.keras.layers.Dense(embedding_dim)\n        #self.dropout = tf.keras.layers.Dropout(0.5,noise_shape=None,seed=None)\n        \n    def call(self, features):\n        features = self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n        features = tf.nn.relu(features)\n        return features","9bebc99a":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)#build your Dense layer\n        self.W2 = tf.keras.layers.Dense(units)#build your Dense layer\n        self.V = tf.keras.layers.Dense(1)#build your final Dense layer with unit 1\n        self.units=units\n\n    def call(self, features, hidden):\n        #features shape: (batch_size, 8*8, embedding_dim)\n        # hidden shape: (batch_size, hidden_size)\n        hidden_with_time_axis = tf.expand_dims(hidden,1) # Expand the hidden shape to shape: (batch_size, 1, hidden_size)\n        attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n        score = self.V(attention_hidden_layer)# build your score funciton to shape: (batch_size, 8*8, units)\n        attention_weights =  tf.nn.softmax(score,axis=1)# extract your attention weights with shape: (batch_size, 8*8, 1)\n        context_vector = attention_weights * features  #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n        context_vector = tf.reduce_sum(context_vector,axis=1)# reduce the shape to (batch_size, embedding_dim)\n        \n\n        return context_vector, attention_weights","f420e131":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units)#iniitalise your Attention model with units\n        self.embed = tf.keras.layers.Embedding(vocab_size,embed_dim) #build your Embedding layer\n        self.gru = tf.keras.layers.GRU(self.units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n        self.d1 = tf.keras.layers.Dense(self.units)#build your Dense layer\n        self.d2 = tf.keras.layers.Dense(vocab_size)#build your Dense layer\n        self.attention = Attention_model(self.units)\n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features,hidden)#create your context vector & attention weights from attention model\n        # embed your input to shape: (batch_size, 1, embedding_dim)\n        embed =  self.embed(x)\n        embed =  tf.concat([tf.expand_dims(context_vector,1),embed],axis=-1)# Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n        output,state = self.gru(embed)# Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n        x = self.d1(output)\n        x = tf.reshape(x, (-1, x.shape[2])) # shape : (batch_size * max_length, hidden_size)\n        x = self.d2(x) # shape : (batch_size * max_length, vocab_size)\n        \n        return x,state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","efc70785":"encoder=Encoder(embedding_dim)\ndecoder=Decoder(embedding_dim, units, vocab_size)","c81ac294":"features=encoder(train_img_batch)\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * train_cap_batch.shape[0], 1)\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)","a8559929":"class final_Model(tf.keras.Model):\n    def __init__(self, embedding_dim,units,vocab_size, rate=0.1):\n        super(final_Model, self).__init__()\n        self.encoder = Encoder(embedding_dim)#, d_model, num_heads, dff,row_size,col_size, rate)\n        self.decoder = Decoder(embedding_dim, units, vocab_size)\n        self.final_layer = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, inp, tar, training,look_ahead_mask=None,dec_padding_mask=None,enc_padding_mask=None   ):\n        enc_output = self.encoder(embeding_dim)  # (batch_size, inp_seq_len, d_model      )\n        dec_output, attention_weights = self.decoder(embeding_dim,units,vocab_size)\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n        return final_output, attention_weights","9cfbd317":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","ea870ee5":"learning_rate = CustomSchedule(embedding_dim)\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                    epsilon=1e-9)\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\nmetric = tf.keras.metrics.Mean('train_loss',dtype=tf.float32)","fa936ff0":"optimizer = tf.keras.optimizers.Adam()#define the optimizer\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True, reduction='none')#define your loss object\nmetric = tf.keras.metrics.Mean('train_loss',dtype=tf.float32)","c58b52e7":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","37faa2e9":"checkpoint_path_ckpt = \".\/checkpoints\/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer,\n                           metrics=metric)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path_ckpt, max_to_keep=5)","0ffcfe3d":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n    \n    ckpt.restore(ckpt_manager.latest_checkpoint)","b83b78d2":"@tf.function\ndef train_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    \n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor,training =True)\n\n        for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n            loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n    total_loss = (loss \/ int(target.shape[1]))\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n    metric(loss)    \n    return loss, total_loss","a7c24275":"@tf.function\ndef test_step(img_tensor, target):\n    loss = 0\n\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    features = encoder(img_tensor)\n    for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n\n            loss += loss_function(target[:, i], predictions)\n            predicted_id = tf.argmax(predictions[0])\n            dec_input = tf.expand_dims([predicted_id] * target.shape[0] , 1)\n   \n    avg_loss = (loss \/ int(target.shape[1]))\n        \n    return loss, avg_loss","ebc3be48":"def test_loss_cal(dataset):\n    total_loss = 0\n\n    for (batch,(img_tensor,target)) in enumerate(dataset):\n        batch_loss, t_loss = test_step(img_tensor, target)\n        total_loss += t_loss\n   \n    \n    return total_loss\/int(target.shape[1])","ca0e5934":"train_loss = tf.keras.metrics.Mean('train_loss',dtype=tf.float32)","b88f861d":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n    avg_train_loss=total_loss \/ train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()","9070a285":"plt.plot(loss_plot)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","6731c836":"def init_features(image):\n    temp_input = tf.expand_dims(preprocess_image(image)[0], 0) #process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input)# Extract features using our feature extraction model\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)# extract the features by passing the input to encoder\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    return features,dec_input","72b095ac":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n    features,dec_input = init_features(image)\n    \n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input,features,hidden) # get the output from decoder\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n        #extract the predicted id(embedded value) which carries the max value\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n       # print(predicted_id)\n        result.append(tokenizer.index_word[predicted_id])#map the id to the word from tokenizer and append the value to the result list\n\n        if (tokenizer.index_word[predicted_id] == \"<end>\"):\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions\n","d47ad106":"def beam_evaluate(image, beam_index = 3 ):#your value for beam index\n\n    #write your code to evaluate the result using beam search\n    start = [tokenizer.word_index[\"<start>\"]]\n    # result [i][0] : word index of the ith word in result\n    # result [i][1] : probability of ith word being predicted\n    result = [[start, 0.0]]\n    attention_plot = np.zeros((max_length,attention_features_shape))  \n    hidden = decoder.init_state(batch_size=1)\n    features,dec_input = init_features(image)\n    \n    \n    while len(result[0][0]) < max_length:\n        i = 0 \n        temp = []\n        for s in result:\n            preds, hidden, attention_weights = decoder(dec_input,features,hidden)\n            attention_plot = tf.reshape(attention_w,(-1,)).numpy()\n            i +=1\n            # Getting the top <beam_index>(n) predictions and creating a \n            word_preds = np.argsort(preds[0])[-beam_index:]\n            \n            \n            # new list  to put in the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        result = temp\n        # Sorting according to the probabilities\n        result = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        result = result[-beam_index:]\n        predicted_id = result[-1]\n        pred_list = predicted_id[0]\n        \n        pred_id = pred_list[-1]       \n        if(pred_id ==3):\n            break            \n                  \n        dec_input = tf.expand_dims([pred_id],0)\n    \n    result = result[-1][0]\n    intermediate_caption = [tokenizer.index_word[i] for i in start_word]\n\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i == '<end>':\n            break\n        final_caption.append(i)\n        \n    attention_plot = attention_plot[:len(result),:]\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption,attention_plot","63dace5f":"def plot_attmap(caption, attention_plot, image):\n\n    fig = plt.figure(figsize=(10, 10))\n    temp_img = np.array(Image.open(image))\n    \n    len_cap = len(caption)\n    for cap in range(len_cap):\n        weights_img = np.reshape(attention_plot[cap], (8,8))\n        weights_img = np.array(Image.fromarray(weights_img).resize((299, 299), Image.LANCZOS))\n        \n        ax = fig.add_subplot(len_cap\/\/2, len_cap\/\/2, cap+1)\n        ax.set_title(caption[cap], fontsize=15)\n        \n        img=ax.imshow(temp_img)\n        \n        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n        ax.axis('off')\n    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n    plt.show()","7db2620a":"# captions on the validation set\nrid = np.random.randint(0, len(image_test))\nimage = image_test[rid]\n#print(image)\nimage = IMAGE_PATH +(image.split('\/')[-1]).replace(\".npy\",\"\")\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in captions_test[rid] if i not in [0]])\nresult, weight,predictions = evaluate(image)\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', ' '.join(result))","f490757f":"Image.open(image)","d3d5528d":"# n-gram individual BLEU\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import corpus_bleu","7c631cd3":"def filt_text(text):\n    filt=['<start>','<unk>','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","05438c9e":"rid = np.random.randint(0, len(image_test))\ntest_image = image_test[rid]\nprint(test_image)\n\nreal_caption = ' '.join([tokenizer.index_word[i] for i in captions_test[rid] if i not in [0]])\nresult, weight,pred_test = evaluate(test_image)\n\n\nreal_caption=filt_text(real_caption)      \n\n\npred_caption=' '.join(result).rsplit(' ', 1)[0]\n\nreal_appn = []\nprint(real_caption,pred_caption)\nreal_appn.append(real_caption.split())\nreference = list(real_appn)\ncandidate = pred_caption.split()\n#Cummunilative and n\nscore = sentence_bleu(reference, candidate, weights=(0,0,1,0))\nprint(f\"BELU score: {score*100}\")\n\nprint ('Real Caption:', real_caption)\nprint ('Prediction Caption:', pred_caption)\nplot_attmap(result, weight, test_image)\n\n\nImage.open(test_image)","993be35e":"## Encoder","0b0c5464":"1. Image are of different sizes, will resize them to (299, 299)","ca372466":"# Extracting features from each image in the dataset","65064f23":"### Create the vocabulary & the counter for the captions","648ae32e":"## Model Building\n1.Set the parameters\n\n2.Build the Encoder, Attention model & Decoder","3c9a78a5":"## Load the pretrained Imagenet weights of Inception net V3\n\n3.1 To save the memory(RAM) from getting exhausted, extract the features of the image using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n3.2 Resize them into the shape of (299, 299)\n\n3.3 Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. \n","9e92ac23":"## Model training & optimization\n1.Set the optimizer & loss object\n\n2.Create your checkpoint path\n\n3.Create your training & testing step functions\n\n4.Create your loss function for the test dataset","f1cd9ada":"### 5.Create a list which contains all the captions & path","38dd90b5":"### 1.1 Import the dataset and read image & captions into two seperate variables \n### 1.2 Visualise both the images & text present in the dataset","ad836d4c":"## Create the train & test data \n1.Combine both images & captions to create the train & test dataset using tf.data.Dataset API. Create the train-test spliit using 80-20 ratio & random state = 42\n\n2.Make sure you have done Shuffle and batch while building the dataset\n\n3.The shape of each image in the dataset after building should be (batch_size, 299, 299, 3)\n\n4.The shape of each caption in the dataset after building should be(batch_size, max_len)","66537f78":"#### Creating captions Counter","a9efb0df":"## Beam Search","4abcff2f":"## Model Evaluation\n1.Define your evaluation function using greedy search\n\n2.Define your evaluation function using beam search ( optional)\n\n3.Test it on a sample data using BLEU score","6142ba16":"### 1.4. Visualize the top 30 occuring words in the captions\n","28512573":"                                \n*      Calculating the attention weights. \n    *     W1,W2, V must have matching leading dimensions \n    *     sequence length of W2 = sequence length of v\n    *     Mask will have different shape based on look ahead or padding.\n","c0f058f7":"## Decoder","a7afb809":"### 2.1 Create the tokenizer\n Creating captions Counter, and dataframe of top 5000 frequently used words, for observation.","7360f931":"## 3. Pre-processing the images","843b8fc5":"## 2. Pre-Processing the captions\n2.1 Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n\n2.2 Replace all other words with the unknown token \"UNK\" .\n\n2.3 Create word-to-index and index-to-word mappings.\n\n2.4 Pad all sequences to be the same length as the longest one.","f2e8f2c0":"### Observation\n1. For each image (8091) there are 5 captions (8091*5 = 40455)","0b3d5c02":"## 1. Data understanding\n1.1 Import the dataset and read image & captions into two seperate variables\n\n1.2 Visualise both the images & text present in the dataset\n\n1.3 Create a dataframe which summarizes the image, path & captions as a dataframe\n\n1.4 Visualise the top 30 occuring words in the captions\n\n1.5 Create a list which contains all the captions & path\n","035f947b":"### 1.3 Create a dataframe which summarizes the image, path & captions as a dataframe","19dbdb34":"## Attention Model","038319f7":"## Greedy Search","c226097f":"## Load the pretrained Imagenet weights of Inception net V3\n\n1.To save the memory(RAM) from getting exhausted, extract the features of thei mage using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n2.The shape of the output of this layer is 8x8x2048. \n\n3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)"}}