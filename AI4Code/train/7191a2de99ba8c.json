{"cell_type":{"bed148cb":"code","82e4d6a5":"code","9abd4e6b":"code","1e0900b5":"code","60f3a838":"code","e7d69473":"code","89e5834e":"code","92063ca0":"code","a7303162":"code","2690d9bd":"code","35a7bf31":"code","d54edec2":"code","60753213":"code","e134afe3":"code","12029da4":"code","2f94096d":"code","06d0c20a":"code","81257421":"code","91dec4ff":"code","4cfc14d6":"code","d89e2c91":"code","b6d5d06f":"code","bec7d7a8":"code","8c86e264":"code","e63cceff":"code","e2b0effc":"code","dce562ef":"code","acb8dad3":"code","3f2c10ab":"code","e61dd589":"code","fb6d4e53":"code","469644ef":"code","b47597d5":"code","85b3b1c0":"code","984e5768":"code","9a40b761":"code","59f5e059":"code","d1040ebc":"code","2b7fa7ef":"code","5e796ed4":"code","66ed2ffe":"code","555a7960":"code","4991fe0c":"code","8907609f":"code","15d44425":"code","4a229517":"code","82a5d52c":"code","2c91960d":"code","ea50afcd":"code","19e96163":"code","439dd331":"code","15f75999":"code","3c77385f":"code","4baa6bf3":"code","4fa025ca":"code","f68f883d":"code","cc6346bc":"code","49081f7e":"code","f4e9ea04":"code","a90c6aed":"code","6ad0f5da":"code","61f158f0":"code","6009562e":"code","fd39f32b":"code","b31e4ef5":"code","37e53009":"code","b0d94f54":"code","a7893e75":"code","8ab7c194":"code","470e8615":"code","e1bb1794":"markdown","349ee267":"markdown","291f673a":"markdown","86a45359":"markdown","f45eb9b9":"markdown","c1422056":"markdown","72dfe93a":"markdown","598a4750":"markdown","98c0e178":"markdown","0aeff114":"markdown","c3e12e28":"markdown","13c5643f":"markdown","86536686":"markdown"},"source":{"bed148cb":"import pandas as pd \nimport seaborn as sns\nimport re\nimport gc\nimport os\nimport numpy as np\nimport operator\nfrom wordcloud import WordCloud, STOPWORDS\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\npd_ctx = pd.option_context('display.max_colwidth', 100)\n\nimport nltk\nfrom nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n\nfrom gensim.models import KeyedVectors\n\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D,GRU\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","82e4d6a5":"df_train = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\nquora_data = df_train['question_text'].append(df_test['question_text'])","9abd4e6b":"print(\"\\033[1mTrain set info\\033[0m\")\nprint(df_train.info())","1e0900b5":"# Ki\u1ec3m tra c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u c\u1ee7a c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u00e1nh sincere\nprint(\"\\033[1mSincere Questions: \\033[0m\")\ndisplay(df_train[df_train['target']==0].head())\n# Ki\u1ec3m tra c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u c\u1ee7a c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u00e1nh sincere\nprint(\"\\033[1mInsincere Questions: \\033[0m\")\ndisplay(df_train[df_train['target']==1].head())","60f3a838":"df_train.target.value_counts()","e7d69473":"pos_len = len(df_train[df_train['target'] == 1])\nneg_len = len(df_train[df_train['target'] == 0])\ntotal = len(df_train)\nprint(\"\\033[1mTotal = \\033[0m\", total)\nprint(\"\\033[1mSincere questions:\\033[0m {neg} ({percent: .2f}% )\".format(neg = neg_len, percent = neg_len \/ total * 100))\nprint(\"\\033[1mInsincere questions:\\033[0m {pos} ({percent: .2f}% )\".format(pos = pos_len, percent = pos_len \/ total * 100))\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(['sincere', 'insincere'], df_train.target.value_counts())\nplt.show()","89e5834e":"df_test.info()","92063ca0":"# Shuffle t\u1eadp train \u0111\u1ec3 ki\u1ec3m tra nh\u1eefng gi\u00e1 tr\u1ecb ng\u1eabu nhi\u00ean\ntrain = df_train.sample(frac=1).reset_index(drop=True)\ndisplay(train.sample(n=10, random_state=344))","a7303162":"### do vi\u1ec7c lemming words y\u00eau c\u1ea7u s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n wordnet \u0111\u1ec3 lookup c\u00e1c t\u1eeb c\u00f3 trong \u0111\u00f3\n# nltk.download('wordnet')\n# nltk.download('punkt')\ndef clean_tag(x):\n  if '[math]' in x:\n    x = re.sub('\\[math\\].*?math\\]', 'MATH EQUATION', x) #replacing with [MATH EQUATION]\n    \n  if 'http' in x or 'www' in x:\n    x = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', 'URL', x) #replacing with [url]\n  return x","2690d9bd":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n        '\u2022', '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`', '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', \n        '\u2588', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u25ba', '\u2212', '\u00a2', '\u00ac', '\u2591', '\u00a1', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', \n        '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00b8', '\u22c5', '\u2018', '\u221e', \n        '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u2264', '\u2021', '\u221a', '\u25c4', '\u2501', \n        '\u21d2', '\u25b6', '\u2265', '\u255d', '\u2661', '\u25ca', '\u3002', '\u2708', '\u2261', '\u263a', '\u2714', '\u21b5', '\u2248', '\u2713', '\u2663', '\u260e', '\u2103', '\u25e6', '\u2514', '\u201f', '\uff5e', '\uff01', '\u25cb', \n        '\u25c6', '\u2116', '\u2660', '\u258c', '\u273f', '\u25b8', '\u2044', '\u25a1', '\u2756', '\u2726', '\uff0e', '\u00f7', '\uff5c', '\u2503', '\uff0f', '\uffe5', '\u2560', '\u21a9', '\u272d', '\u2590', '\u263c', '\u263b', '\u2510', \n        '\u251c', '\u00ab', '\u223c', '\u250c', '\u2109', '\u262e', '\u0e3f', '\u2266', '\u266c', '\u2727', '\u232a', '\uff0d', '\u2302', '\u2716', '\uff65', '\u25d5', '\u203b', '\u2016', '\u25c0', '\u2030', '\\x97', '\u21ba', \n        '\u2206', '\u2518', '\u252c', '\u256c', '\u060c', '\u2318', '\u2282', '\uff1e', '\u2329', '\u2399', '\uff1f', '\u2620', '\u21d0', '\u25ab', '\u2217', '\u2208', '\u2260', '\u2640', '\u2654', '\u02da', '\u2117', '\u2517', '\uff0a', \n        '\u253c', '\u2740', '\uff06', '\u2229', '\u2642', '\u203f', '\u2211', '\u2023', '\u279c', '\u251b', '\u21d3', '\u262f', '\u2296', '\u2600', '\u2533', '\uff1b', '\u2207', '\u21d1', '\u2730', '\u25c7', '\u266f', '\u261e', '\u00b4', \n        '\u2194', '\u250f', '\uff61', '\u25d8', '\u2202', '\u270c', '\u266d', '\u2523', '\u2534', '\u2513', '\u2728', '\\xa0', '\u02dc', '\u2765', '\u252b', '\u2120', '\u2712', '\uff3b', '\u222b', '\\x93', '\u2267', '\uff3d', \n        '\\x94', '\u2200', '\u265b', '\\x96', '\u2228', '\u25ce', '\u21bb', '\u21e9', '\uff1c', '\u226b', '\u2729', '\u272a', '\u2655', '\u061f', '\u20a4', '\u261b', '\u256e', '\u240a', '\uff0b', '\u2508', '\uff05', \n        '\u254b', '\u25bd', '\u21e8', '\u253b', '\u2297', '\uffe1', '\u0964', '\u2582', '\u272f', '\u2587', '\uff3f', '\u27a4', '\u271e', '\uff1d', '\u25b7', '\u25b3', '\u25d9', '\u2585', '\u271d', '\u2227', '\u2409', '\u262d', \n        '\u250a', '\u256f', '\u263e', '\u2794', '\u2234', '\\x92', '\u2583', '\u21b3', '\uff3e', '\u05f3', '\u27a2', '\u256d', '\u27a1', '\uff20', '\u2299', '\u2622', '\u02dd', '\u220f', '\u201e', '\u2225', '\u275d', '\u2610', \n        '\u2586', '\u2571', '\u22d9', '\u0e4f', '\u2601', '\u21d4', '\u2594', '\\x91', '\u279a', '\u25e1', '\u2570', '\\x85', '\u2662', '\u02d9', '\u06de', '\u2718', '\u272e', '\u2611', '\u22c6', '\u24d8', '\u2752', \n        '\u2623', '\u2709', '\u230a', '\u27a0', '\u2223', '\u2751', '\u25e2', '\u24d2', '\\x80', '\u3012', '\u2215', '\u25ae', '\u29bf', '\u272b', '\u271a', '\u22ef', '\u2669', '\u2602', '\u275e', '\u2017', '\u0702', '\u261c', \n        '\u203e', '\u271c', '\u2572', '\u2218', '\u27e9', '\uff3c', '\u27e8', '\u0387', '\u2717', '\u265a', '\u2205', '\u24d4', '\u25e3', '\u0361', '\u201b', '\u2766', '\u25e0', '\u2704', '\u2744', '\u2203', '\u2423', '\u226a', '\uff62', \n        '\u2245', '\u25ef', '\u263d', '\u220e', '\uff63', '\u2767', '\u0305', '\u24d0', '\u2198', '\u2693', '\u25a3', '\u02d8', '\u222a', '\u21e2', '\u270d', '\u22a5', '\uff03', '\u23af', '\u21a0', '\u06e9', '\u2630', '\u25e5', \n        '\u2286', '\u273d', '\u26a1', '\u21aa', '\u2741', '\u2639', '\u25fc', '\u2603', '\u25e4', '\u274f', '\u24e2', '\u22b1', '\u279d', '\u0323', '\u2721', '\u2220', '\uff40', '\u25b4', '\u2524', '\u221d', '\u264f', '\u24d0', \n        '\u270e', '\u037e', '\u2424', '\uff07', '\u2763', '\u2702', '\u2724', '\u24de', '\u262a', '\u2734', '\u2312', '\u02db', '\u2652', '\uff04', '\u2736', '\u25bb', '\u24d4', '\u25cc', '\u25c8', '\u275a', '\u2742', '\uffe6', \n        '\u25c9', '\u255c', '\u0303', '\u2731', '\u2556', '\u2749', '\u24e1', '\u2197', '\u24e3', '\u267b', '\u27bd', '\u05c0', '\u2732', '\u272c', '\u2609', '\u2589', '\u2252', '\u2625', '\u2310', '\u2668', '\u2715', '\u24dd', \n        '\u22b0', '\u2758', '\uff02', '\u21e7', '\u0335', '\u27aa', '\u2581', '\u258f', '\u2283', '\u24db', '\u201a', '\u2670', '\u0301', '\u270f', '\u23d1', '\u0336', '\u24e2', '\u2a7e', '\uffe0', '\u274d', '\u2243', '\u22f0', '\u264b', \n        '\uff64', '\u0302', '\u274b', '\u2733', '\u24e4', '\u2564', '\u2595', '\u2323', '\u2738', '\u212e', '\u207a', '\u25a8', '\u2568', '\u24e5', '\u2648', '\u2743', '\u261d', '\u273b', '\u2287', '\u227b', '\u2658', '\u265e', \n        '\u25c2', '\u271f', '\u2320', '\u2720', '\u261a', '\u2725', '\u274a', '\u24d2', '\u2308', '\u2745', '\u24e1', '\u2667', '\u24de', '\u25ad', '\u2771', '\u24e3', '\u221f', '\u2615', '\u267a', '\u2235', '\u235d', '\u24d1', \n        '\u2735', '\u2723', '\u066d', '\u2646', '\u24d8', '\u2236', '\u269c', '\u25de', '\u0bcd', '\u2739', '\u27a5', '\u2195', '\u0333', '\u2237', '\u270b', '\u27a7', '\u220b', '\u033f', '\u0367', '\u2505', '\u2964', '\u2b06', '\u22f1', \n        '\u2604', '\u2196', '\u22ee', '\u06d4', '\u264c', '\u24db', '\u2555', '\u2653', '\u276f', '\u264d', '\u258b', '\u273a', '\u2b50', '\u273e', '\u264a', '\u27a3', '\u25bf', '\u24d1', '\u2649', '\u23e0', '\u25fe', '\u25b9', \n        '\u2a7d', '\u21a6', '\u2565', '\u2375', '\u230b', '\u0589', '\u27a8', '\u222e', '\u21e5', '\u24d7', '\u24d3', '\u207b', '\u239d', '\u2325', '\u2309', '\u25d4', '\u25d1', '\u273c', '\u264e', '\u2650', '\u256a', '\u229a', \n        '\u2612', '\u21e4', '\u24dc', '\u23a0', '\u25d0', '\u26a0', '\u255e', '\u25d7', '\u2395', '\u24e8', '\u261f', '\u24df', '\u265f', '\u2748', '\u21ac', '\u24d3', '\u25fb', '\u266e', '\u2759', '\u2664', '\u2209', '\u061b', \n        '\u2042', '\u24dd', '\u05be', '\u2651', '\u256b', '\u2553', '\u2573', '\u2b05', '\u2614', '\u2638', '\u2504', '\u2567', '\u05c3', '\u23a2', '\u2746', '\u22c4', '\u26ab', '\u030f', '\u260f', '\u279e', '\u0342', '\u2419', \n        '\u24e4', '\u25df', '\u030a', '\u2690', '\u2719', '\u2199', '\u033e', '\u2118', '\u2737', '\u237a', '\u274c', '\u22a2', '\u25b5', '\u2705', '\u24d6', '\u2628', '\u25b0', '\u2561', '\u24dc', '\u2624', '\u223d', '\u2558', \n        '\u02f9', '\u21a8', '\u2659', '\u2b07', '\u2671', '\u2321', '\u2800', '\u255b', '\u2755', '\u2509', '\u24df', '\u0300', '\u2656', '\u24da', '\u2506', '\u239c', '\u25dc', '\u26be', '\u2934', '\u2707', '\u255f', '\u239b', \n        '\u2629', '\u27b2', '\u279f', '\u24e5', '\u24d7', '\u23dd', '\u25c3', '\u2562', '\u21af', '\u2706', '\u02c3', '\u2374', '\u2747', '\u26bd', '\u2552', '\u0338', '\u265c', '\u2613', '\u27b3', '\u21c4', '\u262c', '\u2691', \n        '\u2710', '\u2303', '\u25c5', '\u25a2', '\u2750', '\u220a', '\u2608', '\u0965', '\u23ae', '\u25a9', '\u0bc1', '\u22b9', '\u2035', '\u2414', '\u260a', '\u27b8', '\u030c', '\u263f', '\u21c9', '\u22b3', '\u2559', '\u24e6', \n        '\u21e3', '\uff5b', '\u0304', '\u219d', '\u239f', '\u258d', '\u2757', '\u05f4', '\u0384', '\u259e', '\u25c1', '\u26c4', '\u21dd', '\u23aa', '\u2641', '\u21e0', '\u2607', '\u270a', '\u0bbf', '\uff5d', '\u2b55', '\u2798', \n        '\u2040', '\u2619', '\u275b', '\u2753', '\u27f2', '\u21c0', '\u2272', '\u24d5', '\u23a5', '\\u06dd', '\u0364', '\u208b', '\u0331', '\u030e', '\u265d', '\u2273', '\u2599', '\u27ad', '\u0700', '\u24d6', '\u21db', '\u258a', \n        '\u21d7', '\u0337', '\u21f1', '\u2105', '\u24e7', '\u269b', '\u0310', '\u0315', '\u21cc', '\u2400', '\u224c', '\u24e6', '\u22a4', '\u0313', '\u2626', '\u24d5', '\u259c', '\u2799', '\u24e8', '\u2328', '\u25ee', '\u2637', \n        '\u25cd', '\u24da', '\u2254', '\u23e9', '\u2373', '\u211e', '\u250b', '\u02fb', '\u259a', '\u227a', '\u0652', '\u259f', '\u27bb', '\u032a', '\u23ea', '\u0309', '\u239e', '\u2507', '\u235f', '\u21ea', '\u258e', '\u21e6', '\u241d', \n        '\u2937', '\u2256', '\u27f6', '\u2657', '\u0334', '\u2644', '\u0368', '\u0308', '\u275c', '\u0321', '\u259b', '\u2701', '\u27a9', '\u0bbe', '\u02c2', '\u21a5', '\u23ce', '\u23b7', '\u0332', '\u2796', '\u21b2', '\u2a75', '\u0317', '\u2762', \n        '\u224e', '\u2694', '\u21c7', '\u0311', '\u22bf', '\u0316', '\u260d', '\u27b9', '\u294a', '\u2041', '\u2722']\n\ndef clean_punct(x):\n  x = str(x)\n  for punct in puncts:\n    if punct in x:\n      x = x.replace(punct, ' ')\n    return x","35a7bf31":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized',\n                'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime',\n                'cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone',\n                'dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language',\n                'splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu',\n                'weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}\n\ndef correct_mispell(x):\n  words = x.split()\n  for i in range(0, len(words)):\n    if mispell_dict.get(words[i]) is not None:\n      words[i] = mispell_dict.get(words[i])\n    elif mispell_dict.get(words[i].lower()) is not None:\n      words[i] = mispell_dict.get(words[i].lower())\n        \n  words = \" \".join(words)\n  return words","d54edec2":"def remove_stopwords(x):\n  x = [word for word in x.split() if word not in STOPWORDS]\n  x = ' '.join(x)\n  return x","60753213":"contraction_mapping = {\n \"I'm\": 'I am',\n \"I'm'a\": 'I am about to',\n \"I'm'o\": 'I am going to',\n \"I've\": 'I have',\n \"I'll\": 'I will',\n \"I'll've\": 'I will have',\n \"I'd\": 'I would',\n \"I'd've\": 'I would have',\n \"i'm\": 'i am',\n \"i'm'a\": 'i am about to',\n \"i'm'o\": 'i am going to',\n \"i've\": 'i have',\n \"i'll\": 'i will',\n \"i'll've\": 'i will have',\n \"i'd\": 'i would',\n \"i'd've\": 'i would have',\n 'Whatcha': 'What are you',\n 'whatcha': 'what are you',\n \"amn't\": 'am not',\n \"ain't\": 'are not',\n \"aren't\": 'are not',\n \"'cause\": 'because',\n \"can't\": 'can not',\n \"can't've\": 'can not have',\n \"could've\": 'could have',\n \"couldn't\": 'could not',\n \"couldn't've\": 'could not have',\n \"daren't\": 'dare not',\n \"daresn't\": 'dare not',\n \"dasn't\": 'dare not',\n \"didn't\": 'did not',\n 'didn\u2019t': 'did not',\n \"don't\": 'do not',\n 'don\u2019t': 'do not',\n \"doesn't\": 'does not',\n \"e'er\": 'ever',\n \"everyone's\": 'everyone is',\n 'finna': 'fixing to',\n 'gimme': 'give me',\n \"gon't\": 'go not',\n 'gonna': 'going to',\n 'gotta': 'got to',\n \"hadn't\": 'had not',\n \"hadn't've\": 'had not have',\n \"hasn't\": 'has not',\n \"haven't\": 'have not',\n \"he've\": 'he have',\n \"he's\": 'he is',\n \"he'll\": 'he will',\n \"he'll've\": 'he will have',\n \"he'd\": 'he would',\n \"he'd've\": 'he would have',\n \"here's\": 'here is',\n \"how're\": 'how are',\n \"how'd\": 'how did',\n \"how'd'y\": 'how do you',\n \"how's\": 'how is',\n \"how'll\": 'how will',\n \"isn't\": 'is not',\n \"it's\": 'it is',\n \"'tis\": 'it is',\n \"'twas\": 'it was',\n \"it'll\": 'it will',\n \"it'll've\": 'it will have',\n \"it'd\": 'it would',\n \"it'd've\": 'it would have',\n 'kinda': 'kind of',\n \"let's\": 'let us',\n 'luv': 'love',\n \"ma'am\": 'madam',\n \"may've\": 'may have',\n \"mayn't\": 'may not',\n \"might've\": 'might have',\n \"mightn't\": 'might not',\n \"mightn't've\": 'might not have',\n \"must've\": 'must have',\n \"mustn't\": 'must not',\n \"mustn't've\": 'must not have',\n \"needn't\": 'need not',\n \"needn't've\": 'need not have',\n \"ne'er\": 'never',\n \"o'\": 'of',\n \"o'clock\": 'of the clock',\n \"ol'\": 'old',\n \"oughtn't\": 'ought not',\n \"oughtn't've\": 'ought not have',\n \"o'er\": 'over',\n \"shan't\": 'shall not',\n \"sha'n't\": 'shall not',\n \"shalln't\": 'shall not',\n \"shan't've\": 'shall not have',\n \"she's\": 'she is',\n \"she'll\": 'she will',\n \"she'd\": 'she would',\n \"she'd've\": 'she would have',\n \"should've\": 'should have',\n \"shouldn't\": 'should not',\n \"shouldn't've\": 'should not have',\n \"so've\": 'so have',\n \"so's\": 'so is',\n \"somebody's\": 'somebody is',\n \"someone's\": 'someone is',\n \"something's\": 'something is',\n 'sux': 'sucks',\n \"that're\": 'that are',\n \"that's\": 'that is',\n \"that'll\": 'that will',\n \"that'd\": 'that would',\n \"that'd've\": 'that would have',\n 'em': 'them',\n \"there're\": 'there are',\n \"there's\": 'there is',\n \"there'll\": 'there will',\n \"there'd\": 'there would',\n \"there'd've\": 'there would have',\n \"these're\": 'these are',\n \"they're\": 'they are',\n \"they've\": 'they have',\n \"they'll\": 'they will',\n \"they'll've\": 'they will have',\n \"they'd\": 'they would',\n \"they'd've\": 'they would have',\n \"this's\": 'this is',\n \"those're\": 'those are',\n \"to've\": 'to have',\n 'wanna': 'want to',\n \"wasn't\": 'was not',\n \"we're\": 'we are',\n \"we've\": 'we have',\n \"we'll\": 'we will',\n \"we'll've\": 'we will have',\n \"we'd\": 'we would',\n \"we'd've\": 'we would have',\n \"weren't\": 'were not',\n \"what're\": 'what are',\n \"what'd\": 'what did',\n \"what've\": 'what have',\n \"what's\": 'what is',\n \"what'll\": 'what will',\n \"what'll've\": 'what will have',\n \"when've\": 'when have',\n \"when's\": 'when is',\n \"where're\": 'where are',\n \"where'd\": 'where did',\n \"where've\": 'where have',\n \"where's\": 'where is',\n \"which's\": 'which is',\n \"who're\": 'who are',\n \"who've\": 'who have',\n \"who's\": 'who is',\n \"who'll\": 'who will',\n \"who'll've\": 'who will have',\n \"who'd\": 'who would',\n \"who'd've\": 'who would have',\n \"why're\": 'why are',\n \"why'd\": 'why did',\n \"why've\": 'why have',\n \"why's\": 'why is',\n \"will've\": 'will have',\n \"won't\": 'will not',\n \"won't've\": 'will not have',\n \"would've\": 'would have',\n \"wouldn't\": 'would not',\n \"wouldn't've\": 'would not have',\n \"y'all\": 'you all',\n \"y'all're\": 'you all are',\n \"y'all've\": 'you all have',\n \"y'all'd\": 'you all would',\n \"y'all'd've\": 'you all would have',\n \"you're\": 'you are',\n \"you've\": 'you have',\n \"you'll've\": 'you shall have',\n \"you'll\": 'you will',\n \"you'd\": 'you would',\n \"you'd've\": 'you would have',\n 'jan.': 'january',\n 'feb.': 'february',\n 'mar.': 'march',\n 'apr.': 'april',\n 'jun.': 'june',\n 'jul.': 'july',\n 'aug.': 'august',\n 'sep.': 'september',\n 'oct.': 'october',\n 'nov.': 'november',\n 'dec.': 'december',\n 'I\u2019m': 'I am',\n 'I\u2019m\u2019a': 'I am about to',\n 'I\u2019m\u2019o': 'I am going to',\n 'I\u2019ve': 'I have',\n 'I\u2019ll': 'I will',\n 'I\u2019ll\u2019ve': 'I will have',\n 'I\u2019d': 'I would',\n 'I\u2019d\u2019ve': 'I would have',\n 'i\u2019m': 'i am',\n 'i\u2019m\u2019a': 'i am about to',\n 'i\u2019m\u2019o': 'i am going to',\n 'i\u2019ve': 'i have',\n 'i\u2019ll': 'i will',\n 'i\u2019ll\u2019ve': 'i will have',\n 'i\u2019d': 'i would',\n 'i\u2019d\u2019ve': 'i would have',\n 'amn\u2019t': 'am not',\n 'ain\u2019t': 'are not',\n 'aren\u2019t': 'are not',\n '\u2019cause': 'because',\n 'can\u2019t': 'can not',\n 'can\u2019t\u2019ve': 'can not have',\n 'could\u2019ve': 'could have',\n 'couldn\u2019t': 'could not',\n 'couldn\u2019t\u2019ve': 'could not have',\n 'daren\u2019t': 'dare not',\n 'daresn\u2019t': 'dare not',\n 'dasn\u2019t': 'dare not',\n 'doesn\u2019t': 'does not',\n 'e\u2019er': 'ever',\n 'everyone\u2019s': 'everyone is',\n 'gon\u2019t': 'go not',\n 'hadn\u2019t': 'had not',\n 'hadn\u2019t\u2019ve': 'had not have',\n 'hasn\u2019t': 'has not',\n 'haven\u2019t': 'have not',\n 'he\u2019ve': 'he have',\n 'he\u2019s': 'he is',\n 'he\u2019ll': 'he will',\n 'he\u2019ll\u2019ve': 'he will have',\n 'he\u2019d': 'he would',\n 'he\u2019d\u2019ve': 'he would have',\n 'here\u2019s': 'here is',\n 'how\u2019re': 'how are',\n 'how\u2019d': 'how did',\n 'how\u2019d\u2019y': 'how do you',\n 'how\u2019s': 'how is',\n 'how\u2019ll': 'how will',\n 'isn\u2019t': 'is not',\n 'it\u2019s': 'it is',\n '\u2019tis': 'it is',\n '\u2019twas': 'it was',\n 'it\u2019ll': 'it will',\n 'it\u2019ll\u2019ve': 'it will have',\n 'it\u2019d': 'it would',\n 'it\u2019d\u2019ve': 'it would have',\n 'let\u2019s': 'let us',\n 'ma\u2019am': 'madam',\n 'may\u2019ve': 'may have',\n 'mayn\u2019t': 'may not',\n 'might\u2019ve': 'might have',\n 'mightn\u2019t': 'might not',\n 'mightn\u2019t\u2019ve': 'might not have',\n 'must\u2019ve': 'must have',\n 'mustn\u2019t': 'must not',\n 'mustn\u2019t\u2019ve': 'must not have',\n 'needn\u2019t': 'need not',\n 'needn\u2019t\u2019ve': 'need not have',\n 'ne\u2019er': 'never',\n 'o\u2019': 'of',\n 'o\u2019clock': 'of the clock',\n 'ol\u2019': 'old',\n 'oughtn\u2019t': 'ought not',\n 'oughtn\u2019t\u2019ve': 'ought not have',\n 'o\u2019er': 'over',\n 'shan\u2019t': 'shall not',\n 'sha\u2019n\u2019t': 'shall not',\n 'shalln\u2019t': 'shall not',\n 'shan\u2019t\u2019ve': 'shall not have',\n 'she\u2019s': 'she is',\n 'she\u2019ll': 'she will',\n 'she\u2019d': 'she would',\n 'she\u2019d\u2019ve': 'she would have',\n 'should\u2019ve': 'should have',\n 'shouldn\u2019t': 'should not',\n 'shouldn\u2019t\u2019ve': 'should not have',\n 'so\u2019ve': 'so have',\n 'so\u2019s': 'so is',\n 'somebody\u2019s': 'somebody is',\n 'someone\u2019s': 'someone is',\n 'something\u2019s': 'something is',\n 'that\u2019re': 'that are',\n 'that\u2019s': 'that is',\n 'that\u2019ll': 'that will',\n 'that\u2019d': 'that would',\n 'that\u2019d\u2019ve': 'that would have',\n 'there\u2019re': 'there are',\n 'there\u2019s': 'there is',\n 'there\u2019ll': 'there will',\n 'there\u2019d': 'there would',\n 'there\u2019d\u2019ve': 'there would have',\n 'these\u2019re': 'these are',\n 'they\u2019re': 'they are',\n 'they\u2019ve': 'they have',\n 'they\u2019ll': 'they will',\n 'they\u2019ll\u2019ve': 'they will have',\n 'they\u2019d': 'they would',\n 'they\u2019d\u2019ve': 'they would have',\n 'this\u2019s': 'this is',\n 'those\u2019re': 'those are',\n 'to\u2019ve': 'to have',\n 'wasn\u2019t': 'was not',\n 'we\u2019re': 'we are',\n 'we\u2019ve': 'we have',\n 'we\u2019ll': 'we will',\n 'we\u2019ll\u2019ve': 'we will have',\n 'we\u2019d': 'we would',\n 'we\u2019d\u2019ve': 'we would have',\n 'weren\u2019t': 'were not',\n 'what\u2019re': 'what are',\n 'what\u2019d': 'what did',\n 'what\u2019ve': 'what have',\n 'what\u2019s': 'what is',\n 'what\u2019ll': 'what will',\n 'what\u2019ll\u2019ve': 'what will have',\n 'when\u2019ve': 'when have',\n 'when\u2019s': 'when is',\n 'where\u2019re': 'where are',\n 'where\u2019d': 'where did',\n 'where\u2019ve': 'where have',\n 'where\u2019s': 'where is',\n 'which\u2019s': 'which is',\n 'who\u2019re': 'who are',\n 'who\u2019ve': 'who have',\n 'who\u2019s': 'who is',\n 'who\u2019ll': 'who will',\n 'who\u2019ll\u2019ve': 'who will have',\n 'who\u2019d': 'who would',\n 'who\u2019d\u2019ve': 'who would have',\n 'why\u2019re': 'why are',\n 'why\u2019d': 'why did',\n 'why\u2019ve': 'why have',\n 'why\u2019s': 'why is',\n 'will\u2019ve': 'will have',\n 'won\u2019t': 'will not',\n 'won\u2019t\u2019ve': 'will not have',\n 'would\u2019ve': 'would have',\n 'wouldn\u2019t': 'would not',\n 'wouldn\u2019t\u2019ve': 'would not have',\n 'y\u2019all': 'you all',\n 'y\u2019all\u2019re': 'you all are',\n 'y\u2019all\u2019ve': 'you all have',\n 'y\u2019all\u2019d': 'you all would',\n 'y\u2019all\u2019d\u2019ve': 'you all would have',\n 'you\u2019re': 'you are',\n 'you\u2019ve': 'you have',\n 'you\u2019ll\u2019ve': 'you shall have',\n 'you\u2019ll': 'you will',\n 'you\u2019d': 'you would',\n 'you\u2019d\u2019ve': 'you would have'\n}\n\ndef clean_contractions(text):\n#     text = text.lower()\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n    return text","e134afe3":"# lemmatizer = WordNetLemmatizer()\n# def lemma_text(x):\n#   x = x.split()\n#   x = [lemmatizer.lemmatize(word) for word in x]\n#   x = ' '.join(x)\n#   return x","12029da4":"def data_cleaning(x):\n  x = clean_tag(x)\n  x = clean_punct(x)\n  x = correct_mispell(x)\n  x = remove_stopwords(x)\n  x = clean_contractions(x)\n#   x = lemma_text(x)\n  return x","2f94096d":"def Preprocess(doc):\n    corpus=[]\n    for text in tqdm(doc):\n        text=clean_contractions(text)\n        text=correct_mispell(text)\n        text=re.sub(r'[^a-z0-9A-Z]',\" \",text)\n        text=re.sub(r'[0-9]{1}',\"#\",text)\n        text=re.sub(r'[0-9]{2}','##',text)\n        text=re.sub(r'[0-9]{3}','###',text)\n        text=re.sub(r'[0-9]{4}','####',text)\n        text=re.sub(r'[0-9]{5,}','#####',text)\n        corpus.append(text)\n    return corpus","06d0c20a":"tqdm.pandas(desc=\"progress-bar\")\ndf_test['question_text_cleaned'] = df_test['question_text'].progress_map(lambda x: data_cleaning(x))\ndf_train['question_text_cleaned'] = df_train['question_text'].progress_map(lambda x: data_cleaning(x))","81257421":"df_train.head(5)","91dec4ff":"%%time\n### unzipping all the pretrained embeddings\n!unzip ..\/input\/quora-insincere-questions-classification\/embeddings.zip\n!du -h .\/","4cfc14d6":"%%time\n\n### Loading t\u1eadp Google News Pretrained Embeddings v\u00e0o b\u1ed9 nh\u1edb\nfile_name=\".\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin\"\nmodel_embed=KeyedVectors.load_word2vec_format(file_name,binary=True)\n# model_embed = load_embed1('.\/glove.840B.300d\/glove.840B.300d.txt')\n\n### X\u00e2y d\u1ef1ng t\u1eadp t\u1eeb v\u1ef1ng d\u1ef1a tr\u00ean d\u1eef li\u1ec7u c\u1ee7a t\u1eadp Google News\ndef vocab_build(corpus):\n    vocab={}\n    for text in tqdm(corpus):\n        for word in text.split():\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n    return vocab\n\n\n### Ki\u1ec3m tra t\u1eadp Vocabulary xem t\u1eadp vocab \u0111\u00f3 bao ph\u1ee7 bao nhi\u00eau ph\u1ea7n tr\u0103m t\u1eadp d\u1eef li\u1ec7u c\u1ee7a m\u00ecnh\ndef check_voc(vocab,model):\n    embed_words=[]\n    out_vocab={}\n    total_words=0\n    total_text=0\n    for i in tqdm(vocab):\n        try:\n            vec=model[i]\n            embed_words.append(vec)\n            total_words+=vocab[i]\n        except KeyError:\n            out_vocab[i]=vocab[i]\n            total_text+=vocab[i]\n    print(\"The {:.2f}% of vocabularies have Covered of corpus\".format(100*len(embed_words)\/len(vocab)))\n    print(\"The {:.2f}% of total text had coverded \".format((100*total_words\/(total_words+total_text))))\n    return out_vocab","d89e2c91":"### x\u00e2y t\u1eadp vocab v\u00e0 ki\u1ec3m tra \u0111\u1ed9 ph\u1ee7 c\u1ee7a t\u1eadp vocab v\u1edbi d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd\ntotal_text=pd.concat([df_train.question_text_cleaned,df_test.question_text_cleaned])\nvocabulary=vocab_build(total_text)\noov=check_voc(vocabulary,model_embed) #oov: out of vocab","b6d5d06f":"df_test['question_text_cleaned_2'] = Preprocess(df_test['question_text'])\ndf_train['question_text_cleaned_2'] = Preprocess(df_train['question_text'])\ntotal_text_2=pd.concat([df_train.question_text_cleaned_2,df_test.question_text_cleaned_2])\nvocabulary2=vocab_build(total_text_2)\noov2=check_voc(vocabulary2, model_embed)","bec7d7a8":"sort_oov=dict(sorted(oov2.items(), key=operator.itemgetter(1),reverse=True))\ndict(list(sort_oov.items())[:50])","8c86e264":"del oov, oov2,sort_oov,total_text,total_text_2\ngc.collect()","e63cceff":"def get_word_index(vocab):\n    word_index=dict((w,i+1) for i,w in enumerate(vocab.keys()))\n    return word_index\ndef fit_one_hot(word_index,corpus):\n    sent=[]\n    for text in tqdm(corpus):\n        li=[]\n        for word in text.split():\n            try:\n                li.append(word_index[word])\n            except KeyError:\n                li.append(0)\n        sent.append(li)\n    return sent","e2b0effc":"train,val=train_test_split(df_train,test_size=0.2,stratify=df_train.target,random_state=123)\nvocab_size=len(vocabulary2)+1\nmax_len=40\n\nword_index=get_word_index(vocabulary2)\n### Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd\ntrain_text=train['question_text_cleaned_2']\nval_text=val['question_text_cleaned_2']\ntest_text=df_test['question_text_cleaned_2']\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp train sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes=fit_one_hot(word_index,train_text)\ntrain_padded=pad_sequences(encodes,maxlen=max_len,padding=\"post\")\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp validation sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes_=fit_one_hot(word_index,val_text)\nval_padded=pad_sequences(encodes_,maxlen=max_len,padding=\"post\")\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp test sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes__=fit_one_hot(word_index,test_text)\ntest_padded=pad_sequences(encodes__,maxlen=max_len,padding=\"post\")","dce562ef":"count=0\nembedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=model_embed[word]\n        embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","acb8dad3":"def get_model_origin(matrix):\n    inp = Input(shape=(max_len,))\n    x = Embedding(vocab_size,300,weights=[matrix],input_length=max_len,trainable=False)(inp)\n    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n    x = Conv1D(64,3,activation=\"relu\")(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(128, activation=\"relu\")(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    return model","3f2c10ab":"opt=Adam(learning_rate=0.001)\nBATCH_SIZE = 1024\nbin_loss=tf.keras.losses.BinaryCrossentropy(from_logits=False,label_smoothing=0,name='binary_crossentropy')\n\n### X\u00e1c \u0111\u1ecbnh \u0111i\u1ec3m callback \u0111\u1ec3 gi\u1ea3m learning rate, v\u00e0 restore l\u1ea1i tr\u1ecdng s\u1ed1 t\u1ed1t nh\u1ea5t k\u1ec1 tr\u01b0\u1edbc \nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=3,mode=\"min\",restore_best_weights=True\n                                              )\n### Gi\u1ea3m learning rate khi model kh\u00f4ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u00ean (c\u00e0ng h\u1ecdc c\u00e0ng ngu)\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",factor=0.2,patience=2,verbose=1,mode=\"auto\")\n\nmy_callbacks=[early_stopping,reduce_lr]","e61dd589":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","fb6d4e53":"with strategy.scope():\n    google_model = get_model_origin(embedding_mat)\n    google_model.compile(loss=bin_loss, optimizer=opt, metrics=['accuracy'])\nhistory=google_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","469644ef":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b47597d5":"google_y_pre=google_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(google_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","85b3b1c0":"del model_embed, history, best_score, best_thresh, google_model, google_y_pre\ngc.collect()","984e5768":"def get_model(matrix):\n    inp = Input(shape=(max_len,))\n    x = Embedding(vocab_size, 300, weights=[matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(LSTM(256, return_sequences=True))(x)\n    x2 = Bidirectional(GRU(128, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    predictions = Dense(1, activation='sigmoid')(conc)\n    model = Model(inputs=inp, outputs=predictions)\n    adam = optimizers.Adam(lr=0.001)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    return model","9a40b761":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","59f5e059":"with strategy.scope():\n    google_model = get_model(embedding_mat)\n    google_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nhistory=google_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","d1040ebc":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","2b7fa7ef":"google_y_pre=google_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(google_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","5e796ed4":"threshold=best_thresh\ngoogle_y_test_pre=google_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\ngoogle_y_test_pre=(google_y_test_pre>thresh).astype(int)","66ed2ffe":"del embedding_mat, history, best_score, best_thresh\ngc.collect()","555a7960":"def load_embed(file):\n    def get_coefs(word,*arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n\n    return embeddings_index","4991fe0c":"!tree -h .\/","8907609f":"glove_path = '.\/glove.840B.300d\/glove.840B.300d.txt'\nparagram_path = '.\/paragram_300_sl999\/paragram_300_sl999.txt'\nwiki_path = '.\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'","15d44425":"%%time\nglove_embed = load_embed(glove_path)\nprint(\"\\033[1mGlove Coverage: \\033[0m]\")\noov_glove = check_voc(vocabulary2, glove_embed)","4a229517":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","82a5d52c":"count=0\nglove_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=glove_embed[word]\n        glove_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","2c91960d":"with strategy.scope():\n    glove_model = get_model(glove_embedding_mat)\n    glove_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nglove_history=glove_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","ea50afcd":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(glove_history.history['loss'])\nplt.plot(glove_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(glove_history.history['accuracy'])\nplt.plot(glove_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","19e96163":"glove_y_pre=glove_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(glove_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(glove_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(glove_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","439dd331":"threshold=best_thresh\nglove_y_test_pre=glove_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\nglove_y_test_pre=(glove_y_test_pre>thresh).astype(int)","15f75999":"del glove_embed, glove_embedding_mat, glove_history, best_score, best_thresh\ngc.collect()","3c77385f":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","4baa6bf3":"%%time\nparagram_embed = load_embed(paragram_path)\nprint(\"\\033[1mParagram Coverage: \\033[0m]\")\noov_paragram = check_voc(vocabulary2, paragram_embed)","4fa025ca":"count=0\npara_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=paragram_embed[word.lower()]\n        para_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","f68f883d":"with strategy.scope():\n    para_model = get_model(para_embedding_mat)\n    para_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\npara_history=para_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","cc6346bc":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(para_history.history['loss'])\nplt.plot(para_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(para_history.history['accuracy'])\nplt.plot(para_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","49081f7e":"para_y_pre=para_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(para_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(para_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(para_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","f4e9ea04":"threshold=best_thresh\npara_y_test_pre=para_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\npara_y_test_pre=(para_y_test_pre>thresh).astype(int)","a90c6aed":"del paragram_embed, para_embedding_mat, para_history, best_score, best_thresh\ngc.collect()","6ad0f5da":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","61f158f0":"%%time\nwiki_embed = load_embed(wiki_path)\nprint(\"\\033[1mWiki Coverage: \\033[0m]\")\noov_wiki = check_voc(vocabulary2, wiki_embed)","6009562e":"count=0\nwiki_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=wiki_embed[word]\n        wiki_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","fd39f32b":"with strategy.scope():\n    wiki_model = get_model(wiki_embedding_mat)\n    wiki_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nwiki_history=wiki_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","b31e4ef5":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(wiki_history.history['loss'])\nplt.plot(wiki_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(wiki_history.history['accuracy'])\nplt.plot(wiki_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","37e53009":"wiki_y_pre=wiki_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(wiki_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(wiki_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(wiki_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","b0d94f54":"threshold=best_thresh\nwiki_y_test_pre=wiki_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\nwiki_y_test_pre=(wiki_y_test_pre>thresh).astype(int)","a7893e75":"del wiki_embed, wiki_embedding_mat, wiki_history, best_score, best_thresh\ngc.collect()","8ab7c194":"y_pre=0.25*(google_y_pre + glove_y_pre + para_y_pre + wiki_y_pre)\n# y_pre=0.20*google_y_pre + 0.35*glove_y_pre + 0.15*para_y_pre + 0.30*wiki_y_pre\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","470e8615":"# y_test_pre = 0.25 * (google_y_test_pre + glove_y_test_pre + para_y_test_pre + wiki_y_test_pre)\ny_test_pre = 0.2*google_y_test_pre + 0.3*glove_y_test_pre + 0.2*para_y_test_pre + 0.3*wiki_y_test_pre\ny_test_pre=(y_test_pre>thresh).astype(int)\n### T\u1ea1o File submission\nsubmit=pd.DataFrame()\nsubmit[\"qid\"]=df_test.qid\nsubmit[\"prediction\"]=y_test_pre\nsubmit.to_csv(\"submission.csv\",index=False)","e1bb1794":"X\u00e1c \u0111\u1ecbnh c\u00e1c t\u1eadp **train**, **test** v\u00e0 kh\u1edfi t\u1ea1o m\u1ed9t t\u1eadp ch\u1ee9a d\u1eef li\u1ec7u c\u1ee7a c\u1ea3 2 \u0111\u1ec3 v\u1ec1 sau check \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a vocab v\u1edbi t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 sinh","349ee267":"b. Ti\u1ec1n x\u1eed l\u00fd c\u00e1c t\u1eadp d\u1eef li\u1ec7u","291f673a":"# 2. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u","86a45359":"a. Th\u1eed nghi\u1ec7m model 1","f45eb9b9":"# 5. Hu\u1ea5n luy\u1ec7n v\u00e0 D\u1ef1 \u0111o\u00e1n","c1422056":"# 3. Build t\u1eadp vocab","72dfe93a":"b. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u trong t\u1eadp test**","598a4750":"# 4. Chu\u1ea9n b\u1ecb Model","98c0e178":"# 6. So s\u00e1nh, m\u1edf r\u1ed9ng","0aeff114":"\u1ede trong b\u1ed9 d\u1eef li\u1ec7u c\u00f3 3 tr\u01b0\u1eddng, v\u00e0 kh\u1ea3 n\u0103ng d\u1eef li\u1ec7u train n\u1eb1m \u1edf tr\u01b0\u1eddng **question_text**  \nV\u1eady ta s\u1ebd th\u1eed kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u trong tr\u01b0\u1eddng **question_text**","c3e12e28":"# 1. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u","13c5643f":"a. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u trong t\u1eadp train**","86536686":"b. Th\u1eed nghi\u1ec7m model 2"}}