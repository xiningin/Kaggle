{"cell_type":{"e5838d5c":"code","7389fe9a":"code","4f47878a":"code","ae822733":"code","fc1abe2a":"code","f32accda":"code","639b150a":"code","d2636f81":"code","cb045111":"code","df920909":"code","a4fbb391":"code","153f9988":"code","bf81c7ff":"code","e28e34da":"code","9303037a":"code","45b76550":"code","f4926913":"code","20aaf5cb":"code","0f90112b":"code","f1405ec4":"code","9669670c":"code","3974fae1":"code","c7a73c9c":"code","947f191f":"code","d4cf01a4":"code","7126c0f1":"code","8fd6ae61":"code","0cede3ce":"code","3cb1bec0":"code","ce4f1759":"code","17079510":"code","efa557ed":"code","a67b25e1":"code","6d26d1fc":"code","4ad4f2b1":"code","0d8880ae":"code","b9193ad6":"code","8b6d7eec":"code","ceb30401":"code","6acf07a1":"code","89489afa":"code","0904dcf7":"code","d1a332f8":"code","9a895701":"code","e28b1860":"code","4c7b3d73":"code","74d1bb3b":"code","c4ab308b":"code","8950f069":"code","cf7c0665":"code","a319e937":"code","606e0a23":"code","a29606ab":"code","34fc085d":"code","7e576e1c":"code","8e12a881":"code","29927c98":"code","728a50f6":"code","7587f685":"code","95a47ac1":"code","db2fdbfd":"code","8343bf5d":"code","d9ec3f28":"code","a630a7c6":"code","24494591":"code","f9c3a5f6":"code","30100515":"code","d3644813":"code","74b99667":"code","6a9fd38d":"code","47d879ab":"code","932bb027":"code","4a3d471c":"code","054d9c84":"code","49465c06":"code","30807c4f":"code","e8f1efec":"code","312c8da4":"code","7ae92470":"code","91646f65":"code","343a8c03":"code","1cd4ce05":"code","85ebcb05":"code","94bc0e07":"code","2a104ce7":"code","975921a0":"code","b417cc32":"code","523bdeb1":"code","c378e445":"code","e61dab8f":"code","1521671e":"code","a3337b8a":"code","8807b991":"code","17f2ea49":"code","a8a965c3":"code","738c58bf":"code","b5c8abf5":"code","51d0132d":"code","8e26ca88":"code","f5689bc8":"markdown","0aca6966":"markdown","873ab6ab":"markdown","2ce1d0b8":"markdown","909ab836":"markdown","f5401a0c":"markdown","266b53b1":"markdown","38a176a0":"markdown","aff35a97":"markdown","ba05c6c3":"markdown","fe6ca2f7":"markdown","82b46214":"markdown","ebdfc508":"markdown","6f2b2bdd":"markdown","afd78680":"markdown","47f1a15f":"markdown","8094cc5d":"markdown","1288631b":"markdown","10599d59":"markdown","259a149e":"markdown","4321e0f8":"markdown","8f031561":"markdown","3836c96a":"markdown","40baa4e6":"markdown","1dd1941b":"markdown","80c5185c":"markdown","5bc0a2c4":"markdown","ecde1138":"markdown","d729e3dc":"markdown","05ae09e2":"markdown","49cd26ca":"markdown","3a09a90c":"markdown","91a7aead":"markdown","1cc67367":"markdown","0a47434f":"markdown","89cd7172":"markdown","56925d12":"markdown","4a3ef997":"markdown","a98a1d0c":"markdown","19a3e6db":"markdown","d47b93ae":"markdown","13b43d1d":"markdown","59bd437b":"markdown","b66e0050":"markdown","5a1b5750":"markdown","7a6943f3":"markdown","784c6648":"markdown","05dc442c":"markdown","202a9777":"markdown","6b42e524":"markdown","1037e7fa":"markdown","90196dae":"markdown","ae4fc7fe":"markdown","f8e1400c":"markdown","56f54569":"markdown","8d1469a9":"markdown","894841dd":"markdown","42ff45f3":"markdown","92c1ab23":"markdown","f64f103f":"markdown","7513af83":"markdown","b25bc808":"markdown","923f0849":"markdown"},"source":{"e5838d5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7389fe9a":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","4f47878a":"path = '..\/input\/building-damage\/'  ## Add Path if Dataset is not in the same directory","ae822733":"feature_set = pd.read_csv(path + 'ass_02.csv') #add csv file path\ntarget_variables = pd.read_csv(path + 'ass_03.csv') #add csv file path","fc1abe2a":"feature_set.head()","f32accda":"feature_set.info()","639b150a":"feature_set.describe(include='all') #Making one hot encoding for this feature","d2636f81":"feature_set.isnull().sum()","cb045111":"feature_set.isnull().sum().any()","df920909":"feature_set.shape","a4fbb391":"feature_set.columns","153f9988":"feature_set.mean()","bf81c7ff":"feature_set.std()","e28e34da":"feature_set.info()","9303037a":"target_variables.head()","45b76550":"train_data = pd.merge(feature_set, target_variables, on='building_id', how='inner')\ntrain_data","f4926913":"sns.catplot(x=\"damage_grade\", y=\"age\", data=train_data)","20aaf5cb":"# Set up matplotlib figure with three subplots\nf, axes = plt.subplots(1, 3, figsize=(24, 12))\n\n# Plot the basic uni-variate figures for HP\nsns.boxplot(x=train_data['age'], y=train_data['damage_grade'], orient = \"h\", ax = axes[0])\nsns.histplot(x=train_data['age'], y=train_data['damage_grade'], ax = axes[1])\nsns.violinplot(x=train_data['age'], y=train_data['damage_grade'], orient = \"h\", ax = axes[2])","0f90112b":"# Draw jointplot of the two variables in the joined dataframe\nsns.jointplot(data = train_data, x = \"age\", y = \"damage_grade\", height = 12)","f1405ec4":"ax = sns.regplot(x=\"age\", y=\"damage_grade\", data=train_data)","9669670c":"feature_set['land_surface_condition'].unique()","3974fae1":"t = feature_set.groupby(\"land_surface_condition\").mean()\nt","c7a73c9c":"sns.boxplot(x=train_data['land_surface_condition'], y=train_data['damage_grade'], data=t,palette='rainbow')","947f191f":"sns.catplot(x=\"land_surface_condition\", y=\"damage_grade\", hue=\"land_surface_condition\",\n               data=train_data, kind=\"violin\")","d4cf01a4":"data = train_data\nltype = data[\"land_surface_condition\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"land_surface_condition\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(3,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","7126c0f1":"train_data['foundation_type'].unique()","8fd6ae61":"t = feature_set.groupby(\"foundation_type\").mean()\nt","0cede3ce":"sns.boxplot(x=train_data['foundation_type'], y=train_data['damage_grade'], data=t,palette='rainbow')","3cb1bec0":"sns.catplot(x=\"foundation_type\", y=\"damage_grade\", hue=\"foundation_type\",\n               data=train_data, kind=\"violin\")","ce4f1759":"data = train_data\nltype = data[\"foundation_type\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"foundation_type\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(5,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","17079510":"train_data['roof_type'].unique()","efa557ed":"t = feature_set.groupby(\"roof_type\").mean()\nt","a67b25e1":"sns.boxplot(x=train_data['roof_type'], y=train_data['damage_grade'], data=t,palette='rainbow')","6d26d1fc":"sns.catplot(x=\"roof_type\", y=\"damage_grade\", hue=\"roof_type\",\n               data=train_data, kind=\"violin\")","4ad4f2b1":"data = train_data\nltype = data[\"roof_type\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"roof_type\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(3,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","0d8880ae":"train_data['ground_floor_type'].unique()","b9193ad6":"t = feature_set.groupby(\"ground_floor_type\").mean()\nt","8b6d7eec":"sns.boxplot(x=train_data['ground_floor_type'], y=train_data['damage_grade'],data=t,palette='rainbow')","ceb30401":"sns.catplot(x=\"ground_floor_type\", y=\"damage_grade\", hue=\"ground_floor_type\",\n               data=train_data, kind=\"violin\")","6acf07a1":"data = train_data\nltype = data[\"ground_floor_type\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"ground_floor_type\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(5,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","89489afa":"train_data['other_floor_type'].unique()","0904dcf7":"t = feature_set.groupby(\"other_floor_type\").mean()\nt","d1a332f8":"sns.boxplot(x=train_data['other_floor_type'], y=train_data['damage_grade'], data=t,palette='rainbow')","9a895701":"sns.catplot(x=\"other_floor_type\", y=\"damage_grade\", hue=\"other_floor_type\",\n               data=train_data, kind=\"violin\")","e28b1860":"data = train_data\nltype = data[\"other_floor_type\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"other_floor_type\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(4,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","4c7b3d73":"train_data['position'].unique()","74d1bb3b":"t = feature_set.groupby(\"position\").mean()\nt","c4ab308b":"sns.boxplot(x=train_data['position'], y=train_data['damage_grade'],data=t,palette='rainbow')","8950f069":"sns.catplot(x=\"position\", y=\"damage_grade\", hue=\"position\",\n               data=train_data, kind=\"violin\")","cf7c0665":"data = train_data\nltype = data[\"position\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"position\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(4,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","a319e937":"train_data['plan_configuration'].unique()","606e0a23":"t = feature_set.groupby(\"plan_configuration\").mean()\nt","a29606ab":"sns.boxplot(x=train_data['plan_configuration'], y=train_data['damage_grade'],data=t,palette='rainbow')","34fc085d":"sns.catplot(x=\"plan_configuration\", y=\"damage_grade\", hue=\"plan_configuration\",\n               data=train_data, kind=\"violin\")","7e576e1c":"data = train_data\nltype = data[\"plan_configuration\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"plan_configuration\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(10,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","8e12a881":"train_data['legal_ownership_status'].unique()","29927c98":"t = feature_set.groupby(\"legal_ownership_status\").mean()\nt","728a50f6":"sns.boxplot(x=train_data['legal_ownership_status'], y=train_data['damage_grade'],data=t,palette='rainbow')","7587f685":"sns.catplot(x=\"legal_ownership_status\", y=\"damage_grade\", hue=\"legal_ownership_status\",\n               data=train_data, kind=\"violin\")","95a47ac1":"data = train_data\nltype = data[\"legal_ownership_status\"].unique() #Extract all the land surface cond\n#---Create a table to store number of all damage grade in every land surface conditi\ndmg = data[\"damage_grade\"].unique()\na = [] #Create empty array to store values\nsum = 0\n# The following codes below is to find out which type is most resistant to Earthquak\n# An array is used to store all the numeric values\nfor i in ltype:\n     for j in dmg:\n        a = np.append(a,len(data[(data[\"legal_ownership_status\"] == i) & (data[\"damage_grade\"] == j)]))\n\na = a.reshape(4,3).astype(\"int\") #Reshape the array to a 3rows*3cols matrix and co\n\ndf = pd.DataFrame(data = a, index = ltype, columns = dmg)\n                            \nfor i in ltype:\n    for j in dmg:\n        sum = sum + df.loc[i,j]\n    ratio = df.loc[i,3] \/ sum\n    sum = 0\n    print(\"Percentage 3rd grade damage of\", i, \"=\", round(ratio*100,2),'%')","db2fdbfd":"colums = train_data.columns\ncolums","8343bf5d":"# Showing the number of damaged buildings based on superstructure conditions and lev\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_adobe_mud\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_mud_mortar_stone\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_stone_flag\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_cement_mortar_stone\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_mud_mortar_brick\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_cement_mortar_brick\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_timber\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_bamboo\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_rc_non_engineered\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_rc_engineered\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_superstructure_other\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use\", kind=\"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_agriculture\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_hotel\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_rental\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_institution\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_school\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_industry\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_health_post\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_gov_office\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_use_police\", kind= \"count\")\nsns.catplot(x=\"damage_grade\", data = train_data, col = \"has_secondary_use_other\", kind= \"count\")","d9ec3f28":"train_data_features = pd.get_dummies(train_data,columns=['land_surface_condition','foundation_type','roof_type','ground_floor_type',\n                                                         'other_floor_type','position','plan_configuration','legal_ownership_status'])","a630a7c6":"train_data_features","24494591":"train_data.corr()","f9c3a5f6":"plt.figure(figsize=(20, 10))\nax = sns.heatmap(train_data.corr())\nax.tick_params(axis=\"x\", labelsize=14)\nax.tick_params(axis=\"y\", labelsize=14)\n\nplt.title('Correlation matrix',  fontsize=20);","30100515":"cols = train_data_features.columns\ncols","d3644813":"train_data_features.info()","74b99667":"train_data_features.isnull().sum()","6a9fd38d":"train_data_features.mean()","47d879ab":"train_data_features.std()","932bb027":"train_data_features.drop('building_id', inplace=True, axis=1)","4a3d471c":"features_sets = train_data_features.iloc[:,:-1]\nfeatures_sets","054d9c84":"data_describe = features_sets.describe().T.reset_index()\ndata_describe","49465c06":"plt.figure(figsize=(10,20))\nax = sns.scatterplot(x='mean', y='index', data=data_describe)\nN = len(data_describe)\nax.set_yticks(np.arange(0, N, 2))\nax.set_yticklabels(data_describe['index'].values[::2], fontsize=12)\nax.set_ylabel('index', fontsize=16)\nax.set_xlabel('mean', fontsize=16)\nax.set_title('Mean of Feature_set', fontsize=24)","30807c4f":"plt.figure(figsize=(10,20))\nax = sns.scatterplot(x='std', y='index', data=data_describe)\nN = len(data_describe)\nax.set_yticks(np.arange(0, N, 2))\nax.set_yticklabels(data_describe['index'].values[::2], fontsize=12)\nax.set_ylabel('index', fontsize=16)\nax.set_xlabel('std', fontsize=16)\nax.set_title('Standard Deviation of Target Variables', fontsize=24)","e8f1efec":"features_cols = list(features_sets.columns)\nfeatures_cols","312c8da4":"features_columns = features_sets[features_cols].sum(axis=1)\nfeatures_columns","7ae92470":"fig, ax = plt.subplots(1, 1, figsize=(10, 5))\nsns.countplot(features_columns, ax=ax)\nax.set_xlabel('Number of labels', fontsize=18)\nax.set_ylabel('Frequency', fontsize=18)\nax.tick_params(axis=\"x\", labelsize=16)\nax.tick_params(axis=\"y\", labelsize=16)\nax.set_title('Distribution of Features set', fontsize=20)","91646f65":"X = train_data_features[features_cols].values\nY = target_variables['damage_grade'].values","343a8c03":"X","1cd4ce05":"Y","85ebcb05":"import random\nindices = random.choices(range(len(X)), k=5000)\nX = X[indices,]\nY = Y[indices, ]\n ","94bc0e07":"print('X shape:', X.shape)\nprint('y shape:', Y.shape)","2a104ce7":"# First we reduce the number of features using PCA\npca = PCA(n_components=50)\nX_red = pca.fit_transform(X)\n\n# Then we can apply TSNE for low-dimensional visualization of the data points\ntsne_result = TSNE(n_components=2).fit_transform(X_red)","975921a0":"fig, ax = plt.subplots(1, 1, figsize=(16,10))\n\nsns.scatterplot(tsne_result[:, 0], tsne_result[:, 1], hue=Y,\n                palette=sns.color_palette(\"colorblind\", len(np.unique(Y))), legend=\"full\",\n                alpha=0.3, ax=ax)\nax.legend(prop=dict(size=18))\nax.set_title('2D visualization of T-SNE components on PCA - Compressed Features vs Target', fontsize=20);","b417cc32":"X = train_data_features.astype(float)\nY = target_variables['damage_grade'] ","523bdeb1":"scaler = preprocessing.StandardScaler()\nX = scaler.fit_transform(X)","c378e445":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)","e61dab8f":"# Knowing the shapes\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","1521671e":"model_l1 = LogisticRegression(penalty='l1', C=0.003, solver='liblinear')\nmodel_l1.fit(X_train,Y_train)\n\nprint(model_l1.intercept_)\nprint(model_l1.coef_)","a3337b8a":"score_l1 = model_l1.score(X_test, Y_test)","8807b991":"score_l1","17f2ea49":"y_pred = model_l1.predict(X_test)\ny_train_pred = model_l1.predict(X_train)","a8a965c3":"# Classification Accuracy\nresult = confusion_matrix(Y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(Y_test, y_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(Y_test,y_pred)\nprint(\"Accuracy:\",result2)","738c58bf":"# Check the Goodness of Fit (on Train Data)\nprint(\"Goodness of Fit of Model \\tTrain Dataset\")\nprint(\"Classification Accuracy \\t:\", model_l1.score(X_train, Y_train))\nprint()\n\n# Check the Goodness of Fit (on Test Data)\nprint(\"Goodness of Fit of Model \\tTest Dataset\")\nprint(\"Classification Accuracy \\t:\", model_l1.score(X_test, Y_test))\nprint()\n\n# Plot the Confusion Matrix for Train and Test\nf, axes = plt.subplots(2, 1, figsize=(12, 24))\nsns.heatmap(confusion_matrix(Y_train, y_train_pred),\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsns.heatmap(confusion_matrix(Y_test, y_pred), \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])","b5c8abf5":"# Decision Tree using Train Data\ndectree = DecisionTreeClassifier(max_depth = 4)  # create the decision tree object\ndectree.fit(X_train, Y_train)                    # train the decision tree model\n\n# Predict Response corresponding to Predictors\ny_train_pred = dectree.predict(X_train)\ny_test_pred = dectree.predict(X_test)","51d0132d":"# Classification Accuracy\nresult = confusion_matrix(Y_test, y_test_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(Y_test, y_test_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(Y_test,y_test_pred)\nprint(\"Accuracy:\",result2)","8e26ca88":"# Check the Goodness of Fit (on Train Data)\nprint(\"Goodness of Fit of Model \\tTrain Dataset\")\nprint(\"Classification Accuracy \\t:\", dectree.score(X_train, Y_train))\nprint()\n\n# Check the Goodness of Fit (on Test Data)\nprint(\"Goodness of Fit of Model \\tTest Dataset\")\nprint(\"Classification Accuracy \\t:\", dectree.score(X_test, Y_test))\nprint()\n\n# Plot the Confusion Matrix for Train and Test\nf, axes = plt.subplots(2, 1, figsize=(12, 24))\nsns.heatmap(confusion_matrix(Y_train, y_train_pred),\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsns.heatmap(confusion_matrix(Y_test, y_test_pred), \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])","f5689bc8":"- **Feature set contains**\n- **Columns : 39**\n- **Rows\/Records : 260601**","0aca6966":"## Formating the Dataset\n\n- **Here 8 columns are of Object type so we are apply Data Science Techniques in order to make them in proper format**\n- **First we checked these columns they contains categorical values e.g \"t\", \"o\", \"n\"**\n- **So we are explore each column**\n- **First we find unique values in them and then generate a table from them using a function called groupby and finally plot boxplot for them to express the quantitative values against damage**","873ab6ab":"- **\"v\" and \"m\" has the least impact on high demage grade then the other**","2ce1d0b8":"## PCA and TNSE For Data Visualization\n\n- **There are 39 columns in features set so displaying 39 columns is somehow impossible even if we could display its impossible to understand**\n- **So we are applying PCA and TNSE to the feature set in order to compress it to 2 features which is very easy to plot and very easy to understand**","909ab836":"## Importing Important Libraries\n\n- **Here are some important libraries to work with**\n- **Libraries for Visualization of Data Matplotlib and seaborn**\n- **Libraries for PCA and TNSE**\n- **Libraries for Machine Learning Model sklearn**\n\n**We are using that libraries all over this project**","f5401a0c":"**Correlation Heatmap which indicates the correlation of each column that how a column is correlation to each other and how it is correlated to damage_grade**\n\n- **Like it this heatmap the mud_motar_stone is more prone to damage than cement_motar_brick**\n- **Area_percentage has very little impact on damage_grade while the height percentage are more prone to damage**\n- **Age of building has medium correlation that means the sometime old building has more damage and sometimes new and so on**\n\n**This shows us the correlation heatmap**","266b53b1":"- **The boxplot of t, o and n are same so the mean median and quartile are same for all category**","38a176a0":"## Decision Tree Classifier","aff35a97":"### How the a Classification problem like Prediction of Earthquake damage can be predicted using ML Model and How to set evaluation criteria","ba05c6c3":"- **Statistical distribution of \"r\" and \"h\" with damage grade are same and \"w\" \"i\" \"u\" has same distribution against damage grade**","fe6ca2f7":"### Data Preprocess and Spliting into Train and Test set","82b46214":"### f) Position vs Damage","ebdfc508":"- **\"q\" \"x\" and \"j\" has a higher distribution in level 2 and 3 while \"s\" has a greater distribution at level 1**","6f2b2bdd":"### The above graph indicates;\n\n- **The boxplot shows the mean median and quartile between age and damage_grade along with some outliers**\n- **Similarily histogram show us the distribution of age to the damage_grade**\n- **violionplot shows the distribution of numeric value througout**\n\n","afd78680":"### e) Other Floor type vs Damage","47f1a15f":"#### Displaying Information about the Dataset","8094cc5d":"### How the features are distrubuted according to it's \"mean\"","1288631b":"## Correlation Table and Plot","10599d59":"- **\"a\" is dribution is mostly at level 1 while others are distributed in high level demage** ","259a149e":"- **Distribution of \"t\" \"o\" and \"n\" along with damage grade 1 2 and 3 the most wide distribution is of \"o\" and the most least distribution is of \"n\"**","4321e0f8":"## Merging Target Variable with Feature set\n\n\n#### The Target variables contains two columns \"Building_id\" and \"damage_grade\"\n#### The Feature set contains 39 columns including \"Building_id\"\n#### Use panda library function \"merge\" to merge these two data on column \"Building_id\"","8f031561":"### c) roof type vs Damage","3836c96a":"- **This plot shows the relationship between damage grade and age**\n- **At damage level 1 2 and 3 the age distribution is also same**","40baa4e6":"#### List of all Columns in the Features set","1dd1941b":"### h) Legal Ownership status vs Damage","80c5185c":"## Does these Categorical values have any impact on damage after earthquake","5bc0a2c4":"- **All category has the same distribution and has the same effect on high demage**","ecde1138":"- **Mean of each and every column in dataset**","d729e3dc":"### g) Plan Configuration vs Damage","05ae09e2":"#### Mean Value of each Column","49cd26ca":"**PCA Principal Compenent Analysis is a Machine Learning Technique use for dementiality reduction also very effective for data Visualization the above plot is draw through PCA. First all the columns in the feature set inculding label encoding a total of 69 columns are compressed to two columns than we display these two data points with target variable demage grade to visualize how closely the data is distributed**","3a09a90c":"## Data Visualization","91a7aead":"- **Simlarly the impact of \"c\" and \"a\" are the Least ones**","1cc67367":"### a) Land surface condition vs Damage","0a47434f":"- **Seems \"a\" has a Least impact on high demage than \"v\", \"r\" and \"w\"**","89cd7172":"- **From this plot we can see that v are mostly distributed in level 1 and also m has some outliers only**","56925d12":"# Machine Learning Models","4a3ef997":"- **From this plot we can indicate the \"x\" has the least impact on demage than \"n\" and \"q\"**","a98a1d0c":"### Does Building age has any impact on Damage after Earthquake\n","19a3e6db":"- **The \"d\" \"q\" and \"n\" has a greater impact on higher demage the least impact on demage are of \"c\" and \"a\"**","d47b93ae":"### b) Foundation type vs Damage","13b43d1d":"## Checking For Null Values","59bd437b":"# Assignment\n\n","b66e0050":"### Countplot of Feature variables with damage_grade","5a1b5750":"**The distribution of x is more in demage 1 while distribution of \"n\" and \"q\" are distributed in level 2 and 3**","7a6943f3":"- **it combines three different plot to show in one form**","784c6648":"### d) Ground floor type vs Damage","05dc442c":"## Logistic Regression Model","202a9777":"- **Distrubtion of Standard Deviation of each columns from age to legal_ownership_status**","6b42e524":"- **\"i\" \"w\" and \"u\" has the least impact on damage while \"h\" and \"r\" has more prone to demage**","1037e7fa":"## One Hot Encoding of Categorical Values","90196dae":"- **land_surface_condition n has the Least effect while the o has the significant Effect**","ae4fc7fe":"### Which features are most correlated to demage grade","f8e1400c":"#### Standard Deviation of each Columns","56f54569":"### Properties of Features set\n- **Mean, Standard Deviation etc**\n- **Finding NULL Values**\n- **Validating Format**","8d1469a9":"### Information about Dataset\n\n- **Total Columns: 39**\n- **Object Type: 8 columns. These Columns are of object type that means there the values of these columns are non numeric so by apply Machine Learning Model we need Numeric Dataset**\n\n**So we need to convert these columns to numeric**","894841dd":"- **Regression Plot shows the correlation of age with damage_grade**\n\n\n**NOTE: So from the above plots the age not that much correlated and has the same effect on damage level 1 2 and 3**","42ff45f3":"- **\"s\" has the least impact on high demage then the \"q\", \"x\" and \"j\"**","92c1ab23":"- **the distribution of higher demage is almost same her as well thought the \"o\" show least demage from the other but the difference is not that much**","f64f103f":"### How the features are distrubuted according to it's \"Standard Deviation\"","7513af83":"## Exploring Dataset","b25bc808":"## Importing Data\n\n- **Use Panda library to import dataset in the juypternotebook**\n- **There are two csv File one contain the feature set and other contain target variables**","923f0849":"### Is there a way to display all the features vs damage grade"}}