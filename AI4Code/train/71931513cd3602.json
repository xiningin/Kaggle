{"cell_type":{"26f45140":"code","1ab29c6e":"code","c77a59f7":"code","b5443c6e":"code","833c1abd":"code","4f4ac0f0":"code","afac166e":"code","fac83a15":"code","c093ae99":"code","1571a879":"code","f2329fcc":"code","af2654dc":"code","cbec8ac9":"code","69ed7fa6":"code","48e3ff92":"code","abf15c32":"code","a0483a82":"code","f6336009":"code","18561067":"code","cb57d16b":"code","dc828899":"code","cb06ba90":"code","744c552e":"code","6caf7019":"code","22e1dd45":"code","06a2ccc8":"code","9cf73fa1":"code","5ae30ac9":"code","329ab6af":"code","0b99e357":"code","e92f3f1c":"code","b11767e0":"code","da521bac":"code","262ef32e":"code","7481feac":"code","7a9d592e":"markdown","8ff9d915":"markdown","cc3f97d6":"markdown","9f6c66d3":"markdown","f37d00ff":"markdown","d4c7532e":"markdown"},"source":{"26f45140":"!pip install transformers","1ab29c6e":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nfrom tokenizers import BertWordPieceTokenizer\nfrom tqdm.notebook import tqdm\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import backend as K\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nimport matplotlib.pyplot as plt","c77a59f7":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()","b5443c6e":"test=pd.read_csv(r'..\/input\/nlp-getting-started\/test.csv')\ntrain=pd.read_csv(r'..\/input\/nlp-getting-started\/train.csv')\nsample_submission=pd.read_csv(r'..\/input\/nlp-getting-started\/sample_submission.csv')","833c1abd":"train.head()","4f4ac0f0":"test.head()","afac166e":"train.isnull().sum()","fac83a15":"test.isnull().sum()","c093ae99":"# dropping id, location column due to large no of Nan.\n\ntrain.drop(['id','location'],axis=1,inplace=True)\ntest.drop(['id','location'],axis=1,inplace=True)","1571a879":"x=0  # counter for rows containing 'ablaze' keyword and target=1\ny=0  # counter for total rows having keyword 'ablaze'","f2329fcc":"for i in range(len(train)):\n    if (train['keyword'].iloc[i]=='ablaze'):\n        x+=train['target'].iloc[i]\n        y+=1","af2654dc":"x,y","cbec8ac9":"train.drop(['keyword'],axis=1,inplace=True)\ntest.drop(['keyword'],axis=1,inplace=True)","69ed7fa6":"train['target'].value_counts()","48e3ff92":"!pip install clean-text[gpl]","abf15c32":"from cleantext import clean","a0483a82":"def text_cleaning(text):\n    text=clean(text,\n    fix_unicode=True,               # fix various unicode errors\n    to_ascii=True,                  # transliterate to closest ASCII representation\n    lower=True,                     # lowercase text\n    no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n    no_urls=True,                  # replace all URLs with a special token\n    no_emails=True,                # replace all email addresses with a special token\n    no_phone_numbers=True,         # replace all phone numbers with a special token\n    no_numbers=True,               # replace all numbers with a special token\n    no_digits=True,                # replace all digits with a special token\n    no_currency_symbols=True,      # replace all currency symbols with a special token\n    no_punct=True,                 # fully remove punctuation\n    replace_with_url=\"<URL>\",\n    replace_with_email=\"<EMAIL>\",\n    replace_with_phone_number=\"<PHONE>\",\n    replace_with_number=\"<NUMBER>\",\n    replace_with_digit=\"0\",\n    replace_with_currency_symbol=\"<CUR>\",\n    lang=\"en\"                       # set to 'de' for German special handling\n    )\n    return text","f6336009":"for i in range(len(train)):\n    train['text'].iloc[i]=text_cleaning(train['text'].iloc[i])\n\nfor i in range(len(test)):\n    test['text'].iloc[i]=text_cleaning(test['text'].iloc[i])  ","18561067":"train['text']","cb57d16b":"def build_model(transformer, max_len=512): \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    x = tf.keras.layers.Dropout(0.35)(cls_token)\n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=3e-5), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    \n    return model","dc828899":"with strategy.scope():\n    transformer_layer = transformers.TFBertModel.from_pretrained('bert-base-uncased')\n    model = build_model(transformer_layer, max_len=512)\nmodel.summary()","cb06ba90":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')","744c552e":"save_path = 'distilbert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)","6caf7019":"fast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased\/vocab.txt', lowercase=True)\nfast_tokenizer","22e1dd45":"def fast_encode(texts, tokenizer, size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    ids_full = []\n    \n    for i in tqdm(range(0, len(texts), size)):\n        text = texts[i:i+size].tolist()\n        encs = tokenizer.encode_batch(text)\n        ids_full.extend([enc.ids for enc in encs])\n    \n    return np.array(ids_full)","06a2ccc8":"x = fast_encode(train.text.astype(str), fast_tokenizer, maxlen=512)\nx_test = fast_encode(test.text.astype(str), fast_tokenizer, maxlen=512)","9cf73fa1":"BATCH_SIZE=64\n\ntest_data = (\n    tf.data.Dataset# create dataset\n    .from_tensor_slices(x_test) \n    .batch(BATCH_SIZE)\n)","5ae30ac9":"y=train['target'].values","329ab6af":"train_dataset = (\n    tf.data.Dataset \n      .from_tensor_slices((x, y))\n      .repeat()\n      .shuffle(2048)\n      .batch(BATCH_SIZE)\n    .prefetch(tf.data.experimental.AUTOTUNE) \n)","0b99e357":"with strategy.scope():\n    train_history = model.fit(\n      train_dataset,\n\n      steps_per_epoch=150,\n\n      epochs=3\n    )","e92f3f1c":"final=sample_submission[['id']]\nfinal['target'] = model.predict(test_data, verbose=1)","b11767e0":"def replace(col_val):\n    if col_val >=0.5:\n        col_val=1\n    else:\n        col_val=0\n    return col_val","da521bac":"final['target']=final['target'].apply(lambda x : replace(x))\nfinal['target'].value_counts()","262ef32e":"final","7481feac":"final.to_csv('sub_1.csv', index=False)","7a9d592e":"## Configure TPU after downloading necessary packages.","8ff9d915":"## Modeling","cc3f97d6":"### Checking importance of 'keyword' column.","9f6c66d3":"## Prediction on test data","f37d00ff":"## Text-cleaning using clean-text","d4c7532e":"### Hence, the keyword column is not helping much, drop it."}}