{"cell_type":{"1d6f76d3":"code","0bb65b7f":"code","8ef70dd5":"code","6bbdd688":"code","8258d4e0":"code","430201dd":"code","00fa8d41":"code","aa2b80b6":"code","e2ba75c5":"code","ccdb70ee":"code","0e592ad7":"code","39521929":"code","8a31854e":"code","03ce780d":"code","41cf3fcc":"code","9715da59":"code","2b5ae50b":"code","250bbc5c":"code","6a6002a9":"code","a916ead6":"code","26852665":"code","70d10c1f":"code","b310845e":"code","482bbe4e":"code","b067660a":"code","c96ab1c5":"code","12549e9d":"code","f1eef1bb":"code","d62aae05":"code","29ba04bb":"code","f52aa5ce":"code","35d295fb":"code","3ece8eb7":"code","ebe43284":"code","8fbe264d":"code","28e3c17e":"code","bfc5bdac":"code","48009486":"code","df28b099":"code","b258e3bc":"code","503d20c5":"code","1578c7f4":"code","1cbd2c52":"code","32aee746":"code","b9e0d306":"code","5e509ff3":"code","8eb240ac":"code","1887d102":"markdown","5c2cd470":"markdown","e6471875":"markdown","d13880a2":"markdown","973f1f23":"markdown","b2f5d11f":"markdown","2e4584e3":"markdown","1841e4b8":"markdown","a2fd56ff":"markdown","d7af55cc":"markdown","020e042d":"markdown","ac377185":"markdown","8ed3b323":"markdown"},"source":{"1d6f76d3":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport copy\nimport time\nfrom math import sqrt\nimport seaborn as sns\n\nimport re\nimport pickle\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.svm import SVR,SVC\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import BernoulliNB,GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\nfrom rgf.sklearn import RGFClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","0bb65b7f":"!mkdir data\n!cp \/kaggle\/input\/titanic\/train.csv .\/data\/train.csv\n!cp \/kaggle\/input\/titanic\/test.csv .\/data\/test.csv","8ef70dd5":"def execution_time_calculator(start_time):\n    delta = time.time() - start_time\n    print(\"--- %s seconds ---\" % (delta))\n    \n\n# def df_clean(df):\n#     df[\"date\"] = df[\"date\"].apply(time_to_standard)\n#     df = df.drop_duplicates(subset=['date'])\n#     dropped_df = df.dropna(axis=1).dropna().drop(['unix', 'symbol'], axis=1)  \n\n\n\ndef plot_train_data(df):\n    for col in df.columns :\n        plt.figure(figsize=(10, 5))\n        sns.distplot(df[col])\n        plt.title(col)\n#     plt.plot(df.index, df['Age'])\n\n#     plt.show()\n    return None\n\n\ndef impute_age(cols,age_mean_female,age_mean_male):\n    Age = cols[0]\n    Sex = cols[1]\n    \n    if pd.isnull(Age):\n        if Sex == \"female\":\n            return age_mean_female\n        elif Sex == \"male\":\n            return age_mean_male\n    else:\n        return Age\n\n\ndef df_clean(df,mode=0,pre_age_values=None):\n    if mode==0:\n        age_mean_male = df.groupby(['Sex']).mean()[\"Age\"][1]\n        age_mean_female = df.groupby(['Sex']).mean()[\"Age\"][0]\n        age_mean_list = [age_mean_female,age_mean_male]\n#         df.groupby(['Pclass']).mean()[\"Age\"]\n    elif mode==1:\n        age_mean_male   = pre_age_values[1]\n        age_mean_female = pre_age_values[0]\n        age_mean_list = pre_age_values\n    \n    values = {\"Cabin\": \"-\", \"Embarked\": \"-\",\"Pclass\":0,\"Sex\":\"male\",\"SibSp\":0,\"Parch\":0,\"Ticket\":\"-\",\"Fare\":0}\n    df['Age'] = df[['Age', 'Sex']].apply(impute_age, args = (age_mean_male,age_mean_female) ,axis=1)\n    \n    df = df.fillna(value=values)\n    df[\"Family size\"] = df['SibSp'] + df['Parch'] + 1 \n    # df = df[['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\", \"Survived\"]]\n    if mode==0:\n#         df = df\n        df = df[['Pclass', 'Sex', 'Age', \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Cabin\", \"Family size\", \"Survived\"]]\n    elif mode==1:\n        df = df[['Pclass', 'Sex', 'Age', \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Cabin\", \"Family size\",]]\n\n    return df,age_mean_list\n\n\ndef column_onehot_encoder(df,column,encoder,fit=0):\n    if fit==0:\n        encoder.fit(df[column].values.reshape(-1,1))\n    feature_names = encoder.get_feature_names([column])\n    data = encoder.transform(df[column].values.reshape(-1,1))\n    df_one_hot = pd.DataFrame(data, columns = feature_names).astype('int64')\n\n    return encoder, df_one_hot\n\n\ndef df_one_column_encoder(df,column,index,mode,encoder_mode,E_list):\n    if mode==0:\n        if encoder_mode==\"L\":\n            E_col = LabelEncoder()\n        elif encoder_mode==\"O\":\n            E_col = OneHotEncoder(sparse=False,drop=\"first\")\n    elif mode==1:\n        E_col = E_list[index]\n        \n    if encoder_mode==\"L\": \n        df[column] = E_col.fit_transform(df[column])\n        \n    elif encoder_mode==\"O\":\n        E_col, df_column = column_onehot_encoder(df,column,E_col,fit=mode)\n        df = df.drop(columns=[column])\n        df = pd.concat([df, df_column],axis=1)\n\n    return df,E_col\n\n\ndef df_encoder(df,columns,mode=0,encoder_mode=\"L\",E_list=None):\n    encoders_list = []\n#     feature_names_list = []\n    for index in range(len(columns)):\n        df,E_col = df_one_column_encoder(df,columns[index],index,mode,encoder_mode,E_list)\n        encoders_list.append(E_col)\n#         feature_names_list.append(feature_names)\n        \n    \n    return df, encoders_list\n\n\n# def df_encoder(df,columns,mode=0,encoder_mode=\"L\",LEs=None):\n    \n#     if mode==0:\n#         if encoder_mode==\"L\":\n#             E_Sex = LabelEncoder()\n#             E_Cabin = LabelEncoder()\n#             E_Embarked = LabelEncoder()\n#         elif encoder_mode==\"O\":\n#             E_Sex = OneHotEncoder(sparse=False)\n#             E_Cabin = OneHotEncoder(sparse=False)\n#             E_Embarked = OneHotEncoder(sparse=False)\n            \n#     elif mode==1:\n#         E_Sex = LEs[0]\n#         E_Cabin = LEs[1]\n#         E_Embarked = LEs[2]\n#     if encoder_mode==\"L\": \n#         df['Sex'] = E_Sex.fit_transform(df['Sex'])\n#         df['Cabin'] = E_Cabin.fit_transform(df['Cabin'])\n#         df['Embarked'] = E_Embarked.fit_transform(df['Embarked'])\n#     elif encoder_mode==\"O\":\n#         E_Sex      , df_Sex      = column_onehot_encoder(df,\"Sex\",E_Sex,fit=mode)\n#         E_Cabin    , df_Cabin    = column_onehot_encoder(df,\"Cabin\",E_Cabin,fit=mode)\n#         E_Embarked , df_Embarked = column_onehot_encoder(df,\"Embarked\",E_Embarked,fit=mode)\n\n#     return df, E_Sex, E_Cabin, E_Embarked\n\n\n\ndef metrics_calculator(real,pred,mode):\n    print(\"********\",mode,\"********\",\":\")\n    metrics_lists=[metrics.accuracy_score,metrics.f1_score,metrics.precision_score,metrics.recall_score,metrics.average_precision_score,metrics.roc_auc_score,metrics.mean_squared_error,metrics.mean_squared_error]\n    metric_names=[\"Accuracy\",\"F1\",\"Precision\",\"Recall\",\"AP\",\"RoC_AuC\",\"MSE\"]\n    for metric in range(len(metrics_lists)):\n        if metric== len(metrics_lists)-1:\n            metric_value = metrics_lists[metric](real, pred)\n            print(\"{:20}\".format(\"{}: \".format(\"RMSE\")),sqrt(metric_value))\n        else:  \n            metric_value = metrics_lists[metric](real, pred)\n            print(\"{:20}\".format(\"{}: \".format(metric_names[metric])),metric_value)\n        \n#     report = classification_report(real, pred)\n#     print(report)\n\n\n    \ndef all_models_test(df,models,models_name,x_train,y_train):\n\n    classifiers = []\n    for i in range(len(models)):\n        print(\"--------\",models_name[i],\"--------\")\n        clf = models[i].fit(x_train, y_train)\n        y_pred_train = clf.predict(x_train)\n        metrics_calculator(y_pred_train,y_train,\"train\")\n        classifiers.append(clf)\n    return classifiers","6bbdd688":"df = pd.read_csv(\".\/data\/train.csv\")#,skiprows=1)#.iloc[::-1].reset_index().drop([\"index\"], axis=1)","8258d4e0":"print(df.shape)\ndf.head()","430201dd":"df_cleaned,age_mean_list = df_clean(df)","00fa8d41":"df_cleaned.head()","aa2b80b6":"# df_cleaned.columns\n# X","e2ba75c5":"global_encoding_mode = \"L\"\ndf_encoded, encoders_list = df_encoder(df_cleaned,[\"Sex\",\"Embarked\",\"Cabin\"],mode=0,encoder_mode=global_encoding_mode)\n\n# pandas encoding - not good when we want to apply on test data\n\n# first way\n# df['Sex'] = df['Sex'].astype('category')\n# df['Cabin'] = df['Cabin'].astype('category')\n# df['Embarked'] = df['Embarked'].astype('category')\n# OneHotEncoder().fit_transform(df)\n# cat_columns = df.select_dtypes(['category']).columns\n# df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n\n\n# second way\n# X = pd.get_dummies(df_cleaned[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked','Survived']])\n# X_test = pd.get_dummies(test_data[features])","ccdb70ee":"df_encoded.head()","0e592ad7":"array = df_encoded.values\nX = array[:,:-1]\nY = array[:,-1]","39521929":"# plot na fields\nsns.heatmap(df.isna());","8a31854e":"df_cleaned.info();","03ce780d":"# print(df_cleaned[\"Cabin\"].unique())\n# print(df_cleaned[\"Cabin\"].describe())\n# print(df_cleaned.describe())\n# print(df_cleaned[\"Cabin\"].value_counts())\n# print(df_cleaned['Embarked'].value_counts().idxmax())\n# sns.countplot(x='Embarked',data=df_clean,palette='Set2')","41cf3fcc":"pd.plotting.scatter_matrix(df_encoded, diagonal='kde', figsize=(10, 10));","9715da59":"sns.heatmap(df_encoded.corr(),annot=True,cmap=\"RdYlGn\");","2b5ae50b":"plot_train_data(df_encoded);","250bbc5c":"df_encoded.columns","6a6002a9":"sns.countplot(x = 'Survived', hue='Sex', data=df_encoded)","a916ead6":"# sns.countplot(x = 'Survived', hue='Age', data=df_encoded)","26852665":"sns.countplot(x = 'Survived', hue='SibSp', data=df_encoded)","70d10c1f":"# sns.countplot(x = 'Survived', hue='Fare', data=df_encoded)","b310845e":"# sns.countplot(x = 'Survived', hue='Cabin', data=df_encoded)","482bbe4e":"sns.countplot(x = 'Survived', hue='Embarked', data=df_encoded)","b067660a":"sns.countplot(x = 'Survived', hue='Parch', data=df_encoded)","c96ab1c5":"sns.countplot(x = 'Survived', hue='Pclass', data=df_encoded)","12549e9d":"sns.countplot(x = 'Survived', hue=\"Family size\", data=df_encoded)","f1eef1bb":"pca = PCA(n_components=2)\nfit = pca.fit(X)\n# summarize components\nexplained_variance_ratio=fit.explained_variance_ratio_\n\nprint(explained_variance_ratio)\nprint(len(explained_variance_ratio))\nprint(sum(explained_variance_ratio))\nx_pca_train = pca.transform(X)\n\ny_train = Y\nx_train = X","d62aae05":"def all_classifiers_generator(mode=0):\n    if mode==0:\n        BNB = BernoulliNB()\n        GNB = GaussianNB()\n        RF = RandomForestClassifier()\n        DTree = DecisionTreeClassifier()\n        KNN = KNeighborsClassifier()\n        LR = LogisticRegression()\n        MLP = make_pipeline(MinMaxScaler(), MLPClassifier(random_state=0, shuffle=False))\n        XGB = xgb.XGBClassifier(eval_metric='mlogloss')\n        SVC_auto = make_pipeline(MinMaxScaler(), SVC(gamma=\"auto\"))\n        RGF = RGFClassifier()\n        Bagging = BaggingClassifier(random_state=0)\n        custom_RF=RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=6, min_samples_leaf=6, warm_start=True)\n        AdaBoost = AdaBoostClassifier(n_estimators=100, random_state=0)\n\n    elif mode==1:\n        BNB = make_pipeline(MinMaxScaler(), BernoulliNB())\n        GNB = make_pipeline(MinMaxScaler(), GaussianNB())\n        RF = make_pipeline(MinMaxScaler(), RandomForestClassifier())\n        DTree = make_pipeline(MinMaxScaler(), DecisionTreeClassifier())\n        KNN = make_pipeline(MinMaxScaler(), KNeighborsClassifier())\n        LR = make_pipeline(MinMaxScaler(), LogisticRegression())\n        MLP = make_pipeline(MinMaxScaler(), MLPClassifier(random_state=0, shuffle=False))\n        XGB = make_pipeline(MinMaxScaler(), xgb.XGBClassifier(eval_metric='mlogloss'))\n        SVC_auto = make_pipeline(MinMaxScaler(), SVC(gamma=\"auto\"))\n        RGF = make_pipeline(MinMaxScaler(), RGFClassifier())\n        Bagging = make_pipeline(MinMaxScaler(), BaggingClassifier(random_state=0))\n        custom_RF=make_pipeline(MinMaxScaler(), RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=6, min_samples_leaf=6, warm_start=True))\n        AdaBoost = make_pipeline(MinMaxScaler(),AdaBoostClassifier(n_estimators=100, random_state=0))\n\n    estimators = [\n       ('xgb', xgb.XGBClassifier(eval_metric='mlogloss')),\n       ('lr', LogisticRegression()),\n       ('nb', BernoulliNB()),\n       ('rgf', RGFClassifier()),\n       ('svm', make_pipeline(StandardScaler(), SVC(gamma=\"auto\"))),\n       ('mlp', make_pipeline(StandardScaler(), MLPClassifier(random_state=0, shuffle=False))),\n    ]\n    Voting = VotingClassifier(estimators)\n    Stacking = StackingClassifier(copy.deepcopy(estimators), LogisticRegression())\n\n    models = [BNB,GNB,RF,custom_RF,DTree,KNN,LR,MLP,XGB,SVC_auto,RGF,Bagging,AdaBoost]#,Voting,Stacking]\n    models_name = [\"BNB\",\"GNB\",\"RF\",\"custom_RF\",\"DTree\",\"KNN\",\"LR\",\"MLP\",\"XGB\",\"SVC_auto\",\"RGF\",\"Bagging\",\"AdaBoost\"]#,\"Voting\",\"Stacking\"]\n    \n    return models, models_name","29ba04bb":"models, models_name = all_classifiers_generator(mode=1)","f52aa5ce":"start_time = time.time()\n\nclassifiers = all_models_test(df_encoded,models[:],models_name,x_train,y_train)\n\nexecution_time_calculator(start_time)","35d295fb":"def grid_search(param_grid,model,X_train,Y_train):\n    \n    clf = GridSearchCV(model, param_grid, verbose=3)\n    clf.fit(X_train,Y_train)\n#     sorted(clf.cv_results_.keys())\n\n    y_pred_train = clf.predict(X_train)\n    metrics_calculator(y_pred_train,Y,\"train\")\n    return clf","3ece8eb7":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n# grid_rf = grid_search(param_grid,models[2],X,Y)","ebe43284":"param_grid = {\n    'n_neighbors': [3,5,11,19],\n    'weights': ['uniform','distance'],\n    'metric': ['euclidean','manhattan']\n}\n\n# grid_knn = grid_search(param_grid,KNeighborsClassifier(),X,Y)","8fbe264d":"param_grid = {\n#     \"base_estimator\" : [\"gini\", \"entropy\"],\n#     \"algorithm\" :   ['SAMME', 'SAMME.R'],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n# grid_ada = grid_search(param_grid,AdaBoostClassifier(),X,Y)","28e3c17e":"# df_labels = pd.read_csv(\".\/data\/Titanic.csv\")","bfc5bdac":"# df_labels.head()","48009486":"def create_synced_srvived_col(df_test,df_labels):\n    v_test = df_test.values\n    v_labels = df_labels.values\n    labels_col = []\n    for i in range(len(v_test)):\n        for j in range(len(v_labels)):\n#             print(v_test[i][2],\"-------\",v_labels[j][0])\n\n            test_string  = re.sub(r\"\\([^()]*\\)\", \"\", v_test[i][2])\n            label_string = re.sub(r\"\\([^()]*\\)\", \"\", v_labels[j][0])\n        \n            test_string = re.sub(r\"[\\s\\.]\", \"\", test_string)\n            label_string = re.sub(r\"[\\s\\.]\", \"\", label_string)\n\n            \n            if (test_string==label_string):# and (v_test[i][4]==v_labels[j][2]):\n#                 print(j)\n                labels_col.append(j)\n                break\n\n    return labels_col","df28b099":"# labels_col = create_synced_srvived_col(df_test,df_labels)\n# len(labels_col)\n\n# v_labels = df_labels.values\n# df_labels[\"ticket\"].describe()\n# df_labels\n# list(np.where(v_labels == \"Allen, Miss. Elisabeth Walton\"))\n\n# df_test['Name'].where(df_labels['name'] == df_test['Name'])\n# result = pd.concat([df_test, df_labels['survived']], axis=1).reindex(df_test.Name)\n# pd.concat([df_test, df_labels], axis=1)\n# pd.concat([df_test, df_labels], join_axes=[df_test.Name])","b258e3bc":"df_test = pd.read_csv(\".\/data\/test.csv\")\nids_col = df_test[\"PassengerId\"].values","503d20c5":"# df_test.info()\n# df_test","1578c7f4":"df_test,_ = df_clean(df_test,1,age_mean_list)\ndf_test,_ = df_encoder(df_test,[\"Sex\",\"Embarked\",\"Cabin\"],mode=1,encoder_mode=global_encoding_mode,E_list=encoders_list)\n\narray_test = df_test.values\nx_test = array_test[:,]","1cbd2c52":"def predict_one_classifier(classifier,x_test,labels_col=None):\n    y_pred_test = classifier.predict(x_test)\n    if labels_col!=None:\n        metrics_calculator(labels_col,y_pred_test,\"test\")\n    return y_pred_test\n\ndef predic_with_all_classifiers(classifiers,x_test,labels_col=None):\n    for i in range(len(classifiers)):\n        predict_one_classifier(classifiers[i],x_test)","32aee746":"# predic_with_all_classifiers(classifiers,x_test)\ny_pred_test = predict_one_classifier(classifiers[3],x_test)\n\n# y_pred_test = grid_ada.predict(x_test)","b9e0d306":"df_result = pd.DataFrame(list(zip(list(ids_col),list(y_pred_test))), columns =['PassengerId','Survived'])\ndf_result['Survived'] = df_result['Survived'].astype('int64')","5e509ff3":"# df_result","8eb240ac":"# compression_opts = dict(method='zip',archive_name='out.csv')  \n\ndf_result.to_csv('out.csv', index=False)  ","1887d102":"<a id=\"5.0\"><\/a>\n<h1>5.Encode categorical data<\/h1>","5c2cd470":"<p style=\"background-color:#CCE3F2; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><a id=\"outlines\">References : <\/a><\/p>\n\nhttps:\/\/stackoverflow.com\/questions\/43355044\/cumulative-explained-variance-for-pca-in-python\n\nhttps:\/\/stackoverflow.com\/questions\/32011359\/convert-categorical-data-in-pandas-dataframe\n\nhttps:\/\/stackoverflow.com\/questions\/23294616\/how-to-use-scikit-learn-pca-for-features-reduction-and-know-which-features-are-d\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n\nhttps:\/\/www.geeksforgeeks.org\/create-a-pandas-dataframe-from-lists\/\n\nhttps:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html\n\nhttps:\/\/stackoverflow.com\/questions\/15777951\/how-to-suppress-pandas-future-warning\n\nhttps:\/\/www.kaggle.com\/khkuggle\/simple-and-intermediate-eda-modeling-for-titanic for some visualizations\n\nhttps:\/\/www.kaggle.com\/alaasedeeq\/object-oriented-programming-for-data-science for outlines and markdown\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html\n\nhttps:\/\/www.geeksforgeeks.org\/different-ways-to-create-pandas-dataframe\/\n\nhttps:\/\/machinelearningmastery.com\/one-hot-encoding-for-categorical-data\/\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/merging.html\n\nhttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.drop.html\n\nhttps:\/\/github.com\/tpradeep8\/tableau-data-visualization\/blob\/master\/titanic%20passenger%20list.csv to create ground truth for testing\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","e6471875":"<a id=\"9.0\"><\/a>\n<h1>9.Grid Search<\/h1>","d13880a2":"<a id=\"10.0\"><\/a>\n<h1>10.Load test data<\/h1>","973f1f23":"<a id=\"2.0\"><\/a>\n<h1>2.Kaggle  preparing commands<\/h1>","b2f5d11f":"# <p style=\"background-color:#CCE3F2; font-family:newtimeroman; font-size:120%; text-align:center; border-radius: 15px 50px;\">Goal : Predict if a passenger can survive on the titanic or not.\n<\/p>\n\n<p style=\"background-color:#CCE3F2; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><a id=\"outlines\">Feel free to share your ideas with me.<\/a><\/p>","2e4584e3":"<p style=\"background-color:#CCE3F2; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 10px 100px;\"><a id=\"outlines\">Outlines : <\/a><\/p>\n<ul>\n    <li><a href=\"#1.0\"><b>1.Imports<\/b><\/a>\n    <li><a href=\"#2.0\"><b>2.Kaggle  preparing commands<\/b><\/a>\n    <li><a href=\"#3.0\"><b>3.Functions<\/b><\/a>\n    <li><a href=\"#4.0\"><b>4.Read and clean train data<\/b><\/a>\n    <li><a href=\"#5.0\"><b>5.Encode categorical data<\/b><\/a>\n    <li><a href=\"#6.0\"><b>6.Some info about data and visualizations<\/b><\/a>\n    <li><a href=\"#7.0\"><b>7.PCA<\/b><\/a>\n    <li><a href=\"#8.0\"><b>8.Classifiers<\/b><\/a>\n    <li><a href=\"#9.0\"><b>9.Grid Search<\/b><\/a>\n    <li><a href=\"#10.0\"><b>10.Load test data<\/b><\/a>\n        \n<\/ul>","1841e4b8":"<a id=\"4.0\"><\/a>\n<h1>4.Read and clean train data<\/h1>","a2fd56ff":"<a id=\"8.0\"><\/a>\n<h1>8.Classifiers<\/h1>","d7af55cc":"<a id=\"7.0\"><\/a>\n<h1>7.PCA<\/h1>","020e042d":"<a id=\"6.0\"><\/a>\n<h1>6.Some info about data and visualizations<\/h1>","ac377185":"<a id=\"3.0\"><\/a>\n<h1>3.Functions<\/h1>","8ed3b323":"<a id=\"1.0\"><\/a>\n<h1>1.Imports<\/h1>"}}