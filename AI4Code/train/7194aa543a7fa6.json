{"cell_type":{"2ed0f1f7":"code","545ff187":"code","943a9042":"code","06d9e945":"code","8610eb1a":"code","ada67577":"code","f17f98bd":"code","c8142618":"code","6c12e412":"code","653bd7c4":"code","6bd6412f":"code","4e63cd80":"code","7893a385":"code","ecdd8e84":"code","9aa2563d":"code","570ea551":"code","4b067a21":"code","a743aa68":"code","ded60e32":"code","0093a11a":"code","5f20c971":"code","4c01c1c3":"code","c952c0f4":"code","98cb97b9":"code","b15d149a":"code","9293330f":"code","52a20e8d":"code","e369054f":"code","e95cfb08":"code","bcb8514c":"code","1d9cd49f":"code","b51eb561":"code","5fb0247c":"code","7ab3d3e2":"markdown","07adf417":"markdown","8ed3d31b":"markdown","a7c83e8f":"markdown","89e89b00":"markdown","61b6b284":"markdown","395075d2":"markdown","5d536b83":"markdown","678a9619":"markdown","81f8d2ab":"markdown","a92b1709":"markdown","fbb2b3c7":"markdown","d690a98b":"markdown","65e7d51a":"markdown","d53227f5":"markdown","717ee73a":"markdown","acfd75a7":"markdown","03a1f93d":"markdown","1f43f8c2":"markdown","654144af":"markdown","8e8f42a4":"markdown","a64866e5":"markdown","8d0aeaa5":"markdown","da7e85cc":"markdown","946d5264":"markdown","ec52e46c":"markdown","210557a4":"markdown"},"source":{"2ed0f1f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","545ff187":"import pandas as pd\nimport os\nimport numpy as np\nimport seaborn as sns\nimport warnings\nfrom matplotlib import pylab as plt\nfrom statsmodels.graphics.gofplots import qqplot\nfrom IPython.core.interactiveshell import InteractiveShell","943a9042":"# let's make a list comprehension for all the data in the folder\nfiles = [file for file in os.listdir('..\/input\/sales-product-data')] \n# let's make a pandas DataFrame\nall_months_data = pd.DataFrame()\n# makes a loop for concat the data\nfor file in files:\n    data = pd.read_csv(\"..\/input\/sales-product-data\/\" + file)\n    all_months_data = pd.concat([all_months_data, data])\n# export all data to csv    \nall_months_data.to_csv(\"all_data.csv\", index=False)","06d9e945":"sales_data = pd.read_csv('.\/all_data.csv') # read data\nsales_data # show data","8610eb1a":"\"Head\"\nsales_data.head() # Checking the first 5 rows of data","ada67577":"\"Tail\"\nsales_data.tail() # Checking the last 5 rows of data","f17f98bd":"# getting the information \nsales_data.info()","c8142618":"categorical = sales_data.select_dtypes(['category', 'object']).columns # getting the Uniqueness catrgorical variable\nfor col in categorical:\n    print('{} : {} unique value(s)'.format(col, sales_data[col].nunique()))","6c12e412":"# get the number of missing data points per column\nmissing_values_count = sales_data.isnull().sum()\n\nmissing_values_count","653bd7c4":"# let's drop the rows of NaN data!\nsales_data = sales_data.dropna(how='all')\n# okay, let's check it again!\n\"NaN Value:\"\nsales_data[sales_data.isna().any(axis=1)]\n# future warning! ValueError: invalid literal for int() with base 10: 'Or'\n\"Clean Future Warnings:\"\nsales_data = sales_data[sales_data['Order Date'].str[0:2] != 'Or']\nsales_data.head()","6bd6412f":"# convert the data\nsales_data['Quantity Ordered'], sales_data['Price Each'] = sales_data['Quantity Ordered'].astype('int64'), sales_data['Price Each'].astype('float')\n# and check it \nsales_data.info()","4e63cd80":"# convert it using to_datetime() funct\nsales_data['Order Date'] = pd.to_datetime(sales_data['Order Date'])\n# let's see it\nsales_data.head()","7893a385":"def augment_data(data):\n    \n    \"\"\"\n    Adding new features to\n    our data, adding Month Data,\n    Hour Data, Minute Data, Sales Data,\n    and Cities Column\n    \n    Returning:\n        data with new features\n    \"\"\"\n    \n    # funtction to get the city in the data\n    def get_city(address):\n        return address.split(',')[1]\n    \n    # funtction to get the state in the data\n    def get_state(address):\n        return address.split(',')[2].split(' ')[1]\n\n    # let's get the year data in order date column\n    data['Year'] = data['Order Date'].dt.year\n    \n    # let's get the month data in order date column\n    data['Month'] = data['Order Date'].dt.month\n    \n    # let's get the houe data in order date column\n    data['Hour'] = data['Order Date'].dt.hour \n    \n    # let's get the minute data in order date column\n    data['Minute'] = data['Order Date'].dt.minute \n    \n    # let's make the sales column by multiplying the quantity ordered colum with price each column\n    data['Sales'] = data['Quantity Ordered'] * data['Price Each'] \n    \n    # let's get the cities data in order date column\n    data['Cities'] = data['Purchase Address'].apply(lambda x: f\"{get_city(x)} ({get_state(x)})\") \n    \n    return data # returning data\n\n# and see it\nsales_data = augment_data(sales_data)\nsales_data\n","ecdd8e84":"# let's make a correlation matrix\nplt.figure(figsize=(24, 8))\nsns.heatmap(sales_data.corr(), annot=True);","9aa2563d":"sales_data.corr()['Sales'].sort_values(ascending=False) # the correlation","570ea551":"# statistical measure of sales data without object type of data\nsales_data_numeric = sales_data.describe(include=[np.number]) \n# statistical measure of sales data without numeric type of data\nsales_data_object = sales_data.describe(exclude=[np.number])\n\"Statistical Measure of Sales Data in Numeric Data\"\nprint(sales_data_numeric)\n\"Statistical Measure of Sales Data in Object \/ Str Data\"\nprint(sales_data_object)","4b067a21":"# checking and visualizing the type of distribution of a feature column\ndef univariate_analysis(data, color, title1, title2):\n    \n    \"\"\"\n    Showing visualization of univariate\n    analysis with displot and qqplot\n    visualization from seaborn and statsmodel\n    library.\n    \n    Parameters\n    ----------\n    data : DataFrame, array, or list of arrays, optional\n        Dataset for plotting. If ``x`` and ``y`` are absent, this is\n        interpreted as wide-form. Otherwise it is expected to be long-form. \n    title1: The title of the visualization, title1 for displot visualization\n        And title2 for quantile plot from statsmodel.\n    title2: The title of the visualization, title1 for displot visualization\n        And title2 for quantile plot from statsmodel.\n        \n    Returns\n    -------\n    fig : matplotlib figure\n        Returns the Figure object with the plot drawn onto it.\n    \"\"\"\n    \n    fig, (ax1, ax2) = plt.subplots( # subplots\n        ncols=2, # num of cols\n        nrows=1, # num of rows\n        figsize=(20, 6) # set the width and high\n    )\n\n    sns.distplot( # create a distplot visualization\n        data, # data\n        ax=ax1, # axes 1\n        kde=True, # kde\n        color=color # color\n    )\n    \n    ax1.set(title=title1) # set the title 1\n    \n    qqplot( # qqplot (quantile plot)\n        data, # data\n        ax=ax2, # axes 2\n        line='s' # line \n    )\n    \n    ax2.set(title=title2) # set the title 2\n    \n    return fig # returning the figure\n\n# Quantity Ordered Data\nunivariate_analysis( # call the function\n    data=sales_data['Quantity Ordered'], # put the data\n    color='red', # pick the color\n    title1='Quantity Ordered Data Distribution', # title1\n    title2='Quantile Plot' # title2\n);\n# Price Each Data\nunivariate_analysis( # call the function\n    data=sales_data['Price Each'], # put the data\n    color='blue', # pick the color \n    title1='Price Each Data Distribution', # title1 \n    title2='Quantile Plot' # title2\n);\n# Year Data\nunivariate_analysis( # call the function\n    data=sales_data['Year'], # put the data\n    color='green', # pick the color\n    title1='Year Data Distribution', # title1 \n    title2='Quantile Plot' # title2\n);\n# Month Data\nunivariate_analysis( # call the function\n    data=sales_data['Month'], # put the data \n    color='brown', # pick the color\n    title1='Month Data Distribution', # title1 \n    title2='Quantile Plot' # title2\n);\n# Hour Data\nunivariate_analysis( # call the function\n    data=sales_data['Hour'], # put the data \n    color='violet', # pick the color\n    title1='Hour Data Distribution', # title1 \n    title2='Quantile Plot' # title2\n);\n# Minute Data\nunivariate_analysis( # call the function\n    data=sales_data['Minute'], # put the data \n    color='orange', # pick the color \n    title1='Minute Data Distribution', # title1 \n    title2='Quantile Plot' # title2\n);\n# Sales Data\nunivariate_analysis( # call the function\n    data=sales_data['Sales'], # put the data \n    color='black', # pick the color\n    title1='Sales Data Distribution', # title1 \n    title2='Quantile Plot' # title2\n);","a743aa68":" # checking skewness value\n# if value lies between -0.5 to 0.5  then it is normal otherwise skewed\nskew_value = sales_data.skew().sort_values(ascending=False)\nskew_value","ded60e32":"# What was the best Year for sales? How much was earned that Year?\n\n# let's plot it\nplt.figure(figsize=(24, 4)) # figuring the size\n# makes count plot \nsns.countplot(x='Year', data=sales_data)\n# title\nplt.title('Year Sales and Much Earned in that Year', \n          fontname='monospace', fontweight='bold', fontsize=15\n)\n# x-label\nplt.xlabel('Years')\n# y-label\nplt.ylabel('Sales in USD ($)');\n","0093a11a":"sales_data.groupby(['Year']).sum().astype('int')","5f20c971":"# What was the best month for sales? How much was earned that month?\nsales_data.groupby(['Month']).sum().astype('int')","4c01c1c3":" #group the Month cols\nsum_of_month_and_earned = sales_data.groupby('Month').sum().astype('int')\n# let's plot it\nplt.figure(figsize=(24, 6)) # figuring the size\n# makes bar plot \nsns.barplot(\n    x=sum_of_month_and_earned.index, \n    y=sum_of_month_and_earned['Sales'], \n    data=sum_of_month_and_earned, palette='deep'\n)\n# title\nplt.title('Month Sales and Much Earned in that Months', \n          fontname='monospace', weight='bold'\n)\n# x-label\nplt.xlabel('Months')\n# y-label\nplt.ylabel('Sales in USD ($)');\n","c952c0f4":"# Which City had the highest number of sales?\nsales_data.groupby(['Cities']).sum().astype('int')","98cb97b9":"# group of the highest number of sales in city\nhighest_number_of_sales = sales_data.groupby('Cities').sum().astype('int')\n# let's plot it\nplt.figure(figsize=(24, 6)) # figuring the size\n# makes bar plot \nsns.barplot(\n    x=highest_number_of_sales.index, \n    y=highest_number_of_sales['Sales'], \n    data=highest_number_of_sales, palette='deep'\n)\n# title\nplt.title(\n    'City with the Highest number of Sales', \n    fontname='monospace', weight='bold'\n)\n# x-label\nplt.xlabel('Cities')\n# y-label\nplt.ylabel('Sales in USD ($)');","b15d149a":"# What time should we display adverstisement to maximize likelihood of customer's buying product?\n\nsales_data.head()","9293330f":"# let's prepare the value for the x-axis\nhours = [hour for hour, df in sales_data.groupby('Hour')]\n# let's plot it\nplt.figure(figsize=(24, 4)) # figuring the size\n# makes bar plot \nplt.plot(hours, sales_data.groupby(['Hour']).count())\n# xticks\nplt.xticks(hours)\n# let's add grid\nplt.grid(True)\n# title\nplt.title(\n    'Time of Customer buying product', \n    fontname='monospace', weight='bold'\n)\n# x-label\nplt.xlabel('Hours')\n# y-label\nplt.ylabel('Number of Orders');\n","52a20e8d":"# What product sold the most? Why do you think it sold the most?\n\n# lets prepare the varables for the plotting\n# group the product\nproduct_group = sales_data.groupby('Product')\nquantity_ordered = product_group.sum()['Quantity Ordered']\nprices = sales_data.groupby('Product').mean()['Price Each']\nproducts = [product for product, df in product_group]\n\"\"\"Visualization\"\"\"\n# let's make a subplots\nfig, ax1 = plt.subplots(figsize=(24, 8))\nax2 = ax1.twinx()\n# AXES 1\nax1.bar(products, quantity_ordered)\nax1.set_xlabel('Product Name')\nax1.set_ylabel('Quantity Ordered', color='b')\nax1.set_xticklabels(products, rotation='vertical')\n# AXES 2\nax2.plot(products, prices, 'r-')\nax2.set_ylabel('Price in USD ($)', color='r');","e369054f":"#What products are most often sold together?\n\nfrom itertools import combinations\nfrom collections import Counter\n\n# drop it using duplicated() funct\ndata = sales_data[sales_data['Order ID'].duplicated(keep=False)]\n# create a new column\ndata['Grouped'] = sales_data.groupby('Order ID')['Product'].transform(lambda x: ','.join(x))\n# let's make a new variable\ndata = data[['Order ID', 'Grouped']].drop_duplicates()\n# create a new variable for Counter\ncount = Counter()\n# make a for loop\nfor row in data['Grouped']:\n    row_list = row.split(',')\n    count.update(Counter(combinations(row_list, 2)))\n# and make another for loop    \nfor key, value in count.most_common(10):\n    print(key, value)","e95cfb08":"sales_data.Product.value_counts().to_frame().T","bcb8514c":"\ndef statistical_probability(frequency, total_frequency):\n    return frequency \/ total_frequency\n\nproduct = sales_data.Product.value_counts().sum() ","1d9cd49f":"# How much probability for next people will order USB-C Charging Cable?\n\nusb_charging = sales_data[sales_data.Product == 'USB-C Charging Cable'].value_counts().sum()\n# Calculating USB-C Charging Probability\nP_USB = statistical_probability(usb_charging, product)\nPprime_USB = 1 - P_USB\nprint('Probability for next people will order USB-C Charging Cable: %.4f%%' % P_USB)\nprint('Probability for next people will not order USB-C Charging Cable: %.4f%%' % Pprime_USB)","b51eb561":"# How much probability for next people will order iPhone?\n\niphone = sales_data[sales_data.Product == 'iPhone'].value_counts().sum()\n# Calculating iPhone Probability\nP_iphone = statistical_probability(iphone, product)\nPprime_iphone = 1 - P_iphone\nprint('Probability for next people will order iPhone: %.4f%%' % P_iphone)\nprint('Probability for next people will not order iPhone: %.4f%%' % Pprime_iphone)","5fb0247c":"# How much probability for next people will order Google Phone?\n\ngoogle_phone = sales_data[sales_data.Product == 'Google Phone'].value_counts().sum()\n# Calculating Google Phone Probability\nP_google_phone = statistical_probability(google_phone, product)\nPprime_google_phone = 1 - P_google_phone\nprint('Probability for next people will order Google Phone: %.4f%%' % P_google_phone)\nprint('Probability for next people will not order Google Phone: %.4f%%' % Pprime_google_phone)","7ab3d3e2":"##### Since this missing value is not too big, let's just drop the missing value, I think this will not affect the data since the data is big enough.\n\n#### Clean up the Data!\n##### The first step in this is figuring out what we need to clean. I have found in practice, that you find things you need to clean as you perform operations and get errors. Based on the error, you decide how you should go about cleaning the data.","07adf417":"# In-depth Analysis on Sales Dataset","8ed3d31b":"Products that are often sold simultaneously are iPhone and Lightning Charging Cable which sold 1005 orders, and Google Phone, USB-C Charging Cable ranked second with 987 orders.","a7c83e8f":"My recommendation if you want to place an ad, place the ad at 9 am or 10 am, because there is an increase in the number of orders at that time.","89e89b00":"##### Add Month, Hour, Minute, Sales, Cities Column","61b6b284":"#### Merge the 12 months of sales data into a single CSV file","395075d2":"#### Convert Quantity Ordered column and Price Each column\n##### Let's convert the Quantity Ordered column and Price Each column to Numeric Type, because we will add some future features, and we need to multiply this two column.","5d536b83":"# Data Preprocessing\n#### Data preprocessing can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects.","678a9619":"The city that has the most sales data in the above visualization is San Francisco, with total sales reaching  8,262,203 $","81f8d2ab":"When viewed from the data above, 2019 was the best year that had the highest number of sales, which was  34,483,365 , compared to 2020 which only had  8,670  in sales, this is due to the lack of data in 2020 which caused a data imbalance.","a92b1709":"# Data Preparation\n\n##### Data preparation is the act of manipulating raw data into a form that can readily and accurately be analysed, e.g. for business purposes. Data Preparation is a pre-processing step in which data from one or more sources is cleaned and transformed to improve its quality prior to its use in business analytics.","fbb2b3c7":"# Thank you for reading!\n\nKindly leave your feedback or give me yor insight about this notebook. ","d690a98b":"#### Uniqueness Categorical Variables\n##### Let's have a look at categorical variables. How many unique values of these variables.","65e7d51a":"# Import Necessary Libraries","d53227f5":"#### Univariate Analysis\n##### Univariate analysis is perhaps the simplest form of statistical analysis. Like other forms of statistics, it can be inferential or descriptive. The key fact is that only one variable is involved. Univariate analysis can yield misleading results in cases in which multivariate analysis is more appropriate.","717ee73a":"#### Read in updated DataFrame\nLet's see, how the data looks!!","acfd75a7":"#### Convert Order Date column\n##### And let's convert Order Date column too, so we can take the Year, Month, and the other date easily.","03a1f93d":"The most sold products are AA Batteries (4-pack), AA Batteries (4-pack), Lightning Charging Cable, USB-C Charging Cable, and Wired Headphones. Why do these products sell more than others? This is because the prices of the most ordered products have a low price compared to other products, for example, the LG Dryer, or the LG Washing Machine, which have a price of around  500\u2212700 $.","1f43f8c2":"#### How many missing data points do we have?\n##### Ok, now we know that we do have some missing values. Let's see how many we have in each column.","654144af":"# Conclusion\n#### 2019 was the best year that had the most sales, which was 34,483,365 USD, compared to 2020 which only had 8,670 USD. this is due to data shortage in 2020 which causes data imbalance, I need 2020 data to be able to continue this data analysis. The best month to sell if shown in the visualization above is December which has a record number of sales reaching 4,613,443 USD, sales. This may be because December is Christmas, where many people buy groceries to make cakes or toys as gifts for loved ones. The city that has the most sales data in the above visualization is the city of San Francisco, with total sales reaching 8,262,203 USD.\n\n#### My recommendation if you want to place an ad, place the ad at 9 am or 10 am, because there is an increase in the number of orders at that time. Products that are often sold simultaneously are iPhone and Lightning Charging Cable which sold 1005 orders, and Google Phone, USB-C Charging Cable ranked second with 987 orders. The most sold products are AA Batteries (4-pack), AA Batteries (4-pack), Lightning Charging Cable, USB-C Charging Cable, and Wired Headphones. Why do these products sell more than others? This is because the prices of the most ordered products are cheaper than other products, for example the LG Dryer, or the LG Washing Machine, which costs around 500-700 USD.","8e8f42a4":"### Recap Data\n#### This is some point that we have.\n\nWe have total 186850 records and 6 columns cateogircal type\nThe total of missing value that we have is 0.29167%\n\n* Order ID : 178438 unique value(s)\n* Product : 20 unique value(s)\n* Quantity Ordered : 10 unique value(s)\n* Price Each : 24 unique value(s)\n* Order Date : 142396 unique value(s)\n* Purchase Address : 140788 unique value(s)\n\n\n##### Next, we will try to do some exploration and visualization. But we need to do some Data Preparation first.","a64866e5":"### How Much Probability?\n#### Formula:\n#### P(E)=FrequencyEventETotalFrequency \n\n#### Rule of Complementary Events:\n#### P(E)+P(E\u2032)=1 \n#### P(E)=1\u2212P(E\u2032) \n#### P(E\u2032)=1\u2212P(E) \n\n","8d0aeaa5":"##### In the above information, we can see there are some missing values","da7e85cc":"# Data Analysis\n#### Data Analysis is the process of systematically applying statistical and\/or logical techniques to describe and illustrate, condense and recap, and evaluate data. Indeed, researchers generally analyze for patterns in observations through the entire data collection phase (Savenye, Robinson, 2004). analyze and investigate data sets and summarize their main characteristics, often employing data visualization methods.","946d5264":"![Sales Analysis](https:\/\/www.finereport.com\/en\/wp-content\/uploads\/2020\/06\/2020062201I-1024x576.png)","ec52e46c":"# Inspiration\n\nhttps:\/\/www.kaggle.com\/knightbearr\/sales-data-deep-analysis-knightbearr\n","210557a4":"The best month to sell is shown in the visualization above is December which has a record number of sales reaching  4,613,443 $, sales. This may be because in December there is Christmas, where many people buy groceries to make cakes or toys as gifts for loved ones."}}