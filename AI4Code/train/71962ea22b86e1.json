{"cell_type":{"e42824bc":"code","630146b1":"code","7349c79e":"code","e08eb748":"code","6e36240a":"code","6492f982":"code","e70e7720":"code","f1c55459":"code","8f033f4a":"code","39bff074":"code","0707ee7f":"code","0ab84935":"code","0fc5bd41":"code","f7cc2de6":"code","fc8c2b4b":"code","884f2dd2":"code","6ef67558":"code","c1518aae":"code","f3aaa37d":"code","4a7872fa":"code","656787a9":"code","dcddbb41":"code","ec8c9f19":"code","ae16f469":"code","fc21bd75":"code","44b1491c":"code","e60b2f06":"markdown","151e103e":"markdown","bb147ef9":"markdown","d432339e":"markdown","3d888637":"markdown","95b12ce0":"markdown","f8bdcb15":"markdown","e04a1822":"markdown","a3a12784":"markdown","87ec0942":"markdown","f107e368":"markdown","174be623":"markdown","72af7e4c":"markdown","bba83a00":"markdown","ca421759":"markdown","b2c72a53":"markdown","52ac9615":"markdown","5709562e":"markdown","1ede6a81":"markdown","912fd43b":"markdown","c55bb229":"markdown","690652d8":"markdown","d5dbbd2a":"markdown","f013b9ff":"markdown","e340b98c":"markdown","a1fd05bb":"markdown","7437d926":"markdown","933527df":"markdown","7cec24b9":"markdown","3b9757c6":"markdown","03282186":"markdown","5cb2d88a":"markdown","00e79b96":"markdown","9eda317d":"markdown","94f8aef5":"markdown","3fe895fc":"markdown","4e004e6a":"markdown","a26c0c1e":"markdown","0c19f2ba":"markdown","a146f8ce":"markdown","d754f781":"markdown","cb02702a":"markdown","b4022506":"markdown","b7c9804e":"markdown","f061dcf1":"markdown"},"source":{"e42824bc":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\n\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","630146b1":"VERBOSE=1","7349c79e":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","e08eb748":"lg_clf = LGBMClassifier(verbosity=VERBOSE,\n                        random_state=RANDOM_STATE,\n                        n_jobs=N_JOBS)","6e36240a":"parameters = {\n    'boosting_type': ['gbdt', 'dart', 'goss'] # 'rf' fails\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","6492f982":"parameters = {\n    'num_leaves': [2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","e70e7720":"parameters = {\n    'max_depth': [0, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","f1c55459":"parameters = {\n    'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","8f033f4a":"parameters = {\n    'n_estimators': [20, 50, 100, 200, 500, 1000, 1500, 1900, 2000, 2100, 2500]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","39bff074":"parameters = {\n    'subsample_for_bin': [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","0707ee7f":"parameters = {\n    'objective': ['regression', 'binary', 'multiclass', 'lambdarank']\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","0ab84935":"parameters = {\n    'class_weight': ['balanced', None, weight]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\ntable_grid_search(clf, all_ranks=True)","0fc5bd41":"parameters = {\n    'min_split_gain': [x \/ 10 for x in range(0, 11)] \n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","f7cc2de6":"parameters = {\n    'min_child_weight': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","fc8c2b4b":"parameters = {\n    'min_child_samples': [1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","884f2dd2":"parameters = {\n    'subsample': [x \/ 10 for x in range(1, 11)]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","6ef67558":"parameters = {\n    'subsample_freq': [1, 2, 3, 5, 8, 13, 21, 34, 55, 89]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","c1518aae":"parameters = {\n    'colsample_bytree': [x \/ 10 for x in range(1, 11)]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","f3aaa37d":"parameters = {\n    'reg_alpha': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","4a7872fa":"parameters = {\n    'reg_lambda': [x \/ 10 for x in range(0, 11)]\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","656787a9":"parameters = {\n    'importance_type': ['split', 'gain']\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","dcddbb41":"lg_clf = LGBMClassifier(verbosity=VERBOSE,\n                        random_state=RANDOM_STATE,\n                        n_jobs=N_JOBS)\nlg_clf.min_child_weight = 1e-15\nlg_clf.boosting_type = 'gbdt'\nlg_clf.num_leaves = 144\nlg_clf.max_depth = 21\nlg_clf.learning_rate = 0.6\nlg_clf.n_estimators = 2000\nlg_clf.subsample_for_bin = 987\nlg_clf.min_child_samples = 5\nlg_clf.colsample_bytree = 0.7\nlg_clf.reg_alpha = 0.7\nlg_clf.reg_lambda = 0.4\nparameters = {\n#     'boosting_type': ['gbdt', 'goss'],\n#     'num_leaves': [34, 50, 55, 60, 89],\n#     'max_depth': [13, 21, 34],\n#     'learning_rate': [0.5, 0.6, 0.7],\n#     'n_estimators': [1900, 2000, 2100],\n#     'subsample_for_bin': [610, 987, 1597],\n#     'min_child_samples': [3, 5, 8, 20],\n#     'colsample_bytree': [0.85, 0.9, 0.95],\n#     'reg_alpha': [0.0, 0.7],\n#     'reg_lambda': [0.0, 0.4]\n}\n# clf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\n# clf.fit(X_train, y_train)\n# plot_grid_search(clf)\n# table_grid_search(clf)","ec8c9f19":"lg_clf = LGBMClassifier(verbosity=VERBOSE,\n                        random_state=RANDOM_STATE,\n                        n_jobs=N_JOBS)\nlg_clf.min_child_weight = 1e-15\nlg_clf.boosting_type = 'gbdt'\nlg_clf.num_leaves = 55\nlg_clf.max_depth = 21\nlg_clf.learning_rate = 0.6\nlg_clf.n_estimators = 2100\nlg_clf.subsample_for_bin = 987\nlg_clf.min_child_samples = 5\nlg_clf.colsample_bytree = 0.9\nlg_clf.reg_alpha = 0.0\nlg_clf.reg_lambda = 0.0\nparameters = {\n#     'boosting_type': ['gbdt', 'goss'],\n#     'num_leaves': [34, 50, 55, 60, 89],\n#     'max_depth': [13, 21, 34],\n#     'learning_rate': [0.5, 0.6, 0.7],\n#     'n_estimators': [1900, 2000, 2100],\n#     'subsample_for_bin': [610, 987, 1597],\n#     'min_child_samples': [3, 5, 8, 20],\n#     'colsample_bytree': [0.85, 0.9, 0.95],\n#     'reg_alpha': [0.0, 0.7],\n#     'reg_lambda': [0.0, 0.4]\n}\n# clf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\n# clf.fit(X_train, y_train)\n# plot_grid_search(clf)\n# table_grid_search(clf)","ae16f469":"lg_clf = LGBMClassifier(verbosity=VERBOSE,\n                        random_state=RANDOM_STATE,\n                        n_jobs=N_JOBS)\nlg_clf.min_child_weight = 1e-15\nlg_clf.boosting_type = 'gbdt'\nlg_clf.num_leaves = 55\nlg_clf.max_depth = 21\nlg_clf.learning_rate = 0.6\nlg_clf.n_estimators = 2100\nlg_clf.subsample_for_bin = 987\nlg_clf.min_child_samples = 5\nlg_clf.colsample_bytree = 0.9\nlg_clf.reg_alpha = 0.0\nlg_clf.reg_lambda = 0.0\n# parameters = {\n#     'max_depth': [15, 20, 25],\n# }\nparameters = {\n    'boosting_type': ['gbdt', 'goss'],\n    'n_estimators': [500, 1000, 1500, 2000],\n    'num_leaves': [13, 21, 34, 55, 89, 144, 233, 377],\n    'learning_rate': [0.5, 0.6, 0.7],\n}\nclf = GridSearchCV(lg_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","fc21bd75":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","44b1491c":"clf.best_estimator_","e60b2f06":"# Grid search","151e103e":"More `max_depth`, greater the score, up to some point.\n21 hast the greatest score.","bb147ef9":"5 child samples has a greater score than the default value.","d432339e":"# importance_type\n##### : string, optional (default='split')\n\nThe type of feature importance to be filled into ``feature_importances_``.\nIf 'split', result contains numbers of times the feature is used in a model.\nIf 'gain', result contains total gains of splits which use the feature.","3d888637":"Subsample seems to have no influence over the score.","95b12ce0":"# Prepare data","f8bdcb15":"# class_weight\n##### : dict, 'balanced' or None, optional (default=None)\n\nWeights associated with classes in the form ``{class_label: weight}``.\n\nUse this parameter only for multi-class classification task;\nfor binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\nNote, that the usage of all these parameters will result in poor estimates of the individual class probabilities.\n\nYou may want to consider performing probability calibration\n(https:\/\/scikit-learn.org\/stable\/modules\/calibration.html) of your model.\n\nThe 'balanced' mode uses the values of y to automatically adjust weights\ninversely proportional to class frequencies in the input data as ``n_samples \/ (n_classes * np.bincount(y))``.\n\nIf None, all classes are supposed to have weight one.\n\nNote, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\nif ``sample_weight`` is specified.","e04a1822":"`colsample_bytree` increases the score from 0 to 0.7,\nwhich is a maximum, and then decays a litte.","a3a12784":"# subsample_freq\n##### : int, optional (default=0)\n\nFrequence of subsample, <=0 means no enable.","87ec0942":"## Export grid search results","f107e368":"# Introduction\n\nThe aim of this notebook is to optimize the Extra-trees model.\n\nFirst, all [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html#lightgbm.LGBMClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","174be623":"# min_split_gain\n##### : float, optional (default=0.)\n\nMinimum loss reduction required to make a further partition on a leaf node of the tree.","72af7e4c":"Subsample seems to have no influence over the score.","bba83a00":"`reg_lambda` as a chaotic behaviour. It has a maximum at 0.4.","ca421759":"The score and also the fit and score times increase with the number of estimators.\nThe greatest score is reached at 2000 estimators.","b2c72a53":"# num_leaves\n##### : int, optional (default=31)\n\nMaximum tree leaves for base learners.","52ac9615":"## First iteration","5709562e":"# boosting_type\n##### : string, optional (default='gbdt')\n\n- 'gbdt', traditional Gradient Boosting Decision Tree.\n- 'dart', Dropouts meet Multiple Additive Regression Trees.\n- 'goss', Gradient-based One-Side Sampling.\n- 'rf', Random Forest.","1ede6a81":"The score increases with the learning rate up to some point.\nFrom 0.5 begin to decay.\n`learning_rate` 0.5 has the greatest score.","912fd43b":"All the objectives has the same score.","c55bb229":"All child weight scores the same.\n\nBut someones have less score and fit time.\n\n`1e-15` has the least score and fit time.","690652d8":"# max_depth\n##### : int, optional (default=-1)\n\nMaximum tree depth for base learners, <=0 means no limit.","d5dbbd2a":"The sum of the improvements of parameters is not the improvements when joining all parameters together.\n\nThis score is below some separate improvement.\n\nThen, a manual sensibility analysis is done for each of fixed parameters to retune it.","f013b9ff":"# subsample\n##### : float, optional (default=1.)\n\nSubsample ratio of the training instance.","e340b98c":"# objective\n##### : string, callable or None, optional (default=None)\n\nSpecify the learning task and the corresponding learning objective or\na custom objective function to be used (see note below).\n\nDefault: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.","a1fd05bb":"# learning_rate\n##### : float, optional (default=0.1)\n\nBoosting learning rate.\n\nYou can use ``callbacks`` parameter of ``fit`` method to shrink\/adapt learning rate\nin training using ``reset_parameter`` callback.\n\nNote, that this will ignore the ``learning_rate`` argument in training.","7437d926":"**Note**: Not evaluated","933527df":"# min_child_weight\n##### : float, optional (default=1e-3)\n\nMinimum sum of instance weight (hessian) needed in a child (leaf).","7cec24b9":"# Exhaustive search","3b9757c6":"The default value has the greatest score.","03282186":"# min_child_samples\n##### : int, optional (default=20)\n\nMinimum number of data needed in a child (leaf).","5cb2d88a":"# colsample_bytree\n##### : float, optional (default=1.)\n\nSubsample ratio of columns when constructing each tree.","00e79b96":"Both importance types have the same score.","9eda317d":"# silent\n##### : bool, optional (default=True)\n\nWhether to print messages while running boosting.","94f8aef5":"# n_estimators\n##### : int, optional (default=100)\n\nNumber of boosted trees to fit.","3fe895fc":"The subsample optimum is reached at 987 samples.","4e004e6a":"As all the categories has the same number of samples,\n`balanced` and `None` options has the same result.","a26c0c1e":"## Second iteration","0c19f2ba":"`reg_alpha` as a descendend trend from 0 to 1,\nbut some chaotic.\n\nIt has a maximum at 0.7.","a146f8ce":"# reg_alpha\n##### : float, optional (default=0.)\n\nL1 regularization term on weights.","d754f781":"More number of leaves, greater the score, up to some point.\n144 leaves has the greatest score.\n","cb02702a":"# reg_lambda\n##### : float, optional (default=0.)\n\nL2 regularization term on weights.","b4022506":"# subsample_for_bin\n##### : int, optional (default=200000)\n\nNumber of samples for constructing bins.","b7c9804e":"# Search over parameters","f061dcf1":"The best boosting type is `gbdt`,\nfollowed by `goss`.\n`dart` is the lesser scored type and with greatest fit time."}}