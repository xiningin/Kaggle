{"cell_type":{"a876b85f":"code","19199dde":"code","cdfa5a2f":"code","c1067ba8":"code","f3367ce9":"code","f7d13a93":"code","d360d02e":"code","88c1c663":"code","ffa949a4":"code","c3f056e9":"code","a266f861":"code","d78ada29":"code","8d7d136d":"code","4797aa67":"code","8ba4b30d":"code","ee88c886":"code","d6654a3b":"code","6f2cc4aa":"code","a5a8a8ec":"code","b1a1e629":"code","362c29ec":"code","dbe02719":"code","ff10468a":"code","a2215e37":"code","bf97777f":"code","aa374095":"code","48be9123":"code","b12174e9":"code","ae5a9a49":"code","b7820fbf":"code","35c7947a":"code","31c9330e":"code","dd03841b":"code","3fe06c17":"code","1a9e2295":"code","8cba0cc5":"code","a5cab2ad":"code","85a94ec9":"code","d330d4ba":"code","ae934b59":"code","521da15b":"code","9ab96280":"code","607f5aeb":"code","df455e30":"code","fb402fb2":"code","bf86bafb":"code","5c93b443":"code","51c87573":"code","50dfb2f4":"code","8640778e":"code","290bfd05":"code","9867f62e":"code","5d652e47":"code","2663dd5e":"code","f53bf159":"code","fc0008c2":"code","16f2ad2a":"code","1b1e228d":"code","923b0b00":"code","18eaa4e4":"code","59daf9f2":"code","1d5367e7":"code","f650662a":"code","eb2a5015":"code","b4afe4c0":"code","28ba6e33":"code","a253a555":"code","2d2b8536":"code","d16f184b":"code","b78d5bbd":"code","85f9e3a9":"markdown","0d93a320":"markdown","e71fd56d":"markdown","be5446f6":"markdown","de1a124d":"markdown"},"source":{"a876b85f":"\"\"\"Columns\nid - a unique identifier for each tweet\ntext - the text of the tweet\nlocation - the location the tweet was sent from (may be blank)\nkeyword - a particular keyword from the tweet (may be blank)\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\"\"\"","19199dde":"#Importing all libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport missingno\n\n\n%matplotlib inline\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,CategoricalNB\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection","cdfa5a2f":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest= pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","c1067ba8":"#No of rows and columns\ntrain.shape","f3367ce9":"#Prints first 5 rows\ntrain.head(5)","f7d13a93":"train[\"location\"].value_counts()","d360d02e":"train.info()","88c1c663":"train.describe()","ffa949a4":"#checking for null values\ntrain.isnull().sum()","c3f056e9":"test.isnull().sum()","a266f861":"#Remove redundant samples\ntrain=train.drop_duplicates(subset=['text', 'target'], keep='first')\ntrain.shape","d78ada29":"#plot graph for missing values\n\nmissingno.matrix(train, figsize = (5,5))","8d7d136d":"#location is having most number of null values","4797aa67":"train.isna().sum().plot(kind=\"bar\")\nplt.title(\"no of null values in train data\")\nplt.show()","8ba4b30d":"#Location with maximum null values\n#Keyword follows Location with null values","ee88c886":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","d6654a3b":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","6f2cc4aa":"# drop location and keyword column\ntrain = train.drop(['location','keyword'],axis=1)\ntest = test.drop(['location','keyword'],axis=1)","a5a8a8ec":"train.head(5)","b1a1e629":"#Let check how many real tweets and fake tweets\n\ntweetreal = len(train[train[\"target\"]==1])","362c29ec":"#percentage of real tweets\nRealTweetPercentage = tweetreal\/train.shape[0]*100\nRealTweetPercentage","dbe02719":"#Percentage of fake tweet\nFakeTweetPercentage = 100-RealTweetPercentage\nFakeTweetPercentage","ff10468a":"#plot target variables\nsns.countplot(x ='target', data= train)","a2215e37":"#Now lets understand the density of tweets in both test and train dataset.\n\nden_train = train['text'].str.len()\nden_test = test['text'].str.len()\n\nplt.hist(den_train, label = \"train_tweets\")\nplt.hist(den_test, label= \"text_tweets\")","bf97777f":"#So here train data is having more tweets compared to the test data.","aa374095":"#Fetch wordcount for each abstract\ntrain['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\ntrain[['text','word_count']].head(10)","48be9123":"##Descriptive statistics of word counts\ntrain.word_count.describe()\n\n#The average word count is about 15 words per abstract. The word count ranges from a minimum of 1 to a maximum of 54.","b12174e9":"#Identify common words: Its the frequently used words as well as it could be potential data specific stop words.\nimport pandas as pd\n\nfreq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\nfreq","ae5a9a49":"#Identify uncommon words: Uncommon words in the train dataset\nfreq1 =  pd.Series(' '.join(train ['text']).split()).value_counts()[-20:]\nfreq1","b7820fbf":"#Disaster tweet\n\ndisaster_tweets = train[train['target'] ==1 ]['text']\nfor i in range(1,10):\n    print(disaster_tweets[i])","35c7947a":"# non-disaster tweets\nnon_disaster_tweets = train[train['target'] !=1 ]['text']\nnon_disaster_tweets","31c9330e":"#wordcloud\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=20);\n\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=20);\n","dd03841b":"train.head(5)","3fe06c17":"import nltk\nnltk.download('stopwords')","1a9e2295":"#Normalization is the method of handling multiple occurances of the same word\n#Stemming normalizes text by removing suffixes.\n#Lemmatisation is a more advanced technique which works based on the root of the word.\n","8cba0cc5":"#After removing all the redundant values lets check te counts\ntrain.target.value_counts()","a5cab2ad":"stopwords.words('english')","85a94ec9":"#List of punctuations and we will remove them from our corpus\nimport string\nstring.punctuation","d330d4ba":"#Cleaning data","ae934b59":"def clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","521da15b":"train['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","9ab96280":"train['text'].head()","607f5aeb":"tweets = train[\"text\"]","df455e30":"#Tokenize\n\n\ntokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x:tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x:tokenizer.tokenize(x))\ntrain['text'].head()","fb402fb2":"#Removing stopwords\n\n\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words \ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntest.head()","bf86bafb":"disaster_tweets = train[train['target'] ==1 ]['text']\nfor i in range(1,10):\n    print(disaster_tweets[i])","5c93b443":"# non-disaster tweets\nnon_disaster_tweets = train[train['target'] !=1 ]['text']\nnon_disaster_tweets.head()","51c87573":"#Lemmatization\n\nnltk.download()\nimport nltk\nnltk.download('averaged_perceptron_tagger')","50dfb2f4":"import nltk\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","8640778e":"# lemmatization\nlem = WordNetLemmatizer()\ndef lem_word(x):\n    return [lem.lemmatize(w) for w in x]\n\n    ","290bfd05":"train['text'] = train['text'].apply(lem_word)\ntest['text'] = test['text'].apply(lem_word)","9867f62e":"def combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","5d652e47":"from nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\nstem = PorterStemmer()\n\nnltk.download()","2663dd5e":"count_vectorizer = CountVectorizer()\ntrain_vector = count_vectorizer.fit_transform(train['text'])\ntest_vector = count_vectorizer.transform(test['text'])\nprint(train_vector[0].todense())","f53bf159":"#TF IDF\n\ntfidf = TfidfVectorizer(min_df = 2,max_df = 0.5,ngram_range = (1,2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test['text'])\n\n","fc0008c2":"test_tfidf","16f2ad2a":"mnb = MultinomialNB(alpha = 2.0)\nscores_vector = model_selection.cross_val_score(mnb,train_vector,train['target'],cv = 10,scoring = 'f1')\nprint(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(mnb,train_tfidf,train['target'],cv = 10,scoring = 'f1')\nprint(\"score of tfidf:\",scores_tfidf)","1b1e228d":"mnb.get_params()","923b0b00":"#Logistic Regression","18eaa4e4":"lg = LogisticRegression(C = 1.0)\nscores_vector = model_selection.cross_val_score(lg, train_vector, train[\"target\"], cv = 5, scoring = \"f1\")\nprint(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(lg, train_tfidf, train[\"target\"], cv = 5, scoring = \"f1\")\nprint(\"score of tfidf:\",scores_tfidf)","59daf9f2":"lg.get_params()","1d5367e7":"mnb.fit(train_tfidf, train[\"target\"])\ny_pred = mnb.predict(test_tfidf)","f650662a":"y_pred","eb2a5015":"submission_file = pd.DataFrame({'Id':test['id'],'target':y_pred})","b4afe4c0":"submission_file.to_csv('submission_file.csv',index=False)","28ba6e33":"submission_file = pd.read_csv('submission_file.csv')","a253a555":"submission_file.head(10)","2d2b8536":"\"\"\"Text in the corpus needs to be converted to a format that can be interpreted by the machine learning algorithms. There are 2 parts of this conversion \u2014 Tokenisation and Vectorisation.\"\"\"","d16f184b":"\"\"\"Tokenisation is the process of converting the continuous text into a list of words. The list of words is then converted to a matrix of integers by the process of vectorisation. Vectorisation is also called feature extraction.\"\"\"","b78d5bbd":"\"\"\"For text preparation we use the bag of words model which ignores the sequence of the words and only considers word frequencies.\"\"\"","85f9e3a9":"We have 92 redundants sapmles in our dataset (7613-7521)=92\n\n---\n\n","0d93a320":"Text preparation","e71fd56d":"Now lets do **Text-preprocessing**:\n1. Reduce sparsity\n2. Text clean-up\n3. Shrinkage the vocabulary to retain only the relevant words\n\nText preprocessing\n1: Noise Removal\n\n      a: Removing redundant text components\n\n      b: Punctuations, Tags, URL, stopwords\n\n2: Normalization\n\n      a: Stemming - Remove suffixes\n      b: Lemmatization- Works based on the roots of the word \n\n\n\n\n","be5446f6":"Cleaning","de1a124d":"Submission"}}