{"cell_type":{"6b087e54":"code","cc071b5d":"code","6c9000ff":"code","6c9669c8":"code","2b9ad4c3":"code","812707bb":"code","d58dfefe":"code","6d20ab76":"code","42094229":"code","ad079db2":"code","4ae91a9c":"code","3d93372f":"code","0b50d7d8":"code","6f85dc00":"code","c608feff":"code","f1dc1625":"code","2abc5012":"code","fa2c8f08":"code","75935437":"code","486de7f0":"code","b6f9536d":"code","a293f13f":"code","4420362b":"code","11cea572":"markdown","ea113fea":"markdown","3fc2a566":"markdown","3e86c98a":"markdown","068ec932":"markdown","9967bde0":"markdown","713fdde4":"markdown","2d837848":"markdown","9270d772":"markdown","83f6ba8e":"markdown","49a25b62":"markdown","c1e0236c":"markdown","4a13e2f3":"markdown","a7f14721":"markdown"},"source":{"6b087e54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc071b5d":"# import logging\nimport json\nimport re\n\n# JSON formatting functions\ndef convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n    training_data = []\n    lines=[]\n    with open(dataturks_JSON_FilePath, 'r') as f:\n        lines = f.readlines()\n\n    for line in lines:\n        data = json.loads(line)\n        text = data['content'].replace(\"\\n\", \" \")\n        entities = []\n        data_annotations = data['annotation']\n        if data_annotations is not None:\n            for annotation in data_annotations:\n                #only a single point in text annotation.\n                point = annotation['points'][0]\n                labels = annotation['label']\n                # handle both list of labels or a single label.\n                if not isinstance(labels, list):\n                    labels = [labels]\n\n                for label in labels:\n                    point_start = point['start']\n                    point_end = point['end']\n                    point_text = point['text']\n\n                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n                    if lstrip_diff != 0:\n                        point_start = point_start + lstrip_diff\n                    if rstrip_diff != 0:\n                        point_end = point_end - rstrip_diff\n                    entities.append((point_start, point_end + 1 , label))\n        training_data.append((text, {\"entities\" : entities}))\n    return training_data\n\ndef trim_entity_spans(data: list) -> list:\n    \"\"\"Removes leading and trailing white spaces from entity spans.\n\n    Args:\n        data (list): The data to be cleaned in spaCy JSON format.\n\n    Returns:\n        list: The cleaned data.\n    \"\"\"\n    invalid_span_tokens = re.compile(r'\\s')\n\n    cleaned_data = []\n    for text, annotations in data:\n        entities = annotations['entities']\n        valid_entities = []\n        for start, end, label in entities:\n            valid_start = start\n            valid_end = end\n            while valid_start < len(text) and invalid_span_tokens.match(\n                    text[valid_start]):\n                valid_start += 1\n            while valid_end > 1 and invalid_span_tokens.match(\n                    text[valid_end - 1]):\n                valid_end -= 1\n            valid_entities.append([valid_start, valid_end, label])\n        cleaned_data.append([text, {'entities': valid_entities}])\n    return cleaned_data","6c9000ff":"data = trim_entity_spans(convert_dataturks_to_spacy(\"..\/input\/resume-entities-for-ner\/Entity Recognition in Resumes.json\"))\ndata[0]","6c9669c8":"# def clean_entities(training_data):\n    \n#     clean_data = []\n#     for text, annotation in training_data:\n        \n#         entities = annotation.get('entities')\n#         entities_copy = entities.copy()\n        \n#         # append entity only if it is longer than its overlapping entity\n#         i = 0\n#         for entity in entities_copy:\n#             j = 0\n#             for overlapping_entity in entities_copy:\n#                 # Skip self\n#                 if i != j:\n#                     e_start, e_end, oe_start, oe_end = entity[0], entity[1], overlapping_entity[0], overlapping_entity[1]\n#                     # Delete any entity that overlaps, keep if longer\n#                     if ((e_start >= oe_start and e_start <= oe_end) \\\n#                     or (e_end <= oe_end and e_end >= oe_start)) \\\n#                     and ((e_end - e_start) <= (oe_end - oe_start)):\n#                         entities.remove(entity)\n#                 j += 1\n#             i += 1\n#         clean_data.append((text, {'entities': entities}))\n                \n#     return clean_data\n\n# data = clean_entities(data)","2b9ad4c3":"!pip install spacy==2.1.4","812707bb":"from spacy.lang.en import English  # Or whichever language you need\nfrom spacy.gold import biluo_tags_from_offsets\n\ndef bilou_tags(data):\n    \n    docs  = []\n    annots = []\n    nlp = English()\n    for text, annotations in data:\n        offsets = annotations[\"entities\"]\n        doc = nlp(text)\n        tags = biluo_tags_from_offsets(doc, offsets)\n        for i in range(len(tags)):\n            if tags[i].startswith(\"U\"):\n                tags[i] = \"B\" + tags[i][1:]\n            elif tags[i].startswith(\"L\"):\n                tags[i] = \"I\" + tags[i][1:]\n            if not (doc[i].text.isalnum() or len(doc[i].text) > 1):\n                tags[i] = \"O\"\n        docs.append([token.text for token in doc])\n        annots.append(tags)\n        \n    df_data = pd.DataFrame({'docs': docs, 'annots': annots})\n\n    return df_data\n\ndf_data = bilou_tags(data)\n# [(k, v) for k, v in zip(df_data[\"docs\"][0], df_data[\"annots\"][0])]","d58dfefe":"for i in range(len(df_data)):\n    if \"-\" in df_data.loc[i, \"annots\"]:\n        df_data.drop(i, axis = \"index\", inplace = True)\ndf_data.reset_index(inplace = True)\nlen(df_data)","6d20ab76":"from nltk import pos_tag\nsentences = [[(w, p, t) for w, p, t in zip(df_data[\"docs\"][i], [y for x, y in pos_tag(df_data[\"docs\"][i])], df_data[\"annots\"][i]) if w.isalnum() or len(w) > 1] for i in range(0, len(df_data))]","42094229":"def word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n    \n    features = {\n        'bias': 1.0, \n        'word.lower()': word.lower(), \n        'word[-3:]': word[-3:],\n        'word[-2:]': word[-2:],\n        'word.isupper()': word.isupper(),\n        'word.istitle()': word.istitle(),\n        'word.isdigit()': word.isdigit(),\n        'postag': postag,\n        'postag[:2]': postag[:2]\n    }\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.update({\n            '-1:word.lower()': word1.lower(),\n            '-1:word.istitle()': word1.istitle(),\n            '-1:word.isupper()': word1.isupper(),\n            '-1:postag': postag1,\n            '-1:postag[:2]': postag1[:2]\n        })\n    else:\n        features['BOS'] = True\n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.update({\n            '+1:word.lower()': word1.lower(),\n            '+1:word.istitle()': word1.istitle(),\n            '+1:word.isupper()': word1.isupper(),\n            '+1:postag': postag1,\n            '+1:postag[:2]': postag1[:2]\n        })\n    else:\n        features['EOS'] = True\n    return features\ndef sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\ndef sent2labels(sent):\n    return [label for token, postag, label in sent]\ndef sent2tokens(sent):\n    return [token for token, postag, label in sent]","ad079db2":"%%time\nfrom sklearn.model_selection import train_test_split\n\nX = [sent2features(s) for s in sentences]\ny = [sent2labels(s) for s in sentences]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)","4ae91a9c":"!pip install python-crfsuite","3d93372f":"import pycrfsuite","0b50d7d8":"%%time\n\ntrainer = pycrfsuite.Trainer(verbose = True)\n\nfor xseq, yseq in zip(X_train, y_train):\n    trainer.append(xseq, yseq)","6f85dc00":"trainer.set_params({\n    'c1': 1.0,   # coefficient for L1 penalty\n    'c2': 1e-3,  # coefficient for L2 penalty\n    'max_iterations': 100,  # stop earlier\n\n    # include transitions that are possible, but not observed\n    'feature.possible_transitions': True\n})","c608feff":"trainer.params()","f1dc1625":"%%time\ntrainer.train('resume-ner.crfsuite')","2abc5012":"trainer.logparser.last_iteration","fa2c8f08":"tagger = pycrfsuite.Tagger()\ntagger.open('.\/resume-ner.crfsuite')","75935437":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom itertools import chain\n\ndef ner_report(y_true, y_pred):\n    \"\"\"\n    Classification report for a list of BIO-encoded sequences.\n    It computes token-level metrics and discards \"O\" labels.\n    \n    Note that it requires scikit-learn 0.15+ (or a version from github master)\n    to calculate averages properly!\n    \"\"\"\n    lb = LabelBinarizer()\n    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n        \n    tagset = set(lb.classes_)\n    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n    \n    return classification_report(\n        y_true_combined,\n        y_pred_combined,\n        labels = [class_indices[cls] for cls in tagset],\n        target_names = tagset\n    ), accuracy_score(y_true_combined, y_pred_combined)","486de7f0":"%%time\ny_pred = [tagger.tag(xseq) for xseq in X_test]","b6f9536d":"report, accuracy = ner_report(y_test, y_pred)","a293f13f":"print(report)","4420362b":"print(accuracy)","11cea572":"## Modeling","ea113fea":"### Processing Indexes","3fc2a566":"#### Feature Extraction","3e86c98a":"#### Train-Test Split","068ec932":"### Entity Mapping","9967bde0":"### Conditional Random Fields","713fdde4":"### Removing Mislabeled Examples","2d837848":"#### Evaluation","9270d772":"## Dataset","83f6ba8e":"## Cleaning Entities","49a25b62":"# Named Entity Recognition Dataset","c1e0236c":"### Overlapping Entities","4a13e2f3":"#### Training","a7f14721":"#### Sentence Getter"}}