{"cell_type":{"74ecc72c":"code","f57f1fc8":"code","597e358e":"code","fee69007":"code","d69ef700":"code","0809e53e":"code","8cd79531":"code","bcdc4328":"code","2d6bdb95":"code","debdf25c":"code","5e8b6d4b":"code","e8bf773c":"code","3b04e081":"code","1b4a8862":"code","2a265469":"code","c35ba366":"code","2c197f0c":"code","37069e01":"code","e7fd5a82":"code","d99d054a":"code","981314fb":"code","6a7113bd":"code","42489ca0":"code","9b9a97d4":"code","3607078d":"code","bb8c7b4e":"code","6806602a":"code","1b2903bb":"code","2d6db672":"code","f0dabd6f":"code","4f9ab75a":"code","c4876932":"markdown","484402fd":"markdown","208dc5cf":"markdown","e7e82ef6":"markdown","84d42eea":"markdown","d7c0a369":"markdown"},"source":{"74ecc72c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f57f1fc8":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\nimport pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n%matplotlib inline","597e358e":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n","fee69007":"data.head(10)","d69ef700":"data.dtypes","0809e53e":"data[\"Outcome\"].value_counts()","8cd79531":"data.describe()","bcdc4328":"plt.figure(figsize=(7, 5))\nsns.countplot(data=data, x='Outcome')\nplt.show()","2d6bdb95":"data.shape\n","debdf25c":"# check if any null value is present\ndata.isnull().values.any()","5e8b6d4b":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True)","e8bf773c":"data.corr()","3b04e081":"Y=data[\"Outcome\"]\nY","1b4a8862":"X=data.drop(\"Outcome\",axis=1)","2a265469":"X.head(10)","c35ba366":"X=X.astype(float)","2c197f0c":"X.dtypes","37069e01":"X.head(10)","e7fd5a82":"lst=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\"]","d99d054a":"for i in lst:\n  tem=X[i]\n  mean=X[i].mean()\n  for j in range(len(tem)):\n    if tem[j]==0:\n      tem[j]=mean\n  X.replace(i,tem,inplace=True)\n\n  \n  ","981314fb":"X.head(10)","6a7113bd":"model = Sequential()\nmodel.add(Dense(64, input_dim=8, activation='relu',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(32,activation='relu',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(16,activation='relu',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","42489ca0":"history=model.fit(X, Y,validation_split=0.33, epochs=300)","9b9a97d4":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper left')","3607078d":"from sklearn import preprocessing\n\nx = X.values \nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nx_transformed = pd.DataFrame(x_scaled)","bb8c7b4e":"model_2 = Sequential()\nmodel_2.add(Dense(64, input_dim=8, activation='relu',kernel_initializer='glorot_uniform'))\nmodel_2.add(Dense(32,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_2.add(Dense(16,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_2.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","6806602a":"history_2=model_2.fit(x_transformed, Y,validation_split=0.33, epochs=100)","1b2903bb":"plt.plot(history_2.history['loss'])\nplt.plot(history_2.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper left')","2d6db672":"model_3 = Sequential()\nmodel_3.add(Dense(64, input_dim=8, activation='relu',kernel_initializer='glorot_uniform'))\nmodel_3.add(Dense(32,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_3.add(Dense(16,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_3.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","f0dabd6f":"history_3=model_3.fit(x_transformed, Y,validation_split=0.33, epochs=100, batch_size=20)","4f9ab75a":"plt.plot(history_3.history['loss'])\nplt.plot(history_3.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper left')","c4876932":"As we can see here that using batch propagation is not much helpful in small datasets","484402fd":"We can clearly see that after noramlising our dataset we need much less epochs (less than 50) and we also get much higher accuracy. ","208dc5cf":"NOW LETS NORMALISE OUR DATASET ANT THEN FEED IT INTO OUR MODEL","e7e82ef6":"INTRODUCING BATCH SIZE  ","84d42eea":"# Beginner's Level Notebook \n## The following notebook is written only to practice the implementation of ANN on a basic dataset. \n## Basic hyperparameter optimisation is discussed.\n","d7c0a369":"As we can see epochs less than 300 are sufficient"}}