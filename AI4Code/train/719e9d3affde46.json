{"cell_type":{"7a12ec18":"code","5a3b5cb4":"code","0e0ca7b9":"code","b771dafc":"code","1f9d8ffb":"code","cba38113":"code","7b932192":"code","8123e123":"code","8f927cf3":"code","e88dbd03":"code","a3e8fa6d":"code","fc355e27":"code","58aeec8e":"code","26bf13ab":"code","124d30dd":"code","21d665e9":"code","a781ef22":"code","8c3f5a2a":"code","072d2b54":"code","dcb0a710":"code","72b1ccb4":"code","74938fef":"code","6c8f3583":"code","59bba5ab":"code","456efad6":"code","e0c5f54b":"markdown","e50a1451":"markdown","04d0c601":"markdown","9446b7e7":"markdown","121c236b":"markdown","7cdd7c25":"markdown","f516f759":"markdown","7cd2b5a4":"markdown","df032177":"markdown","c5d4778c":"markdown","cff99627":"markdown","62dd6d58":"markdown","6569420e":"markdown","4b5bd9ba":"markdown","1d67e856":"markdown","f35f4486":"markdown","97d8eee6":"markdown","06969cf5":"markdown","19f5f7c3":"markdown","0a533d27":"markdown","1ffe87bb":"markdown","11271482":"markdown","c28c8e9d":"markdown","d73f7dc6":"markdown","21005fee":"markdown","2905cc71":"markdown","d0dc566b":"markdown","7bbc7d64":"markdown","409cd41a":"markdown","0f69463d":"markdown","95ac0d8d":"markdown","c56aab22":"markdown","119256c1":"markdown","7b9bf5e5":"markdown","ca2feeaa":"markdown","4d1b23d7":"markdown","2f522f3f":"markdown","44fb6046":"markdown","256f3e98":"markdown","46dc64fe":"markdown","664b6483":"markdown","8d51050e":"markdown","a1ec2312":"markdown","8cab650f":"markdown","37fbf2bf":"markdown","8f424f6c":"markdown"},"source":{"7a12ec18":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score","5a3b5cb4":"to_remove = ['school', 'sex', 'age', 'Mjob', 'Fjob', 'reason',\n             'guardian', 'paid', 'activities', 'nursery', 'higher',\n             'internet', 'romantic', 'freetime', 'goout', 'Dalc', 'Walc']","0e0ca7b9":"data = pd.read_csv('..\/input\/acamedicperfomance\/student_math.csv', delimiter=';')\n\nprint(data.head())\ndata = data.drop(to_remove, axis=1)","b771dafc":"print(data.head())","1f9d8ffb":"sns.set(style='ticks', context='talk')\nsns.set(font_scale=2)","cba38113":"sns.barplot(x='address', y='G3', data=data)","7b932192":"plt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nplt.subplot(1, 2, 1)\nsns.barplot(x='famsize', y='G3', data=data)\nplt.xlabel('Family size', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nplt.subplot(1, 2, 2)\nsns.barplot(x='Pstatus', y='G3', data=data)\nplt.xlabel('Parents status', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.tight_layout()\n\nplt.show()","8123e123":"plt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nplt.subplot(1, 2, 1)\nsns.barplot(x='Medu', y='G3', data=data)\nplt.xlabel(\"Mother's education\", fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nplt.subplot(1, 2, 2)\nsns.barplot(x='Fedu', y='G3', data=data)\nplt.xlabel(\"Father's education\", fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.tight_layout()\n\nplt.show()","8f927cf3":"plt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nplt.subplot(1, 2, 1)\nsns.barplot(x='traveltime', y='G3', data=data)\nplt.xlabel('travel time', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nplt.subplot(1, 2, 2)\nsns.barplot(x='studytime', y='G3', data=data)\nplt.xlabel('Study time', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n","e88dbd03":"sns.barplot(x='failures', y='G3', data=data)\nplt.xlabel('Failures', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.show()","a3e8fa6d":"plt.subplot(1, 2, 1)\nsns.barplot(x='schoolsup', y='G3', data=data)\nplt.xlabel('School support', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.subplot(1, 2, 2)\nsns.barplot(x='famsup', y='G3', data=data)\nplt.xlabel('Family support', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","fc355e27":"plt.subplot(1, 2, 1)\nsns.barplot(x='famrel', y='G3', data=data)\nplt.xlabel('Family relationship', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\nplt.subplot(1, 2, 2)\nsns.barplot(x='health', y='G3', data=data)\nplt.xlabel('Health', fontsize=15, fontweight='bold')\nplt.ylabel('G3', fontsize=15, fontweight='bold')\n\n\nplt.tight_layout()\nplt.show()","58aeec8e":"sns.barplot(x='G3', y='absences', data=data)\nplt.xlabel('G3', fontsize=25, fontweight='bold')\nplt.ylabel('Absences', fontsize=25, fontweight='bold')\n\nplt.tight_layout()\nplt.show()","26bf13ab":"plt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nsns.barplot(x='G1', y='G3', data=data)\nplt.xlabel('G1', fontsize=20, fontweight='bold')\nplt.ylabel('G3', fontsize=20, fontweight='bold')\n\nplt.show()\n\nplt.rc('xtick', labelsize=15)\nplt.rc('ytick', labelsize=15)\nsns.barplot(x='G2', y='G3', data=data)\nplt.xlabel('G2', fontsize=20, fontweight='bold')\nplt.ylabel('G3', fontsize=20, fontweight='bold')\n\nplt.show()","124d30dd":"x = data.iloc[:, :-1].values     \ny = data.iloc[:, -1].values","21d665e9":"ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0, 1, 2, 8, 9])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))","a781ef22":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)","8c3f5a2a":"regressor = LinearRegression()\nregressor.fit(x_train, y_train)","072d2b54":"y_pred = regressor.predict(x_test)\n\nfor i in range(len(y_pred)):\n    print(\"Y_pred: {}, x_test: {}, y_test: {}\".format(y_pred[i], x_test[i], y_test[i]))","dcb0a710":"print(\"constants: \\n\", regressor.coef_)         # The slope\nprint(\"intercepts: \\n\", regressor.intercept_)   # The intercept points\n\nprint(r2_score(y_pred, y_test))","72b1ccb4":"accuracies = cross_val_score(estimator=regressor, X=x_train, y=y_train, cv=10)\n\nprint('Linear regression')\n\nprint(\"Accuracy with k_fold cross validation: {:.2f} %\".format(accuracies.mean() * 100))\nprint(\"Standard deviation with k_fold cross validation: {:.2f} %\".format((accuracies.std() * 100)))\n","74938fef":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=5)\nx_poly = poly_reg.fit_transform(x_train)\n\nregressor = LinearRegression()\nregressor.fit(x_poly, y_train)\n\ny_pred = regressor.predict(poly_reg.transform(x_test))\n\naccuracies = cross_val_score(estimator=regressor, X=x_train, y=y_train, cv=10)\n\nprint('Polynomial regression')\n\nprint(\"Accuracy with k_fold cross validation: {:.2f} %\".format(accuracies.mean() * 100))\nprint(\"Standard deviation with k_fold cross validation: {:.2f} %\".format((accuracies.std() * 100)))\n\n","6c8f3583":"# training the decision tree model\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor(random_state=0)\nregressor.fit(x_train, y_train)\n\n\n# Predicting the new results\n\ny_pred = regressor.predict(x_test)\n\naccuracies = cross_val_score(estimator=regressor, X=x_train, y=y_train, cv=10)\n\nprint('Decision tree')\n\nprint(\"Accuracy with k_fold cross validation: {:.2f} %\".format(accuracies.mean() * 100))\nprint(\"Standard deviation with k_fold cross validation: {:.2f} %\".format((accuracies.std() * 100)))","59bba5ab":"# training the decision tree model\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=10, random_state=0)\nregressor.fit(x_train, np.ravel(y_train))\n\n\n# Predicting the new results\n\ny_pred = regressor.predict(x_test)\n\naccuracies = cross_val_score(estimator=regressor, X=x_train, y=y_train, cv=10)\n\nprint('Decision tree')\n\nprint(\"Accuracy with k_fold cross validation: {:.2f} %\".format(accuracies.mean() * 100))\nprint(\"Standard deviation with k_fold cross validation: {:.2f} %\".format((accuracies.std() * 100)))","456efad6":"# training the XG boost model\n\nfrom xgboost import XGBRegressor\nregressor = XGBRegressor()\nregressor.fit(x_train, y_train)\n\n\n# Predicting the new results\n\ny_pred = regressor.predict(x_test)\n\n\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator=regressor, X=x_train, y=y_train, cv=10)\n\nprint('XG boost')\n\nprint(\"Accuracy with k_fold cross validation: {:.2f} %\".format(accuracies.mean() * 100))\nprint(\"Standard deviation with k_fold cross validation: {:.2f} %\".format((accuracies.std() * 100)))","e0c5f54b":"The above two plots shows that less travela time and more study time gives better results and less failures gives better results. ","e50a1451":"There are some string values that needs to be encoded, the columns that contains strings are binary values.","04d0c601":"School support and family support","9446b7e7":"Travel time and study time","121c236b":"We can clearly identify that the greater the scores of G1 and G2 the greater is G3.","7cdd7c25":"To improve the model accuracies, we can try to delete some more columns or to include them.","f516f759":"First let us remove the unwanted columns or, in other words which does not have a considerable effect on the desired results for both training and predictions.","7cd2b5a4":"### **Some obvious features that can be considered from the dataset.**","df032177":"\n\n---\n\n---\n\n\n\n","c5d4778c":"The regression model is trained by fitting it with x_train and y_train","cff99627":"First we have to separate the data into independent variables (x) and dependent variables (y)","62dd6d58":"# **Importing the Dataset**","6569420e":"Polynomial regression","4b5bd9ba":"## **Training the regression models on the training set**","1d67e856":"\n\n---\n\n","f35f4486":"# **Importing the libraries**","97d8eee6":"The coming up plots are the features which does not depend much on the student","06969cf5":"The first thing we see from the above plot is that we have larger score for 0, the reason is that the data does not contain much of 0 education values and if either Father or mother has value 0 the other parent will have a greater value.","19f5f7c3":"\n\n---\n\n","0a533d27":"XG boost with k-fold cross validation","1ffe87bb":"The constants and intercepts of formula of regression and the r2 score","11271482":"The *intersection* drops G3 column except all the columns whereas the *difference* drops all the columns except G3.","c28c8e9d":"linear regression - for each model the training data consists of 80% and test data consists of 20% from the dataset.","d73f7dc6":"From the data it seems like family relationship and health really did not matter for these students for their scores.","21005fee":"Lets us consider G1 and G2, because these two values are the results of all the above considered features and G can also be predicited by training with just G1 and G2.","2905cc71":"### **Splitting the dataset into test and train set**","d0dc566b":"**Thank you for your time, I know I can improve my approach by learning more deeply, I am very happy to welcome all of your comments and suggestions.**","7bbc7d64":"Plotting the relationship between each feature considered with the dependent variable (G3) and analysing the impact of different classes in that feature.","409cd41a":"# **Analysing the data using barplots**","0f69463d":"Random forest regrssion model","95ac0d8d":"The student's address (Urban and Rural) ","c56aab22":"\n\n---\n\n","119256c1":"Making a list of all the column titles to remove which will be used later. This list was previously analysed and concluded as not important.","7b9bf5e5":"\n\n---\n\n","ca2feeaa":"Mother's education and Father's education","4d1b23d7":"Family relationship and health of an individual student","2f522f3f":"Predcting the test results from the test set and verifying it with the original test set results.","44fb6046":"Decision tree model has a very poor results in this case","256f3e98":"Using Pandas we import the dataset in CSV foramt, but the the delimiter is not a comma (,) but a semicolon (;).","46dc64fe":"There is no much difference between Rural and Urban students even rural has less scores, when we consider a huge data it does not really matter.","664b6483":"Total number of failures of a student","8d51050e":"From the above plot we can clearly see that less absences leads to a better scoring","a1ec2312":"# ## **Using Regression for training and prediction**","8cab650f":"Family size and parents status","37fbf2bf":"K-fold cross validation for accuracy and standard deviation of the model, k_fold cross validation uses different set of test data for the validation, the number of test folds is equal to the *cv* parameter.","8f424f6c":"Absences"}}