{"cell_type":{"5fc323f0":"code","8a816304":"code","71a2339b":"code","00875563":"code","923f4c4c":"code","bc43d933":"code","cc68a8c9":"code","5b3f3534":"code","81d99690":"code","325ea7f4":"markdown","44662ff1":"markdown","dfb51957":"markdown","2cdd0b78":"markdown","89280818":"markdown","757d7e12":"markdown","3e8da491":"markdown","c156c55e":"markdown"},"source":{"5fc323f0":"# Make sure to use tensorflow.keras and not keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Input, Flatten\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\n# Paths to train and test images\ntrain_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train'\nval_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/val'\ntest_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/test'","8a816304":"target_size = (224, 224)\ncolormode = 'rgb'\nseed = 666\nbatch_size = 64\n\n# Training ImageDataGenerator will have data augumentation parameters.\ntrain_datagen = ImageDataGenerator(rescale = 1.0\/255.0,\n                             rotation_range=5,\n                             width_shift_range=0.2,\n                             height_shift_range=0.2,\n                             zoom_range=0.1)\n\nval_datagen = ImageDataGenerator(rescale = 1.0\/255.0)\ntest_datagen = ImageDataGenerator(rescale = 1.0\/255.0)\n\n\n# Creating training, validation and test generators\ntrain_generator = train_datagen.flow_from_directory(directory = train_path, \n                                             target_size = target_size, \n                                             color_mode = colormode, \n                                             batch_size = batch_size,\n                                             class_mode = 'binary',\n                                             shuffle = True,\n                                             seed = seed)\n\nvalid_generator = val_datagen.flow_from_directory(directory = val_path,\n                                             target_size = target_size,\n                                             color_mode = colormode,\n                                             batch_size = batch_size,\n                                             class_mode = 'binary',\n                                             shuffle = True,\n                                             seed = seed)\n\ntest_generator = test_datagen.flow_from_directory(directory = test_path,\n                                            target_size = target_size,\n                                            color_mode = colormode,\n                                            batch_size = 1,\n                                            class_mode = 'binary',\n                                            shuffle = False, \n                                            seed = seed)\n\n# Define number of steps for fit_generator function\nSTEP_SIZE_TRAIN = train_generator.n \/\/ train_generator.batch_size\nSTEP_SIZE_VALID = valid_generator.n \/\/ valid_generator.batch_size\nSTEP_SIZE_TEST=test_generator.n\/\/test_generator.batch_size","71a2339b":"base_model = keras.applications.VGG16(include_top=False, weights='imagenet',input_shape = (224,224,3))\nx = keras.layers.Flatten() (base_model.output)\nx = keras.layers.Dense(256, activation=\"relu\")(x)\nx = keras.layers.Dropout(0.25)(x)\noutput = keras.layers.Dense(1, activation='sigmoid')(x)\nmodel = keras.models.Model(inputs=base_model.input, outputs=output)\n\n# The newly added layers are initialized with random values.\n# Make sure based model remain unchanged until newly added layers weights get reasonable values.\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])\n\n# Uncomment these lines to see the final model architecture:\n# model.summary()\n# See all the layers with index.\n# for index, layer in enumerate(base_model.layers):\n#  ","00875563":"# Defining checkpoint callback\ncheckpoint = ModelCheckpoint('..\/working\/best_model.hdf5', verbose = 1, monitor = 'val_binary_accuracy', save_best_only = True)\n\n# Fit model to get reasonable weights for newly added layers.\nhistory = model.fit_generator(generator = train_generator,\n                             steps_per_epoch = STEP_SIZE_TRAIN,\n                             validation_data = valid_generator,\n                             validation_steps = STEP_SIZE_VALID,\n                             epochs = 5, callbacks = [checkpoint])","923f4c4c":"fig, ax = plt.subplots()\nax.set(xlabel='Epoch', ylabel='Accuracy', title='Accuracy as training progresses')\nplt.plot(history.history['binary_accuracy'],'r--', label = \"Training Accuracy\" , linewidth=4.0)\nplt.plot(history.history['val_binary_accuracy'], 'b--', label = \"Validation Accuracy\",  linewidth=4.0)\nplt.legend()\nplt.annotate('High Accuracy - Weights of top layers acceptable now', xy=(3.5, .93), xytext=(4.5, 0.90),\n             arrowprops=dict(facecolor='green', shrink=0.05),\n             )\nplt.show()\n\nfig, ax = plt.subplots()\nax.set(xlabel='Epoch', ylabel='Loss', title='Loss as training progresses')\nplt.plot(history.history['loss'], 'r--', label = \"Train loss\", linewidth=4.0)\nplt.plot(history.history['val_loss'], 'b--', label = \"Val loss\", linewidth=4.0)\nplt.legend()\nplt.annotate('Very low loss - Weights of top layers acceptable now', xy=(3.5, .2), xytext=(4.5, 0.3),\n             arrowprops=dict(facecolor='green', shrink=0.05),\n             )\nplt.show()\n","bc43d933":"# Now let's train the full model and update all weights.\nfor layer in base_model.layers:\n    layer.trainable = True\n\n# compile the model with a SGD\/momentum optimizer\n# and a very slow learning rate (This ensures the base model weights do not change a lot)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n              metrics=['binary_accuracy'])","cc68a8c9":"# Fit model\nhistory = model.fit_generator(generator = train_generator,\n                             steps_per_epoch = STEP_SIZE_TRAIN,\n                             validation_data = valid_generator,\n                             validation_steps = STEP_SIZE_VALID,\n                             epochs = 10, callbacks = [checkpoint])","5b3f3534":"fig, ax = plt.subplots()\nax.set(xlabel='Epoch', ylabel='Accuracy', title='Accuracy as training progresses')\nplt.plot(history.history['binary_accuracy'],'r--', label = \"Training Accuracy\" , linewidth=4.0)\nplt.plot(history.history['val_binary_accuracy'], 'b--', label = \"Validation Accuracy\",  linewidth=4.0)\nplt.legend()\nplt.annotate('High Accuracy only for training set - Potential overfitting', xy=(8.5, .974), xytext=(9.5, 0.96),\n             arrowprops=dict(facecolor='green', shrink=0.05),\n             )\nplt.show()\n\nfig, ax = plt.subplots()\nax.set(xlabel='Epoch', ylabel='Loss', title='Loss as training progresses')\nplt.plot(history.history['loss'], 'r--', label = \"Train loss\", linewidth=4.0)\nplt.plot(history.history['val_loss'], 'b--', label = \"Val loss\", linewidth=4.0)\nplt.legend()\nplt.annotate('Low loss only for training set - Potential overfitting', xy=(8, .07), xytext=(9.5, 0.07),\n             arrowprops=dict(facecolor='green', shrink=0.05),\n             )\nplt.show()\n","81d99690":"saved_model = keras.models.load_model('..\/working\/best_model.hdf5')\nvalidation_set_performance = saved_model.evaluate_generator(generator=valid_generator,\nsteps=STEP_SIZE_VALID)\ntest_set_performance = saved_model.evaluate_generator(generator=test_generator,\nsteps=STEP_SIZE_TEST)\nprint(\"Validation set accuracy in %: \" + str(validation_set_performance[1]*100))\nprint(\"Test set accuracy in %: \" + str(test_set_performance[1]*100))","325ea7f4":"# Step 4:\n**Visualize Accuracy and loss as training progresses.**","44662ff1":"# Step 2:\n1. ** Transfer learning:** Load VGG16 CNN model trained on ImageNet. Remove the top layer.\n2. **Add custom Layers: ** Flatten the base model output and add a dense later. Add dropout layer for regularization.\n3. Freeze the weights of base model and train the custom layers so so that they get reasonable wegiths\n4. Use rmsprop optimizer with default learning rate.","dfb51957":"\n# Step 3:\n1. **Train ONLY custom layers:** Freeze base model weights and train rest of the weights.","2cdd0b78":"# Step 7:\n**Visualize Accuracy and loss as training progresses.**","89280818":"# Step 1:\n1. **Data Loading:** Using Keras ImageDataGenerator: Ensures batch fetching.\n2. **Data preprocessing: ** Suffle data and use  training data and validation data.\n3. **Data augumentation:** Use Keras on-the-fly data augumentation. Use width shift and height shiftflip techniques. This will prevent overfitting.\n4. Define batch size.","757d7e12":"# Summary of the classifier\n* **Architecture**: Repurpose pretrained CNN model (VGG16) trained on ImageNet: Transfer learning and fine tunning.\n* **Addressing overfitting:** Use of data augumentation and regularization using additional dropout layer. Use low learning rate for low impact on the weights of pretrained bottom convolution layers.\n* **Performance**: Test accuracy: 91.8%\n* **Approach**:  Use tensorflow.keras library for all steps: Data loading, data prepossing, data augumentation, training and evaluation.","3e8da491":"# Step 5:\n1. Unfreeze all layers so that complete model can be trained. Complie the model again.\n2. Keep very low learning rate","c156c55e":"# Step 6:\n1. Train all layers of the model.\n2. Save the model which has highest accuracy on validation set.\n3. Similar accuracy of training and validation set shows that there is no overfitting."}}