{"cell_type":{"7ca14f71":"code","17f825d7":"code","7a8e5ecf":"code","d7d465af":"code","5a598404":"code","d8b783be":"code","5d7d5d6c":"code","d25dcc3f":"code","c5579f1f":"code","078f3073":"code","0f56340b":"code","96c63f9d":"code","23f93ec1":"code","e8a3df7f":"code","d597d8b4":"code","9d3fa335":"code","ba517466":"code","bc47cd58":"code","c110c084":"code","c1b0b56b":"code","5c56a4f3":"code","193bfd0f":"code","c0c8c88e":"code","51afc325":"code","ef3d9d88":"code","3d66e2c0":"code","990d06fd":"code","40692895":"code","3e8dc595":"code","e5784014":"markdown","f5ac7965":"markdown","e4d94b38":"markdown","47dd7370":"markdown","6caf7b43":"markdown","59c22e3e":"markdown","84d84b85":"markdown","144e5af8":"markdown","dc27065c":"markdown"},"source":{"7ca14f71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","17f825d7":"# set the file path\ntitanic_test_file = '..\/input\/titanic\/test.csv'\ntitanic_train_file = '..\/input\/titanic\/train.csv'\n\n# open pd file\ntest = pd.read_csv(titanic_test_file, index_col='PassengerId')\ntrain = pd.read_csv(titanic_train_file, index_col='PassengerId')","7a8e5ecf":"import seaborn as sns\nimport matplotlib. pyplot as plt\n\nsns.barplot(x=train['Sex'],y=train['Survived'])\nplt.show()\n\n# more females surviving than males, but are there more female in general?","d7d465af":"sns.barplot(x=train['Pclass'],y=train['Survived'])\nplt.show()\n\n# Higher class, higher survival rate","5a598404":"sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", kind=\"violin\", data=train)\nplt.show()\n\n# 1st calss female is almost guaranteed to survived","d8b783be":"sns.barplot(x=train['Embarked'],y=train['Survived'])\nplt.show()","5d7d5d6c":"sns.distplot(a=train['Fare'])\nplt.show()","d25dcc3f":"sns.distplot(a=train['Age'])\nplt.show()","c5579f1f":"train.isnull().sum()","078f3073":"test.isnull().sum()","0f56340b":"train[['Last','rest']] = train.Name.str.split(\",\",expand=True)\ntrain[['Title','end']] = train.rest.str.split(\".\",n=1,expand=True)\nage_median = train[['Age']].median()\nfare_median = train[['Fare']].median()\ntrain[[\"Age\"]] = train[[\"Age\"]].fillna(age_median)\ntrain[[\"Embarked\"]] = train[[\"Embarked\"]].fillna('Unknown')\ntrain['fam_size'] = train.apply(lambda row: row.SibSp + row.Parch, axis=1)\n\ntitle_names = (train['Title'].value_counts() < 20) # true false mask with title name as index\ntrain['Title'] = train['Title'].apply(lambda x: 'High' if title_names.loc[x] == True else x)\ntrain.Title.value_counts()\n\n\n# probability not worth having title as a column, since the info is mostly encoded in the sex and fare price\ntrain.isnull().sum()","96c63f9d":"test[['Last','rest']] = test.Name.str.split(\",\",expand=True)\ntest[['Title','end']] = test.rest.str.split(\".\",n=1,expand=True)\ntest[[\"Age\"]] = test[[\"Age\"]].fillna(age_median)\ntest[[\"Fare\"]] = test[[\"Fare\"]].fillna(fare_median)\ntest[[\"Embarked\"]] = test[[\"Embarked\"]].fillna('Unknown')\ntest_names = (test['Title'].value_counts() < 20) # true false mask with title name as index\n\ntest['Title'] = test['Title'].apply(lambda x: 'High' if test_names.loc[x] == True else x)\ntest['fam_size'] = test.apply(lambda row: row.SibSp + row.Parch, axis=1)\n\nprint(test.isnull().sum())","23f93ec1":"test.head()","e8a3df7f":"useful_columns_train=['Survived','Pclass','Sex','Age','Fare','Embarked','fam_size']\nuseful_columns_test=['Pclass','Sex','Age','Fare','Embarked','fam_size']\n\n\ntrain_sel = train[useful_columns_train]\ntest_sel = test[useful_columns_test]","d597d8b4":"test_sel.head()","9d3fa335":"# generate binary values using get_dummies\ndummies_train = pd.get_dummies(train_sel, columns=[\"Sex\",\"Embarked\"] )\n\n# generate binary values using get_dummies\ndummies_test = pd.get_dummies(test_sel, columns=[\"Sex\",\"Embarked\"])","ba517466":"dummies_test['Embarked_Unknown'] = 0\ndummies_test.describe()","bc47cd58":"y = dummies_train['Survived']\nX = dummies_train.iloc[:,1:]\n\ntest_X = dummies_test","c110c084":"from sklearn.model_selection import train_test_split\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.2)\n\nprint(\"train_X Shape: {}\".format(train_X.shape))\nprint(\"valid_X Shape: {}\".format(valid_X.shape))\nprint(\"test_X Shape: {}\".format(test_X.shape))","c1b0b56b":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n\nmodel = RandomForestClassifier(n_estimators=200)\n\nmodel.fit(train_X,train_y)\n\ntrain_pred = model.predict(train_X)\nvalid_pred = model.predict(valid_X)\n    \nprint(\"training misclassified:\",(train_y-train_pred).sum())\nprint(\"validation misclassified:\",(valid_y-valid_pred).sum())\nprint(\"training accuracy is %2.3f\" % accuracy_score(train_y,train_pred))\nprint(\"validaiton accuracy is %2.3f\" % accuracy_score(valid_y,valid_pred))\nprint('Precision:%.3f'%precision_score(y_true=valid_y,y_pred=valid_pred))\nprint('Recall:%.3f'%recall_score(y_true=valid_y,y_pred=valid_pred))\nprint('F1:%.3f'%f1_score(y_true=valid_y,y_pred=valid_pred))\n\n\nperm = PermutationImportance(model).fit(train_X, train_y)\neli5.show_weights(perm, feature_names = train_X.columns.tolist())\n    ","5c56a4f3":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\npipe_lr = make_pipeline(StandardScaler(),\n                        PCA(),\n                        LogisticRegression(solver='lbfgs'))\n\npipe_lr.fit(train_X, train_y)\nvalid_pred = pipe_lr.predict(valid_X)\nprint('Test Accuracy: %.3f' % pipe_lr.score(valid_X,valid_y))","193bfd0f":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        LogisticRegression(penalty='l2', random_state=1,\n                                           solver='lbfgs', max_iter=10000))\n\ntrain_sizes, train_scores, test_scores =\\\n                learning_curve(estimator=pipe_lr,\n                               X=train_X,\n                               y=train_y,\n                               train_sizes=np.linspace(0.1, 1.0, 10),\n                               cv=10,\n                               n_jobs=1)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean,\n         color='blue', marker='o',\n         markersize=5, label='Training accuracy')\n\nplt.fill_between(train_sizes,\n                 train_mean + train_std,\n                 train_mean - train_std,\n                 alpha=0.15, color='blue')\n\nplt.plot(train_sizes, test_mean,\n         color='green', linestyle='--',\n         marker='s', markersize=5,\n         label='Validation accuracy')\n\nplt.fill_between(train_sizes,\n                 test_mean + test_std,\n                 test_mean - test_std,\n                 alpha=0.15, color='green')\n\nplt.grid()\nplt.xlabel('Number of training examples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.6, 1.03])\nplt.tight_layout()\nplt.show()","c0c8c88e":"from sklearn.model_selection import validation_curve\n\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(\n                estimator=pipe_lr, \n                X=train_X, \n                y=train_y, \n                param_name='logisticregression__C', \n                param_range=param_range,\n                cv=10)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(param_range, train_mean, \n         color='blue', marker='o', \n         markersize=5, label='Training accuracy')\n\nplt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, alpha=0.15,\n                 color='blue')\n\nplt.plot(param_range, test_mean, \n         color='green', linestyle='--', \n         marker='s', markersize=5, \n         label='Validation accuracy')\n\nplt.fill_between(param_range, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color='green')\n\nplt.grid()\nplt.xscale('log')\nplt.legend(loc='lower right')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')\nplt.ylim([0.6, 1.0])\nplt.tight_layout()\n# plt.savefig('images\/06_06.png', dpi=300)\nplt.show()","51afc325":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\npipe_svc = make_pipeline(StandardScaler(),\n                         SVC(random_state=1))\n\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n\nparam_grid = [{'svc__C': param_range, \n               'svc__kernel': ['linear']},\n              {'svc__C': param_range, \n               'svc__gamma': param_range, \n               'svc__kernel': ['rbf']}]\n\ngs = GridSearchCV(estimator=pipe_svc, \n                  param_grid=param_grid, \n                  scoring='accuracy', \n                  refit=True,\n                  cv=10,\n                  n_jobs=-1)\ngs = gs.fit(train_X, train_y)\nprint(gs.best_score_)\nprint(gs.best_params_)\n\"\"\"","ef3d9d88":"clf = gs.best_estimator_\n\nprint('Test accuracy: %.3f' % clf.score(valid_X,valid_y))","3d66e2c0":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(train_X,train_y)\n\ntrain_pred = knn.predict(train_X)\nvalid_pred = knn.predict(valid_X)\n    \nprint(\"training misclassified:\",(train_y-train_pred).sum())\nprint(\"validation misclassified:\",(valid_y-valid_pred).sum())\nprint(\"training accuracy is %2.3f\" % accuracy_score(train_y,train_pred))\nprint(\"validaiton accuracy is %2.3f\" % accuracy_score(valid_y,valid_pred))\n","990d06fd":"from sklearn.linear_model import SGDClassifier\n\ndef SGD_model(train_X,train_y,valid_X,valid_y):\n    model = SGDClassifier()\n    model.fit(train_X,train_y)\n\n    train_pred = model.predict(train_X)\n    valid_pred = model.predict(valid_X)\n\n    print(\"training misclassified:\",(train_y-train_pred).sum())\n    print(\"validation misclassified:\",(valid_y-valid_pred).sum())\n    print(\"training accuracy is %2.3f\" % accuracy_score(train_y,train_pred))\n    print(\"validaiton accuracy is %2.3f\" % accuracy_score(valid_y,valid_pred))\n\n    \nSGD_model(train_X,train_y,valid_X,valid_y)","40692895":"useful_columns2=['Survived','Pclass','Sex','Age','Fare','fam_size']\nuseful_columns2_test=['Pclass','Sex','Age','Fare','fam_size']\n\n\ntrain_sel2 = train[useful_columns2]\ntest_sel2 = test[useful_columns2_test]\n\n# generate binary values using get_dummies\ndummies_train2 = pd.get_dummies(train_sel2, columns=[\"Sex\"] )\n\n# generate binary values using get_dummies\ndummies_test2 = pd.get_dummies(test_sel2, columns=[\"Sex\"])\n\ny2 = dummies_train2['Survived']\nX2 = dummies_train2.iloc[:,1:]\n\ntest_X2 = dummies_test2\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=200)\n\nmodel.fit(X2,y2)\n\ntest_pred = model.predict(test_X2)\n    \n","3e8dc595":"submission = pd.DataFrame({\n        \"PassengerId\": test_y2.index,\n        \"Survived\": test_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","e5784014":"## Step 1. get the files","f5ac7965":"## Step 3. preprocess","e4d94b38":"## Step 5. focus on one model and hyperparamter\n* We will use the Random Forest Classifier\n* Disgard Embark since the influence it has is not high","47dd7370":"# Data columns provided\n\nVariable\tDefinition\tKey\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\tSex\t\nAge\tAge in years\t\nsibsp\t# of siblings \/ spouses aboard the Titanic\t\nparch\t# of parents \/ children aboard the Titanic\t\nticket\tTicket number\t\nfare\tPassenger fare\t\ncabin\tCabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\n# Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","6caf7b43":"## Step 6. get output file and submit","59c22e3e":"## Step 4. try out different classifier and hyperparamter tuning\n","84d84b85":"# Steps to take\n\n1. get the files\n2. check the relationship between input and output through graphs\n3. check missing data and preprocess\n4. try out different classifier and hyperparamter tuning\n5. focus on one model and hyperparamter\n6. export output file","144e5af8":"## Step 2.  check the relationship between input and output through graphs\n","dc27065c":"# Titanic Predicting the surivial rate - a classification problem"}}