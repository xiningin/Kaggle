{"cell_type":{"e488d281":"code","e9a9097e":"code","4633a53e":"code","f43ab3ee":"code","74197fb0":"code","343f9282":"code","be16339e":"code","848a12a1":"code","10b7e67c":"code","6c50801c":"code","43f44981":"code","3b33620b":"code","2dc4dec3":"code","c467c52c":"code","0f9a4843":"code","1eea6fc1":"code","e1b4816f":"code","63af9db7":"code","01287fdc":"code","d1bcbcd3":"code","6a4b8987":"code","dd157cc2":"code","7e2f6f68":"code","3893a147":"code","90ec606c":"markdown","9d9173d8":"markdown","40936069":"markdown","b64ddc70":"markdown","5cc283ad":"markdown","c30580f6":"markdown","8c565bc5":"markdown","8835b852":"markdown"},"source":{"e488d281":"## Recommender System : Market Basket analysis using MLExtend Library.","e9a9097e":"import pandas as pd\nimport numpy as np\n# import streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport networkx as nx\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori,association_rules\nimport matplotlib.pyplot as plt\nplt.style.use('default')\n \n","4633a53e":"## Load data \n\n\ndata = pd.read_csv(\"..\/input\/market-basket-optimization\/Market_Basket_Optimisation.csv\" , header = None ,engine='python')","f43ab3ee":"data .head()","74197fb0":"data.shape","343f9282":"##  capture just row lables by acessing the index in shape\n\ndata[1]","be16339e":"## Lets do the EDA to Understand the DAta Better, \n\n# 1.  Get all the items in the dataset, to get the total number of items.\n\n\"\"\"Create an empty list to gather the values, and convert in np.array\"\"\"\nitems = []\n\n\"\"\"Loop through each value in rows and columns to get each value in the list, no matter even if they are repeated.\"\"\"\nfor i in range(0, data.shape[0]):\n    for j in range(0, data.shape[1]):\n        items.append(data.values[i,j])\n\nitems = np.array(items)","848a12a1":"print(\"Total Number of items present in the dataset\",len(items))\ndf = pd.DataFrame(items, columns = ['items'])\ndf['items'].value_counts().head(10)","10b7e67c":"df['items'].unique()","6c50801c":"## get the Count of the unique values in items by grouping them.\n\n\"\"\"Make a New column in the dataset , by putting a value of digit 1, so that we can add them by groupping the similar values.\"\"\"\n\ndf['item_count'] = 1","43f44981":"df.shape","3b33620b":"## drop the nan values from the dataset as it will not represent any kind of transaction\n\nnan_drop = df[df['items'].isnull()].index\ndf.drop(nan_drop , inplace = True)","2dc4dec3":"df.shape","c467c52c":"## create items list by ascending order to view which one is occuring most\n\ndf_items_list = df.groupby(['items']).sum().reset_index().sort_values(by = 'item_count' , ascending = False)\ndf_items_list.head(10).style.background_gradient(cmap='BuPu')","0f9a4843":"\"\"\"USing TransactionEncoder method to encode the values\"\"\"\n\n\"\"\"Create an Empty List, loop through the items in rows, and then all the columns, and convert them in 0 or 1\"\"\"\nitems = []\nfor i in range(data.shape[0]):\n    items.append([str(data.values[i,j]) for j in range(data.shape[1])])\n    \n    \nitems = np.array(items)\ntr = TransactionEncoder()\ntr_Array = tr.fit(items).transform(items)\n\ndf = pd.DataFrame(tr_Array , columns = tr.columns_)\ndf","1eea6fc1":"# Convert dataset into 1-0 encoding\n\ndef encode_units(x):\n    if x == False:\n        return 0 \n    if x == True:\n        return 1\n    \ndf = df.applymap(encode_units)\ndf.head(10)","e1b4816f":"## Support ##Confidence ## LIFT   \n\n# Extracting the most frequest itemsets via Mlxtend.\n# The length column has been added to increase ease of filtering.\n\nfrequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets.sort_values(by = 'length' , ascending = False).head(10)","63af9db7":"frequent_itemsets[ (frequent_itemsets['length'] == 2) &\n                   (frequent_itemsets['support'] >= 0.05) ].head()","01287fdc":"## Definging the Rules for the Algorithm\n\nrules = association_rules(frequent_itemsets , metric='lift'  ,min_threshold= 1.2)\n\nrules[\"ante._count\"] = rules[\"antecedents\"].apply(lambda x: len(x))\nrules[\"cont._count\"] = rules[\"consequents\"].apply(lambda x: len(x))\nrules.sort_values(by = \"lift\" , ascending =False)","d1bcbcd3":"# Sort values based on confidence\n\nrules.sort_values(\"confidence\",ascending=False)","6a4b8987":"rules[~rules['antecedents'].str.contains(\"mineral water\" , regex=False) & \n      ~rules['consequents'].str.contains(\"mineral water\", regex=False)].sort_values(\"confidence\" , ascending = False)","dd157cc2":"rules[rules[\"antecedents\"].str.contains(\"ground beef\", regex=False) &\n      rules[\"ante._count\"] == 1].sort_values(\"confidence\", ascending=False).head(10)","7e2f6f68":"# For optimum results, you can filter out both the metrics and filter out the most relevant associated items.\n\nrules[(rules['lift'] >= 2) & (rules['confidence'] >= 0.50)]","3893a147":"# Results","90ec606c":"##### WE see, that the dataset is mainly dominated by the product minreal water, hence lets exclude that and see what is the confidence","9d9173d8":"## Algorithm Implementation","40936069":"> ### Codes for other colors to try in Cmap\n\n**Colormap green is not recognized. Possible values are: Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cividis, cividis_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, inferno, inferno_r, jet, jet_r, magma, magma_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, seismic, seismic_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, twilight, twilight_r, twilight_shifted, twilight_shifted_r, viridis, viridis_r, winter, winter_r","b64ddc70":"##### So , as per our intution, we do see a lot of transactions , and we can proceed further.\n\n\n### We can conclude here with this results and we can take this associaion data to further create Recommendations for the recommendations engine.","5cc283ad":"# **Implementation of apriori algorithm using MLextend library and creating the association rule, we can definelty generate relations between the items in the transactions.\n# **This implementaion could be very well expanded to many similar datasets and business problems.","c30580f6":"Here, we see that if a customer purchased eggs, and ground beef, there is a 50% likelyhood that he will purchase the mineral Water too,\n\nSO, keeping them colse to each other in shopping shel, will be better for sales.","8c565bc5":"Inorder for Apriori algorith to work, we need to pass only the 0-1 binay values.\n\nHere, we need to convert our data into a matrix where rows must be one transaction, and column will the items. \n\nEncode the data with binary values, \"1\" will represent that the item has been purchased and \" 0\" shows its not purhcased.\n\n","8835b852":"here, we see new items in relation by excluding the mineral water.\n\none of which is ground beef, so lets see all the transactions where ground beef is antecendent."}}