{"cell_type":{"6c5a66ba":"code","d5a23c39":"code","a5908341":"code","3e16ce60":"code","0c43285c":"code","12ac211f":"code","d1ce6302":"code","73e6fac0":"code","09600353":"code","0f749fe1":"code","31f1d82f":"code","2593e04c":"code","529477ab":"code","68cdc200":"code","1c0b2acf":"code","487e0ca1":"code","d7340e09":"code","33f04623":"code","ff335798":"code","764dfcb2":"code","e3f561e5":"code","f0c40a0c":"code","b6b2d3f5":"code","54c75094":"markdown","c3a5e7a8":"markdown","820eac33":"markdown","abaeb81f":"markdown","39a2ae79":"markdown","6f783e98":"markdown","22450c0c":"markdown","c1c86fd0":"markdown","52a9205e":"markdown"},"source":{"6c5a66ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d5a23c39":"df = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")\ndf.head()","a5908341":"df.describe()","3e16ce60":"df.plot(kind='box', subplots=True, layout=(5,5),\nsharex=False, sharey=False, figsize=(20,10))\nplt.show()","0c43285c":"df.info()","12ac211f":"# changing datatype for nominal\/ordinal data\nfor column in ['sex', 'smoker', 'region']:\n    df[column] = df[column].astype('category')","d1ce6302":"# this function plots the frequency distribution of a categorical feature\ndef show_frequency_distribution(feature_name, feature_values):\n    freq = (feature_name.value_counts() ).sort_index()\n    df = pd.DataFrame((feature_name.value_counts()).sort_index())\n    bars = feature_values\n    fig = plt.figure(figsize=(5,3))\n    ax1 = fig.add_subplot(1, 2, 1)\n    y = np.arange(len(bars))\n    _ = plt.bar(freq.index, freq , color = 'salmon');\n    _ = plt.xticks(freq.index, bars, rotation = 45);\n    _ = plt.ylabel(\"Frequency count\");\n    _ = plt.xlabel(str(feature_name.name) + \" type\");\n    _ = plt.title(\"Frequency Distribution of \" + str(feature_name.name));\n    ax2 = fig.add_subplot(1, 2, 2)\n    font_size=14\n    bbox=[1, 0, 1, 1]\n    ax2.axis('off')\n    table = ax2.table(cellText = df.values, rowLabels = feature_values, bbox=bbox, colLabels=df.columns)\n    table.auto_set_font_size(False)\n    table.set_fontsize(font_size)\n    plt.show();","73e6fac0":"for column in ['sex', 'smoker', 'region', 'children']:\n    show_frequency_distribution(df[column], list(df[column].value_counts().sort_index().index))","09600353":"sns.scatterplot(df['age'], df['charges'], hue = df['sex'], alpha = 0.5)\nplt.title(\"Exploring relation between 'age' and 'charges'\")\nplt.show()","0f749fe1":"sns.scatterplot(df['bmi'], df['charges'], hue = df['sex'], alpha = 0.5)\nplt.title(\"Exploring relation between 'bmi' and 'charges'\")\nplt.show()","31f1d82f":"# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\n\nfor column in ['sex', 'smoker', 'region']:\n    df[column] = labelencoder.fit_transform(df[column])\n    \ndf.head()","2593e04c":"X = df.drop('charges', axis = 1)\ny = df['charges']","529477ab":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns = X.columns)","68cdc200":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)","1c0b2acf":"from sklearn.linear_model import Ridge\nmodel = Ridge()\nmodel.fit(X_train, y_train)\nimportance = model.coef_\nfeat_importances = pd.Series(model.coef_, index=X.columns)\nfeat_importances.plot(kind='barh')\nplt.show()","487e0ca1":"#apply SelectKBest class to extract top 10 best features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nbestfeatures = SelectKBest(score_func=f_regression, k=6)\nfit = bestfeatures.fit(X_train,y_train)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(6,'Score'))  #print 10 best features","d7340e09":"X_train = X_train.drop(['sex', 'region'], axis = 1)\nX_test = X_test.drop(['sex', 'region'], axis = 1)","33f04623":"from sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\nmae = mean_absolute_error(y_test, y_pred_lr) \nr2_value = r2_score(y_test, y_pred_lr)                     \n\nprint(\"*** Multiple Linear Regression ***\")\nprint(\"Mean Absolute Error:\", mae)\nprint(\"R^2 Value:\", r2_value)","ff335798":"# Fit the data to polynomial linear regression model and check the accuracy\nfrom sklearn.preprocessing import PolynomialFeatures\npolyFeat = PolynomialFeatures(degree=3, include_bias=True)\npolyTrainX = polyFeat.fit_transform(X_train)\npolyTestX = polyFeat.fit_transform(X_test)\npr = LinearRegression()\npr.fit(polyTrainX, y_train)\ny_pred_pr = pr.predict(polyTestX)\n\nmae = mean_absolute_error(y_test, y_pred_pr)   \nr2_value = r2_score(y_test, y_pred_pr)                     \n\nprint(\"*** Polynomial Linear Regression ***\")\nprint(\"Mean Absolute Error:\", mae)\nprint(\"R^2 Value:\", r2_value)","764dfcb2":"from sklearn.model_selection import validation_curve, learning_curve\n\ndef draw_learning_curve(model, x, y):\n    train_sizes,train_scores, test_scores = learning_curve(model, x, y, \n                                                       train_sizes=[50, 100, 300, 500, 700, 900], cv=10)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    \n    plt.plot(train_sizes, train_scores_mean, color='blue', label='Train score')\n    plt.plot(train_sizes, test_scores_mean, color='red', label='Cross-validation score')\n    \n    plt.legend(loc='best')\n    plt.xlabel('Training size')\n    plt.ylabel('score')","e3f561e5":"draw_learning_curve(lr,X_train, y_train)\nplt.title(\"Learning curve for Multiple Linear Regressor\")","f0c40a0c":"draw_learning_curve(pr,polyTrainX, y_train)\nplt.title(\"Learning curve for Polynomial Regressor\")","b6b2d3f5":"predTest = pd.DataFrame({\"prediction\": y_pred_pr, \"observed\": y_test})\nplt.scatter(predTest['prediction'], predTest['observed'])\nplt.title(\"Polynomial Regressor: Prediction Vs Actual Data\")\nplt.xlabel(\"Predicted Medical Charges\") \nplt.ylabel(\"Observed Medical Charges\")\nplt.show()","54c75094":"# Explore the Data","c3a5e7a8":"While using Polynomial Regressor the R^2 score increases by around 10%. Therefore, we will finalize the Polynomial Regressor for this problem.","820eac33":"There are no null values in our data set.","abaeb81f":"The mean and median values of variables 'age', 'bmi' and 'children' shows they are normally distributed.","39a2ae79":"Above two methods of feature ranking show that 'sex' and 'region' are not important to predict the medical charges. Therefore, we will exclude them before building the final model.","6f783e98":"**Data Modelling**","22450c0c":"# Looking for the most significant features to predict medical charges","c1c86fd0":"# Learning curve for both the models","52a9205e":"# Building Regressor"}}