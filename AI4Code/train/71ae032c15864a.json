{"cell_type":{"e72c49f8":"code","46f6ea9d":"code","baeb3090":"code","32cfc490":"code","a204e43d":"code","3aec8947":"code","1f77f326":"code","652129f4":"code","c5bbae68":"code","3b58f6ae":"code","f63b5d7e":"code","e9c2ef7f":"code","c3632012":"code","8b74bde9":"code","bfc8e9ad":"code","9f3d2263":"code","ab9d80ca":"code","07c94215":"code","91b203c0":"code","b8325729":"code","53654e49":"code","75487f72":"code","4048724e":"code","6d44d0a6":"code","20c00110":"code","5ad7db73":"code","406e0e1a":"code","cb486f38":"code","7135fc4d":"code","5d3fc6f1":"code","fe0c1c5d":"code","55b141d1":"code","9b1cb99c":"code","f824e769":"code","1b7eea31":"code","0c0f3582":"code","c12a84e3":"code","3cb1837a":"code","214bbae4":"code","8d654063":"code","351461dd":"code","bddcd994":"code","42b85b4a":"code","b98463ef":"code","0c334af2":"code","336ad7a3":"code","a3e88f61":"code","e1af897c":"code","4d0ab47c":"code","4c17d1ec":"code","bf34e11d":"code","3255ca28":"code","6cafb705":"code","d5b36924":"code","a74ac72a":"markdown","fbb1e8cd":"markdown","ee9e2c14":"markdown","fe36e03b":"markdown","a4138a32":"markdown","b2e20345":"markdown","c8a95194":"markdown","6109f366":"markdown","238e615b":"markdown","f81a14f4":"markdown","4a21cec4":"markdown","ebd59e13":"markdown","4cd28034":"markdown","fe7e2852":"markdown"},"source":{"e72c49f8":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport numpy as np","46f6ea9d":"df = pd.read_csv('..\/input\/insurance\/insurance.csv') # loading data into df","baeb3090":"df.head()\n\n# from the data below we get to know that, there are 7 features, out of which 3 are categorical.\n","32cfc490":"df.info()\n\n# Points to be considered:\n\n# 1. There are no NULL values\n# 2. 3 of them are object type","a204e43d":"df['region'].value_counts()\n\n# There are 4 types of region, with approx same number of people.","3aec8947":"print(f'mean age:  {np.mean(df.age)}')\nprint(f'minimum age: {min(df.age)}, maximum age: {max(df.age)}')","1f77f326":"print(f'mean charges: {np.mean(df.charges)}')\nprint(f'minimum charges: {min(df.charges)}, maximum charges: {max(df.charges)}')","652129f4":"print(df['smoker'].value_counts()) \n\n# there are 1064 people who do not smoke, others smoke","c5bbae68":"print(df['children'].value_counts())\n\n# here is the split between the number of children.. ","3b58f6ae":"print(df['sex'].value_counts()) \n\n# Approx same number of male and female ","f63b5d7e":"df1 = pd.get_dummies(df)\ndf1\n# here we can use OneHotEncoding to encode the data as well.\n# get_dummies is a class of pandas, which encodes the categorical data into numeric form.\n# This method divides the categories into it's types, for eg: if we want to encode sex category which have 'male', and 'female' as its two\n# categories, then get_dummies will create two new features in your dataframe -- as 'sex_male' and 'sex_female'","e9c2ef7f":"# But we for now we do not require extra features like 'sex_male', as we can use 'sex_female' only to get relevant information \n# So, we can drop one of the extra feature created by the get_dummies class.\n\ndf1 = pd.get_dummies(df, drop_first = True)\ndf1.head()","c3632012":"df1['charges'].describe()","8b74bde9":"df1['charges'].quantile(0.95) # maximum charge is around 64k, while 95% of the data is less than 41k...\n# so we can assume that there are some outliers, but major i","bfc8e9ad":"plt.boxplot(df1[ 'charges']) # lots of outliers","9f3d2263":"df1['age'].describe()","ab9d80ca":"df1['age'].quantile(0.95) # 75% of the data is less than 51 years, and 95% of people are less than 62 years..\n\n# maximum age of a person in the given data is 64,,, so there is no such need to remove any of the age,, or to consider any age as a outlier.\n","07c94215":"plt.boxplot(df1['age']) #there are approx no outliers","91b203c0":"df1['bmi'].describe()","b8325729":"df1['bmi'].quantile(0.95) # 95% of people are less than 41,, while maximum bmi = 53.13","53654e49":"plt.boxplot(df1['bmi']) # there are few outliers","75487f72":"# Removing outliers:\nfrom numpy import percentile\nq05, q95 = percentile(df['charges'], 25), percentile(df['charges'], 75)\niqr = q95 - q05\n\ncut_off = iqr * 1.5\nlower, upper = q05 - cut_off, q95 + cut_off\noutliers = [x for x in df['charges'] if x < lower or x > upper]\noutliers_removed = [x for x in df['charges'] if x > lower and x < upper]\n\nprint(len(outliers))\n\n# we know that, for the given data, 75% percentile has significant values of charges, because major change in charges comes after 80-90% of percentile.\n# so if we check in the range of 5-95 % percentile -->\n\nfrom numpy import percentile\nq05, q95 = percentile(df['charges'], 5), percentile(df['charges'], 95)\niqr = q95 - q05\n\ncut_off = iqr * 1.5\nlower, upper = q05 - cut_off, q95 + cut_off\noutliers = [x for x in df['charges'] if x < lower or x > upper]\noutliers_removed = [x for x in df['charges'] if x > lower and x < upper]\n\nprint(len(outliers))\n\n# In the above range we find no outliers, so we can say that, if we consider 25-75% percentile as the base parameter, then there are around 10% of outliers, which may not change the \n# significane of the data in large, as the number of outliers are quite low. \n# Also, we find quite significant values between 25-75% percentile, so it will not be a good idea to choose this range, and if we check other range values, then the number of \n# outliers decreases.","4048724e":"# as we can see that, charges values are quite large, which can deviate our results, as large value can be give higher prefence. \n# So, we need scale data, so that the above situation does not occur.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Here, I'll be using StandardScaler which is a class of sklearn\n\nscaler = StandardScaler() # creating an object of StandardScaler class\ndf2 = pd.DataFrame(scaler.fit_transform(df1), columns = df1.columns) # first fitting the dataframe to scaler, and then transforming into the standardarised form\n\n# storing this standardarised form in df2\n# as fit_transform gives us an array of scaled features, therefore to convert that array into dataframe format we use, pandas library pd.DataFrame().\n\ndf2.head()","6d44d0a6":"# We know that, Linear Regression performs better if the distribution of the data is gaussian. \n# Therefore, we will be heading to see the distribution of each feature..\n\nimport scipy.stats as stat # importing scipy library and accessing stats class\nimport pylab\n# creating a function, to plot a normal distribution \n\ndef check_dist(df, feature):       # It takes 2 parameters, one is the dataframe and other is the feature\n    plt.figure(figsize = (10,6))   # giving size to the figure\n    plt.subplot(1,2,1)             # I want to plots in parllel, so (1,2,1) --> 1st row, 2nd column and 1st index  \n    sn.distplot(df[feature])           # creating a distribution plot \n    plt.subplot(1,2,2)             # 1st row, 2nd column and 2nd index\n    stat.probplot(df[feature], dist = 'norm', plot = pylab) # This plot tells us that whether the given feature will be of normal\/gaussian form or not \n    plt.show()\n    \n    ","20c00110":"print('Age:')\nprint(check_dist(df2,'age')) # almost gaussian","5ad7db73":"print('BMI:')\nprint(check_dist(df2,'bmi')) # Fully Gaussian","406e0e1a":"print('Charges:')\nprint(check_dist(df2,'charges')) # It is right skewed, so we can convert it into gaussian form","cb486f38":"# Converting\/Transforming 'Charges' distribution into gaussian form.\n\ndf2['new_charges']= df.charges**(1\/2)\ndf2.head() # new_charges column has been created, which transformed into gaussian distribution","7135fc4d":"check_dist(df2, 'new_charges')","5d3fc6f1":"del(df2['charges']) # deleting charges coloumn","fe0c1c5d":"df2.head() # charges --> column have been removed","55b141d1":"f1 = ['age', 'bmi', 'children', 'sex_male', 'smoker_yes', 'new_charges']\nf2 = ['region_northwest' ,'region_southwest', 'region_southeast', 'new_charges']\nsn.heatmap(df2[f1].corr(), annot = True)\n\n\n# Smokers and charges are the most correlated parameters followed by age and charges","9b1cb99c":"sn.heatmap(df2[f2].corr(), annot = True)\n\n# Not much relation between the charges and the region.","f824e769":"features = ['age', 'bmi', 'children']\nsn.pairplot(df[features])","1b7eea31":"df2.head()","0c0f3582":"x = df2.iloc[:, : -1]\ny = df2.iloc[:, -1]","c12a84e3":"x.head() # all columns other than new_charges","3cb1837a":"y.head() # only new_charges column","214bbae4":"# splliting the data into test and train:\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\nprint(f'Size of Training set: {len(x_train), len(y_train)}')\nprint(f'Size of Test set {len(x_test), len(y_test)}')\n","8d654063":"from sklearn.linear_model import LinearRegression\n\nlinear = LinearRegression()","351461dd":"linear.fit(x_train, y_train)","bddcd994":"linear.score(x_test, y_test) #R^2 value... which means that most of the features in the test dataset was able to predict the variation occuring in the 'new_charges'.","42b85b4a":"linear.coef_","b98463ef":"linear.intercept_","0c334af2":"linear.score(x_train, y_train) # so in training set, 76% of the new_charges are being explained by our parameters, while 81% of the new_charges are being explained by the\n                               # parameters.\n                               # This suggests that our model is not overfitted to the training data.","336ad7a3":"# Now lets find out the significance of data cleaning:","a3e88f61":"# In this I'll be using 'df1' dataframe for this analysis.. using df1 and not df because, df1 is encoded, whereas df is not encoded.\n# Therefore,\n\ndf1.head()","e1af897c":"# splitting the data:\n\nfeatures = ['age', 'bmi', 'children', 'sex_male', 'smoker_yes', 'region_northwest', 'region_southeast', 'region_southwest']\nx1 = df1[features]\ny1 = df1['charges']","4d0ab47c":"x1.head()","4c17d1ec":"y1.head()\n","bf34e11d":"x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size = 0.2, random_state = 0)","3255ca28":"linear2 = LinearRegression()     # fitting data to a linear model\nlinear2.fit(x1_train, y1_train)","6cafb705":"linear2.score(x1_test, y1_test)  # So, 79% of the charges was explained by the parameters in the test set.","d5b36924":"linear2.score(x1_train, y1_train) # So, 73% of the charges was explained by the parameters in the test set.","a74ac72a":"### Now the current data, is scaled + follows gaussian distribution + no categorical features + no missing values + outliers -> analysed\n\n","fbb1e8cd":"## 2. Encoding data:\n\nSo, as there are no NULL values, we can move ahead and encode the categorical features, as we can only use libraries on numerical\nfeautures.\nThis will be the first step in this case, before we dive into understanding the given data.\n\n","ee9e2c14":"## 1. Basic Information about the data:","fe36e03b":"### Therefore, from the above data we can write the equation of the line as --->\n\n#### y = 104.82 + 19.59(age) + 6.33(bmi) + 3.74(children) + (-0.51)(sex_male) + 36.55(smoker_yes) + (-0.86)(region_northwest) + (-2.42)(region_southeast) + (-1.93)(region_southwest)","a4138a32":"## 1. Splitting the data:","b2e20345":"# Understanding the data:","c8a95194":"## 3. Standardising the data:","6109f366":"## 2. Applying model:","238e615b":"### Clearly, cleaned data performed well as compared to uncleaned data.","f81a14f4":"## 4. Visualizing the data:\n\nHere we will try to understand the data, visually and try to find any correlation between the data. We will see whether there are outliers or not, and how these outliers effect the data.\n","4a21cec4":"# Loading data and few libraries:","ebd59e13":"# Significance of data cleaning:","4cd28034":"## 3. Analysing outliers:","fe7e2852":"# Model Creation:"}}