{"cell_type":{"92e5aa9c":"code","52a8cf61":"code","eb228423":"code","834a750d":"code","740caf65":"code","a1812b05":"code","e7b3f098":"code","afec490e":"code","a83c2585":"code","a90fb3ed":"code","9272a38b":"code","9abbb2f3":"code","be4b0381":"code","7ae831da":"code","5cf439d9":"code","bb4bef4d":"code","df398244":"code","de18b159":"code","bdb37f1c":"code","9dcae129":"code","f807e6ce":"code","b4c0f80f":"code","0fc82500":"code","2d6afebe":"code","e76db45e":"code","3ad4927b":"code","f24af20b":"code","4ed44a3f":"code","93711978":"code","7f35b513":"code","47870767":"code","841429de":"code","063a933a":"code","ddb26ec7":"code","5929b0f1":"code","7a096e58":"code","858cb0ef":"code","e074c9bd":"code","275494be":"code","f4da7b61":"markdown","7d872089":"markdown","e6389407":"markdown","da1798d7":"markdown","062feba5":"markdown","53442009":"markdown","c5b2fdc6":"markdown","5fbc57d2":"markdown","075be6e7":"markdown","31da6017":"markdown","c40208f9":"markdown","608439e2":"markdown","2d5c568c":"markdown","184652fc":"markdown"},"source":{"92e5aa9c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","52a8cf61":"\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\nimport itertools\n%matplotlib inline\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport math\nplt.style.use(\"seaborn-whitegrid\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import VotingClassifier, StackingClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import r2_score, classification_report, confusion_matrix, roc_curve, auc, plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.feature_selection import mutual_info_classif\nfrom tqdm import tqdm\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, GridSearchCV\nfrom scipy import stats\nfrom scipy.stats import skew\nimport warnings\nwarnings.filterwarnings(\"ignore\")","eb228423":"raw_data = pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/train.csv')\nraw_data","834a750d":"raw_data.info()","740caf65":"# Converting dtype of target from object to categorical for label encoding\nraw_data.y = raw_data.y.astype('category')\nraw_data.info()","a1812b05":"raw_data.columns","e7b3f098":"raw_data.isnull().sum()","afec490e":"raw_data.duplicated().sum()","a83c2585":"raw_data.describe().T","a90fb3ed":"raw_data['y'].value_counts()","9272a38b":"plt.figure(figsize=(25, 25))\nfor i, col in enumerate(list(raw_data.columns)):\n    plt.subplot(7, 4, i+1)\n    sns.histplot(raw_data[col], kde=True, bins=10)","9abbb2f3":"def plot_subplots(subplots, plot, n=3):\n    m = len(subplots)\n    height = (m\/\/n + (m%n != 0)) * 4\n    plt.figure(figsize=(14, height))\n    for i, c in enumerate(subplots):\n        plt.subplot(m\/\/n + min(1, m%n), n, i+1)\n        plot(c)\n        plt.tight_layout(pad=2.0)\n        plt.xticks(rotation=45)\n        \nnum_cols = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n           'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n           'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n           'ShapeFactor3', 'ShapeFactor4']\n\nplot_subplots(num_cols, lambda c: sns.barplot(data=raw_data, x='y', alpha=0.1, y=c))","be4b0381":"Strongly_corr_features = raw_data[[\"Area\",\"Perimeter\",\"AspectRation\",\"Eccentricity\",\"roundness\",\"Compactness\",\"y\"]]\nStrongly_corr_features.head()\nsns.set_theme(style=\"whitegrid\")\nsns.pairplot(Strongly_corr_features, hue=\"y\")","7ae831da":"visualization_df=raw_data.drop(['ID'], axis=1)\ni = 1\nplt.figure(figsize = [15, 15], tight_layout = 5)\nfor column in visualization_df.drop(['y'], axis=1).columns:\n    plt.subplot(6, 3, i)\n    plt.scatter(data = visualization_df, x = column, y = 'y', c='c', edgecolors='black')\n    plt.xlabel(column)\n    plt.ylabel('Beans Classes')\n    plt.title(column + ' VS ' + 'Beans Classes')\n    i += 1\nplt.show()","5cf439d9":"raw_data.y.mode()","bb4bef4d":"corr = raw_data.corr()\nf,axes = plt.subplots(1,1,figsize = (20,15))\nsns.heatmap(corr, square=True, annot = True, linewidth = .5, center = 2, ax = axes, cmap='Blues')","df398244":"data = raw_data.copy()\ndata.head(10)","de18b159":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n# Label encoding for categoricals\nfor colname in data.select_dtypes(\"object\"):\n    data[colname], _ = data[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = data.dtypes == int\n\nmi_scores = make_mi_scores(data.drop(columns=['ID', 'y']), data.y, discrete_features=False)\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","bdb37f1c":"#Calc the skeweness of each continous feature\n\ndef calc_skew(df):\n    print(\"\\nIF THE DATA IS HIGHLY SKEWED IF SKWENESS  > 1 OR < -1 \\n\")\n    for col in df.loc[:, df.dtypes != np.object ]:\n        print(\"the skewness of \",col,\"is :\",df[col].skew())\n\ncalc_skew(data.drop(['ID','y'], axis=1))","9dcae129":"def normalize_data(df,column):\n    return StandardScaler().fit_transform(np.array(df[column]).reshape(-1,1))\n\ndef removeOutliers(df,outliersColomns):\n    z_scores = stats. zscore(df[outliersColomns]) \n    abs_z_scores = np. abs(z_scores)\n    filtered_entries = (abs_z_scores < 3). all(axis=1)\n    new_df = df[filtered_entries]\n    return new_df\n\ny_labels = ['DERMASON', 'SIRA', 'SEKER', 'HOROZ', 'CALI', 'BARBUNYA', 'BOMBAY']\ndef removeOutliers_cat(df_raw,outliersColomns):\n    df = df_raw.copy()\n    for cat_ in y_labels:\n        df_ = df[df['y']==cat_]\n        z_scores = stats. zscore(df_[outliersColomns]) \n        abs_z_scores = np. abs(z_scores)\n        filtered_entries = (abs_z_scores < 4). all(axis=1)\n        df_ = df_[filtered_entries]\n        df = df[df['y']!=cat_]\n        df = df.append(df_)\n    return df\n\n# Transform data\ndef transformation(df,columns,func):\n    for col in columns:\n        df[col]=func(df[col])\n    return df\n\ndef encodingTarget(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({'DERMASON' :0, 'SIRA':1, 'SEKER':2, 'HOROZ':3, 'CALI':4, 'BARBUNYA':5, 'BOMBAY':6})\n    return dataset\n\ndef decodingTarget(dataset, cols):\n    for col_name in cols:\n        dataset[col_name] = dataset[col_name].replace({0:'DERMASON' ,1: 'SIRA', 2:'SEKER', 3:'HOROZ', 4:'CALI',5: 'BARBUNYA', 6:'BOMBAY'})\n    return dataset","f807e6ce":"data = raw_data.copy()\n\ndef prepare_data(df, isTest=False):\n    \n    outliersColomns = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n                       'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n                       'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n                       'ShapeFactor3', 'ShapeFactor4']\n    if isTest == False:\n#         pass\n\n        before_ = len(df)\n        df = removeOutliers_cat(df,outliersColomns)\n        print('Removed ',before_ - len(df),' outliers')\n#         df = encodingTarget(df,['y'])\n        \n    # take log1p for right skewed and square for left skewedfeatures.\n    right_skew_features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'EquivDiameter', 'ConvexArea']\n    left_skew_features = ['Eccentricity', 'Solidity', 'roundness', 'ShapeFactor4', 'Extent']\n    transformation(data, right_skew_features, np.log1p)\n    transformation(data, left_skew_features, np.square)\n    \n    df['ShapeFactor5'] = df['MajorAxisLength'] \/ df['Perimeter']\n    df['ShapeFactor6'] = df['MinorAxisLength'] \/ df['Perimeter']\n    df['ShapeFactor7'] = df['Eccentricity'] * df['Area']\n    df['ShapeFactor8'] = df['Eccentricity'] * df['Perimeter']\n    df['ShapeFactor9'] = df['Extent'] * df['Area']\n    df['ShapeFactor10'] = df['Extent'] * df['Perimeter']\n    # pi\/6 = 0.52359877559\n    df['Volume'] = 0.52359877559 * (df['EquivDiameter'] ** 3)\n    df['Spherecity'] = 100 * (df['EquivDiameter'] \/ df['MajorAxisLength'])\n    \n    NonScaledFeatures=['ID', 'y','Index']\n    for col_name in df.columns:\n        if col_name not in NonScaledFeatures:\n            scaled_col = col_name + '_r'\n            df[col_name] = df[col_name].fillna(df[col_name].mean())  \n            df[scaled_col]= df[col_name]\n            df[col_name]=normalize_data(df,col_name)\n\n    return df\n  \ndata = prepare_data(data)\n# labelencoder\nlabelencoder = LabelEncoder()\ndata['y'] = labelencoder.fit_transform(data['y'])\n        ","b4c0f80f":"data.describe()","0fc82500":"data","2d6afebe":"data.columns","e76db45e":"data2 = data[['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n       'AspectRation', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent',\n       'Solidity', 'roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2',\n       'ShapeFactor3', 'ShapeFactor4', 'y', 'ShapeFactor5', 'ShapeFactor6',\n       'ShapeFactor7', 'ShapeFactor8', 'ShapeFactor9', 'ShapeFactor10',\n       'Volume', 'Spherecity']]\ncorr2 = data2.corr()\nf,axes = plt.subplots(1,1,figsize = (20,20))\nsns.heatmap(corr2, square=True, annot = True, linewidth = .5, center = 2, ax = axes, cmap='rainbow')","3ad4927b":"visualization_df=data.drop(['ID','Area_r', 'Perimeter_r', 'MajorAxisLength_r', 'MinorAxisLength_r',\n       'AspectRation_r', 'Eccentricity_r', 'ConvexArea_r', 'EquivDiameter_r',\n       'Extent_r', 'Solidity_r', 'roundness_r', 'Compactness_r',\n       'ShapeFactor1_r', 'ShapeFactor2_r', 'ShapeFactor3_r', 'ShapeFactor4_r',\n       'ShapeFactor5_r', 'ShapeFactor6_r', 'ShapeFactor7_r', 'ShapeFactor8_r',\n       'ShapeFactor9_r', 'ShapeFactor10_r'], axis=1)\ni = 1\nplt.figure(figsize = [20, 20], tight_layout = 5)\nfor column in visualization_df.drop(['y'], axis=1).columns:\n    plt.subplot(6, 5, i)\n    plt.scatter(data = visualization_df, x = column, y = 'y', c='c', edgecolors='blue')\n    plt.xlabel(column)\n    plt.ylabel('Beans Classes')\n    plt.title(column + ' VS ' + 'Beans Classes')\n    i += 1\nplt.show()","f24af20b":"train_df, val_df = train_test_split(data, test_size=0.20, random_state=42, shuffle=True, stratify=data.y) \n\nx_train = train_df.drop(columns=['ID','y'])\ny_train = train_df['y']\n\nx_val = val_df.drop(columns=['ID','y'])\ny_val = val_df['y']","4ed44a3f":"x_train.shape[0], x_val.shape[0]","93711978":"#dropped high correlated redundant features: 'ConvexArea', 'Compactness'\n\nmodel_columns = [\n                  'Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', \n                  'AspectRation', 'Eccentricity', 'roundness', 'Extent',  'ShapeFactor5',\n                  'Solidity','ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4',\n                  'Volume', 'Spherecity',  'ShapeFactor9',\n\n                  ]","7f35b513":"# build the lightgbm model\n\nlgb_model = LGBMClassifier(objective='multiclass', random_state=42, n_estimators=100, learning_rate=0.03, reg_alpha=0.00001)\nlgb_model.fit(x_train[model_columns], y_train)\npredictions_LGB = lgb_model.predict(x_val[model_columns])\nprint('Mean_F1_score', f1_score(y_val, predictions_LGB, average='micro'))\nprint(\"Classification Report: \\n\", classification_report(y_val, predictions_LGB))","47870767":"# Create an instance of the MLPclassifier\n\nmlp_model = MLPClassifier(solver='adam', activation='logistic', alpha=1e-5, random_state=42, max_iter=500, early_stopping=True, validation_fraction=0.01, warm_start=True, verbose=False, learning_rate ='adaptive', learning_rate_init=0.001)\nmlp_model = mlp_model.fit(x_train[model_columns], y_train)\n\npredictions_MLP = mlp_model.predict(x_val[model_columns])\nprint('Mean_F1_score', f1_score(y_val, predictions_MLP, average='micro'))\nprint(\"Classification Report\")\nprint(classification_report(y_val, predictions_MLP))","841429de":"kn_model = KNeighborsClassifier(algorithm='auto', weights ='distance', n_neighbors=10)\nkn_model.fit(x_train[model_columns], y_train)\n          \npredictions_KN = kn_model.predict(x_val[model_columns])\nprint('Mean_F1_score', f1_score(y_val, predictions_KN, average='micro'))\nprint(\"Classification Report: \\n\", classification_report(y_val, predictions_KN))","063a933a":"svc_model = SVC(C=1.0, kernel='rbf', max_iter=-1, random_state=42, decision_function_shape='ovo', gamma=0.20)\n# svc_model = SVC(C=1.0, kernel='poly', degree=3, max_iter=-1, random_state=42, decision_function_shape='ovo', gamma=0.20)\nsvc_model.fit(x_train[model_columns], y_train)\n\npredictions_SVC = svc_model.predict(x_val[model_columns])\nprint('Mean_F1_score', f1_score(y_val, predictions_SVC, average='micro'))\nprint(\"Classification Report: \\n\", classification_report(y_val, predictions_SVC))","ddb26ec7":"dt_model = DecisionTreeClassifier(max_depth=5, min_samples_split=16, ccp_alpha=0.00001, random_state=42, criterion='gini')\ndt_model.fit(x_train[model_columns], y_train)\n\npredictions_DT = dt_model.predict(x_val[model_columns])\nprint('Mean_F1_score', f1_score(y_val, predictions_DT, average='micro'))\n\nprint(\"Classification Report: \\n\", classification_report(y_val, predictions_DT))","5929b0f1":"#voting\n\nclassifiers = [\n                ('SVC:', svc_model),\n                ('MLP:', mlp_model),\n                ('LGB:', lgb_model),\n                ('KN:', kn_model),\n                ('DT:', dt_model),\n                ]\n    \nfor clf_name, clf in classifiers:\n#     clf.fit(x_train[model_columns], y_train)\n    y_pred = clf.predict(x_val[model_columns])\n    print(clf_name, f1_score(y_val, y_pred, average='micro'))\n\nvc = VotingClassifier(estimators=classifiers)\nkfold = StratifiedKFold(n_splits=10, random_state=42)\ncv_results = cross_val_score(vc, x_train[model_columns], y_train, cv=kfold, scoring='f1_micro')\nvc.fit(x_train[model_columns], y_train)\ny_pred_voting = vc.predict(x_val[model_columns])\nprint()\nprint('Voting Classifier: ',f1_score(y_val, y_pred_voting, average='micro'))","7a096e58":"raw_test = pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/test.csv')\nraw_test.sample(10)","858cb0ef":"raw_test.isnull().sum()","e074c9bd":"X_test = raw_test.copy()\n\nX_test = prepare_data(X_test, isTest=True)\n\nX_test = X_test.drop(columns=['ID'])\n\ny_test_predicted_vc = vc.predict(X_test[model_columns])\ny_test_predicted_vc = labelencoder.inverse_transform(y_test_predicted_vc)\n\nraw_test['y'] = y_test_predicted_vc\n\nraw_test","275494be":"raw_test[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)\n","f4da7b61":"#### Ensemble method:","7d872089":"###  KNeighborsClassifier:","e6389407":"## Training different classification models on the dataset\n","da1798d7":"## Data Preparation","062feba5":"# Loading and exploring the dataset","53442009":"## Features-labels split and train-validation split","c5b2fdc6":"###  MultiLayerPerceptronClassifier:","5fbc57d2":"## Reading the test file","075be6e7":"###  DecisionTreeClassifier:","31da6017":"## Data visualization","c40208f9":"### Data Fields:\n`ID` - an ID for this instance.\\\nArea - `(A)`, The area of a bean zone and the number of pixels within its boundaries.\\\nPerimeter - `(P)`, Bean circumference is defined as the length of its border.\\\nMajorAxisLength - `(L)`, The distance between the ends of the longest line that can be drawn from a bean.\\\nMinorAxisLength - `(I)`, The longest line that can be drawn from the bean while standing perpendicular to the main axis.\\\nAspectRatio - `(K)`, Defines the relationship between L and l`(L\/I)`.\\\nEccentricity - `(Ec)`, Eccentricity of the ellipse having the same moments as the region.\\\nConvexArea - `(C)`, Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\\\nEquivDiameter - `(Ed)`, The diameter of a circle having the same area as a bean seed area `sqrt(4*A\/pi)`.\\\nExtent -`(Ex)`, The ratio of the pixels in the bounding box to the bean area.\\\nSolidity - `(S)`, Also known as convexity. The ratio of the pixels in the convex shell to those found in beans `(A\/c)`.\\\nRoundness - `(R)`, Calculated with the following formula: `(4*pi*A)\/(P^2)`.\\\nCompactness - `(CO)`, Measures the roundness of an object: `(Ed\/L)`.\\\nShapeFactor1 - `(SF1=L\/A)`.\\\nShapeFactor2 - `(SF2=I\/A)`.\\\nShapeFactor3 - `(SF3=A\/(pi*L\/2*L\/2))`.\\\nShapeFactor4 - `(SF4=A\/(pi*L\/2*I\/2))`.\\\n`y` - the class of the bean. It can be any of BARBUNYA, SIRA, HOROZ, DERMASON, CALI, BOMBAY, and SEKER.","608439e2":"###  SupportVectorClassifier:","2d5c568c":"###  LightGB Classifier:","184652fc":"## Feature Engineering:"}}