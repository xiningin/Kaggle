{"cell_type":{"28b76e38":"code","10618ae3":"code","30d8e5f6":"code","3137a999":"code","4c57383b":"code","1ad84bf1":"code","9631df69":"code","ed3a4951":"code","d62e07f8":"code","0f8d17c6":"code","f98d16d3":"code","eb3c4f0d":"code","d8f709f6":"code","2a67c9dd":"code","4cebf594":"code","4c679d1d":"code","938e8957":"code","ae5fdf2f":"code","233d01d6":"code","ea0f776c":"code","e7ca214d":"code","5d0bb07a":"code","a6f776fe":"code","2aca5c5f":"code","3210bea1":"markdown"},"source":{"28b76e38":"import re\nimport multiprocessing\nfrom tqdm import tqdm\n\nimport pandas as pd\nfrom wordcloud import WordCloud, STOPWORDS\nfrom scipy.stats import norm\nfrom gensim.models import word2vec\nfrom kaggle.competitions import twosigmanews\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.plotting import figure, show, output_notebook\n\nfrom nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\n%matplotlib inline","10618ae3":"cpu_count = 2*multiprocessing.cpu_count()-1\nprint('Number of CPUs: {}'.format(cpu_count))","30d8e5f6":"env = twosigmanews.make_env()\nprint('Done!')","3137a999":"(market_train_df, news_train_df) = env.get_training_data()","4c57383b":"assetName = 'Companhia de Bebidas das Americas Ambev'","1ad84bf1":"df = market_train_df[market_train_df['assetName']==assetName]\ndf_news = news_train_df[news_train_df['assetName'] == assetName]","9631df69":"df.head()","ed3a4951":"data_from = '2012-03-10'\ndata_to = '2014-09-10'\n\ndf_small = df[ (df['time']>data_from) & (df['time'] < data_to)]\ndf_news_small = df_news[(df_news['time'] > data_from) & (df_news['time'] < data_to)]","d62e07f8":"plt.figure(figsize=(16,5))\n\n#plt.subplot(2,1,1)\nplt.plot(df_small['time'], df_small['close'])\nplt.title(assetName)\nplt.xlabel('Time', fontsize=16)\n'''\nplt.subplot(2,1,2)\nplt.plot(df_news_small['time'], df_news_small['sentimentNegative'], '--ro', label='Neg')\nplt.plot(df_news_small['time'], df_news_small['sentimentNeutral'], '--bo', label='Neu')\nplt.plot(df_news_small['time'], df_news_small['sentimentPositive'], '--go', label='Pos')\nplt.legend()\n'''","0f8d17c6":"plt.figure(figsize=(20,8))\npd.value_counts(df_news_small['headlineTag']).plot(kind=\"barh\")","f98d16d3":"df_news_small['headline'].head()","eb3c4f0d":"def get_wordCloud(corpus):\n    wordCloud = WordCloud(background_color='white',\n                              stopwords=STOPWORDS,\n                              width=3000,\n                              height=2500,\n                              max_words=200,\n                              random_state=42\n                         ).generate(str(corpus))\n    return wordCloud","d8f709f6":"def get_corpus(data):\n    corpus = []\n    for phrase in data:\n        for word in phrase.split():\n            corpus.append(word)\n    return corpus","2a67c9dd":"corpus = get_corpus(df_news_small['headline'].values)\nprocWordCloud = get_wordCloud(corpus)","4cebf594":"df_news_small['headline'].head()","4c679d1d":"fig = plt.figure(figsize=(20, 8))\nplt.subplot(1,2,1)\nplt.imshow(procWordCloud)\nplt.axis('off')\n\nplt.subplot(1,2,2)\n\nwords_count = [len(x.split(' ')) for x in df_news_small['headline'].values]\nsns.distplot(words_count,hist=True, kde=False, bins=10, fit=norm)\nplt.title(\"Distribution of words in headline news\")\nplt.xlabel('Number of words in headline news')","938e8957":"#from:https:\/\/github.com\/RenatoBMLR\/nlpPy\/tree\/master\/src\n\nclass TextDataset():\n\n    def __init__(self, df, lang = 'english'):\n\n        self.data = df\n\n        self.tokenizer = TweetTokenizer()\n        self.stop_words = set(stopwords.words(lang))\n        self.lemmatizer = WordNetLemmatizer()\n        self.ps = PorterStemmer()\n        \n    def _get_tokens(self, words):    \n        return [word.lower() for word in words.split()]\n    \n    def _removeStopwords(self, words):\n        # Removing all the stopwords\n        return [word for word in words if word not in self.stop_words]\n\n    def _removePonctuation(self, words):\n        return re.sub(r'[^\\w\\s]', '', words)\n\n    def _lemmatizing(self, words):\n        #Lemmatizing\n        return [self.lemmatizer.lemmatize(word) for word in words]\n\n    def _stemming(self, words):\n        #Stemming\n        return [self.ps.stem(word) for word in words]\n\n\n    def process_data(self, col = 'content', remove_pontuation=True, remove_stopw = True, lemmalize = False, stem = False):\n\n        self.data = self.data.drop_duplicates(subset=col, keep=\"last\")\n        \n        proc_col = col\n        if remove_pontuation:\n            proc_col = col + '_data'\n            self.data[proc_col] = self.data[col].apply(lambda x: self._removePonctuation(x) )\n        \n        # get tokens of the sentence\n        self.data[proc_col] = self.data[proc_col].apply(lambda x: self._get_tokens(x))\n        if remove_stopw:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._removeStopwords(x)) \n        if lemmalize:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._lemmatizing(x) )\n        if stem:\n            self.data[proc_col] = self.data[proc_col].apply(lambda x: self._stemming(x))\n\n        self.data['nb_words'] = self.data[proc_col].apply(lambda x: len(x))\n        self.proc_col = proc_col\n        \n    def __len__(self):\n        return len(self.data)","ae5fdf2f":"textDataset = TextDataset(df_news_small)","233d01d6":"textDataset.process_data(col='headline')","ea0f776c":"textDataset.data['headline_data'].head()","e7ca214d":"X = textDataset.data['headline_data'].values","5d0bb07a":"# Set values for various parameters\nnum_features = 3    # Word vector dimensionality                      \nmin_word_count = 2   # Minimum word count                        \nnum_workers = cpu_count  # Number of threads to run in parallel\ncontext = 3          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n\nW2Vmodel = word2vec.Word2Vec(workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n\nW2Vmodel.build_vocab(X)\n\nW2Vmodel.train(X, \\\n            total_examples=W2Vmodel.corpus_count, epochs=W2Vmodel.epochs)","a6f776fe":"def plot_tSNE(model,n_samples = 5000):\n\n    #https:\/\/www.oreilly.com\/learning\/an-illustrated-introduction-to-the-t-sne-algorithm\n    output_notebook()\n    fig = bp.figure(plot_width=700, plot_height=600, title=\"A map of \" + str(n_samples) + \" word vectors\",\n        tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n        x_axis_type=None, y_axis_type=None, min_border=1)\n\n\n    word_vectors = [model[w] for w in model.wv.vocab.keys()][:n_samples]\n    #word_vectors = [token for token in f_matrix_train][0:n_samples]\n\n    tsne_model = TSNE(n_components=2, verbose=1, random_state=23)\n    tsne_w2v = tsne_model.fit_transform(word_vectors)\n\n    tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n    tsne_df['words'] = [k for k in model.wv.vocab.keys()][:n_samples]\n\n    fig.scatter(x='x', y='y', source=tsne_df)\n    hover = fig.select(dict(type=HoverTool))\n    hover.tooltips={\"word\": \"@words\"}\n    show(fig)","2aca5c5f":"plot_tSNE(W2Vmodel)","3210bea1":"**Text Features Extraction**"}}