{"cell_type":{"32802ec1":"code","0535e2e3":"code","abe4a89c":"code","57273beb":"code","9d75f069":"code","96c79a15":"code","acedb835":"code","2648dd27":"code","5e467721":"code","385fd855":"code","b67ec472":"code","a5bdde39":"code","6fa46cb5":"code","7b8f63b0":"code","423e1b8d":"code","7e384f0b":"code","ac3554a6":"code","054af465":"code","04640d92":"code","4a917555":"code","a0c0dcf7":"code","0e8db133":"code","1b0fa215":"markdown","67b9c846":"markdown","6bf3008c":"markdown","b11fb054":"markdown","7e0df56f":"markdown","d40af4ac":"markdown","c0ec5ea2":"markdown","d9a343d2":"markdown","2c6117aa":"markdown","e0854224":"markdown","634d016c":"markdown","fefaf6b4":"markdown","cc5ce5b5":"markdown","834e021a":"markdown","680ef86a":"markdown","12c98f54":"markdown","4e542535":"markdown"},"source":{"32802ec1":"!pip install syllables\n!pip install rich\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\npd.set_option(\"display.max_colwidth\", None)  # setting the maximum width in characters when displaying pandas column. \"None\" value means unlimited.\n\nimport os      \nimport syllables\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom wordcloud import WordCloud\nfrom rich import theme, console","0535e2e3":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","abe4a89c":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","57273beb":"print(f\"Total entries in train.csv: {len(train_data)}\")\nprint(f\"Total entries in test.csv: {len(test_data)}\")","9d75f069":"train_data.head(3)","96c79a15":"test_data.head(3)","acedb835":"train_data.info()","2648dd27":"test_data.info()","5e467721":"custom_theme = theme.Theme({\n    \"info\" : \"cyan\",\n    \"warning\": \"magenta\",\n    \"danger\": \"bold red\"\n})\n\nconsole = console.Console(theme=custom_theme)","385fd855":"i = random.randrange(train_data.shape[0])   # selecting a random row from the dataframe\n\nconsole.print(train_data.iloc[i].excerpt+'\\n',style='warning')\nconsole.print(f\"Target (standard error) --> {train_data.iloc[i].target} ({train_data.iloc[i].standard_error})\", style = 'info')","b67ec472":"train_target = train_data['target'].values\ntrain_se = train_data['standard_error'].values\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,6))\n\n# hardcoding the appearance of the plots\n\nsns.kdeplot(train_target, ax=ax1, label='Train', lw=5, alpha=0.6)\nax1.axvline(train_target.mean(),linestyle='--', linewidth=2)\nax1.annotate('mean', fontsize=18, xy=(train_target.mean(), 0.2), \n            xytext=(0.8, 0.9), textcoords='axes fraction',\n            arrowprops=dict(facecolor='black', shrink=0.02, connectionstyle=\"arc3,rad=-0.3\",),\n             bbox=dict(boxstyle=\"square\", fc=\"w\", ec=\"k\")\n            )\nax1.tick_params(axis='both', which='major', labelsize=18)\nax1.set_xlabel('target', fontsize=18)\nax1.set_ylabel('density', fontsize=18)\n\n\nsns.kdeplot(train_se, ax=ax2, label='Train', lw=5, alpha=0.6)\nax2.axvline(train_se.mean(),linestyle='--', linewidth=2)\nax2.annotate('mean', fontsize=18, xy=(train_se.mean(), 6),  \n            xytext=(0.4, 0.9), textcoords='axes fraction',\n            arrowprops=dict(facecolor='black', shrink=0.02, connectionstyle=\"arc3,rad=0.3\",),\n             bbox=dict(boxstyle=\"square\", fc=\"w\", ec=\"k\")\n            )\nax2.tick_params(axis='both', which='major', labelsize=18)\nax2.set_xlabel('standard_error', fontsize=18)\nax2.set_ylabel('density', fontsize=18)\n\n\nplt.suptitle('Distribution of targets and standard errors in train.csv', fontsize = 24)\nplt.show()","a5bdde39":"sns.set_theme(style=\"whitegrid\")\n\nfig, ax = plt.subplots(figsize=(8,6))\nsns.scatterplot(y=train_target, x=train_se, color = 'blue', alpha = 0.3, s=50)\nax.tick_params(axis='both', which='major', labelsize=18)\nax.set_xlabel('standard error', fontsize=18)\nax.set_ylabel('target', fontsize=18)\nplt.show()","6fa46cb5":"train_data.sort_values(by='standard_error').head(3)","7b8f63b0":"sns.jointplot(y=train_data['target'], x=train_data['standard_error'], \n              kind='hex', \n              edgecolor='tab:blue', \n              marginal_kws=dict(bins=25), \n              height=8)\nplt.show()","423e1b8d":"console.print(f\"Word cloud generated from the whole dataset:\", style = \"info\")\n\nplt.figure(figsize=(10,10))\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=500).generate(\" \".join(train_data['excerpt']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","7e384f0b":"i = random.randrange(train_data.shape[0])\n\nconsole.print(f\"Word cloud generated from the {i}-th excerpt:\", style = \"info\")\n\nplt.figure(figsize=(10,10))\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=500).generate(train_data.iloc[i].excerpt)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","ac3554a6":"train_data['letter_count'] = train_data['excerpt'].str.len()\ntrain_data['word_count'] = train_data['excerpt'].str.split().apply(len)\ntrain_data['sentence_count'] = train_data['excerpt'].str.split(pat='[.!?]+').str.len()-1\ntrain_data['syllable_count'] = train_data['excerpt'].apply(lambda x: syllables.estimate(x.lower()))","054af465":"train_data['avg_word_len'] = train_data['letter_count']\/train_data['word_count']\ntrain_data['avg_sentence_len'] = train_data['word_count']\/train_data['sentence_count']\ntrain_data['avg_word_syll'] = train_data['syllable_count']\/train_data['word_count']\ntrain_data['avg_sen_syll'] = train_data['syllable_count']\/train_data['sentence_count']","04640d92":"i = random.randrange(train_data.shape[0])\n\nconsole.print(train_data.iloc[i].excerpt+'\\n',style='warning')\nconsole.print(f\"Target (standard error) --> {train_data.iloc[i].target} ({train_data.iloc[i].standard_error})\", style = 'info')\nconsole.print(f\"Total sentences: {train_data.iloc[i].sentence_count}\", style = 'info')\nconsole.print(f\"Total words: {train_data.iloc[i].word_count}\", style = 'info')\nconsole.print(f\"Total syllables: {train_data.iloc[i].syllable_count}\", style = 'info')\nconsole.print(f\"Total letters: {train_data.iloc[i].letter_count}\", style = 'info')","4a917555":"corr = train_data.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True  # so that we do not duplicate values in the correlation matrix\n\nfig, ax = plt.subplots(figsize=(8,8))\nax = sns.heatmap(corr, ax=ax, annot=True, mask=mask, square=True, fmt = '.2f', annot_kws={\"fontsize\":15}, cbar=False)\nax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 18)\nax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 18)\nplt.title('Correlation matrix', fontsize=20)\nplt.show()","a0c0dcf7":"sns.jointplot(y=train_data['target'], x=train_data['avg_word_syll'], \n              kind='hex', \n              edgecolor='tab:blue', \n              marginal_kws=dict(bins=25), \n              height=8)\nplt.show()","0e8db133":"sns.pairplot(train_data, diag_kind='kde')","1b0fa215":"Now, we can print out a random text passage together with the corresponding target and the standard error.","67b9c846":"Finally, let's display the wordclouds. The WordCloud is a technique for showing which words are the most frequent in a given text. ","6bf3008c":"This time, we print out a random text passage together with the corresponding target, the standard error, and some newly generated linguistic features.","b11fb054":"For having clearer and styled output in the terminal, we can configure the print output by using *console* and *theme* classes from the Python **rich** library (check out [the official link](https:\/\/github.com\/willmcgugan\/rich)).","7e0df56f":"Then, we render a scatter plot to reveal the relationship between targets and standard errors.","d40af4ac":"To conclude, we have performed exploratory data analysis on our training dataset. This helps us to understand what kind of input we will be working with when building a readability model.","c0ec5ea2":"It looks like one entry has weird numeric values. Let's inspect the raw dataframe for confirmation.","d9a343d2":"# Basic Exploratory Data Analysis","2c6117aa":"It is also possible to display all correlations and distributions using *pairplot* from the Python **seaborn** library.","e0854224":"Indeed, the excerpt with id=*436ce79fe* has both target and standard error equal to zero. However, as mentioned in [this discussion](http:\/\/https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/236403), this sample is the baseline for all other samples.\n\nWe can also combine results from two previous figures into one using *jointplot* from the **seaborn** package.","634d016c":"To be able to see the relationship between numeric data in the training dataset, we display the correlation matrix.","fefaf6b4":"## Introduction\nThe goal of this competition is to build a model that is able to estimate the readability score of short text excerpts. Traditionally, this can be achieved by applying various readability formulas that depend on linguistic features (e.g. average number of syllables per word, averane sentence length, etc.). Often these features are indeed highly correlated with the reading difficulty, however general measure of the readability score is more complex, as it involves level of abstraction, the use of images and difficult concepts, active and passive voice and so on. Therefore, a data-driven approach together with recent developments in the field of Natural Language Processing can be the next step to improve the current estimations.\n\nThe good models will significantly help teachers, administrators, students, and literacy curriculum developers. ","cc5ce5b5":"# Advanced exploratory data analysis\nIn this section, we are going to display some graphs and figures using the raw data in *train.csv*.\n\nFirst, we can visualize the distribution of target values and standard errors using *seaborn* and *matplotlib*.","834e021a":"# Feature Engineering\nIn the final section, we will construct some extra linguistic features on top of the raw data:\n- letter_count - the total number of letters\n- word_count - the total number of words \n- sentence_count - the total number of sentences\n- syllable_count - the total number of syllables (for this, we can use the Python **syllables** library, see [this reference](https:\/\/github.com\/prosegrinder\/python-syllables))\n- avg_word_len - the average length of words\n- avg_sentence_len - the average length of sentences\n- avg_word_syll - the average number of syllables per word\n- avg_sen_syll - the average number of syllables per sentence","680ef86a":"For example, as one can see, there is a strong correlation between the target and the average number of syllables per word (which is not surprise). So, let's plot the scatter plot for these two columns.","12c98f54":"# Importing packages and loading data","4e542535":"# CommonLit Readability Prize EDA\n<img src=\"https:\/\/library.ucf.edu\/wp-content\/uploads\/sites\/5\/2016\/03\/sort-by-color.jpg\" alt=\"title\">"}}