{"cell_type":{"e549f1b0":"code","f6453ad6":"code","37b350f3":"code","5352bac4":"code","22877b66":"code","f5dcc51e":"code","75bd1ae7":"code","07f457d4":"code","967e1b4d":"code","2824d73b":"code","c76aa791":"code","f709bd50":"code","20c07a3d":"code","ac1ec35c":"code","1b018a5f":"code","0db28123":"code","1e8de858":"code","369b11cb":"code","fd3c2a44":"code","e45e8278":"code","fe4a8896":"code","475dca55":"code","b88c7e5a":"code","b1a6739b":"code","9efc83cf":"code","2acb2ca2":"code","9c345fa8":"code","1e9e644b":"code","6b438e20":"code","f8f1a31d":"code","1192c70f":"code","dba64ceb":"code","e50972ff":"code","df80fb4f":"code","a70f3b02":"code","d8a4abba":"code","3e64cf9d":"code","e630bb88":"code","07d9bac6":"code","e2a79f07":"code","877621ec":"code","85c47eab":"code","2a555300":"code","2fc84b02":"code","3d6652c7":"code","c551ae32":"code","18922f6f":"code","38b3c37a":"code","2ee229f3":"code","b8da5691":"code","56c75500":"code","5ea82ec4":"markdown","29b9d9a0":"markdown","059b7111":"markdown","1c7664da":"markdown","f77147e6":"markdown","a3719238":"markdown","62e16961":"markdown","2afa9259":"markdown","613ff67e":"markdown","b28d5473":"markdown","11095495":"markdown","0f993e6b":"markdown","3b8c8375":"markdown","96db430b":"markdown","9a22ac4c":"markdown","140146ad":"markdown","74fa5fc9":"markdown","af41241b":"markdown","fa568af4":"markdown","7f68fcda":"markdown","5063028a":"markdown","dfeea9c2":"markdown","22719a4b":"markdown","dfc36903":"markdown","bfef92ee":"markdown","ee8eccd1":"markdown","78fae8cc":"markdown","5552ae8f":"markdown","2bc981e6":"markdown","c47d2022":"markdown","987fd1bf":"markdown","1f233993":"markdown"},"source":{"e549f1b0":"import os, random\nimport itertools\nimport math\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer","f6453ad6":"INDEX = 'ID'\nTARGET = 'ACTIVITY'\n\nSEED = 2021\nNUM_CLASSES = 6\nNFOLDS = 5\n\nnum_boost_round = 10000\nearly_stopping_rounds = 100\nverbose_eval = 1000\n\ndef seed_everything(seed=42, is_tensorflow=False, is_torch=False, verbose=True):\n\n    os.environ['PYTHONHASHSEED'] = str(seed) # os\n    random.seed(seed) # random\n    np.random.seed(seed) # numpy\n\n    if is_tensorflow:\n        import tensorflow as tf\n        tf.random.set_seed(seed)\n        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n        tf.config.threading.set_inter_op_parallelism_threads(1)\n        tf.config.threading.set_intra_op_parallelism_threads(1)\n\n    if is_torch:\n        import torch\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False # True: \u518d\u73fe\u6027\u306a\u304f\u306a\u308b\u304c\u3001\u8a08\u7b97\u901f\u304f\u306a\u308b False: \u518d\u73fe\u6027\u304c\u62c5\u4fdd\u3055\u308c\u308b\u304c\u3001\u8a08\u7b97\u9045\u304f\u306a\u308b\n\n    if verbose:\n        print(f'set random seed: {seed}')","37b350f3":"def display_true_pred(y, oof_train):\n    ''' \u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u4e88\u6e2c\u30e9\u30d9\u30eb\u306e\u5206\u5e03\u30d7\u30ed\u30c3\u30c8\u51fa\u529b\u7528 '''\n    fig, ax = plt.subplots(figsize=(8, 8))\n    sns.histplot(oof_train, label='pred', ax=ax, color='black')\n    sns.histplot(y, label='true', ax=ax)\n    ax.legend()\n    ax.grid()\n    plt.show()\n\ndef display_cmx(y, oof_train):\n    ''' \u6df7\u540c\u884c\u5217\u306e\u30d7\u30ed\u30c3\u30c8\u51fa\u529b\u7528 '''\n    labels = sorted(list(set(y)))\n    cmx_data = confusion_matrix(y_true=y, y_pred=oof_train, labels=labels)\n    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(df_cmx, annot=True)\n    plt.xlabel(\"predict\", fontsize=13)\n    plt.ylabel(\"true\", fontsize=13)\n    plt.show()","5352bac4":"class BaseModel:\n    def __init__(self):\n        pass\n    def fit(self, X_train, y_train, X_val, y_val):\n        pass\n    def predict(self, test):\n        pass\n\ndef run_train(train_df, model, is_proba=True, is_torch=False, is_tensorflow=False):\n    ''' \u5b66\u7fd2\u3068\u5404\u7a2e\u5b66\u7fd2\u7d50\u679c\u306e\u51fa\u529b '''\n    X = train_df.drop([INDEX, TARGET], axis=1)\n    y = train_df[TARGET].astype('int')\n    X_std = StandardScaler().fit_transform(X) # \u7dda\u5f62\u30e2\u30c7\u30eb\u3092\u8a66\u3059\u306e\u3067\u4e88\u3081\u6a19\u6e96\u5316\uff08\u30c4\u30ea\u30fc\u30e2\u30c7\u30eb\u306b\u306f\u610f\u5473\u306a\u3044\uff09\n    \n    seed_everything(seed=SEED, is_tensorflow=is_tensorflow, is_torch=is_torch)\n    fold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    oof_train = np.zeros(len(X))\n    \n    for i, (train_idx, val_idx) in enumerate(fold.split(X_std, y)):\n        print(f'**************** FOLD {i+1} ****************')\n        X_train, X_val = X_std[train_idx], X_std[val_idx]\n        y_train, y_val = y.values[train_idx], y.values[val_idx]\n\n        model.fit(X_train, y_train, X_val, y_val)\n\n        # evaluation\n        oof_val = model.predict(X_val)\n        \n        if is_proba:\n            y_val_pred = np.argmax(oof_val, axis=1)\n        else:\n            y_val_pred = oof_val\n        \n        oof_train[val_idx] = y_val_pred\n        print(f'[FOLD {i+1}] OOF Accuracy: {accuracy_score(y_true=y_val, y_pred=y_val_pred):.5f}\\n')\n\n    print(f'[RESULT] Accuracy: {accuracy_score(y_true=y, y_pred=oof_train):.5f}\\n')\n    \n    print('\u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u4e88\u6e2c\u30e9\u30d9\u30eb\u306e\u5206\u5e03\u30d7\u30ed\u30c3\u30c8\uff1a')\n    display_true_pred(y, oof_train)\n    \n    print('\u6df7\u540c\u884c\u5217\uff1a')\n    display_cmx(y, oof_train)\n    \n    del model, X, X_std\n    del X_train, X_val\n    del y, y_train, y_val\n    gc.collect()","22877b66":"train = pd.read_csv(\"..\/input\/cdleyouth01\/df_train.csv\")","f5dcc51e":"from sklearn.linear_model import LogisticRegression\n\nclass LogitModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = LogisticRegression(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","75bd1ae7":"params = {\n    'random_state': SEED,\n    'max_iter': 10000,\n    'class_weight': 'balanced'\n}\nrun_train(train_df=train, model=LogitModel(params))","07f457d4":"from sklearn.linear_model import RidgeClassifier\n\nclass RidgeModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = RidgeClassifier(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict(test)","967e1b4d":"params = {\n    'random_state': SEED,\n    'max_iter': 10000,\n    'class_weight': 'balanced'\n}\nrun_train(train_df=train, model=RidgeModel(params), is_proba=False)","2824d73b":"from sklearn.tree import DecisionTreeClassifier\n\nclass DecisionTreeModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = DecisionTreeClassifier(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","c76aa791":"params = {\n    'random_state': SEED,\n    'criterion': 'entropy',\n    'splitter': 'best',\n    'max_depth': 56,\n    'min_weight_fraction_leaf': 0.01,\n    'class_weight': 'balanced'\n}\nrun_train(train_df=train, model=DecisionTreeModel(params))","f709bd50":"from sklearn.ensemble import ExtraTreesClassifier\n\nclass ExtraTreesModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = ExtraTreesClassifier(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","20c07a3d":"params = {\n    'n_estimators': 1000,\n    'max_depth': 56,\n    'criterion': 'entropy',\n    'bootstrap': True,\n    'oob_score': False,\n    'n_jobs': -1,\n    'verbose': 0,\n    'random_state': SEED,\n    'class_weight': 'balanced',\n}\nrun_train(train_df=train, model=ExtraTreesModel(params))","ac1ec35c":"from sklearn.ensemble import RandomForestClassifier\n\nclass RandomForestModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = RandomForestClassifier(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","1b018a5f":"params = {\n    'n_estimators': 1000,\n    'max_depth': 56,\n    'criterion': 'entropy',\n    'bootstrap': True,\n    'oob_score': False,\n    'n_jobs': -1,\n    'verbose': 0,\n    'random_state': SEED,\n    'class_weight': 'balanced',\n}\nrun_train(train_df=train, model=RandomForestModel(params))","0db28123":"import xgboost as xgb\n\nclass XGBModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val, y_val):\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n        \n        self.model = xgb.train(params=self.params, dtrain=dtrain, \n                               evals=[(dtrain, 'train'), (dval, 'eval')],\n                               num_boost_round=num_boost_round, \n                               early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose_eval)\n        \n    def predict(self, test):\n        dtest = xgb.DMatrix(test)\n        return self.model.predict(dtest, ntree_limit=self.model.best_ntree_limit)","1e8de858":"params = {\n    'objective': 'multi:softprob',\n    'booster': 'gbtree',\n    'eval_metric': 'mlogloss',\n    'seed': SEED,\n    'num_class': NUM_CLASSES\n}\n\nrun_train(train_df=train, model=XGBModel(params))","369b11cb":"import catboost as cb\n\nclass CBModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val, y_val):\n        cat_train = cb.Pool(X_train, label=y_train)\n        cat_val = cb.Pool(X_val, label=y_val)\n        \n        self.model = cb.CatBoost(self.params)\n        self.model.fit(cat_train,\n                       eval_set=[cat_val],\n                       use_best_model=True,\n                       verbose_eval=verbose_eval)\n        \n    def predict(self, test):\n        return self.model.predict(test)","fd3c2a44":"params = {\n    'loss_function' : 'MultiClassOneVsAll',\n    'eval_metric' : 'MultiClassOneVsAll',\n    'bootstrap_type': 'Bayesian',\n    'boosting_type': 'Plain',\n    'max_depth': 8,\n    'learning_rate': 0.007,\n    'max_bin': 350,\n    'min_data_in_leaf': 64,\n    'l2_leaf_reg': 0.001,\n    'bagging_temperature': 0.001,\n    'od_type' : 'Iter',\n    'task_type' : 'GPU',\n    'auto_class_weights': 'Balanced',\n    'classes_count': NUM_CLASSES,\n    'random_seed': SEED,\n    'num_boost_round': num_boost_round,\n    'early_stopping_rounds': early_stopping_rounds,\n}\n\n# \u6642\u9593\u304b\u304b\u308b\u306e\u3067\u3084\u3081\u3068\u304d\u307e\u3059\n# run_train(train_df=train, model=CBModel(params))","e45e8278":"from sklearn.ensemble import GradientBoostingClassifier\n\nclass GBDTModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = GradientBoostingClassifier(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","fe4a8896":"params = dict(\n    loss='deviance',\n    learning_rate=0.1,\n    n_estimators=10000,\n    subsample=1.0,\n    criterion='friedman_mse',\n    min_samples_split=2,\n    min_samples_leaf=1,\n    min_weight_fraction_leaf=0.0,\n    max_depth=3,\n    min_impurity_decrease=0.0,\n    min_impurity_split=None,\n    init=None,\n    random_state=SEED,\n    max_features=None,\n    verbose=0,\n    max_leaf_nodes=None,\n    warm_start=False,\n    validation_fraction=0.1,\n    n_iter_no_change=None,\n    tol=0.0001,\n    ccp_alpha=0.0,\n)\n\n# \u5b66\u7fd2\u306b\u6642\u9593\u304b\u304b\u308b\u306e\u3067\u4eca\u56de\u3084\u308a\u307e\u305b\u3093\n# run_train(train_df=train, model=GBDTModel(params))","475dca55":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nclass HistGBDTModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = HistGradientBoostingClassifier(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","b88c7e5a":"params = dict(\n    loss='auto',\n    learning_rate=0.1,\n    max_iter=10000,\n    max_leaf_nodes=31,\n    max_depth=None,\n    min_samples_leaf=20,\n    l2_regularization=0.0,\n    max_bins=255,\n    categorical_features=None,\n    monotonic_cst=None,\n    warm_start=False,\n    early_stopping='auto',\n    scoring='loss',\n    validation_fraction=0.1,\n    n_iter_no_change=10,\n    tol=1e-07,\n    verbose=0,\n    random_state=SEED,\n)\n\nrun_train(train_df=train, model=HistGBDTModel(params))","b1a6739b":"from sklearn.svm import SVC\n\nclass SVMModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = SVC(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","9efc83cf":"params = {\n    'C': 20,\n    'kernel': 'rbf',\n    'degree': 3,\n    'gamma': 'scale',\n    'class_weight': 'balanced',\n    'max_iter': 10000,\n    'probability': True,\n    'random_state': SEED\n}\n\n# \u5b66\u7fd2\u306b\u6642\u9593\u304b\u304b\u308b\u306e\u3067\u4eca\u56de\u3084\u308a\u307e\u305b\u3093\n# run_train(train_df=train, model=SVMModel(params))","2acb2ca2":"from sklearn.neighbors import KNeighborsClassifier\n\nclass KNNModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = KNeighborsClassifier(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict(test)","9c345fa8":"params = {\n    'weights': 'distance',\n    'algorithm': 'auto',\n    'p': 2,\n    'metric': 'minkowski',\n    'n_neighbors': 7,\n}\n\nrun_train(train_df=train, model=KNNModel(params), is_proba=False)","1e9e644b":"from sklearn.semi_supervised import LabelSpreading\n\nclass LSModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = LabelSpreading(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","6b438e20":"params = {\n    'kernel': 'rbf',\n    'gamma': 50,\n    'alpha': 0.3,\n    'n_neighbors': 7,\n    'max_iter': 10000,\n    'tol': 1e-3\n}\n\nrun_train(train_df=train, model=LSModel(params))","f8f1a31d":"from sklearn.semi_supervised import LabelPropagation\n\nclass LPModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val=None, y_val=None):\n        self.model = LabelPropagation(**self.params)\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","1192c70f":"params = {\n    'kernel': 'rbf',\n    'gamma': 50.389,\n    'n_neighbors': 7,\n    'max_iter': 10000,\n    'tol': 1e-3\n}\n\nrun_train(train_df=train, model=LPModel(params))","dba64ceb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.utils import class_weight","e50972ff":"# MLP model\ndef MLP(input_shape, optim_params, num_class=11):\n\n    inputs = Input(shape=input_shape)\n    x = Dense(2 ** 12, activation='relu', kernel_initializer='normal')(inputs)\n    x = Dense(2 ** 10, activation='relu', kernel_initializer='normal')(x)\n    x = BatchNormalization()(x)\n    x = Dense(2 ** 8, activation='relu', kernel_initializer='normal')(x)\n    x = Dense(2 ** 6, activation='relu', kernel_initializer='normal')(x)\n    x = Dropout(0.25)(x)\n    x = Dense(num_class, activation='softmax')(x)\n\n    model = Model(inputs=inputs, outputs=x)\n    model.compile(optimizer=SGD(**optim_params), loss='categorical_crossentropy')\n    return model","df80fb4f":"class MLPModel(BaseModel):\n    def __init__(self, optimizer_params, scheduler_params, batch_size, epochs, num_class=2):\n        self.optimizer_params = optimizer_params\n        self.scheduler_params = scheduler_params\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.num_class = num_class\n        self.lr_reduction = ReduceLROnPlateau(**scheduler_params)\n        \n    def fit(self, X_train, y_train, X_val, y_val):\n        self.model = MLP(input_shape=(X_train.shape[1], ),\n                         optim_params=self.optimizer_params,\n                         num_class=self.num_class)\n        \n        train_weight = class_weight.compute_class_weight('balanced', \n                                                         np.unique(y_train), \n                                                         y_train)\n        \n        weight = dict()\n        for i, w in enumerate(train_weight):\n            weight[i] = train_weight[i]\n        \n        # onehot\n        y_train = keras.utils.to_categorical(y_train, num_classes=self.num_class)\n        y_val = keras.utils.to_categorical(y_val, num_classes=self.num_class)\n        \n        self.model.fit(X_train, y_train, batch_size=batch_size,\n                       epochs=epochs, validation_data=(X_val, y_val),\n                       class_weight=weight,\n                       callbacks=[self.lr_reduction],\n                       verbose=False)\n        \n    def predict(self, test):\n        return self.model.predict(test)","a70f3b02":"batch_size = 128\nepochs = 50\noptimizer_params = {\n    'lr': 0.005,\n    'momentum': 0.9,\n    'nesterov': True\n}\nscheduler_params = {\n    'monitor': 'val_loss',\n    'factor': 0.8,\n    'patience': 5,\n    'verbose': True,\n    'mode': 'auto',\n    'cooldown': 0,\n    'min_lr': 1e-8\n}\n\nrun_train(train_df=train, model=MLPModel(optimizer_params, scheduler_params, batch_size=batch_size, epochs=epochs, num_class=NUM_CLASSES), is_tensorflow=True)","d8a4abba":"!pip install pytorch-tabnet","3e64cf9d":"import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom pytorch_tabnet.tab_model import TabNetClassifier # https:\/\/github.com\/dreamquark-ai\/tabnet","e630bb88":"# tabnet\u7528\nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes=5, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","07d9bac6":"class TabNetModel(BaseModel):\n    def __init__(self, fitting_params, optimizer_params, scheduler_params, num_class=2):\n        self.fitting_params = fitting_params\n        self.optimizer_params = optimizer_params\n        self.scheduler_params = scheduler_params\n        self.num_class = num_class\n        \n    def fit(self, X_train, y_train, X_val, y_val):\n        self.model = TabNetClassifier(n_d=54, n_a=54,\n                                     n_steps=6, gamma=0.85,\n                                     n_independent=2, n_shared=2,\n                                     momentum=0.02,\n                                     lambda_sparse=0,\n                                     optimizer_fn=Adam,\n                                     optimizer_params=self.optimizer_params,\n                                     scheduler_fn=CosineAnnealingWarmRestarts,\n                                     scheduler_params=self.scheduler_params,\n                                     mask_type='entmax',\n                                     seed=SEED)\n        \n        # train weight\n        w_tr = 1 \/ pd.DataFrame(y_train).reset_index().groupby(0).count().values\n        w_tr = w_tr[y_train].ravel()\n        w_tr \/= w_tr.sum()\n        train_weight = list(w_tr)\n        \n        self.model.fit(X_train=X_train, y_train=y_train,\n                       eval_set=[(X_train, y_train), (X_val, y_val)],\n                       eval_name=['train', 'valid'],\n                       eval_metric=['accuracy'],\n                       weights=train_weight,\n                       loss_fn=LabelSmoothingLoss(classes=self.num_class, smoothing=0.001),\n                       drop_last=True,\n                       **self.fitting_params)\n        \n    def predict(self, test):\n        return self.model.predict_proba(test)","e2a79f07":"fitting_params = {\n    'batch_size': 2048,\n    'virtual_batch_size': 256,\n    'max_epochs': 1000,\n    'patience': 50\n}\noptimizer_params = {\n    'lr': 0.05,\n    'weight_decay': 1e-6\n}\nscheduler_params = {\n    'T_0': 50,\n    'T_mult': 3,\n    'eta_min': 0.001,\n    'last_epoch': -1\n}\n\nrun_train(train_df=train, model=TabNetModel(fitting_params, optimizer_params, scheduler_params, num_class=NUM_CLASSES), is_torch=True)","877621ec":"def noise(array):\n    height = len(array)\n    width = len(array[0])\n    rands = np.random.uniform(0, 1, (height, width) )\n    copy  = np.copy(array)\n\n    for h in range(height):\n        for w in range(width):\n            if rands[h, w] <= 0.20:\n                swap_target_h = random.randint(0,height)\n                copy[h, w] = array[swap_target_h-1, w]\n    return copy\n\ndef AutoEncoder(input_shape):\n    inputs = Input(shape=input_shape)\n    encoded = Dense(2 ** 6, activation='elu')(inputs)\n    encoded = Dense(2 ** 4, activation='elu')(encoded)\n    decoded = Dense(2 ** 6, activation='elu')(encoded)\n    decoded = Dense(input_shape[0], activation='elu')(decoded)\n\n    autoencoder = Model(inputs=inputs, outputs=decoded)\n    autoencoder.compile(optimizer='adadelta', loss='mse')\n\n    return autoencoder","85c47eab":"def DAETransfomer(train, test=None):\n    seed_everything(SEED, verbose=False, is_tensorflow=True)\n    X = train.drop([INDEX, TARGET], axis=1)\n#     test = test.drop([INDEX], axis=1)\n\n    X_std = StandardScaler().fit_transform(X)\n    autoencoder = AutoEncoder(input_shape=(X.shape[1], ))\n    # add noise\n    X_noised = noise(X_std)\n    autoencoder.fit(x=X_noised,\n                    y=X_std,\n                    epochs=2000,\n                    batch_size=1024,\n                    shuffle=True,\n                    validation_split=0.2)\n\n    encoder = Model(inputs=autoencoder.input, outputs=autoencoder.layers[2].output)\n    train_transformed = pd.DataFrame(encoder.predict(X_std))\n#     test_transformed = pd.DataFrame(encoder.predict(test))\n\n    train_transformed[INDEX] = train[INDEX]\n    train_transformed[TARGET] = train[TARGET]\n\n#     return train_transformed, test_transformed\n\n    return train_transformed","2a555300":"train_dae = DAETransfomer(train)","2fc84b02":"def MLP_with_dae(input_shape, optim_params, num_class=11):\n    inputs = Input(shape=input_shape)\n    x = BatchNormalization()(inputs)\n    x = Dense(2 ** 9, activation=None, kernel_initializer='normal')(x)\n    x = LeakyReLU(alpha=0.1)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n\n    x = Dense(2 ** 9, activation=None, kernel_initializer='normal')(x)\n    x = LeakyReLU(alpha=0.1)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n\n    x = Dense(2 ** 8, activation=None, kernel_initializer='normal')(x)\n    x = LeakyReLU(alpha=0.1)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.25)(x)\n\n    x = Dense(num_class, activation='softmax', kernel_initializer='normal')(x)\n\n    model = Model(inputs=inputs, outputs=x)\n    model.compile(optimizer=tf.keras.optimizers.Adam(**optim_params), loss='categorical_crossentropy')\n    return model","3d6652c7":"class DAEMLPModel(BaseModel):\n    def __init__(self, optimizer_params, scheduler_params, batch_size, epochs, num_class=2):\n        self.optimizer_params = optimizer_params\n        self.scheduler_params = scheduler_params\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.num_class = num_class\n        self.lr_reduction = ReduceLROnPlateau(**scheduler_params)\n        \n    def fit(self, X_train, y_train, X_val, y_val):\n        self.model = MLP_with_dae(input_shape=(X_train.shape[1], ),\n                                  optim_params=self.optimizer_params,\n                                  num_class=self.num_class)\n        \n        train_weight = class_weight.compute_class_weight('balanced', \n                                                         np.unique(y_train), \n                                                         y_train)\n        \n        weight = dict()\n        for i, w in enumerate(train_weight):\n            weight[i] = train_weight[i]\n        \n        # onehot\n        y_train = keras.utils.to_categorical(y_train, num_classes=self.num_class)\n        y_val = keras.utils.to_categorical(y_val, num_classes=self.num_class)\n        \n        self.model.fit(X_train, y_train, batch_size=batch_size,\n                       epochs=epochs, validation_data=(X_val, y_val),\n                       class_weight=weight,\n                       callbacks=[self.lr_reduction],\n                       verbose=False)\n        \n    def predict(self, test):\n        return self.model.predict(test)","c551ae32":"batch_size = 1024\nepochs = 50\noptimizer_params = {\n    'learning_rate': 0.01\n}\nscheduler_params = {\n    'monitor': 'val_loss',\n    'factor': 0.8,\n    'patience': 5,\n    'verbose': True,\n    'mode': 'auto',\n    'cooldown': 0,\n    'min_lr': 1e-8\n}\n\nrun_train(train_df=train_dae, model=DAEMLPModel(optimizer_params, scheduler_params, batch_size=batch_size, epochs=epochs, num_class=NUM_CLASSES), is_tensorflow=True)","18922f6f":"import lightgbm as lgb\n\nclass LGBModel(BaseModel):\n    def __init__(self, params):\n        self.params = params\n        \n    def fit(self, X_train, y_train, X_val, y_val):\n        lgb_train = lgb.Dataset(X_train, label=y_train)\n        lgb_val = lgb.Dataset(X_val, label=y_val)\n        \n        self.model = lgb.train(params,\n                               lgb_train,\n                               valid_names=[\"train\", \"valid\"],\n                               valid_sets=[lgb_train, lgb_val],\n                               num_boost_round=num_boost_round,\n                               early_stopping_rounds=early_stopping_rounds,\n                               verbose_eval=verbose_eval)\n        \n    def predict(self, test):\n        return self.model.predict(test, num_iteration=self.model.best_iteration)","38b3c37a":"params = {\n    'objective': 'multiclass',\n    'boosting': 'gbdt',\n    'metric': 'multi_logloss',\n    'feature_pre_filter': False,\n    'num_leaves': 39,\n    'bagging_freq': 3,\n    'min_child_samples': 32,\n    'learning_rate': 0.0926,\n    'lambda_l1': 0.00000049,\n    'lambda_l2': 0.00000058,\n    'feature_fraction': 0.56,\n    'bagging_fraction': 0.99,\n    'max_bin': 349,\n    'verbosity': -1,\n    'num_class': NUM_CLASSES,\n    'seed': SEED\n}\n\nrun_train(train_df=train_dae, model=LGBModel(params))","2ee229f3":"def run_train_ensemble(train_df, models: list, is_proba=True, is_torch=False, is_tensorflow=False):\n    ''' \u5b66\u7fd2\u3068\u5404\u7a2e\u5b66\u7fd2\u7d50\u679c\u306e\u51fa\u529b '''\n    X = train_df.drop([INDEX, TARGET], axis=1)\n    y = train_df[TARGET].astype('int')\n    X_std = StandardScaler().fit_transform(X) # \u7dda\u5f62\u30e2\u30c7\u30eb\u3092\u8a66\u3059\u306e\u3067\u4e88\u3081\u6a19\u6e96\u5316\uff08\u30c4\u30ea\u30fc\u30e2\u30c7\u30eb\u306b\u306f\u610f\u5473\u306a\u3044\uff09\n    \n    for j, model in enumerate(models):\n        seed_everything(seed=SEED, is_tensorflow=is_tensorflow, is_torch=is_torch)\n        fold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n        oof_train = np.zeros(len(X))\n        oof_train_proba = np.zeros(shape=(len(X), NUM_CLASSES))\n\n        for i, (train_idx, val_idx) in enumerate(fold.split(X_std, y)):\n            print(f'**************** FOLD {i+1} ****************')\n            X_train, X_val = X_std[train_idx], X_std[val_idx]\n            y_train, y_val = y.values[train_idx], y.values[val_idx]\n\n            model.fit(X_train, y_train, X_val, y_val)\n\n            # evaluation\n            oof_val_proba = model.predict(X_val)\n\n            if is_proba:\n                oof_val = np.argmax(oof_val_proba, axis=1)\n\n            oof_train[val_idx] = oof_val\n            oof_train_proba[val_idx] = oof_val_proba\n            print(f'[FOLD {i+1}] OOF Accuracy: {accuracy_score(y_true=y_val, y_pred=oof_val):.5f}\\n')\n\n        print(f'[RESULT] Accuracy: {accuracy_score(y_true=y, y_pred=oof_train):.5f}\\n')\n        \n        if j == 0:\n            train_pred_proba = oof_train_proba\n        else:\n            train_pred_proba += oof_train_proba\n            \n        del model\n        gc.collect()\n            \n    y_pred = np.argmax(train_pred_proba, axis=1)\n    print(f'[RESULT] Ensemble Accuracy: {accuracy_score(y_true=y, y_pred=y_pred):.5f}\\n')\n    \n    print('\u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u4e88\u6e2c\u30e9\u30d9\u30eb\u306e\u5206\u5e03\u30d7\u30ed\u30c3\u30c8\uff1a')\n    display_true_pred(y, y_pred)\n    \n    print('\u6df7\u540c\u884c\u5217\uff1a')\n    display_cmx(y, y_pred)","b8da5691":"dt_params = {\n    'random_state': SEED,\n    'criterion': 'entropy',\n    'splitter': 'best',\n    'max_depth': 56,\n    'min_weight_fraction_leaf': 0.01,\n    'class_weight': 'balanced'\n}\n\nex_params = {\n    'n_estimators': 1000,\n    'max_depth': 56,\n    'criterion': 'entropy',\n    'bootstrap': True,\n    'oob_score': False,\n    'n_jobs': -1,\n    'verbose': 0,\n    'random_state': SEED,\n    'class_weight': 'balanced',\n}\n\nrf_params = {\n    'n_estimators': 1000,\n    'max_depth': 56,\n    'criterion': 'entropy',\n    'bootstrap': True,\n    'oob_score': False,\n    'n_jobs': -1,\n    'verbose': 0,\n    'random_state': SEED,\n    'class_weight': 'balanced',\n}\n\nxgb_params = {\n    'objective': 'multi:softprob',\n    'booster': 'gbtree',\n    'eval_metric': 'mlogloss',\n    'seed': SEED,\n    'num_class': NUM_CLASSES\n}\n\nhgbdt_params = dict(\n    loss='auto',\n    learning_rate=0.1,\n    max_iter=10000,\n    max_leaf_nodes=31,\n    max_depth=None,\n    min_samples_leaf=20,\n    l2_regularization=0.0,\n    max_bins=255,\n    categorical_features=None,\n    monotonic_cst=None,\n    warm_start=False,\n    early_stopping='auto',\n    scoring='loss',\n    validation_fraction=0.1,\n    n_iter_no_change=10,\n    tol=1e-07,\n    verbose=0,\n    random_state=SEED,\n)\n\nsvm_params = {\n    'C': 38,\n    'kernel': 'rbf',\n    'degree': 3,\n    'gamma': 'scale',\n    'class_weight': 'balanced',\n    'max_iter': 10000,\n    'probability': True,\n    'random_state': SEED\n}\n\nls_params = {\n    'kernel': 'rbf',\n    'gamma': 50,\n    'alpha': 0.3,\n    'n_neighbors': 7,\n    'max_iter': 10000,\n    'tol': 1e-3\n}\n\nbatch_size = 128\nepochs = 50\noptimizer_params = {\n    'lr': 0.005,\n    'momentum': 0.9,\n    'nesterov': True\n}\nscheduler_params = {\n    'monitor': 'val_loss',\n    'factor': 0.8,\n    'patience': 5,\n    'verbose': True,\n    'mode': 'auto',\n    'cooldown': 0,\n    'min_lr': 1e-8\n}\n\nmodels = [\n    DecisionTreeModel(dt_params),\n    ExtraTreesModel(ex_params),\n    RandomForestModel(rf_params),\n    XGBModel(xgb_params),\n    HistGBDTModel(hgbdt_params),\n    SVMModel(svm_params),\n    LSModel(ls_params),\n    MLPModel(optimizer_params, scheduler_params, batch_size=batch_size, epochs=epochs, num_class=NUM_CLASSES)    \n]","56c75500":"run_train_ensemble(train_df=train, models=models, is_proba=True, is_torch=True, is_tensorflow=True)","5ea82ec4":"## 1. ensemble\n\u69d8\u3005\u306a\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u624b\u6cd5\u3042\u308a\u307e\u3059\u304c\u3001\u4eca\u56de\u306f\u5404\u30e2\u30c7\u30eb\u3067\u306e\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3067\u7b97\u51fa\u3057\u307e\u3059\u3002","29b9d9a0":"## 5. HistGradientBoosting","059b7111":"## 2. LightGBM","1c7664da":"## 2. Extra Trees","f77147e6":"### DAE+LGB","a3719238":"## 4. GradientBoosting","62e16961":"## 3. Random Forest","2afa9259":"## 2. TabNet","613ff67e":"## 1. KNN","b28d5473":"# \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30e2\u30c7\u30eb(NN)","11095495":"## 2. stacking\n\u5c11\u3005\u51e6\u7406\u304c\u9762\u5012\u306a\u306e\u3067\u3001\u6642\u9593\u304c\u3042\u3063\u305f\u3089\u5f8c\u3005\u8ffd\u8a18\u3057\u307e\u3059\u3002","0f993e6b":"## 2. Ridge","3b8c8375":"# \u52fe\u914d\u30d6\u30fc\u30b9\u30c6\u30a3\u30f3\u30b0\u6c7a\u5b9a\u6728(GBDT)","96db430b":"## 2. Label Propagation","9a22ac4c":"## 1. Logistic regression","140146ad":"# \u534a\u6559\u5e2b\u3042\u308a(semi-supervised)","74fa5fc9":"## 3. CatBoost","af41241b":"## 1. Label Spreading","fa568af4":"# \u6728\u30e2\u30c7\u30eb(Tree Model)","7f68fcda":"### DAE+MLP","5063028a":"# \u4e8b\u524d\u6e96\u5099","dfeea9c2":"## 1. SVM","22719a4b":"## 1. MLP","dfc36903":"## 1. Decision Tree","bfef92ee":"# \u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30de\u30b7\u30f3(SVM)","ee8eccd1":"# \u3053\u306eNoteBook\u3067\u306f\u69d8\u3005\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u30e2\u30c7\u30eb\u3092\u7d44\u3093\u3067\u3044\u304d\u307e\u3059\n\u4eca\u307e\u3067\u7d39\u4ecb\u3057\u305f\u306e\u306f\u3059\u3079\u3066LightGBM\u306b\u3088\u308b\u30e2\u30c7\u30eb\u3067\u3057\u305f\u304c\u3001\u4eca\u56de\u306f\u305d\u306e\u4ed6\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u7d39\u4ecb\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002  \n\u306a\u304a\u3001\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u8aac\u660e\u306f\u7701\u7565\u3057\u307e\u3059\u306e\u3067\u6c17\u306b\u306a\u3063\u305f\u3089\u5404\u81ea\u8abf\u3079\u3066\u304f\u3060\u3055\u3044\u3002\n- \u4e8b\u524d\u6e96\u5099\n- \u7dda\u5f62\u30e2\u30c7\u30eb(Linear Model)\n 1. Logistic Regression\n 2. Ridge\n- \u6728\u30e2\u30c7\u30eb(Tree Model)\n 1. Decision Tree\n 2. Extra Trees\n 3. Random Forest\n- \u52fe\u914d\u30d6\u30fc\u30b9\u30c6\u30a3\u30f3\u30b0\u6c7a\u5b9a\u6728(GBDT)\n 1. XGBoost\n 2. LightGBM\n 4. Catboost\n 6. GradientBoosting\n 7. HistGradientBoosting\n- \u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30de\u30b7\u30f3(SVM)\n 1. SVM\n- k\u8fd1\u508d\u6cd5(K-Nearest Neighbors)\n 1. KNN\n- \u534a\u6559\u5e2b\u3042\u308a(semi-supervised)\n 1. Label Spreading\n- \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30e2\u30c7\u30eb(NN)\n 1. MLP\n 2. TabNet\n 3. DAE+alpha\n- \u8907\u6570\u30e2\u30c7\u30eb\u306b\u3088\u308b\u7cbe\u5ea6\u5411\u4e0a\u624b\u6cd5\n 1. ensemble\n 2. stacking","78fae8cc":"# k\u8fd1\u508d\u6cd5(K-Nearest Neighbors)","5552ae8f":"## 3. Denoising Auto Encoder +alpha","2bc981e6":"# \u7dda\u5f62\u30e2\u30c7\u30eb(Linear Model)","c47d2022":"## 1. XGBoost","987fd1bf":"LightGBM\u306f\u7701\u7565\u3057\u307e\u3059\u3002","1f233993":"# \u8907\u6570\u30e2\u30c7\u30eb\u306b\u3088\u308b\u7cbe\u5ea6\u5411\u4e0a\u624b\u6cd5"}}