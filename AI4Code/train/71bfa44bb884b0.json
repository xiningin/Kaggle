{"cell_type":{"97194579":"code","a4b88494":"code","15507ecb":"code","cf2711b5":"code","7d079e80":"code","6822d3ac":"code","7e5fff8b":"code","1c79f69d":"code","786f4e94":"code","205cfca1":"code","df48b0f9":"code","fc42eaaf":"markdown","a26675d6":"markdown","ce3118b2":"markdown","238dc565":"markdown","c92534e5":"markdown","94cb9466":"markdown","b17fce8b":"markdown","3273f8f0":"markdown","d6c3a5fb":"markdown","c359950d":"markdown","b04e37c3":"markdown","41e62ea0":"markdown","49003b1a":"markdown","432e2ddd":"markdown","c8cea66c":"markdown","9c624fd7":"markdown","d5c056d2":"markdown","9a7d9de2":"markdown","bcd82243":"markdown","4a9e8992":"markdown","0dc62241":"markdown"},"source":{"97194579":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","a4b88494":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten","15507ecb":"from keras.datasets import cifar10\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()","cf2711b5":"print('Training data shape : ', train_images.shape, train_labels.shape)\n\nprint('Testing data shape : ', test_images.shape, test_labels.shape)\n\n# Find the unique numbers from the train labels\nclasses = np.unique(train_labels)\nnClasses = len(classes)\nprint('Total number of outputs : ', nClasses)\nprint('Output classes : ', classes)\n\nplt.figure(figsize=[4,2])\n\n# Display the first image in training data\nplt.subplot(121)\nplt.imshow(train_images[0,:,:], cmap='gray')\nplt.title(\"Ground Truth : {}\".format(train_labels[0]))\n\n# Display the first image in testing data\nplt.subplot(122)\nplt.imshow(test_images[0,:,:], cmap='gray')\nplt.title(\"Ground Truth : {}\".format(test_labels[0]))","7d079e80":"nRows,nCols,nDims = train_images.shape[1:]\ntrain_data = train_images.reshape(train_images.shape[0], nRows, nCols, nDims)\ntest_data = test_images.reshape(test_images.shape[0], nRows, nCols, nDims)\ninput_shape = (nRows, nCols, nDims)\n\ntrain_data = train_data.astype('float32')\ntest_data = test_data.astype('float32')","6822d3ac":"train_data \/= 255\ntest_data \/= 255\n\ntrain_labels_one_hot = to_categorical(train_labels)\ntest_labels_one_hot = to_categorical(test_labels)","7e5fff8b":"print('Original label 0 : ', train_labels[0])\nprint('After conversion to categorical ( one-hot ) : ', train_labels_one_hot[0])","1c79f69d":"def createModel():\n    model = Sequential()\n    # The first two layers with 32 filters of window size 3x3\n    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(nClasses, activation='softmax'))\n    \n    return model","786f4e94":"model1 = createModel()\nbatch_size = 256\nepochs = 50\nmodel1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","205cfca1":"model1.summary()\n","df48b0f9":"history = model1.fit(train_data, train_labels_one_hot, batch_size=batch_size, epochs=epochs, verbose=1, \n                   validation_data=(test_data, test_labels_one_hot))\nmodel1.evaluate(test_data, test_labels_one_hot)","fc42eaaf":"Pooling layer is used to reduce the spatial volume of input image after convolution. It is used between two convolution layer. If we apply FC after Convo layer without applying pooling or max pooling, then it will be computationally expensive and we don\u2019t want it. So, the max pooling is only way to reduce the spatial volume of input image. In the above example, we have applied max pooling in single depth slice with Stride of 2. You can observe the 4 x 4 dimension input is reduce to 2 x 2 dimension.\n\nThere is no parameter in pooling layer but it has two hyperparameters \u2014 Filter(F) and Stride(S).\n\nIn general, if we have input dimension W1 x H1 x D1, then\n\n\nW2 = (W1\u2212F)\/S+1\n\nH2 = (H1\u2212F)\/S+1\n\nD2 = D1\n\nWhere W2, H2 and D2 are the width, height and depth of output.","a26675d6":"Display the change for category label using one-hot encoding.\n","ce3118b2":"![img](https:\/\/miro.medium.com\/max\/875\/1*0bCOXLJLxkRktiTTDug6GA.png)\n\n\n# 4.1 Input Layer\nInput layer in CNN should contain image data. Image data is represented by three dimensional matrix as we saw earlier. You need to reshape it into a single column. Suppose you have image of dimension 28 x 28 =784, you need to convert it into 784 x 1 before feeding into input. If you have \u201cm\u201d training examples then dimension of input will be (784, m).\n# 4.2. Convo Layer\nConvo layer is sometimes called feature extractor layer because features of the image are get extracted within this layer. First of all, a part of image is connected to Convo layer to perform convolution operation as we saw earlier and calculating the dot product between receptive field(it is a local region of the input image that has the same size as that of filter) and the filter. Result of the operation is single integer of the output volume. Then we slide the filter over the next receptive field of the same input image by a Stride and do the same operation again. We will repeat the same process again and again until we go through the whole image. The output will be the input for the next layer.\n\nConvo layer also contains ReLU activation to make all negative value to zero.\n# 4.3. Pooling Layer\n![img](https:\/\/miro.medium.com\/max\/875\/1*GksqN5XY8HPpIddm5wzm7A.jpeg)","238dc565":"# 4. Layers in CNN\nThere are five different layers in CNN\n* Input layer\n* Convo layer (Convo + ReLU)\n* Pooling layer\n* Fully connected(FC) layer\n* Softmax\/logistic layer\n* Output layer","c92534e5":"Normalize the data between 0\u20131 by dividing train data and test data with 255 then convert all labels into one-hot vector with to_catagorical() function.","94cb9466":"# 5. Keras Implementation\nWe will use CIFAR-10 dataset to build a CNN image classifier. CIFAR-10 dataset has 10 different labels\n* Airplane\n* Automobile\n* Bird\n* Cat\n* Deer\n* Dog\n* Frog\n* Horse\n* Ship\n* Truck\n\nIt has 50,000 training data and 10,000 testing image data. Image size in CIFAR-10 is 32 x 32 x 3. It comes with Keras library.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*_p1DwmIPQU_gAiB197PBMQ.png)\n\n\nIf you are using google colaboratory, then make sure you are using GPU. To check whether your GPU is on or not. Try following code.\n","b17fce8b":"# 4.4. Fully Connected Layer(FC)\nFully connected layer involves weights, biases, and neurons. It connects neurons in one layer to neurons in another layer. It is used to classify images between different category by training.\n# 4.5. Softmax \/ Logistic Layer\nSoftmax or Logistic layer is the last layer of CNN. It resides at the end of FC layer. Logistic is used for binary classification and softmax is for multi-classification.\n# 4.6. Output Layer\nOutput layer contains the label which is in the form of one-hot encoded.\n\nNow you have a good understanding of CNN. Let\u2019s implement a CNN in Keras.","3273f8f0":"Find the shape of input image then reshape it into input format for training and testing sets. After that change all datatypes into floats.","d6c3a5fb":"# 3.2 Edge Detection\nEvery image has vertical and horizontal edges which actually combining to form a image. Convolution operation is used with some filters for detecting edges. Suppose you have gray scale image with dimension 6 x 6 and filter of dimension 3 x 3(say). When 6 x 6 grey scale image convolve with 3 x 3 filter, we get 4 x 4 image. First of all 3 x 3 filter matrix get multiplied with first 3 x 3 size of our grey scale image, then we shift one column right up to end , after that we shift one row and so on.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*Ekm4QJ1rHE-bJbQllBWLPA.png)\n\nThe convolution operation can be visualized in the following way. Here our image dimension is 4 x 4 and filter is 3 x 3, hence we are getting output after convolution is 2 x 2.\n\n\n![img](https:\/\/miro.medium.com\/max\/305\/1*4h_J0Zpx93_sFHKxWUoHAw.gif)\n\nIf we have N x N image size and F x F filter size then after convolution result will be\n\n> (N x N) * (F x F) = (N-F+1)x(N-F+1)(Apply this for above case)\n","c359950d":"# 3. Few Definitions\nThere are few definitions you should know before understanding CNN\n\n## 3.1 Image Representation\nThinking about images, its easy to understand that it has a height and width, so it would make sense to represent the information contained in it with a two dimensional structure (a matrix) until you remember that images have colors, and to add information about the colors, we need another dimension, and that is when Tensors become particularly helpful.\n\nImages are encoded into color channels, the image data is represented into each color intensity in a color channel at a given point, the most common one being RGB, which means Red, Blue and Green. The information contained into an image is the intensity of each channel color into the width and height of the image, just like this\n![](http:\/\/)![img](https:\/\/miro.medium.com\/max\/875\/1*125JKUHmij9bzKcREpq9ew.png)\n\nSo the intensity of the red channel at each point with width and height can be represented into a matrix, the same goes for the blue and green channels, so we end up having three matrices, and when these are combined they form a tensor.","b04e37c3":"# 1. What is CNN ?\nComputer vision is evolving rapidly day-by-day. Its one of the reason is deep learning. When we talk about computer vision, a term convolutional neural network( abbreviated as CNN) comes in our mind because CNN is heavily used here. Examples of CNN in computer vision are face recognition, image classification etc. It is similar to the basic neural network. CNN also have learnable parameter like neural network i.e, weights, biases etc.","41e62ea0":"We will print training sample shape, test sample shape and total number of classes present in CIFAR-10. There are 10 classes as we saw earlier. For the sake of example, we will print two example image from training set and test set.","49003b1a":"After compiling our model, we will train our model by fit() method, then evaluate it.\n","432e2ddd":"# Learn Convolutional Neural Network from basic and its implementation in Keras\n![img](https:\/\/miro.medium.com\/max\/875\/1*z7hd8FZeI_eodazwIapvAw.png)","c8cea66c":"# 3.3 Stride and Padding\nStride denotes how many steps we are moving in each steps in convolution.By default it is one.\n![img](https:\/\/miro.medium.com\/max\/875\/1*g0OmDI1w9KqN7Rpw6Qo8Xg@2x.gif)\n\nWe can observe that the size of output is smaller that input. To maintain the dimension of output as in input , we use padding. Padding is a process of adding zeros to the input matrix symmetrically. In the following example,the extra grey blocks denote the padding. It is used to make the dimension of output same as input.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*17TNPi4m0pBqOCGrXzU27w.gif)\n\n\nLet say \u2018p\u2019 is the padding\nInitially(without padding)\n\n> (N x N) * (F x F) = (N-F+1)x(N-F+1)---(1)\n\nAfter applying padding\n\n![img](https:\/\/miro.medium.com\/max\/750\/1*8VwvmOay_k_0MLTrwqQtEg.png)\n\n\n\n\nIf we apply filter F x F in (N+2p) x (N+2p) input matrix with padding, then we will get output matrix dimension (N+2p-F+1) x (N+2p-F+1). As we know that after applying padding we will get the same dimension as original input dimension (N x N). Hence we have,\n\n(N+2p-F+1)x(N+2p-F+1) equivalent to NxN\n\n N+2p-F+1 = N ---(2)\n \n p = (F-1)\/2 ---(3)\n \n \n The equation (3) clearly shows that Padding depends on the dimension of filter.","9c624fd7":"model.summary() is used to see all parameters and shapes in each layers in our models. You can observe that total parameters are 276, 138 and total trainable parameters are 276, 138. Non-trainable parameter is 0.","d5c056d2":"After training we got 83.86% accuracy and 75.48% validation accuracy. It is not actually bad at all.\n![img](https:\/\/miro.medium.com\/max\/875\/1*wm4Q77KFHqlFNNJDPfs3Jg.png)\n![img](https:\/\/miro.medium.com\/max\/870\/1*QYevfO6ll_naJiPX0EDxhA.png)","9a7d9de2":"Initialize all parameters and compile our model with rmsprops optimizer. There are many optimizers for example adam, SGD, GradientDescent, Adagrad, Adadelta and Adamax ,feel free to experiment with it. Here batch is 256 with 50 epochs.\n","bcd82243":"# Table of contents\n* What is CNN ?\n* Why should we use CNN ?\n* Few Definitions\n* Layers in CNN\n* Keras Implementation\n","4a9e8992":"Now create our model. We will add up Convo layers followed by pooling layers. Then we will connect Dense(FC) layer to predict the classes. Input data fed to first Convo layer, output of that Convo layer acts as input for next Convo layer and so on. Finally data is fed to FC layer which try to predict the correct labels.\n","0dc62241":"# 2. Why should we use CNN ?\n### Problem with Feedforward Neural Network\nSuppose you are working with MNIST dataset, you know each image in MNIST is 28 x 28 x 1(black & white image contains only 1 channel). Total number of neurons in input layer will 28 x 28 = 784, this can be manageable. What if the size of image is 1000 x 1000 which means you need 10\u2076 neurons in input layer. Oh! This seems a huge number of neurons are required for operation. It is computationally ineffective right. So here comes Convolutional Neural Network or CNN. In simple word what CNN does is, it extract the feature of image and convert it into lower dimension without loosing its characteristics. In the following example you can see that initial the size of the image is 224 x 224 x 3. If you proceed without convolution then you need 224 x 224 x 3 = 100, 352 numbers of neurons in input layer but after applying convolution you input tensor dimension is reduced to 1 x 1 x 1000. It means you only need 1000 neurons in first layer of feedforward neural network.\n\n\n\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*V6hPq-srR86AIWYrgFYLfA.png)"}}