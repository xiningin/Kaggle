{"cell_type":{"c6c04b8e":"code","55da397d":"code","5f5f9dde":"code","ac0319ac":"code","1ecf67bb":"code","ac8703de":"code","7e6afdc2":"code","3a445efc":"code","a0fe7765":"code","7afca7c9":"code","041c9243":"code","540a9e96":"code","9ebf3316":"code","a0a17408":"code","63caf74e":"code","42dfcb70":"code","befffd6a":"code","025c0ebe":"code","d2bc66b9":"code","8d11e3af":"code","314f09ae":"code","54fa404f":"code","fbdbbd8a":"code","aa7e954b":"code","e39fb52c":"code","489f00a6":"markdown","5b24432d":"markdown","53e9c91d":"markdown","0e720195":"markdown","b32fcbe5":"markdown","24c1cd25":"markdown","afb4aea5":"markdown","d85c4440":"markdown","81b65cf9":"markdown","927339e2":"markdown","1e10706b":"markdown","8a556587":"markdown","df9b95e5":"markdown","93390ba0":"markdown"},"source":{"c6c04b8e":"import pandas as pd\nimport numpy as np\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import FastText\nfrom gensim.models import TfidfModel\nfrom gensim.corpora import Dictionary\nfrom sklearn.neighbors import NearestNeighbors","55da397d":"df = pd.read_csv('..\/input\/Questions.csv', encoding = \"ISO-8859-1\", nrows=30000, usecols=['Id', 'Title', 'Body'])\ndf.shape","5f5f9dde":"#Let's take a look at some of the questions\nprint('Question1: ', df.iloc[0, 2])\nprint('Question2: ', df.iloc[1, 2])\nprint('Question3: ', df.iloc[2, 2])","ac0319ac":"#Using beautiful soup to grab text inside 'p' tags and concatenate it\ndef get_question(html_text):\n  soup = BeautifulSoup(html_text, 'lxml')\n  question = ' '.join([t.text for t in soup.find_all('p')]) #concatenating all p tags\n  return question\n\n#Transforming questions to list for ease of processing\nquestion_list = df['Body'].apply(get_question).values.tolist()","1ecf67bb":"question_list[0]","ac8703de":"#Tokenizing with simple preprocess gensim's simple preprocess\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(simple_preprocess(str(sentence), deacc=True)) # returns lowercase tokens, ignoring tokens that are too short or too long\n\nquestion_words = list(sent_to_words(question_list))","7e6afdc2":"question_words[0][0:5] #first 5 question tokens","3a445efc":"lengths = [len(question) for question in question_words]\nplt.hist(lengths, bins = 25)\nplt.show()\n\nprint('Mean word count of questions is %s' % np.mean(lengths))","a0fe7765":"#Getting rid of stopwords\nstop_words = stopwords.words('english')\n\ndef remove_stopwords(sentence):\n  filtered_words = [word for word in sentence if word not in stop_words]\n  return filtered_words\n\nfiltered_questions = [remove_stopwords(question) for question in question_words]","7afca7c9":"#Examining word counts after removal of stop words\n\nlengths = [len(question) for question in filtered_questions]\nplt.hist(lengths, bins = 25)\nplt.show()\n\nprint('Mean word count of questions is %s' % np.mean(lengths))","041c9243":"len(filtered_questions)","540a9e96":"#Instantiating the model\nn = 50\nmodel = Word2Vec(filtered_questions, size = n, window = 8)\n\n#Training model using questions corpora\nmodel.train(filtered_questions, total_examples=len(filtered_questions), epochs=10)","9ebf3316":"#Let's see how it worked\nword_vectors = model.wv\n\nprint('Words similar to \"array\" are: ', word_vectors.most_similar(positive='array'))\nprint('Words similar to \"database\" are: ', word_vectors.most_similar(positive='database'))","a0a17408":"ft_model = FastText(filtered_questions, size=n, window=8, min_count=5, workers=2,sg=1)","63caf74e":"print('Words similar to \"array\" are: ', ft_model.wv.most_similar('array'))\nprint('Words similar to \"database\" are: ', ft_model.wv.most_similar('database'))","42dfcb70":"#dct = Dictionary(filtered_questions)  # fit dictionary\n#corpus = [dct.doc2bow(line) for line in filtered_questions]  # convert corpus to BoW format\n#tfidf_model = TfidfModel(corpus)  # fit model","befffd6a":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(question_list)\nprint(X.shape)","025c0ebe":"#To proprely work with scikit's vectorizer\nmerged_questions = [' '.join(question) for question in filtered_questions]\ndocument_names = ['Doc {:d}'.format(i) for i in range(len(merged_questions))]\n\ndef get_tfidf(docs, ngram_range=(1,1), index=None):\n    vect = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\n    tfidf = vect.fit_transform(docs).todense()\n    return pd.DataFrame(tfidf, columns=vect.get_feature_names(), index=index).T\n\ntfidf = get_tfidf(merged_questions, ngram_range=(1,1), index=document_names)","d2bc66b9":"def get_sent_embs(emb_model):\n    sent_embs = []\n    for desc in range(len(filtered_questions)):\n        sent_emb = np.zeros((1, n))\n        if len(filtered_questions[desc]) > 0:\n            sent_emb = np.zeros((1, n))\n            div = 0\n            model = emb_model\n            for word in filtered_questions[desc]:\n                if word in model.wv.vocab and word in tfidf.index:\n                    word_emb = model.wv[word]\n                    weight = tfidf.loc[word, 'Doc {:d}'.format(desc)]\n                    sent_emb = np.add(sent_emb, word_emb * weight)\n                    div += weight\n                else:\n                    div += 1e-13 #to avoid dividing by 0\n        if div == 0:\n            print(desc)\n\n        sent_emb = np.divide(sent_emb, div)\n        sent_embs.append(sent_emb.flatten())\n    return sent_embs","8d11e3af":"ft_sent = get_sent_embs(emb_model = ft_model) ","314f09ae":"def get_n_most_similar(interest_index, embeddings, n):\n    \"\"\"\n    Takes the embedding vector of interest, the list with all embeddings, and the number of similar questions to \n    retrieve.\n    Outputs the disctionary IDs and distances\n    \"\"\"\n    nbrs = NearestNeighbors(n_neighbors=n, metric='cosine').fit(embeddings)\n    distances, indices = nbrs.kneighbors(embeddings)\n    similar_indices = indices[interest_index][1:]\n    similar_distances = distances[interest_index][1:]\n    return similar_indices, similar_distances\n\ndef print_similar(interest_index, embeddings, n):\n    \"\"\"\n    Convenience function for visual analysis\n    \"\"\"\n    closest_ind, closest_dist = get_n_most_similar(interest_index, embeddings, n)\n    print('Question %s \\n \\n is most similar to these %s questions: \\n' % (question_list[interest_index], n))\n    for question in closest_ind:\n        print('ID ', question, ': ',question_list[question])","54fa404f":"print_similar(42, ft_sent, 5)","fbdbbd8a":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(filtered_questions)]\nmodel = Doc2Vec(documents, vector_size=n, window=8, min_count=5, workers=2, dm = 1, epochs=20)","aa7e954b":"print(question_list[42], ' \\nis similar to \\n')\nprint([question_list[similar[0]] for similar in model.docvecs.most_similar(42)])","e39fb52c":"print_similar(101, ft_sent, 5)","489f00a6":"# Finding Similar Stack Overflow Questions: Comparing Centroid Method with Doc2Vec\nAs you may know, Stack Overflow is one of the most popular Q&A forums for developers. There are around 17.2 million questions on the platform with 8K\u201310K new questions asked every day. While a lot of these questions are unique, I believe that a lot of questions are repeated, so it would be interesting to see what similar questions are asked most frequently.\u00a0\n\nThe topic of frequent questions extraction is somewhat connected to my Master's Thesis which I'm currently writing, so these kernels will reflect my progress in terms of research in this area.\u00a0\n\nThis notebook will attempt to answer the question __How can we embed sentences in such a way that similar questions will appear closer to each other as measured by cosine distance?__ by comparing two embedding methods __Centroid Method__ and __Doc2Vec__. \n\n\n\n## Imports and Data","5b24432d":"## FastText\nThe main difference of FastText from Word2Vec is that it uses sub-word information (i.e character n-grams). While it brings additional utility to the embeddings, it also considerably slows down the process. ","53e9c91d":"### TF-IDF","0e720195":"The mean word count fell from 80 to 44 words. There's still engough information for the models to learn context, and the noice is reduced. \n\n## Sentence Embedding \nAs a first approach, I will be using a so called __centroid method__ to dervie the sentence embeddings (taken from this research paper http:\/\/www2.aueb.gr\/users\/ion\/docs\/BioNLP_2016.pdf). It derives sentence embeddings as the sum of individual word embeddings in a sentece weighted by their tf-idf score, and divided by the sum of these tf-idf scores.  For the sake of simplicity, I'm going to be compare just two alternatives for word embeddings Word2Vec and FastText. I'll be using gensim implementations of both.\n\n## Word2Vec\nWord2Vec model learns vector representation of a word by either predicting the context around it (skip-gram), or predicting a word based on its context (CBoW). The most important parameters to specify here are the size of embbedding vector and the size of context window. The number of dimensions is usually between 100-300, with 128 being a standard choice for a lot of applications. Context window depends on the nature of text and embbeddings that you want to get. We'll start with context of 5, and see if the embbedings make sense. \n","b32fcbe5":"### Centroid Function","24c1cd25":"As we can see, some questions can be quite long and elaborate while others are just asking for recommendations. Seems like I can expect that most of the question will be in paragraph  tags. Also, there's a lot of not needed tags which will have to be cleaned. General strategy for initial cleaning will be the following:\n\n* Get text which is inside paragraph tags\n* Tokenize\n* Remove stopwords\n* Make frequency thresholds\n\n## Text Pre-processing","afb4aea5":"Let's inspect the results by looking at the most similar words (vectors) of a word 'array' and 'database''","d85c4440":"## Finding Similar Questions\nNow we have sentence embeddings which in theory should reflect the similarity of some questions. To check if this assumption is valid, let's pick a question and find top 5 similar questions (knearest neighbours) as measured by cosine distance.","81b65cf9":"Looks like word2vec knows that array is related to list, matrix, and is commonly used in context of slicing. For 'database', we can see that mode has lerned the abbreviation of db, and the related topics like tables and sqlite. In general, these results should be good enough to construct the sentence embeddings out of them.","927339e2":"Now, let's take a look at the questions word count before and after stop words removal.","1e10706b":"Results are quite interesting. All of the questions are about some kind of text processing. Not exactly repeating questions, but we are definitely onto something. Possible explanation for a weak perfromance is that questions are too long and the final embedding is influenced by too much noise. My hope was that tf-idf score would counteract this, but apparently this is not the case. However, for shorter texts, this method works quite well. \n\nNext appraoch will be a more complicated (in terms of theory, not implementation) model called __Doc2Vec__. \n\n## Doc2Vec\nDoc2Vec improves on simple averaging method by training a 'document' vector along the word vectors. As in Word2Vec there are two algortihms available to train the model, but I will be using the 'distributed memory' (that's why dm=1 in my model). It trains a model which predicts a word based on its context, by averaging the context word and paragraph ID vectors.  ","8a556587":"Next steps to improve embeddings would be to:\n* Add more tags to Doc2Vec which, in theory, would push questions with similar tags closer together\n* Concatenate question headers and code parts with question text \n* Experiment with more questions (now we are training on a limited dataset)","df9b95e5":"Here we can see that FastText has produced different vector embeddings. 'Array' now is close to the words which also contain the ngram 'array' and 'database' is close to different ngrams of the word database plus some variations of database tools. \n\nWe can clearly see the difference between embbedding methods - Word2Vec puts the words which occur in the same context closer in the vector space, while FastText does the same but also allows to incorporate less frequent words into this vector space. Use of n-grams really does play a key role in word embbedings and hence, **I will proceed with using FastText embbeddings** as a basis for sentence embeddings. ","93390ba0":"Results are less than impressive. Some results are about string manipulations or SQL, but Doc2Vec has failed to capture the main meaning of the reference question. \n\nFrom the current analysis I can conclude that with current parameters, __Centroid Method outperforms Doc2Vec__. Here's is another example of similar questions being close to each-other under the Centroid Method Embedding."}}