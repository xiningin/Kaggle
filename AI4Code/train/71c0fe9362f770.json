{"cell_type":{"35b1f61d":"code","14f10076":"code","a0609643":"code","1d566dea":"code","981f240e":"code","3fc7b654":"code","7ff10a3c":"code","5ede36bd":"code","8afbd320":"code","dd807b69":"code","ffb63308":"code","2124ebb4":"code","b8aae481":"code","b73283b5":"code","942f07fa":"code","3dc9f5b5":"code","da32a459":"code","60d63ce3":"code","ac89e6ac":"code","461d6485":"code","0b0d7ace":"code","7134b5f7":"code","9950f244":"code","e9706b4e":"code","24e62223":"code","efe58e3d":"code","d97f34f3":"code","f93b0dad":"code","30b004c6":"markdown","67f6a3d0":"markdown","0bbbbce6":"markdown","bdaac37a":"markdown","0d4addb9":"markdown","9dd69c95":"markdown","a6343764":"markdown","65bc6419":"markdown"},"source":{"35b1f61d":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","14f10076":"!pip3 install nlpaug","a0609643":"import pandas as pd\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\nimport nlpaug.augmenter.word as naw\n\nimport torch\nfrom torch import tensor\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom collections import OrderedDict\nimport random\nrandom.seed(42)","1d566dea":"p = {\n    'BATCH_SZ': 10,\n    'MAX_LEN': 120,\n    'MODEL': 'bert-base-cased',\n    'TRAIN_VALID_SPLIT': 0.07,\n    'DROPOUT_0': 0.8,\n    'DROPOUT_1': 0.8,\n    'N_CLASSES': 2,\n    'CLIPPING': True,\n    'SCHEDULER': True,\n    'LR': 2e-5,\n    'LIN_0_HIDDEN_SZ': 256,\n    'ADDED_AUGMENTED_TWEETS': 100,\n    'AUGMENTATION_MODEL': 'bert-base-cased',\n    'MAX_EPOCH': 100\n}","981f240e":"DATA = Path('..\/input\/nlp-getting-started')\ntrain_df, test_df = pd.read_csv(DATA\/'train.csv'), pd.read_csv(DATA\/'test.csv')","3fc7b654":"class DS(Dataset):\n    def __init__(self, texts, targets, tokenizer, max_len):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self): return len(self.texts)\n    \n    def __getitem__(self, item):\n        text = str(self.texts[item])\n        target = self.targets[item]\n        \n        encoding = self.tokenizer.encode_plus(\n          text,\n          add_special_tokens=True,\n          max_length=self.max_len,\n          return_token_type_ids=False,\n          pad_to_max_length=True,\n          return_attention_mask=True,\n          return_tensors='pt',\n          truncation=True\n        )\n        \n        return {\n          'text': text,\n          'input_ids': encoding['input_ids'].flatten(),\n          'attention_mask': encoding['attention_mask'].flatten(),\n          'targets': torch.tensor(target, dtype=torch.long)\n        }","7ff10a3c":"train_df, valid_df = train_test_split(train_df, test_size=p['TRAIN_VALID_SPLIT'], random_state=42)","5ede36bd":"train_df.reset_index(inplace=True)\nvalid_df.reset_index(inplace=True)","8afbd320":"len(train_df), len(valid_df)","dd807b69":"train_df.head()","ffb63308":"aug = naw.ContextualWordEmbsAug(model_path=p['AUGMENTATION_MODEL'], action=\"substitute\")","2124ebb4":"def augment(train_df):\n    n_tweets = p['ADDED_AUGMENTED_TWEETS']\n    for i in range(n_tweets):\n        if i % 100 == 0: print(f'{i}\/{n_tweets}')\n        idx = random.randint(0, len(train_df))\n        target = train_df.target[idx]\n        text = train_df.text[idx]\n        text_aug = aug.augment(text)\n        train_df = train_df.append({'text': text_aug, 'target': target}, ignore_index=True)\n    return train_df","b8aae481":"train_df = augment(train_df); len(train_df)","b73283b5":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = DS(\n        texts=df.text.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len,\n        targets=df.target.to_numpy()\n    )\n    \n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=4,\n        shuffle=True\n    )","942f07fa":"tokenizer = BertTokenizer.from_pretrained(p['MODEL'])","3dc9f5b5":"class BertClassifier(pl.LightningModule):\n    def __init__(self, train_df, valid_df, n_c=2, params=p):\n        super().__init__()\n        self.hparams = p\n        self.train_df, self.valid_df = train_df, valid_df\n        \n        self.bert = BertModel.from_pretrained(p['MODEL'])\n        self.drop0 = nn.Dropout(p=p['DROPOUT_0'])\n        self.drop1 = nn.Dropout(p=p['DROPOUT_1'])\n        self.lin0 = nn.Linear(self.bert.config.hidden_size, p['LIN_0_HIDDEN_SZ'])\n        self.lin1 = nn.Linear(p['LIN_0_HIDDEN_SZ'], p['N_CLASSES'])\n    \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        y = self.drop0(pooled_output)\n        y = self.lin0(y)\n        y = self.drop1(y)\n        return self.lin1(y)\n    \n    def step(self, batch):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets = batch[\"targets\"]\n        \n        outputs = self(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        \n        _, preds = torch.max(outputs, dim=1)\n        \n        acc = (preds == targets).float().mean()\n        loss =  F.cross_entropy(outputs, targets)\n        return OrderedDict({\n            'loss': loss,\n            'accuracy': acc\n        })\n    \n    def training_step(self, batch, batch_idx):\n        return self.step(batch)\n    \n    def training_epoch_end(self, outputs):\n        loss_mean = torch.stack([output['loss'] for output in outputs]).float().mean()\n        acc_mean = torch.stack([output['accuracy'] for output in outputs]).float().mean()\n        self.log('train_loss', loss_mean)\n        self.log('train_accuracy', acc_mean, prog_bar=True)\n        \n        if p['CLIPPING']: nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n        if p['SCHEDULER']: self.scheduler.step()\n    \n    def validation_step(self, batch, batch_idx):\n        return self.step(batch)\n    \n    def validation_epoch_end(self, outputs):\n        loss_mean = torch.stack([output['loss'] for output in outputs]).float().mean()\n        acc_mean = torch.stack([output['accuracy'] for output in outputs]).float().mean()\n        self.log('valid_loss', loss_mean, prog_bar=True)\n        self.log('valid_accuracy', acc_mean, prog_bar=True)\n        \n    def configure_optimizers(self):\n        optim = AdamW(self.parameters(), lr=p['LR'], correct_bias=False) \n        self.scheduler = get_linear_schedule_with_warmup(\n          optim,\n          num_warmup_steps=0,\n          num_training_steps=len(self.train_dataloader())*100\n        )\n        return optim\n            \n    def train_dataloader(self): return create_data_loader(self.train_df, tokenizer, p['MAX_LEN'], p['BATCH_SZ'])\n    def val_dataloader(self): return create_data_loader(self.valid_df, tokenizer, p['MAX_LEN'], p['BATCH_SZ'])","da32a459":"classifier = BertClassifier(train_df, valid_df, params=p)","60d63ce3":"checkpoint = ModelCheckpoint(\n    monitor='valid_accuracy',\n    filename='bert',\n    mode='max'\n)\n\nearly_stopping = EarlyStopping(\n    monitor='valid_accuracy',\n    patience=2,\n    verbose=True,\n    mode='max'\n)","ac89e6ac":"trainer = pl.Trainer(gpus=1, max_epochs=p['MAX_EPOCH'], deterministic=True, callbacks=[checkpoint, early_stopping])\ntrainer.fit(classifier)","461d6485":"ckp_f = '.\/lightning_logs\/version_0\/checkpoints\/bert-v0.ckpt'","0b0d7ace":"classifier = BertClassifier(None, None, params=p)","7134b5f7":"def load_model(p):\n    ckp = torch.load(p)\n    classifier.load_state_dict(ckp['state_dict'])","9950f244":"load_model(ckp_f)","e9706b4e":"tokenizer = BertTokenizer.from_pretrained(p['MODEL'])","24e62223":"def predict(text):\n    encoding = tokenizer.encode_plus(\n              text,\n              add_special_tokens=True,\n              max_length=120,\n              return_token_type_ids=False,\n              pad_to_max_length=True,\n              return_attention_mask=True,\n              return_tensors='pt',\n              truncation=True\n    )\n    y_hat = classifier(encoding['input_ids'], encoding['attention_mask'])\n    return y_hat.argmax().item()","efe58e3d":"preds = []\nfor i, row in enumerate(test_df.values):\n    if i % 100 == 0: print(f'{i}\/{len(test_df)}')\n    id = row[0]\n    s = row[-1]\n    y_hat = predict(s)\n    preds.append([id, y_hat])","d97f34f3":"submission = pd.DataFrame(preds, columns=['id', 'target']).set_index('id')\nsubmission.head()","f93b0dad":"submission.to_csv('submission.csv')","30b004c6":"## Inference","67f6a3d0":"## Setup","0bbbbce6":"## Submission","bdaac37a":"## Dataset","0d4addb9":"Here we define a PyTorch Dataset. Notice the `encode_plus` function in `__getitem__`. It is used to create the tokens needed for Bert. `__getitem__` will return the text itself, its encoding, attention mask and the corresponding label. ","9dd69c95":"## Augmenatation\n\nWe will do a little bit of contextual word augmentation. To increase our training data.","a6343764":"## Description\n\nIn this notebook I want to demonstrate how to do transfer learning using Bert in Pytorch Lightning. I archive a score of **81.12%** on the test set.","65bc6419":"## Model + Training"}}