{"cell_type":{"6614667c":"code","4aacd87a":"code","921cb799":"code","1380b9ed":"code","8c0fb386":"code","69fbff20":"code","eea6dccb":"code","3fd20f42":"code","5c72c939":"code","941bead5":"code","a8bcabbb":"code","84bbef78":"code","457dab10":"code","5a249059":"code","995a1cd8":"code","838494fe":"code","79ccee00":"code","3971f823":"code","310d979b":"code","9c88181d":"code","cc94abe1":"code","cd57ad7d":"code","2850730b":"markdown","0b0b4401":"markdown","1c37b165":"markdown","bd8d8293":"markdown","724da70a":"markdown"},"source":{"6614667c":"import itertools\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.spatial import distance_matrix\nimport matplotlib\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4aacd87a":"# Define TSP\n\nn_customer = 14\nn_point = n_customer + 1\n\ndf = pd.DataFrame({\n    'x': np.random.randint(0, 100, n_point),\n    'y': np.random.randint(0, 100, n_point),\n})\n\ndf.iloc[0]['x'] = 0\ndf.iloc[0]['y'] = 0\n\ndf","921cb799":"# Get the distance matrix\n\ndistances = pd.DataFrame(distance_matrix(df[['x', 'y']].values, df[['x', 'y']].values), index=df.index, columns=df.index).values\n\nfig, ax = plt.subplots(figsize=(13, 11))\nsns.heatmap(distances, ax=ax, cmap='Blues', annot=True, fmt='.0f', cbar=True, cbar_kws={\"shrink\": .3}, linewidths=.1)\nplt.title('distance matrix')\nplt.show()","1380b9ed":"# Check TSP state\n\nplt.figure(figsize=(5, 5))\n\n# Draw the problem state\nfor i, row in df.iterrows():\n    if i == 0:\n        plt.scatter(row['x'], row['y'], c='r')\n        plt.text(row['x'] + 1, row['y'] + 1, 'depot')\n    else:\n        plt.scatter(row['x'], row['y'], c='black')\n        plt.text(row['x'] + 1, row['y'] + 1, f'{i}')\n        \nplt.xlim([-10, 110])\nplt.ylim([-10, 110])\nplt.title('points: id')\nplt.show()","8c0fb386":"!pip install pulp","69fbff20":"import pulp","eea6dccb":"%%time\n\n# Set the problem\nproblem = pulp.LpProblem('tsp_mip', pulp.LpMinimize)\n\n# Set valiables\nx = pulp.LpVariable.dicts('x', ((i, j) for i in range(n_point) for j in range(n_point)), lowBound=0, upBound=1, cat='Binary')\n# We need to keep track of the order in the tour to eliminate the possibility of subtours\nu = pulp.LpVariable.dicts('u', (i for i in range(n_point)), lowBound=1, upBound=n_point, cat='Integer')\n\n# Set objective function\nproblem += pulp.lpSum(distances[i][j] * x[i, j] for i in range(n_point) for j in range(n_point))\n\n# Set constrains\nfor i in range(n_point):\n    problem += x[i, i] == 0\n\nfor i in range(n_point):\n    problem += pulp.lpSum(x[i, j] for j in range(n_point)) == 1\n    problem += pulp.lpSum(x[j, i] for j in range(n_point)) == 1\n\n# Eliminate subtour\nfor i in range(n_point):\n    for j in range(n_point):\n        if i != j and (i != 0 and j != 0):\n            problem += u[i] - u[j] <= n_point * (1 - x[i, j]) - 1\n            \n# Solve the problem\nstatus = problem.solve()\n\n# output status, value of objective function\nstatus, pulp.LpStatus[status], pulp.value(problem.objective)","3fd20f42":"# Check TSP state and optimized route\n\nplt.figure(figsize=(5, 5))\n\n# Draw the problem state\nfor i, row in df.iterrows():\n    if i == 0:\n        plt.scatter(row['x'], row['y'], c='r')\n        plt.text(row['x'] + 1, row['y'] + 1, 'depot')\n        \n    else:\n        plt.scatter(row['x'], row['y'], c='black')\n        plt.text(row['x'] + 1, row['y'] + 1, f'{i}')\n        \nplt.xlim([-10, 110])\nplt.ylim([-10, 110])\nplt.title('points: id')\n\n# Draw the optimal route\nroutes = [(i, j) for i in range(n_point) for j in range(n_point) if pulp.value(x[i, j]) == 1]\narrowprops = dict(arrowstyle='->', connectionstyle='arc3', edgecolor='blue')\nfor i, j in routes:\n    plt.annotate('', xy=[df.iloc[j]['x'], df.iloc[j]['y']], xytext=[df.iloc[i]['x'], df.iloc[i]['y']], arrowprops=arrowprops)\n                \nplt.show()","5c72c939":"import math\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\n\nuse_cuda = torch.cuda.is_available()\n\ntorch.__version__, use_cuda","941bead5":"def reward(sample_solution, use_cuda=use_cuda):\n    batch_size = sample_solution[0].size(0)\n    n = len(sample_solution) # sample_solution is seq_len of [batch_size]\n    tour_len = Variable(torch.zeros([batch_size])).cuda() if use_cuda else Variable(torch.zeros([batch_size]))\n    for i in range(n - 1):\n        tour_len += torch.norm(sample_solution[i] - sample_solution[i + 1], dim=1)\n    tour_len += torch.norm(sample_solution[n - 1] - sample_solution[0], dim=1)\n\n    return tour_len","a8bcabbb":"class Attention(nn.Module):\n    def __init__(self, hidden_size, use_tanh=False, C=10, use_cuda=use_cuda):\n        super(Attention, self).__init__()\n        self.use_tanh = use_tanh\n        self.C = C\n\n        # Bahdanau algo\n        self.W_query = nn.Linear(hidden_size, hidden_size)\n        self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n\n        V = torch.cuda.FloatTensor(hidden_size) if use_cuda else torch.FloatTensor(hidden_size)\n        self.V = nn.Parameter(V)\n        self.V.data.uniform_(-(1. \/ math.sqrt(hidden_size)) , 1. \/ math.sqrt(hidden_size))\n        \n    def forward(self, query, ref):\n        # query = [batch_size, hidden_size]\n        # ref   = [batch_size, seq_len, hidden_size]\n        batch_size = ref.size(0)\n        seq_len    = ref.size(1)\n        \n        # Bahdanau algo\n        ref = ref.permute(0, 2, 1)\n        query = self.W_query(query).unsqueeze(2)                           # [batch_size x hidden_size x           1]\n        ref   = self.W_ref(ref)                                            # [batch_size x hidden_size x     seq_len] \n        expanded_query = query.repeat(1, 1, seq_len)                       # [batch_size x hidden_size x     seq_len]\n        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1)      # [batch_size x           1 x hidden_size]\n        logits = torch.bmm(V, torch.tanh(expanded_query + ref)).squeeze(1)\n        \n        if self.use_tanh:\n            logits = self.C * torch.tanh(logits)\n            \n        return ref, logits","84bbef78":"class GraphEmbedding(nn.Module):\n    def __init__(self, input_size, embedding_size, use_cuda=use_cuda):\n        super(GraphEmbedding, self).__init__()\n        self.embedding_size = embedding_size\n        self.use_cuda = use_cuda\n        \n        self.embedding = nn.Parameter(torch.FloatTensor(input_size, embedding_size)) \n        self.embedding.data.uniform_(-(1. \/ math.sqrt(embedding_size)), 1. \/ math.sqrt(embedding_size))\n        \n    def forward(self, inputs):\n        # inputs = [batch_size, input_size, seq_len]\n        batch_size = inputs.size(0)\n        seq_len    = inputs.size(2)\n        embedding = self.embedding.repeat(batch_size, 1, 1)  \n        embedded = []\n        inputs = inputs.unsqueeze(1)\n        for i in range(seq_len):\n            embedded.append(torch.bmm(inputs[:, :, :, i].float(), embedding))\n        embedded = torch.cat(embedded, 1)\n        return embedded","457dab10":"class PointerNet(nn.Module):\n    def __init__(self, embedding_size, hidden_size, seq_len, n_glimpses, tanh_exploration, use_tanh, use_cuda=use_cuda):\n        super(PointerNet, self).__init__()\n        self.embedding_size = embedding_size\n        self.hidden_size    = hidden_size\n        self.n_glimpses     = n_glimpses\n        self.seq_len        = seq_len\n        self.use_cuda       = use_cuda\n        \n        self.embedding = GraphEmbedding(2, embedding_size, use_cuda=use_cuda)\n        self.encoder   = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n        self.decoder   = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n        self.pointer   = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n        self.glimpse   = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n        \n        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n        self.decoder_start_input.data.uniform_(-(1. \/ math.sqrt(embedding_size)), 1. \/ math.sqrt(embedding_size))\n        \n    def apply_mask_to_logits(self, logits, mask, idxs):\n        batch_size = logits.size(0)\n        clone_mask = mask.clone()\n\n        if idxs is not None:\n            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n            logits[clone_mask] = -np.inf\n        return logits, clone_mask\n            \n    def forward(self, inputs):\n        # inputs = [batch_size, 1, seq_len]\n        batch_size = inputs.size(0)\n        seq_len    = inputs.size(2)\n        \n        embedded = self.embedding(inputs)\n        encoder_outputs, (hidden, context) = self.encoder(embedded)\n        \n        prev_probs    = []\n        prev_idxs     = []\n        mask          = torch.zeros(batch_size, seq_len).byte().cuda() if self.use_cuda else torch.zeros(batch_size, seq_len).byte()        \n        idxs          = None\n        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n        \n        for i in range(seq_len):\n            \n            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n            \n            query = hidden.squeeze(0)\n            for i in range(self.n_glimpses):\n                ref, logits = self.glimpse(query, encoder_outputs)\n                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n                \n            _, logits = self.pointer(query, encoder_outputs)\n            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n            probs = F.softmax(logits)\n            \n            idxs = probs.multinomial(num_samples=1).squeeze(1)\n            for old_idxs in prev_idxs:\n                if old_idxs.eq(idxs).data.any():\n                    idxs = probs.multinomial(num_samples=1).squeeze(1)\n                    break\n            decoder_input = embedded[[i for i in range(batch_size)], idxs.data, :] \n            \n            prev_probs.append(probs)\n            prev_idxs.append(idxs)\n            \n        return prev_probs, prev_idxs","5a249059":"class NeuralCombinatorialOptimization(nn.Module):\n    def __init__(self, embedding_size, hidden_size, seq_len, n_glimpses, tanh_exploration, use_tanh, reward, use_cuda=use_cuda):\n        super(NeuralCombinatorialOptimization, self).__init__()\n        self.reward   = reward\n        self.use_cuda = use_cuda\n        \n        self.actor = PointerNet(embedding_size, hidden_size, seq_len, n_glimpses, tanh_exploration, use_tanh, use_cuda)\n\n    def forward(self, inputs):\n        # inputs = [batch_size, input_size, seq_len]\n        batch_size = inputs.size(0)\n        input_size = inputs.size(1)\n        seq_len    = inputs.size(2)\n        \n        probs, action_idxs = self.actor(inputs)\n       \n        actions = []\n        inputs = inputs.transpose(1, 2)\n        for action_id in action_idxs:\n            actions.append(inputs[[x for x in range(batch_size)], action_id.data, :])\n            \n        action_probs = []    \n        for prob, action_id in zip(probs, action_idxs):\n            action_probs.append(prob[[x for x in range(batch_size)], action_id.data])\n\n        R = self.reward(actions, self.use_cuda)\n        \n        return R, action_probs, actions, action_idxs","995a1cd8":"class Trainer:\n    def __init__(self, model, train_data, max_grad_norm=2., use_cuda=use_cuda):\n        self.model         = model\n        self.use_cuda      = use_cuda\n        \n        self.train_data    = train_data.unsqueeze(0)\n        self.train_data    = Variable(self.train_data).cuda() if use_cuda else Variable(self.train_data)\n\n        self.actor_optim   = optim.Adam(model.actor.parameters(), lr=1e-4)\n        self.max_grad_norm = max_grad_norm\n        \n        self.train_loss = []\n        self.train_tour = []\n        self.epochs     = 0\n        \n    def train(self, n_epochs):\n        critic_exp_mvg_avg = torch.zeros(1).cuda() if self.use_cuda else torch.zeros(1)\n        \n        self.model.train() # Set train mode\n        iterator = tqdm(range(n_epochs))\n        \n        for epoch in iterator:\n            iterator.set_description(f'epoch {epoch + 1}\/{n_epochs}')\n            \n            R, probs, actions, actions_idxs = self.model(self.train_data)\n\n            critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())\n            advantage = R - critic_exp_mvg_avg\n\n            logprobs = 0\n            for prob in probs: \n                logprob = torch.log(prob)\n                logprobs += logprob\n            logprobs[logprobs < -1000] = 0.  \n\n            reinforce = advantage * logprobs\n            actor_loss = reinforce.mean()\n\n            self.actor_optim.zero_grad()\n            actor_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.actor.parameters(), float(self.max_grad_norm), norm_type=2)\n\n            self.actor_optim.step()\n            critic_exp_mvg_avg = critic_exp_mvg_avg.detach()\n\n            l = float(actor_loss.data)\n            r = float(R.mean().data)\n            self.train_loss.append(l)\n            self.train_tour.append(r)\n            self.epochs += 1\n            \n            iterator.set_postfix(loss='{}'.format(l), reward='{}'.format(r))\n                    \n        self.model.eval() # Set not train mode\n            \n    def predict(self):\n        R, probs, actions, actions_idxs = self.model(self.train_data)\n        return R, probs, actions, actions_idxs","838494fe":"# Set hyper parameters\n\nembedding_size   = 128\nhidden_size      = 128\nn_glimpses       = 1\ntanh_exploration = 10\nuse_tanh         = True\n\nbeta             = 0.99\nmax_grad_norm    = 2.","79ccee00":"# Define the model\n\nmodel = NeuralCombinatorialOptimization(embedding_size, hidden_size, n_point, n_glimpses, tanh_exploration, use_tanh, reward, use_cuda=use_cuda)\n\nif use_cuda:\n    model = model.cuda()","3971f823":"train_data = torch.tensor(df.values.transpose(), dtype=torch.float32)\ntrainer = Trainer(model, train_data)","310d979b":"%%time\n\ntrainer.train(n_epochs=1000)","9c88181d":"fig, ax1 = plt.subplots()\nax2 = ax1.twinx()\n\nax1.plot(trainer.train_tour, color='skyblue')\nax1.set_ylabel('reward')\n\nax2.plot(trainer.train_loss, color='orange')\nax2.set_ylabel('loss')\n\nax1.set_xlabel('epoch')\n\nfig.tight_layout()\nplt.title('reward and loss')\nplt.show()","cc94abe1":"# Predict using trained model\nR, probs, actions, actions_idxs = trainer.predict()","cd57ad7d":"plt.figure(figsize=(5, 5))\n\n# Draw the problem state\nfor i, row in df.iterrows():\n    if i == 0:\n        plt.scatter(row['x'], row['y'], c='r')\n        plt.text(row['x'] + 1, row['y'] + 1, 'depot')\n        \n    else:\n        plt.scatter(row['x'], row['y'], c='black')\n        plt.text(row['x'] + 1, row['y'] + 1, f'{i}')\n        \nplt.xlim([-10, 110])\nplt.ylim([-10, 110])\nplt.title('points: id')\n\n# Draw the optimal route\nroutes = [int(tensor) for tensor in actions_idxs]\nroutes = [(routes[i-1], routes[i]) for i in range(len(routes))]\narrowprops = dict(arrowstyle='->', connectionstyle='arc3', edgecolor='navy')\nfor i, j in routes:\n    plt.annotate('', xy=[df.iloc[j]['x'], df.iloc[j]['y']], xytext=[df.iloc[i]['x'], df.iloc[i]['y']], arrowprops=arrowprops)\n                \nplt.show()","2850730b":"### Thanks.","0b0b4401":"### This notebook is the tutorial that comparison of solving TSP using MIP by PuLP and Deep Reinforcement Learning by PyTorch.","1c37b165":"### 2. Solving using Deep Reinforcement Learning by PyTorch\n\n### Use the following deep learning model and deep reinforcement learning architecture\n\n* <i>Pointer Networks<\/i>: https:\/\/arxiv.org\/abs\/1506.03134\n* <i>Neural Combinatorial Optimization with Reinforcement Learning<\/i>: https:\/\/arxiv.org\/abs\/1611.09940","bd8d8293":"## Traveling Salesman Problem; TSP","724da70a":"### 1. Solving using Mixed Integer Problem; MIP by PuLP"}}