{"cell_type":{"608c3371":"code","8830b17b":"code","727eefd7":"code","4aad30dc":"code","f6bbdb29":"markdown","d8a5532a":"markdown","c039730a":"markdown"},"source":{"608c3371":"import torch\nimport numpy as np\nfrom typing import List, Dict","8830b17b":"class Fbeta:\n    \"\"\"\n    Compute the F-beta metric.\n    \n    Args:\n        - beta (int): Weighting factor between recall and precision.\n        - num_labels (int): Number of label in the dataset.\n        - score_threshold (float): Score threshold to consider a prediction valid.\n        - iou_threshold (float): Threshold on the IoU between the groundtruth and predicted bouding box\n            defining if a prediction is valid or not.\n        - bbox_mode ('xyxy' or 'xywh'): Bounding box format used.\n    \"\"\"\n    \n    def __init__(self, beta: int, num_labels: int, score_threshold: float = 0.5, iou_threshold: float = 0.5, bbox_mode: str = 'xyxy') -> None:\n        self.beta = beta\n        self.num_labels = num_labels\n        assert 0 <= score_threshold <= 1\n        self.score_threshold = score_threshold\n        assert 0 <= iou_threshold <= 1\n        self.iou_threshold = iou_threshold\n        self.bbox_mode = bbox_mode\n        # Create accumumator storing the score for each batch\n        # Here the accumulator will be the confusion matrix\n        # Rows indicate the true label and columns the predicted label\n        # We add one column for the case when we don't have a detection for a given label\n        # and we add one row when we have no free label for a given detection\n        self._acc = np.zeros((self.num_labels + 1, self.num_labels + 1))\n        # NOTE: using the full confusion matrix is not the most space efficient way\n        # to compute the Fbeta score. However it's more convenient to have access\n        # to the confusion matrix when you want to understand better your performances\n        \n    def update(self, preds: List[Dict[str, np.ndarray]], groundtruth: List[Dict[str, np.ndarray]]) -> None:\n        \"\"\"Update the accumulator with the current batch predictions\"\"\"\n        assert len(preds) == len(groundtruth)\n        # Loop over all the predictions in the batch\n        for p, gt in zip(preds, groundtruth):\n            bboxes, bboxes_gt = p['boxes'], gt['boxes']\n            labels, labels_gt = p['labels'], gt['labels']\n            scores = p['scores']\n            # Remove predictions with low scores\n            valid_idx = scores >= self.score_threshold\n            bboxes, labels = bboxes[valid_idx], labels[valid_idx]          \n            # Compute all the groudtruth boxes that have sufficient iou with every predicted box\n            matches = []\n            # Loop over the predicted boxes\n            for i in range(bboxes.shape[0]):\n                # Loop over the groundtruch boxes\n                for j in range(bboxes_gt.shape[0]):\n                    # Compute the IoU between the boxes\n                    iou = self._compute_iou(bboxes[i], bboxes_gt[j])\n                    # If above IoU threshold consider the predicted box\n                    if iou >= self.iou_threshold:\n                        matches.append([i, j, iou])\n            matches = np.array(matches)\n            is_matches = matches.shape[0] > 0\n            if is_matches:\n                # Sort the matches by highest IoU\n                matches = matches[matches[:, 2].argsort()[::-1]]\n                # Keep only the detection with the highest IoU per groundtruth\n                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n                # Sort again since np.unique doesn't preserve the original order\n                matches = matches[matches[:, 2].argsort()[::-1]]\n                # For each predicted box keep the match with the highest IoU\n                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n            # Loop over the grountruth boxes\n            for i in range(labels_gt.shape[0]):\n                # There is a match that correspond to the index of the gt box\n                matched_pred = matches[matches[:, 1] == i]\n                if is_matches and matched_pred.shape[0] == 1:\n                    self._acc[labels_gt[i], labels[int(matched_pred[0, 0])]] += 1\n                else:  # We are in the case of a missed detection\n                    self._acc[labels_gt[i], -1] += 1\n            # Loop over the predictions boxes to treat the case where a predicted box didn't\n            # found a match\n            for i in range(labels.shape[0]):\n                if is_matches and matches[matches[:, 0] == i].shape[0] == 0:\n                    self._acc[-1, labels[i]] += 1\n    \n    def result(self) -> np.ndarray:\n        \"\"\"Return the Fbeta score for each label in the dataset.\"\"\"\n        # Compute true positive, false negative and false positive for each label\n        tp = np.diag(self._acc)[:-1]\n        fn = self._acc[:-1, -1]\n        fp = np.sum(self._acc[:-1, :-1], axis=0) - tp\n        # We compute a Fbeta score for each label\n        fbeta = self._compute_fbeta(tp, fn, fp)\n        return fbeta\n    \n    def reset(self) -> None:\n        # Reset the accumulator\n        self._acc = np.zeros((self.num_labels + 1, self.num_labels + 1))\n    \n    def _compute_fbeta(self, tp: np.ndarray, fn: np.ndarray, fp: np.ndarray) -> float:\n        \"\"\"\n        Compute the fbeta.\n        \n        Args:\n            - tp (float): True positive score.\n            - fn (float): False negative score.\n            - fp (float): False positive score.\n        \"\"\"\n        num = (1 + self.beta**2) * tp\n        den = (1 + self.beta**2) * tp + self.beta**2 * fn + fp\n        return num \/ den\n    \n    def _compute_iou(self, bbox1: List[int], bbox2: List[int], mode: str = 'xyxy'):\n        \"\"\"\n        Compute the IoU between two bounding boxes.\n        \n        Args:\n            - bbox1 (List[int]): Bounding box 1.\n            - bbox2 (List[int]): Bounding box 2.\n            - mode ('xyxy' or 'xywh'): Format of the bounding boxes. 'xyxy' stands for\n                (left, top, right, bottom) and 'xywh' stands for (left, top, width, height)\n        \"\"\"\n        assert mode in ['xyxy', 'xywh']\n        if mode == 'xywh':\n            bbox1[2] += bbox1[0]\n            bbox1[3] += bbox1[1]\n            bbox2[2] += bbox2[0]\n            bbox2[3] += bbox2[1]\n        # Compute the area for box1 and box2\n        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])\n        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])\n        # Compute the area of the intersection\n        xi1 = max(bbox1[0], bbox2[0])\n        xi2 = min(bbox1[2], bbox2[2])\n        yi1 = max(bbox1[1], bbox2[1])\n        yi2 = min(bbox1[3], bbox2[3])\n        intersection = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n        # Compute the area of the union\n        union = area1 + area2 - intersection\n        return intersection \/ union","727eefd7":"NUM_PTS = 100\n# Create the boxes\ntop_left_pts = np.random.randint(0, 256, size=(NUM_PTS, 2))\nbottom_right_pts = top_left_pts + np.random.randint(1, 256, size=(NUM_PTS, 2))\nbboxes = np.concatenate([top_left_pts, bottom_right_pts], axis=1)\n# Create the groundtruth and preds labels\nlabels_gt = np.zeros((NUM_PTS,), dtype=np.int64)\n# Create the prediction and groundtruch object\npreds = [{'boxes': bboxes[:NUM_PTS \/\/ 2], 'labels': labels_gt[:NUM_PTS \/\/ 2], 'scores': labels_gt[:NUM_PTS \/\/ 2] + 1}]\ngroudtruth = [{'boxes': bboxes, 'labels': labels_gt}]\n# Create the F1 score metric object\nf1_score = Fbeta(1, 1)\nf1_score.update(preds, groudtruth)\nprint(\"The confusion matrix should be:\")\nprint(\"[[50, 50],\\n\"\n      \" [0, 0]]\")\nprint(\"The computed confusion matrix is:\")\nprint(f1_score._acc)\nprint(\"The F1 score should be: 0.667\")\nprint(\"Computed F1 score is: \", f1_score.result()[0])","4aad30dc":"NUM_PTS = 120\n# Create the boxes\ntop_left_pts = np.random.randint(0, 256, size=(NUM_PTS, 2))\nbottom_right_pts = top_left_pts + np.random.randint(1, 256, size=(NUM_PTS, 2))\nbboxes = np.concatenate([top_left_pts, bottom_right_pts], axis=1)\n# Create the groundtruth and preds labels\nlabels_gt = np.zeros((NUM_PTS,), dtype=np.int64)\n# Create the prediction and groundtruch object\ntwothird = int(NUM_PTS * 2 \/ 3)\nbboxes_pred = bboxes[:twothird]\nlabels_pred = labels_gt[:twothird].copy()\nlabels_pred[len(labels_pred) \/\/ 2:] = 1\nlabels_gt[:20] = 1\npreds = [{'boxes': bboxes_pred, 'labels': labels_pred, 'scores': np.ones_like(labels_pred)}]\ngroudtruth = [{'boxes': bboxes, 'labels': labels_gt}]\n# Create the F1 score metric object\nf2_score = Fbeta(2, 2)\nf2_score.update(preds, groudtruth)\nprint(\"The confusion matrix should be:\")\nprint(\"[[20, 40, 40],\\n\"\n      \" [20, 0, 0],\\n\"\n      \" [0, 0, 0]]\")\nprint(\"The computed confusion matrix is:\")\nprint(f2_score._acc)\nprint(\"The F2 score should be: [0.357142, 0.]\")\nprint(\"Computed F2 score is: \", f2_score.result())","f6bbdb29":"# Implementation of F-beta score for object detection\n\nAs the title say we'll implement the F-beta score for object detection predictions. For those unfamiliar with the F-beta score check this [link](https:\/\/en.wikipedia.org\/wiki\/F-score#F%CE%B2).\n\nOur implementation will follow the next guidelines:\n*      we will implement a score class using a similar architecture as the metric class commonly used in high level deep learning API such as fastai or keras\n* the predictions and labels format will be the one described in the [`torchvision.models.RetinaNet`](https:\/\/pytorch.org\/vision\/stable\/models.html#id58) except the torch.Tensor have been converted to numpy array.","d8a5532a":"### F2-score for object detection with two classes\n\nIn this case we'll consider a groudtruth of 120 boxes with label 1 for the first 20 and label 0 for the rest. The prediction boxes will be the two third of the groundtruth boxes and the last half is misclassified.","c039730a":"## Unit tests\n\nWe will now run some units tests to check that our metrics behace as expected\n\n### F1-score for object detection with one class\n\nFor the sake of simplicity we'll take 100 randomly generated boxes as being the groudtruth and preds will be half of those groundtruths."}}