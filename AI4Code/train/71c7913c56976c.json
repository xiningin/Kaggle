{"cell_type":{"8f09c378":"code","4c523852":"code","e2f755e6":"code","1f3b631d":"code","6765374f":"code","ed26192e":"code","6ff76204":"code","271a6bfe":"code","56bd6ccf":"code","6e3fea84":"code","aa35a788":"code","7475c503":"code","a38f61fc":"code","56e48c63":"code","1aac5013":"code","f18dc5cd":"code","83c3d528":"code","ab37dbb8":"code","cc240de6":"code","626461b8":"code","c0626dec":"code","a7d237f2":"code","42a62a9a":"code","c9f4e329":"code","8b80c1cf":"code","69ac6d61":"code","d4fdcaf6":"code","84f9c7e2":"code","3d7b736d":"code","9cd2c3c9":"code","3d9a77f1":"code","62341810":"code","ae507165":"code","6b5f687a":"code","bfcdc284":"code","99bd12a8":"code","81de32b4":"code","48e01125":"code","905652ed":"code","4ea457b7":"code","6ef572c5":"code","c2f70d52":"code","780a923b":"code","730ed198":"code","c5153bde":"code","908c2c5d":"code","77ff8448":"code","7a6550bd":"code","1e04f274":"code","d95f5540":"code","ece81d39":"code","10c3a5d0":"code","85c6b142":"code","1842c9e6":"code","ca598be6":"code","2f403a7d":"code","a5db2f86":"code","d2c66c1f":"code","da9a3297":"code","fdff0ed3":"code","a0ac167e":"code","00d5d2fa":"code","e368f788":"code","c85ac041":"code","385f564f":"code","741a427e":"code","aa016958":"code","9fa23a05":"code","6575ff7d":"code","87941bd4":"code","22c566a6":"code","55b20c35":"code","8c8c5f48":"code","e9c958a3":"code","888c9a02":"code","37c6ffbe":"code","87129e86":"markdown","7cdf456c":"markdown","e6e18721":"markdown","908c23c6":"markdown","d9bf5458":"markdown","fe1f66d1":"markdown","fefb7bf2":"markdown","726c4ae2":"markdown","3ba96791":"markdown","a5c2b036":"markdown","ae29ed2e":"markdown","6f5b03ab":"markdown","42255134":"markdown","061ed88c":"markdown","24e51525":"markdown","8ac1cccc":"markdown","a0d14135":"markdown","b4605c62":"markdown","b59ad16b":"markdown","2d997711":"markdown"},"source":{"8f09c378":"## Adding imports\n\nimport numpy as np\nimport pandas as pd\nimport gc\nimport re\nimport string\nimport operator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport operator\nfrom time import time\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, word_tokenize, sent_tokenize","4c523852":"## Loading necessary datasets\n\ntraining_set = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_set = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\noutput_file = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nprint('There are {} rows and {} columns in Training Dataset'.format(training_set.shape[0],training_set.shape[1]))\nprint('There are {} rows and {} columns in Test Dataset'.format(test_set.shape[0],test_set.shape[1]))","e2f755e6":"## Basic Data Exploration\n\ntraining_set.head()","1f3b631d":"training_set.tail()","6765374f":"print (training_set.shape, test_set.shape, output_file.shape)","ed26192e":"training_set.duplicated().sum()\ntraining_set = training_set.drop_duplicates().reset_index(drop=True)\nprint(training_set.shape)","6ff76204":"# Target distribution in Training dataset\n\nbar_plot=training_set.target.value_counts()\nsns.barplot(bar_plot.index,bar_plot)\nplt.gca().set_ylabel('Training Samples')","271a6bfe":"## Summary statistics for Training dataset\ntraining_set.describe()\ntraining_set.describe(include=['object'])\ntraining_set.isnull().sum()","56bd6ccf":"## Summary statistics for Test dataset\ntest_set.describe()\ntest_set.describe(include=['object'])\ntest_set.isnull().sum()","6e3fea84":"# Top keywords for disaster tweets\n\nkeywords_disaster = [kw for kw in training_set.loc[training_set.target == 1].keyword]\ntop_keywords_disaster = training_set[training_set.target==1].keyword.value_counts().head(10)\nprint ('Top keywords for disaster tweets in Training: ')\nprint (top_keywords_disaster)","aa35a788":"# Top keywords for non-disaster tweets\n\nkeywords_non_disaster = [kw for kw in training_set.loc[training_set.target == 0].keyword]\ntop_keywords_non_disaster = training_set[training_set.target==0].keyword.value_counts().head(10)\nprint ('Top keywords for non-disaster tweets in Training: ')\nprint (top_keywords_non_disaster)","7475c503":"# Checking if the same keywords are present in both the positive and negative classes\n\ndisaster_KW_cnts = dict(pd.DataFrame(data={'x': keywords_disaster}).x.value_counts())\nnon_disaster_KW_cnts = dict(pd.DataFrame(data={'x': keywords_non_disaster}).x.value_counts())\n\nall_keywords_counts =  dict(pd.DataFrame(data={'x': training_set.keyword.values}).x.value_counts())\n\nfor keyword, _ in sorted(all_keywords_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n    print(\"<Keyword>: {}\".format(keyword))\n    print(\"* # in Disaster tweets: {}\".format(disaster_KW_cnts.get(keyword, 0)))\n    print(\"* # in Non-Disaster tweets: {}\".format(non_disaster_KW_cnts.get(keyword, 0)))\n    print('')","a38f61fc":"# Finding URL, line breaks and extra spaces from series of tweets\n\ndef standardize_text(text):\n    #Removing available URLs\n    text = re.sub(r'https?:\/\/\\S+', '', text)\n    \n    # Removing line breaks\n    text = re.sub(r'\\n',' ', text)\n    \n    # Removing trailing and leading spaces \n    text = re.sub('\\s+', ' ', text).strip()\n    \n    #Removing non-ASCII characters \n    text = ''.join([x for x in text if x in string.printable])\n    \n    #Removing HTML tags\n    text = re.sub(r'<.*?>', ' ', text)\n    \n    return text\n\n\ntraining_set['text_cleaned']=training_set['text'].apply(lambda x : standardize_text(x))\ntest_set['text_cleaned']=test_set['text'].apply(lambda x : standardize_text(x))\n","56e48c63":"# Removing stopwords\n\ndef remove_stopwords(text):\n        stop_words=set(stopwords.words('english'))\n        if text is not None:\n            word_tokens = [x for x in word_tokenize(text) if x not in stop_words]\n            return \" \".join(word_tokens)\n        else:\n            return None\n\ntraining_set['text_cleaned']=training_set['text_cleaned'].apply(lambda x : remove_stopwords(x))\ntest_set['text_cleaned']=test_set['text_cleaned'].apply(lambda x : remove_stopwords(x))\n","1aac5013":"# Removing Emojis (source: online) and punctuations\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_punct(text):\n    trans_table=str.maketrans('','',string.punctuation)\n    return text.translate(trans_table)\n\ntraining_set['text_cleaned']=training_set['text_cleaned'].apply(lambda x: remove_emoji(x))\ntraining_set['text_cleaned']=training_set['text_cleaned'].apply(lambda x: remove_punct(x))\n\ntest_set['text_cleaned']=test_set['text_cleaned'].apply(lambda x: remove_emoji(x))\ntest_set['text_cleaned']=test_set['text_cleaned'].apply(lambda x: remove_punct(x))\n\ntraining_set.head(10)","f18dc5cd":"# Taking a look at the Top words for disaster tweets post basic pre-processing\n\n# Locating english stopwords\nstop_words = set(stopwords.words('english'))\n\nfreq_disaster = FreqDist(w for w in word_tokenize(' '.join(training_set.loc[training_set.target==1, 'text_cleaned']).lower()) if \n                     (w not in stop_words) & (w.isalpha()))\ndisaster_tweets = pd.DataFrame.from_dict(freq_disaster, orient='index', columns=['count'])\nTop_disaster_tweets = disaster_tweets.sort_values('count',ascending=False).head(15)\nprint ('Top words for disaster tweets in Training: ')\nprint (Top_disaster_tweets)","83c3d528":"# Taking a look at the Top words for non-disaster tweets post basic pre-processing\n\nfreq_nondisaster = FreqDist(w for w in word_tokenize(' '.join(training_set.loc[training_set.target==0, 'text_cleaned']).lower()) if \n                     (w not in stop_words) & (w.isalpha()))\nnon_disaster_tweets = pd.DataFrame.from_dict(freq_nondisaster, orient='index', columns=['count'])\nTop_non_disaster_tweets = non_disaster_tweets.sort_values('count',ascending=False).head(15)\nprint ('Top words for non-disaster tweets in Training: ')\nprint (Top_non_disaster_tweets)","ab37dbb8":"# Getting statistics to find the effect of stop words\n\ntraining_set['text_length'] = training_set['text_cleaned'].apply(len)\ntraining_set['word_count'] = training_set[\"text_cleaned\"].apply(lambda x: len(str(x).split()))\ntraining_set['stop_word_count'] = training_set['text_cleaned'].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\ntraining_set['caps_count'] = training_set['text_cleaned'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\ntraining_set['caps_ratio'] = training_set['caps_count'] \/ training_set['text_length']\n\nprint(training_set.shape, test_set.shape)","cc240de6":"#Checking for correlation of statistics features with the target variable\ntraining_set.corr()['target'].drop('target').sort_values()","626461b8":"def additional_cleaning(tweet): \n\n    # Correcting character based references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Correcting informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Correcting URL's\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n    \n    # Words with punctuations and special characters (source - online)\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n    \n    # Correcting acronymns that we could find\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    \n    # Removing line breaks\n    tweet = re.sub(r'\\n',' ', tweet) \n    \n    # Removing leading, trailing and extra spaces\n    tweet = re.sub('\\s+', ' ', tweet).strip() \n    \n    return tweet\n\n\n# Building cleaner versions of both the training and test datasets\n\ntraining_set['text_cleaned'] = training_set['text_cleaned'].apply(lambda s : additional_cleaning(s))\ntest_set['text_cleaned'] = test_set['text_cleaned'].apply(lambda s : additional_cleaning(s))","c0626dec":"# Set of other abbreviations found online\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, word_tokenize\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ntraining_set[\"text_cleaned\"] = training_set[\"text_cleaned\"].apply(lambda x: convert_abbrev_in_text(x))\ntest_set[\"text_cleaned\"] = test_set[\"text_cleaned\"].apply(lambda x: convert_abbrev_in_text(x))","a7d237f2":"from nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\nporter = PorterStemmer()\nlancaster = LancasterStemmer()\nsnowball = SnowballStemmer(\"english\")\n\ndef stemTweet_Porter(tweet):\n    token_words=word_tokenize(tweet)\n    clean_tweet=[]\n    for word in token_words:\n        clean_tweet.append(porter.stem(word))\n        clean_tweet.append(\" \")\n    return \"\".join(clean_tweet)\n\ndef stemTweet_Lancaster(tweet):\n    token_words=word_tokenize(tweet)\n    clean_tweet=[]\n    for word in token_words:\n        clean_tweet.append(lancaster.stem(word))\n        clean_tweet.append(\" \")\n    return \"\".join(clean_tweet)\n\ndef stemTweet_Snowball(tweet):\n    token_words=word_tokenize(tweet)\n    clean_tweet=[]\n    for word in token_words:\n        clean_tweet.append(snowball.stem(word))\n        clean_tweet.append(\" \")\n    return \"\".join(clean_tweet)\n\ntraining_set[\"text_cleaned_Port\"] = training_set[\"text_cleaned\"].apply(lambda x: stemTweet_Porter(x))\ntest_set[\"text_cleaned_Port\"] = test_set[\"text_cleaned\"].apply(lambda x: stemTweet_Porter(x))\n\ntraining_set[\"text_cleaned_Lanc\"] = training_set[\"text_cleaned\"].apply(lambda x: stemTweet_Lancaster(x))\ntest_set[\"text_cleaned_Lanc\"] = test_set[\"text_cleaned\"].apply(lambda x: stemTweet_Lancaster(x))\n\ntraining_set[\"text_cleaned_Snow\"] = training_set[\"text_cleaned\"].apply(lambda x: stemTweet_Lancaster(x))\ntest_set[\"text_cleaned_Snow\"] = test_set[\"text_cleaned\"].apply(lambda x: stemTweet_Lancaster(x))","42a62a9a":"from nltk.stem import WordNetLemmatizer\n\nlemmmatizer=WordNetLemmatizer()\n\ndef lemmatization_wordnet(tweet):\n    token_words=word_tokenize(tweet)\n    tokens = [lemmmatizer.lemmatize(word.lower(), pos = \"v\") for word in token_words]\n    clean_tweet = ' '.join(tokens)\n    return clean_tweet\n\ntraining_set[\"text_cleaned_Wordnet\"] = training_set[\"text_cleaned\"].apply(lambda x: lemmatization_wordnet(x))\ntest_set[\"text_cleaned_Wordnet\"] = test_set[\"text_cleaned\"].apply(lambda x: lemmatization_wordnet(x))","c9f4e329":"training_set[\"text_cleaned\"] = training_set[\"text_cleaned_Wordnet\"]\ntest_set[\"text_cleaned\"] = test_set[\"text_cleaned_Wordnet\"]\n","8b80c1cf":"mislabeled_tweets = training_set.groupby(['text']).nunique().sort_values(by='target', ascending=False)\nmislabeled_tweets = mislabeled_tweets[mislabeled_tweets['target'] > 1]['target']\nmislabeled_tweets.index.tolist()","69ac6d61":"## Treating mislabled samples in training set\n\ntraining_set['target_updated'] = training_set['target'].copy() \n\ntraining_set.loc[training_set['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife', 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == 'To fight bioterrorism sir.', 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4', 'target_updated'] = 1\ntraining_set.loc[training_set['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring', 'target_updated'] = 1\ntraining_set.loc[training_set['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_updated'] = 1\ntraining_set.loc[training_set['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG', 'target_updated'] = 1\ntraining_set.loc[training_set['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\", 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_updated'] = 1\ntraining_set.loc[training_set['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_updated'] = 0\ntraining_set.loc[training_set['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_updated'] = 0","d4fdcaf6":"# Using Bag of words count\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef bag_of_words_count(data):\n    count_vectorizer = CountVectorizer()\n    embed = count_vectorizer.fit_transform(data)\n    return embed, count_vectorizer\n\nlist_corpus = training_set[\"text_cleaned\"].tolist()\nlist_labels = training_set[\"target_updated\"].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.3, random_state=40)\n\nX_train_counts, count_vectorizer = bag_of_words_count(X_train)\n\nX_test_counts = count_vectorizer.transform(X_test)","84f9c7e2":"# Fitting a Classifier using Logistic Regression - Baseline model\n\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression(solver='liblinear', random_state=777)\n\nclassifier.fit(X_train_counts, y_train)\n\ny_predicted_counts = classifier.predict(X_test_counts)","3d7b736d":"# Function to get model evaluation metrics\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n\naccuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","9cd2c3c9":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\nprint ('Confusion Matrix for Logistic Model')\nprint(metrics.confusion_matrix(y_test, y_predicted_counts))","3d9a77f1":"# Creating submission file for the basic bag-of-words model\n\npreds_corpus = test_set[\"text_cleaned\"].tolist()\npreds_counts = count_vectorizer.transform(preds_corpus)\npreds_1 = classifier.predict(preds_counts)\n\noutput_file_1 = output_file.copy()\noutput_file_1.target = preds_1\noutput_file_1.to_csv('submission_LR_1.csv',index=False)\n","62341810":"# Using Tf-iDF Vectorizer for Text\n\ndef tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    \n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","ae507165":"# Simple Logistic Regression Model post TFIDF\n\nclassifier_tfidf = LogisticRegression(solver='liblinear', random_state=777)\nclassifier_tfidf.fit(X_train_tfidf, y_train)\n\ny_predicted_tfidf = classifier_tfidf.predict(X_test_tfidf)\n\naccuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, \n                                                                       recall_tfidf, f1_tfidf))","6b5f687a":"# Confusion Matrix for the LR model post TFIDF\n\nprint ('Confusion Matrix for Logistic Model')\nprint(metrics.confusion_matrix(y_test, y_predicted_tfidf))","bfcdc284":"# Creating submission file for TFIDF Model\n\npreds_corpus = test_set[\"text_cleaned\"].tolist()\npreds_tfidf = tfidf_vectorizer.transform(preds_corpus)\npreds_2 = classifier_tfidf.predict(preds_tfidf)\n\noutput_file_2 = output_file.copy()\noutput_file_2.target = preds_2\noutput_file_2.to_csv('submission_TFIDF_2.csv',index=False)","99bd12a8":"## Loading necessary datasets back again\n\ntraining_set_w2v = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_set_w2v = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\noutput_file_w2v = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nprint('There are {} rows and {} columns in Training Dataset'.format(training_set_w2v.shape[0],training_set_w2v.shape[1]))\nprint('There are {} rows and {} columns in Test Dataset'.format(test_set_w2v.shape[0],test_set_w2v.shape[1]))","81de32b4":"def additional_cleaning(tweet): \n\n    # Correcting character based references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Correcting informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Correcting URL's\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n    \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n    \n    # Correcting acronymns that we could find\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    \n    # Removing line breaks\n    tweet = re.sub(r'\\n',' ', tweet) \n    \n    # Removing leading, trailing and extra spaces\n    tweet = re.sub('\\s+', ' ', tweet).strip() \n    \n    # Removing non-ASCII characters\n    tweet = ''.join([x for x in tweet if x in string.printable])\n    \n    #Removing HTML tags\n    tweet = re.sub(r'<.*?>', ' ', tweet)\n    \n    return tweet\n\n# Building cleaner versions of both the training and test datasets\n\ntraining_set_w2v['text_cleaned'] = training_set_w2v['text'].apply(lambda s : additional_cleaning(s))\ntest_set_w2v['text_cleaned'] = test_set_w2v['text'].apply(lambda s : additional_cleaning(s))","48e01125":"# Set of other abbreviations found online\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ntraining_set_w2v[\"text_cleaned\"] = training_set_w2v[\"text_cleaned\"].apply(lambda x: convert_abbrev_in_text(x))\ntest_set_w2v[\"text_cleaned\"] = test_set_w2v[\"text_cleaned\"].apply(lambda x: convert_abbrev_in_text(x))","905652ed":"# Lemmatization using WordNet Lemmatizer\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmmatizer=WordNetLemmatizer()\n\ndef lemmatization_wordnet(tweet):\n    token_words=word_tokenize(tweet)\n    tokens = [lemmmatizer.lemmatize(word.lower(), pos = \"v\") for word in token_words]\n    clean_tweet = ' '.join(tokens)\n    return clean_tweet\n\ntraining_set_w2v[\"text_cleaned\"] = training_set_w2v[\"text_cleaned\"].apply(lambda x: lemmatization_wordnet(x))\ntest_set_w2v[\"text_cleaned\"] = test_set_w2v[\"text_cleaned\"].apply(lambda x: lemmatization_wordnet(x))","4ea457b7":"## Treating mislabled samples in training set\n\ntraining_set_w2v['target_updated'] = training_set_w2v['target'].copy() \n\ntraining_set_w2v.loc[training_set_w2v['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife', 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == 'To fight bioterrorism sir.', 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4', 'target_updated'] = 1\ntraining_set_w2v.loc[training_set_w2v['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring', 'target_updated'] = 1\ntraining_set_w2v.loc[training_set_w2v['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_updated'] = 1\ntraining_set_w2v.loc[training_set_w2v['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG', 'target_updated'] = 1\ntraining_set_w2v.loc[training_set_w2v['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\", 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_updated'] = 1\ntraining_set_w2v.loc[training_set_w2v['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_updated'] = 0\ntraining_set_w2v.loc[training_set_w2v['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_updated'] = 0","6ef572c5":"# Using word2vec\n\nimport gensim\n\nword2vec_path = \"..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","c2f70d52":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n    embeddings = training_set_w2v['text_cleaned'].apply(lambda x: get_average_word2vec(x, vectors, \n                                                                                generate_missing=generate_missing))\n    return list(embeddings)","780a923b":"list_labels = training_set_w2v[\"target_updated\"].tolist()","730ed198":"embeddings = get_word2vec_embeddings(word2vec, training_set_w2v)\nX_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels, \n                                                                                        test_size=0.3, random_state=40)","c5153bde":"classifier_w2v = LogisticRegression(solver='liblinear', random_state=777)\n\nclassifier_w2v.fit(X_train_word2vec, y_train_word2vec)\n\ny_predicted_word2vec = classifier_w2v.predict(X_test_word2vec)","908c2c5d":"accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n                                                                       recall_word2vec, f1_word2vec))","77ff8448":"# Confusion Matrix for the LR model Word2Vec\n\nprint ('Confusion Matrix for Logistic Model')\nprint(metrics.confusion_matrix(y_test_word2vec, y_predicted_word2vec))","7a6550bd":"import re\nimport string\nimport operator\nimport numpy as np\nimport pandas as pd\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import FreqDist, word_tokenize, sent_tokenize\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n# Using official tokenization script used by Google Team\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n\nimport tokenization","1e04f274":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","d95f5540":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=3e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","ece81d39":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","10c3a5d0":"## Loading necessary datasets back again\n\ntraining_set_bert = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_set_bert = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\noutput_file_bert = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nprint('There are {} rows and {} columns in Training Dataset'.format(training_set_bert.shape[0],training_set_bert.shape[1]))\nprint('There are {} rows and {} columns in Test Dataset'.format(test_set_bert.shape[0],test_set_bert.shape[1]))","85c6b142":"# Loading tokenizer from BERT layer\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","1842c9e6":"def additional_cleaning(tweet): \n\n    # Correcting character based references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Correcting informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Correcting URL's\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n    \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n    \n    # Correcting acronymns that we could find\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    \n    # Removing line breaks\n    tweet = re.sub(r'\\n',' ', tweet) \n    \n    # Removing leading, trailing and extra spaces\n    tweet = re.sub('\\s+', ' ', tweet).strip() \n    \n    # Removing HTML tags\n    tweet = re.sub(r'<.*?>', ' ', tweet)\n    \n    # Removing non-ASCII characters\n    tweet = ''.join([x for x in tweet if x in string.printable])\n    \n    return tweet\n\n# Building cleaner versions of both the training and test datasets\n\ntraining_set_bert['text_cleaned'] = training_set_bert['text'].apply(lambda s : additional_cleaning(s))\ntest_set_bert['text_cleaned'] = test_set_bert['text'].apply(lambda s : additional_cleaning(s))","ca598be6":"# Set of other abbreviations found online\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ntraining_set_bert[\"text_cleaned\"] = training_set_bert[\"text_cleaned\"].apply(lambda x: convert_abbrev_in_text(x))\ntest_set_bert[\"text_cleaned\"] = test_set_bert[\"text_cleaned\"].apply(lambda x: convert_abbrev_in_text(x))","2f403a7d":"# Removing Emojis (source: online) and punctuations\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_punct(text):\n    trans_table=str.maketrans('','',string.punctuation)\n    return text.translate(trans_table)\n\ntraining_set_bert['text_cleaned']=training_set_bert['text_cleaned'].apply(lambda x: remove_emoji(x))\ntraining_set_bert['text_cleaned']=training_set_bert['text_cleaned'].apply(lambda x: remove_punct(x))\n\ntest_set_bert['text_cleaned']=test_set_bert['text_cleaned'].apply(lambda x: remove_emoji(x))\ntest_set_bert['text_cleaned']=test_set_bert['text_cleaned'].apply(lambda x: remove_punct(x))","a5db2f86":"## Treating mislabled samples in training set\n\ntraining_set_bert['target_updated'] = training_set_bert['target'].copy() \n\ntraining_set_bert.loc[training_set_bert['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife', 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == 'To fight bioterrorism sir.', 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4', 'target_updated'] = 1\ntraining_set_bert.loc[training_set_bert['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring', 'target_updated'] = 1\ntraining_set_bert.loc[training_set_bert['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_updated'] = 1\ntraining_set_bert.loc[training_set_bert['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG', 'target_updated'] = 1\ntraining_set_bert.loc[training_set_bert['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\", 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_updated'] = 1\ntraining_set_bert.loc[training_set_bert['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_updated'] = 0\ntraining_set_bert.loc[training_set_bert['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_updated'] = 0","d2c66c1f":"# Encoding tweets into tokens, masks and segment flags\n\ntrain_input = bert_encode(training_set_bert.text_cleaned.values, tokenizer, max_len=160)\ntest_input = bert_encode(test_set_bert.text_cleaned.values, tokenizer, max_len=160)\ntrain_labels = training_set_bert.target_updated.values\n\nprint (training_set_bert.text_cleaned.values)\nprint (training_set_bert.target_updated.values)\n","da9a3297":"# Model build and statistics\n\nmodel = build_model(bert_layer, max_len=160)\nmodel.summary()","fdff0ed3":"# Model Train and save\n\ncheckpoint = ModelCheckpoint('model3.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","a0ac167e":"# BERT Predictions and Submission\n\nmodel.load_weights('model2.h5')\ntest_pred = model.predict(test_input)\n\noutput_file_bert['target'] = test_pred.round().astype(int)\noutput_file_bert.to_csv('BERT_submission_3.csv', index=False)","00d5d2fa":"## Imports, Loading necessary data sets and tokenization\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import array\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import one_hot,Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential,Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import LSTM,Dense,SpatialDropout1D,Flatten,Input\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\n\n\n## Loading necessary datasets back again\n\ntraining_set_glove = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_set_glove = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\noutput_file_glove = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\nprint('There are {} rows and {} columns in Training Dataset'.format(training_set_glove.shape[0],training_set_glove.shape[1]))\nprint('There are {} rows and {} columns in Test Dataset'.format(test_set_glove.shape[0],test_set_glove.shape[1]))\n\ndataset_glove = training_set_glove.append(test_set_glove,ignore_index=True)","e368f788":"# Loading glove\n\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\nglove_file = open('..\/input\/glove-common-crawl-42b-tokens\/glove.42B.300d.txt','r')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\n\nglove_file.close()","c85ac041":"def build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\n\ndef check_embeddings_coverage(X, embeddings):\n    vocab = build_vocab(X)    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n    return covered, oov, n_covered, n_oov","385f564f":"covered, oov, n_covered, n_oov = check_embeddings_coverage(dataset_glove[\"text\"], embeddings_dictionary)\nprint(f\"Number of words covered by Glove embeddings :: {n_covered}\")\nprint(f\"Number of words not covered by Glove embeddings :: {n_oov}\")\nprint(f\"Percentage of words covered by Glove embeddings :: {(n_covered\/(n_covered + n_oov)) * 100}%\")","741a427e":"dataset_glove[\"text\"] = dataset_glove[\"text\"].apply(lambda x : x.lower())\ndataset_glove[\"keyword\"].fillna(\"keyword\", inplace = True)\n\ndataset_glove[\"text\"] = dataset_glove[\"text\"] + \" \" + dataset_glove[\"keyword\"]\ndataset_glove.drop([\"keyword\", \"location\"], axis = 1, inplace = True)\n\nwords_list = \" \".join(dataset_glove[\"text\"])\nnot_english = [word for word in words_list.split() if word.isalpha() == False]","aa016958":"def additional_cleaning(tweet): \n\n    # Correcting character based references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Correcting informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Correcting URL's\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet)\n    \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, f' {p} ')\n    \n    # Correcting acronymns that we could find\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"m\u00cc\u00bcsica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    \n    # Removing line breaks\n    tweet = re.sub(r'\\n',' ', tweet) \n    \n    # Removing leading, trailing and extra spaces\n    tweet = re.sub('\\s+', ' ', tweet).strip() \n    \n    # Removing HTML tags\n    tweet = re.sub(r'<.*?>', ' ', tweet)\n    \n    # Removing non-ASCII characters\n    tweet = ''.join([x for x in tweet if x in string.printable])\n    \n    # Removing words that are not alphabets\n    t = [w for w in tweet.split() if w not in not_english]\n    data = \" \".join(t)\n    \n    return tweet\n\n# Building cleaner version of the dataset\n\ndataset_glove['text_cleaned'] = dataset_glove['text'].apply(lambda s : additional_cleaning(s))","9fa23a05":"# Set of other abbreviations found online\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\ndataset_glove[\"text_cleaned\"] = dataset_glove[\"text_cleaned\"].apply(lambda x: convert_abbrev_in_text(x))\n","6575ff7d":"# Removing Emojis (source: online) and punctuations\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_punct(text):\n    trans_table=str.maketrans('','',string.punctuation)\n    return text.translate(trans_table)\n\ndataset_glove['text_cleaned']=dataset_glove['text_cleaned'].apply(lambda x: remove_emoji(x))\ndataset_glove['text_cleaned']=dataset_glove['text_cleaned'].apply(lambda x: remove_punct(x))","87941bd4":"## Treating mislabled samples in training set\n\ntraining_set_glove['target_updated'] = training_set_glove['target'].copy() \n\ntraining_set_glove.loc[training_set_glove['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife', 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == 'To fight bioterrorism sir.', 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4', 'target_updated'] = 1\ntraining_set_glove.loc[training_set_glove['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring', 'target_updated'] = 1\ntraining_set_glove.loc[training_set_glove['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_updated'] = 1\ntraining_set_glove.loc[training_set_glove['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG', 'target_updated'] = 1\ntraining_set_glove.loc[training_set_glove['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\", 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_updated'] = 1\ntraining_set_glove.loc[training_set_glove['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_updated'] = 0\ntraining_set_glove.loc[training_set_glove['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_updated'] = 0","22c566a6":"# Checking again after some data cleaning\ncovered, oov, n_covered, n_oov = check_embeddings_coverage(dataset_glove[\"text_cleaned\"], embeddings_dictionary)\nprint(f\"Number of words covered by Glove embeddings --> {n_covered}\")\nprint(f\"Number of words not covered by Glove embeddings --> {n_oov}\")\nprint(f\"Percentage of words covered by Glove embeddings --> {(n_covered\/(n_covered + n_oov)) * 100}%\")","55b20c35":"# Tokenization\n\nembed_size = 300 \nmaxlen = 20\nmax_features = 20000\n\ntokenizer = Tokenizer(oov_token = \"<OOV>\", num_words = max_features)\ntokenizer.fit_on_texts(dataset_glove[\"text_cleaned\"])\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(dataset_glove[\"text_cleaned\"])\npadded = pad_sequences(sequences, padding = \"post\", maxlen = maxlen)\n\ntraining_glove = padded[:7613, :]\ntest_glove = padded[7613:, :]\ntrain_y = dataset_glove[dataset_glove[\"target\"].isnull() == False][\"target\"].apply(int).values.reshape(-1, 1)","8c8c5f48":"num_words = min(max_features, len(word_index)) + 1\n\nembedding_dim = 300\n\n# first create a matrix of zeros, this is our embedding matrix\nembedding_matrix = np.zeros((num_words, embedding_dim))\n\nfor word, i in word_index.items():\n    if i > max_features:\n        continue\n    embedding_vector = covered.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","e9c958a3":"# Model build\n\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(num_words,\n                    embedding_dim,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=maxlen,\n                    trainable=False),\n    tf.keras.layers.SpatialDropout1D(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dropout(0.10),\n    tf.keras.layers.Dense(units=32, activation=\"relu\"),\n    tf.keras.layers.Dense(units=8, activation=\"relu\"),\n    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")\n])\n\nmodel.compile(loss = \"binary_crossentropy\", optimizer='adam', metrics = [\"accuracy\"])\nmodel.summary()","888c9a02":"# Model compilation and summary\n\nbatch_size = 128\nnum_epochs = 20\n\nhistory = model.fit(training_glove, train_y, batch_size = batch_size, epochs = num_epochs)","37c6ffbe":"# Creating submission file for Glove\n\ny_predict=model.predict(test_glove)\ny_predict = np.round(y_predict).astype(int)\n#print (y_predict)\noutput_file_glove['target'] = y_predict.round().astype(int)\noutput_file_glove.to_csv('Glove_submission_2.csv', index=False)","87129e86":"# Handling Mislabeled tweets","7cdf456c":"# Finding if a tweet refers to an actual disaster or not using NLP\n\nThis is a competition where we predict whether a tweet actually refers to real disasters or not. ","e6e18721":"# Using pre-trained Glove embeddings","908c23c6":"**Conclusion:** Location data is too sparse to be used as a feature.","d9bf5458":"# Location Analysis","fe1f66d1":"# Additional Data sanitization","fefb7bf2":"# BERT using TensorFlow Hub\n\nBERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. BERT\u2019s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training.","726c4ae2":"**LR Classifier (using Porter stemmer) Metrics**\n\naccuracy = 0.798, precision = 0.798, recall = 0.798, f1 = 0.796\n\n**LR Classifier (using Lancaster stemmer) Metrics**\n\naccuracy = 0.799, precision = 0.800, recall = 0.799, f1 = 0.796\n\n**LR Classifier (using Snowball stemmer) Metrics**\n\naccuracy = 0.799, precision = 0.800, recall = 0.799, f1 = 0.796\n\n**LR Classifier (using WordNet lemmatizer) Metrics**\n\naccuracy = 0.799, precision = 0.800, recall = 0.799, f1 = 0.797\n","3ba96791":"# Bag of words - Using TF-IDF\n\nTF-IDF stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. TF-IDF can be successfully used for stop-words filtering in various subject fields including text summarization and classification.\n\nTF-IDF follows a similar logic than the one-hot encoded vectors explained above. However, instead of only counting the occurence of a word in a single document it also does so in relation to the entire corpus. This allows us to detect how important a word is to a document in a corpus.","a5c2b036":"# Text Normalization - Stemming","ae29ed2e":"**LR Classifier (using Porter stemmer) Metrics**\n\naccuracy = 0.808, precision = 0.812, recall = 0.808, f1 = 0.804\n\n**LR Classifier (using Lancaster stemmer) Metrics**\n\naccuracy = 0.808, precision = 0.814, recall = 0.808, f1 = 0.804\n\n**LR Classifier (using Snowball stemmer) Metrics**\n\naccuracy = 0.808, precision = 0.814, recall = 0.808, f1 = 0.804\n\n**LR Classifier (using WordNet Lemmatizer) Metrics**\n\naccuracy = 0.812, precision = 0.816, recall = 0.812, f1 = 0.808\n","6f5b03ab":"# Basic pre-processing","42255134":"# Keywords Analysis","061ed88c":"# Basic EDA","24e51525":"# Pre-trained Word2vec Embedding\n\nTrying to use GloVe, fastText and Word2vec pre-trained embeddings since they would have a better vocabulary than the default nltk vectorizers. Pickled versions of both GloVe and fastText are also available in Kaggle data sources. \n\n**GloVe** (Global Vectors for word representation) is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space.\n\n**fastText** is a library for learning of word embeddings and text classification created by Facebook's AI Research (FAIR) lab. The model allows to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words. Facebook makes available pretrained models for 294 languages.\n\n**Word2vec** is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence.\n\nWhen we use pre-trained embeddings, we would avoid the standard preprocessing steps since we may lose some valuable information in the due process. So, we would feed the raw text to both GloVe and fastText algorithms than feeding the cleaned versions of the data using other preprocessing methods.","8ac1cccc":"# Bag of words - using One-Hot Encoding\n\nOur goal is to first create a useful embedding for each sentence (or tweet) in our dataset, and then use these embeddings to accurately predict the relevant category.\n\nThe simplest approach we can start with is to use one hot encoding, and apply a logistic regression on top. The one-hot encoding just associates an index to each word in our vocabulary, and embeds each sentence as a list of 0s, with a 1 at each index corresponding to a word present in the sentence.\n\nThe **CountVectorizer** provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\n","a0d14135":"There are 18 unique tweets in training set that are labeled differently in their duplicates. Some of the meaning of the tweets are not very clear and maybe they were labeled by different people across different intervals of time. Tweets with two unique target values are relabeled since they can affect the training score.","b4605c62":"**Conclusion:** Many Keywords are present both in the negative as well as the positive class. So, even the Keywords cannot be used as a feature directly. ","b59ad16b":"# Text Normalization - Lemmatization","2d997711":"> **BERT Model epoch results: **\n\n**Commit 1 - model.h5 - Adam(lr=1e-5), epochs = 3, batch_size = 16, validation_split = 0.3** (Basic Text Cleaning + Target correction)\n\n**LB - 0.8326**\n\nEpoch 1\/3\n334\/334  - 383s 1s\/step - loss: 0.4855 - accuracy: 0.7733 - val_loss: 0.5491 - val_accuracy: 0.7504\n\nEpoch 2\/3\n334\/334  - 383s 1s\/step - loss: 0.3474 - accuracy: 0.8604 - val_loss: 0.3967 - val_accuracy: 0.8341\n\nEpoch 3\/3\n334\/334  - 345s 1s\/step - loss: 0.2442 - accuracy: 0.9096 - val_loss: 0.4138 - val_accuracy: 0.8371\n\n\n**Commit 2 - model1.h5 - Adam(lr=1e-5), epochs = 3, batch_size = 16, validation_split = 0.3** (Basic Text cleaning + Rem Emojis, punc + Rem Abbreviations + Target correction )\n\n**LB - 0.8348**\n\nEpoch 1\/3\n334\/334  - 380s 1s\/step - loss: 0.4285 - accuracy: 0.8148 - val_loss: 0.3855 - val_accuracy: 0.8288\n\nEpoch 2\/3\n334\/334  - 345s 1s\/step - loss: 0.2760 - accuracy: 0.8945 - val_loss: 0.4091 - val_accuracy: 0.8327\n\nEpoch 3\/3\n334\/334  - 345s 1s\/step - loss: 0.1562 - accuracy: 0.9420 - val_loss: 0.4924 - val_accuracy: 0.8327\n\n**Commit 3 - model2.h5 - Adam(lr=1e-5), epochs = 3, batch_size = 16, validation_split = 0.2** (Basic Text cleaning + Rem Emojis, punc + Rem Abbreviations + Target correction )\n\n**LB - 0.83879** **(Best with BERT so far)**\n\nEpoch 1\/3\n381\/381  - 403s 1s\/step - loss: 0.4198 - accuracy: 0.8194 - val_loss: 0.3569 - val_accuracy: 0.8444\n\nEpoch 2\/3\n381\/381  - 373s 980ms\/step - loss: 0.2378 - accuracy: 0.9056 - val_loss: 0.4193 - val_accuracy: 0.8181\n\nEpoch 3\/3\n381\/381  - 374s 981ms\/step - loss: 0.1015 - accuracy: 0.9634 - val_loss: 0.5450 - val_accuracy: 0.8299"}}