{"cell_type":{"b2cc11f9":"code","3a43acef":"code","ad470f6d":"code","d0c43abe":"code","abb8da53":"code","3b32e3a8":"code","5fc48e01":"code","f2409bd0":"code","09e6a855":"code","78180501":"code","b98bc44b":"code","a12c78f9":"code","a409ddd1":"code","e78c373f":"code","220788de":"code","3a171144":"code","1af50523":"code","88f4f4aa":"code","6eebfddb":"code","0cfc0da2":"code","72f0759c":"code","f1fbc3ec":"code","7775e4b0":"code","7ed1477c":"code","256e38f0":"code","747e403e":"code","382a1a55":"code","8e79fba9":"code","7e4a0453":"code","2114e1f7":"code","d2784b17":"code","b82af830":"markdown","58356ec6":"markdown"},"source":{"b2cc11f9":"#import liobraries \nimport tensorflow as tf\nfrom tensorflow.keras import models, layers\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","3a43acef":"IMAGE_SIZE = 256\nBATCH_SIZE = 32\nCHANNELS = 3\nEPOCHS = 40","ad470f6d":"#load dataset\ndataset = tf.keras.preprocessing.image_dataset_from_directory(\n        '\/kaggle\/input\/rice-leaf-images\/rice_images',\n        shuffle=True,\n        image_size = (IMAGE_SIZE, IMAGE_SIZE),\n        batch_size = BATCH_SIZE\n)","d0c43abe":"class_names = dataset.class_names","abb8da53":"len(dataset)","3b32e3a8":"#exploring a single batch\nplt.figure(figsize=(10,10)) #increase image area to 10x10\nfor image_batch, label_batch in dataset.take(1):\n    for i in range(12):\n        ax = plt.subplot(3,4,i+1)\n        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[label_batch[i]])#display title\n        plt.axis(\"off\")\n        #print(image_batch[0].shape)#exploration\n        #print(label_batch.numpy())#covert to numpy","5fc48e01":"#split dataset in function\ndef get_dataset_partition_tf (ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n    \n    ds_size = len(ds) #dataset size\n    if shuffle:\n        ds = ds.shuffle(shuffle_size, seed=12)\n        \n    train_size = int(train_split * ds_size) #train size converted to intger to avoid float\n    val_size = int(val_split * ds_size)\n    \n    train_ds = ds.take(train_size)\n    val_ds = ds.skip(train_size).take(val_size)\n    test_ds = ds.skip(train_size).skip(val_size)\n    \n    return train_ds, val_ds, test_ds","f2409bd0":"train_ds, val_ds, test_ds = get_dataset_partition_tf(dataset)","09e6a855":"len(train_ds)","78180501":"len(val_ds)","b98bc44b":"len(test_ds)","a12c78f9":"#cache will read image from disk and keep image in memeory for next iteration. \n#prefetch and cache to optimize our input pipeline\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) #buffer_size=tf.data.AUTOTUNE is to allow tf determine the batch size\nval_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\ntest_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)","a409ddd1":"#resize and rescale\nresize_and_rescale = tf.keras.Sequential([\n    layers.experimental.preprocessing.Resizing(IMAGE_SIZE,IMAGE_SIZE), #resize new inputs \n    layers.experimental.preprocessing.Rescaling(1.0\/255) #scale down RGB\n])","e78c373f":"#data_augmentation\ndata_augmentation = tf.keras.Sequential([\n    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n    layers.experimental.preprocessing.RandomRotation(0.2),\n])","220788de":"input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\nn_classes = 4\n\nmodel = models.Sequential([\n    resize_and_rescale,\n    data_augmentation,\n    layers.Conv2D(32, (3,3), activation='relu', input_shape = input_shape), #google Conv2D for all arguement. 32=no. of layers\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64,(3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64,(3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    layers.Conv2D(64,(3,3), activation='relu'),\n    layers.MaxPooling2D((2,2)),\n    #flatten into an array of neurons\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'), #dense layer of 64 neurons\n    layers.Dense(n_classes, activation='softmax'), #softmax normalizes the prob of classes.  \n])\n\nmodel.build(input_shape=input_shape)","3a171144":"model.summary()","1af50523":"model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n    metrics=['accuracy'] #track training process\n)","88f4f4aa":"#good to record the history of every epochs in params\nhistory = model.fit(\n    train_ds,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    verbose=1,\n    validation_data=val_ds\n)","6eebfddb":"#test model with test_ds\nscores = model.evaluate(test_ds)","0cfc0da2":"history #google variable output for arguements ","72f0759c":"history.params","f1fbc3ec":"history.history.keys()","7775e4b0":"history.history['accuracy']","7ed1477c":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","256e38f0":"#plotting training and validation accuracy\nplt.figure(figsize=(8,8))\nplt.subplot(1,2,1)\nplt.plot(range(EPOCHS), acc, label = 'Training Accuracy')\nplt.plot(range(EPOCHS), val_acc, label ='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy Plot')\n\nplt.subplot(1,2,2)\nplt.plot(range(EPOCHS), loss, label = 'Training Loss')\nplt.plot(range(EPOCHS), val_loss, label ='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Accuracy Loss')\nplt.show()","747e403e":"history.history['val_accuracy']","382a1a55":"np.argmax([0.04980154,0.5683826,0.2425103,0.13930552])","8e79fba9":"#make a prediction - 1 batch - test_ds\n\nfor images_batch, labels_batch in test_ds.take(1):\n    #print(images_batch[0].numpy().astype('uint8'))\n    #plt.imshow(images_batch[0].numpy().astype('uint8'))\n    first_image = images_batch[0].numpy().astype('uint8')\n    first_label = labels_batch[0].numpy()\n    \n    print(\"first image to predict\")\n    plt.imshow(first_image)\n    print(\"actual label:\", class_names[first_label])\n    \n    #model is ready so call predict function \n    batch_prediction = model.predict(images_batch)\n    print(\"prediction: \", class_names[np.argmax(batch_prediction[0])])","7e4a0453":"#prediction class vs confidence\ndef predict(model,img):\n    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n    img_array = tf.expand_dims(img_array,0) #create a batch\n    \n    predictions = model.predict(img_array)\n    \n    predicted_class = class_names[np.argmax(predictions[0])]\n    confidence = round(100 * (np.max(predictions[0])),2)\n    return predicted_class, confidence","2114e1f7":"plt.figure(figsize=(15,15))\nfor images, labels in test_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3,3,i+1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        \n        predicted_class, confidence = predict(model, images[i].numpy())\n        actual_class = class_names[labels[i]]\n        \n        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\") #using python format string\n        \n        plt.axis(\"off\")","d2784b17":"model_version = 1\nmodel.save(f\"rice_models\/{model_version}\")","b82af830":"# split dataset into train and test \n### 80% ==> training \n### 20% ==> 10% validation, 10% test","58356ec6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}