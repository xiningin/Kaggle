{"cell_type":{"50bc0b87":"code","6ba1b72d":"code","fb42ff66":"code","ca371f16":"code","f0ebd80b":"code","d258e961":"code","51804c8d":"code","5145c5f4":"code","fc7a322b":"code","c6bb7d8d":"code","34b251f9":"code","79e7feb1":"code","691e5d75":"code","07a98c0d":"code","b7150304":"code","426a959f":"code","e0f20dc8":"code","03f5bef9":"code","b7f2d418":"code","1072e55e":"code","c92260ef":"markdown","a3cb9b8c":"markdown","a9bc672c":"markdown","5ac7878a":"markdown","ea97fb12":"markdown","25961156":"markdown","fd8027f3":"markdown","9061fbcc":"markdown","e6ef8ed3":"markdown","24a9e675":"markdown","00395b12":"markdown","e729598b":"markdown","b2023a6f":"markdown"},"source":{"50bc0b87":"import numpy as np\nimport pandas as pd\nimport time\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import PowerTransformer\n\nimport warnings\nwarnings.filterwarnings('ignore')","6ba1b72d":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","fb42ff66":"#observe the different feature type present in the data\ndf.info()","ca371f16":"classes=df['Class'].value_counts()\nnormal_share=classes[0]\/df['Class'].count()*100\nfraud_share=classes[1]\/df['Class'].count()*100\n\nprint(\"normal_share=\",normal_share,\"            \",\"fraud_share=\",fraud_share)\n\nimbalance= (fraud_share\/normal_share)*100\nprint('Imbalance Percentage = ' + str(imbalance))","f0ebd80b":"# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\nclasses.plot(kind='bar', rot=0, ax=ax[0])\nax[0].set_title('Number of Class Distributions \\n (0: No Fraud || 1: Fraud)')\n\n(classes\/df['Class'].count()*100).plot(kind='bar', rot=0, ax=ax[1])\nax[1].set_title('Percentage of Distributions \\n (0: No Fraud || 1: Fraud)')\n\nplt.show()","d258e961":"# Create a scatter plot to observe the distribution of classes with time\ndf.plot.scatter(y='Class', x='Time',figsize=(18,4))","51804c8d":"# Create a scatter plot to observe the distribution of classes with Amount\ndf.plot.scatter(y='Class', x='Amount',figsize=(18,4))","5145c5f4":"# Drop unnecessary columns\ndf = df.drop(['Time'],axis=1)\ndf.head()","fc7a322b":"y= df['Class']\nX= df.loc[:, df.columns != 'Class']","c6bb7d8d":"from sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 42, stratify=y)","34b251f9":"print(np.sum(y))\nprint(np.sum(y_train))\nprint(np.sum(y_test))","79e7feb1":"# plot the histogram of a variable from the dataset to see the skewness\n\nk=0\nfig, ax = plt.subplots(7, 4, figsize=(20,20))\nfor i in range(7):\n    for j in range(4):\n        k=k+1\n        sns.distplot(X_train['V'+str(k)], ax=ax[i][j])\n        ax[i][j].set_title('V'+str(k))\n       ","691e5d75":"# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\npt= preprocessing.PowerTransformer(method='yeo-johnson', copy=True)\npt.fit(X_train)                       \n\nX_train_pt = pt.transform(X_train)\nX_test_pt = pt.transform(X_test)\n\ny_train_pt = y_train\ny_test_pt = y_test","07a98c0d":"print(X_train_pt.shape)\nprint(y_train_pt.shape)","b7150304":"# plot the histogram of a variable from the dataset again to see the result \nX_train_pt_df = pd.DataFrame(X_train_pt,columns=X_train.columns)\nk=0\nfig, ax = plt.subplots(7, 4, figsize=(20,20))\nfor i in range(7):\n    for j in range(4):\n        k=k+1\n        sns.distplot(X_train_pt_df['V'+str(k)], ax=ax[i][j])\n        ax[i][j].set_title('V'+str(k))","426a959f":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom imblearn import over_sampling\n","e0f20dc8":"# perfom cross validation on the X_train & y_train \nskf = StratifiedKFold(n_splits=3, random_state=None, shuffle=False)\n\nprint(\"XGBOOST Classifier: --------------------------\")\ncv_score_mean=0\nfor n_estimators in [100,50]:\n    for learning_rate in [0.2,0.6]:\n        for subsample in [0.3, 0.6, 0.9]:\n            print(\"n_estimators=\",n_estimators,\"learning_rate=\",learning_rate, \"subsample=\",subsample)\n            for train_index, test_index in skf.split(X_train_pt, y_train_pt):\n                print(\"Train:\", train_index, \"Test:\", test_index)\n                X_train_cv, X_test_cv = X_train_pt[train_index], X_train_pt[test_index]\n                y_train_cv, y_test_cv = y_train_pt.iloc[train_index], y_train_pt.iloc[test_index]\n\n                ros = over_sampling.SMOTE(sampling_strategy='minority', random_state=42)\n                X_ros_cv,y_ros_cv = ros.fit_resample(X_train_cv,y_train_cv)\n\n                xgboost_classifier= XGBClassifier(n_estimators=n_estimators,\n                                                learning_rate=learning_rate,\n                                                subsample=subsample, n_jobs=-1)\n                xgboost_classifier.fit(X_ros_cv,y_ros_cv)\n\n                y_test_pred= xgboost_classifier.predict_proba(X_test_cv)\n                cv_score= metrics.roc_auc_score(y_true=y_test_cv,y_score=y_test_pred[:,1])\n                cv_score_mean=cv_score_mean+cv_score\n            print(\"Cross Val ROC-AUC Score=\", cv_score_mean\/3)\n  ","03f5bef9":"clf = XGBClassifier(n_estimators=100,learning_rate=0.2,subsample=0.3, n_jobs=-1) \nros = over_sampling.SMOTE(sampling_strategy='minority', random_state=42)\nX_ros,y_ros = ros.fit_resample(X_train,y_train) \nclf.fit(X_ros.values,y_ros)\ny_pred= clf.predict_proba(X_test.values)\nscore= metrics.roc_auc_score(y_true=y_test,y_score=y_pred[:,1])\nprint(\"XGBOOST Classifier Test ROC-AUC Score =\", score)","b7f2d418":"var_imp = []\n\nfor i in clf.feature_importances_:\n    var_imp.append(i)\nprint('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\nprint('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\nprint('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n\n# Variable on Index-13 and Index-9 seems to be the top 2 variables\ntop_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\nsecond_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\nnp.random.shuffle(X_train_0)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [20, 20]\n\nplt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n            label='Actual Class-0 Examples')\nplt.legend()","1072e55e":"print('Train auc =', metrics.roc_auc_score(y_test, y_pred[:,1]))\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred[:,1])\nthreshold = thresholds[np.argmax(tpr-fpr)]\nprint(threshold)","c92260ef":"Here we will observe the distribution of our classes","a3cb9b8c":"### Print the important features of the best model to understand the dataset","a9bc672c":"### Splitting the data into train & test data\n","5ac7878a":"Difierent \"Thresholds\" can be used to calculate Precision and Recall. Then a decision can be made to choose a threshold for higher Precision or higher Recall.\n \n* Is it more important for a predicted fraud to actually be a fraud (higher Precision)?\n* Or prediction potential fraud is more important sych that none escapes (higher Recall)?","ea97fb12":"## Credit Card Fraud Detection\n\nPredict fraudulent credit card transactions with the help of Machine learning models. \n\nImport the following libraries to get started.","25961156":"### There is skewness present in the distribution use:\n- <b>Power Transformer<\/b> package present in the <b>preprocessing library provided by sklearn<\/b> to make distribution more gaussian","fd8027f3":"##### Preserve X_test & y_test to evaluate on the test data once you build the model","9061fbcc":"'Time' variable is uniformly distributed and thus doesn't provide any variation for classification. This can be dropped.","e6ef8ed3":"## Model building\n\n* Build XGBOOST\n* Perform class balancing with SMOTE\n* Perform Hyperparameter tuning using Cross Validation\n* Evaluate using ROC-AUC score on test set","24a9e675":"### Use the best model to Predict on the test dataset","00395b12":"### Plotting the distribution of variables","e729598b":"### Print the best threshold from the roc curve","b2023a6f":"## Exploratory data analysis"}}