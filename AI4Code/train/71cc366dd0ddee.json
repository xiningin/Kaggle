{"cell_type":{"01e7bcbd":"code","d76b70d6":"code","6f789159":"code","8ffd9d96":"code","dcbcf6a0":"code","78971d69":"code","e3d43c8d":"code","7bd5067e":"code","fc4d9ae3":"code","32ff936e":"code","5332ee37":"code","a544af33":"code","952ef386":"code","ad66ed06":"code","f7c21e01":"code","bc8092bc":"code","271e7ff3":"code","bb527afe":"code","05d341fe":"code","7c9ecf83":"code","4a12d7e2":"code","b128084a":"code","8495f633":"code","9fac0938":"code","73363670":"code","1c76cfeb":"code","1247e21c":"code","74e23ff2":"code","b9a393b1":"code","d233668a":"code","a674849c":"code","69dc59ee":"code","a15acffa":"code","b7a9e3ee":"code","d339f587":"code","75637937":"code","a1b485e7":"code","46e88946":"code","94968585":"code","010bb904":"code","34fbc611":"code","a95ed61b":"code","6b24178a":"code","5d752800":"code","47a39728":"code","e7440dc2":"code","0c5dc8e3":"code","e76553c8":"markdown","0256284b":"markdown","e6223c21":"markdown"},"source":{"01e7bcbd":"!unzip \/content\/glove.zip -d \/content\/drive\/MyDrive\/dataset\/","d76b70d6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Activation,Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM","6f789159":"df=pd.read_csv(\"\/content\/drive\/MyDrive\/dataset\/cleaned_train.csv\")\ndf.columns","8ffd9d96":"df.drop(['text','location'],axis=1,inplace=True)","dcbcf6a0":"df.keyword.fillna('unknown',inplace=True)\ndf.isnull().sum()","78971d69":"df1=pd.read_csv(\"\/content\/drive\/MyDrive\/dataset\/cleaned_test.csv\")\ndf1.columns","e3d43c8d":"df1.drop(['text','location'],axis=1,inplace=True)","7bd5067e":"df1.keyword.fillna('unknown',inplace=True)\ndf1.isnull().sum()","fc4d9ae3":"# Bag of Words model\nfrom keras.preprocessing.text import Tokenizer\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","32ff936e":"# lets use only tweet text to build the model\nX = df.text_cleaned\ny = df.target","5332ee37":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","a544af33":"# create and apply tokenizer on the training dataset\ntokenizer = create_tokenizer(X_train)\nX_train_set = tokenizer.texts_to_matrix(X_train, mode = 'freq')","952ef386":"# define the model\ndef define_model(n_words):\n    # define network\n    model = Sequential()\n    model.add(Dense(128, input_shape=(n_words,), activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [get_score])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","ad66ed06":"# function to calculate f1 score for each epoch\nimport keras.backend as K\ndef get_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","f7c21e01":"# create the model\nn_words = X_train_set.shape[1]\nmodel = define_model(n_words)","bc8092bc":"#fit network\nmodel.fit(X_train_set,y_train,epochs=10,verbose=2)","271e7ff3":"# prediction on the test dataset\nX_test_set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\n#y_pred = model.predict_classes(X_test_set)\ny_pred1=model.predict(X_test_set) \ny_pred=np.argmax(y_pred1,axis=1)","bb527afe":"# important metrices\nprint(classification_report(y_test, y_pred))","05d341fe":"# apply tokenizer on the test dataset\ntest_set1 = tokenizer.texts_to_matrix(df1.text_cleaned, mode = 'freq')","7c9ecf83":"# make predictions on the test dataset\n#y_test_pred = model.predict_classes(test_set)\ny_test_pred2=model.predict(test_set1) \ny_test_pred3=np.argmax(y_test_pred2,axis=1)","4a12d7e2":"test_id=df1.id","b128084a":"len(test_id)","8495f633":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred3\nsub.to_csv('submission_1.csv',index=False)","9fac0938":"# Fitting a tokenizer on text will create a list of unique words with an integer assigned to it\ntoken = Tokenizer()\ntoken.fit_on_texts(X_train.tolist())","73363670":"# lets save the size of the vocab\nvocab_size = len(token.word_index) + 1","1c76cfeb":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('\/content\/drive\/MyDrive\/dataset\/glove.6B.100d.txt', mode='rt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","1247e21c":"# we will now perform the encoding\nencoded_docs = token.texts_to_sequences(X_train.tolist())\n\n# embedding layer require all the encoded sequences to be of the same length, lets take max lenght as 100\n# and apply padding on the sequences which are of lower size\nmax_length = 100\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","74e23ff2":"# create a weight matrix for words in training docs\nmis_spelled = []\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in token.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        mis_spelled.append(word)","b9a393b1":"# lets check how many words are not spelled correctly \nlen(mis_spelled)","d233668a":"# define model\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[get_score])\n# summarize the model\nmodel.summary()\n# fit the model\nmodel.fit(padded_docs, y_train, epochs=50, verbose=0)","a674849c":"loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)","69dc59ee":"print(accuracy)","a15acffa":"encoded_docs = token.texts_to_sequences(X_test.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","b7a9e3ee":"# prediction on the test dataset\npredict_x=model.predict(padded_docs) \ny_pred=np.argmax(predict_x,axis=1)","d339f587":"encoded_docs = token.texts_to_sequences(df1.text_cleaned.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","75637937":"predict_y = model.predict(padded_docs)\ny_test_pred=np.argmax(predict_y,axis=1)","a1b485e7":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_2.csv',index=False)","46e88946":"max_length = max([len(s) for s in df.text_cleaned])\nprint('Maximum length: %d' % max_length)","94968585":"# we will now perform the encoding\nencoded_docs = token.texts_to_sequences(X_train.tolist())\n\n# embedding layer require all the encoded sequences to be of the same length, lets take max lenght as 100\n# and apply padding on the sequences which are of lower size\n\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","010bb904":"# define the model\ndef define_model(vocab_size, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, input_length=max_length))\n    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","34fbc611":"# define model\nmodel = define_model(vocab_size, max_length)\n# fit network\nmodel.fit(padded_docs, y_train, epochs=10, verbose=2)","a95ed61b":"loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)","6b24178a":"print(accuracy)\n","5d752800":"encoded_docs = token.texts_to_sequences(X_test.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n# prediction on the test dataset","47a39728":"preds = model.predict(padded_docs)\ny_pred=np.argmax(preds,axis=1)","e7440dc2":"\nencoded_docs = token.texts_to_sequences(df1.text_cleaned.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\ny_test_pred1 = model.predict(padded_docs)\ny_test_pred=np.argmax(y_test_pred1,axis=1)","0c5dc8e3":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_cnn.csv',index=False)","e76553c8":"GLOVE with KERAS Word Embeddings","0256284b":"creation of vocabulary","e6223c21":"CNN with word Embeddings"}}