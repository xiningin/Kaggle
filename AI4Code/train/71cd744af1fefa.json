{"cell_type":{"0b1e5c8d":"code","623fe3c7":"code","24042e86":"code","61c91923":"code","d1f5eeee":"code","c729882c":"code","3433531d":"code","7aed41ae":"code","320bc6ab":"code","0ea16f73":"code","192d37e2":"code","d2cf60b2":"code","297ec4bb":"code","fbb06d76":"code","5b532239":"code","e6d7032b":"code","75815684":"code","7491307e":"code","f7f2bb88":"code","303a3fa0":"code","18243b8a":"code","60d877ad":"code","cecd9413":"code","2270b901":"code","63b23283":"code","83a5c580":"code","a375b3cb":"code","f6b4b644":"code","3a682433":"code","3d6a1e88":"code","06e71828":"code","21342273":"code","7557d48e":"code","b83e4026":"code","fb6d3e6d":"code","10bd3ab5":"code","d7b639ef":"code","f886743b":"code","2829ce73":"code","aa5f98c2":"markdown","0e20b1af":"markdown","1b4c8e56":"markdown","227fad12":"markdown","ddff9371":"markdown","9950fc75":"markdown","80cb2c07":"markdown","a7e41834":"markdown"},"source":{"0b1e5c8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","623fe3c7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n# models\nfrom sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n","24042e86":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","61c91923":"#Thanks to:\n# https:\/\/www.kaggle.com\/mauricef\/titanic\n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - \\\n                                    df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.replace(np.nan, 0)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\n\n#Thanks to https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ndf['Title'] = df['Title'].replace('Ms','Miss')\ndf['Title'] = df['Title'].replace('Mlle','Miss')\ndf['Title'] = df['Title'].replace('Mme','Mrs')\n# Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n# Cabin, Deck\ndf['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf.loc[(df['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(med_fare)\n#Age\ndf['Age'] = df.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ndf['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ndf = df.drop(cols_to_drop, axis=1)\n\ndf.WomanOrBoySurvived = df.WomanOrBoySurvived.fillna(0)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.fillna(0)\ndf.FamilySurvivedCount = df.FamilySurvivedCount.fillna(0)\ndf.Alone = df.Alone.fillna(0)","d1f5eeee":"target = df.Survived.loc[traindf.index]\ndf = df.drop(['Survived'], axis=1)\ntrain, test = df.loc[traindf.index], df.loc[testdf.index]","c729882c":"train.head()\n","3433531d":"test.head()","7aed41ae":"target[:5]","320bc6ab":"# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = train.columns.values.tolist()\nfor col in features:\n    if train[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","0ea16f73":"train.info()","192d37e2":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) ","d2cf60b2":"train.info()","297ec4bb":"test.info()","fbb06d76":"#%% split training set to validation set\nSEED = 100\n# Xtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=SEED)","5b532239":"# Preparing datasets for only 3 features ('WomanOrBoySurvived', 'Sex', 'Alone')\ncols_to_drop3 = ['SibSp', 'Parch', 'Fare', 'LastName', 'Deck',\n               'Pclass', 'Age', 'Embarked', 'Title', 'IsWomanOrBoy',\n               'WomanOrBoyCount', 'FamilySurvivedCount', 'Family_Size']\ntrain = train.drop(cols_to_drop3, axis=1)\ntest = test.drop(cols_to_drop3, axis=1)\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.3, random_state=SEED)\ntrain.info()","e6d7032b":"# 1. Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train, target)\nY_pred = logreg.predict(test).astype(int)\nacc3_log = round(logreg.score(train, target) * 100, 2)\nsubmission_logreg3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_log\n#submission_logreg3.to_csv('output\/submission_logreg3.csv', index=False)","75815684":"# 2. Support Vector Machines\n\nsvc = SVC()\nsvc.fit(train, target)\nY_pred = svc.predict(test).astype(int)\nacc3_svc = round(svc.score(train, target) * 100, 2)\nsubmission_svm3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_svc\n#submission_svm3.to_csv('output\/submission_svm3.csv', index=False)","7491307e":"# 3. Linear SVC\n\nlinear_svc = LinearSVC(dual=False)\nlinear_svc.fit(train, target)\nY_pred = linear_svc.predict(test).astype(int)\nacc3_linear_svc = round(linear_svc.score(train, target) * 100, 2)\nsubmission_linear_svc3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_linear_svc\n#submission_linear_svc3.to_csv('output\/submission_linear_svc3.csv', index=False)","f7f2bb88":"# 4. k-Nearest Neighbors algorithm\n\nknn = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': [2, 3]}, cv=10).fit(train, target)\nY_pred = knn.predict(test).astype(int)\nacc3_knn = round(knn.score(train, target) * 100, 2)\nsubmission_knn3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_knn\n#submission_knn3.to_csv('output\/submission_knn3.csv', index=False)","303a3fa0":"# 5. Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(train, target)\nY_pred = gaussian.predict(test).astype(int)\nacc3_gaussian = round(gaussian.score(train, target) * 100, 2)\nsubmission_GaussianNB3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_gaussian\n#submission_GaussianNB3.to_csv('output\/submission_GaussianNB3.csv', index=False)","18243b8a":"# 6. Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(train, target)\nY_pred = perceptron.predict(test).astype(int)\nacc3_perceptron = round(perceptron.score(train, target) * 100, 2)\nsubmission_perceptron3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_perceptron\n#submission_perceptron3.to_csv('output\/submission_perceptron3.csv', index=False)","60d877ad":"# 7. Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(train, target)\nY_pred = sgd.predict(test).astype(int)\nacc3_sgd = round(sgd.score(train, target) * 100, 2)\nsubmission_sgd3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_sgd\n#submission_sgd3.to_csv('output\/submission_sgd3.csv', index=False)","cecd9413":"# 8. Decision Tree Classifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train, target)\nY_pred = decision_tree.predict(test).astype(int)\nacc3_decision_tree = round(decision_tree.score(train, target) * 100, 2)\nsubmission_decision_tree3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_decision_tree\n#submission_decision_tree3.to_csv('output\/submission_decision_tree3.csv', index=False)","2270b901":"# 9. Random Forest\n\nrandom_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 300]}, cv=5).fit(train, target)\nrandom_forest.fit(train, target)\nY_pred = random_forest.predict(test).astype(int)\nrandom_forest.score(train, target)\nacc3_random_forest = round(random_forest.score(train, target) * 100, 2)\nsubmission_random_forest3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_random_forest\n#submission_random_forest3.to_csv('output\/submission_random_forest3.csv', index=False)","63b23283":"# 10. XGB_Classifier\n\ndef hyperopt_xgb_score(params):\n    clf = XGBClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_xgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'eta': hp.quniform('eta', 0.025, 0.5, 0.005),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'subsample': hp.quniform('subsample', 0.5, 1, 0.005),\n            'gamma': hp.quniform('gamma', 0.5, 1, 0.005),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'eval_metric': 'auc',\n            'objective': 'binary:logistic',\n            'booster': 'gbtree',\n            'tree_method': 'exact',\n            'silent': 1,\n            'missing': None\n        }\n \nbest = fmin(fn=hyperopt_xgb_score, space=space_xgb, algo=tpe.suggest, max_evals=10)\nparams = space_eval(space_xgb, best)\nXGB_Classifier = XGBClassifier(**params)\nXGB_Classifier.fit(train, target)\nY_pred = XGB_Classifier.predict(test).astype(int)\nXGB_Classifier.score(train, target)\nacc3_XGB_Classifier = round(XGB_Classifier.score(train, target) * 100, 2)\nsubmission_XGB_Classifier3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_XGB_Classifier\n#submission_XGB_Classifier3.to_csv('output\/submission_XGB_Classifier3.csv', index=False)\n","83a5c580":"# 11. LGBM_Classifier\n\ndef hyperopt_lgb_score(params):\n    clf = LGBMClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=10).mean()\n    print(current_score, params)\n    return current_score \n \nspace_lgb = {\n            'learning_rate': hp.quniform('learning_rate', 0, 0.05, 0.0001),\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 12, dtype=int)),\n            'num_leaves': hp.choice('num_leaves', 2*np.arange(2, 2**11, dtype=int)),\n            'min_child_weight': hp.quniform('min_child_weight', 1, 9, 0.025),\n            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.005),\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            }\n \nbest = fmin(fn=hyperopt_lgb_score, space=space_lgb, algo=tpe.suggest, max_evals=10)\nparams = space_eval(space_lgb, best)\nLGB_Classifier = LGBMClassifier(**params)\nLGB_Classifier.fit(train, target)\nY_pred = LGB_Classifier.predict(test).astype(int)\nLGB_Classifier.score(train, target)\nacc3_LGB_Classifier = round(LGB_Classifier.score(train, target) * 100, 2)\nsubmission_LGB_Classifier3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_LGB_Classifier\n#submission_LGB_Classifier3.to_csv('output\/submission_LGB_Classifier3.csv', index=False)\n","a375b3cb":"# 12. GradientBoostingClassifier\n\ndef hyperopt_gb_score(params):\n    clf = GradientBoostingClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=5).mean()\n    print(current_score, params)\n    return current_score \n \nspace_gb = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_depth': hp.choice('max_depth', np.arange(2, 10, dtype=int)),\n            'max_features': None\n        }\n \nbest = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=5)\nparams = space_eval(space_gb, best)\ngradient_boosting = GradientBoostingClassifier(**params)\ngradient_boosting.fit(train, target)\nY_pred = gradient_boosting.predict(test).astype(int)\ngradient_boosting.score(train, target)\nacc3_gradient_boosting = round(gradient_boosting.score(train, target) * 100, 2)\nsubmission_gradient_boosting3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_gradient_boosting\n#submission_gradient_boosting3.to_csv('output\/submission_gradient_boosting3.csv', index=False)\n","f6b4b644":"# 13. Ridge Classifier\n\nridge_classifier = RidgeClassifier()\nridge_classifier.fit(train, target)\nY_pred = ridge_classifier.predict(test).astype(int)\nridge_classifier.score(train, target)\nacc3_ridge_classifier = round(ridge_classifier.score(train, target) * 100, 2)\nsubmission_ridge_classifier3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_ridge_classifier\n#submission_ridge_classifier3.to_csv('output\/submission_ridge_classifier3.csv', index=False)","3a682433":"# 14. Bagging Classifier\n\nbagging_classifier = BaggingClassifier()\nbagging_classifier.fit(train, target)\nY_pred = bagging_classifier.predict(test).astype(int)\nbagging_classifier.score(train, target)\nacc3_bagging_classifier = round(bagging_classifier.score(train, target) * 100, 2)\nsubmission_bagging_classifier3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_bagging_classifier\n#submission.to_csv('output\/submission_bagging_classifier3.csv', index=False)","3d6a1e88":"# 15. Extra Trees Classifier\n\ndef hyperopt_etc_score(params):\n    clf = ExtraTreesClassifier(**params)\n    current_score = cross_val_score(clf, train, target, cv=5).mean()\n    print(current_score, params)\n    return current_score \n \nspace_etc = {\n            'n_estimators': hp.choice('n_estimators', range(100, 1000)),\n            'max_features': hp.choice('max_features', np.arange(2, 17, dtype=int)),\n            'min_samples_leaf': hp.choice('min_samples_leaf', np.arange(1, 5, dtype=int)),\n            'max_depth':  hp.choice('max_depth', np.arange(2, 10, dtype=int)),\n            'max_features': None\n        }\n \nbest = fmin(fn=hyperopt_etc_score, space=space_etc, algo=tpe.suggest, max_evals=5)\nparams = space_eval(space_etc, best)\nextra_trees_classifier = ExtraTreesClassifier(**params)\nextra_trees_classifier.fit(train, target)\nY_pred = extra_trees_classifier.predict(test).astype(int)\nextra_trees_classifier.score(train, target)\nacc3_etc = round(extra_trees_classifier.score(train, target) * 100, 2)\nsubmission_etc3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_etc\n#submission_etc3.to_csv('output\/submission_etc3.csv', index=False)\n","06e71828":"# 16. Neural Network 1 \n\ndef build_ann(optimizer='adam'):\n    \n    # Initializing the ANN\n    ann = Sequential()\n    \n    # Adding the input layer and the first hidden layer of the ANN with dropout\n    ann.add(Dense(units=32, kernel_initializer='glorot_uniform', activation='relu', input_shape=(3,)))\n    \n    # Add other layers, it is not necessary to pass the shape because there is a layer before\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    ann.add(Dense(units=64, kernel_initializer='glorot_uniform', activation='relu'))\n    ann.add(Dropout(rate=0.5))\n    \n    # Adding the output layer\n    ann.add(Dense(units=1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n    \n    # Compiling the ANN\n    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return ann\nopt = optimizers.Adam(lr=0.001)\nann = build_ann(opt)\nhistory = ann.fit(Xtrain, Ztrain, batch_size=16, epochs=100, validation_data=(Xval, Zval))\nY_pred = ann.predict(test)\nY_pred = (Y_pred > 0.5)*1 # convert probabilities to binary output\nann_prediction = ann.predict(train)\nann_prediction = (ann_prediction > 0.5)*1 # convert probabilities to binary output\nacc3_ann1 = round(metrics.accuracy_score(target, ann_prediction) * 100, 2)\nsubmission_ann1_3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": np.reshape(Y_pred, len(Y_pred))})\nacc3_ann1\n#submission_ann1_3.to_csv('output\/submission_ann1_3.csv', index=False)\n","21342273":"# 17. Neural Network 2\n\n# Model\nmodel = Sequential()\n# model.add(Dense(16, input_dim = train.shape[1], init = 'he_normal', activation = 'relu'))\nmodel.add(Dense(16, input_dim = train.shape[1], activation = 'relu'))\nmodel.add(Dropout(0.3))\n# model.add(Dense(64, init = 'he_normal', activation = 'relu'))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dropout(0.3))\n# model.add(Dense(32, init = 'he_normal', activation = 'relu'))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nes = EarlyStopping(monitor='val_accuracy', patience=20, mode='max')\nhist = model.fit(train, target, batch_size=64, validation_data=(Xval, Zval),epochs=500, verbose=1, callbacks=[es])\nY_pred = model.predict(test)\nY_pred = (Y_pred > 0.5)*1 # convert probabilities to binary output\nnn_prediction = model.predict(train)\nnn_prediction = (nn_prediction > 0.5)*1 # convert probabilities to binary output\nacc3_ann2 = round(metrics.accuracy_score(target, nn_prediction) * 100, 2)\nsubmission_ann2_3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": np.reshape(Y_pred, len(Y_pred))})\nacc3_ann2\n#submission_ann2_3.to_csv('output\/submission_ann2_3.csv', index=False)\n","7557d48e":"# 18 VotingClassifier (hard voting)\n\nVoting_Classifier_hard = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('gbc', gradient_boosting)], voting='hard')\nfor clf, label in zip([logreg, random_forest, gradient_boosting, Voting_Classifier_hard], \n                      ['Logistic Regression', 'Random Forest', 'Gradient Boosting Classifier', 'Ensemble']):\n    scores = cross_val_score(clf, train, target, cv=10, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\nVoting_Classifier_hard.fit(train, target)\nY_pred = Voting_Classifier_hard.predict(test).astype(int)\nVoting_Classifier_hard.score(train, target)\nacc3_VC_hard = round(Voting_Classifier_hard.score(train, target) * 100, 2)\nsubmission_VC_hard3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_VC_hard\n#submission_VC_hard3.to_csv('output\/submission_VC_hard3.csv', index=False)\n","b83e4026":"# 19 VotingClassifier (soft voting)\n\neclf = VotingClassifier(estimators=[('lr', logreg), ('rf', random_forest), ('gbc', gradient_boosting)], voting='soft')\nparams = {'lr__C': [1.0, 100.0], 'gbc__learning_rate': [0.05, 1]}\nVoting_Classifier_soft = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\nVoting_Classifier_soft.fit(train, target)\nY_pred = Voting_Classifier_soft.predict(test).astype(int)\nVoting_Classifier_soft.score(train, target)\nacc3_VC_soft = round(Voting_Classifier_soft.score(train, target) * 100, 2)\nsubmission_VC_soft3 = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\nacc3_VC_soft\n#submission_VC_soft3.to_csv('output\/submission_VC_soft3.csv', index=False)","fb6d3e6d":"submission_VC_soft3","10bd3ab5":"# 5.20 The simple rule in one line\nsimple_rule_model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=1118, splitter='best') \nsimple_rule_model.fit(train, target)\nY_pred = simple_rule_model.predict(test).astype(int)\n# simple_rule_model.score(train, target)\n# acc_simple_rule = round(simple_rule_model.score(train, target) * 100, 2)\n# print(acc_simple_rule)\n# Y_pred = (((test.WomanOrBoySurvived <= 0.238) & (test.Sex > 0.5) & (test.Alone > 0.5)) | \\\n#           ((test.WomanOrBoySurvived > 0.238) & \\\n#            ~((test.WomanOrBoySurvived > 0.55) & (test.WomanOrBoySurvived <= 0.633))))\nsimple_rule_model.score(train, target)\nacc3_simple_rule = round(simple_rule_model.score(train, target) * 100, 2)\nsubmission_SRIOL = pd.DataFrame({\"PassengerId\": submission[\"PassengerId\"],\"Survived\": Y_pred})\n#submission_SRIOL.to_csv('output\/submission_SRIOL.csv', index=False)\nprint(acc3_simple_rule)\n","d7b639ef":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', 'k-Nearest Neighbors', 'Naive Bayes', \n              'Perceptron', 'Stochastic Gradient Decent', \n              'Decision Tree Classifier', 'Random Forest',  'XGBClassifier', 'LGBMClassifier',\n              'GradientBoostingClassifier', 'RidgeClassifier', 'BaggingClassifier', 'ExtraTreesClassifier', \n              'Neural Network 1', 'Neural Network 2', \n              'VotingClassifier-hard voiting', 'VotingClassifier-soft voting',\n              'Simple rule'],\n    'Score': [acc3_log, acc3_svc, acc3_linear_svc, acc3_knn, acc3_gaussian, \n              acc3_perceptron, acc3_sgd, \n              acc3_decision_tree, acc3_random_forest, acc3_XGB_Classifier, acc3_LGB_Classifier,\n              acc3_gradient_boosting, acc3_ridge_classifier, acc3_bagging_classifier, acc3_etc, \n              acc3_ann1, acc3_ann2, \n              acc3_VC_hard, acc3_VC_soft,\n              acc3_simple_rule],\n\n})\n\nmodels.sort_values(by=['Score'], ascending=False)","f886743b":"submission_SRIOL","2829ce73":"submission_SRIOL.to_csv('\/kaggle\/working\/submission_SRIOL.csv', index=False)\nprint(\"submission file generated\")","aa5f98c2":"> # Import libraries","0e20b1af":"> #   Import datasets","1b4c8e56":"> > # Encoding categorical features","227fad12":"> > # Creation of training and validation sets","ddff9371":"> # Preparing to modeling","9950fc75":"> # Tuning models and test for 3 features","80cb2c07":"> # Features engineering (FE)","a7e41834":"> # Models evaluation"}}