{"cell_type":{"a30fdac4":"code","9e16f712":"code","e1bbbeb9":"code","be72456f":"code","48452417":"code","fc2b4a9e":"code","d332ca43":"code","db58021d":"code","54eb5958":"code","da8fc512":"code","a1f86cd3":"code","fd1bc7a3":"code","fb0b6116":"code","3a77b5be":"code","da7add0a":"code","31b94f24":"code","13f3bd69":"code","62d76101":"code","e4d3c862":"code","d8d08e22":"code","5ba56469":"code","64e272da":"code","ce41de02":"code","9109fbe0":"code","deb66e38":"code","99adda3d":"code","dbdf23ee":"code","89720e35":"code","2ee7c52f":"code","c48a9495":"code","9bfd0d94":"code","a8703868":"code","846f506d":"code","0fc5e8cb":"code","bf2d9b36":"markdown","f8e14f48":"markdown","1f2fdaee":"markdown","3a0b356e":"markdown","be6f6953":"markdown","183e4d73":"markdown","8726086f":"markdown","5e703e44":"markdown","662c9551":"markdown","edd6c5db":"markdown","6a37e71a":"markdown","aab744e7":"markdown","ef22d907":"markdown","ced954d2":"markdown"},"source":{"a30fdac4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\nfrom tqdm import tqdm\nimport datetime\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.callbacks import (\n    History, EarlyStopping, ModelCheckpoint,\n    LearningRateScheduler, TensorBoard\n)\nfrom tensorflow.keras.metrics import categorical_accuracy\n\n\nimport os","9e16f712":"now = datetime.datetime.now(datetime.timezone.utc)\nstart_time = now.strftime(\"%Y%m%d_%H%M%S\")\nbest_save_model_file = f'model_{start_time}.h5'\ntf_log_dir = f'log\/run-{start_time}\/'\n\nprint(f'script start time (uses file name of model weight) : {start_time}')\nprint(f'tensorflow version : {tf.__version__}')\n\n# from tensorflow.python.client import device_lib\n# print(f'tensorflow use devices(bellow) : {device_lib.list_local_devices()}')","e1bbbeb9":"INPUT_BASE_DIR = '\/kaggle\/input\/10-monkey-species\/training\/training\/'\nTEST_BASE_DIR = '\/kaggle\/input\/10-monkey-species\/validation\/validation'\n\n# training configurations\nLABELS = 10\n# EPOCHS = 5 # for cpu, if you use gpu, set 10~100 but large values consume GPU time.\nEPOCHS = 10 # for gpu, if you use cpu, set 5~10\nSEED = 0\nVALIDATION_SPLIT = 0.3\nBATCH_SIZE = 32\nLEARNING_RATE = 0.001\n\n# URL for tensorflow hub and image size of recommended.\n# you can find a lot of pre-trained models here. https:\/\/tfhub.dev\/s?module-type=image-classification&tf-version=tf2\nIMG_SHAPE = (600, 600, 3)\nFEATURE_EXTRACTOR_URL = 'https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b7\/feature-vector\/1'","be72456f":"np.random.seed(SEED)\ntf.random.set_seed(SEED)","48452417":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# be careful! image size doesn't need color chanel.\ntrain_gen = ImageDataGenerator(\n    rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    rotation_range=90,\n    featurewise_center=True,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    validation_split=VALIDATION_SPLIT\n).flow_from_directory(\n    INPUT_BASE_DIR,\n    target_size=IMG_SHAPE[:2],\n    batch_size=BATCH_SIZE,\n    subset='training',\n    seed=SEED\n)\n\nvalidate_gen = ImageDataGenerator(\n    rescale=1.\/255,\n    validation_split=VALIDATION_SPLIT\n).flow_from_directory(\n    INPUT_BASE_DIR,\n    target_size=IMG_SHAPE[:2],\n    batch_size=BATCH_SIZE,\n    subset='validation',\n    seed=SEED\n)","fc2b4a9e":"feature_extractor_layer = hub.KerasLayer(\n    FEATURE_EXTRACTOR_URL, input_shape=IMG_SHAPE)\nfeature_extractor_layer.trainable = False","d332ca43":"inputs = tf.keras.layers.Input(shape=IMG_SHAPE)\nx = feature_extractor_layer(inputs)\nx = tf.keras.layers.Dense(1024, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(512, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(256, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = tf.keras.layers.Dense(LABELS, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.summary()","db58021d":"from tensorflow.keras.callbacks import (\n    History, EarlyStopping,\n    ModelCheckpoint, TensorBoard\n)\n\nhistory = History()\ncallbacks = [EarlyStopping(monitor='val_loss',\n                           patience=20,\n                           verbose=1,\n                           min_delta=0.00001,\n                           mode='min'),\n             ModelCheckpoint(monitor='val_loss',save_weights_only=False,\n                             filepath=best_save_model_file,\n                             save_best_only=True,\n                             mode='min'),\n#              TensorBoard(log_dir=tf_log_dir, \n#                          histogram_freq=1, \n#                          write_graph=False, \n#                          write_grads=True),\n             history,\n             ]","54eb5958":"from tensorflow.keras.metrics import categorical_accuracy, categorical_crossentropy\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='categorical_crossentropy',\n    metrics=[categorical_accuracy])","da8fc512":"train_steps_per_epoch = np.floor(train_gen.samples \/ train_gen.batch_size)\nvalidate_steps_per_epoch = np.ceil(validate_gen.samples \/ validate_gen.batch_size)\n\nhistory = model.fit(train_gen, epochs=EPOCHS,\n                    validation_data=validate_gen, \n                    steps_per_epoch=train_steps_per_epoch,\n                    validation_steps=validate_steps_per_epoch,\n                    callbacks=callbacks)","a1f86cd3":"def plot_train_history(history, metrics_name):\n    fig, ax = plt.subplots()\n    ax.set_xlabel('Epoch')\n    ax.set_ylabel(f'{metrics_name}\/val_{metrics_name}')\n    ax.set_title(metrics_name)\n    ax.plot(history.history[metrics_name], label=metrics_name)\n    ax.plot(history.history[f'val_{metrics_name}'], label=f'val_{metrics_name}')\n    ax.legend(loc=0)\n    plt.show()","fd1bc7a3":"# fig, ax = plt.subplots()\n# ax.set_xlabel('Epoch')\n# ax.set_ylabel('acc\/val_acc')\n# ax.set_title('acc')\n# ax.plot(history.history['categorical_accuracy'], label='categorical_accuracy')\n# ax.plot(history.history['val_categorical_accuracy'], label='val_categorical_accuracy')\n# ax.legend(loc=0)\n# plt.show()\nplot_train_history(history, 'categorical_accuracy')","fb0b6116":"# fig, ax = plt.subplots()\n# ax.set_xlabel('Epoch')\n# ax.set_ylabel('loss\/val_loss')\n# ax.set_title('loss')\n# ax.plot(history.history['loss'], label='loss')\n# ax.plot(history.history['val_loss'], label='val_loss')\n# ax.legend(loc=0)\n# plt.show()\nplot_train_history(history, 'loss')","3a77b5be":"class_names = sorted(train_gen.class_indices.items(), key=lambda pair:pair[1])\nclass_names = np.array([key.title() for key, value in class_names])","da7add0a":"def calc_tiles(batch_size):\n    for i in range(batch_size):\n        if i**2 > batch_size:\n            return i\n    return batch_size\n\ntiles = calc_tiles(BATCH_SIZE)","31b94f24":"model.load_weights(best_save_model_file)","13f3bd69":"import math\ndef plot_predict_images(data_gen, model, class_names, show_images_at_once=BATCH_SIZE):\n    for i in range(math.ceil(data_gen.samples \/ data_gen.batch_size)):\n        image_batch, label_batch = data_gen.next()\n        predicted_batch = model.predict(image_batch)\n        predicted_id = np.argmax(predicted_batch, axis=-1)\n        predicted_label_batch = class_names[predicted_id]\n\n        label_id = np.argmax(label_batch, axis=-1)\n\n        tiles = calc_tiles(show_images_at_once)\n        plt.figure(figsize=(10,9))\n        plt.subplots_adjust(hspace=0.5)\n        for n in range(image_batch.shape[0]):\n            plt.subplot(tiles,tiles,n+1)\n            plt.imshow(image_batch[n])\n            color = \"green\" if predicted_id[n] == label_id[n] else \"red\"\n            plt.title(f'{predicted_label_batch[n]}\/{class_names[label_id[n]]}', color=color)\n            plt.axis('off')\n        _ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")\n        if image_batch.shape[0] != show_images_at_once:\n            break\n        if n > 5:\n            break","62d76101":"# for i in range(math.ceil(validate_gen.samples \/ validate_gen.batch_size)):\n#     val_image_batch, val_label_batch = validate_gen.next()\n#     predicted_batch = model.predict(val_image_batch)\n#     predicted_id = np.argmax(predicted_batch, axis=-1)\n#     predicted_label_batch = class_names[predicted_id]\n    \n#     label_id = np.argmax(val_label_batch, axis=-1)\n    \n#     plt.figure(figsize=(10,9))\n#     plt.subplots_adjust(hspace=0.5)\n#     for n in range(val_image_batch.shape[0]):\n#         plt.subplot(tiles,tiles,n+1)\n#         plt.imshow(val_image_batch[n])\n#         color = \"green\" if predicted_id[n] == label_id[n] else \"red\"\n#         plt.title(f'{predicted_label_batch[n]}\/{class_names[label_id[n]]}', color=color)\n#         plt.axis('off')\n#     _ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")\n#     if val_image_batch.shape[0] != BATCH_SIZE:\n#         break\n#     if n > 5:\n#         break\nplot_predict_images(validate_gen, model, class_names)","e4d3c862":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef make_test_predict_df(test_base_dir, model, img_shape):\n    img_paths = []\n    pred_labels = []\n    actual_labels = []\n    for label_dir in os.listdir(test_base_dir):\n        test_dir = os.path.join(test_base_dir, label_dir)\n        for img_name in tqdm(os.listdir(test_dir)):\n            img = img_to_array(load_img(os.path.join(test_dir, img_name)).resize(img_shape[:2])) \/ 255\n            pred = model.predict(np.asarray([img]))\n            predicted_id = np.argmax(pred, axis=-1)\n            predicted_label = class_names[predicted_id][0]\n\n            img_paths.append(os.path.join(test_dir, img_name))\n            pred_labels.append(predicted_label)\n            actual_labels.append(label_dir.upper())\n    df = pd.DataFrame(img_paths, columns=['img_path'])\n    df['pred_label'] = pred_labels\n    df['actual_label'] = actual_labels\n    return df","d8d08e22":"# img_paths = []\n# pred_labels = []\n# actual_labels = []\n# for label_dir in os.listdir(TEST_BASE_DIR):\n#     test_dir = os.path.join(TEST_BASE_DIR, label_dir)\n#     for img_name in tqdm(os.listdir(test_dir)):\n#         img = img_to_array(load_img(os.path.join(test_dir, img_name)).resize(IMG_SHAPE[:2])) \/ 255\n#         pred = model.predict(np.asarray([img]))\n#         predicted_id = np.argmax(pred, axis=-1)\n#         predicted_label = class_names[predicted_id][0]\n\n#         img_paths.append(os.path.join(test_dir, img_name))\n#         pred_labels.append(predicted_label)\n#         actual_labels.append(label_dir.upper())\ndf = make_test_predict_df(TEST_BASE_DIR, model, IMG_SHAPE)","5ba56469":"# df = pd.DataFrame(img_paths, columns=['img_path'])\n# df['pred_label'] = pred_labels\n# df['actual_label'] = actual_labels","64e272da":"miss_label_df = df[df['pred_label'] != df['actual_label']]\ndisplay(miss_label_df)\nprint(f'miss label \/ all: {miss_label_df.shape[0]}\/{df.shape[0]}')","ce41de02":"plt.figure(figsize=(24,24))\nplt.subplots_adjust(hspace=0.5)\nsampling_size = BATCH_SIZE - miss_label_df.shape[0]\nsampling_with_miss_label_df = pd.concat([miss_label_df, df.sample(sampling_size)])\nfor n in range(sampling_with_miss_label_df.shape[0]):\n    tmp_pred_label = sampling_with_miss_label_df['pred_label'].values[n]\n    tmp_actual_label = sampling_with_miss_label_df['actual_label'].values[n]\n    tmp_img_path = sampling_with_miss_label_df['img_path'].values[n]\n    tmp_img = img_to_array(load_img(tmp_img_path).resize(IMG_SHAPE[:2])) \/ 255\n#     tmp_img = load_img(os.path.join(test_dir, img_name)).resize(IMG_SHAPE[:2])\n\n    plt.subplot(tiles,tiles,n+1)\n    plt.imshow(np.asarray([tmp_img]).reshape(IMG_SHAPE))\n#     plt.imshow(tmp_img)\n\n    color = \"green\" if tmp_pred_label == tmp_actual_label else \"red\"\n    plt.title(f'{tmp_pred_label}\/{tmp_actual_label}', color=color)\n    plt.axis('off')\n_ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")","9109fbe0":"# reset tensorflow session to make model clean environment.\n# see https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/backend\/clear_session\ntf.keras.backend.clear_session()\n# change save model name\nbest_save_model_file = f'scratch_model_{start_time}.h5'","deb66e38":"scratch_img_shape = (128, 128, 3)\nscratch_batch_size = BATCH_SIZE \/\/ 4","99adda3d":"block_count = 3\nconv_layers_in_block = 3\ninputs = tf.keras.layers.Input(shape=scratch_img_shape)\nx = inputs\nfor block_i in range(block_count):\n    for layer_i in range(conv_layers_in_block):\n        x = tf.keras.layers.Conv2D(IMG_SHAPE[0]*(2**block_i), 3, padding=\"same\")(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(\"relu\")(x)\n    if block_i != (block_count - 1):\n        x = tf.keras.layers.MaxPooling2D((2, 2))(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(32)(x)\n\noutputs = tf.keras.layers.Dense(LABELS, activation='softmax')(x)\n\nscratch_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nscratch_model.summary()","dbdf23ee":"train_gen = ImageDataGenerator(\n    rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    rotation_range=90,\n    featurewise_center=True,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    validation_split=VALIDATION_SPLIT\n).flow_from_directory(\n    INPUT_BASE_DIR,\n    target_size=scratch_img_shape[:2],\n    batch_size=scratch_batch_size,\n    subset='training',\n    seed=SEED\n)\n\nvalidate_gen = ImageDataGenerator(\n    rescale=1.\/255,\n    validation_split=VALIDATION_SPLIT\n).flow_from_directory(\n    INPUT_BASE_DIR,\n    target_size=scratch_img_shape[:2],\n    batch_size=scratch_batch_size,\n    subset='validation',\n    seed=SEED\n)","89720e35":"history = History()\ncallbacks = [EarlyStopping(monitor='val_loss',\n                           patience=20,\n                           verbose=1,\n                           min_delta=0.00001,\n                           mode='min'),\n             ModelCheckpoint(monitor='val_loss',save_weights_only=False,\n                             filepath=best_save_model_file,\n                             save_best_only=True,\n                             mode='min'),\n#              TensorBoard(log_dir=tf_log_dir, \n#                          histogram_freq=1, \n#                          write_graph=False, \n#                          write_grads=True),\n             history,\n             ]\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n\nscratch_model.compile(\n    optimizer=optimizer,\n    loss='categorical_crossentropy',\n    metrics=[categorical_accuracy])","2ee7c52f":"train_steps_per_epoch = np.floor(train_gen.samples \/ train_gen.batch_size)\nvalidate_steps_per_epoch = np.ceil(validate_gen.samples \/ validate_gen.batch_size)\n\nhistory = scratch_model.fit(train_gen, epochs=EPOCHS,\n                            validation_data=validate_gen, \n                            steps_per_epoch=train_steps_per_epoch,\n                            validation_steps=validate_steps_per_epoch,\n                            callbacks=callbacks)","c48a9495":"plot_train_history(history, 'categorical_accuracy')","9bfd0d94":"plot_train_history(history, 'loss')","a8703868":"for _ in range(4):\n    plot_predict_images(validate_gen, scratch_model, class_names)","846f506d":"df = make_test_predict_df(TEST_BASE_DIR, scratch_model, scratch_img_shape)","0fc5e8cb":"miss_label_df = df[df['pred_label'] != df['actual_label']]\ndisplay(miss_label_df)\n# no good\nprint(f'miss label \/ all: {miss_label_df.shape[0]}\/{df.shape[0]}')","bf2d9b36":"## Note\nUnlike the previous model where the trained model was used as a feature extractor, this time we need to train the feature extraction part as well. Therefore, the number of parameters to be trained increases explosively, so let's make the input image of the model much smaller. The batch size should also be kept small for the same reason.\n\n\u5148\u7a0b\u306e\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u7279\u5fb4\u91cf\u62bd\u51fa\u5668\u3068\u3057\u3066\u5229\u7528\u3057\u305f\u30e2\u30c7\u30eb\u3068\u306f\u7570\u306a\u308a\u3001\u4eca\u56de\u306f\u7279\u5fb4\u91cf\u62bd\u51fa\u90e8\u5206\u3082\u5b66\u7fd2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u8a13\u7df4\u3059\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u6570\u304c\u7206\u767a\u7684\u306b\u5897\u3048\u3066\u3057\u307e\u3046\u305f\u3081\u30e2\u30c7\u30eb\u306e\u5165\u529b\u753b\u50cf\u3092\u5927\u5e45\u306b\u5c0f\u3055\u304f\u3057\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3082\u540c\u69d8\u306e\u7406\u7531\u306b\u5c0f\u3055\u304f\u3057\u3066\u304a\u304d\u307e\u3059\u3002","f8e14f48":"# Predict test images","1f2fdaee":"# Predict\nNow that the training is over, let's let the model make inferences. We already know that it performed very well in the evaluation data since we displayed the training history, but let's pick up some images and plot the correct and incorrect answers.\nNote that the evaluation dataset will be used in the assessment at the time of training, so we originally needed to split the data into three parts: training\/validation\/test.\n\n\u8a13\u7df4\u304c\u7d42\u308f\u3063\u305f\u306e\u3067\u30e2\u30c7\u30eb\u306b\u63a8\u8ad6\u3055\u305b\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u8a13\u7df4\u306e\u5c65\u6b74\u3092\u8868\u793a\u3057\u305f\u306e\u3067\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u3067\u306f\u975e\u5e38\u306b\u826f\u3044\u6210\u7e3e\u3092\u51fa\u3057\u3066\u3044\u305f\u3053\u3068\u306f\u65e2\u306b\u308f\u304b\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u3044\u304f\u3064\u304b\u753b\u50cf\u3092\u30d4\u30c3\u30af\u30a2\u30c3\u30d7\u3057\u3066\u6b63\u89e3\u4e0d\u6b63\u89e3\u3092\u30d7\u30ed\u30c3\u30c8\u3057\u3066\u307f\u307e\u3059\u3002\n\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306f\u5b66\u7fd2\u6642\u306e\u8a55\u4fa1\u3067\u4f7f\u308f\u308c\u308b\u306e\u3067\u3001\u672c\u6765\u306f\u5b66\u7fd2\/\u8a55\u4fa1\/\u30c6\u30b9\u30c8\u306e3\u3064\u306b\u30c7\u30fc\u30bf\u3092\u5206\u5272\u3057\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308b\u3053\u3068\u306f\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002","3a0b356e":"This model can probably be trained to a reasonable level of accuracy if we spend a lot of time on it, but we don't want to waste GPU time, so we will stop after a little training. If you want to get a pre-trained model and train it on your own (or kaggle) dataset, try searching for transfer learning or finetuning. Here is a reference site.  \nhttps:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning\n\n\u6050\u3089\u304f\u3053\u306e\u30e2\u30c7\u30eb\u3082\u6642\u9593\u3092\u304b\u3051\u3066\u8a13\u7df4\u3059\u308c\u3070\u305d\u308c\u306a\u308a\u306e\u7cbe\u5ea6\u306f\u51fa\u305b\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u3001GPU\u6642\u9593\u3092\u6d6a\u8cbb\u3057\u305f\u304f\u306a\u3044\u306e\u3067\u5c11\u3057\u5b66\u7fd2\u3055\u305b\u305f\u7a0b\u5ea6\u3067\u7d42\u4e86\u3055\u305b\u307e\u3059\u3002\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u53d6\u5f97\u3057\u3066\u304d\u3066\u81ea\u5206\u306e(kaggle\u306e)\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3044\u5834\u5408\u306ftransfer learning \u3084 finetuning \u3067\u691c\u7d22\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002\u53c2\u8003\u30b5\u30a4\u30c8\u306f\u3053\u3061\u3089\u3067\u3059\u3002","be6f6953":"# Plot train history","183e4d73":"\u52c9\u5f37\u4f1a\u7528\u306b\u30b5\u30af\u30c3\u3068\u753b\u50cf\u5206\u985e\u30e2\u30c7\u30eb\u3092\u66f8\u304f\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\u30b9\u30af\u30e9\u30c3\u30c1\u3067\u753b\u50cf\u5206\u985e\u30e2\u30c7\u30eb\u3092\u4f5c\u308b\u306e\u306f\u5927\u5909\u306a\u306e\u3067\u3001\u65e2\u306b\u8a13\u7df4\u6e08\u307f\u306e\u30e2\u30c7\u30eb\u3092\u5229\u7528\u3059\u308b\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002  \nkaggle\u306b\u65e5\u672c\u8a9e\u3060\u3051\u306e\u30ce\u30fc\u30c8\u3092\u4e0a\u3052\u308b\u3053\u3068\u306b\u62b5\u6297\u304c\u5f37\u3044\u306e\u3067\u6a5f\u68b0\u7ffb\u8a33\u306e\u6587\u7ae0\u3082\u968f\u6642\u631f\u3093\u3067\u3044\u307e\u3059\u3002\u82f1\u8a9e\u5f97\u610f\u306a\u4eba\u306f\u5909\u306a\u6587\u7ae0\u3042\u3063\u305f\u3089\u6307\u6458\u3057\u3066\u307b\u3057\u3044\u3067\u3059\u3002","8726086f":"# Build scratch model\n\nImage classification generally uses a Neural Network that uses convolution, called a CNN. Convolution makes it possible to capture the features of an image efficiently. The purpose of this kernel is to make a simple image classification model, so please learn more about it in a professional course or book. Just as the features of a monkey's right eye do not change even if the monkey's right eye moves a few pixels laterally, the visual world is basically invariant to movement, so learning partial features is more efficient than learning features of the entire image.\n\nThere are many networks such as VGG, Resnet, EfficientNet, etc. There are many networks such as VGG, Resnet, EfficientNet, etc., which you can refer to. Each DeepLearning framework provides APIs for building major networks, so you can build them in a few lines and check the network structure.\n\nHowever, unless your goal is to learn, it is recommended to use trained models or fine tuning as mentioned above.\n\n\u753b\u50cf\u5206\u985e\u306f\u4e00\u822c\u7684\u306bCNN\u3068\u547c\u3070\u308c\u308b\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528\u3057\u305fNeural Network\u3092\u5229\u7528\u3057\u307e\u3059\u3002\u753b\u50cf\u5206\u985e\u306f\u4e00\u822c\u7684\u306bCNN\u3068\u547c\u3070\u308c\u308b\u7573\u307f\u8fbc\u307f\u3092\u5229\u7528\u3057\u305fNeural Network\u3092\u5229\u7528\u3057\u307e\u3059\u3002\u7573\u307f\u8fbc\u307f\u3092\u884c\u3046\u3053\u3068\u3067\u753b\u50cf\u306e\u7279\u5fb4\u3092\u52b9\u7387\u3088\u304f\u6349\u3048\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306ekernel\u306e\u76ee\u7684\u306f\u753b\u50cf\u5206\u985e\u30e2\u30c7\u30eb\u3092\u7c21\u5358\u306b\u4f5c\u3063\u3066\u307f\u308b\u3053\u3068\u306a\u306e\u3067\u8a73\u3057\u304f\u306f\u5c02\u9580\u7684\u306a\u8b1b\u5ea7\u3084\u672c\u3067\u5b66\u3093\u3067\u307b\u3057\u3044\u3067\u3059\u304c\u3001\u7573\u8fbc\u307f\u306f\u753b\u50cf\u306e\u4e00\u90e8\u5206\u3092\u5207\u308a\u51fa\u3057\u3066\u305d\u306e\u9818\u57df\u306e\u7279\u5fb4\u3092\u6349\u3048\u308b\u3088\u3046\u306b\u5b66\u7fd2\u3057\u307e\u3059\u3002\u733f\u306e\u53f3\u76ee\u304c\u6a2a\u65b9\u5411\u306b\u6570\u30d4\u30af\u30bb\u30eb\u79fb\u52d5\u3057\u3066\u3082\u733f\u306e\u53f3\u76ee\u306e\u7279\u5fb4\u304c\u5909\u308f\u3089\u306a\u3044\u3088\u3046\u306b\u3001\u57fa\u672c\u7684\u306b\u8996\u899a\u306e\u4e16\u754c\u306f\u79fb\u52d5\u4e0d\u5909\u306a\u306e\u3067\u753b\u50cf\u5168\u4f53\u306e\u7279\u5fb4\u3092\u5b66\u7fd2\u3059\u308b\u3088\u308a\u90e8\u5206\u7684\u306a\u7279\u5fb4\u3092\u5b66\u7fd2\u3059\u308b\u307b\u3046\u304c\u52b9\u7387\u3088\u304f\u5b66\u7fd2\u3067\u304d\u307e\u3059\u3002\n\n\u4eca\u56de\u306f\u30c7\u30fc\u30bf\u91cf\u3082\u5c11\u306a\u3044\u306e\u3067\u6d45\u3081\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f5c\u308a\u307e\u3059\u304c\u3001\u30c7\u30fc\u30bf\u304c\u5927\u91cf\u306b\u5165\u624b\u3067\u304d\u308b(\u304b\u3064\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30ea\u30bd\u30fc\u30b9\u304c\u5341\u5206\u306b\u3042\u308b)\u5834\u5408\u306f\u6df1\u3044\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3057\u305f\u308a\u5206\u5c90\u306e\u3042\u308b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4f5c\u3063\u3066\u307f\u308b\u306e\u3082\u826f\u3044\u3068\u601d\u3044\u307e\u3059\u3002VGG\u3084Resnet\u3001EfficientNet\u306a\u3069\u975e\u5e38\u306b\u591a\u304f\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3042\u308b\u306e\u3067\u53c2\u8003\u306b\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u4e3b\u8981\u306a\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306f\u5404DeepLearning\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3067\u7c21\u5358\u306b\u69cb\u7bc9\u3067\u304d\u308bAPI\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u6570\u884c\u3067\u69cb\u7bc9\u3057\u3066\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u69cb\u9020\u3092\u78ba\u8a8d\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002\n\u305f\u3060\u3057\u3001\u52c9\u5f37\u76ee\u7684\u3067\u306a\u3051\u308c\u3070\u5148\u8ff0\u306e\u901a\u308a\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u5229\u7528\u3059\u308b\u304bFine tuning\u3059\u308b\u3053\u3068\u3092\u304a\u3059\u3059\u3081\u3057\u307e\u3059\u3002","5e703e44":"# next action\nIf you want to improve the model, you can change the data augmentation, train it for a longer time, or change the activation function. Data augmentation can be mixup, rand augment, cutmix, between class learning are good options.\nIt would be interesting to use a specialized activation function for image classification, FReLU, or to change some of the convolution layers to Depthwise Convolution. If you are interested, you can check out MobileNet, which is a lightweight but accurate network that uses this technique. As an application of Depthwise Convolution, MixConv was published in a paper in 2020, so you may want to check it out as well.\n\nHowever, I think transfer learning and fine tuning are better options.\n\n\u3082\u3057\u3053\u306e\u307e\u307e\u3053\u306e\u30e2\u30c7\u30eb\u3092\u6539\u826f\u3057\u305f\u3044\u5834\u5408\u306fdata augmentation\u3092\u5909\u66f4\u3057\u305f\u308a\u3001\u3088\u308a\u9577\u3044\u6642\u9593\u5b66\u7fd2\u3055\u305b\u305f\u308a\u6d3b\u6027\u5316\u95a2\u6570\u3092\u5909\u66f4\u3059\u308b\u306a\u3069\u306e\u5bfe\u5fdc\u304c\u8003\u3048\u3089\u308c\u307e\u3059\u3002data augmentation\u306f mixup, rand augment, cutmix, between class learning\u306a\u3069\u304c\u826f\u3044\u9078\u629e\u80a2\u3060\u3068\u601d\u3044\u307e\u3059\u3002\n\u753b\u50cf\u5206\u985e\u306b\u7279\u5316\u3057\u305f\u6d3b\u6027\u5316\u95a2\u6570FReLU\u3092\u4f7f\u3063\u3066\u307f\u308b\u306e\u3082\u9762\u767d\u3044\u3068\u601d\u3044\u307e\u3059\u3057\u3001\u7573\u307f\u8fbc\u307f\u306e\u4e00\u90e8\u306e\u30ec\u30a4\u30e4\u30fc\u3092Depthwise Convolution\u306b\u5909\u66f4\u3057\u3066\u307f\u308b\u306e\u3082\u826f\u3044\u3068\u601d\u3044\u307e\u3059\u3002Depthwise Convolution\u306f\u5165\u529b\u30c7\u30fc\u30bf\u306e\u4f4d\u7f6e\u95a2\u4fc2\u306f\u610f\u5473\u304c\u3042\u308b\u304c\u3001\u30c1\u30e3\u30cd\u30eb\u9593\u306e\u95a2\u4fc2\u306f\u7121\u3044\u3068\u4eee\u5b9a\u3057\u3066\u7573\u307f\u8fbc\u307f\u306e\u8a08\u7b97\u91cf\u3092\u524a\u6e1b\u3059\u308b\u624b\u6cd5\u3067\u3059\u3002keras\u3067\u3042\u308c\u3070Conv2D\u3092SeparableConv2D\u306b\u5909\u66f4\u3059\u308b\u3060\u3051\u3067\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002MobileNet\u304c\u3053\u306e\u624b\u6cd5\u3092\u4f7f\u3044\u8efd\u91cf\u3067\u3082\u7cbe\u5ea6\u3092\u4fdd\u3063\u305f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3068\u3057\u3066\u4f5c\u3089\u308c\u305f\u306e\u3067\u8208\u5473\u304c\u3042\u308c\u3070MobileNet\u3092\u8abf\u3079\u3066\u307f\u308b\u3068\u826f\u3044\u3067\u3057\u3087\u3046\u3002Depthwise Convolution\u306e\u5fdc\u7528\u3068\u3057\u3066MixConv\u304c2020\u5e74\u306b\u8ad6\u6587\u767a\u8868\u3055\u308c\u305f\u306e\u3067\u3053\u3061\u3089\u3082\u78ba\u8a8d\u3059\u308b\u3068\u826f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\n\n\u305f\u3060\u3057\u3001\u3057\u3064\u3053\u3044\u3088\u3046\u3067\u3059\u304c\u3001transfer learning\u3084fine tuning\u304c\u3088\u308a\u826f\u3044\u9078\u629e\u80a2\u3060\u3068\u601d\u3044\u307e\u3059\u3002","662c9551":"Set current time to save file names. Saving the model weights to a file makes it possible to save the learned model and use it in retraining and inference.\n\n\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u3092\u30d5\u30a1\u30a4\u30eb\u306b\u4fdd\u5b58\u3057\u3066\u304a\u304f\u3053\u3068\u3067\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3057\u3066\u518d\u5b66\u7fd2\u3084\u63a8\u8ad6\u3067\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002","edd6c5db":"# Build model\nDownload the pre-trained model provided by Google and use it as a feature extractor. It is important to make the feature extractor untrainable so as not to destroy the already trained model. If you want to train the pre-trained model as well, make only the part close to the output layer trainable and retrain it. The shallow layers close to the input layer are trained to capture generic features and generally do not need to be retrained. (I think the performance is likely to be worse if retrained.)\n\nGoogle\u304c\u63d0\u4f9b\u3057\u3066\u3044\u308b\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3066\u7279\u5fb4\u91cf\u62bd\u51fa\u5668\u3068\u3057\u3066\u5229\u7528\u3057\u307e\u3059\u3002\u65e2\u306b\u8a13\u7df4\u3057\u305f\u30e2\u30c7\u30eb\u3092\u7834\u58ca\u3057\u306a\u3044\u3088\u3046\u306b\u3001\u7279\u5fb4\u91cf\u62bd\u51fa\u5668\u306f\u8a13\u7df4\u4e0d\u53ef\u3068\u3057\u3066\u304a\u304f\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\u3002\u3082\u3057\u3082\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3082\u5408\u308f\u305b\u3066\u8a13\u7df4\u3055\u305b\u305f\u3044\u5834\u5408\u306f\u51fa\u529b\u5c64\u306b\u8fd1\u3044\u90e8\u5206\u3060\u3051\u3092\u5b66\u7fd2\u53ef\u80fd\u306b\u3055\u305b\u3066\u518d\u5b66\u7fd2\u3055\u305b\u307e\u3057\u3087\u3046\u3002\u5165\u529b\u5c64\u306b\u8fd1\u3044\u6d45\u3044\u5c64\u3067\u306f\u6c4e\u7528\u7684\u306a\u7279\u5fb4\u3092\u6349\u3048\u308b\u3088\u3046\u306b\u8a13\u7df4\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u4e00\u822c\u7684\u306b\u518d\u5b66\u7fd2\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u305b\u3093\u3002(\u6027\u80fd\u304c\u60aa\u304f\u306a\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3068\u601d\u3044\u307e\u3059\u3002)","6a37e71a":"# Image Loader\nCreate Image data generator. Keras  provides us with a utility function for loading images. This is very useful but a little bit slow, so we recommend using tensorflow's <a href='https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset'>Dataset API<\/a>, if possible. The Dataset API allows your program to compute resource efficiency <a href='https:\/\/www.tensorflow.org\/guide\/data_performance'>(see here)<\/a>. You can use cpu to prepare data when gpu train model. Kernel resource shows sometimes gpu doesn't working while training. This means gpu resource rest when cpu load data.  \nThis time we will use ImageDataGenerator for simplicity. Let's do Data Augmentation by specifying the options for processing the image. Processing the images at each iteration makes it possible to train the model with more images than the data you have.\n\nkeras\u306f\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080\u305f\u3081\u306e\u4fbf\u5229\u306a\u95a2\u6570\u3092\u63d0\u4f9b\u3057\u3066\u304f\u308c\u3066\u3044\u308b\u306e\u3067\u305d\u308c\u3092\u5229\u7528\u3057\u307e\u3059\u3002\u3068\u3066\u3082\u4fbf\u5229\u306a\u306e\u3067\u3059\u304c\u3001\u3084\u3084\u9045\u3044\u306e\u3067\u53ef\u80fd\u3067\u3042\u308c\u3070tensorflow\u306eDataset API\u3092\u5229\u7528\u3059\u308b\u3053\u3068\u3092\u304a\u3059\u3059\u3081\u3057\u307e\u3059\u3002Dataset API\u3092\u4f7f\u3046\u3053\u3068\u3067CPU\u3068GPU\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u4e0a\u624b\u306b\u4f7f\u3046\u3053\u3068\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002Kernel\u306e\u30ea\u30bd\u30fc\u30b9\u3092\u898b\u308c\u3070\u308f\u304b\u308b\u3068\u304a\u308a\u3001\u5b66\u7fd2\u4e2d\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u6642\u3005GPU\u304c\u4f11\u3093\u3067\u3044\u308b\u6642\u9593\u5e2f\u304c\u3042\u308a\u307e\u3059\u3002Dataset API\u306fGPU\u3067\u5b66\u7fd2\u4e2d\u306bCPU\u3067\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3001GPU\u3092\u3042\u307e\u308a\u4f11\u307e\u305b\u305a\u306b\u5b66\u7fd2\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002  \n\u4eca\u56de\u306f\u7c21\u5358\u306e\u305f\u3081ImageDataGenerator\u3092\u5229\u7528\u3057\u307e\u3059\u3002\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080\u969b\u306e\u52a0\u5de5\u65b9\u6cd5\u3092\u6307\u5b9a\u3057\u3066Data Augmentation\u3092\u5b9f\u73fe\u3057\u307e\u3059\u3002\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u3080\u90fd\u5ea6\u56de\u8ee2\u3084\u62e1\u5927\u7e2e\u5c0f\u306a\u3069\u306e\u52a0\u5de5\u3092\u65bd\u3059\u3053\u3068\u3067\u3001\u5b9f\u969b\u306e\u30c7\u30fc\u30bf\u91cf\u3088\u308a\u591a\u304f\u306e\u753b\u50cf\u3067\u5b66\u7fd2\u3055\u305b\u305f\u3053\u3068\u306b\u306a\u308a\u307e\u3059\u3002","aab744e7":"# TODO\nShow how difficult it is to create a model from scratch and produce high performance as in this case.\n\n\u30e2\u30c7\u30eb\u3092\u30b9\u30af\u30e9\u30c3\u30c1\u3067\u4f5c\u6210\u3057\u3066\u3001\u4eca\u56de\u306e\u3088\u3046\u306b\u9ad8\u3044\u6027\u80fd\u3092\u51fa\u3059\u3053\u3068\u304c\u3044\u304b\u306b\u96e3\u3057\u3044\u304b\u793a\u3059\u3002","ef22d907":"# Quick Tutorial Image Classification\n\nThis tutorial will show you how to create a image classification model. The easiest way is to reuse a pre-trained model. So, at first I will show how to use pre-trained model. After that, I will describe how to create a model from scratch, and show how difficult it is.\n\nPlease note that this notebook was made for a study group and it contains Japanese text. This English sentence is actually a machine translation using DeepL. There are some parts that have been modified, but I rely on DeepL for most of it since I am not good at English.","ced954d2":"It depends on the number of blocks and the number of layers in a block, but we can see that there are far more Trainable params than in the previous model. The more parameters to train, the harder it is to learn, and the more time and computer resources are required. \n\n\u30d6\u30ed\u30c3\u30af\u306e\u6570\u3068\u30d6\u30ed\u30c3\u30af\u5185\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u6570\u306b\u3082\u4f9d\u5b58\u3057\u307e\u3059\u304c\u3001\u5148\u7a0b\u306e\u30e2\u30c7\u30eb\u3068\u6bd4\u8f03\u3057\u3066Trainable params\u304c\u5727\u5012\u7684\u306b\u591a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u8a13\u7df4\u3059\u3079\u304d\u30d1\u30e9\u30e1\u30fc\u30bf\u304c\u591a\u3044\u3068\u5b66\u7fd2\u304c\u56f0\u96e3\u304b\u3064\u6642\u9593\u3082\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30ea\u30bd\u30fc\u30b9\u3082\u5fc5\u8981\u306b\u306a\u308a\u307e\u3059\u3002"}}