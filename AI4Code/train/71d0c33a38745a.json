{"cell_type":{"7fa50eb0":"code","16f564b2":"code","b5c04f75":"code","0aa82a88":"code","617902b8":"code","79b4daa9":"code","544a187b":"code","7a7f0636":"code","24a759ec":"code","c2526429":"code","7d3579f1":"code","3d5ac0d6":"code","5a828ec7":"code","9e9036b2":"code","a57776cb":"code","4c222472":"code","a16aa719":"code","eaf29e37":"code","42adb5cb":"code","d09d7fb9":"code","da7c5dfa":"code","e76b3214":"code","a91e25ed":"code","15e7dbfe":"code","54d3ab7f":"code","10d81e18":"code","4728180b":"markdown","e27829fe":"markdown","1ec42825":"markdown","df38c6ca":"markdown","de8f5525":"markdown","020cbc52":"markdown","1f37438e":"markdown","5be35805":"markdown","4eb1bda5":"markdown","9fc68677":"markdown"},"source":{"7fa50eb0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error, confusion_matrix,accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","16f564b2":"# loading data into dataframes\ndf = pd.read_csv('\/kaggle\/input\/loan-defaulter\/application_data.csv')\ncolumns_desc = pd.read_csv('\/kaggle\/input\/loan-defaulter\/columns_description.csv')","b5c04f75":"# First we need to understand the shape of the data and the types of columns (categorical or numerical) we are working with\nprint(f'Shape of Data: {df.shape}')\ncat_columns = df.select_dtypes(include=[object]).columns\nnum_columns = df.select_dtypes(include=[int, float]).columns\nprint(f'Number of Categorical Columns: {len(cat_columns)} \\nNumerical Columns: {len(num_columns)}')\ndf.describe()","0aa82a88":"# Evaluate the amount of null values in our dataset so we can clean it later\nmost_null_columns = df.isnull().sum().sort_values(ascending=False)\ngt_5_percent = sum(most_null_columns > df.shape[0] * 0.05)\nprint(f'Number of columns with missing data greater than 5% of the entire dataset: {gt_5_percent} \\n')\nprint(list(most_null_columns))\nprint()\nprint(list(most_null_columns.keys()))","617902b8":"# plotting the difference between non-performing loans and those that paid correctly\ntarget_values = df[['TARGET']].value_counts()\n\nprint(target_values[0] \/ df.shape[0])\nprint(target_values[1] \/ df.shape[0])\ntarget_values.plot.bar()","79b4daa9":"# understanding difference between gender on the loan distribution\nprint(df['CODE_GENDER'].value_counts() \/ df.shape[0])\ndf['CODE_GENDER'].value_counts().plot.bar()","544a187b":"# undertanding how is the distribution between gender across defaulted loans\nprint(df[df['TARGET'] == 1]['CODE_GENDER'].value_counts() \/ df[df['TARGET'] == 1].shape[0])\ndf[df['TARGET'] == 1]['CODE_GENDER'].value_counts().plot.bar()","7a7f0636":"# understading the correlation between numerical variables and target\nprint(df[num_columns].abs().apply(lambda x: x.corr(df.TARGET)).sort_values().to_string())","24a759ec":"# understading the correlation between categorical variables and target\ncategorical_variables = pd.get_dummies(df[cat_columns], drop_first=True)\nprint(categorical_variables.apply(lambda x: x.corr(df.TARGET)).sort_values().to_string())","c2526429":"# analyzing loans based on contract type\nprint(df['NAME_CONTRACT_TYPE'].value_counts())\ndf['NAME_CONTRACT_TYPE'].value_counts().plot.bar(rot=0, title='Total loans by Contract Type')\nfig = df['NAME_CONTRACT_TYPE'].value_counts().plot.bar(rot=0,title='Total loans by Contract Type').get_figure()\nfig.savefig('loan_type_plot.png')","7d3579f1":"# analyzing defaulted loans based on contract type\ndefaulted_df = df[df['TARGET'] == 1]\nprint(defaulted_df['NAME_CONTRACT_TYPE'].value_counts())\ndefaulted_df['NAME_CONTRACT_TYPE'].value_counts().plot.bar(rot=0)\nfig = defaulted_df['NAME_CONTRACT_TYPE'].value_counts().plot.bar(rot=0, title='Total Defaulted loans by Contract Type').get_figure()\nfig.savefig('default_loan_type_plot.png')","3d5ac0d6":"# plotting the difference between contract types\nbar_plot = pd.concat(\n    [df.rename(columns={\"NAME_CONTRACT_TYPE\": \"Total Loans\",})['Total Loans'].value_counts().to_frame().transpose(),\n     df.rename(columns={\"NAME_CONTRACT_TYPE\": \"Total Defaulted Loans\",})[df['TARGET'] == 1]['Total Defaulted Loans'].value_counts().to_frame().transpose(),]\n).plot.bar(rot=0, title='Total Loans by Default Status and Contract Type', figsize=(12,8))\nfig = bar_plot.get_figure()\nfig.savefig('total_default_loan_type_plot.png')","5a828ec7":"# plotting relative region population\nbar_plot = df[df['TARGET'] == 1]['REGION_POPULATION_RELATIVE'].plot.hist(title='Relative Region Population (default loans)', figsize=(10,3))\nfig = bar_plot.get_figure()\nfig.savefig('relative_pop.png')","9e9036b2":"# plotting the age distribution in defaulted loans\nfig, ax = plt.subplots()\n(df[df['TARGET'] == 1]['DAYS_BIRTH'].abs()\/\/365).value_counts().to_frame().sort_index().plot(\n    kind='line', \n    ax=ax, \n    title='Age Distribution (default loans)'\n)\n\nax.legend([])\nfig = fig.get_figure()\nfig.savefig('total_defaulte_age.png')","a57776cb":"bar_plot = df[df['TARGET'] == 1]['NAME_FAMILY_STATUS'].replace('Single \/ not married', 'Single').value_counts()[:-1].plot.bar(\n    color='green',\n    title='Marital status (default loans)',\n    rot=0\n)\nfig = bar_plot.get_figure()\nfig.savefig('marital_default.png')","4c222472":"bar_plot = df[df['TARGET'] == 1]['CODE_GENDER'].value_counts().plot.bar(\n    title='Gender (default loans)',\n    rot=0\n)\nfig = bar_plot.get_figure()\nfig.savefig('gender_default.png')","a16aa719":"fig, ax = plt.subplots()\ndf[df['TARGET'] == 1].rename(columns={'NAME_EDUCATION_TYPE': \"Education\",})['Education'].value_counts()[:-1].plot(\n    kind='pie',  \n    fontsize=10,\n     autopct='%.2f',\n    figsize=(8,6),\n    title='Education distribution (default loans)'\n)\nfig = fig.get_figure()\nfig.savefig('education_default.png')","eaf29e37":"b_plot = df[df['TARGET'] == 1].groupby(pd.cut(df[df['TARGET'] == 1]['AMT_INCOME_TOTAL'], np.arange(0, 600_000, 50000))).sum()['TARGET'].plot.bar(\n    rot=20,\n    title=\"Customer's Income by ranges of 50k (default loans)\",\n    figsize=(12,6),\n    xlabel=''\n)\nfig = b_plot.get_figure()\nfig.savefig('income_graph.png')","42adb5cb":"# Dropping \/ Imputing NaNs - Removing unnecessary data and imputing some with average or mode\n# Decided to remove columns with 50%+ of NaN values without ext_sources\npercentage = 50\nmin_count =  int(((100 - percentage) \/ 100) * df.shape[0] + 1)\ndropped_nans_df = df[df.columns[~df.columns.isin(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'])]].dropna(axis=1, thresh=min_count)\ndropped_nans_df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']] = df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].dropna()\n\n# for numerical ones imputing average\napply_mean = lambda col: col.fillna(col.mean())\nimputed_mean_num_df = dropped_nans_df.select_dtypes(include=[int, float]).abs().apply(apply_mean)\ndropped_nans_df[imputed_mean_num_df.columns] = imputed_mean_num_df\ndropped_nans_df.head()","d09d7fb9":"# One Hot Enconding\nprint(dropped_nans_df.shape)\nencoded_df = pd.get_dummies(dropped_nans_df)\nencoded_df.head()","da7c5dfa":"# Removing low variance columns\nencoded_df.loc[:, encoded_df.std() > .2]","e76b3214":"# Removing outliers\nfrom scipy.stats import zscore\nzscores = zscore(encoded_df)\nabs_z_scores = np.abs(zscores)\nfiltered_entries = (abs_z_scores < 11).all(axis=1)\nnew_df = encoded_df[filtered_entries]\nnew_df","a91e25ed":"# train\/test splitting\nX = new_df[new_df.columns[~new_df.columns.isin(['TARGET', 'SK_ID_CURR'])]]\ny = new_df['TARGET']\n\n# pre processing\nstandardizer = StandardScaler()\nX = standardizer.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)","15e7dbfe":"# setting up and applying logistic regression\nlg_model = LogisticRegression(solver='liblinear')\nlg_model.fit(X_train, y_train)\n\n#Make predictions using the testing set\ny_pred = lg_model.predict(X_test)","54d3ab7f":"cm = confusion_matrix(y_test, y_pred)\nTN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n\nprint('True Positive(TP)  = ', TP)\nprint('False Positive(FP) = ', FP)\nprint('True Negative(TN)  = ', TN)\nprint('False Negative(FN) = ', FN)\n\naccuracy =  (TP+TN) \/(TP+FP+TN+FN)\nprint('Accuracy of the binary classification = {:0.3f}'.format(accuracy_score(y_test, y_pred)))","10d81e18":"weights = lg_model.coef_[0]\nabs_weights = np.abs(weights)\n\n#get the sorting indices\nsorted_index = np.argsort(abs_weights)[::-1]\n\n\n#get the index of the top-3 features\ntop_3 = sorted_index[:3]\n\n#get the names of the top 3 most important features\nprint(list(new_df.iloc[:, top_3].columns))\n","4728180b":"# Conclusion","e27829fe":"# Business Understanding\n\nIn this section its discussed about what questions the analysis of this data must answer. \n<br> \nThe financial sector is one that always has data to be analyzed. Based on this dataset we can understand that we're dealing with a company that loans money to people.\n<br>\nTo guide this analysis I developed 3 questions that we need to answer to gather the benefits from it.\n\n* Which type of contract is the most defaulted ? \n* How the default behavior is divided across regions and social context.\n* What are the factors that most relate a costumer to default a loan. And how the analysis can help avoid this.","1ec42825":"From this analysis, we achieved to answer the questions that were brought up. \n<br>\nIt was discovered that the Contract Type of cash loan is the one that has the most default behavior in terms of size and percentage.\n<br>\nAlso that the behavior of defaulting loans happens more often at an early age people, usually female, with a great chance of been married and having at most secondary education and earning at most 200k annually.\n<br>\nAnd that the variables that  most relate for predicting a loan to default are relative region population, credit amount and organization type.","df38c6ca":"# Modeling\n\nFor this project I decided to use a simpler approach by using the LogisticRegression algorithm","de8f5525":"### Data Preparation\n\nNow we get hands dirty on the dataset and focus on improving the overall data quality. \n<br>\nSome of the common techniques that it will be used are:\n\n* Dropping\u00a0\/\u00a0Imputing NaNs - Removing unnecessary data and imputing some with average or mode\n* One Hot Enconding - For adding the categorial values to the evaluation and predicting of the Target variable\n* Outliers - find and remove outliers that can bias the analysis\n* Low Variance variables - remove variables that doesn't have impact on the target variable","020cbc52":"# Credit Company Analysis\n\nFor this notebook, it will be analyzed patterns in a dataset that has characteristics about loans. And answer questions that can help businesses that works in the loan industry to avoid customers that have a high chance of defaulting a loan.\n<br>I'll follow the CRISP-DM approach, so the outcome can be standardized across its development.\n","1f37438e":"This notebook will be divided by the CRISP-DM standard and it will be as follows\n* Business Understanding\n* Data Understanding\n* Data Preparation\n* Modeling\n* Evaluation\n* Deploy","5be35805":"# Evaluation","4eb1bda5":"Some insights from the Data Understanding part:\n\n* Shape of Data: 307511 rows, 122 columns\n* Number of Categorical Columns: 16 \n* Numerical Columns: 106\n* Number of columns with missing data greater than 5% of the entire dataset: 57 \n* From the total of 307511 loans, 91.9% (282602) were Ok and 8.1% (24631) were default.\n* From all the variables the ones that had a slighlty more correlation with Target was EXT_SOURCE_1, 2 and 3, tha represents the normalized score from an external source.\n* This shows a slight negative relation, which makes sense. If the score is high the chance of the customer default will be lower.","9fc68677":"# Data Understading\n\nThis part is where the real work begins. Data Understanding and Data preparation are the parts where we need to understand, filter, clean, impute, remove, and much more so the data processed can generate results in a more reliable way for the analysis and for model ingestion."}}