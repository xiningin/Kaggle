{"cell_type":{"a1acf808":"code","eb8407ff":"code","5d62e862":"code","85be0834":"code","95849904":"code","4b1c7ebd":"code","b801b218":"code","7c1cb3b1":"code","06c7c539":"code","dac0b5b4":"code","41505b18":"code","feacca3b":"code","af8cb4df":"code","c81a1d60":"code","5d593957":"code","390e8feb":"markdown","a9b61db7":"markdown"},"source":{"a1acf808":"# importing required packages for building classification machine learning model\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.tree import export_graphviz\nimport six\nimport pydot\nfrom sklearn import tree\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import accuracy_score, confusion_matrix #to check the accuracy of the model","eb8407ff":"# reading forest types datasets which is downloaded from UCI\n# https:\/\/archive.ics.uci.edu\/ml\/datasets\/Forest+type+mapping\n# download the data and upload it to Kaggle by clicking \"Add Data\"\ndf_train = pd.read_csv('..\/input\/training.csv')\ndf_Test = pd.read_csv('..\/input\/testing.csv')","5d62e862":"# displaying top 5 records to check whether its reading properly from kaggle server\n# here the first column 'class' is the output and rest of the columns are inputs\n# so we need to split the datasets into two\n# 1 dataset contains only output, (i.e) column 'class'\n# 2 dataset will have rest of the columns\ndf_train.head()","85be0834":"df_train['class'].unique()\n# Class: 's' ('Sugi' forest), 'h' ('Hinoki' forest), 'd' ('Mixed deciduous' forest), 'o' ('Other' non-forest land)","95849904":"X_train = df_train.iloc[:, 1:].values # extracting inputs from training dataset - column 1 to till end\ny_train = df_train.iloc[:, 0].values # extracting output from training dataset - column 0\n\nX_test = df_Test.iloc[:, 1:].values # extracting inputs from testing dataset - column 1 to till end\ny_test = df_Test.iloc[:, 0].values # extracting output from training dataset - column 0","4b1c7ebd":"# Feature Scaling - Our dataset values are not scaled, (i.e) columns values are not in specific range\n# By applying feature scaling - the sum of values in all columns will be zero\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","b801b218":"# implementing through logistic regression algorithm with penalty l1 and liblinear solver - 86%\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty ='l1',solver='liblinear',\n                                multi_class='ovr', C=6)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","7c1cb3b1":"# displaying confusion matrix for calculating accuracy\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","06c7c539":"# implementing through random forest algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","dac0b5b4":"# to check feature importance\nfeature_imp = pd.Series(classifier.feature_importances_, index= df_train.columns[1:]).sort_values(ascending=False)\nsns.barplot(x=feature_imp, y =feature_imp.index)\nplt.xlabel('Feature importance Score')\nplt.ylabel('Features')\nplt.title('Visualizing Important Features')\nplt.legend(handles=[])\nplt.show()","41505b18":"# Saving inner decision trees generated by random forest algorithm\ncol = df_train.columns[1:]\ndotfile = six.StringIO()\ni_tree = 1\nfor tree_in_forest in classifier.estimators_:\n    export_graphviz(tree_in_forest,out_file='tree.dot',\n    feature_names=col,\n    filled=True,\n    rounded=True)\n    (graph,) = pydot.graph_from_dot_file('tree.dot')\n    name = 'tree' + str(i_tree)\n    graph.write_png(name+  '.png')\n    os.system('dot -Tpng tree.dot -o tree.png')\n    i_tree +=1","feacca3b":"# Showing inner decision trees generated by random forest algorithm\nfig=plt.figure(figsize=(50, 50), dpi=150, facecolor='w', edgecolor='k')\ncolumns = 2\nrows = 5\nfor i in range(1, 2):\n    img = mpimg.imread('tree' + str(i) + '.png')    \n    fig.add_subplot(rows, columns, i)\n    plt.title('Random Forest Decision Tree' + str(i))\n    plt.imshow(img)\nplt.show()","af8cb4df":"# implementing through naive bayes algorithm - 80%\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","c81a1d60":"# implementing through XGB classifier algorithm - 78%\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","5d593957":"# implementing through decision tree algorithm - 76%\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\nprint(accuracy_score(y_test, y_pred)*100)","390e8feb":"**Implementing through Logistic Regression Algorithm which has highest accuracy - 86%**","a9b61db7":"**Implementing through other algorithm which has accuracy less than random forest**"}}