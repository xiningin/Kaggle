{"cell_type":{"722f5240":"code","55571b87":"code","5b390e27":"code","80659dcd":"code","a7d88603":"code","262c0ad1":"code","3d28484e":"code","434f43f5":"code","ead2b422":"code","440cf055":"code","c20d0277":"code","249723f7":"code","9f0c6899":"code","a2d57583":"code","96172e3a":"code","de019656":"code","4837e4ed":"code","32dd6e95":"code","6ba29958":"code","e41a1bf6":"code","95557f14":"code","080a0613":"code","11cab361":"code","ba4c1e67":"code","e86de872":"code","e2c2cc8c":"code","257775a4":"code","b0f24416":"code","4523a849":"code","354517e6":"code","30d0538f":"code","6b3b342a":"code","75507973":"code","e4fe44b5":"code","9f5c00d1":"code","1c2fdd0d":"code","6a5cbac4":"markdown","eda3a21c":"markdown","96e5d3f5":"markdown","e4e16cf0":"markdown","61bbed97":"markdown","401eecd1":"markdown","5b77c1d6":"markdown","be0372f6":"markdown","869cf9cc":"markdown","b50cc8ef":"markdown","27e01ff3":"markdown","56a23378":"markdown","93e295d7":"markdown","b2df984c":"markdown","3c62b4c9":"markdown","32c29b29":"markdown","ca5b93b3":"markdown","07462238":"markdown","c18e5139":"markdown"},"source":{"722f5240":"from keras.preprocessing.image import load_img\nim = load_img('..\/input\/unetresnet34\/model.png')\nim","55571b87":"import numpy as np\nimport pandas as pd\nimport six\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\n\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils import conv_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.engine.topology import get_source_inputs\nfrom keras.engine import InputSpec\nfrom keras import backend as K\nfrom keras.applications.imagenet_utils import _obtain_input_shape\nfrom keras.regularizers import l2\n\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate,add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D","5b390e27":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","80659dcd":"train_df = pd.read_csv(\"..\/input\/tgs-salt-identification-challenge\/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"..\/input\/tgs-salt-identification-challenge\/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","a7d88603":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/tgs-salt-identification-challenge\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","262c0ad1":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/tgs-salt-identification-challenge\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","3d28484e":"train_df[\"coverage\"] = train_df.masks.map(np.sum) \/ pow(img_size_ori, 2)","434f43f5":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","ead2b422":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","440cf055":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","c20d0277":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","249723f7":"# https:\/\/github.com\/qubvel\/segmentation_models\/blob\/master\/segmentation_models\/unet\/models.py\n\ndef handle_block_names(stage):\n    conv_name = 'decoder_stage{}_conv'.format(stage)\n    bn_name = 'decoder_stage{}_bn'.format(stage)\n    relu_name = 'decoder_stage{}_relu'.format(stage)\n    up_name = 'decoder_stage{}_upsample'.format(stage)\n    return conv_name, bn_name, relu_name, up_name\n\n\ndef Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                     batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n\n        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'1')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'1')(x)\n        x = Activation('relu', name=relu_name+'1')(x)\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'2')(x)\n        x = Activation('relu', name=relu_name+'2')(x)\n\n        return x\n    return layer\n\n\ndef Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n                      transpose_kernel_size=(4,4), batchnorm=False, skip=None):\n\n    def layer(input_tensor):\n\n        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n\n        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n                            padding='same', name=up_name)(input_tensor)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'1')(x)\n        x = Activation('relu', name=relu_name+'1')(x)\n\n        if skip is not None:\n            x = Concatenate()([x, skip])\n\n        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n        if batchnorm:\n            x = BatchNormalization(name=bn_name+'2')(x)\n        x = Activation('relu', name=relu_name+'2')(x)\n\n        return x\n    return layer","9f0c6899":"def build_unet(backbone, classes, last_block_filters, skip_layers,\n               n_upsample_blocks=5, upsample_rates=(2,2,2,2,2),\n               block_type='upsampling', activation='sigmoid',\n               **kwargs):\n\n    input = backbone.input\n    x = backbone.output\n\n    if block_type == 'transpose':\n        up_block = Transpose2D_block\n    else:\n        up_block = Upsample2D_block\n\n    # convert layer names to indices\n    skip_layers = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n                    for l in skip_layers])\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        if i < len(skip_layers):\n#             print(backbone.layers[skip_layers[i]])\n#             print(backbone.layers[skip_layers[i]].output)\n            skip = backbone.layers[skip_layers[i]].output\n        else:\n            skip = None\n\n        up_size = (upsample_rates[i], upsample_rates[i])\n        filters = last_block_filters * 2**(n_upsample_blocks-(i+1))\n\n        x = up_block(filters, i, upsample_rate=up_size, skip=skip, **kwargs)(x)\n\n    if classes < 2:\n        activation = 'sigmoid'\n\n    x = Conv2D(classes, (3,3), padding='same', name='final_conv')(x)\n    x = Activation(activation, name=activation)(x)\n\n    model = Model(input, x)\n\n    return model","a2d57583":"# https:\/\/github.com\/raghakot\/keras-resnet\/blob\/master\/resnet.py\ndef _bn_relu(input):\n    \"\"\"Helper to build a BN -> relu block\n    \"\"\"\n    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n    return Activation(\"relu\")(norm)\n\n\ndef _conv_bn_relu(**conv_params):\n    \"\"\"Helper to build a conv -> BN -> relu block\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(input)\n        return _bn_relu(conv)\n\n    return f\n\n\ndef _bn_relu_conv(**conv_params):\n    \"\"\"Helper to build a BN -> relu -> conv block.\n    This is an improved scheme proposed in http:\/\/arxiv.org\/pdf\/1603.05027v2.pdf\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        activation = _bn_relu(input)\n        return Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(activation)\n\n    return f\n\n\ndef _shortcut(input, residual):\n    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n    \"\"\"\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    input_shape = K.int_shape(input)\n    residual_shape = K.int_shape(residual)\n    stride_width = int(round(input_shape[ROW_AXIS] \/ residual_shape[ROW_AXIS]))\n    stride_height = int(round(input_shape[COL_AXIS] \/ residual_shape[COL_AXIS]))\n    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n\n    shortcut = input\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n                          kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          padding=\"valid\",\n                          kernel_initializer=\"he_normal\",\n                          kernel_regularizer=l2(0.0001))(input)\n\n    return add([shortcut, residual])\n\ndef basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n                           strides=init_strides,\n                           padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n                                  strides=init_strides)(input)\n\n        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n        return _shortcut(input, residual)\n\n    return f\n\ndef _residual_block(block_function, filters, repetitions, is_first_layer=False):\n    \"\"\"Builds a residual block with repeating bottleneck blocks.\n    \"\"\"\n    def f(input):\n        for i in range(repetitions):\n            init_strides = (1, 1)\n            if i == 0 and not is_first_layer:\n                init_strides = (2, 2)\n            input = block_function(filters=filters, init_strides=init_strides,\n                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n        return input\n\n    return f\n\ndef _handle_dim_ordering():\n    global ROW_AXIS\n    global COL_AXIS\n    global CHANNEL_AXIS\n    if K.image_dim_ordering() == 'tf':\n        ROW_AXIS = 1\n        COL_AXIS = 2\n        CHANNEL_AXIS = 3\n    else:\n        CHANNEL_AXIS = 1\n        ROW_AXIS = 2\n        COL_AXIS = 3\n\n\ndef _get_block(identifier):\n    if isinstance(identifier, six.string_types):\n        res = globals().get(identifier)\n        if not res:\n            raise ValueError('Invalid {}'.format(identifier))\n        return res\n    return identifier\n\n\nclass ResnetBuilder(object):\n    @staticmethod\n    def build(input_shape, block_fn, repetitions,input_tensor):\n        _handle_dim_ordering()\n        if len(input_shape) != 3:\n            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n\n        # Permute dimension order if necessary\n        if K.image_dim_ordering() == 'tf':\n            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n\n        # Load function from str if needed.\n        block_fn = _get_block(block_fn)\n        \n        if input_tensor is None:\n            img_input = Input(shape=input_shape)\n        else:\n            if not K.is_keras_tensor(input_tensor):\n                img_input = Input(tensor=input_tensor, shape=input_shape)\n            else:\n                img_input = input_tensor\n                \n        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(img_input)\n        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n\n        block = pool1\n        filters = 64\n        for i, r in enumerate(repetitions):\n            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n            filters *= 2\n\n        # Last activation\n        block = _bn_relu(block)\n\n        model = Model(inputs=img_input, outputs=block)\n        return model\n\n    @staticmethod\n    def build_resnet_34(input_shape,input_tensor):\n        return ResnetBuilder.build(input_shape, basic_block, [3, 4, 6, 3],input_tensor)","96172e3a":"def UResNet34(input_shape=(None, None, 3), classes=1, decoder_filters=16, decoder_block_type='upsampling',\n                       encoder_weights=None, input_tensor=None, activation='sigmoid', **kwargs):\n\n    backbone = ResnetBuilder.build_resnet_34(input_shape=input_shape,input_tensor=input_tensor)\n\n    skip_connections = list([97,54,25])  # for resnet 34\n    model = build_unet(backbone, classes, decoder_filters,\n                       skip_connections, block_type=decoder_block_type,\n                       activation=activation, **kwargs)\n    model.name = 'u-resnet34'\n\n    return model","de019656":"model = UResNet34(input_shape=(1,img_size_target,img_size_target))\n# model.summary()","4837e4ed":"from keras.losses import binary_crossentropy\nfrom keras import backend as K\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) \/ (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred \/ (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) \/ K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) \/ (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 \/ w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss","32dd6e95":"model.compile(loss=bce_dice_loss, optimizer=\"adam\", metrics=[\"accuracy\"])","6ba29958":"x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)","e41a1bf6":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)\/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)\/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","95557f14":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\".\/keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)\n\nepochs = 100\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr],shuffle=True)","080a0613":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")","11cab361":"# Load best model\nmodel.load_weights('.\/keras.model')","ba4c1e67":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","e86de872":"offset = 10\nmax_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[offset:offset+max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","e2c2cc8c":"# src: https:\/\/www.kaggle.com\/aglotero\/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection \/ union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp \/ (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","257775a4":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","b0f24416":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","4523a849":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","354517e6":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","30d0538f":"# Source https:\/\/www.kaggle.com\/bguberfain\/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","6b3b342a":"del x_train, x_valid, y_train, y_valid, cov_train,y_valid_ori,train_df","75507973":"x_test = np.array([upsample(np.array(load_img(\"..\/input\/tgs-salt-identification-challenge\/test\/images\/{}.png\".format(idx), grayscale=True))) \/ 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","e4fe44b5":"preds_test = model.predict(x_test)","9f5c00d1":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","1c2fdd0d":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","6a5cbac4":"# Define Loss Function","eda3a21c":"# Loading of training\/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train.","96e5d3f5":"# Build U-Net Model","e4e16cf0":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","61bbed97":"# U-Net with ResNet34 Encoder","401eecd1":"# Sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold.","5b77c1d6":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255.","be0372f6":"# ResNet 34","869cf9cc":"# Augmentation","b50cc8ef":"# Scoring\nScore the model and do a threshold optimization by the best IoU.","27e01ff3":"# Create train\/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling.","56a23378":"# Show some example images","93e295d7":"# Submission\nLoad, predict and submit the test image predictions.","b2df984c":"# Training","3c62b4c9":"# The Model Architecture","32c29b29":"# Params and helpers","ca5b93b3":"In this notebook I use ResNet34 as backbone for U-Net. Augmentation and model loss is same as that of [Ding's Notebook](https:\/\/www.kaggle.com\/dingli\/seismic-data-analysis-with-u-net) details remain The resulting model architecture is shown below. ","07462238":"# Load Libraries","c18e5139":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."}}