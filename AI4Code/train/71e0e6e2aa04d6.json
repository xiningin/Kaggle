{"cell_type":{"eda1d3f7":"code","f78841de":"code","640fe4de":"code","ec7b980c":"code","8e2390db":"code","52246f5d":"code","7ce14316":"code","5ae874f4":"code","6ccd0fb0":"code","484bc784":"code","e57de500":"code","64db4608":"code","4ebaf4c4":"code","119107b0":"code","8599a3cd":"code","b83ba860":"code","5895574c":"code","c2f5792b":"code","d8ff2b93":"code","15cc909b":"code","a08bcada":"code","d65d8d2e":"code","e9a3d88f":"code","d7e8b08a":"code","44282090":"code","ee958656":"code","0c38fe04":"code","70958597":"code","0ce24805":"code","dc52a74e":"code","02a954e7":"code","20f97d7f":"code","241e281e":"code","952fc5e7":"code","67adfa67":"code","f51f7121":"code","286b474a":"code","ab46dbfc":"code","66e989a6":"code","235b7fd3":"code","4d954352":"code","74a25b43":"code","174ed935":"code","66b173ea":"code","dfd35de2":"code","24aed2bb":"code","96021b80":"code","9b02b93e":"code","994afe75":"code","1ee6bd66":"code","b1a02cec":"code","24b9aa8e":"code","487c07ee":"code","fa1f7d42":"code","2154ea12":"code","8c9c53f8":"code","732a3525":"code","c942e171":"code","720755ed":"code","af30e671":"code","5bc93381":"code","9db7edd5":"code","f51c5ff1":"code","105f7ef2":"code","11e5dd8a":"code","7c2b7fd5":"code","43e498b3":"code","16c43d3b":"code","154418e1":"code","f6abd1a1":"code","b52f0c86":"code","af197796":"code","8256e4ca":"code","cb9c82d3":"code","ff478e15":"code","d1e5c58f":"code","b1a7a1e6":"code","1f6e8ae5":"code","de9ce891":"code","f3faa60c":"code","2dad3c70":"code","70df8741":"code","799f7e5c":"code","1ba38b74":"code","8f057a3b":"code","7bf7c46a":"code","fb768538":"code","83db02b3":"code","3cfef7ea":"code","04c8cfd0":"code","3976bea3":"code","287bc35d":"code","07fc2cdc":"code","29bc5d5f":"markdown","89e8b1f6":"markdown","b0ad2fde":"markdown","e213b8f9":"markdown","edca8318":"markdown","efb55cc2":"markdown","9891d884":"markdown","b2ce6fbf":"markdown","59dd2243":"markdown","126c9b27":"markdown","a224b96c":"markdown","2f7f6c89":"markdown","06c73a0f":"markdown","d3bb6d23":"markdown","cdc630cf":"markdown","3fc97db2":"markdown","d6135364":"markdown","9e3ac54f":"markdown","e699a629":"markdown","61c43288":"markdown","28b420d9":"markdown","b2f85748":"markdown","f8311e6b":"markdown","2224a9ee":"markdown","d3896f5b":"markdown","88d6589c":"markdown","a2b88ba2":"markdown","4e8e5cda":"markdown","9b608c7f":"markdown","3dca2de3":"markdown","d62e8878":"markdown","5c52d8e5":"markdown","4343a542":"markdown","fd5949ea":"markdown","ce9024a4":"markdown","9fa4b9ab":"markdown","4c2b29de":"markdown","27b04823":"markdown","bd2b972e":"markdown","5b1ef9cf":"markdown","a51dee84":"markdown","8b439897":"markdown","a70eed21":"markdown","6c31a3a3":"markdown","b905821f":"markdown","f36feee6":"markdown","6b46df7f":"markdown","f9a97b50":"markdown","6494d087":"markdown","49929a6c":"markdown","ed07d710":"markdown","9ab74883":"markdown","d79ec357":"markdown","4b18a901":"markdown","d8ee40ae":"markdown","eab20709":"markdown","d497765f":"markdown","f0536888":"markdown","cf219dd6":"markdown","4493f746":"markdown","2ec3fa16":"markdown","da7a131f":"markdown","baf9bce0":"markdown","5d3b41fb":"markdown","c6d72122":"markdown","8b9f7d7c":"markdown","8c749f78":"markdown"},"source":{"eda1d3f7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f78841de":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n\nfrom scipy.optimize import minimize\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\n\nfrom tqdm import tqdm_notebook\n\nfrom itertools import product","640fe4de":"#for kaggle\nDATAPATH = '\/kaggle\/input\/into-the-future\/train.csv' \n#for my jupyter lab\n# DATAPATH = 'Data\/train.csv' ","ec7b980c":"data = pd.read_csv(DATAPATH, index_col=['time'], parse_dates=['time'])\ndata.head(10)","8e2390db":"data.shape","52246f5d":"data = data[1:]\ndata.shape\ndata","7ce14316":"# -------------do not execute------------------#\n# data = data.resample('20S').mean()\n# print(data.shape)\n# data","5ae874f4":"plt.figure(figsize=(17, 8))\nplt.plot(data.feature_2)\nplt.ylabel('feature_2')\nplt.xlabel('Time')\nplt.grid(False)\nplt.show()","6ccd0fb0":"def plot_moving_average(series, window):\n\n    rolling_mean = series.rolling(window=window).mean()\n    \n    plt.figure(figsize=(17,8))\n    plt.title('Moving average\\n window size = {}'.format(window))\n    plt.plot(rolling_mean, 'g', label='Rolling mean trend')       \n#     plt.plot(series[window:], label='Actual values')\n    plt.legend(loc='best')\n    plt.grid(True)","484bc784":"plot_moving_average(data.feature_2,6)","e57de500":"from pylab import rcParams\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(data.feature_2, model='additive', period=100)\nfig = decomposition.plot()\nplt.show()","64db4608":"def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n    \n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style='bmh'):\n        fig = plt.figure(figsize=figsize)\n        layout = (2,2)\n        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1,0))\n        pacf_ax = plt.subplot2grid(layout, (1,1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()\n        \ntsplot(data.feature_2, lags=30)","4ebaf4c4":"data_diff = data.feature_2 - data.feature_2.shift(1)\n\ntsplot(data_diff[5:], lags=30)","119107b0":"data_diff = data.feature_2 - data.feature_2.shift(2)\n\ntsplot(data_diff[5:], lags=30)","8599a3cd":"from statsmodels.tsa.arima_model import ARIMA","b83ba860":"#Set initial values and some bounds\nps = range(0, 1+1)\nd = range(1,2+1)\nqs = range(0, 2+1)\n\n#Create a list with all possible combinations of parameters\nparameters = product(ps, d, qs)\nparameters_list = list(parameters)\nlen(parameters_list)","5895574c":"def optimize_ARIMA(parameters_list):\n    \n    results = []\n    best_aic = float('inf')\n    \n    for p in parameters_list:\n        try: model = ARIMA(data.feature_2, order=(p[0],p[1],p[2]),freq='10S').fit(disp=-1)\n        except:\n            continue\n            \n        aic = model.aic\n        print(aic)\n        \n        #Save best model, AIC and peters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_p = p\n        results.append([p, model.aic])\n        \n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    #Sort in ascending order, lower AIC is better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table\n\nresult_table = optimize_ARIMA(parameters_list)","c2f5792b":"result_table","d8ff2b93":"p,d, q = result_table.parameters[0]\n\nbest_model = ARIMA(data.feature_2, order=(p, d, q)).fit(disp=-1)\n\nprint(best_model.summary())","15cc909b":"# ploting residual errors\nresiduals = pd.DataFrame(best_model.resid)\nresiduals.plot()\nplt.show()\n\nresiduals.plot(kind='kde')\nplt.show()\n\nprint(residuals.describe())","a08bcada":"old = list(data['feature_2'].values)\nlen(old)","d65d8d2e":"def make_pred(array=old, steps=375, window=10):\n    l=len(array)\n    for i in range(1,steps+1):\n        pos=len(array)\n        sum=0\n        for j in range(pos-window,pos):\n            sum = sum + array[j]\n        array.append(sum\/window)\n    return array","e9a3d88f":"new = make_pred(old,375)\nnew=new[563:]","d7e8b08a":"sol = pd.DataFrame()\nsol['feature_2'] = new\nsol.index =[i for i in range(564,(564+375))]\nsol.index.name = 'id'\nsol.to_csv(\"Sol_10mean.csv\")","44282090":"from fbprophet import Prophet","ee958656":"#facebook has it's own outlier dection so I will not delete the first data \ndata = pd.read_csv(DATAPATH, index_col=['time'], parse_dates=['time'])","0c38fe04":"data_fb = data[['feature_2']].reset_index()\ndata_fb.columns = ['ds','y']\ndata_fb","70958597":"model = Prophet()\nmodel.fit(data_fb)\n\ntest = model.make_future_dataframe(periods=375, freq='10S')\npred = model.predict(test)\npred.head()","0ce24805":"#read full documentation carefully\nfrom fbprophet.diagnostics import cross_validation\ndf_cv = cross_validation(model, horizon = '900 seconds')\ndf_cv.head()","dc52a74e":"old = pred[:564].set_index('ds') #change to 563 for outlier correction\ncheck = pd.concat([data['feature_2'],old['yhat']],axis=1)\ncheck","02a954e7":"mean_squared_error(check['feature_2'],check['yhat'])**.5","20f97d7f":"# #since I had removed the 1st data in the account of being an outlier, I have to now add one digit to the id\n# pred['id'] = pred.index +1\n# pred.set_index('id',inplace=True)","241e281e":"solution_fb=pred[564:]['yhat'] #change to 563 for outlier correction\nsolution_fb = pd.DataFrame(solution_fb) \nsolution_fb.index.name = 'id'\nsolution_fb.columns = ['feature_2']\nsolution_fb.to_csv(\"Solution_fb.csv\")","952fc5e7":"DATAPATH = '\/kaggle\/input\/into-the-future\/train.csv' \n\ndata = pd.read_csv(DATAPATH, index_col=['time'], parse_dates=['time'])\ndata.head(10)","67adfa67":"log_check = data[['id','feature_2']]\nlog_check = log_check[1:]","f51f7121":"log_check['id_log'] = np.log1p(log_check.id)","286b474a":"sns.lineplot(data = log_check, x='id_log',y='feature_2')","ab46dbfc":"sns.regplot(data = log_check, x='id_log',y='feature_2')","66e989a6":"sns.residplot(data = log_check, x='id_log',y='feature_2')","235b7fd3":"log_check","4d954352":"from sklearn.linear_model import LinearRegression\n\nlr_log = LinearRegression()\nlr_log.fit(log_check[['id_log']],log_check.feature_2)\n\nprint(mean_squared_error(log_check.feature_2,lr_log.predict(log_check[['id_log']]))**.5)\n\nlog_check['LogPredict']=lr_log.predict(log_check[['id_log']])\nlog_check['diff']=log_check['LogPredict']-log_check['feature_2']","74a25b43":"log_check","174ed935":"mean_absolute_error(log_check.feature_2,log_check.LogPredict)","66b173ea":"DATAPATH = '\/kaggle\/input\/into-the-future\/test.csv' \ntest = pd.read_csv(DATAPATH, index_col=['time'], parse_dates=['time'])\ntest['id_log']=np.log1p(test.id)\ntest['feature_2']=lr_log.predict(test[['id_log']])\n# test","dfd35de2":"test = test[['id','feature_2']]\ntest.set_index('id',inplace=True)\ntest.to_csv(\"Solution_logT.csv\")","24aed2bb":"DATAPATH = '\/kaggle\/input\/into-the-future\/train.csv' \n\ndata = pd.read_csv(DATAPATH, index_col=['time'], parse_dates=['time'])","96021b80":"from random import seed\nfrom random import randint\n\nvalid_id = list()\nseed(randint(1,10))\nfor i in range (0,90):\n    valid_id.append(randint(1,540))\n\nvalid_id = list(set(valid_id))\nvalid_id.sort()\n\n#note anyone running this notebook, my data will be different from yours, I have kept in mind to check the length of the valid_id set to be 15%(approx) of the total data ","9b02b93e":"TRAIN = data[~data['id'].isin(valid_id)]\nVALID = data[data['id'].isin(valid_id)]","994afe75":"TRAIN.feature_2.plot()","1ee6bd66":"VALID.shape[0]+TRAIN.shape[0]","b1a02cec":"plot_moving_average(data.feature_2,10)","24b9aa8e":"graph_2 = data[data.index > pd.to_datetime('2019-03-19 00:35:00')]\ngraph_2.feature_2.plot(legend=True)\ngraph_2['mean_pred']=graph_2.feature_2.mean() \ngraph_2.mean_pred.plot(legend=True)\n# graph_2['MA'] = graph_2.feature_2.rolling(window=30).mean()\n# graph_2.MA.plot(legend=True)","487c07ee":"data","fa1f7d42":"mean_squared_error(graph_2.feature_2,graph_2.mean_pred)**.5","2154ea12":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(graph_2[['id']],graph_2.feature_2)\n\nprint(mean_squared_error(graph_2.feature_2,lr.predict(graph_2[['id']]))**.5)\n\ngraph_2['LinearPrediction']=lr.predict(graph_2[['id']])\n","8c9c53f8":"graph_2.feature_2.mean()","732a3525":"graph_2.feature_2.plot(legend=True)\ngraph_2.LinearPrediction.plot(legend=True)","c942e171":"DATAPATH = '\/kaggle\/input\/into-the-future\/test.csv' \ntest = pd.read_csv(DATAPATH, index_col=['time'], parse_dates=['time'])\ntest['feature_2']=lr.predict(test[['id']])\ntest","720755ed":"test = test[['id','feature_2']]\ntest.set_index('id',inplace=True)\ntest.to_csv(\"Solution_LR_population.csv\")","af30e671":"train = TRAIN.copy()\nvalid = VALID.copy()\ntrain = train[1:]","5bc93381":"train['id_log']=np.log1p(train['id'])","9db7edd5":"lr_log2 = LinearRegression()\nlr_log2.fit(train[['id_log']],train['feature_2'].values)\n\ny_hat = lr_log2.predict(train[['id_log']])\ntrain['pred_f2'] = y_hat\ntrain['diff'] = train['pred_f2']-train['feature_2']","f51c5ff1":"mean_squared_error(train['feature_2'],train['pred_f2'])**0.5","105f7ef2":"train.reset_index() #redundent ??","11e5dd8a":"sns.regplot(data=train,x='id',y='diff')","7c2b7fd5":"train['diff'].mean()","43e498b3":"from sklearn.ensemble import RandomForestRegressor\n\nrf_diff = RandomForestRegressor().fit(train[['id']],train['diff']) \ntrain['pred_diff_rf'] = rf_diff.predict(train[['id']])","16c43d3b":"print(\"RMSE of RF on Diff of Seen Data : {}\".format(mean_squared_error(train['diff'],train['pred_diff_rf'])**0.5))","154418e1":"#log-LR prediction\nvalid['id_log']=np.log1p(valid['id'])\nvalid['pred_f2'] = lr_log2.predict(valid[['id_log']])\n#rf prediction\nvalid['pred_diff_rf'] = rf_diff.predict(valid[['id']])\nvalid['pred_f2_2nd_order_rf']=valid['pred_f2']+valid['pred_diff_rf'] #Mistake <- This will get clearer later! Just keep in mind. \"Mistake!\"","f6abd1a1":"print(\"RMSE of log-LR on feature_2 of 'UN-Seen' Data : {}\".format(mean_squared_error(valid['feature_2'],valid['pred_f2'])**0.5))","b52f0c86":"print(\"RMSE of log-LR + RF on feature_2 of 'UN-Seen' Data : {}\".format(mean_squared_error(valid['feature_2'],valid['pred_f2_2nd_order_rf'])**0.5))","af197796":"sns.scatterplot(data=train,x='id',y='diff')\nsns.scatterplot(data=train,x='id',y='pred_diff_rf')","8256e4ca":"from pylab import rcParams\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(train['diff'], model='additive', period=160)\nfig = decomposition.plot()\nplt.show()","cb9c82d3":"def sinusoid(x,A,offset,omega,phase):\n return A*np.sin(omega*x+phase) + offset\n\nT = 300\ndef get_p0(x, y):\n \n A0 = (max(y[0:T]) - min(y[0:T]))\/2\n offset0 = y[0]\n phase0 = 0\n omega0 = 2.*np.pi\/T\n return [A0, offset0,omega0, phase0]","ff478e15":"from scipy.optimize import curve_fit\n\nX = train['id']\ny = train['diff']\n\nimport math\nparam, covariance = curve_fit(sinusoid, X, y, p0=get_p0(X,y))\nplt.plot(X, y, color='red', linewidth=1,linestyle='dashed')\nplt.plot(X, sinusoid(X, *param), color='blue', linewidth=1)\nplt.show()","d1e5c58f":"# train.drop('pred_diff',inplace=True,axis=1)","b1a7a1e6":"X = train['id']\ny = train['diff']\n\nparam, covariance = curve_fit(sinusoid, X, y, p0=get_p0(X,y))\ntrain['pred_diff'] = sinusoid(X, *param)","1f6e8ae5":"train['pred_diff_2nd_oreder_sin']=train['feature_2']+train['pred_diff']","de9ce891":"mean_squared_error(train['feature_2'],train['pred_diff_2nd_oreder_sin'])**0.5","f3faa60c":"valid.drop('pred_f2_2nd_order_rf',axis=1,inplace=True)\n# valid.drop('pred_diff',axis=1,inplace=True)","2dad3c70":"#on unseen data, see we train the model on tain data, predict using \n\nX = train['id']\ny = train['diff']\n\nparam, covariance = curve_fit(sinusoid, X, y, p0=get_p0(X,y)) #training\nvalid['pred_diff'] = sinusoid(valid['id'], *param) # predicting","70df8741":"valid['pred_diff_2nd_order_sin']=valid['pred_f2']-valid['pred_diff']","799f7e5c":"mean_squared_error(valid['feature_2'],valid['pred_diff_2nd_order_sin'])**0.5","1ba38b74":"valid['diff']=valid['pred_f2']-valid['feature_2']","8f057a3b":"sns.scatterplot(data=valid,x='id',y='diff',legend='brief')\nsns.scatterplot(data=valid,x='id',y='pred_diff',legend='full')\n\n#idk why there are no legends I am tired, so I will forgo this for now. I can understand the data... I am just happy to be able get rmse of 200 here... :)","7bf7c46a":"valid['pred_diff_rf'] = rf_diff.predict(valid[['id']])\nvalid['pred_f2_2nd_order_rf']=valid['pred_f2'] - valid['pred_diff_rf'] #Mistake <- This will get clearer later! Just keep in mind. \"Mistake!\"","fb768538":"mean_squared_error(valid['feature_2'],valid['pred_f2_2nd_order_rf'])**0.5","83db02b3":"#data from the log model\nlog_check.head()","3cfef7ea":"#id : 564-938\nsol = pd.DataFrame()\nsol['id']= [i for i in range(564,939)]\nsol['id_log']= np.log1p(sol['id'])\n\n\n#Sin predictor of the difference\nX = log_check['id']\ny = log_check['diff']\nparam, covariance = curve_fit(sinusoid, X, y, p0=get_p0(X,y)) #training\n\n\n#finally\nsol['feature_2']= lr_log.predict(sol[['id_log']]) + sinusoid(sol['id'], *param) ","04c8cfd0":"sol = sol[['id','feature_2']]\nsol.set_index('id',inplace=True)\nsol.to_csv(\"solution_log_sin.csv\")","3976bea3":"#id : 564-938\nsol = pd.DataFrame()\nsol['id']= [i for i in range(564,939)]\nsol['id_log']= np.log1p(sol['id'])\n\n\n#RF predictor of the difference\nrf_diff = RandomForestRegressor().fit(log_check[['id']],log_check['diff']) \n\n\n#finally\nsol['feature_2']= lr_log.predict(sol[['id_log']]) + rf_diff.predict(sol[['id']]) ","287bc35d":"sol = sol[['id','feature_2']]\nsol.set_index('id',inplace=True)\nsol.to_csv(\"solution_log_RF.csv\")","07fc2cdc":"log_check['ans']= log_check['LogPredict'] - sinusoid(log_check['id'], *param)\nlog_check['sin_diff']=sinusoid(log_check['id'], *param)","29bc5d5f":"OHHH HOLLLYY !!!","89e8b1f6":"This notebook will flow in the order of the models I made, so as to give a clearer picture of the though process. \n\nPS : I had a ensemble model of (SARIMAX + (liear regression of feature_2 based on feature_1)). That I have not added to the notebook as it was exploratory. But it gives a notion as to where this notebooks starts from. \n\nWhat I observed from plottig the data was that it was showing logarithmic tendencies. But since it has had a time association with the reading, I was decided to begin with time series analysis. ","b0ad2fde":"### Public Test Data ","e213b8f9":"### Conclusion for this model \nI am getting a score of 600 with simple MA\n\n\nI am pretty With Auto regression component & proper singularity we can achieve much better results","edca8318":"Since even af the flatterning we can see a small amount of slope, I am planning to perform linear regresion and use that as a predictor","efb55cc2":"## REFRENCES\n\nTime Series (general)\n\n* https:\/\/towardsdatascience.com\/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775\n* https:\/\/towardsdatascience.com\/the-basics-time-series-and-seasonal-decomposition-b39fef4aa976\n* https:\/\/towardsdatascience.com\/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b\n* https:\/\/www.kaggle.com\/sumi25\/understand-arima-and-tune-p-d-q **[Best material to understand stationarity]**\n\n\n\nFor fbprophet\n\n* https:\/\/facebook.github.io\/prophet\/docs\/trend_changepoints.html#automatic-changepoint-detection-in-prophet\n* https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/generate-accurate-forecasts-facebook-prophet-python-r\/\n\nFor Sinusoidal Graph\n\n* https:\/\/towardsdatascience.com\/how-to-predict-a-variable-sinusoid-in-python-4a21404dc061","9891d884":"Unexpected, I was expecting better results\n* 243 : score on Log + LR + RF\n* 475 : score on Log + LR + Sin ","b2ce6fbf":"#### Performing on Unseen data","59dd2243":"### A. Random Forest ","126c9b27":"### Predicted readings from the ACF and PACF graphs\n\n* p-val = 0-2\n* d = 1 or 2\n* q = 0-2","a224b96c":"### The plan is to get a linear prediction after logrithmic transfromation, then find the difference between the prediction and the actual value, and check for seasonality ! If there is Seasonality ","2f7f6c89":"# 4. Solving using fbprohet","06c73a0f":"#### Sin Model","d3bb6d23":"A deep dive inspection shows :\n* Good slope that until the 47th minute \n* Post which we see a flattening trend but more undulations \n* Predicting the undulation might be difficult \n* But we can asume due to logarithmic curve it will be close to linearity","cdc630cf":"## Cleaning data ","3fc97db2":"Doesnt look like a good linear plot after log check, but lets try","d6135364":"Quenstion needs answering\n\nWith increase in seasonality there is an increase in residuals also... so there can be increasing seasonality doesnt reduce reseduals... this seasonality is being extracted at the cost of residual ?","9e3ac54f":"# 5. Using Logarithmic fit on the data as it resembles a logarithmic graph","e699a629":"# 8. logT + LR + Sin","61c43288":"### I decided to read up online, on good time-series models\n\nThings we were aware of : \n1. Primary challenege in ARIMA(p,d,q) is to determine p,d,q accuaretly. \n2. We can see there is no seasonality.\n3. I have got a score of ~600 (rmse) by using simple MA (10 days), tuning it for other lags would result in a slightly better score!\n4. Moreover, It can be drastically improved by including a Auto Regression component. \n5. For auto regression we need to get staionarity, removing of trend from the graph. \n5. From suggestions online, I have decided to go ahead with building an fbprophet model, as it is a self staionrizing model. plus p,d,q value is predicted by the package and incase I have made an error there, It will be highlighted. \n","28b420d9":"### NOTE : \nPS : With the outlier my ARIMA model was getting the AIC scores in the 8000s\n\nJust removing the first Data point lead my AIC score down to 6000s ","b2f85748":"### $diff = sin(Omega*id)$","f8311e6b":"### Moving average","2224a9ee":"I dont think we have enough data to resample. On top of that, resampling (by 30s) is reducing the number of data to perform AR and MA... means... after resampling...we can simply use rolling average as predictor... \n\nI will have good general accuracy, but will tend to higher RMSE compared to a more tuned ARIMA\n\nThus we shall not resample...","d3896f5b":"How to use this ? \n\n__When the target variable(feature_2) is a logarithmic distribution:__\n\nI log transformed the id so that the feature_2 is linearly related to the new value (id_log)\n\n\n\n__When the target variable(feature_2) is a logarithmic distribution:__\n\nhere I sin transform the id so that the difference is leanearly related to the new value (id_sin)","88d6589c":"nope this is not it ","a2b88ba2":"__Comparing the models__","4e8e5cda":"#### On seen data the RMSE :","9b608c7f":"We can observe a very low difference, between y and yhat","3dca2de3":"# 1. Lets Begin ","d62e8878":"We can take this graph and predict a lienear trend and assume it will follow the linear trend, as there is no way of predicting the undulation\/noise!","5c52d8e5":"This is not normal, sum of residuals is 0. This feels contrived\n\nMoreover it looks like a sin curve kind of a graph, I am going to brute code it ! ","4343a542":"305 is a drastic drop from 417 that we observed by RMSE, on seen data, by only using Log-LR\n\n","fd5949ea":"the training set dosent seem to be affected a lot, thanks to the randomization, therefore I will save this data, since the seed is different, It can lead to any result when run again, and I dont want that","ce9024a4":"This Notebook is Prepared by Samriddh Lakhmani : \n* Linked-In : <a href='https:\/\/www.linkedin.com\/in\/samriddh-lakhmani\/'> Click Here<\/a>\n* Link to Resume : <a href='https:\/\/drive.google.com\/file\/d\/1FkWkY74zbWmLKNaEhoC2sMxECrlB17C9\/view?usp=sharing'>Click Here<\/a>\n* My other work : <a href='https:\/\/github.com\/samlakhmani'>Click Here<\/a>","9fa4b9ab":"## log and a linear regression","4c2b29de":"# 7. Using population data and assuming the polulation will be flattening the curve","27b04823":"* No trend is observed in residuals plot\n* It shows a good sort of normality, it has peakesness, and base is a little wanky, but!!\n* It \n\nConclusion : There is no problem with the ARIMA model, but on the public score, I got a bad RMSE. \nThere is a limitations in using ARIMA as a predictor model. ","bd2b972e":"Removing the first observation as it is seems a outlie. Could be live data and a result of fluctuation due to starting?","5b1ef9cf":"# 2. ARIMA","a51dee84":"I think I have gone into unchartered terretory (by me) ! \n\nSee for seen data this regressor makes prediction like high accuracy(rmse) we can see in the graph, but when it comes to unseen data poof! \nMy speculation :\nI think the data points are low in number for random forest!","8b439897":"### Speculation that caused me to check fbprophet\n\nBefore I go ahead I need to add another speculation, I am again not entirely sure of this, need to check this. I am speculating that the ARIMA model can remove trend only whent the trend is linear, since \"I\" component of ARIMA used an diffentiating component 'd'\n\nAgain this need to be checked and if anyone knows the answers please feel free to comment. I am going to dive into the depth of derivation of ARIMA to determine if I am true on not. Understand how the model tranforms the data to stationarity. \n\nI am 100% sure that autoregression has the same limitations as linear modeling \n\nPS:\nStationarity doesnt only mean removing the trend, it also means removing co-varience and homeoskedacity..etc ","a70eed21":"As logarithmic curves tend to flatten in the end maybe I can just check the trend of the flattening part and assume the poluation data is flattening. ","6c31a3a3":"### Solving for the error","b905821f":"#### RF model","f36feee6":"# Introduction","6b46df7f":"I am thinking what if we use this as the predictor line! ","f9a97b50":"### I just realized I made a \"Mistake\" I '+' the difference term for thr random forest model and went on!! SHHHIIII***","6494d087":"### Conclusion Left","49929a6c":"### Thoughts on what to do next \n\ntime series has its limitations as SARIMAX AIC score was 4000 and lead to a public (rmse) score of 1600s, ARIMA AIC score was 6000 and did not have yield better results. Compared to SARIMAX the AIC is higher, and we can see from the residual graphs that it is a well fit model.  Even fbprohet failed to get a score better than 900s. \n\nI am speculating, but asuming that these models can only pick up linear trends and cannot handle logarithmic trend. But then again I have spoken about this in the notebok too as I explain my observations.\n\n[Only easy way to confirm is I can fit time\/id and feature_2 into a linear model and predict values, if it yields results close to the prediction my speculations would be correct. But again due to limitations of time and the number of tests, I decided to indulge into it later.]\n\nComing back, I am Concluding that AR cannot predict the logarithmic relation, so I decided to use a logarithmic transformation and use LR to predict it myself.\n","ed07d710":"Woaaaaaaahhhhh.... !! \nRMSE : 55\n\nWait this is over fitting, I just checked the graph of the actual 'diff' vs the 'predicted diff' (shown above)\n\nanyways, I will use both the models to test and we will test model no. 7 tomorrow morning on the public test data!","9ab74883":"### Conclusion of the Log Tranform - LR Model \n\nThis model leads to an RMSE score of 225s in the public set.  ","d79ec357":"## Stationarity ","4b18a901":"# 6. Preparing a Validation set","d8ee40ae":"## Exploratory data analysis (EDA)","eab20709":"# 3. Making Prediction using mean of last 10 data points ","d497765f":"### Conclusion of fbprophet\nNow that fbprohet model leads to an RMSE score of 900s in the public set. I am not sure time series is that effective\n","f0536888":"Now coming to the Scores I got from the different models. And summing up my entire thought process in a gist here, before we dive into the notbook. \n\nRMSE Score on public data uploaded by the competition host : Model Desription\n\n__ALL MODEL SCORES__\n\n* __1154.30073__ : ensemble of SARIMAX + LR of feature_1 and feature_2 (I shall upload them on github, I dont mean to crowd this notebook!)\n* __1412.25521__ : Polynomial Regression : Linear Regression on feature_1 and feature_2, by polynimial transformation of the order 3.\n* __1602.09697__ : Only SARIMAX (AIC score of 4000s)\n* I dont submit ARIMA for public data estimation due to limitations challenge. I had last try before intial deadline. (AIC score of 6000s as seen in this notebook)\n* __596.78058__  : Using the mean of the last 10 data points as a perdiction for the next data point. \n\nThen I got inspired by the best score on the Leaderboard by Saurabh, who had used fbprohet, I choose to develop the model myself to check. Since the initial deadline was over, and I wanted to develop my skill on time-series:\n\n* __926.39330__  : using fbprohet. \n\nReason to go for Logrithmic transformation is given in the notebook\n\n* __225.78494__ : Logarithmic transformation\n* __243.19927__ : Logarithmic transformation and RandomForrest Regressor on the difference\/remaining\n* __475.38877__ : Logarithmic transformation and Sinusoidal on the difference\/remaining","cf219dd6":"### B. Time series decomposition","4493f746":"I have chosen the wrong model.. That is all !","2ec3fa16":"## Decomp to understand Seasonality","da7a131f":"__We check on Validation Set__","baf9bce0":"### Conclusion for ARIMA","5d3b41fb":"#### C. Sinusoidal ?","c6d72122":"Just for Exploration sake : \n\n1. I will use some Random forest regressor to determine this !\n1. I will use time series decomp. on this ","8b9f7d7c":"PS : the EDA, Stationarity & SARIMAX was copy pasted from __Marco Peixeiro's__ Medium article on Stock data. Refrenced Below \n\nthe ARIMAX model I developed editing the SARIMAX code. Moreover, Decomposition was added, which he ignored.\n\nOnly the code was taken to simple the process and reduce debugging time period and quickly itereate through parameters. I have given explanations as to my paramter selection throught the notebook.","8c749f78":"I am thinking of a few radicle tests on the data that is why I am building my own validation set "}}