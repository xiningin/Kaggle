{"cell_type":{"6ee505de":"code","66920f81":"code","972dfd2f":"code","9e81e611":"code","00c87f9c":"code","c4b4e047":"code","7ff928d2":"code","7f1fd4e8":"code","650857f8":"code","dd6c4eea":"code","cb0c04ee":"code","97b4708b":"code","dce7565e":"code","3f9e2c81":"code","09326b2a":"code","acc73112":"code","63c6f594":"code","c046193b":"code","641f8773":"code","6c5b646a":"code","ad4450db":"code","bf26ec61":"code","444e7fe8":"code","9695a548":"code","24a0cd07":"code","983893b5":"code","51d1223e":"markdown","622729ef":"markdown"},"source":{"6ee505de":"# Importing libraries\nimport os\nimport random\nimport gc\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, concatenate, BatchNormalization\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, LeakyReLU\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.utils import plot_model\nimport keras_tuner as kt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nwarnings.filterwarnings('ignore')","66920f81":"# Importing the training data\nTrain_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\nTrain_df.head()","972dfd2f":"# Importing the test data\nTest_df = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nTest_id = Test_df.Id.copy()\nTest_df.head()","9e81e611":"# Setting seeds\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\ntf.random.set_seed(seed)\n\n# Setting constants\nbatch_size = 32\nimage_size = 300\nchannels = 3\nshuffle_size = 1024 \n\n# Setting auto tune\nAUTOTUNE = tf.data.experimental.AUTOTUNE  ","00c87f9c":"# Mapping the images ID to the image paths\nTrain_df.Id = Train_df.Id.map(lambda x: '..\/input\/petfinder-pawpularity-score\/train\/' + x + '.jpg')\nTest_df.Id = Test_df.Id.map(lambda x: '..\/input\/petfinder-pawpularity-score\/test\/' + x + '.jpg')","c4b4e047":"# Defining functions to decode image paths and preprocess images \ndef read_img():\n    def img_to_array(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=channels)\n        image = tf.cast(image, tf.float32)\n        image = tf.image.resize(image, (image_size, image_size))\n        image = tf.keras.applications.efficientnet.preprocess_input(image)\n        return image\n    def mapping(path, struct_data, score):\n        return (img_to_array(path),struct_data), score\n    return mapping\n\ndef augment(data, score):\n    image, struct_data = data\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0.95, 1.05)\n    image = tf.image.random_contrast(image, 0.95, 1.05)\n    image = tf.image.random_brightness(image, 0.1)\n    return (image, struct_data), score\n\ndef preprocess(ds, batch_size, ds_type):\n    labeled_read_img = read_img()\n    ds = ds.map(labeled_read_img, num_parallel_calls=AUTOTUNE)\n    if ds_type=='train':\n        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n        ds = ds.shuffle(shuffle_size, reshuffle_each_iteration=True)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(AUTOTUNE)\n    return ds","7ff928d2":"def create_ds(df, ds_type):\n    ds = Dataset.from_tensor_slices((df['Id'].values,df.iloc[:,1:-1],df['Pawpularity'].values))\n    ds = preprocess(ds, batch_size, ds_type)\n    return ds","7f1fd4e8":"Test_ds = Dataset.from_tensor_slices((Test_df['Id'].values,Test_df.iloc[:,1:], np.multiply(Test_df.iloc[:,1].values,0)))\nTest_ds = preprocess(Test_ds, batch_size, 'test')","650857f8":"Train_ds_one = create_ds(Train_df.iloc[:9000], 'train')\nTrain_ds_two = create_ds(Train_df.iloc[:9000], 'train')\nVal_ds = create_ds(Train_df.iloc[9000:], 'train')\n\nTrain_ds = tf.data.Dataset.zip((Train_ds_one, Train_ds_two))","dd6c4eea":"# Mixup augmentation functions\ndef sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n    return gamma_1_sample \/ (gamma_1_sample + gamma_2_sample)\n\n\ndef mix_up(ds_one, ds_two, alpha=0.2):\n    # Unpack two datasets\n    (images_one, struct_one), labels_one = ds_one\n    (images_two, struct_two), labels_two = ds_two\n    batch_size = tf.shape(images_one)[0]\n\n    # Sample lambda and reshape it to do the mixup\n    l = sample_beta_distribution(batch_size, alpha, alpha)\n    i_l = tf.reshape(l, (batch_size, 1, 1, 1))\n    s_l = tf.reshape(l, (batch_size, 1))\n    y_l = tf.reshape(l, (batch_size, 1))\n\n    # Perform mixup on both images and labels by combining a pair of images\/labels\n    # (one from each dataset) into one image\/label\n    images = images_one * i_l + images_two * (1 - i_l)\n    struct = tf.cast(struct_one, dtype='float32') * s_l + tf.cast(struct_one, dtype='float32') * (1 - s_l)\n    labels = tf.math.multiply(tf.expand_dims(tf.cast(labels_one, dtype='float32'), axis=1), y_l) + \\\n             tf.math.multiply(tf.expand_dims(tf.cast(labels_two, dtype='float32'), axis=1), (1 - y_l))\n    return (images, struct), labels","cb0c04ee":"# create the new dataset using our mix_up mapping\nTrain_ds_mu = Train_ds.map(\n    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTOTUNE)","97b4708b":"# Let's preview 9 samples from the dataset\n(sample_images, sample_structs), sample_labels = next(iter(Train_ds_mu.take(1)))\nplt.figure(figsize=(10, 10))\nfor i, (image, label) in enumerate(zip(sample_images[:9], sample_labels[:9])):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image.numpy().astype(np.uint8))\n    ax.set_title(label.numpy().tolist())\n    plt.axis(\"off\")","dce7565e":"# Importing EfficientNetB3 pretrained model\nEffNetB3_path = \"..\/input\/efficientnetb3pretrained\/EfficientNetB3.h5\"\nEffB3 = tf.keras.models.load_model(EffNetB3_path)\nEffB3.load_weights('..\/input\/efficientnet-keras-noisystudent-weights-b0b7\/noisystudent\/noisy.student.notop-b3.h5')\nEffB3.trainable=False","3f9e2c81":"# Defining the neural network model used for hyperparameters optimization\ndef create_model_hp(hp):\n    Inp1 = Input(shape=(image_size,image_size,channels))\n    out1 = EffB3(Inp1)\n    out1 = GlobalAveragePooling2D()(out1)\n    out1 = BatchNormalization()(out1)\n    Eff_drop = hp.Float('eff_drop_rate', min_value=0.1, max_value=0.5, step=0.05)\n    out1 = Dropout(Eff_drop)(out1)\n    hp_activation = 'selu'  # ReLU, ELU were tested in previous trials.\n    hp_units_eff = hp.Int('units_eff', min_value=4, max_value=32, step=4)\n    hp_layers_eff = hp.Int('layers_eff', min_value=1, max_value=3, step=1)\n    for l in range(hp_layers_eff):\n        out1 = Dense(hp_units_eff, activation=hp_activation, kernel_initializer='he_normal')(out1)\n\n    Inp2 = Input(shape=(12,))\n    hp_units_meta = hp.Int('units_meta', min_value=2, max_value=16, step=2)\n    hp_layers_meta = hp.Int('layers_meta', min_value=1, max_value=3, step=1)\n    for k in range(hp_layers_meta):\n        out2 = Dense(hp_units_meta, activation=hp_activation, kernel_initializer='he_normal')(Inp2)\n\n    out = concatenate([out1,out2], axis=1)\n    Top_drop = hp.Float('top_drop_rate', min_value=0.1, max_value=0.5, step=0.05)\n    out = Dropout(Top_drop)(out)\n    hp_units_top = hp.Int('units_top', min_value=4, max_value=32, step=4)\n    out = Dense(hp_units_top, activation=hp_activation, kernel_initializer='he_normal')(out)\n    out = Dense(1, activation='relu')(out)\n\n    PawModel = Model(inputs=[Inp1,Inp2], outputs=out)\n    \n    PawModel.compile(loss='mse', \n              optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),   # LR was chosen based on previous trials.\n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    return PawModel","09326b2a":"# setting up the keras-tuner\ntuner = kt.RandomSearch(create_model_hp,\n                     objective=kt.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                     max_trials=80,\n                     seed=42,\n                     directory='my_dir',\n                     project_name='KT_Paw')","acc73112":"tuner.search_space_summary(extended=True)","63c6f594":"# Running the hyperparameter tuning process\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, min_lr=1e-6)\n\ntuner.search(Train_ds_mu, validation_data=Val_ds, callbacks=[stop_early, reduce_lr])","c046193b":"best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\nbest_activation = 'selu'\nbest_LR = 0.001\nbest_drop_eff = best_hps.get('eff_drop_rate')\nbest_units_eff = best_hps.get('units_eff')\nbest_layers_eff = best_hps.get('layers_eff')\nbest_units_meta = best_hps.get('units_meta')\nbest_layers_meta = best_hps.get('layers_meta')\nbest_drop_top = best_hps.get('top_drop_rate')\nbest_units_top = best_hps.get('units_top')\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units for the dense layers are the following:\nEffB0 drop rate: {best_drop_eff}\nEffB0 number of layers: {best_layers_eff}\nEffB0 dense layers units: {best_units_eff}\nMeta number of layers: {best_layers_meta}\nMeta data dense layers units: {best_units_meta}\nTop drop rate: {best_drop_top}\nTop dense layer: {best_units_top}\\n\nThe optimal activation function is {best_activation} and the optimal learning rate is {best_LR}.\n\"\"\")\n","641f8773":"# Freeing up memory\ndel Train_ds, Train_ds_mu, Val_ds, Train_ds_one, Train_ds_two\ngc.collect()","6c5b646a":"# Defining the neural network model for cross-validation\ndef create_model_cv():\n    Inp1 = Input(shape=(image_size,image_size,channels))\n    out1 = EffB3(Inp1)\n    out1 = GlobalAveragePooling2D()(out1)\n    out1 = BatchNormalization()(out1)\n    out1 = Dropout(0.2)(out1)\n    for l in range(best_layers_eff):\n        out1 = Dense(best_units_eff , activation=best_activation, kernel_initializer='he_normal')(out1)\n\n    Inp2 = Input(shape=(12,))\n    for l in range(best_layers_meta):\n        out2 = Dense(best_units_meta, activation=best_activation, kernel_initializer='he_normal')(Inp2)\n\n    out = concatenate([out1,out2], axis=1)\n    out = Dense(best_units_top, activation=best_activation, kernel_initializer='he_normal')(out)\n    out = Dense(1, activation='relu')(out)\n\n    PawModel = Model(inputs=[Inp1,Inp2], outputs=out)\n    \n    PawModel.compile(loss='mse', \n              optimizer = tf.keras.optimizers.Adam(learning_rate=best_LR), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    \n    return PawModel","ad4450db":"# Visualizing the model architecture\nmod = create_model_cv()\nplot_model(mod, show_shapes=True)","bf26ec61":"# Custorm metric that prints the learning rate parameter used in the current epoch\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer._decayed_lr(tf.float32)\n    return lr","444e7fe8":"# Variables for the cross-validation trining process \ncounter=0\ntest_pred = np.zeros((Test_df.shape[0],))\nresults_list=[]\npred_list=[]\n\n# 7-fold Cross-validation loop\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\nfor idx, (train, val) in enumerate(kfold.split(X=Train_df.iloc[:,:-1], y=Train_df.iloc[:,-1])):\n    counter += 1\n    print(f'\\n Fold {counter}:\\n')\n\n    Train_fold = Train_df.iloc[train,:]\n    Val_fold = Train_df.iloc[val,:]\n    \n    Train_ds_one = create_ds(Train_fold, 'train')\n    Train_ds_two = create_ds(Train_fold, 'train')\n    Train_ds = tf.data.Dataset.zip((Train_ds_one, Train_ds_two))\n    Train_ds_mu = Train_ds.map(lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2),\\\n                               num_parallel_calls=AUTOTUNE)\n    Val_ds = create_ds(Val_fold, 'train')\n    \n    \n    model = create_model_cv()\n    \n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, min_lr=1e-6)\n    early_stopping = EarlyStopping(patience=7, restore_best_weights=True)\n    results = model.fit(Train_ds_mu,\n                      epochs=15,\n                      validation_data = Val_ds,\n                      callbacks=[early_stopping, reduce_lr], verbose=1)\n    results_list.append(results)\n    \n    # Prediction on test\n    pred_list.append(model.predict(Test_ds))\n    print('='*25)\n    \n    # Freeing up memory\n    del model, results\n    del Train_ds, Val_ds, Train_ds_one, Train_ds_two, Train_ds_mu\n    del Train_fold, Val_fold\n    gc.collect()","9695a548":"# Claculating average prediction on test\nAverage_pred = sum(pred_list)\/counter","24a0cd07":"# Creating the Submission file\nSubmission_df=pd.DataFrame()\nSubmission_df['Id']=Test_id\nSubmission_df['Pawpularity']=Average_pred\nSubmission_df.to_csv('submission.csv',index=False)","983893b5":"Submission_df","51d1223e":"**If you find this notebook useful, don't forget to upvote it!**","622729ef":"# Predicting Pawpularity using EfficientNet\n\nThe results that are publicly shared in this competition shows that the models which involves using only EfficientNets (No ViT, RAPIDS SVR, etc...) have achieved an average performance of 18.60 on the test set, and the best performance that I came across is of 18.56 accuracy, shared by [LEANDRO ROSER](https:\/\/www.kaggle.com\/leangab\/tf-pawpularity-efficientnet-metadata-ensamble).\n\nI am trying to achieve the best possible score using only EfficientNets as backbone of the model, and this notebook will be updated whenever a new score is acheived.\n\nThe model architecture:\n- EfficientNetB3 with noisy-student pre-trained weights.\n- Fully connected layer for meta data.\n- Fully connected head.\n \nTools used to optimize the results:\n- Keras Tuner for hyperparameter optimization with Hyperband search.\n- 7-fold cross-validation.\n- Data Augmentation.\n\n**Note:** This notebook takes around 6 hours to fully run.\n\nIf you find this Notebook usefull feel free to share it.\n"}}