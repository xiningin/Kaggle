{"cell_type":{"6dea37b7":"code","a4936113":"code","6493dc7e":"code","239bee2f":"code","24857f01":"code","db0aad4c":"code","cd65fbf1":"code","255f368f":"code","c7e2ae04":"code","9c0dfab7":"code","4f567835":"code","6c058b4f":"code","52ab0d74":"code","acbc2d42":"code","110e8f19":"code","00f49ab9":"code","2009d2ac":"code","f2b9e004":"code","b4c61e51":"code","0c37569c":"code","fa91c65c":"code","ce820d10":"code","f44b162f":"code","39854a6c":"code","8dffd4a0":"code","68a329c0":"code","89d8cb08":"code","c300b895":"code","9b8af753":"code","c830b29a":"code","64282cce":"code","aaf6e878":"code","c327ed23":"code","b756109c":"code","ebd8874d":"code","b6a78169":"code","3f587dac":"code","2a2d4b79":"code","a8dedd6c":"code","0c08810f":"code","bfc0f47e":"code","daddf272":"code","6e470375":"code","e28ef298":"code","590adf6c":"code","fa88c000":"code","0c88b20a":"code","5d874f4c":"code","4efe7823":"code","c139ce46":"code","eaea363e":"code","c15debac":"code","3b4b7e43":"code","b5bd03fe":"code","2e114b9b":"code","7a57827e":"code","48b6a468":"code","1e530f56":"code","14450f17":"code","286959d8":"code","ef52f30d":"code","2e5025da":"code","d8470e7e":"code","989fc30e":"code","5c77f4b1":"code","9bc67dbf":"code","70ee764b":"code","9bd7b8f2":"code","d8cc2c75":"code","2ac6089d":"code","50891b93":"code","b465b757":"code","229b3607":"code","3a9f551c":"code","f199a694":"code","7a84bb48":"code","6ab1bebf":"code","c7a25e8e":"code","1638ec3b":"code","5de597ae":"code","79c023b0":"code","b75d55d2":"code","d7a63a21":"code","3c55a6e6":"code","8a68eb20":"code","886b4e67":"code","3548a978":"code","17ba87e9":"code","2b77aabd":"code","5eb950db":"code","60b097c1":"code","e649b2a3":"code","42cdd156":"code","c6ca4026":"code","eaa5573b":"code","74c55978":"code","8b2675a2":"code","d7d576f9":"markdown","29dfe40d":"markdown","7b6d0a72":"markdown","29a063fa":"markdown","cabf7033":"markdown","8d876f36":"markdown","94173a24":"markdown","1ca2e463":"markdown","b887b32d":"markdown","8b47a88b":"markdown","a9d431e8":"markdown","06937f54":"markdown","c430d00a":"markdown","ff61006a":"markdown","20c61092":"markdown","9e5b0b2f":"markdown","9b30fc12":"markdown","1266ca3b":"markdown","ae8d0770":"markdown","50fe94d8":"markdown","e42ae198":"markdown","aa3b86b2":"markdown","fb72b12e":"markdown","96a31233":"markdown","ff2de37e":"markdown","0d9fd44a":"markdown","697f559f":"markdown","c9c9070a":"markdown","0b6690f8":"markdown","3eeb74ec":"markdown","32f0971c":"markdown","5bdb6f4a":"markdown","2325373f":"markdown","3fcd55e9":"markdown","0b84522b":"markdown","22144032":"markdown","f8c6e349":"markdown","4f158e47":"markdown","5ac85813":"markdown","69dd498d":"markdown","2da85e5e":"markdown","e919c658":"markdown","d3e3d543":"markdown","d9a0c9f3":"markdown","782bac04":"markdown","7558ddd5":"markdown","1cf3d475":"markdown","8b96756e":"markdown","97e1ee97":"markdown","e68d8e5d":"markdown","f6990812":"markdown","928cb7d5":"markdown","91adfc05":"markdown","0e33b144":"markdown","71499c23":"markdown","f1e6ecd8":"markdown","1f4c9d45":"markdown","8add513f":"markdown","c60c9b7c":"markdown","f34a01b8":"markdown","9c00f860":"markdown","2beec176":"markdown","2582814e":"markdown","4909614a":"markdown","c3c93bb2":"markdown","7fdce22c":"markdown","c275460f":"markdown","c87fce55":"markdown","495e982c":"markdown","533f4df6":"markdown","4844baa3":"markdown","5d160b40":"markdown","e3c3a63f":"markdown","3b970143":"markdown","98d98cff":"markdown","b520fa29":"markdown","01757b6d":"markdown","47fcbbd7":"markdown","271adb3f":"markdown","23208b1b":"markdown","84e0aefb":"markdown","8698568f":"markdown","298af3cc":"markdown","954c2b08":"markdown","2597ebf3":"markdown","f6f590da":"markdown","aafed424":"markdown","7d79eff2":"markdown","acec2574":"markdown","13a6a5a8":"markdown","13773c60":"markdown","6929560c":"markdown","a3a0f128":"markdown","8cf751ab":"markdown","2ac99aae":"markdown","e427d711":"markdown","a71192cf":"markdown","9cb1dbf2":"markdown","79e9148d":"markdown","8b9d09c4":"markdown","0ae66437":"markdown","aa267494":"markdown","2ae6cd7c":"markdown","67242f78":"markdown","cf7f4b12":"markdown","3559c334":"markdown","7b28b5ee":"markdown","13da5ba0":"markdown","138bf8cc":"markdown","bb090860":"markdown"},"source":{"6dea37b7":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime, nltk, warnings\nimport matplotlib.cm as cm\nimport itertools\nfrom pathlib import Path\n#from kmodes import kmodes\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn import preprocessing, model_selection, metrics, feature_selection\nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, linear_model, svm, tree, ensemble\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.decomposition import PCA\nfrom IPython.display import display, HTML\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams[\"patch.force_edgecolor\"] = True\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n%matplotlib inline","a4936113":"#__________________\n# read the datafile\ndf_initial = pd.read_excel('..\/input\/Online_Retail.xlsx',dtype={'CustomerID': str,'InvoiceID': str}).sample(100000, \n                                                                                                   random_state=44)\nprint('Dataframe dimensions:', df_initial.shape)\n#______\ndf_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n#____________________________________________________________\n# gives some infos on columns types and numer of null values\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()\/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)\n#__________________\n# show first lines\ndisplay(df_initial[:5])","6493dc7e":"df_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\nprint('Dataframe dimensions:', df_initial.shape)\n#____________________________________________________________\n# gives some infos on columns types and numer of null values\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()\/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)","239bee2f":"print('Entr\u00e9es dupliqu\u00e9es: {}'.format(df_initial.duplicated().sum()))\ndf_initial.drop_duplicates(inplace = True)","24857f01":"test = df_initial['Country'].value_counts()\nprint('Nb. de pays dans le dataframe: {}'.format(len(test)))\n#_________________________________\nfig = plt.figure(figsize = (13,5))\nx_axis = range(len(test))\nplt.bar(x_axis, np.array(test.values), align = 'center', log = True)\nx_label = np.array(test.index)\nplt.xticks(x_axis, x_label, ha = 'right', fontsize = 14)\nplt.yticks(fontsize = 15)\nplt.ylabel(\"number of ...\", fontsize = 18, labelpad = 10)\nplt.xticks(rotation=80);","db0aad4c":"pd.DataFrame([{'products': len(df_initial['StockCode'].value_counts()),    \n               'transactions': len(df_initial['InvoiceNo'].value_counts()),\n               'customers': len(df_initial['CustomerID'].value_counts()),  \n              }], columns = ['products', 'transactions', 'customers'], index = ['quantity'])","cd65fbf1":"temp = df_initial.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\nnb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\nnb_products_per_basket[:10].sort_values('CustomerID')","255f368f":"nb_products_per_basket['order_canceled'] = nb_products_per_basket['InvoiceNo'].astype(str).apply(lambda x:int('C' in x))\ndisplay(nb_products_per_basket[:5])\n#______________________________________________________________________________________________\nn1 = nb_products_per_basket['order_canceled'].sum()\nn2 = nb_products_per_basket.shape[0]\nprint('Number of orders cancelled: {}\/{} ({:.2f}%) '.format(n1, n2, n1\/n2*100))","c7e2ae04":"display(df_initial.sort_values('CustomerID')[:5])","9c0dfab7":"df_check = df_initial[df_initial['Quantity'] < 0][['CustomerID', 'Quantity', 'StockCode', 'Description', 'UnitPrice']]\nfor index, col in  df_check.iterrows():\n    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n                & (df_initial['Description'] == col[2])].shape[0] == 0: \n        print(df_check.loc[index])\n        print('Hypoth\u00e8se non valide')\n        break","4f567835":"df_check = df_initial[(df_initial['Quantity'] < 0) & (df_initial['Description'] != 'Discount')][\n                                 ['CustomerID', 'Quantity', 'StockCode', 'Description', 'UnitPrice']]\n\nfor index, col in  df_check.iterrows():\n    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n                & (df_initial['Description'] == col[2])].shape[0] == 0: \n        print(index, df_check.loc[index])\n        print('Hypoth\u00e8se non valide')\n        break","6c058b4f":"df_cleaned = df_initial.copy(deep = True)\ndf_cleaned['QuantityCanceled'] = 0\n\nentry_to_remove = [] ; doubtfull_entry = []\n\nfor index, col in  df_initial.iterrows():\n#    if index%1000 == 0: print(index)\n    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n    df_test = df_initial[(df_initial['CustomerID'] == col['CustomerID']) &\n                         (df_initial['StockCode']  == col['StockCode']) & \n                         (df_initial['InvoiceDate'] < col['InvoiceDate']) & \n                         (df_initial['Quantity']   > 0)].copy()\n    #___________________________________________________\n    # annulation sans contrepartie au niveau des achats\n    if (df_test.shape[0] == 0): \n        doubtfull_entry.append(index)\n    #_____________________________________________________\n    # annulation avec 1 contrepartie au niveau des achats\n    elif (df_test.shape[0] == 1): \n        index_order = df_test.index[0]\n        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n        entry_to_remove.append(index)        \n    #_______________________________________________________________________________\n    # il existe plusieurs contreparties au niveau des achats: on annule la derni\u00e8re\n    elif (df_test.shape[0] > 1): \n        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n        for ind, val in df_test.iterrows():\n            if val['Quantity'] < -col['Quantity']: continue\n            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n            entry_to_remove.append(index) \n            break            ","52ab0d74":"print(len(entry_to_remove), len(doubtfull_entry))","acbc2d42":"df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\ndf_cleaned.drop(doubtfull_entry, axis = 0, inplace = True)\nremaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\nprint(\"nb d'entr\u00e9es non supprim\u00e9es: {}\".format(remaining_entries.shape[0]))\nremaining_entries[:5]","110e8f19":"df_cleaned[(df_cleaned['CustomerID'] == 14048) & (df_cleaned['StockCode'] == '22464')]","00f49ab9":"list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+',na=True, regex=True)]['StockCode'].unique()\nlist_special_codes","2009d2ac":"for code in list_special_codes:\n    print(\"{:<15} -> {:<30}\".format(code, df_cleaned[df_cleaned['StockCode'] == code]['Description'].unique()[0]))","f2b9e004":"df_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\ndf_cleaned.sort_values('CustomerID')[:5]","b4c61e51":"#___________________________________________\n# somme des achats \/ utilisateur & commande\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n#_____________________\n# date de la commande\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n#______________________________________\n# selection des entr\u00e9es significatives:\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID')[:6]","0c37569c":"#____________________\n# D\u00e9compte des achats\nprice_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\ncount_price = []\nfor i, price in enumerate(price_range):\n    if i == 0: continue\n    val = basket_price[(basket_price['Basket Price'] < price) &\n                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n    count_price.append(val)\n#____________________________________________\n# Repr\u00e9sentation du nombre d'achats \/ montant        \nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(11, 6))\ncolors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']\nlabels = [ '{}<.<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]\nsizes  = count_price\nexplode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]\nax.pie(sizes, explode = explode, labels=labels, colors = colors,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow = False, startangle=0)\nax.axis('equal')\nf.text(0.5, 1.01, \"R\u00e9partition des montants des commandes\", ha='center', fontsize = 18);","fa91c65c":"nb_transactions_per_user = basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean'])\nnb_transactions_per_user.reset_index(drop = False, inplace = True)\nnb_transactions_per_user.sort_values('CustomerID', ascending = True)[:5]","ce820d10":"is_noun = lambda pos: pos[:2] == 'NN'\n\ndef keywords_inventory(dataframe, colonne = 'Description'):\n    stemmer = nltk.stem.SnowballStemmer(\"english\")\n    keywords_roots  = dict()  # collect the words \/ root\n    keywords_select = dict()  # association: root <-> keyword\n    category_keys   = []\n    count_keywords  = dict()\n    icount = 0\n    for s in dataframe[colonne]:\n        if pd.isnull(s): continue\n        lines = str(s).lower()\n        tokenized = nltk.word_tokenize(lines)\n        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n        \n        for t in nouns:\n            t = t.lower() ; racine = stemmer.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n                count_keywords[racine] += 1                \n            else:\n                keywords_roots[racine] = {t}\n                count_keywords[racine] = 1\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]\n                   \n    print(\"Nb of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n    return category_keys, keywords_roots, keywords_select, count_keywords","f44b162f":"df_produits = pd.DataFrame(df_initial['Description'].unique()).rename(columns = {0:'Description'})","39854a6c":"keywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_produits)","8dffd4a0":"list_products = []\nfor k,v in count_keywords.items():\n    list_products.append([keywords_select[k],v])\nlist_products.sort(key = lambda x:x[1], reverse = True)","68a329c0":"liste = sorted(list_products, key = lambda x:x[1], reverse = True)\n#_______________________________\nplt.rc('font', weight='normal')\nfig, ax = plt.subplots(figsize=(7, 25))\ny_axis = [i[1] for i in liste[:125]]\nx_axis = [k for k,i in enumerate(liste[:125])]\nx_label = [i[0] for i in liste[:125]]\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 13)\nplt.yticks(x_axis, x_label)\nplt.xlabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax.barh(x_axis, y_axis, align = 'center')\nax = plt.gca()\nax.invert_yaxis()\n#_______________________________________________________________________________________\nplt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='w',fontsize = 25)\nplt.show()","89d8cb08":"list_products = []\nfor k,v in count_keywords.items():\n    word = keywords_select[k]\n    if word in ['pink', 'blue', 'tag', 'green', 'orange']: continue\n    if len(word) < 3 or v < 13: continue\n    if ('+' in word) or ('\/' in word): continue\n    list_products.append([word, v])\n#______________________________________________________    \nlist_products.sort(key = lambda x:x[1], reverse = True)\nprint('mots conserv\u00e9s:', len(list_products))","c300b895":"liste_produits = df_cleaned['Description'].unique()\nX = pd.DataFrame()\nfor key, occurence in list_products:\n    X.loc[:, key] = list(map(lambda x:int(key.upper() in x), liste_produits.astype(str)))","9b8af753":"threshold = [0, 1, 2, 3, 5, 10]\nlabel_col = []\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])\n    label_col.append(col)\n    X.loc[:, col] = 0\n\nfor i, prod in enumerate(liste_produits):\n    prix = df_cleaned[ df_cleaned['Description'] == prod]['UnitPrice'].mean()\n    j = 0\n    while prix > threshold[j]:\n        j+=1\n        if j == len(threshold): break\n    X.loc[i, label_col[j-1]] = 1","c830b29a":"print(\"{:<8} {:<20} \\n\".format('gamme', 'nb. produits') + 20*'-')\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])    \n    print(\"{:<10}  {:<20}\".format(col, X.loc[:, col].sum()))","64282cce":"matrix = X.as_matrix()\nfor n_clusters in range(3,10):\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","aaf6e878":"#matrix = X.as_matrix()\n#_____________________________________________\n# D\u00e9termination de nombre optimal de clusters\n#for n_clusters in range(3,10):\n#    km = kmodes.KModes(n_clusters = n_clusters, init='Huang', n_init=3, verbose=0)\n#    clusters = km.fit_predict(matrix)\n#    silhouette_avg = silhouette_score(matrix, clusters)\n#    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","c327ed23":"n_clusters = 5\nsilhouette_avg = -1\nwhile silhouette_avg < 0.145:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)\n    \n    #km = kmodes.KModes(n_clusters = n_clusters, init='Huang', n_init=2, verbose=0)\n    #clusters = km.fit_predict(matrix)\n    #silhouette_avg = silhouette_score(matrix, clusters)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","b756109c":"pd.Series(clusters).value_counts()","ebd8874d":"def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n    plt.rcParams[\"patch.force_edgecolor\"] = True\n    plt.style.use('fivethirtyeight')\n    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n    #____________________________\n    fig, ax1 = plt.subplots(1, 1)\n    fig.set_size_inches(8, 8)\n    ax1.set_xlim([lim_x[0], lim_x[1]])\n    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n    y_lower = 10\n    for i in range(n_clusters):\n        #___________________________________________________________________________________\n        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        cmap = cm.get_cmap(\"Spectral\")\n        color = cmap(float(i) \/ n_clusters)\n        #color = plt.cm.spectral(float(i) \/ n_clusters)        \n        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n                           facecolor=color, edgecolor=color, alpha=0.8)\n        #____________________________________________________________________\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n        #______________________________________\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  ","b6a78169":"#____________________________________\n# define individual silouhette scores\nsample_silhouette_values = silhouette_samples(matrix, clusters)\n#__________________\n# and do the graph\ngraph_component_silhouette(n_clusters, [-0.07, 0.33], len(X), sample_silhouette_values, clusters)","3f587dac":"pca = PCA()\npca.fit(matrix)\npca_samples = pca.transform(matrix)","2a2d4b79":"fig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n            label='individual explained variance')\nplt.xlim(0, 100)\n\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='upper left', fontsize = 13);","a8dedd6c":"pca = PCA(n_components=50)\nmatrix_9D = pca.fit_transform(matrix)\nmat = pd.DataFrame(matrix_9D)\nmat['cluster'] = pd.Series(clusters)","0c08810f":"import matplotlib.patches as mpatches\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'gold', 2:'b', 3:'k', 4:'c', 5:'g'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (10,20))\nincrement = 0\nfor ix in range(4):\n    for iy in range(ix+1, 4):    \n        increment += 1\n        ax = fig.add_subplot(3,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.4) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray',linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 9: break\n    if increment == 9: break\n        \n#_______________________________________________\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(5):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.97), \n           title='Cluster', facecolor = 'lightgrey',\n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n\nplt.tight_layout()","bfc0f47e":"corresp = dict()\nfor key, val in zip (liste_produits, clusters):\n    corresp[key] = val \n#__________________________________________________________________________\ndf_cleaned['categ_product'] = df_cleaned.loc[:, 'Description'].map(corresp)","daddf272":"for i in range(5):\n    col = 'categ_{}'.format(i)        \n    df_temp = df_cleaned[df_cleaned['categ_product'] == i]\n    price_temp = df_temp['UnitPrice'] * (df_temp['Quantity'] - df_temp['QuantityCanceled'])\n    price_temp = price_temp.apply(lambda x:x if x > 0 else 0)\n    df_cleaned.loc[:, col] = price_temp\n    df_cleaned[col].fillna(0, inplace = True)\n#__________________________________________________________________________________________________\ndf_cleaned[['InvoiceNo', 'Description', 'categ_product', 'categ_0', 'categ_1', 'categ_2', 'categ_3','categ_4']][:5]","6e470375":"#___________________________________________\n# somme des achats \/ utilisateur & commande\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n#____________________________________________________________\n# pourcentage du prix de la commande \/ categorie de produit\nfor i in range(5):\n    col = 'categ_{}'.format(i) \n    temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)[col].sum()\n    basket_price.loc[:, col] = temp \n#_____________________\n# date de la commande\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n#______________________________________\n# selection des entr\u00e9es significatives:\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID', ascending = True)[:5]","e28ef298":"print(basket_price['InvoiceDate'].min(), '->',  basket_price['InvoiceDate'].max())","590adf6c":"set_entrainement = basket_price[basket_price['InvoiceDate'] < datetime.date(2011,10,1)]\nset_test         = basket_price[basket_price['InvoiceDate'] >= datetime.date(2011,10,1)]\nbasket_price = set_entrainement.copy(deep = True)","fa88c000":"#________________________________________________________________\n# nb de visites et stats sur le montant du panier \/ utilisateurs\ntransactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\nfor i in range(5):\n    col = 'categ_{}'.format(i)\n    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() \/\\\n                                            transactions_per_user['sum']*100\n\ntransactions_per_user.reset_index(drop = False, inplace = True)\nbasket_price.groupby(by=['CustomerID'])['categ_0'].sum()\ntransactions_per_user.sort_values('CustomerID', ascending = True)[:5]","0c88b20a":"last_date = basket_price['InvoiceDate'].max().date()\n\nfirst_registration = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].min())\nlast_purchase      = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].max())\n\ntest  = first_registration.applymap(lambda x:(last_date - x.date()).days)\ntest2 = last_purchase.applymap(lambda x:(last_date - x.date()).days)\n\ntransactions_per_user.loc[:, 'LastPurchase'] = test2.reset_index(drop = False)['InvoiceDate']\ntransactions_per_user.loc[:, 'FirstPurchase'] = test.reset_index(drop = False)['InvoiceDate']\n\ntransactions_per_user[:5]","5d874f4c":"n1 = transactions_per_user[transactions_per_user['count'] == 1].shape[0]\nn2 = transactions_per_user.shape[0]\nprint(\"nb. de clients avec achat unique: {:<2}\/{:<5} ({:<2.2f}%)\".format(n1,n2,n1\/n2*100))","4efe7823":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n#_____________________________________________________________\nselected_customers = transactions_per_user.copy(deep = True)\nmatrix = selected_customers[list_cols].as_matrix()","c139ce46":"scaler = StandardScaler()\nscaler.fit(matrix)\nprint('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\nscaled_matrix = scaler.transform(matrix)","eaea363e":"pca = PCA()\npca.fit(scaled_matrix)\npca_samples = pca.transform(scaled_matrix)","c15debac":"fig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n            label='individual explained variance')\nplt.xlim(0, 10)\n\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='best', fontsize = 13);","3b4b7e43":"n_clusters = 11\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=100)\nkmeans.fit(scaled_matrix)\nclusters_clients = kmeans.predict(scaled_matrix)\nsilhouette_avg = silhouette_score(scaled_matrix, clusters_clients)\nprint('score de silhouette: {:<.3f}'.format(silhouette_avg))","b5bd03fe":"pd.DataFrame(pd.Series(clusters_clients).value_counts(), columns = ['nb. de clients']).T","2e114b9b":"pca = PCA(n_components=6)\nmatrix_3D = pca.fit_transform(scaled_matrix)\nmat = pd.DataFrame(matrix_3D)\nmat['cluster'] = pd.Series(clusters_clients)","7a57827e":"import matplotlib.patches as mpatches\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'tan', 2:'b', 3:'k', 4:'c', 5:'g', 6:'deeppink', 7:'skyblue', 8:'darkcyan', 9:'orange',\n                   10:'yellow', 11:'tomato', 12:'seagreen'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (12,10))\nincrement = 0\nfor ix in range(6):\n    for iy in range(ix+1, 6):   \n        increment += 1\n        ax = fig.add_subplot(4,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.5) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 12: break\n    if increment == 12: break\n        \n#_______________________________________________\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(n_clusters):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.9), \n           title='Cluster', facecolor = 'lightgrey',\n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n\nplt.tight_layout()","48b6a468":"sample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#____________________________________\n# define individual silouhette scores\nsample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#__________________\n# and do the graph\ngraph_component_silhouette(n_clusters, [-0.15, 0.55], len(scaled_matrix), sample_silhouette_values, clusters_clients)","1e530f56":"selected_customers.loc[:, 'cluster'] = clusters_clients","14450f17":"merged_df = pd.DataFrame()\nfor i in range(n_clusters):\n    test = pd.DataFrame(selected_customers[selected_customers['cluster'] == i].mean())\n    test = test.T.set_index('cluster', drop = True)\n    test['size'] = selected_customers[selected_customers['cluster'] == i].shape[0]\n    merged_df = pd.concat([merged_df, test])\n#_____________________________________________________\nmerged_df.drop('CustomerID', axis = 1, inplace = True)\nprint('number of customers:', merged_df['size'].sum())\n\nmerged_df = merged_df.sort_values('sum')","286959d8":"liste_index = []\nfor i in range(5):\n    column = 'categ_{}'.format(i)\n    liste_index.append(merged_df[merged_df[column] > 45].index.values[0])\n#___________________________________\nliste_index_reordered = liste_index\nliste_index_reordered += [ s for s in merged_df.index if s not in liste_index]\n#___________________________________________________________\nmerged_df = merged_df.reindex(index = liste_index_reordered)\nmerged_df = merged_df.reset_index(drop = False)\ndisplay(merged_df[['cluster', 'count', 'min', 'max', 'mean', 'sum', 'categ_0',\n                   'categ_1', 'categ_2', 'categ_3', 'categ_4', 'size']])","ef52f30d":"def _scale_data(data, ranges):\n    (x1, x2) = ranges[0]\n    d = data[0]\n    return [(d - y1) \/ (y2 - y1) * (x2 - x1) + x1 for d, (y1, y2) in zip(data, ranges)]\n\nclass RadarChart():\n    def __init__(self, fig, location, sizes, variables, ranges, n_ordinate_levels = 6):\n\n        angles = np.arange(0, 360, 360.\/len(variables))\n\n        ix, iy = location[:] ; size_x, size_y = sizes[:]\n        \n        axes = [fig.add_axes([ix, iy, size_x, size_y], polar = True, \n        label = \"axes{}\".format(i)) for i in range(len(variables))]\n\n        _, text = axes[0].set_thetagrids(angles, labels = variables)\n        \n        for txt, angle in zip(text, angles):\n            if angle > -1 and angle < 181:\n                txt.set_rotation(angle - 90)\n            else:\n                txt.set_rotation(angle - 270)\n        \n        for ax in axes[1:]:\n            ax.patch.set_visible(False)\n            ax.xaxis.set_visible(False)\n            ax.grid(\"off\")\n        \n        for i, ax in enumerate(axes):\n            grid = np.linspace(*ranges[i],num = n_ordinate_levels)\n            grid_label = [\"\"]+[\"{:.0f}\".format(x) for x in grid[1:-1]]\n            ax.set_rgrids(grid, labels = grid_label, angle = angles[i])\n            ax.set_ylim(*ranges[i])\n        \n        self.angle = np.deg2rad(np.r_[angles, angles[0]])\n        self.ranges = ranges\n        self.ax = axes[0]\n                \n    def plot(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def fill(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def legend(self, *args, **kw):\n        self.ax.legend(*args, **kw)\n        \n    def title(self, title, *args, **kw):\n        self.ax.text(0.9, 1, title, transform = self.ax.transAxes, *args, **kw)\n","2e5025da":"fig = plt.figure(figsize=(10,12))\n\nattributes = ['count', 'mean', 'sum', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4']\nranges = [[0.01, 10], [0.01, 1500], [0.01, 10000], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75]]\nindex  = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n\nn_groups = n_clusters ; i_cols = 3\ni_rows = n_groups\/\/i_cols\nsize_x, size_y = (1\/i_cols), (1\/i_rows)\n\nfor ind in range(n_clusters):\n    ix = ind%3 ; iy = i_rows - ind\/\/3\n    pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n    location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n    #______________________________________________________\n    data = np.array(merged_df.loc[index[ind], attributes])    \n    radar = RadarChart(fig, location, sizes, attributes, ranges)\n    radar.plot(data, color = 'b', linewidth=2.0)\n    radar.fill(data, alpha = 0.2, color = 'b')\n    radar.title(title = 'cluster n\u00ba{}'.format(index[ind]), color = 'r')\n    ind += 1 ","d8470e7e":"class Class_Fit(object):\n    def __init__(self, clf, params=None):\n        if params:            \n            self.clf = clf(**params)\n        else:\n            self.clf = clf()\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def grid_search(self, parameters, Kfold):\n        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold)\n        \n    def grid_fit(self, X, Y):\n        self.grid.fit(X, Y)\n        \n    def grid_predict(self, X, Y):\n        self.predictions = self.grid.predict(X)\n        print(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, self.predictions)))\n        ","989fc30e":"columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = selected_customers[columns]\nY = selected_customers['cluster']","5c77f4b1":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, train_size = 0.8)","9bc67dbf":"svc = Class_Fit(clf = svm.LinearSVC)\nsvc.grid_search(parameters = [{'C':np.logspace(-2,2,10)}], Kfold = 5)","70ee764b":"svc.grid_fit(X = X_train, Y = Y_train)","9bd7b8f2":"svc.grid_predict(X_test, Y_test)","d8cc2c75":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    #_________________________________________________\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    #_________________________________________________\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    #_________________________________________________\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","2ac6089d":"class_names = [i for i in range(11)]\ncnf_matrix = confusion_matrix(Y_test, svc.predictions) \nnp.set_printoptions(precision=2)\nplt.figure(figsize = (8,8))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')","50891b93":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","b465b757":"g = plot_learning_curve(svc.grid.best_estimator_, \"SVC learning curves\", X_train, Y_train, ylim = [1.01, 0.6],\n                        cv = 5,  train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","229b3607":"lr = Class_Fit(clf = linear_model.LogisticRegression)\nlr.grid_search(parameters = [{'C':np.logspace(-2,2,20)}], Kfold = 5)\nlr.grid_fit(X = X_train, Y = Y_train)\nlr.grid_predict(X_test, Y_test)","3a9f551c":"g = plot_learning_curve(lr.grid.best_estimator_, \"Logistic Regression learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","f199a694":"knn = Class_Fit(clf = neighbors.KNeighborsClassifier)\nknn.grid_search(parameters = [{'n_neighbors': np.arange(1,50,1)}], Kfold = 5)\nknn.grid_fit(X = X_train, Y = Y_train)\nknn.grid_predict(X_test, Y_test)","7a84bb48":"g = plot_learning_curve(knn.grid.best_estimator_, \"Nearest Neighbors learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","6ab1bebf":"tr = Class_Fit(clf = tree.DecisionTreeClassifier)\ntr.grid_search(parameters = [{'criterion' : ['entropy', 'gini'], 'max_features' :['sqrt', 'log2']}], Kfold = 5)\ntr.grid_fit(X = X_train, Y = Y_train)\ntr.grid_predict(X_test, Y_test)","c7a25e8e":"g = plot_learning_curve(tr.grid.best_estimator_, \"Decision tree learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","1638ec3b":"rf = Class_Fit(clf = ensemble.RandomForestClassifier)\nparam_grid = {'criterion' : ['entropy', 'gini'], 'n_estimators' : [20, 40, 60, 80, 100],\n               'max_features' :['sqrt', 'log2']}\nrf.grid_search(parameters = param_grid, Kfold = 5)\nrf.grid_fit(X = X_train, Y = Y_train)\nrf.grid_predict(X_test, Y_test)","5de597ae":"rf.grid.best_params_","79c023b0":"g = plot_learning_curve(rf.grid.best_estimator_, \"Random Forest learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","b75d55d2":"from sklearn.ensemble import AdaBoostClassifier","d7a63a21":"ada = Class_Fit(clf = AdaBoostClassifier)\nparam_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\nada.grid_search(parameters = param_grid, Kfold = 5)\nada.grid_fit(X = X_train, Y = Y_train)\nada.grid_predict(X_test, Y_test)","3c55a6e6":"g = plot_learning_curve(ada.grid.best_estimator_, \"AdaBoost learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.4], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","8a68eb20":"gb = Class_Fit(clf = ensemble.GradientBoostingClassifier)\nparam_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\ngb.grid_search(parameters = param_grid, Kfold = 5)\ngb.grid_fit(X = X_train, Y = Y_train)\ngb.grid_predict(X_test, Y_test)","886b4e67":"g = plot_learning_curve(gb.grid.best_estimator_, \"Gradient Boosting learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","3548a978":"rf_best  = ensemble.RandomForestClassifier(**rf.grid.best_params_)\ngb_best  = ensemble.GradientBoostingClassifier(**gb.grid.best_params_)\nsvc_best = svm.LinearSVC(**svc.grid.best_params_)\ntr_best  = tree.DecisionTreeClassifier(**tr.grid.best_params_)\nknn_best = neighbors.KNeighborsClassifier(**knn.grid.best_params_)\nlr_best  = linear_model.LogisticRegression(**lr.grid.best_params_)","17ba87e9":"votingC = ensemble.VotingClassifier(estimators=[('rf', rf_best),('gb', gb_best),('knn', knn_best)], voting='soft')    ","2b77aabd":"votingC = votingC.fit(X_train, Y_train)","5eb950db":"predictions = votingC.predict(X_test)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y_test, predictions)))","60b097c1":"basket_price = set_test.copy(deep = True)","e649b2a3":"transactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\nfor i in range(5):\n    col = 'categ_{}'.format(i)\n    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() \/\\\n                                            transactions_per_user['sum']*100\n\ntransactions_per_user.reset_index(drop = False, inplace = True)\nbasket_price.groupby(by=['CustomerID'])['categ_0'].sum()\n\n#_________________________________________\n# Correction sur les diff\u00e9rences de dur\u00e9e\ntransactions_per_user['count'] = 5 * transactions_per_user['count']\ntransactions_per_user['sum']   = transactions_per_user['count'] * transactions_per_user['mean']\n\ntransactions_per_user.sort_values('CustomerID', ascending = True)[:5]","42cdd156":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n#_____________________________________________________________\nmatrix_test = transactions_per_user[list_cols].as_matrix()\nscaled_test_matrix = scaler.transform(matrix_test)","c6ca4026":"Y = kmeans.predict(scaled_test_matrix)","eaa5573b":"columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = transactions_per_user[columns]","74c55978":"classifiers = [(svc, 'Support Vector Machine'),\n                (lr, 'Logostic Regression'),\n                (knn, 'k-Nearest Neighbors'),\n                (tr, 'Decision Tree'),\n                (rf, 'Random Forest'),\n                (gb, 'Gradient Boosting')]\n#______________________________\nfor clf, label in classifiers:\n    print(30*'_', '\\n{}'.format(label))\n    clf.grid_predict(X, Y)","8b2675a2":"predictions = votingC.predict(X)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, predictions)))","d7d576f9":"___\n#### 4.2.2 Creating customer categories","29dfe40d":"The execution of this function returns three variables:\n- `keywords`: the list of extracted keywords\n- `keywords_roots`: a dictionary where the keys are the roots of the keywords and the values are list of words corresponding to the roots\n- `count_keywords`: dictionary listing the number of products where the different names appear\n\nAt this point, I turn the contents of the dictionary `count_keywords` into a list, so that you can sort the keywords according to their frequency:","7b6d0a72":"At this point, I define client clusters from the standardized matrix that was defined earlier and using the `k-means` algorithm from` scikit-learn`. I choose the number of clusters based on the silhouette score and I find that the best score is obtained by defining 11 clusters:","29a063fa":"The first step of the analysis is to retrieve the list of products present in the dataset:","cabf7033":"The classifiers are then defined:","8d876f36":"___\n## 4. Categories de consommateurs","94173a24":"Once this instance is created, I adjust the classifier to the training data:","1ca2e463":"**1. data preprocessing**\n\n**2. Exploration**\n\n   - 2.1 Countries\n   \n\u00a0\u00a0\u00a0- 2.2 Consumers and products\n   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 2.2.1 Cancelling orders\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 2.2.2 StockCode\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 2.2.3 Basket Price\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 2.2.4 Frequency of purchases\n\n**3. Indications on the type of products purchased**\n\n   - 3.1 Description of the products\n   \n\u00a0\u00a0\u00a0- 3.2 Definition of product categories\n   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 3.2.1 Data encoding\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 3.2.2 Creating product clusters\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 3.2.3 Characterization of the content of clusters\n        \n   \n**4. Categories of consumers**\n\n\n   - 4.1 Formatting data\n   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 4.1.1 Grouping products by order\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 4.1.2 Separation of data over time\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 4.1.3 Consumer Order Combinations\n        \n\u00a0\u00a0\u00a0- 4.2 Creating customer categories\n   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 4.2.1 Data encoding\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 4.2.2 Creating customer categories\n        \n\n**5. Classification of customers**\n\n\n   - 5.1 Support Vector Machine Classifier (SVC)\n   \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 5.1.1 Confusion matrix\n        \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 * 5.1.2 Learning curve\n        \n\u00a0\u00a0\u00a0- 5.2 Logistic Regression\n   \n\u00a0\u00a0\u00a0- 5.3 k-Nearest Neighbors\n   \n\u00a0\u00a0\u00a0- 5.4 Decision Tree\n   \n\u00a0\u00a0\u00a0- 5.5 Random Forest\n   \n\u00a0\u00a0\u00a0- 5.6 AdaBoost\n   \n\u00a0\u00a0\u00a0- 5.7 Gradient Boosting Classifier\n   \n\u00a0\u00a0\u00a0- 5.8 A democratic choice\n   \n   \n**6. Testing predictions**\n\n\n**7. Conclusion**","b887b32d":"In practice, the different variables that have been selected have quite different domains of variation and before continuing the analysis, I therefore create a matrix where these data will be standardized:","8b47a88b":"It can be seen that the data concern 4372 users and that they bought 3684 different products. The total number of transactions carried out is in the order of $\\sim$22'000, that is, on average, 5 transactions per customer per year.\n\nNow I will determine the number of products purchased in each transaction:","a9d431e8":"To the extent that the objective is to define the class to which a client belongs at the first visit, I only keep the variables that describe the content of the basket and do not take into account the variables related to the frequency of visits or changes in the time spent on money:","06937f54":"which makes it possible to create a representation of the most common keywords:","c430d00a":"In a first step, I regroup reformattes these data according to the same procedure as used on the training set. However, I am correcting the data to take into account the difference in time between the two datasets and weights the variables ** count ** and ** sum ** to obtain an equivalence with the training set:","ff61006a":"On this curve, it can be seen, for example, that the drive and cross-validation curves converge to the same value when the sample size increases. This is typical of modeling with low variance and proves that the model does not suffer from over-learning. Also, it can be seen that the accuracy of the drive curve is correct. At this level, good precision is synonymous with low bias, which is equivalent to a model that does not suffer from under-learning.\n\n___\n### 5.2 Logistic Regression\n\nI now look at the predictions obtained using a logistic regression. As before, I create an instance of the `Class_Fit` class, just the model on the training data and see how the predictions compare to the real values:","20c61092":"We note that the number of cancellations is consistent ($\\sim$16% of the total number of transactions).\nAs an illustration, let's look at the first lines of the dataframe:","9e5b0b2f":"If one looks, for example, at the purchases of the consumer of one of the above entries and corresponding to the same product as that of the cancellation, one observes:","9b30fc12":"I check for duplicate entries and delete them if necessary:","1266ca3b":"Then I draw the learning curve to make sure of the quality of fit:","ae8d0770":"###\u00a05.7 Gradient Boosting Classifier","50fe94d8":"The list that was obtained contains more than 1400 keywords of which the most frequent appear in more than 200 products. On the level of this list, however, I note that some of the names are abh\u00e9rent. Others are informative, like colors.\nI therefore dismis these words from the analysis that follows and moreover, I only consider the key words that appear more than 13 time","e42ae198":"Once this list is created, I use the function I previously defined in order to analyze the content of the descriptions of the different products:","aa3b86b2":"Among these entries, the lines listed in the * doubtfull_entry * list correspond to the entries indicating a cancellation but for which there is no command beforehand. In practice, I decide to delete all of these entries, which count respectively for  $\\sim$1.4% and 0.2% of the dataframe entries.","fb72b12e":"**a \/ _Report via the PCA_**\n\nThere is a certain disparity in the different groups that have been created, so I will now try to understand what each of these clusters is in order to validate (or not) the separation that has been made. At first, I use the result of the PCA:","96a31233":"It is found that the number of components required to explain the data set is extremely important: it is necessary to use more than 100 main components to explain 90% of the variance of the data. In practice, I decide to keep only a limited number of components insofar as this decomposition is only intended to visualize the data:","ff2de37e":"___\n###\u00a02.2 Customers and products","0d9fd44a":"which allows me to represent the learning curve of the SVC classifier:","697f559f":"The number of entries to delete in the dataframe is:","c9c9070a":"**InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.  <br>\n**StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. <br>\n**Description**: Product (item) name. Nominal. <br>\n**Quantity**: The quantities of each product (item) per transaction. Numeric.\t<br>\n**InvoiceDate**: Invice Date and time. Numeric, the day and time when each transaction was generated. <br>\n**UnitPrice**: Unit price. Numeric, Product price per unit in sterling. <br>\n**CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. <br>\n**Country**: Country name. Nominal, the name of the country where each customer resides.<br>","0b6690f8":"There are several types of peculiar transactions, connected e.g. to port charges or bank charges.\n\n\n___\n#### 2.2.3 Basket Price","3eeb74ec":"Now I check the number of entries that correspond to cancellations and that have not been deleted with the previous filter:","32f0971c":"____\n#### 3.2.1 Data encoding\n\nNow I will use these keywords to create product groups. Firstly,\nI create the matrix $X$ according to the schema:\n    \n|   | word 1  |  ...  | word j  | ...  |  N  |\n|:-:|---|---|---|---|---|\n| product 1  | $a_{1,1}$  |     |   |   | $a_{1,N}$  |\n| ...        |            |     | ...  |   |   |\n|product i   |    ...     |     | $a_{i,j}$    |   | ...  |\n|...         |            |     |  ... |   |   |\n| product M  | $a_{M,1}$  |     |   |   | $a_{M,N}$   |\n\nwhere the coefficient $ a_ {i, j} $ is 1 if the description of the product n $ $ i $ contains the word n $ $ j $, and 0 otherwise:","5bdb6f4a":"I check the number of elements in each class:","2325373f":"___\n### 5.8 A democratic choice\n\nFinally, the results of the different classifiers presented in the previous sections can be combined to improve the results of the classification model. A simple way to do this is to choose the most likely category for the clients based on the set of results provided by the different classifiers. To do this, I use the `VotingClassifier` method of` sklearn` package set. In a first step, I set the parameters of the different classifiers using the parameters that have been defined previously:","3fcd55e9":"The work described in this notebook is based on a database providing details on purchases made on an e-commerce platform over a period of one year. Each entry in the dataset describes the purchase of a product, by a particular customer and on a given date. In total, approximately $ \\ sim $ 4000 clients appear at the database level. Given the available information, I decided to develop a classifier that allows to anticipate the type of purchase that a customer will make, as well as the number of visits that he will make during a year, and this from its first visit to the e-commerce site.\n\nThe first stage of this work consisted in describing the different products sold by the site, which was the subject of a first classification. During this stage, I grouped the different products into 5 main categories of goods. In a second step, I performed a classification of the customers by analyzing their consumption habits over a period of 10 months. I have classified clients into 11 major categories based on the type of products they usually buy, the number of visitors they make in a year and the amount they spend on average and in total during the 10 months. These categories established, I finally trained several classifiers whose objective is to be able to classify consumers in one of these 11 categories and this from their first purchase. For this, the classifier is based on 5 variables which are:\n- **mean**: amount of the basket of the current purchase\n- **categ_N** with $ N \\ in [0: 4] $: percentage spent in product category with index $ N $\n\n\nFinally, the quality of the predictions of the different classifiers was tested over the last two months of the dataset. The data were then processed in two steps: first, the set of data was used to define the category to which each client belongs, and then the classifier predictions were compared to this category assignment. I then found that 75% of clients are awarded the right classes.\nThe performance of the classifier therefore seems correct given the potential shortcomings of the current model. In particular, a bias that has not been dealt with concerns the seasonality of purchases and the fact that purchasing habits will potentially depend on the time of year (for example, Christmas ). In practice, this seasonal effect may cause the categories defined over a 10-month period to be quite different from those extrapolated from the last two months. In order to correct such bias, it would be beneficial to have data that would cover a longer period of time.\n\n","0b84522b":"## 7. Conclusion","22144032":"At the level of this dataframe, the amounts are indicated for each product purchased during an order. I collect all the purchases made during a single order and to recover the total amount:","f8c6e349":"Finally, I separate the dataset into test and training sets:","4f158e47":"Next, I am interested in the frequency of visits of different users. For this, I look at the number of transactions associated with each of them. I also describe the amounts associated with all these transactions:","5ac85813":"At this stage, I have verified that the different clusters are indeed disjoint (at least, in a global way). It remains to define the content of each of them and to understand the typical behavior of the customers present in each of them. In a first time, therefore, I add to the dataframe `selected_customers` a variable that defines the cluster to which each client belongs:","69dd498d":"###\u00a05.4 Decision Tree","2da85e5e":"In the following, I will create client clusters. In practice, before creating these clusters, it is interesting to define a base of smaller dimension allowing to describe the `scaled_matrix` matrix. In this case, I will use this base in order to create a representation of the different clusters and thus verify the quality of the separation of the different groups. I therefore perform a PCA beforehand:","e919c658":"\n**a \/ _Silhouette intra-cluster score_**\n\nIn order to have an insight into the quality of the classification, we can represent the silhouette scores of each element of the different clusters. This is the object of the following figure from the [documentation](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html) of sklearn (and the corresponding python code):","d3e3d543":"and I again find that this hypothesis is not verified. So the cancellations do not necessarily correspond to orders that would have been made beforehand.\n\nAt this point, I decide to create a new variable in the dataframe or I indicate if part of the command has been canceled. At the level of entries that indicate a cancellation but without compensation at the level of the orders, a certain number of them correspond to the cancellation of orders placed before the month of December 2010 (entry point of the database). I decide to delete these entries.","d9a0c9f3":"It remains only to examine the predictions of the different classifiers that have been trained in section 5:","782bac04":"#### 4.1.2 Separation of data over time\n\nThe dataframe `basket_price` contains information for a period of 12 months. Later, one of the objectives will be to develop a model capable of characterizing and anticipating the habits of the customers visiting the site and this, from their first visit. In order to be able to test the model in a realistic way, I split the data set by retaining the first 10 months to develop the model and the following two months to test it:","7558ddd5":"It can be seen that the vast majority of orders made relate to relatively large purchases given that $\\sim$65% of purchases have an amount in excess of \u00a3 200.","1cf3d475":"Then, I average the contents of this dataframe by first selecting the different groups of clients. This gives access to, for example, the average baskets, the number of visits or the total sums spent by the clients of the different clusters. I also determine the number of clients in each group (variable ** size **):","8b96756e":"I can then test the accuracy of the predictions on the data of the test set:","97e1ee97":"#### 4.2.1 Data encoding\n\nThe dataframe `transactions_per_user` contains a summary of all the commands made by the different clients. Each entry in this dataframe corresponds to a particular client. I use this information to characterize the different types of clients and for this I use a subset of variables:","e68d8e5d":"### 4.1 Formatting data\n\nIn the previous section, the different products were grouped into five clusters. In order to prepare the rest of the analysis, a first step consists in introducing this information into the dataframe containing the set of commands. To do this, I create the categorical variable **categ_product**:","f6990812":"**b\/ _Score de silhouette intra-cluster_**","928cb7d5":"___\n#### 2.2.1 Cancelling orders","91adfc05":"I create a new variable that indicates the amount of the purchase:","0e33b144":"Finally, in order to re-organize the content of the dataframe by ordering the different clusters in relation to the expenditure in the different categories on the one hand and then on the total amount of expenditure, on the other hand:","71499c23":"then the dataframe containing the data from e-commerce. Below, I also give some basic information on the content of the dataframe: the type of the various variables, the number of null values and the percentage with respect to the set of dataframe entries.","f1e6ecd8":"**d \/ _Customers morphology_**\n\nFinally, I created a representation of the different morphotypes. To do this, I define a class to create \"Radar Charts\" (which has been adapted from this [kernel](https:\/\/www.kaggle.com\/yassineghouzam\/don-t-know-why-employees-leave -read-this)):","1f4c9d45":"____\n#### 2.2.4 Frequency of purchases","8add513f":"In order to have a global view of the type of order performed at this dataset, I determine how the purchases are divided up in relation to their amounts:","c60c9b7c":"We see that the quantity canceled is greater than the sum of the previous purchases.\n___\n#### 2.2.2 StockCode\n\nAbove, it has been seen that some values of the ** StockCode ** variable indicate a particular transaction (i.e. D for Discount). I check the contents of this variable by looking for the set of codes that would contain only letters:","f34a01b8":"This function takes as input the dataframe and analyzes the contents of the **Description** column by performing the following operations:\n\n- retrieval of the names (own, common) appearing in the description of the products\n- for each name, I extract the root of the word and save the set of names associated with a particular root\n- count the number of times each root appears at the dataframe level\n- when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular \/ plural variants)","9c00f860":"First of all, I count the number of transactions corresponding to cancellations of orders.","2beec176":"Note that at the definition of the `votingC` classifier, I used a sub-sample of all the classifiers defined above, and only retained the` Random Forest`, the `k-Nearest Neighbors` and the `Gradient Boosting`. In practice, this choice is based on the performance of the classification compared to the test carried out in the following section.\n\n___\n## 6. Testing predictions\n\nIn the previous section, more classifiers were trained so as to be able to determine the cattometry to which the consumers belong. The whole analysis relating to the definition of the categories or the training of the classifier is based on the data of the first 10 months of the dataset. Since the model has been defined, it is now a test and I use the data of the last two months that are in the dataframe `set_test`:","2582814e":"et je regarde dans quelle mesure les diff\u00e9rents vecteurs expliquent la variance des donn\u00e9es:","4909614a":"___\n### 5.1 Support Vector Machine Classifier (SVC)\n\nThe first classifier I use is the SVC classifier. In order to use it, I create an instance of the `Class_Fit` class and then call` grid_search () `. When calling this method, I provide as parameters:\n- the hyperparameters for which I will seek an optimal value\n- the number of folds to be used for cross-validation","c3c93bb2":"Then, I convert the dataframe into a matrix and retain only variables that define the category to which consumers belong. At this level, I recall the method of normalization that had been used on the training set:","7fdce22c":"# Customer segmentation\n","c275460f":"**c\/ _Description d\u00e9taill\u00e9e des clients types_**","c87fce55":"___\n#### 5.1.1 Confusion matrix\n\nThe accuracy of the results seems to be correct. Nevertheless, let us remember that when the different classes were defined, there was an imbalance in size between the different classes obtained. In particular, one class contains about 40% of clients. It is therefore interesting to look at how the predictions and real values compare to the breasts of the different classes. This is the subject of the confusion matrices and to represent them, I use the code of the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html):","495e982c":"___\n### 3.2 Definition of product categories","533f4df6":"At this stage, it is found that the information relating to the same command is dispersed over several lines of the dataframe (one line per product). So I decide to collect the information related to a particular order on a single entry. I therefore create a new dataframe containing for each order the amount of the basket, as well as the way that amount is distributed according to the 5 categories of products:","4844baa3":"A customer category that will pay particular attention is that of customers who make only one purchase, and one of the objectives may be, for example, to target these customers in order to retain them. In part, I find that this type of customer represents 1\/3 of the customers listed:","5d160b40":"### 5.3 k-Nearest Neighbors","e3c3a63f":"### 2.1 Countries","3b970143":"This notebook aims at analyzing the content of an e-commerce database that lists purchases made by $\\sim$542000 customers over a period of one year. Based on this analysis, I develop a model that allows to anticipate the purchases that will be made by a new customer, during the following year and this, from its first purchase.\n____","98d98cff":"Finally, in order to prepare the execution of the classifier, it is sufficient to select the variables on which it acts:","b520fa29":"First, I look at the number of clients in each cluster:","01757b6d":"In practice, the scores obtained above can be considered equivalent since, depending on the run, scores of $ 0.1 \\pm 0.05 $ will be obtained for all clusters with `n_clusters` $> $ 3 (one obtains possibly slightly lower scores for the first cluster). On the other hand, I found that, beyond 5 clusters, some clusters contained very few elements. I therefore choose to separate the dataset into 5 clusters. In order to ensure a good classification, I go so far as to obtain the best accessible silhouette score, which in this case is around 0.15:","47fcbbd7":"Each line in this matrix contains a consumer's buying habits. At this stage, it is a question of using these habits in order to define the category to which the consumer belongs. These categories have been established in Section 4. ** At this stage, it is important to bear in mind that this step does not correspond to the classification stage itself. Here, we prepare the test data by defining the category to which the customers belong. However, this definition uses data obtained over a period of 2 months (via the variables **count**, **min**, **max** and **sum**). The classifier defined in Section 5 uses a more restricted set of variables that will be defined from the first purchase of a client.\n\nHere it is a question of using the available data over a period of two months and using this data to define the category to which the customers belong. Then, the classifier can be tested by comparing its predictions with these categories. In order to define the category to which the clients belong, I recall the instance of the `kmeans` method used in section 4. The` predict` method of this instance calculates the distance of the consumers from the centroids of the 11 client classes and the smallest distance will define the belonging to the different categories:","271adb3f":"____\n## 3. Indications on the type of products purchased","23208b1b":"to create a representation of the different clusters:","84e0aefb":"____\n#### 4.1.3 Consumer Order Combinations\n\nIn a second step, I group together the different entries that correspond to the same user. I thus determine the number of purchases made by the user, as well as the minimum, maximum, average amounts and the total amount spent during all the visits:","8698568f":"The first lines of this list indicate several things worthy of interest, which will guide the rest of the analysis:\n- the existence of an entry with the prefix C at the level of the variable InvoiceNo: this indicates the transactions that have been canceled\n- the existence of users who only came once and only purchased one product (e.g. no. 12346)\n- the existence of frequent users and buying a large number of items with each order","298af3cc":"## 1. Data preprocessing","954c2b08":"First step: we load all the modules that will be used in this notebook.","2597ebf3":"At the dataset level, products are uniquely identified using the ** StockCode ** variable. The type of product in question is described in the variable ** Description **. In this section, I am looking at whether it is possible to use the content of this variable in order to group the products into different categories.\n\n___\n### 3.1 Description of the products\n\nAs a first step, I extract from the variable ** Description ** the information that will be useful. To do this, I use the following function:","f6f590da":"This last matrix indicates the words contained in the description of the products on the principle of *one-hot-encoding*. In practice, I have found that by indicating moreover the price range in which the different products are located, more balanced groups are obtained in terms of the number of elements.\nSo I add 6 columns to this matrix, where I indicate the price range of the products:","aafed424":"I note that the starting hypothesis is not verified because of the existence of an entry whose description is \"_Discount_\". I test again the hypothesis made initially by not taking into account these inputs:","7d79eff2":"Finally, we can then create a prediction based on the predictions of the different classifiers:","acec2574":"OK, therefore, by removing these entries we end up with a dataframe filled to 100% on all variables!\n\n___\n## 2. Exploration\n\nThis dataframe contains 8 variables that correspond to:","13a6a5a8":"**c \/ Component Analysis**\n\nIn order to ensure that these clusters are truly distinct, I look at the different elements within these clusters. Given the large number of variables of the initial matrix, I first make a PCA:","13773c60":"____\n#### 3.2.2 Creating Product Clusters\n\nNow, I am grouping products into different classes. In the case of matrices with binary encoding, the most suitable metric for the calculation of distances is [Hamming's metric](https:\/\/en.wikipedia.org\/wiki\/Distance_de_Hamming). I use this metric by importing the module [kmodes](https:\/\/pypi.python.org\/pypi\/kmodes\/). Note that the ** kmeans ** method of sklearn uses a Euclidean distance which could be used but which is not to be progrege in the case of categorical variables. In order to define (approximately) the number of clusters that best represents the data, I use the silhouette score:","6929560c":"### 5.6 AdaBoost Classifier","a3a0f128":"On these few lines, we notice that when canceling, we end up with 2 transactions in the dataframe (ordering and cancellation). So I decide to check if this is true for all entries.\nTo do this, I decide to locate the entries indicating a negative quantity and check whether there is * systematically * a command indicating the same quantity (but positive) and with the same description (** CustomerID **, ** Description ** and ** UnitPrice **):","8cf751ab":"This provides a global view of the contents of each cluster:","2ac99aae":"___\n###\u00a04.2 Creating customer categories","e427d711":"If one looks at the number of null values in the dataframe, an interesting finding is that $\\sim$25% of entries are not assigned to a particular user. It is impossible to impute values for the user and these entries are thus a priori unusable. So I delete them from the dataframe.","a71192cf":"from which I create the following representation:","9cb1dbf2":"and I represents the amount of variance explained by each of the components:","79e9148d":"___\n#### 5.1.2 Learning curve\n\nA typical way to test the quality of a fit is to draw a learning curve. This type of curve makes it possible in particular to detect possible gaps in the model, linked for example to over-learning or under-learning. This also makes it possible to see if the modeling could benefit from a larger sampling of data. In order to represent this type of curve, I use the [scikit-learn documentation code again](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr- self-examples-model-selection-pad-learning-curve-py)","8b9d09c4":"that we then train:","0ae66437":"It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages (**mean**), the total sum spent by the clients (**sum**) or the total number of visits made (**count**).\n\n____\n## 5. Classification of customers\n\nIn this part, the objective will be to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. The objective is to make this classification possible at the first visit. To fulfill this objective, I will test several classifiers implemented in `scikit-learn`. First, in order to simplify their use, I define a class that allows to interface several of the functionalities common to these different classifiers: ","aa267494":"___\n#### 3.2.3 Characterization of the contents of clusters","2ae6cd7c":"Finally, as anticipated in Section 5.8, it is possible to improve the quality of the classifier by combining their respective predictions. At this level, I chose to mix Random Forest, Gradient Boosting and k-Nearest Neighbors predictions because this leads to a slight improvement in predictions:","67242f78":"As with product categories, another way to look at the quality of separation is to look at silouhette scores within different clusters:\n","cf7f4b12":"___\n#### 4.1.1 Grouping products by order\n\nIn a second step, I decide to create the ** categ_N ** variables with $ N \\ in [0: 4] $ which contain the amounts spent in each product category:","3559c334":"###\u00a05.5 Random Forest","7b28b5ee":"From this representation, it can be seen, for example, that the first principal component makes it possible to separate the clusters with few elements from the other clusters. More generally, it can be seen that, depending on the pair of PCAs considered, a specific separation of the different clusters is obtained that ultimately. Overall, we see that there is always a representation in which two clusters will appear to be disjoint.","13da5ba0":"and I check the number of products in the different price ranges:","138bf8cc":"Finally, I define two additional variables that give the number of days elapsed since the first purchase (**FirstPurchase**) and the number of days since the last purchase (**LastPurchase**):","bb090860":"The dataframe contains $\\sim$ 400,000 entries. What is the number of users and products on these entries?"}}