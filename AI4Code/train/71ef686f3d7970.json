{"cell_type":{"56098160":"code","79bba06f":"code","d81ddcbc":"code","526f70ee":"code","ab7b8b9a":"code","ef9e3845":"code","6b5161fc":"code","119e31d4":"code","577e3c11":"code","425021f7":"code","0a091969":"code","b6e0e1ec":"code","d00f2c5b":"code","fe7c628c":"code","820167c2":"code","a792aa49":"code","19b9e4fd":"code","6a33b4ff":"code","a00dacb2":"code","243f51a3":"code","362885b6":"code","65b8e30e":"markdown","75b1ffa8":"markdown","e68a287d":"markdown","9f2cf6a2":"markdown","c5b49287":"markdown","33f5f6e6":"markdown","0757d658":"markdown","cc5dfe5d":"markdown","32720aaf":"markdown","c04a03d2":"markdown","126936a0":"markdown"},"source":{"56098160":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score","79bba06f":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndata.head()","d81ddcbc":"data = data.drop(columns=[\"id\"], axis=1)","526f70ee":"print(\"Number of entries before removing duplicates: \", data.shape[0])\ndata = data.drop_duplicates()\nprint(\"Number of entries after removing duplicates: \", data.shape[0])","ab7b8b9a":"data.describe().T","ef9e3845":"data = data.drop(data.columns[-1], axis=1)\ndata.head()","6b5161fc":"print(data.isna().sum())","119e31d4":"data = data.sample(frac=1)\ndata.reset_index(drop=\"Index\", inplace=True)\ndata.head()\n\n## Converting diagnosis from string to integer ##\n## We'll set Malignant as 1 and Benigne as 0 ##\ndata.diagnosis = data.diagnosis.apply(lambda x: 1 if x==\"M\" else 0)","577e3c11":"target_data = data.diagnosis\nfeature_data = data.drop(columns=[\"diagnosis\"], axis=1)","425021f7":"feature_names = [\"radius\", \"texture\", \"perimeter\", \"area\", \"smoothness\", \"compactness\", \"concavity\", \"concave points\", \"symmetry\", \"fractal_dimension\"]\nfeature_mean_dataset = feature_data[[x+\"_mean\" for x in feature_names]]\nscaler = MinMaxScaler()\nfeature_mean_dataset[feature_mean_dataset.columns] = scaler.fit_transform(feature_mean_dataset)\nfeature_mean_dataset.plot.density(figsize=(10, 10))\nfeature_mean_dataset.plot.box(figsize=(25, 10))\nplt.figure(figsize=(15, 15))\nsns.heatmap(feature_mean_dataset.corr(), annot=True)","0a091969":"feature_se_dataset = feature_data[[x+\"_se\" for x in feature_names]]\nscaler = MinMaxScaler()\nfeature_se_dataset[feature_se_dataset.columns] = scaler.fit_transform(feature_se_dataset)\nfeature_se_dataset.plot.density(figsize=(10, 10))\nfeature_se_dataset.plot.box(figsize=(25, 10))\nplt.figure(figsize=(15, 15))\nsns.heatmap(feature_se_dataset.corr(), annot=True)","b6e0e1ec":"feature_worst_dataset = feature_data[[x+\"_worst\" for x in feature_names]]\nscaler = MinMaxScaler()\nfeature_worst_dataset[feature_worst_dataset.columns] = scaler.fit_transform(feature_worst_dataset)\nfeature_worst_dataset.plot.density(figsize=(10, 10))\nfeature_worst_dataset.plot.box(figsize=(25, 10))\nplt.figure(figsize=(15, 15))\nsns.heatmap(feature_worst_dataset.corr(), annot=True)","d00f2c5b":"drop_features = [x+\"_\"+y for x in [\"perimeter\", \"area\", \"compactness\", \"concave points\"] for y in [\"mean\", \"se\", \"worst\"]]\nfeature_data.drop(columns=drop_features, axis=1)","fe7c628c":"feature_names = [\"radius\", \"texture\", \"smoothness\", \"concavity\", \"symmetry\", \"fractal_dimension\"]\nfeature_mean_dataset = feature_data[[x+\"_mean\" for x in feature_names]]\nscaler = MinMaxScaler()\nfeature_mean_dataset[feature_mean_dataset.columns] = scaler.fit_transform(feature_mean_dataset)\nfeature_mean_dataset.plot.density(figsize=(10, 10))\nfeature_mean_dataset.plot.box(figsize=(25, 10))\nplt.figure(figsize=(15, 15))\nsns.heatmap(feature_mean_dataset.corr(), annot=True)","820167c2":"X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, random_state=10)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","a792aa49":"\nlist_of_models = [KNeighborsClassifier(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]\nfor model in list_of_models:\n    print(model.__str__())\n    model.fit(X_train, y_train)\n    print(\"Training Score\", model.score(X_train, y_train))\n    print(\"Test Score\", model.score(X_test, y_test))\n    print(cross_val_score(model, feature_data, target_data))\n    print(\"\\n\")","19b9e4fd":"training_score = []\ntest_score = []\nval_score = []\n\nfor n_neighbors in range(2, 7):\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    model.fit(X_train, y_train)\n    training_score.append(model.score(X_train, y_train))\n    test_score.append(model.score(X_test, y_test))\n    val_score.append(cross_val_score(model, feature_data, target_data).mean())\n\nplt.figure(figsize=(10,10))\nplt.plot(range(2, 7), training_score, label=\"training_score\")\nplt.plot(range(2, 7), test_score, label=\"test_score\")\nplt.plot(range(2, 7), val_score, label=\"cross_val_score\")\nplt.legend()","6a33b4ff":"# Selecting the number of neighbors as 5 based on the above results\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","a00dacb2":"print(confusion_matrix(y_test, y_pred))","243f51a3":"auc_score = roc_auc_score(y_test, y_pred)\nprint(auc_score)","362885b6":"a, b, c = roc_curve(y_test, y_pred)\nplt.plot(a, b)","65b8e30e":"# No NULL entries found. Pheeeeeeeww!!! ;)\n# Looking at the column names, we basically have three different data sets viz. mean, se and worst, each for radius, texture, perimeter, area, smoothness, compactness, concavity, concave_points, symmetry, fractal_dimension\n# Now, let's shuffle the data just that we change the orderly arranged diagnosis. I generally do this, don't remember why though. Most **probably** because I read somehere that orderly arranged data might result in biased results.","75b1ffa8":"# Validating the outcome using confusion matrix","e68a287d":"# Removing ID as it won't have any effect on the outcome","9f2cf6a2":"# Count how many null entries are present","c5b49287":"# Looking at the above results, KNN seems to be the best option without over-fitting. DecisionTree and RandomForest seems to overfit on the training data","33f5f6e6":"# Plotting the ROC Curve","0757d658":"# Calculating the area under ROC Curve","cc5dfe5d":"# Looking at the above description we can remove the last column as well","32720aaf":"# Looking at the heatmap, its quite evident that there are certain colinear features in our dataset. \n# For example, radius, perimeter and area are highly colinear, one can only wonder why ;)\n# Next, we can see some amount of colinearlity between compactness, concavity and concave points. For no specific reason, I'll select concavity among the three.","c04a03d2":"# Check for duplicates and null entries","126936a0":"# Dividing the dataset into features and targets"}}