{"cell_type":{"8af67c28":"code","8ce78578":"code","537dde25":"code","ff61732e":"code","7bfb7e6d":"code","285741a1":"code","64e8c62e":"code","61b55014":"code","49de5df3":"code","b6659243":"code","b5f8d2aa":"code","42946d99":"code","a06acf68":"code","ee273c70":"code","cc0e8cee":"code","018f5632":"code","773d1370":"code","468876ab":"code","e8c1512b":"code","920f688d":"code","85b4620c":"code","4fba84a4":"code","50dc5231":"code","2bb65d82":"code","30f23dce":"code","71e45aaf":"code","51f98443":"code","fc69cb76":"code","7b570c43":"code","4996b2a2":"code","c35a1abc":"code","973095d6":"code","0c62a183":"code","c09dd7dc":"code","6fb0f9e5":"code","a3333602":"code","70a297f0":"code","93e90e68":"code","7bc69990":"code","199d6ce7":"code","31dba0d1":"code","d6d5c286":"code","0790cd48":"code","244c19d1":"code","a2f45bd9":"code","f4224670":"code","647f17d9":"code","6cae792c":"code","ce442b65":"code","dc81d5ea":"code","76d45eaf":"code","b06da3b1":"code","63da8798":"code","0c1be0c0":"code","860b060e":"code","a9be1d80":"code","71d9b867":"code","dbd2e6fe":"code","b95c0e19":"code","fe3bb0cc":"code","26eb436f":"code","5b73a4d1":"code","c691d03f":"code","d38f737f":"code","6ba9bea6":"code","66483a4d":"code","a1cb4ad2":"code","d826e49a":"code","38f01404":"code","fdc1b6d4":"code","dc719bf0":"code","4550978a":"code","d02b49af":"code","266616f7":"code","353ebb29":"code","1621e02d":"code","95ed9dc4":"code","53240dbf":"code","ddfa99a3":"code","ef865793":"code","f06fde39":"code","77b1b2a1":"code","7aab7f3c":"code","18f2712f":"code","385c97bc":"code","c541071b":"code","3f61f867":"code","e959d3f0":"code","0418c7f7":"code","8ddf0750":"code","3c987665":"code","4e23a883":"code","4a0472c2":"code","628665f7":"code","23e2825f":"code","34818ede":"code","020a9450":"code","9392e11d":"code","4cace8c6":"code","96048b40":"code","4957bbee":"code","03e8d05f":"code","4daf6a8a":"code","2bf6c81a":"code","b32f3445":"code","7fd45eb9":"code","1f2be785":"code","8c4e2998":"code","9a723814":"code","6e8ff945":"code","4ed36240":"code","9858cc8c":"code","bf202732":"code","a7d405ae":"code","c4768020":"code","20f538be":"code","81baefeb":"code","ef41b885":"code","5aef50db":"code","b7fc33b3":"code","aa1c890c":"code","fa5c7c57":"code","9849cfd5":"code","80eee698":"code","48b1bf80":"code","7603dd04":"code","5daeb7bc":"code","f1789b95":"code","1068a7b5":"code","404f71af":"code","37659c02":"code","afff8f9f":"code","38e7cb41":"markdown","e1edd9e8":"markdown","5ebda146":"markdown","63a2b779":"markdown","edd0db1f":"markdown","6834dac2":"markdown","14a76109":"markdown","c963f691":"markdown","3e707a4a":"markdown","8a12a328":"markdown","2fedfb26":"markdown","980b66f5":"markdown","53ff7ac0":"markdown","a5391276":"markdown","c083c047":"markdown","ac0205b0":"markdown","352aea6a":"markdown","e4e450b6":"markdown","418744d6":"markdown","e3fce6b4":"markdown","5680b5d6":"markdown","f6706382":"markdown","f5f123af":"markdown","828fd3a9":"markdown","311b127b":"markdown","e89cc762":"markdown","e0666fa2":"markdown","d497d7f3":"markdown","a62ef0af":"markdown","4b6675df":"markdown","ce5ebcd9":"markdown","76f2e461":"markdown","1d3d656a":"markdown","ef7f66a8":"markdown","eb5cba1e":"markdown","a859bf29":"markdown","f973ed14":"markdown","db466077":"markdown","95bd8df7":"markdown","a886ef0e":"markdown","0ec314fe":"markdown","d0e26567":"markdown","3dfa1bae":"markdown","fe7e8ebe":"markdown","7f75e717":"markdown","abe129a3":"markdown","178ef896":"markdown","54dc2733":"markdown","e09589dd":"markdown","16f548c6":"markdown","b26df13f":"markdown","e7eca19e":"markdown","7f15287a":"markdown","ea16abd0":"markdown","6cd99bc6":"markdown","f5132a03":"markdown","6eb1eaa2":"markdown","e4ef63d9":"markdown","0df1173a":"markdown","43508a8f":"markdown","fd16d2ea":"markdown","db9f028f":"markdown","f40239bb":"markdown","21034afd":"markdown","a4ca1a58":"markdown","67306615":"markdown","ac4a20b2":"markdown","3af62916":"markdown","cd1b1832":"markdown","eb6358cd":"markdown","547d8611":"markdown","8f4ca241":"markdown","5313365d":"markdown","7743adc7":"markdown","cc19ce3a":"markdown","540c7592":"markdown","f71e1c3b":"markdown","71f816a6":"markdown","884ad678":"markdown","65b73764":"markdown"},"source":{"8af67c28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ce78578":"bank_data=pd.read_csv(\"\/kaggle\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv\")","537dde25":"bank_data.head()","ff61732e":"pip install pandas-profiling","7bfb7e6d":"bank_data.columns","285741a1":"from pandas_profiling import ProfileReport\nreport=ProfileReport(bank_data,title=\"profile report\")","64e8c62e":"report.to_widgets()","61b55014":"report.to_file(\"report.html\")","49de5df3":"pip install sweetviz","b6659243":"import sweetviz","b5f8d2aa":"my_report = sweetviz.analyze([bank_data, \"data\"],target_feat='Exited')","42946d99":"my_report.show_html('my_report.html')","a06acf68":"genderwise=bank_data.groupby('Gender')['Exited'].sum()\ngenderwise","ee273c70":"y=genderwise.tolist()\nlabel=['Female','Male']","cc0e8cee":"plt.pie(y, labels=label,shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['blue','orange'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","018f5632":"geographywise=bank_data.groupby('Geography')['Exited'].sum()\ngeographywise","773d1370":"y=geographywise.tolist()\nlabel=['France','Germany','Spain']","468876ab":"plt.pie(y, labels=label,shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['blue','orange','green'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","e8c1512b":"nofproductswise=bank_data.groupby('NumOfProducts')['Exited'].sum()\nnofproductswise","920f688d":"y=nofproductswise.tolist()\nlabel=['1','2','3','4']","85b4620c":"plt.pie(y, labels=label,shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['orange','indigo','blue','green'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","4fba84a4":"hascreditwise=bank_data.groupby('HasCrCard')['Exited'].sum()\nhascreditwise","50dc5231":"plt.pie(hascreditwise.tolist(), labels=['0','1'],shadow=True,\n   counterclock=False, startangle=90,autopct='%1.f%%',\n       colors=['orange','yellow'])\nplt.pie([1],colors=['white'],radius=.4)\nplt.show()","2bb65d82":"sns.set(style=\"darkgrid\")\nsns.countplot(x='Geography', hue = 'Exited',data = bank_data, palette=\"Set3\")","30f23dce":"sns.set(style=\"darkgrid\")\nsns.countplot(x='Gender', hue = 'Exited',data = bank_data, palette=\"Set1\")","71e45aaf":"sns.set(style=\"darkgrid\")\nsns.countplot(x='HasCrCard', hue = 'Exited',data = bank_data, palette=\"Set2\")","51f98443":"sns.set(style=\"darkgrid\")\nsns.countplot(x='IsActiveMember', hue = 'Exited',data = bank_data, palette=\"Set3\")","fc69cb76":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set3\",dodge=False)","7b570c43":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = bank_data,dodge=False)","4996b2a2":"g = sns.FacetGrid(bank_data, col = \"Exited\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","c35a1abc":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set2\",dodge=False)","973095d6":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set1\",dodge=False)","0c62a183":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set3\",dodge=False)","c09dd7dc":"sns.set(style=\"darkgrid\")\nsns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = bank_data,palette=\"Set1\")","6fb0f9e5":"target_count = bank_data.Exited.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])","a3333602":"print('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","70a297f0":"# Class count\ncount_class_0, count_class_1 = bank_data.Exited.value_counts()\n\n# Divide by class\ndf_class_0 = bank_data[bank_data['Exited'] == 0]\ndf_class_1 = bank_data[bank_data['Exited'] == 1]","93e90e68":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\nbank_data_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(bank_data_over.Exited.value_counts())\n\nbank_data_over.Exited.value_counts().plot(kind='bar', title='Count (target)')","7bc69990":"X=bank_data_over.iloc[:,3:13]\ny=bank_data_over.iloc[:,13]","199d6ce7":"geography=pd.get_dummies(X['Geography'], drop_first= True)\ngender=pd.get_dummies(X['Gender'], drop_first= True)","31dba0d1":"X=pd.concat([X,geography,gender],axis=1)","d6d5c286":"X=X.drop(['Geography','Gender'],axis=1)\n","0790cd48":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","244c19d1":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)","a2f45bd9":"X_train","f4224670":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(C=100,random_state=0)\nlr.fit(X_train,y_train)\ny_pred_logistic=lr.predict(X_test)\ncorrect = (y_test == y_pred_logistic).sum()\nincorrect = (y_test != y_pred_logistic).sum()\naccuracy = correct \/ (correct + incorrect) * 100\n\nprint('\\nPercent Accuracy: %0.1f' %accuracy)","647f17d9":"prediction = pd.DataFrame()\nprediction['actual'] = y_test\nprediction['predicted'] = y_pred_logistic\nprediction['correct'] = prediction['actual'] == prediction['predicted']\n\nprint ('\\nDetailed results for first 20 tests:')\nprint (prediction.head(20))","6cae792c":"#Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nc_logistic=confusion_matrix(y_test,y_pred_logistic)\nprint(c_logistic)\n\n#Accuracy of our model.\nAccuracy_logistic=sum(np.diag(c_logistic))\/(np.sum(c_logistic))\nAccuracy_logistic","ce442b65":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred_logistic))","dc81d5ea":"# ROC_AUC \nfrom sklearn.metrics import roc_auc_score, roc_curve\nlogistic_roc_auc = roc_auc_score(y_test, y_pred_logistic , average = 'macro', sample_weight = None)\nlogistic_roc_auc","76d45eaf":"from sklearn.naive_bayes import GaussianNB\nclassifier_naive=GaussianNB()\n\n# Fitting the model with training data\nclassifier_naive.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predict_naive=classifier_naive.predict(X_test)","b06da3b1":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_naive=confusion_matrix(y_test,y_predict_naive)\nprint(c_naive)\n\n#Accuracy of our model.\nAccuracy_naive=sum(np.diag(c_naive))\/(np.sum(c_naive))\nAccuracy_naive","63da8798":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_naive))","0c1be0c0":"# ROC AUC\nnaive_roc_auc = roc_auc_score(y_test, y_predict_naive , average = 'macro')\nnaive_roc_auc","860b060e":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n\n# Fitting the model with training data\nclassifier_tree=classifier_tree.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predict_tree = classifier_tree.predict(X_test)","a9be1d80":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_tree=confusion_matrix(y_test,y_predict_tree)\nprint(c_tree)\n\n#Accuracy of our model.\nAccuracy_tree=sum(np.diag(c_tree))\/(np.sum(c_tree))\nAccuracy_tree","71d9b867":"#Evaluation \n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_tree))","dbd2e6fe":"# ROC AUC\ntree_roc_auc = roc_auc_score(y_test, y_predict_tree , average = 'macro')\ntree_roc_auc","b95c0e19":"features_label = X.columns\nimportances = classifier_tree.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","fe3bb0cc":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], color = \"magenta\", align = \"center\")\nplt.xticks(range(X.shape[1]), features_label, rotation = 90)\nplt.show()","26eb436f":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_ensemble = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n\n# Fitting the model with training data\nclassifier_ensemble.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_predict_ensemble = classifier_ensemble.predict(X_test)","5b73a4d1":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_ensemble=confusion_matrix(y_test,y_predict_ensemble)\nprint(c_ensemble)\n\n#Accuracy of our model.\nAccuracy_ensemble=sum(np.diag(c_ensemble))\/(np.sum(c_ensemble))\nAccuracy_ensemble","c691d03f":"#Evaluation \n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_ensemble))","d38f737f":"# ROC AUC\nensemble_roc_auc = roc_auc_score(y_test, y_predict_ensemble , average = 'macro')\nensemble_roc_auc","6ba9bea6":"features_label = X.columns\nimportances = classifier_ensemble.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","66483a4d":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], color = \"red\", align = \"center\")\nplt.xticks(range(X.shape[1]), features_label, rotation = 90)\nplt.show()","a1cb4ad2":"!pip install xgboost","d826e49a":"# Fitting the XGBoost to the training set\nfrom xgboost import XGBClassifier\nclassifier_xg=XGBClassifier()\n\n# Fitting the model with training data\nclassifier_xg.fit(X_train,y_train)\n\n# Predicting the test results\ny_predict_xg= classifier_xg.predict(X_test)","38f01404":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_xg=confusion_matrix(y_test,y_predict_xg)\nprint(c_xg)\n\n#Accuracy of our model.\nAccuracy_xg=sum(np.diag(c_xg))\/(np.sum(c_xg))\nAccuracy_xg","fdc1b6d4":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_xg))","dc719bf0":"#ROC AUC\nxg_roc_auc = roc_auc_score(y_test, y_predict_xg , average = 'macro')\nxg_roc_auc","4550978a":"features_label = X.columns\nimportances = classifier_xg.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i], importances[indices[i]]))","d02b49af":"# Visualization of the Feature importances\nplt.title('Feature Importances')\nplt.bar(range(X.shape[1]), importances[indices], color = \"yellow\", align = \"center\")\nplt.xticks(range(X.shape[1]), features_label, rotation = 90)\nplt.show()","266616f7":"from sklearn.ensemble import AdaBoostClassifier\nclassifier_ada = AdaBoostClassifier(base_estimator = None, n_estimators = 200, learning_rate = 1.0)\n\n# Fitting the model with training data \nclassifier_ada.fit(X_train, y_train)\n\n\n# Predicting the test results\ny_predict_ada= classifier_ada.predict(X_test)","353ebb29":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_ada=confusion_matrix(y_test,y_predict_ada)\nprint(c_ada)\n\n#Accuracy of our model.\nAccuracy_ada=sum(np.diag(c_ada))\/(np.sum(c_ada))\nAccuracy_ada","1621e02d":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_ada))","95ed9dc4":"#ROC AUC\nada_roc_auc = roc_auc_score(y_test, y_predict_ada , average = 'macro')\nada_roc_auc","53240dbf":"from sklearn.ensemble import GradientBoostingClassifier\nclassifier_GB = GradientBoostingClassifier(loss = 'deviance', n_estimators = 200)\n\n# Fitting the model with training data \nclassifier_GB.fit(X_train, y_train)\n\n# Predicting the test results\ny_predict_GB= classifier_GB.predict(X_test)","ddfa99a3":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nc_GB=confusion_matrix(y_test,y_predict_GB)\nprint(c_GB)\n\n#Accuracy of our model\nAccuracy_GB=sum(np.diag(c_GB))\/(np.sum(c_GB))\nAccuracy_GB","ef865793":"#Evaluation \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_predict_GB))","f06fde39":"# ROC AUC\nGB_roc_auc = roc_auc_score(y_test, y_predict_GB , average = 'macro')\nGB_roc_auc","77b1b2a1":"models = ['Logistic Regression', 'Naive Bayes', 'Decision Tree', 'Random Forest', 'XGBoost', 'AdaBoost', 'GradientBoosting']\naccuracy = [Accuracy_logistic, Accuracy_naive, Accuracy_tree, Accuracy_ensemble, Accuracy_xg, Accuracy_ada, Accuracy_GB]\nroc_auc = [logistic_roc_auc, naive_roc_auc, tree_roc_auc, ensemble_roc_auc, xg_roc_auc, ada_roc_auc, GB_roc_auc]\n\nmetrics = {'accuracy': accuracy, 'roc_auc': roc_auc}\ntable_metrics = pd.DataFrame(metrics, index = models)\ntable_metrics","7aab7f3c":"from sklearn.model_selection import cross_val_score\n\n# Function that will track the mean value and the standard deviation of the metric\n\ndef cvDictGen(functions, score, X_train = X, y_train = y, cv = 5):\n    cvDict = {}\n    for func in functions:\n        cvScore = cross_val_score(func, X_train, y_train, cv = cv, scoring = score)\n        cvDict[str(func).split('(')[0]] = [cvScore.mean(), cvScore.std()]\n    \n    return cvDict","18f2712f":"model = [lr, classifier_naive, classifier_tree, classifier_ensemble, classifier_xg, classifier_ada, classifier_GB]\ncvD = cvDictGen(model, score = 'roc_auc')\ncvD","385c97bc":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}","c541071b":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\nmax_depth.append(None)\nmin_samples_split = [5, 10]\nmin_samples_leaf = [4, 6, 8, 10]\nccp_alpha= [0.001,0.005,0.010,0.015,0.020,0.025,0.030]\n\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'criterion':[\"entropy\", \"gini\"],\n               'ccp_alpha' : ccp_alpha\n               }\n\nprint(random_grid)","3f61f867":"rf_random = RandomizedSearchCV(estimator = classifier_ensemble, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2,scoring='roc_auc',refit='AUC', random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)\nrf_random.best_params_,rf_random.best_score_","e959d3f0":"rf_random = RandomizedSearchCV(estimator = classifier_ensemble, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2,scoring='accuracy', random_state=42, n_jobs = -1)\nrf_random.fit(X_train, y_train)\nrf_random.best_params_,rf_random.best_score_","0418c7f7":"min_child_weight = [1,3, 5, 7, 10]\ngamma = [0.1,0.2,0.3,0.4, 0.5, 1, 1.5, 2, 5]\nsubsample = [0.6, 0.8, 1.0]\ncolsample_bytree = [0.3,0.4,0.5,0.6, 0.8, 1.0]\nmax_depth = [int(x) for x in np.linspace(3, 50, num = 5)]\nmax_depth.append(None)\nsampling_method = ['uniform','gradient_based']\nlearning_rate = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ]\n\nxg_grid = {\n        'min_child_weight': min_child_weight,\n        'gamma': gamma,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'max_depth': max_depth,\n        'sampling_method' : sampling_method,\n        'learning_rate' : learning_rate\n        }\n\nprint(xg_grid)","8ddf0750":"xg_random = RandomizedSearchCV(estimator = classifier_xg, param_distributions = xg_grid, n_iter = 100, cv = 3, verbose=2, random_state=42,scoring='roc_auc', n_jobs = -1)\nxg_random.fit(X_train, y_train)\nxg_random.best_params_,xg_random.best_score_","3c987665":"xg_random = RandomizedSearchCV(estimator = classifier_xg, param_distributions = xg_grid, n_iter = 100, cv = 3, verbose=2, random_state=42,scoring='accuracy', n_jobs = -1)\nxg_random.fit(X_train, y_train)\nxg_random.best_params_,xg_random.best_score_","4e23a883":"max_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(3, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [5, 10]\nmin_samples_leaf = [4,6,8,10]\nccp_alpha = [0.001,0.005,0.010,0.015,0.020,0.025,0.030]\n\ndecision_grid = {'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'criterion':[\"entropy\", \"gini\"],\n               'ccp_alpha' : ccp_alpha\n                }\n\nprint(decision_grid)","4a0472c2":"decision_random = RandomizedSearchCV(estimator = classifier_tree, param_distributions = decision_grid, n_iter = 100, cv = 3, verbose=2,scoring='roc_auc', random_state=42, n_jobs = -1)\ndecision_random.fit(X_train, y_train)\ndecision_random.best_params_,decision_random.best_score_","628665f7":"decision_random = RandomizedSearchCV(estimator = classifier_tree, param_distributions = decision_grid, n_iter = 100, cv = 3, verbose=2,scoring='accuracy', random_state=42, n_jobs = -1)\ndecision_random.fit(X_train, y_train)\ndecision_random.best_params_,decision_random.best_score_","23e2825f":"final_model_xg=XGBClassifier(subsample= 1.0, min_child_weight= 1, max_depth= 38, gamma= 0.3, colsample_bytree= 0.3, sampling_method= 'uniform',learning_rate= 0.15)\nfinal_model_xg.fit(X_train,y_train)","34818ede":"y_pred = final_model_xg.predict(X_test)\nprint(final_model_xg.__class__.__name__, accuracy_score(y_test, y_pred))","020a9450":"results = confusion_matrix(y_test, y_pred)","9392e11d":"print ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","4cace8c6":"final_model_rf=RandomForestClassifier(n_estimators= 2000, min_samples_split= 5,criterion=\"entropy\", min_samples_leaf= 6, max_features= 'log2', max_depth= 20,ccp_alpha = 0.001)\nfinal_model_rf.fit(X_train,y_train)","96048b40":"y_pred = final_model_rf.predict(X_test)\nprint(final_model_rf.__class__.__name__, accuracy_score(y_test, y_pred))","4957bbee":"results = confusion_matrix(y_test, y_pred)","03e8d05f":"print ('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","4daf6a8a":"final_model_rf.estimators_","2bf6c81a":"len(final_model_rf.estimators_)","b32f3445":"from sklearn import tree\n\nplt.figure(figsize=(50,20))\ntree.plot_tree(final_model_rf.estimators_[2],filled= True)","7fd45eb9":"# look for the explaianbility of 2000 decision trees.\n#for i in range(len(final_model_rf.estimators_)):\n    #print(tree.export_text(final_model_rf.estimators_[i]))\n\n    \n\n##Look for the explaianability of which ever tree you wants by entering the number between 0-1999    \nprint(tree.export_text(final_model_rf.estimators_[1]))","1f2be785":"# Importing the libraries\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU,PReLU,ELU\nfrom keras.layers import Dropout","8c4e2998":"classifier=Sequential()","9a723814":"classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu',input_dim=11))","6e8ff945":"classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu'))","4ed36240":"classifier.add(Dense(units = 6, kernel_initializer = 'he_uniform',activation='relu'))","9858cc8c":"classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform',activation='sigmoid'))","bf202732":"classifier.summary()","a7d405ae":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","c4768020":"model_history=classifier.fit(X_train,y_train,validation_split=0.33,batch_size=10,nb_epoch=100)","20f538be":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","81baefeb":"from sklearn.metrics import confusion_matrix\nc=confusion_matrix(y_test,y_pred)\n\nfrom sklearn.metrics import accuracy_score\nscore=accuracy_score(y_pred,y_test)","ef41b885":"print(c,score)","5aef50db":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Embedding,Flatten,LeakyReLU,BatchNormalization\nfrom keras.activations import relu, sigmoid\nfrom keras.layers import Dropout","b7fc33b3":"def create_model(layers,activation):\n    model=Sequential()\n    for i,nodes in enumerate(layers):\n        if i==0:\n            model.add(Dense(nodes,input_dim=X_train.shape[1]))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n            \n        else:\n            model.add(Dense(nodes))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n    model.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n    model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return model\nmodel=KerasClassifier(build_fn=create_model, verbose=0)","aa1c890c":"layers=[(20,),(40,20),(20,10),(30,20),(25,20)]\nactivations=['sigmoid','relu']\nparam_grid = dict(layers= layers,activation= activations,batch_size= [128,256],epochs= [30])\n\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,cv=5)","fa5c7c57":"grid_result=grid.fit(X_train,y_train)","9849cfd5":"print(grid_result.best_score_,grid_result.best_params_)","80eee698":"pred_y=grid.predict(X_test)\ny_pred=(pred_y>0.5)","48b1bf80":"from sklearn.metrics import confusion_matrix,accuracy_score\nc1=confusion_matrix(y_pred,y_test)\nscore=accuracy_score(y_pred,y_test)\nprint(c1,score)","7603dd04":"import shap","5daeb7bc":"# Initialize JS For Plot\nshap.initjs()","f1789b95":"rf_explainer = shap.TreeExplainer(final_model_rf)\nrf_shap_values = rf_explainer.shap_values(X_test)","1068a7b5":"shap.summary_plot(rf_shap_values, X_train)","404f71af":"# summarize the effects of all the features\nshap.summary_plot(rf_shap_values, X)","37659c02":"bank_data.columns","afff8f9f":"shap.force_plot(rf_explainer.expected_value[0], rf_shap_values[0], X)","38e7cb41":"# Loading the Dataset","e1edd9e8":"* The bank is losing customers with significant bank balances which is likely to hit their available capital for lending.","5ebda146":"* It seems younger customers tend to stick with the company more compared to older customers.","63a2b779":"* The salary has no significant effect on the likelihood to churn.","edd0db1f":"from sklearn import tree\nfrom sklearn.tree import export_text\nplt.figure(figsize=(60,50))\ntree.plot_tree(classifier_tree,filled=True)","6834dac2":"# Entire EDA at a single go by using Pandas-Profiling ","14a76109":"There is a slight increase in the accuracy even after some hyper parameter tuning.","c963f691":"* After looking at the scores, we can clearly see that randome forest accuracy and ROC_AUC score is ggod to go with.\n* accuracy is 91% and roc-auc score is 97% for train data and lets check for the test data as well","3e707a4a":"* Target variable count","8a12a328":"* customer using only one product are the most churned","2fedfb26":"* Variable importance plot.\u2014 Global Interpretability","980b66f5":"# Importing the libraries","53ff7ac0":"We also note the following.\n* Row number is not required for our analysis.\n* Customer Id is not required\n* Surname has high cardinality and i will remove it from the analysis part.\n* Credit score has minimal left skewed and almost normally distributed\n* Geography has 3 categories. Germany,Spain, France. so we need to encode this \n* Gender is categorical and has two categories and we need to encode this.\n* Age is slightly right skewed.\n* Tenure has zeros but no ned of any opeartion there.\n* Balance has zeros but no need of any operation there.\n* Number of products is categorical.\n* Has credit card is boolean which will explain whether the customer has credit card or not.\n* Is Active member is boolean and it says that the customer is an active member or not.\n* Estimated Salary is having its importance in the analysis.\n* Exited is boolean which is our target variable and we have whether the customer is exited or not.","a5391276":"* Majority of the data is from persons from France. \n* The proportion of churned customers is with inversely related to the population of customers alluding to the bank possibly having a problem in the areas where it has fewer clients.","c083c047":"* The number of product has no significant effect on the likelihood to churn.","ac0205b0":"These are the best parameters","352aea6a":"We need to get the best algorithm which is giving us the best output as predictability. So, we need to try out with various ML algorithms and then we can select the best ones which wll give us the best performance. Then we can choose the baseline models and then we can proceed further by doing hyper parametr tuning to that baseline models to get the better accuracy.\n#The algorithms we are implementing is as follows\n1. Logistic Regression\n2. Naive Bayes Model.\n3. Decision tree\n4. Random forest\n5. XG Boost\n6. Ada Boost\n7. Gradient Boosting.\n\n#So, we will be comparing the accuracies and ROC AUC scores and finalise the baseline models.\n","e4e450b6":"* Training my models.","418744d6":"Now we will go for the best ones to go for hyper parameter tuning.","e3fce6b4":"Lets us do the cross validation approach and then move on to hyper parametr tuning.","5680b5d6":"# lets concentrate on ANN","f6706382":"* The people who are having credit card churned more.","f5f123af":"* Dropping the unwanted columns now which are Geography and Gender","828fd3a9":"# Lets concatenate the data frames","311b127b":"* Majority of the customers that churned are those with credit cards.","e89cc762":"The SHAP values for RF explain the margin output of the model.\n\nThis summary plot replaces the typical bar chart of feature importance. It tells which features are most important, and also their range of effects over the dataset. The color allows us match how changes in the value of a feature effect the change in risk.","e0666fa2":"# So, now after this XGBoost,RANDOM FOREST has taken the complete edge over the Decision Tree.","d497d7f3":"* No Strong correlation can be seen on either sides.","a62ef0af":"* lists the most significant variables in descending order. The top variables contribute more to the model than the bottom ones and thus have high predictive power.","4b6675df":"Adding the output layer","ce5ebcd9":"# Compiling the ANN","76f2e461":"* Setting refit='AUC', refits an estimator on the whole dataset with the\n* parameter setting that has the best cross-validated AUC score.","1d3d656a":"* Female is churning more.","ef7f66a8":"* With regard to the tenure, the clients on either extreme end (spent little time with the bank or a lot of time with the bank) are more likely to churn compared to those that are of average tenure.","eb5cba1e":"# GradientBoosting","a859bf29":"* Germany the most churned.","f973ed14":"Adding the Input layer and the first hidden layer","db466077":"# Decision tree","95bd8df7":"Adding the third hidden layer","a886ef0e":"# Lets split the data to train and test","0ec314fe":"Initialising the ANN","d0e26567":"* The older customers are churning at more than the younger ones hinting to a difference in service preference in the age categories. The bank may need to review their target market or review the strategy for retention between the different age groups","3dfa1bae":"The acuuracy is 82.29% for test set.","fe7e8ebe":"* Lets dive into Machine learning models","7f75e717":"# Adaboost","abe129a3":"# Hyper parameter tuning","178ef896":"* Row number, Customer ID, Surname are not part of our analysis, So, I dont include them .","54dc2733":"* Unsurprisingly the inactive members have a greater churn. Worryingly is that the overall proportion of inactive mebers is quite high suggesting that the bank may need a program implemented to turn this group to active customers as this will definately have a positive impact on the customer churn.","e09589dd":"# XG Boost","16f548c6":"* There is not much difference in the credit score distribution between retained and churned customers.","b26df13f":"This is performing well for test dataset. The accuracy turns out to be 95%","e7eca19e":"# Naive Bayes Model","7f15287a":"Only 78% accuracy is obtained.","ea16abd0":"# xgboost","6cd99bc6":"Accuracy and validation accuracy are almost similar.","f5132a03":"# Relations based on the continuous data attributes","6eb1eaa2":"* The accuracy of the train set is 83.09% and the ROC_AUC score is 91% for random forest.\n* The accuracy of the train set is 92.58% and the ROC_AUC score is 97% for xg boost.\n* The accuracy of the train set is 77.7% and the ROC_AUC score is 85.9% for decision tree.","e4ef63d9":"# Predicting the test set results","0df1173a":"# Geography and gender features are categorical and we need to create dumies for them.","43508a8f":" * The collective force plot","fd16d2ea":"# Lets do the hyper parameter tuning to get the best hyper parameters","db9f028f":"* The proportion of female customers churning is also greater than that of male customers","f40239bb":"# Random forest","21034afd":"# confusion matrix and the metrics","a4ca1a58":"# Decision Tree","67306615":"# SHAP Summary Plot","ac4a20b2":"# Data Preprocessing","3af62916":"# Logistic Regression","cd1b1832":"# Lets do the feature scaling","eb6358cd":"# Random forest","547d8611":"We note the following.\n* No Missing values. \n* Number of variables is 14.\n* Number of observations is 10000\n* Numerical variables are 7, categorical variables are 4, and boolean variables are 3.\n* Number of missing values is zero for all variables.\n","8f4ca241":"Adding the second hidden layer","5313365d":"# Final XGBoost Model","7743adc7":"Implementing a cross-validation based approach","cc19ce3a":"* Lets see the proportion of the imbalance of the classes.","540c7592":"# Interpretability of the  ML models.","f71e1c3b":"# Final Random Forest Model.","71f816a6":"# We first review for categorical variables","884ad678":"\n1. Decision Tree\n2. Random forest\n3. xgboost\n\nAre performing well.","65b73764":"# Fitting the ANN to training set"}}