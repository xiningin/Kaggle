{"cell_type":{"9d896aae":"code","2b116cf8":"code","fe28ee98":"code","6baf3a29":"code","737652d8":"code","8f1448f6":"code","2fdaeadb":"code","85871876":"code","54083843":"code","d3d5c9a5":"code","dad17dbd":"code","2f03d8c1":"code","4986e6a6":"code","6b50c428":"code","22bb7834":"code","d9640e26":"code","d4afc345":"code","60793ba8":"code","0051dff4":"code","be26b984":"code","248ba640":"code","674e5ea8":"code","d6c0b511":"code","ff9fa7eb":"code","7f07971e":"code","a999be26":"code","be6f7754":"code","8bc95780":"code","1c569432":"code","1c7615ef":"code","413e3ab2":"code","bbb58409":"code","b220ab81":"code","6906738c":"code","ba509e0e":"code","8db659ea":"markdown","188fae31":"markdown","21443be1":"markdown","313738c9":"markdown","9ec1026f":"markdown","5b8c3adf":"markdown","bea1deea":"markdown","e56736d4":"markdown","61e56177":"markdown","dff23d12":"markdown","1e8543e1":"markdown","74c18bc5":"markdown","560077a1":"markdown","7f30ba25":"markdown","b452c174":"markdown","7cafc67a":"markdown","3bb5cdcf":"markdown","98b9389c":"markdown","289f2daf":"markdown","eacafa32":"markdown","598f4571":"markdown","eea42dc8":"markdown","45d591f0":"markdown","d22453f1":"markdown","e4442395":"markdown"},"source":{"9d896aae":"# We're using support vector machine model here.\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler ","2b116cf8":"test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nsub_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","fe28ee98":"train_data","6baf3a29":"train_data.info()  #for checking null values & data type","737652d8":"test_data.describe() #for gettig some more insights","8f1448f6":"print('Number of null values in different columns are: ')\nprint('--------------------------------------------------')\nprint(train_data.isna().sum())\nprint('--------------------------------------------------')","2fdaeadb":"train_data.loc[(train_data['Cabin'].isna() == False), 'Cabin'] = 1\ntrain_data.loc[(train_data['Cabin'].isna() == True), 'Cabin'] = 0","85871876":"train_data.loc[(train_data['Age'].isna() == True), 'Age'] = train_data['Age'].mean()","54083843":"train_data['Embarked'].mode()","d3d5c9a5":"train_data.loc[(train_data['Embarked'].isna() == True), 'Embarked'] = 'S'","dad17dbd":"print('Number of null values in different columns are: ')\nprint('--------------------------------------------------')\nprint(train_data.isna().sum())\nprint('--------------------------------------------------')","2f03d8c1":"train_data.drop(['PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)","4986e6a6":"train_data.loc[(train_data['Sex'] == 'female'), 'Sex'] = 1\ntrain_data.loc[(train_data['Sex'] == 'male'), 'Sex'] = 0","6b50c428":"ohe = OneHotEncoder(sparse=False, handle_unknown='error', drop='first')\nohe_df = pd.DataFrame(ohe.fit_transform(train_data[['Embarked']]))\n\nohe_df.columns = ohe.get_feature_names(['Embarked'])\n\nohe_df.head()","22bb7834":"train_data = pd.concat([train_data,ohe_df], axis=1)\ntrain_data.drop(['Embarked'], axis = 1, inplace = True)","d9640e26":"ColumnToScale = ['Age', 'Fare']      #(Rest others are almost on same scale)\n\ntrain_data[ColumnToScale] = MinMaxScaler().fit_transform(train_data[ColumnToScale])\n\ntrain_data['Fare'] = train_data['Fare'] * 10     #as most of values in Fare became negligible after scaling","d4afc345":"train_data #Whoa! We changed it...","60793ba8":"train_data.describe()","0051dff4":"train_data.shape","be26b984":"X_train = train_data.iloc[:,1:10].values\nY_train = train_data.iloc[:,0].values","248ba640":"classifier = SVC(kernel='rbf', random_state = 1)\nclassifier.fit(X_train,Y_train)","674e5ea8":"test_data","d6c0b511":"test_data.info()","ff9fa7eb":"test_data.loc[(test_data['Fare'].isna() == True), 'Fare'] = test_data['Fare'].median()","7f07971e":"test_data.loc[(test_data['Cabin'].isna() == False), 'Cabin'] = 1\ntest_data.loc[(test_data['Cabin'].isna() == True), 'Cabin'] = 0","a999be26":"test_data.loc[(test_data['Age'].isna() == True), 'Age'] = train_data['Age'].mean()   \n\n#We'll use train_data average here as well.","be6f7754":"test_data.drop(['PassengerId', 'Name', 'Ticket'], axis = 1, inplace = True)","8bc95780":"test_data.loc[(test_data['Sex'] == 'female'), 'Sex'] = 1\ntest_data.loc[(test_data['Sex'] == 'male'), 'Sex'] = 0","1c569432":"ohe = OneHotEncoder(sparse=False, handle_unknown='error', drop='first')\nohe_df = pd.DataFrame(ohe.fit_transform(test_data[['Embarked']]))\n\nohe_df.columns = ohe.get_feature_names(['Embarked'])\n\nohe_df.head()","1c7615ef":"test_data = pd.concat([test_data,ohe_df], axis=1)\ntest_data.drop(['Embarked'], axis = 1, inplace = True)","413e3ab2":"ColumnToScale = ['Age', 'Fare']      #(Rest others are almost on same scale)\n\ntest_data[ColumnToScale] = MinMaxScaler().fit_transform(test_data[ColumnToScale])\n\ntest_data['Fare'] = test_data['Fare'] * 10     #as most of values in Fare became negligible after scaling","bbb58409":"test_data","b220ab81":"X_test = test_data.iloc[:,0:9].values","6906738c":"Y_pred = classifier.predict(X_test)   ","ba509e0e":"df_sub = sub_data['PassengerId']\ndf_sub = pd.DataFrame(df_sub)\ndf_sub['Survived'] = Y_pred\ndf_sub.to_csv('submission.csv', index = False)\n\ndf_sub.head()","8db659ea":"**Thank you for scrolling down.**\n\nWe haven't placed any graphs & also haven't find single evaluation metric. But I know you can do so yourself as per need.\n\nStay Safe!","188fae31":"## Data Preprocessing","21443be1":"**Age Column**","313738c9":"**Drop some columns**\n\nLets drop some of the columns which doesnt't seem to be relevant. Here we go!","9ec1026f":"**Fare Column**","5b8c3adf":"**Filling Missing Values in Embarked Column**\n\nThere're only 2 values missing. So lets assume those values to be equal to the mode of this column.\n","bea1deea":"**VISUALIZING TRAINING DATASET**","e56736d4":"**Exporting prediction**","61e56177":"**Just re-check for missing values**","dff23d12":"## Importing & visualizing data","1e8543e1":"## Model Applying\n\nNow everything seems good.\nLets apply our learning model. We're using **SVM with Gaussian Kernel** here.","74c18bc5":"**Importing Data**","560077a1":"## Welcome!\n**Suggesstion, if any, are most welcomed. I would love hearing from you & improving myself. Please consider providing your opinion.**\n\n* Model Used: Support Vector Machine with Gaussian Kernel\n* Run Time: 22 sec (may vary in different verions)\n* Score: 0.77990\n\n*This notebook is for beginnner made by a beginner. If you're unable to understand anything leave a comment.*","7f30ba25":"**Feature Scaling**","b452c174":"**Filling Missing Values in Age Column**\n\nLets 1st keep it simple! Just take average age & put it at missing values.","7cafc67a":"**Time to scale the Data**\n\nAlright! Don't you think its a good time to scale the data. Lets do it...","3bb5cdcf":"**Make all columns to have numerical values**\n\n* Put 1 for female & 0 for male in 'Sex' Column\n* Use one-hot encoding for 'Embarked' Column","98b9389c":"## Importing Libraries","289f2daf":"**Cabin Column**","eacafa32":"**Changing all columns' data to numerical values**","598f4571":"**Various columns in dataset are:**\n1. **PassengerId:** Unique Id for each passenger\n1. **Survived:** 0 = No, 1 = Yes\n1. **Pclass:** Ticket class \n1. **Sex:** Gender \n1. **Age:** Age of passenger in years\n1. **SibSp:** Number of siblings\/spouses alongwith\n1. **Parch:** Number of parents\/children alongwith\n1. **Ticket:** Ticket number\n1. **Fare:** Ticket price\n1. **Cabin:** Cabin number\n1. **Embarked:** Embarkation Port (3 entries C, Q, S)","eea42dc8":"## Prediction Making","45d591f0":"Here there're missing values in Age, Fare & Cabin. Age & Cabin can be dealt the same way as we did in Training set. In Fare Column only one value is missing. So we take the median of Fare column & put that missing value equal to that.","d22453f1":"## Preparing Test Data\n\nWe'll apply same steps that we applied on Training set.","e4442395":"**Dealing with Cabin Column**\n\nIn the Cabin column, 687 out of 891 entries are missing. So, here either we should drop this column or assign 1 to that column having Cabin value & 0 that misses the value. Maybe that give some information. Who knows!\n\nLets try it..."}}