{"cell_type":{"fb26f2dc":"code","2e7c26bf":"code","a3234912":"code","65f5ca4d":"code","071d9dd7":"code","21e5741d":"code","3038d813":"code","66ef88cd":"code","0a8006a5":"code","7db1b7c2":"code","39ad7ab4":"code","1c64be50":"code","cc1b3469":"code","20d97c9e":"code","cd72635a":"code","15d4a278":"code","51e7494c":"code","ad6d0208":"code","1d985f13":"code","d4e0203c":"code","f7ac6094":"code","3f22c9f3":"code","469e8a7b":"code","9232d276":"code","3340ffa4":"markdown","17eb4970":"markdown","a4c070d0":"markdown","eba8c4cb":"markdown","f58e15a6":"markdown"},"source":{"fb26f2dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e7c26bf":"# Import \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Dropout,Conv2D,MaxPooling2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping, Callback\nfrom keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\n\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a3234912":"print(tf.config.list_physical_devices('GPU'),'\/\/',tf.test.is_built_with_cuda())","65f5ca4d":"# Read data\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\ntrain.head()","071d9dd7":"train.describe()","21e5741d":"# Checking missing values in train and test set\nprint('N\u00ba of missing values in train set: ', train.isnull().any().sum())\nprint()\nprint('N\u00ba of missing values in test set: ', test.isnull().any().sum())","3038d813":"X = np.array(train.drop('label',axis=1))\/ 255.\nX = X.reshape((-1,28,28,1))\n\ny = np.array(train['label'])","66ef88cd":"# Split train and test data\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n\nprint('X_train: ' + str(train_X.shape))\nprint('Y_train: ' + str(train_y.shape))\nprint('X_test:  '  + str(test_X.shape))\nprint('Y_test:  '  + str(test_y.shape))","0a8006a5":"# pick a sample to plot\nsample = 5\nimage = train_X[sample].reshape(28,28)\n\n# plot the sample\nfig = plt.figure\nplt.imshow(image, cmap='gray')\nplt.show()","7db1b7c2":"num = 10 # Remember num = num_row * num_col or some images will be blank\nnum_row = 2\nnum_col = 5\n\n# plot images\nfig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\nfor i in range(num):\n    ax = axes[i\/\/num_col, i%num_col]\n    ax.imshow(train_X[i].reshape(28,28), cmap='gray')\n    ax.set_title('Label: {}'.format(train_y[i]))\nplt.tight_layout()\nplt.show()","39ad7ab4":"# defining model parameters \nbatch_size = 128\nepochs = 110\nepochs_to_wait_to_improve = 10\nnum_classes = max(pd.unique(train['label'])) +1 # 10 classes\n\n# fix random seed for reproducibility\nseed = 7\nrandom.seed(seed)","1c64be50":"# create a data generator\ndatagen = ImageDataGenerator(\n                             rotation_range=12,\n                             width_shift_range=0.11,\n                             height_shift_range=0.11,\n                             shear_range=0.15,\n                             zoom_range = 0.09, \n                             validation_split=0.3,\n                             horizontal_flip=False, \n                             vertical_flip=False\n                            )","cc1b3469":"# Define the training generator\ntrain_generator = datagen.flow(train_X, \n                               train_y, \n                               batch_size=batch_size,\n                               shuffle=True,\n                               subset='training')\n\n# Define the testing generator\nval_generator = datagen.flow(test_X, \n                             test_y, \n                             batch_size=batch_size,\n                             subset='validation')","20d97c9e":"# Define the CNN model \n\nmodel = Sequential() # Initialize the sequential model\n\n# Add CNN convolutions with BatchNormalization and MaxPooling2D\n# Avoid overfitting with Dropout\nmodel.add(Conv2D(32, kernel_size = (3,3), input_shape=(28, 28, 1), padding = 'Same', activation='relu'))\nmodel.add(Conv2D(64, kernel_size = (3,3), padding = 'Same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, kernel_size = (3,3), padding = 'Same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=1, padding='valid'))\nmodel.add(Dropout(0.2))\n\n# Convert our matrix to 1-D set of features \nmodel.add(Flatten())\n\n# Add fully-conected layers\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.20))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.35))\nmodel.add(Dense(num_classes, activation='softmax'))\n","cd72635a":"# Defining the call backs EarlyStopping and myCallback which will stop the training\n# if the accuracy reaches 99%\n\nclass myCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')>0.999):\n            print(\"\\nReached 99.9% accuracy so cancelling training!\")\n            self.model.stop_training = True\n\n# Instantiate callback\nmycallback = myCallback()\n\n\nearly_stopping_callback = EarlyStopping(monitor='val_loss', \n                                        patience=epochs_to_wait_to_improve,\n                                        verbose = 2,\n                                        restore_best_weights=True)\n\n# Define the optimizer\noptimizer = Adam(lr=0.001, beta_1=0.9)\n#optimizer = 'RMSprop'\n\n# Compile the model\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","15d4a278":"model.summary()","51e7494c":"history = model.fit(train_generator,\n          epochs=epochs,\n          validation_data=val_generator, \n          callbacks=[mycallback,early_stopping_callback])","ad6d0208":"# Visualize training results with matplotlib\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","1d985f13":"# evaluate model with test_X from train_test_split\ntest_loss, test_acc = model.evaluate(test_X, test_y, verbose=5)\n\nprint('\\nTest accuracy:', test_acc)\n","d4e0203c":"# Get confusion matrix with test_X\nY_pred = model.predict(test_X)\nval_preds = np.argmax(Y_pred, axis=1)\n\nval_trues = test_y\ncm = metrics.confusion_matrix(val_trues, val_preds)\ncm\n\nclass_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n# Plot confusion matrix in a beautiful manner\nfig = plt.figure(figsize=(16, 14))\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n# labels, title and ticks\nax.set_xlabel('Predicted', fontsize=20)\nax.xaxis.set_label_position('bottom')\nplt.xticks(rotation=90)\nax.xaxis.set_ticklabels(class_names, fontsize = 10)\nax.xaxis.tick_bottom()\n\nax.set_ylabel('True', fontsize=20)\nax.yaxis.set_ticklabels(class_names, fontsize = 10)\nplt.yticks(rotation=0)\n\nplt.title('Confusion Matrix', fontsize=20)\n\nplt.show()","f7ac6094":"metrics.classification_report(val_trues, val_preds)","3f22c9f3":"# Visualize the first 4 test samples and show their predicted digit value in the title\ntest_X_reshaped = test_X\n_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\nfor ax, image, prediction in zip(axes, test_X_reshaped, val_preds):\n    ax.set_axis_off()\n    if (len(image.shape) == 3):\n        image = image.reshape(28,28)\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    ax.set_title(f'Prediction: {prediction}')","469e8a7b":"test_pred = np.array(test\/255.)\ntest_pred = test_pred.reshape((-1,28,28,1))\n\ntest_predictions = model.predict_classes(test_pred)","9232d276":"# Submitting predictions\n# Predictions of test.csv\nsub_df = {'ImageId':list(range(1, len(test_predictions) + 1)),'Label': test_predictions}\nsubmission = pd.DataFrame(sub_df).astype('int')\nsubmission.head()\n\nsubmission.to_csv('submission.csv', index=False)","3340ffa4":"# Prepare and train the model","17eb4970":"# Evaluate model\n\nCompute accuracy and loss values in test set. Print confusion matrix of the test set with ground truth values and predicted values.","a4c070d0":"# Predictions submition","eba8c4cb":"# Visualize training results\n\n","f58e15a6":"# Data augmentation with DataGenerator"}}