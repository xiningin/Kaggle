{"cell_type":{"9ff45cdb":"code","ad40cdb9":"code","ddcb6448":"code","38675539":"code","be2f8a03":"code","a05fddd5":"code","ccafe162":"code","d3c6e9ce":"code","27b0f855":"code","b2990634":"code","3e1abfde":"code","96e91912":"code","826fb330":"code","685eff7f":"code","313a4f1e":"code","5708c120":"code","0fc7c67f":"code","e7f4e07b":"code","82239d4c":"code","a8508dcc":"code","a42bbada":"code","6f58ad04":"code","bf6aabfb":"code","a44a7a06":"code","ed3d0c89":"code","0bb1cc5e":"code","32238e17":"code","d957cff0":"code","edee1778":"code","5c7f5506":"code","edf1d58f":"code","0bc744e4":"code","97edc8db":"code","227a4c93":"code","2a5a789e":"code","df65f370":"code","61e93e67":"code","084e04b5":"code","e067a7fc":"code","db4a4f4b":"code","eb0746c1":"code","d660f06d":"code","3c5bcae2":"code","e3433bfa":"code","07f9886d":"code","53019245":"markdown","cda9ad90":"markdown","40caacb9":"markdown","8f942562":"markdown","9bb79183":"markdown","3721e31f":"markdown","114ad416":"markdown","6b1abe7c":"markdown","3d1169af":"markdown","23e52e14":"markdown","d83c21a5":"markdown","85e9738b":"markdown","3c62ee1e":"markdown","66d3cd38":"markdown","65f12bdc":"markdown","6599d0ba":"markdown","32e51082":"markdown","d3a60bc3":"markdown","adb14ffc":"markdown","799cc4eb":"markdown","fb378aa0":"markdown","aaed69b6":"markdown","fc449c70":"markdown","dce530a2":"markdown","71927376":"markdown","02c059cf":"markdown","cb58076e":"markdown","0a6b313d":"markdown","582fa67c":"markdown","99747338":"markdown","cecf6bbe":"markdown","846f86a7":"markdown","6fb9a7b5":"markdown","7a4355b9":"markdown","de993d41":"markdown","0ce39003":"markdown","c2a3c4d8":"markdown","c57c6652":"markdown","ddc5b21c":"markdown","d9526407":"markdown","4f111eb1":"markdown"},"source":{"9ff45cdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd #data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\n#sns.set_style(\"whitegrid\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad40cdb9":"train = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/test.csv')\nstores = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/stores.csv')\ntransactions = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/transactions.csv')\noil = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/oil.csv')\nholidays_events = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/holidays_events.csv')","ddcb6448":"##TODO: add train insights regarding the datapoints\ntrain.info()","38675539":"# add store information\ntrain = pd.merge(train, stores, how='left', on='store_nbr')","be2f8a03":"fig = go.Figure(data=[go.Table(header=dict(values=['KPI', 'Value']),\n                 cells=dict(values=[['Number of Stores', 'Number of Different Products', \n                                     'Window Start Date', 'Window End Date',\n                                    '#Rows in training set', '#Date Points in Train Dataset'], \n                                    [train['store_nbr'].nunique(), train['family'].nunique(), \n                                     train['date'].min(), train['date'].max(),\n                                    train.shape[0], train['date'].nunique()]]))\n                     ])\n\nfig.update_layout({\"title\": f'BASIC KPIS of TRAIN DATA'}, height=500, width=500)\nfig.show()","a05fddd5":"train_aux = train[['date', 'sales', 'onpromotion']].groupby('date').mean()\ntrain_aux = train_aux.reset_index()\nfig = go.Figure(data=go.Scatter(x=train_aux['date'], \n                                y=train_aux['sales'],\n                                marker_color='red', text=\"sales\"))\nfig.update_layout({\"title\": f'Avg Sales by date for all stores and products',\n                   \"xaxis\": {\"title\":\"Date\"},\n                   \"yaxis\": {\"title\":\"Avg Unit Sold\"},\n                   \"showlegend\": False})\nfig.show()","ccafe162":"fig = px.scatter(train_aux[train_aux['onpromotion'] > 0], x=\"onpromotion\", y=\"sales\", color='sales', \n                           color_continuous_scale=\"earth\",\n                 size='sales', log_x=True, size_max=30)\n\nfig.update_layout({\"title\": f'Correlation between OnPromotion and Sales (total avg sales and promotion on each day)',\n                   \"xaxis\": {\"title\":\"On Promotion\"},\n                   \"yaxis\": {\"title\":\"Sales\"},\n                   \"showlegend\": False})\nfig.show()","d3c6e9ce":"#create new features\ntrain['year'] = pd.to_datetime(train['date']).dt.year\ntrain['month'] = pd.to_datetime(train['date']).dt.strftime(\"%B\")\ntrain['day_of_week'] = pd.to_datetime(train['date']).dt.day_name()","27b0f855":"df_year_s = train.groupby('year').mean()[['sales']]\ndf_year_s = df_year_s.reset_index()\ndf_year_s['color'] =['rgb(210, 251, 212)', 'rgb(165, 219,194)', 'rgb(123,188, 176)', 'rgb(85, 156,158)', 'rgb(58,124, 137)']\n\ndf_month_s = train.groupby('month').mean()[['sales']]\ndf_month_s = df_month_s.sort_values('sales', ascending=True)\ndf_month_s['color'] = ['#bfbf40','#abab39','#989833','#85852c','#727226','#5f5f20','#5f5f20','#4c4c19','#393913','#26260c','#131306','#000000']\nnew_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\ndf_month_s = df_month_s.reindex(new_order, axis=0)\ndf_month_s = df_month_s.reset_index()\n\n\ndf_day_of_week_s = train.groupby('day_of_week').mean()[['sales']]\ndf_day_of_week_s = df_day_of_week_s.sort_values('sales', ascending=False)\ndf_day_of_week_s['color'] = ['rgb(255, 0, 0)','rgb(255, 36, 36)','rgb(255, 71, 71)','rgb(255, 107, 107)','rgb(255, 143, 143)','rgb(255, 179, 179)','rgb(255, 214, 214)']\nnew_order_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\ndf_day_of_week_s = df_day_of_week_s.reindex(new_order_week, axis=0)\ndf_day_of_week_s = df_day_of_week_s.reset_index()","b2990634":"df_year = train.groupby('year').mean()[['onpromotion']]\ndf_year = df_year.reset_index()\ndf_year['color'] =['rgb(210, 251, 212)', 'rgb(165, 219,194)', 'rgb(123,188, 176)', 'rgb(85, 156,158)', 'rgb(58,124, 137)']\n\n\n\ndf_month = train.groupby('month').mean()[['onpromotion']]\ndf_month = df_month.sort_values('onpromotion', ascending=True)\ndf_month['color'] = ['#bfbf40','#abab39','#989833','#85852c','#727226','#5f5f20','#5f5f20','#4c4c19','#393913','#26260c','#131306','#000000']\nnew_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\ndf_month = df_month.reindex(new_order, axis=0)\ndf_month = df_month.reset_index()\n\n\ndf_day_of_week = train.groupby('day_of_week').mean()[['onpromotion']]\ndf_day_of_week = df_day_of_week.sort_values('onpromotion', ascending=False)\ndf_day_of_week['color'] = ['rgb(255, 0, 0)','rgb(255, 36, 36)','rgb(255, 71, 71)','rgb(255, 107, 107)','rgb(255, 143, 143)','rgb(255, 179, 179)','rgb(255, 214, 214)']\nnew_order_week = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\ndf_day_of_week = df_day_of_week.reindex(new_order_week, axis=0)\ndf_day_of_week = df_day_of_week.reset_index()","3e1abfde":"fig = make_subplots(rows=3, cols=2, \n                    subplot_titles=(\"Avg Sales by Year\", \"Avg On Promotion by Year\", \"Avg Sales by Month\",\n                                   \"Avg On Promotion by MOnth\", \"Avg Sales by Day of Week\", \"Avg On Promotion by Day of Week\"))\n#SALES \nfig.append_trace(go.Bar(x=df_year_s['year'], y=df_year_s['sales'], marker = {'color': list(df_year_s['color'])}),\n                row=1, col=1)\n\n\nfig.append_trace(go.Bar(x=df_month_s['month'], y=df_month_s['sales'], marker = {'color': list(df_month_s['color'])}), \n                 row=2, col=1)\n\nfig.append_trace(go.Bar(x=df_day_of_week_s['day_of_week'], y=df_day_of_week_s['sales'], marker = {'color': list(df_day_of_week_s['color'])}), row=3, col=1)\n\n##ONPROMOTION\nfig.append_trace(go.Bar(x=df_year['year'], y=df_year['onpromotion'], marker = {'color': list(df_year['color'])}),\n                row=1, col=2)\n\n\nfig.append_trace(go.Bar(x=df_month['month'], y=df_month['onpromotion'], marker = {'color': list(df_month['color'])}), \n                 row=2, col=2)\n\nfig.append_trace(go.Bar(x=df_day_of_week['day_of_week'], y=df_day_of_week['onpromotion'],\n                        marker = {'color': list(df_day_of_week['color'])}), row=3, col=2)\n#styling\n#fig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\n#fig.update_xaxes(visible=False, row=1, col=1)\n\nfig.update_layout(height=1000, width=1400, title_text=\"AVERAGE SALES & ONPROMOTION ANALYSIS\",  \n                  title_font=dict(size=30, color='#8a8d93'), showlegend=False)\nfig.show()","96e91912":"df_sesonality = train.copy()\n\ndf_sesonality = df_sesonality.groupby('date').mean()[['sales']].reset_index()\n#create month and year variables from date colum\ndf_sesonality['date'] = pd.to_datetime(df_sesonality['date'])\ndf_sesonality['year'] = df_sesonality['date'].dt.year\ndf_sesonality['month'] = df_sesonality['date'].dt.month\ndf_sesonality['weekofyear'] = df_sesonality['date'].dt.weekofyear\ndf_sesonality['day'] = df_sesonality['date'].dt.day\ndf_sesonality['dayofweek'] = df_sesonality['date'].dt.day_name()\n#let\u00b4s calculate de avg of units sold by Favorita stores everyday","826fb330":"variable = 'sales'\nfig, ax = plt.subplots(figsize=(15, 6))\n\npalette = sns.color_palette(\"ch:2.5,-.2,dark=.3\", 10)\nsns.lineplot(df_sesonality['month'], df_sesonality[variable], hue=df_sesonality['year'])\nax.set_title('Seasonal plot of Sales', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Month', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Sales', fontsize = 16, fontdict=dict(weight='bold'))\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(17, 6))\n\nsns.boxplot(df_sesonality['year'], df_sesonality[variable], palette=\"turbo\",ax=ax[0])\nax[0].set_title('Year-wise Box Plot\\n(The Trend)', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax[0].set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax[0].set_ylabel('Sales by Year', fontsize = 16, fontdict=dict(weight='bold'))\n\nsns.boxplot(df_sesonality['month'], df_sesonality[variable], palette=\"Pastel2\", ax=ax[1])\nax[1].set_title('Month-wise Box Plot\\n(The Seasonality)', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax[1].set_xlabel('Month', fontsize = 16, fontdict=dict(weight='bold'))\nax[1].set_ylabel('Sales by Month', fontsize = 16, fontdict=dict(weight='bold'))\n\n#plot boxplots for every week of the year\nfig = plt.figure(figsize=(17,7))\nsns.boxplot(df_sesonality['weekofyear'], df_sesonality['sales'], palette=\"magma\")\n\n#plot boxplots for every day\nfig = plt.figure(figsize=(17,7))\nsns.boxplot(df_sesonality['day'], df_sesonality['sales'], palette=\"YlOrRd\")\n\n#plot boxplots for every day\nfig = plt.figure(figsize=(17,7))\nsns.boxplot(df_sesonality['dayofweek'], df_sesonality['sales'], palette=\"GnBu\",\n            order=['Monday', 'Tuesday', 'Wednesday', \n                    'Thursday', 'Friday', 'Saturday','Sunday'])","685eff7f":"LAGS = [16, 17, 18, 21, 28, 29, 35, 42, 49, 56]","313a4f1e":"df_lag = train.copy()\nfor lag in LAGS:\n    df_lag[f\"sales_lag_{lag}\"] = df_lag.groupby(['store_nbr', 'family'])['sales'].transform(lambda x: x.shift(lag))","5708c120":"##check heatmap of lag features. \ndf_lag_corr = df_lag[[col for col in list(df_lag.columns) if col.startswith('sales_')]]\nplt.figure(figsize=(10,10))\nsns.heatmap(df_lag_corr.corr(), annot=True, cmap=\"YlGnBu\")\nplt.show()","0fc7c67f":"category = 'BEVERAGES'\nSTORE_NBR = 45\ndf_aux = df_lag[(df_lag['family'] == category) & (train['store_nbr'] == STORE_NBR)]\n#df_aux = df_lag[(df_lag['family'] == category)]\n#df_aux = df_lag\nxlim = (0, 26000)\nylim = (0, 26000)","e7f4e07b":"fig, axes = plt.subplots(2, 5, figsize=(20,7))\naxes[0,0].set_xlim(xlim)\naxes[0,0].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_16\", y=\"sales\", ax=axes[0,0])\naxes[0,1].set_xlim(xlim)\naxes[0,1].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_17\", y=\"sales\",  ax=axes[0,1])\naxes[0,2].set_xlim(xlim)\naxes[0,2].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_18\", y=\"sales\",  ax=axes[0,2])\naxes[0,3].set_xlim(xlim)\naxes[0,3].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_21\", y=\"sales\",  ax=axes[0,3])\naxes[0,4].set_xlim(xlim)\naxes[0,4].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_28\", y=\"sales\",  ax=axes[0,4])\n###\naxes[1,0].set_xlim(xlim)\naxes[1,0].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_29\", y=\"sales\",  ax=axes[1,0])\naxes[1,1].set_xlim(xlim)\naxes[1,1].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_35\", y=\"sales\",  ax=axes[1,1])\naxes[1,2].set_xlim(xlim)\naxes[1,2].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_42\", y=\"sales\",  ax=axes[1,2])\naxes[1,3].set_xlim(xlim)\naxes[1,3].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_49\", y=\"sales\",  ax=axes[1,3])\naxes[1,4].set_xlim(xlim)\naxes[1,4].set_ylim(ylim)\nsns.regplot(data=df_aux, x=\"sales_lag_56\", y=\"sales\",  ax=axes[1,4])\n\n\n","82239d4c":"# filter holidays for the training dataset window.\nholidays_events = holidays_events[(holidays_events['date'] >= \"2013-01-01\") & (holidays_events['date'] <= \"2017-08-15\")]","a8508dcc":"holidays_events.head()","a42bbada":"##Let's look at the sales behavior for the whole data\ntrain_aux = train[['date', 'sales']].groupby('date').mean()\ntrain_aux = train_aux.reset_index()\nfig = go.Figure(data=go.Scatter(x=train_aux['date'], \n                                y=train_aux['sales'],\n                                marker_color='red', text=\"sales\"))\nfor holiday_date in list(holidays_events['date']):\n    fig.add_vline(x=holiday_date, line_width=0.5, line_dash=\"dash\", line_color=\"green\")\n#fig.add_vline(x=\"2013-08-08\", line_width=0.5, line_dash=\"dash\", line_color=\"green\", annotation=\"test\")\n\n\nfig.update_layout({\"title\": f'Avg Sales by date with Holidays Events',\n                   \"xaxis\": {\"title\":\"Date\"},\n                   \"yaxis\": {\"title\":\"Avg Unit Sold\"},\n                   \"showlegend\": False})\nfig.show()","6f58ad04":"df_plot = pd.merge(holidays_events, train_aux, on='date', how='inner')\ndf_plot.loc[df_plot['description'].isin(['Black Friday', 'Cyber Monday']), 'type'] = 'black_friday_cyber_monday'","bf6aabfb":"#pd. set_option(\"display.max_rows\", 300)\n#df_plot","a44a7a06":"fig = px.scatter(df_plot, x=\"date\", y=\"sales\", size='sales', color='type')\n                 #size='sales', color='sales',\n                  #color_continuous_scale=\"pinkyl\")\n\nfig.update_layout({\"title\": f'Avg Sales on Holiday Events days',\n                   \"xaxis\": {\"title\":\"HOLIDAY EVENT DATE\"},\n                   \"yaxis\": {\"title\":\"Avg Sales\"},\n                   \"showlegend\": True})\n\nfig.add_annotation(x='2014-07-05',y=500,xref=\"x\",yref=\"y\",text=\"WORLD CUP\",showarrow=True, align=\"center\",arrowhead=2,arrowsize=1,\n        arrowwidth=2,arrowcolor=\"#636363\",ax=0,ay=-30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"#ca8ee8\",opacity=0.8  )\n\nfig.add_annotation(x='2016-04-20',y=800,xref=\"x\",yref=\"y\",text=\"EARTHQUAKE\",showarrow=True,align=\"center\",arrowhead=2,arrowsize=1,\n        arrowwidth=2,arrowcolor=\"#636363\",ax=0,ay=-30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"#ca8ee8\",opacity=0.8)\n\nfig.add_annotation(x='2013-12-30',y=200,xref=\"x\",yref=\"y\",text=\"CHRISTAMS 13\/14\",showarrow=True,align=\"center\",arrowhead=2,arrowsize=1,\n        arrowwidth=2,arrowcolor=\"#636363\",ax=0,ay=30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"#3ce685\",opacity=0.8)\n\n\nfig.add_annotation(x='2014-12-30',y=200,xref=\"x\",yref=\"y\",text=\"CHRISTAMS 14\/15\",showarrow=True,align=\"center\",arrowhead=2,arrowsize=1,\n        arrowwidth=2,arrowcolor=\"#636363\",ax=0,ay=30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"#3ce685\",opacity=0.8)\n\n\nfig.add_annotation(x='2015-12-30',y=200,xref=\"x\",yref=\"y\",text=\"CHRISTAMS 15\/16\",showarrow=True,align=\"center\",arrowhead=2,arrowsize=1,\n        arrowwidth=2,arrowcolor=\"#636363\",ax=0,ay=30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"#3ce685\",opacity=0.8)\n\n\nfig.add_annotation(x='2016-12-30',y=200,xref=\"x\",yref=\"y\",text=\"CHRISTAMS 16\/17\",showarrow=True,align=\"center\",arrowhead=2,arrowsize=1,\n        arrowwidth=2,arrowcolor=\"#636363\",ax=0,ay=30,bordercolor=\"#c7c7c7\",borderwidth=2,borderpad=4,bgcolor=\"#3ce685\",opacity=0.8)\n\n\n\nfig.show()","ed3d0c89":"#create a list with the dates of when salaries are paid. \n#start_date = '2013-01-01'\n#end_date = '2017-08-15'\n#salary_dates = pd.date_range(start=start_date , end=end_date , freq='SM').tolist()\n#salary_dates = [pd.to_datetime(date.to_datetime64()).strftime('%Y-%m-%d') for date in salary_dates]\n#print(len(salary_dates))","0bb1cc5e":"##Let's look at the sales behavior for the whole data\n#train_aux = train[['date', 'sales']].groupby('date').mean()\n#train_aux = train_aux.reset_index()\n#fig = go.Figure(data=go.Scatter(x=train_aux['date'], \n#                                y=train_aux['sales'],\n#                                marker_color='brown', text=\"sales\"))\n#for salary_date in salary_dates:\n#    fig.add_vline(x=salary_date, line_width=0.5, line_dash=\"dash\", line_color=\"blue\")\n\n\n#fig.update_layout({\"title\": f'Avg Sales by date with Holidays Events',\n#                   \"xaxis\": {\"title\":\"Date\"},\n #                  \"yaxis\": {\"title\":\"Avg Unit Sold\"},\n  #                 \"showlegend\": False})\n#fig.show()","32238e17":"oil.info()","d957cff0":"#price of oil on 2013-01-01 (first element of the series) is missing, let\u00b4s fill it with the value of the next day and interpolate the next ones. \noil.loc[oil['date'] == '2013-01-01', 'dcoilwtico'] = 93.14\noil = oil.interpolate(method='linear', limit=20)","edee1778":"##Let's look at the sales behavior for the whole data\ntrain_aux = train[['date', 'sales']].groupby('date').mean()\ntrain_aux = train_aux.reset_index()\nfig = go.Figure(data=go.Scatter(x=oil['date'], \n                                y=oil['dcoilwtico'],\n                                marker_color='blue', text=\"sales\"))\n\n\nfig.update_layout({\"title\": f'Oil Prices Chart',\n                   \"xaxis\": {\"title\":\"Date\"},\n                   \"yaxis\": {\"title\":\"Oil Price\"},\n                   \"showlegend\": False})\nfig.show()","5c7f5506":"#Scatter plot to the see correlation between average unit sold and oil price each day \nsales_oil = train.groupby('date').mean()['sales']\nsales_oil = sales_oil.reset_index()\nsales_oil = pd.merge(sales_oil, oil, on ='date', how='left')\n# we don't have all the oil prices available, we impute them \nsales_oil = sales_oil.interpolate(method='linear', limit=20)","edf1d58f":"fig = px.scatter(sales_oil, x=\"dcoilwtico\", y=\"sales\", size='sales', color='sales',\n                  color_continuous_scale=\"pinkyl\")\n\nfig.update_layout({\"title\": f'Correlation between Oil Prices and Sales (total avg sales and promotion each day)',\n                   \"xaxis\": {\"title\":\"Oil Price\"},\n                   \"yaxis\": {\"title\":\"Sales\"},\n                   \"showlegend\": False})\nfig.show()","0bc744e4":"oil = pd.read_csv('\/kaggle\/input\/store-sales-time-series-forecasting\/oil.csv')\noil.loc[oil['date'] == '2013-01-01', 'dcoilwtico'] = 93.14\noil = oil.interpolate(method='linear')\n#oil['date'] = pd.to_datetime(oil['date'])\ndf_oil = pd.merge(train, oil, on='date', how='outer')\ndf_oil = df_oil.interpolate(method='ffill', limit=20000)\n\ndf_oil = df_oil[['date', 'dcoilwtico']].drop_duplicates().reset_index(drop=True)\n\n#compute lags for oil price\nLAGS_OIL = [1,2,3,4,5,6,7, 16,21,28,35, 90, 180, 365]\n\n#compute lags for oil prices\ndf_lag_oil = df_oil.copy()\nfor lag in LAGS_OIL:\n    df_lag_oil[f\"oil_lag_{lag}\"] = df_lag_oil['dcoilwtico'].transform(lambda x: x.shift(lag))\n    \ndf_aux = pd.merge(train, df_lag_oil, on='date', how='left')\ndf_aux = df_aux.groupby('date').mean()[['sales', 'dcoilwtico'] + [col for col in list(df_aux.columns) if col.startswith('oil_lag')]].reset_index()\ndf_aux = df_aux[['sales', 'dcoilwtico'] + [col for col in list(df_aux.columns) if col.startswith('oil_lag')]]\nplt.figure(figsize=(15,10))\nsns.heatmap(df_aux.corr(), annot=True)","97edc8db":"## Ranking of units solds by products at each store. \ndf_family = train[['family', 'sales']].groupby('family').mean().sort_values('sales', ascending=True)\ndf_family = df_family.reset_index()\ndf_family['sales'] = df_family['sales'] ","227a4c93":"fig = px.bar(df_family,  x='sales', y='family', color='sales', color_continuous_scale=\"earth\")\nfig.update_layout({\"title\": f'AVG SALES FOR EACH FAMILTY PRODUCT',\n                   \"xaxis\": {\"title\":\"Avg Unit Sold\"},\n                   \"yaxis\": {\"title\":\"Category Product\"},\n                   \"showlegend\": True},\n                 width=1000,\n                height=700)\n\nfig.show()","2a5a789e":"## Ranking of units solds by store, taking into account all products.\ndf_store = train[['store_nbr', 'sales']].groupby('store_nbr').mean().sort_values('sales', ascending=False)\ndf_store = df_store.reset_index()\n#df_store['store_nbr'] = 'store_' + df_store['store_nbr'].astype(str)\ndf_store['sales'] = df_store['sales'] \ndf_store = pd.merge(df_store, stores, how='left')\ndf_store['store_nbr'] = df_store['store_nbr'].astype(str)","df65f370":"fig = px.bar(df_store, x='store_nbr', y='sales', color='type', category_orders={\"store_nbr\": list(df_store['store_nbr']),\n                             \"sales\": list(df_store['sales'])\n                              })\nfig.update_layout({\"title\": f'AVG SALES FOR EACH STORE NUMBER',\n                   \"xaxis\": {\"title\":\"STORE NUMBER\"},\n                   \"yaxis\": {\"title\":\"Avg Unit Sold\"},\n                   \"showlegend\": True})\n\n\nfig.show()","61e93e67":"df = stores.groupby(['city', 'type']).count()[['store_nbr']].reset_index(level=0).reset_index(level=0)[['city', 'type', 'store_nbr']]\nmap_colors = {'A': '#4d4d00', 'B':'#999900', 'C':'#e6e600', 'D':'#ffff00', 'E':'#ffff99'}\ndf['colors'] = df['type'].map(map_colors)","084e04b5":"fig = make_subplots(rows=1, cols=2,  specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}]],\n                    subplot_titles=(\"Number of Stores in each City by Type\", \"Proportion of number of cities in Ecuador\"))\n#SALES \nfig.append_trace(go.Bar(x=df['city'], y=df['store_nbr'], text = df[\"type\"], marker=dict(color= df['colors'])),\n                row=1, col=1)\n\n\nfig.append_trace(go.Pie(values=stores['city'].value_counts(), labels=stores['city'].value_counts().index,\n                       hoverinfo='label+percent+value', textinfo='label+percent'),\n                 row=1, col=2)\n\n\n\n\n##styling\n#fig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=True, row=1, col=1)\n\nfig.update_layout(template=\"ggplot2\",\n                  bargap=0.4,\n                  height=700,\n                  width=1300,\n                  showlegend=False)\n\nfig.show()","e067a7fc":"df_cities = stores['city'].value_counts().reset_index().rename(columns={\"index\":\"city\", \"city\":\"num_store_per_city\"}) # number of stores per city\ndf_store_aux = train.groupby('city').mean()[['sales', 'onpromotion']]\ndf_store_aux = df_store_aux.reset_index()\ndf_store_aux = pd.merge(df_store_aux, df_cities, on='city', how='left')\nfig = px.scatter(df_store_aux, x=\"sales\", y=\"onpromotion\", color='city', \n                  text=df_store_aux['city'],\n                 size='num_store_per_city', log_x=True, size_max=30)\n\nfig.update_layout({\"title\": f'Avg Sales vs On Promotion for Each City. Size of bubbles is number of store in each city.',\n                   \"xaxis\": {\"title\":\" Avg Sales\"},\n                   \"yaxis\": {\"title\":\"On Promotion\"},\n                   \"showlegend\": True})\nfig.show()","db4a4f4b":"## Ranking of units solds by store, taking into account all products.\ndf_store = train[['state', 'sales']].groupby('state').mean().sort_values('sales', ascending=False)\ndf_store = df_store.reset_index()\n#df_store['store_nbr'] = 'store_' + df_store['store_nbr'].astype(str)\ndf_store['sales'] = df_store['sales'] \ndf_store = pd.merge(df_store, stores, how='left')\ndf_store['state'] = df_store['state'].astype(str)","eb0746c1":"fig = px.bar(df_store, x='state', y='sales')\nfig.update_layout({\"title\": f'TOTAL SALES FOR EACH STATE',\n                   \"xaxis\": {\"title\":\"STORE NUMBER\"},\n                   \"yaxis\": {\"title\":\"Avg Unit Sold\"},\n                   \"showlegend\": True},template=\"ggplot2\")\n\n\nfig.show()","d660f06d":"## Ranking of units solds by store, taking into account all products.\ndf_store = train[['city', 'sales']].groupby('city').sum().sort_values('sales', ascending=False)\ndf_store = df_store.reset_index()\n#df_store['store_nbr'] = 'store_' + df_store['store_nbr'].astype(str)\ndf_store['sales'] = df_store['sales'] \ndf_store = pd.merge(df_store, stores, how='left')\ndf_store['city'] = df_store['city'].astype(str)","3c5bcae2":"fig = px.bar(df_store, x='city', y='sales', category_orders={\"city\": list(df_store['city']),\n                             \"sales\": list(df_store['sales'])\n                              })\nfig.update_layout({\"title\": f'TOTAL SALES FOR EACH CITY',\n                   \"xaxis\": {\"title\":\"STORE NUMBER\"},\n                   \"yaxis\": {\"title\":\"Avg Unit Sold\"},\n                   \"showlegend\": True},template=\"ggplot2\")\n\nfig.show()","e3433bfa":"for family_group in list(train['family'].unique()):\n    df = train[train['family'] == family_group]\n    fig = make_subplots(rows=1, cols=2,  specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]],\n                        subplot_titles=(f\"Sales by Date {family_group}\", f\"Distribution of {family_group}\"))\n\n    #SALES\n    fig.append_trace(go.Scatter(x=df['date'], y=df['sales'],\n                        mode='lines',\n                        name='lines'),\n                    row=1, col=1)\n\n\n    fig.append_trace(go.Histogram(x=df['sales'], histnorm='probability'),\n                     row=1, col=2)\n    \n    fig.update_layout(template=\"ggplot2\",\n                  bargap=0.4,\n                  height=500,\n                  width=1400,\n                  showlegend=False)\n\n    fig.show()","07f9886d":"import plotly.io as pio\nlist(pio.templates)","53019245":"## 5.3 Number of stores in each city by type of store<a class=\"anchor\"  id=\"section5.3\"><\/a>","cda9ad90":"## 5.4 Avg Sales vs On promotion in each city displaying number of stores<a class=\"anchor\"  id=\"section5.4\"><\/a>\n","40caacb9":"- **<font size=\"2\">Increasing trend of the sales.<\/font>**\n- **<font size=\"2\">In the last two years (since July 2015) the trend has been stable (almost stationary TS).<\/font>**\n- **<font size=\"2\">By zooming in one can realize that there is a seasonality every 7 days, same pattern (during the weekends higher sales). The peak of the weekly sesoanlity is on Sundays\/Saturdays.<\/font>**\n- **<font size=\"2\">The 1st of Jan of every year the supermarkets are not open (sales=0)<\/font>.**","8f942562":"## 2.5 Sesonality of All the Time Series <a class=\"anchor\"  id=\"section2.5\"><\/a>\n","9bb79183":"## 2.7 Lags of Sales Variable <a class=\"anchor\"  id=\"section2.7\"><\/a>","3721e31f":"## 2.1 KPI Variables <a class=\"anchor\"  id=\"section2.1\"><\/a>","114ad416":"## 5.2 Avg Sales vs Store Number colored by Store Type <a class=\"anchor\"  id=\"section5.2\"><\/a>\n","6b1abe7c":"## 3.3 Analysis of Salaries \n\n- Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. Let\u00b4s study if supermarket sales are affected by this.\n    ","3d1169af":"- **<font size=\"2\">There is a positive correlation between onpromotion and sales units sold. Thus, when more items of the supermarkets are on promotion, it's more likely to sell them. It means that the price elasticity of some items is high and the demand of some items in the sumermarkets is highly correlated to cjanges in price of the itrems.<\/font>**\n- **<font size=\"2\">What family products have higher elasticity? I guess that the data scientist at Favorita supermarkets have done that analysis. They have determined what product are more sensitve to changes in price by maximixing the revenue.<\/font>**","23e52e14":"- **<font size=\"2\">The chart above clearly tells us that when there are lower oil prices the average units sold increases. Therefore, oil prices will be used as a variable for training.<\/font>**","d83c21a5":"## 2.6 Lags of Sales Variable <a class=\"anchor\"  id=\"section2.6\"><\/a>","85e9738b":"- **<font size=\"2\">The size of the above bubbles represent the number of stores that each city has. We can see that Cayambe has the second highest avg sales despite there is only one store in Cayambe. In Cayambe has the highest avg number of items in promotion. PLease, note that it refers to avg sales not total sales. On average each day, Cayambe sells more than Guayaquil, yet Guayaquil has in total more units sold as represented in the section 5.5 <\/font>**\n- **<font size=\"2\">We can again confirm the linear relationship between sales and onpromotion items. <\/font>**\n- **<font size=\"2\">Although Guayaquil has 8 stores (2nd highest), the avg sales is not as high as expected. There are cities with less stores that have more avg sales. <\/font>**\n- **<font size=\"2\">There are cities like Ambato (2 stores), Daule (1store), Loja (1store), Babahoyo (1 store) that have avg sales higher than 300 (probably because of onpromotion items) <\/font>**","3c62ee1e":"## 5.1 Top Family Products by Avg Sales <a class=\"anchor\"  id=\"section5.1\"><\/a>\n","66d3cd38":"## 2.3 ON PROMOTION VS AVG SALES CHART <a class=\"anchor\"  id=\"section2.3\"><\/a>","65f12bdc":"## 4.3 Oil Lags <a class=\"anchor\"  id=\"section4.3\"><\/a>","6599d0ba":"## 2.4 AVG SALES\/ON PROMOTION BY YEAR, MONTH AND DAY OF WEEK CHARTS <a class=\"anchor\"  id=\"section2.4\"><\/a>","32e51082":"- **<font size=\"2\">There's an increasing trend in sales every year. It probably has to do with the decrease in oil prices (as it gives more purchasing power to consumers) or with growing econonmic conditions of Ecuador.<\/font>**\n- **<font size=\"2\">December is the month with the highst average units sold (Christmas is in December).<\/font>**\n- **<font size=\"2\">During the weekends customers purchase more compared to week days.<\/font>**\n\n- **<font size=\"2\">There has been an exponential increase in teh promotions trhoughout the years. No promotions in 2013.<\/font>**\n- **<font size=\"2\">Highest promotions are during summer (June, July) and December (Christmas).<\/font>**\n- **<font size=\"2\">Altough the highest sale son Sunday, the highest promotions are on Wednesdays and Fridays.<\/font>**","d3a60bc3":"# 6.Sales across different variables <a class=\"anchor\"  id=\"section6\"><\/a>\n- **<font size=\"2\">The goal of this section is to identify the way we are going to train the model. We can train the model all at once or based on the following charts in would make sense to train one model for each product family, city, state or sotre number. <\/font>**\n","adb14ffc":"- **<font size=\"2\">We can see different sesonal patterns in the day of the week, day, week of year and month. For the feature engineering part, we will derive these features from the date column.<\/font>**","799cc4eb":"**Table of Contents**\n* [1. INTRODUCTION & GOAL](#section1)\n\n* [2. ANALYSIS OF TRAIN DATASET](#section2)\n    * [2.1 KPI Variables](#section2.1)\n    * [2.2 Chart Time Series Avg Sales On Each Day](#section2.2)\n    * [2.3 On Promotion vs Avg Sales Chart](#section2.3)\n    * [2.4 Avg Sales\/On Promotion By Year, Month and Day Of Week](#section2.4)\n    * [2.5 Sesonality of All the Time Series](#section2.5)\n    * [2.6 Lags of Sales Variables](#section2.6)\n    * [2.7 Lag PLots](#section2.7)\n    \n    \n* [3. HOLIDAYS & EVENTS](#section3)\n    * [3.1 Average sales chart with events](#section3.1)\n    * [3.2 Avg sales on Event Dates](#section3.2)\n    * [3.3 Analysis of Salaries](#section3.3)\n    \n* [4. OIL DATA](#section4)\n    * [4.1 Historic Oil Prices Chart ](#section4.1)\n    * [4.2 Avg Sales vs Oil Prices Chart](#section4.2)\n    * [4.3 Oil Lags](#section4.3)\n    \n    \n* [5. STORE ANALYSIS ](#section5)\n    * [5.1 Historic Oil Prices Chart ](#section5.1)\n    * [5.2 Top Family Products by Avg Sales](#section5.2)\n    * [5.3 Number of stores in each city by type of store](#section5.3)\n    * [5.4 Avg Sales vs On promotion in each city displaying number of stores](#section5.4)\n    * [5.5 Avg Total Units Sold by State and City](#section5.5)\n    \n* [6.Sales across different variables ](#section6)\n    * [6.1 Sales by product category for all the stores](#section6.1)\n\n    \n    \n    \n    \n\n ","fb378aa0":"- **<font size=\"2\">Stores of type A (blue ones) have the highest avg sales, whereas stores of type C (purple ones) and E (orange)the lowest. Type D stores (red) have a high variability <\/font>**\n- **<font size=\"2\">Special mention to store number 52, which is type A, but has the lowest average sales.  <\/font>**","aaed69b6":"- **<font size=\"2\">In the above chart we can also confirm the average increasing trend in sales.<\/font>**\n- **<font size=\"2\">We can clearly see two zones with purple bubbles on consecutive days. If we check the description column, we can clearly see that the purple bubbles of June 2014 correspond to the Football World Cup of Brazil and the purple bubbles of Apr 2016 correspond to days off given by the goverment due to the earthquake. During these two events, the sales went up.<\/font>**\n- **<font size=\"2\">The majority of light green bubles at the end of every year correspond to Christmas celebrations (in December). During this time, people gather togeteher and spend money on quality food and presents for Christmas gatherings with family and friends. We can see that on these days he avg amount spent is he highest for each year.<\/font>**\n- **<font size=\"2\">Black Friday and Ciber Monday (light blue) are also considered. From the chart we can realize that in 2013 and 2014 Black Friday didn't exist in Ecuador. Regarding the avg sales, we can see that they are not as high as expected if we compare them with Christmas sales. It might be because the supermarkets are brick and mortar stores, not e-commerce sites.  <\/font>**","fc449c70":"## 4.2 Avg Sales vs Oil Prices Chart <a class=\"anchor\"  id=\"section4.2\"><\/a>","dce530a2":"## 3.1 Average sales chart with events<a class=\"anchor\"  id=\"section3.1\"><\/a>\n","71927376":"- **<font size=\"2\">A lag plot is used to help evaluate whether the values in a dataset or time series are random. If the data are\nrandom, the lag plot will exhibit no identifiable pattern. If the data are not random, the lag plot will demonstrate a\nclearly identifiable pattern. The type of pattern can aid the user in identifying the non-random structure in the\ndata. Lag plots can also help to identify outliers<\/font>**\n\n- **<font size=\"2\">Lag plots can provide answers to the following questions:<\/font>**\n    - **<font size=\"2\">Is the data random?<\/font>**\n    - **<font size=\"2\">Is there serial correlation in the data?<\/font>**\n    - **<font size=\"2\">What is a suitable model for the data?<\/font>**\n    - **<font size=\"2\">Are there outliers in the data?<\/font>**\n\n\n- **<font size=\"2\">If the lag plot of the data set exhibits a linear pattern, it shows that the data are strongly non-random and further suggests that an autoregressive (AR) model might be appropriate.<\/font>**\n\n\n","02c059cf":"## 5.5 Avg Total Units Sold by State and City <a class=\"anchor\"  id=\"section5.5\"><\/a>\n- **<font size=\"2\"> This section aims to identify the cities and states that sell the most. <\/font>**\n","cb58076e":"- **<font size=\"2\">Higher oil prices tend to make production more expensive for businesses, just as they make it more expensive for households to do the things they normally do. It turns out that oil and gasoline prices are indeed very closely related.<\/font>**\n- **<font size=\"2\">At a consumer level, lower oil prices means more purchasing power for the customers. This explains why there's an increase in average sales since mid-2015.<\/font>**\n- **<font size=\"2\">Oil prices will be used as a variable for training.<\/font>**","0a6b313d":"## 3.2 Avg sales on Event Dates<a class=\"anchor\"  id=\"section3.2\"><\/a>","582fa67c":"# 4. OIL DATA<a class=\"anchor\"  id=\"section4\"><\/a>\n\n- Ecuador is highly dependant on oil prices, therefore the prices of some items might be affected by variations in the oil prices. \n- The oil industry is driven by booms and busts. Prices typically rise during periods of global economic strength during which demand outpaces supply. Prices fall when the reverse is true, and supply exceeds demand. Meanwhile, oil supply and demand are driven by a number of key factors:\n\n    - Changes in the value of the U.S. dollar\n    - Changes in the policies of the Organization of Petroleum Exporting Countries (OPEC)\n    - Changes in the levels of oil production and inventory\n    - The health of the global economy\n    - The implementation (or collapse) of international agreements","99747338":"- **<font size=\"2\">Analyzing the graphs of sales by product family we see two possible outcomes. The distribution is continuos or the it has interment values (some periods of time without any sales, ex: BABY CARE). This introduces a challenge as we will have a lot of zero-inflated distributions. The distributions can be modelled with Tweedie distributions. It  has an important chunk of data items at zero and this property makes it useful in modelling any zero-inflated distributions. As we can see all of the distributions are zero-inflated.<\/font>**\n- **<font size=\"2\">In the following image we can see different tweedie distributions. By adjusting p of the tweedie distribution we can match the shape of the distribution of the sales by product family. Thus, one option is to train one model per product family using the corresponding distribution as a loss function. <\/font>**\n- **<font size=\"2\">Tweedie(1.6) could be SEAFOOD and Tweedie(1.2) matches with PERSONAL CARE.<\/font>**\n![Screenshot 2022-01-24 at 19.41.33.png](attachment:8e88b9eb-43fb-4f06-aa3f-ba40ad4ec86f.png)\n\n","cecf6bbe":" # 1. INTRODUCTION & GOAL  <a class=\"anchor\"  id=\"section1\"><\/a>\n- **<font size=\"2\">The goal of this notebook is to carry out an EDA in order to understand the data and generate some charts to draw some conslusions that will be helpful for training the model. An EDA should also include clonsusions derived from the charts to better understand the training data and variable importance.<\/font>**\n- **<font size=\"2\"> Please read the conclusions that I have written after each chart because they will help to understand the data for training. Feel free to give feedback. <\/font>**\n","846f86a7":"- **<font size=\"2\">The earthquake hit the 16th April 2016. One month onwards from that date we can see a slight increase in sales. After the earthquake there were more days off than usual.<\/font>**","6fb9a7b5":"# 5. STORE ANALYSIS <a class=\"anchor\"  id=\"section5\"><\/a>","7a4355b9":"# 3. HOLIDAYS & EVENTS<a class=\"anchor\"  id=\"section3\"><\/a>\n- Study the average sales on each day for different type of holidays. ","de993d41":"- **<font size=\"2\">Quito (the capital) and Guayaquil have 47% of the stores, despite the fact that both cities account for a 24% of the population of Ecuador. <\/font>**\n- **<font size=\"2\">We can see the variability of stores across different cities. Quito and Guayaquil have a high variety of stores. The rest of the cities have at least 2 differnet types. They are very small cities compared to Quito and Guayaquil. <\/font>**","0ce39003":"- **<font size=\"5\">If you like the charts, give some love and upvote :) <\/font>**","c2a3c4d8":"## 4.1 Historic Oil Prices Chart <a class=\"anchor\"  id=\"section4.1\"><\/a>","c57c6652":"# 2. ANALYSIS OF TRAIN DATASET  <a class=\"anchor\"  id=\"section2\"><\/a>","ddc5b21c":"- **<font size=\"2\">We can clearly see that lags close to the actual day have a high correlation with sales and dcoilwtico. It means that it is almost the same to use dcoilwtico and dcoilwtico lagged X days.<\/font>**\n- **<font size=\"2\">Actually, it would be better to use the oil price of 10-14 days ago to see the effect it has on sales and purchasing power. Poeple don\u00b4t refuel every day, they fill up the tank once or twice every 15 days. To see an effect it would more reasonable to use lagged oil price of 14-21 days ago, which has a 0.98-0.99 correlation with dcoilwtico.<\/font>**","d9526407":"## 6.1 Sales by product category for all the stores <a class=\"anchor\"  id=\"section6.1\"><\/a>","4f111eb1":"## 2.2 CHART TIME SERIES AVG SALES ON EACH DAY <a class=\"anchor\"  id=\"section2.2\"><\/a>"}}