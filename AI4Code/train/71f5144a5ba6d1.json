{"cell_type":{"e520a223":"code","4d9a5fef":"code","b42220f6":"code","02fb65dd":"code","6c897875":"code","aafd4cbb":"code","6ffebaff":"code","6cbe7451":"code","c889a01f":"code","31dfc541":"code","a1574215":"code","e2cbaa89":"code","03024c91":"code","72be2883":"code","01c95a31":"code","51fdd68a":"code","a988aa99":"code","a188e16a":"code","eddf5f39":"code","f787ba0c":"code","b2808e1c":"code","9426fe0f":"code","d6121e81":"code","1fe0b035":"code","e67f08ff":"code","64525ad7":"code","add6a6f4":"code","014d4c0c":"code","48bc365a":"code","5e2c9448":"code","1be5b35e":"code","94256ec5":"code","5e33c39e":"code","fb010849":"code","98f7887b":"code","19764c9d":"code","8f75b239":"code","3f9881c1":"code","75f53ab4":"code","375aaeba":"code","8f7abbe4":"code","4d1055af":"code","5c031272":"code","7381c824":"code","6471b5eb":"code","4ba2270f":"code","f950f91e":"code","39835d4d":"code","18fe590b":"code","ed14c808":"code","14bcb5da":"code","53f47445":"code","c962c3b5":"code","667f2e98":"code","1c9fa3f9":"code","f8d5ee01":"code","ed46ab98":"code","cd7ff55c":"markdown","5c9b6cb7":"markdown","881fdd68":"markdown","4ab9ca06":"markdown","a0847e06":"markdown","f2d4ea5a":"markdown","72d41c90":"markdown","039fffbe":"markdown","550379a2":"markdown","40f4268e":"markdown","66fff484":"markdown","033d7169":"markdown","4e5df7b2":"markdown","cebed79d":"markdown","b8e099ea":"markdown","44f9e5f2":"markdown","8d07dea3":"markdown","b2d089ea":"markdown","e601d6b3":"markdown","efdca261":"markdown","4882d197":"markdown","d9621563":"markdown","88410dc8":"markdown","fb5b3462":"markdown","54f44778":"markdown","b91e7c68":"markdown"},"source":{"e520a223":"!pip install scikit-learn -U","4d9a5fef":"import datetime as dt\nfrom math import ceil, log\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm,skew\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.feature_selection import mutual_info_regression, SelectFromModel\nfrom sklearn.linear_model import LinearRegression, ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nimport warnings\n\n\nwarnings.filterwarnings('ignore')","b42220f6":"train_set = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain_set.head()","02fb65dd":"test_set = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_set.head()","6c897875":"set(train_set.columns) - set(test_set.columns)","aafd4cbb":"train_set[\"SalePrice\"].describe()","6ffebaff":"# Let's take a look at the distribution of the SalePrice \nsns.distplot(train_set['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train_set['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","6cbe7451":"price = np.log1p(train_set[\"SalePrice\"])\n \nsns.distplot(price , fit=norm);\n(mu, sigma) = norm.fit(price)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","c889a01f":"plt.figure(figsize=(18,10))\nplots = train_set[\"YrSold\"].value_counts().plot(kind=\"bar\")\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.0f'),\n                   (bar.get_x() + bar.get_width() \/ 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=15, xytext=(0, 8),\n                   textcoords='offset points')\nplt.title(\"Houses Sold over the Years\")\nplt.ylabel(\"Number\")\nplt.show()","31dfc541":"plt.figure(figsize=(18,10))\nplots = train_set[\"SaleType\"].value_counts().plot(kind=\"bar\")\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.0f'),\n                   (bar.get_x() + bar.get_width() \/ 2,\n                    bar.get_height()), ha='center', va='center',\n                   size=15, xytext=(0, 8),\n                   textcoords='offset points')\nplt.title(\"Most purchased Sale Type\")\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Sale Type\")\nplt.show()","a1574215":"years_and_months = train_set.groupby([\"YrSold\", \"MoSold\"]).count().index.values\n[years for years in years_and_months if years[1] == 12]","e2cbaa89":"#correlation matrix\ncorr = train_set.corr()\nf, ax = plt.subplots(figsize=(16, 10))\nsns.heatmap(corr)","03024c91":"train_set.corr()['SalePrice'].sort_values(ascending = False)","72be2883":"train_set.isnull().sum()[train_set.isnull().sum() > 0]","01c95a31":"ms_sub_class = {20:'1-STORY 1946 & NEWER ALL STYLES',\n                30:'1-STORY 1945 & OLDER',\n                40:'1-STORY W\/FINISHED ATTIC ALL AGES',\n                50:'1-1\/2 STORY FINISHED ALL AGES',\n                60: '2-STORY 1946 & NEWER',\n                70: '2-STORY 1945 & OLDER', \n                75:'2-1\/2 STORY ALL AGES', \n                80:'SPLIT OR MULTI-LEVEL', \n                85:'SPLIT FOYER',\n                90: 'DUPLEX - ALL STYLES AND AGES', \n                120: '1-STORY PUD (Planned Unit Development) - 1946 & NEWER', \n                150:'1-1\/2 STORY PUD - ALL AGES',\n                160:'2-STORY PUD - 1946 & NEWER', \n                180:'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER', \n                190:'2 FAMILY CONVERSION - ALL STYLES AND AGES'}\n\nordinal_ranking = {\n                    'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0, \n                    'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1, 'Reg':3, \n                    'IR1':2, 'IR2':1,'IR3':0,'AllPub':4, 'NoSewr':3, \n                    'NoSeWa':2,'ELO':1, 'Gtl':3, 'Mod':2, 'Sev':1, 'Av':3, \n                    'Mn':2, 'No':1, 'GLQ':5, 'ALQ':4, 'BLQ':3, 'Rec':2, \n                    'LwQ':1, 'Unf':-1, 'Typ':8, 'Min1':7, 'Min2':6, 'Mod':5,\n                    'Maj1':4, 'Maj2':3, 'Sev':2, 'Sal':1, 'Fin':2, 'RFn':1\n                  }\n\ncontinous_features =['LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n                     '1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea','total_area_of_house',\n                      'WoodDeckSF', 'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n                     'PoolArea','MiscVal','LotFrontage', 'age_of_building','total_square_footage',\n                      ]\nnorminal_features = ['MSSubClass', 'MSZoning', 'Street', 'LandContour', 'LotConfig','Neighborhood','Condition1',\n                     'Condition2', 'BldgType','HouseStyle','RoofStyle', 'RoofMatl', 'GarageYrBlt','PavedDrive', 'SaleType',\n                    'Exterior1st', 'Exterior2nd','MasVnrType','Foundation','Heating','CentralAir','GarageType',\n                    'SaleCondition','Electrical', 'Alley', 'MiscFeature', 'remodelling_done', 'selling_season'\n                    ]\ndiscrete_features = ['YearBuilt','YearRemodAdd','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\n                     'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageCars','MoSold', 'YrSold'\n                    ]\nordinal_cat_features = ['LotShape', 'Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond', \n                        'BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','PoolQC','Fence'\n                        ,'KitchenQual', 'Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond'\n                       ]\nordinal_num_features = ['OverallQual','OverallCond' ]","51fdd68a":"from typing import Union\n\ndef verify_remodelling(remodelled_year: int, built_year: int) -> Union[bool, int]:\n    '''\n        Compute if remodelling has been done on a house\n    '''\n    if remodelled_year > built_year:\n        return True\n    elif remodelled_year == built_year:\n        return False\n    else:\n        return -1\n\n\ndef selling_period(month_sold: int) -> str:\n    '''\n        Compute selling season\n    '''\n    if month_sold >= 3 and month_sold <= 5:\n        return 'spring'\n    elif month_sold >= 6 and month_sold <= 8:\n        return 'summer'\n    elif month_sold >= 9 and month_sold <= 11:\n        return 'autumn'\n    else:\n        return 'winter'","a988aa99":"train_set['total_square_footage'] = train_set['TotalBsmtSF'] + train_set['GrLivArea']\ntest_set['total_square_footage'] = test_set['TotalBsmtSF'] + test_set['GrLivArea']\n\ntrain_set['remodelling_done'] = train_set.apply(lambda x: verify_remodelling(x['YearRemodAdd'], x['YearBuilt']), axis=1)\ntest_set['remodelling_done'] = test_set.apply(lambda x: verify_remodelling(x['YearRemodAdd'], x['YearBuilt']), axis=1)\n\n\ntrain_set['selling_season'] = train_set.apply(lambda x: selling_period(x['MoSold']), axis=1)\ntest_set['selling_season'] = test_set.apply(lambda x: selling_period(x['MoSold']), axis=1)\n\n\ntrain_set['total_area_of_house'] = train_set['TotalBsmtSF'] + train_set['1stFlrSF'] + train_set['2ndFlrSF']\ntest_set['total_area_of_house'] = test_set['TotalBsmtSF'] + test_set['1stFlrSF'] + test_set['2ndFlrSF']\n\n\ntrain_set['age_of_building'] = train_set['YearBuilt'].apply(lambda x: pd.datetime.now().year - x)\ntest_set['age_of_building'] = test_set['YearBuilt'].apply(lambda x: pd.datetime.now().year - x)","a188e16a":"for col in ordinal_cat_features:\n    train_set[col] = train_set[col].map(ordinal_ranking)\n    test_set[col] = test_set[col].map(ordinal_ranking)\n    train_set[col] = train_set[col].fillna(0)\n    test_set[col] = test_set[col].fillna(0)","eddf5f39":"train_set[continous_features].isnull().sum()","f787ba0c":"test_set[continous_features].isnull().sum()","b2808e1c":"imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nfor col in continous_features:\n    imp_mean.fit(train_set[col].values.reshape(-1,1))\n    train_set[col] = imp_mean.transform(train_set[col].values.reshape(-1,1))\n    test_set[col] = imp_mean.transform(test_set[col].values.reshape(-1,1))\n    train_set[col] =  np.log(train_set[col] + 1)\n    test_set[col] = np.log(test_set[col] + 1)","9426fe0f":"for col in norminal_features:\n    train_set[col] = train_set[col].astype(str)\n    test_set[col] = test_set[col].astype(str)\n    train_set[col] = train_set[col].fillna('Unknown')\n    test_set[col] = test_set[col].fillna('Unknown')","d6121e81":"ohe = OneHotEncoder(handle_unknown = \"ignore\", sparse=False)\nohe_train_set = pd.DataFrame(ohe.fit_transform(train_set[norminal_features]))\nohe_test_set= pd.DataFrame(ohe.transform(test_set[norminal_features]))\nohe_train_set.index = train_set.index\nohe_test_set.index = test_set.index\ntrain_set = train_set.drop(norminal_features, axis=1)\ntest_set = test_set.drop(norminal_features, axis=1)","1fe0b035":"train_set = pd.concat([train_set, ohe_train_set], axis=1)\ntest_set = pd.concat([test_set, ohe_test_set], axis =1)\nfor col in discrete_features:\n    test_set[col] = test_set[col].fillna(0)","e67f08ff":"id = test_set['Id']\ntrain = train_set.drop(columns=['SalePrice', 'Id'])\ntest_set = test_set.drop(columns = 'Id')","64525ad7":"numeric_feats = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n                   'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n                   'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                   'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n                   'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n                   'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n                   'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n                   'MoSold', 'YrSold']","add6a6f4":"# Let's take a look at the distribution of the numeric features \nsns.set_theme(rc = {'figure.dpi': 300, 'axes.labelsize' : 8, \n                    'axes.facecolor': '#E5E5E5', 'grid.color': '#faf5f5'}, \n                    font_scale = 0.55)\n\nfor i, feature in enumerate(numeric_feats):\n    plt.figure(i)\n    plt.title(f\"{feature} skewness\")\n    sns.distplot(train[feature], fit=norm)","014d4c0c":"skewed_feats = train[numeric_feats].apply(lambda x: skew(x)) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\ntrain[skewed_feats] = np.log1p(train[skewed_feats])\ntest_set[skewed_feats] = np.log1p(test_set[skewed_feats])","48bc365a":"scaler = StandardScaler()\nscaler.fit(train)\ntrain_data = scaler.transform(train)\ntest_data = scaler.transform(test_set)","5e2c9448":"#For cross validation\ndef rmsle_cv(model):\n    kf = KFold(\n        n_splits=5,\n        shuffle=True,\n        random_state=42).get_n_splits(train_data)\n\n    rmse = np.sqrt(-cross_val_score(\n        model,\n        train_data,\n        price,\n        scoring=\"neg_mean_squared_error\",\n        cv = kf)\n    )\n    return rmse","1be5b35e":"# Baseline Model\nlinear = LinearRegression()\nlinear.fit(train_data, price)","94256ec5":"rmsle_cv(linear)","5e33c39e":"# Predict the test data using the Basic Linear Model.\nstart_time = dt.datetime.now()\npredicted_linear = linear.predict(test_data)\ndt.datetime.now() - start_time","fb010849":"pd.DataFrame({'id': id, 'SalePrice': np.round(np.expm1(predicted_linear))}).head(10)","98f7887b":"lin_selector = SelectFromModel(\n    estimator=LinearRegression(),\n    threshold=\"median\"\n)\nlin_selector.fit(train_data, price)","19764c9d":"linear_reg = LinearRegression()\nlinear_reg.fit(lin_selector.transform(train_data), price)","8f75b239":"rmsle_cv(linear_reg)","3f9881c1":"# Predict the test data using the Linear Model.\nstart_time = dt.datetime.now()\npredicted_linear_reg = linear_reg.predict(lin_selector.transform(test_data))\ndt.datetime.now() - start_time","75f53ab4":"pd.DataFrame({'id': id, 'SalePrice': np.round(np.expm1(predicted_linear_reg))}).head(10)","375aaeba":"lasso = LassoCV(alphas = [1, 0.1, 0.01, 0.001, 0.0001, 0.0005, 0.0003], random_state=0)\nlasso.fit(train_data, price)","8f7abbe4":"rmsle_cv(lasso)","4d1055af":"# Predict the test data using the Lasso Model.\nstart = dt.datetime.now()\npredicted_lasso = lasso.predict(test_data)\ndt.datetime.now() - start","5c031272":"pd.DataFrame({'id': id, 'SalePrice': np.expm1(predicted_lasso)}).head(10)","7381c824":"ridge = RidgeCV(alphas=[1, 0.1, 0.01, 0.001, 0.0001, 0.0005, 0.0003])\nridge.fit(train_data, price) ","6471b5eb":"rmsle_cv(ridge)","4ba2270f":"# Predict the test data using the Ridge.\nstart_time = dt.datetime.now()\npredicted_ridge = ridge.predict(test_data)\ndt.datetime.now() - start_time","f950f91e":"pd.DataFrame({'id': id, 'SalePrice': np.expm1(predicted_ridge)}).head(10)","39835d4d":"elastic_net = ElasticNetCV(cv=5, alphas=[1e-3, 1e-2, 1e-1, 1, 0.0005, 0.0003, 0.005], random_state=0)\nelastic_net.fit(train_data, price)","18fe590b":"rmsle_cv(elastic_net)","ed14c808":"# Predict the test data using the Elastic Net.\nbegan = dt.datetime.now()\npredicted_elastic = elastic_net.predict(test_data)\ndt.datetime.now() - began","14bcb5da":"pd.DataFrame({'id': id, 'SalePrice': np.expm1(predicted_elastic)}).head(10)","53f47445":"## making predictions using the Random Forest algorithm \nforest_model = RandomForestRegressor(max_features=0.5, n_estimators=2000, random_state=0)\nforest_model.fit(train_data, price)","c962c3b5":"rmsle_cv(forest_model)","667f2e98":"# Predict the test data using the Random Forest Model.\nstart = dt.datetime.now()\npredicted_rf = forest_model.predict(test_data)\ndt.datetime.now() - start","1c9fa3f9":"pd.DataFrame({'id': id, 'SalePrice': np.expm1(predicted_rf)}).head(10)","f8d5ee01":"results = pd.DataFrame()\nresults[\"Model\"] = [\"Basic Linear Regression\", \"Linear Regression with Selected Features\", \n                    \"LassoCV\", \"RidgeCV\", \"ElasticNetCV\", \"Random Forest\"]\nresults[\"RSMLE Mean Score\"] = [rmsle_cv(linear).mean(), rmsle_cv(linear_reg).mean(), rmsle_cv(lasso).mean(),\n                               rmsle_cv(ridge).mean(), rmsle_cv(elastic_net).mean(), rmsle_cv(forest_model).mean()]\nresults","ed46ab98":"submission = pd.DataFrame({'id': id, 'SalePrice': np.expm1(predicted_elastic)})\nsubmission.to_csv(\"submission.csv\",index = False)\nprint(\"predictions successfully submitted\")","cd7ff55c":"#### Looks like we had more house sales in 2009 and the least sales in 2010.\n#### What type of sale was the most purchased?","5c9b6cb7":"## Data Cleaning and Feature Engineering","881fdd68":"#### How about another look at the data","4ab9ca06":"# Features Processing","a0847e06":"## Linear Models","f2d4ea5a":"### Lasso ","72d41c90":"#### Looks like the price is left skewed so we work with the log of the prices.","039fffbe":"#### Linear Regression with Selected Features","550379a2":"#### In order to determine the features that play important roles in the determination of the sales price, we can start by checking the features that have strong positive or negative correlations with the sales price. ","40f4268e":"#### Well, a closer look at the charts and data just shows that nothing spectacular happened in 2009. The number of purchases increase by the year and it is safe to say that the data available to us at the moment does not have complete records for the whole year 2010, only records up to the month of July in 2010 are available.","66fff484":"### Linear Regression","033d7169":"#### From the above matrix, we can see that `OverallQual` has the highest positive correlation with `SalesPrice` followed closely by `GrLivarea` while `OverallCond`, `KitchenAbvGr`, `EnclosedPorch` have the lowest correlations with `SalesPrice`. This simply means that the features with high correlation can be very useful in predicting prices and we should pay close attention to them. We could drop the ones with negative correlation or no correlation at all to the sales price but i choose not to as information, no matter how little can still be gotten from those.","4e5df7b2":"#### The train set consists of 1460 rows and 81 features while the test set consists of 1459 rows and 80 features. What feature is missing in the test set?","cebed79d":"#### What year did we have the most sales?","b8e099ea":"## Random Forest","44f9e5f2":"## Ridge","8d07dea3":"# Modelling","b2d089ea":"# Exploratory Data Analysis","e601d6b3":"#### Basic Linear Regression","efdca261":"#### How about we take a closer look into the type of prices available in the dataset?","4882d197":"## Model Results","d9621563":"# Submission","88410dc8":"### We start by loading and viewing the data available to us.","fb5b3462":"#### Apparently, the `SalePrice` for the test_set is what we are to predict.","54f44778":"## Elastic Net","b91e7c68":"#### Now that looks more like it..."}}