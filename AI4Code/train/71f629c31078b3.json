{"cell_type":{"274092be":"code","7deeb641":"code","49981058":"code","bb0558bd":"code","fbd014bd":"code","9172f338":"code","66a3e47c":"code","134680d2":"code","5bd60939":"code","d48fb052":"code","4f830bb7":"code","f1797c82":"code","347d7dab":"code","b634025d":"code","eff6c089":"code","d403a08c":"code","89d5dd4a":"code","51f7831a":"code","b865a461":"code","7df53136":"code","c1ad7cad":"code","dd669bd2":"code","a609af53":"code","6ff0f2bc":"code","22fffaa2":"code","05346f41":"code","e03b5f1c":"code","8c4e1374":"code","45ddafb0":"code","368f8fe5":"code","c460dd57":"code","f5a4d1be":"code","74a44055":"markdown","5882679c":"markdown","3ee8c73b":"markdown","5b15095c":"markdown","69ce5dc9":"markdown","0b9c619b":"markdown","0d579579":"markdown","778f3032":"markdown","0567853d":"markdown"},"source":{"274092be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7deeb641":"%load_ext tensorboard\n","49981058":"df=pd.read_csv(\"..\/input\/cancer-classification\/cancer_classification.csv\")\ndf.head()","bb0558bd":"import matplotlib.pyplot as plt\nimport seaborn as sns","fbd014bd":"df.select_dtypes(\"object\")\n#All the data in the dataset is numerical","9172f338":"df.columns\n# benign_0__mal_1 is the our model will predict","66a3e47c":"plt.figure(figsize=(15,10))\nsns.set_style(\"darkgrid\")\nsns.countplot(x=\"benign_0__mal_1\",data=df)","134680d2":"df.corr()[\"benign_0__mal_1\"].sort_values(ascending=False)\n#Almost all the columns have a negative correlation with the target column","5bd60939":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap=\"jet\",linewidths=0.1)\n#This is visualization of all columns","d48fb052":"df.isnull().sum()\n#There is not any missing value in the columns","4f830bb7":"X=df.drop(\"benign_0__mal_1\",axis=1).values\nX.shape","f1797c82":"y=df[\"benign_0__mal_1\"].values\ny.shape","347d7dab":"from sklearn.model_selection import train_test_split","b634025d":"X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.3)","eff6c089":"from sklearn.preprocessing import MinMaxScaler\n","d403a08c":"scaler=MinMaxScaler()","89d5dd4a":"X_train=scaler.fit_transform(X_train) #we will only rescale and transform the X data features, not the target columns\nX_train","51f7831a":"X_test=scaler.transform(X_test)\nX_test","b865a461":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout","7df53136":"from tensorflow.keras.callbacks import EarlyStopping , TensorBoard","c1ad7cad":"early_stop=EarlyStopping(monitor=\"val_loss\",patience=25, mode=\"min\", verbose=1)","dd669bd2":"pwd","a609af53":"from datetime import datetime","6ff0f2bc":"datetime.now().strftime(\"%Y-%m-%d--%H%M\")","22fffaa2":"\nlog_directory = 'logs\\\\fit'\n\nboard = TensorBoard(log_dir=log_directory,histogram_freq=1,write_graph=True,\n    write_images=True,update_freq='epoch',profile_batch=2,embeddings_freq=1)\n","05346f41":"model=Sequential()\nmodel.add(Dense(units=30, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=15,activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\")","e03b5f1c":"model.fit(x=X_train, y=y_train,epochs=600,validation_data=(X_test,y_test),callbacks=[early_stop,board])","8c4e1374":"\npd.DataFrame(model.history.history).plot(figsize=(15,10))","45ddafb0":"%tensorboard --logdir logs\/fit\n#This will not work because the kaggle disabled this property in that this makes kaggle slower\n#We have to run this in our local computer","368f8fe5":"predictions=model.predict_classes(X_test)","c460dd57":"from sklearn.metrics import classification_report, confusion_matrix","f5a4d1be":"print(classification_report(y_test,predictions))\nprint(\"*****************\")\nprint(confusion_matrix(y_test,predictions))\n#As we can see below, the predicitons of our model is pretty good","74a44055":"# 2. Training the Deep Learning Model:","5882679c":"# 1. Exploratory Dana Analysis:","3ee8c73b":"2.3. Creating TensorBoard Callback","5b15095c":"2.1. Splitting the Data as Train and Test Set:","69ce5dc9":" TensorBoard requires a running kernel, so its output will only be available in an editor session.","0b9c619b":"TensorBoard is a visualization tool provided with TensorFlow.\n\nThis callback logs events for TensorBoard, including:\n* Metrics summary plots\n* Training graph visualization\n* Activation histograms\n* Sampled profiling\n\nIf you have installed TensorFlow with pip, you should be able\nto launch TensorBoard from the command line:\n\n```sh\ntensorboard --logdir=path_to_your_logs\n```\n\n\n    Arguments:\n        log_dir: the path of the directory where to save the log files to be\n          parsed by TensorBoard.\n        histogram_freq: frequency (in epochs) at which to compute activation and\n          weight histograms for the layers of the model. If set to 0, histograms\n          won't be computed. Validation data (or split) must be specified for\n          histogram visualizations.\n        write_graph: whether to visualize the graph in TensorBoard. The log file\n          can become quite large when write_graph is set to True.\n        write_images: whether to write model weights to visualize as image in\n          TensorBoard.\n        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n          writes the losses and metrics to TensorBoard after each batch. The same\n          applies for `'epoch'`. If using an integer, let's say `1000`, the\n          callback will write the metrics and losses to TensorBoard every 1000\n          samples. Note that writing too frequently to TensorBoard can slow down\n          your training.\n        profile_batch: Profile the batch to sample compute characteristics. By\n          default, it will profile the second batch. Set profile_batch=0 to\n          disable profiling. Must run in TensorFlow eager mode.\n        embeddings_freq: frequency (in epochs) at which embedding layers will\n          be visualized. If set to 0, embeddings won't be visualized.\n       ","0d579579":"2.2.Scaling Data:","778f3032":"![](http:\/\/)> **In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.**","0567853d":"2.4. Creating the Model:"}}