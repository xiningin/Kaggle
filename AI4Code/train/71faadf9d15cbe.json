{"cell_type":{"a5228352":"code","39246b42":"code","3281db28":"code","88156aa5":"code","dde0220a":"code","27e929f6":"code","ce688b7d":"code","5ec347c9":"code","117531a2":"code","e09ef278":"code","ad161639":"code","05d754d6":"code","39af1263":"code","08cdab38":"code","8c0598f6":"code","9a46fa99":"code","f583aeba":"code","7c6a7c57":"code","26cb1a06":"code","25a9629d":"code","0a9af385":"code","fcaa70d1":"code","479ca27e":"code","8e6df8f6":"code","71e771df":"code","7d00d848":"code","5ec0832c":"code","bfd95283":"code","c9ff65dc":"code","3d82e765":"code","ae1c2d30":"code","22ee2ad2":"code","7cb37d09":"code","08980565":"code","75e4f537":"markdown","74902568":"markdown","4219ea75":"markdown","a6fe67bc":"markdown","6f7cd178":"markdown","94db53bc":"markdown","1398befd":"markdown","6f3809f1":"markdown","e7be09cd":"markdown","07798ab7":"markdown","9443c25d":"markdown","edbc482b":"markdown","b0a4d90e":"markdown","6c222928":"markdown","df9ab148":"markdown","0b067b5e":"markdown","7532e04e":"markdown","efa2f839":"markdown","13044d31":"markdown","ed66085a":"markdown"},"source":{"a5228352":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39246b42":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split","3281db28":"train_df =  pd.read_csv('..\/input\/titanic\/train.csv')\n","88156aa5":"test_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_df.head()","dde0220a":"train_df.head(8)\n","27e929f6":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","ce688b7d":"train_df=train_df.drop(['Cabin'], axis=1)","5ec347c9":"test_df = test_df.drop(['Cabin'], axis=1)","117531a2":"train_df.columns.values\n","e09ef278":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","ad161639":"train_df = train_df.drop(['PassengerId'], axis=1)\n","05d754d6":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\ntrain_df[\"Age\"].isnull().sum()","39af1263":"train_df['Embarked'].describe()\n","08cdab38":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","8c0598f6":"train_df.info()\n","9a46fa99":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","f583aeba":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","7c6a7c57":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","26cb1a06":"train_df['Ticket'].describe()\n","25a9629d":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","0a9af385":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","fcaa70d1":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed train_df['Age'].value_counts()","479ca27e":"train_df.head(10)\n","8e6df8f6":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","71e771df":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","7d00d848":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()","5ec0832c":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","bfd95283":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","c9ff65dc":"# KNN \nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","3d82e765":"gaussian = GaussianNB() \ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test) \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","ae1c2d30":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","22ee2ad2":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","7cb37d09":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","08980565":"output = pd.DataFrame({\"PassengerId\":X_test.PassengerId,\"Survived\":predictions})\noutput.to_csv('my_submission.csv', index=False) \nprint(\"Your submission was successfully saved!\")","75e4f537":"### Embarked","74902568":"### Name:\n","4219ea75":"## Which is the best Model ?\n","a6fe67bc":"### Fare","6f7cd178":"## Data Preprocessing\n","94db53bc":"### Sex","1398befd":"### Gaussian Naive Bayes:","6f3809f1":"### Ticket","e7be09cd":"### Logistic Regression:\n","07798ab7":"### Decision Tree\n","9443c25d":"### Linear Support Vector Machine:\n","edbc482b":"### Missing Data:\n","b0a4d90e":"### Embarked","6c222928":"## Converting Features:","df9ab148":"### Perceptron","0b067b5e":"### K Nearest Neighbor","7532e04e":"## Building Machine Learning Models\n","efa2f839":"### Age","13044d31":"1. Age and Sex","ed66085a":"### Random_forest"}}