{"cell_type":{"d31f6c12":"code","d22eeb66":"code","2b90746a":"code","5736d6ff":"code","ccdb2c8e":"code","776e9e23":"code","609f2b26":"code","c67dfb61":"code","fd5c21a2":"code","4aa1784c":"code","21ebec07":"code","0c856472":"code","e0845dbc":"code","b09c27ce":"code","b50f2df1":"code","c8d667a0":"code","5b76bdad":"code","71c39485":"code","f171e3fb":"code","5ece5c3c":"code","474b76db":"code","413234f5":"code","81e8339e":"code","24b9ba97":"code","ddde193b":"code","17ca9d14":"code","7e194345":"code","2fc8e79f":"markdown","d472ca17":"markdown","2f8c47e3":"markdown","067cd659":"markdown","7a7bc1c5":"markdown","4c96a8d1":"markdown","702060fe":"markdown","d8ddc081":"markdown","c014c395":"markdown","95f54f58":"markdown","a5b61c09":"markdown","bc34281f":"markdown","8fa2f5f8":"markdown","b98df6d7":"markdown","db5896ac":"markdown","3edd9177":"markdown"},"source":{"d31f6c12":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom wordcloud import WordCloud,STOPWORDS\nimport spacy as sp\nimport string\nimport nltk\nimport re\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nnltk.download('vader_lexicon')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.manifold import Isomap,TSNE\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nnlps = sp.load('en')\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem.porter import *\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.layers import Dense,LSTM,Input,Dropout,SimpleRNN\nfrom keras import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import one_hot\nfrom tqdm.notebook import tqdm\nfrom tqdm.keras import TqdmCallback\n\n%matplotlib inline\n\n\nplt.rc('figure',figsize=(18,11))\nsns.set_context('paper',font_scale=2)\n","d22eeb66":"gba_df = pd.read_csv('\/kaggle\/input\/1500-gameboy-advanced-games-information\/GBA_Games.csv')\ngba_df.head(3)","2b90746a":"plt.title('Missing Value Count by Column')\nmissing = gba_df.isna().sum().to_frame().rename(columns={0:'count'})['count'].sort_values(ascending=False).to_frame()\nsns.heatmap(missing,annot=True,fmt='d',cmap='vlag')\nplt.show()","5736d6ff":"Region_Related = [t.strip() for item in gba_df['Region Released'].str.split(',').to_list() if type(item) != float for t in item ]\nRegion_Related = pd.Series(Region_Related).value_counts()\npal = sns.color_palette(\"coolwarm\", len(Region_Related))[::-1]\nplt.title('Distribution of Realeses by Region')\nsns.barplot(x=Region_Related.index,y=Region_Related.values,palette=pal)\nplt.show()","ccdb2c8e":"Developers = gba_df.Developers.value_counts()[:30]\nplt.title('Distribution of Game Releases by Top 30 Developer')\npal = sns.color_palette(\"coolwarm\", len(Developers))\nax = sns.barplot(x=Developers.index,y=Developers.values,palette=pal[::-1])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","776e9e23":"Region_Related = [t.strip() for item in gba_df['Publishers'].str.split('\/').to_list() if type(item) != float for t in item ]\nRegion_Related = pd.Series(Region_Related).value_counts()[:30]\npal = sns.color_palette(\"coolwarm\", len(Region_Related))\nplt.title('Games Releases of Top 30 Publishers')\nax= sns.barplot(x=Region_Related.index,y=Region_Related.values,palette=pal[::-1])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()","609f2b26":"release_yeras = gba_df['Year Released'].apply(lambda x: max(re.findall(r'2[0-9]{3}',x.strip())) if type(x) != float else 'nan' ).value_counts().drop(index='nan')\npal = sns.color_palette(\"coolwarm\", len(release_yeras))\nplt.title('Yearly Amount of Games Realsed')\nax = sns.barplot(x=release_yeras.index,y=release_yeras.values,palette=pal[::-1])\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.show()\n","c67dfb61":"multiplayer_status = gba_df['Multiplayer'].apply(lambda x: re.findall(r'No|Yes',x)[0] if type(x) != float else 'nan')\nex.pie(values=multiplayer_status.value_counts().values,names=multiplayer_status.value_counts().index,title='Proportion of Multiplayer Statuses')","fd5c21a2":"stemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n#Preprocessing Titles\n#Extracting Words Using Regex and Converting To Lowercase\ntitles         = gba_df.Titles.str.lower().apply(lambda x: ' '.join([i for i in re.findall(r'[a-zA-Z]+',x) if len(i) > 2]))\n#Creating A Singal Text Block\ntitles_text    = ' '.join(titles)\n#Removing Redundent Words Leaked During Scrapping\ntitles_text    = titles_text.replace('PAL','')\ntitles_text    = titles_text.replace('JP','')\n#Creating A Tonken Array of Lemmatized and Stemmed Words\ntitle_tokens   = nltk.FreqDist([lemmatizer.lemmatize(stemmer.stem(i)) for i in titles_text.split(' ') if i not in STOPWORDS])","4aa1784c":"#Preprocessing Titles\n#Extracting Words Using Regex and Converting To Lowercase\ngameplay        = gba_df.Gameplay[gba_df.Gameplay.notna()].str.lower().apply(lambda x: ' '.join([i for i in re.findall(r'[a-zA-Z]+',x) if len(i) > 2]))\n#Creating A Singal Text Block\ngameplay_text   = ' '.join(gameplay)\n#Removing Redundent Words Leaked During Scrapping\ngameplay_text   = gameplay_text.replace('PAL','')\ngameplay_text   = gameplay_text.replace('JP','')\n#Creating A Tonken Array of Lemmatized and Stemmed Words\ngameplay_tokens = nltk.FreqDist([lemmatizer.lemmatize(stemmer.stem(i)) for i in gameplay_text.split(' ') if i not in STOPWORDS])","21ebec07":"#Preprocessing Titles\n#Extracting Words Using Regex and Converting To Lowercase\nplot        = gba_df.Plot[gba_df.Plot.notna()].str.lower().apply(lambda x: ' '.join([i for i in re.findall(r'[a-zA-Z]+',x) if len(i) > 2]))\n#Creating A Singal Text Block\nplot_text   = ' '.join(plot)\n#Removing Redundent Words Leaked During Scrapping\nplot_text   = gameplay_text.replace('PAL','')\nplot_text   = gameplay_text.replace('JP','')\n#Creating A Tonken Array of Lemmatized and Stemmed Words\nplot_text   = nltk.FreqDist([lemmatizer.lemmatize(stemmer.stem(i)) for i in plot_text.split(' ') if i not in STOPWORDS])","0c856472":"NUMBER_OF_COMPONENTS = 450\n\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = titles\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x) if word not in STOPWORDS]))\n\nCV = CountVectorizer(stop_words='english',ngram_range=(1,1))\ncv = CV.fit_transform(text_data)\n\nC_vector = cv\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Variance in the Titles Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","e0845dbc":"best_fearures = [[CV.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:NUMBER_OF_COMPONENTS])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in worddf.Word:\n    total_count = 1\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Accountable For X% of Variance<b>\",'<b>Appeared On X Reviews<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\n\nfig.show()","b09c27ce":"NUMBER_OF_COMPONENTS = 2\nisomap = TSNE(NUMBER_OF_COMPONENTS)\npc_matrix = isomap.fit_transform(C_vector)\n\ndec_df = gba_df.copy()\ndec_df = dec_df.assign(dim_1 = pc_matrix[:,0],dim_2 = pc_matrix[:,1])\ndec_df = dec_df.assign(RY = gba_df['Year Released'].apply(lambda x: max(re.findall(r'2[0-9]{3}',x.strip())) if type(x) != float else -1 ))\nex.scatter(dec_df,x='dim_1',y='dim_2',title=r'Gameboy Title Projected From R^2734 --> R^2',color='RY',\n          hover_data=['Titles'])","b50f2df1":"NUMBER_OF_COMPONENTS = 100\n\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = gameplay\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x) if word not in STOPWORDS]))\n\nCV = CountVectorizer(stop_words='english',ngram_range=(1,1))\ncv = CV.fit_transform(text_data)\n\nC_vector = cv\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Variance in the Gameplay Description Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","c8d667a0":"best_fearures = [[CV.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:NUMBER_OF_COMPONENTS])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in worddf.Word:\n    total_count = 1\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Accountable For X% of Variance<b>\",'<b>Appeared On X Reviews<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\n\nfig.show()","5b76bdad":"NUMBER_OF_COMPONENTS = 2\nisomap = TSNE(NUMBER_OF_COMPONENTS)\npc_matrix = isomap.fit_transform(C_vector)\n\ndec_df = gba_df[gba_df.Gameplay.notna()]\ndec_df = dec_df.assign(dim_1 = pc_matrix[:,0],dim_2 = pc_matrix[:,1])\ndec_df = dec_df.assign(RY = gba_df['Year Released'].apply(lambda x: max(re.findall(r'2[0-9]{3}',x.strip())) if type(x) != float else -1 ))\ndec_df = dec_df[dec_df.Titles != 'Snood']\nex.scatter(dec_df,x='dim_1',y='dim_2',title=r'Gameboy Gameplay Description Projected From R^7154 --> R^2',color='RY',\n          hover_data=['Titles'])","71c39485":"NUMBER_OF_COMPONENTS = 100\n\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = plot\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x) if word not in STOPWORDS]))\n\nCV = CountVectorizer(stop_words='english',ngram_range=(1,1))\ncv = CV.fit_transform(text_data)\n\nC_vector = cv\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Variance in the Game Plots Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","f171e3fb":"best_fearures = [[CV.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:NUMBER_OF_COMPONENTS])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in worddf.Word:\n    total_count = 1\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Accountable For X% of Variance<b>\",'<b>Appeared On X Reviews<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\n\nfig.show()","5ece5c3c":"NUMBER_OF_COMPONENTS = 2\nisomap = TSNE(NUMBER_OF_COMPONENTS)\npc_matrix = isomap.fit_transform(C_vector)\n\ndec_df = gba_df[gba_df.Plot.notna()]\ndec_df = dec_df.assign(dim_1 = pc_matrix[:,0],dim_2 = pc_matrix[:,1])\ndec_df = dec_df.assign(RY = gba_df['Year Released'].apply(lambda x: max(re.findall(r'2[0-9]{3}',x.strip())) if type(x) != float else -1 ))\nex.scatter(dec_df,x='dim_1',y='dim_2',title=r'Gameboy Plot Description Projected From R^9378 --> R^2',color='RY',\n          hover_data=['Titles'])","474b76db":"titles = gba_df.Gameplay[gba_df.Gameplay.notna()]\nvocab = list(nltk.FreqDist(' '.join(titles).split(' ')).keys())\nvocab_size = len(vocab)\nword_index = { ch:i for i,ch in enumerate(vocab) }\nindex_word = { i:ch for i,ch in zip(word_index.values(),word_index.keys())}\nFTD = ' '.join(titles).split(' ')","413234f5":"S = []  \nC = []\nstride = 25\nT = 25\nfor i in range(0, len(FTD) - T, stride):\n    S.append(FTD[i: i + T])\n    C.append(FTD[i + T])\nX = np.zeros((len(S), T, vocab_size), dtype='bool')\nY = np.zeros((len(S), vocab_size), dtype='bool')    \nfor i, seq in tqdm(enumerate(S)):\n    for t, char in enumerate(seq):\n        X[i, t, word_index[char]] = 1\n        Y[i, word_index[C[i]]] = 1","81e8339e":"lstm_nn = Sequential()\nlstm_nn.add(Input((T,vocab_size)))\nlstm_nn.add(LSTM(64))\nlstm_nn.add(Dropout(0.5))\nlstm_nn.add(Dense(vocab_size,activation='softmax'))\n\nlstm_nn.compile(optimizer='adam',loss='categorical_crossentropy')\nlstm_nn.summary()","24b9ba97":"history = lstm_nn.fit(X, Y, epochs=500, batch_size=128,verbose=0, callbacks=[TqdmCallback(verbose=0)])","ddde193b":"plt.plot(history.history['loss'],'.-')\nplt.ylabel('loss',fontsize=14)\nplt.show()","17ca9d14":"generated = []\nfor itr in tqdm(range(0,10)):\n    start = np.random.randint(0, len(X)-1)\n    input_buffer = X[start] \n    generated_text = S[start].copy()\n\n    for i in (range(10)):\n        yhat = lstm_nn.predict(input_buffer[None,:])[0]\n        #ix = np.argmax(yhat)\n        ix = np.random.choice(range(vocab_size), p=yhat)\n\n        ch = index_word[ix]\n        generated_text += [ch]\n        input_buffer = np.r_[input_buffer[1:,:], np.zeros((1,vocab_size))]\n        input_buffer[-1,ix] = 1\n\n    generated.append(' '.join(generated_text))","7e194345":"for idx,text in enumerate(generated):\n    print(idx+1,') ',text)","2fc8e79f":"**Observation**: Konami is by far the most iconic game developer company, as seen in the plot above, holding first place with a staggering amount of 48 games released!","d472ca17":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Game Plot Description<\/h3>\n\n","2f8c47e3":"**Observation**:  Very interesting fact we learn from the above plot, apparently we can describe more than 80 percent of the original variance in Gameboy game gameplay description using just 100 words (dimensions), indicating that there are not that many different and unique gameplays when it comes done the Gameboy advanced games.","067cd659":"**Observation**:  As for inner variability in the Gameboy advanced game titles, we see that we need at least 450 words (dimensions) to describe roughly 80 percent of the variance in the original dimension.\nThe insight provided from this test tells us that the game titles' inner variance is very high and that many different names are \"far\" away from each other, meaning there is no resemblance between,any of the names.","7a7bc1c5":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h3>\n\n","4c96a8d1":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Initial Data Assessment<\/h3>\n\n","702060fe":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Libraries and Utilities<\/h3>\n\n","d8ddc081":"**Observation**:  We can see the high variance we concluded in the previous plot come into play when we inspect that data from a lower dimension perspective.\nWe see that there are mini clusters formed contacting different game installments from the same series but overall, the graph does not tell us any interesting relationship between the games.","c014c395":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h3>\n\n","95f54f58":"**Observation**:  Apparently, Konami leads in the game development category and rules the market as the number one game publisher! We can see that 1 in 10 Gameboy advanced games that were produced between 2002 and 2008 are published by Konami!","a5b61c09":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Game Titles<\/h3>\n\n","bc34281f":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Generating New Gampeplay Formats<\/h3>\n\n","8fa2f5f8":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Game Gameplay Description<\/h3>\n\n","b98df6d7":"**Observation**:  Looking at the spread of the gameplay description project onto R^2 we see that there are many more mini clusters appearing and visually, there are games that are close together because they are from the same series but notice that there is a cluster containing : ('harry potter quidditch, 'NFL' and 'Soccer Game' ) and overall sports games are more closer together action\/arcade games are closer together, in comparison to the Titles this information gives us very interesting connection between different games based on their gameplay, a very interesting and effective recommendation system can be created based on this 2 dimension and KNN in my opinion.\n","db5896ac":"**Observation**:  the Gameboy advanced was first released on June 11, 2001, and not so surprisingly, we see that the peak of game releases was in 2002, from which a steady decline in-game production continues until 2008 where the last document game was released.","3edd9177":"**Observation**: The PAL region is a television publication territory that covers most of Asia, Africa, Europe, South America, and Oceania,\nIt seems that the Pal region has slightly more games in comparison to the next \"giant,\" which is JAP - the Japanese region."}}