{"cell_type":{"31778158":"code","4ac3b901":"code","51e45fb3":"code","fdf1fdf3":"code","3544486f":"code","5bf00259":"code","be8ccf72":"code","59103f3f":"code","a93ae265":"code","a08c0f29":"code","d1469084":"code","795c6e86":"code","96122ecb":"code","a9821f84":"code","a1de534b":"code","b3359305":"code","744d860f":"code","97dd814c":"code","37b30661":"code","dd639fec":"code","57b32105":"code","1b643c35":"code","57f1d9a8":"code","1111a722":"markdown","6ba49430":"markdown","95209407":"markdown","72042665":"markdown","ebd9f94a":"markdown","4f20cb76":"markdown","58e63bb4":"markdown","5887a0de":"markdown","fb5d866e":"markdown","d982bbb1":"markdown","9e50c2b8":"markdown","e22ea2cc":"markdown","83bbd2f8":"markdown","5d3a2723":"markdown","29c90567":"markdown","d0367461":"markdown","3c56b4ac":"markdown","827d2fee":"markdown","1620f3a0":"markdown","d1e284b1":"markdown","7a6c7ca4":"markdown","540f29f5":"markdown","c19a9a22":"markdown","d26cf7dd":"markdown","5ab5d030":"markdown","f78a1065":"markdown"},"source":{"31778158":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom scipy.stats import norm, skew\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n\ntarget_name = 'SalePrice'\ndataset_train_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndataset_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ndataset_train_raw","4ac3b901":"ignore_feature = ['Id']\ny_train = dataset_train_raw[target_name]\ndataset_train = dataset_train_raw.drop([target_name] + ignore_feature, axis=1, inplace=False)\ndataset_test.drop(ignore_feature, axis=1, inplace=True)\n\nall_data = pd.concat([dataset_train, dataset_test], axis=0, sort=False)\nall_data","51e45fb3":"correlation_train = dataset_train_raw.corr()\nsb.set(font_scale=0.5)\nplt.figure(figsize=(15, 10))\nax = sb.heatmap(correlation_train, annot=True, annot_kws={'size': 10}, fmt='.1f', cmap='PiYG', linewidths=.2)\nplt.show()","fdf1fdf3":"fig, ax = plt.subplots(figsize=(20, 10))\nsb.heatmap(all_data.isnull(), yticklabels=False, cbar=True)\nplt.show()","3544486f":"specially_missed = ['Alley',\n                    'PoolQC',\n                    'MiscFeature',\n                    'Fence',\n                    'FireplaceQu',\n                    'GarageType',\n                    'GarageFinish',\n                    'GarageQual',\n                    'GarageCond',\n                    'BsmtQual',\n                    'BsmtCond',\n                    'BsmtExposure',\n                    'BsmtFinType1',\n                    'BsmtFinType2',\n                    'MasVnrType']\n\nfor feature in specially_missed:\n    all_data[feature] = all_data[feature].fillna('None')","5bf00259":"numeric_missed = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath',\n                  'BsmtHalfBath',\n                  'GarageYrBlt',\n                  'GarageArea',\n                  'GarageCars',\n                  'MasVnrArea']\n\nfor feature in numeric_missed:\n    all_data[feature] = all_data[feature].fillna(0)","be8ccf72":"all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","59103f3f":"all_data['Functional'] = all_data['Functional'].fillna('Typ')\nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub')\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna('TA')\nall_data['Electrical'] = all_data['Electrical'].fillna('SBrkr')\n\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","a93ae265":"fig, ax = plt.subplots(figsize=(20, 10))\nsb.heatmap(all_data.isnull(), yticklabels=False, cbar=True)\nplt.show()","a08c0f29":"plt.subplots(figsize=(14, 9))\nsb.distplot(y_train, kde=True, hist=True, fit=norm)\nplt.show()","d1469084":"y_train = np.log1p(y_train)\n\nplt.subplots(figsize=(14, 9))\nsb.distplot(y_train, kde=True, hist=True, fit=norm)\nplt.show()","795c6e86":"numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew","96122ecb":"for feature in high_skew.index:\n    all_data[feature] = np.log1p(all_data[feature])","a9821f84":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['SqFtPerRoom'] = all_data['GrLivArea'] \/ (all_data['TotRmsAbvGrd'] + all_data['FullBath'] +\n                                                       all_data['HalfBath'] + all_data['KitchenAbvGr'])\n\nall_data['TotalHomeQuality'] = all_data['OverallQual'] + all_data['OverallCond']\n\nall_data['TotalBathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n                                  all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))","a1de534b":"X_all = pd.get_dummies(all_data)\nX_train = X_all[:len(y_train)]\nX_test = X_all[len(y_train):]\n\nX_train","b3359305":"X_test","744d860f":"import xgboost as xg\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom mlxtend.regressor import StackingRegressor\n\n\nkf = KFold(n_splits=8, random_state=42, shuffle=True)\n\ndef cv_rmse(model):\n    return -cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kf)\n\nmodels = ['Linear', 'SVR', 'Random_Forest', 'XGBR', 'Cat_Boost', 'Ridge', 'Elastic_Net', 'Lasso', 'Stack']\nscores = []\n\nlin = LinearRegression()\nscore_lin = cv_rmse(lin)\nscores.append(score_lin.mean())\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\nscores.append(score_svr.mean())\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\nscores.append(score_rfr.mean())\n\nxgb = xg.XGBRegressor()\nscore_xgb = cv_rmse(xgb)\nscores.append(score_xgb.mean())\n\ncatb = CatBoostRegressor(verbose=0, allow_writing_files=False)\nscore_catb = cv_rmse(catb)\nscores.append(score_catb.mean())\n\nrid = Ridge()\nscore_rid = cv_rmse(rid)\nscores.append(score_rid.mean())\n\nel = ElasticNet()\nscore_el = cv_rmse(el)\nscores.append(score_el.mean())\n\nlas = Lasso()\nscore_las = cv_rmse(las)\nscores.append(score_las.mean())\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(verbose=0, allow_writing_files=False),\n                                          Ridge(),\n                                          xg.XGBRegressor(),\n                                          RandomForestRegressor()),\n                              meta_regressor=CatBoostRegressor(verbose=0, allow_writing_files=False),\n                              use_features_in_secondary=True)\nscore_stack_gen = cv_rmse(stack_gen)\nscores.append(score_stack_gen.mean())\n\ncv_score = pd.DataFrame(models, columns=['Regressors'])\ncv_score['RMSE_mean'] = scores\ncv_score","97dd814c":"plt.figure(figsize=(15, 11))\nsb.barplot(cv_score['Regressors'], cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize=16)\nplt.ylabel('CV_Mean_RMSE', fontsize=16)\nplt.xticks(rotation=45, fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","37b30661":"predictions = {}\n\ndef xgbr(X_train, y_train, X_test):\n    xgbrM = xg.XGBRegressor()\n    params = {'max_depth': [3, 4, 5, 6, 7, 8],\n              'min_child_weight': [0, 4, 5, 6, 7, 8],\n              'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.25, 0.8, 1],\n              'n_estimators': [10, 30, 50, 100, 200, 400, 1000]}\n\n    grid_search_xg = RandomizedSearchCV(estimator=xgbrM, scoring='neg_root_mean_squared_error', param_distributions=params, n_iter=200, cv=4, verbose=2,\n                                         random_state=42, n_jobs=-1)\n    grid_search_xg.fit(X_train, y_train)\n    xgbrModel = grid_search_xg.best_estimator_\n    print('Best params(XGBR):',grid_search_xg.best_params_)\n    print('RMSE(XGBR):', -grid_search_xg.best_score_)\n    return xgbrModel\n\n#xgbrModel = xgbr(X_train, y_train, X_test)\nxgbrModel = xg.XGBRegressor(n_estimators=400, min_child_weight=5, max_depth=7, learning_rate=0.05)\nxgbrModel.fit(X_train, y_train)\npredictions['XGBR'] = xgbrModel.predict(X_test)","dd639fec":"def ridge(X_train, y_train, X_test):\n    alpha_ridge = {'alpha': [-3, -2, -1, 1e-15, 1e-10, 1e-8, 1e-5, 1e-4, 1e-3, 1e-2, 0.5, 1, 1.5, 2, 3, 4, 5, 10, 20, 30, 40]}\n\n    rd = Ridge()\n    grid_search_rd = GridSearchCV(estimator=rd, scoring='neg_root_mean_squared_error', param_grid=alpha_ridge, cv=4, n_jobs=-1, verbose=3)\n    grid_search_rd.fit(X_train, y_train)\n    ridgeModel = grid_search_rd.best_estimator_\n    print('Best params(Ridge):', grid_search_rd.best_params_)\n    print('RMSE(Ridge):', -grid_search_rd.best_score_)\n    return ridgeModel\n\n#ridgeModel = ridge(X_train, y_train, X_test)\nridgeModel = Ridge(alpha=10)\nridgeModel.fit(X_train, y_train)\npredictions['Ridge'] = ridgeModel.predict(X_test)","57b32105":"def catBoost(X_train, y_train, X_test):\n    catM = CatBoostRegressor(verbose=0, allow_writing_files=False)\n    params = {'learning_rate': [0.01, 0.05, 0.005, 0.0005],\n              'depth': [4, 6, 10],\n              'l2_leaf_reg': [1, 2, 3, 5, 9]}\n\n    grid_search_cat = RandomizedSearchCV(estimator=catM, scoring='neg_root_mean_squared_error', param_distributions=params, n_iter=10, cv=4, verbose=2,\n                                     random_state=42, n_jobs=-1)\n    grid_search_cat.fit(X_train, y_train)\n    catModel = grid_search_cat.best_estimator_\n    print('Best params(CatBoost):',grid_search_cat.best_params_)\n    print('RMSE(CatBoost):', -grid_search_cat.best_score_)\n    return catModel\n\n#catModel = catBoost(X_train, y_train, X_test)\ncatModel = CatBoostRegressor(verbose=0, allow_writing_files=False, learning_rate=0.05, l2_leaf_reg=2, depth=4)\ncatModel.fit(X_train, y_train)\npredictions['CatBoost'] = catModel.predict(X_test)","1b643c35":"final_prediction = 0.25 * predictions['XGBR'] + 0.35 * predictions['CatBoost'] + 0.4 * predictions['Ridge']","57f1d9a8":"result = pd.DataFrame([len(y_train) + 1 + i for i in range(len(X_test))], columns=['Id'])\nresult[target_name] = np.expm1(final_prediction)\nresult.to_csv('result.csv', index=False, header=True)\nresult","1111a722":"<h3>Observation<\/h3>","6ba49430":"<h4>XGBR<\/h4>","95209407":"Let's change important numerical characteristics that take a small number of variants of values (for example, year) into categorical ones by casting them to strings.","72042665":"<h3>Adding new features<\/h3>","ebd9f94a":"As we can see, target variable is not normally distributed, let's try to fix it by using log.","4f20cb76":"<h4>Ridge<\/h4>","58e63bb4":"<h3>Fixing skewness<\/h3>","5887a0de":"<h3>Modeling<\/h3>","fb5d866e":"<h4>Result<\/h4>","d982bbb1":"<h3>Importing necessary libraries and reading datasets<\/h3>","9e50c2b8":"<h3>Dealing with missing values<\/h3>","e22ea2cc":"Now let's fix the high skewness in the rest of the values ","83bbd2f8":"Now let's move on to optimizing the hyperparameters of the models that showed the best results. ","5d3a2723":"At first it may seem that there are a lot of missing values, but in fact, some of them were omitted on purpose, and I made a separate category for them.","29c90567":"To begin with, let's conduct a superficial analysis of the models in order to weed out the inappropriate in this task. ","d0367461":"Let's create a histogram to see if the target variable (SalePrice) is normally distributed.","3c56b4ac":"No more missing values!","827d2fee":"<h3>Processed dataset<\/h3>","1620f3a0":"Just adding new more significant features based on old minor features ","d1e284b1":"Test data should contain the same type of data of the training set to preprocess them in the same way. The easiest way to solve this problem is to concatenate train and test datasets, preprocess, and then divide them again. It would also be a good idea to drop out the features that definitely do not affect the price, in our case - 'Id'.","7a6c7ca4":"<h4>Blending models<\/h4>","540f29f5":"Fill in the remaining missing values with the values that are most common for this feature. ","c19a9a22":"<h1>House prices prediction<\/h1>\n\n<img src=\"https:\/\/olegleyz.github.io\/images\/header.jpg\" alt=\"Header\" width=\"800\"\/><br>\n\nThis kernel is going to describe the method by which I predicted the selling prices of houses. <br>\nThis method will include the following steps:\n<ul>\n<li>Observation<\/li>\n<li>Dealing with missing values<\/li>\n<li>Fixing skewness<\/li>\n<li>Adding new features<\/li>\n<li>Modeling<\/li>\n<\/ul>","d26cf7dd":"If the missing feature is numerical, then fill it with zero, or it would also be a good idea to fill it with the median of the feature","5ab5d030":"As we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and let the regularization models do the clean up later on.","f78a1065":"<h4>Cat Boost<\/h4>"}}