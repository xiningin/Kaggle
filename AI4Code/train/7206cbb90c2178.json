{"cell_type":{"2dcd56d5":"code","d597c359":"code","be182106":"code","df9e22d7":"code","7c53c511":"code","9196f04a":"code","68dfc334":"code","c3d0d2b3":"code","0dfb1c9b":"code","c7849df6":"code","d8da922a":"code","3a3b0025":"code","80396fbd":"code","05534155":"code","ebfe71fb":"code","6e04afe0":"code","be943cd4":"code","3dba1042":"code","f33d3bfd":"code","a4c0678c":"code","e6a98b63":"code","8c8fa1ac":"code","f4587f4d":"code","026e5741":"code","a700a967":"code","aa6c1c29":"code","32a68f8e":"code","b0787a45":"code","ceb7e355":"code","b17f6471":"code","b3495ad9":"code","64117172":"code","22e99b0f":"code","d76bcdfd":"code","d7521b43":"code","62eb1440":"code","d49fcc93":"code","3a5040e7":"code","bb9c5f53":"code","31886ddd":"code","74de0070":"code","0dd6f87c":"code","452c660f":"markdown","981daa2d":"markdown","325d5be9":"markdown","ecaaa6de":"markdown","9ed6498b":"markdown","71ef7742":"markdown","f5e950e5":"markdown","35d62b18":"markdown","bd1a9769":"markdown","45ad90b6":"markdown","24106f85":"markdown","263a05c3":"markdown","1295a516":"markdown","2a78827c":"markdown","53c0c33a":"markdown","869f1759":"markdown","492957d5":"markdown","46bbd50d":"markdown","492be3ff":"markdown","b3f8ebb6":"markdown","bd1f3785":"markdown","8c019865":"markdown","03ad70ca":"markdown","b4f90b3f":"markdown","48c6eccc":"markdown","07d789f7":"markdown","594c7cfe":"markdown","525e19e9":"markdown","acaefa81":"markdown","e4134719":"markdown","cab58117":"markdown","93cefa58":"markdown","fd9eadef":"markdown","c027ffe3":"markdown","6e4ce730":"markdown"},"source":{"2dcd56d5":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d597c359":"from sklearn.linear_model import LinearRegression\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n\n# y = W1 * x_0 + W2 * x_1 + C\ny = np.dot(X, np.array([1, 2])) + 3\n'''\n[x_0, x_1].[W1]  + C   #ignore if not familiar with vector notation\n           [W2]\nhere for explanation W1 = 1 | W2=2 | C = 3\n'''\nprint(y)","be182106":"LinearRegression_model = LinearRegression().fit(X, y) #My linear model have X,y points \n# now it will find the best suited line for this\n# metric used in ordinary least square method\nLinearRegression_model.score(X, y) #Return the coefficient of determination R^2 of the prediction.","df9e22d7":"LinearRegression_model.coef_ #coefficient of x_0 and x_1`","7c53c511":"LinearRegression_model.intercept_","9196f04a":"LinearRegression_model.predict(np.array([[3, 8]]))","68dfc334":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error","c3d0d2b3":"np.random.seed(0)\nx = 2 - 3 * np.random.normal(0, 1, 20)\ny = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n\n# transforming the data to include another axis\nx = x[:, np.newaxis]\ny = y[:, np.newaxis]\n\npolynomial_features= PolynomialFeatures(degree=2)\nx_poly = polynomial_features.fit_transform(x)","0dfb1c9b":"model = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)\n\nrmse = np.sqrt(mean_squared_error(y,y_poly_pred))\nprint(rmse)\n\n#sorting values of x before plotting \nimport operator\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\nx, y_poly_pred = zip(*sorted_zip)\n#ploting \nplt.scatter(x, y, s=10)\nplt.plot(x, y_poly_pred, color='m')\nplt.show()","c7849df6":"!pip install sklearn-contrib-py-earth","d8da922a":"from sklearn.datasets import make_regression\nfrom pyearth import Earth\n# define dataset\n# generating the regression problem\nX, y = make_regression(n_samples=10000, n_features=20, n_informative=15, noise=0.5, random_state=7)\n# define the model\nmodel = Earth()\n# fit the model on the whole dataset\nmodel.fit(X, y)\n# define a single row of data\nrow = [-0.6305395, -0.1381388, -1.23954844, 0.32992515, -0.36612979, 0.74962718, 0.21532504, 0.90983424, -0.60309177, -1.46455027, -0.06788126, -0.30329357, -0.60350541, 0.7369983, 0.21774321, -1.2365456, 0.69159078, -0.16074843, -1.39313206, 1.16044301]\n# make a prediction for a single row of data\nyhat = model.predict([row])\n# summarize the prediction\nprint('Prediction: %d' % yhat[0])","3a3b0025":"# code from https:\/\/stackoverflow.com\/questions\/36252434\/predicting-on-new-data-using-locally-weighted-regression-loess-lowess\nfrom scipy.interpolate import interp1d\nimport statsmodels.api as sm\n\n# introduce some floats in our x-values\nx = list(range(3, 33)) + [3.2, 6.2]\ny = [1,2,1,2,1,1,3,4,5,4,5,6,5,6,7,8,9,10,11,11,12,11,11,10,12,11,11,10,9,8,2,13]\n\n# lowess will return our \"smoothed\" data with a y value for at every x-value\nlowess = sm.nonparametric.lowess(y, x, frac=.3)\n\n# unpack the lowess smoothed points to their values\nlowess_x = list(zip(*lowess))[0]\nlowess_y = list(zip(*lowess))[1]\n\n# run scipy's interpolation. There is also extrapolation I believe\nf = interp1d(lowess_x, lowess_y, bounds_error=False)\n\nxnew = [i\/10. for i in range(400)]\n\n# this this generate y values for our xvalues by our interpolator\n# it will MISS values outsite of the x window (less than 3, greater than 33)\n# There might be a better approach, but you can run a for loop\n#and if the value is out of the range, use f(min(lowess_x)) or f(max(lowess_x))\nynew = f(xnew)\n\n\nplt.plot(x, y, 'o')\nplt.plot(lowess_x, lowess_y, '*')\nplt.plot(xnew, ynew, '-')\nplt.show()\nfrom scipy.interpolate import interp1d\nimport statsmodels.api as sm\n\n# introduce some floats in our x-values\nx = list(range(3, 33)) + [3.2, 6.2]\ny = [1,2,1,2,1,1,3,4,5,4,5,6,5,6,7,8,9,10,11,11,12,11,11,10,12,11,11,10,9,8,2,13]\n\n# lowess will return our \"smoothed\" data with a y value for at every x-value\nlowess = sm.nonparametric.lowess(y, x, frac=.3)\n\n# unpack the lowess smoothed points to their values\nlowess_x = list(zip(*lowess))[0]\nlowess_y = list(zip(*lowess))[1]\n\n# run scipy's interpolation. There is also extrapolation I believe\nf = interp1d(lowess_x, lowess_y, bounds_error=False)\n\nxnew = [i\/10. for i in range(400)]\n\n# this this generate y values for our xvalues by our interpolator\n# it will MISS values outsite of the x window (less than 3, greater than 33)\n# There might be a better approach, but you can run a for loop\n#and if the value is out of the range, use f(min(lowess_x)) or f(max(lowess_x))\nynew = f(xnew)\n\n\nplt.plot(x, y, 'o')\nplt.plot(lowess_x, lowess_y, '*')\nplt.plot(xnew, ynew, '-')\nplt.show()","80396fbd":"from sklearn.linear_model import Ridge\nX, y = make_regression(n_samples=10000, n_features=20, n_informative=15, noise=0.5, random_state=7)\nRidge_model = Ridge(alpha=1.0)\nRidge_model.fit(X, y)\n","05534155":"# define a single row of data\nrow = [-0.6305395, -0.1381388, -1.23954844, 0.32992515, -0.36612979, 0.74962718, 0.21532504, 0.90983424, -0.60309177, -1.46455027, -0.06788126, -0.30329357, -0.60350541, 0.7369983, 0.21774321, -1.2365456, 0.69159078, -0.16074843, -1.39313206, 1.16044301]\n# make a prediction for a single row of data\nyhat = Ridge_model.predict([row])\n# summarize the prediction\nprint('Prediction: %d' % yhat[0])","ebfe71fb":"from sklearn.linear_model import Lasso\nlasso_RegresModel = Lasso(alpha=0.1)\nlasso_RegresModel.fit(X,y)","6e04afe0":"# define a single row of data\nrow = [-0.6305395, -0.1381388, -1.23954844, 0.32992515, -0.36612979, 0.74962718, 0.21532504, 0.90983424, -0.60309177, -1.46455027, -0.06788126, -0.30329357, -0.60350541, 0.7369983, 0.21774321, -1.2365456, 0.69159078, -0.16074843, -1.39313206, 1.16044301]\n# make a prediction for a single row of data\nyhat = lasso_RegresModel.predict([row])\n# summarize the prediction\nprint('Prediction: %d' % yhat[0])","be943cd4":"exam_result = pd.read_csv('..\/input\/exam-marks-of-two-subjectclassification-passfail\/marks.csv')\nprint(f\"the columns in data {exam_result.shape[1]} and the rows in data {exam_result.shape[0]}\")\nexam_result.head(10)","3dba1042":"X = exam_result.drop('result',axis=1)\ny = exam_result['result']\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=7)","f33d3bfd":"from sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train, y_train)","a4c0678c":"y_pred = logistic_regression.predict(x_test)\n\n# Evaluation metric\nfrom sklearn import metrics\naccuracy = metrics.accuracy_score(y_test, y_pred)\naccuracy_percentage = 100 * accuracy\naccuracy_percentage","e6a98b63":"from sklearn.datasets import make_regression\nX, y = make_regression(n_samples=10000, n_features=20, n_informative=15, noise=0.5, random_state=7)\nfrom sklearn.model_selection import train_test_split","8c8fa1ac":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)","f4587f4d":"n_neighbor = [1,3,7,15,25,35]","026e5741":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nrmse_val = []\nfor neighbor in n_neighbor:\n    model = KNeighborsRegressor(n_neighbors = neighbor)\n\n    model.fit(X_train, y_train)\n    pred=model.predict(X_test)\n    error = np.sqrt(mean_squared_error(y_test,pred))\n    rmse_val.append(error) #store rmse values\n\nplt.figure(figsize=(8,8))\nplt.plot(n_neighbor, rmse_val, 'bx-')\nplt.xlabel('k')\nplt.ylabel('rmse')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","a700a967":"text_data = pd.read_csv('\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv')\ntext_data.head(10)","aa6c1c29":"from nltk.tokenize import RegexpTokenizer, word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords","32a68f8e":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer","b0787a45":"le = LabelEncoder()","ceb7e355":"X = text_data['Message']\ny = text_data['Category']","b17f6471":"tokenizer = RegexpTokenizer('\\w+')\nsw = set(stopwords.words('english'))\nps = PorterStemmer()","b3495ad9":"def getStem(review):\n    review = review.lower()\n    tokens = tokenizer.tokenize(review) # breaking into small words\n    removed_stopwords = [w for w in tokens if w not in sw]\n    stemmed_words = [ps.stem(token) for token in removed_stopwords]\n    clean_review = ' '.join(stemmed_words)\n    return clean_review","64117172":"def getDoc(document):\n    d = []\n    for doc in document:\n        d.append(getStem(doc))\n    return d","22e99b0f":"\nstemmed_doc = getDoc(X)\n\n# print(stemmed_doc)","d76bcdfd":"cv = CountVectorizer()","d7521b43":"vc = cv.fit_transform(stemmed_doc)\n","62eb1440":"X = vc.todense()\n","d49fcc93":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=7)\n","3a5040e7":"from sklearn.naive_bayes import MultinomialNB","bb9c5f53":"model = MultinomialNB()\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)","31886ddd":"message = [\"\"\" \n\nInternship Program for College Students\n\n \n\nAbout Us:\nWe are an IT Company who works on various Development Services from last 6+ years. We are working on various technologies like Data science,  Machine learning, Software Development, Web Development, Digital  Marketing, Mobile App Development & Many more. . .\n\n \n\nDuration :  1 Month \/ 3 Months \/ 6 Months\n\n \n\nEligibility : Only pursuing candidates who are passing out in 2021, 2022, 2023, 2024 eligible\n\nLast Chance to Register, Few Seats Left Only, No More Registrations Accepted after Vacancies Full.\n\n \n\nBenefits:\n\nJoining Without Interview\nInternship Offer Letter\nInternship Completion Certificate\nStart working on Live Projects\nChoose your Internship Start Date as per your convenience at the time of registration\nAll formalities will be completed from company side which students needs for college submission.\"\"\",\n          \n\"\"\"\nMicrosoft next round of selection process is scheduled on 18th December 2020 as per mentioned timings\n\nPlease find below-shortlisted students list\n\nStudents will receive the interview invite directly from the company.\n\"\"\", \n           \"\"\"\n           We really appreciate your interest and wanted to let you know that we have received your application.\nThere is strong competition for jobs at Intel, and we receive many applications. As a result, it may take some time to get back to you.\nWhether or not this position ends up being a fit, we will keep your information per data retention policies, \nso we can contact you for other positions that align to your experience and skill set.\n\n\"\"\"]","74de0070":"\ndef prepare(messages):\n    d = getDoc(messages)\n    # dont do fit_transform!! it will create new vocab.\n    return cv.transform(d)\n\nmessages = prepare(message)","0dd6f87c":"y_pred = model.predict(messages)\ny_pred","452c660f":"image and code from [link](https:\/\/stackoverflow.com\/questions\/36252434\/predicting-on-new-data-using-locally-weighted-regression-loess-lowess)\n\n[reference](https:\/\/xavierbourretsicotte.github.io\/loess.html)","981daa2d":"## Logistic Regression\nIt\u2019s a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.\n\nThe type of logistic Regression in which the target variable is pass or fail, two values either `Class 0` or `Class 1`.This type is called Binomial Logistic Regression. Multinomial Logistic Regression deals with situations where the target variable can have three or more possible values.","325d5be9":"##  stepwise regression \nGarbage data is the worst part of machine learning model(almost), so Stepwise regression is a way to build a model by adding or removing predictor variables, usually via a series of F-tests or T-tests.\n\nThe main approaches are:\n\n* Forward selection, which involves starting with no variables in the model, testing the addition of each variable using a chosen model fit criterion, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit, and repeating this process until none improves the model to a statistically significant extent.\n* Backward elimination, which involves starting with all candidate variables, testing the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit, and repeating this process until no further variables can be deleted without a statistically insignificant loss of fit.\n* Bidirectional elimination, a combination of the above, testing at each step for variables to be included or excluded.\n\n* [Refference](https:\/\/www.statisticshowto.com\/stepwise-regression\/)\n* [wiki](https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression)","ecaaa6de":"What will happen if Linear regression is used for classification it wont distinguish the categories, we are having a image for this\n\n[image source :](https:\/\/www.javatpoint.com\/linear-regression-vs-logistic-regression-in-machine-learning)\n![image source : Javatpoint](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/linear-regression-vs-logistic-regression.png)\n\n[image source :](http:\/\/juangabrielgomila.com\/en\/logistic-regression-derivation\/)\n![img](https:\/\/lh3.googleusercontent.com\/proxy\/-O9byed5g8l75VldzC4lE_GWzk-WcUGI2hLfk0mReEGW8zrkD82nihWNBWoIU-ugCHL2ogeRb8eQTgAKU-OOjUaKLPR3mQRoHxoYGyLFPdj-e1I2s1iVJNCY5VU)","9ed6498b":"Ref : for NLP\n* [Krish Naik NLP](https:\/\/www.youtube.com\/watch?v=fM4qTMfCoak&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm)\n* [NLTK book](http:\/\/www.nltk.org\/book\/)","71ef7742":"What is mean_squared_error ?\n\n[wiki](https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error)","f5e950e5":"The MARS algorithm is not provided in the scikit-learn library; instead, a third-party library must be used.MARS is provided by the [py-earth Python library](http:\/\/https:\/\/github.com\/scikit-learn-contrib\/py-earth).","35d62b18":"# Bayesian Algorithm\n","bd1a9769":"## Naive Bayes\nNaive Bayes is a probabilistic algorithm (mainly used for classification problem). It takes the simplest assumption all the features used should be independent of each other. If you have read some other article of naive they would use an example of playing outside. I was thinking of using that but i found a article which covers it in a way better way [GeeksforGeeks](https:\/\/www.geeksforgeeks.org\/naive-bayes-classifiers\/) so i thought it would be great if i use it for its perfect way that is Spam Ham classifier.\n","45ad90b6":"** code from codeForCause -- https:\/\/github.com\/codeforcauseorg\/ML-Bootcamp-July\/blob\/master\/1stAug\/SpamEmail.ipynb **","24106f85":"Boom 15 is the best k value we want.\n\nAbout **classification problem**\nWe will do the similar things just we would use `sklearn.neighbors.KNeighborsClassifier` [refer to sklearn doc](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)","263a05c3":"# Instance-Based Algorithms","1295a516":"## Ridge Regression (L2 regulization)\n\nIt is also the extended version of linear Regression. Ridge regression is a way to create a parsimonious model when the number of predictor variables in a set exceeds the number of observations, or when a data set has multicollinearity (correlations between predictor variables).Least squares regression isn\u2019t defined at all when the number of predictors exceeds the number of observations; It doesn\u2019t differentiate \u201cimportant\u201d from \u201cless-important\u201d predictors in a model, so it includes all of them. This leads to overfitting a model and failure to find unique solutions. Least squares also has issues dealing with multicollinearity in data. Ridge regression avoids all of these problems.\n\nRidge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values.\n\n[refference](https:\/\/www.statisticshowto.com\/ridge-regression\/#:~:text=Ridge%20regression%20is%20a%20way,(correlations%20between%20predictor%20variables).)","2a78827c":"[reference](https:\/\/medium.com\/data-science-group-iitr\/logistic-regression-simplified-9b4efe801389)","53c0c33a":"## Linear Regression\n\nSimply finding a line that best fits the data.\nLinear regression is a linear approach to modelling the relationship between a dependent variable and one or more independent variables.\n\nIf one independent variable then simple linear regression.For more than one independent variable, the process is called mulitple linear regression. \n","869f1759":"I would encourage everyone to see the [sklearn doc](https:\/\/scikit-learn.org\/stable\/modules\/svm.html) for SVM. This will give deeper understanding cout the kernels and all.","492957d5":"## KNN Algorithm\nThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.K - Nearest Neighbor is simple and easy to implement supervised learning algortihm used to solve Regression and Classification problems.\n\nImage you are going to have dinner with your friends.Most of them wanted to go to KFC and 2 of your friends to go with Dominos so you would also feel to go with them. It is the most common human nature and as we say \" Birds of a feather flock together \". So we have applied it to machine too. Going to 'Dominos' or 'KFC' is the classification problem.\n\nAfter eating all of you decided to measure your BMI just to ensure you haven't overeaten. But you don't know what your weight is so you decided to ask your closest 5 friends about their weight and take there average. This is KNN regression problem.\n\n[reference : How KNN works & code explained in detail](https:\/\/www.analyticsvidhya.com\/blog\/2018\/08\/k-nearest-neighbor-introduction-regression-python\/)","46bbd50d":"When you are trying to solve the supervised learning problem, if the target variable is continous then we can say it is a Regression problem. If the target variable is discrete in nature then it is a classification problem.","492be3ff":"The extension version of KNN is kd tree & ball Tree I actually cannot explain in my words but here are some resources which i can share\n* [Tutorialspoint](https:\/\/www.tutorialspoint.com\/scikit_learn\/scikit_learn_k_nearest_neighbors.htm)\n* [Tree algorithms explained: Ball Tree Algorithm vs. KD Tree vs. Brute Force](https:\/\/towardsdatascience.com\/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-force-9746debcd940)\n* [sklearn](https:\/\/scikit-learn.org\/stable\/modules\/neighbors.html)\n","b3f8ebb6":"## LASSO regression (L1 regularizer)\nAnother kind of regularized regression that you could use instead of ridge regression is called Lasso Regression or L1 Regularization.Like ridge regression, lasso regression adds a regularisation penalty term to the ordinary least-squares objective.With lasso regression, a subset of the coefficients are forced to be precisely zero. Which is a kind of automatic feature selection, since with the weight of zero the features are essentially ignored completely in the model. This sparse solution where only a subset of the most important features are left with non-zero weights, also makes the model easier to interpret which is a huge advantage.\n\n[reference](https:\/\/www.statisticshowto.com\/lasso-regression\/#:~:text=Lasso%20regression%20is%20a%20type,i.e.%20models%20with%20fewer%20parameters)","bd1f3785":"## Elastic Net\nElastic Net combines characteristics of both lasso and ridge. Elastic Net reduces the impact of different features while not eliminating all of the features.\n\n[reference](https:\/\/machinelearningmastery.com\/elastic-net-regression-in-python\/)","8c019865":"As you remeber from above the same test point was used for MARS model `prediction : -393` and here it is `prediction: -390` so we can say our model is performing quite well.","03ad70ca":"## Polynomial Linear Regressio\n\nExtended version of multiple linear version where x_0 = x | x_1 = x^1 |............| x_n = x^n\n\nIf degree of variable is n so why it is still called linear?\n\n[Ans] This is still considered to be linear model as the coefficients\/weights associated with the features are still linear. x^2 is only a feature. However the curve that we are fitting is quadratic in nature.","b4f90b3f":"NOTE: In real world we get the cases of semi - supervised learning. Some data points are labelled and some of them are not. And if you are not lucky then you have to scratch the data on your own.","48c6eccc":"# Regression Algorithm","07d789f7":"Think so -390 is the right answere","594c7cfe":"A detailed version of naive bayes is given [sklearn doc](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html) You can get more baseian algorithm in this sklearn doc. I will continue more algorithms in the next notebook.","525e19e9":"A basic about **Supervised** & **Unsupervised learning**. Imagine a computer as a child you are going to teach him how does a cat looks like.You bring a cat picture and say \"This is a cat\" and by bring a non-cat image (any other animal) you say \"Not a cat\". In this the child will know what is cat and what is not a cat this is supervised learning. You have give hundreds of images and classify them as \"Cat\" or \"Not a cat\", a target variable is given or in other words it is labelled data.\n\nNow moving on Unsupervised Learning learning you give hundreds of images to the child and say them to classify them on the basis of their properties(legs, tail ,......) and name them as animal1, animal2, animal3...........animalN,by this the child will classify them on the basis of the observation | on the basis of similarity bw two images, he makes the `clusters` .This is Unsupervised learning Algorithm.The data given here is not labelled. ","acaefa81":"## Multivariate adaptive regression spline\n\nIt is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.\nI will encourage every one to read this [wiki](https:\/\/en.wikipedia.org\/wiki\/Multivariate_adaptive_regression_spline). This will give you the basics and the model building processes. ","e4134719":"## Support Vector Machine\nNobita : Hey Doremon! Can you help me in seperating the oranges and apples from this basket. I played a prank on Gyan and he give me this task.\n\nDoremon : Why dont you use a ML model for this classification task. Here is one SVM.......\n\nNobita : SVM ?????\n\nDoremon : SVM tries to find the worst orange which looks like apple & the worst apple which looks like orange these will be support vectors.\n\n![medium img article](https:\/\/miro.medium.com\/max\/3312\/1*q5bEEGVSQrj5FFzGYCpkzw.png)","cab58117":"It is responsible for finding the decision boundary to separate different classes and maximize the margin.\n\nNobita : But Doremon how are so sure that we get the optimal lines, there would be infinite lines which can pass through these points.\n\nDoremon : By the help of constrains\n           * we would select separate hyperplanes(e.g. separate lines), hyperplanes that classify classes correctly\n           * Conduct optimization: pick up the one that maximizes the margin\n\nFor non - linear data say a red dot is mixed in the green cluster, For these non-linearly seperable cases here comes the two warriors Soft Margin and Kernel Tricks.Soft Margin tries to find a line to separate, but tolerate one or few misclassified dots.But the kernel will find the non-linear decision boundry.\n![img src medium article : https:\/\/medium.com\/@zxr.nju\/what-is-the-kernel-trick-why-is-it-important-98a98db0961d](https:\/\/miro.medium.com\/max\/875\/1*mCwnu5kXot6buL7jeIafqQ.png)","93cefa58":"There are some assumption which should be there while using linear regression\n* linearity - The relationship between X and the mean of Y is linear\n* Homoscedasticity - The variance of residual is the same for any value of X.\n* Multivariate normaltiy - For any fixed value of X, Y is normally distributed\n* Independent of errors - Observations are independent of each other\n* Lack of multicollineartiy\n\n[Refference](https:\/\/www.statisticssolutions.com\/assumptions-of-linear-regression\/?__cf_chl_jschl_tk__=eadab9f727bd97c6942ee74635d20ce4f7abaa68-1606892278-0-ATFotQ1xMx8lDGiRVC0Gh6V-hsWz-Uw5rIcn77wTDY4FCHwyM3r6juFE8ot1pnVCwaGqAeSf7fy77gBxUVPPMd3c1LMyXuS_CW9wCL07hKZ27SSstPFm0z3RRzjNhYQpoOd6pR9-D6GqZKvXoUV7CLpxBO4bJHc4iDK8pkHLuW42W27XGAtvtXk7r-ZzEuG6_P9P20l3yfhFnx7McjC9QvhYo0CPPhetsk2fn0Raf7yTqVA-vqXy9aH72CDiouRoNVtYUWLvDVNKrsl1C7dNupvUAos2L_n6REf4uci4MsNHVxa216m5BoqP5QISfZCh0wrKWnTZ0XK6EDlF0prXohOTxgL-WY2t7wxZOxxNcbo3)","fd9eadef":"I want to take a easy example for classification implementation so i choosed this data. It is available on kaggle.","c027ffe3":"## LOESS - Locally Estimated Scatterplot Smoothing\n\nLOESS or LOWESS are non-parametric regression methods that combine multiple regression models in a k-nearest-neighbor-based meta-model.\n\nImagine you are having a data of signals that would be having noise, There is no way to use linear regression model there.So there comes an idea of considering a specialized model for each point we need to smooth out.\n![LOESS](https:\/\/i.stack.imgur.com\/qEDCN.png)","6e4ce730":"What is meant by coefficient of determination ?\n\n[R^2](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Coefficient_of_determination**) follow the link"}}