{"cell_type":{"91567f0d":"code","050d5666":"code","84e1a999":"code","7c0814b2":"code","9b3e06f5":"code","f98dd8e4":"code","9455fb93":"code","8c30bad1":"code","a69a57fe":"code","d2644850":"code","2cc25d90":"code","446f2f5c":"code","72e4b525":"code","c75c8210":"code","c26955dc":"code","64876145":"code","6e05bc32":"code","9473a535":"code","702fe422":"code","652e408a":"code","af09ca3a":"code","ba2bea33":"code","2b92e52e":"code","f41571ed":"code","af12cc91":"code","fac7628a":"code","2bff66f0":"code","4f533e42":"code","fa5da6a9":"code","f041f927":"code","ce58a829":"code","f6c7e42a":"code","16d25d67":"code","930809a2":"code","cf30bd44":"code","55a554b4":"code","a32d2f83":"code","7faaa091":"code","6fe93d8c":"code","33b1e908":"code","c370ef73":"code","34424da7":"code","50fd71e8":"code","d942ebec":"code","6e056316":"code","bf29be7a":"code","2099493e":"code","d32237c1":"code","72d3bed0":"code","007b5a4d":"code","29d8a500":"code","af27775c":"code","9e3418a3":"code","9b70472c":"code","fbf48570":"code","f65673a1":"code","bac0c5fd":"markdown","992ad934":"markdown","d674d8ab":"markdown","57b3a172":"markdown","7f23b3f1":"markdown","8effc509":"markdown","831a224e":"markdown","504c8638":"markdown","072af9f1":"markdown","e49d5f24":"markdown","05e18099":"markdown","89c120a8":"markdown","99d5bf02":"markdown","2c7bd6ae":"markdown","7a4138d5":"markdown","4cfcd823":"markdown"},"source":{"91567f0d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","050d5666":"train = pd.read_csv(\"\/kaggle\/input\/data-analytics-challenge-part-2\/X_train.csv\")\nlabels = pd.read_csv(\"\/kaggle\/input\/data-analytics-challenge-part-2\/Y_train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/data-analytics-challenge-part-2\/X_test.csv\")\n","84e1a999":"!pip install transformers","7c0814b2":"!pip install beautifulsoup4","9b3e06f5":"import os\nimport math\n\nimport torch\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW, XLNetTokenizer, XLNetModel, XLNetLMHeadModel, XLNetConfig\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.tokenize import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nimport re\n\nfrom bs4 import BeautifulSoup\n%matplotlib inline","f98dd8e4":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\'\\n\", \" \", text)\n    text = re.sub(r\"\\'\\xa0\", \" \", text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","9455fb93":"token=ToktokTokenizer()","8c30bad1":"punct = '!\"#$%&\\'()*+,.\/:;<=>?@[\\\\]^_`{|}~'","a69a57fe":"def strip_list_noempty(mylist):\n    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n    return [item for item in newlist if item != '']","d2644850":"def clean_punct(text): \n    words=token.tokenize(text)\n    punctuation_filtered = []\n    regex = re.compile('[%s]' % re.escape(punct))\n    remove_punctuation = str.maketrans(' ', ' ', punct)\n    for w in words:\n        if w in tags_features:\n            punctuation_filtered.append(w)\n        else:\n            punctuation_filtered.append(regex.sub('', w))\n  \n    filtered_list = strip_list_noempty(punctuation_filtered)\n        \n    return ' '.join(map(str, filtered_list))","2cc25d90":"lemma=WordNetLemmatizer()\nstop_words = set(stopwords.words(\"english\"))","446f2f5c":"def lemitizeWords(text):\n    words=token.tokenize(text)\n    listLemma=[]\n    for w in words:\n        x=lemma.lemmatize(w, pos=\"v\")\n        listLemma.append(x)\n    return ' '.join(map(str, listLemma))\n\ndef stopWordsRemove(text):\n    \n    stop_words = set(stopwords.words(\"english\"))\n    \n    words=token.tokenize(text)\n    \n    filtered = [w for w in words if not w in stop_words]\n    \n    return ' '.join(map(str, filtered))","72e4b525":"train['title'] = train['title'].apply(lambda x: str(x))\ntrain['title'] = train['title'].apply(lambda x: clean_text(x)) \ntrain['title'] = train['title'].apply(lambda x: clean_punct(x)) \ntrain['title'] = train['title'].apply(lambda x: lemitizeWords(x)) \ntrain['title'] = train['title'].apply(lambda x: stopWordsRemove(x)) ","c75c8210":"train.to_csv('new_train.csv',index=False)","c26955dc":"test_df['body'] = test_df['body'].apply(lambda x: lemitizeWords(x)) \ntest_df['body'] = test_df['body'].apply(lambda x: stopWordsRemove(x)) \n\ntest_df['title'] = test_df['title'].apply(lambda x: str(x))\ntest_df['title'] = test_df['title'].apply(lambda x: clean_text(x)) \ntest_df['title'] = test_df['title'].apply(lambda x: clean_punct(x)) \ntest_df['title'] = test_df['title'].apply(lambda x: lemitizeWords(x)) \ntest_df['title'] = test_df['title'].apply(lambda x: stopWordsRemove(x)) ","64876145":"test_df.to_csv('new_test.csv',index=False)","6e05bc32":"from sklearn.preprocessing import MultiLabelBinarizer","9473a535":"labels[\"Expected_prep\"] = labels[\"Expected\"].apply(lambda x: x.split(' '))","702fe422":"labels[\"Expected_prep\"]","652e408a":"mlb = MultiLabelBinarizer()","af09ca3a":"test = mlb.fit_transform(labels[\"Expected_prep\"])","ba2bea33":"list(mlb.classes_)","2b92e52e":"tags_features=mlb.classes_","f41571ed":"def to_class(mlb, vec):\n    indexes = [i for i in range(len(vec)) if vec[i] != 0]\n    return ' '.join([list(mlb.classes_)[i] for i in indexes])","af12cc91":"to_class(mlb, test[0])","fac7628a":"from transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 128\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)","2bff66f0":"train","4f533e42":"main = []\ntitle = train.title\nbody = train.body\nfor i in tqdm(range(len(title))):\n    main.append(str(title[i]) + \" \" + str(body[i]))","fa5da6a9":"train[\"main\"] = main","f041f927":"input_ids = tokenize_sentences(train['main'], tokenizer, MAX_LEN)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","ce58a829":"input_ids","f6c7e42a":"attention_masks","16d25d67":"with open('train_input_ids_128.npy', 'wb') as f:\n    np.save(f, input_ids)","930809a2":"with open('attention_masks_128.npy', 'wb') as f:\n    np.save(f, attention_masks)","cf30bd44":"with open('..\/input\/stackoverflow-bert-preprocessed-datasets\/train_input_ids_128.npy', 'rb') as f:\n    input_ids = np.load(f)","55a554b4":"with open('..\/input\/stackoverflow-bert-preprocessed-datasets\/attention_masks_128.npy', 'rb') as f:\n    attention_masks = np.load(f)","a32d2f83":"test_df","7faaa091":"main = []\ntitle = test_df.title\nbody = test_df.body\nfor i in tqdm(range(len(title))):\n    main.append(str(title[i]) + \" \" + str(body[i]))","6fe93d8c":"test_df[\"main\"] = main","33b1e908":"test_input_ids = tokenize_sentences(test_df['main'], tokenizer, MAX_LEN)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","c370ef73":"with open('test_input_ids_128.npy', 'wb') as f:\n    np.save(f, test_input_ids)","34424da7":"with open('test_attention_masks_128.npy', 'wb') as f:\n    np.save(f, test_attention_masks)","50fd71e8":"with open('..\/input\/stackoverflowbertpreprocessedtest\/test_input_ids_128.npy', 'rb') as f:\n    test_input_ids = np.load(f)","d942ebec":"with open('..\/input\/stackoverflowbertpreprocessedtest\/test_attention_masks_128.npy', 'rb') as f:\n    test_attention_masks = np.load(f)","6e056316":"from sklearn.model_selection import train_test_split\n\nlabels =  test\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n\ntrain_inputs = input_ids\ntrain_labels = labels\n\ntrain_masks = attention_masks\n\n\ntrain_size = len(train_inputs)\nvalidation_size = len(validation_inputs)","bf29be7a":"BATCH_SIZE = 16\nNR_EPOCHS = 1\n\ndef create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    if train:\n        dataset = dataset.shuffle(buffer_size=buffer_size)\n    dataset = dataset.repeat(epochs)\n    dataset = dataset.batch(batch_size)\n    if train:\n        dataset = dataset.prefetch(1)\n    \n    return dataset\n\ntrain_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\nvalidation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)","2099493e":"label_cols=list(mlb.classes_)","d32237c1":"from transformers import TFBertModel\nfrom tensorflow.keras.layers import Dense, Flatten\n\nclass BertClassifier(tf.keras.Model):    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        super().__init__()\n        self.bert = bert\n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    @tf.function\n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        cls_output = outputs[1]\n        cls_output = self.classifier(cls_output)\n                \n        return cls_output\n\nmodel = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))","72d3bed0":"import time\nfrom transformers import create_optimizer\n\nsteps_per_epoch = train_size \/\/ BATCH_SIZE\nvalidation_steps = validation_size \/\/ BATCH_SIZE\n\n# | Loss Function\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nvalidation_loss = tf.keras.metrics.Mean(name='test_loss')\n\n# | Optimizer (with 1-cycle-policy)\nwarmup_steps = steps_per_epoch \/\/ 3\ntotal_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\noptimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n# | Metrics\ntrain_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\nvalidation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n@tf.function\ndef train_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    with tf.GradientTape() as tape:\n        predictions = model(token_ids, attention_mask=masks)\n        loss = loss_object(labels, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer[0].apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n        \n@tf.function\ndef validation_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    v_loss = loss_object(labels, predictions)\n\n    validation_loss(v_loss)\n    for i, auc in enumerate(validation_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n                                              \ndef train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n    for epoch in range(epochs):\n        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n\n        start = time.time()\n\n        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n            train_step(model, token_ids, masks, labels)\n            if i % 1000 == 0:\n                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n                for i, label_name in enumerate(label_cols):\n                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n                    train_auc_metrics[i].reset_states()\n        \n        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n            validation_step(model, token_ids, masks, labels)\n\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n\n        for i, label_name in enumerate(label_cols):\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n            validation_auc_metrics[i].reset_states()\n\n        print('\\n')\n\n        \ntrain(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)","007b5a4d":"test_df = pd.read_csv(\"\/kaggle\/input\/preprocessed-stackoverflow-data\/new_test.csv\")","29d8a500":"test_df","af27775c":"TEST_BATCH_SIZE = 8\ntest_steps = len(test_df) \/\/ TEST_BATCH_SIZE\n\ntest_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n\n# df_submission = pd.DataFrame()\nprediction = []\n\nfor i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n    predictions = model(token_ids, attention_mask=masks, training=False).numpy()\n    new_predictions = [to_class(mlb, p >= .5) for p in predictions]\n    for j in range(len(new_predictions)):\n        if new_predictions[j] == \"\":\n            new_predictions[j] = list(mlb.classes_)[np.argmax(predictions[j])]\n\n    prediction += new_predictions","9e3418a3":"prediction","9b70472c":"for i in range(len(prediction)):\n    if prediction[i] == 'django':\n        prediction[i] = \"django python\"\n    if prediction[i] == \"pandas\":\n        prediction[i] = \"pandas python\"\n    if prediction[i] == \"numpy\":\n        prediction[i] = \"numpy python\"\n    if prediction[i] == \"tensorflow\":\n        prediction[i] = \"python tensorflow\"\n    if prediction[i] == \"python-3.x\":\n        prediction[i] = \"python python-3.x\"\n    if prediction[i] == \"keras\":\n        prediction[i] = \"keras python\"","fbf48570":"submission = pd.DataFrame()\nsubmission[\"Predicted\"] = prediction\nsubmission[\"id\"] = test_df[\"id\"]","f65673a1":"submission.to_csv('NEW_submission.csv', index=False)","bac0c5fd":"  # \u23f0 PREDICTION TIME","992ad934":"The first step is to use the BERT tokenizer to first split the word into tokens. \nThen, we add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence).\nThe third step the tokenizer does is to replace each token with its id from the embedding table which is a component we get with the trained model. Read [The Illustrated Word2vec](http:\/\/jalammar.github.io\/illustrated-word2vec\/) for a background on word embeddings .","d674d8ab":"Thus, without much data preprocessing and in just one epoch, we got a fairly good result. \nThe disadvantage of this model, however, is the very long execution of the prediction step.","57b3a172":"# **CREATE TARGET COLUMN**","7f23b3f1":"![image.png](attachment:image.png)","8effc509":"![image.png](attachment:image.png)","831a224e":"![image.png](attachment:image.png)","504c8638":"# Text preprocessing","072af9f1":"# Simple BERT Solution\n\nIn this notebook we will consider using BERT to solve the multi-label classification problem.","e49d5f24":" We will use the pretrained BERT model, which can be taken from the hugging face library (https:\/\/github.com\/huggingface\/transformers).\n Huging face provide a collection of state-of-the-art easy-to-use solutions. A full list of models can be found here: https:\/\/github.com\/huggingface\/transformers#models-architectures","05e18099":"# **CLASSIFICATION**","89c120a8":"The whole GPU training took about 2.5 hours. I have not tried running this code on a TPU. I would be glad if someone tries.","99d5bf02":"# What is BERT\naccording to wikipeida BERT is a Bidirectional Encoder Representations from Transformers - a Transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google.\n\nBERT has performed well in solving NLP problems, showing on average + 1.5-2% of the other best solutions, both DL and the best classical machine learning solutions.","2c7bd6ae":"# \u23f0 TRAINING TIME","7a4138d5":" Prediction also took about 50 minutes","4cfcd823":"The BERT architecture itself solves a lot of problems. \nIn our case, we are trying to solve a multi-label classification problem. According to it we will add a additional layer - simple Dense neural network layer with sigmoid activation function."}}