{"cell_type":{"2c4d6cb6":"code","1e05c4d2":"code","45ca3f5d":"code","049fd0d2":"code","2cdeb400":"code","770bfd88":"code","2af1f4a7":"code","63aae50c":"code","25b3d2bc":"code","38e63117":"code","dd153d22":"code","19aeca58":"code","eb46267a":"code","3fc75fde":"code","409648c2":"code","6dc7eb11":"code","c1286479":"code","73641b87":"code","e7cfc879":"code","23609e57":"code","97fc1f9f":"code","13b53f13":"code","eb97b13e":"code","15b57e40":"code","a041ccd7":"code","b9fa6b2b":"code","75f3a4b0":"markdown","65e02004":"markdown","ff5e76db":"markdown","bd830cf7":"markdown","6b8780b2":"markdown","0ccf12ce":"markdown","960f3d8c":"markdown","b15c17b6":"markdown","6592e166":"markdown","df44c0cb":"markdown","ca06c589":"markdown","c9e46fe4":"markdown","b32e2fd4":"markdown","e2551289":"markdown","05ae9e6d":"markdown","0f65dde9":"markdown","45e5f6d4":"markdown","fa9e9a24":"markdown","d856c2e3":"markdown","f2365c48":"markdown","c4aaf02c":"markdown","ed07e7ae":"markdown"},"source":{"2c4d6cb6":"import pandas as pd\nimport datatable as dt\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, RobustScaler\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, LogisticRegression, PassiveAggressiveClassifier,RidgeClassifierCV\nfrom sklearn.neural_network import MLPClassifier\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.ensemble import (AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier,VotingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss, roc_auc_score\nfrom sklearn.model_selection import cross_validate,cross_val_score,train_test_split, KFold, GridSearchCV\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns\n","1e05c4d2":"train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nsample=pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")","45ca3f5d":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')\ntrain_im = pd.DataFrame(imputer.fit_transform(train))\ntest_im = pd.DataFrame(imputer.fit_transform(test))\n#remove column\ntrain_im.columns = train.columns\ntest_im.columns = test.columns\n\ntrain = train_im\ntest = test_im","049fd0d2":"\npal=['#FFA15A','#00CC96']\n\nfig = px.histogram(train, x=\"song_popularity\", color=\"song_popularity\", color_discrete_sequence=px.colors.qualitative.Pastel1)\nfig.show()\n\n","2cdeb400":"TARGET = 'song_popularity'\nFEATURES = [col for col in train.columns if col not in ['id', TARGET]]\ncat_features = [col for col in FEATURES if test[col].nunique() < 15]\ncont_features = [col for col in FEATURES if test[col].nunique() >= 15]\nprint(f'Continious features obtained {len(cont_features)} and categorical features obtained are {len(cat_features)}')","770bfd88":"%%time\nscale = RobustScaler()\n#scale = MinMaxScaler()\n#scale = StandardScaler()\ntrain[cont_features]=scale.fit_transform(train[cont_features])\ntest[cont_features]= scale.transform(test[cont_features])  \n\nprint('Data scaled using : ', scale)","2af1f4a7":"def cnf_matrix_and_report(y_validation,y_prediction):\n    cf_matrix = confusion_matrix(y_validation, y_prediction)\n    sns_plot=sns.heatmap(cf_matrix, annot=True,  cmap='Blues')\n    fig = sns_plot.get_figure()\n    #fig.savefig(\"output.png\")\n    print(classification_report(y_validation, y_prediction))","63aae50c":"#lets train only on 10% data and identify what models perform better and then later can develop a piepline for the best performing model.\nX=train.drop(['song_popularity'],axis=1)\ny=train.song_popularity\nX_train,X_valid,y_train,y_valid=train_test_split(X,y,test_size=0.15,random_state=2001)","25b3d2bc":"accuracy_list=[]\nmodel_list=[]","38e63117":"#1. Linear SVC\n\nmodel_type='Linear_svc'\nlinear_svc=LinearSVC()\nlinear_svc.fit(X_train,y_train)\nypreds=linear_svc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)","dd153d22":"#2. Decision Tree\nmodel_type='DecisionTreeClassifier'\ndt=DecisionTreeClassifier()\ndt.fit(X_train,y_train)\nypreds=dt.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","19aeca58":"#3. Random Forest\n\nmodel_type='RandomForestClassifier'\nclf=RandomForestClassifier()\nclf.fit(X_train,y_train)\nypreds=clf.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","eb46267a":"#4. AdaBoostClassifier\n\nmodel_type='AdaBoostClassifier'\nadb=AdaBoostClassifier()\nadb.fit(X_train,y_train)\nypreds=adb.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","3fc75fde":"#5. BaggingClassifier\n\nmodel_type='BaggingClassifier'\nbagging=BaggingClassifier()\nbagging.fit(X_train,y_train)\nypreds=bagging.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","409648c2":"#6. ExtraTreesClassifier\nmodel_type='ExtraTreesClassifier'\netc=ExtraTreesClassifier()\netc.fit(X_train,y_train)\nypreds=etc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","6dc7eb11":"#7. GradientBoostingClassifier\nmodel_type='GradientBoostingClassifier'\ngbc=GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\nypreds=gbc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","c1286479":"#8. LGBMClassifier\nmodel_type='LGBMClassifier'\nlgbm=LGBMClassifier()\nlgbm.fit(X_train,y_train)\nypreds=lgbm.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n\n\n","73641b87":"#9 XGBClassifier\nmodel_type='XGBClassifier'\nxgb=XGBClassifier()\nxgb.fit(X_train,y_train)\nypreds=xgb.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","e7cfc879":"#10 SGDClassifier\nmodel_type='SGDClassifier'\nsgd=SGDClassifier()\nsgd.fit(X_train,y_train)\nypreds=sgd.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","23609e57":"#11 Ridge Classifier\nmodel_type='Ridge Classifier'\nrgc=RidgeClassifierCV()\nrgc.fit(X_train,y_train)\nypreds=rgc.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","97fc1f9f":"#12 Perceptron\nmodel_type='Perceptron'\npercp=Perceptron()\npercp.fit(X_train,y_train)\nypreds=percp.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","13b53f13":"#13 passive aggresive\nmodel_type='Passive Aggresive'\npac=PassiveAggressiveClassifier()\npac.fit(X_train,y_train)\nypreds=pac.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","eb97b13e":"#14 MLP sklearn\nmodel_type='MLP'\nmlp=MLPClassifier()\nmlp.fit(X_train,y_train)\nypreds=mlp.predict(X_valid)\naccuracy=accuracy_score(ypreds,y_valid)\nprint(f'accuracy- {accuracy}')\ncnf_matrix_and_report(y_valid,ypreds)\naccuracy_list.append(accuracy)\nmodel_list.append(model_type)\n","15b57e40":"list_of_tuples = list(zip(model_list, accuracy_list))\ndf = pd.DataFrame(list_of_tuples, columns = ['Models', 'scores'])\ndf=df.sort_values(by='scores', ascending=False)\ndf=df.reset_index(drop=True)","a041ccd7":"fig = px.bar(df, x='Models', y='scores',color_discrete_sequence=px.colors.qualitative.Pastel)\nfig.show()","b9fa6b2b":"test_pre = lgbm.predict(test)\nsample.song_popularity = test_pre\nsample.to_csv(\"Submission_lgbm.csv\",index=False)\nprint(\"success\")","75f3a4b0":"\n<a id=\"5.9\"><\/a>\n### XgBoost\n","65e02004":"# Song Popularity Prediction(14 Baseline Models)\n## Dataset\nMLSpace is happy to present the first competition of the series.\nThis is a limited competition and everyone is welcome to join. To request invitation link visit MLSpace Discord: https:\/\/discord.gg\/4RMwz64gdH\n\nIn this competition, you are supposed to predict the popularity of a song given features like acousticness, danceability, key, loudness, etc.\n\n\nMax team size is 3 and you are allowed up to 7 submissions a day.\nYou can select 2 submission for final evaluation.\n\n\n# Table of Contents\n<a id=\"toc\"><\/a>\n- [1. Imports](#1)\n- [2. Load Data](#2)\n- [3. Target Distribution](#3)\n- [4. Function for confusion matrix and classification report](#2)\n- [5. Modelling](#2)\n    - [5.1 Linear SVC](#5.1)\n    - [5.2 Decision Tree](#5.2)\n    - [5.3 Random Forest](#5.3)\n    - [5.4 ADA Boost](#5.4)\n    - [5.5 Bagging Classifier](#5.5)\n    - [5.6 Extra Trees](#5.6)\n    - [5.7 Gradient Boosting](#5.7)\n    - [5.8 Light Gradient Boosting](#5.8)\n    - [5.9 XGB](#5.9)\n    - [5.10 SGD Classifier](#5.10)\n    - [5.11 Ridge](#5.11)\n    - [5.12 Passive Aggresive](#5.12)\n    - [5.13 Perceptron](#5.13)\n    - [5.14 MLP](#5.14)\n- [6. Scores](#6)\n- [7. Sample Submission](#7)\n\n.\n## Observations\n- num_classes = 2\n- Highly Imbalanced dataset\n\n- No of models fitted - 14\n- Boosting and tree based classifiers performed much better although each one of them needs to be finetuned as lots of FP,FN .\n- Weighted classes shall be used as regularizers for better results.\n## Contact\n\n- [Click here ](https:\/\/twitter.com\/bambose_) - to connect me on twitter\n- [Optuna study notebook link ](https:\/\/www.kaggle.com\/datastrophy\/spp-optuna-xgb-200-trials)\n\n\n\n\n\n\n\n\n\n","ff5e76db":"\n<a id=\"5.10\"><\/a>\n### Stocastic gradient Descent\n","bd830cf7":"\n<a id=\"5.11\"><\/a>\n### Ridge Classifier\n","6b8780b2":"\n<a id=\"5\"><\/a>\n## Modelling\n<br><hr>\n<br>\n<a id=\"5.1\"><\/a>\n### Linear SVC","0ccf12ce":"\n<a id=\"5.13\"><\/a>\n### Passive Aggresive\n","960f3d8c":"\n<a id=\"5.3\"><\/a>\n### Random Forest\n","b15c17b6":"\n<a id=\"7\"><\/a>\n## Sample Submission\n","6592e166":"\u200b\n<a id=\"5.4\"><\/a>\n### AdaBoost Classifier\n","df44c0cb":"<a id=\"4\"><\/a>\n## Function for confusion matrix and classification report","ca06c589":"<a id=\"2\"><\/a>\n## Load Data","c9e46fe4":"\n<a id=\"5.12\"><\/a>\n### Perceptron\n","b32e2fd4":"<a id=\"1\"><\/a>\n## Imports","e2551289":"\n<a id=\"5.6\"><\/a>\n### Extra Trees Classifier\n","05ae9e6d":"\n<a id=\"6\"><\/a>\n## Scores plot\n","0f65dde9":"\n<a id=\"5.8\"><\/a>\n### Light Gradient Boosting\n","45e5f6d4":"<a id=\"3\"><\/a>\n## Target distribution","fa9e9a24":"\n<a id=\"5.5\"><\/a>\n### Bagging Classifier\n","d856c2e3":"# My Observations\n**14 models fitted.**\nWe can see that some models performed better than others but there is  huge class imbalance and that is going to affect the f1 score.\n\nThe next step for model training can be finetuning the hyperparams for top 5 models on a single fold and then further full training on 10 fold dataset with including weighted classes.\n\nFurther more the next step should be get the feature importance of this dataset, we might use a few tecniques to get the important features.\n\n\n","f2365c48":"\n<a id=\"5.14\"><\/a>\n### MLP\n","c4aaf02c":"\n<a id=\"5.2\"><\/a>\n### Decision Tree","ed07e7ae":"\n<a id=\"5.7\"><\/a>\n### Gradient Boosting\n"}}