{"cell_type":{"0b8d06bb":"code","314a1252":"code","de0590a9":"code","b21dd21c":"code","014c9c79":"code","5b78bd44":"code","c123dece":"code","c8252253":"code","0308e971":"code","ab5059b6":"code","2acc2713":"code","5067be84":"code","7f39d55d":"code","1c883f79":"code","ddcc1d60":"code","96230ec7":"markdown","4f25a745":"markdown","8780e61f":"markdown"},"source":{"0b8d06bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","314a1252":"!pip install keras-unet-collection\n! pip install segmentation-models","de0590a9":"# required libraries \nimport numpy as np \nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport cv2\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nfrom keras_unet_collection import models,losses\n\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt","b21dd21c":"train_df = pd.read_csv('..\/input\/sartorius-cell-instance-segmentation\/train.csv')\nprint(train_df.shape)\ntrain_df.head(4)","014c9c79":"def rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\n\ndef build_masks(labels,input_shape, colors=True):\n    height, width = input_shape\n    if colors:\n        mask = np.zeros((height, width, 3))\n        for label in labels:\n            mask += rle_decode(label, shape=(height,width , 3), color=np.random.rand(3))\n    else:\n        mask = np.zeros((height, width, 1))\n        for label in labels:\n            mask += rle_decode(label, shape=(height, width, 1))\n    mask = mask.clip(0, 1)\n    return mask\n\ndef rle2maskResize(rle):\n    # CONVERT RLE TO MASK \n    if (len(rle)==0): \n        return np.zeros((256,256) ,dtype=np.uint8)\n    \n    height= 520\n    width = 704\n    mask= np.zeros( width*height ,dtype=np.uint8)\n\n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]-1\n    lengths = array[1::2]    \n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n    \n    return mask.reshape( (height,width), order='F' )[::2,::2]","5b78bd44":"sample_filename = '0030fd0e6378'\nsample_image_df = train_df[train_df['id'] == sample_filename]\nsample_path = f\"..\/input\/sartorius-cell-instance-segmentation\/train\/{sample_image_df['id'].iloc[0]}.png\"\nsample_img = cv2.imread(sample_path)\nsample_rles = sample_image_df['annotation'].values\n\nsample_masks1=build_masks(sample_rles,input_shape=(520, 704), colors=False)\nsample_masks2=build_masks(sample_rles,input_shape=(520, 704), colors=True)\n\nfig, axs = plt.subplots(3, figsize=(20, 20))\naxs[0].imshow(sample_img)\naxs[0].axis('off')\n\naxs[1].imshow(sample_masks1)\naxs[1].axis('off')\n\naxs[2].imshow(sample_masks2)\naxs[2].axis('off')","c123dece":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n                 base_path='..\/input\/sartorius-cell-instance-segmentation\/train',\n                 batch_size=32, dim=(256, 256), n_channels=3,\n                 n_classes=3, random_state=2019, shuffle=True):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.df = df\n        self.mode = mode\n        self.base_path = base_path\n        self.target_df = target_df\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n        \n        X = self.__generate_X(list_IDs_batch)\n        \n        if self.mode == 'fit':\n            y = self.__generate_y(list_IDs_batch)\n            return X, y\n        \n        elif self.mode == 'predict':\n            return X\n\n        else:\n            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.seed(self.random_state)\n            np.random.shuffle(self.indexes)\n    \n    def __generate_X(self, list_IDs_batch):\n        'Generates data containing batch_size samples'\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        \n        # Generate data\n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['id'].iloc[ID]\n            img_path = f\"{self.base_path}\/{im_name}.png\"\n            img = self.__load_grayscale(img_path)\n            \n            \n            # Store samples\n            X[i,] = img \n\n        return X\n    \n    def __generate_y(self, list_IDs_batch):\n        y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n        \n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['id'].iloc[ID]\n            image_df = self.target_df[self.target_df['id'] == im_name]\n            \n            rles = image_df['annotation'].values\n            masks = build_masks(rles,(520,704), colors=False)\n            masks = cv2.resize(masks, (256, 256))\n            #masks=masks.transpose(1,0)\n            masks=np.expand_dims(masks, axis=-1)\n            y[i, ] = masks\n\n        return y\n    \n    def __load_grayscale(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        # resize image\n        dsize = (256, 256)\n        img = cv2.resize(img, dsize)\n        \n        img = img.astype(np.float32) \/ 255.\n        img = np.expand_dims(img, axis=-1)\n\n        return img\n    \n    def __load_rgb(self, img_path):\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = img.astype(np.float32) \/ 255.\n\n        return img","c8252253":"from segmentation_models.losses import bce_jaccard_loss\nfrom segmentation_models.metrics import iou_score","0308e971":"BATCH_SIZE = 16\n\ntrain_idx, val_idx = train_test_split(\n    train_df.index, random_state=2019, test_size=0.2 # mask_count_df\n)\ntrain_generator = DataGenerator(\n    train_idx, \n    df=train_df,\n    target_df=train_df,\n    batch_size=BATCH_SIZE, \n    n_classes=3\n)\nval_generator = DataGenerator(\n    val_idx, \n    df=train_df,\n    target_df=train_df,\n    batch_size=BATCH_SIZE, \n    n_classes=3\n)","ab5059b6":"model1=models.att_unet_2d((256, 256, 3), [64, 128, 256, 512, 1024],n_labels=3,activation='ReLU',output_activation='Sigmoid',weights='imagenet',backbone='EfficientNetB0',batch_norm=True,freeze_backbone=True,pool=False,unpool=False,freeze_batch_norm=True)","2acc2713":"import segmentation_models as sm\nsm.set_framework('tf.keras')\nsm.framework()","5067be84":"from segmentation_models import Unet\nfrom segmentation_models.utils import set_trainable\n\n\nmodel = Unet('efficientnetb0',input_shape=(256, 256, 3), classes=3, activation='sigmoid',encoder_weights='imagenet')\n#inp = Input(shape=(512, 640, 1))\n#l1 = Conv2D(3, (1, 1))(inp) # map N channels data to 3 channels\n#out = base_model(l1)\n#model = Model(inp, out, name=base_model.name)\n\nmodel.compile(optimizer='adam', loss=bce_jaccard_loss,metrics=[iou_score,'accuracy']) #bce_dice_loss binary_crossentropy\n","7f39d55d":"model1.compile(optimizer='adam', loss=bce_dice_loss,metrics=[dice_coef,iou_coef,'accuracy'])","1c883f79":"from tensorflow.keras.callbacks import Callback, ModelCheckpoint\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)","ddcc1d60":"\nhistory = model1.fit(\n    train_generator,\n    validation_data=val_generator,\n    callbacks=[checkpoint],\n    use_multiprocessing=False,\n    workers=4,\n    epochs=10\n)","96230ec7":"### Defining model","4f25a745":"### With Segmentation models library","8780e61f":"### More Comming Soon..Please Upvote if you like it.."}}