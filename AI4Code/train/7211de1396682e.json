{"cell_type":{"48a4b279":"code","ef577153":"code","c6e72909":"code","212dcddc":"code","4c7643e0":"code","01d43605":"code","a1cfa56b":"code","3082f8a9":"code","835a6d6d":"code","7fb072aa":"code","1425e272":"code","bde13bab":"code","2d480a12":"code","2a5cc0e8":"code","baf3319a":"code","9a2af8bb":"code","acf5699d":"code","86a7bb9d":"code","af183467":"code","e6254f8a":"code","7ec0d41c":"code","db3b94de":"code","8cb7bc3d":"code","8e9746e7":"code","d1e31472":"code","910da995":"code","59c9f4d4":"code","01e6b75e":"code","08f80fcb":"code","942ee377":"code","555b3b00":"code","037653ad":"code","f6df9016":"code","48a59a8d":"code","a64a926d":"code","8a42fa47":"code","668fe126":"code","42d66af1":"code","078241f7":"code","4b00b06b":"code","2992a693":"code","b8a7bb70":"code","732decd2":"code","9a54f422":"code","1e90fbd0":"code","acd95e03":"code","adde4953":"code","57971bff":"code","6a3a8dcd":"code","9eef3538":"code","918faee9":"code","9a041b5b":"code","64b44077":"code","6d718fa5":"code","ddb45118":"code","afb429b9":"code","92d62a6b":"code","38031ed3":"code","068ad869":"code","8e7a89c2":"code","85de75a1":"code","c1fe6998":"code","bd432f23":"code","056dfb0c":"code","72d22bdf":"code","4d784cfa":"code","5c0ea6bc":"code","bbf47de5":"code","99a47358":"code","ae42c292":"code","273cdca0":"code","fb28d7a2":"code","e080d410":"markdown","9bb470ad":"markdown","dc24160c":"markdown","254ad38a":"markdown","c83bf8de":"markdown","16cfdfc9":"markdown","129a639a":"markdown","e58893e7":"markdown","e26a79a0":"markdown","a678f64c":"markdown","6783783a":"markdown","f18fd0ae":"markdown","dbffc409":"markdown","1dcbd98b":"markdown","a61128a2":"markdown","9fbb6e60":"markdown","90fb6b62":"markdown","6377294b":"markdown","1f8f5f1f":"markdown","e1642e31":"markdown","44d4a276":"markdown","23d8a0ec":"markdown","167074f1":"markdown","cbb270e7":"markdown","13cb8f60":"markdown","635115a1":"markdown","ab663eef":"markdown","2aa58246":"markdown","b898f4e5":"markdown","481577fb":"markdown","749163e7":"markdown","7c46ca05":"markdown","d18e20e1":"markdown","768f870a":"markdown","ee2986f7":"markdown","3769c504":"markdown","fb5b368d":"markdown","c5e736ec":"markdown","7da24831":"markdown","42af8f51":"markdown","d1934a53":"markdown","24e5cba9":"markdown","eb5af2a8":"markdown","7aaba9df":"markdown","cad96f35":"markdown","ee81066d":"markdown","145bb684":"markdown","f5f8d11d":"markdown","fae87ecc":"markdown","3e02ccb8":"markdown","8136fb1b":"markdown","aa1172a3":"markdown","1abb5dc4":"markdown","dfbfae30":"markdown","23d3ff5d":"markdown","99a4820c":"markdown","c9069c56":"markdown","404d46a5":"markdown","7cd604b4":"markdown","c3431523":"markdown","12cf007e":"markdown","f9675b7f":"markdown","ac6d3563":"markdown","de41a571":"markdown","9ea6c415":"markdown","dd6e2136":"markdown","bb8b14a6":"markdown","9ed9ef87":"markdown","bd4cec5f":"markdown"},"source":{"48a4b279":"%matplotlib inline\n\n# Data Processing and Visualization Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nsns.set(style = 'white' , context = 'notebook', palette = 'deep')\n\n#For ignoring warning\nimport warnings\nwarnings.filterwarnings('ignore', category = DeprecationWarning)","ef577153":"# load the datasets using pandas's read_csv method\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\nPassengerId = test['PassengerId']\n\n# combine train and teat datasets to run certain operations on both datasets together.\ncombine =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","c6e72909":"# View the training data\ntrain.head()","212dcddc":"test.head()","4c7643e0":"print(\"train dataset:\")\nprint(train.shape)   #shows datapoints and features in train dataset                   \nprint(train.columns) #displays column names in our train dataset\nprint(\"test dataset:\")\nprint(test.shape)   #shows datapoints and features in test dataset                    \nprint(test.columns) #displays column names in our test dataset","01d43605":"# Different data types in the dataset\ntrain.dtypes","a1cfa56b":"combine.shape","3082f8a9":"train.describe(include = 'all')","835a6d6d":"# Plot graphic of missing values\nmissingno.matrix(combine, figsize = (30,10))","7fb072aa":"# Alternatively, you can see the number of missing values like this\ntrain.isnull().sum()","1425e272":"test.isnull().sum()","bde13bab":"sns.countplot(x='Survived', data=train);","2d480a12":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \nsns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"RdYlGn\")","2a5cc0e8":"# Age vs Survived\nsns.histplot(data=train, x='Age', hue='Survived',fill=True, legend=True, kde=True, palette=\"magma\");\nplt.show()","baf3319a":"train['Fare_Range'] = pd.qcut(train['Fare'],4)\ntrain.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","9a2af8bb":"pd.crosstab([train.SibSp],train.Survived).style.background_gradient(cmap='summer_r')\n","acf5699d":"sns.barplot(x=\"SibSp\",y=\"Survived\",data=train, palette = \"muted\")","86a7bb9d":"pd.crosstab([train.Parch],train.Survived).style.background_gradient(cmap='summer_r')","af183467":"sns.barplot(x=\"Parch\",y=\"Survived\",data=train, palette = \"muted\")","e6254f8a":"pd.crosstab(train.Pclass,train.Survived,margins=True).style.background_gradient(cmap='summer_r')","7ec0d41c":"# Visualise survival of each passenger class\nsns.set_style('whitegrid')\nsns.barplot(x='Pclass' , y='Survived' , data=train)\nplt.show()","db3b94de":"# Visualise survival of each Sex\nsns.set_style('whitegrid')\nsns.catplot(x='Sex', col='Survived', kind='count', data=train, height=4,);\n#sns.countplot(x='Sex' , hue='Survived' , data=train)\nplt.show()","8cb7bc3d":"# Calculating the value counts for our attributes\nmale_total = train[train['Sex']=='male'].shape[0]\nfemale_total = train[train['Sex']=='female'].shape[0]\nprint('Total male in our dataset:', male_total)\nprint('Total female in our dataset:', female_total)\n\ntrain[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","8e9746e7":"pd.crosstab([train.Sex,train.Survived],train.Pclass,margins=True).style.background_gradient(cmap='summer_r')","d1e31472":"# Pclass vs Survived by Sex\ng = sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train, height=4, kind=\"bar\", palette=\"viridis\")\ng = g.set_ylabels(\"survival probability\")","910da995":"sns.barplot(x='Embarked',y='Survived',data=train)\nplt.show()","59c9f4d4":"pd.crosstab(train.Pclass,train.Embarked,margins=True).style.background_gradient(cmap='summer_r')","01e6b75e":"# Pclass vs Embarked \nsns.catplot(x=\"Pclass\", col=\"Embarked\",  data=train, height=4, kind=\"count\", palette=\"muted\")","08f80fcb":"# create a new feature to extract title names from the Name column\ncombine['Title'] = combine.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n\n# count\ncombine[\"Title\"].value_counts()","942ee377":"# create a new feature to extract title names from the Name column\ntest['Title'] = test.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n\n# count\ntest[\"Title\"].value_counts()","555b3b00":"# map normalized title to Title feature vector\ncombine[\"Title\"] = combine[\"Title\"].map({\n    \"Mr\" :        \"Mr\", \n    \"Miss\" :      \"Miss\", \n    \"Mrs\" :       \"Mrs\", \n    \"Master\" :    \"Master\", \n    \"Dr\":         \"Officer\", \n    \"Rev\":        \"Officer\", \n    \"Mlle\":       \"Miss\", \n    \"Col\":        \"Officer\", \n    \"Major\":      \"Officer\", \n    \"Jonkheer\":   \"Royalty\", \n    \"Don\":        \"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Lady\":      \"Royalty\",\n    \"the Countess\":\"Royalty\", \n    \"Mme\":        \"Mrs\", \n    \"Capt\":       \"Officer\", \n    \"Ms\":         \"Mrs\", \n    \"Sir\" :       \"Officer\"\n})\n\n# print value counts\nprint(combine.Title.value_counts())","037653ad":"pivot = pd.pivot_table(combine, index=['Title'], values=['Survived'], aggfunc=np.mean)\npivot.plot(kind=\"bar\")","f6df9016":"combine = combine.drop(columns=['Name'])","48a59a8d":"# print null value count\nprint(\"Null values:\", combine['Age'].isnull().sum())\n\n# calculate zscore for each value\nzscore = (combine.Age - combine.Age.mean()) \/ combine.Age.std()\n\n# calculate outliers using zscore\noutliers = combine.loc[abs(zscore) > 3]\n\n# print outliers\nprint(\"Outliers:\", len(outliers))","a64a926d":"#visualise using boxplot\nsns.boxplot(x='Sex',y='Age', data=train)\nplt.title(\"Box_plot for age and sex\")\nplt.show()\n\nsns.boxplot(x='Pclass',y='Age', data=train)\nplt.title(\"Box_plot for age and Ticket class\")\nplt.show()\n\nsns.boxplot(x='Title',y='Age', data=combine)\nplt.title(\"Box_plot for age and Title\")\nplt.show()","8a42fa47":"# we first need to map Title to numerical values to allow the algorithm to run\ncombine['TitleMap'] = combine['Title'].map({'Mr':0, 'Mrs':1, 'Miss':2, 'Master':3, 'Royalty':4, 'Officer':5})\n\n# getting all data with known age values to train our model\ndata = combine.loc[combine['Age'].notna()]\n\n# creating X = features (Title and Class) and Y = response variable (Age)\nX = data[['TitleMap' , 'Pclass']]\nY = data['Age']\n\n# imputing age using regression imputation\nregression_classifier = LinearRegression()\n\n# splitting our data for training and testing \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=263)\n\nmodel = regression_classifier.fit(X_train,Y_train) # data fitted to model to train\n\n# predict missing ages\nage_result = model.predict(X_test)\n\n# age_result contains the imputed values and can be imputed by:\ncombine.loc[combine['Age'].isnull(), 'Age'] = age_result\n\n# check all age values have been filled\nprint(\"Null values:\", combine['Age'].isnull().sum())","668fe126":"# drop Title as it is no longer required\ncombine = combine.drop(columns=\"Title\")","42d66af1":"# calculate zscore for each value\nzscore = (combine.Age - combine.Age.mean()) \/ combine.Age.std()\n\n# calculate outliers using zscore\noutliers = combine[abs(zscore) > 3]\n\n# print outliers\nprint(\"Outlier count:\", len(outliers))\nprint(\"Outliers:\") \noutliers","078241f7":"# print null value count\nprint(\"Null values:\", combine.Fare.isnull().sum())\n\nzscore = (combine.Fare - combine.Fare.mean()) \/ combine.Fare.std()\n\n# calculate outliers using zscore\noutliers = combine.loc[abs(zscore) > 3]\n\n# print outlier count\nprint(\"Outlier count:\", len(outliers))\n\nprint(\"Outliers:\")\noutliers","4b00b06b":"#Fill the one missing value of Fare with the median value\ncombine[\"Fare\"] = combine[\"Fare\"].fillna(combine[\"Fare\"].median())","2992a693":"# print null value count\nprint(\"Null values:\", combine.Cabin.isnull().sum())\n\n# print list of unique values to check for anything unusual\nprint(\"Unique count:\", combine.Cabin.nunique())","b8a7bb70":"combine = combine.drop(columns='Cabin')","732decd2":"# print null value count\nprint(\"Null values:\", combine.Embarked.isnull().sum())\n\n# print list of unique values to check for anything unusual\nprint(\"Unique values:\", combine.Embarked.dropna().unique())","9a54f422":"# print embarked value counts\nprint(combine.Embarked.value_counts())","1e90fbd0":"# fill emabrked na with 'S'\ncombine.Embarked = combine.Embarked.fillna('S')\n\n# confirm there are no more nulls\nprint(\"Null values:\", combine.Embarked.isnull().sum())\nprint(combine.Embarked.value_counts())","acd95e03":"# print null value count\nprint(\"Null values:\", combine.Ticket.isnull().sum())\n\n# print list of unique values to check for anything unusual\nprint(\"Unique values:\", combine.Ticket.nunique())","adde4953":"combine = combine.drop(columns=['Ticket'])","57971bff":"combine.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","6a3a8dcd":"combine.head()","9eef3538":"combine['Sex'].replace(['male','female'],[0,1],inplace=True)","918faee9":"#creating the bins that we need to cut each range of ages\nbins = (0, 5, 12, 18, 35, 60, 120) \n\n#Seting the names that we want use to the categories\ncategories = ['babies', 'Children', 'Teen', 'YoungAdult', 'Adult', 'Senior']\n\n#Applying the pd.cut and using the parameters that we created \ncombine[\"AgeGroup\"] = pd.cut(combine.Age, bins, labels=categories)","9a041b5b":"combine['AgeGroup'].replace(['babies','Children','Teen','YoungAdult','Adult','Senior'],[0,1,2,3,4,5],inplace=True)","64b44077":"combine = combine.drop(columns='Age')","6d718fa5":"# create a new feature to calculate number of relatives\ncombine['Relatives'] = combine['SibSp'] + combine['Parch']\n\n# print value counts\nprint(combine.Relatives.value_counts())","ddb45118":"# create new feature to show if passenger was alone or with family\ncombine['Alone'] = 0\ncombine.loc[combine['Relatives'] == 0, 'Alone'] = 1\n\n# print value counts\nprint(combine.Alone.value_counts())","afb429b9":"f,ax=plt.subplots(1,2,figsize=(12,4))\n\nsns.barplot(data=combine, x=\"Relatives\", y=\"Survived\", ax=ax[0])\nax[0].set_title('Relatives vs Survived')\n\nsns.barplot(x='Alone', y='Survived', data=combine, ax=ax[1])\nax[1].set_title('Alone vs Survived')\n\nplt.close(2)\nplt.show()","92d62a6b":"combine = combine.drop(columns=['SibSp','Parch'])","38031ed3":"combine['Fare_Cat']=0\ncombine.loc[combine['Fare']<=7.91,'Fare_Cat']=0\ncombine.loc[(combine['Fare']>7.91) & (combine['Fare']<=14.454),'Fare_Cat']=1\ncombine.loc[(combine['Fare']>14.454) & (combine['Fare']<=31),'Fare_Cat']=2\ncombine.loc[(combine['Fare']>31) & (combine['Fare']<=513),'Fare_Cat']=3","068ad869":"combine = combine.drop(columns=['Fare'])","8e7a89c2":"combine['Embarked'] = combine['Embarked'].map({'S': 0, 'Q': 1, 'C': 2})","85de75a1":"combine.head()","c1fe6998":"## Separate train dataset and test dataset\n\ntrain_len = len(train)\ntrain = combine[:train_len]\ntest = combine[train_len:]","bd432f23":"print(test)","056dfb0c":"X_train = train.drop(\"Survived\",axis=1)\nY_train = train[\"Survived\"].astype(int)\nX_test  = test.drop(\"Survived\",axis=1)","72d22bdf":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","4d784cfa":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","5c0ea6bc":"# KNN\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","bbf47de5":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","99a47358":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","ae42c292":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","273cdca0":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","fb28d7a2":"submission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": Y_pred\n    })\n\n\nsubmission.to_csv('submission.csv', index=False)\n","e080d410":"**5.2 Fare**","9bb470ad":"**3.1 Data preview**","dc24160c":"**6.3 Relatives and Alone**\n\nBoth SibSp and Parch can be combined as they represent the number of relatives on board. We can create '**Relatives**' feature which is sum of SibSp and Parch.","254ad38a":"**4.2.3 SibSp**\n\nSibSp- number of siblings\/spouse ","c83bf8de":"*   The age distribution seems almost same in male and female population. Thus 'Sex' feature is not informative.\n*   The Pclass distribution seems pretty informative. There are more young people from class 3. Class 1 passengers are older than class 2 passengers. Class 2 passengers are older than class 3 passengers. We can easily visualize that roughly 37, 29, 24 respectively are the median values of each classes.\n*   The Title distribution gives more information about the age of passengers. Like the median age of those having title 'Master' is roughly 5.\n\nWe will impute our missing ages using Linear Regression imputation.","16cfdfc9":"# 2. Load Datasets","129a639a":"We can see that irrespective of Pclass, Female were given priority than Male. Even Male from Pclass 1 have low survival rate. \n\nThe lowest survival rate is for Male from Pclass 2 and 3. \n\nThe highest survival rate is for Females from Pclass 1(about 95%). Only 3 out of 94 Females from Pclass 1 died.\n\nPclass is definetly an important feature.","e58893e7":"Passengers boarded from Port c has the highest survival chance, while Port S has lowest.","e26a79a0":"We can see that People with small number of siblings\/spouse had more survival rate.\nPassengers having more Siblings\/Spouse has low chance of survival.","a678f64c":"**5.4  Embarked**","6783783a":"There 18 titles and most of them are rare. So we can group similar titles.","f18fd0ae":"**4.2.4 Parch**\n\nParch- number of parents\/children ","dbffc409":"We can clearly see that as Fare_Range increases, the Survival chance also increases.","1dcbd98b":"We can also drop PassengerId as it is useless","a61128a2":"<a href=\"https:\/\/colab.research.google.com\/github\/Kavya-sree\/Git-tutorial\/blob\/main\/Titanic_Survival_beginner.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","9fbb6e60":"# 5. Data Cleaning\n\nWe need to identify errors and missing datas in the dataset that may negatively impact a predictive model. Then clean and impute datas.","90fb6b62":"**5.5 Ticket**\n","6377294b":"**4.2.1 Age**\n\nAge is a continuous feature","1f8f5f1f":"**5.3 Cabin**","e1642e31":"\n\n*   The feature \"Fare\" has  significant correlation with Survival\n*   There is a high correlation between SibSp and Parch\n\nWe can explore all these features","44d4a276":"**4.2 Numerical Features**\n\nThe numerical features in our dataset are SibSp, Parch, Age, Fare.\nWe use heatmap to find correlated features. Only numeric features are compared as correlation between strings is not possible.\n","23d8a0ec":"**Pclass vs Embarked**","167074f1":"**3.2 Data Descriptions**\n\nSurvival (dependent variable): 0 = No, 1 = Yes\n\npclass (Ticket class): 1 = upper class, 2 = middle class, and 3 = lower class\n\nsex: Sex\n\nAge: Age in years\n\nsibsp: number of siblings\/spouses aboard the Titanic\n\nparch: number of parents\/children aboard the Titanic\n\nticket: Ticket number\n\nfare: Passenger fare\n\ncabin: Cabin number\n\nembarked: Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton","cbb270e7":"**4.3.1 Pclass**","13cb8f60":"Since we have Fare_Cat, We can Fare as it is not needed.","635115a1":"**6.1 Sex**","ab663eef":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. It sank in the north Atlantic ocean on April 15, 1912 during her maiden voyage after colliding with an iceberg and killing 1502 out of 2224 passengers and crew.\n\n**Objective**\n\nThe goal is to create a Model to predict which passengers survived the shipwreck.  ","2aa58246":"We cannot pass strings to a machine learning model, so we need to convert features 'Sex' into numeric values.\n","b898f4e5":"# 4. EDA- Exploratory Data Analysis","481577fb":"Embarked contains 2 missing values and no unusual values.","749163e7":"# 1. Importing Libraries","7c46ca05":"**4.1.1 Name**\n\nName values are unique. So no correlation is possible using 'Name' feature. But it contains both name and title of the passenger. Let's extract title from this feature. (feature engineering)\n\nPeople's title influences how they are treated. So it is essential to extract Title.","d18e20e1":"**6.4 Fare**\n\nSince Fare cannot be passed as it is, we can convert Fare it into categorical.","768f870a":"Title shows an uneven distribution of survival rates across the different titles. Females, children and royalty appear to have the highest chance of survival. This suggests there is some correlation between Title and Survived.","ee2986f7":"Cabin is missing a huge amount of data. Imputation is futile. We can drop this feature from dataset.","3769c504":"**5.4.1 Impute Missing Embarked Values**\n\nAs only 2 of 891 values are missing, we can simply fill these with the most common embarked value.","fb5b368d":"**6.6 Embarked**\n\nSince we cannot pass strings to a machine learning model, we need to convert feature Embarked into numeric values.","c5e736ec":"**4.4 Nominal Feature**","7da24831":"**Categorical features:**\n\nCategorical: Survived, Sex, and Embarked.\n\nOrdinal: Pclass, cabin\n\n\n**Numerical features:** \n\nContinous: Age, Fare \n\nDiscrete: SibSp, Parch\n\n\n","42af8f51":"Most common value is 'S'. Fill missing embarked values with 'S'.","d1934a53":"**3.3 Missing values**\n\nRows which are missing a value or have NaN.","24e5cba9":"There are 3 features that contain null values:\n\nAge: 177 missing values out of 891\n\nCabin: 687 missing values out of 891. It constitute a significant amount of data. Imputation may likely cause bias.\n\nEmbarked: Only 2 missing values. Any simple imputation is sufficient. ","eb5af2a8":"**Sex and Pclass vs Survival**\n\nLets check survival with Sex and Pclass together using crosstab and catplot","7aaba9df":"**5.1.1 Impute Missing Age Values**\n\nBefore estimating the missing values, we must first find correlated features of age to use as the basis of our imputation.","cad96f35":"# 6. Feature Engineering","ee81066d":"\n\n*   Babies(Age <=5) had high survival rate\n*   Most passengers are in 15-35 age range","145bb684":"**4.3.2 Sex**","f5f8d11d":"*Observation* : We can clearly see that Survival of first class passengers was prioritised, followed by second class, then third. Even though the the number of Passengers in Pclass 3 were a lot higher, the number of people survived is very low. May be Pclass 1 were prioritised during the evacuation due to their influence.","fae87ecc":"There are only 7 outliers for age.\nAll outliers are valid ages for a passenger. They can be retained in the dataset.","3e02ccb8":"\n\n*   Majority passengers boarded from S and most of them from Pclass 3.\n*   Port Q has low number of passengers. Most of the passengers from port Q were from Pclass 3\n*   Passengers boarded from Port c are mostly from Pclass 1 which seem to have high survival rate\n","8136fb1b":"790 passengers were travelling alone. Let us represent this as a separate feature '**Alone**'.","aa1172a3":"# 7. Modeling","1abb5dc4":"More people died than survived.","dfbfae30":"We can see that People with small number of parents\/children had more survival rate. ","23d3ff5d":"**4.3 Categorical Features**\n\nWe have Sex, Pclass and Embarked as categorical features","99a4820c":"**5.1 Age**","c9069c56":"**4.1 Target Feature: Survived**\n\nDescription: Whether the passenger survived or not.\n\nKey: 0 = did not survive, 1 = survived\n\nThis is the variable we want our machine learning model to predict based off all the others.","404d46a5":"Since there are 929 random unique Tickets, they have no impact on outcome. So we can drop Ticket column.","7cd604b4":"**4.2.2 Fare**\n\nFare is a continous feature. \n\nWe can use pandas.qcut. (Quantile-based discretization function). Discretize variable into equal-sized buckets based on rank or based on sample quantiles. ","c3431523":"First we need to impute the missing values.","12cf007e":"We no longer want Name feature, since Title is extracted from it. We can drop \"Name\"","f9675b7f":"*Observation*: The number of men is lot more than the number of women. Still the survival rates for a women on the ship is 74% while that for men in around 19%.","ac6d3563":"**5.1.2 Validate Age outliers**\n\nIdentify and validte outliers in Age data","de41a571":"We dont need Age feature. So we drop Age\n","9ea6c415":"There are multiple instances of the most extreme outliers. All outliers are also from first class, which would expect to have higher ticket costs. These values may not be  erronous and will be retained in the dataset.","dd6e2136":"**6.2 Age**\n\nSince Age is a continous feature, we can create clusters or bins.\n\n we cannot directly pass to Machine Learning Models. We already created bins, but assigned string values (for easy interpretation and EDA). We can assign numeric values to those string values.","bb8b14a6":"Relatives=0 means that the passenger is alone. \nIf you are alone or Relatives=0,then chances for survival is very low.\n\nFor Relatives > 3, the chance of survival decrease. ","9ed9ef87":"# 3. Data Overiew","bd4cec5f":"**4.3.3 Embarked**"}}