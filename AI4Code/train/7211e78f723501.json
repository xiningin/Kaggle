{"cell_type":{"7f7e3a37":"code","34521ea6":"code","6963c5be":"code","c88eb0b5":"code","d22bac36":"code","340fd70a":"code","e0b82bd9":"code","bb5a3eea":"code","33926869":"code","b8b6c837":"code","4ff0d82e":"code","e2d6294d":"code","c0e0a93b":"code","e3eb477f":"code","29093dfa":"code","638f2a5b":"code","ca704d0a":"code","4509e5b0":"code","c30078b7":"code","5802092b":"code","10ed0946":"code","636ff08c":"markdown","e03e1d6d":"markdown","5974bdab":"markdown"},"source":{"7f7e3a37":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor\nimport string\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm_notebook as tqdm\n%matplotlib inline","34521ea6":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nstop_words = set(stopwords.words('english'))\nstem = SnowballStemmer('english')","6963c5be":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntrain_df = train_df[['id','comment_text', 'target']]\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","c88eb0b5":"train_df.head()","d22bac36":"train_df.target.hist()","340fd70a":"train_df.shape","e0b82bd9":"test_df.head()","bb5a3eea":"test_df.shape","33926869":"train_df = train_df.sample(100000, random_state=42)","b8b6c837":"train_df.shape","4ff0d82e":"def tokenize(text):\n    \n    tokens = []\n    for token in word_tokenize(text):\n        if token in string.punctuation: continue\n        if token in stop_words: continue\n        tokens.append(stem.stem(token))\n    \n    return \" \".join(tokens)","e2d6294d":"train_tokens = Parallel(n_jobs=-1, verbose=1)(delayed(tokenize)(text) for text in train_df['comment_text'].tolist())","c0e0a93b":"train_tokens[0]","e3eb477f":"test_tokens = Parallel(n_jobs=-1, verbose=1)(delayed(tokenize)(text) for text in test_df['comment_text'].tolist())","29093dfa":"len(train_tokens + test_tokens)","638f2a5b":"vect = TfidfVectorizer()\nvect.fit(train_tokens + test_tokens)","ca704d0a":"X = vect.transform(train_tokens)\ny = train_df['target']","4509e5b0":"reg = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42, max_depth=10)\nreg.fit(X, y)","c30078b7":"test_X =  vect.transform(test_tokens)\ntest_y = reg.predict(test_X)","5802092b":"submisson_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmisson_df['prediction'] = test_y\nsubmisson_df['prediction'] = submisson_df['prediction'].apply(lambda x: \"%.5f\" % x if x > 0 else 0.0)","10ed0946":"submisson_df.to_csv(\"submission.csv\", index=False)","636ff08c":"# Project 3\n\n\n# Conversations Toxicity Detection\n\nJigsaw Unintended Bias in Toxicity Classification \n\nDetect toxicity across a diverse range of conversations\n\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/data#\n\n# Sample Solution\n\n### Install the Kaggle API and download the datasets","e03e1d6d":"Create tokens","5974bdab":"## Model with TF-IDF and Ranfom Forest"}}