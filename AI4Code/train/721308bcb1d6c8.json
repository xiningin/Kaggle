{"cell_type":{"dc643fef":"code","adbd9e99":"code","b9e1a6d9":"code","27b6c1c5":"code","09a909af":"code","e43f95d6":"code","25675385":"code","3cee58a4":"code","d57aea93":"code","b145e8ca":"code","2853b7be":"code","e3cae934":"code","50ca71bc":"code","c90fbc11":"code","4ed7a112":"code","b1fa6ae2":"code","966f9bf6":"code","6e77ff4f":"code","88470109":"code","a57c1d78":"code","61e8754b":"code","8a0b7ce0":"code","e3abb124":"code","ceab0cd2":"code","e76e9279":"code","cc7f5339":"code","da426d00":"code","e665928e":"code","882046a1":"code","47902ef3":"code","6b1580ae":"markdown","e98c0a17":"markdown","0217d28e":"markdown","7674ef1c":"markdown","ba7c1dae":"markdown","3c1a485c":"markdown","68dc5501":"markdown","2b689466":"markdown","9052afaa":"markdown","21f33274":"markdown","df12ee4e":"markdown","86d75f99":"markdown","d5e8e73c":"markdown","99660416":"markdown","e6b43028":"markdown","b104de2e":"markdown","56b93630":"markdown","02bb4823":"markdown","924b6969":"markdown","c8bb6fe3":"markdown","232b4e3a":"markdown","a95b0a16":"markdown","1c1059d9":"markdown","e1cdcdbe":"markdown","22ade33a":"markdown","3e1635d9":"markdown","ca1778b7":"markdown","1afd527f":"markdown","5baa023d":"markdown","44b20e9b":"markdown","c422a82d":"markdown","786fe772":"markdown","baeaaeee":"markdown","a4256b25":"markdown","bccc0842":"markdown","22ff7fac":"markdown"},"source":{"dc643fef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport statsmodels.api as sm\n\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import ConnectionStyle\nfrom matplotlib import patches\nfrom matplotlib.patches import ConnectionPatch\n!pip install joypy\n\nsns.set_style('dark')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","adbd9e99":"import sqlite3 \n\n# Read sqlite query results into a pandas DataFrame\nwith sqlite3.connect(\"\/kaggle\/input\/california-traffic-collision-data-from-switrs\/switrs.sqlite\") as con:\n\n    query = (\n        \"SELECT injured_victims,collision_date,collision_time,pedestrian_injured_count,bicyclist_injured_count,motorcyclist_injured_count \"\n        \"FROM collisions \"\n        \"WHERE latitude IS NOT NULL AND longitude IS NOT NULL AND collision_date NOT NULL AND collision_time NOT NULL \"\n    )\n\n    # Construct a Dataframe from the results\n    dataframe = pd.read_sql_query(query, con)\n    df = dataframe.iloc[:,:3]\n    df2 = dataframe.iloc[:,3:]\n","b9e1a6d9":"plt.rcParams['figure.figsize'] = (20,8)\n\ndata_time = df.query('\"04:00:00\"<collision_time<\"12:00:00\"').groupby('collision_date').sum()\ndata_time.columns = ['data_morning']\ndata_time['data_evening'] = df.query('\"20:00:00\">collision_time>\"12:00:00\"').groupby('collision_date').sum().iloc[:,0]\ndata_time['data_night'] = df.query('\"20:00:00\"<collision_time|collision_time<\"04:00:00\"').groupby('collision_date').sum().iloc[:,0]\ndata_time.index = data_time.index.astype('datetime64[ns]')\n\ndata_time.plot()\n\nplt.title('number of injured people per day over 14 years')\nb = plt.ylabel('number of injured people per day')","27b6c1c5":"data_time = df.query('\"04:00:00\"<collision_time<\"12:00:00\"').groupby('collision_date').sum()\ndata_time.columns = ['data_morning']\ndata_time['data_evening'] = df.query('\"20:00:00\">collision_time>\"12:00:00\"').groupby('collision_date').sum().iloc[:,0]\ndata_time['data_night'] = df.query('\"20:00:00\"<collision_time|collision_time<\"04:00:00\"').groupby('collision_date').sum().iloc[:,0]\ndata_time.index = data_time.index.astype('datetime64[ns]')\n\nax = data_time.plot()\nplt.title('Number of injured people per day in california over 14 years')\n\nbbox_props = dict(boxstyle=\"rarrow,pad=0.3\", fc=\"cyan\", ec=\"b\", lw=2)\nt = ax.text('2008\/01\/15', 100, \"Increasing\", ha=\"center\", va=\"center\", rotation=45,\n            size=15,\n            bbox=bbox_props)\nt = ax.text('2015\/03\/15', 230, \"Increasing\", ha=\"center\", va=\"center\", rotation=45,\n            size=15,\n            bbox=bbox_props)\nt = ax.text('2020\/01\/15', 10, \"Covid\", ha=\"center\", va=\"center\", rotation=-25,\n            size=15,\n            bbox=bbox_props)","09a909af":"plt.rcParams['figure.figsize'] = (20,15)\n\ndef draw_time(data, ax, period):\n    \"\"\"Used to draw the collision time series\"\"\"\n    data_time = data.query('\"04:00:00\"<=collision_time<=\"12:00:00\"').groupby('collision_date').sum()\n    data_time.columns = ['data_morning']\n    data_time['data_evening'] = data.query('\"20:00:00\">=collision_time>\"12:00:00\"').groupby('collision_date').sum().iloc[:,0]\n    data_time['data_night'] = data.query('\"20:00:00\"<collision_time|collision_time<\"04:00:00\"').groupby('collision_date').sum().iloc[:,0]\n    data_time.index = data_time.index.astype('datetime64[ns]')\n\n    data_time.plot(alpha=0.2, ax=ax)\n\n    # Decomposition of a multipllicative series doesn't support 0 or nan\n    dec_m = sm.tsa.seasonal_decompose(data_time.data_morning.fillna(1).replace(0,1), period=period, model='multiplicative')\n    data_time['moving_average_morning'] = dec_m.trend\n    dec_e = sm.tsa.seasonal_decompose(data_time.data_evening.fillna(1).replace(0,1), period=period, model='multiplicative')\n    data_time['moving_average_evening'] = dec_e.trend\n    dec_n = sm.tsa.seasonal_decompose(data_time.data_night.fillna(1).replace(0,1), period=period, model='multiplicative')\n    data_time['moving_average_night'] = dec_n.trend\n\n    data_time.iloc[:,-3:].plot(ax=ax)\n\n    plt.xlabel('Date')\n    plt.ylabel('number of injured people')\n    plt.grid()\n\n    return ax, data_time, dec_m, dec_e, dec_n\n\n# Data\ndata = df[['injured_victims','collision_date','collision_time']]\n\n# Plot data and trends\nax = plt.subplot(211)\nax, data_time, dec_m, dec_e, dec_n = draw_time(data, ax, 364)\n\nax.axvline(x='2007\/10\/01', ymin=0, ymax=0.42,color=\"red\", linestyle=\"--\")\nax.axvline(x='2009\/09\/01', ymin=0, ymax=0.42,color=\"red\", linestyle=\"--\")\n\nax.axvline(x='2015\/02\/01', ymin=0, ymax=0.80,color=\"red\", linestyle=\"--\")\nax.axvline(x='2016\/05\/15', ymin=0, ymax=0.80,color=\"red\", linestyle=\"--\")\n\nax.axvline(x='2019\/08\/30', ymin=0, ymax=0.80,color=\"red\", linestyle=\"--\")\nax.axvline(x='2020\/04\/23', ymin=0, ymax=0.80,color=\"red\", linestyle=\"--\")\n\nfor start, end in zip(['2007\/10\/01','2015\/02\/01','2019\/08\/30'],\n                      ['2009\/09\/01','2016\/05\/15','2020\/04\/23']):\n    for d, color, period in zip([80,120,160],\n                                ['brown','red','purple'],\n                                ['moving_average_night','moving_average_morning','moving_average_evening']):\n        deltay = data_time.loc[end,period] - data_time.loc[start,period]\n        deltax = len(data_time.query(f'index<\"{end}\"')) - len(data_time.query(f'index<\"{start}\"'))\n        bbox_props = dict(boxstyle=\"rarrow,pad=0.3\", fc=color, alpha=0.5, ec=\"b\", lw=2)\n        t = ax.text(pd.to_datetime(start)+pd.Timedelta(deltax\/2, unit='D'),\n                    data_time.loc[end,'moving_average_evening']+d, \n                    f\"$\\delta={np.round(deltay)}$\",\n                    ha=\"center\",\n                    va=\"center\", \n                    rotation=np.arctan(deltay\/deltax)*180\/np.pi,\n                    size=15,\n                    bbox=bbox_props)\n        \nb = ax.set_title('Trend of the number of injured people per day over 14 years')","e43f95d6":"plt.rcParams['figure.figsize'] = (20,15)\n\n# Data\ndata = df[['injured_victims','collision_date','collision_time']]\n\n# Plot data and trends\nax = plt.subplot(211)\nax, data_time, dec_m, dec_e, dec_n = draw_time(data, ax, 364)\nax.set_title('Local trend during covid period')\n\n# Plot data and trends of covid period\nax2 = plt.subplot(212)\ndata = df[['injured_victims','collision_date','collision_time']].query('\"2020-09-12\">collision_date>\"2019-12-19\"')\nax2, data_time2, dec_m2, dec_e2, dec_n2 = draw_time(data, ax2, 30)\n\n# Get limit of each axes\nxlim, xlim2 = ax.get_xlim()\nylim, ylim2 = ax.get_ylim()\n\nx2lim, x2lim2 = ax2.get_xlim()\ny2lim, y2lim2 = ax2.get_ylim()\n\n# Create covid annotation on the first ax\nbbox_props = dict(boxstyle=\"Round,pad=0.3\", color='w', edgecolor='black', lw=2)\nax.annotate(\"Covid period\", fontsize= 'x-large',\n            xy=('2020\/04\/30', 270), xycoords='data',\n            xytext=(-150, 110), textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"wedge\",\n                            connectionstyle=ConnectionStyle(\"angle3, angleA=0, angleB=-90\"), color='black', edgecolor='black'),\n            bbox=bbox_props)\n\n# Salmon rectangle\nax.add_patch(\n     patches.Rectangle(\n        (xlim+5255, ylim+30),\n        150,\n        250,\n        edgecolor = 'y',\n        facecolor = 'y',\n        alpha=0.3,\n        fill=True\n     ) )\n\n# Create 2nd covid annotation on the first ax\nbbox_props = dict(boxstyle=\"Round,pad=0.3\", color='w', edgecolor='black', lw=2)\nax.annotate(\"2nd Covid period\", fontsize= 'x-large',\n            xy=('2020\/08\/30', 270), xycoords='data',\n            xytext=(-50, 110), textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"wedge\",\n                            connectionstyle=ConnectionStyle(\"angle3, angleA=0, angleB=-90\"), color='black', edgecolor='black'),\n            bbox=bbox_props)\n\n# Cyan Rectangle\nax.add_patch(\n     patches.Rectangle(\n        (xlim+5450, ylim+30),\n        100,\n        250,\n        edgecolor = 'lightcyan',\n        facecolor = 'lightcyan',\n        alpha=0.75,\n        fill=True\n     ) )\n\n# Black line to zoom on The covid period\nxy = (1013, 50)\nxy2 = (0, 325)\ncon = ConnectionPatch(xyA=xy, xyB=xy2, coordsA='axes points', coordsB='axes points',\n                      axesA=ax, axesB=ax2, color='black')\nax2.add_artist(con)\n\nxy = (1046, 50)\nxy2 = (1110, 325)\ncon = ConnectionPatch(xyA=xy, xyB=xy2, coordsA='axes points', coordsB='axes points',\n                      axesA=ax, axesB=ax2, color='black')\nax2.add_artist(con)\n\n# Display lines in the 2nd ax\ny_purp_line_max = data_time2.moving_average_evening.max()\ny_purp_line_min = data_time2.moving_average_evening.min()\nax2.axhline(y=data_time2.moving_average_evening.min(), xmin=0.35, xmax=0.45,color=\"purple\", linestyle=\"--\")\nax2.axhline(y=data_time2.moving_average_evening.max(), xmin=0, xmax=0.45, color=\"purple\", linestyle=\"--\")\n\ny_red_line_max = data_time2.moving_average_morning.max()\ny_red_line_min = data_time2.moving_average_morning.min()\nax2.axhline(y=data_time2.moving_average_morning.min(), xmin=0.10, xmax=0.42,color=\"red\", linestyle=\"--\")\nax2.axhline(y=data_time2.moving_average_morning.max(), xmin=0.10, xmax=0.25, color=\"red\", linestyle=\"--\")\n\ny_brown_line_max = data_time2.moving_average_night.max()\ny_brown_line_min = data_time2.moving_average_night.min()\nax2.axhline(y=data_time2.moving_average_night.min(), xmin=0.25, xmax=0.42,color=\"brown\", linestyle=\"--\")\nax2.axhline(y=data_time2.moving_average_night.max(), xmin=0.25, xmax=0.32, color=\"brown\", linestyle=\"--\")\n\n# Display delta in the 2nd ax\nax2.annotate(\"\", xy=('2020\/04\/18',y_purp_line_max), xytext=('2020\/04\/18',y_purp_line_min),\n             arrowprops=dict(arrowstyle=\"<->\", color='purple', connectionstyle=\"arc3\"))\nplt.text('2020\/04\/18', (y_purp_line_max+y_purp_line_min)\/2, fr'$\\delta_e = {round((y_purp_line_max+y_purp_line_min)\/2)}$',\n         {'color': 'purple', 'fontsize': 18, 'ha': 'center', 'va': 'center',\n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)})\n\nax2.annotate(\"\", xy=('2020\/01\/23',y_red_line_max), xytext=('2020\/01\/23',y_red_line_min),\n             arrowprops=dict(arrowstyle=\"<->\", color='red', connectionstyle=\"arc3\"))\nplt.text('2020\/01\/23', (y_red_line_max+y_red_line_min)\/2, fr'$\\delta_m = {round((y_red_line_max+y_red_line_min)\/2)}$',\n         {'color': 'red', 'fontsize': 18, 'ha': 'center', 'va': 'center',\n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)})\n\nax2.annotate(\"\", xy=('2020\/02\/28',y_brown_line_max), xytext=('2020\/02\/28',y_brown_line_min),\n             arrowprops=dict(arrowstyle=\"<->\", color='brown', connectionstyle=\"arc3\"))\nb = plt.text('2020\/02\/28', (y_brown_line_max+y_brown_line_min)\/5, fr'$\\delta_n = {round((y_brown_line_max+y_brown_line_min)\/2)}$',\n         {'color': 'brown', 'fontsize': 18, 'ha': 'center', 'va': 'center',\n          'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)})","25675385":"import joypy\nplt.rcParams['figure.figsize'] = (20,10)\n\nperiod1 = pd.to_datetime('2020\/03\/19')\nperiod2 = pd.to_datetime('2020\/06\/14')\n\ndata_joypy_m = dec_m2.observed.to_frame()\ndata_joypy_n = dec_n2.observed.to_frame()\ndata_joypy_e = dec_e2.observed.to_frame()\n\ndata_joypy = pd.DataFrame()\n\nfor dataframe in [data_joypy_m, data_joypy_n, data_joypy_e]:\n    dataframe['Period'] = ['covid' if period1<=val<=period2 else 'after covid' if val>period2 else 'before covid' for val in dataframe.index]\n\nfor period in ['covid','after covid','before covid']:\n    df_ = data_joypy_m.query(f'Period==\"{period}\"').reset_index().iloc[:88,1]\n    df_ = pd.concat([df_,data_joypy_n.query(f'Period==\"{period}\"').reset_index().iloc[:88,1]]) \n    df_ = pd.concat([df_,data_joypy_e.query(f'Period==\"{period}\"').reset_index().iloc[:88,1]])\n    data_joypy[period] = df_\n\ndata_joypy['time'] = ['morning' for i in range(88)]+['night' for i in range(88)]+['evening' for i in range(88)]\n\nfig, axes = joypy.joyplot(data_joypy, by='time', alpha=0.5)\n\nfor idx,time in enumerate(['evening', 'morning','night']):\n    mean = data_joypy.query(f'time==\"{time}\"').mean().round()\n    axes[idx].text(mean[0],-0.003,str(mean[0]),\n                 {'color': 'blue', 'fontsize': 18, 'ha': 'center', 'va': 'center',\n                  'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)})\n    axes[idx].text(mean[1],-0.003,str(mean[1]),\n                 {'color': 'orange', 'fontsize': 18, 'ha': 'center', 'va': 'center',\n                  'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)})\n    axes[idx].text(mean[2],-0.003,str(mean[2]),\n                 {'color': 'green', 'fontsize': 18, 'ha': 'center', 'va': 'center',\n                  'bbox': dict(boxstyle=\"round\", fc=\"white\", ec=\"black\", pad=0.2)})\n\naxes[0].legend()\nb = axes[0].set_title('local variation of the average around covid period at each moment of the day')","3cee58a4":"from statsmodels.tsa.seasonal import STL\nimport matplotlib\n\nplt.rcParams['figure.figsize'] = (20,30)\ngs = matplotlib.gridspec.GridSpec(9, 1)\n\nax = plt.subplot(gs[0,0])\n\nplt.title('Trend and seasonality between STL and naive decomposition')\n\nax1 = plt.subplot(gs[1,0])\nax2 = plt.subplot(gs[2,0])\nax3 = plt.subplot(gs[3:5,0])\n\nplt.title('Zoom on seasonality schemas')\n\nax4 = plt.subplot(gs[5:7,0])\nax5 = plt.subplot(gs[7:9,0])\n\nfor period, dec, axis, axis2 in [('data_morning', dec_m, ax, ax3),\n                                  ('data_evening', dec_e, ax1, ax4),\n                                  ('data_night', dec_n, ax2, ax5)]:\n    decomposition = STL(data_time.loc[:, period].dropna(), period=364)\n    deco = decomposition.fit()\n    (deco.trend + deco.seasonal).plot(ax=axis)\n    (dec.seasonal*dec.trend).plot(alpha=0.5, ax=axis)\n    plt.ylabel('number of people injured')\n    \n    print(f'''{period} \n    residuals mean for naive decomposition : {((dec.seasonal*dec_n.trend) - dec.observed).abs().mean()}\n    residuals mean for STL : {deco.resid.abs().mean()}''')\n    \n    deco.seasonal.iloc[-1500:-365].plot(ax=axis2)\n\n    axis.legend((f'STL {period}',f'naive {period}'))\n    axis2.legend([f'{period}'])","d57aea93":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import ParameterGrid\n\nplt.rcParams['figure.figsize'] = (20,15)\n\nvariables = df2.copy()\nvariables['collision_time'] = [0 if '04:00:00'<=val<='12:00:00' else 1 if '12:00:00'<val<='20:00:00' else 2 if ('20:00:00'<val) or (val<'04:00:00') else -1 for val in df.collision_time]\nvariables['collision_date'] = pd.to_datetime(df.collision_date).dt.month\nvariables = variables.groupby(['collision_time','collision_date']).sum()\n\nfor i,j,period in zip([0,1,2],[0,12,24],['morning','evening','night']):\n    ax = plt.subplot(3,1,i+1)\n    variables.iloc[0+j:12+j,0].plot(ax=ax)\n    variables.iloc[0+j:12+j,1].plot(ax=ax)\n    variables.iloc[0+j:12+j,2].plot(ax=ax)\n    plt.legend(['pedestrian','bicyclist','motorcyclist'])\n    plt.title('Number of injured people per months in '+period)","b145e8ca":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import acf, pacf \n\ndef diff(df, shifts):\n    first_data = []\n    for s in shifts:\n        first_data.append(df.iloc[:s])\n        df = (df - df.shift(s)).dropna()\n    return df, first_data\n\ndef draw_dependency(df, ax1, ax2, nb):\n    # Draw Plot\n    plot_acf(df, ax=ax1, lags=40, title=f'Autocorrelation (fig-{nb})')\n    plot_pacf(df, ax=ax2, lags=40, title=f'Partial Autocorrelation (fig-{nb+1})')\n\n    # Decorate\n    # lighten the borders\n    ax1.spines[\"top\"].set_alpha(.3); ax2.spines[\"top\"].set_alpha(.3)\n    ax1.spines[\"bottom\"].set_alpha(.3); ax2.spines[\"bottom\"].set_alpha(.3)\n    ax1.spines[\"right\"].set_alpha(.3); ax2.spines[\"right\"].set_alpha(.3)\n    ax1.spines[\"left\"].set_alpha(.3); ax2.spines[\"left\"].set_alpha(.3)\n\n    # font size of tick labels\n    ax1.tick_params(axis='both', labelsize=12)\n    ax2.tick_params(axis='both', labelsize=12)\n    plt.show()\n    \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(dec_e.observed, ax1, ax2, 1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(diff(dec_e.observed,[1])[0],ax1, ax2, 3)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(diff(dec_e.observed,[1,7])[0],ax1, ax2, 5)","2853b7be":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(dec_n.observed, ax1, ax2, 1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(diff(dec_n.observed,[1])[0],ax1, ax2, 3)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(diff(dec_n.observed,[1,7])[0],ax1, ax2, 5)","e3cae934":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(dec_m.observed, ax1, ax2, 1)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(diff(dec_m.observed,[1])[0],ax1, ax2, 3)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6), dpi= 80)\ndraw_dependency(diff(dec_m.observed,[1,7])[0],ax1, ax2, 5)","50ca71bc":"plt.rcParams['figure.figsize'] = (20,10)\n\nax = plt.subplot(311)\nplt.plot(diff(dec_e.observed,[1,7])[0])\nb = plt.title('Stationary evening data')\n\nax = plt.subplot(312)\nplt.plot(diff(dec_n.observed,[1,7])[0])\nb = plt.title('Stationary night data')\n\nax = plt.subplot(313)\nplt.plot(diff(dec_m.observed,[1,7])[0])\nb = plt.title('Stationary morning data')","c90fbc11":"from statsmodels.tsa.stattools import adfuller\nfrom numpy import log\n\nfor data in [dec_e.observed,dec_n.observed,dec_m.observed]:\n    X = diff(data,[1,7])[0]\n    result = adfuller(X)\n    print(str(data.name))\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))","4ed7a112":"# Data\n# evening\ndata_e = dec_e.observed.reset_index()\ndata_e.columns = ['ds','y']\n\ndf_train_e = data_e.iloc[:-600]\ndf_predict_e = data_e.iloc[-600:-300][['ds']]\n\n# night\ndata_n = dec_n.observed.reset_index()\ndata_n.columns = ['ds','y']\n\ndf_train_n = data_n.iloc[:-600]\ndf_predict_n = data_n.iloc[-600:-300][['ds']]\n\n# morning\ndata_m = dec_m.observed.reset_index()\ndata_m.columns = ['ds','y']\n\ndf_train_m = data_m.iloc[:-600]\ndf_predict_m = data_m.iloc[-600:-300][['ds']]","b1fa6ae2":"from statsmodels.tsa.arima.model import ARIMA # It includes SARIMA model\n\ndef sarima(order, seasonal_order, df_train, df_real):\n    model = ARIMA(df_train.y, order=order, seasonal_order=seasonal_order)\n    res = model.fit()\n    print(res.summary())\n    forecast = res.predict(start=4618,end=4917,dynamic=True)\n    print('Average error: ',np.mean(abs(df_real.observed.iloc[-600:-300].values-forecast.values)))\n    \n    return forecast, res\n\n# Model 1\nforecast, res = sarima((6, 1, 1),(5, 1, 1, 7), df_train_e, dec_e)","966f9bf6":"forecast, res = sarima((1, 1, 1),(1, 1, 1, 7), df_train_e, dec_e)","6e77ff4f":"data_e.iloc[-600:-300].y.plot(figsize=(20,7))\nb = plt.plot(forecast)","88470109":"b = res.plot_diagnostics()","a57c1d78":"forecast,res = sarima((1, 1, 1),(1, 1, 1, 7), df_train_n, dec_n)\n\n#Results\ndata_n.iloc[-600:-300].y.plot(figsize=(20,7))\nplt.plot(forecast)\nb = plt.title('Forecasting of 300 days')","61e8754b":"forecast, res = sarima((1, 1, 1),(1, 1, 1, 7), df_train_m, dec_m)\n\n#Results\ndata_m.iloc[-600:-300].y.plot(figsize=(20,7))\nb = plt.plot(forecast)","8a0b7ce0":"import itertools\nimport numpy as np\nimport pandas as pd\nfrom fbprophet.diagnostics import performance_metrics, cross_validation\nimport fbprophet\n\nparam_grid = {  \n    'changepoint_prior_scale': [0.5, 1, 1.5, 2],\n    'seasonality_mode': ['multiplicative','additive']\n}\n\n# Generate all combinations of parameters\nall_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\nMAE = []  # Store the RMSEs for each params here\n\n# Use cross validation to evaluate all parameters\nfor params in all_params:\n    m = fbprophet.Prophet(**params)\n    m.fit(df_train_e)  # Fit model with given params\n    df_cv = cross_validation(m,  initial='600 days', period='200 days', horizon='300 days', parallel=\"processes\")\n    df_p = performance_metrics(df_cv, rolling_window=1)\n    MAE.append(df_p['mae'].values[0])\n\n# Find the best parameters\ntuning_results = pd.DataFrame(all_params)\ntuning_results['mae'] = MAE\nprint(tuning_results)","e3abb124":"best_params = all_params[np.argmin(MAE)]\nprint(best_params)","ceab0cd2":"m_e = fbprophet.Prophet(**best_params)\n\n# Add seasonality studied before\nm_e.add_seasonality(name='seasonality', period=7, fourier_order=100, mode='multiplicative')\nm_e.fit(df_train_e)\n\nforecast = m_e.predict(df_predict_e)\nm_e.plot(forecast, uncertainty=True)\ndec_e.observed.iloc[-600:-300].plot(color='orange', alpha=0.5)\nplt.title('Forecasting of 300 days')\n\nprint('Average error: ',round(np.mean(abs(dec_e.observed.iloc[-600:-300].values-forecast.yhat.values)),2))","e76e9279":"plt.plot(forecast.yhat)\ndec_e.observed.iloc[-600:-300].reset_index(drop=True).plot(color='orange', alpha=0.5)\nplt.legend(['pred','real'])\nb = plt.title('Forecasting of 300 days')","cc7f5339":"m_n = fbprophet.Prophet(**best_params)\n\n# Add seasonality studied before\nm_n.add_seasonality(name='seasonality', period=7, fourier_order=100, mode='multiplicative')\nm_n.fit(df_train_n)\nforecast = m_n.predict(df_predict_n)\nm_n.plot(forecast, uncertainty=True)\ndec_n.observed.iloc[-600:-300].plot(color='orange', alpha=0.5)\nplt.title('Forecasting of 300 days')\nplt.legend()\n\nprint('Average error: ',round(np.mean(abs(dec_n.observed.iloc[-600:-300].values-forecast.yhat.values)),2))","da426d00":"plt.plot(forecast.yhat)\ndec_n.observed.iloc[-600:-300].reset_index(drop=True).plot(color='orange', alpha=0.5)\nplt.legend()\nplt.legend(['pred','real'])\nb = plt.title('Forecasting of 300 days')","e665928e":"m_m = fbprophet.Prophet(**best_params)\nm_m.add_seasonality(name='seasonality', period=7, fourier_order=100, mode='multiplicative')\nm_m.fit(df_train_m)\nforecast = m_m.predict(df_predict_m)\nm_m.plot(forecast, uncertainty=True)\ndec_m.observed.iloc[-600:-300].plot(color='orange', alpha=0.5)\nplt.title('Forecasting of 300 days')\n\nprint('Average error: ',round(np.mean(abs(dec_m.observed.iloc[-600:-300].values-forecast.yhat.values)),2))","882046a1":"plt.plot(forecast.yhat)\ndec_m.observed.iloc[-600:-300].reset_index(drop=True).plot(color='orange', alpha=0.5)\nplt.legend(['pred','real'])\nb = plt.title('Forecasting of 300 days')","47902ef3":"# Evening\ndata = dec_e.observed.reset_index()\ndata.columns = ['ds','y']\n\ndf_predict = data.iloc[-300:][['ds']]\n\nax = plt.subplot(311)\nforecast = m_e.predict(df_predict)\nplt.plot(forecast.yhat)\ndec_e.observed.iloc[-300:].reset_index(drop=True).plot(color='orange', alpha=0.5)\nplt.legend(['pred','real'])\nb = plt.title('Evening: Forecasting of 300 days')\n\n# Night\ndata = dec_n.observed.reset_index()\ndata.columns = ['ds','y']\n\ndf_predict = data.iloc[-300:][['ds']]\n\nax = plt.subplot(312)\nforecast = m_n.predict(df_predict)\nplt.plot(forecast.yhat)\ndec_n.observed.iloc[-300:].reset_index(drop=True).plot(color='orange', alpha=0.5)\nplt.legend(['pred','real'])\nb = plt.title('Night: Forecasting of 300 days')\n\n# Morning\ndata = dec_m.observed.reset_index()\ndata.columns = ['ds','y']\n\ndf_predict = data.iloc[-300:][['ds']]\n\nax = plt.subplot(313)\nforecast = m_m.predict(df_predict)\nplt.plot(forecast.yhat)\ndec_m.observed.iloc[-300:].reset_index(drop=True).plot(color='orange', alpha=0.5)\nplt.legend(['pred','real'])\nb = plt.title('Morning: Forecasting of 300 days')","6b1580ae":"<h1 style=\"text-align: center; font-size:50px;\"> Prediction<\/h1><a id=d><\/a>\n\n<hr> ","e98c0a17":"<strong><u>Evening<\/u><\/strong>","0217d28e":"<h1 style=\"text-align: center; font-size:50px;\"> Trend<\/h1><a id=a><\/a>\n\n<hr> ","7674ef1c":"<strong><u>Evening:<\/u><\/strong>","ba7c1dae":"<strong><u>Night<\/u><\/strong>","3c1a485c":"Second model: ","68dc5501":"<strong><u>Best parameters:<\/u><\/strong>\n","2b689466":"<strong><u>Evening:<\/u><\/strong>","9052afaa":"## Introduction\n\nThe main goal of this notebook is to compare prophet model with SARIMA model on the california traffic collision dataset. \n\n<p><strong><u>Inputs :<\/u><\/strong>\n       \n    Number of injured people per day over 500 days.\n<\/p>\n<p><strong><u>Outputs :<\/u><\/strong>\n    \n    Number of injured people per day over 300 days.\n<\/p>\n\nIn order to have an interesting analysis, i divided the dataset into 3 categories: \n - Collisions arriving between 4 am and noon called data_morning\n - Collisions arriving between noon and 8 pm called data_evening\n - Collisions arriving between 8 pm and 4 am called data_night\n \nTogether, we will analyse the trend, the seasonality and even the impact of Covid on the dataset. So stay tuned :) \n\nIf you liked my notebook, let's upvoted it. Thank you and have a good read :) \n\n<hr>","21f33274":"Covid period could bias the results.\nSo, we trained our model on data[:-600] and we tested it on data[-600:-300].","df12ee4e":"<h2 style=\"text-align: center; font-size:25px;\"> SARIMA<\/h2><a id=da><\/a>\n\n<hr> \n\nWe are trying to make prediction with SARIMA method. We saw that a differentiation of 1 and 7 was enough to have a stationnary response.\n\nThen $(p,d,q)(P,D,Q,S)=(6,1,1)(5,1,1,7)$","86d75f99":"First model: In the column P>|z|, some of our parameters have significant p value (>5%). Let's remove them. ","d5e8e73c":"We will focus on the first covid period (the yellow rectangle). As we are currently in the second Covid period, we won't analyse it!\n\n<strong><u>Explanation:<\/u><\/strong>\n\nWe took into account 3 months before and after the containment as it lasted approximately 3 months (~19th of march to the 11th of may). So, in total 9 months are studied.\n\nLet's zoom. I applied a shorter moving average (30 days) to calculate the deltas which correspond to the larger  difference between the covid period and its neighboors. Extremums are found at the end of february and at the begining of the containment.\n\nAnd the winner is evening with 153 injured people in less.\n\nWe accompany the trend with the kde plot.","99660416":"<strong><u>Night:<\/u><\/strong>\n","e6b43028":"<strong><u>Explanation:<\/u><\/strong>\n\nOn the following graph, I compare the three periods studied above (before covid, covid, after covid) at each moment of a day. It shows that:\n\n- $\\delta_{mean~evening}: 115 - 196 = -81 \\rightarrow -70.5\\%$\n- $\\delta_{mean~morning}: 54 - 114 = -60 \\rightarrow -111.1\\%$\n- $\\delta_{mean~evening}: 47 - 71 = -24 \\rightarrow -51\\%$\n\nEven if the evening has known the higher decline in term of number of people, this is the morning which denote the higher rate with 111% of people injured per day in less compared to the period before covid.","b104de2e":"<h1 style=\"text-align: center; font-size:50px;\"> Autocorrelation<\/h1><a id=c><\/a>\n\n<hr> ","56b93630":"<h2 style=\"text-align: center; font-size:25px;\"> Prophet<\/h2><a id=db><\/a>\n\n<hr> \n\nNow, we compare SARIMA model with prophet model.\nWe started by creating the model and finding the best hyperparameters.\nHypertuning with cross validation.","02bb4823":"<strong><u>Morning:<\/u><\/strong>","924b6969":"We need to be sure that our errors are gaussian and centered ","c8bb6fe3":"<h1 style=\"text-align: center; font-size:50px;\"> California traffic collision : Forecasting and covid impact<\/h1>\n\n<hr>","232b4e3a":"<h1 style=\"text-align: center; font-size:50px;\"> Seasonality<\/h1><a id=b><\/a>\n\n<hr> ","a95b0a16":"<strong><u>Explanation:<\/u><\/strong>\n\nThe data seem stationary. Also we can reject the null hypothesis from the adfuller test which means that the data can be considered as stationary.","1c1059d9":"The graph above is a little bit messy. To analyse the increasing and covid period in a better way we can introduce the trend, there are multiple methods to estimate it:\n\n - by regression\n - by moving average \n - by exponential smoothing\n \nMoving average is really easy to calculate and pandas gives us a way to do it quickly.\n \n<strong><u>Remainder:<\/u><\/strong>\n\nA moving average is a linear operator (or a filter linear) of general shape given by\n\n$\\begin{gather*} \\sum_{i=-m_1}^{m_2} (\\theta_{i}B^{i}) \\end{gather*}$\n\n- $\\begin{gather*} Where~m_1, m_2 \\in \\mathbb{N^*} \\times \\mathbb{N^*}~and~\\theta_i \\in \\mathbb{R}~for~all~i \\end{gather*}$\n- $\\begin{gather*}B^{i}~is~the~delay~operator\\end{gather*}$\n\n<strong><u>Explanation:<\/u><\/strong>\n\nWe applied a moving average with a centered window of 364 days to remove the seasonality. The effect of that long window is the suppression of 182 and 181 days at the extremities of the curve.\nIt allows us to see the effect of each increasing and decreasing period of the trend. The red lines correspond to the days taken for the delta calculation.\n\nThe evening has the greatest variations then comes morning and finally, night.\n\nThis graph is clear. However, it doesn't give us the local impact of the covid because the moving average removed it. Let's see the real impact!","e1cdcdbe":"We can calculate the real impact of the covid period by comparing it to our expected data. ","22ade33a":"<strong><u>Morning<\/u><\/strong>","3e1635d9":"First, let's visualize the data. \n\n<strong><u>Explanation:<\/u><\/strong>\n\nThe graph below shows the 3 variables presented above. We can easily see that evening is more dangerous for people than morning and night which can be explain by more traffic on the roads. Also, it shows 2 increasing periods and 1 decreasing period : the two first could be due to the setting up of the storage system and the third one is obviously due to Covid.","ca1778b7":"After analysing the trend, we are now focusing on the seasonality.\n\n<strong><u>Remainder:<\/u><\/strong>\n\nWe can describe a temporal serie as,\n\n- an additive function: $Y_t = m_t + s_t + \\epsilon_t$\n- a multipicative function: $Y_t = m_ts_t\\epsilon_t$\n\nwith $m_t~the~trend,~s_t~the~seasonality~and~epsilon~the~error$\n\nIt is possible to estimate the trend by moving average and then seasonality.\n\n<strong><u>Explanation:<\/u><\/strong>\n\nAs the variance of our series is increasing over time, we are expecting a multiplicative model. \n\nWe fisrt estimate the seasonality with a naive decomposition (see stats models https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.seasonal.seasonal_decompose.html). We will compare it to a more reliable method : STL which has the advantage to do not remove the extremities.\n\nEven if they have close schemas, the residuals are lower for STL decomposition. \n\nThe seasonality curves have been plot. We can see:\n\n- Morning: Gap in january and during summer. A maximum between august and january.\n- Evening: Same patterns than morning with the only difference that there is a more timid decline during summer.\n- Night: Gap in january and then a bell curve is drawn with a maximum during summer.\n\nOk, we have the seasonality. Now, how can we explain it ? What if we take their means of transport ? The weather? We can take into account the period of sunny day too.","1afd527f":"# thank you for reading my notebook and let's upvote it! ","5baa023d":"The residuals are not perfectly gaussian. We could improve the model by differentiating one more time $(d,D)=(2,1)$.","44b20e9b":"The following graph shows the number of people injured per month over the 14 years.\n\n<strong><u>Explanation:<\/u><\/strong>\n\nFrom internet:\nOn average, the hottest months are July, August and September, and the coldest months are January, February and December.\nThe rainiest months are January, February and December.\n\nAccording to the following collisions data, we can clearly see that motos and bikes are most commonly used during summer when the weather is nice. Walking is chosen when it's raining and cold. The gap in january can be explained by the fact that motos and bikes represent the biggest part in collisions. The fact that the pedestrian collisions number is increasing in January can be explain by darker days.","c422a82d":"Before making any prediction. We will analyse the autocorrelation of the values by drawing ACF and PACF. This will be very usefull for the SARIMA model.\n\n<strong><u>Remainder:<\/u><\/strong>\n\n- Autocorrelation function (ACF). At k-offset, it is the correlation between the values of series separated by k intervals.\n- Partial autocorrelation function (PACF). At k-offset, this is the correlation between the values of series separated by k intervals, taking into account the values of the intermediate intervals.\n\nSARIMA model: See\n\n- https:\/\/towardsdatascience.com\/understanding-sarima-955fe217bc77\n- https:\/\/machinelearningmastery.com\/sarima-for-time-series-forecasting-in-python\/.\n\n<strong><u>Explanation:<\/u><\/strong>\n\nThe autocorrelation study have been made for evening, night and morning data. As they have the same behaviour, the following explanation is valid for the three of them.\n\nACF shows us a strong autocorrelation for every previous data. It's clear that our data are not stationary.\nThe partial autocorrelation shows us that all these strong autocorrelations are due to just few of them like lag 1, 2 or 7 **(fig 1 and 2)**.\nAfter having differentiated by 1, we can see significant lag 7 **(fig 3 and 4)**. Let's differentiate by 7. Now, a big part of lags are not autocorrelated **(fig 5 and 6)**.\nPartial autocorrelation shows us an inverse correlation.\n\nThis is an important step to search SARIMA model's parameters. ACF gives us informations about MA parameters and PACF gives us informations about AR parameters. We know also that we need to differentiate by 1 then by 7. \n\n$(d, D)=(1,1)~with~a~period~s=7.$\n\nAs the last autocorrelation graph **(fig 5)** has first 7 significant lags and s=7 we take \n\n$(q, Q) = (6,1)$\n\nThis way the first 7 lags are taken into account. As the partial AC **(fig 6)** has significant lags at each 7 interval (and 6 but we won't take it into account) and at the 6 first lags we take \n\n$(p,P)=(6,5)$\n\nWe are stopping here let's see if the data are stationary.","786fe772":"# Content\n\n1. [Trend](#a)\n2. [Seasonality](#b)\n3. [autocorrelation](#c)\n4. [Prediction](#d)\n    - [SARIMA model](#da)\n    - [Prophet model](#db)\n    - [Expected values for covid period](#dc)","baeaaeee":"<strong><u>Morning:<\/u><\/strong>","a4256b25":"<h2 style=\"text-align: center; font-size:25px;\"> Expected values for covid period<\/h2><a id=dc><\/a>\n\n<hr> \n\nProphet model gives us better results. We are now forecasting the covid period.","bccc0842":"Here is the graph of the results","22ff7fac":"<strong><u>Night:<\/u><\/strong>"}}