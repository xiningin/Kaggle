{"cell_type":{"ebc4b247":"code","65055175":"code","fde8eb44":"code","06985c0c":"code","96940d52":"code","ecf55af3":"code","0d1d76a2":"code","a18ee57d":"code","cd6a36dd":"code","d536ab84":"code","f99e2885":"code","7d5017ea":"code","c7462283":"code","d8bbf462":"code","31ba68cd":"code","3ac42987":"code","5e40537d":"code","362a9336":"code","32a3cc9a":"code","514d7d54":"code","264fcba0":"code","90b30efc":"code","8ac33b4f":"code","ad21ba38":"code","26d96c62":"code","6d6b3f29":"code","ca0817b1":"code","a6350ae1":"code","93bca976":"code","c25b2a73":"code","4740ea23":"code","2000fe0a":"code","d94cb468":"code","f7b00243":"code","9b667e5b":"code","a1a570d1":"code","1ca1eedb":"markdown","649bcddc":"markdown","a2324516":"markdown","4704a2fb":"markdown","26d44c35":"markdown","4e72da21":"markdown","b7a81950":"markdown","9dbdbf5c":"markdown","9eda4042":"markdown","3867bb84":"markdown"},"source":{"ebc4b247":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\n# Preparation  \nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import KBinsDiscretizer\n# Import StandardScaler from scikit-learn\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer,IterativeImputer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline,FeatureUnion\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve,confusion_matrix\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif,chi2\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif,VarianceThreshold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom xgboost import XGBClassifier\nfrom sklearn import set_config\n\nfrom itertools import combinations\n#import smong \n\nimport category_encoders as ce\nimport warnings\nimport optuna \nfrom joblib import Parallel, delayed\nimport joblib \nfrom sklearn import set_config\nimport preparation\nset_config(display='diagram')\nimport ray\nwarnings.filterwarnings('ignore')","65055175":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\n# Preview the data\ntrain.head()","fde8eb44":"# Author : https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        name =df[col].dtype.name \n        \n        if col_type != object and col_type.name != 'category':\n        #if name != \"category\":    \n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","06985c0c":"num_features= train.drop(['id','claim'], axis=1).columns\ncat_features = [w.replace('f', 'c') for w in num_features]\ny = train['claim']\nnum_features","96940d52":"# create the discretizer object with strategy quantile and 1000 bins\ndiscretizer = KBinsDiscretizer(n_bins=47, encode='ordinal',strategy='quantile')\n\npipeline = Pipeline([\n        ('imputer', SimpleImputer( strategy='median')),\n        ('bin', discretizer)\n    ])\n# fit the discretizer to the train set\npipeline.fit(train.loc[:,num_features])\n\n# apply the discretisation\ntrain2 = pipeline.transform(train.loc[:,num_features])\n\ntest2 = pipeline.transform(test.loc[:,num_features])\n\ntrain2_df=pd.DataFrame(train2,columns=cat_features).astype('category')\ntest2_df=pd.DataFrame(test2,columns=cat_features).astype('category')\n# Apply the mask to create a reduced dataframe\ntrain2_df_red =train2_df.iloc[:, 0:10]\ntest2_df_red=test2_df.iloc[:, 0:10]\n#print(\"Dimensionality reduced from {} to {}.\".format(train2_df.shape[1], train2_df_red.shape[1])","ecf55af3":"x_final= pd.concat( [train.loc[:,num_features[0:10]], train2_df_red], axis=1) \nx_test_final= pd.concat( [test.loc[:,num_features[0:10]], test2_df_red], axis=1)","0d1d76a2":"del train\ndel train2_df\ndel test \ndel test2_df ","a18ee57d":"x_final[x_final.select_dtypes(['float64']).columns] = x_final[x_final.select_dtypes(['float64']).columns].apply(pd.to_numeric)\nx_final[x_final.select_dtypes(['object']).columns] = x_final.select_dtypes(['object']).apply(lambda x: x.astype('category'))\nx_test_final[x_test_final.select_dtypes(['float64']).columns] = x_test_final[x_test_final.select_dtypes(['float64']).columns].apply(pd.to_numeric)\nx_test_final[x_test_final.select_dtypes(['object']).columns] = x_test_final.select_dtypes(['object']).apply(lambda x: x.astype('category'))","cd6a36dd":"# select non-numeric columns\ncat_columns = x_final.select_dtypes(include=['object','category']).columns\ncat_columns","d536ab84":"# select the float columns\nnum_columns = x_final.select_dtypes(exclude=['object','category']).columns\nnum_columns","f99e2885":"# select the float\/cat columns\ncat_columns = x_final.select_dtypes(include=['object','category']).columns\nnum_columns = x_final.select_dtypes(exclude=['object','category']).columns\n#Define vcat pipeline\n#cat_feautres2=['c10', 'c11', 'c24']\ncat_pipe = Pipeline([('selector',preparation.ColumnsSelector(cat_columns)),\n                    # ('Encoder', ce.target_encoder.TargetEncoder())\n                    ])\n#Define vnum pipeline\nnum_pipe = Pipeline([('selector',preparation.ColumnsSelector(num_columns)),\n                     # ('features',preparation.FeaturesEngineerTPS()),\n                      #('outlier',preparation.OutlierReplace()),\n                     ('imputer', SimpleImputer(strategy='median',add_indicator=False)),\n                     ('scaler', QuantileTransformer()),\n                     #('kmeans', preparation.MiniKmeansTransformerStand())\n                    ])\n#Featureunion fitting training data\npreprocessor = FeatureUnion(transformer_list=[('cat', cat_pipe),\n                                              ('num', num_pipe)])\n# preprocessor.fit(X_train)\n#############################\n# Complete Pipe \ndef create_pipeline(model, preprocessor):\n    pipeline = Pipeline([\n        ('pre', preprocessor),\n        ('estimator', model)\n    ])\n    return pipeline\n","7d5017ea":"%%time \npreprocessor.fit(x_final,y)\nx_final_pre = preprocessor.transform(x_final)\n#x_test_final_pre = preprocessor.transform(x_test_final)\ndel x_final\ndel x_test_final","c7462283":"# split into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(x_final_pre, y, test_size=0.1)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","d8bbf462":"# set learning rate scheduler\n# we can chage learning rate during learning\ndef lr_schedul(epoch):\n    x = 0.001\n    if epoch >= 5:\n        x = 0.00005\n    if epoch >= 10:\n        x = 0.00001\n    if epoch >= 20:\n        x = 0.000005\n    if epoch >= 30:\n        x = 0.00001\n    if epoch >= 60:\n        x = 0.0000001        \n    return x\n\nlr_decay = LearningRateScheduler(\n    lr_schedul,\n    verbose=1,\n)","31ba68cd":"from tensorflow.keras.metrics import AUC\ndef generate_categorical_feature_tf(cat_features,num_features,data):\n    models= []\n    inputs = []\n    for cat in cat_features:\n        \n        vocab_size = data[cat].nunique()+1\n        \n        no_of_unique_cat  = len(np.unique(data[cat]))\n        #Jeremy Howard provides the following rule of thumb; embedding size = min(50, number of categories\/2).\n        embedding_size = min(np.ceil((no_of_unique_cat)\/2), 50 )\n        embedding_size = int(embedding_size)\n        \n        inpt = tf.keras.layers.Input(shape=(1,),name='input_'+'_'.join(cat.split(' ')))\n        inputs.append(inpt)\n        embed = tf.keras.layers.Embedding(vocab_size,embedding_size,\\\n                                          trainable=True\n                                          \n            #,embeddings_initializer=tf.initializers.random_normal\n                        ,embeddings_initializer=tf.initializers.zeros()\n                                     )(inpt)\n        embed_rehsaped =tf.keras.layers.Reshape(target_shape=(embedding_size,))(embed)\n        models.append(embed_rehsaped)\n    num_input = tf.keras.layers.Input(shape=(len(num_columns)),\\\n                                      name='input_number_features')\n    inputs.append(num_input)\n    models.append(num_input)\n    merge_models= tf.keras.layers.concatenate(models)\n    pre_preds = tf.keras.layers.Dense(160)(merge_models)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(120)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(80)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(60)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(20)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pred = tf.keras.layers.Dense(1,activation='sigmoid')(pre_preds)\n    model_full = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\n  \n    optimizer_adam = tf.keras.optimizers.Adam(learning_rate=2e-7)\n    model_full.compile(loss=tf.keras.losses.binary_crossentropy,\\\n                           metrics=[AUC(name='auc')],\n                           optimizer=optimizer_adam)\n    return model_full\n\n\ndef generate_categorical_feature_tf_Bilstm(cat_features,num_features,data):\n    models= []\n    inputs = []\n    models1=[]\n    for cat in cat_features:\n        \n        vocab_size = data[cat].nunique()+1\n        \n        no_of_unique_cat  = len(np.unique(data[cat]))\n        #Jeremy Howard provides the following rule of thumb; embedding size = min(50, number of categories\/2).\n        embedding_size = min(np.ceil((no_of_unique_cat)\/2), 50 )\n        embedding_size = int(embedding_size)\n        \n        inpt = tf.keras.layers.Input(shape=(1,),name='input_'+'_'.join(cat.split(' ')))\n        inputs.append(inpt)\n        embed = tf.keras.layers.Embedding(vocab_size,embedding_size,\\\n                                          trainable=True\n                                          \n            #,embeddings_initializer=tf.initializers.random_normal\n                        ,embeddings_initializer=tf.initializers.zeros()\n                                     )(inpt)\n        #embed_rehsaped =tf.keras.layers.Reshape(target_shape=(embedding_size,))(embed)\n        models1.append(embed)\n    merge_emb= tf.keras.layers.concatenate(models1)\n    bilstm1 =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,\n                                            return_sequences=False))(merge_emb)\n    x_emb = tf.keras.layers.Dense(4)(bilstm1)\n    # bilstm2 = Bidirectional(LSTM(32, return_sequences=True))(bilstm1)\n    \n    num_input = tf.keras.layers.Input(shape=(len(num_columns)),\\\n                                      name='input_number_features')\n    inputs.append(num_input)\n    models.append(x_emb)\n    models.append(num_input)\n    merge_models= tf.keras.layers.concatenate(models)\n    pre_preds = tf.keras.layers.Dense(160)(merge_models)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(120)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(80)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(60)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(20)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pred = tf.keras.layers.Dense(1,activation='sigmoid')(pre_preds)\n    model_full = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\n  \n    optimizer_adam = tf.keras.optimizers.Adam(learning_rate=2e-7)\n    model_full.compile(loss=tf.keras.losses.binary_crossentropy,\\\n                           metrics=[AUC(name='auc')],\n                           optimizer=optimizer_adam)\n    return model_full\n\n\n\ndef ClassModel(input_shape):\n    X_input_bin = Input(input_shape)\n    X_input = Input(input_shape)\n    X = Embedding (input_dim=96, output_dim=18)(X_input_bin)\n    #X = Dropout(0.4)(X)\n    X = Flatten()(X)\n    X = Dense(30, activation='swish')(X)\n    X1 = Dense(30, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(X_input)\n    X = Add()([X,X1])\n    X = Dropout(0.5)(X)\n    X = Dense(30, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(X)\n#     X = BatchNormalization()(X)\n    X = Dropout(0.5)(X)\n\n    X = Dense(1, kernel_initializer=tf.keras.initializers.GlorotNormal(),activation='sigmoid', name='output2')(X)\n    model = Model(inputs = [X_input_bin,X_input], outputs = X, name='ClassModel')\n    return model","3ac42987":"def generate_categorical_feature_tf_Bilstm2(cat_features,num_features,data):\n    models= []\n    inputs = []\n    models1=[]\n    for cat in cat_features:\n        \n        vocab_size = data[cat].nunique()+1\n        \n        no_of_unique_cat  = len(np.unique(data[cat]))\n        #Jeremy Howard provides the following rule of thumb; embedding size = min(50, number of categories\/2).\n        embedding_size = min(np.ceil((no_of_unique_cat)\/2), 50 )\n        embedding_size = int(embedding_size)\n        \n        inpt = tf.keras.layers.Input(shape=(1,),name='input_'+'_'.join(cat.split(' ')))\n        inputs.append(inpt)\n        embed = tf.keras.layers.Embedding(vocab_size,embedding_size,\\\n                                          trainable=True\n                                          \n            #,embeddings_initializer=tf.initializers.random_normal\n                        ,embeddings_initializer=tf.initializers.zeros()\n                                     )(inpt)\n        #embed_rehsaped =tf.keras.layers.Reshape(target_shape=(embedding_size,))(embed)\n        models1.append(embed)\n        #merge_emb= tf.keras.layers.concatenate(models1,axis=-1)\n        \n    merge_emb= tf.keras.layers.concatenate(models1,axis=-1)\n    cat_merged = tf.keras.layers.Reshape((10, -1))(merge_emb)\n    bilstm1 =  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, \n                                            return_sequences=False))(cat_merged)\n    x_emb = tf.keras.layers.Dense(4)(bilstm1)\n    \n    # bilstm2 = Bidirectional(LSTM(32, return_sequences=True))(bilstm1)\n    \n    num_input = tf.keras.layers.Input(shape=(len(num_columns)),\\\n                                      name='input_number_features')\n    inputs.append(num_input)\n    models.append(x_emb)\n    models.append(num_input)\n    merge_models= tf.keras.layers.concatenate(models)\n    pre_preds = tf.keras.layers.Dense(160)(merge_models)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(120)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(80)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(60)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(20)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pred = tf.keras.layers.Dense(1,activation='sigmoid')(pre_preds)\n    model_full = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\n  \n    optimizer_adam = tf.keras.optimizers.Adam(learning_rate=2e-7)\n    model_full.compile(loss=tf.keras.losses.binary_crossentropy,\\\n                           metrics=[AUC(name='auc')],\n                           optimizer=optimizer_adam)\n    return model_full","5e40537d":"def generate_categorical_feature_tf_CNN_LSTM(cat_features,num_features,data):\n    models= []\n    inputs = []\n    models1=[]\n    # first input model\n    for cat in cat_features:\n        \n        vocab_size = data[cat].nunique()+1\n        \n        no_of_unique_cat  = len(np.unique(data[cat]))\n        #Jeremy Howard provides the following rule of thumb; embedding size = min(50, number of categories\/2).\n        embedding_size = min(np.ceil((no_of_unique_cat)\/2), 50 )\n        embedding_size = int(embedding_size)\n        \n        inpt = tf.keras.layers.Input(shape=(1,),name='input_'+'_'.join(cat.split(' ')))\n        inputs.append(inpt)\n        embed = tf.keras.layers.Embedding(vocab_size,embedding_size,\\\n                                          trainable=True\n                                          \n            #,embeddings_initializer=tf.initializers.random_normal\n                        ,embeddings_initializer=tf.initializers.zeros()\n                                     )(inpt)\n        #embed_rehsaped =tf.keras.layers.Reshape(target_shape=(embedding_size,))(embed)\n        models1.append(embed)\n        #merge_emb= tf.keras.layers.concatenate(models1,axis=-1)\n    merge_emb= tf.keras.layers.concatenate(models1,axis=-1)\n    cat_merged = tf.keras.layers.Reshape((10, -1))(merge_emb)\n    conv1 = tf.keras.layers.Conv1D(filters=4,\n               kernel_size=4,\n               strides=1,\n               activation='relu')(cat_merged)\n    pool1 = tf.keras.layers.MaxPooling1D(pool_size=4)(conv1)\n    lstm1 = tf.keras.layers.LSTM(32)(pool1)\n    pre_cat = tf.keras.layers.Dense(4)(lstm1)\n    # bilstm2 = Bidirectional(LSTM(32, return_sequences=True))(bilstm1)\n    \n    # Second input model\n    num_input = tf.keras.layers.Input(shape=(len(num_columns)),\\\n                                      name='input_number_features')\n    inputs.append(num_input)\n    pre_preds = tf.keras.layers.Dense(60)(num_input)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(40)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(20)(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_num = tf.keras.layers.Dense(4)(pre_preds)\n    \n    # merge input models\n    merge = tf.keras.layers.concatenate([pre_num, pre_cat])\n    pred = tf.keras.layers.Dense(1,activation='sigmoid')(merge)\n    model_full = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\n  \n    optimizer_adam = tf.keras.optimizers.Adam(learning_rate=2e-7)\n    model_full.compile(loss=tf.keras.losses.binary_crossentropy,\\\n                           metrics=[AUC(name='auc')],\n                           optimizer=optimizer_adam)\n    return model_full\n","362a9336":"def generate_categorical_Grouped_tf_CNN_LSTM(cat_columns,num_columns,data):\n    inputs=[]\n    # first input model \n    BINS=47\n    cat_input =  tf.keras.layers.Input(shape=(len(cat_columns)),\\\n                                      name='input_cat_features')\n    inputs.append(cat_input)\n    X = tf.keras.layers.Embedding(input_dim=BINS, output_dim=10, embeddings_initializer = \"glorot_uniform\")(cat_input)\n    conv1 = tf.keras.layers.Conv1D(filters=4,\n               kernel_size=4,\n               strides=1,\n                activation='swish')(X)\n    pool1 = tf.keras.layers.MaxPooling1D(pool_size=4)(conv1)\n    lstm1 = tf.keras.layers.LSTM(32)(pool1)\n    pre_cat = tf.keras.layers.Dense(4)(lstm1)\n    # bilstm2 = Bidirectional(LSTM(32, return_sequences=True))(bilstm1)\n    # Second input model\n    num_input = tf.keras.layers.Input(shape=(len(num_columns)),\\\n                                      name='input_number_features')\n    inputs.append(num_input)\n    pre_preds = tf.keras.layers.Dense(60, activation='swish')(num_input)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(40, activation='swish')(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_preds = tf.keras.layers.Dense(20, activation='swish')(pre_preds)\n    pre_preds = tf.keras.layers.BatchNormalization()(pre_preds)\n    pre_num = tf.keras.layers.Dense(4, activation='swish')(pre_preds)\n    \n    # merge input models\n    merge = tf.keras.layers.concatenate([pre_num, pre_cat])\n    pred = tf.keras.layers.Dense(1,activation='sigmoid')(merge)\n    model_full = tf.keras.models.Model(inputs= inputs,\\\n                                       outputs =pred)\n  \n    optimizer_adam = tf.keras.optimizers.Adam(learning_rate=2e-7)\n    model_full.compile(loss=tf.keras.losses.binary_crossentropy,\\\n                           metrics=[AUC(name='auc')],\n                           optimizer=optimizer_adam)\n    return model_full","32a3cc9a":"joinedlist = list(cat_columns)+list(num_columns)","514d7d54":"x_final_pre_df=pd.DataFrame(x_final_pre,columns=joinedlist)\n#x_test_final_pre_df=pd.DataFrame(x_test_final_pre,columns=joinedlist)","264fcba0":"x_final_pre_df[x_final_pre_df.select_dtypes(['float64']).columns] = x_final_pre_df[x_final_pre_df.select_dtypes(['float64']).columns].apply(pd.to_numeric)\nx_final_pre_df[x_final_pre_df.select_dtypes(['object']).columns] = x_final_pre_df.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n#x_test_final_pre_df[x_test_final_pre_df.select_dtypes(['float64']).columns] = x_test_final_pre_df[x_test_final_pre_df.select_dtypes(['float64']).columns].apply(pd.to_numeric)\n#x_test_final_pre_df[x_test_final_pre_df.select_dtypes(['object']).columns] = x_test_final_pre_df.select_dtypes(['object']).apply(lambda x: x.astype('category'))","90b30efc":"del x_final_pre\n#del x_test_final_pre","8ac33b4f":"model = generate_categorical_Grouped_tf_CNN_LSTM(cat_columns,num_columns,x_final_pre_df)","ad21ba38":"model.summary()","26d96c62":" tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76, )","6d6b3f29":"input_dict={}\nfor i in range(len(cat_columns)):\n        input_dict['input_c'+ str(i+1)] = x_final_pre_df[cat_columns[i]]\ninput_dict['input_number_features']=x_final_pre_df[num_columns]","ca0817b1":"\n \ninput_dict2={ 'input_cat_features': np.float32(x_final_pre_df[cat_columns]),\n'input_number_features':np.float32(x_final_pre_df[num_columns])}","a6350ae1":"### del x_final_pre_df\n# del x_test_final_pre_df","93bca976":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['auc']\n    val_acc = history.history['val_auc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training auc')\n    plt.plot(x, val_acc, 'r', label='Validation auc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","c25b2a73":"EPOCHS =1000\n# configure early stopping\nes = EarlyStopping(monitor='val_auc',min_delta=0.00000000000001,\n                   restore_best_weights=True,patience=10)\n#filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n# define the checkpoint\nfilepath = \"model.hdf5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath=filepath, \n                             monitor='val_auc',\n                             verbose=1, \n                            #save_weights_only=True,\n                             save_best_only=True,\n                             mode='max')\n#batch_size=1000\n# fit model using our gpu\nwith tf.device('\/gpu:0'):\n     history = model.fit(input_dict2,y,batch_size=256,epochs=EPOCHS, \n                         validation_split = 0.08,\n                         verbose=1 ,callbacks=[lr_decay ,es,checkpoint],shuffle=True)","4740ea23":"plot_history(history)","2000fe0a":"# evaluate the keras model\nmodel = tf.keras.models.load_model(filepath)\nloss, auc =model.evaluate( input_dict2, y, verbose=2)\noof=model.predict( input_dict2)\nprint(\" auc\".format(auc))\nprint(\"Roc auc score OOF %0.3f\" % roc_auc_score(y_true=y, y_score=oof))\nprint(\"Average precision OOF %0.3f\" % average_precision_score(y_true=y, y_score=oof))","d94cb468":"from sklearn.metrics import auc\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_true=y, y_score=oof)\nauc_keras = auc(fpr_keras, tpr_keras)\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","f7b00243":"# Zoom in view of the upper left corner.\nplt.figure(2)\nplt.xlim(0, 0.4)\nplt.ylim(0.8, 1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve (zoomed in at top left)')\nplt.legend('loct')\nplt.show()","9b667e5b":"#predictions = model.predict(test_dict)\n#predictions = predictions.flatten()\n#predictions","a1a570d1":"# Save the predictions to a CSV file\n#sub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n#sub['claim']=predictions\n#sub.to_csv('CV_submission_deep.csv', index=False)\n#sub","1ca1eedb":"test_dict={}\nfor i in range(len(cat_columns)):\n        test_dict['input_cat'+ str(i+1)] = x_test_final_pre_df[cat_columns[i]]\ntest_dict['input_number_features']=x_test_final_pre_df[num_columns]","649bcddc":"best link to follow : \n\nhttps:\/\/www.pyimagesearch.com\/2019\/02\/04\/keras-multiple-inputs-and-mixed-data\/\n\n https:\/\/stackoverflow.com\/questions\/52627739\/how-to-merge-numerical-and-embedding-sequential-models-to-treat-categories-in-rn\n\nhttps:\/\/machinelearningmastery.com\/keras-functional-api-deep-learning\/\n\nshape for Lstm : \n\nhttps:\/\/shiva-verma.medium.com\/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n\n\n","a2324516":" \n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Get Started in Deep Learning<\/center><\/h3>\n\n* [The 5-Step Model Life-Cycle](#1)\n* [Import](#2)\n* [Prepare Data\/Impute Missing Value](#3)\n* [equential Model API](#4)   \n* [Modeling](#100)\n* [Submission](#100)\n    \nPredictive modeling with deep learning is a skill that modern developers need to know.\n\nTensorFlow is the premier open-source deep learning framework developed and maintained by Google. Although using TensorFlow directly can be challenging, the modern tf.keras API beings the simplicity and ease of use of Keras to the TensorFlow project.\n\nUsing tf.keras allows you to design, fit, evaluate, and use deep learning models to make predictions in just a few lines of code. It makes common deep learning tasks, such as classification and regression predictive modeling, accessible to average developers looking to get things done.\n\nIn this Notebook, you will discover a step-by-step guide to developing deep learning models in TensorFlow using the tf.keras API.\n Deep Learning Model Life-Cycle\n\nIn this section, you will discover the life-cycle for a deep learning model and the two tf.keras APIs that you can use to define models.\n    \n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>The 5-Step Model Life-Cycle<\/center><\/h3>\n\nA model has a life-cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the tf.keras API.\n\nThe five steps in the life-cycle are as follows:\n    \n\n                Define the model.\n\n                Compile the model.\n\n                Fit the model.\n\n                Evaluate the model.\n\n                Make predictions.    \n\n    \n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Import<\/center><\/h3>","4704a2fb":"# https:\/\/www.kaggle.com\/bannourchaker\/single-nn\/edit\n# To try  : \n%%time\nBATCH_SIZE=256\nSHUFFLE_BUFFER_SIZE = 256\nN_FOLD = 10\nEPOCH = 30\nLR = 5e-4\nDECAY_STEP = 3000\nDECAY_RATE =0.9\nQUANT = 256\nBINS = 256\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=QUANT, output_distribution='uniform')),\n        (\"binning\", KBinsDiscretizer(n_bins=BINS, encode='ordinal',strategy='uniform'))])\nX = pipe.fit_transform(X)\nx_test = pipe.transform(x_test)\n\n","26d44c35":"model_CNN_LSTM.add(tf.keras.layers.Conv1D(200, 7, input_shape=(1,768),activation='relu',padding='same'))\n#model_CNN_LSTM.add(tf.keras.layers.MaxPooling1D(pool_size=3,padding=\"VALID\"))\nmodel_CNN_LSTM.add(tf.keras.layers.Dropout(rate=0.2))\nmodel_CNN_LSTM.add(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))","4e72da21":"# Regularization \/more deep ","b7a81950":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Sequential Model API (Simple)<\/center><\/h3>\n\n## Define model\n","9dbdbf5c":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Load Data<\/center><\/h3>\n## Load the training data","9eda4042":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Prepare Data\/Impute Missing Value<\/center><\/h3>\n\n\n## Prepare Data :","3867bb84":"add data , minimise one and \n\n# Reference  : \nhttps:\/\/www.kaggle.com\/lucamassaron\/deep-learning-for-tabular-data\n\nhttps:\/\/www.kaggle.com\/lukaszborecki\/tps-09-nn\n\nhttps:\/\/www.kaggle.com\/siavrez\/kerasembeddings\n\n\nThings to try : \n\nhttps:\/\/www.kaggle.com\/datafan07\/top-1-approach-eda-new-models-and-stacking\n\n\nDeep  : \nhttps:\/\/www.kaggle.com\/shivansh002\/tame-your-neural-network-once-for-all\n\nhttps:\/\/www.kaggle.com\/lukaszborecki\/tps-09-nn\/\n\n\n\n"}}