{"cell_type":{"3a384c26":"code","6388abb6":"code","a87c57f2":"code","9fb12b02":"code","962713c7":"code","092be74d":"code","5f0d91b8":"code","07e1a79a":"code","92f84dc5":"code","a3d4c76b":"code","b755bcb3":"code","4ff6cf8c":"markdown","1a9d0c2e":"markdown"},"source":{"3a384c26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6388abb6":"\n#Importign libraries\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras","a87c57f2":"#Importing the Dataset\ndf = pd.read_csv('..\/input\/Concrete_Data_Yeh.csv')\nx_org = df.drop('csMPa',axis=1).values\ny_org = df['csMPa'].values\n\n","9fb12b02":"## Knowing The Data\n# #Correlation heatmap\ncorr = df.corr()\nsns.heatmap(corr,xticklabels=True,yticklabels=True,annot = True,cmap ='coolwarm')\nplt.title(\"Correlation Between Variables\")\nplt.savefig('1.png')\n\n# # pair Plot\nsns.pairplot(df,palette=\"husl\",diag_kind=\"kde\")\nplt.savefig('2.png')\n","962713c7":"# Using Test\/Train Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_org,y_org, test_size=0.3)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","092be74d":"# Building ANN As a Regressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import backend\n\n","5f0d91b8":"#Defining Root Mean Square Error As our Metric Function \ndef rmse(y_true, y_pred):\n\treturn backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n\n","07e1a79a":"#Building  first layer Layers \nmodel=Sequential()\n\nmodel.add(Dense(64,input_dim=8,activation = 'relu'))\n\n# Bulding Second and third layer\nmodel.add(Dense(32,activation='relu'))\nmodel.add(keras.layers.normalization.BatchNormalization())\n\n# Output Layer\nmodel.add(Dense(1,activation='linear'))\n\n","92f84dc5":"# Optimize , Compile And Train The Model \nopt =keras.optimizers.Adam(lr=0.0015)\n\nmodel.compile(optimizer=opt,loss='mean_squared_error',metrics=[rmse])\nhistory = model.fit(X_train,y_train,epochs = 35 ,batch_size=32,validation_split=0.1)\n\nprint(model.summary())\n","a3d4c76b":"# Predicting and Finding R Squared Score\n\ny_predict = model.predict(X_test)\n\nfrom sklearn.metrics import r2_score\nprint(r2_score(y_test,y_predict))\n","b755bcb3":"# Plotting Loss And Root Mean Square Error For both Training And Test Sets\nplt.plot(history.history['rmse'])\nplt.plot(history.history['val_rmse'])\nplt.title('Root Mean Squared Error')\nplt.ylabel('rmse')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.savefig('4.png')\nplt.show()\n","4ff6cf8c":"**Applying A Deep Neural Network**","1a9d0c2e":"***looks like a good Score***"}}