{"cell_type":{"705a3487":"code","a57c1717":"code","69d6f9b1":"code","9363f56f":"code","83f605e0":"code","01cf535b":"code","b119ab64":"code","ca467f26":"code","46ee0f20":"code","4835c7c9":"code","e4f7ed1a":"code","caae3d4d":"markdown","2713b0fc":"markdown","3087a14f":"markdown","042cae2b":"markdown","6cc5c9cf":"markdown","ff58d1e9":"markdown","7fc1a5d2":"markdown","bc57c9d3":"markdown","94a4809f":"markdown"},"source":{"705a3487":"# Importing required packages\nimport matplotlib\nmatplotlib.use('nbagg')\nimport numpy as np\nimport matplotlib.pyplot as plt","a57c1717":"def error(w):\n    return (w**2) + (2*w) + 2","69d6f9b1":"w = list(range(-10,10))\nerr = []\nfor i in w:\n    err.append(error(i))","9363f56f":"def gradient(w):\n    return 2*w + 2","83f605e0":"def delta(w, eta):\n    return eta*gradient(w)\n\ndef gradient_descent(eta, w, nb_of_iterations):\n    w_err = [np.array([w, error(w)])] # List to store the w, error values\n    for i in range(nb_of_iterations):\n        dw = delta(w, eta)  # Get the delta w update\n        w = w - dw  # Update the current w value\n        w_err.append(np.array([w, error(w)]))  # Add w, error to list\n    return np.array(w_err)","01cf535b":"# Set the learning rate\neta = 0.2\n\n#Set the initial parameter\nw = 5\n\n# number of gradient descent updates\nnb_of_iterations = 20\n\nw_err_02 = gradient_descent(eta, w, nb_of_iterations)","b119ab64":"# Set the learning rate\neta = 0.5\n\n#Set the initial parameter\nw = 5\n\n# number of gradient descent updates\nnb_of_iterations = 20\n\nw_err_05 = gradient_descent(eta, w, nb_of_iterations)","ca467f26":"# Set the learning rate\neta = 0.7\n\n#Set the initial parameter\nw = 5\n\n# number of gradient descent updates\nnb_of_iterations = 20\n\nw_err_07 = gradient_descent(eta, w, nb_of_iterations)","46ee0f20":"# Print the final w, and cost\nfor i in range(0, len(w_err_07)):\n    print('w({}): {:.4f} \\t cost: {:.4f}'.format(i, w_err_07[i][0], w_err_07[i][1]))","4835c7c9":"w = list(range(-10,10))\nerr = []\nfor i in w:\n    err.append(error(i))","e4f7ed1a":"plt.figure(figsize=(8, 10))\nplt.grid(True)\nplt.subplot(311)\nplt.plot(w, err)\nplt.plot(w_err_02[:,0], w_err_02[:,1],\"o\")\nplt.title([\"x vs m\",\"eta = 0.2\"])\nn = range(1, len(w_err_02[:,0]))\nfor i, txt in enumerate(n):\n    plt.annotate(txt, (w_err_02[:,0][i], w_err_02[:,1][i]))\nplt.subplot(312)\nplt.plot(w, err)\nplt.plot(w_err_05[:,0], w_err_05[:,1],\"o\")\nplt.title([\"x vs m\",\"eta = 0.5\"])\nn = range(1, len(w_err_05[:,0]))\nfor i, txt in enumerate(n):\n    plt.annotate(txt, (w_err_05[:,0][i], w_err_05[:,1][i]))\nplt.subplot(313)\nplt.plot(w, err)\nplt.plot(w_err_07[:,0], w_err_07[:,1],\"o\")\nplt.title([\"x vs m\",\"eta = 0.7\"])\nn = range(1, len(w_err_02[:,0]))\nfor i, txt in enumerate(n):\n    plt.annotate(txt, (w_err_07[:,0][i], w_err_07[:,1][i]))\nplt.show()","caae3d4d":"The objective of this experiment is to plot a Quadratic Equation representing an error function and see how to arrive at the minima in the plot.","2713b0fc":"### $eta$ = 0.7","3087a14f":"### $ eta $ = 0.2","042cae2b":"#### Derivative of the error function is $ 2w $ + $ 2 $","6cc5c9cf":"#### Making the data","ff58d1e9":"#### Let us see how to arrive at local minima","7fc1a5d2":"### $eta $ = 0.5","bc57c9d3":"Let us write a function for gradient descent that can be reused:","94a4809f":"In this experiment we will use a simple quadratic equation for error function as below:\n\n$ w^2 $ + $ 2w $ + $ 2 $"}}