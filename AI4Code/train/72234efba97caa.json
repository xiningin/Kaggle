{"cell_type":{"541d1a12":"code","db13731f":"code","f5ca3f38":"code","2254ac98":"code","71278670":"code","3f9420bf":"code","d478d02c":"code","953faabb":"code","4abb84db":"code","eb3a51cc":"code","0eab7ae9":"code","06b052ca":"code","5a3e4822":"code","adb0ac7f":"code","a1fbdd38":"markdown","36f0db11":"markdown","8b88bfb2":"markdown","993c773f":"markdown","ed73a0b7":"markdown","295461e6":"markdown","469c8827":"markdown","7b06bdbd":"markdown","59a9b21e":"markdown","ae371d5e":"markdown","f5139536":"markdown"},"source":{"541d1a12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt \nimport seaborn as sn\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.model_selection import GridSearchCV\nimport pydotplus as pdot\nfrom IPython.display import Image\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db13731f":"!pip install pydotplus","f5ca3f38":"diabetes = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndiabetes.info()\n\nprint(diabetes.shape)\n\nprint(diabetes.head(5))","2254ac98":"# Create Correlation Matrix\ncorrMatrix = diabetes.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","71278670":"# Split predictors and target variable\n\nX = diabetes.loc[:, diabetes.columns != 'Outcome']\ny = diabetes[\"Outcome\"]\n\n# Split data into Test & Training dataset\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20)","3f9420bf":"##Importing the metrics  from sklearn import metrics \n##Defining the matrix to draw the confusion matrix from actual and  ##predicted class labels \ndef draw_cm( actual, predicted):\n    #Invoking confusion_matrix from metric package. The matrix  \n    #will be oriented as [1,0] i.e. the classes with label 1 will be  \n    #represented by the first row and 0 as second row  \n    cm = metrics.confusion_matrix( actual, predicted, [1,0])  \n    #Confusion will be plotted as heatmap for better visualization  \n    #The labels are configured to better interpretation from the plot  \n    sn.heatmap(cm, annot=True, fmt='.2f',  \n               xticklabels = [\"Diabetes-Yes\", \"Diabetes-No\"],  \n               yticklabels = [\"Diabetes-Yes\", \"Diabetes-No\"])  \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","d478d02c":"## Initializing the Random Forest Classifier with max_depth and n_estimators\nrf_classifier = RandomForestClassifier(max_depth=10, n_estimators=10)\nrf_classifier.fit(train_X, train_y)","953faabb":"pred_y = rf_classifier.predict(test_X)\nprint( metrics.classification_report( test_y, pred_y)) ","4abb84db":"#Print confusion matrix\ncm = draw_cm(test_y, pred_y)","eb3a51cc":"#Create a dataframe to store the features and their corresponding #importances \nfeature_rank = pd.DataFrame( {'feature': train_X.columns,  'importance': rf_classifier.feature_importances_  }) \n##Sorting the features based on their importances with most  ##important feature at top. \nfeature_rank = feature_rank.sort_values('importance', ascending =  False) \nplt.figure(figsize=(8, 6)) \n#plot the values \nsn.barplot( y = 'feature', x = 'importance', data = feature_rank[0:10]);","0eab7ae9":"## Building Decision Tree Classifier using Gini Criteria\nclf_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 4)\nclf_tree.fit(train_X, train_y)","06b052ca":"pred_y = clf_tree.predict(test_X)\nprint( metrics.classification_report( test_y, pred_y)) ","5a3e4822":"## Displaying the tree\nexport_graphviz(clf_tree,\n               out_file = \"chd_tree.odt\",\n               feature_names = train_X.columns,\n               filled = True)\n\n## Create the image file\nchd_tree_graph = pdot.graphviz.graph_from_dot_file('chd_tree.odt')\nchd_tree_graph.write_jpg('chd_tree.png')\n## Render the png file\nImage(filename='chd_tree.png')","adb0ac7f":"#Print confusion matrix\ncm = draw_cm(test_y, pred_y)","a1fbdd38":"## Exploratory Data Analysis: EDA","36f0db11":"### Build \/ Train Random Forest Model","8b88bfb2":"## Split predictors and target variable","993c773f":"### Calculate Precision, Recall, f1-score, Support values","ed73a0b7":"### Print confusion matrix","295461e6":"## Build \/ Train Decision Tree Classifier","469c8827":"<b> The correlation matrix suggests that there is notable positive corelation of value 0.47 between Glucose and Outcome. <b>","7b06bdbd":"### Calculate Precision, Recall, f1-score, Support values","59a9b21e":"### EDA#1: Correlation Matrix","ae371d5e":"### Finding important Features","f5139536":"### Build a Confusion Matrix"}}