{"cell_type":{"a0b6dd49":"code","42cb05de":"code","98be1bb6":"code","57c65258":"code","aea48b47":"code","cb27294e":"code","80431525":"code","58a866ca":"code","e490e568":"code","b052a0f9":"code","104cf639":"code","634ea2d4":"code","6b315215":"code","8af104cf":"code","453b807e":"code","47b3ff66":"code","b4d21e40":"code","b5b2604c":"code","b39e412a":"code","46ae5ef6":"code","94eda40a":"code","a1a2bd88":"markdown","b2a01f75":"markdown","3780b778":"markdown","60ce8894":"markdown","581bb03f":"markdown","16bf31b5":"markdown","309d5088":"markdown","ba13874b":"markdown","c63f3470":"markdown"},"source":{"a0b6dd49":"import numpy as np \nimport pandas as pd \nimport os\nimport shutil\nimport wave\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\nimport sklearn\nfrom scipy.fftpack import fft\nfrom scipy import signal\nfrom scipy.io import wavfile\nSAMPLE_RATE = 44100\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nmatplotlib.style.use('ggplot')\nprint(os.listdir(\"..\/input\"))","42cb05de":"train = pd.read_csv(\"..\/input\/train_curated.csv\")\ntrain_noisy = pd.read_csv(\"..\/input\/train_noisy.csv\")\ntest = pd.read_csv(\"..\/input\/sample_submission.csv\")\n\ndef explode_str(df, col, sep):\n    s = df[col]\n    i = np.arange(len(s)).repeat(s.str.count(sep) + 1)\n    return df.iloc[i].assign(**{col: sep.join(s).split(sep)})\n\n# def explode_list(df, col):\n#     s = df[col]\n#     i = np.arange(len(s)).repeat(s.str.len())\n#     return df.iloc[i].assign(**{col: np.concatenate(s)})\n\ndef load_wav_file(name, path):\n    _, b = wavfile.read(path + name)\n    assert _ == SAMPLE_RATE\n    return b","98be1bb6":"print('Train Curated :')\ntrain.head(10)","57c65258":"# %%time\n# pd.concat([pd.Series(row['fname'], row['labels'].split(','))              \n#                     for _, row in train.iterrows()]).reset_index()\n# CPU times: user 2.27 s, sys: 76 ms, total: 2.35 s\n# Wall time: 2.33 s\nntrain = explode_str(train, 'labels', ',')\nprint('Train Curated after exploding :')\nntrain.head(10)","aea48b47":"print(\"Number of curated training examples=\", train.shape[0], \"  Number of curated training classes=\", len(train.labels.unique()))\nprint(\"Number of curated training examples after exploding=\", ntrain.shape[0], \"  Number of curated training classes after exploding=\", len(ntrain.labels.unique()))","cb27294e":"# pd.DataFrame({'unique_train_labels':ntrain.labels.unique()})\n# ntrain.labels.unique()\nprint(\"Total number of labels in curated training data : \",len(ntrain['labels'].value_counts()))\nprint(\"Labels are : \", ntrain['labels'].unique())\nplt.figure(figsize=(10,6))\naudio_type = ntrain['labels'].value_counts().head(10)\nsns.barplot(audio_type.values, audio_type.index)\nfor i, v in enumerate(audio_type.values):\n    plt.text(0.8,i,v,color='k',fontsize=12)\nplt.xticks(rotation='vertical')\nplt.xlabel('Frequency')\nplt.ylabel('Label Name')\nplt.title(\"First few labels based on their frequencies in curated training data\")\nplt.show()","80431525":"plt.figure(figsize=(10,6))\nnaudio_type = ntrain['labels'].value_counts().tail(10)\nsns.barplot(naudio_type.values, naudio_type.index)\nfor i, v in enumerate(naudio_type.values):\n    plt.text(0.8,i,v,color='k',fontsize=12)\nplt.xticks(rotation='vertical')\nplt.xlabel('Frequency')\nplt.ylabel('Label Name')\nplt.title(\"Last few labels based on their frequencies in curated training data\")\nplt.show()","58a866ca":"INPUT_LIB = '..\/input\/'\nnew_train = ntrain.sort_values('labels').reset_index()\nnew_train['nframes'] = new_train['fname'].apply(lambda f: wave.open('..\/input\/train_curated\/' + f).getnframes())\n\nnew_train['series'] = new_train['fname'].apply(load_wav_file, \n                                                      path=INPUT_LIB + 'train_curated\/')\n\n_, ax = plt.subplots(figsize=(18, 5))\nsns.violinplot(ax=ax, x=\"labels\", y=\"nframes\", data=new_train)\nplt.xticks(rotation=90)\nplt.title('Distribution of audio frames, per label in train curated', fontsize=16)\nplt.show()","e490e568":"print('Histogram of nframes with respect to Train Curated :')\nplt.figure(figsize=(12,8))\nsns.distplot(new_train.nframes.values, bins=50, kde=False)\nplt.xlabel('nframes', fontsize=12)\nplt.title(\"Histogram of #frames\")\nplt.show()","b052a0f9":"print('We can see an outlier in the above plot which belongs to the label - Stream')\nnew_train.loc[new_train['nframes'] > 2000000]","104cf639":"print('Temporary data for series plotting :')\ntemp = new_train.sort_values(by='labels')\ntemp.head()","634ea2d4":"print(\"Accelerating_and_revving_and_vroom : \")\nfig, ax = plt.subplots(10, 4, figsize = (12, 16))\nfor i in range(40):\n    ax[i\/\/4, i%4].plot(temp['series'][i])\n    ax[i\/\/4, i%4].set_title(temp['fname'][i][:-4])\n    ax[i\/\/4, i%4].get_xaxis().set_ticks([])\nfig.savefig(\"Accelerating_and_revving_and_vroom\", dpi=900) ","6b315215":"from wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=50, width=800, height=500).generate(' '.join(new_train.labels))\nplt.figure(figsize=(18,10))\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Labels in Train Curated\", fontsize=25)\nplt.axis(\"off\")\nplt.show()","8af104cf":"print('Train Noisy :')\ntrain_noisy.head(10)","453b807e":"print('Train Noisy after exploding :')\nntrain_noisy = explode_str(train_noisy, 'labels', ',')\nntrain_noisy.head(10)","47b3ff66":"print(\"Number of noisy training examples=\", train_noisy.shape[0], \"  Number of noisy training classes=\", len(train_noisy.labels.unique()))\nprint(\"Number of noisy training examples after exploding=\", ntrain_noisy.shape[0], \"  Number of noisy training classes after exploding=\", len(ntrain_noisy.labels.unique()))","b4d21e40":"# pd.DataFrame({'unique_noisytrain_labels':ntrain_noisy.labels.unique()})\n# ntrain_noisy.labels.unique()\nprint(\"Total number of labels in curated training data : \",len(ntrain_noisy['labels'].value_counts()))\nprint(\"Labels are : \", ntrain_noisy['labels'].unique())\nplt.figure(figsize=(10,6))\naudio_type = ntrain_noisy['labels'].value_counts().head(10)\nsns.barplot(audio_type.values, audio_type.index)\nfor i, v in enumerate(audio_type.values):\n    plt.text(0.8,i,v,color='k',fontsize=12)\nplt.xticks(rotation='vertical')\nplt.xlabel('Frequency')\nplt.ylabel('Label Name')\nplt.title(\"First few labels based on their frequencies in noisy training data\")\nplt.show()","b5b2604c":"plt.figure(figsize=(10,6))\nnaudio_type = ntrain_noisy['labels'].value_counts().tail(10)\nsns.barplot(naudio_type.values, naudio_type.index)\nfor i, v in enumerate(naudio_type.values):\n    plt.text(0.8,i,v,color='k',fontsize=12)\nplt.xticks(rotation='vertical')\nplt.xlabel('Frequency')\nplt.ylabel('Label Name')\nplt.title(\"Last few labels based on their frequencies in noisy training data\")\nplt.show()","b39e412a":"new_noisytrain = ntrain_noisy.sort_values('labels').reset_index()\nnew_noisytrain['nframes'] = new_noisytrain['fname'].apply(lambda f: wave.open('..\/input\/train_noisy\/' + f).getnframes())\n# new_noisytrain['series'] = new_noisytrain['fname'].apply(load_wav_file, \n#                                                       path=INPUT_LIB + 'train_noisy\/')\n# new_noisytrain['nframes'] = new_noisytrain['series'].apply(len)\n_, ax = plt.subplots(figsize=(18, 5))\nsns.violinplot(ax=ax, x=\"labels\", y=\"nframes\", data=new_noisytrain)\nplt.xticks(rotation=90)\nplt.title('Distribution of audio frames, per label in train noisy', fontsize=16)\nplt.show()","46ae5ef6":"plt.figure(figsize=(12,8))\nsns.distplot(new_noisytrain.nframes.values, bins=50, kde=False)\nplt.xlabel('nframes', fontsize=12)\nplt.title(\"Histogram of #frames\")\nplt.show()","94eda40a":"from wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=50, width=800, height=500).generate(' '.join(new_noisytrain.labels))\nplt.figure(figsize=(18,10))\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Labels in Train Curated\", fontsize=30)\nplt.axis(\"off\")\nplt.show()","a1a2bd88":"### **train_curated.csv unique labels**","b2a01f75":"## **Inspired from these Kernels**\n### **https:\/\/www.kaggle.com\/fizzbuzz\/beginner-s-guide-to-audio-data?scriptVersionId=3061527**\n### **https:\/\/www.kaggle.com\/codename007\/a-very-extensive-freesound-exploratory-analysis**\n### **https:\/\/www.kaggle.com\/dude431\/beginner-s-visualization-and-removing-uniformative**","3780b778":"## **References**\n### **https:\/\/stackoverflow.com\/questions\/12680754\/split-explode-pandas-dataframe-string-entry-to-separate-rows**","60ce8894":"### **train_noisy.csv unique labels**","581bb03f":"# **Freesound Audio Tagging Challenge - 2019**","16bf31b5":"## **# Train Noisy EDA**","309d5088":"### <center>SPECTROGRAM PLOTS AND MODEL BUILDING - IN PROGRESS<\/center>\n## <center>STAY TUNED!<\/center>\n### <center>THANK YOU<\/center>\n# <center>\ud83d\ude0a\ud83d\ude0e\ud83d\ude04<\/center>","ba13874b":"#### **In Noisy data most of the wav files have the same frame count. I am doubting that if we combine these data while training, the nframes won't be of much use to us.**","c63f3470":"## **# Train Curated EDA**"}}