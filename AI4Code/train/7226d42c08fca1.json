{"cell_type":{"2047884a":"code","b0395e0c":"code","d27ac811":"code","32e08f33":"code","22f83857":"code","7353a4c2":"code","179e365a":"code","ef601464":"code","1c887668":"code","9d9ab3e2":"code","71052b86":"code","11ff1e02":"code","2767feee":"code","f8708a63":"code","1380fd43":"code","dcc7e8ce":"code","fa322fa3":"code","5498b97b":"code","5f688057":"code","07cad832":"code","f1b694fa":"code","a9100d98":"code","584cdd4b":"code","db2310e8":"code","f5c3a7a2":"code","9440e90e":"code","5c69cb06":"code","cf182f10":"code","8de177a4":"code","63401c37":"code","992973fe":"code","8af067c1":"code","08efb44e":"code","cd424cdf":"code","5ef38ebd":"code","34968dac":"code","4deb807b":"code","3cd997b6":"code","df254052":"code","79b44986":"code","ba05a294":"code","0298191f":"code","b53e4995":"code","7556935b":"code","102fdfda":"code","3ed3ab42":"code","0e78dc3d":"code","3fcd1281":"code","10a788f0":"code","22ed0b5e":"code","65e7a389":"code","ab8cde73":"code","e4fff875":"code","70e20911":"code","7193f5c2":"code","497e1b57":"code","c7846633":"code","97ba1d18":"code","1d75b7b0":"code","7d122e86":"code","65e0b443":"code","9ebb079f":"code","ab614ee2":"code","4c55abad":"code","5a2bb829":"code","ebe8abe2":"code","68c1e242":"code","b5136dbb":"code","ef1e6d7f":"code","2161450a":"code","f8fc863a":"code","10632e5e":"code","d20b69df":"code","539145c4":"code","75dada9f":"code","1c6067b6":"code","5b15746d":"code","ccd107bf":"code","32d14d79":"code","bfbf4ee9":"code","af5de897":"code","6fb1b68d":"code","496791d9":"code","231bd30a":"code","2ea3298f":"code","960f72e0":"code","2917f78b":"code","5626e9c1":"code","77cf31aa":"code","1205610b":"code","37a2f0a4":"code","0e206155":"code","75a72860":"code","e909fe77":"code","2ca998c0":"code","8b581ef0":"code","99f5e9d4":"code","56163bd7":"code","818458e0":"code","643d6bea":"code","9426a382":"code","82e82195":"code","f425c075":"code","1d201cb9":"code","a99fb1c1":"code","3afd3a96":"code","2d3f14bf":"code","358f4086":"code","4641f7d8":"code","2267b976":"code","86c17e94":"code","33bc1d23":"code","ccbcefd4":"code","dd116d23":"code","f0693ff9":"code","74686041":"code","846127c9":"code","df5ef49f":"code","fb12d2db":"code","bbe3dc82":"code","89264c06":"code","2acc7d3e":"code","d2c647b6":"code","2ec86892":"code","5e6d12df":"code","9189fd92":"code","14e33481":"code","cb95203e":"code","b46dbabf":"code","16df4d5a":"code","27031769":"code","ae2a0886":"code","d618be5a":"code","b3abad25":"code","7117b976":"code","cffe2592":"code","fc1e4dc0":"markdown","f882e7ff":"markdown","ef9f0c7b":"markdown","2479e998":"markdown","aa44921d":"markdown","5e7da4cb":"markdown","69a32b65":"markdown","332c13f7":"markdown","9480998e":"markdown","52c1f8b7":"markdown","89e96a78":"markdown","7c1c822c":"markdown","dfb5081e":"markdown","00ee4d59":"markdown","d584017e":"markdown","7e2714e6":"markdown","771285e2":"markdown","9acd6bf5":"markdown","19efd913":"markdown","cc1a00b4":"markdown","90f506e8":"markdown","e6ec0179":"markdown","f611cc91":"markdown","a8fa6190":"markdown","3e0f358d":"markdown","e3647b22":"markdown","ffd591e3":"markdown","ca8b75ff":"markdown","76aa4352":"markdown","6c849465":"markdown","6b5d49ae":"markdown","d6bd5e4a":"markdown","d39a3e2a":"markdown","2041fa54":"markdown","e80d3837":"markdown","b1179315":"markdown","01e9abfa":"markdown","0193f97a":"markdown","fef2b836":"markdown","f5c5e656":"markdown","7d66e824":"markdown","db0cc956":"markdown","78d8fd31":"markdown","1029219e":"markdown","447e4cc8":"markdown","1c87ee06":"markdown","3bcb6cee":"markdown","7a15bca0":"markdown","f429d6a6":"markdown","bffdb532":"markdown"},"source":{"2047884a":"import warnings\nwarnings.filterwarnings('ignore')","b0395e0c":"import pandas as pd\nimport numpy as np","d27ac811":"from sklearn import model_selection, preprocessing, ensemble, svm, neighbors, linear_model \nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb \nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nfrom sklearn.metrics import mean_squared_error","32e08f33":"import matplotlib.pyplot as plt\nimport seaborn as sns","22f83857":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')","7353a4c2":"train.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","179e365a":"for i, dataset in enumerate([train, test]):\n    if dataset.isna().sum() is not 0:\n        print (f'No null values in dataset {i+1}')\n    else:\n        print(f' The dataset {i+1} had {dataset.isna().sum().sum()} of null values')","ef601464":"{feature: len(train[feature].unique()) for feature in train.select_dtypes('object')}","1c887668":"{feature: len(test[feature].unique()) for feature in test.select_dtypes('object')}","9d9ab3e2":"numeric_features_1 = [feature for feature in train.select_dtypes(['int','float']) if feature not in ('id', 'target')]","71052b86":"categorical_features_1 = [feature for feature in train.columns if train[feature].dtype == 'O']","11ff1e02":"sns.set_style('darkgrid')","2767feee":"plt.figure(figsize=(20, 22))\nfor i , feature in enumerate(numeric_features_1):\n    plt.subplot(4, 4, i*1 + 1)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\nplt.show()","f8708a63":"plt.figure (figsize = (22,24))\nfor i, feature in enumerate(numeric_features_1):\n    plt.subplot(15,3, i*1 + 1)\n    plt.ylabel(feature)\n    g = sns.boxplot(data=train, x=train[feature], color='blue')\n    g.set(xticklabels=[])  \n    g.set(xlabel=None)\n    g.tick_params(bottom=False)  ","1380fd43":"plt.figure (figsize = (22,24))\nfor i, feature in enumerate(numeric_features_1):\n    plt.subplot(15,3, i*1 + 1)\n    plt.ylabel(feature)\n    g = sns.boxplot(data=test, x=test[feature], color='olive')\n    g.set(xticklabels=[])  \n    g.set(xlabel=None)\n    g.tick_params(bottom=False)","dcc7e8ce":"plt.figure(figsize=(20, 22))\n\nfor i, feature in enumerate(categorical_features_1):\n    plt.subplot(3,4, i*1 + 1)\n    plt.ylabel(feature)\n    g1 = sns.histplot(data=train, x= train[feature], color='blue',label='train_'+feature)\n    g2 = sns.histplot(data=test, x = test[feature], color=\"olive\", label='test_'+feature)\n    g1.set(xlabel=None)\n    g2.set(xlabel=None)","fa322fa3":"def preprocessing_dfs(df,num_feat, cat_feat, test_df):\n    \n    df = df.copy()\n    test_df = test_df.copy()\n\n    y = df['target']\n    X = df.drop(['target'], axis = 1)\n       \n    # Split the data\n    xtrain, xtest, ytrain, ytest = model_selection.train_test_split(X,y, train_size = .75, shuffle = True, random_state = 0 )\n    \n    # Apply scaling\n    scaler = preprocessing.MinMaxScaler().fit(xtrain[num_feat])\n    xtrain[num_feat] = scaler.transform(xtrain[num_feat])\n    xtest[num_feat] = scaler.transform(xtest[num_feat])\n    test_df[num_feat] = scaler.transform(test_df[num_feat])\n    \n    # Apply one-hot encoder to the rest of categorical columns\n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore').fit(xtrain[cat_feat])\n    encoded_cols = list(ohe.get_feature_names(cat_feat))\n    \n    ohe_train = pd.DataFrame(ohe.transform(xtrain[cat_feat]), columns = encoded_cols ,index = xtrain.index)\n    ohe_test = pd.DataFrame(ohe.transform(xtest[cat_feat]), columns = encoded_cols, index = xtest.index )\n    ohe_test_df = pd.DataFrame(ohe.transform(test_df[cat_feat]), columns = encoded_cols, index = test_df.index )\n\n    # Remove categorical columns (will replace with one-hot encoding)\n    xtrain.drop(cat_feat, axis=1, inplace = True)\n    xtest.drop(cat_feat, axis=1, inplace = True)\n    test_df.drop(cat_feat, axis=1, inplace = True)\n\n    # Add one-hot encoded columns to numerical features\n    xtrain = pd.concat([xtrain, ohe_train], axis=1)\n    xtest = pd.concat([xtest, ohe_test], axis=1)\n    test_df = pd.concat([test_df, ohe_test_df], axis=1)\n    \n    X = pd.concat([xtrain, xtest], axis=0)\n    y = pd.concat([ytrain, ytest], axis=0)\n    \n    return xtrain, xtest, ytrain, ytest, test_df, X, y","5498b97b":"xtrain, xtest, ytrain, ytest, test_df, X, y = preprocessing_dfs(train, numeric_features_1, categorical_features_1, test)","5f688057":"ym = train['target'].copy()\nXm = train.drop(['target','id'], axis = 1).copy()\n       \n# Label encoding for categoricals\nfor colname in Xm.select_dtypes(\"object\"):\n    Xm[colname], _ = Xm[colname].factorize()\n    ","07cad832":"discrete_features = Xm.dtypes == int","f1b694fa":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","a9100d98":"mi_scores = make_mi_scores(Xm.head(200000), ym.head(200000), discrete_features)","584cdd4b":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","db2310e8":"model_fp_1 = XGBRegressor(random_state = 0, objective = 'reg:squarederror',tree_method='gpu_hist', gpu_id=0,predictor=\"gpu_predictor\")\nmodel_fp_1.fit(xtrain, ytrain)","f5c3a7a2":"importance_df_1 = pd.DataFrame({\n    'feature': xtrain.columns,\n    'importance': model_fp_1.feature_importances_\n}).sort_values('importance', ascending=False)","9440e90e":"plt.figure(figsize=(10,8))\nplt.title('Feature Importance')\nsns.barplot(data=importance_df_1.head(30), x='importance', y='feature');","5c69cb06":"!pip install eli5 --upgrade --quiet","cf182f10":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm1 = PermutationImportance(model_fp_1,scoring = 'neg_mean_squared_error', random_state=0).fit(xtest, ytest)\neli5.show_weights(perm1, feature_names = xtest.columns.tolist())","8de177a4":"from pdpbox import pdp, get_dataset, info_plots","63401c37":"pdp_cont = pdp.pdp_isolate(model=model_fp_1, dataset=xtest, model_features=xtest.columns.tolist(), feature='cont12')\n\n# plot it\npdp.pdp_plot(pdp_cont, 'cont12')\nplt.show()","992973fe":"pdp_cont = pdp.pdp_isolate(model=model_fp_1, dataset=xtest, model_features=xtest.columns.tolist(), feature='cat4_B')\n\n# plot it\npdp.pdp_plot(pdp_cont, 'cat4_B')\nplt.show()","8af067c1":"#for feature in xtest.columns.tolist():\n#    pdp_ = pdp.pdp_isolate(model=model_fp_1, dataset=xtest, model_features=xtest.columns.tolist(), feature=feature)#\n\n    # plot it\n#    pdp.pdp_plot(pdp_, feature)\n#    plt.show()","08efb44e":"df_features_train = pd.DataFrame()\ndf_features_test = pd.DataFrame()","cd424cdf":"df_features_train['freq_cat6'] = X.groupby(by= 'cat6_I' )['cat6_I'].transform('count') \/ X.cat6_I.count()\ndf_features_train['freq_cat7'] = X.groupby(by= 'cat7_G' )['cat7_G'].transform('count') \/ X.cat7_G.count()\ndf_features_train['freq_cat8'] = X.groupby(by= 'cat8_C' )['cat8_C'].transform('count') \/ X.cat8_C.count()\ndf_features_train['freq_cat9'] = X.groupby(by= 'cat9_D' )['cat9_D'].transform('count') \/ X.cat9_D.count()","5ef38ebd":"df_features_test['freq_cat6'] = test_df.groupby(by= 'cat6_I' )['cat6_I'].transform('count') \/ test_df.cat6_I.count()\ndf_features_test['freq_cat7'] = test_df.groupby(by= 'cat7_G' )['cat7_G'].transform('count') \/ test_df.cat7_G.count()\ndf_features_test['freq_cat8'] = test_df.groupby(by= 'cat8_C' )['cat8_C'].transform('count') \/ test_df.cat8_C.count()\ndf_features_test['freq_cat9'] = test_df.groupby(by= 'cat9_D' )['cat9_D'].transform('count') \/ test_df.cat9_D.count()","34968dac":"df_features_train['cat1A_cat3C'] = X['cat1_A'] + X['cat3_C']\ndf_features_test['cat1A_cat3C'] = test_df['cat1_A'] + test_df['cat3_C']","4deb807b":"df_features_train['cat1A_cat6I'] = X['cat1_A'] + X['cat6_I']\ndf_features_test['cat1A_cat6I'] = test_df['cat1_A'] + test_df['cat6_I']","3cd997b6":"df_features_train['cat1A_cat8C'] = X['cat1_A'] + X['cat8_C']\ndf_features_test['cat1A_cat8C'] = test_df['cat1_A'] + test_df['cat8_C']","df254052":"df_features_train['cat6I_cat8C'] = X['cat6_I'] + X['cat8_C']\ndf_features_test['cat6I_cat8C'] = test_df['cat6_I'] + test_df['cat8_C']","79b44986":"df_features_train['cat3C_cat8C'] = X['cat3_C'] + X['cat8_C']\ndf_features_test['cat3C_cat8C'] = test_df['cat3_C'] + test_df['cat8_C']","ba05a294":"df_features_train['cat5A_cat5D'] = X['cat5_A'] + X['cat5_D']\ndf_features_test['cat5A_cat5D'] = test_df['cat5_A'] + test_df['cat5_D']","0298191f":"df_features_train['cat4D_cat9E'] = X['cat4_D'] + X['cat9_E']\ndf_features_test['cat4D_cat9E'] = test_df['cat4_D'] + test_df['cat9_E']","b53e4995":"df_features_train['cat6B_cat7G'] = X['cat6_B'] + X['cat7_G']\ndf_features_test['cat6B_cat7G'] = test_df['cat6_B'] + test_df['cat7_G']","7556935b":"df_features_train['cat1Acat8C_cat5Acat5D'] = df_features_train['cat1A_cat8C'] + df_features_train['cat5A_cat5D']\ndf_features_test['cat1Acat8C_cat5Acat5D'] = df_features_test['cat1A_cat8C'] + df_features_test['cat5A_cat5D']","102fdfda":"df_features_train['cat1Acat6I_cat4D_cat9E'] = df_features_train['cat1A_cat6I'] + df_features_train['cat4D_cat9E']\ndf_features_test['cat1Acat6I_cat4D_cat9E'] = df_features_test['cat1A_cat6I'] + df_features_test['cat4D_cat9E']","3ed3ab42":"df_features_train['cat3Ccat8C_'] = df_features_train['cat3C_cat8C'] + df_features_train['cat6B_cat7G']\ndf_features_test['cat3Ccat8C_'] = df_features_test['cat3C_cat8C'] + df_features_test['cat6B_cat7G']","0e78dc3d":"group_1 = X.groupby(X['cat8_C'])['cont0','cont1','cont4','cont6','cont8'].agg('mean').rename(columns={  'cont0':'mean_cont0_8C', \n                                                                                                              'cont1':'mean_cont1_8C',\n                                                                                                              'cont4':'mean_cont4_8C',\n                                                                                                              'cont6':'mean_cont6_8C',\n                                                                                                              'cont8':'mean_cont8_8C'})","3fcd1281":"group_2 = X.groupby(X['cat1_A'])['cont0','cont1','cont4','cont6','cont8'].agg('mean').rename(columns={  'cont0':'mean_cont0_1A', \n                                                                                                              'cont1':'mean_cont1_1A',\n                                                                                                              'cont4':'mean_cont4_1A',\n                                                                                                              'cont6':'mean_cont6_1A',\n                                                                                                              'cont8':'mean_cont8_1A'})","10a788f0":"group_1_test = test_df.groupby(test_df['cat8_C'])['cont0','cont1','cont4','cont6','cont8'].agg('mean').rename(columns={  \n                                                                                                              'cont0':'mean_cont0_8C', \n                                                                                                              'cont1':'mean_cont1_8C',\n                                                                                                              'cont4':'mean_cont4_8C',\n                                                                                                              'cont6':'mean_cont6_8C',\n                                                                                                              'cont8':'mean_cont8_8C'})","22ed0b5e":"group_2_test = test_df.groupby(test_df['cat1_A'])['cont0','cont1','cont4','cont6','cont8'].agg('mean').rename(columns={  \n                                                                                                                'cont0':'mean_cont0_1A', \n                                                                                                                'cont1':'mean_cont1_1A',\n                                                                                                                'cont4':'mean_cont4_1A',\n                                                                                                                'cont6':'mean_cont6_1A',\n                                                                                                                'cont8':'mean_cont8_1A'})","65e7a389":"for feature in numeric_features_1:\n    if X[feature].skew() > 0.25:\n        X['log_'+ feature] =  X[feature].apply(np.log1p)","ab8cde73":"for feature in numeric_features_1:\n    if test_df[feature].skew() > 0.25:\n        test_df['log_'+ feature] =  X[feature].apply(np.log1p)","e4fff875":"train_df = pd.concat([X, df_features_train], axis = 1)","70e20911":"train_df = train_df.merge(group_1, on = 'cat8_C', how = 'left')","7193f5c2":"train_df = train_df.merge(group_2, on = 'cat1_A', how = 'left')","497e1b57":"train_features = pd.concat([train_df, y], axis = 1)","c7846633":"train_features.to_csv('train_features.csv', index=False)","97ba1d18":"test_df = pd.concat([test_df, df_features_test], axis = 1)","1d75b7b0":"test_df = test_df.merge(group_1_test, on = 'cat8_C', how = 'left')","7d122e86":"test_df = test_df.merge(group_2_test, on = 'cat1_A', how = 'left')","65e0b443":"test_features = test_df.copy()","9ebb079f":"test_features.to_csv('test_features.csv', index=False)","ab614ee2":"numeric_features_2 = [feature for feature in train_df.select_dtypes(['int','float']) if feature not in ('id', 'kfold','target')]\ncategorical_features_2 = [feature for feature in train_df.columns if train_df[feature].dtype == 'O']","4c55abad":"def split (X,y):\n    X = X.copy()\n    y = y.copy()\n    \n    X = X[numeric_features_2+categorical_features_2]\n\n    # Split the data\n    xtrain, xtest, ytrain, ytest = model_selection.train_test_split(X,y, train_size = .75, shuffle = True, random_state = 0 )\n       \n    return xtrain, xtest, ytrain, ytest","5a2bb829":"xtrain, xtest, ytrain, ytest = split (train_df, y)","ebe8abe2":"model_f_1 = XGBRegressor(random_state=0, objective = 'reg:squarederror',\n                         tree_method='gpu_hist', gpu_id=0,predictor=\"gpu_predictor\"\n                        )\n\nmodel_f_1.fit(xtrain, ytrain)","68c1e242":"model_f_2 = lgb.LGBMRegressor(random_state=0)\nmodel_f_2.fit(xtrain, ytrain)","b5136dbb":"importance_df_3 = pd.DataFrame({\n    'feature': xtrain.columns,\n    'importance': model_f_1.feature_importances_\n}).sort_values('importance', ascending=False)","ef1e6d7f":"plt.figure(figsize=(10,6))\nplt.title('Feature Importance XGBoost')\nsns.barplot(data=importance_df_3.head(20), x='importance', y='feature');","2161450a":"importance_df_4 = pd.DataFrame({\n    'feature': xtrain.columns,\n    'importance': model_f_2.feature_importances_\n}).sort_values('importance', ascending=False)","f8fc863a":"plt.figure(figsize=(10,6))\nplt.title('Feature Importance LGBM')\nsns.barplot(data=importance_df_4.head(20), x='importance', y='feature');","10632e5e":"f_eng_xgb = list(importance_df_3.head(20).feature.values)","d20b69df":"f_eng_lgbm = list(importance_df_4.head(20).feature.values)","539145c4":"df_eng_xgb = pd.concat([xtrain[f_eng_xgb], ytrain], axis = 1)\ndf_eng_lgbm = pd.concat([xtrain[f_eng_lgbm], ytrain], axis = 1)","75dada9f":"combined_features = list(set(f_eng_xgb + f_eng_lgbm))","1c6067b6":"plt.figure(figsize=(20,20))\nsns.heatmap(df_eng_xgb.corr(), annot=True);","5b15746d":"plt.figure(figsize=(20,20))\nsns.heatmap(df_eng_lgbm.corr(), annot=True);","ccd107bf":"models = {\n          'XGBoost' : XGBRegressor(random_state = 0, objective = 'reg:squarederror',tree_method='gpu_hist', gpu_id=0,predictor=\"gpu_predictor\"),\n          '   LGBM' : lgb.LGBMRegressor(random_state = 0, device = \"gpu\",gpu_platform_id= 0,gpu_device_id= 0),\n          'Catboost': CatBoostRegressor(random_state = 0, task_type=\"GPU\", verbose=False),          \n          '   HGBR' : HistGradientBoostingRegressor(random_state = 0),\n          '      LR': linear_model.LinearRegression(),\n          '      BR': linear_model.BayesianRidge(), \n          #'RandomF': ensemble.RandomForestRegressor(random_state = 0), takes soo much to train\n          #'ExtraTr': ensemble.ExtraTreesRegressor(bootstrap = True, random_state = 0) takes soo much to train\n\n}","32d14d79":"from sklearn import model_selection","bfbf4ee9":"def model_evaluation (model, X, y):\n\n    kf = model_selection.KFold(n_splits = 3, shuffle = False, random_state = 0)\n\n    rmses = []\n    preds = []\n\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n        X_train = X.iloc[train_idx, :]\n        y_train = y.iloc[train_idx]\n        X_test = X.iloc[test_idx, :]\n        y_test = y.iloc[test_idx]\n        \n        # Model fit\n        model.fit(X_train, y_train)\n        \n        # Model predict\n        y_preds = model.predict(X_test)\n        preds.append(y_preds)\n\n        # RMSE\n        rmse = np.sqrt(np.mean((y_test - y_preds)**2))\n        rmses.append(rmse)\n        \n        print(f' Fold = {fold} RMSE = {rmse: .4f}')\n\n    return np.mean(rmses)","af5de897":"for name, model in models.items():\n    print(name + ' RMSE {:.4f}'.format(model_evaluation(model, train_df[f_eng_xgb],y)) )","6fb1b68d":"for name, model in models.items():\n    print(name + ' RMSE {:.4f}'.format(model_evaluation(model, train_df[f_eng_lgbm],y)) )","496791d9":"for name, model in models.items():\n    print(name + ' RMSE {:.4f}'.format(model_evaluation(model, train_df[combined_features],y)) )","231bd30a":"from sklearn import metrics","2ea3298f":"def test_params_xgb(**params):\n    model = XGBRegressor(random_state=42, n_jobs=-1, objective = 'reg:squarederror',\n                   tree_method='gpu_hist', gpu_id=0,predictor=\"gpu_predictor\",**params).fit(xtrain, ytrain)\n    train_rmse = metrics.mean_squared_error (model.predict(xtrain), ytrain, squared=False)\n    test_rmse = metrics.mean_squared_error (model.predict(xtest), ytest, squared=False)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, test_rmse))\n    return train_rmse, test_rmse","960f72e0":"def test_param_and_plot_xgb(param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, test_rmse = test_params_xgb(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(test_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","2917f78b":"test_param_and_plot_xgb('max_depth', [2, 5, 15])","5626e9c1":"test_param_and_plot_xgb('n_estimators', [5, 10, 50,100, 250])","77cf31aa":"test_param_and_plot_xgb('learning_rate', [0.01, 0.1, 0.4, 0.6])","1205610b":"test_param_and_plot_xgb('subsample', [0.1, 0.4, 0.6, 0.9])","37a2f0a4":"test_param_and_plot_xgb('reg_lambda', [1, 5, 15, 30, 50, 100, 200])","0e206155":"test_param_and_plot_xgb('reg_alpha', [0, 5, 15, 30, 50, 100, 200, 250, 300])","75a72860":"test_param_and_plot_xgb('min_child_weight', [1, 5, 10, 15])","e909fe77":"test_param_and_plot_xgb('colsample_bytree', [0.2, 0.5, 0.7, 1])","2ca998c0":"test_param_and_plot_xgb('colsample_bylevel', [0.2, 0.5, 0.7, 1])","8b581ef0":" #test_params(max_depth= 5, n_estimators = 50,learning_rate = 0.1,subsample = 0.9,\n      #       reg_lambda = 5, reg_alpha = 100, min_child_weight=1, colsample_bytree = 0.2,\n  #           colsample_bylevel = 0.2)","99f5e9d4":"def test_params_lgbm(**params):\n    model = lgb.LGBMRegressor (random_state=42, n_jobs=-1, **params).fit(xtrain, ytrain)\n    train_rmse = metrics.mean_squared_error (model.predict(xtrain), ytrain, squared=False)\n    test_rmse = metrics.mean_squared_error (model.predict(xtest), ytest, squared=False)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, test_rmse))\n    return train_rmse, test_rmse","56163bd7":"def test_param_and_plot_lgbm( param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, test_rmse = test_params_lgbm(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(test_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","818458e0":"test_param_and_plot_lgbm( 'boosting_type', ['gbdt', 'dart','goss'])","643d6bea":"test_param_and_plot_lgbm('max_depth', [2, 5, 15, 25])","9426a382":"test_param_and_plot_lgbm( 'num_leaves', [10, 30, 50, 100])","82e82195":"test_param_and_plot_lgbm( 'subsample', [0.1, 0.3, 0.6, 0.9])","f425c075":"test_param_and_plot_lgbm('learning_rate', [0.01, 0.1, 0.4, 0.6])","1d201cb9":"test_param_and_plot_lgbm( 'max_bin', [255, 500, 1000, 5000])","a99fb1c1":"#test_params(max_depth= 15, num_leaves = 30,learning_rate = 0.1,subsample = defautl,\n           #  max_bin = 1000, boosting_type=default)","3afd3a96":"def test_params_cb(**params):\n    model = CatBoostRegressor (random_state=42,verbose=False, **params).fit(xtrain, ytrain)\n    train_rmse = metrics.mean_squared_error (model.predict(xtrain), ytrain, squared=False)\n    test_rmse = metrics.mean_squared_error (model.predict(xtest), ytest, squared=False)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, test_rmse))\n    return train_rmse, test_rmse","2d3f14bf":"def test_param_and_plot_cb( param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, test_rmse = test_params_cb(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(test_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","358f4086":"test_param_and_plot_cb('n_estimators', [10, 50,100, 250, 500])","4641f7d8":"test_param_and_plot_cb('learning_rate', [0.01, 0.1, 0.4, 0.6])","2267b976":"test_param_and_plot_cb('max_depth', [2,5, 10])","86c17e94":"# test_params(max_depth= 5, n_estimators = 250,learning_rate = 0.1 )","33bc1d23":"def test_params_hgb(**params):\n    model =  HistGradientBoostingRegressor(random_state=42, **params).fit(xtrain, ytrain)\n    train_rmse = metrics.mean_squared_error (model.predict(xtrain), ytrain, squared=False)\n    test_rmse = metrics.mean_squared_error (model.predict(xtest), ytest, squared=False)\n    print('Train RMSE: {}, Validation RMSE: {}'.format(train_rmse, test_rmse))\n    return train_rmse, test_rmse","ccbcefd4":"def test_param_and_plot_hgb( param_name, param_values):\n    train_errors, val_errors = [], [] \n    for value in param_values:\n        params = {param_name: value}\n        train_rmse, test_rmse = test_params_hgb(**params)\n        train_errors.append(train_rmse)\n        val_errors.append(test_rmse)\n    plt.figure(figsize=(10,6))\n    plt.title('Overfitting curve: ' + param_name)\n    plt.plot(param_values, train_errors, 'b-o')\n    plt.plot(param_values, val_errors, 'r-o')\n    plt.xlabel(param_name)\n    plt.ylabel('RMSE')\n    plt.legend(['Training', 'Validation'])","dd116d23":"test_param_and_plot_hgb('max_depth', [2, 5, 15, 25])","f0693ff9":"test_param_and_plot_hgb('max_iter', [50,100, 250, 500])","74686041":"test_param_and_plot_hgb('learning_rate', [0.01, 0.1, 0.4, 0.6])","846127c9":"# test_params(max_iter= 250, max_depth = 15,learning_rate = 0.1)","df5ef49f":"#X_lgbm = train_df[f_eng_lgbm]\n#test_X_lgbm = test_df[f_eng_lgbm]","fb12d2db":"#X_xgb = train_df[f_eng_xgb]\n#test_X_xgb = test_df[f_eng_xgb]","bbe3dc82":"X_c = train_df[combined_features]\ntest_X_c = test_df[combined_features]","89264c06":"XGB = XGBRegressor(max_depth= 5, n_estimators = 50,learning_rate = 0.1,subsample = 0.9,\n                   reg_lambda = 5, reg_alpha = 100, min_child_weight=1, colsample_bytree = 0.2,\n                   colsample_bylevel = 0.2)","2acc7d3e":"XGB.fit(X_c,y)","d2c647b6":"preds_xgb = XGB.predict(test_X_c)","2ec86892":"LGMB = lgb.LGBMRegressor(max_depth= 15, num_leaves = 30,learning_rate = 0.1,\n                         max_bin = 1000)","5e6d12df":"LGMB.fit(X_c, y)","9189fd92":"preds_lgbm = LGMB.predict(test_X_c)","14e33481":"CBR = CatBoostRegressor(max_depth= 5, n_estimators = 500,learning_rate = 0.1, random_state = 0, verbose=False)","cb95203e":"CBR.fit(X_c, y)","b46dbabf":"preds_cbr = CBR.predict(test_X_c)","16df4d5a":"HGB = HistGradientBoostingRegressor(max_iter= 250, max_depth = 15,learning_rate = 0.1, random_state = 0)","27031769":"HGB.fit(X_c, y)","ae2a0886":"preds_hgb = HGB.predict(test_X_c)","d618be5a":"preds_final = (preds_xgb + preds_lgbm + (2 * preds_cbr) + preds_hgb) \/ 5","b3abad25":"sample_submission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","7117b976":"sample_submission.target = preds_final","cffe2592":"sample_submission.to_csv('submission.csv', index = False)","fc1e4dc0":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#CCCC00;font-size:300%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n30 days of ML competition\n\n<\/div>","f882e7ff":"## Catboost","ef9f0c7b":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nOn the opposite side, we hace features like cat4_B, his presence decrease the prediction.    <br><br> ","2479e998":"<a id=\"subsection-four-one\"><\/a>\n## Mutual information ","aa44921d":"* [Introduction](#section-one)\n* [Data checking](#section-two)\n* [EDA](#section-three)\n* [Feature selection](#section-four)\n    - [Mutual information](#subsection-four-one)\n    - [Tree model feature importance](#subsection-four-two)\n    - [Permutation feature importance](#subsection-four-three)\n    - [Partial dependence plots](#subsection-four-four)\n* [Feature engineering](#section-five)\n    - [Popular categories joins](#subsection-five-one)\n    - [Unpopular categories joins](#subsection-five-two)\n    - [Popular and Unpopular categories joins](#subsection-five-three)\n    - [Group transforms](#subsection-five-four)\n    - [Log transform](#subsection-five-five)\n* [Final feature selection](#section-six)\n* [Modeling](#section-seven)\n* [Hyperparameters tunning](#section-eight)\n* [Blending](#section-nine)","5e7da4cb":"![header.png](attachment:39c09ae9-72af-47b2-baa1-8c706c646ef3.png)","69a32b65":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nAnother thing you can do, it's plot the partial dependence of the feature permutation, https:\/\/www.kaggle.com\/dansbecker\/partial-plots<br><br>  ","332c13f7":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nWith the code down bellow I'm creating two list of features on with the top 20 XGB features and the other with the top 20 LGBM features.    <br><br> ","9480998e":"<a id=\"section-eight\"><\/a>\n# Hyperparameters tunning","52c1f8b7":"<a id=\"subsection-four-four\"><\/a>\n## Partial dependence plots","89e96a78":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nIf you want you can plot all the features with this loop from above, I don't do it because it's a very large output.    <br><br> ","7c1c822c":"## XGB","dfb5081e":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nDifferent set of features <br><br>","00ee4d59":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n<b> Final submission Blend: <\/b> Because catboost it's the best model for this set of features, I've assigned him a weight of 2.\n<br><br>\n","d584017e":"### XGB","7e2714e6":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   The two dataset had the same categories, and also the same categories lengh.\n    <br><br>","771285e2":"<a id=\"subsection-four-three\"><\/a>\n# Permutation feature importance\n\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThis is another technique wich consist in randomly shuffle a single column of the validation data, leaving the target and all other columns in place, and how this shuffle affect the accuracy of predictions in that now-shuffled data. There is a mini Kaggle course where you can learn more about it, and also a very good explanation of that course from a youtuber. I share the links for that down below.\n  https:\/\/www.kaggle.com\/learn\/machine-learning-explainability\n  https:\/\/www.youtube.com\/watch?v=CMA2M9lwf0w\n<br><br>","9acd6bf5":"<a id=\"subsection-four-two\"><\/a>\n## Tree Model Feature importance","19efd913":"### Test","cc1a00b4":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nSeems to be the cont's are had more impact at the hour of predict the target variable, only cat's 1, 3 and 8 figure in the top 20 features.    <br><br> ","90f506e8":"<a id=\"section-three\"><\/a>\n# Exploratory Data Analysis (EDA) ","e6ec0179":"<a id=\"subsection-five-two\"><\/a>\n## - Popular categories join","f611cc91":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThis approach describes relationships in terms of uncertainty, it's an univariate analysis, and can help you to understand the relative potential of a feature as a predictor of the target, considered by itself. If you want to learn more about it yo can check the Kaggle mini course of feature engineering. https:\/\/www.kaggle.com\/learn\/feature-engineering\n    <br><br>","a8fa6190":"### HistGradientBoostingRegressor","3e0f358d":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nFeature Selection is the process of choosing features that are most useful for your prediction. Although it sounds simple it is one of the most complex problems in the work of creating a new machine learning model. To build a good predictive model, what is the right number of features and how to choose which features to keep and which features to drop and which new features to add? This is an important consideration in machine learning projects in managing what\u2019s known as the bias-variance tradeoff.\n<br><br>","e3647b22":"<a id=\"subsection-one\"><\/a>\n## Columns cardinality","ffd591e3":"## HGBoosting","ca8b75ff":"<a id=\"section-one\"><\/a>\n# Introduction\n\n<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThroughout this notebook I will try to develop the almost complete ML process for this competition.\n    <br><br>","76aa4352":"<a id=\"section-five\"><\/a>\n# Feature engineering","6c849465":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThe functions bellow are going to do your life more easy, and also this approach take less time than a Grid Search. You can also try Optuna. https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices.\n<br><br>","6b5d49ae":"<a id=\"subsection-five-five\"><\/a>\n## - Groups transforms","d6bd5e4a":"<a id=\"subsection-five-one\"><\/a>\n## - Frecuency count for the features with high cardinality","d39a3e2a":"### LGBM","2041fa54":"<a id=\"section-nine\"><\/a>\n# Final model - Blend","e80d3837":"### Train","b1179315":"<a id=\"section-six\"><\/a>\n# Feature selection with the new set of features","01e9abfa":"<a id=\"section-two\"><\/a>\n# Data checking","0193f97a":"<a id=\"section-seven\"><\/a>\n# Modeling","fef2b836":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nWe can see how this feature contributes to the prediction, when his value increase, also the prediction do it.    <br><br> ","f5c5e656":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n<b>Important note<\/b>, every plot it's univariate, so if you want to reach the sweet spot, you also must test all the hyperparameters together for find the optimal spot. If you want yo can use the cell bellow to find it.\n    \n<br><br>","7d66e824":"<a id=\"section-four\"><\/a>\n# Feature selection","db0cc956":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nWe can see how the cont's features are better predicting the target variable (except for cat 5, 8 and 1), but that does not mean that the features cat's do not work, but rather that perhaps a bit of feature engineering is needed to make them contribute to the prediction.\n    <br><br>","78d8fd31":"<a id=\"subsection-five-six\"><\/a>\n## - Applying log transform to the high skew features","1029219e":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\nThis approach it's very simple, just consist in train a tree base model to se the feature importance of them, in this case I'm going to try with XGboost, you can check the model documentation here, https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.XGBRegressor.\n<br><br>","447e4cc8":"<a id=\"subsection-four-seven\"><\/a>\n## Merging the dataframes","1c87ee06":"### Catboost","3bcb6cee":"<a id=\"subsection-five-four\"><\/a>\n## - Popular & Unpopular categories join","7a15bca0":"### LGBM","f429d6a6":"<p style=\"font-size:15px; font-family:verdana; line-height: 1.7em\">\n   <b> Observations: <\/b>\n       The distribution of the numerical features differs allot, for this case we can try some log transform in the feature engineering part of the process. There is some outliers prescence, in the cont0, cont6 and cont8 columns. \n    <br><br>","bffdb532":"<a id=\"subsection-five-three\"><\/a>\n## - Unpopular categories join"}}