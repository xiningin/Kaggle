{"cell_type":{"993a2732":"code","b7fc8789":"code","21b93fc5":"code","08f7c32b":"code","555e0e4b":"code","d4fae9eb":"code","b56a2c56":"code","46e92a79":"code","3d819d51":"code","4a8fa16c":"code","7a8f00a5":"markdown","2b54045e":"markdown","83ee4576":"markdown","a91473c9":"markdown","574d0057":"markdown","958fbb1d":"markdown"},"source":{"993a2732":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7fc8789":"# Loading .csv files\n\ndf_tr = pd.read_csv('\/kaggle\/input\/titanic\/train.csv').rename(columns={'Survived':'Target'})\ndf_tr.head()","21b93fc5":"# Preprocessing data\n\ndef sex_sort(x):\n    if x == 'male':\n        return 1\n    elif x == 'female':\n        return 0\n    else:\n        return 0.5\n\ndef embarked_sort(x):\n    if x == 'C':\n        return 0\n    elif x == 'Q':\n        return 0.5\n    elif x == 'S':\n        return 1\n    else:\n        return -2\n\ndef is_cherbourg(x):\n    if x == 'C':\n        return 1\n    else:\n        return 0\n    \ndef is_queenstown(x):\n    if x == 'Q':\n        return 1\n    else:\n        return 0\n    \ndef is_southampton(x):\n    if x == 'S':\n        return 1\n    else:\n        return 0\n\ndef pre_process(df):\n    # Sorting Sex\n    df['numeric_sex'] = df['Sex'].apply(lambda x: sex_sort(x))\n    df['Sex'] = df['numeric_sex']\n    \n    df['Queenstown'] = df['Embarked'].apply(lambda x: is_queenstown(x))\n    df['Cherbourg'] = df['Embarked'].apply(lambda x: is_cherbourg(x))\n    df['Southampton'] = df['Embarked'].apply(lambda x: is_southampton(x))\n    \n    return df\n\ndf_train = pre_process(df_tr).set_index('PassengerId')\n\n\n\n# removing NaNs\ndf_train = df_train.fillna(-999)\n\ndf_train.head()","08f7c32b":"from sklearn.model_selection import train_test_split\n# Function to select features and split into training data into x and y\ndef feature_split(features,df):\n    X = df[features]\n    y = df[['Target']]\n    return X, y\n\n# Loading and processing testing data\ndef load_test_data(features,scaler):\n    df_X = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n    df_X = pre_process(df_X).set_index('PassengerId').fillna(-999)\n    X = df_X[features]\n    X_scaled = scaler.transform(X)\n    \n    return X_scaled, df_X.index\n\n# Fucntion to cross validate model\n\nfrom sklearn.model_selection import cross_val_score\n\ndef cross_val_model(classifier, X, y, cv=10, classifier_name='Classifier'):\n    \n    # Finding Cross Validation Score\n    cross_val = cross_val_score(RandomForestCLF, X, y.values.ravel(), cv=cv)\n    string = f'{classifier_name} Accuracy: {cross_val.mean()} (+\/- {cross_val.std()*2})'\n    return string\n    \n","555e0e4b":"from sklearn.ensemble import RandomForestClassifier\n\n# Random Forest Features\nfeatures_rf = ['Pclass','Sex','Age','SibSp','Parch','Fare','Queenstown','Cherbourg','Southampton']\n\n# Random Forest Object\nRandomForestCLF = RandomForestClassifier(n_estimators=200)\n\n# Feature and Splitting\nX, y = feature_split(features_rf,df_train)\n\nprint(cross_val_model(RandomForestCLF, X, y, 10, 'Random Forest'))","d4fae9eb":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n# KNN Features\nfeatures_knn = ['Pclass','Sex','Age','SibSp','Parch','Fare','Queenstown','Cherbourg','Southampton']\n\n\nfor N in [1,2,3,4,5,6,7,8,9,10,11]:   \n    # KNN Object\n    knnCLF = KNeighborsClassifier(n_neighbors = N)\n    \n    # Feature and Splitting\n    X, y = feature_split(features_knn,df_train)\n    \n    scaler = MinMaxScaler().fit(X)\n    X = scaler.transform(X)\n    \n    print(cross_val_model(knnCLF, X, y, 10, f'{N}-NN'))\n","b56a2c56":"from sklearn.svm import SVC\n\n\n# SVC Features\nfeatures_svc = ['Pclass','Sex','Age','SibSp','Parch','Fare','Queenstown','Cherbourg','Southampton']\n\nkernels = ['rbf','poly']\n\nfor kernel in kernels:\n    svcCLF = SVC(kernel=kernel,probability=True)\n    \n    # Feature and Splitting\n    X, y = feature_split(features_svc,df_train)\n    \n    scaler = MinMaxScaler().fit(X)\n    X = scaler.transform(X)\n    \n    print(cross_val_model(svcCLF, X, y, 10, f'{kernel} SVC'))","46e92a79":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.svm import SVC\n\nfeatures = ['Pclass','Sex','Age','SibSp','Parch','Fare','Queenstown','Cherbourg','Southampton']\n\n# Respective classifier objects\nRandomForestCLF = RandomForestClassifier(n_estimators=500)\nsvcCLF = SVC(kernel='poly',probability=True)\nknnCLF = KNeighborsClassifier(n_neighbors = 101)\n\nvotingCLF = VotingClassifier(estimators=[\n     ('svc',svcCLF),('rf',RandomForestCLF),('knn',knnCLF)], voting='soft')\n\n# Feature and Splitting\nX, y = feature_split(features,df_train)\n\nscaler = MinMaxScaler().fit(X)\nX = scaler.transform(X)\n\nprint(cross_val_model(votingCLF, X, y, 10, f'Voting Classifier'))","3d819d51":"# Fitting Voting classifier with full X,y data\nvotingCLF.fit(X,y.values.ravel())\n\n\nX_to_predict, df_test_index = load_test_data(features,scaler)\npredictions = votingCLF.predict(X_to_predict)\nsubmission = pd.DataFrame({\"PassengerId\": df_test_index,\"Survived\": predictions})\nsubmission","4a8fa16c":"submission.to_csv('\/kaggle\/working\/submission.csv',index=False)","7a8f00a5":"# Voting Classifier\nAttempt to train Voting Classifier using:\n* 200 Tree Random Forest Classifier\n* 7-Nearest Neighbour Classifier\n* Polynomial Support Vector Classifier\n\nEach using features:\nfeatures = [\n    'Pclass',\n    'Sex',\n    'Age',\n    'SibSp',\n    'Parch',\n    'Fare',\n    'Embarked'\n]","2b54045e":"# Function to split df into train, test data using a given set of features","83ee4576":"# SVC Classifier","a91473c9":"# KNN Classifier\nSeem to get best score for 7 nearest neighbours","574d0057":"# Random Forest\nAfter tinkering with parameters, appear to avoid overfitting by choosing n_estimators=200","958fbb1d":"# Loading and Pre-Processing Data"}}