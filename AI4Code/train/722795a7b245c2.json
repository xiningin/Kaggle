{"cell_type":{"9fb29817":"code","2701e953":"code","c586b61d":"code","1a51df6d":"code","d96da2ac":"code","7f68d893":"code","f14f6b72":"code","fe515eac":"code","25c3cf83":"code","7aa51f97":"code","32cfa859":"code","f44d75dc":"code","a9cdadc4":"code","0563a6bf":"code","40dc03d2":"code","df2da91d":"code","27765d50":"code","19f2ae80":"markdown"},"source":{"9fb29817":"import numpy as np \nimport pandas as pd \n\nimport os\n#\u6570\u636e\u6240\u5728\u7684\u76ee\u5f55\nprint(os.listdir(\"..\/input\/kaggle-one-shot-pokemon\/kaggle-one-shot-pokemon\"))","2701e953":"#\u91cd\u7f6e\u56fe\u7247\u7684\u5927\u5c0f\nimport os\nimport cv2\n#\u56fe\u7247\u6570\u636e\u6240\u5728\u7684\u4f4d\u7f6e\nsrc = \"..\/input\/kaggle-one-shot-pokemon\/kaggle-one-shot-pokemon\/pokemon-b\"\n#\u91cd\u7f6e\u540e\u56fe\u7247\u4fdd\u5b58\u7684\u4f4d\u7f6e\ndst = \".\/resizedData\" # resized\ntry:\n    os.mkdir(dst)\nexcept:\n    pass\nfor each in os.listdir(src):\n    img = cv2.imread(os.path.join(src,each))\n    img = cv2.resize(img,(256,256))\n    cv2.imwrite(os.path.join(dst,each), img)\n","c586b61d":"#\u8bfb\u53d6\u5904\u7406\u540e\u7684\u6570\u636e\u96c6\nfrom glob import glob\nimport cv2\nPATH = os.path.abspath(os.path.join( os.getcwd(), 'resizedData'))\nIMGS = glob(os.path.join(PATH, \"*.jpg\"))","1a51df6d":"len(IMGS)","d96da2ac":"#\u5bfc\u5305\nimport tensorflow as tf\nimport random\nimport scipy.misc\nfrom utils import *\nslim = tf.contrib.slim","7f68d893":"#\u8bbe\u7f6e\u9884\u8bad\u7ec3\u56fe\u7247\u7684\u5927\u5c0f\u4ee5\u53ca\u989c\u8272\u901a\u9053\u6570\nHEIGHT, WIDTH, CHANNEL = 128, 128, 3\n#\u6279\u5904\u7406\u7684\u5927\u5c0f\nBATCH_SIZE = 64\n#\u8bad\u7ec3\u7684\u56de\u5408\u6570\nEPOCH = 500\nversion = 'newPokemon'\n#\u751f\u6210\u56fe\u7247\u7684\u4f4d\u7f6e\nnewPoke_path = '.\/' + version","f14f6b72":"#\u751f\u6210\u7684\u65b0\u7684\u795e\u5947\u5b9d\u8d1d\u6240\u5728\u7684\u76ee\u5f55\nnewPoke_path","fe515eac":"#\u67e5\u770b\u751f\u6210\u7684\u65b0\u795e\u5947\u5b9d\u8d1d\nPATH = os.path.abspath(os.path.join( '.\/', 'newPokemon'))\nIMGS = glob(os.path.join(PATH, \"*.jpg\"))\nfor i in range(0,5000,50):\n    try:\n        imgs = plt.imread('\/kaggle\/working\/newPokemon\/epoch{}.jpg'.format(i))\n        plt.imshow(imgs)\n        plt.show()\n    except:\n        pass\n#print(len(IMGS))","25c3cf83":"#\u6fc0\u6d3b\u51fd\u6570\ndef lrelu(x, n, leak=0.2): \n    return tf.maximum(x, leak * x, name=n) ","7aa51f97":"#\u6570\u636e\u5904\u7406\ndef process_data():   \n    current_dir = os.getcwd()\n    # parent = os.path.dirname(current_dir)\n    pokemon_dir = os.path.join(current_dir, 'resizedData')\n    images = []\n    for i in os.listdir(\"\/kaggle\/working\/resizedData\"):\n        images.append(\"\/kaggle\/working\/resizedData\/\"+i)\n    all_images = tf.convert_to_tensor(images, dtype = tf.string)\n    \n    images_queue = tf.train.slice_input_producer(\n                                        [all_images])\n                                        \n    content = tf.read_file(images_queue[0])\n    image = tf.image.decode_jpeg(content, channels = CHANNEL)\n    # sess1 = tf.Session()\n    # print sess1.run(image)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_brightness(image, max_delta = 0.1)\n    image = tf.image.random_contrast(image, lower = 0.9, upper = 1.1)\n    # noise = tf.Variable(tf.truncated_normal(shape = [HEIGHT,WIDTH,CHANNEL], dtype = tf.float32, stddev = 1e-3, name = 'noise')) \n    # print image.get_shape()\n    size = [HEIGHT, WIDTH]\n    image = tf.image.resize_images(image, size)\n    image.set_shape([HEIGHT,WIDTH,CHANNEL])\n    # image = image + noise\n    # image = tf.transpose(image, perm=[2, 0, 1])\n    # print image.get_shape()\n    \n    image = tf.cast(image, tf.float32)\n    image = image \/ 255.0\n    \n    iamges_batch = tf.train.shuffle_batch(\n                                    [image], batch_size = BATCH_SIZE,\n                                    num_threads = 4, capacity = 200 + 3* BATCH_SIZE,\n                                    min_after_dequeue = 200)\n    num_images = len(images)\n\n    return iamges_batch, num_images","32cfa859":"process_data()","f44d75dc":"#\u751f\u6210\u5668\ndef generator(input, random_dim, is_train, reuse=False):\n    c4, c8, c16, c32, c64 = 512, 256, 128, 64, 32 # channel num\n    s4 = 4\n    output_dim = CHANNEL  # RGB image\n    with tf.variable_scope('gen') as scope:\n        if reuse:\n            scope.reuse_variables()\n        w1 = tf.get_variable('w1', shape=[random_dim, s4 * s4 * c4], dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n        b1 = tf.get_variable('b1', shape=[c4 * s4 * s4], dtype=tf.float32,initializer=tf.constant_initializer(0.0))\n        flat_conv1 = tf.add(tf.matmul(input, w1), b1, name='flat_conv1')\n         #Convolution, bias, activation, repeat! \n        conv1 = tf.reshape(flat_conv1, shape=[-1, s4, s4, c4], name='conv1')\n        bn1 = tf.contrib.layers.batch_norm(conv1, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn1')\n        act1 = tf.nn.relu(bn1, name='act1')\n        # 8*8*256\n        #Convolution, bias, activation, repeat! \n        conv2 = tf.layers.conv2d_transpose(act1, c8, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                           name='conv2')\n        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn2')\n        act2 = tf.nn.relu(bn2, name='act2')\n        # 16*16*128\n        conv3 = tf.layers.conv2d_transpose(act2, c16, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                           name='conv3')\n        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn3')\n        act3 = tf.nn.relu(bn3, name='act3')\n        # 32*32*64\n        conv4 = tf.layers.conv2d_transpose(act3, c32, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                           name='conv4')\n        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn4')\n        act4 = tf.nn.relu(bn4, name='act4')\n        # 64*64*32\n        conv5 = tf.layers.conv2d_transpose(act4, c64, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                           name='conv5')\n        bn5 = tf.contrib.layers.batch_norm(conv5, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn5')\n        act5 = tf.nn.relu(bn5, name='act5')\n        \n        #128*128*3\n        conv6 = tf.layers.conv2d_transpose(act5, output_dim, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                           name='conv6')\n        # bn6 = tf.contrib.layers.batch_norm(conv6, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn6')\n        act6 = tf.nn.tanh(conv6, name='act6')\n        return act6\n","a9cdadc4":"#\u8fa8\u8bc6\u5668\ndef discriminator(input, is_train, reuse=False):\n    c2, c4, c8, c16 = 64, 128, 256, 512  # channel num: 64, 128, 256, 512\n    with tf.variable_scope('dis') as scope:\n        if reuse:\n            scope.reuse_variables()\n\n        #Convolution, activation, bias, repeat! \n        conv1 = tf.layers.conv2d(input, c2, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                 name='conv1')\n        bn1 = tf.contrib.layers.batch_norm(conv1, is_training = is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope = 'bn1')\n        act1 = lrelu(conv1, n='act1')\n         #Convolution, activation, bias, repeat! \n        conv2 = tf.layers.conv2d(act1, c4, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                 name='conv2')\n        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn2')\n        act2 = lrelu(bn2, n='act2')\n        #Convolution, activation, bias, repeat! \n        conv3 = tf.layers.conv2d(act2, c8, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                 name='conv3')\n        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn3')\n        act3 = lrelu(bn3, n='act3')\n         #Convolution, activation, bias, repeat! \n        conv4 = tf.layers.conv2d(act3, c16, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                 name='conv4')\n        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn4')\n        act4 = lrelu(bn4, n='act4')\n       \n        # start from act4\n        dim = int(np.prod(act4.get_shape()[1:]))\n        fc1 = tf.reshape(act4, shape=[-1, dim], name='fc1')\n      \n        \n        w2 = tf.get_variable('w2', shape=[fc1.shape[-1], 1], dtype=tf.float32,\n                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n        b2 = tf.get_variable('b2', shape=[1], dtype=tf.float32,\n                             initializer=tf.constant_initializer(0.0))\n\n        # wgan just get rid of the sigmoid\n        logits = tf.add(tf.matmul(fc1, w2), b2, name='logits')\n        # dcgan\n        acted_out = tf.nn.sigmoid(logits)\n        return logits #, acted_out\n","0563a6bf":"#\u8bad\u7ec3\u51fd\u6570\ndef train():\n    random_dim = 100\n    \n    with tf.variable_scope('input'):\n        #real and fake image placholders\n        real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n        random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n        is_train = tf.placeholder(tf.bool, name='is_train')\n    \n    # wgan\n    fake_image = generator(random_input, random_dim, is_train)\n    \n    real_result = discriminator(real_image, is_train)\n    fake_result = discriminator(fake_image, is_train, reuse=True)\n    \n    d_loss = tf.reduce_mean(fake_result) - tf.reduce_mean(real_result)  # This optimizes the discriminator.\n    g_loss = -tf.reduce_mean(fake_result)  # This optimizes the generator.\n            \n\n    t_vars = tf.trainable_variables()\n    d_vars = [var for var in t_vars if 'dis' in var.name]\n    g_vars = [var for var in t_vars if 'gen' in var.name]\n    trainer_d = tf.train.RMSPropOptimizer(learning_rate=2e-4).minimize(d_loss, var_list=d_vars)\n    trainer_g = tf.train.RMSPropOptimizer(learning_rate=2e-4).minimize(g_loss, var_list=g_vars)\n    # clip discriminator weights\n    d_clip = [v.assign(tf.clip_by_value(v, -0.01, 0.01)) for v in d_vars]\n\n    \n    batch_size = BATCH_SIZE\n    image_batch, samples_num = process_data()\n    \n    batch_num = int(samples_num \/ batch_size)\n    total_batch = 0\n    sess = tf.Session()\n    saver = tf.train.Saver()\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    # continue training\n    save_path = saver.save(sess, \"\/tmp\/model.ckpt\")\n    ckpt = tf.train.latest_checkpoint('.\/model\/' + version)\n    saver.restore(sess, save_path)\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    print('total training sample num:%d' % samples_num)\n    print('batch size: %d, batch num per epoch: %d, epoch num: %d' % (batch_size, batch_num, EPOCH))\n    print('start training...')\n    for i in range(EPOCH):\n        print(\"Running epoch {}\/{}...\".format(i, EPOCH))\n        for j in range(batch_num):\n            d_iters = 5\n            g_iters = 1\n\n            train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n            for k in range(d_iters):\n                train_image = sess.run(image_batch)\n                #wgan clip weights\n                sess.run(d_clip)\n                \n                # Update the discriminator\n                _, dLoss = sess.run([trainer_d, d_loss],\n                                    feed_dict={random_input: train_noise, real_image: train_image, is_train: True})\n\n            # Update the generator\n            for k in range(g_iters):\n                # train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n                _, gLoss = sess.run([trainer_g, g_loss],\n                                    feed_dict={random_input: train_noise, is_train: True})\n\n            #print('train:[%d\/%d],d_loss:%f,g_loss:%f' % (i, j, dLoss, gLoss))\n            \n        # save check point every 500 epoch\n        if i%500 == 0:\n            if not os.path.exists('.\/model\/' + version):\n                os.makedirs('.\/model\/' + version)\n            saver.save(sess, '.\/model\/' +version + '\/' + str(i))  \n        if i%10 == 0:\n            # save images\n            if not os.path.exists(newPoke_path):\n                os.makedirs(newPoke_path)\n            sample_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n            imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise, is_train: False})\n            # imgtest = imgtest * 255.0\n            # imgtest.astype(np.uint8)\n            save_images(imgtest, [8,8] ,newPoke_path + '\/epoch' + str(i) + '.jpg')\n            img = plt.imread(newPoke_path + '\/epoch' + str(i) + '.jpg')\n            plt.imshow(img)\n            plt.title('Generate Image After Epoch\/0 Trained')\n            plt.axis('off')\n            plt.show()\n            print('train:[%d],d_loss:%f,g_loss:%f' % (i, dLoss, gLoss))\n            print('-------------------------------------------------------------------------')\n    coord.request_stop()\n    coord.join(threads)","40dc03d2":"#\u56fe\u7247\u5904\u7406\u51fd\u6570\nfrom __future__ import division\nimport math\nimport json\nimport random\nimport pprint\nimport scipy.misc\nimport numpy as np\nfrom time import gmtime, strftime\nfrom six.moves import xrange\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\npp = pprint.PrettyPrinter()\n\nget_stddev = lambda x, k_h, k_w: 1\/math.sqrt(k_w*k_h*x.get_shape()[-1])\n\ndef show_all_variables():\n    model_vars = tf.trainable_variables()\n    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\ndef get_image(image_path, input_height, input_width,\n              resize_height=64, resize_width=64,\n              crop=True, grayscale=False):\n    image = imread(image_path, grayscale)\n    return transform(image, input_height, input_width,\n                   resize_height, resize_width, crop)\n\ndef save_images(images, size, image_path):\n    return imsave(inverse_transform(images), size, image_path)\n\ndef imread(path, grayscale = False):\n    if (grayscale):\n        return scipy.misc.imread(path, flatten = True).astype(np.float)\n    else:\n        return scipy.misc.imread(path).astype(np.float)\n\ndef merge_images(images, size):\n    return inverse_transform(images)\n\ndef merge(images, size):\n    h, w = images.shape[1], images.shape[2]\n    if (images.shape[3] in (3,4)):\n        c = images.shape[3]\n        img = np.zeros((h * size[0], w * size[1], c))\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx \/\/ size[1]\n            img[j * h:j * h + h, i * w:i * w + w, :] = image\n        return img\n    elif images.shape[3]==1:\n        img = np.zeros((h * size[0], w * size[1]))\n        for idx, image in enumerate(images):\n            i = idx % size[1]\n            j = idx \/\/ size[1]\n            img[j * h:j * h + h, i * w:i * w + w] = image[:,:,0]\n        return img\n    else:\n        raise ValueError('in merge(images,size) images parameter '\n                     'must have dimensions: HxW or HxWx3 or HxWx4')\n\ndef imsave(images, size, path):\n    image = np.squeeze(merge(images, size))\n    return scipy.misc.imsave(path, image)\n\ndef center_crop(x, crop_h, crop_w,\n                resize_h=64, resize_w=64):\n    \n    if crop_w is None:\n        crop_w = crop_h\n    h, w = x.shape[:2]\n    j = int(round((h - crop_h)\/2.))\n    i = int(round((w - crop_w)\/2.))\n    return scipy.misc.imresize(\n      x[j:j+crop_h, i:i+crop_w], [resize_h, resize_w])\n\ndef transform(image, input_height, input_width, \n              resize_height=64, resize_width=64, crop=True):\n    \n    if crop:\n        cropped_image = center_crop(\n          image, input_height, input_width, \n          resize_height, resize_width)\n    else:\n        cropped_image = scipy.misc.imresize(image, [resize_height, resize_width])\n    return np.array(cropped_image)\/127.5 - 1.\n\ndef inverse_transform(images):\n    return (images+1.)\/2.","df2da91d":"import time\nimport matplotlib.pyplot as plt","27765d50":"if __name__ == \"__main__\":\n    start_time = time.time()\n    tf.reset_default_graph()\n    train()\n    end_time = time.time()\n    print('\u5171\u7528\u65f6',end_time-start_time,'s')","19f2ae80":"> ***\u901a\u8fc7Gan\u4f7f\u7528\u795e\u5947\u5b9d\u8d1d\u6570\u636e\u96c6\u751f\u6210\u65b0\u7684\u795e\u5947\u5b9d\u8d1d***"}}