{"cell_type":{"1888f662":"code","4be5809e":"code","3853a9a9":"code","c76238f7":"code","a00b99ba":"code","1971f2cc":"code","5e220013":"code","4ce051c9":"code","fdd50689":"code","2778da6c":"code","dd8ae90b":"code","e10d69b3":"code","5b9b22d8":"code","a5caca4a":"code","ff02e7a8":"code","9acf66ca":"code","3349dd46":"code","354daebc":"code","540b9ae1":"code","53a62622":"code","6716430e":"code","ed998b59":"code","06183761":"code","00b67727":"code","46deaf98":"code","dec8fc8f":"code","c6dd4613":"code","02087436":"code","322d8f2b":"code","ee5d1aac":"code","22ef7561":"code","10ce85aa":"code","614c9f60":"markdown","0b8300e3":"markdown","891adb95":"markdown","e7c3b64a":"markdown","9e37bf2b":"markdown","e3cae9a9":"markdown","6c426e42":"markdown","7f3ab3a3":"markdown","cfe2c756":"markdown","07fe49e1":"markdown","2459d971":"markdown","ebb2ddfa":"markdown","f2e6c4c2":"markdown","156a5570":"markdown","21bd172d":"markdown","c7377cb4":"markdown","b9b663a6":"markdown","4305b6e9":"markdown","b73e96bb":"markdown","ce79ebe5":"markdown","1c5ea6ba":"markdown","c73fec7a":"markdown","f4f7d1fe":"markdown","1b244ac2":"markdown","f3e17b6a":"markdown","820fe5f5":"markdown","40a35baf":"markdown","07270d5f":"markdown","a5feb3cc":"markdown","00daee58":"markdown","f3d9d6f5":"markdown","32e1bb5c":"markdown","3f985ebc":"markdown","e1673c1f":"markdown"},"source":{"1888f662":"from sklearn import linear_model\n\nreg = linear_model.LinearRegression()\n\nreg.fit(X, y)\n","4be5809e":"from sklearn import linear_model\n\nreg = linear_model.Ridge(alpha=.5)\n\nreg.fit(X, y)","3853a9a9":"import numpy as np\n\nfrom sklearn import linear_model\n\nreg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\n \nreg.fit(X, y)","c76238f7":"from sklearn import linear_model\n\nreg = linear_model.Lasso(alpha=0.1)\n\nreg.fit(X, y)\n","a00b99ba":"from sklearn import linear_model\n\nclf = linear_model.MultiTaskLasso(alpha=0.1)\n\nclf.fit(X,y)\n","1971f2cc":"from sklearn import linear_model\n\nclf = linear_model.MultiTaskElasticNet(alpha=0.1)\n\nclf.fit(X,y)\n","5e220013":"from sklearn import linear_model\n\nreg = linear_model.Lars(n_nonzero_coefs=1)\n\nreg.fit(X, y)\n","4ce051c9":"from sklearn.linear_model import OrthogonalMatchingPursuit\n\nomp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\n\nomp.fit(X, y)\n","fdd50689":"from sklearn import linear_model\n\nX = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]\n\nY = [0., 1., 2., 3.]\n\nreg = linear_model.BayesianRidge()\n\nreg.fit(X, Y)\n","2778da6c":"from sklearn.linear_model import LogisticRegression\n\nclf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01, solver='saga')\n\nclf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01, solver='saga')\n\nclf_en_LR = LogisticRegression(C=C, penalty='elasticnet', solver='saga', l1_ratio=l1_ratio, tol=0.01)\n\nclf_l1_LR.fit(X, y) clf_l2_LR.fit(X, y) clf_en_LR.fit(X, y)\n","dd8ae90b":"from sklearn.linear_model import TweedieRegressor\n\nreg = TweedieRegressor(power=1, alpha=0.5, link='log')\n\nreg.fit(X, y)\n","e10d69b3":"X = [[0, 0], [1, 1]]\n\ny = [0, 1]\n\nclf = svm.SVC()\n\nclf.fit(X, y)","5b9b22d8":"from sklearn.linear_model import SGDClassifier\n\nX = [[0., 0.], [1., 1.]]\n\ny = [0, 1]\n\nclf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n\nclf.fit(X, y)\n","a5caca4a":"import numpy as np\n\nfrom sklearn.linear_model import SGDRegressor\n\nn_samples, n_features = 10, 5\n\nrng = np.random.RandomState(0)\n\ny = rng.randn(n_samples)\n\nX = rng.randn(n_samples, n_features)\n\nreg = SGDRegressor(max_iter=1000, tol=1e-3)\n\nreg.fit(X, y)\n","ff02e7a8":"from sklearn import neighbors, datasets\n\nn_neighbors = 15\n\nclf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n\nclf.fit(X, y)\n","9acf66ca":"X = [[0], [1], [2], [3]]\n\ny = [0, 0, 1, 1]\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nneigh = KNeighborsRegressor(n_neighbors=2)\n\nneigh.fit(X, y)","3349dd46":"X = [[0], [1], [2], [3]]\n\ny = [0, 0, 1, 1]\n\nfrom sklearn.neighbors import RadiusNeighborsRegressor\n\nneigh = RadiusNeighborsRegressor(radius=1.0)\n\nneigh.fit(X, y)\n","354daebc":"from sklearn.datasets import load_iris\n\nfrom sklearn.gaussian_process import GaussianProcessClassifier\n\nfrom sklearn.gaussian_process.kernels import RBF\n\nX, y = load_iris(return_X_y=True)\n\nkernel = 1.0 * RBF(1.0)\n\ngpc = GaussianProcessClassifier(kernel=kernel,random_state=0).fit(X, y)\n","540b9ae1":"from sklearn.datasets import load_iris\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\n\nX, y = load_iris(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n\ngnb = GaussianNB()\n\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\n","53a62622":"from sklearn import tree\n\nX = [[0, 0], [1, 1]]\n\nY = [0, 1]\n\nclf = tree.DecisionTreeClassifier() \/ clf = tree.DecisionTreeREgressor()\n\nclf = clf.fit(X, Y)\n","6716430e":"from sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nbagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5) #can use other classifier also instead of KNC\n","ed998b59":"from sklearn.ensemble import RandomForestClassifier\n\nX = [[0, 0], [1, 1]]\n\nY = [0, 1]\n\nclf = RandomForestClassifier(n_estimators=10)\n\nclf = clf.fit(X, Y)\n","06183761":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.datasets import make_blobs\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = make_blobs(n_samples=10000, n_features=10, centers=100,random_state=0)\n\nclf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n\nscores = cross_val_score(clf, X, y, cv=5)\n\nscores.mean()\n>0.98...\n\nclf = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n\nscores = cross_val_score(clf, X, y, cv=5)\n\nscores.mean()\n>0.999...\n\nclf = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n\nscores = cross_val_score(clf, X, y, cv=5)\n\nscores.mean() \n# >0.999 True\n","00b67727":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.datasets import load_iris\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nX, y = load_iris(return_X_y=True)\n\nclf = AdaBoostClassifier(n_estimators=100)\n\nscores = cross_val_score(clf, X, y, cv=5)\n\nscores.mean() \n# >0.9...\n","46deaf98":"from sklearn.datasets import make_hastie_10_2\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX, y = make_hastie_10_2(random_state=0)\n\nX_train, X_test = X[:2000], X[2000:]\n\ny_train, y_test = y[:2000], y[2000:]\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\n\nclf.score(X_test, y_test)\n\n# >0.913...\n","dec8fc8f":"import numpy as np\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.datasets import make_friedman1\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nX, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n\nX_train, X_test = X[:200], X[200:]\n\ny_train, y_test = y[:200], y[200:]\n\nest = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n\nmean_squared_error(y_test, est.predict(X_test))\n\n# >5.00...\n","c6dd4613":"_ = est.set_params(n_estimators=200, warm_start=True) # set warm_start and new nr of trees\n\n_ = est.fit(X_train, y_train) # fit additional 100 trees to est\n\nmean_squared_error(y_test, est.predict(X_test))\n\n# >3.84...\n","02087436":"from sklearn.experimental import enable_hist_gradient_boosting\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nfrom sklearn.datasets import make_hastie_10_2\n\nX, y = make_hastie_10_2(random_state=0)\n\nX_train, X_test = X[:2000], X[2000:]\n\ny_train, y_test = y[:2000], y[2000:]\n\nclf = HistGradientBoostingClassifier(min_samples_leaf=1, max_depth=2, learning_rate=1, max_iter=1).fit(X_train, y_train)\n\nclf.score(X_test, y_test)\n\n# >0.8965\n","322d8f2b":" from sklearn import datasets\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.ensemble import VotingClassifier\n\niris = datasets.load_iris()\n\nX, y = iris.data[:, 1:3], iris.target\n\nclf1 = LogisticRegression(random_state=1)\n\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n\nclf3 = GaussianNB()\n\neclf = VotingClassifier( estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n\n    \n    \n    \n# >Accuracy: 0.95 (+\/- 0.04) [Logistic Regression]\n\n# >Accuracy: 0.94 (+\/- 0.04) [Random Forest]\n\n# >Accuracy: 0.91 (+\/- 0.04) [naive Bayes]\n\n# >Accuracy: 0.95 (+\/- 0.04) [Ensemble]\n","ee5d1aac":"from sklearn.model_selection import GridSearchCV\n\nclf1 = LogisticRegression(random_state=1)\n\nclf2 = RandomForestClassifier(random_state=1)\n\nclf3 = GaussianNB()\n\neclf = VotingClassifier( estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft' )\n\nparams = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n\ngrid = grid.fit(iris.data, iris.target)\n","22ef7561":"import xgboost as xgb\n\n#read in data\n\ndtrain = xgb.DMatrix('demo\/data\/agaricus.txt.train')\n\ndtest = xgb.DMatrix('demo\/data\/agaricus.txt.test')\n\n#specify parameters via map\n\nparam = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n\nnum_round = 2\n\nbst = xgb.train(param, dtrain, num_round)\n\n#make prediction\n\npreds = bst.predict(dtest)\n","10ce85aa":"\n\nfrom sklearn import svm, datasets\n\nfrom sklearn.model_selection import GridSearchCV\n\niris = datasets.load_iris()\n\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n\nsvc = svm.SVC()\n\nclf = GridSearchCV(svc, parameters)\n\nclf.fit(iris.data, iris.target)\n\nGridSearchCV(estimator=SVC(), param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n","614c9f60":"<a id = \"8\"><\/a>\n> # **OrthogonalMatchingPursuit(OMP)**\n[Go back to the Table of Contents](#table_of_contents)","0b8300e3":"<a id = \"26\"><\/a>\n# VotingClassifier\n[Go back to the Table of Contents](#table_of_contents)\n\nThe idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.","891adb95":"***Additional fit***","e7c3b64a":"<a id = \"6\"><\/a>\n> # **MultiTaskElasticNet**\n[Go back to the Table of Contents](#table_of_contents)","9e37bf2b":"<a id = \"17\"><\/a>\n> # **GaussianProcessClassifier**\n[Go back to the Table of Contents](#table_of_contents)","e3cae9a9":"<a id = \"14\"><\/a>\n> # **KNeighborsClassifier**\n[Go back to the Table of Contents](#table_of_contents)","6c426e42":"<a id = \"27\"><\/a>\n# Using the VotingClassifier with GridSearchCV\n[Go back to the Table of Contents](#table_of_contents)\n\nThe VotingClassifier can also be used together with GridSearchCV in order to tune the hyperparameters of the individual estimators:","7f3ab3a3":"<a id = \"3\"><\/a>\n> # **RidgeCV**\n[Go back to the Table of Contents](#table_of_contents)","cfe2c756":"<a id = \"19\"><\/a>\n> # **DecisionTreeClassifier and regressor**\n[Go back to the Table of Contents](#table_of_contents)","07fe49e1":"<a id = \"15\"><\/a>\n> #  **KNeighborsRegressor**\n[Go back to the Table of Contents](#table_of_contents)","2459d971":"<a id = \"11\"><\/a>\n> # **TweedieRegressor**\n[Go back to the Table of Contents](#table_of_contents)\n  \n  The choice of the distribution depends on the problem at hand:\n\n    If the target values \n\n    are counts (non-negative integer valued) or relative frequencies (non-negative), you might use a Poisson deviance with log-link.\n\n    If the target values are positive valued and skewed, you might try a Gamma deviance with log-link.\n\n    If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian deviance (or even higher variance powers of the Tweedie family).\n\nExamples of use cases include:\n\n    Agriculture \/ weather modeling: number of rain events per year (Poisson), amount of rainfall per event (Gamma), total rainfall per year (Tweedie \/ Compound Poisson Gamma).\n\n    Risk modeling \/ insurance policy pricing: number of claim events \/ policyholder per year (Poisson), cost per event (Gamma), total cost per policyholder per year (Tweedie \/ Compound Poisson Gamma).\n\n    Predictive maintenance: number of production interruption events per year: Poisson, duration of interruption: Gamma, total interruption time per year (Tweedie \/ Compound Poisson Gamma).\n> # ","ebb2ddfa":"<a id = \"20\"><\/a>\n> # **BaggingClassifier**\n[Go back to the Table of Contents](#table_of_contents)","f2e6c4c2":"# Experimental \n**Use when data size is in tens of thousands no need of imputation ; still in development**","156a5570":"<a id = \"21\"><\/a>\n> # **RandomForestClassifier**\n[Go back to the Table of Contents](#table_of_contents)","21bd172d":"# Welcome to my Notebook!\n* **I recently joined kaggel and couldn't keep a track of models that i can use for the competitions so I have compiled a list of different models. I thought of making it public so anyone else having the same problem can also use it**\n\n* **This notebook contains a simple implementation of each model**\n\n* **You can use them as a reference**\n\n* **Below is the table of contents**\n\n\n\n* **I have used below mentioned resource;  You can read about different models in detail there**\nsource:https:\/\/scikit-learn.org\/stable\/supervised_learning.html\n\n\n* ***Do upvote it, if you like it!!***\n\n\n*Ps. I am new to data science, do correct me if I made any mistake in this kernel*","c7377cb4":"<a id = \"24\"><\/a>\n# **GradientBoostingClassifier and regressor**\n[Go back to the Table of Contents](#table_of_contents)\n\nAt each iteration the base classifier is trained on a fraction subsample of the available training data. The subsample is drawn without replacement. A typical value of subsample is 0.5.","b9b663a6":"<a id = \"7\"><\/a>\n> # **least angle regression (Lars)**\n[Go back to the Table of Contents](#table_of_contents)","4305b6e9":"<a id = \"23\"><\/a>\n> # **AdaBoostClassifier**\n[Go back to the Table of Contents](#table_of_contents)\n\n\n\nThe number of weak learners is controlled by the parameter n_estimators. The learning_rate parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the base_estimator parameter. The main parameters to tune to obtain good results are n_estimators and the complexity of the base estimators (e.g., its depth max_depth or minimum required number of samples to consider a split min_samples_split).","b73e96bb":"# **Ensemble models**","ce79ebe5":"<a id = \"13\"><\/a>\n> # **SGD Classifier and Regressor**\n[Go back to the Table of Contents](#table_of_contents)\n\nThe advantages of Stochastic Gradient Descent are:\n\n        Efficiency.\n\n        Ease of implementation (lots of opportunities for code tuning).\n\nThe disadvantages of Stochastic Gradient Descent include:\n\n        SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n\n        SGD is sensitive to feature scaling.\n\n****","1c5ea6ba":"<a id = \"18\"><\/a>\n> # **GaussianNB**\n[Go back to the Table of Contents](#table_of_contents)","c73fec7a":"<a id = \"1\"><\/a>\n> # **LinearRegression**\n[Go back to the Table of Contents](#table_of_contents)","f4f7d1fe":"<a id = \"2\"><\/a>\n> # **Ridge regression**\n[Go back to the Table of Contents](#table_of_contents)","1b244ac2":"<a id = \"table_of_contents\"><\/a>\n# Table of Contents\n\n[LinearRegression](#1)\n\n[RidgeRegression](#2)\n\n[RidgeCV](#3)\n\n[Lasso](#4)\n\n[MultiTaskLasso](#5)\n\n[MultiTaskElasticNet](#6)\n\n[Lars](#7)\n\n[OMP](#8)\n\n[BayesianRidgeRegression](#9)\n\n[LogisticRegressor](#10)\n\n[TweedieRegressor](#11)\n\n[SVM](#12)\n\n[SGD](#13)\n\n[KNeighbourClassifier](#14)\n\n[KNeighborRegressor](#15)\n\n[RadiusNighborRegressor](#16)\n\n[GaussianProcessClassifier](#17)\n\n[GaussianNB](#18)\n\n[DecisionTree Classifier and Regressor](#19)\n\n[BaggingClassifier](#20)\n\n[RandomForestClassifier](#21)\n\n[ExtraTreeClassifier](#22)\n\n[AdaBoost](#23)\n\n[GradientBoosting](#24)\n\n[HistGradientBoosting](#25)\n\n[VotingClassifier](#26)\n\n[Using the VotingClassifier with GridSearchCV](#27)\n\n[XGBoost](#28)\n\n[GridSearchCV](#29)\n\n\n\n","f3e17b6a":"<a id = \"16\"><\/a>\n> # **RadiusNeighborsRegressor**\n[Go back to the Table of Contents](#table_of_contents)","820fe5f5":"<a id = \"12\"><\/a>\n> # **SVM**\n[Go back to the Table of Contents](#table_of_contents)","40a35baf":"<a id = \"25\"><\/a>\n# **HistGradientBoostingClassifier and regressor**\n[Go back to the Table of Contents](#table_of_contents)","07270d5f":"<a id = \"9\"><\/a>\n> # **BayesianRidge Regression**\n[Go back to the Table of Contents](#table_of_contents)","a5feb3cc":"<a id = \"10\"><\/a>\n> # **LogisticRegression**\n[Go back to the Table of Contents](#table_of_contents)","00daee58":"<a id = \"5\"><\/a>\n# **Multi task lasso**\n[Go back to the Table of Contents](#table_of_contents)","f3d9d6f5":"<a id = \"22\"><\/a>\n> # **ExtraTreesClassifier and its comparison**\n[Go back to the Table of Contents](#table_of_contents)","32e1bb5c":"<a id = \"29\"><\/a>\n# GridsearchCV\n[Go back to the Table of Contents](#table_of_contents)","3f985ebc":"<a id = \"28\"><\/a>\n# XGBoost\n[Go back to the Table of Contents](#table_of_contents)","e1673c1f":"<a id = \"4\"><\/a>\n# **Lasso**\n[Go back to the Table of Contents](#table_of_contents)"}}