{"cell_type":{"e079af9b":"code","da6ba9ee":"code","6ac25d84":"code","445ba286":"code","51703c01":"code","fb8d325f":"code","5014da0d":"code","867606bc":"code","d5f1f239":"code","153108c1":"code","e104de8e":"code","b400c693":"code","061f8a2d":"code","8b2ff41d":"code","2d8b419a":"code","3cb3b890":"code","ead5fd0f":"code","4e365ee2":"code","e65a8a47":"code","53ac2176":"code","c2aea298":"code","47a6e84b":"code","9830b1e6":"code","2a5f213d":"markdown","1c3d5f97":"markdown","d56532fe":"markdown","f7b52e68":"markdown","de4a348c":"markdown","db045292":"markdown","c907ffb3":"markdown","025aba76":"markdown","ba77107a":"markdown","eeaca304":"markdown","3ac83ca9":"markdown","48b834a6":"markdown","cec88f33":"markdown","c2999cb2":"markdown"},"source":{"e079af9b":"import warnings\nwarnings.simplefilter('ignore')","da6ba9ee":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_addons as tfa\n\nfrom tqdm.notebook import tqdm\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\nfrom scipy.stats import pearsonr\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\nimport os\nimport sys\nimport nltk\nimport string\nimport math\nimport logging\nimport glob\nimport random\n\ntf.get_logger().setLevel(logging.ERROR)\n        \ntqdm.pandas()\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","6ac25d84":"# Seed all random sources\ndef set_seeds(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    \nset_seeds(42)","445ba286":"SEQ_LENGTH = 250\n\ntrain = pd.read_pickle('\/kaggle\/input\/simplenormal-wikipedia-abstracts-v1\/wikipedia_abstracts.pkl')","51703c01":"# Example of accessing the part of speech feature\nprint(train.loc[0, 'pos'].split(chr(0)))","fb8d325f":"display(train.head())","5014da0d":"display(train.info())","867606bc":"# Define the model name\nMODEL = 'roberta-base'\n\n# Load tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)","d5f1f239":"# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(excerpt):\n    enc_di = tokenizer.batch_encode_plus(\n        excerpt,\n        padding = 'max_length',\n        truncation = True,\n        max_length = SEQ_LENGTH,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# Compute text encoding, this will take ~5 minutes\ntrain['input_ids'] = regular_encode(train['abstract_clean']).tolist()\ndisplay(train.head())","153108c1":"# Detect hardware, return appropriate distribution strategy\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', TPU.master())\nexcept ValueError:\n    print('Running on GPU')\n    TPU = None\n\nif TPU:\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')\n\n# set half precision policy\nmixed_precision.set_policy('float32')\n\nprint(f'Compute dtype: {mixed_precision.global_policy().compute_dtype}')\nprint(f'Variable dtype: {mixed_precision.global_policy().variable_dtype}')","e104de8e":"def get_model():\n    tf.keras.backend.clear_session()\n\n    with strategy.scope():\n        # RoBERTa\n        transformer = TFRobertaModel.from_pretrained(MODEL)\n        input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\n        sequence_output = transformer(input_ids)[0]\n        # We only need the cls_token, resulting in a 2d array\n        cls_token = sequence_output[:, 0, :]\n        # 2 output neurons for Simple and Normal class\n        output = tf.keras.layers.Dense(2, activation='softmax', dtype=tf.float32)(cls_token)\n        \n        model = tf.keras.models.Model(inputs = [input_ids], outputs = [output])\n\n        loss = tf.keras.losses.SparseCategoricalCrossentropy()\n        optimizer = tf.optimizers.Adam(learning_rate=1e-5)\n        metrics = [\n            tf.keras.metrics.SparseCategoricalAccuracy('accuracy'),\n        ]\n\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)            \n    \n    return model\n\nmodel = get_model()","b400c693":"model.summary()","061f8a2d":"tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=False)","8b2ff41d":"# Training configuration\nBATCH_SIZE_BASE = 32\nBATCH_SIZE = BATCH_SIZE_BASE * REPLICAS\nSTEPS_PER_EPOCH = 100\nEPOCHS = len(train) \/\/ (STEPS_PER_EPOCH * BATCH_SIZE)\nKFOLDS = 5\n\nprint(f'BATCH SIZE: {BATCH_SIZE}, EPOCHS: {EPOCHS}')","2d8b419a":"def get_train_dataset():\n    # Randomize the dataset order, otherwise the model will first be trained on Simple abstracts only\n    idxs = np.arange(len(train))\n    random.Random(42).shuffle(idxs)\n    \n    train_x = { \n        'input_ids': np.array(train.loc[idxs, 'input_ids'].tolist()),\n    }\n    train_y = train.loc[idxs, 'label_int']\n    \n    train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n    \n    train_dataset = train_dataset.batch(BATCH_SIZE)\n    \n    return train_dataset\n\ntrain_dataset = get_train_dataset()","3cb3b890":"# Example of a batch\ntrain_x, train_y = next(iter(train_dataset))\nprint(f'train_x keys: {list(train_x.keys())}, train_x shape: {train_x[\"input_ids\"].shape}')\nprint(f'train_y shape: {train_y.shape}, train_y dtype {train_y.dtype}')\nprint(f'labels: {train_y}')","ead5fd0f":"history = model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    verbose=1,\n    steps_per_epoch=STEPS_PER_EPOCH,\n)","4e365ee2":"def plot_history_metric(history, metric):\n    plt.figure(figsize=(15, 8))\n    N_EPOCHS = len(history.history['loss'])\n    x = [1, 5] + [10 + 5 * idx for idx in range((N_EPOCHS - 10) \/\/ 5 + 1)]\n    x_ticks = np.arange(1, N_EPOCHS+1)\n    val = 'val' in ''.join(history.history.keys())\n    # summarize history for accuracy\n    plt.plot(x_ticks, history.history[metric])\n    if val:\n        val_values = history.history[f'val_{metric}']\n        val_argmin = np.argmin(val_values)\n        plt.scatter(val_argmin + 1, val_values[val_argmin], color='red', s=50, marker='o')\n        plt.plot(x_ticks, val_values)\n    \n    plt.title(f'Model {metric}', fontsize=24)\n    plt.ylabel(metric, fontsize=18)\n    plt.xlabel('epoch', fontsize=18)\n    plt.tick_params(axis='x', labelsize=8)\n    plt.xticks(x, fontsize=16) # set tick step to 1 and let x axis start at 1\n    plt.yticks(fontsize=16)\n    plt.legend(['train'] + ['test'] if val else ['train'],  prop={'size': 18})\n    plt.grid()","e65a8a47":"plot_history_metric(history, 'loss')","53ac2176":"plot_history_metric(history, 'accuracy')","c2aea298":"# Save RoBERTa weight\nfor l_idx, l in enumerate(model.layers):\n    print(l.name)\n    if l.name == 'tf_roberta_model':\n        print(f'Saving layer {l_idx} with name {l.name}')\n        l.save_weights('roberta_pretrained.h5')","47a6e84b":"def get_model():\n    tf.keras.backend.clear_session()\n\n    with strategy.scope():\n        # RoBERTa\n        transformer = TFRobertaModel.from_pretrained(MODEL)\n        # Load saved weights\n        transformer.load_weights('roberta_pretrained.h5')\n        \n        input_ids = tf.keras.layers.Input(shape = (SEQ_LENGTH), dtype=tf.int32, name='input_ids')\n        sequence_output = transformer(input_ids)[0]\n        # We only need the cls_token, resulting in a 2d array\n        cls_token = sequence_output[:, 0, :]\n        output = tf.keras.layers.Dense(1, activation='linear', dtype=tf.float32)(cls_token)\n        \n        # Model\n        model = tf.keras.models.Model(inputs=input_ids, outputs=output)\n\n        loss = tf.keras.losses.MeanSquaredError()\n        optimizer = tf.optimizers.Adam(learning_rate=4e-5)\n        metrics = [\n            tf.keras.metrics.RootMeanSquaredError(name='RMSE'),\n        ]\n\n        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    \n    return model\n\nmodel = get_model()","9830b1e6":"model.summary()","2a5f213d":"# Training History","1c3d5f97":"Hello fellow Kagglers,\n\nThis notebook demonstrates how to pretrain the RoBERTa base model on a binary classification problem for simple and normal Wikipedia abstracts.\n\nThe dataset used for this pretraining is the [Simple\/Normal Wikiepdia Abstracts V1](https:\/\/www.kaggle.com\/markwijkhuizen\/simplenormal-wikipedia-abstracts-v1) dataset containing ~250K simple and normal Wikipedia abstract with an equal distribution.\n\n[This](https:\/\/www.kaggle.com\/markwijkhuizen\/simple-normal-wikipedia-abstract-dataset) notebook gives a demonstration on how the dataset was created.\n\nRoBERTa is trained for masked-language modeling, which is predicting a masked word in a sentence. This task does learn RoBERTa to understand language, but has little in common with the CommonLit competition task. Pretraining RoBERTa on an actual readability prediction task should finetune the model for readability related tasks, such as the CommonLit competition.","d56532fe":"# Training","f7b52e68":"# Save pretrained RoBERTa layer","de4a348c":"The linguistic features will not be used in this notebook, but are saved as strings to reduce memory consumption and can be decoded using ```s.split(chr(0))```","db045292":"Thee next function gives an example on how to use the pretrained weights in a model for CommonLit training. It is as simple using the ```load_weights``` function for the RoBERTa layer.","c907ffb3":"# Configuration","025aba76":"Training metrics are shown here, an accuracy of 90\\%+ is achieved!","ba77107a":"# Hardware Configuration","eeaca304":"Training will be split in epochs of 100 steps and the dataset will be iterated once. This means all data will be seen for the first time and the training accuracy can be interpreted as validation accuracy, because no sample will be used for training twice. Training will take roughly 1.5 hours.","3ac83ca9":"# CommonLit Model","48b834a6":"# Dataset","cec88f33":"# Roberta Tokenize","c2999cb2":"# Model"}}