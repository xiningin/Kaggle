{"cell_type":{"d2c58e1e":"code","a649e1c9":"code","5addff82":"code","04d27e97":"code","ef30c105":"code","04d17725":"code","0a18ec33":"code","cfb8a14f":"code","7957f9ac":"code","adcc2502":"code","cf1a013c":"code","760a472c":"code","d80a731b":"code","5eea1b65":"code","04146ad8":"code","4f2ac821":"code","8ae0536b":"code","468921b2":"code","1fa90464":"code","eec5a4ba":"code","e4fe1113":"code","37604996":"code","1f1d7ad3":"code","adf1bc3d":"code","ce23841f":"code","189261a4":"code","19dd46ae":"code","26f0baee":"code","51d6c7b0":"code","fc767414":"code","6649cb67":"code","76987c1e":"code","01ac85ec":"code","fa6ed6f1":"code","395da487":"code","ec2b5f89":"code","f3c8a9cf":"code","83b8bf64":"code","b800ca96":"code","2afc21e7":"code","a44160f1":"code","3437c18a":"code","c85dede6":"code","0f70a5b1":"markdown","feb2fe13":"markdown","42361f17":"markdown","7d859d45":"markdown","082acde0":"markdown","4affd2eb":"markdown","e1edc69c":"markdown","d4e8988f":"markdown"},"source":{"d2c58e1e":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np","a649e1c9":"network = tf.keras.applications.VGG19(include_top=False, weights='imagenet')","5addff82":"network.summary()","04d27e97":"image_content = tf.keras.preprocessing.image.load_img('..\/input\/neural-networks-homer-and-bart-classification\/homer_bart_1\/homer93.bmp')\nplt.imshow(image_content);","ef30c105":"image_content = tf.keras.preprocessing.image.img_to_array(image_content)","04d17725":"image_content.shape","0a18ec33":"image_content.min(), image_content.max()","cfb8a14f":"image_content = image_content \/ 255","7957f9ac":"image_content.min(), image_content.max()","adcc2502":"image_content","cf1a013c":"image_content = image_content[tf.newaxis, :]  \nimage_content.shape ","760a472c":"image_style = tf.keras.preprocessing.image.load_img('..\/input\/styleimage\/tarsila_amaral.jpg')\nplt.imshow(image_style);","d80a731b":"image_style = tf.keras.preprocessing.image.img_to_array(image_style)\nimage_style = image_style \/ 255\nimage_style = image_style[tf.newaxis, :]\nimage_style.shape","5eea1b65":"#image_style = tf.keras.preprocessing.image.load_img('..\/input\/styleimage\/vangogh.jpg')\n#plt.imshow(image_style);","04146ad8":"#image_style = tf.keras.preprocessing.image.img_to_array(image_style)\n#image_style = image_style \/ 255\n#image_style = image_style[tf.newaxis, :]\n#image_style.shape","4f2ac821":"layers_content = ['block4_conv2']  \nlayers_style = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']","8ae0536b":"number_layers_content = len(layers_content)\nnumber_layers_style = len(layers_style)\nprint(number_layers_content, number_layers_style)","468921b2":"network.get_layer('block1_conv1').output","1fa90464":"network.input","eec5a4ba":"for name in layers_style:\n  print(name)","e4fe1113":"def vgg_layers(layer_name):\n  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n  vgg.trainable = False\n  outputs = [vgg.get_layer(name).output for name in layer_name]\n  network = tf.keras.Model(inputs = [vgg.input], outputs = outputs)\n  return network","37604996":"style_extractor = vgg_layers(layers_style)","1f1d7ad3":"style_extractor.summary()","adf1bc3d":"style_extractor.outputs","ce23841f":"style_outputs = style_extractor(image_style)","189261a4":"len(style_outputs)","19dd46ae":"style_outputs[0].shape, style_outputs[1].shape, style_outputs[2].shape, style_outputs[3].shape, style_outputs[4].shape,  ","26f0baee":"def gram_matrix(activations):\n  result = tf.linalg.einsum('bijc,bijd->bcd', activations, activations)\n  input_shape = tf.shape(activations)\n  num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n  return result \/ num_locations","51d6c7b0":"gram_matrix(style_outputs[0])","fc767414":"class StyleContentModel(tf.keras.models.Model):\n  def __init__(self, layers_style, layers_content):\n    super().__init__()\n    self.vgg = vgg_layers(layers_style + layers_content)\n    self.layers_style = layers_style\n    self.layers_content = layers_content\n    self.number_layers_style = len(layers_style)\n    self.vgg.trainable = False\n\n  def call(self, image):\n    image = image * 255.0\n\n    preprocessed_image = tf.keras.applications.vgg19.preprocess_input(image)\n    outputs = self.vgg(preprocessed_image)\n    style_outputs = outputs[:self.number_layers_style] \n    content_outputs = outputs[self.number_layers_style:] \n\n    style_outputs = [gram_matrix(layer) for layer in style_outputs]\n\n    content_dict = {content_name: value for content_name, value in zip(self.layers_content, content_outputs)}\n    dict_style = {style_name: value for style_name, value in zip(self.layers_style, style_outputs)}\n\n    return {'content': content_dict, 'style': dict_style}","6649cb67":"layers_style, layers_content","76987c1e":"extractor = StyleContentModel(layers_style, layers_content)","01ac85ec":"results = extractor(image_content)","fa6ed6f1":"results","395da487":"for key, value in results.items():\n  print(key, value.keys())","ec2b5f89":"goal_style = extractor(image_style)['style']\nobjective_content = extractor(image_content)['content']","f3c8a9cf":"len(goal_style), len(objective_content)","83b8bf64":"new_image = tf.Variable(image_content)","b800ca96":"weight_content = 1\nweight_style = 1000 ","2afc21e7":"optimizer = tf.optimizers.Adam(learning_rate=0.02)","a44160f1":"plt.imshow(tf.squeeze(image_content, axis=0));","3437c18a":"epochs = 3000\nprints = 500\n\nfor epoch in range(epochs):\n  with tf.GradientTape() as tape:    \n    outputs = extractor(new_image)\n\n    content_outputs = outputs['content']\n    style_outputs = outputs['style']\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - objective_content[name]) ** 2) for name in content_outputs.keys()])\n    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - goal_style[name]) ** 2) for name in style_outputs.keys()])\n    total_loss = content_loss * weight_content \/ number_layers_content + style_loss * weight_style \/ number_layers_style\n\n  gradiente = tape.gradient(total_loss, new_image)\n  optimizer.apply_gradients([(gradiente, new_image)])\n  new_image.assign(tf.clip_by_value(new_image, 0.0, 1.0))\n\n  if (epoch + 1) % prints == 0:\n    print('Epoch: ', epoch + 1, ' Content loss: ', content_loss, ' Style loss: ', style_loss, ' Total loss: ', total_loss)\n    plt.imshow(tf.squeeze(new_image, axis=0));\n    plt.show()","c85dede6":"figure, (axis1, axis2, axis3) = plt.subplots(1, 3, figsize=(30,8))\naxis1.imshow(tf.squeeze(image_content, axis = 0))\naxis1.set_title('Content Image')\nplt.axis('off')\naxis2.imshow(tf.squeeze(new_image, axis = 0))\naxis2.set_title('New image')\nplt.axis('off')\naxis3.imshow(tf.squeeze(image_style, axis = 0))\naxis3.set_title('Style Image')\nplt.axis('off');","0f70a5b1":"# **Style Image**","feb2fe13":"# **Construction of the neural network**","42361f17":"# **Training**","7d859d45":"# **Image loading and Processing**","082acde0":"# **Import from Libraries**","4affd2eb":"# **Trained convolutional neural network loading (VGG19)**","e1edc69c":"# **If you find this notebook useful, support with an upvote** \ud83d\udc4d","d4e8988f":"# **Visualization of results**"}}