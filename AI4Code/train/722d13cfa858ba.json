{"cell_type":{"f85dc533":"code","149dda83":"code","41ac2938":"code","0b6b5c02":"code","7c21cce1":"code","54a0a617":"code","9f5efed8":"code","603236fc":"code","56a52a2c":"code","460ab30b":"code","5fd04489":"code","53f29dd1":"markdown","09252d68":"markdown","35d3b43f":"markdown","207f6e46":"markdown","8d3fbafd":"markdown","57247d6a":"markdown","17407dda":"markdown","6b115a4c":"markdown","d12e7e98":"markdown","bdfabb76":"markdown","7a851bae":"markdown"},"source":{"f85dc533":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","149dda83":"# define sigmoid function \ndef sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))\n\nz = np.arange(-7, 7, 0.1)\nphi_z = sigmoid(z)\nplt.plot(z, phi_z)\nplt.axvline(0, color = 'k') # vertical line with 0\nplt.ylim(-0.1, 1.1)\nplt.xlabel('z')\nplt.ylabel('$\\phi (z)$')\n\n# scale and gridlines on the y-axis\nplt.yticks([0, 0.5, 1])\nax = plt.gca()\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.show()","41ac2938":"def cost_1(z):\n    return -np.log(sigmoid(z))\n\ndef cost_0(z):\n    return -np.log(1-sigmoid(z))\n\nz = np.arange(-10, 10, 0.1)\nphi_z = sigmoid(z)\nc1 = [cost_1(x) for x in z]\nplt.plot(phi_z, c1, label = 'J(w) if y = 1')\nc0 = [cost_0(x) for x in z]\nplt.plot(phi_z, c0, label = 'J(w) if y = 0', linestyle = '--')\nplt.ylim(0, 5.1)\nplt.xlim([0, 1])\nplt.xlabel('$\\phi$(z)')\nplt.ylabel('J(w)')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","0b6b5c02":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain.head()","7c21cce1":"# concate dataset(train, test)\ndf = pd.concat(objs = [train, test], axis = 0).reset_index(drop = True)\n\n# fill null-values in Age\nage_by_pclass_sex = df.groupby(['Sex', 'Pclass'])['Age'].median()\ndf['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\n# create Cabin_Initial feature\ndf['Cabin_Initial'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['A', 'B', 'C', 'T'], 'ABCT')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['D', 'E'], 'DE')\ndf['Cabin_Initial'] = df['Cabin_Initial'].replace(['F', 'G'], 'FG')\ndf['Cabin_Initial'].value_counts()\n\n# fill null-values in Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n\n# fill null-values in Fare\ndf['Fare'] = df['Fare'].fillna(df.groupby(['Pclass', 'Embarked'])['Fare'].median()[3]['S'])\n\n\n# binding function for Age and Fare\ndef binding_band(column, binnum):\n    df[column + '_band'] = pd.qcut(df[column].map(int), binnum)\n\n    for i in range(len(df[column + '_band'].value_counts().index)):\n        print('{}_band {} :'.format(column, i), df[column + '_band'].value_counts().index.sort_values(ascending = True)[i])\n        df[column + '_band'] = df[column + '_band'].replace(df[column + '_band'].value_counts().index.sort_values(ascending = True)[i], int(i))\n        \n    df[column + '_band'] = df[column + '_band'].astype(int)    \n    \n    return df.head()\n\nbinding_band('Age',8)\nbinding_band('Fare', 6)\n\n\n# create Initial feature\ndf['Initial'] = 0\nfor i in range(len(df['Name'])):\n    df['Initial'].iloc[i] = df['Name'][i].split(',')[1].split('.')[0].strip()\n\nMrs_Miss_Master = []\nOthers = []\n\nfor i in range(len(df.groupby('Initial')['Survived'].mean().index)):\n    if df.groupby('Initial')['Survived'].mean()[i] > 0.5:\n        Mrs_Miss_Master.append(df.groupby('Initial')['Survived'].mean().index[i])\n    elif df.groupby('Initial')['Survived'].mean().index[i] != 'Mr':\n        Others.append(df.groupby('Initial')['Survived'].mean().index[i])\n    \ndf['Initial'] = df['Initial'].replace(Mrs_Miss_Master, 'Mrs\/Miss\/Master')\ndf['Initial'] = df['Initial'].replace(Others, 'Others')    \n    \n# create Alone feature\ndf['Alone'] = 0\ndf['Alone'].loc[(df['SibSp'] + df['Parch']) == 0] = 1\n\n# create Companinon's survival rate feature\ndf['Ticket_Number'] = df['Ticket'].replace(df['Ticket'].value_counts().index, df['Ticket'].value_counts())\ndf['Family_Size'] = df['Parch'] + df['SibSp'] + 1\ndf['Companion_Survival_Rate'] = 0\nfor i, j in df.groupby(['Family_Size', 'Ticket_Number'])['Survived'].mean().index:\n    df['Companion_Survival_Rate'].loc[(df['Family_Size'] == i) & (df['Ticket_Number'] == j)] = df.groupby(['Family_Size', 'Ticket_Number'])[\"Survived\"].mean()[i, j]\n    \ncomb_sum = df.loc[df['Family_Size'] == 5]['Survived'].sum() + df.loc[df['Ticket_Number'] == 3]['Survived'].sum()\ncomb_counts = df.loc[df['Family_Size'] == 5]['Survived'].count() + df.loc[df['Ticket_Number'] == 3]['Survived'].count()\nmean = comb_sum \/ comb_counts\n\ndf['Companion_Survival_Rate'] = df['Companion_Survival_Rate'].fillna(mean)    \n\n# select categorical features\ncate_col = []\nfor i in [4, 11, 12, 15]:\n    cate_col.append(df.columns[i])\n\ncate_df = pd.get_dummies(df.loc[:,(cate_col)], drop_first = True)\ndf = pd.concat(objs = [df, cate_df], axis = 1).reset_index(drop = True)\n\ndf = df.drop(['Name', 'Sex', 'Age', 'Ticket', 'Fare', 'Embarked', 'Cabin_Initial', 'SibSp', 'Parch',\n              'Cabin', 'Initial', 'Ticket_Number', 'Family_Size'], axis = 1)\n\n# split data\ndf = df.astype(float)\ntrain = df[:891]\ntest = df[891:]\n\ntrain_X = StandardScaler().fit_transform(train.drop(columns = ['PassengerId', 'Survived']))\ntrain_y = train.iloc[:, 1]\ntest_X = StandardScaler().fit_transform(test.drop(columns = ['PassengerId', 'Survived']))","54a0a617":"class LogisticRegressionGD(object):\n    def __init__(self, eta = 0.05, n_iter = 100, random_state = 1):\n        self.eta = eta\n        self.n_iter = n_iter\n        self.random_state = random_state\n    \n    def fit(self, X, y):\n        rgen = np.random.RandomState(self.random_state)\n        self.w_ = rgen.normal(loc = 0, scale = 0.01,\n                             size = 1 + X.shape[1])\n        self.cost_ = []\n        \n        for i in range(self.n_iter):\n            net_input = self.net_input(X)\n            output = self.activation(net_input)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            \n            # logistic cost instead of SSE\n            cost = (-y.dot(np.log(output)) -\n                   ((1 - y).dot(np.log(1 - output))))\n            self.cost_.append(cost)\n        return self\n    \n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n    \n    def activation(self, z):\n        # Sigmoid activation\n        return 1 \/ (1 + np.exp(-np.clip(z, -250, 250)))\n    \n    def predict(self, X):\n        return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)","9f5efed8":"X_train, X_test, y_train, y_test = train_test_split(train_X, train_y,\n                                                   test_size = 0.3, stratify = train_y,\n                                                   random_state = 1)","603236fc":"LRgd = LogisticRegressionGD(eta = 0.05,\n                         n_iter = 1000,\n                         random_state = 1)\nLRgd.fit(train_X, train_y)\ny_pred = LRgd.predict(X_test)\nprint('The accuracy of LogisticRegression Adaline : %.2f' %((y_test == y_pred).sum() \/ len(y_test)))","56a52a2c":"LR = LogisticRegression(solver = 'liblinear', multi_class = 'auto',\n                       C = 100, random_state = 1)\nLR.fit(X_train, y_train)\ny_pred = LR.predict(X_test)\nprint('The accuracy of LogisticRegression Classifier from sklearn : %.2f' % (accuracy_score(y_test, y_pred)))","460ab30b":"# Logistic Regression Adaline\nLRgd = LogisticRegressionGD(eta = 0.05,\n                         n_iter = 1000,\n                         random_state = 1)\nLRgd.fit(train_X, train_y)\npredict = [LRgd.predict(test_X)]\n\n# Sklearn Logistic Regression \nLR = LogisticRegression(solver = 'liblinear', multi_class = 'auto',\n                       C = 100, random_state = 1)\nLR.fit(train_X, train_y)\npredict.append(LR.predict(test_X))\n\nfor i in range(len(predict)):\n    submission = pd.DataFrame(columns = ['PassengerId', 'Survived'])\n    submission['PassengerId'] = df['PassengerId'][891:].map(int)\n    submission['Survived'] = predict[i].astype(int)\n    submission.to_csv('Perceptron_submission_{}.csv'.format(i + 1), header = True, index = False)","5fd04489":"sns.heatmap(df.drop(['PassengerId'], axis = 1).corr(), annot = True, cmap = 'RdYlGn', linewidth = 0.2)\nfig = plt.gcf()\nfig.set_size_inches(15, 10)\nplt.show()","53f29dd1":"The x-axis is the active value of the sigmoid function. The y-axis is the logistic cost. \n\nLooking at the solid line, accurately predicting a sample belonging to class 1 brings the cost close to zero, but the cost of incorrect prediction approaches infinity. Likewise, the exact prediction of samples belonging to class 0 brings the cost closer to zero, but the cost of incorrect prediction approaches infinity. \n\n<b>That is, the advantage of defining the log likelihood function as a cost function is that it imposes a higher cost on the wrong prediction.<\/b>\n\nWe will set log likelihood function as cost function in [Adaline model](https:\/\/www.kaggle.com\/choihanbin\/predict-titanic-survival-by-adaptive-linear-neuron) before I created.","09252d68":"* ## Submission prediction.","35d3b43f":"Feature engineering\nWhen I joined titanic survived prediction competition, I wrote the notebook, [Titanic Survival Prediction(EDA, Ensemble)](https:\/\/www.kaggle.com\/choihanbin\/titanic-survival-prediction-eda-ensemble). So I will pick the features to use based on it.","207f6e46":"### Log likelihood function graph\n- We will use this function as cost function","8d3fbafd":"# Logistic regression principle\n\n\nThis notebook goal:\n\n- Understanding Logistic regression classifier.\n- Trying to predict titanic survived.\n- Checking accuracy of models created without being imported through the library.\n- Comparing adaline model using logistic sigmoid function as activation function and sklearn model.\n\n### You can learn about Logistic regression principle in [my blog](https:\/\/konghana01.tistory.com\/15). ","57247d6a":"### Sklearn Logistic regression model","17407dda":"## Checking the model's accuracy","6b115a4c":"### Sigmoid fuction graph\n- We will use this function as activation function","d12e7e98":"### load the dataset\n","bdfabb76":"<b>\n    \nLogistic Regression Adaline : 0.70813    \n    \nSklearn Logistic Regression : 0.74401\n    \nAdalineBGD : 0.65550\n  \nAdalineSGD : 0.69617\n    \nPerceptron : 0.75358\n\nAdjusted Perceptron : 0.75837\n\nEnsemble : 0.76794 \n    \n<\/b>\n\n\n\n\nWe can see that accuracy of adaline model using log likelihood function as cost function is better than accuracy of adaline model without it. However Logistic regression can has multicollinearity problem. \n\ncorrelationship with features is also very high. Because it is affected by this problem, It's accuracy can be somewhat low.","7a851bae":"### LogisticRegression Adaline model"}}