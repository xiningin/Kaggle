{"cell_type":{"4466cc62":"code","359020c0":"code","89b40471":"code","d5bdf049":"code","6b1de871":"code","262146ae":"code","b40d33da":"code","07b4f457":"code","dd779c9e":"code","27b2c2d2":"code","330c0261":"code","50ff04df":"code","eb165812":"code","41a69d60":"code","2300d148":"code","cd7c86bc":"code","b5b8ee0d":"code","85eae96b":"code","c17d72c0":"code","3b80f114":"code","2182f985":"code","db548485":"code","c95bc437":"code","db3172b1":"code","82ca51f4":"code","0d0e028f":"code","0d32f43b":"code","5cfccc94":"code","fd57791c":"code","a7831c3a":"code","a38871d0":"code","c1b2ebad":"code","662433fd":"code","16d7c36c":"code","bc68aa41":"code","2baaac4a":"code","2dd1ea99":"code","1abca29b":"code","961d02f7":"code","068dc7a2":"code","44fb58d2":"code","019c88bd":"code","2c639335":"code","2a5584d6":"code","4800b049":"code","9dc58fb5":"code","985321d1":"code","990b350e":"code","9584923f":"code","667deba5":"code","7c231841":"code","c21675e3":"code","95aca73b":"code","4d4dcf16":"code","ddd0a190":"code","225dc67f":"code","71800c00":"code","b0856402":"code","25d5d8e3":"code","65be39e5":"code","1c6eed4c":"code","4ed5f06e":"code","98e15925":"code","c9097259":"code","f8ad1f8b":"code","9a49fe52":"code","3d95e898":"code","6e26760f":"code","602426f8":"code","f142ce60":"code","e90301dc":"code","9b0dfbd6":"code","5094e19e":"code","d1deb87b":"code","467dca63":"code","0a322140":"code","7fabc4ab":"code","199fc57f":"code","02ede081":"code","e64eb690":"code","43b2f135":"code","0c17995f":"code","7f4cc4ac":"code","244924b2":"code","90147cf3":"code","9ac98fbc":"code","c96d5356":"code","5d7d38ac":"code","3cc8783d":"code","07718c10":"code","51cf8d6f":"code","f4b0419b":"code","24fa7aa6":"markdown","a7c1390e":"markdown","b1f4b17c":"markdown","fa98ba53":"markdown","51863612":"markdown","0405b732":"markdown","da208579":"markdown","6644af27":"markdown","e4f568c8":"markdown","4c1a78ac":"markdown","fe8bccfb":"markdown","826aa519":"markdown","7a1458c7":"markdown","16ead1e8":"markdown","7f0bc026":"markdown"},"source":{"4466cc62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","359020c0":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom shutil import copyfile\nfrom shutil import move\nfrom tensorflow.keras.utils import plot_model\n%matplotlib inline\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport shutil","89b40471":"len(os.listdir('..\/input\/dog-breed-identification\/test'))","d5bdf049":"df = pd.read_csv('..\/input\/dog-breed-identification\/labels.csv')","6b1de871":"breeds = df.groupby('breed')","262146ae":"os.mkdir('dogbreeds_in')\nos.mkdir('dogbreeds_in\/train')\nos.mkdir('dogbreeds_in\/validation')","b40d33da":"split_size = 0.7","07b4f457":"for breed,group in breeds:\n    files = list(group['id'])\n    ln = len(files)\n    train_p = 'dogbreeds\/train\/'+breed\n    val_p = 'dogbreeds\/validation\/'+breed\n    os.mkdir(train_p)\n    os.mkdir(val_p)\n    random.shuffle(files)\n    for ind,fn in enumerate(files):\n        fn_path = '..\/input\/dog-breed-identification\/train\/' + fn + '.jpg'\n        if ind<split_size*ln:\n            tar_path = train_p +'\/'+ fn + '.jpg'\n            copyfile(fn_path,tar_path)\n        else:\n            tar_path = val_p  +'\/'+ fn + '.jpg'\n            copyfile(fn_path,tar_path)","dd779c9e":"train_dir = '.\/dogbreeds_in\/train'\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    fill_mode = 'nearest' \n\n)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size = (331,331),\n    batch_size = 64,\n    class_mode = 'categorical',\n    shuffle = True\n)","27b2c2d2":"val_dir = '.\/dogbreeds_in\/validation'\n\nval_datagen = ImageDataGenerator()\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size = (331,331),\n    batch_size = 64,\n    class_mode = 'categorical'\n)","330c0261":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input as inceptpp","50ff04df":"dnn_model = InceptionV3(\n      input_shape=(331,331,3),\n      include_top = False,\n      weights = 'imagenet'\n)\n\nfor layer in dnn_model.layers:\n    layer.trainable = False","eb165812":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Lambda(inceptpp,input_shape=(331,331,3)),\n    dnn_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(120,activation='softmax')\n])","41a69d60":"model.summary()","2300d148":"plot_model(model,show_shapes=True)","cd7c86bc":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","b5b8ee0d":"history = model.fit(\n    train_generator,\n    epochs = 10,\n    steps_per_epoch = 7213\/\/64,\n    validation_data = val_generator,\n    validation_steps = 3009\/\/64\n)","85eae96b":"\n%matplotlib inline\n\nimport matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\nplt.plot(epochs, acc, 'r', \"Training Accuracy\")\nplt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\nplt.title('Training and validation accuracy')\nplt.figure()\n\nplt.plot(epochs, loss, 'r', \"Training Loss\")\nplt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n\n\nplt.title('Training and validation loss')","c17d72c0":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input as inceptpp\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input as vggpp\nfrom tensorflow.keras.applications.resnet import ResNet101\nfrom tensorflow.keras.applications.resnet import preprocess_input as respp\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.xception import preprocess_input as xceptpp\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as inceptrespp","3b80f114":"def pretrained_model_prediction(MODEL,Pre,data):\n  dnn_model = MODEL(\n      input_shape=(300,300,3),\n      include_top = False,\n      weights = 'imagenet'\n  )\n  for layer in dnn_model.layers:\n    layer.trainable = False\n  x = data\n  x = tf.keras.layers.Lambda(Pre)(x)\n  x = dnn_model(x)\n  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  return x","2182f985":"inputs = tf.keras.layers.Input((300,300,3))\ninception_prediction = pretrained_model_prediction(InceptionV3,inceptpp,inputs)\nvgg_prediction = pretrained_model_prediction(VGG19,vggpp,inputs)\nxception_prediction = pretrained_model_prediction(Xception,xceptpp,inputs)\nresnet_prediction = pretrained_model_prediction(ResNet101,respp,inputs)\ninceptionres_prediction = pretrained_model_prediction(InceptionResNetV2,inceptrespp,inputs)\nnew_features = tf.keras.layers.Concatenate()([inception_prediction,vgg_prediction,xception_prediction,resnet_prediction,inceptionres_prediction])\nconcatenated_pretrained_model = tf.keras.Model(inputs,new_features)\n","db548485":"concatenated_pretrained_model.summary()","c95bc437":"plot_model(concatenated_pretrained_model,show_shapes=True)","db3172b1":"model = tf.keras.models.Sequential([\n    concatenated_pretrained_model,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(120,activation='softmax')\n])","82ca51f4":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","0d0e028f":"model.summary()","0d32f43b":"plot_model(model,show_shapes=True)","5cfccc94":"history = model.fit(\n    train_generator,\n    epochs = 10,\n    steps_per_epoch = 7213\/\/64,\n    validation_data = val_generator,\n    validation_steps = 3009\/\/64\n)","fd57791c":"acc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\nplt.plot(epochs, acc, 'r', \"Training Accuracy\")\nplt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\nplt.title('Training and validation accuracy')\nplt.figure()\n\nplt.plot(epochs, loss, 'r', \"Training Loss\")\nplt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n\n\nplt.title('Training and validation loss')","a7831c3a":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input as inceptpp\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input as vggpp\nfrom tensorflow.keras.applications.resnet import ResNet101\nfrom tensorflow.keras.applications.resnet import preprocess_input as respp\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.xception import preprocess_input as xceptpp\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as inceptrespp\nfrom tensorflow.keras.applications.nasnet import NASNetLarge\nfrom tensorflow.keras.applications.nasnet import preprocess_input as naspp\nfrom tensorflow.keras.applications.densenet import DenseNet201\nfrom tensorflow.keras.applications.densenet import preprocess_input as densepp\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB7\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as effpp\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobpp","a38871d0":"def pretrained_model_prediction(MODEL,Pre,data):\n  dnn_model = MODEL(\n      input_shape=(331,331,3),\n      include_top = False,\n      weights = 'imagenet'\n  )\n  for layer in dnn_model.layers:\n    layer.trainable = False\n  x = data\n  x = tf.keras.layers.Lambda(Pre)(x)\n  x = dnn_model(x)\n  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  return x","c1b2ebad":"inputs = tf.keras.layers.Input((331,331,3))\ninception_prediction = pretrained_model_prediction(InceptionV3,inceptpp,inputs)\nvgg_prediction = pretrained_model_prediction(VGG19,vggpp,inputs)\nxception_prediction = pretrained_model_prediction(Xception,xceptpp,inputs)\nresnet_prediction = pretrained_model_prediction(ResNet101,respp,inputs)\ninceptionres_prediction = pretrained_model_prediction(InceptionResNetV2,inceptrespp,inputs)\nnasnet_prediction = pretrained_model_prediction(NASNetLarge,naspp,inputs)\ndensenet_prediction = pretrained_model_prediction(DenseNet201,densepp,inputs)\nefficient_prediction = pretrained_model_prediction(EfficientNetB7,effpp,inputs)\nmobile_prediction = pretrained_model_prediction(MobileNetV2,mobpp,inputs)\nnew_features = tf.keras.layers.Concatenate()([inception_prediction,vgg_prediction,xception_prediction,resnet_prediction,inceptionres_prediction,\n                                              nasnet_prediction,densenet_prediction,efficient_prediction,mobile_prediction])\nconcatenated_pretrained_model = tf.keras.Model(inputs,new_features)\n","662433fd":"concatenated_pretrained_model.summary()","16d7c36c":"plot_model(concatenated_pretrained_model,show_shapes=True)","bc68aa41":"model = tf.keras.Sequential([\n                         concatenated_pretrained_model,\n                         tf.keras.layers.Dense(120,activation='softmax')    \n])","2baaac4a":"model.summary()","2dd1ea99":"plot_model(model,show_shapes = True)","1abca29b":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","961d02f7":"history = model.fit(\n    train_generator,\n    epochs = 10,\n    steps_per_epoch = 7213\/\/64,\n    validation_data = val_generator,\n    validation_steps = 3009\/\/64\n)","068dc7a2":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if os.stat(os.path.join(dirname, filename)).st_size == 0:\n            print(os.path.join(dirname, filename))","44fb58d2":"fldr = os.listdir('..\/input\/stanford-dogs-dataset\/images\/Images')","019c88bd":"fldr[0][10:]","2c639335":"df = pd.read_csv('..\/input\/dog-breed-identification\/labels.csv')","2a5584d6":"breeds = df.groupby('breed')","4800b049":"shutil.rmtree('.\/dogbreeds')","9dc58fb5":"os.mkdir('dogbreeds')\nos.mkdir('dogbreeds\/everything')","985321d1":"for breed,group in breeds:\n    files = list(group['id'])\n    ln = len(files)\n    train_p = 'dogbreeds\/everything\/'+breed\n    os.mkdir(train_p)\n    for ind,fn in enumerate(files):\n        fn_path = '..\/input\/dog-breed-identification\/train\/' + fn + '.jpg'\n        tar_path = train_p +'\/'+ fn + '.jpg'\n        copyfile(fn_path,tar_path)","990b350e":"os.mkdir('.\/dogbreeds\/train')","9584923f":"for bre in fldr:\n    newnm = bre[10:].lower()\n    newfldr = os.path.join('.\/dogbreeds\/train',newnm)\n    os.mkdir(newfldr)\n    oldpth = os.path.join('..\/input\/stanford-dogs-dataset\/images\/Images',bre)\n    for f in os.listdir(oldpth):\n        filepth = os.path.join(oldpth,f)\n        newfilepth = os.path.join(newfldr,f)\n        copyfile(filepth,newfilepth)","667deba5":"all_breeds = os.listdir('.\/dogbreeds\/train')\nlen_dir = []\nfor dog_dir in all_breeds:\n    tmp = os.path.join('.\/dogbreeds\/train',dog_dir)\n    len_dir.append(len(os.listdir(tmp)))","7c231841":"len_dir = np.array(len_dir)","c21675e3":"max_at = np.argmax(len_dir)\nprint(\"max number of files is for : \",all_breeds[max_at],len_dir[max_at])","95aca73b":"min_at = np.argmin(len_dir)\nprint(\"max number of files is for : \",all_breeds[min_at],len_dir[min_at])","4d4dcf16":"print(\"Previous data we had for :\",all_breeds[min_at],\"is\",len(os.listdir(os.path.join('.\/dogbreeds\/everything',all_breeds[min_at]))))","ddd0a190":"all_breeds = os.listdir('.\/dogbreeds\/everything')\nlen_dir = []\nfor dog_dir in all_breeds:\n    tmp = os.path.join('.\/dogbreeds\/everything',dog_dir)\n    len_dir.append(len(os.listdir(tmp)))","225dc67f":"len_dir = np.array(len_dir)\nmin_at = np.argmin(len_dir)\nprint(\"max number of files is for : \",all_breeds[min_at],len_dir[min_at])","71800c00":"print(\"Previous data we had for :\",all_breeds[min_at],\"is\",len(os.listdir(os.path.join('.\/dogbreeds\/train',all_breeds[min_at]))))","b0856402":"target_train_size = 210","25d5d8e3":"os.mkdir('.\/dogbreeds\/validation')","65be39e5":"for breed in os.listdir('.\/dogbreeds\/everything'):\n    dog_train = os.path.join('.\/dogbreeds\/train',breed)\n    dog_val = os.path.join('.\/dogbreeds\/validation',breed)\n    os.mkdir(dog_val)\n    dog_every = os.path.join('.\/dogbreeds\/everything',breed)\n    initial_length = len(os.listdir(dog_train))\n    for dogs in os.listdir(dog_every):\n        f_path = os.path.join(dog_every,dogs)\n        if initial_length<target_train_size:\n            dest_path = os.path.join(dog_train,dogs)\n            copyfile(f_path,dest_path)\n        else:\n            dest_path = os.path.join(dog_val,dogs)\n            copyfile(f_path,dest_path)\n        initial_length +=1","1c6eed4c":"all_breeds = os.listdir('.\/dogbreeds\/train')\nlen_dir = []\nfor dog_dir in all_breeds:\n    tmp = os.path.join('.\/dogbreeds\/train',dog_dir)\n    len_dir.append(len(os.listdir(tmp)))","4ed5f06e":"print(max(len_dir))","98e15925":"print(sum(len_dir))","c9097259":"train_breed = {}\nfor i in len_dir:\n    train_breed.setdefault(i,0)\n    train_breed[i]+=1\nprint(train_breed)","f8ad1f8b":"all_breeds = os.listdir('.\/dogbreeds\/validation')\nlen_dir = []\nfor dog_dir in all_breeds:\n    tmp = os.path.join('.\/dogbreeds\/validation',dog_dir)\n    len_dir.append(len(os.listdir(tmp)))","9a49fe52":"print(max(len_dir))","3d95e898":"print(sum(len_dir))","6e26760f":"train_dir = '.\/dogbreeds\/train'\n\ntrain_datagen = ImageDataGenerator()\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size = (331,331),\n    batch_size = 64,\n    class_mode = 'categorical',\n    shuffle = True\n)","602426f8":"val_dir = '.\/dogbreeds\/validation'\n\nval_datagen = ImageDataGenerator()\n\nval_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size = (331,331),\n    batch_size = 64,\n    class_mode = 'categorical'\n)","f142ce60":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input as inceptpp\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input as vggpp\nfrom tensorflow.keras.applications.resnet import ResNet101\nfrom tensorflow.keras.applications.resnet import preprocess_input as respp\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications.xception import preprocess_input as xceptpp\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.applications.inception_resnet_v2 import preprocess_input as inceptrespp\nfrom tensorflow.keras.applications.nasnet import NASNetLarge\nfrom tensorflow.keras.applications.nasnet import preprocess_input as naspp\nfrom tensorflow.keras.applications.densenet import DenseNet201\nfrom tensorflow.keras.applications.densenet import preprocess_input as densepp\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB7\nfrom tensorflow.keras.applications.efficientnet import preprocess_input as effpp\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobpp","e90301dc":"def pretrained_model_prediction(MODEL,Pre,data):\n  dnn_model = MODEL(\n      input_shape=(331,331,3),\n      include_top = False,\n      weights = 'imagenet'\n  )\n  for layer in dnn_model.layers:\n    layer.trainable = False\n  x = data\n  x = tf.keras.layers.Lambda(Pre)(x)\n  x = dnn_model(x)\n  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  return x","9b0dfbd6":"inputs = tf.keras.layers.Input((331,331,3))\ninception_prediction = pretrained_model_prediction(InceptionV3,inceptpp,inputs)\nvgg_prediction = pretrained_model_prediction(VGG19,vggpp,inputs)\nxception_prediction = pretrained_model_prediction(Xception,xceptpp,inputs)\nresnet_prediction = pretrained_model_prediction(ResNet101,respp,inputs)\ninceptionres_prediction = pretrained_model_prediction(InceptionResNetV2,inceptrespp,inputs)\nnasnet_prediction = pretrained_model_prediction(NASNetLarge,naspp,inputs)\ndensenet_prediction = pretrained_model_prediction(DenseNet201,densepp,inputs)\nefficient_prediction = pretrained_model_prediction(EfficientNetB7,effpp,inputs)\nmobile_prediction = pretrained_model_prediction(MobileNetV2,mobpp,inputs)\nnew_features = tf.keras.layers.Concatenate()([inception_prediction,vgg_prediction,xception_prediction,resnet_prediction,inceptionres_prediction,\n                                              nasnet_prediction,densenet_prediction,efficient_prediction,mobile_prediction])\nconcatenated_pretrained_model = tf.keras.Model(inputs,new_features)\n","5094e19e":"concatenated_pretrained_model.summary()","d1deb87b":"plot_model(concatenated_pretrained_model,show_shapes=True)","467dca63":"model = tf.keras.Sequential([\n                         concatenated_pretrained_model,\n                         tf.keras.layers.Dense(120,activation='softmax')    \n])","0a322140":"model.summary()","7fabc4ab":"plot_model(model,show_shapes = True)","199fc57f":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","02ede081":"history = model.fit(\n    train_generator,\n    epochs = 10,\n    steps_per_epoch = 25333\/\/64,\n    validation_data = val_generator,\n    validation_steps = 5469\/\/64\n)","e64eb690":"train_generator.class_indices","43b2f135":"cols = ['id']\ntmp2 = train_generator.class_indices.keys()","0c17995f":"cols.extend(tmp2)","7f4cc4ac":"cols","244924b2":"test_path = '..\/input\/dog-breed-identification\/test'","90147cf3":"import tqdm as tq","9ac98fbc":"from keras.preprocessing import image\nall_labels = []\nfor fn in tq.tqdm(os.listdir(test_path)):\n    file_path = os.path.join(test_path,fn)\n    tmp = []\n    tmp.append(fn)\n    img = image.load_img(file_path, target_size=(300,300))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    classes = model.predict(x)\n    classes = list(classes)\n    tmp.extend(classes)\n    all_labels.append(tmp)","c96d5356":"len(all_labels)","5d7d38ac":"len(all_labels[0])","3cc8783d":"all_labels[0]\n","07718c10":"model.summary()","51cf8d6f":"history = model.fit(\n    train_generator,\n    epochs = 5,\n    steps_per_epoch = 25333\/\/64,\n    validation_data = val_generator,\n    validation_steps = 5469\/\/64\n)","f4b0419b":"history = model.fit(\n    train_generator,\n    epochs = 1,\n    steps_per_epoch = 25333\/\/64,\n    validation_data = val_generator,\n    validation_steps = 5469\/\/64\n)","24fa7aa6":"In this subsection we devide the train data into train and validation. We also made a directory graph so that the ImageDataGenerator can use it to distrubte the data in it's various classes ","a7c1390e":"# Making Directory Flowchart","b1f4b17c":"It is in sorted manner","fa98ba53":"We generate data in a batch size of 64 and use image augmentation on the test data","51863612":"From initial analysis of the dataset we got to know that the test contains images with the name of the image as its id and the file labels.csv contains the classes related to each file.\nWe also get that the dataset is unbalanced","0405b732":"# Training Using Pretrained Models","da208579":"# Datagenerator","6644af27":"# Importing Modules","e4f568c8":"## Single Trained DeepNN","4c1a78ac":"# Generating Databatches for Training","fe8bccfb":"# Understanding Dataset","826aa519":"# Using our 9 Ensembled Model","7a1458c7":"# Adding More Dataset for Training","16ead1e8":"# Ensembling 9 Models","7f0bc026":"## Ensembling 5 Trained Deep NN"}}