{"cell_type":{"21df94d5":"code","4aa023b2":"code","632bd8fb":"code","2b3ec3b6":"code","9a40bfed":"code","b480aeda":"code","fe442ec7":"code","496f9e2c":"code","54c1ef9b":"code","d9f7bbce":"code","ff5e48fb":"code","b25b52d1":"code","db33c11a":"code","9bf51c54":"code","43e08985":"code","b000c05f":"code","a7ac762e":"code","fa86ff2d":"code","55e08f2f":"code","421838ef":"code","3204a665":"code","f4315903":"code","bde70512":"code","488ec106":"code","dc95aca4":"code","2ae5f52e":"code","dc15237a":"code","a0239516":"code","fa658ecc":"code","a1f9b984":"code","059c879a":"code","fac530e6":"code","8a4997a0":"code","b6fadfcb":"code","823bc726":"code","e9a54463":"code","ecb0571b":"code","ac5ca152":"code","f7a92040":"code","3b2d3521":"code","3cb5c346":"code","421de29c":"code","8f194866":"code","9a62b43d":"code","86fb2ed6":"code","4dbfd2ad":"code","256a45c5":"code","b3c0823c":"code","b5973c1b":"code","952c473a":"code","ed26c75f":"code","2a23f9e6":"code","83737280":"code","fb459935":"markdown","8ce77ba5":"markdown","e672ea8d":"markdown","bae7f08b":"markdown","fab1ef17":"markdown"},"source":{"21df94d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4aa023b2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom datetime import datetime\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nimport gc","632bd8fb":"import keras\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Activation, BatchNormalization, LeakyReLU\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\nfrom tensorflow.keras.models import load_model","2b3ec3b6":"# random seed fixed\nnp.random.seed(5)\ntf.compat.v1.set_random_seed(5)","9a40bfed":"# Read training data\ntrain = pd.read_csv('\/kaggle\/input\/dacon-stage-2\/train.csv', index_col='id')\ntrain.tail(2)","b480aeda":"# Read test data\ntest = pd.read_csv('\/kaggle\/input\/dacon-stage-2\/test.csv', index_col='id')\ntest.tail(2)","fe442ec7":"train.shape, test.shape","496f9e2c":"train.describe().T","54c1ef9b":"test.describe().T","d9f7bbce":"train.groupby('type').mean().T","ff5e48fb":"train['fiberID'].value_counts()","b25b52d1":"test['fiberID'].value_counts()","db33c11a":"plt.figure(figsize=(30, 7))\nsns.countplot(x=train['type'])","9bf51c54":"plt.figure(figsize=(30, 7))\nsns.boxplot(train['type'], train['fiberID'])","43e08985":"plt.figure(figsize=(25, 7))\nsns.boxplot(train['type'], train['petroMag_z'])","b000c05f":"plt.figure(figsize=(25, 7))\nsns.boxplot(train['type'], train['psfMag_g'])","a7ac762e":"plt.figure(figsize=(10, 8))\nsns.heatmap(train.corr())","fa86ff2d":"def draw_types(n=6, regex='psf'):\n    labels = train['type'].value_counts().index.tolist()[:n]\n    columns = train.filter(regex=regex).columns.tolist()\n    colors = ['violet', 'green', 'red', 'cyan', 'yellow']\n    waves = [column[-1:] for column in columns]\n\n    fig, axes = plt.subplots(int(n\/2), 2, figsize=(10,n), dpi=100)\n    w = 1.5\n    for i, label in enumerate(labels):\n        for column, color, wave in zip(columns, colors, waves):\n            q1 = train.loc[train['type'] == label, column].quantile(0.25)\n            q3 = train.loc[train['type'] == label, column].quantile(0.75)\n            iqr = q3 - q1\n            lower_bound = q1 - (w * iqr)\n            upper_bound = q3 + (w * iqr)\n            mask = (train.loc[train['type'] == label, column] >= lower_bound)\n            mask = (train.loc[train['type'] == label, column] <= upper_bound)\n            data = train.loc[train['type'] == label, column].loc[mask]\n\n            sns.distplot(data, hist=False, color=color, kde_kws={'shade': True}, \n                         label=wave, ax=axes.flat[i])\n\n        axes.flat[i].set_title(label)\n        axes.flat[i].set_xlabel('')\n        axes.flat[i].grid(axis='x', linestyle='--')\n        axes.flat[i].legend(frameon=True, framealpha=1, shadow=False, \n                            fancybox=False, edgecolor='black')\n\n    fig.tight_layout()\n    plt.show()","55e08f2f":"draw_types(n=4, regex='psf')","421838ef":"draw_types(n=4, regex='fiberMag')","3204a665":"draw_types(n=4, regex='petroMag')","f4315903":"draw_types(n=4, regex='modelMag')","bde70512":"# Adjust training data range\ntrain = train[train['psfMag_u'] < 40]\ntrain = train[train['psfMag_g'] < 185]\ntrain = train[train['psfMag_r'] < 35]\ntrain = train[train['psfMag_i'] < 50]\ntrain = train[train['psfMag_z'] < 35]\ntrain = train[train['fiberMag_u'] < 45]\ntrain = train[train['fiberMag_g'] < 50]\ntrain = train[train['fiberMag_r'] < 30]\ntrain = train[train['fiberMag_i'] < 35]\ntrain = train[train['fiberMag_z'] < 30]\ntrain = train[train['petroMag_u'] < 70]\ntrain = train[train['petroMag_g'] < 110]\ntrain = train[train['petroMag_r'] < 45]\ntrain = train[train['petroMag_i'] < 55]\ntrain = train[train['petroMag_z'] < 75]\ntrain = train[train['modelMag_u'] < 35]\ntrain = train[train['modelMag_g'] < 30]\ntrain = train[train['modelMag_r'] < 30]\ntrain = train[train['modelMag_i'] < 30]\ntrain = train[train['modelMag_z'] < 25]\n\ntrain = train[train['psfMag_u'] > -10]\ntrain = train[train['psfMag_g'] > -45]\ntrain = train[train['psfMag_r'] > 5]\ntrain = train[train['psfMag_i'] > -25]\ntrain = train[train['psfMag_z'] > 10]\ntrain = train[train['fiberMag_u'] > 5]\ntrain = train[train['fiberMag_g'] > 5]\ntrain = train[train['fiberMag_r'] > 10]\ntrain = train[train['fiberMag_i'] > 10]\ntrain = train[train['fiberMag_z'] > -10]\ntrain = train[train['petroMag_u'] > -100]\ntrain = train[train['petroMag_g'] > -1350]\ntrain = train[train['petroMag_r'] > -25]\ntrain = train[train['petroMag_i'] > -10]\ntrain = train[train['petroMag_z'] > -70]\ntrain = train[train['modelMag_u'] > 10]\ntrain = train[train['modelMag_g'] > 10]\ntrain = train[train['modelMag_r'] > 10]\ntrain = train[train['modelMag_i'] > 10]\ntrain = train[train['modelMag_z'] > 10]\n\ntrain.shape","488ec106":"def col_mean(u, g, r, i, z):\n    return np.mean([u, g, r, i, z])\ndef col_median(u, g, r, i, z):\n    return np.median([u, g, r, i, z])\ndef col_std(u, g, r, i, z):\n    return np.std([u, g, r, i, z])\ndef col_sum(u, g, r, i, z):\n    return np.sum([u, g, r, i, z])   \n\n# train['psfMag_mean'] = train.apply(lambda x : col_mean(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n# train['psfMag_median'] = train.apply(lambda x : col_median(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n# train['psfMag_std'] = train.apply(lambda x : col_std(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n# train['psfMag_sum'] = train.apply(lambda x : col_sum(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n\n# train['fiberMag_mean'] = train.apply(lambda x : col_mean(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n# train['fiberMag_median'] = train.apply(lambda x : col_median(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n# train['fiberMag_std'] = train.apply(lambda x : col_std(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n# train['fiberMag_sum'] = train.apply(lambda x : col_sum(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n\n# train['petroMag_mean'] = train.apply(lambda x : col_mean(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n# train['petroMag_median'] = train.apply(lambda x : col_median(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n# train['petroMag_std'] = train.apply(lambda x : col_std(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n# train['petroMag_sum'] = train.apply(lambda x : col_sum(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n\ntrain['modelMag_mean'] = train.apply(lambda x : col_mean(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\ntrain['modelMag_median'] = train.apply(lambda x : col_median(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\n# train['modelMag_std'] = train.apply(lambda x : col_std(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\ntrain['modelMag_sum'] = train.apply(lambda x : col_sum(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\n\ntrain.head(2)","dc95aca4":"# test['psfMag_mean'] = test.apply(lambda x : col_mean(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n# test['psfMag_median'] = test.apply(lambda x : col_median(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n# test['psfMag_std'] = test.apply(lambda x : col_std(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n# test['psfMag_sum'] = test.apply(lambda x : col_sum(x['psfMag_u'], x['psfMag_g'], x['psfMag_r'], x['psfMag_i'], x['psfMag_z']), axis=1)\n\n# test['fiberMag_mean'] = test.apply(lambda x : col_mean(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n# test['fiberMag_median'] = test.apply(lambda x : col_median(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n# test['fiberMag_std'] = test.apply(lambda x : col_std(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n# test['fiberMag_sum'] = test.apply(lambda x : col_sum(x['fiberMag_u'], x['fiberMag_g'], x['fiberMag_r'], x['fiberMag_i'], x['fiberMag_z']), axis=1)\n\n# test['petroMag_mean'] = test.apply(lambda x : col_mean(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n# test['petroMag_median'] = test.apply(lambda x : col_median(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n# test['petroMag_std'] = test.apply(lambda x : col_std(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n# test['petroMag_sum'] = test.apply(lambda x : col_sum(x['petroMag_u'], x['petroMag_g'], x['petroMag_r'], x['petroMag_i'], x['petroMag_z']), axis=1)\n\ntest['modelMag_mean'] = test.apply(lambda x : col_mean(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\ntest['modelMag_median'] = test.apply(lambda x : col_median(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\n# test['modelMag_std'] = test.apply(lambda x : col_std(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\ntest['modelMag_sum'] = test.apply(lambda x : col_sum(x['modelMag_u'], x['modelMag_g'], x['modelMag_r'], x['modelMag_i'], x['modelMag_z']), axis=1)\n\ntest.head(2)","2ae5f52e":"def ugri_mean(u, g, r, i):\n    return np.mean([u, g, r, i])\ndef ugri_max(u, g, r, i):\n    return np.max([u, g, r, i])\ndef ugri_min(u, g, r, i):\n    return np.min([u, g, r, i])\n\ntrain['i_mean'] = train.apply(lambda x : ugri_mean(x['psfMag_i'], x['fiberMag_i'], \n                                                                 x['petroMag_i'], x['modelMag_i']), axis=1)\n\ntrain['g_max'] = train.apply(lambda x : ugri_max(x['psfMag_g'], x['fiberMag_g'], \n                                                                 x['petroMag_g'], x['modelMag_g']), axis=1)\n\ntrain['r_max'] = train.apply(lambda x : ugri_max(x['psfMag_r'], x['fiberMag_r'], \n                                                                 x['petroMag_r'], x['modelMag_r']), axis=1)\n\ntrain['g_min'] = train.apply(lambda x : ugri_min(x['psfMag_g'], x['fiberMag_g'], \n                                                                 x['petroMag_g'], x['modelMag_g']), axis=1)\n\ntrain.head(1)","dc15237a":"test['i_mean'] = test.apply(lambda x : ugri_mean(x['psfMag_i'], x['fiberMag_i'], \n                                                                 x['petroMag_i'], x['modelMag_i']), axis=1)\n\ntest['g_max'] = test.apply(lambda x : ugri_max(x['psfMag_g'], x['fiberMag_g'], \n                                                                 x['petroMag_g'], x['modelMag_g']), axis=1)\n\ntest['r_max'] = test.apply(lambda x : ugri_max(x['psfMag_r'], x['fiberMag_r'], \n                                                                 x['petroMag_r'], x['modelMag_r']), axis=1)\n\ntest['g_min'] = test.apply(lambda x : ugri_min(x['psfMag_g'], x['fiberMag_g'], \n                                                                 x['petroMag_g'], x['modelMag_g']), axis=1)\n\ntest.head(1)","a0239516":"# did not use these features.\n# # Statistics and distance based features \n# gb = train.groupby('fiberID', as_index=False).agg({'fiberMag_u': {'fiberMag_u_max_fiberID': np.max, 'fiberMag_u_min_fiberID': np.min, \n#                                                                   'fiberMag_u_mean_fiberID' : np.mean, 'fiberMag_u_median_fiberID' : np.median,\n#                                                                   'fiberMag_u_std_fiberID' : np.std},\n#                                                    'fiberMag_g': {'fiberMag_g_max_fiberID': np.max, 'fiberMag_g_min_fiberID': np.min, \n#                                                                   'fiberMag_g_mean_fiberID' : np.mean, 'fiberMag_g_median_fiberID' : np.median,\n#                                                                   'fiberMag_g_std_fiberID' : np.std},\n#                                                    'fiberMag_r': {'fiberMag_r_max_fiberID': np.max, 'fiberMag_r_min_fiberID': np.min, \n#                                                                   'fiberMag_r_mean_fiberID' : np.mean, 'fiberMag_r_median_fiberID' : np.median,\n#                                                                   'fiberMag_r_std_fiberID' : np.std},\n#                                                    'fiberMag_i': {'fiberMag_i_max_fiberID': np.max, 'fiberMag_i_min_fiberID': np.min, \n#                                                                   'fiberMag_i_mean_fiberID' : np.mean, 'fiberMag_i_median_fiberID' : np.median,\n#                                                                   'fiberMag_i_std_fiberID' : np.std},\n#                                                    'fiberMag_z': {'fiberMag_z_max_fiberID': np.max, 'fiberMag_z_min_fiberID': np.min, \n#                                                                   'fiberMag_z_mean_fiberID' : np.mean, 'fiberMag_z_median_fiberID' : np.median,\n#                                                                   'fiberMag_z_std_fiberID' : np.std},\n#                                                    'fiberMag_mean' : {'fiberMag_mean_max_fiberID': np.max, 'fiberMag_mean_min_fiberID': np.min, \n#                                                                   'fiberMag_mean_mean_fiberID' : np.mean, 'fiberMag_mean_median_fiberID' : np.median,\n#                                                                   'fiberMag_mean_std_fiberID' : np.std},\n#                                                    'fiberMag_median' : {'fiberMag_median_max_fiberID': np.max, 'fiberMag_median_min_fiberID': np.min, \n#                                                                   'fiberMag_median_mean_fiberID' : np.mean, 'fiberMag_median_median_fiberID' : np.median,\n#                                                                   'fiberMag_median_std_fiberID' : np.std},\n#                                                    'fiberMag_std' : {'fiberMag_std_max_fiberID': np.max, 'fiberMag_std_min_fiberID': np.min, \n#                                                                   'fiberMag_std_mean_fiberID' : np.mean, 'fiberMag_std_median_fiberID' : np.median,\n#                                                                   'fiberMag_std_std_fiberID' : np.std},\n#                                                    'fiberMag_sum' : {'fiberMag_sum_max_fiberID': np.max, 'fiberMag_sum_min_fiberID': np.min, \n#                                                                   'fiberMag_sum_mean_fiberID' : np.mean, 'fiberMag_sum_median_fiberID' : np.median,\n#                                                                   'fiberMag_sum_std_fiberID' : np.std}\n#                                                    })\n\n# gb.columns = ['fiberID', 'fiberMag_u_max_fiberID', 'fiberMag_u_min_fiberID', 'fiberMag_u_mean_fiberID', 'fiberMag_u_median_fiberID', 'fiberMag_u_std_fiberID',\n#               'fiberMag_g_max_fiberID', 'fiberMag_g_min_fiberID', 'fiberMag_g_mean_fiberID', 'fiberMag_g_median_fiberID', 'fiberMag_g_std_fiberID',\n#               'fiberMag_r_max_fiberID', 'fiberMag_r_min_fiberID', 'fiberMag_r_mean_fiberID', 'fiberMag_r_median_fiberID', 'fiberMag_r_std_fiberID',\n#               'fiberMag_i_max_fiberID', 'fiberMag_i_min_fiberID', 'fiberMag_i_mean_fiberID', 'fiberMag_i_median_fiberID', 'fiberMag_i_std_fiberID',\n#               'fiberMag_z_max_fiberID', 'fiberMag_z_min_fiberID', 'fiberMag_z_mean_fiberID', 'fiberMag_z_median_fiberID', 'fiberMag_z_std_fiberID',\n#               'fiberMag_mean_max_fiberID', 'fiberMag_mean_min_fiberID', 'fiberMag_mean_mean_fiberID', 'fiberMag_mean_median_fiberID', 'fiberMag_mean_std_fiberID',\n#               'fiberMag_median_max_fiberID', 'fiberMag_median_min_fiberID', 'fiberMag_median_mean_fiberID', 'fiberMag_median_median_fiberID', 'fiberMag_median_std_fiberID',\n#               'fiberMag_std_max_fiberID', 'fiberMag_std_min_fiberID', 'fiberMag_std_mean_fiberID', 'fiberMag_std_median_fiberID', 'fiberMag_std_std_fiberID',\n#               'fiberMag_sum_max_fiberID', 'fiberMag_sum_min_fiberID', 'fiberMag_sum_mean_fiberID', 'fiberMag_sum_median_fiberID', 'fiberMag_sum_std_fiberID'\n#               ]\n# gb.head(2)\n\n# train = pd.merge(train, gb, how='left', on='fiberID')\n# train.head(2)","fa658ecc":"# did not use these features.\n# # Statistics and distance based features\n# gb2 = test.groupby('fiberID', as_index=False).agg({'fiberMag_u': {'fiberMag_u_max_fiberID': np.max, 'fiberMag_u_min_fiberID': np.min, \n#                                                                   'fiberMag_u_mean_fiberID' : np.mean, 'fiberMag_u_median_fiberID' : np.median,\n#                                                                   'fiberMag_u_std_fiberID' : np.std},\n#                                                    'fiberMag_g': {'fiberMag_g_max_fiberID': np.max, 'fiberMag_g_min_fiberID': np.min, \n#                                                                   'fiberMag_g_mean_fiberID' : np.mean, 'fiberMag_g_median_fiberID' : np.median,\n#                                                                   'fiberMag_g_std_fiberID' : np.std},\n#                                                    'fiberMag_r': {'fiberMag_r_max_fiberID': np.max, 'fiberMag_r_min_fiberID': np.min, \n#                                                                   'fiberMag_r_mean_fiberID' : np.mean, 'fiberMag_r_median_fiberID' : np.median,\n#                                                                   'fiberMag_r_std_fiberID' : np.std},\n#                                                    'fiberMag_i': {'fiberMag_i_max_fiberID': np.max, 'fiberMag_i_min_fiberID': np.min, \n#                                                                   'fiberMag_i_mean_fiberID' : np.mean, 'fiberMag_i_median_fiberID' : np.median,\n#                                                                   'fiberMag_i_std_fiberID' : np.std},\n#                                                    'fiberMag_z': {'fiberMag_z_max_fiberID': np.max, 'fiberMag_z_min_fiberID': np.min, \n#                                                                   'fiberMag_z_mean_fiberID' : np.mean, 'fiberMag_z_median_fiberID' : np.median,\n#                                                                   'fiberMag_z_std_fiberID' : np.std},\n#                                                    'fiberMag_mean' : {'fiberMag_mean_max_fiberID': np.max, 'fiberMag_mean_min_fiberID': np.min, \n#                                                                   'fiberMag_mean_mean_fiberID' : np.mean, 'fiberMag_mean_median_fiberID' : np.median,\n#                                                                   'fiberMag_mean_std_fiberID' : np.std},\n#                                                    'fiberMag_median' : {'fiberMag_median_max_fiberID': np.max, 'fiberMag_median_min_fiberID': np.min, \n#                                                                   'fiberMag_median_mean_fiberID' : np.mean, 'fiberMag_median_median_fiberID' : np.median,\n#                                                                   'fiberMag_median_std_fiberID' : np.std},\n#                                                    'fiberMag_std' : {'fiberMag_std_max_fiberID': np.max, 'fiberMag_std_min_fiberID': np.min, \n#                                                                   'fiberMag_std_mean_fiberID' : np.mean, 'fiberMag_std_median_fiberID' : np.median,\n#                                                                   'fiberMag_std_std_fiberID' : np.std},\n#                                                    'fiberMag_sum' : {'fiberMag_sum_max_fiberID': np.max, 'fiberMag_sum_min_fiberID': np.min, \n#                                                                   'fiberMag_sum_mean_fiberID' : np.mean, 'fiberMag_sum_median_fiberID' : np.median,\n#                                                                   'fiberMag_sum_std_fiberID' : np.std}\n#                                                    }).fillna(0)\n\n# gb2.columns = ['fiberID', 'fiberMag_u_max_fiberID', 'fiberMag_u_min_fiberID', 'fiberMag_u_mean_fiberID', 'fiberMag_u_median_fiberID', 'fiberMag_u_std_fiberID',\n#               'fiberMag_g_max_fiberID', 'fiberMag_g_min_fiberID', 'fiberMag_g_mean_fiberID', 'fiberMag_g_median_fiberID', 'fiberMag_g_std_fiberID',\n#               'fiberMag_r_max_fiberID', 'fiberMag_r_min_fiberID', 'fiberMag_r_mean_fiberID', 'fiberMag_r_median_fiberID', 'fiberMag_r_std_fiberID',\n#               'fiberMag_i_max_fiberID', 'fiberMag_i_min_fiberID', 'fiberMag_i_mean_fiberID', 'fiberMag_i_median_fiberID', 'fiberMag_i_std_fiberID',\n#               'fiberMag_z_max_fiberID', 'fiberMag_z_min_fiberID', 'fiberMag_z_mean_fiberID', 'fiberMag_z_median_fiberID', 'fiberMag_z_std_fiberID',\n#               'fiberMag_mean_max_fiberID', 'fiberMag_mean_min_fiberID', 'fiberMag_mean_mean_fiberID', 'fiberMag_mean_median_fiberID', 'fiberMag_mean_std_fiberID',\n#               'fiberMag_median_max_fiberID', 'fiberMag_median_min_fiberID', 'fiberMag_median_mean_fiberID', 'fiberMag_median_median_fiberID', 'fiberMag_median_std_fiberID',\n#               'fiberMag_std_max_fiberID', 'fiberMag_std_min_fiberID', 'fiberMag_std_mean_fiberID', 'fiberMag_std_median_fiberID', 'fiberMag_std_std_fiberID',\n#               'fiberMag_sum_max_fiberID', 'fiberMag_sum_min_fiberID', 'fiberMag_sum_mean_fiberID', 'fiberMag_sum_median_fiberID', 'fiberMag_sum_std_fiberID'\n#               ]\n\n# gb2.isnull().sum()\n# test = pd.merge(test, gb2, how='left', on='fiberID')\n# print(train.isnull().sum().sum())\n# print(test.isnull().sum().sum())\n# test.head(2)","a1f9b984":"train_gb = train.copy()\nX_train_gb = train_gb.drop('type', axis=1)\ny_train_gb = train_gb['type']\ntest_gb = test.copy()","059c879a":"unique_labels = train['type'].unique()\nlabel_dict = {val: i for i, val in enumerate(unique_labels)}\ni2lb = {v:k for k, v in label_dict.items()}\ni2lb","fac530e6":"scaler = StandardScaler()\nlabels = train['type']\ntrain = train.drop('type', axis=1)\n_mat = scaler.fit_transform(train)\ntrain = pd.DataFrame(_mat, columns=train.columns, index=train.index)\ntrain_x = train\ntrain_y = labels.replace(label_dict)\ntest = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\ntest.head(2)","8a4997a0":"test_ids = test.index.copy()\ntest_ids = pd.DataFrame(test_ids, index=test_ids)\ntest_ids","b6fadfcb":"# # data set split\n# X_train, X_vali, y_train, y_vali = train_test_split(X_train_gb, y_train_gb, test_size=0.2,\n#                                                                     stratify=y_train_gb, random_state=5)","823bc726":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod(\n            (datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n              (thour, tmin, round(tsec, 2)))","e9a54463":"# XGboost Cross Validation\n\nfolds = 5\n\nxgb_cv_sum = 0\nxgb_pred = []\nxgb_fpred = []\n\navreal = y_train_gb\n\n# blend_train = []\n# blend_test = []\n\ntrain_time = timer(None)\nkf = StratifiedKFold(n_splits=folds, random_state=5, shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(X_train_gb, y_train_gb)):\n    start_time = timer(None)\n    Xtrain, Xval = X_train_gb.iloc[train_index], X_train_gb.iloc[val_index]\n    ytrain, yval = y_train_gb.iloc[train_index], y_train_gb.iloc[val_index]\n\n    model = XGBClassifier(random_state=5,\n                         max_depth=10,\n                         n_estimators=3500,\n                         learning_rate=0.005,\n                         objective='multi:softprob',\n                         tree_method='gpu_hist',\n                         colsample_bytree=0.5,\n                         subsample=0.6\n                          )\n    \n    model.fit(Xtrain, ytrain, eval_set=[(Xval, yval)],\n              eval_metric='mlogloss', early_stopping_rounds=100, verbose=1000)\n              \n    xgb_scores_val = model.predict_proba(Xval)\n    xgb_log_loss = log_loss(yval, xgb_scores_val)\n    print('\\n Fold %02d xgb mlogloss: %.6f' % ((i + 1), xgb_log_loss))\n    xgb_y_pred = model.predict_proba(test_gb)\n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    if i > 0:\n        xgb_fpred = xgb_pred + xgb_y_pred\n    else:\n        xgb_fpred = xgb_y_pred\n    xgb_pred = xgb_fpred\n    xgb_cv_sum = xgb_cv_sum + xgb_log_loss\n\ntimer(train_time)\n\nxgb_cv_score = (xgb_cv_sum \/ folds)\n\nprint('\\n Average xgb mlogloss:\\t%.6f' % xgb_cv_score)\nxgb_score = round(xgb_cv_score, 6)\n\nxgb_mpred = xgb_pred \/ folds","ecb0571b":"# LGBM Cross Validation\n\nfolds = 5\n\nlgb_cv_sum = 0\nlgb_pred = []\nlgb_fpred = []\n\navreal = y_train_gb\n\n# blend_train = []\n# blend_test = []\n\ntrain_time = timer(None)\nkf = StratifiedKFold(n_splits=folds, random_state=5, shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(X_train_gb, y_train_gb)):\n    start_time = timer(None)\n    Xtrain, Xval = X_train_gb.iloc[train_index], X_train_gb.iloc[val_index]\n    ytrain, yval = y_train_gb.iloc[train_index], y_train_gb.iloc[val_index]\n\n    model = LGBMClassifier(random_state=15,\n                           num_leaves=140,\n                           n_estimators=2000,\n                           learning_rate=0.01,\n                           boost_from_average=True,\n                           colsample_bytree=0.4,\n                           subsample=0.6,\n                           min_child_samples=200,\n                           objective='multiclass'\n    )\n    model.fit(Xtrain, ytrain, eval_set=(Xval, yval),\n              early_stopping_rounds=100, verbose=500)\n              \n    lgb_scores_val = model.predict_proba(Xval)\n    lgb_log_loss = log_loss(yval, lgb_scores_val)\n    print('\\n Fold %02d lgb mlogloss: %.6f' % ((i + 1), lgb_log_loss))\n    lgb_y_pred = model.predict_proba(test_gb)\n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    if i > 0:\n        lgb_fpred = lgb_pred + lgb_y_pred\n    else:\n        lgb_fpred = lgb_y_pred\n    lgb_pred = lgb_fpred\n    lgb_cv_sum = lgb_cv_sum + lgb_log_loss\n\ntimer(train_time)\n\nlgb_cv_score = (lgb_cv_sum \/ folds)\n\nprint('\\n Average lgb mlogloss:\\t%.6f' % lgb_cv_score)\nlgb_score = round(lgb_cv_score, 6)\n\nlgb_mpred = lgb_pred \/ folds","ac5ca152":"from keras.callbacks import LearningRateScheduler\nimport math\n# learning rate schedule\ndef step_decay(epoch):\n\tinitial_lrate = 0.002\n\tdrop = 0.1\n\tepochs_drop = 30\n\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n\treturn lrate\n\n# learning schedule callback\nlrate = LearningRateScheduler(step_decay)","f7a92040":"es = EarlyStopping(monitor='val_loss',patience=30)","3b2d3521":"opti = keras.optimizers.Adam(lr = 0.002)","3cb5c346":"def build_model():\n    model = Sequential()\n    model.add(Dense(units=1024, input_dim=len(train_x.columns)))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=1024))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=1024))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=1024))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=1024))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=1024))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=1024))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=19, activation='softmax'))\n\n    model.compile(optimizer=opti,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","421de29c":"def build_model_2():\n    model = Sequential()\n    model.add(Dense(units=1024, input_dim=len(train_x.columns)))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=2048))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dropout(0.05))\n    model.add(Dense(units=3000))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dropout(0.1))\n    model.add(Dense(units=3000))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dropout(0.1))\n    model.add(Dense(units=2048))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dropout(0.05))\n    model.add(Dense(units=1024))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(alpha=0.1))\n    model.add(Dense(units=512))\n    model.add(BatchNormalization())\n    model.add(Dense(units=19, activation='softmax'))\n\n    model.compile(optimizer=opti,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","8f194866":"# Keras NN model_1 CV\nfolds = 10\n\nMLP_cv_sum = 0\nMLP_pred = []\nMLP_fpred = []\n\navreal = train_y\n\n# blend_train = []\n# blend_test = []\n\ntrain_time = timer(None)\nkf = StratifiedKFold(n_splits=folds, random_state=5, shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(train_x, train_y)):\n    start_time = timer(None)\n    Xtrain, Xval = train_x.iloc[train_index], train_x.iloc[val_index]\n    ytrain, yval = train_y.iloc[train_index], train_y.iloc[val_index]\n\n    model = build_model()\n    \n    path = \"fold \" + str(i+1) + \" bestmodel.hdf5\"\n    check_best_model = keras.callbacks.ModelCheckpoint(filepath=path, monitor='val_loss',\n                                                   verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    callback_list = [lrate, es, check_best_model]\n    \n    model.fit(Xtrain, ytrain,\n              validation_data=(Xval, yval),\n              batch_size=512,\n              epochs=150,\n              callbacks=callback_list,\n              verbose=0\n              )\n    \n    # best model load\n    model = load_model(path)\n    \n    MLP_scores_val = model.predict(Xval)\n    MLP_log_loss = log_loss(yval, MLP_scores_val)\n    print('\\n Fold %02d MLP mlogloss: %.6f' % ((i + 1), MLP_log_loss))\n    MLP_y_pred = model.predict(test)\n    \n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    if i > 0:\n        MLP_fpred = MLP_pred + MLP_y_pred\n    else:\n        MLP_fpred = MLP_y_pred\n    MLP_pred = MLP_fpred\n    MLP_cv_sum = MLP_cv_sum + MLP_log_loss\n\ntimer(train_time)\n\nMLP_cv_score = (MLP_cv_sum \/ folds)\n\nprint('\\n Average MLP mlogloss:\\t%.6f' % MLP_cv_score)\nMLP_score = round(MLP_cv_score, 6)\n\nMLP_mpred_1 = MLP_pred \/ folds","9a62b43d":"# Keras NN model_2 CV\nfolds = 10\n\nMLP_cv_sum = 0\nMLP_pred = []\nMLP_fpred = []\n\navreal = train_y\n\n# blend_train = []\n# blend_test = []\n\ntrain_time = timer(None)\nkf = StratifiedKFold(n_splits=folds, random_state=5, shuffle=True)\nfor i, (train_index, val_index) in enumerate(kf.split(train_x, train_y)):\n    start_time = timer(None)\n    Xtrain, Xval = train_x.iloc[train_index], train_x.iloc[val_index]\n    ytrain, yval = train_y.iloc[train_index], train_y.iloc[val_index]\n\n    model = build_model_2()\n    \n    path = \"fold \" + str(i+1) + \" bestmodel_2.hdf5\"\n    check_best_model = keras.callbacks.ModelCheckpoint(filepath=path, monitor='val_loss',\n                                                   verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n    callback_list = [lrate, es, check_best_model]\n    \n    model.fit(Xtrain, ytrain,\n              validation_data=(Xval, yval),\n              batch_size=512,\n              epochs=150,\n              callbacks=callback_list,\n              verbose=0\n              )\n    \n    # best model load\n    model = load_model(path)\n    \n    MLP_scores_val = model.predict(Xval)\n    MLP_log_loss = log_loss(yval, MLP_scores_val)\n    print('\\n Fold %02d MLP mlogloss: %.6f' % ((i + 1), MLP_log_loss))\n    MLP_y_pred = model.predict(test)\n    \n\n    del Xtrain, Xval\n    gc.collect()\n\n    timer(start_time)\n\n    if i > 0:\n        MLP_fpred = MLP_pred + MLP_y_pred\n    else:\n        MLP_fpred = MLP_y_pred\n    MLP_pred = MLP_fpred\n    MLP_cv_sum = MLP_cv_sum + MLP_log_loss\n\ntimer(train_time)\n\nMLP_cv_score = (MLP_cv_sum \/ folds)\n\nprint('\\n Average MLP mlogloss:\\t%.6f' % MLP_cv_score)\nMLP_score = round(MLP_cv_score, 6)\n\nMLP_mpred_2 = MLP_pred \/ folds","86fb2ed6":"xgb = XGBClassifier(n_estimators=1)\nxgb.fit(X_train_gb, y_train_gb)\nprint(xgb.classes_)","4dbfd2ad":"lgb = LGBMClassifier(n_estimators=1)\nlgb.fit(X_train_gb, y_train_gb)\nprint(lgb.classes_)","256a45c5":"sample = pd.read_csv('\/kaggle\/input\/dacon-stage-2\/sample_submission.csv')\nsample.head(2)","b3c0823c":"xgb_mat = pd.DataFrame(xgb_mpred, index=test_gb.index, columns=xgb.classes_)\nxgb_mat = pd.concat([test_ids, xgb_mat], axis=1)\nxgb_mat = xgb_mat[sample.columns]\nxgb_mat = xgb_mat.drop('id', axis=1)\nxgb_mat.head(2)","b5973c1b":"lgb_mat = pd.DataFrame(lgb_mpred, index=test_gb.index, columns=lgb.classes_)\nlgb_mat = pd.concat([test_ids, lgb_mat], axis=1)\nlgb_mat = lgb_mat[sample.columns]\nlgb_mat = lgb_mat.drop('id', axis=1)\nlgb_mat.head(2)","952c473a":"MLP_mat_1 = pd.DataFrame(MLP_mpred_1, index=test.index)\nMLP_mat_1 = MLP_mat_1.rename(columns=i2lb)\nMLP_mat_1 = pd.concat([test_ids, MLP_mat_1], axis=1)\nMLP_mat_1 = MLP_mat_1[sample.columns]\nMLP_mat_1 = MLP_mat_1.drop('id', axis=1)\nMLP_mat_1.head(2)","ed26c75f":"MLP_mat_2 = pd.DataFrame(MLP_mpred_2, index=test.index)\nMLP_mat_2 = MLP_mat_2.rename(columns=i2lb)\nMLP_mat_2 = pd.concat([test_ids, MLP_mat_2], axis=1)\nMLP_mat_2 = MLP_mat_2[sample.columns]\nMLP_mat_2 = MLP_mat_2.drop('id', axis=1)\nMLP_mat_2.head(2)","2a23f9e6":"submission = 0.4*MLP_mat_1 + 0.3*MLP_mat_2 + 0.2*xgb_mat + 0.1*lgb_mat\nsubmission = pd.concat([test_ids, submission], axis=1)\nsubmission.head()","83737280":"submission.to_csv('dacon_stage_2_submission_model_ensemble.csv', index=False)","fb459935":"**Preprocessing \/ Feature Engineering**","8ce77ba5":"**Modeling - XGB, LGB, Keras neural net**","e672ea8d":"I participated in 'Monthly Dacon 2 Celestial Type Classification' held by DACON, a Korean data science\/machine learning competition platform with this code\n\nUsing this code, I was able to get in the top 10.\n\nA description of the data and competition rules are attached to the address of Dacon below.\n\nI'm still a student studying machine learning, so there may be some inefficient code. I hope you understand that.\n\nThank you.\n\nAddress: https:\/\/dacon.io\/competitions\/official\/235573\/overview\/","bae7f08b":"**Model ensemble \/ Prediction**","fab1ef17":"**Read data & EDA**"}}