{"cell_type":{"3da60026":"code","bb295f96":"code","5ed2fa09":"code","26ffb0f6":"code","d4059a7a":"code","1a7e7483":"code","e15ac620":"code","7567066a":"code","dbdb6e26":"code","6ffb362b":"code","217982e6":"code","cceb550b":"code","6d0f45c6":"code","f0d1c21b":"code","332f1d92":"code","c45ec240":"code","92c1f90e":"code","1c4607e9":"code","fe6640c5":"code","be64467a":"code","ac9eabf0":"code","ed77b42c":"code","6697692f":"code","e527a864":"code","9471a12a":"code","829a607d":"code","72680090":"code","51690545":"code","3fc22a6c":"code","bfa8988a":"code","a6054e29":"code","e33a04e4":"code","4b7a4195":"code","e2857ac6":"code","efd6c8d0":"code","79e83d9b":"code","79666765":"markdown","7f0832fc":"markdown","3d0d5dd2":"markdown","9e9cb7d3":"markdown","e77fe060":"markdown","436a001f":"markdown","f147d165":"markdown","9b16824b":"markdown","ee630167":"markdown"},"source":{"3da60026":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nimport re \npd.set_option(\"display.max_colwidth\",200)\nimport nltk\nimport string\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import KFold, cross_val_score\nplt.style.use('seaborn')","bb295f96":"wn = nltk.WordNetLemmatizer()\nstopwords=nltk.corpus.stopwords.words(\"english\")","5ed2fa09":"# training dataset\ntrain_dataset=pd.read_csv(\"..\/input\/train_sentiment.csv\") \n#training labels\nlabel=pd.read_csv(\"..\/input\/train_sentiment.csv\") \n# testing dataset\ntest_dataset=pd.read_csv(\"..\/input\/test_sentiment.csv\")","26ffb0f6":"# printing the size of the dataset\nprint(\"train_dataset--->\",train_dataset.shape)\nprint(\"test_dataset--->\",test_dataset.shape)","d4059a7a":"train_dataset=train_dataset.drop(\"label\",axis=1)\n## convertinig each text into lower case\ntrain_dataset[\"tweet\"]=train_dataset[\"tweet\"].str.lower()\n\n","1a7e7483":"# copying the train_dataset to train.\ntrain=train_dataset","e15ac620":"train.head()","7567066a":"# copying the test_dataset to test.\ntest=test_dataset","dbdb6e26":"test.head()","6ffb362b":"# conctinating the test and train dataset in order to avoid the repetation of cleaning process on this datasets.\n# we will again split the datasets into train and test after the preprocessing of the datasets.\nfinal_dataset=pd.concat([train,test],axis=0)","217982e6":"final_dataset.head()","cceb550b":"## adding a new feature teweet length to the dataset\n## it shows the length of the tweets\nfinal_dataset[\"tweet_length\"]=final_dataset[\"tweet\"].apply(lambda x:len(x)-x.count(\" \"))","6d0f45c6":"# adding new column : punct%\n# punct% shows the percentage of punctuations used in the tweets\n\n# function for calculating punctuation percentage in each tweet\ndef punct(text):\n    count=sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text)-text.count(\" \")),3)*100\nfinal_dataset['punct%']=final_dataset[\"tweet\"].apply(lambda x: punct(x))\nfinal_dataset.head()","f0d1c21b":"## remove the non-alphabets\nfinal_dataset[\"tweet\"]=final_dataset[\"tweet\"].str.replace(\"[^a-z ]\",\"\")","332f1d92":"# visualising the distribution of tweet length in the datasets\nfig,ax=plt.subplots(1,3,figsize=(15,5))\nsns.distplot(final_dataset[\"tweet_length\"],ax=ax[0],color=\"mediumturquoise\")\nsns.boxplot(final_dataset[\"tweet_length\"],hue=label[\"label\"],ax=ax[1],color=\"turquoise\")\nsns.violinplot(final_dataset[\"tweet_length\"],hue=label[\"label\"],ax=ax[2],orient=\"v\",color=\"lightgreen\")\nplt.grid(True)\n# visualising the distribution of punct% in the datasets\nfig,ax=plt.subplots(1,3,figsize=(15,5))\nsns.distplot(final_dataset[\"punct%\"],ax=ax[0],color=\"lightgreen\")\nsns.boxplot(final_dataset[\"punct%\"],ax=ax[1],color=\"turquoise\")\nsns.violinplot(final_dataset[\"punct%\"],ax=ax[2],orient=\"v\",color=\"mediumturquoise\")\nplt.grid(True)\n","c45ec240":"# function for converting raw tweets into tokenized tweets with no stopwords.\n\ndef clean_text(text):\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [word for word in tokens if word not in stopwords]\n    return text\n# adding new column tweet_nostopwords which consists of tokenized tweets with no stopwords.\nfinal_dataset['tweet_nostopwords'] = final_dataset['tweet'].apply(lambda x: clean_text(x.lower()))\nfinal_dataset.head()","92c1f90e":"# function for converting tokenized tweets into lemmatized tweets.\ndef lemmatizing(tokenized_text):\n    text =\" \".join([wn.lemmatize(word) for word in tokenized_text])\n    return text\n\n# adding new column tweet_lemmatized which consists of lemmatized tweets.\nfinal_dataset['tweet_lemmatized'] = final_dataset['tweet_nostopwords'].apply(lambda x: lemmatizing(x))\n\nfinal_dataset.head()","1c4607e9":"# imporitng count vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","fe6640c5":"# converting the finsl dataset into vector matrix\ncount_vect=CountVectorizer(analyzer=clean_text)\nX_counts=count_vect.fit_transform(final_dataset[\"tweet_lemmatized\"])\nprint(X_counts.shape)","be64467a":"X_counts # we cannot print the matrix because it is a sparse matrix","ac9eabf0":"# in order to print the matrix we have to convert it to array\nX_counts_df=pd.DataFrame(X_counts.toarray())\n#X_counts_df.columns=count_vect.get_feature_names()\nX_counts_df.head(10)","ed77b42c":"# adding the tweet length and punct% column to the  vectorized dataset\na=pd.DataFrame(np.array(final_dataset['tweet_length']).reshape(-1,1),columns=[\"tweet_length\"])\nb=pd.DataFrame(np.array(final_dataset['punct%']).reshape(-1,1),columns=[\"punct%\"])\nfinal_dataset1= pd.concat([a,b,X_counts_df],axis=1)","6697692f":"# final dataset for fitting ML model\nfinal_dataset1.head()","e527a864":"# splitting the dataset into train and test as we specified above.\nfinal_train=final_dataset1.iloc[0:7920,:]\nfinal_test=final_dataset1.iloc[7920:,:]","9471a12a":"# creating object of the model\nrf = RandomForestClassifier(n_jobs=-1,n_estimators=300)","829a607d":"# fitting the model\nrf.fit(final_train,label['label'])","72680090":"# predicting on test dataset\ny_pred=rf.predict(final_test)\ny_pred=pd.DataFrame(y_pred)\ny_pred.head()","51690545":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(analyzer=clean_text)\nx_tfidf=tfidf.fit_transform(final_dataset[\"tweet_lemmatized\"])\nprint(x_tfidf.shape)","3fc22a6c":"# in order to print the matrix we have to convert it to array\nx_tfidf_df=pd.DataFrame(x_tfidf.toarray())\n#x_tfidf_df.columns=tfidf.get_feature_names()\nx_tfidf_df.head()","bfa8988a":"# adding the tweet length and punct% column to the  vectorized dataset\na=pd.DataFrame(np.array(final_dataset['tweet_length']).reshape(-1,1))\nb=pd.DataFrame(np.array(final_dataset['punct%']).reshape(-1,1))","a6054e29":"final_dataset1= pd.concat([a,b,x_tfidf_df],axis=1)","e33a04e4":"# final dataset for fitting ML model\nfinal_dataset1.head()","4b7a4195":"# splitting the dataset into train and test as we specified above.\nfinal_train=final_dataset1.iloc[0:7920,:]\nfinal_test=final_dataset1.iloc[7920:,:]","e2857ac6":"# creating object of the model\nrf = RandomForestClassifier(n_jobs=-1,n_estimators=300)","efd6c8d0":"# fitting the model\nrf.fit(final_train,label['label'])","79e83d9b":"# predicting on test dataset\ny_pred=rf.predict(final_test)\ny_pred=pd.DataFrame(y_pred)\ny_pred.head()","79666765":"**----: Importing Word Net Lemmatizer and stopwords :----**","7f0832fc":"## Sentiment Analysis","3d0d5dd2":"**----: Exploring Datasets :----**","9e9cb7d3":"### ***Random Forest Model Using Count Vectorizer***","e77fe060":"### ***Random Forest Model Using TF-IDF Vectorizer***","436a001f":"#### ----: Importing necessary liabraries :----","f147d165":"#### ----: Reading Datasets :----","9b16824b":"# Natural Language Processing","ee630167":"**----: Problem Statement :----**\n- **About Practice Problem: Identify the Sentiments**\n   - Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. Brands can use this data to measure the success of their products in an objective manner. In this challenge, you are provided with tweet data to predict sentiment on electronic products of netizens.\n\n  - Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing. This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products."}}