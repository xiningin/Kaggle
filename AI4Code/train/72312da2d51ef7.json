{"cell_type":{"ab4eb047":"code","c7be9a86":"code","672a525e":"code","e47baae1":"code","38810b20":"code","4f821f70":"code","c5b9fbf3":"code","8a8bcf54":"code","d1103da8":"code","817105f9":"code","9f0989e9":"code","1e8bc399":"code","de74db01":"code","a5208d9b":"code","5f14d69d":"code","5b5804ff":"markdown","3032556f":"markdown","e7f77012":"markdown","0b9c618e":"markdown","409b0f72":"markdown","929ba7cf":"markdown","0893d59c":"markdown","7d68ff64":"markdown","57b68f5d":"markdown"},"source":{"ab4eb047":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7be9a86":"# Model Configuration\nUNITS = 2 ** 11 # 2048\nACTIVATION = 'relu'\nDROPOUT = 0.1\n\n# Training Configuration\nBATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best","672a525e":"# TensorFlow\nimport tensorflow as tf\nimport tensorflow_hub as hub\nprint(\"Tensorflow version \" + tf.__version__)\n\n# TF 2.3 version\n# Detect and init the TPU\n# try: # detect TPUs\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n#     strategy = tf.distribute.TPUStrategy(tpu)\n# except ValueError: # detect GPUs\n#     strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n# TF 2.2 version\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    \n# Plotting\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Matplotlib defaults\nplt.style.use('seaborn-whitegrid')\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\n\n\n# Data\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.io import FixedLenFeature\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n# Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","e47baae1":"def make_decoder(feature_description):\n    def decoder(example):\n        example = tf.io.parse_single_example(example, feature_description)\n        features = tf.io.parse_tensor(example['features'], tf.float32)\n        features = tf.reshape(features, [28])\n        label = example['label']\n        return features, label\n    return decoder\n\ndef load_dataset(filenames, decoder, ordered=False):\n    AUTO = tf.data.experimental.AUTOTUNE\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    dataset = (\n        tf.data\n        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n        .with_options(ignore_order)\n        .map(decoder, AUTO)\n    )\n    return dataset","38810b20":"dataset_size = int(11e6)\nvalidation_size = int(5e5)\ntraining_size = dataset_size - validation_size\n\n# For model.fit\nbatch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nsteps_per_epoch = training_size \/\/ batch_size\nvalidation_steps = validation_size \/\/ batch_size\n\n# For model.compile\nsteps_per_execution = steps_per_epoch","4f821f70":"feature_description = {\n    'features': FixedLenFeature([], tf.string),\n    'label': FixedLenFeature([], tf.float32),\n}\ndecoder = make_decoder(feature_description)\n\ndata_dir = KaggleDatasets().get_gcs_path('higgs-boson')\ntrain_files = tf.io.gfile.glob(data_dir + '\/training' + '\/*.tfrecord')\nvalid_files = tf.io.gfile.glob(data_dir + '\/validation' + '\/*.tfrecord')\n\nds_train = load_dataset(train_files, decoder, ordered=False)\nds_train = (\n    ds_train\n    .cache()\n    .repeat()\n    .shuffle(2 ** 19)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nds_valid = load_dataset(valid_files, decoder, ordered=False)\nds_valid = (\n    ds_valid\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\nepo = 50","c5b9fbf3":"def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n    def make(inputs):\n        x = layers.Dense(units)(inputs)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(activation)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        return x\n    return make\n\n\n# Wide Network\nwide = keras.experimental.LinearModel()\n\n# Deep Network\ninputs = keras.Input(shape=[28])\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\nx = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\noutputs = layers.Dense(1)(x)\ndeep = keras.Model(inputs=inputs, outputs=outputs)\n\n# Wide and Deep Network\nwide_and_deep = keras.experimental.WideDeepModel(\n    linear_model=wide,\n    dnn_model=deep,\n    activation='sigmoid',\n)\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n#     experimental_steps_per_execution=steps_per_execution,\n)\n\nearly_stopping = callbacks.EarlyStopping(\n    patience=2,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nlr_schedule = callbacks.ReduceLROnPlateau(\n    patience=0,\n    factor=0.2,\n    min_lr=0.001,\n)\n\nhistory = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=epo,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","8a8bcf54":"model = keras.Sequential(\n    [   \n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(150, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(150,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer3\",activation='sigmoid'),\n    ]\n)\n\nfrom keras import losses \nfrom keras import optimizers \nfrom keras import metrics \n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['AUC','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs = epo,steps_per_epoch=steps_per_epoch)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","d1103da8":"model = keras.Sequential(\n    [   \n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(150, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(150,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer3\",activation='sigmoid'),\n    ]\n)\n\nfrom keras import losses \nfrom keras import optimizers \nfrom keras import metrics \n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs = epo,steps_per_epoch=steps_per_epoch)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","817105f9":"model = keras.Sequential(\n    [\n        layers.Dense(1000, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(1000,)),\n        layers.Dense(750, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(750,)),\n        layers.BatchNormalization(),\n        layers.Dense(500, name=\"layer3\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(500,)),\n        layers.BatchNormalization(),\n        layers.Dense(200, activation=\"relu\", name=\"layer4\"),\n        layers.Dropout(0.2, input_shape=(200,)),\n        layers.BatchNormalization(),\n        layers.Dense(100, activation=\"relu\", name=\"layer5\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(50, name=\"layer6\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(50,)),\n        layers.BatchNormalization(),\n        layers.Dense(10, activation=\"relu\", name=\"layer7\"),\n        layers.Dropout(0.2, input_shape=(10,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer8\",activation='sigmoid'),\n    ])\n\nfrom keras import losses \nfrom keras import optimizers \nfrom keras import metrics \n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","9f0989e9":"model = keras.Sequential(\n    [\n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(75, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(75,)),\n        layers.BatchNormalization(),\n        layers.Dense(50, name=\"layer3\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(50,)),\n        layers.BatchNormalization(),\n        layers.Dense(20, activation=\"relu\", name=\"layer4\"),\n        layers.Dropout(0.2, input_shape=(20,)),\n        layers.BatchNormalization(),\n        layers.Dense(10, activation=\"relu\", name=\"layer5\"),\n        layers.Dropout(0.2, input_shape=(10,)),\n        layers.BatchNormalization(),\n        layers.Dense(5, name=\"layer6\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(5,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer7\",activation='sigmoid'),\n    ]\n)\nwide=model\n# from keras import losses \n# from keras import optimizers \n# from keras import metrics \n\n# model.compile(loss = 'binary_crossentropy',  \n#    optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\n# history = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)\n\n# history_frame = pd.DataFrame(history.history)\n# history_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\n# history_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","1e8bc399":"model = keras.Sequential(\n    [\n        layers.Dense(100, activation=\"relu\", name=\"layer1\"),\n        layers.Dropout(0.2, input_shape=(100,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, activation=\"relu\", name=\"layer2\"),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer3\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer4\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer5\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(700, name=\"layer6\",activation='relu'),\n        layers.Dropout(0.2, input_shape=(700,)),\n        layers.BatchNormalization(),\n        layers.Dense(1, name=\"layer7\",activation='sigmoid'),\n    ]\n)\ndeep = model\n\n# from keras import losses \n# from keras import optimizers \n# from keras import metrics \n\n# model.compile(loss = 'binary_crossentropy',  \n#    optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\n# history = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)\n\n# history_frame = pd.DataFrame(history.history)\n# history_frame.loc[:, ['loss','val_loss']].plot(title='Cross-entropy Loss')\n# history_frame.loc[:, ['accuracy', 'val_accuracy']].plot(title='accuracy');","de74db01":"wide_and_deep = keras.experimental.WideDeepModel(\n    linear_model=wide,\n    dnn_model=deep,\n    activation='sigmoid',\n)\n\nwide_and_deep.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['AUC', 'binary_accuracy'],\n#     experimental_steps_per_execution=steps_per_execution,\n)\n\nhistory = wide_and_deep.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n    steps_per_epoch=steps_per_epoch,\n    validation_steps=validation_steps,\n)\n\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\nhistory_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');","a5208d9b":"import tensorflow as tf\nimport numpy as np\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import layers\nnum_classes = 5\n\nmodel = tf.keras.Sequential([\n    hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/on_device_vision\/classifier\/landmarks_classifier_oceania_antarctica_V1\/1\", output_shape=[2048],\n                   trainable=False),  # Can be True, see below.\n    tf.keras.layers.Dense(1, activation='softmax')\n])\n\n\n# model.build([None, 299, 299, 3])  # Batch input shape.\n\n\nmodel.compile(loss = 'binary_crossentropy',  \n   optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\nhistory = model.fit(ds_train,validation_data=ds_valid, epochs =epo ,steps_per_epoch=steps_per_epoch)","5f14d69d":"# import tensorflow.compat.v2 as tf\n# import tensorflow_hub as hub\n\n# m = hub.KerasLayer('https:\/\/tfhub.dev\/google\/on_device_vision\/classifier\/landmarks_classifier_oceania_antarctica_V1\/1')\n# m = tf.keras.Sequential([\n#     m,\n#     tf.keras.layers.Dense(2, activation='softmax'),\n# ])\n\n\n# m.compile(loss = 'binary_crossentropy',  \n#    optimizer = 'adam', metrics = ['accuracy','binary_accuracy'])\n# history = m.fit(ds_train,validation_data=ds_valid, epochs =12 ,steps_per_epoch=13)","5b5804ff":"**try simple model with cross entropy and AUC this time**","3032556f":"# Simple Model","e7f77012":"# Deep","0b9c618e":"**use your wide and deep model together putting them both in wide_and_deep.fit()","409b0f72":"# simple model with AUC instead of accuracy","929ba7cf":"# Wide and Deep model (by tensorflow team)","0893d59c":"# Wide model","7d68ff64":"# wide and deep model using my two arcitectures","57b68f5d":"# Highly Dense, wide and deep Model"}}