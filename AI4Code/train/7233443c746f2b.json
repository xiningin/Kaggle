{"cell_type":{"e87021b2":"code","e51df66b":"code","6a972aac":"code","22cfd427":"code","40f4f06e":"code","c5f22cd3":"code","344d1c82":"code","04fa8533":"code","60fce9f2":"code","ee9cc34e":"code","ba591f6e":"code","a1152303":"code","b7833cd8":"code","dc6e14f8":"code","0b7299ee":"code","bfb869ab":"code","98bdcb43":"code","099568f9":"code","1780e5c8":"code","608b1a8d":"code","8f25cd67":"code","8764d3b9":"code","238d7716":"code","1700074d":"code","1e6231dd":"code","a8b49ec4":"code","ed167553":"markdown","0e02e0fd":"markdown","1014be37":"markdown","0c987d3b":"markdown","daf33954":"markdown","6d594de8":"markdown","c6e26161":"markdown","06935a8a":"markdown","b9402845":"markdown","aa623356":"markdown","11152504":"markdown","209ce397":"markdown","1fc2de04":"markdown","15b7c335":"markdown","1c9f4b4b":"markdown"},"source":{"e87021b2":"import warnings, re\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","e51df66b":"train_data=pd.read_csv('\/kaggle\/input\/fake-news\/train.csv',index_col='id')\ntest_data=pd.read_csv('\/kaggle\/input\/fake-news\/test.csv',index_col='id')","6a972aac":"train_data.isna().sum()","22cfd427":"train_data.dropna(how='all').isna().sum()","40f4f06e":"print(test_data.isna().sum(),'\\n')\nprint(test_data.dropna(how='all').isna().sum())","c5f22cd3":"train_data.fillna('Not Mentioned',inplace=True)\ntest_data.fillna('Not Mentioned',inplace=True)","344d1c82":"train_data.head()","04fa8533":"train_data['author'].value_counts()","60fce9f2":"len(train_data.loc[train_data['label']==1]), len(train_data.loc[train_data['label']==0])","ee9cc34e":"def text_cleaning(text):\n    \"\"\"\n    Removing all characters except alphabets\n    \"\"\"\n    text = re.sub(r'[^a-z]', ' ', text.lower())\n    return text\n\ntrain_data['text']=train_data['text'].apply(text_cleaning)\ntrain_data['title']=train_data['title'].apply(text_cleaning)\ntrain_data['author']=train_data['author'].apply(text_cleaning)\n\n#applying the same preprocessing for test data\ntest_data['text']=test_data['text'].apply(text_cleaning)\ntest_data['title']=test_data['title'].apply(text_cleaning)\ntest_data['author']=test_data['author'].apply(text_cleaning)","ba591f6e":"from sklearn.model_selection import train_test_split\n\ntrain_data, val_data=train_test_split(train_data, test_size=0.1, shuffle=True)\nlen(train_data), len(val_data)","a1152303":"# !mkdir cache\n\n# train_data.to_csv('cache\/train.csv', index=False)\n# val_data.to_csv('cache\/val.csv', index=False)\n# test_data.to_csv('cache\/test.csv', index=False)\n\n# del val_data, test_data","b7833cd8":"import spacy\nfrom nltk.corpus import stopwords\n\nspacy_nlp=spacy.load('en')\nstopword_list=stopwords.words('english')\n\ndef tokenization(text, MAX_LEN=20000):\n    text=re.sub(' +', ' ',\n                re.sub(r\"[\\*\\\"\u201c\u201d\\n\\\\\u2026\\+\\-\\\/\\=\\(\\)\u2018\u2022:\\[\\]\\|\u2019\\!;]\",\n                       ' ',text))\n    text=text if len(text)<=MAX_LEN else text[:MAX_LEN]\n    return [x.text for x in spacy_nlp.tokenizer(text) if (x.text!=' ') and (x.text not in stopword_list)]","dc6e14f8":"from torchtext.vocab import Vectors, Vocab\nfrom collections import Counter\n\n\ngloveVectors=Vectors(name='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\n\ncounter = Counter()\nfor i in tqdm(train_data.index):\n    counter.update(tokenization(train_data['text'][i]+' '+train_data['title'][i]+' '+train_data['author'][i]))\n#     counter.update(i.text+i.keyword+i.location)\n    \nvocabulary=Vocab(counter, max_size=20000, min_freq=4, vectors=gloveVectors, specials=['<pad>', '<unk>'])\n\nprint('Embedding vocab size: ', vocabulary.vectors.size(0))","0b7299ee":"import torch, torchtext, warnings\nfrom torchtext.data import Field, LabelField, Dataset, Example, TabularDataset, BucketIterator\n\nwarnings.filterwarnings(\"ignore\")\n\n\nclass NewsDataset(Dataset):\n    def __init__(self, df, fields, **kwargs):\n        examples=[]\n        for i, row in df.iterrows():\n            examples.append(Example.fromlist([row.title, row.author, row.text, row.label],\n                                                           fields))\n        #print(i,row.author)\n        super().__init__(examples, fields, **kwargs)\n        \n    @staticmethod\n    def sort_key(x):\n        return len(x.text)\n    \n    @classmethod\n    def splits(cls, fields, train_df=None, val_df=None, **kwargs):\n        train_data, val_data=(None, None)\n        \n        if train_df is not None:\n            train_data=cls(train_df.copy(), fields, **kwargs)\n            \n        if val_df is not None:\n            val_data=cls(val_df.copy(), fields, **kwargs)\n            \n        return tuple(d for d in (train_data, val_data) if d is not None)\n    \n\nText=Field(tokenization, include_lengths=True)\nTitle=Field(tokenization, include_lengths=True)\nAuthor=Field(tokenization, include_lengths=True)\nLabel=LabelField(dtype=torch.float)\n\nText.vocab=vocabulary\nTitle.vocab=vocabulary\nAuthor.vocab=vocabulary\n\nfields= [('title', Title),('author', Author), ('text', Text),('label', Label)]\n\ntrain_ds, val_ds= NewsDataset.splits(fields, train_df=train_data, val_df=val_data)\n\n# train_data, val_data= TabularDataset.splits(path='cache',\n#                                            train='train.csv',\n#                                            validation='val.csv',\n#                                            skip_header=True,\n#                                            format='csv',\n#                                            fields=fields)\n\nLabel.build_vocab(train_ds)\n\n\ndel train_data, val_data\n#sampling random example\n#print(vars(train_ds[61]))","bfb869ab":"#print(vars(train_data.examples[0]))\nprint(f'Number of training examples: {len(train_ds)}')\nprint(f'Number of validation examples: {len(val_ds)}')","98bdcb43":"for i, elem in enumerate(train_ds):\n    bucket=vars(elem)\n    if len(bucket['text'])<1:\n#         print(bucket)\n        \n        if len(bucket['title'])>0:\n            train_ds[i].text=train_ds[i].title\n        else:\n            train_ds[i].text=['not','mentioned']\n        print(vars(train_ds[i]))\n        \nfor i, elem in enumerate(val_ds):\n    bucket=vars(elem)\n    if len(bucket['text'])<1:\n#         print(bucket)\n        \n        if len(bucket['title'])>0:\n            val_ds[i].text=val_ds[i].title\n        else:\n            val_ds[i].text=['not','mentioned']\n        print(vars(val_ds[i]))","099568f9":"batch_size=64\ndevice=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator= BucketIterator.splits(\n                                (train_ds, val_ds),\n                                batch_size=batch_size,\n                                sort_within_batch=True,\n                                device=device)","1780e5c8":"for i in train_iterator:\n    if 0 in i.text[1]:\n        print(i.text[1], i.text[0])\n\nfor i in valid_iterator:\n    if 0 in i.text[1]:\n        print(i.text[1], i.text[0])","608b1a8d":"import torch.nn as nn\n\nlr=0.001\n\ninput_dims=len(vocabulary)\nembedding_dims=100\nhidden_dims=128\noutput_dims=1\nn_layers=2\nbidirectional=True\ndropoutP=0.2\n\npad_idx=0\n\ndef bin_accuracy(preds, y):\n    correct=(torch.round(torch.sigmoid(preds))==y).float()\n    return correct.sum()\/len(correct)\n\nclass biLSTM_single(nn.Module):\n    def __init__(self, vocab_size, embedding_dims, hidden_dims,\n              n_layers,bidirectional, dropoutP, output_dims, pad_idx):\n        \n        super().__init__()\n        self.embeddings=nn.Embedding(vocab_size, embedding_dims, padding_idx=pad_idx)\n        \n        self.lstm=nn.LSTM(embedding_dims, hidden_dims, \n                          num_layers=n_layers, bidirectional=bidirectional,\n                          dropout=dropoutP)\n        \n        self.fc1=nn.Linear(hidden_dims*2, hidden_dims)\n        self.fc2=nn.Linear(hidden_dims, output_dims)\n        self.drop=nn.Dropout(dropoutP)\n        \n    def forward(self, text, text_len):\n        #print(text.shape)\n        #[seq_len, batch_size]-->[seq_len, batch_size, embedding_dims]\n        embedding=self.embeddings(text) \n        #print(embedding.shape)\n        #[seq_len, batch_size, embedding_dims] -> [seq_len*batch_size, embedding_dims]\n        packed_embeddings=nn.utils.rnn.pack_padded_sequence(embedding, text_len)\n        #hidden:[num_layers * num_dir, batch_size, hidden_dims]\n        packed_out, (hidden, cell_state)=self.lstm(packed_embeddings)\n        #print(hidden.shape)\n        #[num_layers * num_dir, batch_size, hidden_dims] -> [batch_size, hidden_dims*2]\n        hidden=self.drop(torch.cat((hidden[-2,:,:],hidden[-1,:,:]), dim=1))\n        #print(hidden.shape)\n        #[batch_size, hidden_dims*2] --> [batch_size, hidden_dims]\n        output=self.fc1(hidden)\n        #print(output.shape)\n        #[batch_size, hidden_dims]->[batch_size, out_dims]\n        output=self.drop(self.fc2(output))\n        #print(output.shape)\n        return output\n    \nsingleChannelBiLSTM=biLSTM_single(input_dims, embedding_dims, hidden_dims,\n                                  n_layers, bidirectional, dropoutP, output_dims, pad_idx)\n\nsingleChannelBiLSTM.embeddings.weight.data.copy_(vocabulary.vectors)\n\nsingleChannelBiLSTM.to(device)\n\ncriterion=nn.BCEWithLogitsLoss()\noptimizer=torch.optim.Adam(singleChannelBiLSTM.parameters(), lr=lr)\nprint(sum(p.numel() for p in singleChannelBiLSTM.parameters() if p.requires_grad),' trainable prams')","8f25cd67":"def train(model, iterator):\n    epoch_loss, epoch_acc = 0, 0\n    \n    model.train()\n    \n    for batch in tqdm(iterator, total=len(iterator)):\n        text, text_len = batch.text\n        #print(text_len)\n        optimizer.zero_grad()\n        preds=model(text, text_len).squeeze(1)\n        #print(preds)\n        loss=criterion(preds, batch.label)\n        acc=bin_accuracy(preds, batch.label)\n        \n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss+=loss.item()\n        epoch_acc+=acc.item()\n        #print(epoch_loss, epoch_acc)\n    return (epoch_loss\/len(iterator), epoch_acc\/len(iterator))\n\ndef evaluate(model, iterator):\n    epoch_loss, epoch_acc = 0, 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for batch in tqdm(iterator, total=len(iterator)):\n            text, text_len = batch.text\n\n            preds=model(text, text_len).squeeze(1)\n            #print(preds)\n            loss=criterion(preds, batch.label)\n            acc=bin_accuracy(preds, batch.label)\n            \n            epoch_acc+=acc.item()\n            epoch_loss+=loss.item()\n            #print(epoch_acc)\n    return (epoch_loss\/len(iterator),epoch_acc\/len(iterator))\n\nfrom time import time\n\nstartT=time()\n\nloss, val_loss, acc, val_acc=[], [], [], []\n\nEPOCHS=10\n\nbest_loss=999\n\nfor epoch in range(EPOCHS):\n    train_loss, train_acc= train(singleChannelBiLSTM, train_iterator)\n    print(f'Epoch {epoch}\/{EPOCHS}\\nTraining: Loss {train_loss} Accuracy {train_acc}')\n    valid_loss, valid_acc= evaluate(singleChannelBiLSTM, valid_iterator)\n    print(f'Epoch {epoch}\/{EPOCHS}\\nValidation: Loss {valid_loss} Accuracy {valid_acc}')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    \n    val_acc.append(valid_acc)\n    val_loss.append(valid_loss)\n\n    if valid_loss<best_loss:\n        best_loss=valid_loss\n        torch.save(singleChannelBiLSTM.state_dict(),'singleBiLstm-bestLoss.pt')\n        \nprint(time()-startT)","8764d3b9":"torch.save(singleChannelBiLSTM.state_dict(),'singleBiLstm.pt')","238d7716":"plt.plot(range(EPOCHS),loss)\nplt.plot(range(EPOCHS),acc)\nplt.plot(range(EPOCHS),val_loss)\nplt.plot(range(EPOCHS),val_acc)\nplt.show()","1700074d":"import json\n\nwith open('history.json','w') as fp:\n    json.dump({'train loss':loss,\n               'train accuracy':acc,\n                'train loss':val_loss,\n               'train accuracy':val_acc,})","1e6231dd":"singleChannelBiLSTM.load_state_dict(torch.load('..\/input\/fakenewsmodelweights\/singleBiLstm.pt',map_location='cpu'))\n# singleChannelBiLSTM.eval()\n\ndef infer(text, author=None, title=None):\n    singleChannelBiLSTM.eval()\n    \n    text_arr=[vocabulary.stoi[token] for token in tokenization(text)]\n    if len(text_arr):\n        with torch.no_grad():\n            text=torch.LongTensor([text_arr]).view(-1,1)\n            text_len=torch.LongTensor([text.shape[1]])\n            return int(torch.round(torch.sigmoid(singleChannelBiLSTM(text, text_len).squeeze(1))).item())\n    else:\n        return 0\n\ntest_preds=[]\nfor i in tqdm(test_data.iterrows(), total=len(test_data)):\n    test_preds.append(infer(i[1]['text']))","a8b49ec4":"my_submissions=pd.DataFrame({'id':test_data.index.values,'label':test_preds})\nmy_submissions.to_csv('submission.csv', index=False)","ed167553":"Patching missing **text** in process of the tokenization","0e02e0fd":"As the data consists of **Author** to a certain article, ","1014be37":"## Data Ingestion Pipe using TorchText","0c987d3b":"### Handeling Missing Values\n\n> **id**: *No NaNs* <br>\n> **title**: *'Not Mentioned' to replace NaN*<br>\n> **author**: *'Not Mentioned' to replace NaN*<br>\n> **text**: *'Not Mentioned' to replace NaN*<br>\n> **label**: *No NaN*<br>","daf33954":"### Word Embeddings\nThis ingestion pipeline is for using **Word Embeddings**","6d594de8":"Checking if there are overlapping missing values from **all** arrtibutes of a given **entry**","c6e26161":"## Data Preprocessing","06935a8a":"#### Sanity check for zero-len texts","b9402845":"Same is applicable for the **Test Data** too","aa623356":"Number of fake labels and  no. real labels:","11152504":"## Simple Single channel--text biLSTM","209ce397":"Its almost a 50-50 split, and we can say the training data is balanced","1fc2de04":"Aparently there's no missing values from the labels, but there are some on the attributes,<br>\nfollowing is to remove and check overlapping missing values","15b7c335":"### Data Cleaning","1c9f4b4b":"## Evaluate"}}