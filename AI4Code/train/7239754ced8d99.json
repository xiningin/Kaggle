{"cell_type":{"ccc5b6f4":"code","50e9839e":"code","2273d35f":"code","72a956ae":"code","fd4cf65d":"code","c1142cde":"code","2ee4d973":"code","c07d5047":"code","b780423c":"code","31f675c8":"code","0907d8a7":"code","848bc5b2":"code","f39b1692":"code","d364bf61":"code","91ad0888":"code","a761d5c8":"code","5f19c204":"code","db077a04":"code","12691940":"code","7ed2143c":"code","8e128d1c":"code","c6ad5593":"code","992f600c":"code","70fc08c5":"code","4a0ecd28":"code","4e2cb21a":"code","7aedb0bd":"code","63c6a554":"code","80e5804c":"code","02afb62e":"code","2ee96367":"code","fa8b6b09":"code","3f9e7f82":"code","ab3014e4":"code","92580526":"code","c788c1d9":"code","55ba0b1d":"code","f85c87df":"code","d04665e0":"code","5812fcb9":"code","ad1d0476":"code","c6a652da":"code","825316c0":"code","cc70e123":"code","b771277b":"code","7a808192":"code","e42a72c4":"code","e99415e5":"code","97a0f99c":"code","6bfea75a":"code","e85e50a1":"code","f837f4cf":"code","4acc71fb":"code","1ee81efe":"code","6fc2c5a9":"markdown","fa5d1a2f":"markdown","e290644b":"markdown","2a71ba20":"markdown","e722ce54":"markdown","b22fc508":"markdown","cefbbfa2":"markdown","ba589a28":"markdown","146dafe3":"markdown","63ca0571":"markdown","8b275cd1":"markdown","9b9976e0":"markdown","650f2415":"markdown","5178c3a9":"markdown","7eac9764":"markdown","ec94885f":"markdown","6fedf685":"markdown","fc6c3f5b":"markdown","f829aa6f":"markdown","1351f9fc":"markdown","0f227a2f":"markdown","89de6f8d":"markdown","900354f1":"markdown","52eaff42":"markdown","76115e54":"markdown","c7387694":"markdown","5ca0757d":"markdown","704d8a6d":"markdown","fd2c667e":"markdown","bc2c0d13":"markdown"},"source":{"ccc5b6f4":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","50e9839e":"# load data\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ncats = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')","2273d35f":"train.head()","72a956ae":"print('train size, item in train, shop in train', train.shape[0], train.item_id.nunique(), train.shop_id.nunique())\nprint('train size, item in train, shop in train', test.shape[0], test.item_id.nunique(),test.shop_id.nunique())\nprint('new items:', len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test))","fd4cf65d":"train.isnull().sum()","c1142cde":"sale_by_month = train.groupby('date_block_num')['item_cnt_day'].sum()\nsale_by_month.plot()","2ee4d973":"block_item_shop_sale = train.groupby(['date_block_num','item_id','shop_id'])['item_cnt_day'].sum()\nblock_item_shop_sale.clip(0,20).plot.hist(bins=20)","c07d5047":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)\n\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","b780423c":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","31f675c8":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","0907d8a7":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","848bc5b2":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","f39b1692":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","d364bf61":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float32))\ntime.time() - ts","91ad0888":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","a761d5c8":"ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\ntime.time() - ts","5f19c204":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","db077a04":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","12691940":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","7ed2143c":"def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n    if not 'date_block_num' in groupby_feats:\n        print ('date_block_num must in groupby_feats')\n        return matrix_\n    \n    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n    max_lags = np.max(last_periods)\n    for i in range(1,max_lags+1):\n        shifted = group[groupby_feats+[target]].copy(deep=True)\n        shifted['date_block_num'] += i\n        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n        group = group.merge(shifted, on=groupby_feats, how='left')\n    group.fillna(0,inplace=True)\n    for period in last_periods:\n        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n        # we do not use mean and svd directly because we want to include months with sales = 0\n        mean = group[lag_feats].sum(axis=1)\/float(period)\n        mean2 = (group[lag_feats]**2).sum(axis=1)\/float(period)\n        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n        # divide by mean, this scales the features for NN\n        group[enc_feat+'_avg_sale_last_'+str(period)] \/= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n        group[enc_feat+'_std_sale_last_'+str(period)] \/= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n    return matrix","8e128d1c":"ts = time.time()\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'item', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'shop', [6,12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'category', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'city', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'type_code'], 'item_cnt_month', 'type', [12])\nmatrix = add_group_stats(matrix, ['date_block_num', 'subtype_code'], 'item_cnt_month', 'subtype', [12])\ntime.time() - ts","c6ad5593":"#first use target encoding each group, then shift month to creat lag features\ndef target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n    print ('target encoding for',groupby_feats)\n    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n    group.columns = [enc_feat]\n    group.reset_index(inplace=True)\n    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n    matrix = lag_feature(matrix, lags, enc_feat)\n    matrix.drop(enc_feat, axis=1, inplace=True)\n    return matrix","992f600c":"ts = time.time()\nmatrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id'], 'item_cnt_month', 'date_shop_avg_item_cnt', [1,2,3,6,12])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_category_id'], 'item_cnt_month', 'date_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'shop_id', 'item_category_id'], 'item_cnt_month', 'date_shop_cat_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'city_code'], 'item_cnt_month', 'date_city_avg_item_cnt', [1])\nmatrix = target_encoding(matrix, ['date_block_num', 'item_id', 'city_code'], 'item_cnt_month', 'date_item_city_avg_item_cnt', [1])\ntime.time() - ts","70fc08c5":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","4a0ecd28":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","4e2cb21a":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix['year'] = (matrix['date_block_num'] \/ 12).astype(np.int8)","7aedb0bd":"#Month since last sale for each shop\/item pair.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['item_id','shop_id'])['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.get_level_values(0).values,\n                       'shop_id': last_month.index.get_level_values(1).values,\n                       'item_shop_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id','shop_id'], how='left')\ntime.time() - ts","63c6a554":"#Month since last sale for each item.\nts = time.time()\nlast_sale = pd.DataFrame()\nfor month in range(1,35):    \n    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby('item_id')['date_block_num'].max()\n    df = pd.DataFrame({'date_block_num':np.ones([last_month.shape[0],])*month,\n                       'item_id': last_month.index.values,\n                       'item_last_sale': last_month.values})\n    last_sale = last_sale.append(df)\nlast_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int8)\n\nmatrix = matrix.merge(last_sale, on=['date_block_num','item_id'], how='left')\ntime.time() - ts","80e5804c":"# Months since the first sale for each shop\/item pair and for item only.\nts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","02afb62e":"matrix = matrix[matrix.date_block_num > 11]\nmatrix.columns","2ee96367":"matrix.to_pickle('..\/working\/data.pkl')\n# del matrix\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","fa8b6b09":"data = pd.read_pickle('..\/working\/data.pkl')","3f9e7f82":"from xgboost import XGBRegressor","ab3014e4":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]","92580526":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();","c788c1d9":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=7,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    gamma = 0.005,\n    eta=0.1,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    )\n\ntime.time() - ts","55ba0b1d":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nX_train_level2 = pd.DataFrame({\n    \"ID\": np.arange(Y_pred.shape[0]), \n    \"item_cnt_month\": Y_pred\n})\nX_train_level2.to_csv('..\/working\/xgb_valid.csv', index=False)\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test.shape[0]), \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('..\/working\/xgb_submission.csv', index=False)","f85c87df":"from sklearn.metrics import mean_squared_error\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.regularizers import l2, l1\nfrom keras.optimizers import RMSprop, Adam\nfrom tensorflow.random import set_seed\nset_seed(23333)\nnp.random.seed(233333)","d04665e0":"data = pd.read_pickle('..\/working\/data.pkl')\n# do not use ID features\ndata = data[[\n    'date_block_num',\n    #'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    #'city_code',\n    #'item_category_id',\n    #'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]","5812fcb9":"\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();","ad1d0476":"def Sales_prediction_model(input_shape):\n    in_layer = Input(input_shape)\n    x = Dense(16,kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(in_layer)\n    x = Dense(8, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    x = Dense(1, kernel_initializer='RandomUniform', kernel_regularizer=l2(0.02), activation = \"relu\")(x)\n    \n    model = Model(inputs = in_layer, outputs = x, name='Sales_prediction_model')\n    return model\n\n# NN cannot take missing values, fill NaN with 0.\nX_train.fillna(0,inplace=True)\nX_valid.fillna(0,inplace=True)\nX_test.fillna(0,inplace=True)\n\n# We do no feature scaling here. \n# Some features like 'item_avg_sale_last_6' are already scaled in feature engineering part.\n\ninput_shape = [X_train.shape[1]]\nmodel = Sales_prediction_model(input_shape)\nmodel.compile(optimizer = Adam(lr=0.0005) , loss = [\"mse\"], metrics=['mse'])\nmodel.fit(X_train, Y_train, validation_data = (X_valid, Y_valid), batch_size = 10000, epochs=5)","c6a652da":"Y_pred = model.predict(X_valid).clip(0, 20)[:,0]\nY_test = model.predict(X_test).clip(0, 20)[:,0]\n\nX_train_level2 = pd.DataFrame({\n    \"ID\": np.arange(Y_pred.shape[0]), \n    \"item_cnt_month\": Y_pred\n})\nX_train_level2.to_csv('..\/working\/nn_valid.csv', index=False)\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test.shape[0]), \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('..\/working\/nn_submission.csv', index=False)","825316c0":"from lightgbm import LGBMRegressor","cc70e123":"data = pd.read_pickle('..\/working\/data.pkl')\ndata = data[[\n    'date_block_num',\n    'shop_id',\n    #'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code','subtype_code',\n    'item_cnt_month_lag_1','item_cnt_month_lag_2','item_cnt_month_lag_3','item_cnt_month_lag_6','item_cnt_month_lag_12',\n    'item_avg_sale_last_6', 'item_std_sale_last_6',\n    'item_avg_sale_last_12', 'item_std_sale_last_12',\n    'shop_avg_sale_last_6', 'shop_std_sale_last_6',\n    'shop_avg_sale_last_12', 'shop_std_sale_last_12',\n    'category_avg_sale_last_12', 'category_std_sale_last_12',\n    'city_avg_sale_last_12', 'city_std_sale_last_12',\n    'type_avg_sale_last_12', 'type_std_sale_last_12',\n    'subtype_avg_sale_last_12', 'subtype_std_sale_last_12',\n    'date_avg_item_cnt_lag_1',\n#     'date_item_avg_item_cnt_lag_1','date_item_avg_item_cnt_lag_2','date_item_avg_item_cnt_lag_3','date_item_avg_item_cnt_lag_6','date_item_avg_item_cnt_lag_12',\n#     'date_shop_avg_item_cnt_lag_1','date_shop_avg_item_cnt_lag_2','date_shop_avg_item_cnt_lag_3','date_shop_avg_item_cnt_lag_6','date_shop_avg_item_cnt_lag_12',\n#     'date_cat_avg_item_cnt_lag_1',\n#     'date_shop_cat_avg_item_cnt_lag_1',\n#     'date_city_avg_item_cnt_lag_1',\n#     'date_item_city_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month','year',\n    'item_shop_last_sale','item_last_sale',\n    'item_shop_first_sale','item_first_sale',\n]]\n\ncat_feats = ['shop_id','city_code','item_category_id','type_code','subtype_code']","b771277b":"\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ndel data\ngc.collect();","7a808192":"from sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.pipeline import Pipeline\n# define dataset\n\n# create pipeline\nrfe = RFE(estimator=model, n_features_to_select=10)","e42a72c4":"# pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n# # fit the model on all available data\n# pipeline.fit(X_train, Y_train)\n# # make a prediction for one example\n# yhat = pipeline.predict(X_test)\n# print('Predicted: %.3f' % (yhat))","e99415e5":"ts = time.time()\n\nmodel = LGBMRegressor(\n    max_depth = 8,\n    n_estimators = 500,\n    colsample_bytree=0.7,\n    min_child_weight = 300,\n    reg_alpha = 0.1,\n    reg_lambda = 1,\n    random_state = 42,\n)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=10, \n    early_stopping_rounds = 40,\n    categorical_feature = cat_feats) # use LGBM's build-in categroical features.\n\ntime.time() - ts","97a0f99c":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nX_train_level2 = pd.DataFrame({\n    \"ID\": np.arange(Y_pred.shape[0]), \n    \"item_cnt_month\": Y_pred\n})\nX_train_level2.to_csv('..\/working\/lgb_valid.csv', index=False)\n\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test.shape[0]), \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('..\/working\/lgb_submission.csv', index=False)","6bfea75a":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, LinearRegression\nimport gc","e85e50a1":"data = pd.read_pickle('..\/working\/data.pkl')\nY_train_level2 = data[data.date_block_num == 33]['item_cnt_month']\ndel data\ngc.collect()","f837f4cf":"X_train_level2 = pd.DataFrame()\ndf = pd.read_csv('..\/working\/lgb_valid.csv')\nX_train_level2['lgb'] = df['item_cnt_month']\ndf = pd.read_csv('..\/working\/xgb_valid.csv')\nX_train_level2['xgb'] = df['item_cnt_month'] \ndf = pd.read_csv('..\/working\/nn_valid.csv')\nX_train_level2['nn'] = df['item_cnt_month'] \n\nX_test_level2 = pd.DataFrame()\ndf = pd.read_csv('..\/working\/lgb_submission.csv')\nX_test_level2['lgb'] = df['item_cnt_month']\ndf = pd.read_csv('..\/working\/xgb_submission.csv')\nX_test_level2['xgb'] = df['item_cnt_month'] \ndf = pd.read_csv('..\/working\/nn_submission.csv')\nX_test_level2['nn'] = df['item_cnt_month']","4acc71fb":"best_alpha = 1;\nbest_rmse = 100;\nfor alpha in np.arange(0,1,0.02):\n    Y_pred_level2 = alpha*X_train_level2['lgb'] + (1-alpha)*X_train_level2['xgb']\n    rmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\n    if (rmse<best_rmse):\n        best_rmse = rmse\n        best_alpha = alpha\n\nY_test_level2 = best_alpha*X_test_level2['lgb'] + (1-best_alpha)*X_test_level2['xgb']\nprint('best alpha:', best_alpha)\nprint('weighted average of lgb and xgb validation rmse: ',best_rmse)\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test_level2.shape[0]), \n    \"item_cnt_month\": Y_test_level2\n})\nsubmission.to_csv('..\/working\/blended_submission1.csv', index=False)\n\n# Linear regression\nmodel = LinearRegression()\nmodel.fit(X_train_level2, Y_train_level2)\nY_pred_level2 = model.predict(X_train_level2)\nY_test_level2 = model.predict(X_test_level2)\nrmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\nprint('Linear regression validation rmse: ',rmse)\nsubmission = pd.DataFrame({\n    \"ID\": np.arange(Y_test_level2.shape[0]), \n    \"item_cnt_month\": Y_test_level2\n})\nsubmission.to_csv('..\/working\/blended_submission2.csv', index=False)","1ee81efe":"### Kaggle rank - 2060","6fc2c5a9":"## Traget lags","fa5d1a2f":"### Model Training ","e290644b":"## Add month and year","2a71ba20":"## Lag features","e722ce54":"There is no missing value in the training set.\n\nPlot the total sale of each month, we see a clear trend and seasonality. The overall sale is decreasing with time, and there are peaks in November.","b22fc508":"## Happy Leaning \n## Please do share and upvote if you found this useful","cefbbfa2":"Price trend for the last six months.","ba589a28":"#### remove outliers\nRemove outliers with very large item_cnt_day and item_price.","146dafe3":"There is one item with price below zero. Fill it with median.","63ca0571":"## Shops\/Items\/Cats features","8b275cd1":"This notebook does exploratory data analysis and feature engineering. The results are dumped to file for modeling.\n\n#### Exploratary Data Analysis\n* load data\n* trend of sales\n* distribution of target\n\n#### Data Cleaning & Feature Engineering\n* heal data and remove outliers\n* work with shops\/items\/cats objects and features\n* expand training set to include all item-shop pairs\n* clip item_cnt_month by (0,20)\n* append test to the matrix, fill 34 month nans with zeros\n* merge shops\/items\/cats dataframe to training set.\n* add group sale stats in recent months\n* add lag features\n* add trend features\n* add month and year\n* add months since last sale\/months since first sale features\n* cut first year and drop columns which can not be calculated for the test set","9b9976e0":"## Using Neural Network","650f2415":"Last month shop revenue trend","5178c3a9":"#### Shops\/Cats\/Items preprocessing\nObservations:\n* Each shop_name starts with the city name.\n* Each category contains type and subtype in its name.","7eac9764":"## Using ensemble","ec94885f":"## Add month since the last and first sale\nThe code has been simplified to reduce run time, though still may not be optimal -- ideally we don't need to compute max for each month.","6fedf685":"Several shops are duplicates of each other (according to its name). Fix train and test set.","fc6c3f5b":"## Group sale stats in recent\ncreate stats (mean\/var) of sales of certain groups during the past 12 months","f829aa6f":"\n# Feature engineering and data cleaning","1351f9fc":"### Using XGBoost","0f227a2f":"Aggregate train set by shop\/item pairs to calculate target aggreagates, then <b>clip(0,20)<\/b> target value. This way train target will be similar to the test predictions.\n\nDowncast item_cnt_month to float32 -- float16 was too small to perform sum operation.","89de6f8d":"## Monthly sales\nMost of the items in the test set target value should be zero, while train set contains only pairs which were sold or returned in the past. So we expand the train set to include those item-shop pairs with zero monthly sales. This way train data will be similar to test data.","900354f1":"## Trend features","52eaff42":"The distribution of sale grouped by month, item and shop, we see most item-shop pairs have small monthly sale.","76115e54":"# Exploratory Data Analysis","c7387694":"## Final preparations\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).\nLightgbm and XGBboost can deal with missing values, so we will leave the NaNs as it is. Later for neural network, we will fill na with 0.","5ca0757d":"## Test set\nTo use time tricks append test pairs to the matrix.","704d8a6d":"## Using LGB","fd2c667e":"Test set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs. 363 items are new compared to the train. ","bc2c0d13":"Check for missing values."}}