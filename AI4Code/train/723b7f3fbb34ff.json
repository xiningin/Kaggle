{"cell_type":{"44ba5e72":"code","fffa8329":"code","b5b5401a":"code","b2207880":"code","d8f61cd0":"code","f76fe19c":"code","49d4ab14":"code","4c2a3db0":"code","5b190cd4":"code","4a7070dc":"code","47dbad8c":"code","0f1b9f22":"code","ab137c89":"code","57623c46":"code","4ee5bdbf":"code","33937989":"code","b5918963":"code","dc7d6b72":"code","32f0287d":"code","71f40250":"code","a84aeba7":"code","36c2d22c":"code","eb1daf6f":"code","76105857":"code","0c17ba90":"code","d34e0b5c":"code","2cc0cc59":"code","e11234ea":"code","ae017e3d":"code","1daee765":"code","419620b2":"code","59503375":"code","df38a201":"code","24a4cb64":"code","ac9be8d5":"code","9079ba2a":"code","11e22d5c":"code","eb092b26":"code","850afd9c":"code","4d74aef4":"code","1fdb3084":"code","c84f580d":"code","cca8c858":"code","9702667f":"code","6f54544f":"code","28618a04":"code","5d552b5c":"code","8d169ab0":"code","54d084a2":"code","9dea445c":"markdown","3756ed05":"markdown","1bc0adad":"markdown","e95d3db8":"markdown","18167279":"markdown","4b92346f":"markdown","41cea705":"markdown","94a071d5":"markdown","0b76ddea":"markdown","bfda2f0b":"markdown","5b3531af":"markdown","94e7618b":"markdown","07f04e64":"markdown","f2f793b8":"markdown","039fb0ba":"markdown","8409220b":"markdown","79b3a091":"markdown"},"source":{"44ba5e72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport math\nimport matplotlib as p\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy.stats as sps\nimport re","fffa8329":"train = pd.read_csv('..\/input\/widsdatathon2020\/training_v2.csv')\ntest = pd.read_csv('..\/input\/widsdatathon2020\/unlabeled.csv')\nst = pd.read_csv('..\/input\/widsdatathon2020\/solution_template.csv')\nss = pd.read_csv('..\/input\/widsdatathon2020\/samplesubmission.csv')\ndictionary = pd.read_csv('..\/input\/widsdatathon2020\/WiDS Datathon 2020 Dictionary.csv')\n\npd.set_option('display.max_columns', 500)\nprint('solution template shape', st.shape)\ndisplay(st.head())\nprint('dictionary shape', dictionary.shape)\ndisplay(dictionary.T.head())\nprint('train shape', train.shape)\ndisplay(train.head())\nprint('test shape', test.shape)\ndisplay(test.head())","b5b5401a":"# Dropping patient_id for now\ntrain = train.copy().drop('patient_id', axis = 1)\ntest = test.copy().drop('patient_id', axis = 1)","b2207880":"from sklearn.model_selection import train_test_split\n\nTrain, Validation = train_test_split(train, test_size = 0.3)","d8f61cd0":"X_train = Train.copy().drop('hospital_death', axis = 1)\ny_train = Train[['encounter_id','hospital_death']]\nX_val = Validation.copy().drop('hospital_death', axis = 1)\ny_val = Validation[['encounter_id','hospital_death']]","f76fe19c":"X_test = test.copy().drop('hospital_death', axis = 1)\ny_test = test[['encounter_id','hospital_death']]","49d4ab14":"sns.catplot('hospital_death', data= train, kind='count', alpha=0.7, height=6, aspect=1)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = train['hospital_death'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Hospital Deaths', fontsize = 20, color = 'black')\nplt.show()","4c2a3db0":"plt.figure(figsize=(30,15))\nethnicity_vs_death = sns.catplot(x='ethnicity', col='hospital_death', kind='count', data=train, \n                                 order = train['ethnicity'].value_counts().index, height = 7, aspect = 1);\nethnicity_vs_death.set_xticklabels(rotation=90);","5b190cd4":"plt.figure(figsize=(30,15))\nhas_vs_death = sns.catplot(x='hospital_admit_source', col='hospital_death', kind='count', data=train, \n                           order = train['hospital_admit_source'].value_counts().index, height = 7, aspect = 1.5);\nhas_vs_death.set_xticklabels(rotation=90);","4a7070dc":"plt.figure(figsize=(30,15))\nias_vs_death = sns.catplot(x='icu_admit_source', col='hospital_death', kind='count', data=train, \n                           order = train['icu_admit_source'].value_counts().index, height = 7, aspect = 1.5);\nias_vs_death.set_xticklabels(rotation=90);","47dbad8c":"# Freq plot of Hospital ID for hospital_death = 0\ntrain_hID_0 = train[train['hospital_death'] == 0]\nplt.figure(figsize=(30,15))\nhID_vs_death = sns.catplot(y='hospital_id',  orient = \"v\", kind='count', data=train_hID_0, order = train_hID_0['hospital_id'].value_counts().index, \n                           height = 30, aspect = 1)","0f1b9f22":"# Freq plot of Hospital ID for hospital_death = 1\ntrain_hID_1 = train[train['hospital_death'] != 0]\nplt.figure(figsize=(30,20))\nhID_vs_death = sns.catplot(y='hospital_id',  orient = \"v\", kind='count', data=train_hID_1, order = train_hID_1['hospital_id'].value_counts().index, \n                           height = 30, aspect = 1);","ab137c89":"# Freq plot of Hospital ID for hospital_death = 0 & 1\nplt.figure(figsize=(30,40))\nhID_vs_death = sns.catplot(x = 'hospital_id', col='hospital_death', kind='count', data=train, order = train['hospital_id'].value_counts().index, \n                           height = 5, aspect = 2.8);\nhID_vs_death.set_xticklabels(rotation=90);","57623c46":"plt.figure(figsize = (15,5))\nsns.kdeplot(train_hID_1['age'], shade=True, color=\"r\")\nsns.kdeplot(train_hID_0['age'], shade=True, color=\"b\")","4ee5bdbf":"sns.jointplot(x=\"age\", y=\"bmi\", data=train, kind = \"kde\")\nsns.jointplot(x=\"age\", y=\"height\", data=train, kind = \"kde\")\nsns.jointplot(x=\"age\", y=\"weight\", data=train, kind = \"kde\")","33937989":"dataset = pd.concat(objs=[X_train, X_val], axis=0)","b5918963":"col_1 = dataset.columns","dc7d6b72":"for i in col_1:\n    if X_train[i].nunique() == 1:\n        print('in Train', i)\n    if X_val[i].nunique() == 1:\n        print('in Val', i)\n    if X_test[i].nunique() == 1:\n        print('in Test', i)\n    ","32f0287d":"# Dropping 'readmission_status'\nX_train = X_train.drop(['readmission_status'], axis=1)\nX_val = X_val.drop(['readmission_status'], axis=1)\nX_test = X_test.drop(['readmission_status'], axis=1)","71f40250":"print('For Train')\nd1 = X_train.nunique()\nprint(sorted(d1))\nprint(\"==============================\")\nprint('For Validation')\nd2 = X_val.nunique()\nprint(sorted(d2))\n\n# Considering columns with <= 15 unique values for conversion","a84aeba7":"d = pd.concat(objs=[X_train, X_val], axis=0)","36c2d22c":"col = d.columns ","eb1daf6f":"# For Train data\nl1 = []\nfor i in col:\n    if X_train[i].nunique() <= 15:\n        l1.append(i)\n        \nl1","76105857":"# For Val data\nl2 = []\nfor i in col:\n    if X_val[i].nunique() <= 15:\n        l2.append(i)\n        \nl2","0c17ba90":"# For Test data\nl3 = []\nfor i in col:\n    if X_test[i].nunique() <= 15:\n        l3.append(i)\n        \nl3","d34e0b5c":"# Checking for columns in X_train and X_validation\nset(l1) & set(l2)","2cc0cc59":"# Checking for columns in X_train and X_test\nset(l1) & set(l3)","e11234ea":"print('Train', len(l1))\nprint('Validation', len(l2))\nprint('Common', len(set(l1) & set(l2)))","ae017e3d":"print('Train', len(l1))\nprint('Test', len(l3))\nprint('Common', len(set(l1) & set(l3)))","1daee765":"X_train[l1].dtypes","419620b2":"X_val[l2].dtypes\n# Not a necessary step since we already confirmed the common columns. Included just for reference. ","59503375":"X_train[l1] = pd.Categorical(X_train[l1])\nX_val[l2] = pd.Categorical(X_val[l2])\nX_test[l3] = pd.Categorical(X_test[l3])\nprint('Train dtypes:')\nprint(X_train[l1].dtypes)\nprint('======================================')\nprint('Validation dtypes:')\nprint(X_val[l2].dtypes)\nprint('======================================')\nprint('Test dtypes:')\nprint(X_test[l3].dtypes)","df38a201":"# On train data\npd.set_option('display.max_rows', 500)\nNA_col = pd.DataFrame(X_train.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(X_train))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","24a4cb64":"# On val data\npd.set_option('display.max_rows', 500)\nNA_col = pd.DataFrame(X_val.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(X_val))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","ac9be8d5":"# On test data\npd.set_option('display.max_rows', 500)\nNA_col = pd.DataFrame(X_test.isna().sum(), columns = ['NA_Count'])\nNA_col['% of NA'] = (NA_col.NA_Count\/len(X_test))*100\nNA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')","9079ba2a":"cols = X_train.columns\nnum_cols = X_train._get_numeric_data().columns\ncat_cols = list(set(cols) - set(num_cols))\ncat_cols","11e22d5c":"# Courtesy: https:\/\/www.kaggle.com\/jayjay75\/wids2020-lgb-starter-script\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfor usecol in cat_cols:\n    X_train[usecol] = X_train[usecol].astype('str')\n    X_val[usecol] = X_val[usecol].astype('str')\n    X_test[usecol] = X_test[usecol].astype('str')\n    \n    #Fit LabelEncoder\n    le = LabelEncoder().fit(\n            np.unique(X_train[usecol].unique().tolist()+\n                      X_val[usecol].unique().tolist()+\n                     X_test[usecol].unique().tolist()))\n\n    #At the end 0 will be used for dropped values\n    X_train[usecol] = le.transform(X_train[usecol])+1\n    X_val[usecol]  = le.transform(X_val[usecol])+1\n    X_test[usecol]  = le.transform(X_test[usecol])+1\n    \n    X_train[usecol] = X_train[usecol].replace(np.nan, 0).astype('int').astype('category')\n    X_val[usecol]  = X_val[usecol].replace(np.nan, 0).astype('int').astype('category')\n    X_test[usecol]  = X_test[usecol].replace(np.nan, 0).astype('int').astype('category')","eb092b26":"X_train.set_index('encounter_id', inplace = True)\ny_train.set_index('encounter_id', inplace = True)\nX_val.set_index('encounter_id', inplace = True)\ny_val.set_index('encounter_id', inplace = True)\nX_test.set_index('encounter_id', inplace = True)\ny_test.set_index('encounter_id', inplace = True)","850afd9c":"# y_test.hospital_death = y_test.hospital_death.fillna(0)","4d74aef4":"# y_train['hospital_death'] = pd.Categorical(y_train['hospital_death'])\n# y_train.dtypes","1fdb3084":"# y_test['hospital_death'] = pd.Categorical(y_test['hospital_death'])\n# y_test.dtypes","c84f580d":"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n# from pandas import Series\n# l=LabelEncoder() \n# l.fit(y_train['hospital_death']) \n# l.classes_ \n# y_train['hospital_death']=Series(l.transform(y_train['hospital_death']))  #label encoding our target variable \n# y_train['hospital_death'].value_counts() ","cca8c858":"# l.fit(y_test['hospital_death']) \n# l.classes_ \n#y_test['hospital_death'].fillna(0.0, inplace = True)\n# y_test['hospital_death']=Series(l.transform(y_test['hospital_death']))  #label encoding our target variable \n# y_test['hospital_death'].value_counts() ","9702667f":"import lightgbm as lgbm\n\nlgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)\n# lgbm_test = lgbm.Dataset(X_test, y_test, categorical_feature=cat_cols)\nlgbm_val = lgbm.Dataset(X_val, y_val, reference = lgbm_train)","6f54544f":"params = {'feature_fraction': 0.9,\n          'lambda_l1': 1,\n          'lambda_l2': 1,\n          'learning_rate': 0.01,\n          'max_depth': 10,\n          'metric': 'auc',\n          'num_leaves': 500,\n          'min_data_in_leaf': 100,\n          'subsample_freq': 1,\n          'scale_pos_weight':1,\n          'metric': 'auc',\n          'is_unbalance': 'true',\n          'boosting': 'gbdt',\n          'bagging_fraction': 0.5,\n          'bagging_freq': 10,}","28618a04":"evals_result = {}  # to record eval results for plotting\nmodel_lgbm = lgbm.train(params,\n                lgbm_train,\n                num_boost_round=100,\n                valid_sets=[lgbm_train, lgbm_val],\n                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],\n                categorical_feature= [182],\n                evals_result=evals_result,\n                verbose_eval=10)","5d552b5c":"ax = lgbm.plot_metric(evals_result, metric='auc', figsize=(15, 8))\nplt.show()","8d169ab0":"test[\"hospital_death\"] = model_lgbm.predict(X_test, predition_type = 'Probability')\ntest[[\"encounter_id\",\"hospital_death\"]].to_csv(\"submission_lgbm.csv\",index=False)","54d084a2":"test[[\"encounter_id\",\"hospital_death\"]].head()","9dea445c":"**5. Hospital ID vs Hospital Deaths**","3756ed05":"## Extracting column with unique values <= 15","1bc0adad":"## Checking for columns with only one value","e95d3db8":"# Model Building","18167279":"## Reading and Preview","4b92346f":"## Checking NAs","41cea705":"**7. Age vs Bmi, Height, Weight**","94a071d5":"**4. ICU Admit Source vs Hospital Deaths**","0b76ddea":"**2. Ethnicity vs Hospital Deaths**","bfda2f0b":"**6. Freq plot of Age for hospital_death 0 and 1**","5b3531af":"## Checking the number of unique values for remaining columns","94e7618b":"**1. Frequency plot of Hospital Deaths**","07f04e64":"## Checking the data types of these columns for conversion to Categorical","f2f793b8":"# Visualization","039fb0ba":"# Libraries and Reading Data","8409220b":"**3. Hospital Admit Source vs Hospital Deaths**","79b3a091":"## Converting to Categorical"}}