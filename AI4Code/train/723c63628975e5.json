{"cell_type":{"fcfa34c8":"code","e5de11ad":"code","7dd62966":"code","03294543":"code","8794b870":"code","185f9c8d":"code","fde21e6b":"code","1e69cd8a":"code","02c4ffb8":"code","c8f5abc9":"code","6d6ac716":"code","3bd6d895":"code","afdac419":"code","8479d3e4":"code","c1c87a30":"code","ae5c6051":"code","8c31ae02":"code","dbff0606":"code","ec7473c1":"code","46eec68a":"code","04f85a4e":"code","30a02472":"code","b9565b02":"code","0ede929f":"code","718e3089":"code","bb234a7a":"code","5d177134":"code","e0e78d80":"code","e7c2862e":"code","d832fc03":"code","1d0f2a0c":"code","d96a3ad9":"code","5047b17e":"code","e5341c36":"code","3a737f06":"code","71e51242":"code","b6d716d8":"code","40c28d4b":"code","34119699":"code","bd0b3341":"code","71cc9933":"code","8570e748":"code","3496e63e":"code","cface909":"code","4a5bc203":"code","61a079b3":"code","029b018e":"code","ca3a9c0c":"code","07da4e6b":"markdown","5aa70ead":"markdown","d13e5ac9":"markdown","de83aa43":"markdown","c77c1ebc":"markdown","fc00a8c5":"markdown","5215eafe":"markdown","1917f7a4":"markdown","65482e01":"markdown","9fb2851f":"markdown","324ea590":"markdown","4c9263ec":"markdown","d1f11060":"markdown","af81a9e9":"markdown","4edfe38d":"markdown","69e37f5f":"markdown","8fa88b2a":"markdown","bda29667":"markdown","46814436":"markdown"},"source":{"fcfa34c8":"import pandas as pd\nimport numpy as np\n\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, AvgPool2D, BatchNormalization, Dropout\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\n\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nsns.set_style ('darkgrid')\n%matplotlib inline\nnp.random.seed(42)","e5de11ad":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","7dd62966":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","03294543":"train.head()","8794b870":"test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","185f9c8d":"test.head()","fde21e6b":"features = train.drop('label', axis=1)","1e69cd8a":"target = train['label']","02c4ffb8":"X_ = np.array(features)","c8f5abc9":"X_test = np.array(test)","6d6ac716":"X_train = X_.reshape(X_.shape[0], 28, 28)","3bd6d895":"X_train.shape","afdac419":"fig = plt.figure(figsize=(10,5))\n\nfor i in range(16):\n    fig.add_subplot(4, 4, i+1)\n    \n    plt.imshow(X_train[i], cmap='gray')\n    \n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n    plt.title('Digit: ' + str(target[i]))\n","8479d3e4":"target.value_counts(normalize=True)","c1c87a30":"len(target.value_counts())","ae5c6051":"X_train = X_.reshape(X_.shape[0], 28, 28, 1)\nX_test_reshape = X_test.reshape(X_test.shape[0], 28, 28, 1)\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, target, test_size=0.25, \n                                            random_state=42)","8c31ae02":"model = Sequential()\n\n\nmodel.add(Conv2D(filters = 64, \n                 kernel_size = (5,5), \n                 padding = 'same', \n                 activation ='elu', \n                 input_shape = (28,28,1)))\n\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(filters = 64, \n                 kernel_size = (5,5), \n                 padding = 'same', \n                 activation ='elu', \n                 input_shape = (28,28,1)))\n\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(AvgPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters = 64, \n                 kernel_size = (5,5), \n                 padding = 'valid', \n                 activation ='elu'))\n\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(filters = 32, \n                 kernel_size = (3,3), \n                 padding = 'valid', \n                 activation ='elu'))\n\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(AvgPool2D(pool_size=(2,2)))\n\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(300, activation = \"elu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(200, activation = \"elu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(100, activation = \"elu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(10, activation = \"softmax\"))\n\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', \n              metrics=['acc'])","dbff0606":"model.summary()","ec7473c1":"plot_model(model, show_shapes=True, show_layer_names=False)","46eec68a":"early_stopping = EarlyStopping(\n    min_delta=0.0002,\n    mode='min', \n    patience=20,\n    restore_best_weights=True,\n)","04f85a4e":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","30a02472":"history = model.fit(X_tr, y_tr, \n          validation_data=(X_val, y_val),\n          verbose=1, epochs=75, batch_size=16,\n          callbacks=[early_stopping])\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))","b9565b02":"X_test_reshape.shape","0ede929f":"preds = np.argmax(model.predict(X_test_reshape), axis=1)","718e3089":"submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","bb234a7a":"submission.shape","5d177134":"submission['Label'] = preds\nsubmission.to_csv('my_submission_keras.csv',index=False)\n\n\nsubmission.head()","e0e78d80":"tf.reset_default_graph()","e7c2862e":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","d832fc03":"INPUTS = 28 * 28 # MNIST size\n\nHIDDEN_1 = 300\nHIDDEN_2 = 100\nOUTPUTS = 10","1d0f2a0c":"X = tf.placeholder(tf.float32, shape=(None, INPUTS), name='X')\ny = tf.placeholder(tf.int32, shape=(None), name='y')","d96a3ad9":"with tf.name_scope('DNN'):\n  hidden1 = tf.layers.dense(X, HIDDEN_1, name='hidden_1', \n                    activation = tf.nn.leaky_relu)\n  hidden2 = tf.layers.dense(hidden1, HIDDEN_2, name='hidden_2', \n                    activation = tf.nn.leaky_relu)\n  logits = tf.layers.dense(hidden2, OUTPUTS, name='outputs')","5047b17e":"with tf.name_scope('loss_func'):\n  cross_entr = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n                                                              logits=logits)\n  loss = tf.reduce_mean(tf.cast(cross_entr, tf.float32))","e5341c36":"with tf.name_scope('train'):\n  optimizer = tf.train.RMSPropOptimizer(learning_rate=0.01)\n  training = optimizer.minimize(loss)","3a737f06":"with tf.name_scope('eval'):\n  valid = tf.nn.in_top_k(logits, y, 1)\n  accuracy_score = tf.reduce_mean(tf.cast(valid, tf.float32))","71e51242":"epochs = 5\nbatch_size = 50","b6d716d8":"X_train = features.to_numpy().astype(np.float32).reshape(-1, 28*28) \/ 255.0\nX_test = test.to_numpy().astype(np.float32).reshape(-1, 28*28) \/ 255.0\ny_train = target.to_numpy().astype(np.int32)\nX_train.shape, y_train.shape","40c28d4b":"X_valid, X_train = X_train[:5000], X_train[5000:]\ny_valid, y_train = y_train[:5000], y_train[5000:]","34119699":"X_valid.shape","bd0b3341":"def shuffle(X, y, batch_size):\n    idx = np.random.permutation(len(X))\n    rnd_batches = len(X) \/\/ batch_size\n    for batch_idx in np.array_split(idx, rnd_batches):\n        X_batch, y_batch = X[idx], y[idx]\n        yield X_batch, y_batch","71cc9933":"init = tf.global_variables_initializer()\nsaver = tf.train.Saver()","8570e748":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","3496e63e":"%%time\n\nnow = datetime.utcnow().strftime('Y%m%d %H%M%S')\nroot_logdir = 'tf_logs'\nlogdir = '{}\/run-{}\/'.format(root_logdir, now)\n\ntf.debugging.set_log_device_placement(True)\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n\nwith tf.Session() as sess:\n\n  init.run()\n\n  for epoch in range(epochs):\n\n      for X_batch, y_batch in shuffle(X_train, y_train, batch_size):\n        sess.run(training, feed_dict={X: X_batch, y: y_batch})\n\n      if epoch % 1 == 0:\n\n        acc_train = accuracy_score.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_valid = accuracy_score.eval(feed_dict={X: X_valid, y: y_valid})\n\n        print(epoch, 'Accuracy on training', acc_train, \n                     'Accuracy on validation', acc_valid)\n\n  save_path = saver.save(sess, '.\/model.ckpt')","cface909":"file_writer.close()","4a5bc203":"# %load_ext tensorboard\n# %tensorboard --logdir logs --bind_all","61a079b3":"with tf.Session() as sess:\n    saver.restore(sess, '.\/model.ckpt')\n    Z = logits.eval(feed_dict={X: X_test})\n    y_pred = np.argmax(Z, axis=1)","029b018e":"submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","ca3a9c0c":"submission['Label'] = y_pred\nsubmission.to_csv('my_submission_tensorflow.csv',index=False)\n\n\nsubmission.head()","07da4e6b":"Training (with GPU) (let us use only 5 epochs to compute result faster)","5aa70ead":"Now we have to check the count of values for our output layer","d13e5ac9":"to make sure we use GPU","de83aa43":"We have to reshape our data","c77c1ebc":"Restore our model and predict our Test","fc00a8c5":"## CNN\n","5215eafe":"Let us build our CNN. We do not need millions of params","1917f7a4":"Results from Google Collab:\n\n![](https:\/\/i.ibb.co\/4s4K6Xf\/2021-09-03-23-04-10.png)","65482e01":"## TensorFlow","9fb2851f":"We indicate the number of neurons in layers, inputs and outputs","324ea590":"Let us try to do it via only Tensorflow. Just for practice - want to share some hints.  \n\nThanks to my favourite book: Hands-On Machine Learning with Scikit-Learn and TensorFlow - I love this book","4c9263ec":"This func will help us to make batches for training","d1f11060":"Thank you for reading. Good luck with learning. You can add some layers to CNN and it will improve score","af81a9e9":"It is for runnig tensorboard. But now support is disabled","4edfe38d":"Let us take a look to out objects","69e37f5f":"Create placeholder nodes","8fa88b2a":"![](https:\/\/i.ibb.co\/3vF9yD8\/Screenshot-from-2019-05-29-21-23-47.png)\n\n**MNIST (\"Modified National Institute of Standards and Technology\")** is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\n\nUsing CNN Keras and Tensorflow (v1) DNN Baseline with GPU. You can try to add more layers, you can try to change learning rate. Thank you for reading\n","bda29667":"Now we can create layers","46814436":"logits - output of neurons before passing through softmax"}}