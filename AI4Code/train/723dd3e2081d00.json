{"cell_type":{"dca2e8be":"code","8640d798":"code","b2fdd876":"code","82b24b01":"code","4225401e":"code","57d55695":"code","5f979f31":"code","acf8eb1d":"code","5cf5ba71":"code","907f8cd5":"code","b1d5d0c1":"code","d27464a4":"code","ad6e4ed6":"code","538c531f":"code","35548906":"code","c0c48a52":"code","58a27535":"code","d4ce1ab1":"code","26012c21":"code","f2d394c9":"code","1d26a94a":"code","0ce06734":"code","6cdcfd16":"code","b8c37c93":"code","288e0b15":"code","19bfe729":"code","75ae082b":"markdown","f892fb31":"markdown","983af982":"markdown","7bc1bc13":"markdown","27d01ac3":"markdown"},"source":{"dca2e8be":"# for TPU\n# !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","8640d798":"# for TPU\n# import torch_xla\n# import torch_xla.core.xla_model as xm","b2fdd876":"!export CUDA_LAUNCH_BLOCKING=1","82b24b01":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","4225401e":"import math, random\nimport gc, os\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.kernel_approximation import Nystroem\nfrom sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import FeatureAgglomeration, AgglomerativeClustering, KMeans\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA \nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')","57d55695":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ncols = [c for c in ss.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","5f979f31":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n#     df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\n# def log_loss_metric(y_true, y_pred):\n#     metrics = []\n#     for _target in train_targets.columns:\n#         metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n#     return np.mean(metrics)\n\ndef log_loss_metric(y_true, y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    for i in range(y_true.shape[1]):\n        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss \/ y_true.shape[1]\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","acf8eb1d":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(output_distribution = 'normal', random_state = 42)\nqt.fit(pd.concat([pd.DataFrame(train[GENES+CELLS]), pd.DataFrame(test[GENES+CELLS])]))\ntrain[GENES+CELLS] = qt.transform(train[GENES+CELLS])\ntest[GENES+CELLS] = qt.transform(test[GENES+CELLS])","5cf5ba71":"train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)","907f8cd5":"top_feats = np.arange(1, train.shape[1])\nprint(top_feats)","b1d5d0c1":"train.head()","d27464a4":"def create_folds(num_starts, num_splits):\n    \n    folds = []\n    \n    # LOAD FILES\n    train_feats = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    drug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\n    scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on='sig_id', how='left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    \n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n        \n        del scored['fold']\n        \n    return np.stack(folds)","ad6e4ed6":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","538c531f":"nfolds = 5\nnstarts = 1\nnepochs = 1000\nbatch_size = 128\nval_batch_size = batch_size * 4\nntargets = train_targets.shape[1]\ntargets = [col for col in train_targets.columns]\ncriterion = nn.BCELoss()\n\n# for GPU\/CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# for TPU\n# device = xm.xla_device()\n# torch.set_default_tensor_type('torch.FloatTensor')","35548906":"import contextlib\nfrom torch.autograd import Function\nfrom collections import OrderedDict\nfrom torch.jit import script\nfrom warnings import warn\nimport os\nimport glob\nimport hashlib\nimport gc\nimport time\nimport requests\n\ndef to_one_hot(y, depth=None):\n    r\"\"\"\n    Takes integer with n dims and converts it to 1-hot representation with n + 1 dims.\n    The n+1'st dimension will have zeros everywhere but at y'th index, where it will be equal to 1.\n    Args:\n        y: input integer (IntTensor, LongTensor or Variable) of any shape\n        depth (int):  the size of the one hot dimension\n    \"\"\"\n    y_flat = y.to(torch.int64).view(-1, 1)\n    depth = depth if depth is not None else int(torch.max(y_flat)) + 1\n    y_one_hot = torch.zeros(y_flat.size()[0], depth, device=y.device).scatter_(1, y_flat, 1)\n    y_one_hot = y_one_hot.view(*(tuple(y.shape) + (-1,)))\n    return y_one_hot\n\n\ndef _make_ix_like(input, dim=0):\n    d = input.size(dim)\n    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n    view = [1] * input.dim()\n    view[0] = -1\n    return rho.view(view).transpose(0, dim)\n\n\nclass SparsemaxFunction(Function):\n    \"\"\"\n    An implementation of sparsemax (Martins & Astudillo, 2016). See\n    :cite:`DBLP:journals\/corr\/MartinsA16` for detailed description.\n    By Ben Peters and Vlad Niculae\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input, dim=-1):\n        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n        Parameters:\n            input (Tensor): any shape\n            dim: dimension along which to apply sparsemax\n        Returns:\n            output (Tensor): same shape as input\n        \"\"\"\n        ctx.dim = dim\n        max_val, _ = input.max(dim=dim, keepdim=True)\n        input -= max_val  # same numerical stability trick as for softmax\n        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n        output = torch.clamp(input - tau, min=0)\n        ctx.save_for_backward(supp_size, output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output = ctx.saved_tensors\n        dim = ctx.dim\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n\n        v_hat = grad_input.sum(dim=dim) \/ supp_size.to(output.dtype).squeeze()\n        v_hat = v_hat.unsqueeze(dim)\n        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n        return grad_input, None\n\n\n    @staticmethod\n    def _threshold_and_support(input, dim=-1):\n        \"\"\"Sparsemax building block: compute the threshold\n        Args:\n            input: any dimension\n            dim: dimension along which to apply the sparsemax\n        Returns:\n            the threshold value\n        \"\"\"\n\n        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n        input_cumsum = input_srt.cumsum(dim) - 1\n        rhos = _make_ix_like(input, dim)\n        support = rhos * input_srt > input_cumsum\n\n        support_size = support.sum(dim=dim).unsqueeze(dim)\n        tau = input_cumsum.gather(dim, support_size - 1)\n        tau \/= support_size.to(input.dtype)\n        return tau, support_size\n\n\nsparsemax = lambda input, dim=-1: SparsemaxFunction.apply(input, dim)\nsparsemoid = lambda input: (0.5 * input + 0.5).clamp_(0, 1)\n\n\nclass Entmax15Function(Function):\n    \"\"\"\n    An implementation of exact Entmax with alpha=1.5 (B. Peters, V. Niculae, A. Martins). See\n    :cite:`https:\/\/arxiv.org\/abs\/1905.05702 for detailed description.\n    Source: https:\/\/github.com\/deep-spin\/entmax\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input, dim=-1):\n        ctx.dim = dim\n\n        max_val, _ = input.max(dim=dim, keepdim=True)\n        input = input - max_val  # same numerical stability trick as for softmax\n        input = input \/ 2  # divide by 2 to solve actual Entmax\n\n        tau_star, _ = Entmax15Function._threshold_and_support(input, dim)\n        output = torch.clamp(input - tau_star, min=0) ** 2\n        ctx.save_for_backward(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        Y, = ctx.saved_tensors\n        gppr = Y.sqrt()  # = 1 \/ g'' (Y)\n        dX = grad_output * gppr\n        q = dX.sum(ctx.dim) \/ gppr.sum(ctx.dim)\n        q = q.unsqueeze(ctx.dim)\n        dX -= q * gppr\n        return dX, None\n\n    @staticmethod\n    def _threshold_and_support(input, dim=-1):\n        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n\n        rho = _make_ix_like(input, dim)\n        mean = Xsrt.cumsum(dim) \/ rho\n        mean_sq = (Xsrt ** 2).cumsum(dim) \/ rho\n        ss = rho * (mean_sq - mean ** 2)\n        delta = (1 - ss) \/ rho\n\n        # NOTE this is not exactly the same as in reference algo\n        # Fortunately it seems the clamped values never wrongly\n        # get selected by tau <= sorted_z. Prove this!\n        delta_nz = torch.clamp(delta, 0)\n        tau = mean - torch.sqrt(delta_nz)\n\n        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n        tau_star = tau.gather(dim, support_size - 1)\n        return tau_star, support_size\n\n\nclass Entmoid15(Function):\n    \"\"\" A highly optimized equivalent of labda x: Entmax15([x, 0]) \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        output = Entmoid15._forward(input)\n        ctx.save_for_backward(output)\n        return output\n\n    @staticmethod\n    @script\n    def _forward(input):\n        input, is_pos = abs(input), input >= 0\n        tau = (input + torch.sqrt(F.relu(8 - input ** 2))) \/ 2\n        tau.masked_fill_(tau <= input, 2.0)\n        y_neg = 0.25 * F.relu(tau - input, inplace=True) ** 2\n        return torch.where(is_pos, 1 - y_neg, y_neg)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return Entmoid15._backward(ctx.saved_tensors[0], grad_output)\n\n    @staticmethod\n    @script\n    def _backward(output, grad_output):\n        gppr0, gppr1 = output.sqrt(), (1 - output).sqrt()\n        grad_input = grad_output * gppr0\n        q = grad_input \/ (gppr0 + gppr1)\n        grad_input -= q * gppr0\n        return grad_input\n\n\nentmax15 = lambda input, dim=-1: Entmax15Function.apply(input, dim)\nentmoid15 = Entmoid15.apply\n\n\nclass Lambda(nn.Module):\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n\nclass ModuleWithInit(nn.Module):\n    \"\"\" Base class for pytorch module with data-aware initializer on first batch \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._is_initialized_tensor = nn.Parameter(torch.tensor(0, dtype=torch.uint8), requires_grad=False)\n        self._is_initialized_bool = None\n        # Note: this module uses a separate flag self._is_initialized so as to achieve both\n        # * persistence: is_initialized is saved alongside model in state_dict\n        # * speed: model doesn't need to cache\n        # please DO NOT use these flags in child modules\n\n    def initialize(self, *args, **kwargs):\n        \"\"\" initialize module tensors using first batch of data \"\"\"\n        raise NotImplementedError(\"Please implement \")\n\n    def __call__(self, *args, **kwargs):\n        if self._is_initialized_bool is None:\n            self._is_initialized_bool = bool(self._is_initialized_tensor.item())\n        if not self._is_initialized_bool:\n            self.initialize(*args, **kwargs)\n            self._is_initialized_tensor.data[...] = 1\n            self._is_initialized_bool = True\n        return super().__call__(*args, **kwargs)\n\ndef download(url, filename, delete_if_interrupted=True, chunk_size=4096):\n    \"\"\" saves file from url to filename with a fancy progressbar \"\"\"\n    try:\n        with open(filename, \"wb\") as f:\n            print(\"Downloading {} > {}\".format(url, filename))\n            response = requests.get(url, stream=True)\n            total_length = response.headers.get('content-length')\n\n            if total_length is None:  # no content length header\n                f.write(response.content)\n            else:\n                total_length = int(total_length)\n                with tqdm(total=total_length) as progressbar:\n                    for data in response.iter_content(chunk_size=chunk_size):\n                        if data:  # filter-out keep-alive chunks\n                            f.write(data)\n                            progressbar.update(len(data))\n    except Exception as e:\n        if delete_if_interrupted:\n            print(\"Removing incomplete download {}.\".format(filename))\n            os.remove(filename)\n        raise e\n    return filename\n\n\ndef iterate_minibatches(*tensors, batch_size, shuffle=True, epochs=1,\n                        allow_incomplete=True, callback=lambda x:x):\n    indices = np.arange(len(tensors[0]))\n    upper_bound = int((np.ceil if allow_incomplete else np.floor) (len(indices) \/ batch_size)) * batch_size\n    epoch = 0\n    while True:\n        if shuffle:\n            np.random.shuffle(indices)\n        for batch_start in callback(range(0, upper_bound, batch_size)):\n            batch_ix = indices[batch_start: batch_start + batch_size]\n            batch = [tensor[batch_ix] for tensor in tensors]\n            yield batch if len(tensors) > 1 else batch[0]\n        epoch += 1\n        if epoch >= epochs:\n            break\n\n\ndef process_in_chunks(function, *args, batch_size, out=None, **kwargs):\n    \"\"\"\n    Computes output by applying batch-parallel function to large data tensor in chunks\n    :param function: a function(*[x[indices, ...] for x in args]) -> out[indices, ...]\n    :param args: one or many tensors, each [num_instances, ...]\n    :param batch_size: maximum chunk size processed in one go\n    :param out: memory buffer for out, defaults to torch.zeros of appropriate size and type\n    :returns: function(data), computed in a memory-efficient way\n    \"\"\"\n    total_size = args[0].shape[0]\n    first_output = function(*[x[0: batch_size] for x in args])\n    output_shape = (total_size,) + tuple(first_output.shape[1:])\n    if out is None:\n        out = torch.zeros(*output_shape, dtype=first_output.dtype, device=first_output.device,\n                          layout=first_output.layout, **kwargs)\n\n    out[0: batch_size] = first_output\n    for i in range(batch_size, total_size, batch_size):\n        batch_ix = slice(i, min(i + batch_size, total_size))\n        out[batch_ix] = function(*[x[batch_ix] for x in args])\n    return out\n\n\ndef check_numpy(x):\n    \"\"\" Makes sure x is a numpy array \"\"\"\n    if isinstance(x, torch.Tensor):\n        x = x.detach().cpu().numpy()\n    x = np.asarray(x)\n    assert isinstance(x, np.ndarray)\n    return x\n\n\n@contextlib.contextmanager\ndef nop_ctx():\n    yield None\n\n\ndef get_latest_file(pattern):\n    list_of_files = glob.glob(pattern) # * means all if need specific format then *.csv\n    assert len(list_of_files) > 0, \"No files found: \" + pattern\n    return max(list_of_files, key=os.path.getctime)\n\n\ndef md5sum(fname):\n    \"\"\" Computes mdp checksum of a file \"\"\"\n    hash_md5 = hashlib.md5()\n    with open(fname, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n\n\ndef free_memory(sleep_time=0.1):\n    \"\"\" Black magic function to free torch memory and some jupyter whims \"\"\"\n    gc.collect()\n    torch.cuda.synchronize()\n    gc.collect()\n    torch.cuda.empty_cache()\n    time.sleep(sleep_time)\n\ndef to_float_str(element):\n    try:\n        return str(float(element))\n    except ValueError:\n        return element\n    \nclass ODST(ModuleWithInit):\n    def __init__(self, in_features, num_trees, depth=6, tree_dim=1, flatten_output=True,\n                 choice_function=sparsemax, bin_function=sparsemoid,\n                 initialize_response_=nn.init.normal_, initialize_selection_logits_=nn.init.uniform_,\n                 threshold_init_beta=1.0, threshold_init_cutoff=1.0,\n                 ):\n        \"\"\"\n        Oblivious Differentiable Sparsemax Trees. http:\/\/tinyurl.com\/odst-readmore\n        One can drop (sic!) this module anywhere instead of nn.Linear\n        :param in_features: number of features in the input tensor\n        :param num_trees: number of trees in this layer\n        :param tree_dim: number of response channels in the response of individual tree\n        :param depth: number of splits in every tree\n        :param flatten_output: if False, returns [..., num_trees, tree_dim],\n            by default returns [..., num_trees * tree_dim]\n        :param choice_function: f(tensor, dim) -> R_simplex computes feature weights s.t. f(tensor, dim).sum(dim) == 1\n        :param bin_function: f(tensor) -> R[0, 1], computes tree leaf weights\n        :param initialize_response_: in-place initializer for tree output tensor\n        :param initialize_selection_logits_: in-place initializer for logits that select features for the tree\n        both thresholds and scales are initialized with data-aware init (or .load_state_dict)\n        :param threshold_init_beta: initializes threshold to a q-th quantile of data points\n            where q ~ Beta(:threshold_init_beta:, :threshold_init_beta:)\n            If this param is set to 1, initial thresholds will have the same distribution as data points\n            If greater than 1 (e.g. 10), thresholds will be closer to median data value\n            If less than 1 (e.g. 0.1), thresholds will approach min\/max data values.\n        :param threshold_init_cutoff: threshold log-temperatures initializer, \\in (0, inf)\n            By default(1.0), log-remperatures are initialized in such a way that all bin selectors\n            end up in the linear region of sparse-sigmoid. The temperatures are then scaled by this parameter.\n            Setting this value > 1.0 will result in some margin between data points and sparse-sigmoid cutoff value\n            Setting this value < 1.0 will cause (1 - value) part of data points to end up in flat sparse-sigmoid region\n            For instance, threshold_init_cutoff = 0.9 will set 10% points equal to 0.0 or 1.0\n            Setting this value > 1.0 will result in a margin between data points and sparse-sigmoid cutoff value\n            All points will be between (0.5 - 0.5 \/ threshold_init_cutoff) and (0.5 + 0.5 \/ threshold_init_cutoff)\n        \"\"\"\n        super().__init__()\n        self.depth, self.num_trees, self.tree_dim, self.flatten_output = depth, num_trees, tree_dim, flatten_output\n        self.choice_function, self.bin_function = choice_function, bin_function\n        self.threshold_init_beta, self.threshold_init_cutoff = threshold_init_beta, threshold_init_cutoff\n\n        self.response = nn.Parameter(torch.zeros([num_trees, tree_dim, 2 ** depth]), requires_grad=True)\n        initialize_response_(self.response)\n\n        self.feature_selection_logits = nn.Parameter(\n            torch.zeros([in_features, num_trees, depth]), requires_grad=True\n        )\n        initialize_selection_logits_(self.feature_selection_logits)\n\n        self.feature_thresholds = nn.Parameter(\n            torch.full([num_trees, depth], float('nan'), dtype=torch.float32), requires_grad=True\n        )  # nan values will be initialized on first batch (data-aware init)\n\n        self.log_temperatures = nn.Parameter(\n            torch.full([num_trees, depth], float('nan'), dtype=torch.float32), requires_grad=True\n        )\n\n        # binary codes for mapping between 1-hot vectors and bin indices\n        with torch.no_grad():\n            indices = torch.arange(2 ** self.depth)\n            offsets = 2 ** torch.arange(self.depth)\n            bin_codes = (indices.view(1, -1) \/\/ offsets.view(-1, 1) % 2).to(torch.float32)\n            bin_codes_1hot = torch.stack([bin_codes, 1.0 - bin_codes], dim=-1)\n            self.bin_codes_1hot = nn.Parameter(bin_codes_1hot, requires_grad=False)\n            # ^-- [depth, 2 ** depth, 2]\n\n    def forward(self, input):\n        assert len(input.shape) >= 2\n        if len(input.shape) > 2:\n            return self.forward(input.view(-1, input.shape[-1])).view(*input.shape[:-1], -1)\n        # new input shape: [batch_size, in_features]\n\n        feature_logits = self.feature_selection_logits\n        feature_selectors = self.choice_function(feature_logits, dim=0)\n        # ^--[in_features, num_trees, depth]\n\n        feature_values = torch.einsum('bi,ind->bnd', input, feature_selectors)\n        # ^--[batch_size, num_trees, depth]\n\n        threshold_logits = (feature_values - self.feature_thresholds) * torch.exp(-self.log_temperatures)\n\n        threshold_logits = torch.stack([-threshold_logits, threshold_logits], dim=-1)\n        # ^--[batch_size, num_trees, depth, 2]\n\n        bins = self.bin_function(threshold_logits)\n        # ^--[batch_size, num_trees, depth, 2], approximately binary\n\n        bin_matches = torch.einsum('btds,dcs->btdc', bins, self.bin_codes_1hot)\n        # ^--[batch_size, num_trees, depth, 2 ** depth]\n\n        response_weights = torch.prod(bin_matches, dim=-2)\n        # ^-- [batch_size, num_trees, 2 ** depth]\n\n        response = torch.einsum('bnd,ncd->bnc', response_weights, self.response)\n        # ^-- [batch_size, num_trees, tree_dim]\n\n        return response.flatten(1, 2) if self.flatten_output else response\n\n    def initialize(self, input, eps=1e-6):\n        # data-aware initializer\n        assert len(input.shape) == 2\n        if input.shape[0] < 1000:\n            warn(\"Data-aware initialization is performed on less than 1000 data points. This may cause instability.\"\n                 \"To avoid potential problems, run this model on a data batch with at least 1000 data samples.\"\n                 \"You can do so manually before training. Use with torch.no_grad() for memory efficiency.\")\n        with torch.no_grad():\n            feature_selectors = self.choice_function(self.feature_selection_logits, dim=0)\n            # ^--[in_features, num_trees, depth]\n\n            feature_values = torch.einsum('bi,ind->bnd', input, feature_selectors)\n            # ^--[batch_size, num_trees, depth]\n\n            # initialize thresholds: sample random percentiles of data\n            percentiles_q = 100 * np.random.beta(self.threshold_init_beta, self.threshold_init_beta,\n                                                 size=[self.num_trees, self.depth])\n            self.feature_thresholds.data[...] = torch.as_tensor(\n                list(map(np.percentile, check_numpy(feature_values.flatten(1, 2).t()), percentiles_q.flatten())),\n                dtype=feature_values.dtype, device=feature_values.device\n            ).view(self.num_trees, self.depth)\n\n            # init temperatures: make sure enough data points are in the linear region of sparse-sigmoid\n            temperatures = np.percentile(check_numpy(abs(feature_values - self.feature_thresholds)),\n                                         q=100 * min(1.0, self.threshold_init_cutoff), axis=0)\n\n            # if threshold_init_cutoff > 1, scale everything down by it\n            temperatures \/= max(1.0, self.threshold_init_cutoff)\n            self.log_temperatures.data[...] = torch.log(torch.as_tensor(temperatures) + eps)\n\n    def __repr__(self):\n        return \"{}(in_features={}, num_trees={}, depth={}, tree_dim={}, flatten_output={})\".format(\n            self.__class__.__name__, self.feature_selection_logits.shape[0],\n            self.num_trees, self.depth, self.tree_dim, self.flatten_output\n        )\n    \nclass DenseBlock(nn.Sequential):\n    def __init__(self, input_dim, layer_dim, num_layers, tree_dim=1, max_features=None,\n                 input_dropout=0.0, flatten_output=True, Module=ODST, **kwargs):\n        layers = []\n        for i in range(num_layers):\n            oddt = Module(input_dim, layer_dim, tree_dim=tree_dim, flatten_output=True, **kwargs)\n            input_dim = min(input_dim + layer_dim * tree_dim, max_features or float('inf'))\n            layers.append(oddt)\n\n        super().__init__(*layers)\n        self.num_layers, self.layer_dim, self.tree_dim = num_layers, layer_dim, tree_dim\n        self.max_features, self.flatten_output = max_features, flatten_output\n        self.input_dropout = input_dropout\n\n    def forward(self, x):\n        initial_features = x.shape[-1]\n        for layer in self:\n            layer_inp = x\n            if self.max_features is not None:\n                tail_features = min(self.max_features, layer_inp.shape[-1]) - initial_features\n                if tail_features != 0:\n                    layer_inp = torch.cat([layer_inp[..., :initial_features], layer_inp[..., -tail_features:]], dim=-1)\n            if self.training and self.input_dropout:\n                layer_inp = F.dropout(layer_inp, self.input_dropout)\n            h = layer(layer_inp)\n            x = torch.cat([x, h], dim=-1)\n\n        outputs = x[..., initial_features:]\n        if not self.flatten_output:\n            outputs = outputs.view(*outputs.shape[:-1], self.num_layers * self.layer_dim, self.tree_dim)\n        return outputs","c0c48a52":"# dataset class\nclass MoaDataset(Dataset):\n    def __init__(self, df, targets, feats_idx, mode='train'):\n        self.mode = mode\n        self.feats = feats_idx\n        self.data = df[:, feats_idx]\n        if mode=='train':\n            self.targets = targets\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        if self.mode == 'train':\n            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n        elif self.mode == 'test':\n            return torch.FloatTensor(self.data[idx]), 0","58a27535":"train = train.values\ntest = test.values\ntrain_targets = train_targets.values","d4ce1ab1":"layer_dim = 8\nnum_layers = 3\ntree_dim = 512\ndepth = 6","26012c21":"folds_split = create_folds(nstarts, nfolds)\nprint(folds_split)","f2d394c9":"for nums, seed in enumerate(range(nstarts)):\n#     kfold = MultilabelStratifiedKFold(n_splits = nfolds, random_state = seed, shuffle = True)\n    for n, foldno in enumerate(set(folds_split[nums])):\n        tr = folds_split[nums] != foldno\n        te = folds_split[nums] == foldno\n        start_time = time.time()\n        xtrain, xval = train[tr], train[te]\n        ytrain, yval = train_targets[tr], train_targets[te]\n\n        train_set = MoaDataset(xtrain, ytrain, top_feats)\n        val_set = MoaDataset(xval, yval, top_feats)\n\n        dataloaders = {\n            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False)\n        }\n        \n        model = nn.Sequential(nn.BatchNorm1d(len(top_feats)), \n                              DenseBlock(len(top_feats), layer_dim = layer_dim, num_layers = num_layers, tree_dim = tree_dim, depth = depth, \n                                         input_dropout = 0.3, flatten_output = True, choice_function = entmax15, bin_function = entmoid15), \n                              nn.BatchNorm1d(layer_dim * num_layers * tree_dim), \n                              nn.Dropout(0.3), \n                              nn.Linear(layer_dim * num_layers * tree_dim, 206), \n                              nn.Sigmoid(), \n                             ).to(device)\n        checkpoint_path = f'Model_{seed}_Fold_{n+1}.pt'\n        optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 1e-5)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 3, eps = 1e-4, verbose = False)\n        best_loss = {'train': np.inf, 'val': np.inf}\n        \n        es_count = 0\n        for epoch in range(nepochs):\n            epoch_loss = {'train': 0.0, 'val': 0.0}\n\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n\n                running_loss = 0.0\n\n                for i, (x, y) in enumerate(dataloaders[phase]):\n                    x, y = x.to(device), y.to(device)\n\n                    optimizer.zero_grad()\n\n                    with torch.set_grad_enabled(phase=='train'):\n                        preds = model(x)\n                        loss = criterion(preds, y)\n\n                        if phase=='train':\n                            loss.backward()\n                            optimizer.step()\n#                             xm.optimizer_step(optimizer, barrier = True)\n\n                    running_loss += loss.item() \/ len(dataloaders[phase])\n\n                epoch_loss[phase] = running_loss\n\n            scheduler.step(epoch_loss['val'])\n\n            if epoch_loss['val'] < best_loss['val']:\n                best_loss = epoch_loss\n                torch.save(model.state_dict(), checkpoint_path)\n                es_count = 0\n            else:\n                es_count += 1\n                \n#             print(\"Epoch {}\/{} - loss: {:5.5f} - val_loss: {:5.5f} - es: {}\".format(epoch+1, nepochs, epoch_loss['train'], epoch_loss['val'], es_count))\n            \n            if es_count > 10:\n                break\n        \n        print(\"[{}] - seed: {} - fold: {} - best val_loss: {:5.5f}\".format(str(datetime.timedelta(seconds = time.time() - start_time))[0:7], seed, n, best_loss['val']))","1d26a94a":"oof = np.zeros((len(train), nstarts, ntargets))\noof_targets = np.zeros((len(train), ntargets))\npreds = np.zeros((len(test), ntargets))","0ce06734":"def mean_log_loss(y_true, y_pred):\n    metrics = []\n    for i, target in enumerate(targets):\n        metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1]))\n    return np.mean(metrics)","6cdcfd16":"res = np.zeros(train_targets.shape)\nfor nums, seed in enumerate(range(nstarts)):\n    print(f\"Inference for seed {seed}\")\n    seed_targets = []\n    seed_oof = []\n    seed_preds = np.zeros((len(test), ntargets, nfolds))\n    \n    for n, foldno in enumerate(set(folds_split[nums])):\n        tr = folds_split[nums] != foldno\n        te = folds_split[nums] == foldno\n        xval, yval = train[te], train_targets[te]\n        fold_preds = []\n        \n        val_set = MoaDataset(xval, yval, top_feats)\n        test_set = MoaDataset(test, None, top_feats, mode='test')\n        \n        dataloaders = {\n            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False),\n            'test': DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n        }\n        \n        checkpoint_path = f'Model_{seed}_Fold_{n+1}.pt'\n        model = nn.Sequential(nn.BatchNorm1d(len(top_feats)), \n                      DenseBlock(len(top_feats), layer_dim = layer_dim, num_layers = num_layers, tree_dim = tree_dim, depth = depth, \n                                 input_dropout = 0.3, flatten_output = True, choice_function = entmax15, bin_function = entmoid15), \n                      nn.BatchNorm1d(layer_dim * num_layers * tree_dim), \n                      nn.Dropout(0.3), \n                      nn.Linear(layer_dim * num_layers * tree_dim, 206), \n                      nn.Sigmoid(), \n                     ).to(device)\n        model.load_state_dict(torch.load(checkpoint_path))\n        model.eval()\n        \n        fold_oof = []\n        for phase in ['val', 'test']:\n            for i, (x, y) in enumerate(dataloaders[phase]):\n                if phase == 'val':\n                    x, y = x.to(device), y.to(device)\n                elif phase == 'test':\n                    x = x.to(device)\n                \n                with torch.no_grad():\n                    batch_preds = model(x)\n                    \n                    if phase == 'val':\n                        seed_targets.append(y)\n                        seed_oof.append(batch_preds)\n                        fold_oof.append(batch_preds)\n                    elif phase == 'test':\n                        fold_preds.append(batch_preds)\n                        \n        res[te] += torch.cat(fold_oof, dim=0).cpu().numpy() \/ nstarts\n                    \n        fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n        seed_preds[:, :, n] = fold_preds\n        \n    seed_targets = torch.cat(seed_targets, dim=0).cpu().numpy()\n    seed_oof = torch.cat(seed_oof, dim=0).cpu().numpy()\n    seed_preds = np.mean(seed_preds, axis=2)\n    \n    print(\"Score for this seed {:5.5f}\".format(mean_log_loss(seed_targets, seed_oof)))\n    oof_targets = seed_targets\n    oof[:, seed, :] = seed_oof\n    preds += seed_preds \/ nstarts","b8c37c93":"oof = np.mean(oof, axis = 1)\nprint(\"Overall score is {:5.5f}\".format(mean_log_loss(oof_targets, oof)))\nnp.save('NODE_oof.npy', res)","288e0b15":"# OOF CV Score With Control Group\ntr_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv').drop('sig_id', axis = 1)\nres_all = np.zeros(tr_targets[cols].shape)\nres_all[train_features['cp_type'] == 0] = res\noverall_oof_score = log_loss_metric(tr_targets[cols].values, res_all)\nprint('OOF CV Score With Control Group:', overall_oof_score)","19bfe729":"ss[targets] = preds\nss.loc[test_features['cp_type'] == 1, targets] = 0\nss.to_csv('submission.csv', index = False)\nnp.save('NODE_sub.npy', ss.loc[test_features['cp_type'] == 0, cols].values)","75ae082b":"# Features","f892fb31":"# Inference","983af982":"# Training\n\nThe model I use here is different from the one suggested in the paper. Here, I flatten all the outputs of the trees and pass a final dense layer. I found this approach can accelerate the convergence and generate better results.\n\nP.S. Using more trees significantly increases the training time but no obvious improvement on CV and LB.","7bc1bc13":"# ODST\/NODE\n\nhttps:\/\/github.com\/Qwicen\/node","27d01ac3":"# MoA Pytorch Neural Oblivious Decision Ensembles (NODE)\n\nThe NODE model is potential. Here, I post a custom architecture different from the original paper and get an baseline result. I don't find QHAdam (used in original paper) to be better than Adam.\n\n**Version 1:** CV 0.01654, LB 0.01900\n\nFrom paper [Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data][1]. \n\nGithub: https:\/\/github.com\/Qwicen\/node\n\n![image.png](attachment:image.png)\n\n[1]: https:\/\/arxiv.org\/pdf\/1909.06312.pdf"}}