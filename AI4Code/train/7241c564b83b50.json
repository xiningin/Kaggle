{"cell_type":{"f2c868f1":"code","b4af0d6a":"code","ea8fed59":"code","44717270":"code","900ae313":"code","fd97bab3":"code","1003814f":"code","20985e4d":"code","aa5d1e38":"code","9c612cad":"code","d366ea54":"code","681bb0ee":"code","9da7a75c":"code","feb56882":"code","1adc3f31":"code","5f362463":"code","6ae3e118":"code","538cf5f6":"code","c9c4cee6":"markdown","665a477b":"markdown","b637e286":"markdown"},"source":{"f2c868f1":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","b4af0d6a":"%%time\nfiles = ['..\/input\/lish-moa\/test_features.csv', \n         '..\/input\/lish-moa\/train_targets_scored.csv',\n         '..\/input\/lish-moa\/train_features.csv',\n         '..\/input\/lish-moa\/train_targets_nonscored.csv',\n         '..\/input\/lish-moa\/sample_submission.csv']\n\ndef load_data(file):\n    return pd.read_csv(file)\n\nwith multiprocessing.Pool() as pool:\n    test, train_target, train, train_nonscored, sub = pool.map(load_data, files)","ea8fed59":"targets = [col for col in train_target.columns if col != 'sig_id']\nprint('Number of different labels:', len(targets))","44717270":"noscored = [col for col in train_nonscored.columns if col != 'sig_id']\nprint('Number of noscored labels:', len(noscored))","900ae313":"features = [col for col in train.columns if col != 'sig_id']\nprint('Number of features:', len(features))","fd97bab3":"for feature in ['cp_type', 'cp_dose']:\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))","1003814f":"X = train[features]","20985e4d":"params = {'num_leaves': 100,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.6,\n          'bagging_fraction': 0.9,\n          'min_data_in_leaf': 30,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'binary_logloss',\n          \"verbosity\": 0,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.1,\n          'random_state': 47\n         }","aa5d1e38":"accumulative_loss = 0\nskf = StratifiedKFold(n_splits=3, random_state=47, shuffle=True)\n\n# 402 different models. One for each label\nfor model, target in enumerate(noscored, 1):\n#     if model > 10:\n#         break\n    y = train_nonscored[target]\n    if y.std() == 0:\n        print (target)\n        continue\n    start_time = time()\n    preds = np.zeros(test.shape[0])\n    oof = np.zeros(X.shape[0])\n\n    for trn_idx, test_idx in skf.split(X, y):\n        trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n        val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n        clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds=25)\n        oof[test_idx] = clf.predict(X.iloc[test_idx])\n        preds += clf.predict(test[features]) \/ skf.n_splits\n\n    train[target] = oof    \n    test[target] = preds\n    loss = log_loss(y, oof)\n    accumulative_loss += loss\n    print('[{}] Model: {} logloss: {:.3f}'.format(str(datetime.timedelta(seconds=time() - start_time))[:7], model, loss))\n","9c612cad":"meta_features = [col for col in train.columns if col != 'sig_id' if col not in features]\nprint('Number of meta_features:', len(meta_features))","d366ea54":"for n,f in enumerate(meta_features):\n    train.rename(columns={f:'m-'+str(n)},inplace=True)\n    test.rename(columns={f:'m-'+str(n)},inplace=True)","681bb0ee":"train.head()","9da7a75c":"test.head()","feb56882":"features = [col for col in train.columns if col != 'sig_id']\nprint('Number of features:', len(features))","1adc3f31":"X = train[features]","5f362463":"accumulative_loss = 0\nskf = StratifiedKFold(n_splits=3, random_state=47, shuffle=True)\n\n# 206 different models. One for each label\nfor model, target in enumerate(targets, 1):\n    y = train_target[target]\n    start_time = time()\n    preds = np.zeros(test.shape[0])\n    oof = np.zeros(X.shape[0])\n\n    for trn_idx, test_idx in skf.split(X, y):\n        trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n        val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n        clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds=25)\n        oof[test_idx] = clf.predict(X.iloc[test_idx])\n        preds += clf.predict(test[features]) \/ skf.n_splits\n\n    sub[target] = preds\n    loss = log_loss(y, oof)\n    accumulative_loss += loss\n    print('[{}] Model: {} logloss: {:.3f}'.format(str(datetime.timedelta(seconds=time() - start_time))[:7], model, loss))\n\n    del preds, oof, start_time, y, loss\n    gc.collect();","6ae3e118":"print('Overall mean loss: {:.3f}'.format(accumulative_loss \/ 206))","538cf5f6":"sub.to_csv('submission.csv', index=False)","c9c4cee6":"# This notebook is based on https:\/\/www.kaggle.com\/nroman\/moa-lightgbm-starter","665a477b":"# predict noscored values as meta features","b637e286":"# predict scored label"}}