{"cell_type":{"abc64f94":"code","161f5d84":"code","e48c9a3c":"code","fab55005":"code","7405b1c3":"code","16a38be3":"code","cfc202e5":"code","00b50364":"code","e7679908":"code","9b540afc":"code","ed649c3d":"code","5c6c78cc":"code","fac88ea4":"code","10836855":"code","a9ba1622":"code","ae8cfb03":"markdown","49470401":"markdown","e6473a54":"markdown","74a522a1":"markdown","71eb2673":"markdown","f1c0209e":"markdown","9d4c1bc0":"markdown","7eeb93a4":"markdown","7716a848":"markdown","bd53c9f0":"markdown"},"source":{"abc64f94":"# Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport gc,warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, GRU, Dropout, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.utils import to_categorical\n\n#Gensim Library for Text Processing\nimport gensim.parsing.preprocessing as gsp\nfrom gensim import utils","161f5d84":"# Load Data\n\ndf_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', header='infer')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', header='infer')\ndf_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv', header='infer')\n\nprint(\"Total Records(train): \",df_train.shape[0])\nprint(\"Total Records(test): \",df_test.shape[0])\n","e48c9a3c":"# Drop Unwanted columnms\n\nunwanted_cols = ['keyword','location']\ndf_train.drop(unwanted_cols,axis=1,inplace=True)\ndf_test.drop(unwanted_cols,axis=1,inplace=True)","fab55005":"'''Text Cleaning Utility Function'''\n\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short\n            ]\n\n# Utility Function\ndef clean_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return text\n\n# Applying the function to text column\ndf_train['text'] = df_train['text'].apply(lambda x: clean_txt(x))\ndf_test['text'] = df_test['text'].apply(lambda x: clean_txt(x))\n","7405b1c3":"# Data Split\nX_train, X_test, y_train, y_test = train_test_split(df_train['text'].values, df_train['target'].values, test_size=0.1)","16a38be3":"# Garbage Collect\ngc.collect()","cfc202e5":"# initialize Tokenizer to encode strings into integers\ntokenizer = Tokenizer()\n\n# calculate number of rows in our dataset\nnum_rows = df_train.shape[0]\n\n\n# create vocabulary from all words in our dataset for encoding\ntokenizer.fit_on_texts(df_train['text'].values)\n\n# max length of 1 row (number of words)\nrow_max_length = max([len(x.split()) for x in df_train['text'].values])\n\n# count number of unique words\nvocabulary_size = len(tokenizer.word_index) + 1\n\n# convert words into integers\nX_train_tokens = tokenizer.texts_to_sequences(X_train)\nX_test_tokens = tokenizer.texts_to_sequences(X_test)\n\n# ensure every row has same size - pad missing with zeros\nX_train_pad = pad_sequences(X_train_tokens, maxlen=row_max_length, padding='post')\nX_test_pad = pad_sequences(X_test_tokens, maxlen=row_max_length, padding='post')\n","00b50364":"'''Data Preparation for Test Data'''\n\n# initialize Tokenizer to encode strings into integers\ntok_tst = Tokenizer()\n\n# calculate number of rows in our dataset\nnum_rows_tst = df_test.shape[0]\n\n\n# create vocabulary from all words in our dataset for encoding\ntok_tst.fit_on_texts(df_test['text'].values)\n\n# max length of 1 row (number of words)\nrow_max_length_tst = max([len(x.split()) for x in df_test['text'].values])\n\n# count number of unique words\nvocab_size_tst = len(tok_tst.word_index) + 1\n\n# convert words into integers\nX_TST_tokens = tok_tst.texts_to_sequences(df_test['text'])\n\n# ensure every row has same size - pad missing with zeros\nX_TST_pad = pad_sequences(X_TST_tokens, maxlen=row_max_length_tst, padding='post')","e7679908":"y_train_cat = to_categorical(y_train)\ny_test_cat = to_categorical(y_test)\n\ntarget_length = y_train_cat.shape[1]\nprint('Original vector size: {}'.format(y_train.shape))\nprint('Converted vector size: {}'.format(y_train_cat.shape))","9b540afc":"EMBEDDING_DIM = 256\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=row_max_length))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(GRU(128)))\nmodel.add(Dense(128, activation='sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(target_length, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\nhistory = model.fit(X_train_pad, y_train_cat, epochs=5, validation_data=(X_test_pad, y_test_cat), batch_size=64, callbacks=[callback])","ed649c3d":"# Evaluation\nresults = model.evaluate(X_test_pad, y_test_cat, batch_size=128, verbose=0)\nprint(\"Model Accuracy: \",'{:.2%}'.format(results[1]))","5c6c78cc":"# Garbage Collection\ngc.collect()","fac88ea4":"# Making Prediction\ny_pred = model.predict(X_TST_pad)","10836855":"# Copying the predicted target to submission\ndf_submission['target'] = np.round(y_pred).astype('int')","a9ba1622":"# save to csv\ndf_submission.to_csv('Submission.csv', index = False)\nprint('Submission saved!')","ae8cfb03":"## Data Split","49470401":"## Labels preprocessing\nIn tensorflow, if we deal with multinomial target, we must convert vector to matrix having as many columns as much targets we have. I.e. having target values 0-4, vector must be converted to matrix of 5 cols. It's actually same as one hot encoding.","e6473a54":"## Setup","74a522a1":"## Data preparation\/preprocessing\n\nSeveral simple steps has been performed in data preparation:\n\n* initialize tokenizer, calculate size of dataset\n* fit tokenizer to create mapping vocabulary (string\/words -> integer)\n* calculate vocabulary size\n* convert words to integers (texts to sequences), as each sentence has different length, we must uniform sizes of arrays using padding with zeros\n* pad sequences to same length","71eb2673":"## Predict test data\nModel is completed and trained, it's time to predict our test data for submission and save it to CSV.","f1c0209e":"For modelling, we will use sequential model using spatial dropout, bidirectional GRU with 128 units and 2 dense layers. This combination gave me best results (Tested single-multi LSTM, GRU, Convolutional nets, ...)","9d4c1bc0":"### Hope that was helpful, please do consider to UPVOTE","7eeb93a4":"# Disaster Tweets classification using Tensorflow - Bi-directional GRU\n\nIn this notebook, I will perform a text classification on the [Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview) by implementing a deep-learning model in Tensorflow using Bi-Directional GRU architecture.","7716a848":"## Text Cleaning","bd53c9f0":"## Modeling & Training\n\n**Embedding** - same as dimensionality reduction. Can be 100,500 or even 1000. Basically if EMBEDDING_DIM == vocabulary_size, then it's identical to bag of words, but you cannot handle it if you have 300 000 of words, would be sparse matrix with 99.99% of values to be zero.\nInstead, embedding layer will be dense and have much smaller dimension.\n"}}