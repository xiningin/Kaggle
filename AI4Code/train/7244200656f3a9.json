{"cell_type":{"e6e23594":"code","6e342c5c":"code","c7432bf0":"code","6b1c9081":"code","f13fe75b":"code","4bbce5c3":"code","fc2a4f48":"code","d7537131":"code","2b416ce0":"code","96d4dd26":"code","804e5ed9":"code","b930ece9":"code","10cad25a":"code","d0dae9d0":"code","873b2f88":"code","954e3f95":"code","52d0c01b":"code","2f4c1644":"code","d0dfa602":"code","cd5f0278":"code","94ea7f93":"code","c02f465f":"code","c7081093":"code","b12d92e1":"code","7783c331":"code","13289c00":"code","f7d35e82":"code","05d4da95":"code","c74f2d94":"code","9b4ece45":"code","85e387e6":"code","870e90e4":"code","309d5de3":"code","166939e0":"code","91149484":"code","2b192a6f":"code","09e3947e":"code","c8d267dd":"code","74d1826a":"code","96976b46":"code","280c522e":"code","0316630b":"code","e6d94be0":"code","1f64d0b1":"code","7afd1f66":"code","76eac598":"code","c979ed98":"code","4c0c6be8":"markdown","da4d0136":"markdown","eed2b56a":"markdown","579c6a9c":"markdown","2265f863":"markdown","0bbed0f2":"markdown","d12c9113":"markdown","b065893b":"markdown","413c3c0e":"markdown","c8cb9ddf":"markdown","325b9229":"markdown","45439e60":"markdown","58969174":"markdown","5266f86e":"markdown","fbf182be":"markdown"},"source":{"e6e23594":"import numpy as np\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' ##disable tensorflow warning logs\n\nimport tensorflow_datasets as tfds ##pre-generated dataset\nimport tensorflow as tf\n\ntfds.disable_progress_bar() ## if to disable progress bar while loading data through tfds","6e342c5c":"##load dataset\ndataset, info = tfds.load('imdb_reviews', with_info=True,as_supervised=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']\n##print(info)","c7432bf0":"for example, label in train_dataset.take(1): ##built-in method\n    print('text: ', example.numpy())\n    print('label: ', label.numpy())","6b1c9081":"BUFFER_SIZE = 10000\nBATCH_SIZE = 64\n\n\n##shuffle, make batches\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","f13fe75b":"for example, label in train_dataset.take(1):\n    print('texts: ', example.numpy()[:1])\n    print()\n    print('labels: ', label.numpy()[:1])","4bbce5c3":"##building text encoder (text should be digitized)\nVOCAB_SIZE = 200000\nencoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\nencoder.adapt(train_dataset.map(lambda text, label: text))","fc2a4f48":"vocab = np.array(encoder.get_vocabulary())\nvocab[:20]","d7537131":"encoded_example = encoder(example)[:3].numpy()\nprint(encoded_example)","2b416ce0":"for n in range(1):\n    print(\"Original: \", example[n].numpy())\n    print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n    print()\n","96d4dd26":"## build model\n\nmodel = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\n\n## train\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30)\n\n\n##evaluation\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\nsample_text = ('The movie was cool. The animation and the graphics '\n               'were out of this world. I would recommend this movie.')\npredictions = model.predict(np.array([sample_text]))\nprint(['positive' if x >= 0 else 'negative' for x in predictions[0]][0])","804e5ed9":"## import matplotlib to visualize\nimport matplotlib.pyplot as plt  \n\n## define function\ndef plot_graphs(history, metric): \n    plt.plot(history.history[metric])  \n    plt.plot(history.history[f'val_{metric}'], '')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([metric, f'val_{metric}']) #'val_'+metric\n    \n\n## plot\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.ylim(None, 1)\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')\nplt.ylim(0, None)","b930ece9":"##bi-directional stacked lstm\nmodel = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\n\n##train\nhistory = model.fit(train_dataset, epochs=10,\n                    validation_data=test_dataset,\n                    validation_steps=30)\n\n\n##eval\ntest_loss, test_acc = model.evaluate(test_dataset)\n\nprint('Test Loss:', test_loss)\nprint('Test Accuracy:', test_acc)\n\nsample_text = ('The movie was not good. The animation and the graphics '\n               'were terrible. I would not recommend this movie.')\npredictions = model.predict(np.array([sample_text]))\nprint(['positive' if x >= 0 else 'negative' for x in predictions[0]][0])\n\n\n## plot\nplt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplot_graphs(history, 'accuracy')\nplt.subplot(1, 2, 2)\nplot_graphs(history, 'loss')","10cad25a":"import os\nimport datetime\n\nimport IPython\nimport IPython.display\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False\n\n## get data\nzip_path = tf.keras.utils.get_file(\n    origin='https:\/\/storage.googleapis.com\/tensorflow\/tf-keras-datasets\/jena_climate_2009_2016.csv.zip',\n    fname='jena_climate_2009_2016.csv.zip',\n    extract=True)\ncsv_path, _ = os.path.splitext(zip_path)\n\n## open with pandas\ndf = pd.read_csv(csv_path)\nprint(df)","d0dae9d0":"# slice [start:stop:step], starting from index 5 take every 6th record. \n# (every single hour)\ndf = df[5::6]\n\n# pop 'Date Time' column and save for future with name 'date_time'\ndate_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n\ndf.head()","873b2f88":"## visualize some selected columns\nplot_cols = ['T (degC)', 'p (mbar)', 'rho (g\/m**3)']\nplot_features = df[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True)\n\nplot_features = df[plot_cols][:480]\nplot_features.index = date_time[:480]\n_ = plot_features.plot(subplots=True)","954e3f95":"## check data\ndf.describe().transpose()","52d0c01b":"## wind speed(wv, wind velocity) -9999 is weird\n\nwv = df['wv (m\/s)']\nbad_wv = wv == -9999.0\nwv[bad_wv] = 0.0\n\nmax_wv = df['max. wv (m\/s)']\nbad_max_wv = max_wv == -9999.0\nmax_wv[bad_max_wv] = 0.0\n\n# The above inplace edits are reflected in the DataFrame\nprint(df['wv (m\/s)'].min())\n\n\n## visualize\nplt.hist2d(df['wd (deg)'], df['wv (m\/s)'], bins=(50, 50), vmax=400)\nplt.colorbar()\nplt.xlabel('Wind Direction [deg]')\nplt.ylabel('Wind Velocity [m\/s]')","2f4c1644":"## feature engineering part 1.\n## wind degree 0 is equal to 360, and vector is better than degree \n\nwv = df.pop('wv (m\/s)')\nmax_wv = df.pop('max. wv (m\/s)')\n\n# Convert to radians.\nwd_rad = df.pop('wd (deg)')*np.pi \/ 180\n\n# Calculate the wind x and y components.\ndf['Wx'] = wv*np.cos(wd_rad)\ndf['Wy'] = wv*np.sin(wd_rad)\n\n# Calculate the max wind x and y components.\ndf['max Wx'] = max_wv*np.cos(wd_rad)\ndf['max Wy'] = max_wv*np.sin(wd_rad)\n\n\n## visualize\nplt.hist2d(df['Wx'], df['Wy'], bins=(50, 50), vmax=400)\nplt.colorbar()\nplt.xlabel('Wind X [m\/s]')\nplt.ylabel('Wind Y [m\/s]')\nax = plt.gca()\nax.axis('tight')","d0dfa602":"## feature engineering part 1.\n## string style timestamp should be converted\ntimestamp_s = date_time.map(datetime.datetime.timestamp) #_s means seconds\n\nday = 24*60*60\nyear = (365.2425)*day\n\n## climate data has seasonality; day and night, summer and winter\n## trigonometric functions are most common way to express such seasonality(periodicity).\ndf['Day sin'] = np.sin(timestamp_s * (2 * np.pi \/ day))\ndf['Day cos'] = np.cos(timestamp_s * (2 * np.pi \/ day))\ndf['Year sin'] = np.sin(timestamp_s * (2 * np.pi \/ year))\ndf['Year cos'] = np.cos(timestamp_s * (2 * np.pi \/ year))\n\n\n##visualize\nplt.plot(np.array(df['Day sin'])[:25])\nplt.plot(np.array(df['Day cos'])[:25])\nplt.xlabel('Time [h]')\nplt.title('Time of day signal')","cd5f0278":"## optional : fast fourier transform\n\nfft = tf.signal.rfft(df['T (degC)'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_h = len(df['T (degC)'])\nhours_per_year = 24*365.2524\nyears_per_dataset = n_samples_h\/(hours_per_year)\n\nf_per_year = f_per_dataset\/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 400000)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 365.2524], labels=['1\/Year', '1\/day'])\n_ = plt.xlabel('Frequency (log scale)')","94ea7f93":"## split data 70:20:10\n## note that there is no random shuffle\n\nn = len(df)\ntrain_df = df[0:int(n*0.7)]\nval_df = df[int(n*0.7):int(n*0.9)]\ntest_df = df[int(n*0.9):]","c02f465f":"##save for later\nnum_features = df.shape[1]\ncolumn_indices = {name: i for i, name in enumerate(df.columns)}","c7081093":"## Normalize data\n## Note that validation and test data normalized with training data \n## so that the models have no access to the values in the validation and test sets.\n\ntrain_mean = train_df.mean()\ntrain_std = train_df.std()\n\ntrain_df = (train_df - train_mean) \/ train_std\nval_df = (val_df - train_mean) \/ train_std\ntest_df = (test_df - train_mean) \/ train_std","b12d92e1":"## check data again\ndf_std = (df - train_mean) \/ train_std\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(df.keys(), rotation=90)","7783c331":"class WindowGenerator():\n    def __init__(self, input_width, label_width, shift,\n                   train_df=train_df, val_df=val_df, test_df=test_df,\n                   label_columns=None):\n        # Store the raw data.\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n\n        # Work out the label column indices.\n        self.label_columns = label_columns\n        if label_columns is not None:\n            self.label_columns_indices = {name: i for i, name in\n                                        enumerate(label_columns)}\n        self.column_indices = {name: i for i, name in\n                               enumerate(train_df.columns)}\n\n        # Work out the window parameters.\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n\n        self.total_window_size = input_width + shift\n\n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n\n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None)\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n\n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input indices: {self.input_indices}',\n            f'Label indices: {self.label_indices}',\n            f'Label column name(s): {self.label_columns}'])\n","13289c00":"w1 = WindowGenerator(input_width=24, label_width=1, \n                     shift=24,label_columns=['T (degC)'])\nprint(w1)","f7d35e82":"w2 = WindowGenerator(input_width=6, label_width=1, \n                     shift=1,label_columns=['T (degC)'])\nprint(w2)","05d4da95":"def split_window(self, features):\n    inputs = features[:, self.input_slice, :]\n    labels = features[:, self.labels_slice, :]\n    if self.label_columns is not None:\n        labels = tf.stack(\n            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n            axis=-1)\n\n    # Slicing doesn't preserve static shape information, so set the shapes\n    # manually. This way the `tf.data.Datasets` are easier to inspect.\n    inputs.set_shape([None, self.input_width, None])\n    labels.set_shape([None, self.label_width, None])\n\n    return inputs, labels\n\n##add function to class after WindowGenerator defined\nWindowGenerator.split_window = split_window","c74f2d94":"# Stack three slices, the length of the total window:\nexample_window = tf.stack([np.array(train_df[:w2.total_window_size]),\n                           np.array(train_df[100:100+w2.total_window_size]),\n                           np.array(train_df[200:200+w2.total_window_size])])\n\n\nexample_inputs, example_labels = w2.split_window(example_window)\n\nprint('All shapes are: (batch, time, features)')\nprint(f'Window shape: {example_window.shape}')\nprint(f'Inputs shape: {example_inputs.shape}')\nprint(f'labels shape: {example_labels.shape}')","9b4ece45":"def plot(self, model=None, plot_col='T (degC)', max_subplots=3):\n    inputs, labels = self.example\n    plt.figure(figsize=(12, 8))\n    plot_col_index = self.column_indices[plot_col]\n    max_n = min(max_subplots, len(inputs))\n    for n in range(max_n):\n        plt.subplot(3, 1, n+1)\n        plt.ylabel(f'{plot_col} [normed]')\n        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n                 label='Inputs', marker='.', zorder=-10)\n\n        if self.label_columns:\n            label_col_index = self.label_columns_indices.get(plot_col, None)\n        else:\n            label_col_index = plot_col_index\n\n        if label_col_index is None:\n            continue\n\n        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n        if model is not None:\n            predictions = model(inputs)\n            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                      marker='X', edgecolors='k', label='Predictions',\n                      c='#ff7f0e', s=64)\n\n        if n == 0:\n            plt.legend()\n\n    plt.xlabel('Time [h]')\n\n#add function to class\nWindowGenerator.plot = plot\nw2.example = example_inputs, example_labels","85e387e6":"w2.plot()","870e90e4":"w2.plot(plot_col='p (mbar)')","309d5de3":"def make_dataset(self, data):\n    data = np.array(data, dtype=np.float32)\n    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n          data=data,\n          targets=None,\n          sequence_length=self.total_window_size,\n          sequence_stride=1,\n          shuffle=True,\n          batch_size=32,)\n\n    ds = ds.map(self.split_window)\n\n    return ds\n\n## add function to class\nWindowGenerator.make_dataset = make_dataset","166939e0":"## some more decorators...\n## This is not the focus of this practice \n## Refer to the documents below, if you are interested.\n## https:\/\/dojang.io\/mod\/page\/view.php?id=2427\n## https:\/\/dojang.io\/mod\/page\/view.php?id=2476\n\n@property\ndef train(self):\n    return self.make_dataset(self.train_df)\n\n@property\ndef val(self):\n    return self.make_dataset(self.val_df)\n\n@property\ndef test(self):\n    return self.make_dataset(self.test_df)\n\n@property\ndef example(self):\n    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n    result = getattr(self, '_example', None)\n    if result is None:\n        # No example batch was found, so get one from the `.train` dataset\n        result = next(iter(self.train))\n        # And cache it for next time\n        self._example = result\n    return result\n\nWindowGenerator.train = train\nWindowGenerator.val = val\nWindowGenerator.test = test\nWindowGenerator.example = example","91149484":"# Each element is an (inputs, label) pair\nw2.train.element_spec","2b192a6f":"for example_inputs, example_labels in w2.train.take(1):\n    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n    print(f'Labels shape (batch, time, features): {example_labels.shape}')","09e3947e":"single_step_window = WindowGenerator(\n    input_width=1, label_width=1, shift=1,\n    label_columns=['T (degC)'])\n\nprint(single_step_window)","c8d267dd":"for example_inputs, example_labels in single_step_window.train.take(1):\n    print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n    print(f'Labels shape (batch, time, features): {example_labels.shape}')","74d1826a":"## build baseline model\n## assumes that future will simply same with now\n\nclass Baseline(tf.keras.Model):\n    def __init__(self, label_index=None):\n        super().__init__()\n        self.label_index = label_index\n\n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n        result = inputs[:, :, self.label_index]\n        return result[:, :, tf.newaxis]\n    \nbaseline = Baseline(label_index=column_indices['T (degC)'])\n\nbaseline.compile(loss=tf.losses.MeanSquaredError(),\n                 metrics=[tf.metrics.MeanAbsoluteError()])\n\nval_performance = {}\nperformance = {}\nval_performance['Baseline'] = baseline.evaluate(single_step_window.val)\nperformance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)","96976b46":"wide_window = WindowGenerator(\n    input_width=24, label_width=24, shift=1,\n    label_columns=['T (degC)'])\n\nprint(wide_window)\nprint('Input shape:', single_step_window.example[0].shape)\nprint('Output shape:', baseline(single_step_window.example[0]).shape)\nprint()\n\nwide_window.plot(baseline)","280c522e":"def compile_and_fit(model, window, patience=2):\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                    patience=patience,\n                                                    mode='min')\n\n    model.compile(loss=tf.losses.MeanSquaredError(),\n                optimizer=tf.optimizers.Adam(),\n                metrics=[tf.metrics.MeanAbsoluteError()])\n\n    history = model.fit(window.train, epochs=MAX_EPOCHS,\n                      validation_data=window.val,\n                      callbacks=[early_stopping])\n    return history\n\n\nMAX_EPOCHS = 20","0316630b":"lstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=1)\n])\n\nhistory = compile_and_fit(lstm_model, wide_window)\n\nIPython.display.clear_output() ## remove after print\nval_performance['LSTM'] = lstm_model.evaluate(wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)","e6d94be0":"single_step_window = WindowGenerator(\n    # `WindowGenerator` returns all features as labels if you \n    # don't set the `label_columns` argument.\n    input_width=1, label_width=1, shift=1)\n\n\nwide_window = WindowGenerator(\n    input_width=24, label_width=24, shift=1,\n    )\n\nprint(wide_window)\nprint('Input shape:', single_step_window.example[0].shape)\nprint('Output shape:', baseline(single_step_window.example[0]).shape)\nprint()\n\n#wide_window.plot(baseline)","1f64d0b1":"lstm_model = tf.keras.models.Sequential([\n    # Shape [batch, time, features] => [batch, time, lstm_units]\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    # Shape => [batch, time, features]\n    tf.keras.layers.Dense(units=num_features)\n])\n\nhistory = compile_and_fit(lstm_model, wide_window)\n\nIPython.display.clear_output()\nval_performance['LSTM'] = lstm_model.evaluate( wide_window.val)\nperformance['LSTM'] = lstm_model.evaluate( wide_window.test, verbose=0)\n","7afd1f66":"OUT_STEPS = 24\nmulti_window = WindowGenerator(input_width=24,\n                               label_width=OUT_STEPS,\n                               shift=OUT_STEPS)\n\nprint(multi_window)\nprint()\n\nmulti_window.plot()","76eac598":"multi_lstm_model = tf.keras.Sequential([\n    # Shape [batch, time, features] => [batch, lstm_units].\n    # Adding more `lstm_units` just overfits more quickly.\n    tf.keras.layers.LSTM(32, return_sequences=False),\n    # Shape => [batch, out_steps*features].\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    # Shape => [batch, out_steps, features].\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])\n])\n\nhistory = compile_and_fit(multi_lstm_model, multi_window)\n\nIPython.display.clear_output()\n\nval_performance['multiLSTM'] = multi_lstm_model.evaluate(multi_window.val)\nperformance['multiLSTM'] = multi_lstm_model.evaluate(multi_window.test, verbose=0)\nmulti_window.plot(multi_lstm_model)","c979ed98":"x = np.arange(len(performance))\nwidth = 0.3\n\nmetric_name = 'mean_absolute_error'\nmetric_index = lstm_model.metrics_names.index('mean_absolute_error')\nval_mae = [v[metric_index] for v in val_performance.values()]\ntest_mae = [v[metric_index] for v in performance.values()]\n\nplt.bar(x - 0.17, val_mae, width, label='Validation')\nplt.bar(x + 0.17, test_mae, width, label='Test')\nplt.xticks(ticks=x, labels=performance.keys(),\n           rotation=45)\nplt.ylabel(f'MAE (average over all times and outputs)')\n_ = plt.legend()","4c0c6be8":"![](https:\/\/www.tensorflow.org\/text\/tutorials\/images\/layered_bidirectional.png?hl=ko)","da4d0136":"### Data windowing : set length of input, offset, and label\n#### 1. 24h input, 24h offset, 1h label\n![24h input, 24h offset, 1h label](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/images\/raw_window_24h.png)\n\n#### 2. 6h input, 1h offset, 1h label\n![](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/images\/raw_window_1h.png)","eed2b56a":"![](https:\/\/www.tensorflow.org\/text\/tutorials\/images\/bidirectional.png?hl=ko)","579c6a9c":"with_info \tbool, if True, tfds.load will return the tuple (tf.data.Dataset, tfds.core.DatasetInfo), the latter containing the info associated with the builder. \n\nas_supervised \tbool, if True, the returned tf.data.Dataset will have a 2-tuple structure (input, label) according to builder.info.supervised_keys. If False, the default, the returned tf.data.Dataset will have a dictionary with all the features. \n\nhttps:\/\/www.tensorflow.org\/datasets\/api_docs\/python\/tfds\/load","2265f863":"In addition to day and year, there may be important features in the time domain, and we can use the fast fourier transform to find these features.","0bbed0f2":"## 2. Time series forecasting \n\n\n#### Jena Climate is weather timeseries dataset recorded at the Weather Station of the Max Planck Institute for Biogeochemistry in Jena, Germany. \n#### Jena Climate dataset is made up of 14 different quantities (such air temperature, atmospheric pressure, humidity, wind direction, and so on) were recorded every 10 minutes, over several years. \n#### This dataset covers data from January 1st 2009 to December 31st 2016.","d12c9113":"![](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/images\/split_window.png)","b065893b":"## 1. Text classification\n\n#### Binary classification of whether each sentence in the imdb movie review dataset is a positive or negative review","413c3c0e":"### Multi-output models (forecast for all variables)","c8cb9ddf":"### Multi-step model (forecast for next 24 hours)","325b9229":"tf.data.AUTOTUNE used for efficient pipeline \\\nhttps:\/\/www.tensorflow.org\/guide\/data_performance?hl=ko","45439e60":"# **\uad6c\uc131**\n\n#### 1. text classification\n#### 2. time series forecasting\n#### 3. time series classification","58969174":"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Bidirectional\n\nmerge_mode  \tMode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. Default value is 'concat'. \n","5266f86e":"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset","fbf182be":"[UNK] stands for unknown"}}