{"cell_type":{"dd32bc0e":"code","6dbf041b":"code","81f76298":"code","b889fdc6":"code","dd949e8c":"code","62bbb3ca":"code","4e9cdcbe":"code","ef3c9def":"code","74a71878":"code","9662c8a9":"code","f4837bf0":"code","9baf57ac":"code","15a21104":"code","4014a894":"code","6ded2e3e":"code","294468ce":"code","a7a8c541":"code","0f133b82":"code","67696f68":"code","67861cad":"code","bb64ee18":"code","43661c51":"code","14fe709f":"code","bee9afd3":"code","4334a980":"code","a0a564f4":"code","d9f3a639":"code","f336b125":"code","d8ec0fb7":"code","947c0c47":"code","ce0804fd":"code","8bceb228":"code","9e37afab":"code","bcce7896":"code","c05c3615":"code","f591de46":"code","ab82fc89":"code","4e65766e":"code","3c992487":"code","615050a8":"code","b576d615":"markdown","24b3f822":"markdown","13996353":"markdown","4e2d9f95":"markdown","114f6d53":"markdown","59b95f04":"markdown","15ede107":"markdown","eb2f708d":"markdown","df3c5916":"markdown","0905699e":"markdown","4880a7d3":"markdown","063d40da":"markdown","756269df":"markdown","f16fad52":"markdown","e4ac1a08":"markdown","87989e63":"markdown","090167ee":"markdown","598eff18":"markdown","8216e5d4":"markdown","65f8affa":"markdown"},"source":{"dd32bc0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random \n\nrandom.seed(19)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6dbf041b":"# Load data\n\ndf = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')","81f76298":"pd.set_option('display.max_columns', 110)","b889fdc6":"df.head()","dd949e8c":"df.describe()","62bbb3ca":"df.isna().sum()[1:30]","4e9cdcbe":"#original_columns = df.columns.values\n\n#for i in range(len(df.index)) :\n#    print(\"Nan in row \", i , \" : \" ,  df[original_columns[6:110]].iloc[i].isnull().sum())","ef3c9def":"df.groupby(\"SARS-Cov-2 exam result\").count()","74a71878":"df.groupby(\"SARS-Cov-2 exam result\").count()","9662c8a9":"set_columns = ['Patient ID', 'Patient age quantile', 'SARS-Cov-2 exam result',\n       'Patient addmited to regular ward (1=yes, 0=no)',\n       'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n       'Patient addmited to intensive care unit (1=yes, 0=no)',\n       'Hematocrit', 'Hemoglobin', 'Platelets', 'Mean platelet volume ',\n       'Red blood Cells', 'Lymphocytes','Mean corpuscular hemoglobin concentration\\xa0(MCHC)',\n       'Leukocytes', 'Basophils', 'Mean corpuscular hemoglobin (MCH)',\n       'Eosinophils', 'Mean corpuscular volume (MCV)', 'Monocytes',\n       'Red blood cell distribution width (RDW)','Respiratory Syncytial Virus','Influenza A',\n       'Influenza B','Parainfluenza 1','CoronavirusNL63','Rhinovirus\/Enterovirus',\n       'Coronavirus HKU1', 'Parainfluenza 3','Chlamydophila pneumoniae', 'Adenovirus', 'Parainfluenza 4',\n       'Coronavirus229E', 'CoronavirusOC43', 'Inf A H1N1 2009',\n       'Bordetella pertussis', 'Metapneumovirus', 'Parainfluenza 2']","f4837bf0":"df = df[set_columns]\ndf.head()","9baf57ac":"df.columns = [x.lower().strip().replace(' ','_') for x in df.columns]\ndf.columns","15a21104":"df['sars-cov-2_exam_result'] = [1 if a == 'positive' else 0 for a in df['sars-cov-2_exam_result'].values]\n\nfor i in df.columns[20:]:\n    df[i] = [1 if a == 'detected' else 0 for a in df[i].values]\n    \ndf.head(20)","4014a894":"df.describe()","6ded2e3e":"df.isnull().sum()","294468ce":"df = df.dropna()","a7a8c541":"df.describe()","0f133b82":"df.groupby(\"sars-cov-2_exam_result\").count()","67696f68":"df = df[df.columns[:20]]","67861cad":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(16, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","bb64ee18":"corrmat_v1 = corrmat.nlargest(10, 'sars-cov-2_exam_result')\n\nfeatures = corrmat_v1.index.values.tolist()\n\nsns.heatmap(df[features].corr(), yticklabels=features, xticklabels=features, square=True);","43661c51":"# features\n\nprint('Features: ',features[1:])\nprint('Target: ',features[0])","14fe709f":"from sklearn.model_selection import train_test_split\n\nX = df[features[1:]]\nY = df[features[0]]\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=7)\n\nprint('Number of training observations: ',len(X_train))\nprint('Number of test observations: ',len(X_test))","bee9afd3":"# Metrics\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef plot_confusion_matrix(cm):\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax,fmt='g',cmap='Blues');\n    ax.set_xlabel('Predict');ax.set_ylabel('True'); \n    ax.set_title('Confusion matrix'); \n    ax.xaxis.set_ticklabels(['Negative', 'Positive']); ax.yaxis.set_ticklabels(['Negative','Positive']);","4334a980":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\ny_pred = logreg.predict(X_test)\n\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(Y_test, predictions)\n\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","a0a564f4":"cf_matrix = confusion_matrix(predictions,Y_test)\n\nplot_confusion_matrix(cf_matrix)","d9f3a639":"print(classification_report(Y_test, predictions))","f336b125":"from sklearn import tree\n\nmodel_tree = tree.DecisionTreeClassifier()\nmodel_tree = model_tree.fit(X_train, Y_train)\n\ny_pred = model_tree.predict(X_test)\n\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(Y_test, predictions)\n\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","d8ec0fb7":"fig, ax = plt.subplots(figsize=(15,15))\ntm = tree.plot_tree(model_tree, ax=ax)\nplt.show()","947c0c47":"cf_matrix = confusion_matrix(predictions,Y_test)\n\nplot_confusion_matrix(cf_matrix)","ce0804fd":"print(classification_report(Y_test, predictions))","8bceb228":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\n\ntree_cls = tree.DecisionTreeClassifier()\nscores = cross_val_score(tree_cls, X, Y,\n                        scoring='neg_mean_squared_error', cv=10)\n\ny_pred = cross_val_predict(tree_cls, X, Y, cv=10)\n\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(Y, predictions)\n\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","9e37afab":"cf_matrix = confusion_matrix(predictions,Y)\n\nplot_confusion_matrix(cf_matrix)","bcce7896":"print(classification_report(Y, predictions))","c05c3615":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rf = RandomForestClassifier(max_depth=2, random_state=7)\nmodel_rf = model_rf.fit(X_train, Y_train)\n\ny_pred = model_rf.predict(X_test)\n\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(Y_test, predictions)\n\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","f591de46":"cf_matrix = confusion_matrix(predictions,Y_test)\n\nplot_confusion_matrix(cf_matrix)","ab82fc89":"#print(classification_report(Y_test, predictions))","4e65766e":"from xgboost import XGBClassifier\n\nmodel_xgb = XGBClassifier()\n\nmodel_xgb.fit(X_train, Y_train)\n\ny_pred = model_xgb.predict(X_test)\n\npredictions = [round(value) for value in y_pred]\n\naccuracy = accuracy_score(Y_test, predictions)\n\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","3c992487":"cf_matrix = confusion_matrix(predictions,Y_test)\n\nplot_confusion_matrix(cf_matrix)","615050a8":"print(classification_report(Y_test, predictions))","b576d615":"# Exploratory analysis and data manipulation","24b3f822":"Preference: parsimonious models\n    \nVariable selection by correlation. \n\nNumber of features (10) is ad-hoc.","13996353":"# Correlagram","4e2d9f95":"## Final Comment\n\nGiven the results of the tested methods, we had good accuracy. This result would be expected given the natural proportion \nof the variable of interest. If confusion matrices are observed, errors are even worse in correctly identifying \"positive\" (true positives and false negatives).\n\nRegarding the best method, Logistic had slightly better results than the others, when the precision in relation to the positive (1) and the accuracy is observed.","114f6d53":"The model did not predict positives!","59b95f04":"# Task\n\n* Predict confirmed COVID-19 cases among suspected cases\n\nBased on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the emergency room, would it be possible to predict the test result for SARS-Cov-2 (positive\/negative)?","15ede107":"## Logistic","eb2f708d":"Given the output, we can observe and discuss a few things:\n   1. The interest variable is proportionally unbalanced. Positive results represent approximately 10% of the total tested (before dealing with NAs).\n   \n   2. We have many variables being mostly composed of missing values. Some of them containing only NAs. (Ex: 'D-Dimner' or 'Mycoplasma pneumoniae')\n   \n   3. Imputing values (such zeros or averages) can cause problems in modeling and results, given the characteristics of the variables.\n   \n   4. The dataset has data variability problems. Because of this, variables that have a not so large number of NAs were selected.\n   \n   5. The priority is to manipulate the data, maintaining the group of variables that have similar NAs. In this case, it is a range of columns between 'Hematocrit' and 'Red blood cell distribution width (RDW)'.","df3c5916":"Methods:\n    1. Logistic\n    2. Decision Tree;\n    3. Decision Tree with CrossValidation;\n    4. RandomForest;\n    5. XGBoost.\n    \nMetrics:\n    1. Accuracy;\n    2. Confusion matrix.","0905699e":"We can observe moderate to high autocorrelation between the variables: 'hematocrit', 'hemoglobin', 'red_blood_cells', 'mean_corpuscular_hemoglobin (mch)' and 'mean_corpuscular_hemoglobin_(mch)'. Thereby,the first attempt tries to use a model with greater parsimony.","4880a7d3":"We have a high number of missing values. This characteristic is common to almost all dataset lines.\n\nHow to deal with them? ","063d40da":"## Decision Tree","756269df":"# Methods and Metrics","f16fad52":"# Split the data","e4ac1a08":"## Random Forest","87989e63":"1. We have columns of zeros again. They will be dropped.\n\n2. We continue with the unbalanced interest variable (before discarding NAs, the positives corresponded to approximately 10%, now it is approximately 13%).\n\n3. Some variables have many zeros (An example of this is zero going to the 75th quantile). These variables will be discarded.","090167ee":"## XGBoost ","598eff18":"# Features selection","8216e5d4":"Categorical data to dummy:\n1. Variable of interest: 1 is positive and 0 negative.\n\n2. Non-priority variables (previously indicated): 1 represents 'detected' and 0 otherwise.","65f8affa":"> ## Decision Tree with CrossValidation"}}