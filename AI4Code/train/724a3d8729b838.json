{"cell_type":{"e4f21209":"code","2927e3ae":"code","72acd89e":"code","c7f5f784":"code","570eae80":"code","2568e5e3":"code","f8956d93":"code","d2b9fd68":"code","1642652d":"code","9839b023":"code","ee573328":"markdown","e7e9aacd":"markdown","ec0a15b7":"markdown","119e71bd":"markdown","30341093":"markdown","e8fa77e5":"markdown","0e8a851d":"markdown","822cc797":"markdown"},"source":{"e4f21209":"import numpy as np \nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport seaborn as sns\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2927e3ae":"#Something gets lost when you share the file that writes the feather. I guess I'd need to save it as a dataset instead of a file.\n#For this purpose, I'm just loading the csv, even though it takes a few minutes.\n#train = pd.read_feather('\/kaggle\/input\/janestreet-data\/train.feather')\n\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nprint(train.head())","72acd89e":"correlation_matrix = train.corr()","c7f5f784":"cmap = sns.diverging_palette(250, 20, s=99, as_cmap=True)\n\n# Most people prefer to use a mask to zero out the upper half of the plot. I actually prefer it this way,\n# just remember that the plot is symetric around x=y. \nplt.figure(figsize=(18,16))\nsns.heatmap(correlation_matrix, cmap=cmap, center=0, vmax=1, vmin=-1, \n            square=True, linewidths=.02, cbar_kws={\"shrink\": .5})","570eae80":"# The index of feature_0.\nidx_0 = 7\n\n# These values can be changed to get different ranges of features.\nmin_feature = 0\nmax_feature = 42\n\nplt.figure(figsize=(15,12))\nsns.heatmap(correlation_matrix.iloc[idx_0 + min_feature: idx_0 + max_feature, idx_0 + min_feature: idx_0 + max_feature],\n            cmap=cmap, vmax=1, vmin=-1, center= 0, square=True, linewidths=.05, cbar_kws={\"shrink\": .5})","2568e5e3":"# Select the feature you'd like to study by changing this variable.\nfeat_no = 7\n\nfeature = 'feature_' + str(feat_no)\n\n# The first plot is simply a few days of data for your chosen feature. Plotting a couple days is a lot more helpful than the whole dataset.\ndays = 3\nfirst_day= 100\n\ndef plot_time_series(train, first_day, days, feature, ax=None):\n    \"\"\"Plots a single feature in train over a number of days starting with first_day.\"\"\"\n    plt.figure(figsize=(20, 6))\n    y = train.loc[(train.date >= first_day) & (train.date < first_day + days)]\n    x = range(len(y))\n    sns.scatterplot(x, y[feature], hue=y.date, size=0, ax=ax)\n    \nplot_time_series(train, first_day, days, feature)\n\n# This is just a simple distribution plot for the selected feature.\nplt.figure(figsize=(6, 6))\nsns.distplot(train[feature])","f8956d93":"# This section plots the feature specified in the previous block of code.\nprint(feature)\n\ntarget = 'resp'\nbuckets = 20\n\ndef feature_vs_target(train, feature, target, buckets, ax=None):\n    \"\"\"Plot averaged target and feature values after bucketing them on feature values.\"\"\"\n    df = train.loc[:, [target, feature]]\n    # Create a column that represents which bucket a given feature value falls into.\n    df['bins'] = pd.qcut(train[feature], buckets, duplicates='drop')\n    # Group the data on bins, and take its average.\n    plot_data = df.groupby('bins').mean()\n    # Plot the averaged relationship between feature and target.\n    sns.scatterplot(x=plot_data[feature], y=plot_data[target], ax=ax)\n    return plot_data\n\n# The code that calls the above function.\nfig, axes = plt.subplots(1, 2, figsize=(14,6))\nfeature_vs_target(train.loc[train.feature_0 < 0], feature, target, buckets, ax=axes[0]);\naxes[0].set_title(f'{feature} vs. RESP, feature_0 = -1')\nfeature_vs_target(train.loc[train.feature_0 > 0], feature, target, buckets, ax=axes[1]);\naxes[1].set_title(f'{feature} vs. RESP, feature_0 = +1')","d2b9fd68":"# Select the features to be examined.\nfeat1_no = 37\nfeat2_no = 39\n\nfeature1 = 'feature_' + str(feat1_no)\nfeature2 = 'feature_' + str(feat2_no)\n\n# Plot both features relationship with the target variable.\nplt.figure(figsize=(8, 8))\nfeature_vs_target(train, feature1, target, buckets)\nfeature_vs_target(train, feature2, target, buckets)\n\n# Plot the joint distribution of the target variables. To save time, use only a portion of the data.\nplt.figure(figsize=(8, 8))\nsns.jointplot(data=train.iloc[100_000:120_000], x=feature1, y=feature2, kind='kde')","1642652d":"target = 'resp'\nbuckets = 20\nfirst_day = 100\ndays = 2\n\n\n# Plot the unnumbered columns.\nfor col in train.columns[0:6]:\n    fig, axes = plt.subplots(1, 3, figsize=(15,4))\n    fig.suptitle(col.capitalize())\n    axes[0].set_title('Distribution')\n    sns.distplot(train[col], ax=axes[0])\n    feature_vs_target(train, col, target, buckets, ax=axes[1])\n    axes[1].set_title(\"Relationship with RESP\")\n    plot_time_series(train, first_day, days, col, ax=axes[2])\n    axes[2].set_title('Two Days of Data') ","9839b023":"# Plot the numbered columns.\n\n# This gets rid of an annoying warning.\nplt.rcParams.update({'figure.max_open_warning': 0})\n\n# Plot the numbered columns.\nfor col in train.columns[8:]:\n    fig, axes = plt.subplots(1, 4, figsize=(18,4))\n    fig.suptitle(col.capitalize())\n    axes[0].set_title('Distribution')\n    sns.distplot(train[col], ax=axes[0])\n    feature_vs_target(train.loc[train.feature_0 < 0], col, target, buckets, ax=axes[1])\n    axes[1].set_title('Feature vs. RESP, feature_0 = -1')\n    feature_vs_target(train.loc[train.feature_0 > 0], col, target, buckets, ax=axes[2])\n    axes[2].set_title('Feature vs. RESP, feature_0 = +1')\n    plot_time_series(train, first_day, days, col, ax=axes[3])\n    axes[3].set_title('Two Days of Data') ","ee573328":"# This section provides some useful summary plots of a single feature.","e7e9aacd":"# Plot the cross-correlation matrix for the first 42 numbered features.\n\n1. Note that the features come in pairs.\n2. There are related sets of features, including 3-6, 7-16, 17-26, and 27-36. These sets share similar patterns in NA values (not shown). These sets also show shared relationships to feature_0. \n3. If you look closely, the internal structure of the corrlations between features 7-16 and 17-26 follows similar patterns. For example feature_7 is more correlated to feature_11 than feature_9. Following the same pattern, feature_17 is more correlated to feature_21 than feature_19. Such patterns are consistent between the ranges 7-16, 17-26, and 27-36.\n4. Partial speculation. Features 3 to 36 are all closely related. I'm guessing that the eariler features represent something like 'raw data' and that the latter (higher-numbered) features are calulated from the earlier (lower-numbered) features. Features 37-40 are the summary results.","ec0a15b7":"# Using the feature selected in the last section, let's examine its relationship with our target variable, resp.\n\nFor the selected feature, you get very different results based on the value of feature_0. That's often true.","119e71bd":"## Load the training data from a pre-saved feather.","30341093":"# This file provides some useful tools for looking at the numbered features. I'll also provide a quick analysis of the early numbered features.","e8fa77e5":"# Calculate the cross-correlation matrix and plot it.","0e8a851d":"# Create summary plots for all features.\n\nYou aren't meant to study at all these plots. It's more like a reference, but you can see some interesting things by scanning through it all.","822cc797":"# This section allows comparisons of two variable's relationships with the target variable and each other.\n\n1. For example, using features 37 and 39 will show that they both predict resp. Further, they are not independently distributed. In fact, they neatly split the examples (rows) into two different distributions in the joint distribution plot.\n2. All of the features between 7 and 36 appear (to me) to be intermediate calculations that lead to features 37-40, which split the data into cleanly defined groups. These groups would be easily picked up by a machine learning algorithm.\n3. Summary, I believe that you are looking at Janestreet's feature engineering effort, which created features 37-40 from lower-numbered features.\n4. Feature 41 represents the start of a new type of data.\n\nThese conclusions are based on looking at a lot of combinations and individual features. Feel free to use the tools provided (or add new ones) to draw your own conclusions."}}