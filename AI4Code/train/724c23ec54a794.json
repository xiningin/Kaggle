{"cell_type":{"58f2fbc6":"code","af72eff5":"code","9728de95":"code","2f5db00e":"code","a7784bb2":"code","67e1c67c":"code","f36d3c5f":"code","27636f3b":"code","a88f57e9":"code","3bed98b0":"code","70b411df":"code","018b1cd9":"code","0592685b":"code","2f02fda2":"code","fa0c4831":"code","bd463493":"code","e916e46f":"code","25f11a8d":"code","7ff5c5ea":"code","ecb52fb0":"code","8a86232d":"code","4a870e9b":"code","ba314dc1":"code","ba991eb1":"code","4364886a":"code","a99e17b1":"code","1cd1ce39":"code","4338178c":"code","59c352a0":"code","0df18867":"code","425eb6ad":"code","c423b780":"code","b2eb20d0":"code","3f75070b":"code","1510f944":"code","5959ea62":"code","5092533b":"code","1e6a2c8b":"code","4867e765":"code","b173cdf3":"code","bcb4770f":"code","ba37c9f3":"code","453f2da9":"code","c3f28c32":"markdown","202d6ceb":"markdown","3c7549f9":"markdown","6c2a1a56":"markdown","b95c51bc":"markdown","84753473":"markdown","4906d5a1":"markdown","e6d714b8":"markdown","e6f9018a":"markdown","a31d255c":"markdown","8cffbe88":"markdown","ba465205":"markdown","c873238d":"markdown","c6928aeb":"markdown","bb382011":"markdown","4f91dc69":"markdown","b06dc126":"markdown","aae68b88":"markdown","b5aee13d":"markdown","f7d1dd95":"markdown","84a9fe48":"markdown"},"source":{"58f2fbc6":"## Importing libraries\nimport sys, os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nimport seaborn as sns\nimport pandas as pd\n\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, Activation, Dropout,RNN\n\nimport math\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nplotsize = (12,5)","af72eff5":"# reading data\ndataframe  = pd.read_csv(\"..\/input\/covid19-in-india\/covid_19_india.csv\")","9728de95":"dataframe.head()","2f5db00e":"dataframe.tail()","a7784bb2":"dataframe.shape","67e1c67c":"dataframe.info()","f36d3c5f":"# Dropping sno, which is not a necessary column to be present for description\ndataframe.describe().drop('Sno', axis=1)","27636f3b":"import pandas_profiling as pp\nprofile = pp.ProfileReport(dataframe)\nprofile\n# profile.to_file(\"output.html\")","a88f57e9":"dataframe['Date'] = pd.to_datetime(dataframe['Date'])","3bed98b0":"data = dataframe.groupby(by=['Date']).sum().diff()\n# np.diff --> substracting all the values from its previous values\n# That is the reason all the first rows for each column are empty\ndata\n\n# only int type columns got selected","70b411df":"data.fillna(0,inplace=True)\ndata.rename(columns={\"Confirmed\":\"Cases\"},inplace=True)","018b1cd9":"data","0592685b":"figure, axes = plt.subplots(3,sharex=True)\ndata['Cases'].plot(ax=axes[0],title='Cases',figsize=plotsize)\ndata['Deaths'].plot(ax=axes[1],title='Deaths',figsize=plotsize)\ndata['Cured'].plot(ax=axes[2],title='Cured',figsize=plotsize)","2f02fda2":"cases_weekly = data['Cases'].resample('W').sum()\ncases_weekly.plot(title='Weekly cases')","fa0c4831":"cases_monthly = data['Cases'].resample('M').sum()\ncases_monthly.plot(title='Monthly cases')","bd463493":"data\n# date became the index column","e916e46f":"def get_n_last_days(df, series_name, n_days):\n    return df[series_name][-(n_days):] \n\ndef plot_n_last_days(df, series_name, n_days):\n    plt.figure(figsize = (10,5))   \n    plt.plot(get_n_last_days(df, series_name, n_days), 'k-')\n    plt.title('{0} - {1} days'\n              .format(series_name, n_days))\n    plt.xlabel('Recorded day')\n    plt.ylabel('Reading')\n    plt.grid(alpha=0.3)","25f11a8d":"plot_n_last_days(data,'Cases',200)","7ff5c5ea":"def get_keras_format_series(series):\n\n    series = np.array(series)\n    return series.reshape(series.shape[0],series.shape[1],1) # converting to 3d\n\n\n\ndef get_train_test_data(df, series_name, series_days, input_hours, \n                        test_hours, sample_gap=3):\n\n    forecast_series = get_n_last_days(df, series_name, series_days).values # reducing our forecast series to last n days\n\n    train = forecast_series[:-test_hours] # training data is remaining days until amount of test_hours\n    test = forecast_series[-test_hours:] # test data is the remaining test_hours\n\n    train_X, train_y = [], []\n\n    # range 0 through # of train samples - input_hours by sample_gap. \n    # This is to create many samples with corresponding\n    for i in range(0, train.shape[0]-input_hours, sample_gap): \n        train_X.append(train[i:i+input_hours]) # each training sample is of length input hours\n        train_y.append(train[i+input_hours]) # each y is just the next step after training sample\n\n    train_X = get_keras_format_series(train_X) # format our new training set to keras format\n    train_y = np.array(train_y) # make sure y is an array to work properly with keras\n    \n    # The set that we had held out for testing (must be same length as original train input)\n    test_X_init = test[:input_hours] \n    test_y = test[input_hours:] # test_y is remaining values from test set\n    \n    return train_X, test_X_init, train_y, test_y","ecb52fb0":"series_days = 600\ninput_days = 5\ntest_days = 10\n\ntrain_X, test_X_init, train_y, test_y = get_train_test_data(data, 'Cases', series_days, \n                         input_days, test_days)","8a86232d":"print('Training input shape: {}'.format(train_X.shape))\nprint('Training output shape: {}'.format(train_y.shape))\nprint('Test input shape: {}'.format(test_X_init.shape))\nprint('Test output shape: {}'.format(test_y.shape))","4a870e9b":"def fit_LSTM(X_train, y_train, epochs):\n    \n    # initialize model\n    regressor = Sequential()\n\n    # Adding the first LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 45, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n    regressor.add(Dropout(0.2))\n\n    # Adding a second LSTM layer nd some Dropout regularisation\n    regressor.add(LSTM(units = 45, return_sequences = True))\n    regressor.add(Dropout(0.2))\n\n    # Adding a third LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 45, return_sequences = True))\n    regressor.add(Dropout(0.2))\n\n    # Adding a fourth LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 45))\n    regressor.add(Dropout(0.2))\n\n    # Adding the output layer\n    regressor.add(Dense(units = 1))\n    \n    # define the loss function \/ optimization strategy, and fit\n    # the model with the desired number of passes over the data (epochs) \n    regressor.compile(loss='mean_squared_error', optimizer='adam')\n    regressor.fit(train_X, train_y, epochs=epochs, batch_size=64, verbose=1)\n    \n    return regressor","ba314dc1":"model1 = fit_LSTM(train_X, train_y, epochs=1000)","ba991eb1":"def mse(observations, estimates):\n\n    # check arg types\n    assert type(observations) == type(np.array([])), \"'observations' must be a numpy array\"\n    assert type(estimates) == type(np.array([])), \"'estimates' must be a numpy array\"\n    # check length of arrays equal\n    assert len(observations) == len(estimates), \"Arrays must be of equal length\"\n    \n    # calculations\n    difference = observations - estimates\n    sq_diff = difference ** 2\n    mse = sum(sq_diff)\n    \n    return mse","4364886a":"def predict(X_init, n_steps, model):\n\n    \n    X_init = X_init.copy().reshape(1,-1,1)\n    preds = []\n    \n    # iteratively take current input sequence, generate next step pred,\n    # and shift input sequence forward by a step (to end with latest pred).\n    # collect preds as we go.\n    for _ in range(n_steps):\n        pred = model.predict(X_init)\n        preds.append(pred)\n        X_init[:,:-1,:] = X_init[:,1:,:]\n        X_init[:,-1,:] = pred \n    \n    preds = np.array(preds).reshape(-1,1)\n    \n    return preds\n\ndef predict_and_plot(X_init, y, model, title):\n\n    y_preds = predict(test_X_init, n_steps=len(y), model=model) # predict through length of y\n    # Below ranges are to set x-axes\n    start_range = range(1, test_X_init.shape[0]+1) #starting at one through to length of test_X_init to plot X_init\n    predict_range = range(test_X_init.shape[0], test_days)  #predict range is going to be from end of X_init to length of test_hours\n    \n    #using our ranges we plot X_init\n    plt.plot(start_range, test_X_init)\n    #and test and actual preds\n    plt.plot(predict_range, test_y, color='orange')\n    plt.plot(predict_range, y_preds, color='teal', linestyle='--')\n    \n    plt.title(title)\n    plt.legend(['Initial Series','Target Series','Predictions'])\n    print(y_preds)\n    print(\"MSE:{}\".format(np.mean(mse(y,y_preds))))","a99e17b1":"predict_and_plot(test_X_init, test_y, model1,\n                 'Test Data and LSTM Predictions')","1cd1ce39":"from statsmodels.tsa.seasonal import seasonal_decompose\ndata.drop(columns=['Cured','Deaths'],inplace=True)\ndata.columns = ['ds', 'y']\nss_decomposition = seasonal_decompose(x=data['y'], model='additive',freq=7)\nestimated_trend = ss_decomposition.trend\nestimated_seasonal = ss_decomposition.seasonal\nestimated_residual = ss_decomposition.resid","4338178c":"fig, axes = plt.subplots(4, 1)\nfig.set_figheight(10)\nfig.set_figwidth(15)\n\naxes[0].plot(data['y'], label='Original')\naxes[0].legend(loc='upper left');\n\naxes[1].plot(estimated_trend, label='Trend')\naxes[1].legend(loc='upper left');\n\naxes[2].plot(estimated_seasonal, label='Seasonality')\naxes[2].legend(loc='upper left');\n\naxes[3].plot(estimated_residual, label='Residuals')\naxes[3].legend(loc='upper left');","59c352a0":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(data['y'])","0df18867":"def run_sequence_plot(x, y, title, xlabel=\"time\", ylabel=\"series\"):\n    plt.plot(x, y, 'k-')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(alpha=0.3);","425eb6ad":"chunks = np.split(data['y'], indices_or_sections=7)\nprint(\"{} | {:7} | {}\".format(\"Chunk\", \"Mean\", \"Variance\"))\nprint(\"-\" * 26)\nfor i, chunk in enumerate(chunks, 1):\n    print(\"{:5} | {:.6} | {:.6}\".format(i, np.mean(chunk), np.var(chunk)))","c423b780":"pd.Series(data['y']).hist()","b2eb20d0":"from statsmodels.tsa.stattools import adfuller\nadf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(data['y'])","3f75070b":"print(\"ADF:{}\".format(adf))\nprint(\"Pvalue:{}\".format(pvalue))","1510f944":"0.022928166916663568 < 0.05","5959ea62":"train = np.array(data['y'][1:-30])\ntest = np.array(data['y'][-30:])","5092533b":"from statsmodels.tsa.api import SimpleExpSmoothing\n\nsingle = SimpleExpSmoothing(train).fit(optimized=True)\nsingle_preds = single.forecast(len(test))\nsingle_mse = mse(test, single_preds)\nprint(\"Predictions: \", single_preds)\nprint(\"MSE: \", single_mse)","1e6a2c8b":"plt.plot(data.index[1:-30], train, 'b--', label=\"train\")\nplt.plot(data.index[-30:], test, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(data.index[-30:], single_preds, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"Simple Exponential Smoothing\")\nplt.grid(alpha=0.3);","4867e765":"from statsmodels.tsa.api import Holt\n\ndouble = Holt(train).fit(optimized=True)\ndouble_preds = double.forecast(len(test))\ndouble_mse = mse(test, double_preds)\nprint(\"Predictions: \", double_preds)\nprint(\"MSE: \", double_mse)","b173cdf3":"plt.plot(data.index[1:-30], train, 'b--', label=\"train\")\nplt.plot(data.index[-30:], test, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(data.index[-30:], double_preds, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"Double Exponential Smoothing\")\nplt.grid(alpha=0.3);","bcb4770f":"from statsmodels.tsa.api import ExponentialSmoothing\n\ntriple = ExponentialSmoothing(train,\n                              trend=\"additive\",\n                              seasonal=\"additive\",\n                              seasonal_periods=13).fit(optimized=True)\ntriple_preds = triple.forecast(len(test))\ntriple_mse = mse(test, triple_preds)\nprint(\"Predictions: \", triple_preds)\nprint(\"MSE: \", triple_mse)","ba37c9f3":"plt.plot(data.index[1:-30], train, 'b--', label=\"train\")\nplt.plot(data.index[-30:], test, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(data.index[-30:], triple_preds, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"Triple Exponential Smoothing\")\nplt.grid(alpha=0.3);","453f2da9":"print(\"Single MSE :{}\".format(single_mse))\nprint(\"Double MSE :{}\".format(double_mse))\nprint(\"Triple MSE :{}\".format(triple_mse))","c3f28c32":"### Pandas profiling to see automated EDA","202d6ceb":"#### Grouping data by 'Date' to find cumulative sum of cases in India.","3c7549f9":"p-value obtained is less than significance level (0.05). Hence we reject the null hypothesis. Therefore, We conclude the Time series is stationary.","6c2a1a56":"### Some more helper functions\n1. get_keras_format_series :  Convert a series to a numpy array of shape \n    [n_samples, time_steps, features]\n\n\n2. get_train_test_data : Utility processing function that splits an hourly time series into train and test with keras-friendly format, according to user-specified choice of shape.  \n    \n    arguments\n    ---------\n    df (dataframe): dataframe with time series columns.\n\n    series_name (string): column name in df.\n\n    series_days (int): total days to extract.\n\n    input_days (int): length of sequence input to network.\n\n    test_days (int): length of held-out terminal sequence.\n    \n    sample_gap (int): step size between start of train sequences; default 5\n    \n    returns\n    ---------\n    tuple: train_X, test_X_init, train_y, test_y     ","b95c51bc":"### Decomposing the Time series.","84753473":"#### Converting date column to a 'Datetime' object.","4906d5a1":"Any time series has 3 components associated with it:\n1. Trend\n2. Seasonality\n3. Residual","e6d714b8":"### Resampling number of cases by:\n1. Weekly data\n2. Monthly data","e6f9018a":"### Plotting Auto-Corellation function","a31d255c":"### Defining deep learning model architecture\n\n1. LSTM\n\n    Fit LSTM to data train_X, train_y .\n    \n    arguments\n\n    train_X (array): input sequence samples for training.\n\n    train_y (list): next step in sequence targets.\n\n    cell_units (int): number of hidden units for LSTM cells.\n\n    epochs (int): number of training epochs   \n   \n","8cffbe88":"### Augmented Dickey Fuller test (ADF Test)\n* is a common statistical test used to test whether a given Time series is stationary or not. ","ba465205":"### Making predictions.\n\nFunctions used\n1. predict :  Given an input series matching the model's expected format generates model's predictions for next n_steps in the series.\n\n2. predict_and_plot: Given an input series matching the model's expected format generates model's predictions for next n_steps in the series, and plots these predictions against the ground truth for those steps \n    \n    arguments\n\n    X_init (array): initial sequence, must match model's input shape.\n\n    y (array): true sequence values to predict, follow X_init.\n\n    model (keras.models.Sequential): trained neural network.\n\n    title (string): plot title.  ","c873238d":"### Dividing the dataset into chunks to analyze data in specific time periods","c6928aeb":"### Setting up helper functions for forecasting\n\n1. get_n_last_days : Extracting last n_days from the time series.\n2. plot_n_last_days : Plotting last n_days from the time series","bb382011":"### Comparing the results of the 3 statistical models.","4f91dc69":"The deep learning model fails to learn due to small number of training instances.","b06dc126":"#### Removing 'NAN' values and replacing with 0.","aae68b88":"We will be making predictions for 30 days","b5aee13d":"Analysing Number of cases","f7d1dd95":"### Plotting time series of 3 Variables.\n1. Cases\n2. Deaths\n3. Cured","84a9fe48":"### Dividing Time series into Train and test for predictions."}}