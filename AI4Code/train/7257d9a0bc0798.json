{"cell_type":{"dbf4ac36":"code","7b0d8907":"code","132c3159":"code","1b781e89":"code","fcfca592":"code","504e83fa":"code","bc58fded":"code","bfc055f5":"code","9c80af6e":"code","f087bec5":"code","6f6161f3":"code","98855442":"code","7b3e6fbc":"code","42dec077":"code","82c33442":"code","5438baff":"code","6fb6c403":"code","a6dba092":"code","70fe2042":"code","6f17505d":"code","cfb3a964":"code","b7d1cd46":"code","e13ba13f":"markdown","58c656cb":"markdown","ff221518":"markdown","1e01d108":"markdown","4ebea91c":"markdown","e72b57ca":"markdown","1bf00e69":"markdown","a5c29d23":"markdown","93561841":"markdown"},"source":{"dbf4ac36":"import os\nprint(os.listdir(\"..\/input\"))","7b0d8907":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GroupKFold\nimport time\nimport gc\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ngc.enable()\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","132c3159":"# https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\ntrain_df = pd.read_csv('..\/input\/data-flattened\/train-flattened\/train-flattened.csv', dtype={'fullVisitorId': 'str'})\ntest_df = pd.read_csv('..\/input\/data-flattened\/test-flattened\/test-flattened.csv', dtype={'fullVisitorId': 'str'})","1b781e89":"const_cols = [c for c in train_df.columns if train_df[c].nunique(dropna=False) == 1]\ntrain_df.drop(columns=const_cols + ['trafficSource.campaignCode'], inplace=True)\ntest_df.drop(columns=const_cols, inplace=True)","fcfca592":"# https:\/\/www.kaggle.com\/prashantkikani\/teach-lightgbm-to-sum-predictions-fe\ndef browser_mapping(x):\n    browsers = ['chrome', 'safari', 'firefox', 'internet explorer', 'edge', 'opera', 'coc coc', 'maxthon', 'iron']\n    mobile_browsers = ['android', 'samsung', 'mini', 'iphone', 'in-app', 'playstation', 'mozilla', 'chrome',\n                       'blackberry', 'nokia', 'browser', 'amazon', 'lunascape', 'netscape', 'konqueror', 'puffin']\n    if x in browsers:\n        return x\n    elif '(not set)' in x:\n        return x\n    elif sum([(k in x) for k in mobile_browsers]):\n        return 'mobile browser'\n    else:\n        return 'others'\n\ndef adcontents_mapping(x):\n    if 'google' in x:\n        return 'google'\n    elif ('placement' in x) or ('placememnt' in x):\n        return 'placement'\n    elif ('(not set)' in x) or ('nan' in x):\n        return x\n    elif 'ad' in x:\n        return 'ad'\n    else:\n        return 'others'\n\ndef source_mapping(x):\n    sources = ['google', 'youtube', '(not set)', 'nan', 'yahoo', 'facebook', 'reddit', 'bing', 'quora', 'outlook', 'linkedin',\n               'pinterest', 'ask', 'siliconvalley', 'lunametrics', 'amazon', 'mysearch', 'qiita', 'messenger', 'twitter',\n               't.co', 'vk.com', 'search', 'edu', 'mail', 'ad', 'golang', 'direct', 'dealspotr', 'sashihara', 'phandroid',\n               'baidu', 'mdn', 'duckduckgo', 'seroundtable', 'metrics', 'sogou', 'businessinsider', 'github', 'gophergala',\n               'yandex', 'msn', 'dfa', 'feedly', 'arstechnica', 'squishable', 'flipboard', 't-online.de', 'sm.cn', 'wow', \n               'partners']\n    for s in sources:\n        if s in x:\n            return s\n    return 'others'\n\ndef custom(df):\n    print('custom..')\n    df['source.country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n    df['campaign.medium'] = df['trafficSource.campaign'] + '_' + df['trafficSource.medium']\n    df['browser.category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n    df['browser.os'] = df['device.browser'] + '_' + df['device.operatingSystem']\n\n    df['device_deviceCategory_channelGrouping'] = df['device.deviceCategory'] + \"_\" + df['channelGrouping']\n    df['channelGrouping_browser'] = df['device.browser'] + \"_\" + df['channelGrouping']\n    df['channelGrouping_OS'] = df['device.operatingSystem'] + \"_\" + df['channelGrouping']\n    \n    df['content.source'] = df['trafficSource.adContent'] + \"_\" + df['source.country']\n    df['medium.source'] = df['trafficSource.medium'] + \"_\" + df['source.country']\n    \n    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n            df[i + \"_\" + j] = df[i] + \"_\" + df[j]\n    \n    return df\n\nfor df in [train_df, test_df]:\n    df['device.browser'] = df['device.browser'].map(lambda x: browser_mapping(str(x).lower()))\n    df['trafficSource.adContent'] = df['trafficSource.adContent'].map(lambda x: adcontents_mapping(str(x).lower()))\n    df['trafficSource.source'] = df['trafficSource.source'].map(lambda x: source_mapping(str(x).lower()))\n\ntrain_df = custom(train_df)\ntest_df = custom(test_df)","504e83fa":"for df in [train_df, test_df]:\n    df['visitStartTime'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df['sess_date_dow'] = df['visitStartTime'].dt.dayofweek\n    df['sess_date_hours'] = df['visitStartTime'].dt.hour\n    \n    df['totals.hits\/views'] = df['totals.hits'] \/ (df['totals.pageviews'] + 1)\n\n    df.sort_values(['fullVisitorId', 'visitStartTime'], ascending=True, inplace=True)\n    df['next_session_1'] = (\n        df['visitStartTime'] - df[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60\n    df['next_session_2'] = (\n        df['visitStartTime'] - df[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(-1)\n    ).astype(np.int64) \/\/ 1e9 \/\/ 60 \/\/ 60","bc58fded":"num_cols = ['visitNumber', 'totals.hits', 'totals.pageviews', 'totals.hits\/views', 'next_session_1', 'next_session_2']\nexcluded_cols = ['date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', 'visitId', 'visitStartTime']\ncat_cols = [col for col in train_df.columns \n            if (col not in excluded_cols) and (col not in num_cols)]\nall_cols = cat_cols + num_cols\n\nfor col in cat_cols:\n    train_df[col], indexer = pd.factorize(train_df[col].astype(str))\n    test_df[col] = indexer.get_indexer(test_df[col].astype(str))\n    \nfor col in num_cols:\n    train_df[col] = train_df[col].astype(float)\n    test_df[col] = test_df[col].astype(float)\n\ntrain_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)","bfc055f5":"# https:\/\/www.kaggle.com\/mukesh62\/lgb-fe-groupkfold-cv-xgb\ndef get_folds(df=None, n_splits=5, seed=42):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    np.random.seed(seed)\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","9c80af6e":"params = {'learning_rate': 0.01, \n         'objective': 'regression', \n         'metric': 'rmse', \n         'num_leaves': 49, \n         'verbose': 1, \n         'bagging_fraction': 0.94, \n         'feature_fraction': 0.67, \n         'random_state': 42, \n         'max_depth': 14, \n         'random_seed': 42,\n         'bagging_frequency': 5, \n         'lambda_l2': 0.2, \n         'lambda_l1': 0.55, \n         'min_child_samples': 130\n        }","f087bec5":"train_y = train_df[\"totals.transactionRevenue\"]\nfolds = get_folds(train_df, n_splits=5)\n\nimportances = pd.DataFrame()\noof_reg_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(test_df.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds):\n    trn_x, trn_y = train_df[all_cols].iloc[trn_], train_y.iloc[trn_]\n    val_x, val_y = train_df[all_cols].iloc[val_], train_y.iloc[val_]\n    \n    reg = lgb.LGBMRegressor(**params, n_estimators=2000)\n    reg.fit(trn_x, np.log1p(trn_y), eval_set=[(val_x, np.log1p(val_y))], early_stopping_rounds=100, \n            verbose=100, eval_metric='rmse')\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = all_cols\n    imp_df['gain_reg'] = reg.booster_.feature_importance(importance_type='gain')\n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    # LightGBM\n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    lgb_preds = reg.predict(test_df[all_cols], num_iteration=reg.best_iteration_)\n    lgb_preds[lgb_preds < 0] = 0\n    \n    sub_preds += np.expm1(lgb_preds) \/ len(folds)\n    \nprint(\"LGBM session-level error: \", mean_squared_error(np.log1p(train_y.values), oof_reg_preds) ** .5)\n\noof_pred_df = pd.DataFrame({\"fullVisitorId\": train_df[\"fullVisitorId\"].values})\noof_pred_df[\"transactionRevenue\"] = train_df[\"totals.transactionRevenue\"].values\noof_pred_df[\"PredictedRevenue\"] = np.expm1(oof_reg_preds)\noof_pred_df = oof_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\noof_err = np.sqrt(mean_squared_error(np.log1p(oof_pred_df[\"transactionRevenue\"].values), \n                                     np.log1p(oof_pred_df[\"PredictedRevenue\"].values)))\nprint(\"LGBM user-level error: \", oof_err)","6f6161f3":"importances['gain_log'] = np.log1p(importances['gain_reg'])\nmean_gain = importances[['gain_reg', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain_reg'])\n\nplt.figure(figsize=(8, 15))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","98855442":"train_df['predictions'] = np.expm1(oof_reg_preds)\ntest_df['predictions'] = sub_preds","7b3e6fbc":"# Mode functions in scipy and pandas are too slow\ndef find_mode(x):\n    mode, mode_cnt = 0, 0\n    d = dict()\n    for val in x.values:\n        current_cnt = d.get(val, 0) + 1\n        d[val] = current_cnt\n        if current_cnt > mode_cnt:\n            mode = val\n            mode_cnt = current_cnt\n    return mode\n\n# use top_N predictions as features for user-level model\ndef parse_predictions(df):\n    top_N = 20\n    res = dict()\n    res['pred_first'] = df.predictions.values[0]\n    res['pred_last'] = df.predictions.values[-1]\n    for i, pred in enumerate(df.predictions.sort_values(ascending=False).head(top_N)):\n        res['pred_' + str(i)] = pred\n    return res","42dec077":"%%time\n# Aggregate data at User level\n# mean for numerical features and mode for categorical\ntrn_data = train_df[all_cols + ['fullVisitorId']].groupby('fullVisitorId')\\\n    .agg({c: np.mean if c in num_cols else find_mode for c in all_cols})\ntrn_pred_list = train_df[['fullVisitorId', 'predictions']].groupby('fullVisitorId').apply(parse_predictions)\ntrn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)","82c33442":"trn_all_predictions.columns","5438baff":"# Create a DataFrame with VisitorId as index\nsession_pred_cols = trn_all_predictions.columns[:-2]\ntrn_all_predictions['t_log_mean'] = np.log1p(trn_all_predictions[session_pred_cols].mean(axis=1))\ntrn_all_predictions['t_log_median'] = np.log1p(trn_all_predictions[session_pred_cols].median(axis=1))\ntrn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[session_pred_cols]).sum(axis=1)\ntrn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[session_pred_cols].fillna(0).sum(axis=1))\ntrn_all_predictions['t_nb_sess'] = trn_all_predictions[session_pred_cols].isnull().sum(axis=1)\nfull_data = pd.concat([trn_data, trn_all_predictions], axis=1)\ndel trn_data, trn_all_predictions\ngc.collect()\nfull_data.shape","6fb6c403":"%%time\nsub_data = test_df[all_cols + ['fullVisitorId']].groupby('fullVisitorId')\\\n    .agg({c: np.mean if c in num_cols else find_mode for c in all_cols})\nsub_pred_list = test_df[['fullVisitorId', 'predictions']].groupby('fullVisitorId').apply(parse_predictions)\nsub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\nfor f in session_pred_cols:\n    if f not in sub_all_predictions.columns:\n        sub_all_predictions[f] = np.nan\nsub_all_predictions['t_log_mean'] = np.log1p(sub_all_predictions[session_pred_cols].mean(axis=1))\nsub_all_predictions['t_log_median'] = np.log1p(sub_all_predictions[session_pred_cols].median(axis=1))\nsub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[session_pred_cols]).sum(axis=1)\nsub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[session_pred_cols].fillna(0).sum(axis=1))\nsub_all_predictions['t_nb_sess'] = sub_all_predictions[session_pred_cols].isnull().sum(axis=1)\n\nsub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\ndel sub_data, sub_all_predictions\ngc.collect()\nsub_full_data.shape","a6dba092":"target = train_df[['fullVisitorId', 'totals.transactionRevenue']].groupby('fullVisitorId').sum()","70fe2042":"params = {'learning_rate': 0.01, \n         'objective': 'regression', \n         'metric': 'rmse', \n         'num_leaves': 31, \n         'verbose': 1, \n         'bagging_fraction': 0.93, \n         'feature_fraction': 0.57, \n         'random_state': 42, \n         'max_depth': 14, \n         'random_seed': 42,\n         'bagging_frequency': 5, \n         'lambda_l2': 0.62, \n         'lambda_l1': 0.07, \n         'min_child_samples': 179\n        }","6f17505d":"folds = get_folds(df=full_data[['totals.pageviews']].reset_index(), n_splits=5)\n\noof_reg_preds = np.zeros(full_data.shape[0])\nsub_preds = np.zeros(sub_full_data.shape[0])\nvis_importances = pd.DataFrame()\n\nfor fold_, (trn_, val_) in enumerate(folds):\n    trn_x, trn_y = full_data.iloc[trn_], target['totals.transactionRevenue'].iloc[trn_]\n    val_x, val_y = full_data.iloc[val_], target['totals.transactionRevenue'].iloc[val_]\n    \n    reg = lgb.LGBMRegressor(**params,n_estimators=2000)\n    reg.fit(trn_x, np.log1p(trn_y), eval_set=[(val_x, np.log1p(val_y))], early_stopping_rounds=100, \n            verbose=100, eval_metric='rmse')\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = trn_x.columns\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n    \n    # LightGBM\n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    lgb_preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n    lgb_preds[lgb_preds < 0] = 0\n    \n    sub_preds += np.expm1(lgb_preds) \/ len(folds)\n    \nprint(\"LGBM Result: \", mean_squared_error(np.log1p(target['totals.transactionRevenue']), oof_reg_preds) ** .5)","cfb3a964":"vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\nmean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\nvis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 25))\nsns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])","b7d1cd46":"sub_full_data['PredictedLogRevenue'] = np.log1p(sub_preds)\nsub_full_data[['PredictedLogRevenue']].to_csv('submission.csv', index=True)","e13ba13f":"## Train session-level model","58c656cb":"## Create user-level features","ff221518":"## Load data","1e01d108":"## Encode categorical features and convert the numerical variables to float","4ebea91c":"## Drop constant columns","e72b57ca":"## Preprocess some features","1bf00e69":"## Train user-level model","a5c29d23":"## Add several features","93561841":"## KFold split function"}}