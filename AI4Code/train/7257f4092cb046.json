{"cell_type":{"7af21588":"code","c22c2b91":"code","ccfd4916":"code","f74b011c":"code","7896e739":"code","cf8ba8ee":"code","63523fb0":"code","98eff2c3":"code","ab77c336":"code","6fb296e8":"code","d687b1df":"code","1284e807":"code","9ac30a98":"code","69261d88":"code","d7fcaf18":"code","7794f45f":"code","8613522c":"code","5d2f190c":"code","41c399aa":"code","441e5568":"code","12d4711b":"code","23d37641":"code","5578d924":"code","e7d13b66":"code","5152edae":"code","cd1d2016":"code","b0503d49":"code","92fe17cd":"code","02ea99e4":"code","e5333cba":"code","19f3f647":"code","bd638df8":"code","878b4e25":"code","b8810fe8":"code","7bceeae9":"code","101a46d9":"code","057b7573":"code","647d5dd2":"code","230def24":"code","4cf5e62f":"code","0904b488":"code","94e671c3":"code","46e9c3ee":"code","f31135a4":"code","a02b9d0e":"code","e3ce1aa8":"code","fd262095":"code","6f633575":"code","bcef1792":"code","5db683e7":"code","a025cf33":"code","51c41094":"code","c0b2abc6":"code","39b8c203":"code","05efdb56":"code","4cbfe2c3":"code","ee92c090":"code","b1a19a29":"code","f1b9185a":"code","7050bfff":"code","01e2f7bf":"code","39c6d12f":"code","8a48b8ca":"code","d1275fd3":"code","69aa7b84":"code","0af43bd8":"code","50b3a1d5":"code","22d2fd2c":"code","daa99f8f":"code","c2fba272":"code","b618045e":"code","096cc38f":"code","802599f8":"code","96f99daf":"code","cb409ffd":"code","22e38fc7":"code","fcf8986b":"code","d2c004d9":"code","d3e72b0b":"code","ee4021ed":"code","ae6dfae8":"code","d17c65f2":"code","117b7cfa":"code","65363230":"code","86cda7b0":"code","16a75741":"code","dc912226":"code","6f7f23bc":"code","92ca7517":"code","580a4b7a":"code","e22a99fd":"code","bef56073":"code","eaeed6ea":"code","01215e56":"code","14d1b708":"code","aa58d999":"code","480ed2b2":"code","922d913e":"code","c5c04b81":"code","51c68459":"code","9da7fed0":"code","8bfdd7f0":"code","a7047142":"code","39313337":"code","3172a0dc":"code","06f3b05d":"code","d47df492":"code","5ff5162a":"code","bcf46337":"markdown","0846b687":"markdown","8d6c25e8":"markdown","d7bf0e6e":"markdown","a9d6d4b9":"markdown","ee3605fa":"markdown","fc63da9a":"markdown","9415d5f5":"markdown","06fc69f3":"markdown","996b49a9":"markdown","e50ffea3":"markdown","bcb59607":"markdown","26856092":"markdown","2741a72e":"markdown","6b6c9379":"markdown","654010ad":"markdown","262d6129":"markdown","64037950":"markdown","6b4d1921":"markdown","6b14bd4c":"markdown","debef1c2":"markdown","772e1bd3":"markdown","9b470cff":"markdown","65ef2ab6":"markdown","06285de9":"markdown","7f161688":"markdown","e178f9c5":"markdown","f5b4c73a":"markdown","3c0a8fa3":"markdown","bb100273":"markdown","e5e2181a":"markdown"},"source":{"7af21588":"from IPython.display import YouTubeVideo\n \nyoutube_video = YouTubeVideo('GZmGmkOJ9ME') \n \ndisplay(youtube_video)","c22c2b91":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nimport seaborn as sns\nfrom sklearn import datasets\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge \nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport plotly.express as px\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score, recall_score, precision_recall_curve,f1_score, fbeta_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom numpy import mean\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nimport pickle\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegressionCV\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import ttest_ind, chisquare, normaltest\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport statsmodels.api as sm\nimport scipy.stats as stats","ccfd4916":"import warnings\nwarnings.filterwarnings('ignore')","f74b011c":"df =pd.read_csv('..\/input\/youtube-new\/USvideos.csv')","7896e739":"df","cf8ba8ee":"df.columns","63523fb0":"df['country'] = 'USA'","98eff2c3":"df.head(2)","ab77c336":"df1 =pd.read_csv('..\/input\/youtube-new\/CAvideos.csv')","6fb296e8":"df1","d687b1df":"df1.columns","1284e807":"df1['country'] = 'CAN'","9ac30a98":"df1.head(2)","69261d88":"frames = [df, df1]","d7fcaf18":"video = pd.concat(frames)","7794f45f":"video","8613522c":"video.columns","5d2f190c":"video.shape","41c399aa":"video.info()","441e5568":"video['publish_time'] = pd.to_datetime(video['publish_time']\n                                       , errors='coerce', format='%Y-%m-%dT%H:%M:%S.%fZ')","12d4711b":"video.head(2)","23d37641":"video.info()","5578d924":"video['trending_date'] = video['trending_date'].astype('str') \ndate_pieces = (video['trending_date']\n                   .str.split('.'))\nvideo['trending_Year'] = date_pieces.str[0].astype(int)\nvideo['trending_Day'] = date_pieces.str[1].astype(int)\nvideo['trending_Month'] = date_pieces.str[2].astype(int)","e7d13b66":"video.head(2)","5152edae":"video","cd1d2016":"video['publish_hour']=video['publish_time'].dt.hour","b0503d49":"video['publish_day']=video['publish_time'].dt.day","92fe17cd":"video['publish_year']=video['publish_time'].dt.year","02ea99e4":"video['publish_Month']=video['publish_time'].dt.month","e5333cba":"video['score'] = ((video.comment_count \/ video.views)*(video.likes-(1.5 * video.dislikes)))","19f3f647":"video.info()","bd638df8":"conditions = [\n    (video['views'] >= 100000) & (video['score'] >= 300),#y3\n    (video['views'] >= 100000) & (video['score'].between(0,300)),#y2\n    (video['views'] >= 100000) & (video['score'] < 0), #y1\n    ((video['views'] < 100000))#y0\n   ]\nvalues = ['overwhelming praise','neutral videos','overwhelming bad','non- popula']     \nvideo['video_type'] = np.select(conditions,values)","878b4e25":"conditions1 = [\n    (video['video_type'] == \"overwhelming praise\"),\n    (video['video_type'] == \"neutral videos\"),\n    (video['video_type'] == \"overwhelming bad\"),\n    (video['video_type'] == \"non- popula\")\n   ]\nvalues1 = ['3','2','1','0']  \nvideo['video_type_no'] = np.select(conditions1,values1)","b8810fe8":"video.info()","7bceeae9":"video['video_type_no'].value_counts()","101a46d9":"video ","057b7573":"top_10_channel = video[\"channel_title\"].value_counts() \ntop_10_channel.head(10)","647d5dd2":"cdf = video.groupby(\"channel_title\").size().reset_index(name=\"video_count\") \\\n    .sort_values(\"video_count\", ascending=False).head(20)\n\nfig, ax = plt.subplots(figsize=(8,8))\nch_ = sns.barplot(x=\"video_count\", y=\"channel_title\", data=cdf,palette=('Reds_r'), ax=ax)\nch_ = ax.set(xlabel=\"No. of videos\", ylabel=\"Channel\")","230def24":"rep_channel = video.groupby('channel_title')[['views']].sum().reset_index()\nrep_channel.sort_values(by='views',inplace=True,ascending=False)","4cf5e62f":"plt.clf()\nchannel_views = rep_channel.head(20)\nsns.catplot(x='channel_title',y='views',data=channel_views,kind='bar', palette=('Reds_r'),aspect=3)\nplt.xticks(rotation=90)\nplt.xlabel('Channel title')\nplt.ylabel('Number of views (in billions)')\nplt.title('Viewed Channels')\nplt.show()","0904b488":"channel_df = video.groupby(video['channel_title']).agg({'likes': ['sum'],\n                                                           'dislikes': ['sum'],\n                                                           'views': ['sum']})\nchannel_df.columns = ['likes', 'dislikes', 'views']\n\nchannel_df.sort_values(by=[\"dislikes\"], ascending=False).head(10)","94e671c3":"video[\"title_length\"] = video[\"title\"].apply(lambda x: len(x))\n\nfig, ax = plt.subplots()\ntitle_len = sns.distplot(video[\"title_length\"], kde=True, rug=False, hist_kws={'alpha': 1}, color='Red',ax=ax)\ntitle_len = ax.set(xlabel=\"Title Length\", ylabel=\"No. of videos\", xticks=range(0, 110, 10))","46e9c3ee":"fig, ax = plt.subplots()\nnumber_views= ax.scatter(x=video['views'], y=video['title_length'],color='Red', edgecolors=\"#000000\", linewidths=0.5)\nnumber_views = ax.set(xlabel=\"Views\", ylabel=\"Title Length\")","f31135a4":"video.trending_Year.value_counts()","a02b9d0e":"most_year_views= video[\"trending_date\"].apply(lambda x: '20' + x[:2]).value_counts() \\\n            .to_frame().reset_index() \\\n            .rename(columns={\"index\": \"year\", \"trending_date\": \"No_of_videos\"})\n\nfig, ax = plt.subplots()\ntd_ = sns.barplot(x=\"year\", y=\"No_of_videos\", data=most_year_views, \n                palette=('Reds_r'), ax=ax)\ntd_ = ax.set(xlabel=\"Year\", ylabel=\"No. of videos\")","e3ce1aa8":"video['publish_date'] = video['publish_year'].astype(str)+'.'+video['publish_day'].astype(str)+'.'+video['publish_Month'].astype(str)","fd262095":"video","6f633575":"video['trending_date']= pd.to_datetime(video['trending_date'],format=\"%y.%d.%m\")","bcef1792":"video.trending_date.unique","5db683e7":"video['publish_date']= pd.to_datetime(video['publish_date'],format=\"%Y.%d.%m\")","a025cf33":"video.publish_date.unique","51c41094":"video['Difference_day'] = video['trending_date'] - video['publish_date']","c0b2abc6":"video","39b8c203":"video['Difference_day'] = video['Difference_day'].astype(str) ","05efdb56":"video.Difference_day = video.Difference_day.str.replace('days','')","4cbfe2c3":"video","ee92c090":"video['video_type'].value_counts()","b1a19a29":"video.groupby(['country', 'video_type'])['views'].sum()\nsns.countplot(data = video, x='country', hue='video_type', palette=('Reds_r'))\nplt.gcf().set_size_inches(12,6)","f1b9185a":"video[\"video_error_or_removed\"].value_counts()","7050bfff":"value_counts = video[\"video_error_or_removed\"].value_counts().to_dict()\nfig, ax = plt.subplots()\ncounts= ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n        colors=['#FF0000','#FFFFFF'], textprops={'color': '#040204'})\ncounts = ax.axis('equal')\ncounts = ax.set_title('Video Error or Removed?')","01e2f7bf":"video[\"comments_disabled\"].value_counts(normalize=True)","39c6d12f":"value_counts = video[\"comments_disabled\"].value_counts().to_dict()\nfig, ax = plt.subplots()\ncounts_= ax.pie(x=[value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n           colors=['#FF0000','#FFFFFF'], textprops={'color': '#040204'})\ncounts_= ax.axis('equal')\ncounts_= ax.set_title('Comments Disabled?')","8a48b8ca":"numerical_cols = ['views','likes','dislikes','comment_count']\nsns.heatmap(video[numerical_cols].corr(),square=True,annot=True,cmap='Reds_r');","d1275fd3":"# Create converted rates\/ratio columns\nvideo['comment_rate'] = video['comment_count'] \/ video['views']\nvideo['dislike_rate'] =  video ['dislikes'] \/ video['views']\nvideo['dislike_ratio'] = video['dislikes'] \/ (video['dislikes'] + video['likes'])","69aa7b84":"plt.figure(figsize=(12,5))\nplt.scatter(video['dislikes'],video['comment_count'],marker='*',\n            linewidths=0.3, color='Red')\nplt.title(\"dislikes versus comment_count\")\nplt.xlabel(\"Dislikes\")\nplt.ylabel(\"Comment Count\")\nplt.show()","0af43bd8":"plt.figure(figsize=(12,5))\nplt.scatter(video['dislike_rate'],video['comment_rate'],marker='*',linewidths=0.3, \n           color='Red')\nplt.title(\"dislike_rate versus comment_rate\")\nplt.xlabel(\"Dislike Rate\")\nplt.ylabel(\"Comment Rate\")\nplt.show()","50b3a1d5":"# Plot distribution of comment rate, dislike rate, and dislike ratio\nplt.figure(figsize=(12,5))\nplt.hist(video['comment_rate'],label='comment_rate',bins=1500,\n         color='purple', histtype='step')\nplt.hist(video['dislike_rate'],label='dislike_rate',bins=1500,\n         color ='Red', histtype='step')\nplt.hist(video['dislike_ratio'],label='dislike_ratio',bins=1500,\n         color='orange', histtype='step')\nplt.title(\"Distribution of comment_rate, dislike_rate, dislike_ratio\", \n         fontsize=18)\nsns.set(font_scale=1.5)\nplt.xlabel(\"Percentage\", fontsize=15)\nplt.ylabel(\"Count in each bin\", fontsize=15)\nplt.xlim(0,0.15)\nplt.legend()\nplt.show()","22d2fd2c":"fig, ax = plt.subplots()\nshow_views = plt.scatter(x=video['views'], y=video['likes'], linewidths=0.5, color='Red')\nshow_views = ax.set(xlabel=\"Views\", ylabel=\"Likes\")","daa99f8f":"# Subset of views below 5 million\nv5million = video[video['views'] < 5e6]['views']\n\n# Histogram of the views data for videos with views below 5 million\nplt.figure(figsize=(10,5))\nv2 = v5million.hist(color='Red')\nplt.xlabel('Number of Views', fontsize=18)\nplt.ylabel('Number of Videos', fontsize=18)\nplt.title('Figure 8: Videos with Fewer than 5 Million Views ', fontsize=20);","c2fba272":"# Subset of likes below 20,000\nbelow20000 = video[video['views'] < 2e4]['views']\nplt.figure(figsize=(10,5))\n# Histogram of the likes data for videos with likes below 20 thousand\nl2 = below20000.hist(bins=20,color='Red')\nplt.xlabel('Number of Likes', fontsize=18)\nplt.ylabel('Number of Videos', fontsize=18)\nplt.title('Figure 10: Total Likes for Trending Videos Below 20,000 Likes ', fontsize=20)","b618045e":"views_2 = video[video['views'] < 2e6]['views'].count() \/ video['views'].count() * 100\nviews_1 = video[video['views'] < 1e6]['views'].count() \/ video['views'].count() * 100\n\nprint(\"% videos with under 2 million views: \", views_2)\nprint(\"% videos with under 1 million views: \", views_1)","096cc38f":"# Finding max number of likes\nprint(\"max number of likes: \", video['likes'].max())\n\n# Finding min number of likes; excluding videos with 0 likes as those are for videos with ratings disabled\nnonzero = video[video['likes'] != 0]\nprint(\"min number of likes: \", nonzero['likes'].min())\nprint()\n\n# Finding number of videos with max and min number of likes\nprint(\"num with max: \", len(video[video['likes'] ==video['likes'].max()]['likes']))\nprint(\"num with min: \", len(nonzero[nonzero['likes'] == 1]['likes']))","802599f8":"import calendar","96f99daf":"video['publish_time'] = pd.to_datetime(video['publish_time'], format='%Y-%m-%dT%H:%M:%S.%fZ')\nvideo['publish_Month'] = video['publish_time'].dt.month\nvideo['publish_Month'] = video['publish_Month'].apply(lambda x: calendar.month_name[x])\nplt.figure(figsize=(17,10))\nsns.countplot(video['publish_Month'],  palette=('Reds_r'))\nplt.title('Publishing Month')\nplt.xlabel('Month')\nplt.ylabel('No. of videos')\nplt.show()\nprint(\"Publishing Month\\n\")\nprint(video['publish_Month'].value_counts())","cb409ffd":"video['publish_hour'] = video['publish_time'].dt.hour\nplt.figure(figsize=(8,6))\nsns.countplot(video['publish_hour'], palette=('Reds_r'))\nplt.title('Publishing Hour')\nplt.xlabel('Hour')\nplt.ylabel('No. of videos')\nplt.show()\nprint(\"Publishing Hour\\n\")\nprint(video['publish_hour'].value_counts())","22e38fc7":"cnt = Counter()\n\n# Finding the occurence of words in video titles\nfor title in video['title']:\n    for token in title.split():\n        cnt[token] += 1\n\n# Finding the 50 most used words in video titles\ndist = cnt.most_common(50)\ntop_words = []\nfreqs = []\nfor word, freq in dist:\n    top_words.append(word)\n    freqs.append(freq)","fcf8986b":"# Graphing occurrence of words in title\nfig= plt.figure(figsize=(15,6))\nplt.xticks(rotation=90)\nsns.barplot(top_words, freqs, palette=\"Reds_r\")\nsns.set_style(\"darkgrid\")\nsns.set(font_scale=1.2)\nplt.xlabel('Word in Title', fontsize=16)\nplt.ylabel('Number of Videos', fontsize=16)\nplt.title('Figure 1: Occurrence of Words in Video Titles', fontsize=20)\nplt.show()","d2c004d9":"video['tags'] = video['tags'].str.replace('|', '')\nvideo['tags'] = video['tags'].str.replace('\"', '')\nvideo['tags_count'] =video['tags'].str.count(' ') + 1","d3e72b0b":"title_words = list(video[\"title\"].apply(lambda x: x.split()))\ntitle_words = [x for y in title_words for x in y]\nCounter(title_words).most_common(25)","ee4021ed":"wc = WordCloud(width=1200, height=500, \n                         collocations=False, background_color=\"white\", \n                         colormap=\"tab20b\").generate(\" \".join(title_words))\nplt.figure(figsize=(15,10))\nplt.imshow(wc, interpolation='bilinear')\n_ = plt.axis(\"off\")","ae6dfae8":"video","d17c65f2":"video.isna().sum()","117b7cfa":"video.describe()","65363230":"video.duplicated().values.any()","86cda7b0":"video.duplicated().sum()","16a75741":"video = video.drop_duplicates()\nvideo.shape","dc912226":"video.duplicated().values.any()","6f7f23bc":"video_train, video_test = train_test_split(video, test_size=0.2,random_state=0)","92ca7517":"video_train.shape","580a4b7a":"video_test.shape","e22a99fd":"X = video_train[['views','likes','dislikes','comment_count']]\ny= video_train['video_type_no']","bef56073":"X_test = video_test[['views','likes','dislikes','comment_count']]\ny_test = video_test['video_type_no']","eaeed6ea":"print(set(y_test))","01215e56":"from collections import Counter\ncounter = Counter(y)\nfor k, v in counter.items():\n    dist = v \/ len(y) * 100 \n    print(f\"Class= {k}, n={v} ({dist}%)\")","14d1b708":"plt.figure(1, figsize=(10,8))\nplt.bar(counter.keys(),counter.values())","aa58d999":"knn = KNeighborsClassifier(n_neighbors=5)\nscores3 = []\npre_score3 = []\nrecall_scor3 = []\nkfold = KFold(n_splits=5)\n\nfor train_ix, val_ix in kfold.split(X, y):\n\n    train_X, val_X = X.iloc[train_ix], X.iloc[val_ix]\n    train_y, val_y = y.iloc[train_ix], y.iloc[val_ix]\n    \n    \n    oversample = SMOTE(random_state = 0)\n    train_X, train_y = oversample.fit_resample(train_X, train_y)\n    knn.fit(train_X, train_y)\n    y_pred =knn.predict(val_X)\n    scores3.append(metrics.f1_score(val_y, y_pred, pos_label='3',average='micro'))\n    pre_score3.append(metrics.precision_score(val_y, y_pred, pos_label='3',average='micro'))\n    recall_scor3.append(metrics.recall_score(val_y, y_pred, pos_label='3',average='micro'))\n\nprint(\"kNN f1 score: \\t\")\nprint(sum(scores3) \/ len(scores3))\nprint(\"----------------\")\nconf_mat3 = confusion_matrix(val_y, y_pred)\nprint(\"kNN confusion matrix: \\n\",conf_mat3)\nprint(\"----------------\")\nprint(\"KNN precision score\")\nprint(sum(pre_score3) \/ len(pre_score3))\nprint(\"----------------\")\nprint(\"KNN recall_score\")\nprint(sum(recall_scor3) \/ len(recall_scor3))\nprint(\"----------------\")","480ed2b2":"X_test = video_test[['views','likes','dislikes','comment_count']]\ny_test = video_test['video_type_no']","922d913e":"y_pred_test =knn.predict(X_test)\nprint(\"kNN f1 score: \\t\")\nprint(metrics.f1_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"KNN precision score\")\nprint(metrics.precision_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"KNN recall_score\")\nprint((metrics.recall_score(y_test, y_pred_test, pos_label='3',average='micro')))","c5c04b81":"lm = LogisticRegression(solver= 'liblinear',C=0.5)\nscores2 = []\npre_score2 = []\nrecall_score2 = []\nkfold = KFold(n_splits=5)\n\nfor train_ix, val_ix in kfold.split(X, y):\n    train_X, val_X = X.iloc[train_ix], X.iloc[val_ix]\n    train_y, val_y = y.iloc[train_ix], y.iloc[val_ix]\n    oversample = SMOTE(random_state = 0)\n    train_X, train_y = oversample.fit_resample(train_X, train_y)\n    lm.fit(train_X, train_y)\n    y_pred =lm.predict(val_X)\n    #scores2.append((metrics.f1_score(val_y, y_pred, pos_label='3',average='micro')))\n    #pre_score2.append((metrics.precision_score(val_y, y_pred, pos_label='3',average='micro')))\n    #recall_score2.append((metrics.recall_score(val_y, y_pred, pos_label='3',average='micro')))\n    \nprint(\"LogisticRegression score: \\t\")\n#print(sum(scores2) \/ len(scores2))\nprint(\"----------------\")\nconf_mat2 = confusion_matrix(val_y, y_pred)\nprint(\"LogisticRegression confusion matrix: \\n\",conf_mat2)\nprint(\"----------------\")\nprint(\"LogisticRegression precision score\")\n#print(sum(pre_score2) \/ len(pre_score2))\nprint(\"----------------\")\nprint(\"LogisticRegression recall_score\")\n#print(sum(recall_score2) \/ len(recall_score2))\nprint(\"----------------\")","51c68459":"y_pred_test =lm.predict(X_test)\nprint(\"LogisticRegression f1 score: \\t\")\nprint(metrics.f1_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"LogisticRegression precision score\")\nprint(metrics.precision_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"LogisticRegression recall_score\")\nprint((metrics.recall_score(y_test, y_pred_test, pos_label='3',average='micro')))","9da7fed0":"de =  DecisionTreeClassifier(max_depth=4)\nscores1 = []\npre_score1 = []\nrecall_score1 = []\nkfold = KFold(n_splits=5)\n\nfor train_ix, val_ix in kfold.split(X, y):\n    train_X, val_X = X.iloc[train_ix], X.iloc[val_ix]\n    train_y, val_y = y.iloc[train_ix], y.iloc[val_ix]\n    oversample = SMOTE(random_state = 0)\n    train_X, train_y = oversample.fit_resample(train_X, train_y)\n    de.fit(train_X, train_y)\n    y_pred =de.predict(val_X)\n    scores1.append((metrics.f1_score(val_y, y_pred, pos_label='3',average='micro')))\n    pre_score1.append((metrics.precision_score(val_y, y_pred, pos_label='3',average='micro')))\n    recall_score1.append((metrics.recall_score(val_y, y_pred, pos_label='3',average='micro'))) \n    \nprint(\"Decision Tree score: \\t\")\nprint(sum(scores1) \/ len(scores1))\nprint(\"----------------\")\nconf_mat1 = confusion_matrix(val_y, y_pred)\nprint(\"Decision Tree confusion matrix: \\n\",conf_mat1)\nprint(\"----------------\")\nprint(\"Decision Tree precision score\")\nprint(sum(pre_score1) \/ len(pre_score1))\nprint(\"----------------\")\nprint(\"Decision Tree recall_score\")\nprint(sum(recall_score1) \/ len(recall_score1))\nprint(\"----------------\")","8bfdd7f0":"y_pred_test =de.predict(X_test)\nprint(\"Decision Tree f1 score: \\t\")\nprint(metrics.f1_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"Decision Tree precision score\")\nprint(metrics.precision_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"Decision Tree recall_score\")\nprint((metrics.recall_score(y_test, y_pred_test, pos_label='3',average='micro')))","a7047142":"rm =  RandomForestClassifier(n_estimators=100)\nscores = []\npre_score = []\nrecall_score = []\nkfold = KFold(n_splits=5)\n\nfor train_ix, val_ix in kfold.split(X, y):\n    train_X, val_X = X.iloc[train_ix], X.iloc[val_ix]\n    train_y, val_y = y.iloc[train_ix], y.iloc[val_ix]\n    oversample = SMOTE(random_state = 0)\n    train_X, train_y = oversample.fit_resample(train_X, train_y)\n    rm.fit(train_X, train_y)\n    y_pred =rm.predict(val_X)\n    scores.append((metrics.f1_score(val_y, y_pred, pos_label='3',average='micro')))\n    pre_score.append((metrics.precision_score(val_y, y_pred, pos_label='3',average='micro')))\n    recall_score.append((metrics.recall_score(val_y, y_pred, pos_label='3',average='micro'))) \n    \nprint(\"Decision Tree F1: \\t\")\nprint(sum(scores) \/ len(scores))\nprint(\"----------------\")\nconf_mat = confusion_matrix(val_y, y_pred)\nprint(\"Decision Tree confusion matrix: \\n\",conf_mat)\nprint(\"----------------\")\nprint(\"Decision Tree precision score\")\nprint(sum(pre_score) \/ len(pre_score))\nprint(\"----------------\")\nprint(\"Decision Tree recall_score\")\nprint(sum(recall_score) \/ len(recall_score))","39313337":"y_pred_test =rm.predict(X_test)\nprint(\"Random Forest f1 score: \\t\")\nprint(metrics.f1_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"Random Forest precision score\")\nprint(metrics.precision_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"Random Forest recall_score\")\nprint((metrics.recall_score(y_test, y_pred_test, pos_label='3',average='micro')))","3172a0dc":"seed = 765\nscore_vot = []\npre_score_vot = []\nrecall_score_vot = []\nlog_clf = KNeighborsClassifier()\nrnd_clf = RandomForestClassifier(random_state=seed)\ndt_clf = DecisionTreeClassifier(random_state=seed)\nvoting_clf = VotingClassifier(estimators=[('lr', log_clf),('df', dt_clf),('rf',rnd_clf)], voting='hard')\nkfold = KFold(n_splits=5)\n\nfor train_ix, val_ix in kfold.split(X, y):\n    train_X, val_X = X.iloc[train_ix], X.iloc[val_ix]\n    train_y, val_y = y.iloc[train_ix], y.iloc[val_ix]\n    oversample = SMOTE(random_state = 0)\n    train_X, train_y = oversample.fit_resample(train_X, train_y)\n    voting_clf.fit(train_X, train_y)\n    y_pred_tv_vclf =voting_clf.predict(val_X)\n    score_vot.append(metrics.f1_score(val_y, y_pred_tv_vclf, pos_label='3',average='micro'))\n    pre_score_vot.append(metrics.precision_score(val_y, y_pred_tv_vclf, pos_label='3',average='micro'))\n    recall_score_vot.append(metrics.recall_score(val_y, y_pred_tv_vclf, pos_label='3',average='micro'))\n    \nprint(\"voting F1: \\t\")\nprint(sum(score_vot) \/ len(score_vot))\nprint(\"----------------\")\nprint(\"voting precision score\")\nprint(sum(pre_score_vot) \/ len(pre_score_vot))\nprint(\"----------------\")\nprint(\"votingrecall_score\")\nprint(sum(recall_score_vot) \/ len(recall_score_vot))","06f3b05d":"y_pred_test =voting_clf.predict(X_test)\nprint(\"voting f1 score: \\t\")\nprint(metrics.f1_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"voting precision score\")\nprint(metrics.precision_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"voting recall_score\")\nprint((metrics.recall_score(y_test, y_pred_test, pos_label='3',average='micro')))","d47df492":"from sklearn.ensemble import BaggingClassifier\nbg = BaggingClassifier(RandomForestClassifier(), n_estimators=10, max_samples=0.5, bootstrap=True,n_jobs=-1)\nscores_b = []\npre_score_b = []\nrecall_score_b = []\nkfold = KFold(n_splits=5)\nfor train_ix, val_ix in kfold.split(X, y):\n    train_X, val_X = X.iloc[train_ix], X.iloc[val_ix]\n    train_y, val_y = y.iloc[train_ix], y.iloc[val_ix]\n    oversample = SMOTE(random_state = 0)\n    train_X, train_y = oversample.fit_resample(train_X, train_y)\n    bg.fit(train_X, train_y)\n    y_pred_tv_bg1 =bg.predict(val_X)\n    scores_b.append(metrics.recall_score(val_y, y_pred_tv_bg1, pos_label='3',average='micro'))\n    pre_score_b.append(metrics.precision_score(val_y, y_pred_tv_bg1, pos_label='3',average='micro'))\n    recall_score_b.append(metrics.recall_score(val_y, y_pred_tv_bg1, pos_label='3',average='micro'))\nprint(\"Bagging F1: \\t\")\nprint(sum(scores_b) \/ len(scores_b))\nprint(\"Bagging precision score\")\nprint(sum(pre_score_b) \/ len(pre_score_b))\nprint(\"----------------\")\nprint(\"Bagging recall_score\")\nprint(sum(recall_score_b) \/ len(recall_score_b))","5ff5162a":"y_pred_test =bg.predict(X_test)\nprint(\"Bagging f1 score: \\t\")\nprint(metrics.f1_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"Bagging precision score\")\nprint(metrics.precision_score(y_test, y_pred_test, pos_label='3',average='micro'))\nprint(\"----------------\")\nprint(\"Bagging recall_score\")\nprint((metrics.recall_score(y_test, y_pred_test, pos_label='3',average='micro')))","bcf46337":"# Voting ","0846b687":"Number of Comments\nFirst we will consider the relationship between the number of dislikes and the number comments, which we will do by creating a scatter plot we observe that most of the data points cluster near the origin with multiple near-linear lines stemming from this cluster. Although we can see a positive correlation between these two variables, it is not in a linear manner, thus this could not be fitted well by a single line, so we need to consider more variables.","8d6c25e8":"![OIP%20%281%29.jfif](attachment:OIP%20%281%29.jfif)","d7bf0e6e":" What is the most watched channel?","a9d6d4b9":"# Data Analysis and Visualization","ee3605fa":"# Import necessary libraries and read data set","fc63da9a":"# Evaluation metrics  is: Recall","9415d5f5":"We will start by looking at what words are commonly used in video titles, as we predicted that word choice will be an important factor for Trending videos.","06fc69f3":"# Logistic Regression","996b49a9":"# Check null values","e50ffea3":"# We merged America with Canada.","bcb59607":"Video error or removal ratio?","26856092":"Now we will consider the relationship between dislike rate and comment rate, as we feel that factoring in views is important for our analysis since the number of views is a major factor for measuring public exposure. In Figure we have a scatter plot with computed data-normalized points with a variable range of [0,1]. There is a cluster of data points at the origin, but there doesn't seem to be a strong correlation between the variables, so we will continue to explore different relationships.","2741a72e":"# Distribution Visualization","6b6c9379":"# Title Word Choice ","654010ad":"we observe that the majority of trending videos have both low dislike rates and low comment rates. And we can see that the shape of these two distributions are similar (both right skewed). Now that we have observed the distributions, we will now look for relationships between dislikes and comments. When people dislike a video, do they tend to dissent publicly by leaving a comment, thus increasing a video's engagment?","262d6129":"Now it is easy to see that the majority of videos have under 1 million views. We will calculate the exact percentage below","64037950":"# We start with a video that clearly explains the idea of the project.","6b4d1921":"Which country gets the most popular video trend and not trend ?","6b14bd4c":"# Number of Dislikes ","debef1c2":"# RandomForest","772e1bd3":"# Dislike Rate - Comment Rate","9b470cff":"# Bagging","65ef2ab6":"Add some columns for calculate day between publish day and trending date","06285de9":"# Introduction:\nThis project aims to predict the behavior of the video that is going to be uploaded to YouTube. An equation is developed to manually classify all the videos into four groups: overwhelming praise , neutral videos , overwhelming bad , non- popular.\n\n# Data Description:\n\nThe dataset that we will use is obtained from Kaggle here. It contains data about trending videos for many countries. Here we will analyze USA and Canda trending videos.\nInitial Column Observations:\nvideo_id: contains alphanumeric code for video identification. However, this is not helpful for our data exploration\/analysis\ntrending_date: contains the date the video started trending in YYDDMM format\ntitle: contains the title of the video. \nchannel_title: contains the title of the channel\ncategory_id: contains the id number for each category. We will need to match the id number to its corresponding category name\npublish_time: contains the date and time the video was published. It is formatted differently from trending date, which we plan to clean\ntags: contains the tags in one long string, which we will need to separate into a list\nviews, likes, dislikes, comment_count: contains numerical values\nthumbnail_link: contains url for picture of the thumbnail.\ncomments_disabled, ratings_disabled, video_error_or_removed: contains boolean values\ndescription: contains the video description as a string. Some contain non-ascii characters, emojis, and urls which we will need to remove if we use\n\n# Algorithms:\n- Import necessary libraries and read data set.\n- Convert some rows into more than one type.\n- Added a new feature to calculating the number of days between publishing the video and when it becomes trend.\n- Data Analysis and Visualization.\n- Data cleaning.\n- Check null values.\n- Check duplicated values.\n- Describtion of numerical columns.\n- Evaluation metrics  is: Recall.\n- Use more than one modal to choose the best degree.","7f161688":"# Moudel","e178f9c5":"# DecisionTree","f5b4c73a":"Now we will plot the distribution for the comment rate, dislike rate, and dislike ratio to get a feeling for the shape of the distribution","3c0a8fa3":"# Check duplicated values","bb100273":" Are comments disabled?","e5e2181a":"# Knn"}}