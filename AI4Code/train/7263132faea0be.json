{"cell_type":{"20c91e21":"code","4ba8ab66":"code","3aca7578":"code","6228981f":"code","ee5e2fb1":"code","e19cf831":"code","06fa30ea":"code","2c0bef7a":"code","954ff5f8":"code","97163265":"code","905e00f6":"code","dcd0b31c":"code","46c04177":"code","69d8e3a8":"code","a0f637f5":"code","a8d8815f":"code","54007422":"code","24befb75":"code","1ea852a5":"code","06d6c681":"code","1dd11470":"code","b04d08ac":"code","35893224":"code","beba8e55":"code","bef53833":"code","b650bf31":"code","6c57abd0":"code","e7a42bb2":"code","ebb94866":"code","16cdd80d":"code","0a29b090":"code","d73344aa":"code","128f191c":"code","22f55f0f":"code","67ca3bc7":"code","efece9e0":"code","dd534cc3":"code","f29686e2":"code","e49e773b":"code","1e4d1afe":"code","9862044e":"code","e098df93":"code","ece17c6a":"code","e7a3c3e3":"code","5ef2d89e":"code","2dfa7737":"code","1b2b828c":"code","0a217413":"code","2cd939d1":"code","85634d52":"code","10506ddd":"code","65fe8375":"code","bbf4a28b":"code","2509f403":"code","0516b515":"code","c6a08db6":"code","5b86de1c":"code","70589ba8":"code","1f4e1698":"code","0348ba0a":"code","f7966c97":"code","de478c25":"markdown","d84e731e":"markdown","e7868f32":"markdown","0b536eff":"markdown","dd399da0":"markdown","acb1294f":"markdown","13ca20f1":"markdown","f9ae1f1b":"markdown","269e68b6":"markdown","e2b3c8eb":"markdown","9706b734":"markdown","4b17612d":"markdown","7b15b843":"markdown","4904a1a5":"markdown","c348eef4":"markdown","bbaa64fb":"markdown","9ba187dd":"markdown","03a8ee04":"markdown","dc9a5ef6":"markdown","274e7056":"markdown","eb29e605":"markdown","2d328bf4":"markdown","dd0b08f2":"markdown","d23568b5":"markdown","dc53086e":"markdown","68079f67":"markdown","1f6ddd58":"markdown","c6037f7e":"markdown","2a8f86f3":"markdown","581a8271":"markdown","5b1b7ae9":"markdown","dbdcbb20":"markdown","8bc8e04e":"markdown","68049d8d":"markdown","c747f388":"markdown","ba4c83ce":"markdown","cefb484f":"markdown","f1dc62e4":"markdown","062eb874":"markdown","aad81aab":"markdown","8c1412e3":"markdown"},"source":{"20c91e21":"# General Packages\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport time\n\n# General Mathematics package\nimport math as math\n\n# Graphing Packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")\n\n# Statistics Packages\nfrom scipy.stats import randint\nfrom scipy.stats import skew\n\n# Machine Learning Packages\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import scale\nfrom sklearn import preprocessing\nfrom skimage.transform import resize\nimport xgboost as xgb\n\n# Neural Network Packages\nfrom keras.utils import np_utils\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping\nimport tensorflow as tf\n\n# H2o packages\nimport h2o\nfrom h2o.automl import H2OAutoML","4ba8ab66":"student = pd.read_csv(\"..\/input\/StudentsPerformance.csv\")\nstudent.head()","3aca7578":"student[\"overall score\"] = np.nan\nstudent[\"math score letter\"] = np.nan\nstudent[\"reading score letter\"] = np.nan\nstudent[\"writing score letter\"] = np.nan\nstudent[\"overall score letter\"] = np.nan","6228981f":"student[\"overall score\"] = round((student[\"math score\"] + \n                                  student[\"reading score\"] + \n                                  student[\"writing score\"])\/3 , 2)","ee5e2fb1":"student[\"math score letter\"][student[\"math score\"] <= 60] = \"F\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] > 60 , \n                              student[\"math score\"] <= 69)] = \"D\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 70 , \n                              student[\"math score\"] <= 73)] = \"C-\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 74 , \n                              student[\"math score\"] <= 76)] = \"C\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 77 , \n                              student[\"math score\"] <= 79)] = \"C+\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 80 , \n                              student[\"math score\"] <= 83)] = \"B-\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 84 , \n                              student[\"math score\"] <= 86)] = \"B\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 87 , \n                              student[\"math score\"] <= 89)] = \"B+\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 90 , \n                              student[\"math score\"] <= 93)] = \"A-\"\nstudent[\"math score letter\"][np.logical_and\n                             (student[\"math score\"] >= 94 , \n                              student[\"math score\"] <= 96)] = \"A\"\nstudent[\"math score letter\"][student[\"math score\"] >= 97] = \"A+\"","e19cf831":"student[\"reading score letter\"][student[\"reading score\"] <= 60] = \"F\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] > 60 , \n                              student[\"reading score\"] <= 69)] = \"D\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 70 , \n                              student[\"reading score\"] <= 73)] = \"C-\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 74 , \n                              student[\"reading score\"] <= 76)] = \"C\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 77 , \n                              student[\"reading score\"] <= 79)] = \"C+\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 80 , \n                              student[\"reading score\"] <= 83)] = \"B-\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 84 , \n                              student[\"reading score\"] <= 86)] = \"B\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 87 , \n                              student[\"reading score\"] <= 89)] = \"B+\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 90 , \n                              student[\"reading score\"] <= 93)] = \"A-\"\nstudent[\"reading score letter\"][np.logical_and\n                             (student[\"reading score\"] >= 94 , \n                              student[\"reading score\"] <= 96)] = \"A\"\nstudent[\"reading score letter\"][student[\"reading score\"] >= 97] = \"A+\"","06fa30ea":"student[\"writing score letter\"][student[\"writing score\"] <= 60] = \"F\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] > 60 , \n                              student[\"writing score\"] <= 69)] = \"D\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 70 , \n                              student[\"writing score\"] <= 73)] = \"C-\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 74 , \n                              student[\"writing score\"] <= 76)] = \"C\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 77 , \n                              student[\"writing score\"] <= 79)] = \"C+\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 80 , \n                              student[\"writing score\"] <= 83)] = \"B-\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 84 , \n                              student[\"writing score\"] <= 86)] = \"B\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 87 , \n                              student[\"writing score\"] <= 89)] = \"B+\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 90 , \n                              student[\"writing score\"] <= 93)] = \"A-\"\nstudent[\"writing score letter\"][np.logical_and\n                             (student[\"writing score\"] >= 94 , \n                              student[\"writing score\"] <= 96)] = \"A\"\nstudent[\"writing score letter\"][student[\"writing score\"] >= 97] = \"A+\"","2c0bef7a":"student[\"overall score letter\"][student[\"overall score\"] <= 60] = \"F\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] > 60 , \n                              student[\"overall score\"] <= 69.99)] = \"D\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 70 , \n                              student[\"overall score\"] <= 73.99)] = \"C-\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 74 , \n                              student[\"overall score\"] <= 76.99)] = \"C\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 77 , \n                              student[\"overall score\"] <= 79.99)] = \"C+\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 80 , \n                              student[\"overall score\"] <= 83.99)] = \"B-\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 84 , \n                              student[\"overall score\"] <= 86.99)] = \"B\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 87 , \n                              student[\"overall score\"] <= 89.99)] = \"B+\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 90 , \n                              student[\"overall score\"] <= 93.99)] = \"A-\"\nstudent[\"overall score letter\"][np.logical_and\n                             (student[\"overall score\"] >= 94 , \n                              student[\"overall score\"] <= 96.99)] = \"A\"\nstudent[\"overall score letter\"][student[\"overall score\"] >= 97] = \"A+\"","954ff5f8":"student.head()","97163265":"for n in range(0,5):\n    student.iloc[:,n] = pd.Categorical(student.iloc[:,n])\n# 0 to 5 changes gender to test prepartion course into category\nfor n in range(-1,-5):\n    student.iloc[:,n] = pd.Categorical(student.iloc[:,n])\n# -1 to -5 changes math score letter to overall score letter to category\nstudent['overall score'] = pd.to_numeric(student[\"overall score\"])","905e00f6":"student.info()","dcd0b31c":"plt.figure(figsize= (20,5))\nax = plt.subplot(131)\npd.crosstab(student[\"math score letter\"] , student.gender).plot(kind = 'bar', ax = ax)\nplt.xticks(rotation = 360)\nplt.title(\"Math Score\" , size = 20)\nplt.legend(loc=2, prop={'size': 15})\n\nax1 = plt.subplot(132)\npd.crosstab(student[\"reading score letter\"] , student.gender).plot(kind = 'bar', ax = ax1)\nplt.xticks(rotation = 360)\nplt.title(\"Reading Score\" , size = 20)\nplt.legend(loc=2, prop={'size': 15})\n\nax2 = plt.subplot(133)\npd.crosstab(student[\"writing score letter\"] , student.gender).plot(kind = 'bar', ax = ax2)\nplt.title(\"Writing Score\" , size = 20)\nplt.xticks(rotation = 360)\nplt.legend(loc=2, prop={'size': 15})\n\npd.crosstab(student[\"overall score letter\"] , student.gender).plot(kind = 'bar' , figsize = (20,5))\nplt.xlabel(\"overall score\", size = 30)\nplt.xticks(rotation = 360 , size  = 20)\nplt.ylabel(\"count\" , size = 20)\nplt.legend(loc=2, prop={'size': 15})\nplt.show()","46c04177":"pd.crosstab(student[\"overall score letter\"],\n            student[\"parental level of education\"]).plot.bar(figsize = (20,10))\n\nplt.xlabel(\"overall score\", size = 30)\nplt.xticks(rotation = 360 , size  = 20)\nplt.ylabel(\"count\" , size = 30)\nplt.title(\"Parents Degree and Grades\" , size = 20)\nplt.legend(loc=2, prop={'size': 15})\nplt.show()\n\n\nplt.figure(figsize = (20,8))\nsns.boxplot(x=\"parental level of education\", y=\"overall score\", hue=\"gender\" , \n            data=student )\nplt.xticks(rotation=360 , size = 17)\nplt.yticks( size = 20)\nplt.xlabel(\"Level of Education of Parents\", size = 20)\nplt.ylabel(\"overall score\" , size = 20)\nplt.title(\"Parents Degree + Gender and Grades\" , size = 20)\nplt.legend(loc=3, prop={'size': 20})\nplt.show()","69d8e3a8":"pd.crosstab(student[\"overall score letter\"],\n            student[\"race\/ethnicity\"]).plot.bar(figsize = (30,15))\n\nplt.xticks(rotation=360 , size = 30)\nplt.yticks( size = 30)\nplt.xlabel(\"overall score\", size = 30)\nplt.ylabel(\"count\" , size = 30)\nplt.title(\"Ethnicity\/Race and Grades\" , size = 30)\nplt.legend(loc=2, prop={'size': 30})\n\nplt.show()\n\nplt.figure(figsize = (20,8))\nsns.boxplot(x=\"race\/ethnicity\", y=\"overall score\", hue=\"gender\" , \n            data=student)\nplt.xticks(rotation=360 , size = 20)\nplt.yticks( size = 20)\nplt.xlabel(\"Ethnicity\/Race\", size = 20)\nplt.ylabel(\"overall score\" , size = 20)\nplt.title(\"Ethnicity\/Race + Gender and Grades\" , size = 20)\nplt.legend(loc=3, prop={'size': 20})\nplt.show()","a0f637f5":"pd.crosstab(student[\"overall score letter\"],\n            student[\"lunch\"]).plot.bar(figsize = (20,5))\nplt.xticks(rotation=360 , size = 20)\nplt.yticks( size = 20)\nplt.xlabel(\"overall score\", size = 20)\nplt.ylabel(\"count\" , size = 20)\nplt.title(\"Lunch and Grades\" , size = 20)\nplt.legend(loc=2, prop={'size': 20})\nplt.show()\n\nplt.figure(figsize = (20,8))\nsns.boxplot(x=\"lunch\", y=\"overall score\", hue=\"gender\" , \n            data=student)\nplt.xticks(rotation=360 , size = 20)\nplt.yticks( size = 20)\nplt.xlabel(\"Lunch\", size = 20)\nplt.ylabel(\"overall score\" , size = 20)\nplt.title(\"Lunch + Gender and Grades\" , size = 20)\nplt.legend(loc=3, prop={'size': 20})\nplt.show()","a8d8815f":"pd.crosstab(student[\"overall score letter\"],\n            student[\"test preparation course\"]).plot.bar(figsize = (20,5))\nplt.xticks(rotation = 360)\nplt.xticks(rotation=360 , size = 20)\nplt.yticks( size = 20)\nplt.xlabel(\"overall score\", size = 20)\nplt.ylabel(\"count\" , size = 20)\nplt.title(\"Test Prep and Grades\" , size = 20)\nplt.legend(loc=2, prop={'size': 20})\nplt.show()\n\nplt.figure(figsize = (20,8))\nsns.boxplot(x=\"test preparation course\", y=\"overall score\", hue=\"gender\" , \n            data=student)\nplt.xticks(rotation=360 , size = 20)\nplt.yticks( size = 20)\nplt.xlabel(\"Test Prep\", size = 20)\nplt.ylabel(\"Overall Score\" , size = 20)\nplt.title(\"Test Preparation + Gender and Grades\" , size = 20)\nplt.legend(loc=3, prop={'size': 20})\nplt.show()","54007422":"sns.heatmap(student.corr() , square = True , annot = True , \n            linewidths=.8 , cmap=\"YlGnBu\")\nplt.title(\"Correlation Map\")\nplt.show()","24befb75":"sns.lmplot(x=\"reading score\", y=\"writing score\", hue=\"gender\",\n           data=student)\nplt.show()","1ea852a5":"student_1 = student","06d6c681":"le = preprocessing.LabelEncoder()\nfor n in range(0,5):\n    student_1.iloc[:,n] = le.fit_transform(student_1.iloc[:,n])","1dd11470":"for n in range(0,5):\n    student_1.iloc[:,n] = pd.Categorical(student_1.iloc[:,n])\nstudent_1['overall score'] = pd.to_numeric(student_1[\"overall score\"])","b04d08ac":"student_1[\"math score letter\"] = pd.Categorical(student_1[\"math score letter\"])\nstudent_1[\"reading score letter\"] = pd.Categorical(student_1[\"reading score letter\"])\nstudent_1[\"writing score letter\"] = pd.Categorical(student_1[\"writing score letter\"])\nstudent_1[\"overall score letter\"] = pd.Categorical(student_1[\"overall score letter\"])","35893224":"student_2 = student_1","beba8e55":"student_1 = student_1.replace([\"F\" , \"D\" , \"C-\" , \"C\" , \"C+\", \"B-\" , \"B\" , \"B+\", \"A-\" , \"A\" , \"A+\"] , [0,1,2,3,4,5,6,7,8,9,10])\n\n#    F = 0\n#    D = 1\n#    C- = 2\n#    C = 3\n#    C+ = 4\n#    B- = 5\n#    B = 6\n#    B+ = 7\n#    A- = 8\n#    A = 9\n#    A+ = 10","bef53833":"student_1.head()","b650bf31":"X_score = student_1.drop([\"overall score letter\" , \"overall score\"] , axis = 1)\ny_score = student_1[\"overall score letter\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X_score, y_score, test_size = 0.3, random_state=42)","6c57abd0":"logreg = LogisticRegression()\nlogreg.fit(X_train , y_train)\ny_pred = logreg.predict(X_val)\n\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred))\nprint(\"accuracy\",accuracy_score(y_val , y_pred)*100,\"%\")","e7a42bb2":"param_grid = {'n_neighbors' : np.arange(1,50)}\n\nknn_cv = KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn_cv, param_grid, cv = 5)\n\nknn_cv.fit(X_train , y_train)\n\nprint(knn_cv.best_params_)\n\nprint(knn_cv.best_score_)\n\ny_pred_knn = knn_cv.predict(X_val)\n\nprint(\"accuracy:\" , accuracy_score(y_val , y_pred_knn)*100 , \"%\")","ebb94866":"tree = DecisionTreeClassifier()\n\ntree.fit(X_train , y_train)\ny_pred_DT = tree.predict(X_val)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred_DT))\nprint(\"accuracy:\",round(accuracy_score(y_val , y_pred_DT)*100,2) , \"%\")","16cdd80d":"param_dist = {'max_depth' : [3,None],\n             'max_features' : np.arange(1,9),\n             'min_samples_leaf':np.arange(1,9),\n             \"criterion\" :[\"gini\" , \"entropy\"]}\ntree = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree , param_dist , cv = 10 , verbose = 1)\ntree_cv.fit(X_train , y_train)\ny_pred_RF = tree_cv.predict(X_val)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred_RF))\nprint(\"accuracy:\",round(accuracy_score(y_val , y_pred_RF)*100,2),\"%\")","0a29b090":"param_dist = {'max_depth' : [3,None],\n             'max_features' : np.arange(1,9),\n             'min_samples_leaf':np.arange(1,9),\n             \"criterion\" :[\"gini\" , \"entropy\"]}\ntree = DecisionTreeClassifier()\ntree_cv = RandomizedSearchCV(tree , param_dist , cv = 10 , verbose = 1)\ntree_cv.fit(X_train , y_train)\ny_pred_RF = tree_cv.predict(X_val)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred_RF))\nprint(\"accuracy:\",round(accuracy_score(y_val , y_pred_RF)*100,2),\"%\")","d73344aa":"warnings.filterwarnings(\"ignore\")\n\nclf = SVC()\n\nclf.fit(X_train , y_train)\n\ny_pred_SVC = clf.predict(X_val)\n\nprint(classification_report(y_val , y_pred_SVC))\nprint('accuracy:' , accuracy_score(y_val , y_pred_SVC)*100,\"%\")","128f191c":"student_int = student_1[0:-1].astype(\"int64\")\n\nX_score_int = student_int.drop([\"overall score letter\" ,\"overall score\"] , axis = 1)\ny_score_int = student_int[\"overall score letter\"]\n\nX_train_int, X_val_int, y_train_int, y_val_int = train_test_split(X_score_int, y_score_int, test_size = 0.3, random_state=42)","22f55f0f":"warnings.filterwarnings(\"ignore\")\n\nxg = xgb.XGBClassifier(objective='reg:logistic', n_estimators = 10, seed=1234)\nxg.fit(X_train_int, y_train_int)\n\ny_pred_XGB = xg.predict(X_val_int)\n\nprint(\"accuracy:\",round(accuracy_score(y_val_int, y_pred_XGB)*100,2) , \"%\")","67ca3bc7":"student_2.replace([\"C-\" , \"C\" , \"C+\" ,] , (\"C\") , inplace = True)\nstudent_2.replace([\"B-\" , \"B\" , \"B+\" ,] , (\"B\") , inplace = True)\nstudent_2.replace([\"A-\" , \"A\" , \"A+\" ,] , (\"A\") , inplace = True)","efece9e0":"student_2.replace([\"F\" , \"D\" , \"C\" , \"B\" , \"A\"] , (0,1,2,3,4) , inplace = True)\n\n#    F = 0\n#    D = 1\n#    C = 2\n#    B = 3\n#    A = 4","dd534cc3":"X_score = student_2.drop([\"overall score letter\" , \"overall score\"] , axis = 1)\ny_score = student_2[\"overall score letter\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X_score, y_score, test_size = 0.3, random_state=42)","f29686e2":"logreg = LogisticRegression()\nlogreg.fit(X_train , y_train)\ny_pred = logreg.predict(X_test)\n\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_test , y_pred))\nprint(\"accuracy\",accuracy_score(y_test , y_pred)*100,\"%\")","e49e773b":"tree = DecisionTreeClassifier()\n\ntree.fit(X_train , y_train)\ny_pred_DT = tree.predict(X_test)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_test , y_pred_DT))\nprint(\"accuracy:\",round(accuracy_score(y_test , y_pred_DT)*100,2) , \"%\")","1e4d1afe":"param_dist = {'max_depth' : [3,None],\n             'max_features' : np.arange(1,9),\n             'min_samples_leaf':np.arange(1,9),\n             \"criterion\" :[\"gini\" , \"entropy\"]}\ntree = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree , param_dist , cv = 10 , verbose = 1)\ntree_cv.fit(X_train , y_train)\ny_pred_RF = tree_cv.predict(X_test)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_test , y_pred_RF))\nprint(\"accuracy:\",round(accuracy_score(y_test , y_pred_RF)*100,2),\"%\")","9862044e":"param_dist = {'max_depth' : [3,None],\n             'max_features' : np.arange(1,9),\n             'min_samples_leaf':np.arange(1,9),\n             \"criterion\" :[\"gini\" , \"entropy\"]}\ntree = DecisionTreeClassifier()\ntree_cv = RandomizedSearchCV(tree , param_dist , cv = 10 , verbose = 1)\ntree_cv.fit(X_train , y_train)\ny_pred_RF = tree_cv.predict(X_test)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_test , y_pred_RF))\nprint(\"accuracy:\",round(accuracy_score(y_test , y_pred_RF)*100,2),\"%\")","e098df93":"warnings.filterwarnings(\"ignore\")\n\nclf = SVC()\n\nclf.fit(X_train , y_train)\n\ny_pred_SVC = clf.predict(X_test)\n\nprint(classification_report(y_test , y_pred_SVC))\nprint('accuracy:' , round(accuracy_score(y_test , y_pred_SVC)*100,2),\"%\")","ece17c6a":"student_int = student_2[0:-1].astype(\"int64\")\n\nX_score_int = student_int.drop([\"overall score letter\" ,\"overall score\"] , axis = 1)\ny_score_int = student_int[\"overall score letter\"]\n\nX_train_int, X_test_int, y_train_int, y_test_int = train_test_split(X_score_int, y_score_int, test_size = 0.3, random_state=42)","e7a3c3e3":"warnings.filterwarnings(\"ignore\")\n\nxg = xgb.XGBClassifier(objective='reg:logistic', n_estimators = 10, seed=1234)\nxg.fit(X_train_int, y_train_int)\n\ny_pred_XGB = xg.predict(X_test_int)\n\nprint(\"accuracy:\",round(accuracy_score(y_test_int, y_pred_XGB)*100,2) , \"%\")","5ef2d89e":"X_score = student_1.drop([\"overall score letter\" , \"overall score\"] , axis = 1)\ny_score = student_1[\"overall score\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X_score, y_score, test_size = 0.3, random_state=42)","2dfa7737":"reg = linear_model.LinearRegression()\nreg.fit(X_train , y_train)\ny_pred_reg = reg.predict(X_test)\n\nRMSE = np.sqrt(mean_squared_error(y_test,y_pred_reg))\nprint(\"RMSE: \" , RMSE)\nMSE = mean_squared_error(y_test,y_pred_reg )\nprint(\"MSE: \" ,MSE)\nprint(\"R^2: \" , r2_score(y_test, y_pred_reg))","1b2b828c":"ridge = Ridge(alpha = 0.4 , normalize = True)\n\nridge.fit(X_train, y_train)\nridge_pred = ridge.predict(X_test)\nprint(\"RMSE: \" , np.sqrt(mean_squared_error(y_test,ridge_pred)))\nprint(\"MSE: \" , mean_squared_error(y_test,ridge_pred))\nprint(\"R^2: \" , r2_score(y_test, ridge_pred))","0a217413":"lasso = Lasso(alpha = 0.4, normalize = True , max_iter = 1000000)\n\nlasso.fit(X_train,y_train)\n\nlasso_pred = ridge.predict(X_test)\nprint(\"RMSE: \" , np.sqrt(mean_squared_error(y_test,lasso_pred)))\nprint(\"MSE: \" , mean_squared_error(y_test,lasso_pred))\nprint(\"R^2: \" , r2_score(y_test, lasso_pred))","2cd939d1":"X_score = student_1.drop([\"gender\"] , axis = 1)\ny_score = student_1[\"gender\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X_score, y_score, test_size = 0.3, random_state=42)","85634d52":"logreg = LogisticRegression()\nlogreg.fit(X_train , y_train)\ny_pred = logreg.predict(X_val)\n\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred))\nprint(\"accuracy\",round(accuracy_score(y_val , y_pred)*100,2),\"%\")","10506ddd":"param_grid = {'n_neighbors' : np.arange(1,50)}\n\nknn_cv = KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn_cv, param_grid, cv = 5)\n\nknn_cv.fit((X_train) , (y_train))\n\nprint(knn_cv.best_params_)\n\nprint(knn_cv.best_score_)\n\ny_pred_knn = knn_cv.predict(X_val)\n\nprint(\"accuracy:\" , round(accuracy_score(y_val , y_pred_knn)*100,2) , \"%\")","65fe8375":"tree = DecisionTreeClassifier()\n\ntree.fit((X_train) , (y_train))\ny_pred_DT = tree.predict(X_val)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred_DT))\nprint(\"accuracy:\",accuracy_score(y_val , y_pred_DT)*100 , \"%\")","bbf4a28b":"param_dist = {'max_depth' : [3,None],\n             'max_features' : np.arange(1,9),\n             'min_samples_leaf':np.arange(1,9),\n             \"criterion\" :[\"gini\" , \"entropy\"]}\ntree = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree , param_dist , cv = 5 , verbose = 1)\ntree_cv.fit((X_train) , (y_train))\ny_pred_RF = tree_cv.predict(X_val)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred_RF))\nprint(\"accuracy:\",round(accuracy_score(y_val , y_pred_RF)*100,2),\"%\")","2509f403":"param_dist = {'max_depth' : [3,None],\n             'max_features' : np.arange(1,9),\n             'min_samples_leaf':np.arange(1,9),\n             \"criterion\" :[\"gini\" , \"entropy\"]}\ntree = DecisionTreeClassifier()\ntree_cv = RandomizedSearchCV(tree , param_dist , cv = 5 , verbose = 1)\ntree_cv.fit((X_train) ,(y_train))\ny_pred_RF = tree_cv.predict(X_val)\n#print(confusion_matrix(y_val,y_pred))\nprint(classification_report(y_val , y_pred_RF))\nprint(\"accuracy:\",round(accuracy_score(y_val , y_pred_RF)*100,2),\"%\")","0516b515":"warnings.filterwarnings(\"ignore\")\n\nclf = SVC()\n\nclf.fit(X_train , y_train)\n\ny_pred_SVC = clf.predict(X_val)\n\nprint(classification_report(y_val , y_pred_SVC))\nprint('accuracy:' , round(accuracy_score(y_val , y_pred_SVC)*100,2),\"%\")","c6a08db6":"student_int = student_1[0:-1].astype(\"int64\")\n\nX_score_int = student_int.drop([\"gender\"] , axis = 1)\ny_score_int = student_int[\"gender\"]\n\nX_train_int, X_val_int, y_train_int, y_val_int = train_test_split(X_score_int, y_score_int, test_size = 0.3, random_state=42)","5b86de1c":"warnings.filterwarnings(\"ignore\")\n\nxg = xgb.XGBClassifier(objective='reg:logistic', n_estimators = 10, seed=1234)\nxg.fit(X_train_int, y_train_int)\n\ny_pred_XGB = xg.predict(X_val_int)\n\nprint(\"accuracy:\",accuracy_score(y_val_int, y_pred_XGB)*100 , \"%\")","70589ba8":"student = pd.read_csv(\"..\/input\/StudentsPerformance.csv\")","1f4e1698":"from sklearn.cluster import KMeans\n\ngender = student_1['gender'].values\nX_gen = student_1.drop('gender' , axis = 1).values\n","0348ba0a":"model = KMeans(n_clusters = 2)\nmodel.fit(X_gen)\nlabels = model.predict(X_gen)","f7966c97":"xs = X_gen[:,4]\nys = X_gen[:,5]\n\nf, (ax1, ax2) = plt.subplots(figsize=(20, 10) , ncols = 2)\n\nsns.set()\n\nsns.scatterplot(x=\"math score\", y=\"reading score\", hue=\"gender\" , data=student , ax = ax1)\nplt.xticks(rotation=360 , size = 20)\nplt.yticks( size = 20)\nplt.xlabel(\"Math Score\", size = 20)\nplt.ylabel(\"Reading Score\" , size = 20)\nplt.title(\"Actual Gender Data\" , size = 20)\n#plt.legend(loc=2, prop={'size': 20})\n\nsns.scatterplot(x = xs,y = ys , hue = labels , ax=ax2)\nplt.xlabel(\"Math Score\" , size = 20)\nplt.ylabel(\"Reading Score\" , size = 20)\nplt.title(\"KMeans Predicting Gender\" , size = 20)\nplt.legend(loc=2, prop={'size': 20})\nplt.show()\n\nacc = round(accuracy_score(labels , student_1.gender)*100,2)\nprint(('accuracy: '+ str(acc)+\" %\").center(125))","de478c25":"Decision Tree + RandomizedSearchCV: Predicting overall score","d84e731e":"*Those who receieved free\/reduced lunch had the most F's\n*Those who received standard lunch did better on their exams than the ones with free\/reduced lunch","e7868f32":"*The children with the most F's came from the families where the parents highest level education completed was high school.\n\n*Those children who's parents had the highest education (master's degree) got the best overall scores","0b536eff":"The accuracy I obtained is fairly good when trying to predict their overall scores. Our best accuracy is 86% via randomizedsearch cv with deceision tree.\n\n2) Can I get a better accuracy if I reduced the number of categories?","dd399da0":"Linear Regression: Predicting overall numerical score","acb1294f":"Decision Tree: Predicting Overall Score","13ca20f1":"Logistic Regression: Predicting Gender","f9ae1f1b":"4) Can we use unsupervised learning (such as KMeans), to predicting gender based on reading and math scores?","269e68b6":"Decreasing the number of categories increased our accuracy.","e2b3c8eb":"Logistic Regression: Predicting Overall Score","9706b734":"KNN: Predicting Gender","4b17612d":"Based on the heat map there is a strong correlation between reading and writing score as opposed to math scores.","7b15b843":"Reducing the number of categories significantly increased our accuracy, our best accuracy being 91% via XGBoost\n\n4) What will be our R^2 value if we performed regression instead of classification?","4904a1a5":"Decision Tree + RandomizedSearchCV: Predicting overall score letter with reduced categories","c348eef4":"*The most F's came from Group C of the ethnicity\/race\n*The best overall grades came from group E of the data.","bbaa64fb":"SVC: Predicting Gender","9ba187dd":"*Those who took the test prep did better than the ones who did not","03a8ee04":"Now we can perform our machine learning for the following questions:\n1) If I did not make an overall score , can I correctly predict their overall score solely based on reading, writing and math scores? (I can predict the letter grade so we can make this into a classification problem)","dc9a5ef6":"4) As I saw in the graph earlier, girls did better on their score than boys, can we obtain a good accuracy on predicting whether the student is a girl or boy?","274e7056":"Now that we have added some necessary columns, we can begin doing data visualization","eb29e605":"Regression gives us a high R^2 value which is also a noteable method for our machine learning practices.","2d328bf4":"Unsupervised Learning: KMeans Clustering","dd0b08f2":"XGBoost: Predicting overall score letter with reduced categories","d23568b5":"Overall we were able to get 90% accuracy in predicting the gender via Logistic Regression.","dc53086e":"*From our graphs we can see that boys recieved a failing grade more than girls\n*Girls recieved a failing grade in math more than boys","68079f67":"Decision Tree + GridSearch: Predicting Gender","1f6ddd58":"SVC: Predicting Overall Score","c6037f7e":"XGBoost: Predicting Gender","2a8f86f3":"Ridge Regression: Predicting overall numerical score","581a8271":"Lasso Regression: Predicting overall numerical score","5b1b7ae9":"XGBoost: Predicting Overall Score","dbdcbb20":"Overall we can use unsupervised learning to predict gender. However, KMeans clustering only gave us a poor accuracy, the best method for such classification would be using supervised learning.","8bc8e04e":"Decision Tree + GridsearchCV: Predicting overall score letter with reduced categories","68049d8d":"Decision Tree + RandomSearch: Predicting Gender","c747f388":"Decision Tree: Predicting Gender","ba4c83ce":"SVC: Predicting overall score letter with reduced categories","cefb484f":"We can create new columns in the dataframe to better our model and our visualization of the data","f1dc62e4":"Decision Tree + GridSearchCV: Predicting Overall Score","062eb874":"Decision Tree: Predicting overall score letter with reduced categories","aad81aab":"Logistic Regression: Predicting overall score letter with reduced categories","8c1412e3":"KNN: Predicting Overall Score"}}