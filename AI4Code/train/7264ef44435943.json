{"cell_type":{"7c9e8b55":"code","7e702bdf":"code","35326cab":"code","f70230ee":"code","c6a2b777":"code","b64fd521":"code","b29c3dd7":"code","592e0913":"code","e1d584c3":"code","e3990fb9":"code","2f3dccc9":"code","fcee9d2b":"code","61aa57e9":"code","2fe4fd59":"code","7f7b6836":"code","83e98416":"code","6f3904b9":"code","c7d4577c":"code","8bed8bd4":"code","f498489a":"code","ba28a022":"code","9247952f":"code","3827d4ac":"code","7b862362":"code","eb98ad44":"code","1a70f9bc":"code","e3841707":"code","e3c92a8f":"code","6921ec64":"code","fba0a2d1":"code","783cef64":"code","9c830e5c":"code","758fb130":"code","c4d22261":"code","e4d02b64":"code","9e277dc6":"code","132996f9":"code","bd909bc1":"code","c64d7a86":"markdown","846860d9":"markdown","801fab8b":"markdown","87535156":"markdown","63be3a51":"markdown","babd2377":"markdown","014f1aad":"markdown","33358cbc":"markdown","ea17196c":"markdown","1de7c7ca":"markdown","badd0517":"markdown","d7eee31a":"markdown","2ffdde49":"markdown","7c665b89":"markdown","00c5d94a":"markdown","39835824":"markdown","7af51a6e":"markdown","b019e749":"markdown","272e76b2":"markdown","560dd919":"markdown","02f2a070":"markdown","211f4a5f":"markdown","a063e6d0":"markdown","05870f56":"markdown","2cea3201":"markdown","91838001":"markdown","fe0bbdff":"markdown","746a6d21":"markdown","0653b4fd":"markdown","a3543ed6":"markdown"},"source":{"7c9e8b55":"DEBUG_MODE = False","7e702bdf":"!pip install ..\/input\/pytorchlightning-071\/pytorch-lightning-0.7.1\/pytorch-lightning-0.7.1","35326cab":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nimport random\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom collections import Counter\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\n\nimport pytorch_lightning as pl\n\nfrom tqdm import tqdm_notebook as tqdm","f70230ee":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","c6a2b777":"seed_everything()","b64fd521":"DIR = Path('..\/input\/tweet-sentiment-extraction')","b29c3dd7":"df_train = pd.read_csv(DIR \/ 'train.csv')\ndf_test = pd.read_csv(DIR \/ 'test.csv')","592e0913":"df_train['text'] = df_train['text'].apply(lambda x: str(x))\ndf_test['text'] = df_test['text'].apply(lambda x: str(x))\ndf_train['uncased_text'] = df_train['text'].apply(lambda x: x.lower())\ndf_test['uncased_text'] = df_test['text'].apply(lambda x: x.lower())\ndf_train['selected_text'] = df_train['selected_text'].apply(lambda x: str(x).lower())","e1d584c3":"tokenizer = BertTokenizer.from_pretrained('..\/input\/berttokenizer-base-uncased')","e3990fb9":"# Tokenize\ndf_train['tokenized_text'] = df_train['uncased_text'].apply(tokenizer.tokenize)\ndf_test['tokenized_text'] = df_test['uncased_text'].apply(tokenizer.tokenize)\ndf_train['tokenized_selected_text'] = df_train['selected_text'].apply(tokenizer.tokenize)","2f3dccc9":"# Filter train data\nstart_position_candidates = []\nend_position_candidates = []\ndf_train['select_length'] = df_train['tokenized_selected_text'].map(len)\n\nfor i in tqdm(range(len(df_train))):\n    start_position_candidate = [j for j, tok in enumerate(df_train['tokenized_text'].iloc[i]) if tok == df_train['tokenized_selected_text'].iloc[i][0]]\n    end_position_candidate = [j for j, tok in enumerate(df_train['tokenized_text'].iloc[i]) if tok == df_train['tokenized_selected_text'].iloc[i][-1]]\n\n    start_position_candidate = [idx for idx in start_position_candidate if idx + df_train['select_length'].iloc[i] - 1 in end_position_candidate]\n    end_position_candidate = [idx for idx in end_position_candidate if idx - df_train['select_length'].iloc[i] + 1 in start_position_candidate]\n\n    start_position_candidates.append(start_position_candidate)\n    end_position_candidates.append(end_position_candidate)","fcee9d2b":"start_position_candidates = [l[0] if len(l) > 0 else -1 for l in start_position_candidates]\nend_position_candidates = [l[0] if len(l) > 0 else -1 for l in end_position_candidates]","61aa57e9":"df_train['start_position'] = start_position_candidates\ndf_train['end_position'] = end_position_candidates\ndf_test['start_position'] = -1\ndf_test['end_position'] = -1","2fe4fd59":"df_train = df_train.query('start_position!=-1')","7f7b6836":"df_train, df_val = train_test_split(df_train, train_size=0.8)","83e98416":"pos_train = df_train.query('sentiment==\"positive\"')\nneg_train = df_train.query('sentiment==\"negative\"')\nneu_train = df_train.query('sentiment==\"neutral\"')\n\npos_val = df_train.query('sentiment==\"positive\"')\nneg_val = df_train.query('sentiment==\"negative\"')\nneu_val = df_train.query('sentiment==\"neutral\"')\n\npos_test = df_test.query('sentiment==\"positive\"')\nneg_test = df_test.query('sentiment==\"negative\"')\nneu_test = df_test.query('sentiment==\"neutral\"')","6f3904b9":"pos_model = BertForQuestionAnswering.from_pretrained('..\/input\/bertforquestionanswering-base-uncased')\nneg_model = BertForQuestionAnswering.from_pretrained('..\/input\/bertforquestionanswering-base-uncased')","c7d4577c":"MAX_LENGTH = 128\nBATCH_SIZE = 32","8bed8bd4":"class TrainDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['uncased_text'].values\n        self.start_ids = df['start_position'].values\n        self.end_ids = df['end_position'].values\n        self.hash_index = df['textID'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        returns = {\n            'text' : self.texts[idx],\n            'start' : self.start_ids[idx],\n            'end' : self.end_ids[idx],\n            'idx' : idx\n        }\n        return returns","f498489a":"class TestDataset(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.texts = df['uncased_text'].values\n        self.hash_index = df['textID'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        returns = {\n            'text' : self.texts[idx],\n            'idx' : idx\n        }\n        return returns","ba28a022":"ds_pos_train = TrainDataset(pos_train)\nds_neg_train = TrainDataset(neg_train)\n\nds_pos_val = TrainDataset(pos_val)\nds_neg_val = TrainDataset(neg_val)\n\nds_pos_test = TestDataset(pos_test)\nds_neg_test = TestDataset(neg_test)","9247952f":"dl_pos_train = DataLoader(ds_pos_train, batch_size=BATCH_SIZE, shuffle=True)\ndl_neg_train = DataLoader(ds_neg_train, batch_size=BATCH_SIZE, shuffle=True)\n\ndl_pos_val = DataLoader(ds_pos_val, batch_size=BATCH_SIZE, shuffle=False)\ndl_neg_val = DataLoader(ds_neg_val, batch_size=BATCH_SIZE, shuffle=False)\n\ndl_pos_test = DataLoader(ds_pos_test, batch_size=BATCH_SIZE, shuffle=False)\ndl_neg_test = DataLoader(ds_neg_test, batch_size=BATCH_SIZE, shuffle=False)","3827d4ac":"class BaseSuperModule(pl.LightningModule):\n    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n        super().__init__()\n        self.bertmodel = bertmodel\n        self.tokenizer = tokenizer\n        self.prediction_save_path = prediction_save_path\n\n    def get_device(self):\n        return self.bertmodel.state_dict()['bert.embeddings.word_embeddings.weight'].device\n\n    def save_predictions(self, start_positions, end_positions):\n        d = pd.DataFrame({'start_position':start_positions, 'end_position':end_positions})\n        d.to_csv(self.prediction_save_path, index=False)\n\n    def forward(self, batch):\n        \"\"\"\n        Input:\n            batch(dict), where\n                batch['text'] = uncased text: str\n                batch['idx'] = raw text: list(int)\n                batch['start'] = start position indices : list(int) (for train & val batch only)\n                batch['end'] = end position indices : list(int) (for train & val batch only)\n\n        Output:\n            For train batch, which has 'start' key and 'end' key:\n                Tuple of (loss(int), start_score(torch.tensor), end_score(torch.tensor))\n            For test batch, without 'start' key and 'end' key:\n                Tuple of (start_score(torch.tensor), end_score(torch.tensor))\n        \"\"\"\n        encoded_batch = tokenizer.batch_encode_plus(batch['text'], max_length=MAX_LENGTH, pad_to_max_length=True)\n        input_ids = torch.tensor(encoded_batch['input_ids']).to(self.get_device())\n        attention_mask = torch.tensor(encoded_batch['attention_mask']).to(self.get_device())\n        start_positions = batch['start'].to(self.get_device()) + 1  if 'start' in batch.keys() else None\n        end_positions = batch['end'].to(self.get_device()) + 1  if 'end' in batch.keys() else None\n\n        model_inputs = {\n            'input_ids' : input_ids,\n            'attention_mask' : attention_mask,\n            'start_positions' : start_positions,\n            'end_positions' : end_positions\n        }\n        \n        return self.bertmodel(**model_inputs)\n\n    def training_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        # Caution: key for loss function must exactly be 'loss'.\n        \"\"\"\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss':loss, 'idx':idx}\n\n    def validation_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        # Caution: key for loss function must exactly be 'loss'.\n        \"\"\"\n        idx = batch['idx']\n        loss = self.forward(batch)[0]\n        return {'loss':loss, 'idx':idx}\n\n    def test_step(self, batch, batch_nb):\n        \"\"\"\n        (batch) -> (dict or OrderedDict)\n        \"\"\"\n        idx = batch['idx']\n        start_scores = self.forward(batch)[0]\n        end_scores = self.forward(batch)[1]\n        return {'start_scores':start_scores, 'end_scores':end_scores, 'idx':idx}\n\n    def training_end(self, outputs):\n        \"\"\"\n        outputs(dict) -> loss(dict or OrderedDict)\n        # Caution: key must exactly be 'loss'.\n        \"\"\"\n        return {'loss':outputs['loss']}\n\n    def validation_end(self, outputs):\n        \"\"\"\n        For single dataloader:\n            outputs(list of dict) -> (dict or OrderedDict)\n        For multiple dataloaders:\n            outputs(list of (list of dict)) -> (dict or OrderedDict)\n        \"\"\"        \n        return {'loss':torch.mean(torch.tensor([output['loss'] for output in outputs])).detach()}\n\n    def test_end(self, outputs):\n        \"\"\"\n        For single dataloader:\n            outputs(list of dict) -> (dict or OrderedDict)\n        For multiple dataloaders:\n            outputs(list of (list of dict)) -> (dict or OrderedDict)\n        \"\"\"\n        start_scores = torch.cat([output['start_scores'] for output in outputs]).detach().cpu().numpy()\n        start_positions = np.argmax(start_scores, axis=1) - 1\n\n        end_scores = torch.cat([output['end_scores'] for output in outputs]).detach().cpu().numpy()\n        end_positions = np.argmax(end_scores, axis=1) - 1\n        self.save_predictions(start_positions, end_positions)\n        return {}\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=2e-5)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def val_dataloader(self):\n        pass\n\n    @pl.data_loader\n    def test_dataloader(self):\n        pass","7b862362":"class PositiveModule(BaseSuperModule):\n    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n        super().__init__(bertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_pos_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_pos_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_pos_test","eb98ad44":"class NegativeModule(BaseSuperModule):\n    def __init__(self, bertmodel, tokenizer, prediction_save_path):\n        super().__init__(bertmodel, tokenizer, prediction_save_path)\n\n    @pl.data_loader\n    def train_dataloader(self):\n        return dl_neg_train\n\n    @pl.data_loader\n    def val_dataloader(self):\n        return dl_neg_val\n\n    @pl.data_loader\n    def test_dataloader(self):\n        return dl_neg_test","1a70f9bc":"pos_module = PositiveModule(pos_model, tokenizer, 'pos_pred.csv')\nneg_module = NegativeModule(neg_model, tokenizer, 'neg_pred.csv')","e3841707":"device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'","e3c92a8f":"pos_module.to(device)\nneg_module.to(device)","6921ec64":"pos_trainer = pl.Trainer(max_nb_epochs=3, fast_dev_run=DEBUG_MODE)\nneg_trainer = pl.Trainer(max_nb_epochs=3, fast_dev_run=DEBUG_MODE)","fba0a2d1":"pos_trainer.fit(pos_module)\nneg_trainer.fit(neg_module)","783cef64":"pos_trainer.test()\nneg_trainer.test()","9c830e5c":"!ls","758fb130":"pos_pred = pd.read_csv('pos_pred.csv')\nneg_pred = pd.read_csv('neg_pred.csv')\npos_pred","c4d22261":"df_test.index = df_test['textID']\ndf_test['selected_text'] = ''\n\ndf_test.loc[ds_pos_test.hash_index[:BATCH_SIZE if DEBUG_MODE else len(df_test)], 'start_position':'end_position'] = pos_pred.values\ndf_test.loc[ds_neg_test.hash_index[:BATCH_SIZE if DEBUG_MODE else len(df_test)], 'start_position':'end_position'] = neg_pred.values\ndf_test","e4d02b64":"for i in tqdm(range(len(df_test))):\n    if df_test['sentiment'].iloc[i] in ('positive', 'negative'):\n        tokenized_text = df_test['tokenized_text'].iloc[i]\n        start_position = max(df_test['start_position'].iloc[i], 0)\n        end_position = min(df_test['end_position'].iloc[i], len(tokenized_text) - 1)\n        \n        # restore original text\n        selected_text = tokenizer.convert_tokens_to_string(tokenized_text[start_position : end_position + 1])\n        for original_token in df_test['text'].iloc[i].split():\n            tokenized_form = tokenizer.convert_tokens_to_string(tokenizer.tokenize(original_token))\n            selected_text = selected_text.replace(tokenized_form, original_token, 1)\n        \n        df_test['selected_text'].iloc[i] = selected_text","9e277dc6":"for i in tqdm(range(len(df_test))):\n    if df_test['sentiment'].iloc[i] == 'neutral':\n        df_test['selected_text'].iloc[i] = df_test['text'].iloc[i]\n    else:\n        pass","132996f9":"df_test.loc[:, ['textID', 'selected_text']]","bd909bc1":"df_test.loc[:, ['textID', 'selected_text']].to_csv('submission.csv', index=False)","c64d7a86":"# 2. BERT fine-tuning","846860d9":"This notebook requires PyTorch-Lighning.  \n\nTo use PyTorch-Lighning on Internet-off notebook, the following dataset is helpful:  \n\n[pytorch-lightning 0.7.1](https:\/\/www.kaggle.com\/higepon\/pytorchlightning-071) by @[higepon](https:\/\/www.kaggle.com\/higepon)","801fab8b":"Apply predicsion results onto df_test for positive & negative samples.","87535156":"For test data, set default start & end positions to dummy integer (-1).","63be3a51":"# Preface","babd2377":"### 1-5. Start & end positions","014f1aad":"### 0-2. Import modules","33358cbc":"Drop train data with no candidates of start & end positions due to poor segmentation of selected texts.  \n  \nSee also [this discussion](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/138272).","ea17196c":"If more than one candidates are available, we here take only the first candidate and discard the rest for plainness.","1de7c7ca":"# 3. Postprocessing","badd0517":"# 0. Dependencies","d7eee31a":"# 1. Preparation","2ffdde49":"### 1-1. Ensure determinism","7c665b89":"Start positions and end positions of selected texts in tokenized source texts.","00c5d94a":"Convert a few float type samples in 'text' and 'selected_text' columns into strings.","39835824":"Hope this notebook will help you all.  \n\nYour upvotes and comments will greatly be appreciated.\n\nLet us enjoy NLP!! ;)","7af51a6e":"### 1-2. Load dataset","b019e749":"Convert start & end position into selected texts.","272e76b2":"### 0-0. Debug mode flag","560dd919":"This notebook will deal with positive, negative and neutral samples independently.","02f2a070":"Hugging Face Transformers tokenizers must be prepared in your Kaggle dataset to use in off-line notebook.  \n  \nSee also [berttokenizer-base-uncased](https:\/\/www.kaggle.com\/yutanakamura\/berttokenizer-base-uncased).","211f4a5f":"### 0-1. Install PyTorch-Lightning","a063e6d0":"Some non-engineer NLP beginners, like me, may be literate for PyTorch but not for Keras\/TensorFlow.  \n  \nThis notebook has two major objectives:\n\n- To share starter code for PyTorch.\n\n- To provide an example of easy implementation with PyTorch-Lightning. PyTorch does not provide wrappers for training neural networks, but PyTorch-Lightning will free us from explicit for-loops.","05870f56":"### 1-7. Pos\/Neg\/Neu split","2cea3201":"The previous cell will save prediction results as pos_pred.csv and neg_pred.csv.  \nLet us load them.","91838001":"### 1-3. Clean dataset","fe0bbdff":"### 1-4. Tokenize","746a6d21":"Two fine-tuned models will be individual used for positive samples and negative.  \n  \nHugging Face Transformers BERT models must be prepared in your Kaggle dataset for use in the off-line notebook.  \n  \nSee also [bertforquestionanswering-base-uncased](https:\/\/www.kaggle.com\/yutanakamura\/bertforquestionanswering-base-uncased).","0653b4fd":"For neutral samples, use original texts as they are.","a3543ed6":"### 1-6. Train\/Val split"}}