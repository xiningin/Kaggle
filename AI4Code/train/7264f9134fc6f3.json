{"cell_type":{"ddf2f270":"code","3630bdf8":"code","9fed4626":"code","eeee2dce":"code","c6264063":"code","8e4c1099":"code","938c3e7a":"code","5c82a106":"code","b4d2d981":"code","01e86286":"code","13b96bc8":"code","77a0c8ec":"code","3f3904f1":"code","8e5f8e5a":"code","7289f8c8":"code","5f05fe76":"code","7004e23d":"code","be41be95":"code","0e782415":"markdown","9ee9fa02":"markdown","b578d0d4":"markdown"},"source":{"ddf2f270":"# Data processing\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\nimport random\n# Visualization libaries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# check if date is a holiday:\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar","3630bdf8":"# Read data\ntrain = pd.read_csv('..\/input\/train.csv', nrows = 11123456)\n\n#reading in sampled data misses the header row - need to debug\n# n = 55000000 #Approx number of records in file as given in competition description\n# s = 3123456 #desired sample size ~ a few million for now\n# skip = sorted(random.sample(range(n+1),n-s))\n# train = pd.read_csv('..\/input\/train.csv', skiprows=skip,header=0) # sample data. Explicitly note that there's a header, otherwise we can skip it when sampling! \n\ntrain['pickup_datetime'] = pd.to_datetime(train.pickup_datetime,infer_datetime_format=True)\n\ntrain.dropna(how = 'any', axis = 'rows',inplace=True) # drop some outliers\/bad data\\\\\n# todo: drop outliers from train based on distances? \nprint(\"train shape:\",train.shape)\n\ntrain.head()","9fed4626":"test = pd.read_csv('..\/input\/test.csv')\ntest['pickup_datetime'] = pd.to_datetime(test.pickup_datetime,infer_datetime_format=True)\nprint(\"test size:\", test.shape)\nprint('Test NaN values ?:')\nprint(test.isnull().sum()) # no nans in the test, unlike the train data\n\ntest.describe()","eeee2dce":"test.describe()","c6264063":"# Remove some potential location outliers:\n## Taken from : https:\/\/www.kaggle.com\/judesen\/fare-prediction\n## Ideally, check if test also has the extreme values or data errors.. \n\n# Latitude and longitude varies from -3116.28 to 2522.27 whereas the mean is around 40 (pickup_latitude, but goes for all the coordinates)\n# This is probably due to a typo when data was gathered. Let's select a more reasonable value (2 times the standard deviation)\n#columns_to_select = ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n#for column in columns_to_select:\n#    train = train.loc[(train[column] > train[column].mean() - train[column].std() * 2) & (train[column] < train[column].mean() + train[column].std() * 2)]\n\n# Manually picking reasonable levels until I find a smarter way\ntrain = train.loc[(train['fare_amount'] >= 0) & (train['fare_amount'] < 250)]\ntrain = train.loc[(train['pickup_longitude'] > -90) & (train['pickup_longitude'] < 80) & (train['pickup_latitude'] > -80) & (train['pickup_latitude'] < 80)]\ntrain = train.loc[(train['dropoff_longitude'] > -90) & (train['dropoff_longitude'] < 80) & (train['dropoff_latitude'] > -80) & (train['dropoff_latitude'] < 80)]\n\n\n# Let's assume tax's can be mini-busses as well\ntrain = train.loc[train['passenger_count'] <= 7]\ntrain.describe()","8e4c1099":"combine = [train, test]\n# test.dtypes","938c3e7a":"for dataset in combine:\n    # Distance is expected to have an impact on the fare\n    dataset['longitude_distance'] = abs(dataset['pickup_longitude'] - dataset['dropoff_longitude'])\n    dataset['latitude_distance'] = abs(dataset['pickup_latitude'] - dataset['dropoff_latitude'])\n    \n    # Straight distance\n    dataset['distance_travelled'] = (dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5\n#     dataset['distance_travelled_sin'] = np.sin((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5)\n#     dataset['distance_travelled_cos'] = np.cos((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5)\n    \n#     dataset['distance_travelled_sin_sqrd'] = np.sin((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5) ** 2\n#     dataset['distance_travelled_cos_sqrd'] = np.cos((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5) ** 2\n    \n    # Haversine formula for distance\n    # Haversine formula:\ta = sin\u00b2(\u0394\u03c6\/2) + cos \u03c61 \u22c5 cos \u03c62 \u22c5 sin\u00b2(\u0394\u03bb\/2)\n    # c = 2 \u22c5 atan2( \u221aa, \u221a(1\u2212a) )\n    # d = R \u22c5 c\n    R = 6371e3 # Metres\n    phi1 = np.radians(dataset['pickup_latitude'])\n    phi2 = np.radians(dataset['dropoff_latitude'])\n    phi_chg = np.radians(dataset['pickup_latitude'] - dataset['dropoff_latitude'])\n    delta_chg = np.radians(dataset['pickup_longitude'] - dataset['dropoff_longitude'])\n    a = np.sin(phi_chg \/ 2) + np.cos(phi1) * np.cos(phi2) * np.sin(delta_chg \/ 2)\n    c = 2 * np.arctan2(a ** .5, (1-a) ** .5)\n    d = R * c\n    dataset['haversine'] = d\n    \n    # Bearing\n    # Formula:\t\u03b8 = atan2( sin \u0394\u03bb \u22c5 cos \u03c62 , cos \u03c61 \u22c5 sin \u03c62 \u2212 sin \u03c61 \u22c5 cos \u03c62 \u22c5 cos \u0394\u03bb )\n    y = np.sin(delta_chg * np.cos(phi2))\n    x = np.cos(phi1) * np.sin(phi2) - np.sin(phi1) * np.cos(phi2) * np.cos(delta_chg)\n    dataset['bearing'] = np.degrees(np.arctan2(y, x))\n    \n#     # Rhumb lines\n    psi_chg = np.log(np.tan(np.pi \/ 4 + phi2 \/ 2) \/ np.tan(np.pi \/ 4 + phi1 \/ 2))\n    q = phi_chg \/ psi_chg\n    d = (phi_chg + q ** 2 * delta_chg ** 2) ** .5 * R\n    dataset['rhumb_lines'] = d\n    \n    # Maybe time of day matters? Obviously duration is a factor, but there is no data for time arrival\n    # Features: hour of day (night vs day), month (some months may be in higher demand) \n    \n#     dataset['pickup_datetime'] = pd.to_datetime(test['pickup_datetime']) # orig, used only test??\n#     dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'],infer_datetime_format=True) # new - may be wrong? \n    \n    dataset['hour_of_day'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n#     dataset['week'] = dataset.pickup_datetime.dt.week\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset[\"dayofweek\"] = dataset.pickup_datetime.dt.dayofweek\n#     dataset['day_of_year'] = dataset.pickup_datetime.dt.dayofyear\n\n#     dataset['week_of_year'] = dataset.pickup_datetime.dt.weekofyear\n    \n    # check if holiday. Can try other calendars: https:\/\/stackoverflow.com\/questions\/29688899\/pandas-checking-if-a-date-is-a-holiday-and-assigning-boolean-value\n    cal = calendar()\n    holidays = cal.holidays()\n    dataset[\"usFedHoliday\"] =  dataset.pickup_datetime.dt.date.astype('datetime64').isin(holidays)\n    \n    print(dataset.shape)\n    \ntrain.head(3)","5c82a106":"train['distance_travelled'].describe()","b4d2d981":"test['distance_travelled'].describe()","01e86286":"train = train.loc[train['distance_travelled']< 0.6]\nprint(train.shape)","13b96bc8":"print(test.shape)\nprint(test.dropna().shape)","77a0c8ec":"train.isna().sum()","3f3904f1":"test.isna().sum()","8e5f8e5a":"colormap = plt.cm.RdBu\nplt.figure(figsize=(20,20))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","7289f8c8":"# Drop irrelevant features\n# We drop the key\n\n# train_features_to_keep = ['haversine', 'fare_amount']\n# train.drop(train.columns.difference(train_features_to_keep), 1, inplace=True)\n\ntrain.drop([\"key\",\"pickup_datetime\"],axis=1,inplace=True)\ntrain.dropna(inplace=True)\n\n# test_features_to_keep = ['key', 'haversine']\n# test.drop(test.columns.difference(test_features_to_keep), 1, inplace=True)\n\ntest.drop([\"pickup_datetime\"],axis=1,inplace=True) # keep key in data for submission","5f05fe76":"# Let's prepare the test set\nx_pred = test.drop('key', axis=1)\n\n# Let's run XGBoost and predict those fares!\nx_train,x_test,y_train,y_test = train_test_split(train.drop('fare_amount',axis=1),train.pop('fare_amount'),random_state=126,test_size=0.16)\n\ndef XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'}\n                    ,dtrain=matrix_train,num_boost_round=350, \n                    early_stopping_rounds=15,evals=[(matrix_test,'test')],)\n    return model\n\nmodel=XGBmodel(x_train,x_test,y_train,y_test)\nprediction = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","7004e23d":"# Add to submission\nsubmission = pd.DataFrame({\n        \"key\": test['key'],\n        \"fare_amount\": prediction.round(3)\n})\n\nsubmission.to_csv('sub_fare.csv',index=False)","be41be95":"print(submission.shape)\nsubmission.head()","0e782415":"#### Features correlation plot:","9ee9fa02":"### Drop trip distance outliers:","b578d0d4":"#### Modified slightly for a niave baseline\n* Distance wise: we should use Manhattan\/cab distance (rectangles, not straight lines, as there's no diagonal in manhattan).\n* I get a sample of data instead of just the head. \n    * Ideally -  downsample randomly + get a temporally stratified sample based on aiming to overfit the test set.  BUT data is randomly sampled anyway, so likely doesn;'t matter\n* ToDo: more distance, speed, geographic clusters, more datetime\/clustering features.."}}