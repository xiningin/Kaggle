{"cell_type":{"f66cb41d":"code","830b4644":"code","f460f052":"code","fd449aeb":"code","d0d7c161":"code","aa255836":"code","06247363":"code","539730ba":"code","931c498d":"code","c99e0222":"code","53560d7c":"code","8cc6c72f":"code","af0fc902":"code","f5ebbec1":"code","09983c80":"code","6dba4d3f":"code","71987760":"code","73bb6fe6":"code","14e9c563":"code","2f93c113":"code","b8dd76f1":"code","6fa581a2":"code","0f19eb51":"code","1be97584":"code","5d7b98cf":"code","4a470b50":"code","43dd201c":"code","2089c916":"markdown","72c4a589":"markdown","b4f41011":"markdown","3cc5cb2a":"markdown","a2202cff":"markdown","5c196ce0":"markdown","0468156c":"markdown","41c09b79":"markdown","293fdd2b":"markdown","761e9086":"markdown","0bab3cf5":"markdown","0923cb3f":"markdown","0c69ca3d":"markdown","5fb2e5fe":"markdown","f55552a8":"markdown","626ac258":"markdown","700c682f":"markdown","b5dd1624":"markdown","bd384b01":"markdown","72870791":"markdown","e485380f":"markdown","1cbfcec7":"markdown","eaf4b106":"markdown","46df31be":"markdown"},"source":{"f66cb41d":"import tensorflow.python as tf\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n","830b4644":"train_dir = os.path.join('..\/input', 'skin-cancer-malignant-vs-benign\/train')\nvalidation_dir = os.path.join('..\/input', 'skin-cancer-malignant-vs-benign\/test')\n\ntrain_benign_dir = os.path.join(train_dir, 'benign')  # directory with our training benign pictures\ntrain_malignant_dir = os.path.join(train_dir, 'malignant')  # directory with our training malignant pictures\nvalidation_benign_dir = os.path.join(validation_dir, 'benign')  # directory with our validation benign pictures\nvalidation_malignant_dir = os.path.join(validation_dir, 'malignant')  # directory with our validation malignant pictures","f460f052":"num_benign_tr = len(os.listdir(train_benign_dir))\nnum_malignant_tr = len(os.listdir(train_malignant_dir))\n\nnum_benign_val = len(os.listdir(validation_benign_dir))\nnum_malignant_val = len(os.listdir(validation_malignant_dir))\n\ntotal_train = num_benign_tr + num_benign_tr\ntotal_val = num_malignant_val + num_malignant_val","fd449aeb":"print('total training benign images:', num_benign_tr)\nprint('total training malignant images:', num_malignant_tr)\n\nprint(\"--\")\nprint(\"--\")\n\nprint('total validation benign images:', num_benign_val)\nprint('total validation malignant images:', num_malignant_val)\n\nprint(\"--\")\nprint(\"--\")\n\nprint(\"Total training images:\", total_train)\nprint(\"Total validation images:\", total_val)\n","d0d7c161":"batch_size = 256\nepochs = 100\nIMG_HEIGHT = 150\nIMG_WIDTH = 150","aa255836":"train_image_generator = ImageDataGenerator(rescale=1.\/255) # Generator for our training data\nvalidation_image_generator = ImageDataGenerator(rescale=1.\/255) # Generator for our validation data","06247363":"train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                        directory=train_dir,\n                                                        shuffle=True,\n                                                        target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                        class_mode='binary',\n                                                        color_mode='rgb')\n\nval_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n                                                        directory=validation_dir,\n                                                        target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                        class_mode='binary',\n                                                        color_mode='rgb')","539730ba":"sample_training_images, _ = next(train_data_gen)\n\n# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nplotImages(sample_training_images[:5])","931c498d":"model = Sequential([\n    Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(128, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1)\n])","c99e0222":"model.compile(optimizer='adam',\n            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=['accuracy'])\n\nmodel.summary()","53560d7c":"history = model.fit_generator(\ntrain_data_gen,\nsteps_per_epoch=total_train \/\/ batch_size,\nepochs=epochs,\nvalidation_data=val_data_gen,\nvalidation_steps=total_val \/\/ batch_size\n)","8cc6c72f":"filename = '..\/output\/kaggle\/working\/model.sav'\nmodel.save(filename)","af0fc902":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","f5ebbec1":"train_augmented_image_generator = ImageDataGenerator(\n                    rescale=1.\/255,\n                    rotation_range=45,\n                    width_shift_range=.15,\n                    height_shift_range=.15,\n                    horizontal_flip=True,\n                    zoom_range=0.5\n                    )","09983c80":"train_data_gen = train_augmented_image_generator.flow_from_directory(batch_size=batch_size,\n                                                     directory=train_dir,\n                                                     shuffle=True,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                     class_mode='binary',\n                                                     color_mode='rgb')","6dba4d3f":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]\nplotImages(augmented_images)","71987760":"model_augmented = Sequential([\n    Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(128, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1)\n])","73bb6fe6":"model_augmented.compile(optimizer='adam',\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\nmodel_augmented.summary()","14e9c563":"history = model_augmented.fit_generator(\n    train_data_gen,\n    steps_per_epoch=total_train \/\/ batch_size,\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=total_val \/\/ batch_size\n)","2f93c113":"filename = '..\/output\/kaggle\/working\/model_augmented.sav'\nmodel_augmented.save(filename)\n\n","b8dd76f1":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","6fa581a2":"epochs = 200","0f19eb51":"model_augmented_extended = Sequential([\n    Conv2D(32, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(128, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1)\n])","1be97584":"model_augmented_extended.compile(optimizer='adam',\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\nmodel_augmented_extended.summary()","5d7b98cf":"history = model_augmented_extended.fit_generator(\n    train_data_gen,\n    steps_per_epoch=total_train \/\/ batch_size,\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=total_val \/\/ batch_size\n)","4a470b50":"filename = 'model_augmented_extended.sav'\nmodel_augmented.save(filename)","43dd201c":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","2089c916":"We now display one of the images that has been augmented multiple times. This is a good idea to visually check that the transformations look realistic.","72c4a589":"**Display Random Samples:**\n\nHere we display 5 randomly selected training samples to confirm that the data has been shuffled.","b4f41011":"As predicted, augmentation of our training set resulted in an improvement in the accuracy of our model. Our Validation accuracy has improved from almost 82.81% with the standard data, to 88.30% with our augmentented data. This is a considerable improvement.\n\nFurther viewing of the figures show that the validation accuracy is still trending upwards after 100 epochs. furthermore, the validation loss is trending downwards. This implies that training our model for more epochs may result in improved accuracy of out model. \n\nWe will now increase the number of epochs to 200 to observe any further improvements to our model.","3cc5cb2a":"**Check Spread of Dataset:**\n\nIt is important to count the number of samples for the different types of data in our datasets to ensure that the data is not heavily biased towards one category.\n\n**For example:** If we were to train this Neural Network with 950 sample of benign skin cancer and 100 samples of malignant skin cancers, it is more than likely that our model will be heavily biased towards predicting inputs as benign skin cancers. This would result in a model that works great on our training set, but very poorly in the real world, leading to many mis-diagnoses.\n\nThe output of the code below shows that we have an acceptable spread of data for training our neural network. With 1440 traing samples classified as benign images and 1197 samples classified as malignant.","a2202cff":"**Import Modules:**\n\nWe begin our notebook by importing the modules needed for the project. \nThe modules required for this project are: **tensorflow, os, numpy and matplotlib.**","5c196ce0":"**Data Augmentation:**\n\nWhen we have a small number of training samples and high variance in our model, it can be useful to randomly apply transformations to training samples to simulate more data. With tensorflow, this can be done with the Image Data Generator as shown below.","0468156c":"Now that have set up the path to our dataset and ensured that we have a reasonable spread of categorized samples, we can define some of the parameters that we will use in out neural network.\n\n**batch_size:** This parameter will define the amount of samples that we train at a time. Our model will be trained in batches until all (or most) of the training samples have been used in the epoch.\n\n**epochs:** This is the number of times that we re-train the model to improve our hypothesis.\n\n**IMG_HEIGHT:** the number of pixels in the vertical axis of our training samples.\n\n\n**IMG_WIDTH:** the number of pixels in the horizontal axis of our training samples.","41c09b79":"And we again visually display our data.","293fdd2b":"And finally, plot the training and validation curves.","761e9086":"**Define Model:**\n\nNow that we have set-up our dataset, we can define the structure of our model. In this case we define a simple neural network. Dropout() is used as a way to regularize our model and reduce overfitting. In this case, 20% of neurons are randomly ignored during each training step.","0bab3cf5":"**Discussion of Results:**\n\nThe model has been trained for 100 epochs. Reviewing the figures above, it is evident that the hyptothesis is approaching an asymptote at around 83% validation accuracy. The plot also shows that there is a low bias and high variance. In other words, our model is overfitting our training set. This information can be useful for deciding how to further improve our model. \n\nKnowing that our model has high variance, there are several assumptions that we can make to decide which actions are most likely to improve the accuracy of our model. To fix high variance, we need to add bias. The most common ways to do this are by regularization and adding more traing samples. Considering these two options we can look at the size of our training set.\n\nOur training set consists of only 2637 images. this is considered very small for image classification. As we have already implemented some regularization in our model (using Dropout(0.2)), it would seem that adding more samples to our training samples to our dataset. In an ideal world, we would have access 10,000+ traning samples per class to make accurate models. In reality it is often difficult to source more data. In this case, we can investigate the benifits that augmentation will have on the accuracy of our model. ","0923cb3f":"**Image Data Generator:**\n\nWe continue by creating image generators for our training data and verification date. We set the rescale parameter to scale the intensity values of the pixels (RGB, Grayscale, etc) so that our network can deal with smaller numbers. This reduces the complexity of the calculations and makes the network faster to train.","0c69ca3d":"**Conclusion:**\n\nThis investigation has shown that it is possible to build a model to predict if a skin defect is malignant or benign. In its current state the model has not been developed to a standard sufficient for clinical use. To further improve this network more training samples would be most benificial, while different degrees of regularization may also prove helpful.\n\nNeural Networks generally benifit from large, high quality datasets. However, there is no hard rule to estimate the number of training samples required to generate an accurate model. To classify a simple object, 1,000 training samples (or less) may sufficient. For a very complicated object, millions of training sample may be required. In the case of classifying malignant and benign skin cancer, I would suggest that atleast 10,000 training samples are needed per category.","5fb2e5fe":"Now we train the neural network.","f55552a8":"**Set-up Directories:**\n\nnow we set-up the filepaths for the training and validation data as seen below:","626ac258":"We continue to configure our model by selecting our optimizer and setting our metrics.","700c682f":"After setting up the filepaths for our datasets, we can use the following code to count the number of samples in each category.","b5dd1624":"To see the pure affects of data augmentation we keep the same structure for our neural network.","bd384b01":"**Training our Model:**\n\nNow that our model and dataset are configured we can train our model.","72870791":"# **Simple Classification Benign and Malignant Skin Cancers** #\nSkin Cancers (both melanoma and non-melanoma) are some of the most common forms of cancer in the world. In 2018, there were over 1.3 million new diagnoses recorded world-wide. The countries with the highest rate of yearly melanoma diagnoses of skin are Australia and New Zealand with over 33 diagnoses per 100,000 people (see https:\/\/www.wcrf.org\/dietandcancer\/cancer-trends\/skin-cancer-statistics).\n\nA need exists for early detection of malignant skin cancers. Early diagnoses means early intervention and decreased risk to patients during treatment. As malignant skin cancer is commonly diagnosed visually by a specialist, classifying it presents a good opportunity to demonstrate how machine learning can be benificial to medical diagnosis.","e485380f":"Save our model.","1cbfcec7":"**Discussion of Results:**\n\nIn this situation our validation accuracy seems to have approached a steadystate after 150 epochs. running this model for more than 150 epochs increases the likelyhood of overfitting our model to our training samples. At its peak, our model had a training accuracy of 89.65%. This occured at epoch 189. In this example we did not use any form of early stopping but doing so would save the model just before it starts to decline in performance.","eaf4b106":"We then configure the image data generators. We set the shuffle parameter to true to shuffle our training data. This is important to ensure the that our training data is viewed randomly during training. We set the binary parameter as we assume that all samples will be images of a potential skin cancer and will be categorized as either malignant or benign. Lastly we set the color mode. In this instance we are assuming that the colour of a mole or skin spot is a valuable feature for predicting if it is malignant or benign.","46df31be":"Save the model."}}