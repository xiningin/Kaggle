{"cell_type":{"f459f2e4":"code","1f1e5a3e":"code","522a8ddf":"code","a717c778":"code","3b618408":"code","7daefc5f":"code","e6bd98e8":"code","31a058e5":"code","b739ecf8":"code","06c29d9b":"code","34c43814":"code","58737ebb":"code","4246601b":"code","5be3a1d3":"code","c7be0c78":"code","0c4c92f9":"code","4cef1f70":"code","a4e11f08":"code","bca616b1":"code","111757a3":"code","f51a9b90":"code","1938611d":"code","8c88133d":"code","82616d76":"code","976dd1ee":"code","d8b49119":"code","80a1a002":"code","0d519fe0":"code","bb6fe35c":"code","f906356d":"code","e58cad43":"code","ef89fab2":"code","b7bfa442":"code","ca3d6b82":"code","db48790b":"code","b7cc6e00":"code","33685d49":"code","b0104252":"code","e44b8e0b":"code","ddab48ab":"code","3e51f58a":"code","2c6624f0":"code","f110c660":"code","5f830fc0":"code","1f450341":"code","75270d28":"markdown","f114fc0b":"markdown","6c6e3501":"markdown","9b80c3b5":"markdown","48bcca41":"markdown","d60f300c":"markdown","057d9cb8":"markdown","b31e8e46":"markdown","8a5be93b":"markdown","ca7d3757":"markdown","212d4aca":"markdown","e65ba800":"markdown","8d6dd195":"markdown","ac10f335":"markdown","e6b5cd5b":"markdown","d89f2227":"markdown","4ce1c4a2":"markdown","9cbe0a05":"markdown","bf3d9037":"markdown","be7ce099":"markdown","479b3c59":"markdown","eee56ca2":"markdown","da0824b4":"markdown","48104132":"markdown","83d11b87":"markdown","abe4c9ce":"markdown","e2eb03b5":"markdown","b0af539a":"markdown","82c9e71b":"markdown","005675d4":"markdown","0aa2faa9":"markdown","dce7ccb1":"markdown"},"source":{"f459f2e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f1e5a3e":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline","522a8ddf":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","a717c778":"df.head()","3b618408":"df.info()","7daefc5f":"df.isnull().sum()","e6bd98e8":"df.describe()","31a058e5":"df.drop(\"Unnamed: 32\", axis=1, inplace=True)","b739ecf8":"sns.set_style('whitegrid')\nplt.figure(figsize = (12,6))\nsns.countplot(x=\"diagnosis\", data=df, palette='magma')","06c29d9b":"plt.figure(figsize=(20,17))\nmatrix = np.triu(df.corr())\nsns.heatmap(df.corr(), annot=True, linewidth=.5, mask=matrix, cmap=\"Purples\")","34c43814":"fig, ax = plt.subplots(2,2, figsize=(15,15))\nsns.scatterplot(x='fractal_dimension_mean', y='area_mean', hue=\"diagnosis\", \n                data=df, ax=ax[0][0], palette='magma')\nsns.scatterplot(x='fractal_dimension_worst', y='area_worst', hue=\"diagnosis\", \n                data=df, ax=ax[0][1], palette='magma')\nsns.scatterplot(x='smoothness_se', y='radius_worst', hue=\"diagnosis\", \n                data=df, ax=ax[1][0], palette='magma')\nsns.scatterplot(x='symmetry_se', y='radius_worst', hue=\"diagnosis\", \n                data=df, ax=ax[1][1], palette='magma')","58737ebb":"# Creating a list of columns with only the columns that represent the mean.\nmean_cols = ['diagnosis','radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n\n# Creating a list of columns with only the columns that represent the worst values.\nworst_cols = ['diagnosis','radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","4246601b":"sns.pairplot(df[mean_cols], hue=\"diagnosis\", palette='magma')","5be3a1d3":"sns.pairplot(df[worst_cols], hue=\"diagnosis\", palette='viridis')","c7be0c78":"df['diagnosis']","0c4c92f9":"tgt = df['diagnosis']\nfrom sklearn.preprocessing import LabelEncoder\nencode_lbl = LabelEncoder()\ntarget = encode_lbl.fit_transform(tgt)","4cef1f70":"from sklearn.model_selection import train_test_split\nX = df.drop('diagnosis', axis=1)\ny = target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n\nprint(\"Shape of training set:\", X_train.shape)\nprint(\"Shape of test set:\", X_test.shape)","a4e11f08":"from sklearn.preprocessing import StandardScaler\ns_sc = StandardScaler()\nX_train = s_sc.fit_transform(X_train)\nX_test = s_sc.fit_transform(X_test)","bca616b1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score","111757a3":"logmodel = LogisticRegression()","f51a9b90":"logmodel.fit(X_train, y_train)\npredictions1 = logmodel.predict(X_test)","1938611d":"print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions1))\nprint('\\n')\nprint(classification_report(y_test, predictions1))","8c88133d":"logmodel_acc = accuracy_score(y_test, predictions1)\nprint(\"Accuracy of the Logistic Regression Model is: \", logmodel_acc)","82616d76":"from sklearn.neighbors import KNeighborsClassifier","976dd1ee":"knn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\npredictions2 = knn.predict(X_test)","d8b49119":"print(confusion_matrix(y_test, predictions2))\nprint(\"\\n\")\nprint(classification_report(y_test, predictions2))","80a1a002":"error_rate = []\n\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","0d519fe0":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40), error_rate, color='purple', linestyle=\"--\",marker='o', markersize=10, markerfacecolor='red')\nplt.title('Error_Rate vs K value')\nplt.xlabel = ('K')\nplt.ylabel = ('Error Rate')","bb6fe35c":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\npredictions2 = knn.predict(X_test)","f906356d":"print(confusion_matrix(y_test, predictions2))\nprint(\"\\n\")\nprint(classification_report(y_test, predictions2))","e58cad43":"knn_model_acc = accuracy_score(y_test, predictions2)\nprint(\"Accuracy of K Neighbors Classifier Model is: \", knn_model_acc)","ef89fab2":"from sklearn.tree import DecisionTreeClassifier","b7bfa442":"dtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\npredictions3 = dtree.predict(X_test)","ca3d6b82":"print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions3))\nprint(\"\\n\")\nprint(classification_report(y_test, predictions3))","db48790b":"dtree_acc = accuracy_score(y_test, predictions3)\nprint(\"Accuracy of Decision Tree Model is: \", dtree_acc)","b7cc6e00":"from sklearn.ensemble import RandomForestClassifier","33685d49":"rfc = RandomForestClassifier(n_estimators=300)\nrfc.fit(X_train, y_train)\npredictions4 = rfc.predict(X_test)","b0104252":"print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions4))\nprint(\"\\n\")\nprint(classification_report(y_test, predictions4))","e44b8e0b":"rfc_acc = accuracy_score(y_test, predictions4)\nprint(\"Accuracy of Random Forests Model is: \", rfc_acc)","ddab48ab":"from sklearn.svm import SVC","3e51f58a":"svc_model = SVC(kernel=\"rbf\")\nsvc_model.fit(X_train, y_train)\npredictions5 = svc_model.predict(X_test)","2c6624f0":"print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predictions5))\nprint(\"\\n\")\nprint(classification_report(y_test, predictions5))","f110c660":"svm_acc = accuracy_score(y_test, predictions5)\nprint(\"Accuracy of SVM model is: \", svm_acc)","5f830fc0":"print(logmodel_acc)\nprint(knn_model_acc)\nprint(dtree_acc)\nprint(rfc_acc)\nprint(svm_acc)","1f450341":"plt.figure(figsize=(12,6))\nmodel_acc = [logmodel_acc, knn_model_acc, dtree_acc, rfc_acc, svm_acc]\nname_of_model = ['LogisticRegression', 'KNN', 'DecisionTree', 'RandomForests', 'SVM']\nsns.barplot(x= model_acc, y=name_of_model, palette='magma')","75270d28":"<p style=\"font-size:18px; color: #185adb; text-align:center;\">Please do leave your valuable feedbacks in the comments and any improvements or suggestions are welcomed!<\/p>\n<h1 style=\"color: #f55c47; text-align:center;\">The End<\/h1>","f114fc0b":"<a id=\"section3\"><\/a>\n<h1 style=\"font-weight:bold; color:#022e57;\">3. Results<\/h1>","6c6e3501":"<h3 style=\"font-weight:bold; color:#005a8d;\">2.3.2 K Nearest Neighbours<\/h3>","9b80c3b5":"<h3 style=\"color: #2978b5; text-align:center;\">LOGISTIC REGRESSION MODEL PERFORMED THE BEST WITH AN ACCURACY OF 98.24%<\/h3>\n<h3 style=\"color: #2978b5; text-align:center;\">SVM IS JUST BEHIND, ALSO WITH A GOOD ACCURACY OF 97.66%<\/h3>","48bcca41":"<h3 style=\"font-weight:bold; color:#005a8d;\">2.3.4 Random Forests<\/h3>","d60f300c":"<p style=\"font-size: 16px\">Breast cancer is cancer that forms in the cells of the breasts. Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, fluid coming from the nipple, a newly inverted nipple, or a red or scaly patch of skin. <br><br>Most types of breast cancer are easy to diagnose by microscopic analysis of a sample - or biopsy - of the affected area of the breast. Also, there are types of breast cancer that require specialized lab exams.<br><br>The uncontrolled cancer cells often invade other healthy breast tissue and can travel to the lymph nodes under the arms. The lymph nodes are a primary pathway that help the cancer cells move to other parts of the body. <\/p>","057d9cb8":"'target' is our new numerical target column for our modelling.","b31e8e46":"<h3 style=\"font-weight:bold; color:#005a8d;\">2.3.1 Logistic Regression<\/h3>","8a5be93b":"<h2 style=\"font-weight:bold; color:#022e57;\">Content<\/h2>\n\n1. [Exploratory Data Analysis](#section1)\n2. [Data Preprocessing and Building Models](#section2)\n2. [Results](#section3)\n","ca7d3757":"<p style=\"font-size:18px; color: #fb3640; font-weight: 500;\">The accuracy of Logistic Regression Model is 98.245%<br>The accuracy of KNN model is 95.321%<br>The accuracy of Decision Tree Model is 90.643%<br>The accuracy of Random Forest Model is 94.736%<br>The accuracy of SVM Model is 97.660%<\/p>","212d4aca":"<h3 style=\"font-weight:bold; color:#005a8d;\">2.3.5 Support Vector Machines (SVM)<\/h3>","e65ba800":"<p style=\"font-size: 16px;\">The column <b>'fractal_dimension_mean'<\/b> had many negative correlations with many other attributes like <b>'area_mean'<\/b>, <b>'area_worst'<\/b> etc. We'll plot some scatter plots for these.<br><br> For your information Fractal analysis of images of breast tissue specimens provides a numeric description of tumour growth patterns as a continuous number between 1 and 2. This number is known as the Fractal Dimension","8d6dd195":"<h3 style=\"font-weight:bold; color:#005a8d;\">Some Pairplots<\/h3>","ac10f335":"<h3 style=\"font-weight:bold; color:#005a8d;\">Negative Correlations<\/h3>","e6b5cd5b":"<h3 style=\"font-weight:bold; color:#005a8d;\">2.3.3 Decision Tree<\/h3>","d89f2227":"As we can observe from the heatmaps that there are many negative correlations in this dataset. Lets observe these by plotting it out.","4ce1c4a2":"<a id=\"section1\"><\/a>\n<h1 style=\"font-weight:bold; color:#022e57;\">1. Exploratory Data Analysis<\/h1>","9cbe0a05":"<h2 style=\"font-weight:bold; color:#022e57;\">Description of Attributes<\/h2>","bf3d9037":"<h2 style=\"font-weight:bold; color:#005a8d;\">2.3 Classification Models<\/h2>","be7ce099":"<h2 style=\"font-weight:bold; color:#022e57;\">Introduction<\/h2>","479b3c59":"We need to convert this categorical column into a numerical one using Label Encoder","eee56ca2":"From this graph, K value of 3 and 7 seem to show the lowest mean error. So I'll use one of these values and check.","da0824b4":"<h2 style=\"font-weight:bold; color:#005a8d;\">1.1 Data Visualizations<\/h2>","48104132":"So we can observe from the Classification report that we have an accuracy of around 0.95. I'll try to increase the accuracy a bit more by using a better value for n_neighbors or K value.","83d11b87":"<h2 style=\"font-weight:bold; color:#005a8d;\">2.2 Splitting the Data into train and test<\/h2>","abe4c9ce":"1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 \/ area - 1.0)\n\ng) concavity (severity of concave portions of the contour)\n\nh) concave points (number of concave portions of the contour)\n\ni) symmetry\n\nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.","e2eb03b5":"Dropping 'Unnamed: 32' column.","b0af539a":"<p style=\"text-align:center; color: #f21170; font-size: 25px;\"><b>Breast Cancer Data Analysis and Predictions<\/b><\/p>","82c9e71b":"<h2 style=\"font-weight:bold; color:#005a8d;\">2.1 Data Preprocessing<\/h2>","005675d4":"<a id=\"section2\"><\/a>\n<h1 style=\"font-weight:bold; color:#022e57;\">2. Data Preprocessing and Building Models<\/h1>","0aa2faa9":"<p style=\"font-size: 16px;\">It was already given but we still checked for NaN values and found that the whole column'Unamed: 32' had NaN values. So I will drop this column.<\/p>","dce7ccb1":"So, there were not any significant changes in the accuracy score other than the 0.1 increase in macro avg. So for now I'll use this as my accuracy score for KNN"}}