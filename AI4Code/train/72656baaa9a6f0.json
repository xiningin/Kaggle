{"cell_type":{"4ccba119":"code","0ad850b1":"code","0d503173":"code","4eafe6ba":"code","03742937":"code","adf43ef0":"code","6e69a29d":"code","4d8d27c6":"code","3f882619":"markdown","10f1340d":"markdown","5b41c766":"markdown","17768d32":"markdown","259c2f4d":"markdown"},"source":{"4ccba119":"import tensorflow as tf\nimport tensorflow.keras as keras","0ad850b1":"sentences = [\"Hello, my name is Johnny.\",\n             \"Learning deep learning is a trend.\",\n             \"Deep learning is a key to the era of AI.\"]","0d503173":"# create tokenizer object\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=100,\n                                   oov_token=\"<OOV Token>\")","4eafe6ba":"# fit tokenizer on text\ntokenizer.fit_on_texts(sentences)","03742937":"# show the index of words which are tokenize\ntokenizer.word_index","adf43ef0":"sequences = tokenizer.texts_to_sequences(sentences)\nprint(sequences)","6e69a29d":"padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences=sequences,\n                                                              padding=\"post\")","4d8d27c6":"padded_sequences","3f882619":"## Padding Sequences Make Same Length","10f1340d":"## A List of Sentences","5b41c766":"## Import Package","17768d32":"## Create a Tokenizer","259c2f4d":"## Convert List of Sentences to Sequences"}}