{"cell_type":{"6199bace":"code","4b4827e3":"code","1edf5168":"code","e5d09517":"code","e65d1119":"code","b87c6ea6":"code","468a6598":"code","077b8751":"code","ae1b0d8c":"code","1114ca76":"markdown","724e1c64":"markdown","3d980443":"markdown","cffc9c34":"markdown"},"source":{"6199bace":"!pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","4b4827e3":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow_hub as hub\nfrom keras.models import Model\nimport tensorflow as tf\nimport re\nfrom keras.engine.topology import Layer\nfrom sklearn.model_selection import train_test_split\nimport keras.backend as K\nfrom keras.layers import Dense , Input, PReLU , Dropout\nfrom keras_contrib.callbacks import CyclicLR\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","1edf5168":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\ntrain_data['title'] = train_data['title'].apply(lambda s : re.sub(r'[^\\w\\s]','',s))\ntest_data['title'] = test_data['title'].apply(lambda s : re.sub(r'[^\\w\\s]','',s))\n\ntrain_text = train_data['title'].tolist()\ntrain_text = [' '.join(t.split()) for t in train_text]\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\n\ntrain_labels = train_data['Category'].values\n\nn_classes = len(train_data['Category'].unique())\n\nX_train , X_val, y_train  , y_val = train_test_split(train_text , \n                                                     train_labels , \n                                                     stratify = train_labels , \n                                                     train_size = 0.8,\n                                                     random_state = 100)\n\ntest_text = test_data['title'].tolist()\ntest_text = [' '.join(t.split()) for t in test_text]\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]\n\n","e5d09517":"#Reference https:\/\/towardsdatascience.com\/elmo-embeddings-in-keras-with-tensorflow-hub-7eb6f0145440\nclass ElmoEmbeddingLayer(Layer):\n    def __init__(self, trainable = True ,**kwargs):\n        self.dimensions = 1024\n        self.trainable = trainable\n        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.elmo = hub.Module('https:\/\/tfhub.dev\/google\/elmo\/2', trainable=self.trainable, name=\"{}_module\".format(self.name))\n        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module\/.*\".format(self.name))\n        super(ElmoEmbeddingLayer, self).build(input_shape)\n        \n    def call(self, x, mask=None):\n        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n                          as_dict=True,\n                          signature='default',\n                          )['default']\n        return result\n    \n    def compute_mask(self, inputs, mask=None):\n        return K.not_equal(inputs, '--PAD--')\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.dimensions)","e65d1119":"def gen_model(n_classes = 2) :\n    inp = Input(shape = (1,) , name = 'input' , dtype = tf.string)\n    embedding = ElmoEmbeddingLayer()(inp)\n    dense = Dense(512 )(embedding)\n    dense = PReLU()(dense)\n    dense = Dropout(0.3)(dense)\n    dense = Dense(256)(dense)\n    dense = PReLU()(dense)\n    dense = Dropout(0.3)(dense)\n    if n_classes > 1 :\n        pred = Dense(n_classes, activation='softmax')(dense)\n    else :\n        pred = Dense(n_classes, activation='sigmoid')(dense)\n    model = Model(inputs=inp, outputs=pred)\n    model.compile(loss = 'sparse_categorical_crossentropy' , metrics = ['accuracy'] , optimizer = 'nadam')\n    return model","b87c6ea6":"K.clear_session()\nmodel = gen_model(n_classes)\n#model = gen_model(58)\nmodel.summary()\n\nbatch_size = 512\nepochs = 20\nstep_size = int(int(len(X_train)\/batch_size)*epochs\/2)\n\ncb = [\n    CyclicLR(5e-4 , 2e-3 , step_size)\n]","468a6598":"model.fit(X_train, \n          y_train,\n          validation_data = (X_val, y_val),\n          epochs = epochs,\n          batch_size= batch_size,\n          verbose = 1,\n          shuffle = True,\n          callbacks = cb)","077b8751":"preds = model.predict(test_text,verbose = 1).argmax(axis = 1)","ae1b0d8c":"sub_df = test_data[['itemid']].copy()\nsub_df['Category'] = preds\nsub_df.to_csv('submission.csv' , index = False)\nsub_df.head()","1114ca76":"<h1><b>Trying out ELMO Embeddings in Keras<\/b> <\/h1>\n<hr><br>\nThis is a very simple code for those of you who are interested on trying out the state-of-the-art embeddings ELMO. The embeddings itself can be get from tensorflow hub but can be further trained with your data as shown on this kernel.  <br><br>\nThe result shown in this kernel might not produce a state-of-the-art result since I did not do any sophisticated methods in this kernel, but hopefully might help in the future for those who are interested in working with ELMO embeddings.<br>\n<br>\n[Reference code](https:\/\/towardsdatascience.com\/elmo-embeddings-in-keras-with-tensorflow-hub-7eb6f0145440) <br>\n[Cyclical LR](https:\/\/www.datacamp.com\/community\/tutorials\/cyclical-learning-neural-nets)","724e1c64":"<h2><b>Basic data I\/O and preprocessing<\/b><\/h2>\n<hr>\nWe simply remove punctuations since it is very unlikely for it to be useful for our model. Contrary to other embeddings like FastText and Glove, we do not need to actually to split our sentences to words. The implemented model from tensorflow hub is kind enough to actually do this for us.","3d980443":"<h2><b>Generate Model<\/b><\/h2>\n<hr>\nTo finalize our model we simply stacked in two more dense layer before our final prediction. The model will be trained with Nadam (adam with nesterov) optimizer and with Triangular Cyclical Learning Rate for 20 epochs.","cffc9c34":"<h2><b>ELMO in Keras<\/b><\/h2>\n<hr>\nSince ELMO embeddings are consisted layers and are not native to Keras hence we cannot simply stack it with our model. There are two options you can do, using Lambda layer or making a custom layer to wrap around the embedding but in this kernel I will show you the latter. The downloaded weights are actually already trained on a much bigger dataset but we will allow it to be trained with our data."}}