{"cell_type":{"2e987655":"code","9b19d708":"code","f7fc19c0":"code","781a25af":"code","d6b4bf35":"code","00c917b2":"code","d0b24565":"code","08ba5eb9":"code","aa6ee648":"code","cb15f948":"code","53cfc8e7":"code","e6c00d45":"code","b7b77015":"code","a93dc02b":"code","1d7d2bef":"code","ebde171e":"code","3d4a3dd4":"code","e9e42641":"code","4de197bb":"code","0d246d7b":"code","4f0f6534":"code","c86b75ba":"code","06d43383":"code","3653b5b0":"code","1ae2491b":"code","894d2d96":"code","32c744bc":"code","bef2fe8a":"code","13d9ccf2":"code","35dcfab5":"code","05490e7b":"code","af8dcc6f":"code","0c95e216":"markdown","a1d9ff43":"markdown","7671cc7c":"markdown","c44f414c":"markdown","a39397c0":"markdown","b604c277":"markdown","fdde6ea3":"markdown","72d50705":"markdown","7309ab76":"markdown","59fbea0a":"markdown","289afb29":"markdown","d7048102":"markdown","ce87ec26":"markdown","6faad4be":"markdown","43406e77":"markdown","03abbe02":"markdown","50bec015":"markdown","f1a4cfaf":"markdown","c8adf976":"markdown","08f1cc45":"markdown","0e2f1659":"markdown","867e47d1":"markdown","6f190ca3":"markdown","02f8edef":"markdown","c4128a41":"markdown","f6114e2d":"markdown","0bd05cd7":"markdown","9247eaa0":"markdown","4d8508f0":"markdown","ecd1afce":"markdown","10b38ad7":"markdown","d1d26ab7":"markdown","bf80f6a5":"markdown","22c49659":"markdown","f2106110":"markdown"},"source":{"2e987655":"%%capture\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\n!pip install -q tensorflow-gpu==2.0.0-beta1\n!pip install -q tensorflow_hub","9b19d708":"import matplotlib.pylab as plt\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers","f7fc19c0":"classifier_url =\"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/classification\/2\" #@param {type:\"string\"}","781a25af":"IMAGE_SHAPE = (224, 224)\n\nclassifier = tf.keras.Sequential([\n    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(3,))\n])","d6b4bf35":"import numpy as np\nimport PIL.Image as Image\n\ngrace_hopper = tf.keras.utils.get_file('image.jpg','https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/grace_hopper.jpg')\ngrace_hopper = Image.open(grace_hopper).resize(IMAGE_SHAPE)\ngrace_hopper","00c917b2":"grace_hopper = np.array(grace_hopper)\/255\ngrace_hopper.shape","d0b24565":"results = classifier.predict(grace_hopper[np.newaxis, ...])\nresults","08ba5eb9":"predicted_class = np.argmax(results[0], axis=-1)\npredicted_class","aa6ee648":"labels_path = tf.keras.utils.get_file('ImageNetLabels.txt', 'https:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ImageNetLabels.txt')\nimagenet_labels = np.array(open(labels_path).read().splitlines())\nimagenet_labels","cb15f948":"plt.imshow(grace_hopper)\nplt.axis('off')\npredicted_class_name = imagenet_labels[predicted_class]\n_ = plt.title('Predition: '+ predicted_class_name, color='cyan')","53cfc8e7":"data_root = tf.keras.utils.get_file('flower_photos','https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz',\n   untar=True)","e6c00d45":"image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255)\nimage_data = image_generator.flow_from_directory(str(data_root), target_size=IMAGE_SHAPE)","b7b77015":"for image_batch, label_batch in image_data:\n  print(\"Image batch shape: \", image_batch.shape)\n  print(\"Label batch shape: \", label_batch.shape)\n  break","a93dc02b":"results_batch = classifier.predict(image_batch)\nresults_batch.shape","1d7d2bef":"predicted_class_names = imagenet_labels[np.argmax(results_batch, axis=-1)]\npredicted_class_names","ebde171e":"plt.figure(figsize = (10,9))\nplt.subplots_adjust(hspace=0.5)\n\nfor n in range(30):\n  plt.subplot(6, 5, n+1)\n  plt.imshow(image_batch[n])\n  plt.title(predicted_class_names[n], color='cyan')\n  plt.axis('off')\n_ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\", color='cyan')","3d4a3dd4":"feature_extractor_url = \"https:\/\/tfhub.dev\/google\/tf2-preview\/mobilenet_v2\/feature_vector\/2\" #@param {type:\"string\"}","e9e42641":"feature_extractor_layer = hub.KerasLayer(feature_extractor_url, input_shape=IMAGE_SHAPE+(3,))","4de197bb":"feature_batch = feature_extractor_layer(image_batch)\nprint(feature_batch.shape)","0d246d7b":"feature_extractor_layer.trainable = False","4f0f6534":"model = tf.keras.Sequential([\n    feature_extractor_layer,\n    layers.Dense(image_data.num_classes, activation='softmax')\n])\n\nmodel.summary()","c86b75ba":"model.compile(optimizer = tf.keras.optimizers.Adam(),\n             loss = 'categorical_crossentropy',\n             metrics = ['accuracy'])","06d43383":"class ConnectBatchStats(tf.keras.callbacks.Callback):\n  def __init__(self):\n    self.batch_losses = []\n    self.batch_acc = []\n    \n  def on_train_batch_end(self, batch, logs=False):\n    self.batch_losses.append(logs['loss'])\n    self.batch_acc.append(logs['accuracy'])\n    self.model.reset_metrics()","3653b5b0":"steps_per_epoch = np.ceil(image_data.samples\/image_data.batch_size)\n\nbatch_stats_callback = ConnectBatchStats()\n\nhistory = model.fit(image_data,\n                   epochs = 15,\n                   steps_per_epoch = steps_per_epoch,\n                   callbacks = [batch_stats_callback])","1ae2491b":"plt.figure()\nplt.xlabel('Training Steps')\nplt.ylabel('Loss')\nplt.ylim([0, 1])\nplt.plot(batch_stats_callback.batch_losses, color='yellow')","894d2d96":"plt.figure()\nplt.xlabel('Training Steps')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.plot(batch_stats_callback.batch_acc, color='yellow')","32c744bc":"class_names = sorted(image_data.class_indices.items(), key = lambda pair : pair[1])\nclass_names = np.array([key.title() for key, value in class_names])\nclass_names","bef2fe8a":"predicted_batch = model.predict(image_batch)\npredicted_id = np.argmax(predicted_batch, axis=-1)\npredicted_label_batch = class_names[predicted_id]","13d9ccf2":"label_id = np.argmax(label_batch, axis=-1)","35dcfab5":"plt.figure(figsize=(10,9))\nplt.subplots_adjust(hspace=0.5)\n\nfor n in range(30):\n  plt.subplot(6, 5, n+1)\n  plt.imshow(image_batch[n])\n  color = 'green' if predicted_id[n] == label_id[n] else 'red'\n  plt.title(predicted_label_batch[n], color = color)\n  plt.axis('off')\n\n_=plt.suptitle('Model predictions (green: correct, red: incorrect)', color='cyan')","05490e7b":"import time\nt = time.time()\n\nexport_path = \"\/tmp\/saved_models\/{}\".format(int(t))\ntf.keras.experimental.export_saved_model(model, export_path)\n\nexport_path","af8dcc6f":"reloaded = tf.keras.experimental.load_from_saved_model(export_path, custom_objects={'KerasLayer':hub.KerasLayer})\n\nresult_batch = model.predict(image_batch)\nreloaded_result_batch = reloaded.predict(image_batch)\n\nabs(reloaded_result_batch - result_batch).max()","0c95e216":"## Downloading the ImageNet classifier\n\nWe can use hub.module to load any [TensorFlow 2 compatible image classifier URL](https:\/\/https:\/\/tfhub.dev\/s?module-type=image-classification&q=tf2) from tfhub.dev and wrap it in a keras layer using hub.KerasLayer()\n\nWe also need to specify the input shape for the model\n\nPS: The model we are using here is a \"mobilenet_v2\" pretrained in the ImageNet dataset","a1d9ff43":"## Predict\n\nNow we make predictions and convert the indices to class names","7671cc7c":"Now we train the model with .fit\n\nWe can also set up a custom callback to store the loss and accuracy of each batch, instead of the epoch average and later use this to visualize our model's performance.","c44f414c":"## Importing the required libraries","a39397c0":"## Check the predictions\n\nWe will redo the plot from before but this will mark wrongly labeled images as red and correctly labeled ones as green. We need the ordered class names for that, so let's extract those.","b604c277":"Create the feature extractor","fdde6ea3":"The resulting is an iterator that returns image_batch and label_batch pairs","72d50705":"We also extract the label ids of the flowers from the batch","7309ab76":"The results are clearly way off considering the model was not trained on these labels (except daisy)","59fbea0a":"## Train the model\n\nCompile the model by setting the type of optimizer and loss functions and the metrics you would like to evaluate. Here we are using Adam optimizer, categorical cross-entropy as loss function and accuracy as metrics.","289afb29":"## Plot the images","d7048102":"### Plotting the loss","ce87ec26":"Now we can confirm that if we load the model it'll give the same result","6faad4be":"## Export Model\n\nNow that we have seen our model working nearly perfectly we could export it as a saved model","43406e77":"Normalize the image","03abbe02":"One simple way to load image data into our model is by using [tf.keras.preprocessing.image.ImageDataGenerator](https:\/\/www.tensorflow.org\/versions\/r2.0\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator)\n\nAll of TensorFlow Hub's image modules expect float inputs in the [0, 1] range. We will use ImageDataGenerator's rescale parameter to achieve this.","50bec015":"## Simple Transfer Learning\n\nNow that you know how to use tensorflow hub to load a model and make predictions, let's try to do some transfer learning with this.\n\nUsing tf hub it's easy to retrain the top layer (the one classifying the images into 1000 imagenet labels) of the model to recognize the classes from our dataset","f1a4cfaf":"## Attach our own layers\n\nWe wrap the feature extractor layer in tf.keras.Sequential and add our own layers on top of it.\n\nIn this example, we will only add a new softmax classification layer","c8adf976":"Now we add a new dimension\/axis to the image for the batch no and pass the image to our model","08f1cc45":"## Downloading a headless model\n\nTensorflow Hub lets you download models without the top classification layer which can then be used for transfer learning\n\nAny [Tensorflow 2 compatible image feature vector URL](https:\/\/tfhub.dev\/s?module-type=image-feature-vector&q=tf2) from tfhub.dev will work here.","0e2f1659":"This returns a 1280 length vector for each image. This is effectively the representation of the image.\n\n\nNow we want would like to keep the weights up to the feature extractor layer and only train our classification layer that we will add on top of this. This saves big time on the computation as we dont have to train the network again only the layers we have added to it and still take the benefits of the pretrained layers.","867e47d1":"Let's plot the image with the predicted label","6f190ca3":"### Plotting the accuracy","02f8edef":"**Now** let's plot the predicted labels with the images of the batch","c4128a41":"Now we have got one batch of size 32 of the flower image dataset and 32 of its labels","f6114e2d":"## Plotting the model's loss and accuracy\n\nWe will use matplotlib to plot the accuracy and loss that we have stored during the training process through the callback function.","0bd05cd7":"## Trying on a single image\n\nLet's first try the model on a single image","9247eaa0":"# Transfer Learning\n\n[If you already know what transfertransfer learning is, skip to the next section where we implement transfer learning with Tensorflow Hub]\n\nTransfer Learning in simple terms in basically **reusing** a model developed for one task as a starting point for another model\n\n## Why would you want to use it?\n\nSometimes you would not want to train a huge model if you can get it pretrained on big datasets like imagenet. Another use case may be when you have a similar task at hand and you would like to use the pretrained model as a starting point and use your own layers on top of it including your own classification layer like a softmax layer with your own labels.\n\nThe lower level layers of a model trained on big datasets like ImageNet get pretty good at learning low level image features like shapes and figures and you may want to leverage those in your own image recognizer without making your model learn those representations all over again\n\n## Learning resources\n\n- This video by my favourite ML teacher Andrew Ng should give you enough basic understanding about Transfer Learning to get familier with the problem and understand the objective of the following code implementation.\nhttps:\/\/www.youtube.com\/watch?v=yofjFQddwHE [**Recommended**]\n- This article expands more on the topic of Transfer Learning https:\/\/machinelearningmastery.com\/transfer-learning-for-deep-learning\/\n","4d8508f0":"## Decoding the predictions\n\nNow that we have predicted the class ID with the highest probability we need to get the image net labels and decode the image net predictions\n\nYou can get the labels from the following link or if you prefer you can use any other source\n\nImageNet labels:\nhttps:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ImageNetLabels.txt","ecd1afce":"## Loading the Dataset\n\nWe will be using the tensorflow flower dataset in this example","10b38ad7":"I hope you have understood Transfer Learning in tensorflow with tf hub. This code is from the [tensorflow docs](https:\/\/www.tensorflow.org\/beta\/tutorials\/images\/hub_with_keras). I have just given my own explationation in parts where I had problems understanding the 1st time in the hope if someone reads the code of the docs and doesn't understand they have some source to refer to.","d1d26ab7":"We sort the class_indices by their class values (0,1,2,3,4) and then extract the actual names of the flowers","bf80f6a5":"## Run the classifier on the batch of images","22c49659":"# Transfer Learning with Tensorflow Hub\n[This notebook is made from the tensorflow docs with a little of my explaination]\n\n## Installing required libraries\n\nWe have installed tensorflow-gpu and tensorflow_hub","f2106110":"The result is a 1001 vector of logits rating the probability of each class for the image\n\nThe top class ID can be found by argmax"}}