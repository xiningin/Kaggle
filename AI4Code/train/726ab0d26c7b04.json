{"cell_type":{"44c683cf":"code","e1412bd9":"code","36afc841":"code","4f6010c0":"code","bbe4d69e":"code","721e64f6":"code","d4cc5bb5":"code","b5a4a106":"code","4c23798e":"code","d5a66e66":"code","f4377b6b":"code","2e4e0b93":"code","78d5b552":"code","4d21c923":"code","407b0b0f":"code","4986a557":"markdown","2aea97b4":"markdown","663f701f":"markdown","1e07f036":"markdown","20a6cf66":"markdown","a11c81db":"markdown","ed221232":"markdown","8fec8230":"markdown","bc926b99":"markdown","fee6dac8":"markdown","efbfc8ba":"markdown","b2040831":"markdown","36381f09":"markdown","322a1ab7":"markdown"},"source":{"44c683cf":"import pandas as pd","e1412bd9":"df = pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndf","36afc841":"df = df.drop(\"Serial No.\",axis=1)\ndf","4f6010c0":"import seaborn as sns \nimport matplotlib.pyplot as plt\n_,fig = plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","bbe4d69e":"# taking all the coloumns expect the last one i.e 'chance_of_admission'\nx = df.iloc[:,:-1]\nx","721e64f6":"# taking our target coloumn which chance_of_admit\ny = df.iloc[:,-1]\ny","d4cc5bb5":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x, y, test_size = 0.2,random_state = 42)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","b5a4a106":"from sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\nfrom sklearn.svm import SVR,SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error","4c23798e":"import numpy as np\nregressors=[['Linear Regression :',LinearRegression()],\n       ['Decision Tree Regression :',DecisionTreeRegressor()],\n       ['Random Forest Regression :',RandomForestRegressor()],\n       ['Gradient Boosting Regression :', GradientBoostingRegressor()],\n       ['Ada Boosting Regression :',AdaBoostRegressor()],\n       ['Extra Tree Regression :', ExtraTreesRegressor()],\n       ['K-Neighbors Regression :',KNeighborsRegressor()],\n       ['Support Vector Regression :',SVR()]]\nreg_pred=[]\nprint('Results time for regression ...\\n')\nprint(\"Excited !!!!!! :)\")","d5a66e66":"for name,model in regressors:\n    model=model\n    model.fit(X_train,y_train)\n    predictions = model.predict(X_test)\n    rms=np.sqrt(mean_squared_error(y_test, predictions))\n    reg_pred.append(rms)\n    print(name,rms)","f4377b6b":"y_ax=['Linear Regression' ,'Decision Tree Regression', 'Random Forest Regression','Gradient Boosting Regression', 'Ada Boosting Regression','Extra Tree Regression' ,'K-Neighbors Regression', 'Support Vector Regression' ]\nx_ax=reg_pred\nsns.barplot(x=x_ax,y=y_ax,linewidth=1.5,edgecolor=\"0.1\")","2e4e0b93":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=101)\n#If Chance of Admit greater than 80% we classify it as 1\ny_train_c = [1 if each > 0.8 else 0 for each in y_train]\ny_test_c  = [1 if each > 0.8 else 0 for each in y_test]","78d5b552":"classifiers=[['Logistic Regression :',LogisticRegression()],\n       ['Decision Tree Classification :',DecisionTreeClassifier()],\n       ['Random Forest Classification :',RandomForestClassifier()],\n       ['Gradient Boosting Classification :', GradientBoostingClassifier()],\n       ['Ada Boosting Classification :',AdaBoostClassifier()],\n       ['Extra Tree Classification :', ExtraTreesClassifier()],\n       ['K-Neighbors Classification :',KNeighborsClassifier()],\n       ['Support Vector Classification :',SVC()],\n       ['Gausian Naive Bayes :',GaussianNB()]]\n","4d21c923":"cla_pred=[]\nfor name,model in classifiers:\n    model=model\n    model.fit(X_train,y_train_c)\n    predictions = model.predict(X_test)\n    cla_pred.append(accuracy_score(y_test_c,predictions))\n    print(name,accuracy_score(y_test_c,predictions))","407b0b0f":"y_ax=['Logistic Regression' ,\n      'Decision Tree Classifier',\n      'Random Forest Classifier',\n      'Gradient Boosting Classifier',\n      'Ada Boosting Classifier',\n      'Extra Tree Classifier' ,\n      'K-Neighbors Classifier',\n      'Support Vector Classifier',\n       'Gaussian Naive Bayes']\nx_ax=cla_pred\nsns.barplot(x=x_ax,y=y_ax,linewidth=1.5,edgecolor=\"0.8\")\nplt.xlabel('Accuracy')\n","4986a557":"![image.png](attachment:image.png)","2aea97b4":"## Ways to Evaluate the best model in regression\n\nThere are 3 main metrics for model evaluation in regression:\n\n1. R Square\/Adjusted R Square\n2. Mean Square Error(MSE)\/Root Mean Square Error(RMSE)\n3. Mean Absolute Error(MAE)\n\nR Square\/Adjusted R Square are better used to explain the model to other people because you can explain the number as a percentage of the output variability. MSE, RMSE or MAE are better to be used to compare performance between different regression models. Personally, I would prefer using RMSE and I think Kaggle also uses it to assess submission. However, it makes total sense to use MSE if value is not too big and MAE if you do not want to penalize large prediction error.Adjusted R square is the only metric here that considers overfitting problem.\n\nI have used here MSE to compare all the models we have here :)","663f701f":"# Ways to Evaluate best model in classification\n\nWell actually we have a very simply way to just look at the accuracy_score which tells us how model has performed and thus give best model.","1e07f036":"# Classification","20a6cf66":"Hope you like my kernel, and if yes don't forgo to **UPVOTE** and till next meet enjoy and have fun.\n\nHappy learning :)","a11c81db":"# Introduction\n\nThe dataset contains several parameters which are considered important during the application for Masters Programs.The parameters included are :\n\n* GRE Scores (out of 340 )\n* TOEFL Scores (out of 120 )\n* University Rating ( out of 5 )\n* Statement of Purpose(out of 5) :\nA statement of purpose (SOP), in the context of applying for graduate school, is an essay that's one of the most important aspects of your application because it tells the admission committee who you are, why you're applying, why you're a good candidate, and what you want to do in the future.\n\n* Letter of Recommendation Strength ( out of 5 ):\nA letter of recommendation is a personalized letter from one of your current or former teachers. It is given to the admissions boards of the colleges you're applying to. A good letter of recommendation makes a strong case for why you'd be an excellent applicant\n\n* Undergraduate GPA ( out of 10 )\n\n* Research Experience ( either 0 or 1 )\n\n* Chance of Admit ( ranging from 0 to 1 )\n\nNow as your know data very well i think you will understand all the work done by me below.","ed221232":"* Now looking here for the co- relation between the different coloumns which will helps us to take only those coloumns having good correlation with target value.\n\n* The annot means to see the numeric value of the co- relation between the target and other coloumns:)\n","8fec8230":"You can see here we are changing only the target variable(Chances of Admit) to either 1 or 0 based on certain criteria which make this now a classification problem ;)","bc926b99":"# Who Wins Regressors or classifiers ?","fee6dac8":"# # Important Note:\n\nThe coloumns like \"Chance of Admit\" and \"LOS\" are actually having space after the end which means they are actually \"Chance of Admit \" and \"LOS \". If you do any data visualzation remmember this thing :)","efbfc8ba":"* The dataset above contains a serial No column which will actually not add any value to our models and predictions so i am droping that columns","b2040831":"So if you look at the results we have found that \n\n* In Regression the **DecisionTreeRegressor** performed extremely well with MSE(Mean Squared error) of 0.096\n\n* In Classification we found that **DecisionTreeClassifer** performed very well with accuracy_score of 92%.\n\nSo if you see both the results are pretty much good with the Decision Tree which make us to choose any either regression or classification depends on your choice. I havee provided you here both the methods which I think make you understand about other models too.\n\nBut in my opinion, I would choose regression because the i will never neeeded to transform my data in future, if I get more data just i need to change the file name and thats it. Well this is my opinion, you may have a different one, no argues and lets be confortable with which we are already :)","36381f09":"![image.png](attachment:image.png)\n\n# Which one to choose among Regression and classification?\n\nSo now guys a interesting thing for which actually I have made this kernel so that you prople can get the idea of working in this manner also. So if you look at your dataset, particularly at the target coloumn you will find that this problem comes under both Regression and Classification.\n\n**For the Beginners**:\n\nAs the dataset conatins the target variable, we know that this is supervised ML problem and in supervised learning we have two ways either regression or classification and i told here we can apply both because if you see at first sight it might looks like the regression problem as the target is real number. Superb!!\n\nBut, Wait.. Wait..!!\n\nWhat if we just make a change in our target variable by saying that if target variable(chance of Admission in college) is greater than 0.80(for instance) we make them as 1 and all other below 0.80 we can make them as 0. This way it turns out to be a classifier problem(like dog or cat classifier we have). Isn't interesting ??\n\n**Well you may have question why we go for Classification when we have regression** well for that wait until the kernel gets over ;)","322a1ab7":"# Regression"}}