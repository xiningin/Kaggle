{"cell_type":{"526abfd4":"code","a2e5526f":"code","6d5916a1":"code","3cbae2c8":"code","1a17132a":"code","26407184":"code","e8ba5891":"code","ba00af34":"code","19df763d":"code","8397b97d":"code","9b50feb6":"code","79727c6a":"code","f70f6f87":"markdown","dcda434d":"markdown","0adb9a04":"markdown","95fc398d":"markdown"},"source":{"526abfd4":"# importing lib. for text preprocessing\nimport re\nimport spacy\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Importing Gensim for LDA \nimport gensim\nfrom gensim import corpora","a2e5526f":"# NLP pre-processing tex in english language\nnlp = spacy.load('en_core_web_sm')","6d5916a1":"# Creating documents\nD1 = 'I want to watch a movie this weekend.'\nD2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\nD3 =  'I don\u2019t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\nD4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It\u2019s been long!'\nD5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza\u2019s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'","3cbae2c8":"# combining all the documents into a list:\ncorpus = [D1, D2, D3, D4, D5]\nprint(corpus)","1a17132a":"# Apply Preprocessing on the Corpus\n\n# stop loss words \nstop = set(stopwords.words('english'))\n\n# punctuation \nexclude = set(string.punctuation) \n\n# lemmatization\nlemma = WordNetLemmatizer() \n\n# One function for all the steps:\ndef clean(doc):\n    \n    # convert text into lower case + split into words\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    \n    # remove any stop words present\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n    \n    # remove punctuations + normalize the text\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())  \n    return normalized\n\n# clean data stored in a new list\nclean_corpus = [clean(doc).split() for doc in corpus]\nprint(clean_corpus)","26407184":"# Creating the term dictionary of our courpus that is of all the words \n# where every unique term is assigned an index. \n\ndict_ = corpora.Dictionary(clean_corpus)\nprint(dict_)","e8ba5891":"# The dictionary had 52 unqiue words in the cleaned corpus.\nfor i in dict_.values():\n    print(i)","ba00af34":"# Converting list of documents (corpus) into Document Term Matrix using the dictionary \ndoc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]\ndoc_term_matrix","19df763d":"# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n\n# Running and Training LDA model on the document term matrix.\nldamodel = Lda(doc_term_matrix, num_topics=6, id2word = dict_, passes=1, random_state=0, eval_every=None)","8397b97d":"# Prints the topics with the indexes: 0,1,2 :\n\nldamodel.print_topics()\n\n# we need to manually check whethere the topics are different from one another or not","9b50feb6":"# 1) Extracting Topics from the Corpus\n\nprint(ldamodel.print_topics(num_topics=6, num_words=5))\n\n# num_topics: how many topics want to extract \n# num_words: the number of words that want per topic","79727c6a":"# 2) Assigning the topics to the documents\n\n# printing the topic associations with the documents and there frequency. \ncount = 0\nfor i in ldamodel[doc_term_matrix]:\n    print(\"Doc : \",count+1,i)\n    count += 1","f70f6f87":"# **Conclusion:** #\nThe five documents are assigned the topics with the weightage that will help to tell which is the dominant topic for the respective document.\n\nFrom above can see:\n\nDocument 1 has the highest weight of 58.4% for Topic 2.\nTopic 3 dominates the document 2 having the weightage of 94%. Similarly, 1st topic is the main topic for document 3 with ~92% weight.\nDocument 3 is influenced by the Topic 5 with 94% and Topic 4 rules the document 5.","dcda434d":"# **Data Preparation Step**","0adb9a04":"# **Data Cleaning Step**","95fc398d":"# **LDA Output**"}}