{"cell_type":{"91751c71":"code","e397ee32":"code","d1864f61":"code","de4be3e1":"code","960e41ff":"code","95d91c3f":"code","b22f2018":"code","3a0e9349":"code","860a721c":"code","7e383a2e":"code","b1947d61":"code","a1c31654":"code","64c35db5":"code","e7188833":"code","3ed0b8ac":"code","ecc11526":"code","f485ee36":"code","12190234":"code","975ef48d":"code","5109ed34":"code","edbd8cca":"code","04d6b025":"code","66f24d1d":"code","eafda2c3":"code","2564baa1":"code","ca3e1d72":"code","87b05858":"code","f4f4fe77":"code","bc1620fd":"code","daf95773":"code","7ae69cf7":"code","3ef486cd":"code","7cdd86a6":"code","73d79718":"code","b1e722b0":"code","67630b42":"code","b73db8cf":"code","04e1d717":"code","e06177e9":"code","c740881a":"code","f72c733e":"code","0483eac2":"code","462166f4":"code","3b45b066":"code","b3104693":"code","044a7fe0":"code","e1dfbdf9":"code","39c60b1f":"code","eeaf5e3d":"code","aa810911":"code","527fedc4":"code","3ef2ed3a":"code","26c2092e":"code","19024c4f":"code","bdc93043":"code","5e42a981":"code","a022108b":"code","b84f9f47":"code","702f0f71":"code","a9f65ef9":"code","abe96b33":"code","97bc117c":"code","95ac76e2":"code","182e6ae2":"code","c8cff5cd":"code","e689f16f":"code","6ccfafb2":"code","f25e5c73":"code","99f6755d":"code","9dad20e9":"code","91adff27":"code","ebf67a5a":"code","b018ba3f":"code","2bfa59a6":"code","7c6808d4":"code","f01e5679":"code","503c63c0":"code","da199b94":"code","09ec58fc":"code","9ed83700":"code","8bd1d9b5":"code","91873899":"code","cea1134d":"code","58b66b9d":"code","8702ffce":"code","53444cac":"code","5929a8bd":"code","16ba2284":"code","eb0877ac":"code","41f68477":"code","14a16f5c":"code","5064c984":"code","7d8986e3":"code","61e08fa3":"code","fc31168a":"code","632bb9b4":"code","ba85e774":"code","b4e19617":"code","6303aaa6":"code","dbcf0a2d":"code","36049c15":"code","4715ae3d":"code","6637c655":"code","753cb365":"code","bd4a03c1":"code","147e4758":"code","e0864a49":"code","2ae6ffcd":"code","0e35e294":"code","5a88c0b9":"code","4e6c7deb":"code","796e3fc5":"code","84ad8fb9":"code","eaaabd65":"code","53b8e1d6":"code","c8a85cbb":"code","c7b40c8d":"code","e48a6ef0":"code","fb0e037c":"code","2f88dddf":"code","a24d4985":"code","1abfc955":"code","c87106c9":"code","be1fe3f8":"code","6a7ab746":"code","b4d25347":"code","8174868d":"code","8b724169":"code","fab71a2b":"code","24c331cd":"code","256e4ab4":"code","b291cd56":"code","f0ed7823":"code","cfae84b9":"markdown","5af8b1ad":"markdown","8b09d2be":"markdown","61c2978a":"markdown","63bf1b87":"markdown","f4652beb":"markdown","2bf22532":"markdown","0c0f8f51":"markdown","d51ecfe2":"markdown","a721d05f":"markdown","b42c5288":"markdown","e80719ac":"markdown","f20cff49":"markdown","2200ac2a":"markdown","8bcb939d":"markdown","eff45c50":"markdown","3645ebea":"markdown","909e4c4c":"markdown","0ec1e814":"markdown","de7fa123":"markdown","1389ef93":"markdown","6d913ebf":"markdown","dd41f685":"markdown","155983c4":"markdown","7e2bf5e1":"markdown","e1182d42":"markdown","c5261c13":"markdown","74347ea4":"markdown","c3f0494e":"markdown","06b393f6":"markdown","fcef152a":"markdown","b6c72791":"markdown","08ab8e34":"markdown","14801197":"markdown","bf0d9e4b":"markdown","238699ba":"markdown","7275e716":"markdown","06c78172":"markdown","72bb09b4":"markdown","ceb84e89":"markdown","b84b455b":"markdown","e8ef9617":"markdown","f1424923":"markdown","9124d56d":"markdown","e3323d25":"markdown","4460da08":"markdown","181b3210":"markdown","1a189763":"markdown","30f2bbcd":"markdown","28af5d84":"markdown","5ed3e094":"markdown","e14e439e":"markdown","07abaaa0":"markdown","b1c9a92c":"markdown","696bd3f3":"markdown","1758b221":"markdown","6b178e23":"markdown","c10f5b46":"markdown","281064b1":"markdown","228c5056":"markdown","c78a6c0b":"markdown","b90ca803":"markdown","87180d7d":"markdown","e689409b":"markdown","96421680":"markdown","da0529e7":"markdown","6f8ad855":"markdown","d9f77ad5":"markdown","b44c6f1b":"markdown","ccea416c":"markdown","a174f4ba":"markdown","c750a9b7":"markdown","93bf9c09":"markdown","f8757615":"markdown","adf15972":"markdown","9f92ddc7":"markdown","bc6793d0":"markdown","f3d174d3":"markdown","18aecfab":"markdown","c77c4718":"markdown","c5d8ebe4":"markdown","7f6f3ac5":"markdown","f941a77c":"markdown","7f8c34e2":"markdown","00eed7d9":"markdown","e4da72fe":"markdown","99518c4c":"markdown","01ffb1e5":"markdown","d1e9e59d":"markdown","72f088be":"markdown","c2237996":"markdown","eeb38de3":"markdown","8fae2316":"markdown","fa04d790":"markdown","e54276f1":"markdown","2f626ae9":"markdown","7e9cf10a":"markdown","075d1173":"markdown","2a55c52e":"markdown","1ce19f3d":"markdown","8402d135":"markdown","62a2cb2a":"markdown","9eedeba0":"markdown","c8318aef":"markdown","8da947d7":"markdown","ea2032e1":"markdown","9a5f44a5":"markdown","2e99a141":"markdown","d642c4a8":"markdown","f4a108e5":"markdown","48754753":"markdown","30b36907":"markdown","f0f5ae33":"markdown","d0013ee2":"markdown","03891988":"markdown","43b12a80":"markdown","9b2e6416":"markdown","0c05683c":"markdown","0d324dbd":"markdown","04788af1":"markdown","8dc219e2":"markdown","56db91e3":"markdown","4c6d2585":"markdown","58010038":"markdown","539dc84f":"markdown","75e79795":"markdown","fb38d5b2":"markdown","bef4f513":"markdown","920bac2e":"markdown","44509a4e":"markdown","358959ea":"markdown","c80c6b30":"markdown","e2e281f0":"markdown","02dbc391":"markdown","b1410174":"markdown","90bd0c7c":"markdown","a03b98c0":"markdown","4fcaa617":"markdown","a0d3b121":"markdown","a8316ef1":"markdown","ebad3f89":"markdown","dd8cfd59":"markdown","c070714f":"markdown","37d85c2f":"markdown","1936e2a1":"markdown","ff902129":"markdown"},"source":{"91751c71":"import numpy as np\nimport pandas as pd\nimport time\nimport string\nimport re\nimport math\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, TweedieRegressor,HuberRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import mean_squared_error as mse\n\n\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n\nimport kerastuner as kt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.io import curdoc, show, output_notebook\noutput_notebook()\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nstop_words = stopwords.words('english')\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nfrom spacy import displacy\n\nimport transformers\nfrom transformers import BertTokenizer, TFBertModel, RobertaTokenizer, TFRobertaModel","e397ee32":"df_train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nprint(\" Training dataset shape : \" + str(df_train.shape))\nprint(\" Testing dataset shape : \" + str(df_test.shape))\n","d1864f61":"df_train.head()","de4be3e1":"df_train['excerpt'][0]","960e41ff":"df_test.head()","95d91c3f":"df_submission.head()","b22f2018":"df_train.isnull().sum()","3a0e9349":"df_test.isnull().sum()","860a721c":"display(df_train.sort_values(by=['target']).head())","7e383a2e":"display(df_train.sort_values(by=['target'], ascending=False).head())","b1947d61":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(df_train['target'], ax=ax, color ='green')\nplt.show()","a1c31654":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\n#sns.distplot(df_train['standard_error'], ax=ax, color ='green')\nplt.show()","64c35db5":"def msv_1(data, color = 'yellow', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\nmsv_1(df_train, color=sns.color_palette('flare',15))","e7188833":"count = df_train['excerpt'].str.split().str.len()\nprint(\"Number of words in excerpts:\\n\",count)\nprint(\"Max word count from excerpt: \", max(count))","3ed0b8ac":"df_train['excerpt_len'] = df_train['excerpt'].apply(\n    lambda x : len(x)\n)\ndf_train['excerpt_word_count'] = df_train['excerpt'].apply(\n    lambda x : len(x.split(' '))\n)","ecc11526":"fig= plt.subplots(1, 1, figsize=(20, 6))\nsns.kdeplot(df_train['excerpt_len'],  color = 'green').set_title('Excerpt Len')\nplt.show()","f485ee36":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.kdeplot(df_train['excerpt_word_count'], ax=ax, color = 'green').set_title('Excerpt Word Count')\nplt.show()","12190234":"plt.figure(figsize=(10,10))\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=500).generate(\" \".join(df_train['excerpt']))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title('Excerpts',fontsize=40);","975ef48d":"plt.figure(figsize=(16, 8))\nsns.countplot(y=\"license\",data=df_train,palette=\"crest\",linewidth=3)\nplt.title(\"License Distribution\",font=\"Serif\")\nplt.show()","5109ed34":"\nresults = Counter()\ndf_train['excerpt'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","edbd8cca":"longest = max(str(results.keys()).split(), key=len)\nprint(longest)\nprint(len(longest))","04d6b025":"df_train.head()","66f24d1d":"df_train['target'].min()","eafda2c3":"df_train.loc[df_train['target'] == df_train['target'].min()].excerpt","2564baa1":"for word in df_train.loc[df_train['target'] == df_train['target'].min()].excerpt:\n    print(word)","ca3e1d72":"df_train['target'].max()","87b05858":"df_train.loc[df_train['target'] == df_train['target'].max()].excerpt","f4f4fe77":"for word in df_train.loc[df_train['target'] == df_train['target'].max()].excerpt:\n    print(word)","bc1620fd":"def get_top_n_words(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' unigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus) \n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] \n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return freq_sorted[:n]\n\ndef get_top_n_bigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]","daf95773":"def removeStopwords(text):\n    doc = nlp(text)\n    clean_text = ' '\n    for txt in doc:\n        if (txt.is_stop == False):\n            clean_text = clean_text + \" \" + str(txt)        \n    \n    return clean_text\n\nprint(\"\\033[1mText before removeStopwords function: \\033[0m\" + df_train['excerpt'][1])\nprint(\"\\033[1mText after removeStopwords function: \\033[0m\" + removeStopwords(df_train['excerpt'][1]))","7ae69cf7":"def removePunctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\nprint(\"\\033[1mText before removePunctuations function: \\033[0m\" + df_train['excerpt'][1])\nprint(\"\\n\")\nprint(\"\\033[1mText after removePunctuations function: \\033[0m\" + removePunctuations(df_train['excerpt'][1]))","3ef486cd":"def removeLinks(text):\n    clean_text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    #https? will match both http and https\n    #A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B.\n    #\\S Matches any character which is not a whitespace character.\n    #+ Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match \u2018a\u2019 followed by any non-zero number of \u2018b\u2019s; it will not match just \u2018a\u2019.\n    return clean_text\n\ntest_string = \"http:\/\/www.youtube.com\/ and https:\/\/www.youtube.com\/ should be removed \"\n(test_string,removeLinks(test_string))","7cdd86a6":"def removeNumbers(text):\n    clean_text = re.sub(r'\\d+', '', text)\n    return clean_text\n\ntest_string = \"Hi \ud83d\ude48 99 girls are running\"\n(test_string,removeNumbers(test_string))","73d79718":"def clean(text):\n    text = text.lower() #Lets make it lowercase\n    text = removeStopwords(text)\n    text = removePunctuations(text)\n    text = removeNumbers(text)\n    text = removeLinks(text)\n    return text","b1e722b0":"df_train['excerpt_clean'] = df_train['excerpt'].apply(clean)\ndf_test['excerpt_clean'] = df_test['excerpt'].apply(clean)\ndf_train.head()","67630b42":"results = Counter()\ndf_train['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","b73db8cf":"df_train.excerpt_clean","04e1d717":"top_unigram = get_top_n_words(df_train['excerpt_clean'], 20)\nwords = [i[0] for i in top_unigram]\ncount = [i[1] for i in top_unigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#6baed6'] * 20))\n\np = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Unigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\ncurdoc().theme = 'dark_minimal'\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.title.align = 'center'\np.xaxis.major_label_orientation = \"vertical\"\nshow(p)","e06177e9":"top_bigram = get_top_n_bigram(df_train['excerpt_clean'], 20)\nwords = [i[0] for i in top_bigram]\ncount = [i[1] for i in top_bigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#a1dab4'] * 20))\n\np1 = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np1.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n# curdoc().theme = 'dark_minimal'\np1.xgrid.grid_line_color = None\np1.title.align = 'center'\np1.y_range.start = 0\np1.xaxis.major_label_orientation = \"vertical\"\nshow(p1)","c740881a":"stemmer = SnowballStemmer(language='english')\n\ntokens = df_train['excerpt'][1].split()\nclean_text = ' '\n\nfor token in tokens:\n    print(token + ' --> ' + stemmer.stem(token))","f72c733e":"def stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + stemmer.stem(token)      \n    \n    return clean_text\n\nprint(\"\\033[1mText before stemWord function: \\033[0m\" + df_train['excerpt'][1])\nprint(\"\\033[1mText after stemWord function: \\033[0m\" + stemWord(df_train['excerpt'][1]))","0483eac2":"df_train['excerpt_clean'] = df_train['excerpt_clean'].apply(stemWord)\ndf_test['excerpt_clean'] = df_test['excerpt_clean'].apply(stemWord)","462166f4":"results = Counter()\ndf_train['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","3b45b066":"doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n#for token in doc:\n   # print(token.lemma_)\nfor noun in doc.noun_chunks:\n    print(noun.text)","b3104693":"for word in doc:\n    print(word.text,  word.lemma_)","044a7fe0":"def lemmatizeWord(text):\n    tokens=nlp(text)\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + token.lemma_      \n    \n    return clean_text\n\nprint(\"Text before lemmatizeWord function: \" + df_train['excerpt'][1])\nprint(\"Text after lemmatizeWord function: \" + lemmatizeWord(df_train['excerpt'][1]))\n\ndoc = \"Apple is looking at buying U.K. startup for $1 billion\"\nlemmatizeWord(doc)","e1dfbdf9":"rmse = lambda y_true, y_pred: np.sqrt(mse(y_true, y_pred))\nrmse_loss = lambda Estimator, X, y: rmse(y, Estimator.predict(X))","39c60b1f":"# Split into train and test sets\n\n\nx = df_train['excerpt_clean']\ny = df_train['target']\n\nprint(len(x), len(y))\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","eeaf5e3d":"df_train['pos_tags'] = df_train['excerpt_clean'].str.split().map(pos_tag)","aa810911":"df_train['pos_tags']","527fedc4":"def count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ndf_train['tag_counts'] = df_train['pos_tags'].map(count_tags)","3ef2ed3a":"df_train['tag_counts']","26c2092e":"set_pos = set([tag for tags in df_train['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    df_train[tag] = df_train['tag_counts'].map(lambda x: x.get(tag, 0))","19024c4f":"df_train.head()","bdc93043":"pos = df_train[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,10))\nax = sns.barplot(x=pos.index, y=pos.values,palette=\"flare\")\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('Part-Of-Speech tags frequency')\nplt.show()","5e42a981":"df_train.loc[df_train['target'] == df_train['target'].max()].excerpt.to_string()","a022108b":"sent = str()\nfor word in df_train.loc[df_train['target'] == df_train['target'].max()].excerpt:\n    sent = sent  + word\nsent","b84f9f47":"doc1 = nlp(sent)","702f0f71":"displacy.render(doc1, style=\"dep\")","a9f65ef9":"doc1 = nlp(df_train['excerpt'][22])","abe96b33":"displacy.render(doc1, style=\"ent\")","97bc117c":"# document level\nents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc1.ents]\nprint(ents)","95ac76e2":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(1,1): {val_score}')","182e6ae2":"model = make_pipeline(\n    CountVectorizer(ngram_range=(2,2)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(2,2): {val_score}')","c8cff5cd":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(1,2): {val_score}')","e689f16f":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    LinearRegression(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for CountVectorizer(1,3): {val_score}')","6ccfafb2":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    Ridge(),\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for Ridge Regression: {val_score}')","f25e5c73":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    xgb.XGBRegressor() ,\n)\n\nval_score = cross_val_score(\n    model, \n    df_train['excerpt_clean'], \n    df_train['target'], \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for Extreme Gradient Boosting: {val_score}')","99f6755d":"def training(model, X_train, y_train, X_test, y_test, model_name, ngram_range):\n    t1 = time.time()\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=ngram_range),\n        model,\n    )\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = mse(y_test, y_pred)\n    \n    t2 = time.time()\n    training_time = t2-t1 \n    \n    print(\"--- Model:\", model_name,\"---\")\n    print(\"MSE: \",MSE)\n    print(\"Training time:\",training_time)\n    print(\"\\n\")","9dad20e9":"ridge = Ridge(fit_intercept = True, normalize = False)\nlr = LinearRegression()\nxgbr = xgb.XGBRegressor()\nlasso = Lasso(alpha=0.1)\ntr = TweedieRegressor()\nhr = HuberRegressor(max_iter = 300)\nmodels = [ridge,lr,xgbr,lasso,tr,hr]\n\nmodelnames = [\"Ridge Regression\",\"Linear Regression\",\"Extreme Gradient Boosting\", \"Lasso Regression\",\"Tweedie Regressor\",\"Huber Regressor\"]","91adff27":"X = df_train[\"excerpt_clean\"]\ny = df_train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nn_gram_dict = { \"Unigram\" : (1,1), \"Unigrams + Bigrams\": (1,2), \"Bigrams alone\": (2,2), \"Unigrams + Bigrams + Trigrams\": (1,3)}\n\nfor n_gram in n_gram_dict.keys():\n    print(\"\\033[1m \" + n_gram + \" \\n \\033[0m\")\n    for i in range(0,len(models)):\n        training(model=models[i], X_train=X_train, y_train=y_train, X_test=X_test,y_test=y_test, model_name=modelnames[i],ngram_range=n_gram_dict[n_gram])\n    print(\"*\" * 40)\n    \n","ebf67a5a":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_root_mean_squared_error', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","b018ba3f":"\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()","2bfa59a6":"def predict_complexity(model, excerpt):\n  # Create the sequences\n  padding_type='post'\n  sample_sequences = tokenizer.texts_to_sequences(excerpt)\n  excerpt_padded = pad_sequences(sample_sequences, padding=padding_type, \n                                 maxlen=max_length) \n  classes = model.predict(excerpt_padded)\n  for x in range(len(excerpt_padded)):\n    print(excerpt[x])\n    print(classes[x])\n    print('\\n')\n","7c6808d4":"#text = df_train.excerpt\ntext = df_train.excerpt_clean","f01e5679":"vocab_size = 51038\nembedding_dim = 64\nmax_length = 50\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"","503c63c0":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(text)\nword_index = tokenizer.word_index","da199b94":"training_sequences = tokenizer.texts_to_sequences(text)\ntraining_padded = pad_sequences(training_sequences,maxlen=max_length, \n                                truncating=trunc_type, padding=pad_type)\n\n#validation_sequences = tokenizer.texts_to_sequences(text[800:])\n#validation_padded = pad_sequences(validation_sequences,maxlen=max_length)\n\ntraining_labels_final = np.array(df_train.target)\n#validation_labels_final = np.array(df_train[800:].target)","09ec58fc":"training_padded","9ed83700":"print(training_padded.shape)\nprint(training_labels_final.shape)\n#print(validation_labels_final.shape)","8bd1d9b5":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),  \n    tf.keras.layers.Dense(1)\n])\n#model.compile(loss='binary_crossentropy',optimizer='adam',)#metrics=[rmse])\nmodel.compile(loss='mean_squared_error', metrics=[RootMeanSquaredError()])\nmodel.summary()","91873899":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","cea1134d":"\ndef f(num):\n    return math.sqrt(math.sqrt(num))\n\nf(51308)","58b66b9d":"f(16662)","8702ffce":"embedding_dim = 12","53444cac":"xs=[]\nys=[]\ncurrent_item=1\nfor item in text:\n xs.append(current_item)\n current_item=current_item+1\n ys.append(len(item))\nnewys = sorted(ys)\nplt.xlabel('Excerpt')\nplt.ylabel('Word Length')\nplt.title('Length of Words in Excerpt')\nplt.plot(xs,newys)\n\nplt.show()","5929a8bd":"max_length = 800","16ba2284":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n    tf.keras.layers.Dense(1)\n])\nmodel.compile(loss='mean_squared_error',optimizer='adam', metrics=[RootMeanSquaredError()])\nmodel.summary()","eb0877ac":"num_epochs = 100\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","41f68477":"\npredict_complexity(model, df_test['excerpt'])\nplot_graphs(history, \"root_mean_squared_error\")\nplot_graphs(history, \"loss\")","14a16f5c":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(embedding_dim, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(), \n    tf.keras.layers.Dense(24, activation='relu', kernel_regularizer = tf.keras.regularizers.l2(0.01)),\n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.0001\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","5064c984":"num_epochs = 100\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","7d8986e3":"model.save(\"commonlitmodel.h5\")","61e08fa3":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","fc31168a":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","632bb9b4":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)), \n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","ba85e774":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","b4e19617":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, \n                                                       return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n    tf.keras.layers.Dense(1)\n])\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","6303aaa6":"num_epochs = 35\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.1,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","dbcf0a2d":"def model_builder(hp):\n  model = keras.Sequential()\n  model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\n\n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n  hp_units = hp.Int('units', min_value=16, max_value=256, step=8)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  model.add(keras.layers.Dense(1))\n\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n\n  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.MeanSquaredError(),\n                metrics=[RootMeanSquaredError()])\n\n  return model","36049c15":"tuner_search=kt.RandomSearch(model_builder,\n                       objective = kt.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                       max_trials=5,directory='output',project_name=\"nlp\")","4715ae3d":"#tuner_search.search(training_padded,training_labels_final,epochs=10,validation_split=0.1)","6637c655":"tuner = kt.Hyperband(model_builder,\n                     max_epochs=10,\n                     objective = kt.Objective(\"val_root_mean_squared_error\", direction=\"min\"),\n                     factor=3,\n                     directory='my_dir',\n                     project_name='intro_to_kt')","753cb365":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","bd4a03c1":"\n#tuner.search(training_padded, training_labels_final, epochs=5, validation_split=0.1, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\n#best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\n#print(f\"\"\"The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is {best_hps.get('units')} and the optimal learning rate for the optimizeris {best_hps.get('learning_rate')}.\"\"\")","147e4758":"glove_embeddings = dict()\nf = open('\/kaggle\/input\/glove6b\/glove.6B.50d.txt')\nfor line in f:\n values = line.split()\n word = values[0]\n coefs = np.asarray(values[1:], dtype='float32')\n glove_embeddings[word] = coefs\nf.close()","e0864a49":"glove_embeddings['frog']\n","2ae6ffcd":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(glove_embeddings[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(50)\n    return v \/ np.sqrt((v ** 2).sum())","0e35e294":"xtrain, xvalid, ytrain, yvalid = train_test_split(df_train.excerpt_clean, df_train.target, \n \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","5a88c0b9":"# create sentence vectors using the above function for training and validation set\nxtrain_glove = [sent2vec(x) for x in xtrain]\nxvalid_glove = [sent2vec(x) for x in xvalid]","4e6c7deb":"xtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","796e3fc5":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBRegressor(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict(xvalid_glove)\n\nprint (\"MSE: %f \" % mse(yvalid, predictions))","84ad8fb9":"embedding_dim = 50\nvocab_size = 51308","eaaabd65":"embedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n if index > vocab_size - 1:\n     break\n else:\n     embedding_vector = glove_embeddings.get(word)\n if embedding_vector is not None:\n     embedding_matrix[index] = embedding_vector","53b8e1d6":"model = tf.keras.Sequential([\n tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False),\n tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,\n return_sequences=True)),\n tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n tf.keras.layers.Dense(24, activation='relu'),\n tf.keras.layers.Dense(1)\n])","c8a85cbb":"learning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","c7b40c8d":"num_epochs = 100\nhistory = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n                    validation_split=0.3,\n                    #validation_data=(validation_padded, validation_labels_final),\n                   callbacks=[early_stopping,learning_rate_reduction])","e48a6ef0":"xs=[]\nys=[]\ncumulative_x=[]\ncumulative_y=[]\ntotal_y=0\nfor word, index in tokenizer.word_index.items():\n xs.append(index)\n cumulative_x.append(index)\n if glove_embeddings.get(word) is not None:\n     total_y = total_y + 1\n     ys.append(1)\n else:\n     ys.append(0)\n cumulative_y.append(total_y \/ index)","fb0e037c":"fig, ax = plt.subplots(figsize=(12,2))\nax.spines['top'].set_visible(False)\nplt.margins(x=0, y=None, tight=True)\n#plt.axis([13000, 14000, 0, 1])\nplt.fill(ys)","2f88dddf":"plt.plot(cumulative_x, cumulative_y)\nplt.axis([0, 25000, .915, .985])\n","a24d4985":"\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","1abfc955":"def bert_encode(data,maximum_length) :\n  input_ids = []\n  attention_masks = []\n  \n\n  for i in range(len(data.excerpt)):\n      encoded = tokenizer.encode_plus(\n        \n        data.excerpt[i],\n        add_special_tokens=True,\n        max_length=maximum_length,\n        pad_to_max_length=True,\n        \n        return_attention_mask=True,\n        \n      )\n      \n      input_ids.append(encoded['input_ids'])\n      attention_masks.append(encoded['attention_mask'])\n  return np.array(input_ids),np.array(attention_masks)","c87106c9":"train_input_ids,train_attention_masks = bert_encode(df_train,60)\ntest_input_ids,test_attention_masks = bert_encode(df_test,60)","be1fe3f8":"def create_model(bert_model):\n  input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n  \n  output = bert_model([input_ids,attention_masks])\n  output = output[1]\n  #output = tf.keras.layers.Dense(32,activation='relu')(output)\n  #output = tf.keras.layers.Dropout(0.2)(output)\n\n  output = tf.keras.layers.Dense(1)(output)\n  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n  model.compile(tf.keras.optimizers.Adam(lr=6e-6), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n  return model","6a7ab746":"\nbert_model = TFBertModel.from_pretrained('bert-large-uncased')","b4d25347":"model = create_model(bert_model)\nmodel.summary()","8174868d":"history = model.fit([train_input_ids,train_attention_masks],df_train.target,validation_split=0.3, epochs=2,batch_size=10)","8b724169":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","fab71a2b":"\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nBASE_MODEL = TFRobertaModel.from_pretrained('roberta-base')","24c331cd":"\n\ndef create_model(bert_model):\n  input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n  \n  output = bert_model([input_ids,attention_masks])\n  output = output[1]\n  output = tf.keras.layers.Dense(32,activation='relu')(output)\n  output = tf.keras.layers.Dropout(0.2)(output)\n\n  output = tf.keras.layers.Dense(1)(output)\n  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n  \n  return model\n\n\n\nmodel = create_model(BASE_MODEL)\nmodel.compile(tf.keras.optimizers.Adam(lr=6e-6), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n    \nmodel.summary()","256e4ab4":"history = model.fit([train_input_ids,train_attention_masks],df_train.target,validation_split=0.3, epochs=2,batch_size=10)","b291cd56":"def submission(submission_file_path,model,excerpt):\n    padding_type='post'\n    sample_sequences = tokenizer.texts_to_sequences(excerpt)\n    excerpt_padded = pad_sequences(sample_sequences, padding=padding_type, \n                                 maxlen=max_length) \n    classes = model.predict(excerpt_padded)\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = classes\n    sample_submission.to_csv(\"submission.csv\", index=False)\n    ","f0ed7823":"#submission_file_path = \"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\"\n\n#submission(submission_file_path,model,df_test['excerpt'])","cfae84b9":"Data cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset\n\nWe will create a 'clean' function which comprises of various cleaning function such as removal of punctuations etc","5af8b1ad":"\n[Back to Top](#section-zero)","8b09d2be":"View target distribution","61c2978a":"\n[Back to Top](#section-zero)","63bf1b87":"[Back to Top](#section-zero)","f4652beb":"Use sklearn.feature_extraction.text's TfidfVectorizer and make a pipeline comprising TfidfVectorizer and our models","2bf22532":"[Back to Top](#section-zero)","0c0f8f51":"<a id=\"subsection-bow-ridge\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Ridge Regression<\/p>","d51ecfe2":"Use displacy to render the excerpt with the largest target value","a721d05f":"<a id=\"section-EDA\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Exploratory Data Analysis <\/p>","b42c5288":"\n[Back to Top](#section-zero)\n","e80719ac":"# Plotting and Predicting Helper Functions","f20cff49":"[Back to Top](#section-zero)","2200ac2a":"<a id=\"subsection-lemmatization\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Lemmatization <\/p>","8bcb939d":"Let's define a function to build our model. Try optimizing units and learning rate.","eff45c50":"Examples with the 5 highest target values","3645ebea":"<a id=\"subsection-datacleaning\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Data Cleaning <\/p>","909e4c4c":"# Random Search","0ec1e814":"[Back to Top](#section-zero)","de7fa123":"On preprocessing, some NER information is lost.","1389ef93":"View std_error distribution","6d913ebf":"\n[Back to Top](#section-zero)","dd41f685":"Examples with the 5 lowest target values","155983c4":"[Back to Top](#section-zero)","7e2bf5e1":"We will find out vocab size count,i.e. total number of words used. We will use Counter class from collections.","e1182d42":"What if, instead of learning the embeddings for yourself, you could instead use prelearned embeddings, where researchers have already done the hard work of turning words into vectors and those vectors are proven? One example of this is the GloVe (Global Vectors for WordRepresentation) model developed by Jeffrey Pennington, Richard Socher, and Christopher Manning at Stanford","c5261c13":"<a id=\"subsection-embedding\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Create the model using an Embedding <\/p>","74347ea4":"View df_train now","c3f0494e":"<a id=\"subsection-multiple-LSTM\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Multiple Bidirectional Long Short Term Memory (LSTM) <\/p>","06b393f6":"<a id=\"subsection-bow-xgb\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Extreme Gradient Boosting<\/p>","fcef152a":"[Back to Top](#section-zero)","b6c72791":"<a id=\"section-submission\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Making the Submission <\/p>","08ab8e34":"<a id=\"section-gloveembeddings\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Glove Embeddings <\/p>","14801197":"An n-gram is a contiguous sequence of n items from a given sample of text or speech\n\nAn n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\".","bf0d9e4b":"# Hyperband","238699ba":"Uncomment this to run","7275e716":"# Unigram Only","06c78172":"Only url_legal and license columns appear to be having missing values","72bb09b4":"**Using regularization**\nRegularization is a technique that helps prevent overfitting by reducing the polariza\u2010\ntion of weights. If the weights on some of the neurons are too heavy, regularization\neffectively punishes them. Broadly speaking, there are two types of regularization: L1\nand L2.\n* L1 regularization is often called lasso (least absolute shrinkage and selection operator)\n    regularization. It effectively helps us ignore the zero or close-to-zero weights when\n    calculating a result in a layer.\n* L2 regularization is often called ridge regression because it pushes values apart by taking their squares. This tends to amplify the differences between nonzero  values and zero or close-to-zero ones, creating a ridge effect.\n\nFor NLP problems like the one we\u2019re considering, L2 is most commonly used. It can be added as an attribute to the Dense layer using the kernel_regularizers property,\nand takes a floating-point value as the regularization factor. This is another hyperparameter that you can experiment with to improve your model.","ceb84e89":"> In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.","b84b455b":"<a id=\"subsection-gloveLSTM\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> GloVe  Stacked LSTM <\/p>","e8ef9617":"\n[Back to Top](#section-zero)\n","f1424923":"**Exploring embedding dimensions**","9124d56d":"<a id=\"section-wordembeddings\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Embeddings <\/p>","e3323d25":"texts_to_sequences will convert our excerpts to corresponding sequences. We use padding so that each sequence is same (max_length). Commented out validtion part because I am using validation_split in model's fit function instead.","4460da08":"\n[Back to Top](#section-zero)","181b3210":"Adding two columns to the train dataset: \n* *excerpt_len* \n> Length of the excerpt \n* *excerpt_word_count* \n> Count of number of words in the excerpt","1a189763":"<a id=\"section-library-importations\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Library Importations <\/p>","30f2bbcd":"\n[Back to Top](#section-zero)","28af5d84":"Click [here](https:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model) for more information on Bag-of-Words model","5ed3e094":"<a id=\"section-loading-datasets\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Loading Datasets <\/p>","e14e439e":"kerastuner.tuners.hyperband.Hyperband(hypermodel, objective, max_epochs, factor=3, hyperband_iterations=1, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs)","07abaaa0":"<a id=\"section-bert\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> HuggingFace TFBertModel <\/p>","b1c9a92c":"Count number of words in excerpts and maximum count","696bd3f3":"<a id=\"subsection-LSTM\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Bidirectional Long Short Term Memory (LSTM) <\/p>","1758b221":"License Distribution","6b178e23":"<a id=\"section-preprocessing\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Data Preprocessing <\/p>","c10f5b46":"Data preprocessing is the process of converting raw data into a well-readable format to be used by a machine learning model.","281064b1":"Find out the longest word and it's length","228c5056":"We will run different models at the same time.\n1. Ridge Regression\n2. Linear Regression\n3. Extreme Gradient Boosting","c78a6c0b":"Add columns for differnt tags","b90ca803":"We will use NLTK for stemming since Spacy doesn't contain any function for stemming as it relies on lemmatization only There are two types of stemmers in NLTK: Porter Stemmer and Snowball stemmers. Snowball stemmer is a slightly improved version of the Porter stemmer and is usually preferred over the latter. So we will use that.\n\nStemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.","87180d7d":"Use displacy to render with style ent","e689409b":"Lets define Root Mean Squared Error","96421680":"We will create a simple model using an embedding after applying all the optimizations we learned.","da0529e7":"Let's optimize this model and then run in next section.","6f8ad855":"<a id=\"subsection-bow-lr\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Linear Regression <\/p>","d9f77ad5":"Use train_test_split to split data into training and validation","b44c6f1b":"**fit_intercept** bool, default=True\n    Whether to fit the intercept for this model. If set to false, no intercept will be used in calculations (i.e. X and y are expected to be centered).\n    \n**normalizebool**, default=False\n    This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use StandardScaler before calling fit on an estimator with normalize=False","ccea416c":"<a id=\"section-pos\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Part-of-Speech Tagging <\/p>","a174f4ba":"The preprocessing library in TensorFlow Keras provides a number of extremely useful tools to prepare data for machine learning. One of these is a\nTokenizer that will allow you to take words and turn them into tokens.\n","c750a9b7":"\n[Back to Top](#section-zero)","93bf9c09":"[Back to Top](#section-zero)","f8757615":"<a id=\"subsection-GRU\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Gated Recurrent Units  RNN(GRU) <\/p>","adf15972":"Using validation split as 0.1 so that 10% of training data is used for validation purpose.","9f92ddc7":"[Wiki link](https:\/\/en.wikipedia.org\/wiki\/Part-of-speech_tagging)","bc6793d0":"A named entity is a \u201creal-world object\u201d that\u2019s assigned a name \u2013 for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn\u2019t always work perfectly and might need some tuning later, depending on your use case.\n\nNamed entities are available as the ents property of a Doc","f3d174d3":"<a id=\"subsection-glovexgb\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Extreme Gradient Boosting Regressor <\/p>","18aecfab":"After cleaning, see size of vocabulary:","c77c4718":"The num of params of the embedding layer will be (vocab_size) * (embedding_dim).\nThe average pooling layer has 0 trainable parameters, as it\u2019s just averaging the parameters in the embedding layer before it.","c5d8ebe4":"**Max length Optimization** We have arbitarly set max_length as 50 earlier. ","7f6f3ac5":" Click [TFBertModel](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#tfbertmodel) for more information","f941a77c":"<a id=\"section-hyperparametertuning\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Exploring Hyper Parameter Tuning with Keras <\/p>","7f8c34e2":"TF-IDF (stands for Term-Frequency-Inverse-Document Frequency) weights down the common words occuring in almost all the documents and give more importance to the words that appear in a subset of documents. TF-IDF works by penalising these common words by assigning them lower weights while giving importance to some rare words in a particular document.","00eed7d9":"[Back to Top](#section-zero)","e4da72fe":"spaCy features an extremely fast statistical entity recognition system, that assigns labels to contiguous spans of tokens. The default trained pipelines can indentify a variety of named and numeric entities, including companies, locations, organizations and products. You can add arbitrary classes to the entity recognition system, and update the model with new examples.","99518c4c":"*Earlier values:*\nvocab_size = 51038\nembedding_dim = 64\nmax_length = 50\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"","01ffb1e5":"Let us see the predictions for df_test","d1e9e59d":"Use pandas's read_csv function to read dataframe and print it's shape.","72f088be":"Though we could not perform stemming with spaCy, we can perform lemmatization using spaCy. This is a time consuming process.\n\nOutput of lemmatization is an actual word in English unlike Stemming. (word.lemma_ will print word's lemma in SPacy)","c2237996":"Our model is over-fitting now.","eeb38de3":"\n[Back to Top](#section-zero)","8fae2316":"<a id=\"section-ner\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Named Entity Recognition <\/p>","fa04d790":"We will see the excerpt with minimum target value","e54276f1":"\n[Back to Top](#section-zero)","2f626ae9":"<a id=\"section-bow\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Bag of Words (BoW) <\/p>","7e9cf10a":"The abstract from the paper is the following:\n\n> We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n> \n> BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n> \nTips:\n\n* BERT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n* BERT was trained with the masked language modeling (MLM) and next sentence prediction (NSP) objectives. It is efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation.","075d1173":"Write a function to create our model. We will add Dense layer(s) as output layer.\n\nAdd DropOut if overfitting","2a55c52e":"Functions to get top unigrams and bigrams","1ce19f3d":"Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. \n\nAn embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. \n\nA higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.","8402d135":"[Back to Top](#section-zero)","62a2cb2a":"**BERT Encoding**\n\n\nData is encoded according to BERT requirement.There is a very helpful function called encode_plus provided in the Tokenizer class. It can seamlessly perform the following operations:\n\n* Tokenize the text\n\n* Add special tokens - [CLS] and [SEP]\n\n* Create token IDs\n\n* Pad the sentences to a common length\n\n* Create attention masks for the above PAD tokens","9eedeba0":"[Back to Top](#section-zero)","c8318aef":"Generate a word cloud","8da947d7":"# Optimizing the Model","ea2032e1":"\n[Back to Top](#section-zero)","9a5f44a5":"See vocabulary size now","2e99a141":"We will see the excerpt with maximum target value","d642c4a8":"\n[Back to Top](#section-zero)\n","f4a108e5":"Change ngram_range and experiment","48754753":"Uncomment this to run. ","30b36907":"# Simple Model","f0f5ae33":"[Back to Top](#section-zero)","d0013ee2":"Best practice for embedding size is to have it be the fourth root of the vocab size. \n\n","03891988":"Here are some of the different tags:","43b12a80":"<a id=\"section-robertabase\"><\/a>\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> roberta-base Hugging Face Transformer <\/p>","9b2e6416":"We will define some callbacks to be used with the model's fit function:-\n* **Learning rate reduction** - \n\n        tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\",\n        factor=0.1,\n        patience=10,\n        verbose=0,\n        mode=\"auto\",\n        min_delta=0.0001,\n        cooldown=0,\n        min_lr=0,\n        **kwargs\n        )\nReduce learning rate when a metric has stopped improving.\n\nModels often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced.\n\nWe will use that to prevent\/reduce overfitting.\n\n* **Early Stopping**\n\n       tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        min_delta=0,\n        patience=0,\n        verbose=0,\n        mode=\"auto\",\n        baseline=None,\n        restore_best_weights=False,\n        )\nStop training when a monitored metric has stopped improving.\n\nAssuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates.\n\nThe quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.compile().\n\nWe will use that to prevent\/reduce overfitting.","0c05683c":"\n[Back to Top](#section-zero)","0d324dbd":"<a id=\"subsection-stemming\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Stemming <\/p>","04788af1":"Pretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task. These embeddings are trained on large datasets, saved, and then used for solving other tasks. That\u2019s why pretrained word embeddings are a form of Transfer Learning.","8dc219e2":"\n[Back to Top](#section-zero)","56db91e3":"# Bi-grams only","4c6d2585":"kerastuner.tuners.randomsearch.RandomSearch(hypermodel, objective, max_trials, seed=None, hyperparameters=None, tune_new_entries=True, allow_new_entries=True, **kwargs)","58010038":"A question we should ask ourselves is what values to give for vocab_size, max_length, embedding dimension etc.\nVocab_size I have given as 51308 here which is the total number of words (We found this in [Exploratory Data Analysis](#section-EDA))","539dc84f":"# Unigrams + Bi-grams","75e79795":"A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n* A vocabulary of known words.\n* A measure of the presence of known words.\nIt is called a \u201c*bag*\u201d of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.","fb38d5b2":"Refer https:\/\/seaborn.pydata.org\/tutorial\/color_palettes.html for seaborn palette\n\nPlot POS tag frequency for df_train's tag_cols.\nset_yscale as log so that smaller values also get displayed.","bef4f513":"<a id=\"section-tdidf\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> TD IDF <\/p>","920bac2e":"Most excerpts have 800 words or less, so we use that value instead.","44509a4e":"<a id=\"subsection-CNN\"><\/a>\n\n# <p style=\"font-family: Arial; font-size:1.2em;color:tomato;\"> Convolutional Neural Network (CNN\/ConvNet) <\/p>","358959ea":"Use on data excerpt or excerpt_clean?","c80c6b30":"https:\/\/huggingface.co\/roberta-base","e2e281f0":"# CommonLit: Detailed Guide\n\n<a id=\"section-zero\"><\/a>\n\n<p style=\"font-family: Arial; font-size:1.4em;color:gold;\"> Learning NLP <\/p>\n# TABLE OF CONTENTS\n\n\n* [Library Importations](#section-library-importations)\n* [Loading Datasets](#section-loading-datasets)\n* [Exploratory Data Analysis](#section-EDA)\n* [Data Preprocessing](#section-preprocessing)\n    - [Data Cleaning](#subsection-datacleaning) \n    - [Stemming](#subsection-stemming) \n    - [Lemmatization](#subsection-lemmatization)\n* [Part-of-Speech Tagging](#section-pos)\n* [Named Entity Recognition](#section-ner)\n* [Bag of Words + Models](#section-bow)\n    - [Linear Regression](#subsection-bow-lr) \n    - [Ridge Regression](#subsection-bow-ridge)  \n    - [Extreme Gradient Boosting](#subsection-bow-xgb)  \n* [TD IDF + Models](#section-tdidf)\n    - [Linear Regression](#section-tdidf) \n    - [Ridge Regression](#section-tdidf)  \n    - [Extreme Gradient Boosting](#section-tdidf)  \n    - [Lasso Regression](#section-tdidf) \n    - [Tweedie Regression](#section-tdidf)  \n    - [Huber Regression](#section-tdidf)  \n* [Embedding + Models](#section-wordembeddings)\n    - [Simple Embedding](#subsection-embedding)\n    - [Convolutional Neural Networks](#subsection-CNN)\n    - [Gated Recurrent Units](#subsection-GRU)\n    - [Single Long Short Term Memory](#subsection-LSTM)\n    - [Multiple Long Short Term Memory](#subsection-multiple-LSTM)\n* [Hyper Parameters Tuning](#section-hyperparametertuning)\n    - [Random Search](#subsection-randomsearch)\n    - [Hyperband](#subsection-hyperband)\n* [Glove Embeddings](#section-gloveembeddings)\n    - [Extreme Gradient Boosting](#subsection-glovexgb)\n    - [Stacked LSTM](#subsection-gloveLSTM)\n* [BERT Huggingface Transformer](#section-bert)\n* [RoBerta HuggingFace Transformer](#section-robertabase)\n* [Submission](#section-submission)","02dbc391":"Write a function count_tags to count the number of pos_tags and add it as a column to the dataframe.","b1410174":"\n[Back to Top](#section-zero)","90bd0c7c":"# Unigrams + Bi-grams + Tri-grams","a03b98c0":"Change epochs number","4fcaa617":"# Callbacks","a0d3b121":"\n[Back to Top](#section-zero)","a8316ef1":"\n[Back to Top](#section-zero)","ebad3f89":"Encode both train and test dataset","dd8cfd59":"* CC coordinating conjunction\n* CD cardinal digit\n* DT determiner\n* EX existential there (like: \u201cthere is\u201d \u2026 think of it like \u201cthere exists\u201d)\n* FW foreign word\n* IN preposition\/subordinating conjunction\n* JJ adjective \u2018big\u2019\n* JJR adjective, comparative \u2018bigger\u2019\n* JJS adjective, superlative \u2018biggest\u2019\n* LS list marker 1)\n* MD modal could, will\n* NN noun, singular \u2018desk\u2019\n* NNS noun plural \u2018desks\u2019\n* NNP proper noun, singular \u2018Harrison\u2019\n* NNPS proper noun, plural \u2018Americans\u2019\n* PDT predeterminer \u2018all the kids\u2019\n* POS possessive ending parent\u2018s \n* PRP personal pronoun I, he, she\n* RB adverb very, silently,\n* RBR adverb, comparative better\n* RBS adverb, superlative best\n* RP particle give up\n* TO to go \u2018to\u2018 the store.\n* UH interjection errrrrrrrm\n* VB verb, base form take\n* VBD verb, past tense took\n* VBG verb, gerund\/present participle taking\n* VBN verb, past participle taken\n* VBP verb, sing. present, non-3d take\n* VBZ verb, 3rd person sing. present takes\n* WDT wh-determiner which\n* WP wh-pronoun who, what\n* WP$ possessive wh-pronoun whose\n* WRB wh-abverb where, when","c070714f":"Let's define a simple model with embedding layer as the first layer\nFor regression to arbitary values problem, dont give last layer activations","37d85c2f":"The BERT model was proposed in BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\u2019s a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.","1936e2a1":"Follow the same steps as the above section","ff902129":"Before we start, let us define some callbacks, variables and other helper functions"}}