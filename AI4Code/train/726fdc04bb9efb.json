{"cell_type":{"20829277":"code","24b0f9fe":"code","db6cfa87":"code","2752ccef":"code","657b597e":"code","e88fa164":"code","c306c43a":"code","863dfe6a":"code","9ba56c66":"code","414cf014":"code","80c37dde":"code","51d7a14e":"code","0ed82356":"code","64da02d7":"code","ac7d231b":"code","366011e4":"code","b96c1d92":"code","97be7ed1":"code","5e238b71":"code","e6e9c3df":"code","39775026":"code","64f1af36":"code","821f6973":"code","e3298948":"code","fccf5502":"code","61d9a285":"code","2c7f99aa":"code","6c3f7d48":"code","ab16aeb4":"code","46f1a1e0":"code","38d37130":"markdown","2e7f0e5a":"markdown","e0b6edfe":"markdown","f2ffb33d":"markdown","89e729de":"markdown","ce8390d0":"markdown","c2793833":"markdown","52e50696":"markdown","1a326638":"markdown"},"source":{"20829277":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np","24b0f9fe":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","db6cfa87":"(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n\nX_train = X_train_full[5000:]\ny_train = y_train_full[5000:]\nX_valid = X_train_full[:5000]\ny_valid = y_train_full[:5000]","2752ccef":"import matplotlib.pyplot as plt\n\n# see random image\nsome_index = np.random.choice(X_train.shape[0])\nsome_image = X_train[some_index]\nsome_label = y_train[some_index]\nplt.imshow(some_image)\nprint(some_label)","657b597e":"# unique values in y_train\ny_train_unique = np.unique(y_train)\ny_train_unique","e88fa164":"# plot one image from each class in a subplot\nfig = plt.figure(figsize=(16, 8))\nfor label in y_train_unique:\n    image_index = np.random.choice(np.where(y_train == label)[0])\n    plt.subplot(1, len(y_train_unique), label + 1)\n    plt.imshow(X_train[image_index])\n    plt.axis('off')\n    plt.title(label)","c306c43a":"X_train.shape","863dfe6a":"tf.keras.backend.clear_session()\nnp.random.seed(42)","9ba56c66":"with tpu_strategy.scope():\n    model = keras.models.Sequential()\n    for i in range(21):\n        if i == 0:\n            model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n        else:\n            model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))\n    model.add(keras.layers.Dense(10, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy',\n                optimizer=keras.optimizers.Nadam(),\n                metrics=['accuracy'])","414cf014":"# Using Nadam Optimization and early stopping, train the network.\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\nmodel_checkpoint = keras.callbacks.ModelCheckpoint('cifar10_model.h5', save_best_only=True)\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\nhistory = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n                    callbacks=[early_stopping, model_checkpoint],\n                    batch_size=BATCH_SIZE)","80c37dde":"import plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# subplots of loss and accuracy\n\ndef compare_loss_acc(history):\n    # returns a plotly figure of the loss and accuracy curves for training and validation\n    fig = make_subplots(rows=1, cols=2)\n    fig.add_trace(go.Scatter(x=history.epoch, y=history.history['loss'], name='Training Loss'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=history.epoch, y=history.history['val_loss'], name='Validation Loss'), row=1, col=1)\n    fig.add_trace(go.Scatter(x=history.epoch, y=history.history['accuracy'], name='Training Accuracy'), row=1, col=2)\n    fig.add_trace(go.Scatter(x=history.epoch, y=history.history['val_accuracy'], name='Validation Accuracy'), row=1, col=2)\n    fig.update_layout(height=600, width=1000)\n    return fig","51d7a14e":"fig = compare_loss_acc(history)\nfig.show()","0ed82356":"tf.keras.backend.clear_session()\nnp.random.seed(42)","64da02d7":"with tpu_strategy.scope():\n    model_batch = keras.models.Sequential()\n    for i in range(21):\n        if i == 0:\n            model_batch.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n            model_batch.add(keras.layers.BatchNormalization())\n        else:\n            model_batch.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n            model_batch.add(keras.layers.BatchNormalization())\n            model_batch.add(keras.layers.Activation('elu'))\n    model_batch.add(keras.layers.Dense(10, activation='softmax'))\n    model_batch.compile(loss='sparse_categorical_crossentropy',\n                optimizer=keras.optimizers.Nadam(),\n                metrics=['accuracy'])","ac7d231b":"model_batch.summary()","366011e4":"model_checkpoint = keras.callbacks.ModelCheckpoint('cifar10_model_bn.h5', save_best_only=True)\nhistory_batch = model_batch.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n                                callbacks=[early_stopping, model_checkpoint],\n                                batch_size=BATCH_SIZE)","b96c1d92":"def compare_lr_curves(history_1, history_1_name, history_2, history_2_name):\n    # compare the learning curves of the two models\n    fig = make_subplots(rows=1, cols=2)\n    fig.add_trace(go.Scatter(x=history_1.epoch, y=history_1.history['loss'], name=history_1_name+\" Training Loss\"), row=1, col=1)\n    fig.add_trace(go.Scatter(x=history_1.epoch, y=history_1.history['val_loss'], name=history_1_name+\" Validation Loss\"), row=1, col=1)\n    fig.add_trace(go.Scatter(x=history_1.epoch, y=history_1.history['accuracy'], name=history_1_name+\" Training Accuracy\"), row=1, col=2)\n    fig.add_trace(go.Scatter(x=history_1.epoch, y=history_1.history['val_accuracy'], name=history_1_name+\" Validation Accuracy\"), row=1, col=2)\n    fig.add_trace(go.Scatter(x=history_2.epoch, y=history_2.history['loss'], name=history_2_name+\" Training Loss\"), row=1, col=1)\n    fig.add_trace(go.Scatter(x=history_2.epoch, y=history_2.history['val_loss'], name=history_2_name+\" Validation Loss\"), row=1, col=1)\n    fig.add_trace(go.Scatter(x=history_2.epoch, y=history_2.history['accuracy'], name=history_2_name+\" Training Accuracy\"), row=1, col=2)\n    fig.add_trace(go.Scatter(x=history_2.epoch, y=history_2.history['val_accuracy'], name=history_2_name+\" Validation Accuracy\"), row=1, col=2)\n    \n    ## update color of traces\n    fig.data[0].update(marker_color='#ff0000')\n    fig.data[1].update(marker_color='#800000')\n    fig.data[2].update(marker_color='#ff0000')\n    fig.data[3].update(marker_color='#800000')\n    fig.data[4].update(marker_color='#4da6ff')\n    fig.data[5].update(marker_color='#0059b3')\n    fig.data[6].update(marker_color='#4da6ff')\n    fig.data[7].update(marker_color='#0059b3')\n    \n    # update layout\n    fig.update_layout(height=600, width=1000, template='plotly_white')\n    return fig\n    ","97be7ed1":"fig = compare_lr_curves(history_batch, \"Batch Normalization\", history, \"No Batch Normalization\")\nfig.show()","5e238b71":"tf.keras.backend.clear_session()\nnp.random.seed(42)","e6e9c3df":"X_train.shape","39775026":"# standardize the data\nX_means = X_train.mean(axis=0)\nX_stds = X_train.std(axis=0)\nX_train_scaled = (X_train - X_means) \/ X_stds\nX_valid_scaled = (X_valid - X_means) \/ X_stds\nX_test_scaled = (X_test - X_means) \/ X_stds","64f1af36":"# DNN with SELU activation\nwith tpu_strategy.scope():\n    model_selu = keras.models.Sequential()\n    for i in range(21):\n        if i == 0:\n            model_selu.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n        else:\n            model_selu.add(keras.layers.Dense(100, activation='selu', kernel_initializer=\"lecun_normal\"))\n    model_selu.add(keras.layers.Dense(10, activation='softmax'))\n    model_selu.compile(loss='sparse_categorical_crossentropy',\n                optimizer=keras.optimizers.Nadam(),\n                metrics=['accuracy'])","821f6973":"model_selu.summary()","e3298948":"model_checkpoint = keras.callbacks.ModelCheckpoint('cifar10_model_selu.h5', save_best_only=True)\nhistory_selu = model_selu.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), \n                              callbacks=[early_stopping, model_checkpoint],\n                              batch_size=BATCH_SIZE)","fccf5502":"fig = compare_lr_curves(history_selu, \"SELU Activation\", history_batch, \"Batch Normalization\")\nfig.show()","61d9a285":"# add alpha dropout\n# DNN with SELU activation\nwith tpu_strategy.scope():\n    model_alpha = keras.models.Sequential()\n    for i in range(21):\n        if i == 0:\n            model_alpha.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n            model_alpha.add(keras.layers.AlphaDropout(0.1))\n        else:\n            model_alpha.add(keras.layers.Dense(100, activation='selu', kernel_initializer=\"lecun_normal\"))\n            model_alpha.add(keras.layers.AlphaDropout(0.1))\n    model_alpha.add(keras.layers.Dense(10, activation='softmax'))\n    model_alpha.compile(loss='sparse_categorical_crossentropy',\n                optimizer=keras.optimizers.Nadam(),\n                metrics=['accuracy'])","2c7f99aa":"model_alpha.summary()","6c3f7d48":"model_checkpoint = keras.callbacks.ModelCheckpoint('cifar10_model_alpha.h5', save_best_only=True)\nhistory_alpha = model_alpha.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), \n                                callbacks=[early_stopping, model_checkpoint], batch_size=BATCH_SIZE)","ab16aeb4":"compare_lr_curves(history_alpha, \"Alpha Dropout\", history_selu, \"w\/o Alpha Dropout\")","46f1a1e0":"best_model = keras.models.load_model('cifar10_model_bn.h5')\nbest_model.evaluate(X_test, y_test)","38d37130":"## d)\nReplace Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes","2e7f0e5a":"## e)\nTry regularizing the model with alpha dropout, and compare with Dropout only.","e0b6edfe":"With Alpha Dropout, our model didn't overfit like the previous one, but the model with Batch Normalization Layers performed way better.","f2ffb33d":"We can see a huge improvement in both accuracy and loss! With a huge stack of Dense layers (20), probably the gradients of some hidden layers were vanishing or exploding. That's why we had some inconstant training in the first model and a big improvement with Batch Normalization.","89e729de":"### a)\nBuild a DNN with 20 hidden layers of 100 neurons each. Use the He initialization and the ELU activation function.","ce8390d0":"53% of accuracy on the test set. Not bad! I will try to improve this performance in future notebook with CNNs. Make sure to check it out, and let me know how can I improve my work! Thank you!","c2793833":"# Introduction\n\nThis notebook is an exercise from Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, by Aur\u00e9lien Geron.\n\nIt aims to build different Deep Neural Networks to predict images in the dataset CIFAR-10, availiable in https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html","52e50696":"## c)\nAdd Batch Normalization and compare the learning curves","1a326638":"## b)\nUsing Nadam Optimization and early stopping, train the network."}}