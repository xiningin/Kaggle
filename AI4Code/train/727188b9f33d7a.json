{"cell_type":{"372d9e78":"code","e2b65619":"code","56ee274e":"code","5a587bc8":"code","97e29355":"code","7c0d6ab2":"code","525161cc":"code","27a44701":"code","2d7a23bb":"code","3e93c009":"code","6ce014a1":"code","cda7bc56":"markdown","5fdd39e2":"markdown","a451c2db":"markdown","29e1496e":"markdown","6a5eabcf":"markdown","35c90010":"markdown"},"source":{"372d9e78":"import pandas as pd\nimport numpy as np\nfrom category_encoders import LeaveOneOutEncoder\nfrom sklearn.metrics import mean_squared_error\n\npd.set_option('display.max_columns', None)\n#from IPython.core.interactiveshell import InteractiveShell\n#InteractiveShell.ast_node_interactivity = 'all' --> this two last lines print all the variables not only the last one.","e2b65619":"df = pd.read_csv('..\/input\/train-stratkfolds-7\/train_stratkfolds.csv')\ndftest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsample_submission = pd.read_csv('..\/input\/30-days-of-ml-dataset-target-encoding-by-folds\/sample_submission.csv')","56ee274e":"object_cols = [col for col in df.columns if col.startswith('cat')]\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(7):\n        xtrain = df[df['stratkfold'] != fold].reset_index(drop=True)\n        xvalid = df[df['stratkfold'] == fold].reset_index(drop=True)\n        loo_enc = LeaveOneOutEncoder()\n        loo_enc.fit(xtrain[col], xtrain['target'])\n        xvalid[f'loo_enc_{col}'] = loo_enc.transform(xvalid[col])\n        temp_df.append(xvalid) #recreating xtrain according to validation samples\n        if temp_test_feat is None:\n            temp_test_feat = loo_enc.transform(dftest[col])\n        else:\n            temp_test_feat += loo_enc.transform(dftest[col])\n    \n    temp_test_feat \/= 7    \n    dftest[f'loo_enc_{col}'] = temp_test_feat\n    df = pd.concat(temp_df)","5a587bc8":"useful_features = [col for col in df.columns if col not in ('id', 'target', 'kfold', 'cutbin', 'stratkfold')]\ncateg_cols = [col for col in df.columns if col.startswith('cat')]\n#numerical_cols = [col for col in useful_features if col not in categ_cols]\n\nfor col in categ_cols:\n    unique_values = sorted(df[col].unique())\n    print(col, \":\", unique_values)\n    conversion_dict = dict(zip(unique_values, range(len(unique_values))))\n    print('Conversion Dict:', conversion_dict)\n    # When working with the Categorical\u2019s codes, missing values will always have a code of -1.\n    df[col] = df[col].map(conversion_dict, na_action=-1).astype('category', copy=False)\n    dftest[col] = dftest[col].map(conversion_dict, na_action=-1).astype('category', copy=False)","97e29355":"from time import time\nimport pprint\nimport joblib\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Regressor\nfrom lightgbm import LGBMRegressor\n\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n\n# Plotting functions\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n#sns.set(style='whitegrid')","7c0d6ab2":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title=\"model\", callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    \n    if callbacks is not None:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n        \n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    \n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           + u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                   len(optimizer.cv_results_['params']),\n                                   best_score,\n                                   best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","525161cc":"X = df[useful_features].copy()\ny = df['target']","27a44701":"model_lgbmR = LGBMRegressor(boosting_type='gbdt',\n                         objective='regression',\n                         n_jobs=1, \n                         verbose=-1,\n                         random_state=0)\n\n\n\nsearch_spaces = {\n    'learning_rate': Real(0.01, 1.0, 'log-uniform'),     # Boosting learning rate\n    'n_estimators': Integer(30, 5000),                   # Number of boosted trees to fit\n    'num_leaves': Integer(2, 512),                       # Maximum tree leaves for base learners\n    'max_depth': Integer(-1, 256),                       # Maximum tree depth for base learners, <=0 means no limit\n    'min_child_samples': Integer(1, 256),                # Minimal number of data in one leaf\n    'max_bin': Integer(100, 1000),                       # Max number of bins that feature values will be bucketed\n    'subsample': Real(0.01, 1.0, 'uniform'),             # Subsample ratio of the training instance\n    'subsample_freq': Integer(0, 10),                    # Frequency of subsample, <=0 means no enable\n    'colsample_bytree': Real(0.01, 1.0, 'uniform'),      # Subsample ratio of columns when constructing each tree\n    'min_child_weight': Real(0.01, 10.0, 'uniform'),     # Minimum sum of instance weight (hessian) needed in a child (leaf)\n    'reg_lambda': Real(1e-9, 100.0, 'log-uniform'),      # L2 regularization\n    'reg_alpha': Real(1e-9, 100.0, 'log-uniform'),       # L1 regularization\n    #'scale_pos_weight': Real(1.0, 500.0, 'uniform'),     # Weighting of the minority class (Only for binary classification)\n    }","2d7a23bb":"#OPTIMIZER\nopt = BayesSearchCV(estimator=model_lgbmR,                                    \n                    search_spaces=search_spaces,                      \n                    scoring=None,                          \n                    cv=5,                                           \n                    n_iter=3000,                                      # max number of trials\n                    n_points=3,                                       # number of hyperparameter sets evaluated at the same time\n                    n_jobs=-1,                                        # number of jobs\n                    #iid=True,                                        # if not iid it optimizes on the cv score, 'deprecated' in version 0.9\n                    return_train_score=False,                         \n                    refit=False,                                      \n                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n                    random_state=0)                                   # random state for replicability\n\n\noverdone_control = DeltaYStopper(delta=0.0001)               # We stop if the gain of the optimization becomes too small\ntime_limit_control = DeadlineStopper(total_time=60 * 45)     # We impose a time limit (45 minutes)\n\n\n#Calling our optimizer\nbest_params = report_perf(opt, X, y,'LightGBM', \n                          callbacks=[overdone_control, time_limit_control])","3e93c009":"final_predictions = []\nscores = []\nfor fold in range(7):\n    xtrain = df[df['stratkfold'] != fold].reset_index(drop=True)\n    xvalid = df[df['stratkfold'] == fold].reset_index(drop=True)\n    xtest = dftest.copy()\n    \n    ytrain = xtrain['target']\n    yvalid = xvalid['target']\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid [useful_features]\n    xtest = xtest[useful_features]\n    \n    model = LGBMRegressor(boosting_type='gbdt',\n                          objective='regression',\n                          n_jobs=-1,\n                          verbose=-1,\n                          random_state=fold,\n                          **best_params)\n    \n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_predictions.append(test_preds)\n    rmse =  mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \nprint('Average rmse: ', np.mean(scores))","6ce014a1":"#preds = np.mean(np.column_stack(final_predictions), axis=1)\n# sample_submission.head()\n# sample_submission['target'] = preds\n# sample_submission.head()","cda7bc56":"First, we create a wrapper function to deal with running the optimizer and reporting back its best results.","5fdd39e2":"Defining our base model, before the optimization and the seach spaces.","a451c2db":"As I mention before in my other kernel, I've binned this data in 7 folds. I choose the binning number following the sturge's rule = 1 + log(N). The trainning file with those folds it's already uploaded so you can try it.","29e1496e":"Transforming categorical features in categories for **lightGBM**. [Scikit-optimize for LightGBM](https:\/\/www.kaggle.com\/lucamassaron\/scikit-optimize-for-lightgbm) by [@LucaMassaron](https:\/\/www.kaggle.com\/lucamassaron).","6a5eabcf":"# LightGBM Regressor\n### Bayesian Optimizer. [Scikit-optimize for LightGBM](https:\/\/www.kaggle.com\/lucamassaron\/scikit-optimize-for-lightgbm). Thank's to [@LucaMassaron](https:\/\/www.kaggle.com\/lucamassaron).\nI took my time to complete this notebook (kernel) but I think for many of us it worth the time. If you think it is, please upvote!","35c90010":"### Target Encoding\nUsing LeaveOneOut encoder. This last recommendations were from the discussion board from 30Days of ML, 1st place. (https:\/\/www.kaggle.com\/c\/30-days-of-ml\/discussion\/269541)"}}