{"cell_type":{"ec15b6ca":"code","4aafff37":"code","02c6b9e1":"code","5e25cf34":"code","decc0c49":"code","c42b9994":"code","e46bf847":"code","f524a94d":"code","d3975906":"code","4dc44dad":"code","211af45b":"code","6f3f2399":"code","95d29684":"code","f02a4b3e":"code","10517aac":"code","67fa3a6f":"code","3787aa7a":"code","27f822ff":"code","af044f1e":"code","3e06e6ee":"code","72c92cd0":"code","70710413":"code","de10fba7":"code","d71e853b":"code","9446d002":"code","39670f20":"code","c786220c":"code","8c04743f":"code","4dcf8811":"code","fefb4085":"code","db073702":"code","ddab3b8e":"code","43ff9dce":"code","1689b3b2":"code","21b221cb":"code","e6c5bfaf":"code","85a486d3":"code","1bd43e89":"code","6398ae0a":"code","ea7889ec":"code","423767ee":"code","ded89d9e":"code","2f9d9ae6":"code","bdb63f2c":"code","537cc77c":"code","cc14bc8c":"code","a17d12e7":"markdown","11bc9740":"markdown","c3e36535":"markdown","691bfef5":"markdown","44d442b1":"markdown","c86639b5":"markdown","f23be992":"markdown","74593979":"markdown","943cfb48":"markdown","af86c18c":"markdown","6c419cbc":"markdown","416fd80b":"markdown","1643671c":"markdown","a3624275":"markdown","ee33ae41":"markdown","7f25c4c0":"markdown","7ae95208":"markdown","6bab0514":"markdown","61c94bca":"markdown","0f0b7c86":"markdown","45682f50":"markdown","020943b9":"markdown","7ea2b972":"markdown","7b66a324":"markdown","31f15a8c":"markdown","940073f3":"markdown","efff6e5c":"markdown","25ef2b0c":"markdown","eaf3cf88":"markdown","5d127068":"markdown","bbb51c88":"markdown","baa53b60":"markdown","0b3761f9":"markdown","c96d0a79":"markdown","85e6df94":"markdown","ccd83136":"markdown"},"source":{"ec15b6ca":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings  \nwarnings.filterwarnings('ignore')","4aafff37":"data=pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata.head()","02c6b9e1":"data.shape","5e25cf34":"data.info()","decc0c49":"num_cols=[\"age\",\"avg_glucose_level\",\"bmi\"]\ncat_cols=[\"gender\",\"stroke\",\"smoking_status\",\"heart_disease\", \"ever_married\", \"hypertension\", \"work_type\", \"Residence_type\"]","c42b9994":"data[cat_cols]","e46bf847":"gender_stroke=data.groupby([\"gender\",\"stroke\"])[[\"stroke\"]].count()\ngender_stroke","f524a94d":"data=data[data[\"gender\"]!=\"Other\"]","d3975906":"pd.crosstab(data['gender'],data['stroke']).plot.bar()","4dc44dad":"gender_stroke=data.groupby([\"smoking_status\",\"stroke\"])[[\"stroke\"]].count()\ngender_stroke","211af45b":"pd.crosstab(data['smoking_status'],data['stroke']).plot.bar()","6f3f2399":"pd.crosstab(data['heart_disease'],data['stroke'],normalize='index')","95d29684":"pd.crosstab(data['heart_disease'],data['stroke']).plot.bar()","f02a4b3e":"pd.crosstab(data['ever_married'],data['stroke'],normalize='index')","10517aac":"pd.crosstab(data['ever_married'],data['stroke']).plot.bar()","67fa3a6f":"data[num_cols].head()","3787aa7a":"data[\"age_bin\"]=pd.qcut(data['age'], 5,labels=[0,1,2,3,4])\ndata[\"age_bin\"].value_counts()","27f822ff":"age_to_fill=data.groupby([\"gender\",\"age_bin\"])[\"bmi\"].mean()\nage_to_fill","af044f1e":"for cl in range(0,5):\n    for sex in ['Male' , 'Female']:\n        filll = pd.to_numeric(age_to_fill.xs(sex).xs(cl))\n        data.loc[(data.bmi.isna() & (data.age_bin == cl) & (data.gender == sex)),'bmi'] =filll\n        \ndata.drop(columns='age_bin',inplace=True)#We don't need this anymore","3e06e6ee":"fig = plt.figure(figsize=(18, 5))\nax1 = fig.add_subplot(131)\ndata[\"bmi\"].hist(bins=40,color = \"skyblue\")\nax2 = fig.add_subplot(132,sharey=ax1,sharex=ax1)\ndata[data[\"stroke\"]==1][\"bmi\"].hist(bins=40,color = \"red\")\nax3 = fig.add_subplot(133,sharey=ax1,sharex=ax1)\ndata[data[\"stroke\"]==0][\"bmi\"].hist(bins=40,color = \"springgreen\")\nax1.title.set_text('All BMI')\nax2.title.set_text('Stroke=1')\nax3.title.set_text('Stroke=0')\nplt.show()","72c92cd0":"data.groupby([\"stroke\"])[\"bmi\"].agg(['mean','median'])","70710413":"data.groupby([\"stroke\"])[\"avg_glucose_level\"].agg(['mean','median'])","de10fba7":"data.drop(columns='id',inplace=True)\ndata.head()","d71e853b":"data.hypertension.replace({0:'No',1:'Yes'},inplace=True)\ndata.heart_disease.replace({0:'No',1:'Yes'},inplace=True)\ndata.head()","9446d002":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer","39670f20":"num_cols=['age', 'avg_glucose_level', 'bmi']\ncat_cols=['hypertension','heart_disease','ever_married','work_type','Residence_type','smoking_status','stroke']","c786220c":"ct = ColumnTransformer([(\"standard\", StandardScaler(), num_cols)],remainder='passthrough')\ndf= pd.DataFrame(ct.fit_transform(data[num_cols]), columns=data[num_cols].columns)\ndf.head()","8c04743f":"data.reset_index(drop=True, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndata_2=pd.concat((data[cat_cols],df),axis=1)\ndata_2.head()","4dcf8811":"data=pd.get_dummies(data)\ndata.head()","fefb4085":"from sklearn.model_selection import train_test_split\nX=data.drop(columns='stroke')\ny=data['stroke']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","db073702":"y_train.value_counts(),y_test.value_counts()","ddab3b8e":"y.value_counts().plot.pie()","43ff9dce":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n# instantiating over and under sampler\nover = RandomOverSampler(sampling_strategy=0.4)\nunder = RandomUnderSampler(sampling_strategy=0.8)\n# first performing oversampling to minority class\nX_over, y_over = over.fit_resample(X, y)\nprint(f\"Oversampled: {Counter(y_over)}\")\n# now to comine under sampling \nX_combined_sampling, y_combined_sampling = under.fit_resample(X_over, y_over)\nprint(f\"Combined Random Sampling: {Counter(y_combined_sampling)}\")","1689b3b2":"y_combined_sampling.value_counts().plot.pie()","21b221cb":"over = RandomOverSampler(sampling_strategy=0.4)\nunder = RandomUnderSampler(sampling_strategy=0.8)\n\n## Training\n# first performing oversampling to minority class\nX_over, y_over = over.fit_resample(X_train, y_train)\n\n# now to comine under sampling \nX_combined_sampling, y_combined_sampling = under.fit_resample(X_over, y_over)\nX_train=X_combined_sampling\ny_train=y_combined_sampling\nprint(f\"Combined Random Sampling on X_train: {Counter(y_combined_sampling)}\")\n\n## Testing\n# first performing oversampling to minority class\nX_over, y_over = over.fit_resample(X_test, y_test)\n\n# now to comine under sampling \nX_combined_sampling, y_combined_sampling = under.fit_resample(X_over, y_over)\nX_test=X_combined_sampling\ny_test=y_combined_sampling\nprint(f\"Combined Random Sampling on X_train: {Counter(y_combined_sampling)}\")","e6c5bfaf":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import classification_report","85a486d3":"model=LogisticRegression()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_pred,y_test))","1bd43e89":"model=RandomForestClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_pred,y_test))","6398ae0a":"model=KNeighborsClassifier()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_pred,y_test))","ea7889ec":"model=DecisionTreeClassifier(max_depth=4)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_pred,y_test))","423767ee":"model=SVC()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_pred,y_test))","ded89d9e":"model=SVC(kernel='linear')\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint(classification_report(y_pred,y_test))","2f9d9ae6":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score,cross_validate  #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=8) # k=10, split the data into 10 equal parts\n\nxyz=[]\nxyz2=[]\naccuracy=[]\nf1=[]\nstd=[]\nclassifiers=[\"Logistic Reg\",\"SVC\",\"KNN\",\"Decision tree\",\"Random Forest\",\"Linear SVC\"]\nmodels=[LogisticRegression(),SVC(),KNeighborsClassifier(),\n        DecisionTreeClassifier(),\n        RandomForestClassifier(),\n       SVC(kernel='linear')]\nfor i in models:\n    model = i\n    cv_result = cross_validate(model,X_combined_sampling,y_combined_sampling, cv = kfold,scoring = [\"f1_macro\",\"accuracy\"])\n    \n    xyz.append(cv_result['test_accuracy'].mean())\n    xyz2.append(cv_result['test_f1_macro'].mean())\n    #std.append(cv_result.std())\n    accuracy.append(cv_result['test_accuracy'])\n    f1.append(cv_result['test_f1_macro'])\n    \nnew_models_dataframe2=pd.DataFrame({'CV Acc Mean':xyz,'CV f1 mean':xyz2},index=classifiers)       \nnew_models_dataframe2","bdb63f2c":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(f1,index=classifiers)\nbox.T.boxplot()","537cc77c":"ensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier()),\n                                              ('RBF',SVC(probability=True,kernel='rbf')),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,max_depth=5,random_state=1)),\n                                              ('LR',LogisticRegression()),\n                                              ('DT',DecisionTreeClassifier(max_depth=4,criterion='gini', random_state=0)),\n                                              ('LSVC',SVC(probability=True,kernel='linear'))\n                                             ], \n                       voting='hard')\ncv_result=cross_validate(ensemble_lin_rbf,X_combined_sampling,y_combined_sampling, cv = kfold,scoring = [\"f1_macro\",\"accuracy\"])\ncv_result","cc14bc8c":"print(f\"Our ensemble model accuracy:{cv_result['test_accuracy'].mean()} \")\nprint(f\"Our ensemble model f1-score:{cv_result['test_f1_macro'].mean()} \")","a17d12e7":"## ever_married","11bc9740":"# Learning","c3e36535":"Our dataset has 5110 people. Let's check our dataset for missing values and data type by `info()`","691bfef5":"Looks like our dataset is more blanced now. Time to start training","44d442b1":"### Logistic Regression","c86639b5":"## Split\nSplit our data","f23be992":"As we can see our data is now more balanced. We shall split our data again!","74593979":"We can see that stroke patient on average have higher BMI. Let do the same glucose level:","943cfb48":"As we can see BMI has some missing values. And our data types are: (Be aware even though some data types having int64 type is actually categorical variable)\n* Numerical variables\n  * Continous: age, avg_glucose_level, BMI\n  * Distrect: None\n* Categorical : gender, stroke, smoking_status,heart_disease, ever_married, hypertension, work_type, Residence_type\n\nIt's a good idea to store our column data type","af86c18c":"## One hot and Scale","6c419cbc":"## Hypertension, work_type, Residence_type\nIt's the same as above with no much different, I will skip the visualization for them.\n# Continous variables","416fd80b":"Let's walk through each feature and see its correlation to stroke.","1643671c":"Great! our data is complte and ready to go!","a3624275":"Overall, decision tree and random forstest did the best job classifying stroke.","ee33ae41":"# Understanding the dataset","7f25c4c0":"Our dataset is imbalanced. There are only about 4% stroke cases in our training set. If we predict no one has a stroke, we will have an accuracy of 96%! Which is not a good thing. Besides that detecting stroke patients is more important than the healthy ones.\n\nWe can fix them by oversampling or undersampling. Since our dataset is small we will take the first one.","7ae95208":"# Voting ensemble\nVoting regression is a good way to boost model accuracy.","6bab0514":"### KNN","61c94bca":"Wait! Remember that we are missing some BMI values? We need to fill them up. Here I calculate BMI based on the mean gender and age.\n\nFirst, let bin our age into 5 categories and calculate the mean of each one","0f0b7c86":"As we can see, if you have heart disease you have over 17% chance of getting a stroke compared to 4% who don't have heart disease","45682f50":"We have the mean BMI of each gender and age range, let's fill the missing value","020943b9":"## Gender\nWho has a higher chance of getting a stroke? Female or male. Does male suffer more stroke because they smoke? Let's find out","7ea2b972":"# Prediction\nHaving take a look at our dataset, now let's make some prediction! We start by spliting our dataset into train and test set","7b66a324":"# Finishing up","31f15a8c":"Great! Our age divided into 5 bins, let's calculate the mean of each bin and gender","940073f3":"Strange but expected! people who are married are 6 time more likely to get a stroke! 6% compared to 1%","efff6e5c":"We see a similar stroke rate in both gender. There is other gender which only have 1 sample. We will remove this.","25ef2b0c":"## Smoking status","eaf3cf88":"### SVC","5d127068":"With our own eyes, it's hard to see the different. Let's calculate the mean and median.","bbb51c88":"Stroke patient also have higher average glucose level","baa53b60":"### Decision Tree","0b3761f9":"# Cross validation\nCross validation usually give us better result.","c96d0a79":"### RandomForest","85e6df94":"### Linear SVC","ccd83136":"## heart_disease"}}