{"cell_type":{"47c5b112":"code","9cfe521c":"code","3f759594":"code","339eb26a":"code","47603673":"code","5cd835aa":"code","91ee67c3":"code","488c7fc1":"code","c0d0d9f5":"code","5941f0fa":"code","4c9c6472":"code","a7d320ee":"code","5ff28e47":"code","56b0061a":"code","89e148da":"code","e2a01f01":"code","f76f93a6":"code","caa2cfcc":"code","64306b89":"code","697338d0":"code","a9a0797b":"markdown","78076a76":"markdown","b95c5745":"markdown","8dff34e9":"markdown","91cd7a23":"markdown","46aef93c":"markdown","0eb6f967":"markdown","156cf243":"markdown","2eb3bf71":"markdown","7d8907e4":"markdown","7c96dc8d":"markdown","284fe89c":"markdown","06b46e50":"markdown"},"source":{"47c5b112":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom datetime import date\nfrom datetime import datetime\n\nimport requests \nfrom bs4 import BeautifulSoup\nimport re","9cfe521c":"netflix_hist = pd.read_csv('..\/input\/netflix-viewing-history\/NetflixViewingHistory.csv')\n\nnetflix_hist.info()","3f759594":"print(\"Before cleaning NaN titles and dates: \\n\")\nprint(netflix_hist.shape)\n\nprint(\"\\nNumber of rows with NaN titles: {}\".format(netflix_hist['Title'].isna().sum()))\nprint(\"Number of rows with NaN dates: {} \\n\".format(netflix_hist['Date'].isna().sum()))\n\n# Drop NaN titles\nnetflix_hist = netflix_hist.dropna()\nassert netflix_hist['Title'].isna().sum() == 0\n\nprint(\"After cleaning NaN titles and dates: \\n\")\nprint(netflix_hist.shape)\n\nnetflix_hist.set_index('Title', inplace=True)\nnetflix_hist.head()","339eb26a":"netflix_hist[\"is_TV_show\"] = False\n\nnetflix_hist['Date'] = pd.to_datetime( netflix_hist['Date'] )\n\nfirst_day = min(netflix_hist['Date'])\nlast_day = max(netflix_hist['Date'])\n\nfor lab, row  in netflix_hist.iterrows():\n        title = str(lab)\n        is_TVshow = ['Temporada' in title, 'Season' in title, 'Serie' in title, 'Miniserie' in title, 'Cap\u00edtulo' in title, 'Episode' in title, 'Parte' in title]\n        \n        netflix_hist.loc[lab, \"is_TV_show\"] = max(is_TVshow) \nprint('First day: {}\\n'.format(first_day))   \nprint('Last day: {}\\n'.format(last_day))        \nnetflix_hist.head(5)","47603673":"def TV_show_year(netflix_hist, name_TVshow, year):\n    A = netflix_hist.index.str.contains( name_TVshow ) \n    res = [i for i, val in enumerate(list(A)) if val]\n\n    tv_show = netflix_hist.iloc[res]\n    tv_show = tv_show.drop_duplicates()\n\n    tv_show[\"week_no\"] = 0\n\n    for lab, row in tv_show.iterrows():\n        d=row[\"Date\"].strftime(\"%Y\/%m\/%d\").split('\/')\n        wkno = date(int(d[0]),int(d[1]),int(d[2])).isocalendar()[1]\n        tv_show.loc[lab,\"week_no\"] = wkno\n\n    tv_show['Date'] = pd.to_datetime( tv_show['Date'] )\n    tv_show[\"Year\"] = tv_show[\"Date\"].dt.year\n\n    tv_show = tv_show.loc[ tv_show[\"Year\"] == year]\n    summary_tv = tv_show[\"week_no\"].value_counts()\n\n    week_numbers = np.arange(1,54, 1)\n    summary_wkno = []\n\n    for x in week_numbers:\n        if x in list(summary_tv.index):\n            summary_wkno.append(summary_tv.loc[x])\n        else: \n            summary_wkno.append(0)\n    return summary_wkno\n\nweek_numbers = np.arange(1,54, 1)\nbreaking_bad = TV_show_year(netflix_hist, \"Breaking Bad\", 2016)\nsuits = TV_show_year(netflix_hist, \"Suits\", 2016)\nbluemountain = TV_show_year(netflix_hist, \"Blue Mountain State\", 2016)\nwalkingdead = TV_show_year(netflix_hist, \"The Walking Dead\", 2016)\n\nfig, ax = plt.subplots(figsize=(15,3))\nax.plot(week_numbers, breaking_bad, label='Breaking Bad')\nax.plot(week_numbers, suits, label='Suits')\nax.plot(week_numbers, bluemountain, label='Blue Mountain State')\nax.plot(week_numbers, walkingdead, label='The Walking Dead')\nplt.legend()\nplt.xticks(week_numbers)\nplt.ylabel('Number of episodes watched')\nplt.xlabel('Week number')\nplt.show()","5cd835aa":"# Before we add average duration for the series\nseries = netflix_hist[ netflix_hist[\"is_TV_show\"] == True ]\nseries = series.reset_index()\nseries[\"TV_show\"] = series[\"Title\"].str.split(':',expand=True)[0]\n\nTV_show_unique = series[\"TV_show\"].unique()\n\nprint(\"Number of TV shows to scrape: {} \\n\".format(TV_show_unique.shape[0]))\n\nprint(TV_show_unique)","91ee67c3":"def web_scrape (TV_shows, message, default_value):\n    p = 0\n    for lab, row in TV_shows.iterrows():\n\n        search_item = message + row[\"TV_show\"]\n        base = \"http:\/\/www.google.es\"\n        url = \"http:\/\/www.google.es\/search?q=\"+ search_item\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text,\"lxml\")\n        text = soup.text\n        minutes_text_1 = re.findall(\"..min\", text)\n        minutes_text_2 = re.findall(\".. min\", text)\n        min = default_value\n        \n        found = False\n\n        for x in minutes_text_1:\n            try:\n                number = int(x[0:2])\n                if number > 30 and number < 90:\n                    min = number\n                    if found == False:\n                        p = p + 1\n                        found = True\n            except:\n                pass  \n        for x in minutes_text_2:\n            try:\n                number = int(x[0:2])\n                if number > 30 and number < 90:\n                    min = number\n                    if found == False:\n                        p = p + 1\n                        found = True\n            except:\n                pass  \n\n        TV_shows.loc[lab, \"duration\"] = min       \n    return(TV_shows, p)\n\nx1 = 0; x2 = 0\n\nTV_shows = pd.DataFrame(TV_show_unique,columns=[\"TV_show\"])\n\n#TV_shows, x1 = web_scrape (TV_shows, \"average episode \", -50)\n#TV_shows[TV_shows['Duration'] == -50] = web_scrape (TV_shows[TV_shows['Duration'] == -50], \"Duracion episodio \", -50)\n\n# We assign an average duration for episode (for those whose information could not be found)\n#TV_shows[TV_shows['duration'] == -50], x2 = web_scrape (TV_shows[TV_shows['duration'] == -50], \"Duracion capitulo\", 40)\n\nTV_shows[\"duration\"] = 40\n\nx = x1+x2 # Number of TV shows with duration found with webscraping\n\nassert len(TV_shows[TV_shows['duration'] == -50]) == 0\n\nprint('Web Scraping finished for TV shows')\nprint('     Out of {} TV shows, the duration for {} was found.'.format((TV_show_unique.shape[0]), x))","488c7fc1":"# We join information from TV_show with their duration and series dataframe\nseries_with_duration = pd.merge(series[[\"Title\",\"Date\",\"is_TV_show\",\"TV_show\"]], TV_shows, on=\"TV_show\")\nprint(series_with_duration.head())\n\n# Check if a row has missing information\nseries_with_duration.isna().sum()","c0d0d9f5":"TVshow_groupby = series_with_duration.groupby(by='TV_show').sum()\n\nTVshow_groupby = TVshow_groupby.sort_values('duration', ascending=False) \nTVshow_groupby['duration_hours'] = TVshow_groupby['duration']\/60\n\nfig, ax = plt.subplots(figsize=(12,4))\nmost_watched = TVshow_groupby.head(20)\nplt.barh(most_watched.index, most_watched['duration_hours'])\nplt.xlabel('Watched hours')\nplt.show()","5941f0fa":"TVshow_groupby = series_with_duration.groupby(by='TV_show').sum()\n\nTVshow_groupby = TVshow_groupby.sort_values('duration', ascending=False) \n\ntotal_duration = TVshow_groupby['duration'].sum()\nmost_watched_and_others = TVshow_groupby.head(11) \nmost_watched_duration = most_watched_and_others['duration'].sum()\n\nmost_watched_and_others['size'] = most_watched_and_others['duration'] \/ total_duration\n\nothers = pd.DataFrame({\"is_TV_show\":1.0, \"duration\":total_duration-most_watched_duration, \"size\":(total_duration-most_watched_duration)\/total_duration}, index = [\"Other TV shows\"])\n\nmost_watched_and_others = most_watched_and_others.append( others )\n\nmost_watched_and_others = most_watched_and_others[\"size\"]\n\nlabels = most_watched_and_others.index\n\nfig1, ax1 = plt.subplots()\nax1.pie(most_watched_and_others, labels=labels, autopct='%1.1f%%',\n        shadow=False, startangle=0)\nax1.axis('equal')\n\nplt.show()","4c9c6472":"netflix_hist = netflix_hist.reset_index() \n\nfor lab, row in series_with_duration.iterrows():\n    title = row[\"Title\"]\n    duration = row[\"duration\"]\n    netflix_hist.loc[ netflix_hist[\"Title\"] == title, \"duration\"] = duration\n    \nprint(netflix_hist[\"duration\"].describe())","a7d320ee":"netflix_hist.loc[netflix_hist[\"duration\"].isnull(), \"duration\"] = -50\n\nprint(\"Number of films to scrape: {}\".format(netflix_hist.loc[netflix_hist[\"duration\"]== -50].shape[0]))\n\nfilms_sample = netflix_hist.loc[netflix_hist[\"duration\"]== -50].sample(n = 50) \nfilms_sample = films_sample.drop_duplicates()\n\nfilms_sample.head(10)","5ff28e47":"def web_scrape (films, message, default_value):\n    p = 0\n    for lab, row in films.iterrows():\n\n        search_item = message + row[\"Title\"]\n        base = \"http:\/\/www.google.es\"\n        url = \"http:\/\/www.google.es\/search?q=\"+ search_item\n        response = requests.get(url)\n\n        soup = BeautifulSoup(response.text,\"lxml\")\n        text = soup.text\n\n        hours_min_text_1 = re.findall(\".h .m\", text)\n        hours_min_text_2 = re.findall(\".h ..m\", text)\n\n        minutes_text_1 = re.findall(\"..min\", text)\n        minutes_text_2 = re.findall(\"...min\", text)\n\n        min = default_value\n        found = False\n\n        # Hour and 1-digit minutes\n        for x in hours_min_text_1:\n            #print(x)\n            try:\n                hour = int(x[0:1])\n                minutes = int(x[3:4])\n                min = hour*60 + minutes\n                if found == False:\n                    p = p+1\n                    found = True\n            except:\n                pass  \n\n        # Hour and 2-digit minutes\n        for x in hours_min_text_2:\n            #print(x)\n            try:\n                hour = int(x[0:1])\n                minutes = int(x[3:5])\n                min = hour*60 + minutes\n                if found == False:\n                    p = p+1\n                    found = True\n            except:\n                pass  \n\n        # 2-digit minutes\n        for x in minutes_text_1:\n            #print(x)\n            try:\n                number = int(x[0:2])\n                if number > 30 and number < 100:\n                    min = number\n                    if found == False:\n                        p = p+1\n                        found = True\n            except:\n                pass  \n\n        # 3-digit minutes\n        for x in minutes_text_2:\n            #print(x)\n            try:\n                number = int(x[0:3])\n                if number > 30 and number < 200:\n                    min = number\n                    if found == False:\n                        p = p+1\n                        found = True\n            except:\n                pass  \n\n        films.loc[lab, \"duration\"] = min       \n    return(films, p)\n\n#films_sample, x = web_scrape (films_sample, \"Duracion pelicula \", 100)\n\nx=0\nfilms_sample['duration'] = 100\n\nassert len(films_sample[films_sample['duration'] == -50]) == 0\n\nprint('Web Scraping finished for films')\nprint('     Out of {} films, the duration for {} was found.'.format(films_sample.shape[0], x))","56b0061a":"for lab, row in films_sample.iterrows():\n    title = row[\"Title\"]\n    duration = row[\"duration\"]\n    netflix_hist.loc[ netflix_hist[\"Title\"] == title, \"duration\"] = duration\n\nnetflix_hist.loc[netflix_hist[\"duration\"] == -50, \"duration\"] = 100    \nprint(netflix_hist[\"duration\"].describe())","89e148da":"TV_show_vs_films = netflix_hist.groupby(\"is_TV_show\").sum()\nTV_show_vs_films = TV_show_vs_films[\"duration\"]\nTV_show_vs_films = TV_show_vs_films.sort_values(ascending=False)\nsize_TV_show_vs_films = TV_show_vs_films \/ TV_show_vs_films.sum()\n\ncount_TV_show_vs_films = netflix_hist[\"is_TV_show\"].value_counts()\n\nlabels = [\"TV shows\", \"Films & Documentaries\"]\n\nfig, ax = plt.subplots(1,2, figsize=(8,4))\nax[0].pie(size_TV_show_vs_films, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=180)\nax[0].axis('equal')\nax[0].set_title('Watched hours')\n\nax[1].pie(count_TV_show_vs_films, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax[1].axis('equal')\nax[1].set_title('Number of episodes or films watched')\n\nplt.show()","e2a01f01":"netflix_hist[\"month\"] = netflix_hist[\"Date\"].dt.month\nnetflix_hist[\"year\"] = netflix_hist[\"Date\"].dt.year\nnetflix_hist[\"weekday\"] = netflix_hist[\"Date\"].dt.weekday\nmonth_year_groupby = netflix_hist.groupby(by=['month','year']).sum()\n\nmonth_year_groupby['duration_hours'] = month_year_groupby['duration'] \/ 60\n\nmonth_year_groupby = month_year_groupby.reset_index(level=['month','year'])\n\nfor year in month_year_groupby[\"year\"].unique():\n    for month in np.arange(1,13):\n        months_list = list(month_year_groupby.loc[month_year_groupby[\"year\"]==year][\"month\"])\n        bool_m = month in months_list\n        if bool_m == False:\n            month_year_groupby = month_year_groupby.append( {\"month\":int(month), \"year\":int(year), \"is_TV_show\":0, \"duration\":0, \"duration_hours\":0}, ignore_index = True)\n\nmonth_year_groupby[\"year\"] = month_year_groupby[\"year\"].astype(\"int\")\n\nmonth_names = pd.DataFrame([[1,\"Jan\"],[2,\"Feb\"],[3,\"Mar\"],[4,\"Apr\"],[5,\"May\"],[6,\"Jun\"],[7,\"Jul\"],[8,\"Aug\"],[9,\"Sep\"],[10,\"Oct\"],[11,\"Nov\"],[12,\"Dec\"]], columns=['month','month_name'])\n\nmonth_year_groupby = pd.merge(month_year_groupby, month_names, on='month')\nmonth_year_groupby['month-year'] = [\"{}-{}\".format(m, y) for m, y in zip(month_year_groupby['month_name'], month_year_groupby['year'])]\n\nmonth_year_groupby = month_year_groupby.sort_values(['year','month'])\n\nfig, ax = plt.subplots(figsize=(18,4))\n_ = sns.barplot(x='month-year', y='duration_hours', ax = ax, hue='year', data=month_year_groupby, dodge=False)\n\ndef change_width(ax, new_value) :\n    for patch in ax.patches :\n        current_width = patch.get_width()\n        diff = current_width - new_value\n        # we change the bar width\n        patch.set_width(new_value)\n        # we recenter the bar\n        patch.set_x(patch.get_x() + diff * .5)\n\nchange_width(_, .87)\n\nplt.xticks(rotation='vertical')\nplt.ylabel('Watched hours')\nplt.title('Distribution of watched hours across months')\nplt.xlabel('')\nplt.show()","f76f93a6":"year_groupby = month_year_groupby.groupby(by=['year']).sum()\nyear_groupby.reset_index()\n\nyear_type_groupby = netflix_hist.groupby(by=['year','is_TV_show']).sum()\nyear_type_groupby['duration_hours'] = year_type_groupby['duration'] \/ 60\nyear_type_groupby = year_type_groupby.reset_index()\nyear_TV_show = year_type_groupby[ year_type_groupby[\"is_TV_show\"] == True]\nyear_film = year_type_groupby[ year_type_groupby[\"is_TV_show\"] == False]\n\nfig, ax = plt.subplots(figsize=(6,4))\nax.plot(year_groupby.index, year_groupby[\"duration_hours\"], label='Total')\nax.plot(year_groupby.index, year_TV_show[\"duration_hours\"], label='TV shows')\nax.plot(year_groupby.index, year_film[\"duration_hours\"], label='Films')\nplt.legend()\nplt.xticks(year_groupby.index)\nplt.ylabel('Watched hours')\nplt.xlabel('Year')\nplt.title('Evolution of watched hours during the years')\nplt.show()","caa2cfcc":"month_years_pt = month_year_groupby.pivot_table( values=\"duration_hours\", index = \"month_name\",                                                                                     columns = \"year\", aggfunc = sum, fill_value = 0)\n\nmonth_years_pt = month_years_pt.loc[[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]]\n\nfig, ax = plt.subplots(figsize=(12,8))\n\nax.bar( month_years_pt.index, month_years_pt[2016], label=\"2016\" )\nax.bar( month_years_pt.index, month_years_pt[2017], bottom = month_years_pt[2016], label = \"2017\" )\nax.bar( month_years_pt.index, month_years_pt[2018],\nbottom = month_years_pt[2016] + month_years_pt[2017], label = \"2018\" )\nax.bar( month_years_pt.index, month_years_pt[2019],\nbottom = month_years_pt[2016] + month_years_pt[2017] + month_years_pt[2018], label = \"2019\" )\nax.bar( month_years_pt.index, month_years_pt[2020],\nbottom = month_years_pt[2016] + month_years_pt[2017] + month_years_pt[2018] + month_years_pt[2019], label = \"2020\" )\nax.set_xticklabels( month_years_pt.index) \nax.set_ylabel(\"Watched hours\")\nplt.title('Distribution of monthly watched hours during the years')\nax.legend()\nplt.show()","64306b89":"weekday_value_counts = netflix_hist[\"weekday\"].value_counts()\nweekday_groupby = netflix_hist.groupby(by=['weekday','is_TV_show']).sum()\n\nweekday_groupby = weekday_groupby.reset_index()\n\nweekday_TV_shows = weekday_groupby[weekday_groupby[\"is_TV_show\"] == True]\nweekday_films = weekday_groupby[weekday_groupby[\"is_TV_show\"] == False]\n\nweekday_names =[\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"]\n\nfig, ax = plt.subplots(figsize=(8,4))\n\nax.bar( weekday_names, weekday_TV_shows[\"duration\"], label=\"TV shows\" )\nax.bar( weekday_names, weekday_films[\"duration\"], bottom = weekday_TV_shows[\"duration\"], label = \"Films\" )\nax.set_ylabel(\"Watched hours\")\nplt.title('Distribution of watched hours during the each day of the week')\nax.legend()\nplt.show()","697338d0":"print(\"Watched hours: {:.2f}\".format(netflix_hist[\"duration\"].sum()\/60))\nprint(\"Watched days: {:.2f}\".format(netflix_hist[\"duration\"].sum()\/60\/24))\nprint(\"Watched months: {:.2f}\".format(netflix_hist[\"duration\"].sum()\/60\/24\/30))\n\nprint(\"% of time spent in 5 years: {:.2f}\".format(netflix_hist[\"duration\"].sum()\/60\/24\/30 \/ (12*5) * 100))","a9a0797b":"# Monthly and daily comparisons\n\nIn the first plot below, we can see the number of hours watching Netflix for each month. Note that these values are the sum across the years. January is the month with more watched hours probably because of the boom in the first year (when I discovered Netflix).\n\nThis plot answers the question: Which month do I spend more time on Netflix?\n\nIn the second plot, a similar plot is obtained in this case for each day of the week. It is interesting to see that I tend to watch more films at the weekend and that my consumption is greater at the weekend comparing to midweek, as someone would expect.\n\nThis plot answers the question: Which day of the week do I watch more Netflix?\n\nIt is interesting to see these plots and it makes me wander. Why October is the month with less watched hours? Probably because partial exams at the winter semester at University when I am more focused on getting good marks this year. ","78076a76":"We join the information obtained from TV shows to the whole dataframe. To do so, a for loop has been used matching each episode in the series_with_duration dataframe with the Title. Could I avoid using the for loop and implement something more direct?  \n\nAnd we also prepare the data for a webscraping for the duration in this case of each film (or better to be said for each instance in the dataframe that was considered to be a TV show). ","b95c5745":"# Viewing History\n\nIn the first plot below, you can see the evolution across each month in every year of my Netflix viewing history. It seems that my consumption has decreased each year. \n\nThis first plot answers the question: Has my consumption decreased?\n\nOn the other hand, in the second plot plot below, note that most of the watched hours during the first years were mostly coming from TV shows and that my consumption now comes mostly from films and TV shows. ","8dff34e9":"# Comparison between TV shows and films\n\nA comparison between the number of episodes and films is carried out. ","91cd7a23":"# Cleaning the data\n\nAs you can observe, in the column Date there are 5475 non-null entries and in Title there are 5412 non-null entries, what suggests that there are null Titles. So before treating the data, we must clean it. As well as to drop duplicates.\n\nFinally, we can see the first 5 rows of my Netflix Viewing History.","46aef93c":"# Webscraping to get average duration (TV show)\n\nIn this part, we will do some Webscraping to get the average duration of an episode for each TV show. The TV shows found are shown above (with a total of 59 TV shows to look for).\n\nThe Webscraping is done in Google Search and if a solution cannot be found we will assume that the episode duration for this TV show is 40 minutes. \n\nAs in Kaggle I cannot web scrape the function is left uncommented. The average duration is used. ","0eb6f967":"## TV shows watched in 2016\n\nOnce we have identified TV shows, we can select some of them (Breaking Bad, Suits, Blue Mountain State and The Walking Dead) and see the number of episodes watched for each during each week of 2016 (going from week 1 to 53).","156cf243":"# Web scraping to get duration (films) \n\nAs we have 603 films, the webscraping for all of them takes so much time, so instead we will only take a sample and assign a default value of 100 min for the ones where a solution could not be found or for the ones we didn't web scrape.\n\nNote that the sample method, does a random sample without replacement, so we can find duplicates (which is not ideal). Another function here will suit this problem better and a more efficient way to webscraping could be developed in order to decrease the computational time.  \n\nIn the final step we join the information with the whole dataframe. \n\nSame thing happens with web scrape, so we assign default values for the duration.","2eb3bf71":"# Distinguish TV shows from films\n\nNext step is to determine which instances in the dataframe correspond to TV shows and which to films storing this information in a new column with bool. Normally, the Title of a TV show contains words such as Season, Temporada, Cap\u00edtulo, Episode, Parte... ","7d8907e4":"# Importing packages and data\n\nFirst of all, the packages and the csv file containing the information about my Viewing History are imported. ","7c96dc8d":"## Most watched TV shows \n\nThe plots below show the 20 most watched TV shows with the duration in hours for each of them and the 11 most watched TV shows in a pie chart with a percentage of the hours spent watching TV shows.","284fe89c":"# Netflix Viewing History\n### Developed by: Carlos P\u00e9rez Ricardo (18\/12\/2020)\n\nThis project consists on a study of my viewing activity history on Netflix (to get this information you can follow instructions in https:\/\/help.netflix.com\/es-es\/node\/101917). \n\nThe aim of this analysis is to answer some questions:\n* How many hours of content have I watched? \n* Has my consumption decreased? \n* Which day of the week do I tend to watch more Netflix?\n* Which month do I spend more time on Netflix? \n* Which TV show have I watched more episodes? \n\nTo do so, I used Python with the Data Science packages and some webscraping to get the duration of the films and TV shows. And finally I've been able to obtain some plots and graphs to show the insights I found. \n\nIt is surprising to see the amount hours watching Netflix content and the insights found carrying out this analysis. \n\nI am open to any feedback and throughout the analysis I pin pointed some actions and functions that could have been improved, so contact me!","06b46e50":"# Conclusions\n\nAll in all, the results achieved are satisfactory and even though some of them were as expected, it is interesting to see the real numbers and the insights this data provides.  It is suprising to see the amount of hours spent watching films that translate to the crazy amount 6 months of time (of my life). Even though, it is onlt 10.60% of my time during these years.\n\nI have to say that didn't end all of the episodes and films I watched. However, I also have to admit that this is only on Netflix, and I also have watched series and films in Amazon Prime and some in HBO as wella as watched many Youtube Videos. So the results here made me conscious about the actual time spent on entertainment. \n\n# Next Steps \n\nThe next steps can be:\n* Create a website with Flask (for example) so anyone can enter their Netflix Viewing History and see their viewing stats. (on my to-do list)\n\n* Improve the web scraping function in order to be less time consuming.\n\n* Make a recommendation engine using your information and getting other titles information from Netflix API or databases\/datasets. \n"}}