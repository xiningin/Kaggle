{"cell_type":{"ea8aad43":"code","ada0a20d":"code","8d301a18":"code","9e618578":"code","de0e63c4":"code","520c3be3":"code","7d5c3c5f":"code","9d953fea":"code","1bde94e8":"code","609db9f1":"code","2343d0d8":"code","de9bd6ba":"code","4eb2689a":"code","ea477048":"code","169e4954":"code","1112e3fc":"code","c056af11":"code","255f17d9":"code","53c46ef1":"code","724f8bd7":"code","7519b7a4":"code","d5b3f485":"code","e7e29737":"code","b5825457":"code","5126ca01":"code","1586f5c7":"code","c5a9e1e5":"code","8b932797":"code","7bdf2de8":"code","67571d9f":"code","a1465328":"code","f916f9d0":"code","6a9ff665":"code","5d765db4":"code","beade231":"code","97c826d1":"code","548ac436":"code","3e1e60b4":"code","881a7568":"code","9a0d97d6":"code","ca7d69f2":"markdown","386fb3ff":"markdown","e25b844d":"markdown","a9b2348a":"markdown","ae8f4812":"markdown","42d2b9e1":"markdown","ff25dd3f":"markdown","1f162406":"markdown","b3676fe5":"markdown","bb048873":"markdown"},"source":{"ea8aad43":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle \nimport json\nimport os\nfrom pathlib import Path\nfrom IPython.display import Image, Audio\nfrom music21 import note , chord , stream , instrument , converter   \n# from midi2audio import FluidSynth          # to convert midi to wav file","ada0a20d":"# parse the encoded data in a file object to midi stream\nmidi = converter.parse('..\/input\/classical-piano-midi-music\/midi_songs\/rufus.mid')\ntype(midi)","8d301a18":"print(\"Now playing a sample Audio file from dataset....\")\nAudio('..\/input\/sample-input-wav-file\/rufus.wav') ","9e618578":"midi.show('text')","de0e63c4":"# Flat all the elements - notes\/chords\nnotes_to_parse = midi.flat.notes\nprint(len(notes_to_parse))","520c3be3":"for element in notes_to_parse[:100]:\n    print(element , element.offset)   # Offset refers to where the note is located in the piece","7d5c3c5f":"notes_to_parse[0]","9d953fea":"# Pitch refers to the frequency of the sound, or how high or low a particular note is \n# and is represented with the letters [A, B, C, D, E, F, G], with A being the highest and G being the lowest\nnotes_to_parse[0].pitch , str(notes_to_parse[0].pitch)","1bde94e8":"notes_to_parse[50]","609db9f1":"# Return the normal order\/normal form of the Chord as a integer representation\nnotes_to_parse[50].normalOrder","2343d0d8":"notes_demo = []\n\nfor element in notes_to_parse:\n    \n    # if the element is a Note , then store it's Pitch\n    if isinstance(element , note.Note):\n        notes_demo.append(str(element.pitch))\n        \n    # if the element is a Chord , split each of the note of the chord and join them with +\n    elif isinstance(element , chord.Chord):\n        notes_demo.append('+'.join(str(n) for n in element.normalOrder))","de9bd6ba":"len(notes_demo)","4eb2689a":"print(notes_demo[32:50])","ea477048":"# Get all the notes and chords from the midi files in the .\/midi_songs directory \nnotes = []\np = Path(\"..\/input\/classical-piano-midi-music\/midi_songs\")\n\nfor file in p.glob(\"*.mid\"):\n    midi = converter.parse(file)\n    # print(f\"parsing {file}\" , end = \"  \")\n    \n    elements_to_parse = midi.flat.notes\n    # print(f\"length {len(elements_to_parse)}\")\n    \n    for element in elements_to_parse:\n        \n        # if the element is a Note, then store it's Pitch\n        if isinstance(element , note.Note):\n            notes.append(str(element.pitch))\n            \n        # if the element is a Chord , then split each of the note and join with +\n        elif isinstance(element , chord.Chord):\n            notes.append(\"+\".join(str(n) for n in element.normalOrder))","169e4954":"with open(\".\/notes\" , \"wb\") as file:\n    pickle.dump(notes , file)","1112e3fc":"with open(\"..\/input\/notes-corpus\/notes\" , \"rb\") as file:\n    notes = pickle.load(file)","c056af11":"print(\"Total notes: \" , len(notes))\nprint(\"Unique notes: \" , len(set(notes)))","255f17d9":"n_vocab = len(set(notes))","53c46ef1":"# get all pitch names (unique classes)\npitchnames = sorted(set(notes))\n\n# create a dictionary to map pitches to integers\nnote_to_int = dict((element , idx) for idx , element in enumerate(pitchnames))\n\n# create a reverse mapping\nint_to_note = {idx:element for element , idx in note_to_int.items()}\n\nassert len(note_to_int) == n_vocab","724f8bd7":"# How many elements LSTM input should consider\nsequence_len = 100","7519b7a4":"network_input = []     # input sequence data\nnetwork_output = []    # output data\n\nfor i in range(len(notes) - sequence_len):\n    seq_in = notes[i : i+sequence_len]         # contains 100 values\n    seq_out = notes[i+sequence_len]\n    \n    network_input.append([note_to_int[n] for n in seq_in])\n    network_output.append(note_to_int[seq_out])","d5b3f485":"len(network_input) , len(network_output)","e7e29737":"np.asarray(network_input).shape","b5825457":"# reshape input data into a shape compatible with LSTM layers\nnormalised_network_input = np.reshape(network_input , (*(np.asarray(network_input).shape) , 1))  # input_samples, sequence_len, 1\nprint(normalised_network_input.shape)","5126ca01":"normalised_network_input = normalised_network_input\/float(n_vocab)","1586f5c7":"# Network output are the classes, so encode into one hot vector\nfrom tensorflow.keras.utils import to_categorical\nnetwork_output = to_categorical(network_output)","c5a9e1e5":"print(normalised_network_input.shape)\nprint(network_output.shape)","8b932797":"from tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping","7bdf2de8":"model = Sequential()\nmodel.add(LSTM(units = 512 , input_shape = (normalised_network_input.shape[1], normalised_network_input.shape[2])\n               , return_sequences = True))\nmodel.add(Dropout(0.3))\n\nmodel.add(LSTM(units = 512 , return_sequences = True))\nmodel.add(Dropout(0.3))\n\nmodel.add(LSTM(units = 512))\nmodel.add(Dense(256))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(n_vocab , activation = 'softmax'))","67571d9f":"model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\")","a1465328":"model.summary()","f916f9d0":"checkpoint = ModelCheckpoint(\"weights.h5\", monitor = 'loss', save_best_only=True, mode = 'min')\nhist = model.fit(normalised_network_input, network_output, epochs = 100, batch_size = 64, callbacks = [checkpoint])","6a9ff665":"# Plotting the loss curve\nhistory_df = pd.DataFrame(hist.history)\nfig = plt.figure(figsize = (10,4))\nplt.plot(history_df.index.to_list(), history_df[\"loss\"], linewidth=2)\nplt.ylabel(\"Training Loss\" , fontsize = 12)\nplt.xlabel(\"Epochs\" , fontsize = 12)\nplt.title(\"Loss Function over Time\", fontsize=14, fontstyle='italic')\nplt.savefig('learning_curve.png')\nplt.show()","5d765db4":"\"\"\" Generate notes from the neural network based on a sequence of notes\"\"\"\n\n# pick a random sequence from the input as a starting point for the prediction\nstart = np.random.randint(0 , len(network_input)-1)\n# inital sequence\/pattern\npattern = network_input[start]    # 100\n\npredicted_outputs = []\n\n# generate 500 notes\nfor note_index in range(500):\n    inp_seq = np.reshape(pattern , (1, len(pattern), 1))   # convert to desired input shape for model\n    inp_seq = inp_seq\/float(n_vocab)  # normalize\n    \n    prediction = model.predict(inp_seq)\n    pred_idx = np.argmax(prediction)\n    pred_note = int_to_note[pred_idx]\n    \n    predicted_outputs.append(pred_note)\n    \n    # remove the first note of the sequence and insert the output of the previous iteration at the end of the sequence\n    pattern.append(pred_idx)\n    pattern = pattern[1:]","beade231":"print(len(predicted_outputs))\nprint(predicted_outputs[:50])","97c826d1":"# convert the output predictions to notes \noffset = 0 \noutput_notes = []\n\nfor pattern in predicted_outputs:\n    \n    # if the pattern is a chord, first split the string up into an array of notes\n    if ('+' in pattern) or pattern.isdigit():\n        notes_in_chord = pattern.split('+')\n        \n        # Then we loop through the string representation of each note and create a Note object for each of them\n        notes_tmp = []\n        for current_note in notes_in_chord:\n            new_note = note.Note(int(current_note))         \n            new_note.storedInstrument = instrument.Piano()\n            notes_tmp.append(new_note)\n            \n        new_chord = chord.Chord(notes_tmp)   # create Chords from list of notes(strings of pitch names)\n        new_chord.offset = offset\n        output_notes.append(new_chord)\n    \n    # if pattern is a Note, create a Note object using string representation of the pitch contained in the predicted pattern\n    else:\n        new_note = note.Note(pattern)\n        new_note.offset = offset\n        new_note.storedInstrument = instrument.Piano()\n        output_notes.append(new_note) \n        \n    offset += 0.5","548ac436":"# create a midi stream object from the generated notes \n\nmidi_stream = stream.Stream(output_notes)\nmidi_stream.write(fmt = 'midi', fp = 'test_output_stream.mid')","3e1e60b4":"print(len(output_notes))","881a7568":"output_notes[:len(output_notes):20]","9a0d97d6":"print(\"Now playing generated music..\")\nAudio(\"..\/input\/sample-output-wav-file\/test_output_stream.wav\") ","ca7d69f2":"#### PRE-PROCESSING ALL FILES","386fb3ff":"#### PREPARE SEQUENTIAL DATA FOR LSTM","e25b844d":"#### PREDICTIONS","a9b2348a":"Seems like we can't play a midi file on the Kaggle interface, so I have explicitly created a \".wav\" audio format of a sample '.mid' file outside this notebook in order to create an audio interface. ","ae8f4812":"#### CREATE MUSIC FILES","42d2b9e1":"#### Important Points to Note :\n- We have taken all the files and combined them into one single file. So the model does not know how a song ends and how a song starts. It randomly picks notes and does not know that the ending of the song should be soft or the starting should be soft.\n\n- We are taking offset as 0.5 always. So, the implementation we have at the moment does not support varying duration of notes and different offsets between notes.\n\n- The instrument considered in this project is Piano. The project can be extended to more instruments like Guitar, Violin and so on.","ff25dd3f":"#### IMPORT REQUIRED LIBRARIES","1f162406":"#### DEFINE MODEL ARCHITECTURE","b3676fe5":"#### READ MIDI FILES","bb048873":"#### TASK\nTrain a LSTM neural network to generate midi music files that make use of piano as instrument."}}