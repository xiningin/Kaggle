{"cell_type":{"23d6d962":"code","a164e7b9":"code","f5ced77b":"code","540f641e":"code","3397f789":"code","c65ce394":"code","335261bd":"code","baf15d1e":"code","de221777":"code","566ec963":"code","d5df5554":"code","817b77af":"code","324fc69b":"code","b2a2b21f":"code","4e6b9ebe":"code","d63487fd":"code","6ff7832c":"markdown","12da31b8":"markdown","0db85624":"markdown","6923d994":"markdown","ba03af0b":"markdown","d355daae":"markdown","fcaf312c":"markdown","01614a9a":"markdown","f9185732":"markdown"},"source":{"23d6d962":"import pandas as pd\nimport numpy as np\nfrom bs4 import BeautifulSoup\n\nfrom IPython.display import HTML\n\nimport matplotlib.pyplot as plt\n\nfrom nltk.corpus import stopwords\nimport gensim\n\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import FastText\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom functools import partial\nimport random\n\nfrom ipywidgets import interact\n\npd.set_option('display.max_colwidth', -1)","a164e7b9":"!ls '..\/input'","f5ced77b":"def get_text(text):\n    try:\n        soup = BeautifulSoup(text, 'lxml')\n        return soup.get_text()\n    except Exception as e:\n        print(text)\n        raise e","540f641e":"questions = pd.read_csv('..\/input\/questions.csv', parse_dates=['questions_date_added'])\nquestions['questions_body'] = questions['questions_body'].apply(get_text)\n\ndisplay(questions.head(5))","3397f789":"stopword = stopwords.words('english')\n\nquestions['text'] = questions['questions_title'] + questions['questions_body']\nquestions['text_list'] = questions['text'].apply(simple_preprocess)\nquestions['text'] = questions['text_list'].apply(lambda x: ' '.join(x))\n\ndisplay(questions.head(3))","c65ce394":"emb_size = 100\nmodel_question = FastText(questions['text_list'], size=emb_size, window = 6, sg=1, workers=4)\nmodel_question.train(questions['text_list'], total_examples=len(questions.index), epochs=50)","335261bd":"vect_question = TfidfVectorizer(min_df=model_question.vocabulary.min_count)\ntfidf_question = vect_question.fit_transform(questions['text'])","baf15d1e":"def get_sentence_embedding(m, tfidf, vectorizer, emb_size=100):\n    wordvecs = np.zeros((emb_size, tfidf.shape[-1]))\n    for i, name in enumerate(vectorizer.get_feature_names()):\n        wordvecs[:, i] = m.wv[name]\n\n    emb = tfidf @ wordvecs.T\n    emb = emb \/ (tfidf.sum(axis=1) + 1e-10)\n    \n    return emb","de221777":"sen_emb = get_sentence_embedding(model_question, tfidf_question, vect_question, 100)","566ec963":"@interact\ndef get_similar_question(x=200):\n    nn = NearestNeighbors(n_neighbors=6, metric='cosine')\n    nn.fit(sen_emb)\n    dist, idxs = nn.kneighbors(sen_emb[x])\n\n    sim_questions = questions.loc[idxs[0], ['questions_id', 'questions_author_id', 'questions_title', 'questions_body']]\n    sim_questions['Score'] = dist[0]\n\n    #display(HTML('Similar questions (actual question on top):'))\n    #display(sim_questions)\n    return sim_questions","d5df5554":"profs = pd.read_csv('..\/input\/professionals.csv')\ntag_users = pd.read_csv('..\/input\/tag_users.csv')\ntags = pd.read_csv('..\/input\/tags.csv')\ntags['tags_tag_name'] = tags['tags_tag_name'].fillna(' ').apply(get_text)\n\ntag_users = tag_users.merge(tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\ntag_users['tags_tag_name'] += ' '\ntag_users = tag_users.groupby('tag_users_user_id')['tags_tag_name'].sum().to_frame()\n\ngroup_memberships = pd.read_csv('..\/input\/group_memberships.csv')\ngroups = pd.read_csv('..\/input\/groups.csv')\n\ngroup_memberships = group_memberships.merge(groups, left_on='group_memberships_group_id', right_on='groups_id')\ngroup_memberships['groups_group_type'] += ' '\ngroup_memberships = group_memberships.groupby('group_memberships_user_id')['groups_group_type'].sum().to_frame()\n\nprofs = profs.merge(tag_users, left_on='professionals_id', right_on='tag_users_user_id')\nprofs = profs.merge(group_memberships, left_on='professionals_id', right_on='group_memberships_user_id')\nprofs['info'] = profs['tags_tag_name'] + ' ' + profs['groups_group_type']\nprofs = profs.drop(['professionals_location', 'professionals_date_joined'], axis=1)\n\nprofs['info_list'] = profs['info'].apply(simple_preprocess)\nprofs['info'] = profs['info_list'].apply(lambda x: ' '.join(x))\n\ndisplay(profs.sample(5))","817b77af":"#model_profs = FastText(profs['info_list'], size=emb_size, window=8, sg=1, workers=4)\n#model_profs.train(profs['info_list'], total_examples=len(profs.index), epochs=50)\n\n#vect_profs = TfidfVectorizer(min_df=model_profs.vocabulary.min_count)\ntfidf_profs = vect_question.transform(profs['info'])","324fc69b":"prof_emb = get_sentence_embedding(model_question, tfidf_profs, vect_question, 100)","b2a2b21f":"@interact\ndef get_closest_professional(x=200):\n    nn = NearestNeighbors(n_neighbors=6, metric='cosine')\n    nn.fit(prof_emb)\n    dist, idxs = nn.kneighbors(sen_emb[x])\n\n    question = questions.loc[x, ['questions_id', 'questions_author_id', 'questions_title', 'questions_body']].to_frame()\n    display(question)\n    \n    closest_profs = profs.loc[idxs[0], ['professionals_id', 'professionals_industry', 'professionals_headline', 'info']]\n    closest_profs['Score'] = dist[0]\n\n    return closest_profs","4e6b9ebe":"answers = pd.read_csv('..\/input\/answers.csv', parse_dates=['answers_date_added'])\nanswer_score = pd.read_csv('..\/input\/answer_scores.csv')\n\nanswers = answers.dropna(subset=['answers_body'])\nanswers['answers_body'] = answers['answers_body'].apply(get_text)\n\nanswers = answers.merge(answer_score, left_on='answers_id', right_on='id')\nanswers = answers.loc[answers['score'] > 0]\nanswers = answers.merge(profs.reset_index(), left_on='answers_author_id', right_on='professionals_id')\nanswers = answers.merge(questions.reset_index(), left_on='answers_question_id', right_on='questions_id')\nanswers = answers.loc[:, ['answers_id', 'professionals_id', 'score', 'info', 'index_x', 'index_y', 'questions_body', 'text', 'text_list']]\n\ndisplay(answers.head(5))","d63487fd":"sen_emb[519].shape","6ff7832c":"<h2> Finding similar questions <\/h2>\n\nNow lets train embeddings on the questions. These embeddings allow us to find the most similar questions","12da31b8":"<h2> Evaluate our current method <\/h2>","0db85624":"The following files are available for task at hand:","6923d994":"It looks like we are able to find related professionals, but the model seems to have a preference for professionals with a lot of tags. Probably because the model is trained on questions instead of tags of questions.","ba03af0b":"Now that we have a first setup for our model, we have to find a way to evaluate our model. For the evaluation, we are going to look at the previously answered questions and check how the professional which answered the question related to the score our model gave.","d355daae":"<h2> Questions based on tags <\/h2>\nIn case we can't find a similar question, we have to look for a professional that can answer the question at hand. For this, we can create a training set based on previous question\/ answer pairs.","fcaf312c":"The main data source for this kernel is the questions and corresponding answers. So lets load these data sources. We will use beautifulsoup to filter out the HTML that is in some of the questions\/answers.","01614a9a":"Now we need to do some basic preprocessing. For now, we won't do anything complex but more might be added later.","f9185732":"In this kernel, we are going to try to provide the optimal professional given a specific question. In order to do so, we follow the same approach as done by [Antons Rubert](https:\/\/www.kaggle.com\/antonsruberts\/sentence-embeddings-centorid-method-vs-doc2vec). This means that we try to make embeddings on sentence level such that similar questions are close to each other.\n\nBut first lets start with having a look at the available data."}}