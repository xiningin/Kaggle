{"cell_type":{"92146f8c":"code","0b70119f":"code","22618b4e":"code","7e8eb291":"code","eb3befeb":"code","a8deead6":"code","12198b97":"code","5f67c1b5":"code","d0b92791":"code","e5eec6b4":"code","3779367c":"code","036fe0ba":"code","6a160a20":"code","4380b165":"code","17ab7a42":"code","8365fd3a":"code","eb8d3885":"code","d85ad71b":"code","38dacbd6":"code","25673c85":"code","dcde606c":"code","e0cc364b":"code","e0ab0366":"code","1040b6d9":"code","32e4584d":"code","9c930bb5":"code","9f8472ef":"code","e552ab60":"code","946cfab6":"code","eb8a4f6a":"code","9d40beb1":"code","2271d491":"code","d4e4621e":"code","344d4c65":"code","12057a82":"code","ab3486a5":"code","38439534":"code","890db1db":"code","867c9f64":"code","18b363ff":"markdown","2081e1de":"markdown","9326ba9a":"markdown","874ff63d":"markdown","f1b5eeb3":"markdown","3e69f84f":"markdown","854b799b":"markdown","96d31973":"markdown","79a18657":"markdown","e6edf03c":"markdown","f2d810b4":"markdown","a50fdb09":"markdown","4c32598a":"markdown","892a6eca":"markdown","066343e5":"markdown","74933cc2":"markdown","de3283bd":"markdown","da8bc5cc":"markdown","29c7579a":"markdown","f3b4c73b":"markdown","f03770ce":"markdown","9b8b907b":"markdown","325eb1f4":"markdown"},"source":{"92146f8c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split #to create validation data set\nfrom sklearn.cross_validation import KFold,cross_val_score\nfrom sklearn.metrics import confusion_matrix, recall_score,accuracy_score,f1_score, roc_curve, auc, classification_report\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import tree\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport graphviz \nimport itertools\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n","0b70119f":"payment = pd.read_csv(\"..\/input\/payment.csv\")","22618b4e":"payment.head(5)","7e8eb291":"payment.describe()","eb3befeb":"print(payment.keys())","a8deead6":"def nan_table(data):\n    print(pd.isnull(data).sum())\n\nnan_table(payment)","12198b97":"fig = plt.figure(figsize = (16,8))\nsns.barplot(x = \"step\", y = \"fraud\", data = payment)\nplt.title(\"Distribution of Fraud  based on step\")\nplt.show()","5f67c1b5":"fig = plt.figure(figsize = (16,6))\nsns.barplot(x = \"age\", y = \"fraud\", data = payment)\nplt.title(\"Distribution of Fraud based on Age\")\nplt.show()","d0b92791":"fig = plt.figure(figsize = (12,6))\nsns.barplot(x = \"gender\", y = \"fraud\", data = payment)\nplt.title(\"Distribution of Fraud based on Gender\")\nplt.show()","e5eec6b4":"fig = plt.figure(figsize = (16,6))\nsns.barplot(x=\"age\", y=\"fraud\", hue=\"gender\", data=payment)\nplt.title(\"Distribution of Fraud Based on Gender and Age\")\nplt.show()","3779367c":"fig = plt.figure(figsize = (16,6))\nsns.barplot(x = \"category\", y = \"fraud\", data = payment)\nplt.title(\"Distribution of Fraud based on Category\")\nplt.xticks(rotation = 90)\nplt.show()","036fe0ba":"fig = plt.figure(figsize = (16,6))\nsns.barplot(x = \"merchant\", y = \"fraud\", data = payment)\nplt.title(\"Distribution of Fraud based on Merchant\")\nplt.xticks(rotation = 90)\nplt.show()","6a160a20":"payment.dtypes\n","4380b165":"# build a function to check each feature's unique values\n# for the number of unique values is more than 20, we won't print each value\ncheck_cols = payment.select_dtypes(include = [\"object\"])\ndef check_uniques(data):\n    for col in check_cols:\n        if len(data[col].unique().tolist()) <= 20:\n            print(col, \" : \", len(data[col].unique().tolist()), \" : \", data[col].unique())\n        else:\n            print(col, \" : \", len(data[col].unique().tolist()))\n    \ncheck_uniques(payment)","17ab7a42":"# use the labelEncoder to convert category values into numerical values\n# need_trans_features = [\"gender\", \"age\", \"category\", \"merchant\"]\nlabelencoder_feature = LabelEncoder()\npayment[\"gender\"] = labelencoder_feature.fit_transform(payment[\"gender\"])\npayment[\"age\"] = labelencoder_feature.fit_transform(payment[\"age\"])\npayment[\"merchant\"] = labelencoder_feature.fit_transform(payment[\"merchant\"])\npayment[\"category\"] = labelencoder_feature.fit_transform(payment[\"category\"])","8365fd3a":"# check the changed data types\npayment.dtypes","eb8d3885":"# drop the columns of zipcodeOri and zipMerchant, because they have only one constant value\n# we also drop the column of \"customer\" which not used in our predictive model\npayment_copy = payment.copy()\npayment_copy.drop(labels = [\"customer\", \"zipcodeOri\", \"zipMerchant\"], axis = 1, inplace = True)","d85ad71b":"# transform the amount into category type,\n# 5 types\namount_rank = [\"vlow\",\n              \"low\",\n              \"mid\",\n              \"high\",\n              \"vhigh\"]\npayment_copy[\"amountRank\"] = pd.qcut(x = payment_copy[\"amount\"],q =len(amount_rank),labels = amount_rank)\npayment_copy[\"stepRank\"] = pd.qcut(x = payment_copy[\"step\"],q =len(amount_rank),labels = amount_rank)","38dacbd6":"# drop columns of step and amount\npayment_copy.drop(labels = [\"step\",\"amount\"], axis = 1, inplace = True)","25673c85":"# change the category datatype into int\n# vlow -- vhigh: 1-5\nrank_dict = {\"vlow\":1, \"low\":2, \"mid\":3, \"high\":4, \"vhigh\":5}\npayment_copy[\"stepRank\"] = payment_copy[\"stepRank\"].map(rank_dict)\npayment_copy[\"amountRank\"] = payment_copy[\"amountRank\"].map(rank_dict)\n\npayment_copy.head()","dcde606c":"# To decide which features of the data to include in our predictive churn model\n# We'll examine the correlation between churn and each customer feature\n\ncorr = payment_copy.corr()\nsns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot = True, annot_kws={'size':12})\nheat_map=plt.gcf()\nheat_map.set_size_inches(16,10)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","e0cc364b":"# Display the distribution of the sample\nfig = plt.figure(figsize=(12, 4))\ncount_fraud = pd.value_counts(payment[\"fraud\"], sort = True).sort_index()\nprint(count_fraud)\ncount_fraud.plot(kind = \"bar\")\nplt.title(\"Fraud histogram\")\nplt.xlabel(\"Fraud\")\nplt.ylabel(\"Frequency\")\nplt.show()","e0ab0366":"# Split the training and testing dataset\nfeatures = [\"age\", \"gender\", \"category\", \"stepRank\", \"amountRank\", \"merchant\"]\n\nx_train = payment_copy[features]\ny_train = payment_copy[\"fraud\"]\nprint(type(x_train))\n# Standardized the dataset\nscaler = StandardScaler()\n# X = x_train.as_matrix().astype(np.float)\nx_train = scaler.fit_transform(x_train)\nprint(\"after 1: \", type(x_train))\nx_train = pd.DataFrame(x_train)\nprint(\"after 2: \", type(x_train))\n#X_valid and y_valid are the validation sets\nx_training, x_valid, y_training, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=0) ","1040b6d9":"# fold has 5 pieces dataset, 1 piece for validation and 4 pieces for training\nfold = KFold(x_training.shape[0], 5, shuffle = False)","32e4584d":"# build a function to plot the confusion matrix\ndef plot_confusion_matrix(confusion_matrix):\n    confusion_matrix_df = pd.DataFrame(confusion_matrix, ('No Fraud', 'Fraud'), ('No Fraud', 'Fraud'))\n    heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 10}, fmt=\"d\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 12)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 12)\n#     plt.ylabel('True label', fontsize = 10)\n#     plt.xlabel('Predicted label', fontsize = 10)","9c930bb5":"# build a function to print the scores after running servral rounds of cross validation\ndef print_scores_under_cv(model_name):\n    recall_accs = []\n    accuracy_accs = []\n    f1_accs = []\n    auc_accs = []\n    for train,test in fold:\n        # train is the indices of training dataset of x_training \n        model_name.fit(x_training.iloc[train,:], y_training.iloc[train])\n        # predict values using the test indices in the training data\n        y_pred = model_name.predict(x_training.iloc[test,:])      \n        # calculate the recall and accuracy score and append it to a list \n        # for recall and accuracy scores representing the current C parameter\n        recall_acc = recall_score(y_training.iloc[test], y_pred)\n        accuracy_acc = accuracy_score(y_training.iloc[test], y_pred)\n        f1_acc = f1_score(y_training.iloc[test], y_pred)\n        \n        fpr, tpr, thresholds = roc_curve(y_training.iloc[test], y_pred)\n        auc_acc = auc(fpr, tpr)\n        recall_accs.append(recall_acc)\n        accuracy_accs.append(accuracy_acc)\n        f1_accs.append(f1_acc)\n        auc_accs.append(auc_acc)\n        \n    recall_acc_mean = np.mean(recall_accs)\n    accuracy_acc_mean = np.mean(accuracy_accs)\n    f1_acc_mean = np.mean(f1_accs)\n    auc_acc_mean = np.mean(auc_accs)\n    print(\"\")\n    print(\"Mean of the recall score is: \", recall_acc_mean)\n    print(\"Mean of the accuracy score is: \", accuracy_acc_mean)\n    print(\"Mean of the f1 score is: \", f1_acc_mean)\n    print(\"Mean of the auc score is: \", auc_acc_mean)\n    print(\"\")","9f8472ef":"# print the classifier report for each model\ndef classifier_report(model_name, x_train, y_train, x_test, y_test):\n    model_name.fit(x_train, y_train)\n    y_pred = model_name.predict(x_test)\n    print(classification_report(y_test, y_pred))","e552ab60":"# Check the Performances of different models under cross validation on imbalnaced dataset\n\nlr = LogisticRegression()\ndt = tree.DecisionTreeClassifier()\nrf = RandomForestClassifier()\nprint(\"Logistic Regression Performance: \")\nprint_scores_under_cv(lr)\nprint(\"\")\nprint(\"Decision Tree Performance: \")\nprint_scores_under_cv(dt)\nprint(\"\")\nprint(\"Random Forest Performance: \")\nprint_scores_under_cv(rf)\nprint(\"\")\n    ","946cfab6":"# Check the classification report of different models\nprint(\"Logistic Regression: \")\nclassifier_report(lr, x_training, y_training, x_valid, y_valid)\nprint(\"Decision Tree: \")\nclassifier_report(dt, x_training, y_training, x_valid, y_valid)\nprint(\"Random Forest: \")\nclassifier_report(rf, x_training, y_training, x_valid, y_valid)","eb8a4f6a":"# Since we have more than 580000 of instances, we choose to under-sampling the dataset\n \ndata_majority = payment_copy[payment_copy[\"fraud\"]==0]\ndata_minority = payment_copy[payment_copy[\"fraud\"]==1]\n# len(data_minority)\ndata_majority_undersampled = resample(data_majority,\n                                      replace = True,\n                                      n_samples = len(data_minority), # same number of samples as minority classe\n                                      random_state = 1) # set the seed for random resampling\n\n# Combine resampled results\ndata_undersampled = pd.concat([data_minority, data_majority_undersampled])\n \ndata_undersampled[\"fraud\"].value_counts()","9d40beb1":"# Now that we have a 1:1 ratio for our classes, let\u2019s train another logistic regression model\n# Split the training and testing dataset\nfeatures = [\"age\", \"gender\", \"category\", \"stepRank\", \"amountRank\", \"merchant\"]\n\nx_train_undersampled = data_undersampled[features]\ny_train_undersampled = data_undersampled[\"fraud\"]\n\n# Standardized the dataset\nscaler = StandardScaler()\n# X = x_train.as_matrix().astype(np.float)\nx_train_undersampled = scaler.fit_transform(x_train_undersampled)\nx_train_undersampled = pd.DataFrame(x_train_undersampled)\n\n#X_valid and y_valid are the validation sets\nx_training_undersampled, x_valid_undersampled, y_training_undersampled, y_valid_undersampled = train_test_split(x_train_undersampled,\n                                                                                                                y_train_undersampled, \n                                                                                                                test_size=0.2, \n                                                                                                                random_state=0) \n ","2271d491":"fold_undersampled = KFold(x_training_undersampled.shape[0], n_folds = 5, shuffle=False)","d4e4621e":"# build a function to print the scores after running servral rounds of cross validation based resampled dataset\ndef print_scores_under_cv_resample(model_name):\n    recall_accs = []\n    accuracy_accs = []\n    f1_accs = []\n    auc_accs = []\n    for train,test in fold_undersampled:\n        # train is the indices of training dataset of x_training \n        model_name.fit(x_training_undersampled.iloc[train,:], y_training_undersampled.iloc[train])\n        # predict values using the test indices in the training data\n        y_pred = model_name.predict(x_training_undersampled.iloc[test,:])      \n        # calculate the recall and accuracy score and append it to a list \n        # for recall and accuracy scores representing the current C parameter\n        recall_acc = recall_score(y_training_undersampled.iloc[test], y_pred)\n        accuracy_acc = accuracy_score(y_training_undersampled.iloc[test], y_pred)\n        f1_acc = f1_score(y_training_undersampled.iloc[test], y_pred)\n        \n        fpr, tpr, thresholds = roc_curve(y_training_undersampled.iloc[test], y_pred)\n        auc_acc = auc(fpr, tpr)\n        recall_accs.append(recall_acc)\n        accuracy_accs.append(accuracy_acc)\n        f1_accs.append(f1_acc)\n        auc_accs.append(auc_acc)\n        \n    recall_acc_mean = np.mean(recall_accs)\n    accuracy_acc_mean = np.mean(accuracy_accs)\n    f1_acc_mean = np.mean(f1_accs)\n    auc_acc_mean = np.mean(auc_accs)\n    print(\"\")\n    print(\"Mean of the recall score is: \", recall_acc_mean)\n    print(\"Mean of the accuracy score is: \", accuracy_acc_mean)\n    print(\"Mean of the f1 score is: \", f1_acc_mean)\n    print(\"Mean of the auc score is: \", auc_acc_mean)\n    print(\"\")","344d4c65":"# Check the Performances of different models under cross validation\n\nlr_under = LogisticRegression()\ndt_under = tree.DecisionTreeClassifier()\nrf_under = RandomForestClassifier()\nprint(\"Logistic Regression Performance Based On Undersampled Dataset: \")\nprint_scores_under_cv_resample(lr_under)\nprint(\"\")\nprint(\"Decision Tree Performance Based On Undersampled Dataset: \")\nprint_scores_under_cv_resample(dt_under)\nprint(\"\")\nprint(\"Random Forest Performance Based On Undersampled Dataset: \")\nprint_scores_under_cv_resample(rf_under)\nprint(\"\")","12057a82":"# check the classification reports\nprint(\"LR on undersampled test data: \")\nclassifier_report(lr_under, x_training_undersampled, y_training_undersampled, x_valid_undersampled, y_valid_undersampled)\nprint(\"LR on test data: \")\nclassifier_report(lr_under, x_training, y_training, x_valid, y_valid)\nprint(\"DT on undersampled test data: \")\nclassifier_report(dt_under, x_training_undersampled, y_training_undersampled, x_valid_undersampled, y_valid_undersampled)\nprint(\"DT on test data: \")\nclassifier_report(dt_under, x_training, y_training, x_valid, y_valid)\nprint(\"RF on undersampled test data: \")\nclassifier_report(rf_under, x_training_undersampled, y_training_undersampled, x_valid_undersampled, y_valid_undersampled)\nprint(\"RF on test data: \")\nclassifier_report(rf_under, x_training, y_training, x_valid, y_valid)","ab3486a5":"lr_penalized = LogisticRegression(penalty = \"l1\", C = 0.1, class_weight=\"balanced\")\nprint(\"Penalized Logistic Regression Performance: \")\nprint_scores_under_cv(lr_penalized)\nclassifier_report(lr_penalized, x_training, y_training, x_valid, y_valid)","38439534":"# build a function to find the best parameters for improving the performance of random forest\nsample_leaves = [10, 20, 30, 40, 50, 60]\nfor sample_leaf in sample_leaves:\n    RF = RandomForestClassifier(min_samples_leaf=sample_leaf, class_weight=\"balanced\")\n    print(\"min_sample_leaf = \", sample_leaf)\n    print_scores_under_cv(RF)\n    classifier_report(RF, x_training, y_training, x_valid, y_valid)\n    print(\"\")","890db1db":"plt.figure(figsize = (14,10))\nplt.ylabel('True label', fontsize = 10)\nplt.xlabel('Predicted label', fontsize = 10)\nj = 1\nfor sample_leaf in sample_leaves:\n    RF_pruned = RandomForestClassifier(min_samples_leaf=sample_leaf, class_weight=\"balanced\")\n    RF_pruned.fit(x_training, y_training)\n    y_pred_RF_pruned = RF_pruned.predict(x_valid)\n    plt.subplot(3,2,j)\n    plt.title(\"min_samples_leaf = \"+ str(sample_leaf))\n    j = j + 1\n    confusion_matrix_RF_pruned = confusion_matrix(y_valid, y_pred_RF_pruned)\n    plot_confusion_matrix(confusion_matrix_RF_pruned)","867c9f64":"# the best model we choose \nRF_Model = RandomForestClassifier(min_samples_leaf=60, class_weight=\"balanced\")","18b363ff":"### 3. Dealing with missing values","2081e1de":"### 2. Loading and Viewing Data Set","9326ba9a":"### Try Penalized Models","874ff63d":"### 4. Plotting and Visulizing Data","f1b5eeb3":"****Correlations Analysis****","3e69f84f":"The performance of penalized Logistic Regression is better than regular Logistic Regression\n\nAccording to the performances of models, we will choose the algorithm of Random Forest for our predictive model. Then I will try to tune the parameters to imporve the performance of the model.","854b799b":"#### Distribution of Fraud based on Category","96d31973":"#### Distribution of Fraud based on Step","79a18657":"### 6. Model Fitting, Optimizing, and Predicting","e6edf03c":"#### Distribution of Fraud based on Age","f2d810b4":"#### Distribution of Fraud based on Gender","a50fdb09":"#### Distribution of Fraud base on Gender and Age","4c32598a":"#### Distribution of Fraud based on Merchant","892a6eca":"- Too many Object datatype, should be transformed\n- Drop the features not used in predictve models(has only constanct values)","066343e5":"****Avoiding multicollinearity****\n\nThere are no features highly correlated.\n\nWe try to avoid strongly correlated explanatory variables in regression models. Correlation of explanatory variables is known as multicollinearity, and perfect multicollinearity occurs when the correlation between two independent variables is euqal to 1 or -1.\n\nIf there are fetures highly correlate, to avoid unstable estimates of coefficients in our models, we will drop the some variable, as it is highly correlated to other variables.\n\n![](http:\/\/)**In this case, we don't need to drop any variable.**","74933cc2":"**1. zipcodeOri and zipMerchant have only one constant  value, should be dropped **\n\n**2. customer has more than 4000 unique values, should be dropped**\n\n**3. amount and step should be created higher level of features **\n\n**4. age, gender, category and merchant should be changed from object into binary **\n","de3283bd":"### 1. Importing Libraries and Packages","da8bc5cc":"****Resampling the Dataset****\n\n- Undersample the dataset\n- Compare the performances of different models","29c7579a":"**The performances of models on the training dataset of undersampling are much better than imbalanced dataset, but the performances for testing data are not as good as for the training dataset.**","f3b4c73b":"### 5. Feature Engineering","f03770ce":"#### The dataset is imbalanced. In the model fitting should balance the weight of fraud.\n\n****Taticts to Combat the Imbalanced Training Data****\n\n1. Try Changing Your Performance Metric\n - Confusion Matrix\n - Precision(accuracy)\n - Sensitivity(recall)\n - F1-Score(or F-Score)\n - ROC Curve\n - AUC\n2. Try Resampling the Dataset\n - Over-sampling\n - Under-sampling\n3. Try Penalized Models\n4. Try Different Algorithms\n - Decision-Tree often perform well on imbalanced datasets","9b8b907b":"According to the classification report and confusion matrix, we can find the best min_sample_leafs = 60. Then we optimize the random forest model to the best.","325eb1f4":"- ** Too many unique values in steps **\n- ** Need to create a higher level of feature for step **"}}