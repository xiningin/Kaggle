{"cell_type":{"4cb4b235":"code","937f7fff":"code","f726b82f":"code","f96408ad":"code","47680ebe":"code","bc4b3483":"code","125efc2c":"code","287aa2ce":"code","734fc91c":"code","c69eeca5":"code","3638e276":"code","ae4e3bc4":"code","e5a81735":"code","a350a2e6":"code","b042f941":"code","d217aa2f":"code","f71b59f5":"code","c1409b43":"code","3de7b76f":"code","b9994014":"code","187fde98":"code","da1cef01":"code","cad40f6d":"code","122e068c":"code","ae9bd7b7":"code","18ef7d0e":"code","980fa709":"code","a75f1c6f":"code","f30a8bb7":"code","69ee8892":"code","24f2b160":"code","d0769c4a":"markdown","6dddcc29":"markdown","f84ee73f":"markdown","6d7f9282":"markdown","b7fd095d":"markdown","9eb4a7f9":"markdown","6550fb14":"markdown","9af30685":"markdown","0078066f":"markdown","74be53cc":"markdown","da797335":"markdown","bf37c191":"markdown","793d9bbd":"markdown","ca01dddc":"markdown","fba31bf6":"markdown","ec5283b7":"markdown","6f3dce6c":"markdown","d712ba6e":"markdown","515caaff":"markdown","865b84c4":"markdown","b3609227":"markdown","487c4456":"markdown","3ac66a3b":"markdown","7973b69d":"markdown","ef37023c":"markdown"},"source":{"4cb4b235":"import numpy as np\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nimport pydot\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt \nimport pandas as pd\nnp.set_printoptions(suppress=True)\n","937f7fff":"df1 = pd.read_csv('..\/input\/train.csv')\ndf2 = pd.read_csv('..\/input\/test.csv')","f726b82f":"train_data = np.array(df1)\ntest_data  = np.array(df2)\n\nX_train_orig    = np.asarray( train_data[:, 1::], dtype=np.float32 )\nY_train_orig    = np.asarray( train_data[:, 0], dtype=np.float32 )\n\nX_test_orig     = np.asarray( test_data[:, 0::], dtype=np.float32 )","f96408ad":"X_train = X_train_orig \/ 255.\nX_test = X_test_orig \/ 255.","47680ebe":"X_train.shape","bc4b3483":"X_train = X_train.reshape(-1,28,28,1)\n\nX_test = X_test.reshape(-1,28,28,1)\n\nY_train = Y_train_orig.reshape(-1, 1)\n\nY_train = to_categorical(Y_train, num_classes=10)","125efc2c":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.2)","287aa2ce":"print (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))","734fc91c":"plt.figure(figsize=(10,10))   # to fix a shape for each image print\nfor ax in range(10):          # using a for loop to display a number of images\n    plt.subplot(5, 5, ax+1) # we need to use this function to print an array of pictures \n    plt.imshow(X_train[ax,:,:,0],cmap=plt.cm.binary) # this will call the images from train set one by one\n    print(\" Ground Truth Label = \", Y_train[ax,:])   # Lets also look into the labels \n    plt.axis('off')                                  # axis has been turned off to have clear view","c69eeca5":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1)  # randomly shift images vertically (fraction of total height)\n\ndatagen.fit(X_train)","3638e276":"def Keras_Model(input_shape):    \n    \n    X_input = Input(input_shape)\n    \n    # First Convolutional Layer\n    X = Conv2D(64, (3, 3), strides = (1, 1), padding = 'same', name = 'conv0')(X_input) \n    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n    X = Activation('relu')(X)\n    \n    # Second Convolutional Layer\n    X = Conv2D(64, (3, 3), strides = (1, 1), padding = 'same', name = 'conv1')(X) \n    X = BatchNormalization(axis = 3, name = 'bn1')(X)\n    X = Activation('relu')(X)\n    \n    # First Pooling Layer\n    X = MaxPooling2D((2, 2), name='max_pool_1')(X)\n                           \n    X = Dropout(0.35)(X)\n    \n    # Third Convolutional Layer\n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv2')(X) \n    X = BatchNormalization(axis = 3, name = 'bn2')(X)\n    X = Activation('relu')(X)\n    \n    # Fourth Convolutional Layer\n    X = Conv2D(128, (3, 3), strides = (1, 1), padding = 'same', name = 'conv3')(X) \n    X = BatchNormalization(axis = 3, name = 'bn3')(X)\n    X = Activation('relu')(X)\n    \n    # Second Pooling Layers\n    X = MaxPooling2D((2, 2), name='max_pool_2')(X)                       \n                           \n    X = Dropout(0.35)(X)     \n        \n    \n    # Fifth Convolutional Layer\n    X = Conv2D(256, (3, 3), strides = (1, 1), padding = 'same', name = 'conv4')(X) \n    X = BatchNormalization(axis = 3, name = 'bn4')(X)\n    X = Activation('relu')(X)\n\n    X = Dropout(0.35)(X) \n \n    # Flatten the data.\n    X = Flatten()(X)\n    # Dense Layer\n    X = Dense(1000, activation='relu', name='fc0')(X)\n    X = Dropout(0.5)(X) \n    X = Dense(256, activation='relu', name='fc2')(X)\n    \n    # Using softmax function to get the output\n    X = Dense(10, activation='softmax', name='fc3')(X)\n    \n    model = Model(inputs = X_input, outputs = X, name='model')\n    \n    return model","ae4e3bc4":"Keras_Model = Keras_Model(X_train.shape[1:4])","e5a81735":"from keras.optimizers import Adam\nepochs = 100\nbatch_size = 64\nlrate = 0.0001\ndecay = lrate\/epochs\noptimizer = Adam(lr=lrate, epsilon=1e-08, decay = 0.00)","a350a2e6":"Keras_Model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])","b042f941":"from keras.callbacks import ReduceLROnPlateau\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.0000001)","d217aa2f":"from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')","f71b59f5":"history = Keras_Model.fit(x = X_train, y = Y_train, batch_size = batch_size, \n                        epochs=epochs, verbose=1, \n                        validation_data = (X_val, Y_val),\n                        \n                          steps_per_epoch= None, validation_steps=None,\n                                  callbacks=[learning_rate_reduction, early_stopping] )","c1409b43":"preds = Keras_Model.evaluate(X_train, Y_train)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Train set Accuracy = \" + str(preds[1]))","3de7b76f":"preds = Keras_Model.evaluate(X_val, Y_val)\nprint (\"Loss on Val set= \" + str(preds[0]))\nprint (\"Val set Accuracy = \" + str(preds[1]))","b9994014":"history_dict = history.history\nhistory_dict.keys()","187fde98":"val_loss = history_dict['val_loss']\nval_acc = history_dict['val_acc']\nloss = history_dict['loss']\nacc = history_dict['acc']\nepochs = range(1,len(history_dict['val_loss'])+1)","da1cef01":"plt.plot(epochs,acc,'b-')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\n\nplt.plot(epochs,val_acc,'b-', color = 'red')\nplt.title('Accuracy of Model')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.show()","cad40f6d":"plt.plot(epochs,loss,'b-')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('Loss')\n\nplt.plot(epochs,val_loss,'b-', color = 'red')\nplt.title('loss function')\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nplt.show()","122e068c":"# STEP 1:\npredicted_val_probability = Keras_Model.predict(X_val, batch_size=32)\nY_val_pred_label = np.argmax(predicted_val_probability, axis = 1)","ae9bd7b7":"# STEP 2:\nY_val_True_label = np.argmax(Y_val, axis = 1)","18ef7d0e":"# STEP 3: \nj = []\nmax_probability = []\nfor i in range(len(Y_val_pred_label)):\n    if Y_val_pred_label[i] != Y_val_True_label[i]:     \n        j.append(i)","980fa709":"# STEP 4: \nl = []\nfor ele in j:\n    if np.max(predicted_val_probability[ele]) > 0.95:\n        l.append(ele)","a75f1c6f":"# STEP 5: \nc = 1\nplt.figure(figsize=(10,10))   # to fix a shape for each image print\nfor ax in l[:]:          # using a for loop to display a number of images\n    plt.subplot(6,5, c) # we need to use this function to print an array of pictures \n    plt.imshow(X_val[ax,:,:,0],cmap=plt.cm.binary) # this will call the images from train set one by one\n    plt.title('True label:{}\\nPredicted label:{}'.format(Y_val_True_label[ax],Y_val_pred_label[ax]))  # Lets also look into the labels \n    plt.axis('off') \n    c = c+1","f30a8bb7":"classes = Keras_Model.predict(X_test, batch_size=32)","69ee8892":"class_test_set = np.argmax(classes, axis = 1)","24f2b160":"prediction = pd.DataFrame()\nprediction['ImageId'] = np.asarray(range(1,28001))\nprediction['Label'] = class_test_set\n\nprediction.to_csv('submission.csv', index = False)","d0769c4a":"## Build the CNN model:\n### Conv2D(32 filter with 3x3 kernel, stride 1 padding same) * 2 ---> MaxPool ---> Dropout(0.4) ---> Conv2D(64 filter with 3x3 kernel, stride 1 padding same) ---> MaxPool ---> Dropout(0.4) ---> Flatten ---> Dense(1024) ---> Dense(256) ---> Dropout(0.5) ---> Softmax\n\nLets try to define each term above in simple language:\n* Convolution is a process of looking into the image part by part. Part means a small window. In our case Window is of size 3x3. We will use such 3x3 windown 32 times in one single CNN layer. This number 32 is called filter. Filters are cool to detect the edges whether vertical or horizontal. More number of filters means model is capable to handling more number of edges.\n\n* Pooling is a concept same as convolutional layer but here we take the maximum or average of all the number along the size of the window. No gradient decent while implementing pooling layers. No parameters to learn if we implement Pooling.\nWhy we use Pooling\u2026 No one realy knows. In many experiments, it was found that Pooling really works well. Dimension concepts are same as in Convolutional Layer. When using Pooling Padding are rarely used.\n\n* Padding is a concept of putting some extra layer around our image. This helps in keeping our dimensions correct. While we do Convolution or pooling the dimensions of the picture shrinks. But with the use of padding some bits(0 or 1) around the picture we used to retain the dimension of the picture.\n\n* How Dimensions are effected.. If Image size is n x n and window size is 3 x 3 and we use padding p than our output image size will be:\n    n + 2p - f + 1     x      n + 2p - f + 1  \nWe also use padding when we need to retain the input size of the image. To do this we choose padding carefully as :               p  =  (f-1) \/ 2  ,    f is almost always odd.\n\n* What is stride... When we use Stride of 2 we jump over by 2 step while doing convolution. While using Stride we use the below formula for the output image: (  ((n + 2p - f ) \/ s)  +  1  )  x   (  ((n + 2p - f ) \/ s)  +  1  )\n\n* Batchnormalization are used to normalize the activations. Normaly we do normalize the input values and with batch normalization we are doing the same thing to the activations. Simply saying we do normalization in the hidden layers too. This helps in improving accuracy and Speed up tarining process etc.\n\n* Dropout is a cool thing where we nullify the effect of some neuron so as to improve the accuracy of other neuron. Basicaly we set some random weights to zero and thus It helps in a overfitting model and also helps in generalization. Overfitting Model is a model where Training Accuracy is more but Test accuracy is poor. Generalization means how well our model predicts for unseen data.\n\n* At last we flatten our output from CNN so as to get a vector. Lets think Vector as a single column in an excel. We do this to use our data in Fully connected layers. Here we have introduced two FC layers of 1024 neurons and 256 neurons.\n\n* To get the Output we have used a softmax function. This function helps us to give 10 different classes.\n\n## So thats it.. Now lets build our model in keras.\nIn Keras there are two ways to build our model. One is sequential model and another is the functional API. I think sequential model is a bit popular but here we will use functional API model. ","6dddcc29":"## Adam optimizer:\nThere are several Optimizing function to use. this time I preferred Adam Optimizer. This is widely used and fast for converging. We are keeping our Learning rate to 0.0001","f84ee73f":"we have used argmax function to find out the highest probability in a row. axis=1 doing the trick here.","6d7f9282":"We need to compile the model before training. This step is related to keras. We used categorical crossentropy function as we have One hot encoded our labels.","b7fd095d":"## Train the Model\nLets train the model with batch size of 32 and epoch of 75. As we have used data augmentation we need to use a special function called fit generator. I have trained the model to 75 number of epochs beyond which I believe model will overfit the data. Though you can try different number of epochs. I have tried different batch size but it barely effects the accuracy. batch size impacts on speed of training.","9eb4a7f9":"## Data Augmentation:\n#### Let us do some data Augmentation and see what happens.\nIn data Augmentation we are giving a tilt of 15 degree, zooming 10%, shifting the widhth and height by 10% out of total fraction.\nAll this process is done selecting random images in the data set. \n* We achieved 99.314 % accuracy without Data Augmentation.\n* And with Data Augmentation we achieved 99.685%. ","6550fb14":"### Lets see the dimension of each array\nTill now we have 4 array. \n* X_train contains the pixels values of each images from 42000 set. On this set our model will train and learn to recognise digits.\n* Y_train contains the labels of each corresponding images. \n* X_test contains the pixels values of each images from 28000 set. We will test our model on this set. This set does not have labels. We will predict the labels here.\n\nLets look:","9af30685":"## Thank You\nThanks to kaggle for giving this platform to connect to some wonderfull people around the globe. Also Thanks a lot to Mr. Andrew Ng who motivatated milions of people including me to learn and Practice Deep Learning.\n* Please do vote and comment on this kernel. Namaste !!","0078066f":"## Prediction on Test set:\nLets look at our prediction. rememeber that we have one hot encoded our labels and hence here we will get 10 different results for each test set examples. In the next step we will find out the highest number(probability) in a single row.","74be53cc":"### Calling the required function.\n* Keras is a Library which gives us liberty to call different function to use ML algorithms. It is easy to learn. Keras       documentations are easy enough to follow.\n* matplotlib to plot some images and lines.\n* Pandas to load the excell and do simple pre processing.\n* numpy is an awsome mathematical library","da797335":"### Let's look into our data.\nAs our data has been well pre processed now lets look at our data once before proceeding to train our model.\nWe have already turned our Y_train into categorical values. This means that each label has been turned into a vector of 10 values. If our label is 1 than our vector will be like [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]. All zeros except the position 1. In python count always starts from 0. This process is famously known as one hot encoding. We had to do this as we have 10 different classes to predict. 10 different classes are from 0, 1, 2, 3, 4, .....  10.","bf37c191":"###  Normalizing the input. \nThe data what we have in our training set are scattered from 0 to 255. So we are dividing each inoput by 255 so that we have a range of values from 0 to 1. This helps in improve accuracy and faster to converge.","793d9bbd":"### Reshaping the arrays into desired format.\nIn Keras we need to reshape the arrays into [no of images, images size in x & y, no of channels]\nWe have 42000 examples in our training set\nEach Image is 28x28 size\nEach image have only 1 dimension because we only have grey scale images. This value would have been 3 if we have a colored image. Colored image have three channel which is best known as RGB channels.\n\nSo Our Training set have become  [42000,  28,  28,  1]","ca01dddc":"#### History is used to find out accuracy and loss in Tran as well as Validation set.","fba31bf6":"### Here we go !!\nI have used techniques like Convolutional Neural Network with Pooling layers and batch normalisation and dropout. Used Adam Optimization for optimizing the cost. I also have used Data Augmentation to improve accuracy by atleast 0.371% . \nI have not used learning rate decay function and also not used any validation set to measure the accuracy. Trained my model on 42000 images and I have achieved an accuracy of 99.685% with dara Augmentation ON and epochs 75. Without Data Augmentation I achieved test set accuracy of 99.314% [here](https:\/\/www.kaggle.com\/hengulkakaty\/digit-recogniotion-in-cnn)","ec5283b7":"### Let's see where our model is going wrong.\n* STEP 1:  We will predict on Validation set and convert data to integer value using argmax and store it in a variable Y_val_pred_label.\n* STEP 2:  We will find out the integer value from the Ground truth label from Validation set as well and store it in a variable Y_val_True_label.\n* STEP 3:  So Predicted label and True label should be equal to each other in ideal case. If they are different than that is our point of concern.\n* STEP 4:  When there is a difference than we will check the probability. If model is wrong in predicting the label and probability of prediction is quite high than we will check those instance.\n* STEP 5:  We will priint the corresponding image using X_val and also print the predicted label and true label to get an idea how our model is going wrong.","6f3dce6c":"### Let's call the above function with our input shape. Our Input is X_train right....","d712ba6e":"## Lets prepare the submission excell sheet as desired and commit this kernel to see how we have done.","515caaff":"From the above picture it is very clear that some data are inconsistent. Even human can go wrong in these kind of cases. So we can conclude that our model is doing prity well in predicting hand written digits.","865b84c4":"###  Loading the Train and Test Set. paths are set by default if you use kaggle kernel","b3609227":"Above Graphs shows the Loss on Train and Validation set. RED line suggesting Loss on Validation is lower that train loss.","487c4456":"## Train Accuracy:\n\nI have achieved almost same train accuracy with or without data Augmentation. But the model with data augmentation generalizes well. Please see the kernel without data augmentation [here](http:\/\/www.kaggle.com\/hengulkakaty\/digit-recogniotion-in-cnn?scriptVersionId=6090971)","3ac66a3b":" Here we are just formatting the data into an array.","7973b69d":"### **CNN model using keras. Test set Accuracy Achieved is 99.685%. **\nHi all,   I was fascinated by the book written by Michael Nielsen on neural networks sometime ago and since than I have been a ML enthusiast. :)  Happy to share with you my first kernel in digit recognition. \n* In this kernel I have used various ideas shared before in this platform. I am just returning the benefits I have earned here.","ef37023c":"In the above graph we have seen  that our validation accuracy(RED Line) is slightly on higher side than Training accuracy.  So we can expect a good generalisation."}}