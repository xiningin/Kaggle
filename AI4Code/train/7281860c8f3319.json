{"cell_type":{"011fde66":"code","0078f69f":"code","380eb78f":"code","26658ab4":"code","772f7d56":"code","aca85943":"code","ffb78cfe":"code","2d10477c":"code","c96952d6":"code","9966fd60":"code","1a878ef8":"code","338cefb9":"code","2f17d7b7":"code","decfe503":"code","ea24c019":"code","4351ec1c":"code","35682631":"code","cf044b82":"code","d01cb161":"code","295823fa":"code","0e812c97":"code","62781fdb":"code","75d6f4ca":"code","ef3c0853":"code","9e5f38a8":"code","75da5965":"code","728e3089":"code","22b82acd":"code","1b5c4f52":"code","12fd83ec":"code","78e247e3":"code","5546b18f":"code","22cd5ed8":"code","d1b2a694":"code","5900e119":"code","ddf80ecc":"code","5d15a0a0":"code","876be5f7":"code","235e2ae0":"code","34496b93":"code","6a74a774":"code","eb17bd72":"code","f19a718a":"code","2a6aa3a6":"code","cecf03ee":"markdown","e4b675ef":"markdown","0b7273a9":"markdown","77957506":"markdown","2736c470":"markdown","8e554286":"markdown","9fa6dd51":"markdown","369d8562":"markdown","9184d537":"markdown","fdbb9975":"markdown","a1bf06d6":"markdown"},"source":{"011fde66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0078f69f":"data=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","380eb78f":"data.describe()","26658ab4":"data.shape","772f7d56":"Fraud=data[data['Class']==1]\nValid=data[data['Class']==0]\nfraction=len(Fraud)\/(len(Fraud)+len(Valid))\nprint(len(Valid))\nprint(len(Fraud))\nprint(fraction)\n","aca85943":"Fraud.Amount.describe()","ffb78cfe":"Valid.Amount.describe()","2d10477c":"Fraud.Time.describe()","c96952d6":"Valid.Time.describe()","9966fd60":"import matplotlib.pyplot as plt\nimport itertools\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, f1_score, recall_score\nnp.random.seed(2)","1a878ef8":"# Data visualization\nfrom matplotlib import gridspec\nfeatures = data.iloc[:,0:28].columns\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, c in enumerate(data[features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(data[c][data.Class == 1], bins=50)\n    sns.distplot(data[c][data.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(c))\nplt.show()","338cefb9":"#Correlation matrix\ncorrmat = data.corr()\nfig = plt.figure(figsize = (12, 9))\nsns.heatmap(corrmat, vmax = .8, square = True)\nplt.show()","2f17d7b7":"#skewness\nskew_ = data.skew()\nskew_","decfe503":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler2 = StandardScaler()\n#scaling time\nscaled_time = scaler.fit_transform(data[['Time']])\nflat_list1 = [item for sublist in scaled_time.tolist() for item in sublist]\nscaled_time = pd.Series(flat_list1)","ea24c019":"#scaling the amount column\nscaled_amount = scaler2.fit_transform(data[['Amount']])\nflat_list2 = [item for sublist in scaled_amount.tolist() for item in sublist]\nscaled_amount = pd.Series(flat_list2)","4351ec1c":"#concatenating newly created columns w original df\ndata = pd.concat([data, scaled_amount.rename('scaled_amount'), scaled_time.rename('scaled_time')], axis=1)\ndata.sample(5)","35682631":"data.drop(['Amount', 'Time'], axis=1, inplace=True)","cf044b82":"#Splitting the data into training and test sets\n#manual train test split using numpy's random.rand\nmask = np.random.rand(len(data)) < 0.9\ntrain = data[mask]\ntest = data[~mask]\nprint('Train Shape: {}\\nTest Shape: {}'.format(train.shape, test.shape))","d01cb161":"train.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)","295823fa":"no_of_frauds = train.Class.value_counts()[1]\nprint('There are {} fraudulent transactions in the train data.'.format(no_of_frauds))","0e812c97":"#randomly selecting 431 random non-fraudulent transactions\nnon_fraud = train[train['Class'] == 0]\nfraud = train[train['Class'] == 1]","62781fdb":"selected = non_fraud.sample(no_of_frauds)\nselected.head()","75d6f4ca":"#concatenating both into a subsample data set with equal class distribution\nselected.reset_index(drop=True, inplace=True)\nfraud.reset_index(drop=True, inplace=True)\nsubsample = pd.concat([selected, fraud])\nlen(subsample)","ef3c0853":"#shuffling our data set\nsubsample = subsample.sample(frac=1).reset_index(drop=True)\nsubsample.head(10)","9e5f38a8":"new_counts = subsample.Class.value_counts()\nplt.figure(figsize=(8,6))\nsns.barplot(x=new_counts.index, y=new_counts)\nplt.title('Count of Fraudulent vs. Non-Fraudulent Transactions In Subsample')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Non-Fraudulent, 1:Fraudulent)')","75da5965":"#taking a look at correlations once more\ncorr = subsample.corr()\ncorr = corr[['Class']]\ncorr","728e3089":"#negative correlations smaller than -0.5\ncorr[corr.Class < -0.5]","22b82acd":"#positive correlations greater than 0.5\ncorr[corr.Class > 0.5]","1b5c4f52":"#visualizing the features w high negative correlation\nf, axes = plt.subplots(nrows=2, ncols=4, figsize=(26,16))\n\nf.suptitle('Features With High Negative Correlation', size=35)\nsns.boxplot(x=\"Class\", y=\"V3\", data=subsample, ax=axes[0,0])\nsns.boxplot(x=\"Class\", y=\"V9\", data=subsample, ax=axes[0,1])\nsns.boxplot(x=\"Class\", y=\"V10\", data=subsample, ax=axes[0,2])\nsns.boxplot(x=\"Class\", y=\"V12\", data=subsample, ax=axes[0,3])\nsns.boxplot(x=\"Class\", y=\"V14\", data=subsample, ax=axes[1,0])\nsns.boxplot(x=\"Class\", y=\"V16\", data=subsample, ax=axes[1,1])\nsns.boxplot(x=\"Class\", y=\"V17\", data=subsample, ax=axes[1,2])\nf.delaxes(axes[1,3])","12fd83ec":"#visualizing the features w high positive correlation\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,9))\n\nf.suptitle('Features With High Positive Correlation', size=20)\nsns.boxplot(x=\"Class\", y=\"V4\", data=subsample, ax=axes[0])\nsns.boxplot(x=\"Class\", y=\"V11\", data=subsample, ax=axes[1])","78e247e3":"#Only removing extreme outliers\nQ1 = subsample.quantile(0.25)\nQ3 = subsample.quantile(0.75)\nIQR = Q3 - Q1\n\ndf2 = subsample[~((subsample < (Q1 - 2.5 * IQR)) |(subsample > (Q3 + 2.5 * IQR))).any(axis=1)]","5546b18f":"len_after = len(df2)\nlen_before = len(subsample)\nlen_difference = len(subsample) - len(df2)\nprint('We reduced our data size from {} transactions by {} transactions to {} transactions.'.format(len_before, len_difference, len_after))","22cd5ed8":"X = df2.drop('Class', axis=1)\ny = df2['Class']\n\n","d1b2a694":"#Train-test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","5900e119":"X_train = X_train.values\nX_validation = X_test.values\ny_train = y_train.values\ny_validation = y_test.values\nprint('X_shapes:\\n', 'X_train:', 'X_validation:\\n', X_train.shape, X_validation.shape, '\\n')\nprint('Y_shapes:\\n', 'Y_train:', 'Y_validation:\\n', y_train.shape, y_validation.shape)","ddf80ecc":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","5d15a0a0":"# Training and testing different models\nmodels = []\n\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\n\n#testing models\n\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=42)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    results.append(cv_results)\n    names.append(name)\n    msg = '%s: %f (%f)' % (name, cv_results.mean(), cv_results.std())\n    print(msg)","876be5f7":"#Compare Algorithms\n\nfig = plt.figure(figsize=(12,10))\nplt.title('Comparison of Classification Algorithms')\nplt.xlabel('Algorithm')\nplt.ylabel('ROC-AUC Score')\nplt.boxplot(results)\nax = fig.add_subplot(111)\nax.set_xticklabels(names)\nplt.show()","235e2ae0":"#visualizing RF\nmodel = RandomForestClassifier(n_estimators=10)\n\n# Train\nmodel.fit(X_train, y_train)\n# Extract single tree\nestimator = model.estimators_[5]\n\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = X.columns.tolist(),\n                class_names = ['0',' 1'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","34496b93":"y_pred = model.predict(X_validation)","6a74a774":"#Testing\ny_expect = pd.DataFrame(y_validation)\ncm = confusion_matrix(y_expect, y_pred.round())\nprint(accuracy_score(y_test, y_pred.round()))\nprint(precision_score(y_test, y_pred.round()))\nprint(recall_score(y_test, y_pred.round()))\nprint(f1_score(y_test, y_pred.round()))","eb17bd72":"no_of_frauds_t = test.Class.value_counts()[1]\n#randomly selecting 431 random non-fraudulent transactions\nnon_fraudt = test[test['Class'] == 0]\nfraudt = test[test['Class'] == 1]\nselectedt = non_fraudt.sample(no_of_frauds_t)\nselectedt.reset_index(drop=True, inplace=True)\nfraudt.reset_index(drop=True, inplace=True)\nsubsamplet = pd.concat([selectedt, fraudt])\nlen(subsamplet)\nsubsamplet = subsamplet.sample(frac=1).reset_index(drop=True)\nsubsamplet.head(10)","f19a718a":"#Only removing extreme outliers\nQ1 = subsamplet.quantile(0.25)\nQ3 = subsamplet.quantile(0.75)\nIQR = Q3 - Q1\n\ndf3 = subsamplet[~((subsamplet < (Q1 - 2.5 * IQR)) |(subsamplet > (Q3 + 2.5 * IQR))).any(axis=1)]\nlen_aftert = len(df3)\nlen_beforet = len(subsamplet)\nlen_differencet = len(subsamplet) - len(df3)\nprint('We reduced our data size from {} transactions by {} transactions to {} transactions.'.format(len_beforet, len_differencet, len_aftert))","2a6aa3a6":"X_t = df3.drop('Class', axis=1)\ny_t = df3['Class']\nyt_pred = model.predict(X_t)\nyt_expect = pd.DataFrame(y_t)\ncm = confusion_matrix(yt_expect, yt_pred.round())\nprint(accuracy_score(y_t, yt_pred.round()))\nprint(precision_score(y_t, yt_pred.round()))\nprint(recall_score(y_t, yt_pred.round()))\nprint(f1_score(y_t, yt_pred.round()))","cecf03ee":"In the HeatMap we can clearly see that most of the features do not correlate to other features but there are some features that have either a positive or a negative correlation with each other. ","e4b675ef":"# Loading the files and reading the data","0b7273a9":"We can clearly see that mean amount for fraudelent transactions is high.","77957506":"The dataset is highly imbalanced ,only 0.17% transactions are fraudelent. First let's try to make predictions on this unbalanced data.","2736c470":"# Exploratory Data Analysis","8e554286":"There are 30 independent variables,time,amount,V1-V28 and one dependent variable Class.\nThe highest value in amount is Rs. 284807. Also, there are no missing values.","9fa6dd51":"We choose **Random Forest** as the best model since it gives one of the best performances and higher degree of comprehensiveness .","369d8562":"I have done some exploratory data analysis and visualization on the data. Since this is a highly unbalanced dataset , I have used under-sampling to make balanced dataset for model training .I have used various algorithms and in the end I found Random Forest to be the most effective. Thus I have used it as the model for doing the final testing on the dataset. Figures after testing the Random forest model on the test set.\n\nAccuracy - 93.93 %\nPrecision - 95.23%\nRecall- 86.96%\nF1-score - 90.90 %","9184d537":"# Data Pre-processing","fdbb9975":"# Making Predictions","a1bf06d6":"# Modeling"}}