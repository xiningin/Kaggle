{"cell_type":{"98de868c":"code","06b4e32a":"code","9927e958":"code","70a7e320":"code","4aa937e4":"code","9fbb9ece":"code","f5fb2072":"code","543cffea":"code","9291023c":"code","f1a2d2c7":"code","420c085e":"code","6f4b6da4":"code","e4f88793":"code","97c2a763":"code","d22ce41d":"code","860f5d02":"code","48741b3c":"code","18eb1b52":"code","9f6885fd":"code","314817a4":"code","f6a0975f":"code","7ba440fd":"code","de181561":"code","2c35adcc":"code","73ab707d":"code","cb540b4c":"code","f3d77837":"code","8878540c":"code","2348d8a9":"code","16b39980":"code","aacc8bce":"code","e606d7c2":"markdown","2f2b8039":"markdown","70f1905e":"markdown","e199bc60":"markdown"},"source":{"98de868c":"import os\nimport torch\nfrom transformers import *\nfrom transformers import BertTokenizer, BertModel,BertForSequenceClassification,AdamW\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm, trange\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt","06b4e32a":"from fastai.text import * \n","9927e958":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nfrom string import punctuation\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\ndf = pd.read_csv(\"..\/input\/quoraquestions\/data.csv\").fillna(\"\")\ndf.head() ","70a7e320":"df.info()","4aa937e4":"df.shape","9fbb9ece":"df.groupby(\"is_duplicate\")['id'].count().plot.bar()","f5fb2072":"df","543cffea":"dfs = df[0:2500]\ndfs.groupby(\"is_duplicate\")['id'].count().plot.bar()","9291023c":"df","f1a2d2c7":"stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n              'Is','If','While','This']","420c085e":"data.dropna(inplace=True)","6f4b6da4":"data","e4f88793":"##","97c2a763":"def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n    # Clean the text, with the option to remove stop_words and to stem words.\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n    text = re.sub(r\"what's\", \"\", text)\n    text = re.sub(r\"What's\", \"\", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\" m \", \" am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"60k\", \" 60000 \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e-mail\", \"email\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    text = re.sub(r\"quikly\", \"quickly\", text)\n    text = re.sub(r\" usa \", \" America \", text)\n    text = re.sub(r\" USA \", \" America \", text)\n    text = re.sub(r\" u s \", \" America \", text)\n    text = re.sub(r\" uk \", \" England \", text)\n    text = re.sub(r\" UK \", \" England \", text)\n    text = re.sub(r\"india\", \"India\", text)\n    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n    text = re.sub(r\"china\", \"China\", text)\n    text = re.sub(r\"chinese\", \"Chinese\", text) \n    text = re.sub(r\"imrovement\", \"improvement\", text)\n    text = re.sub(r\"intially\", \"initially\", text)\n    text = re.sub(r\"quora\", \"Quora\", text)\n    text = re.sub(r\" dms \", \"direct messages \", text)  \n    text = re.sub(r\"demonitization\", \"demonetization\", text) \n    text = re.sub(r\"actived\", \"active\", text)\n    text = re.sub(r\"kms\", \" kilometers \", text)\n    text = re.sub(r\"KMs\", \" kilometers \", text)\n    text = re.sub(r\" cs \", \" computer science \", text) \n    text = re.sub(r\" upvotes \", \" up votes \", text)\n    text = re.sub(r\" iPhone \", \" phone \", text)\n    text = re.sub(r\"\\0rs \", \" rs \", text) \n    text = re.sub(r\"calender\", \"calendar\", text)\n    text = re.sub(r\"ios\", \"operating system\", text)\n    text = re.sub(r\"gps\", \"GPS\", text)\n    text = re.sub(r\"gst\", \"GST\", text)\n    text = re.sub(r\"programing\", \"programming\", text)\n    text = re.sub(r\"bestfriend\", \"best friend\", text)\n    text = re.sub(r\"dna\", \"DNA\", text)\n    text = re.sub(r\"III\", \"3\", text) \n    text = re.sub(r\"the US\", \"America\", text)\n    text = re.sub(r\"Astrology\", \"astrology\", text)\n    text = re.sub(r\"Method\", \"method\", text)\n    text = re.sub(r\"Find\", \"find\", text) \n    text = re.sub(r\"banglore\", \"Banglore\", text)\n    text = re.sub(r\" J K \", \" JK \", text)\n    \n    # Remove punctuation from text\n    text = ''.join([c for c in text if c not in punctuation])\n    \n    # Optionally, remove stop words\n    if remove_stop_words:\n        text = text.split()\n        text = [w for w in text if not w in stop_words]\n        text = \" \".join(text)\n    \n    # Optionally, shorten words to their stems\n    if stem_words:\n        text = text.split()\n        stemmer = SnowballStemmer('english')\n        stemmed_words = [stemmer.stem(word) for word in text]\n        text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","d22ce41d":"data[1]","860f5d02":"def process_questions(question_list, questions, question_list_name, dataframe):\n    '''transform questions and display progress'''\n    for question in questions:\n        question_list.append(text_to_wordlist(question))\n        if len(question_list) % 100000 == 0:\n            progress = len(question_list)\/len(dataframe) * 100\n            print(\"{} is {}% complete.\".format(question_list_name, round(progress, 1)))","48741b3c":"data1 = []\nprocess_questions(data1, data, 'train_question1', data)","18eb1b52":"data1","9f6885fd":"data = pd.Series(data1)","314817a4":"data","f6a0975f":"data","7ba440fd":"data_up = df[:20000]","de181561":"data_up","2c35adcc":"df.columns","73ab707d":"BATCH_SIZE=10","cb540b4c":"data_lm = (TextList.from_df(data_up)\n           #Inputs: all the text files in path\n            .split_by_rand_pct(0.15)\n           #We randomly split and keep 10% for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=BATCH_SIZE))\ndata_lm.save('tmp_lm')","f3d77837":"data_lm.show_batch()\n","8878540c":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\nlearn.fit_one_cycle(1, 1e-2)","2348d8a9":"learn.unfreeze()\nlearn.fit_one_cycle(1, 1e-3)","16b39980":"learn.predict(\"what is data\", n_words=10)\n","aacc8bce":"!pip install pytorch-transformers\n","e606d7c2":"## PREPROCESSING","2f2b8039":"## Using GPT 2\n","70f1905e":"So we have six columns in total one of which is the label.","e199bc60":"## Using ULMFIT"}}