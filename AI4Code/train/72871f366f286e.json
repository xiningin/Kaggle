{"cell_type":{"a3fbc0f5":"code","822bce00":"code","fd47dab2":"code","a5ca0ebc":"code","ca77dc4c":"code","a9480037":"code","822aa14b":"code","601b4676":"code","6b1e8387":"code","643c2224":"code","e961cfa9":"code","e3d2f1ca":"code","f7fe7937":"code","5eac0cdb":"code","b3a84105":"code","306e6a88":"code","25fa1eb8":"code","c753018c":"code","e86e78c9":"code","59597d61":"code","854d9607":"code","6d7ffe9f":"code","8971fc48":"code","e5837f42":"code","4ce94d86":"code","99b2b78a":"code","a3662b47":"code","19617f8b":"code","d65a0804":"code","b6e7d7f0":"code","9223bf76":"code","817d2758":"code","5eb33f05":"code","e6be08ff":"code","87d3adc5":"code","21ef25a7":"code","385d684f":"code","c35b1233":"code","e971cbf4":"code","9d4c46e9":"code","e13cce65":"code","5f5522c0":"code","ff5320ea":"code","7c28a156":"code","f07fecb1":"code","061f1663":"code","54c8b8e1":"code","9a9de0d6":"code","6d34a841":"code","18c0a42d":"code","01d626ff":"code","c6fb6ec7":"code","367f3316":"code","28051c81":"code","7320577e":"code","8e891181":"code","02ef72fd":"code","4a7c9094":"code","e0c8e75a":"code","3cd89b3d":"code","c759f379":"code","357cc481":"code","43934d70":"code","eca2336a":"code","4db4c8f9":"code","9a24784c":"code","3dc6427e":"code","c5cd4614":"code","c883ae93":"code","2d6c17ea":"code","44c23ea1":"code","50d73e25":"code","a16943e9":"code","0081f83d":"code","5dc8299a":"code","249825ad":"code","9288ffe8":"code","609ccba4":"code","e8ea5f2c":"code","c81ce5c5":"code","7114d697":"code","51660c7e":"code","5b4db03b":"code","29f1ab4f":"code","19c1ff71":"code","dc2bbdb1":"code","548da22c":"code","c3ebfc06":"code","5d6033d8":"code","6f3fc770":"code","5f66259b":"code","6d7dd9d9":"code","eb30c962":"code","642f190c":"code","f8bb7297":"code","a6d7bfb4":"code","27dd69c6":"code","404ef857":"code","0035f335":"code","1cd9a1da":"code","94bc10c4":"code","32c80497":"markdown","e3fef000":"markdown","14b90585":"markdown","01b114a8":"markdown","4dc4405b":"markdown","210366bb":"markdown","4a15a415":"markdown","0eab6366":"markdown","1c4367f3":"markdown","c3b0b544":"markdown","5a4b0254":"markdown","4005adf9":"markdown","4ff78410":"markdown","d5d77eb8":"markdown","dd998068":"markdown","050728c9":"markdown","6833cfab":"markdown","2e433079":"markdown","be08bf84":"markdown","9f7b2e1f":"markdown","d6290c26":"markdown","5637552b":"markdown","09e8400c":"markdown","d1333d2f":"markdown","cb6f4700":"markdown","0383328a":"markdown","bdefb3e4":"markdown","2bf715c5":"markdown","fd41cb53":"markdown","10b30ae9":"markdown","03652449":"markdown","46d56839":"markdown","52e80c4d":"markdown","88496d65":"markdown","67a8613b":"markdown","351b8f42":"markdown","880b23da":"markdown","f2f01099":"markdown","5cc1f293":"markdown","439bf114":"markdown","ba641e47":"markdown"},"source":{"a3fbc0f5":"# import packages to use in the notebook\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfrom datetime import datetime\nimport seaborn as sns\n","822bce00":"# load file\nos.getcwd()","fd47dab2":"ride = pd.read_csv('..\/input\/strava_data.csv')","a5ca0ebc":"# only analysing rides so drop all runs imported \nride = ride[ride['type']==\"Ride\"]","ca77dc4c":"# df dimension\nride.shape","a9480037":"ride.tail()","822aa14b":"# Check 1 ride detail\nride.iloc[1035,:]","601b4676":"# feature information\nride.info()","6b1e8387":"#check columns names\nride.columns","643c2224":"# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","e961cfa9":"missing_values_table(ride)","e3d2f1ca":"#drop not usefull columns\n# ids, booleans, objects and etc no data \nride.drop(columns=['Unnamed: 0','id', 'external_id', 'upload_id','name', 'athlete','map', 'trainer', 'commute', 'manual', 'private',\n       'flagged','device_watts','has_kudoed','max_watts',\n       'weighted_average_watts','gear_id' ,'type','workout_type','photo_count'],axis=1,inplace=True)\n\n","f7fe7937":"# check what is left\nride.info()","5eac0cdb":"# check the object columns which is text and will be hard to sort e.g dates as a string does not make sense\nride.iloc[:,6:11].head()","b3a84105":"# transform to date time\nride['start_date']= ride['start_date'].apply(lambda x: datetime.strptime(x[0:19],'%Y-%m-%d %X'))","306e6a88":"ride['start_date_local']= ride['start_date_local'].apply(lambda x: datetime.strptime(x[0:19],'%Y-%m-%d %X'))","25fa1eb8":"# create a city atribute\nride['city']= ride['timezone'].apply(lambda x: x.split('\/')[-1])","c753018c":"# expand Lat and Lon for start\ns_lat_lon = ride['start_latlng'].str.split(',',expand=True).rename(index=int, columns={0: \"s_lat\", 1: \"s_lon\"})","e86e78c9":"# expand Lat and Lon for end\ne_lat_lon = ride['end_latlng'].str.split(',',expand=True).rename(index=int, columns={0: \"e_lat\", 1: \"e_lon\"})\n","59597d61":"#Add to the main frame\nride =pd.concat([ride,s_lat_lon],axis=1)","854d9607":"#Add to the main frame\nride =pd.concat([ride,e_lat_lon],axis=1)","6d7ffe9f":"# drop original columns\nride.drop(columns=['start_latlng','end_latlng','timezone'],axis=1,inplace=True)","8971fc48":"# Check data frame\nride.head()","e5837f42":"#fix the lat lon columns \n#remove de [] from lat lon and convert to float\nride['s_lat'] = ride['s_lat'].str.replace('[','').astype(float)\nride['e_lat'] = ride['e_lat'].str.replace('[','').astype(float)\nride['s_lon'] = ride['s_lon'].str.replace(']','').astype(float)\nride['e_lon'] = ride['e_lon'].str.replace(']','').astype(float)","4ce94d86":"# check if all numeric\nride.info()","99b2b78a":"#Check head\nride.head()","a3662b47":"# ckeck final null values\nride.isna().sum()","19617f8b":"# only 3 value. will replace with with average\nride = ride.fillna(ride.mean())\n","d65a0804":"# no values nulls\nride.isna().sum()","b6e7d7f0":"# final look at columns\nride.describe()","9223bf76":"#Analyse histogram of all variables\n\nride.hist(bins=50,figsize=(20,15))","817d2758":"# Plotting Scatter Matrix\n# just numeric excluding latitude and city\ng=sns.pairplot(data=ride.iloc[:,:-5]);\ng.fig.set_size_inches(20,20)\n","5eb33f05":"#Correlation against kudos\nride.corr()['kudos_count'].sort_values(ascending=False)[1:]","e6be08ff":"ride.drop(['comment_count'],axis=1,inplace=True)","87d3adc5":"# visualise kudos overtime\n\npd.pivot_table(ride,values='kudos_count',index='start_date_local',aggfunc=np.mean).sort_index().plot()","21ef25a7":"ride.shape # before dropping rows","385d684f":"ride = ride[ride['start_date_local'] > datetime(2017,6,1)]","c35b1233":"ride.shape # 30% dataset reduced","e971cbf4":"# plot again\npd.pivot_table(ride,values='kudos_count',index='start_date_local',aggfunc=np.mean).sort_index().plot()","9d4c46e9":"ride.corr().loc[:,'kudos_count'].sort_values(ascending=False)[1:]","e13cce65":"corr = ride.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15,12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(240, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.title('Correlation Matrix Plot')","5f5522c0":"selected_columns = ride.loc[:,['max_speed',              \n'moving_time',             \n'kilojoules',     \n'total_elevation_gain',    \n'distance',\n'elev_high',\n'total_photo_count',\n'average_watts',\n'athlete_count','kudos_count']]      ","ff5320ea":"# pair plot the non correlated and in order of importance\ng=sns.pairplot(data=selected_columns);\ng.fig.set_size_inches(20,20)","7c28a156":"# plot geographical data\n# only adelaide\nfig, ax = plt.subplots(figsize=(8,8))\nplt.style.use('ggplot')\ndata = ride[ride['city']=='Adelaide']\nplt.scatter(y=data['e_lat'],x=data['e_lon'])\n#ride[ride['city']=='Adelaide'].plot(kind='scatter',y='s_lat',x='s_lon',c='moving_time')\n","f07fecb1":"# check all current selected variables\nride.info()","061f1663":"ride.drop(columns=['e_lon','e_lat','s_lat','s_lon','start_date','start_date_local'],axis=1,inplace=True)","54c8b8e1":"ride.info()","9a9de0d6":"ride = pd.concat([ride,pd.get_dummies(ride['city'])],axis=1)\n","6d34a841":"ride.drop('city',axis=1,inplace=True)","18c0a42d":"# split sets\nfrom sklearn.model_selection import train_test_split\nX = ride.drop('kudos_count',axis=1).values\ny = ride['kudos_count'].values","01d626ff":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","c6fb6ec7":"print(len(X_train)+len(X_test))","367f3316":"# feature scalling\nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\n","28051c81":"# train model\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train_std,y_train)","7320577e":"# select 3 first elements of train set\nsome_data = X_train_std[:3]\nsome_predictions = lr.predict(some_data)\nsome_labels = y_train[:3]\nprint(f'some predictions{some_predictions} and some labels{some_labels}')","8e891181":"# calculate rmse\nfrom sklearn.metrics import mean_squared_error\nkudos_predictions_train = lr.predict(X_train_std)\nkudos_predictions_test = lr.predict(X_test_std)\ntrain_mse_lr = mean_squared_error(y_train,kudos_predictions_train)\ntrain_rmse_lr = np.sqrt(train_mse_lr)\n\n","02ef72fd":"print(f'mse train {train_mse_lr:.2f}')\nprint(f'rmse train {train_rmse_lr:.2f}')","4a7c9094":"# Cross validation code\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(lr, X_train_std,y_train,scoring='neg_mean_squared_error',cv=10)\nlr_rmse_scores = np.sqrt(-scores)\n\n","e0c8e75a":"def display_scores(scores):\n    print('Scores: ', scores)\n    print('Mean: ', scores.mean())\n    print('Standard deviarion: ', scores.std())","3cd89b3d":"# mse Scores\ndisplay_scores(scores)\n\n#rmse Scores\ndisplay_scores(lr_rmse_scores)","c759f379":"# decision tree regressor\nfrom sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeRegressor(max_depth=3)\ntree.fit(X_train_std,y_train)\n","357cc481":"kudos_predictions_train_tree = tree.predict(X_train_std)\nkudos_predictions_test_tree = tree.predict(X_test_std)\ntrain_mse_tree = mean_squared_error(y_train,kudos_predictions_train_tree)\ntrain_rmse_tree = np.sqrt(train_mse_tree)\n","43934d70":"print(f'mse train {train_mse_tree:.2f}')\nprint(f'rmse train {train_rmse_tree:.2f}')","eca2336a":"# Cross Validation tree\n\nscores = cross_val_score(tree, X_train_std,y_train,scoring='neg_mean_squared_error',cv=10)\ntree_rmse_scores = np.sqrt(-scores)","4db4c8f9":"# mse Scores\ndisplay_scores(scores)\n\n#rmse Scores\ndisplay_scores(tree_rmse_scores)","9a24784c":"# random forest  regressor\nfrom sklearn.ensemble import RandomForestRegressor\nforest = RandomForestRegressor(n_estimators=1000,criterion='mse',random_state=1,n_jobs=-1)\nforest.fit(X_train_std,y_train)\n","3dc6427e":"kudos_predictions_train_forest = forest.predict(X_train_std)\nkudos_predictions_test_forest = forest.predict(X_test_std)\ntrain_mse_forest = mean_squared_error(y_train,kudos_predictions_train_forest)\ntrain_rmse_forest = np.sqrt(train_mse_forest)\n","c5cd4614":"print(f'mse train {train_mse_forest:.2f}')\nprint(f'rmse train {train_rmse_forest:.2f}')","c883ae93":"# Cross Validation Forest\n\nscores = cross_val_score(forest, X_train_std,y_train,scoring='neg_mean_squared_error',cv=10)\nforest_rmse_scores = np.sqrt(-scores)","2d6c17ea":"# mse Scores\ndisplay_scores(scores)\n\n#rmse Scores\ndisplay_scores(forest_rmse_scores)","44c23ea1":"# getting the parameters from the first training\nforest.get_params()","50d73e25":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n","a16943e9":"\nprint(random_grid)","0081f83d":"# do a random search first for some guidance and find best hyperoarameters\nrf = RandomForestRegressor()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train_std, y_train)\n\n","5dc8299a":"rf_random.best_params_","249825ad":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [ 10, 10],\n    'max_features': [2, 3],\n    'min_samples_leaf': [1, 2],\n    'min_samples_split': [2, 4],\n    'n_estimators': [100, 500, 1000],\n     'n_jobs': [-1]\n}\n\n# Create a based model\nrf = RandomForestRegressor()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 10, scoring='neg_mean_squared_error')\n# fit\ngrid_search.fit(X_train_std,y_train)","9288ffe8":"# get best params\ngrid_search.best_params_","609ccba4":"# get best estimator\ngrid_search.best_estimator_","e8ea5f2c":"cvres = grid_search.cv_results_","c81ce5c5":"for mean_score, params in zip(cvres['mean_test_score'],cvres['params']):\n    print(np.sqrt(-mean_score), params)","7114d697":"# finally train best estimator and get the rmse for best parameter\n\nforest = grid_search.best_estimator_\nforest.fit(X_train_std,y_train)\n\n# predict and calculate rmse\nkudos_predictions_train_forest = forest.predict(X_train_std)\nkudos_predictions_test_forest = forest.predict(X_test_std)\ntrain_mse_forest = mean_squared_error(y_train,kudos_predictions_train_forest)\ntrain_rmse_forest = np.sqrt(train_mse_forest)\n\nprint(f'mse train {train_mse_forest:.2f}')\nprint(f'rmse train {train_rmse_forest:.2f}')","51660c7e":"# Cross Validation Forest with best estimator\n\nscores = cross_val_score(forest, X_train_std,y_train,scoring='neg_mean_squared_error',cv=10)\nforest_rmse_scores = np.sqrt(-scores)\n\n#rmse Scores\ndisplay_scores(forest_rmse_scores)\n","5b4db03b":"# random forest  regressor\n\nforest = RandomForestRegressor(n_estimators=1000,criterion='mse',random_state=1,n_jobs=-1)\nforest.fit(X_train_std,y_train)\n\nkudos_predictions_train_forest = forest.predict(X_train_std)\nkudos_predictions_test_forest = forest.predict(X_test_std)\ntrain_mse_forest = mean_squared_error(y_train,kudos_predictions_train_forest)\ntrain_rmse_forest = np.sqrt(train_mse_forest)\n\nprint(f'mse train {train_mse_forest:.2f}')\nprint(f'rmse train {train_rmse_forest:.2f}')","29f1ab4f":"# Evaluate Residuals Plain Regression\n\nfig, ax = plt.subplots(figsize=(8,8))\nplt.style.use('ggplot')\nplt.scatter(kudos_predictions_train,kudos_predictions_train-y_train,c='blue',marker='o',label='traing data')\nplt.scatter(kudos_predictions_test,kudos_predictions_test-y_test,c='lightgreen',marker='s',label='test data')\nplt.title('Regression Residuals')\nplt.xlabel('predicted values')\nplt.ylabel('residuals')\nplt.legend(loc='upper left')\nplt.hlines(y=0,xmin=0,xmax=80)\nplt.xlim([0,80])\nplt.show()\n\n","19c1ff71":"# lets import another metric that in the case would be intersting analyse\nfrom sklearn.metrics import r2_score\nr2_train = r2_score(y_train,kudos_predictions_train)\nr2_test = r2_score(y_test,kudos_predictions_test)\n\n\nprint(f'r2 train {r2_train:.2f}')\nprint(f'r2 test {r2_test:.2f}')\n\n# Also do the test evaluation of rmse\n\ntest_mse_lr = mean_squared_error(y_test,kudos_predictions_test)\ntest_rmse_lr = np.sqrt(test_mse_lr)\n\nprint(f'rmse train {train_rmse_lr:.2f}')\nprint(f'rmse test {test_rmse_lr:.2f}')\n\n","dc2bbdb1":"# Evaluate Residuals Decision Tree\n\nfig, ax = plt.subplots(figsize=(8,8))\nplt.style.use('ggplot')\nplt.scatter(kudos_predictions_train_tree,kudos_predictions_train_tree-y_train,c='blue',marker='o',label='traing data')\nplt.scatter(kudos_predictions_test_tree,kudos_predictions_test_tree-y_test,c='lightgreen',marker='s',label='test data')\nplt.title('Decision Tree Regression Residuals')\nplt.xlabel('predicted values')\nplt.ylabel('residuals')\nplt.legend(loc='upper left')\nplt.hlines(y=0,xmin=0,xmax=80)\nplt.xlim([0,80])\nplt.show()\n","548da22c":"np.max(kudos_predictions_test_tree)","c3ebfc06":"r2_train = r2_score(y_train,kudos_predictions_train_tree)\nr2_test = r2_score(y_test,kudos_predictions_test_tree)\n\nprint(f'r2 train {r2_train:.2f}')\nprint(f'r2 test {r2_test:.2f}')\n\n# Also do the test evaluation of rmse\n\ntest_mse_tree = mean_squared_error(y_test,kudos_predictions_test_tree)\ntest_rmse_tree = np.sqrt(test_mse_tree)\n\nprint(f'rmse train {train_rmse_tree:.2f}')\nprint(f'rmse test {test_rmse_tree:.2f}')\n\n","5d6033d8":"# Evaluate Residuals for random forest\n\nfig, ax = plt.subplots(figsize=(8,8))\nplt.style.use('ggplot')\nplt.scatter(kudos_predictions_train_forest,kudos_predictions_train_forest-y_train,c='blue',marker='o',label='traing data')\nplt.scatter(kudos_predictions_test_forest,kudos_predictions_test_forest-y_test,c='lightgreen',marker='s',label='test data')\nplt.title('Random Forest Regression Residuals')\nplt.xlabel('predicted values')\nplt.ylabel('residuals')\nplt.legend(loc='upper left')\nplt.hlines(y=0,xmin=0,xmax=80)\nplt.xlim([0,80])\nplt.show()\n","6f3fc770":"r2_train = r2_score(y_train,kudos_predictions_train_forest)\nr2_test = r2_score(y_test,kudos_predictions_test_forest)\n\nprint(f'r2 train {r2_train:.2f}')\nprint(f'r2 test {r2_test:.2f}')\n\n\n# Also do the test evaluation of rmse\n\ntest_mse_forest = mean_squared_error(y_test,kudos_predictions_test_forest)\ntest_rmse_forest = np.sqrt(test_mse_forest)\n\nprint(f'rmse train {train_rmse_forest:.2f}')\nprint(f'rmse test {test_rmse_forest:.2f}')\n","5f66259b":"# evaluating feature importances\nfeat_labels = ride.drop('kudos_count',axis=1).columns\nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor feature in range(X_train.shape[1]):\n    print(f'{feature} - {feat_labels[indices[feature]]} {importances[indices[feature]]:.2f}')","6d7dd9d9":"# plotting feature importance\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(10,5))\nplt.title('Feature Importance',fontsize=15)\nplt.bar(range(X_train.shape[1]),importances[indices],color='lightblue',align='center')\nplt.xticks(range(X_train.shape[1]),feat_labels[indices],rotation=90)\nplt.xlim([-1,X_train.shape[1]])\nplt.tight_layout()\n","eb30c962":"# list of features in order of importance\nfeature_importance = [feat_labels[indices[feature]] for feature in range(X_train.shape[1]) ]","642f190c":"feature_importance","f8bb7297":"# collecting rmses for training and validation CV from the most import feature to the least important feature\n\ntrain_rmse_s = []\nvalidation_rmse_s = []\nfor i in range(len(feature_importance)):\n    col = feature_importance[:i+1]\n    \n    X = ride.loc[:,col].values\n    y = ride['kudos_count'].values\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n    stdsc = StandardScaler()\n    X_train_std = stdsc.fit_transform(X_train)\n    X_test_std = stdsc.transform(X_test)\n\n    forest = RandomForestRegressor(n_estimators=1000,criterion='mse',random_state=1,n_jobs=-1)\n    forest.fit(X_train_std,y_train)\n\n    kudos_predictions_train_forest = forest.predict(X_train_std)\n    kudos_predictions_test_forest = forest.predict(X_test_std)\n    train_mse = mean_squared_error(y_train,kudos_predictions_train_forest)\n    train_rmse = np.sqrt(train_mse)\n    train_rmse_s.append(train_rmse)\n    scores = cross_val_score(forest, X_train_std,y_train,scoring='neg_mean_squared_error',cv=10)\n    forest_rmse_scores = np.sqrt(-scores)\n    validation_rmse = forest_rmse_scores.mean()\n    validation_rmse_s.append(validation_rmse)\n    \n    \n","a6d7bfb4":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(10,7))\nplt.title('rmse for trainig and test from adding feature by importance',fontsize=15)\nplt.plot(range(1,len(feature_importance)+1),train_rmse_s,color='lightblue',label='rmse train',lw=3,marker='o')\nfor i in range(1,len(feature_importance)):\n    plt.annotate(f'{train_rmse_s[i]:.2f}',\n            xy=(i+1, train_rmse_s[i]), xycoords='data')\nplt.plot(range(1,len(feature_importance)+1),validation_rmse_s,color='lightgreen',label='rmse validation',lw=3,marker='o')\nfor i in range(1,len(feature_importance)):\n    plt.annotate(f'{validation_rmse_s[i]:.2f}',\n            xy=(i+1, validation_rmse_s[i]), xycoords='data')\nplt.xlabel(\"Number of important features\")\nplt.ylabel(\"rmse\")\nplt.xlim(1,max(range(1,len(feature_importance)+1)))\nplt.legend(loc='upper right')\nplt.tight_layout()\n","27dd69c6":"# select top 8 features\n\ntop8 = feature_importance[:8]\n\n# pre-process the data\n\nX = ride.loc[:,top8].values\ny = ride['kudos_count'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)\n\n# Fit model\nforest = RandomForestRegressor(n_estimators=1000,criterion='mse',random_state=1,n_jobs=-1)\nforest.fit(X_train_std,y_train)\n\n# Predict and calculate rmse\nkudos_predictions_train_forest = forest.predict(X_train_std)\nkudos_predictions_test_forest = forest.predict(X_test_std)\ntrain_mse_forest = mean_squared_error(y_train,kudos_predictions_train_forest)\ntest_mse_forest = mean_squared_error(y_test,kudos_predictions_test_forest)\ntrain_rmse_forest = np.sqrt(train_mse_forest)\ntest_rmse_forest = np.sqrt(test_mse_forest)\nr2_train = r2_score(y_train,kudos_predictions_train_forest)\nr2_test = r2_score(y_test,kudos_predictions_test_forest)\n\n\nprint(f'rmse train {train_rmse_forest:.2f}')\nprint(f'rmse test {test_rmse_forest:.2f}')\nprint(f'r2 train {r2_train:.2f}')\nprint(f'r2 test {r2_test:.2f}')\n","404ef857":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(forest,X_train_std,y_train,train_sizes=np.linspace(0.1,1.0,10),cv=10,n_jobs=1)\n","0035f335":"# plot learning curve\nplt.style.use('default')\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(train_sizes, train_mean, color='b',marker='o',markersize=5, label='training accuracy')\nplt.fill_between(train_sizes,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\nplt.plot(train_sizes, test_mean, color='g',marker='s',linestyle='--',markersize=5, label='validation accuracy')\nplt.fill_between(train_sizes,test_mean+test_std,test_mean-test_std,alpha=0.15,color='g')","1cd9a1da":"# plot validation curve\nfrom sklearn.model_selection import validation_curve\n\nparam_range = [1,10,200,500,800,1000]\ntrain_scores, test_scores = validation_curve(forest,X_train_std,y_train,param_range=param_range,cv=10, param_name='n_estimators')\n\n\n","94bc10c4":"train_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\nplt.plot(param_range, train_mean, color='b',marker='o',markersize=5, label='training accuracy')\nplt.fill_between(param_range,train_mean+train_std,train_mean-train_std,alpha=0.15,color='blue')\nplt.plot(param_range, test_mean, color='g',marker='s',linestyle='--',markersize=5, label='validation accuracy')\nplt.fill_between(param_range,test_mean+test_std,test_mean-test_std,alpha=0.15,color='g')\n","32c80497":"The coeficient of determination or R squared ($R^2$) is the proportion of the variance in the dependent variable that is predictable from the independent variable(s). In other words how much x is explaining y.\n\nIt normally vary from 0 to 1 being 1 the perfect explanation. \n\nNot too bad for train but 0.4 for test is not impressive","e3fef000":"#  Feature engineering and selection\n\nNow that we have explored the trends and relationships within the data, we can work on engineering a set of features for our models. We can use the results of the EDA to inform this feature engineering. In particular, we learned the following from EDA which can help us in engineering\/selecting features:\n\nBefore we get any further, we should define what feature engineering and selection are! These definitions are informal and have considerable overlap, but I like to think of them as two separate processes:\n\n* Feature Engineering: The process of taking raw data and extracting or creating new features that allow a machine learning model to learn a mapping beween these features and the target. This might mean taking transformations of variables, such as scalling variables or dummy variables for categorical data, so they can be used in a model. Generally, I think of feature engineering as adding additional features derived from the raw data.\n\n* Feature Selection: The process of choosing the most relevant features in your data. \"Most relevant\" can depend on many factors, but it might be something as simple as the highest correlation with the target, or the features with the most variance. In feature selection, we remove features that do not help our model learn the relationship between features and the target. This can help the model generalize better to new data and results in a more interpretable model. Generally, I think of feature selection as subtracting features so we are left with only those that are most important.\n    \nFeature engineering and selection are iterative processes that will usually require several attempts to get right.\n\nFeature engineering and selection often has the highest returns on time invested in a machine learning problem. It can take quite a while to get right, but is often more important than the exact algorithm and hyperparameters used for the model. If we don't feed the model the correct data, then we are setting it up to fail and we should not expect it to learn!\n\nIn this project, we will take the following steps for feature engineering:\n\n- Standard Scale features for better training\n- Create dummy variables for categorical (city)\n\nFor feature selection, we will do the following:\n\n- Drop all not relevant features we already identified in correlation analysis and cleaning\n- Remove non important features (done later with the random forest model)\n\nLet's get started\n\n\nreference: Template addaped from <a href=\"https:\/\/github.com\/WillKoehrsen\/machine-learning-project-walkthrough\/blob\/master\/Machine%20Learning%20Project%20Part%201.ipynb\">from<\/a>","14b90585":"This exercise was good fun to practive machine learning after studing 2 books the main chapters about Linear Regression, Model Evaluation and Testing\n\n   <ul>\n    <li><a href=\"https:\/\/www.amazon.com\/Python-Machine-Learning-Sebastian-Raschka\/dp\/1783555130\"> Python Machine Learning<\/a><\/li>\n    <li><a href=\"https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1491962291\/ref=sr_1_1?crid=222SHRAAS3TX9&keywords=hands+on+machine+learning+with+scikit+learn+and+tensorflow&qid=1564004741&s=books&sprefix=hands+on+macin%2Cstripbooks-intl-ship%2C476&sr=1-1\"> Hands-on Machine Leanrning with Skit-Learn & TensorFlow<\/a><\/li>\n   <\/ul>\n\nBy the way, those are two great books to give you a good foundation in Machine Learning and also to get started\n\nDespite the practice comment, which was more of a personal note the main conclusions was that the Hyperparameter tunning did nothing to improve the model.\n\nFor this particular model the random forest regressor demostrated to be the best predictior with a margin of erros of ~ 4.6 kudos which I can say is reasonable.\n\nFeature selection by importance did not improve the model and I was not able to reduce overfit by regularisation and playing with the tree parameters. Despite all the attempts the best model for this problem proved to be the Random Forest Regressor compared to all other options tested\n\n   \n   ","01b114a8":"# Exploratory data analysis\n\nAlways wanted to predict number of kudos from my ride and understand what influences it.\n\nWhat I noticed is that I do not received much kudos untill started to have more friends and also when I stsrted to give kudos as psycology explains the <a href=\"https:\/\/www.aqr.org.uk\/glossary\/reciprocity-bias\">reciprocity bias.<\/a> \n\nLets analyse all the features and identify the main patters and the features that has the highest correlation to target","4dc4405b":"Not too bad for a start. Good when there is a device and system that collects data automatically and systematically.","210366bb":"Finally we will then use only the top 8 most important feature, retrain our model and evaluate rmse and go with the best model for productions","4a15a415":"# Data cleaning and formatting \n\nThe goal in this step is to make sure the data is machine learning ready, there is no nulls, the features are in the right numeric format for analysis and modelling","0eab6366":"Out of those 3 models the one that looks most overfit is Random forest\n\nLR\nrmse train: 4.78\nrmse Mean CV: 5.04\n\n\nTree\nrmse train: 4.93\nrmse Mean CV: 5.35\n\n\nRF\nrmse train: 1.62\nrmse Mean CV: 4.52\n\nThe LR model is not much different which suggest some degree of linearity in the data that is good for this exercise. We are on the right directions\n\nWe can now do some Hyperparameter optimisation to fine tune the most promissing model i.e. RF\n\nFor this we can use sklearn to find optimum hyperparameter trying many diferent combinations without being a tedious task","1c4367f3":"Now let's see how the tree based models perform on the data","c3b0b544":"# Machine Learning Workflow\n\nAlthough the exact implementation details can vary, the general structure of a machine learning project stays relatively constant:\n\n- Load Data\n- Data cleaning and formatting\n- Exploratory data analysis \n- Feature engineering and selection\n- Establish a baseline and compare several machine learning models on a performance metric\n- Perform hyperparameter tuning on the best model to optimize it for the problem\n- Evaluate the best model on the testing set\n- Interpret the model results to the extent possible\n- Draw conclusions \n\nSetting up the structure of the pipeline ahead of time lets us see how one step flows into the other. However, the machine learning pipeline is an iterative procedure and so we don't always follow these steps in a linear fashion. We may revisit a previous step based on results from further down the pipeline. For example, while we may perform feature selection before building any models, we may use the modeling results to go back and select a different set of features. Or, the modeling may turn up unexpected results that mean we want to explore our data from another angle. Generally, you have to complete one step before moving on to the next, but don't feel like once you have finished one step the first time, you cannot go back and make improvements!\n\nI try to focus more on the implementations of the methods rather than explaining them at a low-level, but have provided resources for those who want to go deeper. For the single best book (in my opinion) for learning the basics and implementing machine learning practices in Python, check out Hands-On Machine Learning with Scikit-Learn and Tensorflow by Aurelion Geron.\n\n","5a4b0254":"Will drop comments as it leaks information as commends only happens when a ride is complete and is not input to predict kudos. However correlation does not suggest as people not always comment.","4005adf9":"From the above chart we can observe that after the 8th feature, adding an extra feature it not improving the model performance in validation data, actually it is only getting worse and then flatens out","4ff78410":"Lets set it as the baseline to improve the model\n\n","d5d77eb8":"# Analysing feature importance\n\nRandom forest has a interesting result from its traing called  <a href=\"https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\">feature importance<\/a>\n\n\n\nLet's dig into it","dd998068":"Check for missing values","050728c9":"# Predicting Kudos from Strava Activities History\n\nStrava is the most popular social network app for people who cycle or do triathon or running or any or a a comp=bination of them.\nFor me is the social media that I use the most and it tracks your activities in Running, Cycling, Swimming and others like walking and even people record gym workout, which I never did.\n\n99% of my activities are cycling which is the sport I do the most, but I did some sowimming and running as once I did some triatlon training.\n\nAs any social media people are able to give kudos in activities, but different from other social media platforms, you are not allowed to give yourself kudos, which makes totally sense.\n\nOne big motivation for athletes is to receive kudos specially if you go the extra mile and do a long activity.\n\nI have been taking data science as a career more seriously in the last 3 months when I decided to learn python for data science properly and the main packages and part of my learning I am practicing as much as possible to retain and improve what I have learned so far. My intention is to learn and get better and solve more and more challenging problems with data analysis and machine learning.\n\nAs part of this exercise, in this notebook I will analyse my strava feed from the last 3 years and build a regression mode to predict how many kudos an activity woudl receive based on the statistics recorded.\n\nI am not getting activity stream data for this purpuse but only the summary activity statistics.\n\nIf you wanted to understand how to do this I have this ipython notebook where I go step by step on how to do it.\n\nI already downloaded and saved as a csv file which I will use for this notebook","6833cfab":"Everything raise a bit and some droped and some moved significantly like max speed. Now lets plot again scatter matrix and check for potential drops or stay with those variables to train the model.\n\nRegarding correlation now the strongest one is max_speed with 0.67.\n\nGeographical coordinates there is no correlation. Close to zero.\n\nAverage speed is negatively correlated but not very strong.\n\nElevation low is weakly correlated.\n","2e433079":"Amazing how well compared to others random forest performed. The test was better than the others training run.\n\nthe r2 was 0.96 which was great the the rmse 1.62. I would be happy with that in production data, but still the rmse test is 4.63\n\nLet's see if we can do some feature selection and try to improve this\n\n","be08bf84":"actually performs worse \ud83e\udd14 with only 8 features in the test data rmse went from 4.63 to 4.74.\n\nOne final thing I would like to do it to plot the learning and validation curve to check if we can fix the overfiting issue and if we can get a more regularised model","9f7b2e1f":"Lets get down to real test with test data\n\nNow for the analysis and evaluation of the model will plot the residuals compating train and test predictions to true labels\n\nwill do with the regression, then decision tree and finally the random forest","d6290c26":"Not too bad but the model looks a bit overfit. On the train set we get Kudos right within aproximate 5 kudos.\n\nBefore we touch the training set for validation we can use the sklearn cross validation function. This function is a great way to validate the model without touching the training set until we are confident we have a good model.\n\nWhat Cross Validation does is split the train set in k folds, usually 10 and train in k-1 fold and validate(test) on the reminder fold and loop though all combination without replacement. \nSklearn calculate the metric you chose e.g. mse then it is possible to calculate the average and std deviation to evaluate the model and if it is overfit.","5637552b":"Potentially I could drop distance which is very correlated to moving time but less related to kudos count.","09e8400c":"# Load Data\n\nAs mentioned above you can find <a href=\"https:\/\/github.com\/pedrojunqueira\/ML-Practice\/blob\/master\/Strava%20Data%20Analysis.ipynb\">here<\/a> in detail how the data was imported from strava\n\n\n\nfor this exercise I am loading the file from the working directory where this notebook is\n","d1333d2f":"rmse is slighly worse which shows that is not too overfit.\n\nOne way to pick a good model is to quickly train other models and then compare the rmse with the baseline of the plain Linear regression model.\n\nOther two options are decision trees and random forest\n\nLets fit them and validate on the training set","cb6f4700":"The first was far but second and thrid not too bad","0383328a":"for now lets set the parameter of random forest back to original and proceed with the run on test data\n\n","bdefb3e4":"my theory is that kudos is a function of popularity and also number of people following me and also a question of reciprocity. I mean if you give lots of kudos you tend to receive lots of kudos as well. 2017 was the year I really started to get into cycling more regularly and built a coommunity around friends and socialising.\n\nIt is possible to retrieve in strava current static number of friends but there is not historical information of number of friends and how it changed overtime. If had this data would be great feature. I believe a strong correlated one to rides.\n\nNow I Would like to check correlation matrix and pair plot again to check if much changed, since we are looking at a time where I did considerable more riding and longer rides and has much more friends connected.","2bf715c5":"So far I am happy with the data cleanning and we are at the stage data can be analysed and there is no missing value and all columns are numeric. Except for City which will deal with this later.","fd41cb53":"# Establish a baseline \n\nNow it is important to starting evaluating the metric with all features and stablish a baseline for analysis of the results and then do some more feature engineering as well as hyperparameter tunning\n\nfor a regression model the performance metris is rmse <a href=\"https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation\">(root mean squared error)<\/a>\n\n\n\nLets calculate it for a non regularised plain Linear Regression first and calculate rmse for train and test\n\n<a href=\"https:\/\/en.wikipedia.org\/wiki\/Regularization_(mathematics)\">Regularization reference<\/a> \n                          ","10b30ae9":"Plotting the decision tree indicated that it is much better to use to predict discrete variables lets look at the test predictions. Also the model looks to be less overfit and the maximum prediciton is 30.79\n","03652449":"Finally lets plot it to visualise","46d56839":"Total elevation gain, max_speed, moving time, achievement count, kilojoules and distance is responsible to ~ 80% of the model predictions importance","52e80c4d":"The first one the error was quite high and the last 2 were not far off if you round them\nlets calculate mean squared error for the training and test set","88496d65":"In the end it got a bit worse did I do anything wrong?\n\nRF rmse train: 1.62 rmse Mean CV: 4.52\n\nRF rmse train: 2.17 rmse Mean CV: 4.87 (below)\n\nLet's do CV on the best estimator","67a8613b":"Looks like no one cared about kudos or I did not have enough good friends or a supporting cycling community before mid 2017. For this analysis then, let's drop the rides before June 2017","351b8f42":"# How can I improve this project\n\nI really enjoyed this project and I learned a lot as it was the most compregensive analysis using python I have ever done. What I can see that can be improved.\n\n    - Use more def() functions and pipeline from sklearn to automate the process with new data\n    - Try regularised linear model such as LASSO, Rige and Elastic net and play with parameters\n    - Try to solve this with deep learning\n   \nAny suggestions?\n\nWhat went wrong?","880b23da":"# Elimininate features that are very close to each other","f2f01099":"Lets first\n\n - drop coordinates and time rows\n - create city dummy variables\n - Split test and trainset\n - Scale variables\n","5cc1f293":"As I explained above kudos is a relation of friendship so the more friends you have more kudos.\n\nIf you uploaded a ride on strava and no one saw it, did it really happened? \ud83e\udd14\n\nSo let's have a look at my kudos for every ride over time","439bf114":"Does not explain or say much about the data or kudos.\n\nNot much pattern can be seen here.\n\nMy cycling habits are very stable and this plot show the pattern of me commuting to work and going for my rides to the Adelaide Hills on the weekends\n\nMy work is to the west and hills is to the east.\nMy house and city cafes are in the middle.","ba641e47":"# Conclusion"}}