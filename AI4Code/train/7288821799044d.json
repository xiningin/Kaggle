{"cell_type":{"33ae5766":"code","953ba2c3":"code","cdebe24f":"code","3e63b2e0":"code","d23c3099":"code","c1371aec":"code","7e7a9f4d":"code","4874f2c1":"code","6ba24db6":"code","3fd20f9f":"code","bec8574e":"code","86473e19":"code","5302753e":"code","c3d3d6e9":"code","0a682751":"code","839e4a47":"code","a75cf91d":"code","df27745a":"code","db709b21":"code","84e5c812":"code","b8fc90a6":"code","a48950e3":"code","634c311e":"code","ce1371bd":"code","f1dfb458":"code","26c298b0":"code","a6c155e7":"code","b2a0f8f4":"code","4dd3910d":"code","05c027ac":"code","c3327e4b":"code","d88ee7bd":"code","062a5a75":"code","cf4a2d74":"code","56599636":"code","a3c3c53c":"code","b64ed6de":"code","57d40b1b":"code","56abb442":"code","86e596a2":"code","5b005cf3":"code","f79c2271":"code","2d0834f4":"code","1e579962":"code","b1c5a3f0":"code","4028834d":"code","42beaeec":"code","5118395e":"code","e8817c4d":"markdown","86c785c7":"markdown","f9fdc6aa":"markdown","3de69ed0":"markdown","7a0584b8":"markdown","c84288f1":"markdown","43e89303":"markdown","acc84ef4":"markdown","bb63c70f":"markdown","3ec024dc":"markdown","c8911adc":"markdown","6868ee1e":"markdown","504dc22d":"markdown","19080bbf":"markdown","b4023000":"markdown","36c03f08":"markdown"},"source":{"33ae5766":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport glob\nimport random\nimport pickle\nimport joblib\nimport numpy.matlib\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LinearRegression\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adamax\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import LeakyReLU, Reshape, Add\nfrom tensorflow.keras.layers import Activation, Concatenate\nfrom tensorflow.keras.layers import SpatialDropout1D, Conv1D\nfrom tensorflow.keras.layers import AveragePooling1D, Dropout\nfrom tensorflow.keras.layers import Embedding, Dense, Flatten\nfrom tensorflow.keras.layers import Input, BatchNormalization","953ba2c3":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(2021)","cdebe24f":"def rmspe(y_true, y_pred):\n    return (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","3e63b2e0":"def rmspe_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    return (tf.math.sqrt(tf.reduce_mean(tf.math.square((y_true - y_pred) \/ y_true))))","d23c3099":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()","c1371aec":"def realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","7e7a9f4d":"def historical_volatility(series):\n    return np.sqrt(np.sum(series**2)\/len(series))","4874f2c1":"def count_unique(series):\n    return len(np.unique(series))","6ba24db6":"def get_stats_window(df, fe_dict, seconds_in_bucket, add_suffix = False):\n    df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n    \n    if add_suffix:\n        df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n    \n    return df_feature","3fd20f9f":"def process_trade_data(trade_files):\n    \n    trade_df = pd.DataFrame()\n    \n    for file in tqdm(glob.glob(trade_files)):\n        \n        # Read source file\n        df_trade_data = pd.read_parquet(file)\n        \n        # Feature engineering\n        df_trade_data['log_return'] = df_trade_data.groupby('time_id')['price'].apply(log_return)\n        df_trade_data['size_order_count_ratio'] = df_trade_data.apply(lambda x: 0 if x['order_count'] == 0 else x['size']\/x['order_count'], axis=1)\n        price_mean = df_trade_data['price'].mean()\n        df_trade_data['price1'] = df_trade_data['price'].apply(lambda x: x - price_mean)\n        \n        fet_engg_dict = {\n            'price': ['mean','std','sum'],\n            'size': ['mean','std','sum'],\n            'order_count': ['mean','std','sum'],\n            'size_order_count_ratio': ['mean','std','sum'],\n            'seconds_in_bucket': [count_unique],\n            'log_return': [realized_volatility,'mean','std','sum'],\n            'price1': [historical_volatility,'mean','std','sum']\n        }\n        \n        # Get the stats for different windows\n        df_feature = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 0, add_suffix = False)\n        df_feature_120 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 120, add_suffix = True)\n        df_feature_240 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 240, add_suffix = True)\n        df_feature_360 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 360, add_suffix = True)\n        df_feature_480 = get_stats_window(df_trade_data, fet_engg_dict, seconds_in_bucket = 480, add_suffix = True)\n        \n        # Merge all\n        trade_agg_df = df_feature.merge(df_feature_120, how = 'left', left_on = 'time_id_', right_on = 'time_id__120')\n        trade_agg_df = trade_agg_df.merge(df_feature_240, how = 'left', left_on = 'time_id_', right_on = 'time_id__240')\n        trade_agg_df = trade_agg_df.merge(df_feature_360, how = 'left', left_on = 'time_id_', right_on = 'time_id__360')\n        trade_agg_df = trade_agg_df.merge(df_feature_480, how = 'left', left_on = 'time_id_', right_on = 'time_id__480')\n        trade_agg_df = trade_agg_df.add_prefix('trade_')\n        \n        # Generate row_id\n        stock_id = file.split('=')[1]\n        trade_agg_df['row_id'] = trade_agg_df['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n        trade_agg_df.drop(['trade_time_id_'], inplace=True, axis=1)\n        \n        # Merge with parent df\n        trade_df = pd.concat([trade_df, trade_agg_df])\n    \n    del df_trade_data, trade_agg_df, df_feature\n    del df_feature_120, df_feature_240\n    del df_feature_360, df_feature_480\n    gc.collect()\n    \n    return trade_df","bec8574e":"def process_book_data(book_files):\n    \n    book_df = pd.DataFrame()\n    \n    for file in tqdm(glob.glob(book_files)):\n        \n        # Read source file\n        df_book_data = pd.read_parquet(file)\n        \n        # Feature engineering\n        df_book_data['wap1'] = (df_book_data['bid_price1'] *\n                                df_book_data['ask_size1'] +\n                                df_book_data['ask_price1'] *\n                                df_book_data['bid_size1'])  \/ (df_book_data['bid_size1'] +\n                                                               df_book_data['ask_size1'])\n\n        df_book_data['wap2'] = (df_book_data['bid_price2'] *\n                                df_book_data['ask_size2'] +\n                                df_book_data['ask_price2'] *\n                                df_book_data['bid_size2'])  \/ (df_book_data['bid_size2'] +\n                                                               df_book_data['ask_size2'])\n        \n        df_book_data['wap3'] = (df_book_data['bid_price1'] *\n                                df_book_data['bid_size1'] +\n                                df_book_data['ask_price1'] *\n                                df_book_data['ask_size1'])  \/ (df_book_data['bid_size1'] +\n                                                               df_book_data['ask_size1'])\n\n        df_book_data['wap4'] = (df_book_data['bid_price2'] *\n                                df_book_data['bid_size2'] +\n                                df_book_data['ask_price2'] *\n                                df_book_data['ask_size2'])  \/ (df_book_data['bid_size2'] +\n                                                               df_book_data['ask_size2'])\n\n        df_book_data['log_return1'] = df_book_data.groupby(['time_id'])['wap1'].apply(log_return)\n        df_book_data['log_return2'] = df_book_data.groupby(['time_id'])['wap2'].apply(log_return)\n        df_book_data['log_return3'] = df_book_data.groupby(['time_id'])['wap3'].apply(log_return)\n        df_book_data['log_return4'] = df_book_data.groupby(['time_id'])['wap4'].apply(log_return)\n        \n        wap1_mean = df_book_data['wap1'].mean()\n        df_book_data['waph1'] = df_book_data['wap1'].apply(lambda x: x - wap1_mean)\n        \n        wap2_mean = df_book_data['wap2'].mean()\n        df_book_data['waph2'] = df_book_data['wap2'].apply(lambda x: x - wap2_mean)\n        \n        wap3_mean = df_book_data['wap3'].mean()\n        df_book_data['waph3'] = df_book_data['wap3'].apply(lambda x: x - wap3_mean)\n        \n        wap4_mean = df_book_data['wap4'].mean()\n        df_book_data['waph4'] = df_book_data['wap4'].apply(lambda x: x - wap4_mean)\n        \n        df_book_data['wap_balance'] = abs(df_book_data['wap1'] - df_book_data['wap2'])\n        df_book_data['price_spread1'] = (df_book_data['ask_price1'] - df_book_data['bid_price1']) \/ ((df_book_data['ask_price1'] + df_book_data['bid_price1'])\/2)\n        df_book_data['price_spread2'] = (df_book_data['ask_price2'] - df_book_data['bid_price2']) \/ ((df_book_data['ask_price2'] + df_book_data['bid_price2'])\/2)\n        df_book_data['bid_spread'] = df_book_data['bid_price1'] - df_book_data['bid_price2']\n        df_book_data['ask_spread'] = df_book_data['ask_price1'] - df_book_data['ask_price2']\n        df_book_data['bid_ask_spread1'] = abs((df_book_data['bid_price1'] * df_book_data['bid_size1']) - (df_book_data['ask_price1'] * df_book_data['ask_size1']))\n        df_book_data['bid_ask_spread2'] = abs((df_book_data['bid_price2'] * df_book_data['bid_size2']) - (df_book_data['ask_price2'] * df_book_data['ask_size2']))\n        df_book_data['total_volume'] = (df_book_data['ask_size1'] + df_book_data['ask_size2']) + (df_book_data['bid_size1'] + df_book_data['bid_size2'])\n        df_book_data['volume_imbalance'] = abs((df_book_data['ask_size1'] + df_book_data['ask_size2']) - (df_book_data['bid_size1'] + df_book_data['bid_size2']))\n        \n        fet_engg_dict = {\n            'wap1': ['mean','std','sum'],\n            'log_return1': [realized_volatility,'mean','std','sum'],\n            'log_return2': [realized_volatility,'mean','std','sum'],\n            'log_return3': [realized_volatility,'mean','std','sum'],\n            'log_return4': [realized_volatility,'mean','std','sum'],\n            'wap_balance': ['mean','std','sum'],\n            'price_spread1': ['mean','std','sum'],\n            'price_spread2': ['mean','std','sum'],\n            'bid_spread': ['mean','std','sum'],\n            'ask_spread': ['mean','std','sum'],\n            'bid_ask_spread1': ['mean','std','sum'],\n            'bid_ask_spread2': ['mean','std','sum'],\n            'total_volume': ['mean','std','sum'],\n            'volume_imbalance': ['mean','std','sum'],\n            'waph1': [historical_volatility,'mean','std','sum']\n        }\n        \n        fet_engg_time_dict = {\n            'log_return1': [realized_volatility,'mean','std','sum'],\n            'log_return2': [realized_volatility,'mean','std','sum'],\n            'log_return3': [realized_volatility,'mean','std','sum'],\n            'log_return4': [realized_volatility,'mean','std','sum'],\n            'waph1': [historical_volatility,'mean','std','sum']\n        }\n        \n        # Get the stats for different windows\n        df_feature = get_stats_window(df_book_data, fet_engg_dict, seconds_in_bucket = 0, add_suffix = False)\n        df_feature_120 = get_stats_window(df_book_data, fet_engg_time_dict, seconds_in_bucket = 120, add_suffix = True)\n        df_feature_240 = get_stats_window(df_book_data, fet_engg_time_dict, seconds_in_bucket = 240, add_suffix = True)\n        df_feature_360 = get_stats_window(df_book_data, fet_engg_time_dict, seconds_in_bucket = 360, add_suffix = True)\n        df_feature_480 = get_stats_window(df_book_data, fet_engg_time_dict, seconds_in_bucket = 480, add_suffix = True)\n\n        # Merge all\n        book_agg_df = df_feature.merge(df_feature_120, how = 'left', left_on = 'time_id_', right_on = 'time_id__120')\n        book_agg_df = book_agg_df.merge(df_feature_240, how = 'left', left_on = 'time_id_', right_on = 'time_id__240')\n        book_agg_df = book_agg_df.merge(df_feature_360, how = 'left', left_on = 'time_id_', right_on = 'time_id__360')\n        book_agg_df = book_agg_df.merge(df_feature_480, how = 'left', left_on = 'time_id_', right_on = 'time_id__480')\n        book_agg_df = book_agg_df.add_prefix('book_')\n        \n        # Generate row_id\n        stock_id = file.split('=')[1]\n        book_agg_df['row_id'] = book_agg_df['book_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n        book_agg_df.drop(['book_time_id_'], inplace=True, axis=1)\n        \n        # Merge with parent df\n        book_df = pd.concat([book_df, book_agg_df])\n    \n    del df_book_data, book_agg_df, df_feature\n    del df_feature_120, df_feature_240\n    del df_feature_360, df_feature_480\n    gc.collect()\n    \n    return book_df","86473e19":"with open(\"..\/input\/orvp-django-unchained\/ORVP_Ready_Meatballs.txt\", 'rb') as handle: \n    data = handle.read()\n\nprocessed_data = pickle.loads(data)\ntrain_df = processed_data['train_df']\n\ndel processed_data\ngc.collect()","5302753e":"test_df = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\ntest_df['row_id'] = test_df['stock_id'].astype(str) + '-' + test_df['time_id'].astype(str)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","c3d3d6e9":"trade_test_df = process_trade_data('..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\/*')\nprint(f\"trade_test_df: {trade_test_df.shape}\")\ntrade_test_df.head()","0a682751":"test_df = pd.merge(test_df, trade_test_df, \n                   how='left', on='row_id', \n                   sort=False)\n\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","839e4a47":"book_test_df = process_book_data('..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\nprint(f\"book_test_df: {book_test_df.shape}\")\nbook_test_df.head()","a75cf91d":"test_df = pd.merge(test_df, book_test_df, \n                   how='left', on='row_id', \n                   sort=False)\n\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","df27745a":"vol_cols = []\nfor col in test_df.columns:\n    if 'realized_volatility' in col:\n        vol_cols.append(col)\n\nlen(vol_cols)","db709b21":"df_stock_id = test_df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', 'sum']).reset_index()\ndf_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\ndf_stock_id = df_stock_id.add_suffix('_stock')\ndf_stock_id.head()","84e5c812":"df_time_id = test_df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', 'sum']).reset_index()\ndf_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\ndf_time_id = df_time_id.add_suffix('_time')\ndf_time_id.head()","b8fc90a6":"test_df = test_df.merge(df_stock_id, how='left', \n                          left_on=['stock_id'], \n                          right_on=['stock_id__stock'])\n\ntest_df = test_df.merge(df_time_id, how='left', \n                          left_on=['time_id'], \n                          right_on=['time_id__time'])\n\ntest_df.drop(['stock_id__stock', 'time_id__time'], \n              axis = 1, inplace = True)\n\ndel df_stock_id, df_time_id\ngc.collect()\n\nprint(f\"test_df: {test_df.shape}\")","a48950e3":"test_df['size_tau'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique'])\ntest_df['size_tau_120'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_120'])\ntest_df['size_tau_240'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_240'])\ntest_df['size_tau_360'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_360'])\ntest_df['size_tau_480'] = np.sqrt(1\/test_df['trade_seconds_in_bucket_count_unique_480'])\n\ntest_df['size_tau2'] = np.sqrt(1\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_120'] = np.sqrt(0.8\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_240'] = np.sqrt(0.6\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_360'] = np.sqrt(0.4\/test_df['trade_order_count_sum'])\ntest_df['size_tau2_480'] = np.sqrt(0.2\/test_df['trade_order_count_sum'])\n\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","634c311e":"filter_cols = ['trade_time_id__120', 'trade_time_id__240', 'trade_time_id__360', 'trade_time_id__480', \n               'book_time_id__120', 'book_time_id__240', 'book_time_id__360', 'book_time_id__480']\n\ntest_df.drop(filter_cols, axis=1, inplace=True)\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","ce1371bd":"train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n\nl = []\nfor n in range(7):\n    l.append([(x-1) for x in ((ids+1)*(kmeans.labels_ == n)) if x > 0])\n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in tqdm(l):\n    newDf = train_df.loc[train_df['stock_id'].isin(ind)]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append(newDf)\n    \n    newDf = test_df.loc[test_df['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append(newDf)\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\nmat2 = pd.concat(matTest).reset_index()\n\nmat2 = pd.concat([mat2, mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","f1dfb458":"nnn = [\n    'time_id',\n    'trade_size_sum_0c1',\n    'trade_size_sum_1c1',\n    'trade_size_sum_3c1',\n    'trade_size_sum_4c1',\n    'trade_size_sum_6c1',\n    'trade_order_count_sum_0c1',\n    'trade_order_count_sum_1c1',\n    'trade_order_count_sum_2c1',\n    'trade_order_count_sum_4c1',\n    'trade_order_count_sum_6c1',\n    'trade_log_return_realized_volatility_0c1',\n    'trade_log_return_realized_volatility_1c1',\n    'trade_log_return_realized_volatility_3c1',\n    'trade_log_return_realized_volatility_4c1',\n    'trade_log_return_realized_volatility_6c1',\n    'book_log_return1_realized_volatility_0c1',\n    'book_log_return1_realized_volatility_1c1',\n    'book_log_return1_realized_volatility_3c1',\n    'book_log_return1_realized_volatility_4c1',\n    'book_log_return1_realized_volatility_6c1',\n    'book_bid_spread_sum_0c1',\n    'book_bid_spread_sum_1c1',\n    'book_bid_spread_sum_3c1',\n    'book_bid_spread_sum_4c1',\n    'book_bid_spread_sum_6c1',\n    'book_ask_spread_sum_0c1',\n    'book_ask_spread_sum_1c1',\n    'book_ask_spread_sum_3c1',\n    'book_ask_spread_sum_4c1',\n    'book_ask_spread_sum_6c1',\n    'book_total_volume_sum_0c1',\n    'book_total_volume_sum_1c1',\n    'book_total_volume_sum_3c1',\n    'book_total_volume_sum_4c1',\n    'book_total_volume_sum_6c1',\n    'book_volume_imbalance_sum_0c1',\n    'book_volume_imbalance_sum_1c1',\n    'book_volume_imbalance_sum_3c1',\n    'book_volume_imbalance_sum_4c1',\n    'book_volume_imbalance_sum_6c1',\n    'book_waph1_historical_volatility_0c1',\n    'book_waph1_historical_volatility_1c1',\n    'book_waph1_historical_volatility_3c1',\n    'book_waph1_historical_volatility_4c1',\n    'book_waph1_historical_volatility_6c1',\n    'size_tau_0c1',\n    'size_tau_1c1',\n    'size_tau_3c1',\n    'size_tau_4c1',\n    'size_tau_6c1',\n]\n\ntrain_df = pd.merge(train_df, mat1[nnn], how='left', on='time_id')\ntest_df = pd.merge(test_df, mat2[nnn], how='left', on='time_id')\n\ndel mat1, mat2\ngc.collect()\n\ntrain_df.shape, test_df.shape","26c298b0":"colNames = [col for col in list(train_df.columns) if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\ntrain_nn = train_df[colNames].copy()\ntest_nn = test_df[colNames].copy()\n\ntrain_nn.replace([np.inf, -np.inf], np.nan, inplace=True)\ntest_nn.replace([np.inf, -np.inf], np.nan, inplace=True)","a6c155e7":"# Get highly correlated columns to remove\n# df = train_df.loc[:, train_df.columns != 'target'].copy()\n# cor_matrix = df.corr().abs()\n# upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n# to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n# print(f\"Number of columns to drop: {len(to_drop)}\")\n# select_cols = [col for col in test_df.columns if col not in to_drop]\n# print(f\"Selected columns: {len(select_cols)}\")\n# print(select_cols)","b2a0f8f4":"select_cols = [\n    'stock_id', 'time_id', 'row_id', 'trade_price_mean', 'trade_price_std', 'trade_price_sum', 'trade_size_mean', 'trade_size_std', 'trade_size_sum', \n    'trade_order_count_mean', 'trade_order_count_std', 'trade_order_count_sum', 'trade_size_order_count_ratio_mean', 'trade_size_order_count_ratio_std',\n    'trade_size_order_count_ratio_sum', 'trade_log_return_realized_volatility', 'trade_log_return_mean', 'trade_log_return_std', 'trade_log_return_sum',\n    'trade_price1_historical_volatility', 'trade_price1_sum', 'trade_order_count_std_120', 'trade_log_return_mean_120', 'trade_log_return_sum_120',\n    'trade_price_std_240', 'trade_size_mean_240', 'trade_order_count_std_240', 'trade_log_return_mean_240', 'trade_log_return_sum_240', 'trade_price_std_360',\n    'trade_size_mean_360', 'trade_size_std_360', 'trade_order_count_mean_360', 'trade_order_count_std_360', 'trade_size_order_count_ratio_std_360',\n    'trade_log_return_mean_360', 'trade_log_return_sum_360', 'trade_price_std_480', 'trade_size_mean_480', 'trade_size_std_480', 'trade_size_sum_480',\n    'trade_order_count_mean_480', 'trade_order_count_std_480', 'trade_size_order_count_ratio_mean_480', 'trade_size_order_count_ratio_std_480',\n    'trade_log_return_realized_volatility_480', 'trade_log_return_mean_480', 'trade_log_return_std_480', 'trade_log_return_sum_480', 'book_wap1_sum',\n    'book_log_return1_realized_volatility', 'book_log_return1_mean', 'book_log_return3_mean', 'book_log_return4_mean', 'book_wap_balance_mean',\n    'book_wap_balance_sum', 'book_price_spread1_std', 'book_price_spread2_sum', 'book_bid_spread_mean', 'book_bid_spread_std', 'book_bid_spread_sum',\n    'book_ask_spread_mean', 'book_ask_spread_std', 'book_ask_spread_sum', 'book_bid_ask_spread1_mean', 'book_bid_ask_spread1_std', 'book_bid_ask_spread2_mean',\n    'book_bid_ask_spread2_std', 'book_total_volume_mean', 'book_total_volume_std', 'book_volume_imbalance_mean', 'book_log_return1_mean_120',\n    'book_log_return2_mean_120', 'book_log_return3_mean_120', 'book_log_return4_mean_120', 'book_log_return1_mean_240', 'book_log_return2_mean_240',\n    'book_log_return3_mean_240', 'book_log_return4_mean_240', 'book_log_return1_mean_360', 'book_log_return2_mean_360', 'book_log_return3_mean_360',\n    'book_log_return4_mean_360', 'book_log_return1_mean_480', 'book_log_return1_sum_480', 'book_log_return2_mean_480', 'book_log_return2_sum_480', \n    'book_log_return3_mean_480', 'book_log_return3_sum_480', 'book_log_return4_mean_480', 'book_log_return4_sum_480', 'book_waph1_std_480',\n    'trade_log_return_realized_volatility_mean_stock', 'trade_log_return_realized_volatility_std_stock', 'trade_log_return_realized_volatility_max_stock',\n    'trade_log_return_realized_volatility_min_stock', 'trade_log_return_realized_volatility_120_max_stock', 'trade_log_return_realized_volatility_120_min_stock',\n    'trade_log_return_realized_volatility_240_max_stock', 'trade_log_return_realized_volatility_240_min_stock', \n    'trade_log_return_realized_volatility_360_max_stock', 'trade_log_return_realized_volatility_360_min_stock', \n    'trade_log_return_realized_volatility_480_max_stock', 'trade_log_return_realized_volatility_480_min_stock', \n    'book_log_return1_realized_volatility_mean_stock', 'book_log_return1_realized_volatility_std_stock', 'book_log_return1_realized_volatility_max_stock',\n    'book_log_return1_realized_volatility_min_stock', 'book_log_return2_realized_volatility_max_stock', 'book_log_return2_realized_volatility_min_stock',\n    'book_log_return3_realized_volatility_max_stock', 'book_log_return3_realized_volatility_min_stock', 'book_log_return4_realized_volatility_max_stock',\n    'book_log_return4_realized_volatility_min_stock', 'book_log_return1_realized_volatility_120_max_stock', 'book_log_return2_realized_volatility_120_min_stock',\n    'book_log_return1_realized_volatility_240_min_stock', 'book_log_return2_realized_volatility_240_min_stock', \n    'book_log_return3_realized_volatility_240_min_stock', 'book_log_return1_realized_volatility_360_min_stock', \n    'book_log_return2_realized_volatility_360_min_stock', 'book_log_return3_realized_volatility_360_min_stock', \n    'book_log_return4_realized_volatility_360_min_stock', 'book_log_return1_realized_volatility_480_max_stock', \n    'book_log_return1_realized_volatility_480_min_stock', 'book_log_return2_realized_volatility_480_max_stock', \n    'book_log_return2_realized_volatility_480_min_stock', 'book_log_return3_realized_volatility_480_max_stock', \n    'book_log_return3_realized_volatility_480_min_stock', 'book_log_return4_realized_volatility_480_max_stock', \n    'book_log_return4_realized_volatility_480_min_stock', 'trade_log_return_realized_volatility_mean_time', 'trade_log_return_realized_volatility_std_time',\n    'trade_log_return_realized_volatility_max_time', 'trade_log_return_realized_volatility_min_time', 'trade_log_return_realized_volatility_360_min_time',\n    'trade_log_return_realized_volatility_480_max_time', 'trade_log_return_realized_volatility_480_min_time', 'book_log_return1_realized_volatility_std_time',\n    'book_log_return1_realized_volatility_min_time', 'book_log_return2_realized_volatility_480_min_time', 'size_tau', 'size_tau_480', 'size_tau2',\n    'trade_size_sum_0c1', 'trade_size_sum_1c1', 'trade_size_sum_3c1', 'trade_size_sum_4c1', 'trade_size_sum_6c1', 'trade_order_count_sum_1c1',\n    'trade_order_count_sum_2c1', 'trade_order_count_sum_4c1', 'trade_order_count_sum_6c1', 'trade_log_return_realized_volatility_1c1', 'book_bid_spread_sum_1c1',\n    'book_ask_spread_sum_1c1', 'book_total_volume_sum_0c1', 'book_total_volume_sum_1c1', 'book_total_volume_sum_3c1', 'book_total_volume_sum_4c1',\n    'book_total_volume_sum_6c1', 'book_volume_imbalance_sum_0c1', 'book_volume_imbalance_sum_3c1', 'book_volume_imbalance_sum_4c1',\n    'book_volume_imbalance_sum_6c1', 'book_waph1_historical_volatility_0c1', 'book_waph1_historical_volatility_1c1', 'book_waph1_historical_volatility_3c1',\n    'book_waph1_historical_volatility_4c1', 'size_tau_0c1', 'size_tau_1c1', 'size_tau_3c1', 'size_tau_4c1', 'size_tau_6c1', 'target'\n]\n\nlen(select_cols)","4dd3910d":"train_df = train_df[select_cols].copy()\ntest_df = test_df[[col for col in select_cols if col!='target']].copy()\nprint(f\"train_df: {train_df.shape} \\ntest_df: {test_df.shape}\")","05c027ac":"FOLD = 5\nSEEDS = [2018]\n\nfet_imp = 0\ncounter = 0\noof_score = 0\ny_pred_meta_lgb = np.zeros((train_df.shape[0], 1))\ny_pred_final_lgb = 0\n\ncat_cols_indices = [train_df.columns.get_loc('stock_id')]\nfeatures = [col for col in train_df.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\nXtrain = train_df[features].copy()\nYtrain = train_df['target'].copy()\nXtest = test_df[features].copy()\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = KFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(train_df)):\n        counter += 1\n        \n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n        sample_weight = 1\/np.square(train_y)\n        \n        model = LGBMRegressor(\n            boosting_type='gbdt', \n            num_leaves=175, \n            max_depth=9, \n            learning_rate=0.05, \n            n_estimators=1500,\n            objective='regression', \n            importance_type='gain',\n            device='gpu',\n            max_bin=100, \n            min_child_samples=500, \n            subsample=0.85, \n            subsample_freq=5, \n            colsample_bytree=0.55, \n            reg_lambda=0.05,\n            random_state=0,\n            bagging_seed=0,\n            feature_fraction_seed=0\n        )\n        \n        model.fit(train_x, train_y, eval_metric='rmse',\n                  eval_set=[(train_x, train_y), (val_x, val_y)],\n                  early_stopping_rounds=100, verbose=500,\n                  categorical_feature=cat_cols_indices,\n                  sample_weight=sample_weight)\n\n        y_pred = model.predict(val_x, num_iteration=model.best_iteration_)\n        y_pred_meta_lgb[val] += np.array([y_pred]).T\n        y_pred_final_lgb += model.predict(Xtest, num_iteration=model.best_iteration_)\n        \n        score = rmspe(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        fet_imp += model.feature_importances_\n        print(\"\\nSeed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        joblib.dump(model, f'.\/lgb_model_{idx + 1}C.txt')\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_lgb = y_pred_meta_lgb \/ float(len(SEEDS))\ny_pred_final_lgb = y_pred_final_lgb \/ float(counter)\nfet_imp = fet_imp \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))\n\ndel Xtrain, Ytrain, Xtest\ngc.collect()","c3327e4b":"FOLD = 5\nSEEDS = [2018]\n\nfet_imp = 0\ncounter = 0\noof_score = 0\ny_pred_meta_cb = np.zeros((train_df.shape[0], 1))\ny_pred_final_cb = 0\n\ncat_cols_indices = [train_df.columns.get_loc('stock_id')]\nfeatures = [col for col in train_df.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\nXtrain = train_df[features].copy()\nYtrain = train_df['target'].copy()\nXtest = test_df[features].copy()\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = KFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(train_df)):\n        counter += 1\n        \n        train_x, train_y = Xtrain.iloc[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain.iloc[val], Ytrain.iloc[val]\n        sample_weight = 1\/np.square(train_y)\n        \n        model = CatBoostRegressor(\n            objective='RMSE',\n            eval_metric='RMSE',\n            num_boost_round=1500,\n            max_ctr_complexity=15,\n            od_wait=500, \n            od_type='Iter',\n            learning_rate=0.05,\n            reg_lambda=0.05,\n            bootstrap_type='Poisson',\n            use_best_model=True, \n            subsample=0.8,\n            grow_policy='Lossguide',\n            min_data_in_leaf=500, \n            task_type='GPU',\n            random_state=0\n        )\n\n        model.fit(train_x, train_y, eval_set=[(val_x, val_y)], \n                  cat_features=cat_cols_indices, \n                  sample_weight=sample_weight, \n                  early_stopping_rounds=100, verbose=500)\n\n        y_pred = model.predict(val_x)\n        y_pred_meta_cb[val] += np.array([y_pred]).T\n        y_pred_final_cb += model.predict(Xtest)\n        \n        score = rmspe(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        fet_imp += model.feature_importances_\n        print(\"\\nSeed-{} | Fold-{} | OOF Score: {}\\n\".format(seed, idx, score))\n        \n        joblib.dump(model, f'.\/cb_model_{idx + 1}C.txt')\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_meta_cb = y_pred_meta_cb \/ float(len(SEEDS))\ny_pred_final_cb = y_pred_final_cb \/ float(counter)\nfet_imp = fet_imp \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))\n\ndel Xtrain, Ytrain, Xtest\ngc.collect()","d88ee7bd":"# Generate kfolds based on the knn++ algorithm\nout_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\nout_train = out_train.fillna(out_train.mean())\n\nnfolds = 5\nindex = []\ntotDist = []\nvalues = []\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = out_train.values\nmat = scaler.fit_transform(mat)\n\nnind = int(mat.shape[0]\/nfolds)\n\nmat = np.c_[mat, np.arange(mat.shape[0])]\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\nlineNumber = np.sort(lineNumber)[::-1]\n\nfor n in tqdm(range(nfolds)):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\nfor n in tqdm(range(nfolds)):\n    values.append([lineNumber[n]])    \n\ns=[]\nfor n in tqdm(range(nfolds)):\n    s.append(mat[lineNumber[n],:])    \n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in tqdm(range(nind-1)):    \n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n        \n        f = totDist[cycle]\/np.sum(totDist[cycle])\n        j = 0\n        kn = 0\n        \n        for val in f:\n            j += val        \n            if (j > luck[cycle]):\n                break\n            kn +=1\n        \n        lineNumber[cycle] = kn\n        \n        for n_iter in range(nfolds):    \n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\nfor n_mod in tqdm(range(nfolds)):\n    values[n_mod] = out_train.index[values[n_mod]]","062a5a75":"train_nn[['stock_id','time_id','target']] = train_df[['stock_id','time_id','target']]\ntest_nn[['stock_id','time_id']] = test_df[['stock_id','time_id']]\nprint(f\"train_nn: {train_nn.shape} \\ntest_nn: {test_nn.shape}\")\n\ndel train_df\ngc.collect()","cf4a2d74":"# Generate aggregate features\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'], inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\nmat2 = pd.concat([mat2, mat1.loc[mat1.time_id==5]])\n\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)","56599636":"nnn = [\n    'time_id',\n    'trade_size_sum_0c1',\n    'trade_size_sum_1c1',\n    'trade_size_sum_3c1',\n    'trade_size_sum_4c1',\n    'trade_size_sum_6c1',\n    'trade_order_count_sum_0c1',\n    'trade_order_count_sum_1c1',\n    'trade_order_count_sum_2c1',\n    'trade_order_count_sum_4c1',\n    'trade_order_count_sum_6c1',\n    'trade_log_return_realized_volatility_0c1',\n    'trade_log_return_realized_volatility_1c1',\n    'trade_log_return_realized_volatility_3c1',\n    'trade_log_return_realized_volatility_4c1',\n    'trade_log_return_realized_volatility_6c1',\n    'book_log_return1_realized_volatility_0c1',\n    'book_log_return1_realized_volatility_1c1',\n    'book_log_return1_realized_volatility_3c1',\n    'book_log_return1_realized_volatility_4c1',\n    'book_log_return1_realized_volatility_6c1',\n    'book_bid_spread_sum_0c1',\n    'book_bid_spread_sum_1c1',\n    'book_bid_spread_sum_3c1',\n    'book_bid_spread_sum_4c1',\n    'book_bid_spread_sum_6c1',\n    'book_ask_spread_sum_0c1',\n    'book_ask_spread_sum_1c1',\n    'book_ask_spread_sum_3c1',\n    'book_ask_spread_sum_4c1',\n    'book_ask_spread_sum_6c1',\n    'book_total_volume_sum_0c1',\n    'book_total_volume_sum_1c1',\n    'book_total_volume_sum_3c1',\n    'book_total_volume_sum_4c1',\n    'book_total_volume_sum_6c1',\n    'book_volume_imbalance_sum_0c1',\n    'book_volume_imbalance_sum_1c1',\n    'book_volume_imbalance_sum_3c1',\n    'book_volume_imbalance_sum_4c1',\n    'book_volume_imbalance_sum_6c1',\n    'book_waph1_historical_volatility_0c1',\n    'book_waph1_historical_volatility_1c1',\n    'book_waph1_historical_volatility_3c1',\n    'book_waph1_historical_volatility_4c1',\n    'book_waph1_historical_volatility_6c1',\n    'size_tau_0c1',\n    'size_tau_1c1',\n    'size_tau_3c1',\n    'size_tau_4c1',\n    'size_tau_6c1',\n]\n\ntrain_nn = pd.merge(train_nn, mat1[nnn], how='left', on='time_id')\ntest_nn = pd.merge(test_nn, mat2[nnn], how='left', on='time_id')\nprint(f\"train_nn: {train_nn.shape} \\ntest_nn: {test_nn.shape}\")\n\ndel mat1,mat2\ngc.collect()","a3c3c53c":"# Get highly correlated columns to remove\n# df = train_nn.loc[:, train_nn.columns != 'target'].copy()\n# cor_matrix = df.corr().abs()\n# upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n# to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n# print(f\"Number of columns to drop: {len(to_drop)}\")\n# select_cols = [col for col in test_nn.columns if col not in to_drop]\n# print(f\"Selected columns: {len(select_cols)}\")\n# print(select_cols)","b64ed6de":"select_cols = [\n    'trade_price_mean', 'trade_price_std', 'trade_price_sum', 'trade_size_mean', 'trade_size_std', 'trade_size_sum', 'trade_order_count_mean', 'trade_order_count_std',\n    'trade_order_count_sum', 'trade_size_order_count_ratio_mean', 'trade_size_order_count_ratio_std', 'trade_log_return_realized_volatility', 'trade_log_return_mean',\n    'trade_log_return_std', 'trade_log_return_sum', 'trade_price1_historical_volatility', 'trade_price1_sum', 'trade_order_count_std_120', 'trade_log_return_mean_120',\n    'trade_log_return_sum_120', 'trade_price_std_240', 'trade_order_count_mean_240', 'trade_order_count_std_240', 'trade_log_return_mean_240', \n    'trade_log_return_sum_240', 'trade_price_std_360', 'trade_size_std_360', 'trade_order_count_mean_360', 'trade_order_count_std_360', \n    'trade_size_order_count_ratio_std_360', 'trade_log_return_mean_360', 'trade_log_return_sum_360', 'trade_price_std_480', 'trade_size_mean_480', \n    'trade_size_std_480', 'trade_size_sum_480', 'trade_order_count_mean_480', 'trade_order_count_std_480', 'trade_order_count_sum_480', \n    'trade_size_order_count_ratio_mean_480', 'trade_size_order_count_ratio_std_480', 'trade_log_return_realized_volatility_480', 'trade_log_return_mean_480', \n    'trade_log_return_std_480', 'trade_log_return_sum_480', 'book_wap1_sum', 'book_log_return1_realized_volatility', 'book_log_return1_mean', 'book_wap_balance_mean',\n    'book_wap_balance_sum', 'book_price_spread1_std', 'book_price_spread2_mean', 'book_price_spread2_sum', 'book_bid_spread_mean', 'book_bid_spread_std',\n    'book_bid_spread_sum', 'book_ask_spread_mean', 'book_ask_spread_std', 'book_ask_spread_sum', 'book_bid_ask_spread1_mean', 'book_bid_ask_spread1_std',\n    'book_bid_ask_spread1_sum', 'book_bid_ask_spread2_mean', 'book_bid_ask_spread2_std', 'book_bid_ask_spread2_sum', 'book_total_volume_mean', 'book_total_volume_std',\n    'book_log_return1_mean_120', 'book_log_return3_mean_120', 'book_log_return4_mean_120', 'book_log_return1_mean_240', 'book_log_return2_mean_240',\n    'book_log_return3_mean_240', 'book_log_return4_mean_240', 'book_log_return1_mean_360', 'book_log_return2_mean_360', 'book_log_return3_mean_360',\n    'book_log_return4_mean_360', 'book_waph1_std_360', 'book_log_return1_mean_480', 'book_log_return2_mean_480', 'book_log_return3_mean_480',\n    'book_log_return4_mean_480', 'book_waph1_std_480', 'trade_log_return_realized_volatility_mean_stock', 'trade_log_return_realized_volatility_std_stock',\n    'trade_log_return_realized_volatility_max_stock', 'trade_log_return_realized_volatility_min_stock', 'trade_log_return_realized_volatility_120_min_stock',\n    'trade_log_return_realized_volatility_240_min_stock', 'trade_log_return_realized_volatility_360_max_stock', 'trade_log_return_realized_volatility_360_min_stock',\n    'trade_log_return_realized_volatility_480_max_stock', 'trade_log_return_realized_volatility_480_min_stock', 'book_log_return1_realized_volatility_mean_stock',\n    'book_log_return1_realized_volatility_std_stock', 'book_log_return1_realized_volatility_max_stock', 'book_log_return1_realized_volatility_min_stock', \n    'book_log_return2_realized_volatility_max_stock', 'book_log_return2_realized_volatility_min_stock', 'book_log_return3_realized_volatility_min_stock', \n    'book_log_return4_realized_volatility_max_stock', 'book_log_return4_realized_volatility_min_stock', 'book_log_return2_realized_volatility_120_min_stock',\n    'book_log_return3_realized_volatility_120_max_stock', 'book_log_return3_realized_volatility_120_min_stock', 'book_log_return4_realized_volatility_120_max_stock',\n    'book_log_return4_realized_volatility_120_min_stock', 'book_log_return1_realized_volatility_240_min_stock', 'book_log_return3_realized_volatility_240_max_stock',\n    'book_log_return3_realized_volatility_240_min_stock', 'book_log_return4_realized_volatility_240_max_stock', 'book_log_return4_realized_volatility_240_min_stock',\n    'book_log_return1_realized_volatility_360_min_stock', 'book_log_return2_realized_volatility_360_min_stock', 'book_log_return3_realized_volatility_360_max_stock',\n    'book_log_return3_realized_volatility_360_min_stock', 'book_log_return4_realized_volatility_360_min_stock', 'book_log_return1_realized_volatility_480_max_stock',\n    'book_log_return1_realized_volatility_480_min_stock', 'book_log_return2_realized_volatility_480_max_stock', 'book_log_return2_realized_volatility_480_min_stock', \n    'book_log_return3_realized_volatility_480_max_stock', 'book_log_return4_realized_volatility_480_max_stock', 'trade_log_return_realized_volatility_mean_time', \n    'trade_log_return_realized_volatility_std_time', 'trade_log_return_realized_volatility_max_time', 'trade_log_return_realized_volatility_min_time', \n    'trade_log_return_realized_volatility_120_min_time', 'trade_log_return_realized_volatility_240_min_time', 'trade_log_return_realized_volatility_360_min_time',\n    'trade_log_return_realized_volatility_480_max_time', 'trade_log_return_realized_volatility_480_min_time', 'book_log_return1_realized_volatility_std_time',\n    'book_log_return1_realized_volatility_max_time', 'book_log_return1_realized_volatility_min_time', 'book_log_return2_realized_volatility_max_time', \n    'book_log_return4_realized_volatility_min_time', 'book_log_return2_realized_volatility_360_min_time', 'book_log_return1_realized_volatility_480_max_time',\n    'book_log_return1_realized_volatility_480_min_time', 'book_log_return2_realized_volatility_480_max_time', 'book_log_return2_realized_volatility_480_min_time',\n    'book_log_return3_realized_volatility_480_max_time', 'book_log_return4_realized_volatility_480_max_time', 'trade_size_sum_0c1_x', 'trade_size_sum_1c1_x',\n    'trade_size_sum_3c1_x', 'trade_size_sum_4c1_x', 'trade_size_sum_6c1_x', 'trade_order_count_sum_1c1_x', 'trade_order_count_sum_2c1_x',\n    'trade_order_count_sum_4c1_x', 'trade_order_count_sum_6c1_x', 'trade_log_return_realized_volatility_1c1_x', 'book_bid_spread_sum_0c1_x',\n    'book_bid_spread_sum_1c1_x', 'book_ask_spread_sum_1c1_x', 'book_total_volume_sum_0c1_x', 'book_total_volume_sum_1c1_x', 'book_total_volume_sum_3c1_x',\n    'book_total_volume_sum_4c1_x', 'book_total_volume_sum_6c1_x', 'book_volume_imbalance_sum_0c1_x', 'book_volume_imbalance_sum_1c1_x',\n    'book_volume_imbalance_sum_3c1_x', 'book_volume_imbalance_sum_4c1_x', 'book_volume_imbalance_sum_6c1_x', 'book_waph1_historical_volatility_0c1_x',\n    'book_waph1_historical_volatility_1c1_x', 'book_waph1_historical_volatility_3c1_x', 'book_waph1_historical_volatility_4c1_x',\n    'book_waph1_historical_volatility_6c1_x', 'size_tau_0c1_x', 'size_tau_1c1_x', 'size_tau_3c1_x', 'size_tau_4c1_x', 'size_tau_6c1_x', 'stock_id', 'time_id',\n    'trade_size_sum_0c1_y', 'trade_size_sum_1c1_y', 'trade_size_sum_4c1_y', 'trade_size_sum_6c1_y', 'trade_order_count_sum_0c1_y',\n    'trade_order_count_sum_1c1_y', 'trade_order_count_sum_4c1_y', 'book_total_volume_sum_0c1_y', 'book_total_volume_sum_1c1_y', 'book_total_volume_sum_3c1_y',\n    'book_total_volume_sum_4c1_y', 'book_total_volume_sum_6c1_y', 'book_volume_imbalance_sum_0c1_y', 'book_volume_imbalance_sum_1c1_y',\n    'book_volume_imbalance_sum_3c1_y', 'book_volume_imbalance_sum_4c1_y', 'book_volume_imbalance_sum_6c1_y', 'target'\n]\n\nlen(select_cols)","57d40b1b":"for col in tqdm([col for col in select_cols if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]):\n    qt = QuantileTransformer(random_state=21,\n                             n_quantiles=2000, \n                             output_distribution='normal')\n    \n    train_nn[col] = qt.fit_transform(train_nn[[col]])\n    test_nn[col] = qt.transform(test_nn[[col]])","56abb442":"train_nn = train_nn[select_cols].copy()\ntest_nn = test_nn[[col for col in select_cols if col!='target']].copy()\n\nfeatures_to_consider = list(train_nn)\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\n\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\nprint(f\"train_nn: {train_nn.shape} \\ntest_nn: {test_nn.shape}\")","86e596a2":"cat_cols = ['stock_id']\nnum_cols = [col for col in test_nn.columns if not(col in cat_cols or col in {\"time_id\", \"target\", \"row_id\"})]\nlen(cat_cols), len(num_cols)","5b005cf3":"def dnn_model(data, catcols, numcols):\n    \n    num_inp = Input(shape=(numcols,))\n    \n    inputs = []\n    outputs = []\n    \n    for c in catcols:\n        num_unique_values = int(data[c].max())\n        embed_dim = 25\n        inp = Input(shape=(1,))\n        out = Embedding(input_dim=num_unique_values + 1, \n                        output_dim=embed_dim, \n                        embeddings_initializer='he_normal', \n                        name=c)(inp)\n        out = SpatialDropout1D(rate=0.2)(out)\n        out = Reshape(target_shape=(embed_dim, ))(out)\n        inputs.append(inp)\n        outputs.append(out)\n    \n    outputs.append(num_inp)\n    x = Concatenate()(outputs)\n    x = BatchNormalization()(x)\n    \n    x = Dense(units=512, kernel_initializer='he_normal', \n              kernel_regularizer=l2(0.0001))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    x = Dropout(rate=0.5)(x)\n    \n    x = Dense(units=128, kernel_initializer='he_normal', \n                kernel_regularizer=l2(0.0001))(x)\n    x = BatchNormalization()(x)\n    x = LeakyReLU()(x)\n    x = Dropout(rate=0.25)(x)\n\n    x_output = Dense(units=1, kernel_initializer='he_normal')(x)\n    \n    model = Model(inputs=[inputs, num_inp], outputs=x_output, \n                  name='DNN_Model')\n    return model","f79c2271":"model = dnn_model(train_nn, cat_cols, len(num_cols))\nmodel.summary()","2d0834f4":"plot_model(\n    model, to_file='.\/Deep_Neural_Model.png', \n    show_shapes=True, show_layer_names=True\n)","1e579962":"target_name='target'\nscores_folds = {}\nmodel_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\ncat_data = train_nn['stock_id']\nVERBOSE = 0\n\nn_folds = 5\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=2018)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train_nn)\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\n\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\ntest_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n\ntrain_nn[pred_name] = 0\ntest_nn[target_name] = 0\ntest_predictions_nn = np.zeros(test_nn.shape[0])\ny_pred_meta_dnn = np.zeros((train_nn.shape[0], 1))\ny_pred_meta_dnn = pd.DataFrame(y_pred_meta_dnn, columns=['target'])\n\nfor n_count in range(n_folds):\n    print(f'CV {counter}\/{n_folds}')\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n    \n    \n    model = dnn_model(train_nn, cat_cols, len(num_cols))\n    model.compile(loss=rmspe_loss, optimizer=Adamax(lr=1e-2))\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    num_data = X_train[features_to_consider]\n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n    \n    es = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss', patience=20, verbose=VERBOSE,\n        mode='min', restore_best_weights=True)\n\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', factor=0.2, patience=7, \n        verbose=VERBOSE, mode='min')\n    \n    chkpoint = tf.keras.callbacks.ModelCheckpoint(\n        f'.\/dnn_model_{counter}C.h5', \n        monitor='val_loss', verbose=VERBOSE, \n        save_best_only=True, mode='min')\n\n    model.fit([cat_data, num_data], \n              target,               \n              batch_size=2048,\n              epochs=200,\n              validation_data=([cat_data_test, num_data_test], y_test),\n              callbacks=[es, plateau, chkpoint],\n              validation_batch_size=len(y_test),\n              shuffle=True,\n              verbose=VERBOSE)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    y_pred_meta_dnn.loc[train_nn.time_id.isin(values[n_count]), 'target'] = preds\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds), 5)\n    print(f'Fold: {counter} [{model_name}] - OOF RMSPE: {score}')\n    scores_folds[model_name].append(score)\n    \n    tt = scaler.transform(test_nn[features_to_consider].values)\n    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10) \/ n_folds\n       \n    counter += 1\n    features_to_consider.append('stock_id')","b1c5a3f0":"y_pred_final_lgb = np.array([y_pred_final_lgb]).T\ny_pred_final_cb = np.array([y_pred_final_cb]).T\n\ny_pred_final_lgb = y_pred_final_lgb.clip(0, 1e10)\ny_pred_final_cb = y_pred_final_cb.clip(0, 1e10)\n\ny_pred_final_dnn = np.array([test_predictions_nn.clip(0, 1e10)]).T\ny_pred_meta_dnn = y_pred_meta_dnn.values.copy()","4028834d":"Xtrain_meta = np.concatenate((y_pred_meta_lgb, y_pred_meta_cb, y_pred_meta_dnn), axis=1)\nXtest_meta = np.concatenate((y_pred_final_lgb, y_pred_final_cb, y_pred_final_dnn), axis=1)\nYtrain = train_nn['target'].copy()\n\nprint(\"Xtrain_meta shape: {}\".format(Xtrain_meta.shape))\nprint(\"Xtest_meta shape: {}\".format(Xtest_meta.shape))","42beaeec":"FOLD = 5\nSEEDS = [2018]\n\noof_score = 0\ny_pred_final = 0\ncounter = 0\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score = 0\n    \n    kfold = KFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(train_nn)):\n        counter += 1\n\n        train_x, train_y = Xtrain_meta[train], Ytrain.iloc[train]\n        val_x, val_y = Xtrain_meta[val], Ytrain.iloc[val]\n        sample_weight = 1\/np.square(train_y)\n\n        model = LinearRegression()\n\n        model.fit(train_x, train_y, sample_weight = sample_weight)\n\n        y_pred = model.predict(val_x)\n        y_pred_final += model.predict(Xtest_meta)\n        \n        score = rmspe(val_y, y_pred)\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score \/ FOLD)))\n\n\ny_pred_final = y_pred_final \/ float(counter)\noof_score \/= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","5118395e":"submit_df = pd.DataFrame()\nsubmit_df['row_id'] = test_df['row_id']\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\".\/submission.csv\", index=False)\nsubmit_df.head()","e8817c4d":"### Book data","86c785c7":"## CatBoost","f9fdc6aa":"### Test data","3de69ed0":"## Helper Functions","7a0584b8":"## Correlation check","c84288f1":"## Additional features","43e89303":"## LightGBM","acc84ef4":"### Group features","bb63c70f":"### Trade data","3ec024dc":"### **Experiment Log:**\n\n|Version |Models Used |CV Score |LB Score| Changes Made\n| --- | --- | --- | --- | --- |\n|v1 | LightGBM, CatBoost <br> Deep Neural Network | 0.19779 | 0.20699 | Baseline\n|v2 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.19171 | 0.20484 | Linear regression (meta model) added\n|v3 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.19106 | 0.20482 | New features added\n|v4 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.19099 | 0.20449 | K-Fold stratification method modified\n|v5 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.19065 | 0.20549 | New lag features added\n|v6 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18927 | Error | New lag features added <br> Models architecture modified\n|v7 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18941 | 0.20243 | Modified feature engineering process\n|v8 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.189193 | 0.20227 | Models architecture modified\n|v9 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18919 | 0.20261 | DNN model architecture modified\n|v10 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18915 | NA | DNN model architecture modified\n|v11 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18922 | 0.20222 | DNN model architecture modified\n|v12 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18873 | 0.20259 | DNN model architecture modified\n|v13 | LightGBM, CatBoost <br> Deep Neural Network | 0.19200 | 0.20259 | Weighted average ensemble\n|v14 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18888 | Error | New features added\n|v15 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18764 | 0.20332 | Quantile transformation replaced with Min Max Scaler\n|v16 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18768 | Error | Missing values replaced with mean\n|v17 | LightGBM, CatBoost <br> Deep Neural Network <br> Gradient Boosting Regressor <br> Linear Regression (Meta) | 0.20154 | 0.20485 | Modified feature engineering process <br> New base model added\n|v18 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | 0.18807 | 0.20660 | New historical volatility features added\n|v23 | LightGBM, CatBoost <br> Deep Neural Network <br> Linear Regression (Meta) | TBD | TBD | Kfold cross validation <br> Special handling for DNN model","c8911adc":"## Import libraries","6868ee1e":"## Meta Model\n\n* **Linear Regression**","504dc22d":"## Deep Neural Model","19080bbf":"## Prepare testing data","b4023000":"## Create submission file","36c03f08":"## Load training data"}}