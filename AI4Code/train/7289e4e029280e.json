{"cell_type":{"1023a8d5":"code","acd9340c":"code","0868e201":"code","01300c94":"code","401ae63f":"code","6d97c4aa":"code","c869e794":"code","7b7c210d":"code","dc3e0dce":"code","8f958cb7":"code","b7571826":"code","c9401402":"code","5732743f":"code","4f354a49":"code","7d747e41":"markdown","dd819a9d":"markdown","9f50da3f":"markdown","aee71da3":"markdown","38bd11dd":"markdown","61a10cee":"markdown","8df9d10f":"markdown","a82a5f6f":"markdown","e5838c44":"markdown","3f62663d":"markdown","a1485ba6":"markdown","48ee7062":"markdown","f7847790":"markdown","9a3c9f40":"markdown","e63fc0d1":"markdown","06990f72":"markdown","d959a8d1":"markdown","eda58a7a":"markdown","9b204b27":"markdown","1d28df3d":"markdown","722d52bc":"markdown","bd5df163":"markdown"},"source":{"1023a8d5":"from PIL import Image\nImage.open('roosevelt.jpg')","acd9340c":"Image.open('forest_area.PNG')","0868e201":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport scipy.stats as st\npd.set_option('display.max_columns', None)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2,mutual_info_classif\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_selection import RFE","01300c94":"df=pd.read_csv('covertype.csv')\ndf.head(2)","401ae63f":"plt.figure(figsize=(6,6))\ndf['class'].value_counts(normalize=True).plot.pie(autopct='%1.f%%')\nplt.title('Distribution of classes')\nplt.show()","6d97c4aa":"print(\"Any missing sample in set:\",df.isnull().values.any())\nprint('Number of records in dataset are {} and Features are {}.'.format(*df.shape))","c869e794":"anova_results={}\nnum_cols=df.columns.to_list()[:10]\n\nfor i in num_cols:\n    \n    d7=df[df['class']==7][i]\n    d6=df[df['class']==6][i]\n    d5=df[df['class']==5][i]\n    d4=df[df['class']==4][i]\n    d3=df[df['class']==3][i]\n    d2=df[df['class']==2][i]\n    d1=df[df['class']==1][i]\n\n    static,p_value=st.f_oneway(d1,d2,d3,d4,d5,d6,d7)\n    anova_results[i]=[static,p_value]\n    \ndf_anova=pd.DataFrame(anova_results).T\ndf_anova=df_anova.rename(columns={0:'F_statistic',1:'p_value'})\ndf_anova=df_anova.sort_values(by=['p_value','F_statistic'],ascending=[False,False])\ndf_anova['Significant']=df_anova['p_value'].apply(lambda x : 'True' if x<0.05  else 'False')\ndf_anova","7b7c210d":"X=df.drop('class',1)\nY=df['class']\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,stratify=Y)\n\ncat_cols=X_train.columns.to_list()[10:-2]\nnot_satisfied=[]\nchi2_results={}\n\nfor _ in cat_cols:\n    cross_table=pd.crosstab(X_train[_],Y_train)\n        \n    if np.any(np.sum(cross_table,1)<5):\n        not_satisfied.append(_)\n    elif len(cross_table)==1:\n        not_satisfied.append(_)\n        \n    else:\n        stat,p_value,dof=(st.chi2_contingency(cross_table)[0:3])\n        chi2_results[_]=[stat,p_value,dof]    \n            \n            \nprint(f'The column(s) not satisfied for chi2 test {not_satisfied}')\n\ndf_chi2=pd.DataFrame(chi2_results).T\ndf_chi2=df_chi2.rename(columns={0:'chi2_statistic',1:'p_value',2:'dof'})\ndf_chi2=df_chi2.sort_values(by=['p_value','chi2_statistic'],ascending=[False,False])\ndf_chi2['Significant']=df_chi2['p_value'].apply(lambda x : 'True' if x<0.05  else 'False')\ndf_chi2","dc3e0dce":"bestfeatures = SelectKBest(score_func=mutual_info_classif, k=10)\nfit = bestfeatures.fit(X_train,Y_train)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_train.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Columns','Score']  \nprint(featureScores.nlargest(10,'Score')) ","8f958cb7":"signi_cols=list(featureScores.nlargest(10,'Score')['Columns'].values)\n\nX_train_s=X_train[signi_cols]\nY_train_s=Y_train.copy()\nX_test_s=X_test[signi_cols]\nY_test_s=Y_test.copy()\n\nV_values=[vif(sm.add_constant(X_train_s).values,i) for i in range(sm.add_constant(X_train_s).shape[1])]\nl=list(zip(sm.add_constant(X_train_s).columns,V_values))\nl.sort(key=lambda x : x[1],reverse=True)\ndf_vif=pd.DataFrame(l,columns=['Feature','vif_value'])\ndf_vif","b7571826":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\n#X=df.drop('class', axis=1)\nY=df['class']\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=50), max_features=54)\nembeded_rf_selector.fit(X, Y)\n","c9401402":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)),'features are significant enough according to RandomForestClassifier. ')\nprint('\\n',embeded_rf_feature)","5732743f":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, max_features=54)\nembeded_lgb_selector.fit(X, Y)\n\nembeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)),'features are significant enough according to lightGBM')\nprint('\\n',embeded_lgb_feature)","4f354a49":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), \\\n                   n_features_to_select=10, step=10, verbose=5)\nrfe_selector.fit(X, Y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint('\\n',str(len(rfe_feature)), 'most significant features according to RFE are ')\nprint(rfe_feature)","7d747e41":"# 2) Mutual Information","dd819a9d":"## 3.1) SelectFromModel (RandomForestClassifier)","9f50da3f":"* As pvalue<0.5 in all cases, we reject null hypothesis in all cases.\n* Hence all quantitative features are significant in deciding class type.","aee71da3":"The Data set consists of cartographic variables which are used to predict the forest cover types in the four Wilderness Areas of Roosevelt National Forest, Colorado, USA. Independent variables were derived from data obtained from US Geological Survey (USGS) and United States Forest Service (USFS).","38bd11dd":"* We manually selected top 10 features which are having strong dependencies on target class.\n* Elevation being the most important feature with highest score.\n* Just check if our top 10 selected features have multicolinearity using VIF values.\n* High VIF values shows presence of multicolinearity.","61a10cee":"* Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n* The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances. \n* It can be used for univariate features selection using **mutual_info_classif from sklearn package**.","8df9d10f":"* No multicolinearity in these selected features, hence we can proceed with these features for model buliding and can check performance of model bulit on these features with base model built on all features.","a82a5f6f":"**Classes**\n* Spruce\/Fir (Picea pungens)\n* Lodgepole Pine (Pinus contorta)\n* Ponderosa Pine (Pinus ponderosa)\n* Cottonwood\/Willow (Populus deltoides)\n* Aspen (Populus tremuloides)\n* Douglas-fir (Pseudotsuga menziesii)\n* Krummholz","e5838c44":"### 1.1 One-Way Anova","3f62663d":"## 4) Recursive Feature Elimination\n","a1485ba6":"### 1.2 Chi Square","48ee7062":"* **Anova**     : For Continuous Independent feature + Categorical Target feature\n             \n             \n* **Chi-Square** :For Categorical Independent feature + Categorical Target feature","f7847790":"**Lets dive deep into feature information.**\n* All wilderness and soil types are dummified.\n* Target feature is 'class' with 7 categories.\n* Rest all features are scaled between 0 to 1.\n* As Number of features are large in this dataset, we need to select significant features for our analysis.","9a3c9f40":"* We will perform One-way Anova betweeen numerical features and our target categorical feature 'class', which have 7 categories. \n* The one-way ANOVA is used to determine whether there are any statistically significant differences between the means of particular numerical feature when divided into respective 7 class types.","e63fc0d1":"# 1) Anova+ Chi-Square","06990f72":"* Chi-square test also known as Chi-Square Test of Association is a nonparametric test. \n* It compares two variables in a contingency table to see if they are related. \n* In a more general sense, it tests to see whether distributions of categorical variables differ from each another.","d959a8d1":"Welcome to **ROOSEVELT NATIONAL PARK** :)","eda58a7a":"* H0:  \n         \n      Means of qualitative feature in all 7 different class subgroups is same. \n      And hence all these subgroups are similar and cant differentiate between each subgroups.\n      Therefore that particular qualitative feature can not predict class types.\n    \n* H1: \n\n       Means of qualitative feature in all 7 different class subgroups is different.\n       and hence that particular qualitative feature is significant in predicting class types.","9b204b27":"* As pvalue<0.5 in all cases, we reject null hypothesis in all cases.\n* Hence all categorical(dummified) features are significant in deciding class type except 'Soil_Type15' as its counts are less than 5.","1d28df3d":"\n**Happy learning!!**\n\n* **Random hypothesis testing**\n* H0: Opening this notebook was significant enough.\n* H1: Wasn't useful.\n\n**Upvote** If this notebook passed your Random hypothesis testing and you found this work useful :)\n\nFor more info visit:\nhttps:\/\/towardsdatascience.com\/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2\n","722d52bc":"* The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n* First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. \n* Then, the least important features are pruned from current set of features. \n* That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.","bd5df163":"## 3.2) SelectFromModel (LGBMClassifier)"}}