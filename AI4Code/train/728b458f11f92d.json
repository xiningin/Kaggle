{"cell_type":{"effb8865":"code","f7d71de9":"code","69d3e2f9":"code","881b899c":"code","7e9019af":"code","db25b65c":"code","5bd2b3af":"code","5abc33b9":"code","70abc6a8":"code","fe51bc92":"code","aea14feb":"code","474667c8":"code","b3299a77":"code","1184ffa9":"code","437d2001":"code","3c12e7ce":"code","d750f205":"code","86f5e458":"code","4e2c8da8":"code","c4f585b1":"code","d27c7116":"code","deebce16":"code","52c07187":"code","11b6f61a":"code","80918cca":"code","8a69a0d7":"code","64afa462":"code","cc059ba9":"code","ee29b62b":"code","2e813d40":"code","528ea27f":"code","cf156c75":"code","673b04ff":"markdown","93928e9b":"markdown","cca2e2b0":"markdown","84d6c21a":"markdown","ac040dbf":"markdown","82ed5086":"markdown","9330fe30":"markdown","24c33d06":"markdown","dfd53833":"markdown","e06805b9":"markdown","5d1f0437":"markdown","c3792421":"markdown","4e0bbb0d":"markdown","47327c12":"markdown","03efada6":"markdown","d7c48372":"markdown","9ca580bd":"markdown","b346e9eb":"markdown","3cfde5c0":"markdown","d8ef9429":"markdown","ae153d59":"markdown","e34abc6c":"markdown"},"source":{"effb8865":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7d71de9":"import tensorflow\ntensorflow.__version__","69d3e2f9":"df = pd.read_csv(\"..\/input\/bankchurn\/Churn_Modelling.csv\")\ndf.head()","881b899c":"df.isnull().sum()\n#There is no any missing values in the columns:","7e9019af":"df = df.drop([\"RowNumber\",\"CustomerId\",\"Surname\"],axis=1) \n# we drop \"RowNumber\",\"CustomerId\",\"Surname\" columns because they say nothing about bank churn\ndf.head()","db25b65c":"from sklearn.preprocessing import OrdinalEncoder\nle = OrdinalEncoder()\npd.DataFrame(le.fit_transform(df.select_dtypes(\"object\"))).head(20)","5bd2b3af":"df[\"Geography\"].value_counts()","5abc33b9":"for i in df.select_dtypes(\"object\").columns:\n    print(f\"Column {i} has these type of data: {df[i].nunique()}\")\n    print(\"***************************************************\")","70abc6a8":"df = pd.get_dummies(df,columns=[\"Geography\"])\ndf","fe51bc92":"df[\"Gender\"].value_counts() #Because Gender Column has binary values we will use LabelEncoder","aea14feb":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf[\"Gender\"] = le.fit_transform(df[\"Gender\"])\ndf","474667c8":"df.columns","b3299a77":"X = df.drop(\"Exited\",axis=1).values\ny = df[\"Exited\"].values","1184ffa9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","437d2001":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)\nX_train","3c12e7ce":"y_train # We have not touched the target","d750f205":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping","86f5e458":"ann = Sequential() # Initializing the ANN\nann.add(Dense(units=6,activation=\"relu\")) #Adding First Layer\nann.add(Dense(units=6,activation=\"relu\")) # Adding Hidden Layer\nann.add(Dense(units=1,activation=\"sigmoid\")) # Adding Output Layer\n# here we add a the final layer with 1 neurons because we have one output, that is the customer will exit or not","4e2c8da8":"ann.compile(optimizer=\"adam\",loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","c4f585b1":"ann.fit(x=X_train, y= y_train, batch_size =32, epochs=100, validation_data=(X_test,y_test),callbacks=EarlyStopping(monitor='val_loss',patience=4))","d27c7116":"ann.summary()","deebce16":"pd.DataFrame(ann.history.history)","52c07187":"import matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\npd.DataFrame(ann.history.history).plot(figsize=(12,10))","11b6f61a":"#This is the final result of or model in the training set\nann.evaluate(X_train,y_train)","80918cca":"#This is the final result of or model in the test set\nann.evaluate(X_test,y_test)","8a69a0d7":"X[0] ","64afa462":"ann.predict(ss.transform([[600,1,40,3,60000,2,1,1,50000,1,0,0]]))\n#This give us the probability which shows that this customer has very low percent of leaving the bank.","cc059ba9":"print(f\"Is this customer will churn the bank = {np.ravel(ann.predict(ss.transform([[600,1,40,3,60000,2,1,1,50000,1,0,0]])))>0.5}\")","ee29b62b":"predictions = ann.predict(X_test) # This a 2D array, we will transform it into 1D array.\npredictions_df = pd.DataFrame(np.ravel(predictions),columns=[\"Predictions\"])\ncomparison_df = pd.concat([pd.DataFrame(y_test,columns=[\"Real Values\"]), predictions_df],axis=1)\ncomparison_df ","2e813d40":"comparison_df[\"Binary Predictions\"] = comparison_df[\"Predictions\"].apply(lambda x: 1 if x>0.5 else 0)\ncomparison_df[\"Will Customer Churn\"] = comparison_df[\"Predictions\"].apply(lambda x: True if x>0.5 else False)\ncomparison_df","528ea27f":"predictions","cf156c75":"from sklearn.metrics import classification_report, confusion_matrix,plot_confusion_matrix\nprint(confusion_matrix(comparison_df[\"Real Values\"],comparison_df[\"Binary Predictions\"]))\nprint(classification_report(comparison_df[\"Real Values\"],comparison_df[\"Binary Predictions\"]))\n","673b04ff":"<font color=\"green\">\n2.One-Hot Encoding\n    \n    We apply One-Hot Encoding when:\n\n    The categorical feature is not ordinal (like the countries above)\n    The number of categorical features is less so one-hot encoding can be effectively applied\n    This is where the integer encoded variable is removed and one new binary variable is added for each unique integer     value in the variable.In the \u201ccolor\u201d variable example, there are three categories, and, therefore, three binary variables   are needed. A \u201c1\u201d value is placed in the binary variable for the color and \u201c0\u201d values for the other colors. This type is apparantly not suitable for our case.\n    The nominal categorical features having more than two values may get treated as ordinal one by the machine learning model. Although model performance won\u2019t suffer much, it is recommended to use One-Hot encoder for categorical features having more than two different types of value. In such cases, one may want to use One-hot encoder, also called as dummy encoding. One can also use get_dummies method on Pandas package. We will use dummi variables for Geography column which three distinct values.","93928e9b":"<font color=\"blue\">\nOur model has similar prediction performance with over %86 accuracy in both train and test set withou overfitting.","cca2e2b0":"<font color=\"red\">\n2. Step: Lets drop unnecessary columns","84d6c21a":"## PART I : Data Preprocessing:","ac040dbf":"## PART III: BUILDING ARTIFICIAL NEURAL NETWORKS","82ed5086":"<font color=\"green\">\n    Feature Scaling:\n    \n    if we work with deep learning model, we have to rescale features. We should apply feature scaler to only X features, not to the target column.\n    ","9330fe30":"<font color=\"red\">\n1. Step : Lets check missing values of each column","24c33d06":"<font color=\"blue\">\nThis figure above shows that our model does a great job for customer who will stay in the bank, but it does perform worse for customer who wants to leave the bank. ","dfd53833":"<font color=\"green\">\n1.Ordinal Encoding\n    \n    In ordinal encoding, each unique category value is assigned an integer value.\n    For example, \u201cred\u201d is 1, \u201cgreen\u201d is 2, and \u201cblue\u201d is 3. For categorical variables, it imposes an ordinal relationship     where no such relationship may exist. This can cause problems and a one-hot encoding may be used instead. The Ordinal Encoding does not suit to our case because we do not have any ordinal relationship in our categorical columns geography and gender. Thus we will ignore this encoding type","e06805b9":"<font color=\"green\">\n4.Label Encoding\n    \n    We use LabelEncoder when there are only two possible values of a categorical features. For example, features having       value such as yes or no. Or, maybe, gender feature when there are only two possible values including male or female.\n    We use LabelEncoder for label columns in case of supervised learning when it is binary classification problem.\n    We don\u2019t use LabelEncoder when the categorical features have more than two values","5d1f0437":"<font color=\"green\">\n3.1. Initializing the ANN and Adding Layers:","c3792421":"<font color=\"blue\">\nFirst step is to create the input in the same order as we feed the model during training the model.\nSecond Step is to add additional values because there are 12 inputs because geography is represented by 3 different columns.\nThird step is to standadize the input as we did for the feature before training the mode","4e0bbb0d":"<font color=\"blue\">\nChoosing an optimizer: Keep in mind what kind of problem we are trying to solve:\n\nFor a multi-class classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nFor a binary classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nFor a mean squared error regression problem:\n\nmodel.compile(optimizer='rmsprop', loss='mse')","47327c12":"<font color=\"blue\">\nLets make single predictions to test performance of the model\n    \n    f the customer with the following informations will leave the bank:\n\n    Geography: France\n\n    Credit Score: 600\n\n    Gender: Male\n\n    Age: 40 years old\n\n    Tenure: 3 years\n\n    Balance: $ 60000\n\n    Number of Products: 2\n\n    Does this customer have a credit card ? Yes\n\n    Is this customer an Active Member: Yes\n\n    Estimated Salary: $ 50000\n    \n    We need to transform these inputs to 2D array because ann.predict() method accepts only 2D arrays as an input.","03efada6":"## PART IV: MAKING PREDICTIONS AND EVALUATING THE MODEL PERFORMANCE","d7c48372":"<font color=\"green\">\n3.2.Compiling and Training the ANN:","9ca580bd":"<font color=\"blue\">\nWe have predictions as probability version,now I will transform them into 0 or value according to their position with respect to 0.5 probability.","b346e9eb":"## PART II: EXPLORATORY DATA ANALYSIS","3cfde5c0":"<font color=\"green\">\nThe next step is to transform columns with strings into numbers. Machine learning models require all input and output variables to be numeric. This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.\n\nThere are four common approaches for converting ordinal and categorical variables to numerical values. They are:\n\n\n2.One-Hot Encoding\n3.Dummy Variable Encoding\n4.Label Encodeing","d8ef9429":"<font color=\"green\">\n3.Dummy Variable Encoding\n    \n    The one-hot encoding creates one binary variable for each category.\n\n    The problem is that this representation includes redundancy. For example, if we know that [1, 0, 0] represents \u201cblue\u201d     and [0, 1, 0] represents \u201cgreen\u201d we don\u2019t need another binary variable to represent \u201cred\u201c, instead we could use 0         values for both \u201cblue\u201d and \u201cgreen\u201d alone, e.g. [0, 0].","ae153d59":"<font color=\"red\">\n3. Step: Encoding Categorical Data","e34abc6c":"<font color=\"green\">\nAs we can see there is ranking among the different categories for each column, but this is not a good option because there is no rangin among the categories, because they contain just nominal variables.For categorical variables where no ordinal relationship exists, the integer encoding may not be enough, at best, or misleading to the model at worst.\n\nForcing an ordinal relationship via an ordinal encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results"}}