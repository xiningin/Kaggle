{"cell_type":{"ace405f6":"code","08c55833":"code","9a518540":"code","0f29de91":"code","12539452":"code","c1b07077":"code","312b559c":"code","1324c966":"code","c543ce68":"code","eed8ec4b":"code","b8ce7181":"code","12cb92a6":"code","810621db":"code","a73d1ba0":"code","9abfd478":"code","29e9249f":"code","81c7223e":"code","b3a67502":"code","14030546":"code","646c81c1":"code","7c9e827f":"code","179b3f44":"code","324a3c24":"code","a16242cb":"code","d3e87092":"code","7df2c932":"markdown","a5aac601":"markdown","bc577351":"markdown","8e3c2d07":"markdown","6b561284":"markdown","46ce2364":"markdown","06199859":"markdown","23fdeeab":"markdown","b441e068":"markdown","a96709cf":"markdown","5322b612":"markdown","2b0bea8d":"markdown","1be4737c":"markdown","0883b573":"markdown","a7acd328":"markdown","ddf99985":"markdown"},"source":{"ace405f6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","08c55833":"df = pd.read_csv('..\/input\/earthquake-database\/database.csv')","9a518540":"# lets check the first five rows of data\ndf.head()","0f29de91":"# shape of data\ndf.shape","12539452":"df.info()","c1b07077":"# Checking for Missing values\ndf.isnull().sum()","312b559c":"def preprocess_input(df):\n    df = df.copy()\n    # dropping ID as its an unrelevant feature.\n    df.drop('ID', axis=1,inplace=True)\n    # checking for features in which greater than 66%(2\/3) of data is missing\n    null_columns = df.loc[:, df.isna().sum() > 0.66 * df.shape[0]].columns\n    # dropping those columns \n    df.drop(null_columns, axis=1,inplace=True)\n    # filling missing values by substituting mean value in rms columns \n    df['Root Mean Square'] = df['Root Mean Square'].fillna(df['Root Mean Square'].mean())\n    # filling missing values by substituting mode in Magnitude type columns \n    df['Magnitude Type'] = df['Magnitude Type'].fillna(df['Magnitude Type'].mode()[0])\n    df.reset_index(drop=True,inplace=True)\n    # converting Date into pandas datetime \n    df['Date'] = pd.to_datetime(df['Date'],utc=True)\n    # converting Time into pandas datetime \n    df['Time'] = pd.to_datetime(df['Time'],utc=True)\n    # Extracting Year and Month From Date column and converting it into integer\n    df['Year'] = df['Date'].apply(lambda x: str(x)[0:4]).astype(np.int)\n    df['Month'] = df['Date'].apply(lambda x: str(x)[5:7]).astype(np.int)\n    # Extracting Hour From Time column and converting it into integer\n    df['Hour'] = df['Time'].apply(lambda x: str(x)[11:13]).astype(np.int)\n    \n    # dropping Date and Time as we dont need it anymore.\n    df.drop(['Date','Time'], axis=1,inplace=True)\n    \n    # encoding Status to make it numerical.\n    df['Status'] = df['Status'].map({'Automatic':0,\n                                    'Reviewed':1}).astype('int')\n    \n    return df","1324c966":"df = preprocess_input(df)","c543ce68":"# checking missing values again.\ndf.isnull().sum()","eed8ec4b":"# creating list of categorical columns for one hot encoding\ncategorical_columns = [col for col in df.columns if df.dtypes[col] == 'object']\n\n# creating list of numerical columns to standardized data \nnumerical_columns = [col for col in df.columns if ((df.dtypes[col] != 'object') & (col != 'Status'))]\n\nprint('Numerical Features are : ',numerical_columns)\nprint('Categorical Features are : ',categorical_columns)","b8ce7181":"# one hot encoding for categorical features \ndef onehot_encoder(df, cols):\n    df = df.copy()\n    for col in cols:\n        dummies = pd.get_dummies(df[col], drop_first=True)\n        df = pd.concat([df, dummies], axis=1)\n        df.drop(col, axis=1,inplace=True)\n    return df","12cb92a6":"df = onehot_encoder(df,categorical_columns)","810621db":"sc = StandardScaler()\ndf[numerical_columns] = sc.fit_transform(df[numerical_columns])","a73d1ba0":"df.head()","9abfd478":"X = df.drop('Status',axis=1)\ny = df['Status']","29e9249f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)","81c7223e":"# Create a `Sequential` model and add a Dense layer as the first layer.\nmodel = Sequential()\nmodel.add(Input(shape=(X_train.shape[1],)))\nmodel.add(Dense(32, activation='relu'))\n# Now the model will take as input arrays of shape (None, 104)\n# and output arrays of shape (None, 32).\n# Note that after the first layer, you don't need to specify\n# the size of the input anymore:\nmodel.add(Dense(64, activation='relu'))\n# Only 1 output neuron. It will contain a value from 0-1 where 0 for class ('Automatic') \n# and 1 for the other ('Reviewed')\nmodel.add(Dense(units=1,activation='sigmoid'))","b3a67502":"#The following is the model summary of the model:\nmodel.summary()","14030546":"# Early Stopping\nmonitor = EarlyStopping(monitor='val_auc', patience=5, \n                        verbose=1, mode='max',restore_best_weights=True)\n# Defining Callbacks\n\nfilepath = '.\/best_weights.hdf5'\ncheckpoint    = ModelCheckpoint(filepath, \n                                monitor = 'val_auc', \n                                mode='max', \n                                save_best_only=True, \n                                verbose = 1)\n\nreduceLR = ReduceLROnPlateau()\ncallback_list = [monitor, checkpoint,reduceLR]\n\n# model compile\nmodel.compile(\n    optimizer=Adam(lr=0.001),\n    loss='binary_crossentropy',\n    metrics=[tf.keras.metrics.AUC(name='auc')]\n)","646c81c1":"history = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=25,\n    callbacks=callback_list,\n    verbose=1\n)","7c9e827f":"plt.figure(figsize=(20, 6))\n\ntrain_loss, val_loss = history.history['loss'], history.history['val_loss']\ntrain_auc, val_auc = history.history['auc'], history.history['val_auc']\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label=\"Training Loss\")\nplt.plot(history.history['val_loss'], label=\"Validation Loss\")\nplt.legend()\nplt.title(\"Model Loss\")\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['auc'], label=\"Training AUC\")\nplt.plot(history.history['val_auc'], label=\"Validation AUC\")\nplt.legend()\nplt.title(\"Model AUC\")\n\nplt.show()","179b3f44":"model.evaluate(X_test, y_test)","324a3c24":"y_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)","a16242cb":"cf_matrix = confusion_matrix(y_test,y_pred)\nsns.heatmap(cf_matrix,annot=True,fmt=\"d\")\nplt.title('Confusion Matrix', fontsize = 23)\nplt.show()","d3e87092":"y_test.value_counts()","7df2c932":"### Loading Dataset","a5aac601":"##### As we can see there are lot of missing values present, we need to handle it before model building.","bc577351":"### Data Preprocessing","8e3c2d07":"### Scaling Numerical Features","6b561284":"### Model Building","46ce2364":"### Separate dependent and independent variables","06199859":"### Model Prediction","23fdeeab":"### Result Visualization","b441e068":"### Import Required Libraries","a96709cf":"### OneHotEncoding for Categorical Features","5322b612":"###  Model Fitting","2b0bea8d":"## Earthquake Type Prediction","1be4737c":"### Model Evaluation","0883b573":"#### Specifying the optimizer and compile the model","a7acd328":"### Train-Test Split","ddf99985":"### Result"}}