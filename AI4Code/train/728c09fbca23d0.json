{"cell_type":{"647d2fd3":"code","aa88d4e0":"code","553ae7de":"code","ea64d7f4":"code","0f453684":"code","9d6c199f":"code","10cba51a":"code","d14ab5cb":"code","89eec120":"code","759f5e78":"code","f1e390ed":"code","5f73086e":"code","c1accaab":"code","18f0e9bd":"code","99c6051f":"code","11f338b6":"code","0d12120b":"code","46764895":"code","7b37122f":"code","ad2482b5":"code","61ce7ba7":"code","3a98f1cf":"code","2157917d":"code","61b07c80":"code","f2ebca52":"code","cd2d8ee1":"code","c0e79320":"code","12676c70":"code","eb497ecd":"code","974fac0c":"markdown","d98b48bb":"markdown","d2f11c8b":"markdown","64715edf":"markdown"},"source":{"647d2fd3":"from PIL import Image\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras.models import Model, load_model\nfrom keras.initializers import glorot_uniform\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom tensorflow.keras.regularizers import l2\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,Dropout\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","aa88d4e0":"\"\"\"img =  Image.open('..\/input\/label-1-v0\/my_1.png', 'r').convert('RGBA')\npix = pd.DataFrame(data=np.array(img.getdata()))\npix.describe()\"\"\"","553ae7de":"\"\"\"pix_val_test = pd.DataFrame(data=np.array(img.getdata())[:,0:3].sum())\npix_val_test = pix_val_test.values.reshape(-1,28,28,1)\npix_val_test = pix_val_test \/ 255.0\"\"\"","ea64d7f4":"\"\"\"plt.imshow(pix_val_test[0][:,:,0])\"\"\"","0f453684":"\n\"\"\"pix_val =pd.DataFrame(data=np.array(img.getdata())[:,0])\npix_val = pix_val.values.reshape(-1,28,28,1)\npix_val = pix_val \/ 255.0\"\"\"","9d6c199f":"train_set = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ny_train = train_set['label']\nx_train = train_set.drop(labels=['label'],axis=1)","10cba51a":"\"\"\"plt.imshow(pix_val[0][:,:,0])\"\"\"","d14ab5cb":"x_train = x_train.values.reshape(-1,28,28,1)\nx_train = x_train \/ 255.0\ny_train = to_categorical(y_train,num_classes = 10)","89eec120":"x_test =test_set.values.reshape(-1,28,28,1)\nx_test = x_test\/255.0\n","759f5e78":"\"\"\"model.add(Conv2D(32, (3, 3), activation=\"relu\"),\n\tkernel_regularizer=l2(0.0005))\"\"\"","f1e390ed":"# GRADED FUNCTION: identity_block\n\ndef identity_block(X, f, filters, stage, block):\n    \"\"\"\n    Implementation of the identity block as defined in Figure 4\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV's window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string\/character, used to name the layers, depending on their position in the network\n    \n    Returns:\n    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n    \"\"\"\n    \n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. You'll need this later to add back to the main path. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'same', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 1, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    ### START CODE HERE ###\n    \n    # Second component of main path (\u22483 lines)\n    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 1, name = bn_name_base + '2b')(X)\n    X = Activation('relu')(X)\n\n    # Third component of main path (\u22482 lines)\n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'same', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 1, name = bn_name_base + '2c')(X)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (\u22482 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    ### END CODE HERE ###\n    \n    return X","5f73086e":"# GRADED FUNCTION: convolutional_block\n\ndef convolutional_block(X, f, filters, stage, block, s = 1):\n    \"\"\"\n    Implementation of the convolutional block as defined in Figure 4\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV's window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string\/character, used to name the layers, depending on their position in the network\n    s -- Integer, specifying the stride to be used\n    \n    Returns:\n    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n    \"\"\"\n    \n    # defining name basis\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n\n\n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 1, name = bn_name_base + '2a')(X)\n    X = Activation('relu')(X)\n    \n    ### START CODE HERE ###\n\n    # Second component of main path (\u22483 lines)\n    X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0), padding = 'same')(X)\n    X = BatchNormalization(axis = 1, name = bn_name_base + '2b')(X)\n    X = Activation('relu')(X)\n\n    # Third component of main path (\u22482 lines)\n    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 1, name = bn_name_base + '2c')(X)\n\n    ##### SHORTCUT PATH #### (\u22482 lines)\n    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis = 1, name = bn_name_base + '1')(X_shortcut)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (\u22482 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    \n    ### END CODE HERE ###\n    \n    return X","c1accaab":"# GRADED FUNCTION: ResNet50\n\ndef ResNet50(input_shape = (28, 28, 1), classes = 10):\n    \"\"\"\n    Implementation of the popular ResNet50 the following architecture:\n    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n\n    Arguments:\n    input_shape -- shape of the images of the dataset\n    classes -- integer, number of classes\n\n    Returns:\n    model -- a Model() instance in Keras\n    \"\"\"\n    \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(64, (3, 3), strides = (1, 1), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 1, name = 'bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(1, 1))(X)\n\n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n\n    ### START CODE HERE ###\n\n    # Stage 3 (\u22484 lines)\n    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 1)\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n\n    # Stage 4 (\u22486 lines)\n    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 1)\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n\n    # Stage 5 (\u22483 lines)\n    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 1)\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n\n    # AVGPOOL (\u22481 line). Use \"X = AveragePooling2D(...)(X)\"\n    X = AveragePooling2D((2, 2), name = 'avg_pool')(X)\n    \n    ### END CODE HERE ###\n\n    # output layer\n    X = Flatten()(X)\n    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n    \n    \n    # Create model\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","18f0e9bd":"model = ResNet50(input_shape = (28, 28, 1), classes = 10)","99c6051f":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","11f338b6":"X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.1, random_state = 42)","0d12120b":"y_train[:][0:5]","46764895":"\n\"\"\"\n# Convert training and test labels to one hot matrices\ny_train = convert_to_one_hot(y_train, 10).T\ny_test = convert_to_one_hot(y_test, 10).T\"\"\"","7b37122f":"\n\n\nprint (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"Y_test shape: \" + str(y_test.shape))","ad2482b5":"model.fit(X_train, y_train, epochs = 2, batch_size = 16)","61ce7ba7":"preds = model.evaluate(X_test, y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","3a98f1cf":"model = load_model('ResNet50.h5')","2157917d":"preds = model.evaluate(X_test, Y_test)\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","61b07c80":"\"\"\"def create_network():\n    model = Sequential()\n    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                     activation ='relu', input_shape = (28,28,1)))\n    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Valid', \n                     activation ='relu'))\n    model.add(MaxPool2D(pool_size=(2,2)))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Valid', \n                     activation ='relu'))\n    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Valid', \n                     activation ='relu'))\n    model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(Dense(256, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation = \"softmax\"))\n    return model\n    optimizer = RMSprop(lr=0.01)\ndef evaluate(model):\n    model.summary()\n    model.compile(optimizer = 'sgd' , loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=.1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['training', 'validation'], loc='best')\n    plt.show()\"\"\"\n","f2ebca52":"\"\"\"model = create_network()\nevaluate(model)\"\"\"","cd2d8ee1":"plt.imshow(pix_val[0][:,:,0])\nplt.imshow(x_test[0][:,:,0])","c0e79320":"\"\"\"model_acc = model.evaluate(X_test, y_test)\nprint(\" Model Accuracy is : {0:.1f}%\".format(model_acc[1]*100))\"\"\"","12676c70":"\"\"\"y_test = model.predict(x_test)\ny_test = [np.argmax(y_test_) for y_test_ in y_test]\"\"\"","eb497ecd":"sub = pd.DataFrame({'ImageId':[i + 1 for i in range(len(y_test))], 'Label':y_test})\nsub.head()\nsub.to_csv('.\/submission.csv', index = False)\nmodel.save('my_model',save_format='h5')","974fac0c":"Getting labels as ground truth and dropping the column from train_set.","d98b48bb":"Testing on a photo of my own handwriting on a white paper with a black pen.","d2f11c8b":"what .reshape(-1,1) does is first puts the values in array into one row than makes it a column. After adding 28,28 into the middle makes (28,28) sized frames inside the whole arrays values.","64715edf":"Commented out because there was no need for l2, there were no overfitting."}}