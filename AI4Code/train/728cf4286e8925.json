{"cell_type":{"48fc7583":"code","69b9f64b":"code","f34b7d9d":"code","7ada0724":"code","65fb4c62":"code","38b7fa70":"code","6d219af9":"code","39798db9":"code","1f37f676":"code","20b5c0ae":"code","49c4109d":"code","0340d6ea":"markdown","8eaa540f":"markdown","2258af2f":"markdown","4356c91c":"markdown","e175afb7":"markdown","e9caf6eb":"markdown"},"source":{"48fc7583":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom tqdm import tqdm_notebook\nimport pickle\n\nfrom lwoku import get_accuracy, add_features","69b9f64b":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable \ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","f34b7d9d":"# Add new features\nprint('     Number of features (before adding): {}'.format(len(X_train.columns)))\nX_train = add_features(X_train)\nX_test = add_features(X_test)\nprint('     Number of features (after adding): {}'.format(len(X_train.columns)))","7ada0724":"print('  -- Drop features')\nprint('    -- Drop columns with few values (or almost)')\nfeatures_to_drop = ['Soil_Type7', 'Soil_Type8', 'Soil_Type15', 'Soil_Type27', 'Soil_Type37']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('    -- Drop columns with low importance')\nfeatures_to_drop = ['Soil_Type5', 'Slope', 'Soil_Type39', 'Soil_Type18', 'Soil_Type20', 'Soil_Type35', 'Soil_Type11',\n                    'Soil_Type31']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('     Number of features (after drop): {}'.format(len(X_train.columns)))","65fb4c62":"with open('..\/input\/tactic-03-hyperparameter-optimization-lightgbm\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlg_clf = clf.best_estimator_\nlg_clf","38b7fa70":"lg_clf_fast = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.9,\n               importance_type='split', learning_rate=0.7, max_depth=21,\n               min_child_samples=5, min_child_weight=1e-15, min_split_gain=0.0,\n               n_estimators=200, n_jobs=-1, num_leaves=38, objective=None,\n               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=987, subsample_freq=0,\n               verbosity=1)\nlg_clf_fast","6d219af9":"%%time\n## Get accuracy for the original data without feature engineering\naccuracy = get_accuracy(lg_clf_fast, X_train, y_train)\nprint('Accuracy without feature engineering: {:.2f}\\u00A0%'.format(accuracy * 100))","39798db9":"# noinspection PyPep8Naming\ndef accuracy_table_by_expanding():\n    \"\"\"\n    This function calculates the accuracy of the current set of features, and put it in 'Base' column.\n    Then it calculates the accuracy if only one feature is added.\n    The feature with the greatest accuracy, mean that a new set of features with this feature, scores more.\n    Then this feature is added.\n    And then, the functions continues with this new set until there are no new features left. \n    :return: A table with the base accuracy and the accuracy of the added feature for each iteration\n    \"\"\"\n    table = pd.DataFrame(columns=['Base'])\n    X_features = pd.DataFrame()\n\n    progress_bar = tqdm_notebook(total=(len(X_train.columns) + 1) * len(X_train.columns) \/ 2)\n    while len(X_features.columns) <= len(X_train.columns):\n        print(len(X_features.columns))\n        if len(X_features.columns) == 0:\n            row = {'Base': 0}\n        else:\n            row = {'Base': get_accuracy(lg_clf_fast, X_features, y_train)}\n        for feature in X_train.columns.drop(X_features.columns):\n            X_temp = X_features.copy()\n            X_temp[feature] = X_train[feature]\n            row[feature] = get_accuracy(lg_clf_fast, X_temp, y_train)\n        progress_bar.update(len(X_train.columns) - len(X_features.columns))\n        table = table.append(row, ignore_index=True)\n        feature_to_add = table.iloc[-1][1:].idxmax()\n        print('Feature to add: {}'.format(feature_to_add))\n        if pd.isnull(feature_to_add):\n            break\n        else:\n            X_features[feature_to_add] = X_train[feature_to_add]\n#         print(X_features.columns.tolist())\n    progress_bar.close()\n    return table\n\n\n# Get the table for expanding\nexpanding_table = accuracy_table_by_expanding()","1f37f676":"expanding_table.to_csv('expanding_table.csv', index=False)","20b5c0ae":"# Get the largest accuracy\nexpanding_table_accuracy = expanding_table['Base'].max()\nprint('Maximum accuracy: {:.2f}\\u00A0%'.format(expanding_table_accuracy * 100))\n\n# Get the features set with largest accuracy\nexpanding_table_best_row = expanding_table.iloc[expanding_table['Base'].idxmax()]\nexpanding_table_best_row.dropna(inplace=True)\nexpanding_table_best_row.drop('Base', inplace=True)\nexpanding_table_features = X_train.columns.drop(expanding_table_best_row.axes[0].tolist()).tolist()\nprint('For this set of features: {}'.format(expanding_table_features))","49c4109d":"# Predict with test data\nlg_clf.fit(X_train[expanding_table_features], y_train)\ny_test_pred = pd.Series(lg_clf.predict(X_test[expanding_table_features]))\n\n# Submit\noutput = pd.DataFrame({'ID': X_test.index, 'Cover_Type': y_test_pred})\n\noutput.to_csv('submission.csv', index=False)","0340d6ea":"# LightGBM\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)","8eaa540f":"# Export","2258af2f":"# Prepare data","4356c91c":"# Get the features set with largest accuracy","e175afb7":"# Submit","e9caf6eb":"# Introduction\n\nThe aim of this notebook is to select the features that maximizes the score,\nby forward selection.\n\nThe script will begin with no features\nand it will add the feature which contribute more to the score.\n\nFirst the accuracy of the current set of features is calculated.\nThen, for all the features of the set,\nit calculates the accuracy if only this feature is dropped.\nThe feature set with the greatest accuracy,\nmeans that a new set of features without this feature,\nscores more.\nThen this feature is dropped.\n\nFinally, the functions continues with this new set until there are only two values left.\n    \nThe models are fitted and predicted with the optimized parameters and new features of the notebook ([Tactic 06. Feature engineering](https:\/\/www.kaggle.com\/juanmah\/tactic-06-feature-engineering\/)).\n\nThe results are collected at [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary)."}}