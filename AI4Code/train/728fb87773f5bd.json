{"cell_type":{"705f2c56":"code","1e64b56d":"code","238bcdf4":"code","5f8fc43e":"code","f35dbf4a":"code","b9104d9e":"code","903745f4":"code","f0854c2a":"code","e2614972":"code","e74604d3":"code","0cf0e6a5":"code","87593cbc":"code","c78a7ad6":"code","7264f7b4":"code","7904319d":"code","63496132":"code","db4abea0":"code","efd904c3":"code","cf41fb42":"code","69d3b103":"code","a2165cb0":"code","414558de":"code","b41ddab7":"code","795bc72d":"code","40feb51e":"code","32a49fe2":"code","77dc8353":"code","fc89b166":"code","5406e008":"code","dd7871d1":"code","e328b1b0":"code","f32920d7":"code","63a58d84":"code","548db31c":"code","1f48956a":"code","e4498cc2":"code","f0db8492":"code","f6504de4":"code","48650cba":"code","435e27b8":"code","36982b77":"code","ed3c4686":"code","46485a51":"code","d70f7a6b":"code","5b13b123":"code","d3951652":"code","e2a7ec0f":"code","8a3cb6d3":"code","fc3d7409":"code","87c6fa45":"code","684f1293":"code","0341a86b":"code","5c44dca5":"code","64d3f727":"code","94e1e1cf":"code","bdff3313":"code","05d8617c":"code","7a230300":"code","02ceaaee":"markdown","c0c018ae":"markdown","3c7fed35":"markdown"},"source":{"705f2c56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e64b56d":"datest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndatest","238bcdf4":"datrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndatrain","5f8fc43e":"datrain['Embarked'].unique()","f35dbf4a":"datrain.info()","b9104d9e":"num=[]\n\nd=datrain.describe()\nfor i in d:\n    i=i\n    num.append(i)\nprint('list of numerical colummns \\n',num)","903745f4":"cat=[]\n\nd=datrain.describe(include='O')\nfor i in d:\n    cat.append(i)\nprint('list of categorical datatype column \\n',cat)    \n    ","f0854c2a":"datrain.skew()","e2614972":"for i in datrain.columns:\n    a=datrain[i].isnull().sum()\n    if a>0:\n        print('column {} with null value'.format(i), a)","e74604d3":"gender={'female':1,'male':0}\n#emb={'C':0,'S':1,'Q':2}\n\ndatrain['Sex']=datrain['Sex'].apply(lambda x : gender[x])\ndatest['Sex']=datest['Sex'].apply(lambda x : gender[x])\n\n#datrain['Embarked']=datrain['Embarked'].apply(lambda x : emb[x])\n#datest ['Embarked']=datest['Embarked'].apply(lambda x : emb[x])\n","0cf0e6a5":"datrain['Cabin'] = datrain['Cabin'].fillna(0)\ndatest['Cabin']=datest['Cabin'].fillna(0)","87593cbc":"datrain\n","c78a7ad6":"from sklearn.preprocessing import LabelEncoder","7264f7b4":"column_list=['Ticket','Name','Cabin']\nfor i in column_list:\n    lb=LabelEncoder()\n    full_data=pd.concat((datrain[i],datest[i]),axis=0).astype('str')\n    temp=lb.fit_transform(np.array(full_data))\n    datrain[i]=lb.fit_transform(np.array(datrain[i]).astype('str'))\n    datest[i]=lb.fit_transform(np.array(datest[i]).astype('str'))","7904319d":"column_list=['Embarked']\nfor i in column_list:\n    lb=LabelEncoder()\n    full_data=pd.concat((datrain[i],datest[i]),axis=0).astype('str')\n    temp=lb.fit_transform(np.array(full_data))\n    datrain[i]=lb.fit_transform(np.array(datrain[i]).astype('str'))\n    datest[i]=lb.fit_transform(np.array(datest[i]).astype('str'))","63496132":"datrain","db4abea0":"datrain['Embarked'].unique()","efd904c3":"from sklearn.model_selection import train_test_split\nX = datrain.drop(columns=['Survived','Name'])\nY = datrain['Survived']\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.3, random_state=42)","cf41fb42":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\nX_train, Y_train = rus.fit_resample(X_train, Y_train)","69d3b103":"Y_train.value_counts()","a2165cb0":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\n#from sklearn.model_selection import cross_val_score\nresults = list()\nstrategies = ['mean', 'median', 'most_frequent', 'constant']\nfor s in strategies:\n\n    pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestClassifier())])\n   \n    pipeline.fit(X_train,Y_train)\n    scores = pipeline.score(X_test, Y_test)\n    \n    results.append(scores)\n    print('%s %.3f'% (s, np.mean(scores)))","414558de":"a = X_train.fillna((X_train.median()))\nb = X_test.fillna((X_test.median()))\n","b41ddab7":"pipeline = Pipeline(steps=[('m', RandomForestClassifier())])\npipeline.fit(a, Y_train)\nX_test.fillna(X_test.median())\nscores = pipeline.score(b, Y_test)\nprint((scores))","795bc72d":"# from sklearn.preprocessing import MinMaxScaler\n# sc = MinMaxScaler()\n# X_train = sc.fit_transform(X_train)\n# X_test = sc.transform(X_test)","40feb51e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error,r2_score\nr_model=RandomForestClassifier(n_estimators = 500,max_features='auto')\n\nr_model.fit(a,Y_train)\ny_pred = r_model.predict(b)\n\n","32a49fe2":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report, auc, roc_curve\nconfusion_matrix(y_pred, Y_test)","77dc8353":"#import xgboost as xgb\n#dtr = xgb.XGBClassifier(n_estimators=1000)\n#dtr.fit(a,Y_train)\n#y_pred = dtr.predict(b)\n\n","fc89b166":"#from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report, auc, roc_curve\n#confusion_matrix(y_pred, Y_test)\n","5406e008":"p=precision_score(Y_test, y_pred)\nr=recall_score(Y_test, y_pred)\nf1=f1_score(y_pred,Y_test)\nprint('precision score',p)\nprint('recall score',r)\nprint('f1 score', f1)","dd7871d1":"feature_important = r_model.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","e328b1b0":"datrain['Age'] = datrain['Age'].fillna(datrain['Age'].mean())\ndatrain['Cabin']=datrain['Cabin'].fillna(datrain['Cabin'].median())\ndatrain['Embarked']=datrain['Embarked'].fillna(datrain['Embarked'].median())\n\ndatest['Age'] = datest['Age'].fillna(datest['Age'].mean())\ndatest['Cabin']=datest['Cabin'].fillna(datest['Cabin'].median())\ndatest['Embarked']=datest['Embarked'].fillna(datest['Embarked'].median())","f32920d7":"datrain[\"Fare_mean\"]  = datrain.groupby(['Age'])['Fare'].transform('mean')\n\ndatest[\"Fare_mean\"]  = datest.groupby(['Age'])['Fare'].transform('mean')\ndatrain","63a58d84":"X = datrain.drop(columns=['Survived','Name'])\nY = datrain['Survived']\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.3, random_state=42)","548db31c":"r_model=RandomForestClassifier(n_estimators = 500,max_features='auto')\n\nr_model.fit(X_train,Y_train)\ny_pred = r_model.predict(X_test)\n","1f48956a":"confusion_matrix(y_pred, Y_test)\n","e4498cc2":"p=precision_score(Y_test, y_pred)\nr=recall_score(Y_test, y_pred)\nf1=f1_score(y_pred,Y_test)\nprint('precision score',p)\nprint('recall score',r)\nprint('f1 score', f1)","f0db8492":"feature_important = r_model.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","f6504de4":"\n#datrain[\"fare_S\"]  = datrain.groupby(['Parch'])['Fare'].transform('max')\ndatrain[\"age count\"]  = datrain.groupby(['Age'])['Age'].transform('count')\ndatrain[\"pclasscount\"]  = datrain.groupby(['Pclass'])['Pclass'].transform('count')\ndatrain[\"Name_f\"]  = datrain.groupby(['Name'])['Fare'].transform('mean')\ndatest[\"age count\"]  = datest.groupby(['Age'])['Age'].transform('count')\ndatest[\"pclasscount\"]  = datest.groupby(['Pclass'])['Pclass'].transform('count')\ndatest[\"Name_f\"]  = datest.groupby(['Name'])['Fare'].transform('mean')\n\n\n\ndatrain","48650cba":"X = datrain.drop(columns=['Survived','Name'])\nY = datrain['Survived']\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.3, random_state=42)","435e27b8":"r_model=RandomForestClassifier(n_estimators = 500,max_features='auto')\n\nr_model.fit(X_train,Y_train)\ny_pred = r_model.predict(X_test)\n","36982b77":"confusion_matrix(y_pred, Y_test)\n","ed3c4686":"p=precision_score(Y_test, y_pred)\nr=recall_score(Y_test, y_pred)\nf1=f1_score(y_pred,Y_test)\nprint('precision score',p)\nprint('recall score',r)\nprint('f1 score', f1)","46485a51":"feature_important =r_model.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","d70f7a6b":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncor = X.corr()\nplt.figure(figsize=(20,15))\nsns.heatmap(cor<-0.9, annot=True)","5b13b123":"import xgboost as xgb\ndtr =  xgb.XGBClassifier(eta= 0.04,n_estimators=1000)\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\n\np=precision_score(Y_test, y_pred)\nr=recall_score(Y_test, y_pred)\nf1=f1_score(y_pred,Y_test)\nprint('precision score',p)\nprint('recall score',r)\nprint('f1 score', f1)","d3951652":"test_model = xgb.XGBClassifier(\n            eta = 0.04,\n            n_estimators = 1500 \n)\n#model.fit(X_train,Y_train)\ntest_model.fit(X_train, Y_train, eval_metric='logloss', \n          eval_set=[(X_test,Y_test)], early_stopping_rounds=500, verbose=100)","e2a7ec0f":"dtr = xgb.XGBClassifier(\n            eta = 0.04,\n            n_estimators =42,\n            max_depth=9,\n            min_child_weight=0.8\n            \n)\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\np=precision_score(Y_test, y_pred)\nr=recall_score(Y_test, y_pred)\nf1=f1_score(y_pred,Y_test)\nprint('precision score',p)\nprint('recall score',r)\nprint('f1 score', f1)","8a3cb6d3":"test_model = xgb.XGBClassifier(\n            eta = 0.04,\n            n_estimators = 1500,\n            max_depth=9,\n            min_child_weight=0.8,\n            subsample=0.8,\n            colsample_bytree=0.5\n)\n#model.fit(X_train,Y_train)\ntest_model.fit(X_train, Y_train, eval_metric='logloss', \n          eval_set=[(X_test,Y_test)], early_stopping_rounds=500, verbose=100)\n#validation_0-logloss:0.44683\n#validation_0-logloss:0.44198","fc3d7409":"dtr=xgb.XGBClassifier(\n            eta = 0.04,\n            n_estimators = 90,\n            max_depth=9,\n            min_child_weight=0.8,\n            subsample=0.9,\n            colsample_bytree=0.5\n)\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\np=precision_score(Y_test, y_pred)\nr=recall_score(Y_test, y_pred)\nf1=f1_score(y_pred,Y_test)\nprint('precision score',p)\nprint('recall score',r)\nprint('f1 score', f1)","87c6fa45":"test_model = xgb.XGBClassifier(\n            eta = 0.04,\n            n_estimators = 1500,\n            max_depth=9,\n            min_child_weight=0.8,\n            subsample=0.8,\n            colsample_bytree=0.5,\n            gamma=6\n)\n#model.fit(X_train,Y_train)\ntest_model.fit(X_train, Y_train, eval_metric='logloss', \n          eval_set=[(X_test,Y_test)], early_stopping_rounds=500, verbose=100)\n#validation_0-logloss:0.43588\n#validation_0-logloss:0.43379","684f1293":"dtr=xgb.XGBClassifier(\n            eta = 0.04,\n            n_estimators =184,\n            max_depth=9,\n            min_child_weight=0.8,\n            subsample=0.9,\n            colsample_bytree=0.5,\n            gamma=6\n)\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\np=precision_score(Y_test, y_pred)\nr=recall_score(Y_test, y_pred)\nf1=f1_score(y_pred,Y_test)\nprint('precision score',p)\nprint('recall score',r)\nprint('f1 score', f1)","0341a86b":"from sklearn.metrics import accuracy_score \naccuracy_score(Y_test, y_pred)","5c44dca5":"c=confusion_matrix(y_pred, Y_test)","64d3f727":"#plt.figure(figsize=(10,20))\nlabel=[1,2,3,4,5,6]\nsns.heatmap(c,annot=True,cmap='BuGn_r',fmt='.3f')\nplt.show()","94e1e1cf":"datest=datest.drop(columns='Name')\ny=dtr.predict(datest)\ny","bdff3313":"sample=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nsample","05d8617c":"sample['Survived'] = y\nsample.to_csv('y.csv',index=False)","7a230300":"datest","02ceaaee":"NULL VALUES","c0c018ae":"advanced featuring","3c7fed35":"hyperparameter tuning"}}