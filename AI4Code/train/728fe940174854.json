{"cell_type":{"0d1d2ece":"code","c4504533":"code","a19095ee":"code","50b384d5":"code","ae5adf05":"code","cde94b8f":"code","d7d00f95":"code","ac4d3b95":"code","486f06a8":"code","c2dcbe57":"code","641255f0":"code","03a3ccaa":"code","2791d9cf":"code","9ad91c7f":"code","36d77903":"code","12707db7":"code","22e0ae47":"code","3bf03755":"code","9730f83f":"code","9c49389a":"code","a6507729":"code","76e9731c":"code","3f2f14c0":"code","c9fde675":"code","1b75ef74":"code","57a40ed7":"code","4a9fa109":"code","f8311490":"code","8fa812fb":"code","9946bf71":"markdown","11d4a805":"markdown","c3858c63":"markdown","5136265d":"markdown","e7591359":"markdown","4fc81102":"markdown","c8188e69":"markdown","5a715dfa":"markdown","721748a7":"markdown","0e99116f":"markdown","4e891a26":"markdown","adc74e96":"markdown","4d0d615c":"markdown","7b257cb2":"markdown","e7c6bc27":"markdown"},"source":{"0d1d2ece":"!pip install pytorch_pretrained_bert","c4504533":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nStopWords = set(stopwords.words('english'))\n\nimport tensorflow as tf\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","a19095ee":"train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsumbission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","50b384d5":"train.head()","ae5adf05":"print('Shape of Train Data :', train.shape)\nprint('Shape of Test Data :', test.shape)","cde94b8f":"sns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"target\", data=train)","d7d00f95":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,6))\ntarget_1_len = train[train['target']==1]['text'].str.split().map(lambda x : len(x))\nax1.hist(target_1_len,color='red')\nax1.set_title(\"Disaster_Tweets\")\ntarget_0_len = train[train['target']==0]['text'].str.split().map(lambda x : len(x))\nax2.hist(target_0_len,color='blue')\nax2.set_title(\"No_Disaster_Tweets\")\nfig.suptitle('No. Of Words in a Tweet')\nplt.show()","ac4d3b95":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,6))\ntarget_1_len = train[train['target']==1]['text'].map(lambda x : len(x))\nax1.hist(target_1_len,color='red')\nax1.set_title(\"Disaster_Tweets\")\ntarget_0_len = train[train['target']==0]['text'].map(lambda x : len(x))\nax2.hist(target_0_len,color='blue')\nax2.set_title(\"No_Disaster_Tweets\")\nfig.suptitle('Sentence length in a Tweet')\nplt.show()","486f06a8":"def data_cleaning(text):\n    remove_url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    remove_url = remove_url.sub(r'',text)\n    remove_html=re.compile(r'<.*?>')\n    remove_html = remove_html.sub(r'',remove_url)\n    remove_emoji = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    remove_emoji = remove_emoji.sub(r'', remove_html)\n    remove_punct=str.maketrans('','',string.punctuation)\n    remove_punct = remove_emoji.translate(remove_punct)\n    \n    return remove_punct","c2dcbe57":"train['text'] = train['text'].apply(lambda x: data_cleaning(x))\ntrain.head()","641255f0":"# Create sentence and label lists\nsentences = train.text.values\n\n# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\nlabels = train.target.values","03a3ccaa":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])","2791d9cf":"# Set the maximum sequence length. The longest sequence in our training set is 55 so we will take it 64 \nMAX_LEN = 64\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)","9ad91c7f":"# Use train_test_split to split our data into train and validation sets for training\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n                                                            random_state=42, test_size=0.2)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=42, test_size=0.2)","36d77903":"# Convert all of our data into torch tensors, the required datatype for our model\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)","12707db7":"# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","22e0ae47":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)","3bf03755":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]","9730f83f":"# This variable contains all of the hyperparemeter information our training loop needs\noptimizer = BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)","9c49389a":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","a6507729":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n#optimizer = optimizer.to(device)\n#train_dataloader = train_dataloader.to(device)\n","76e9731c":"t = [] \n\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs \nepochs = 10\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n  \n  \n  # Training\n  \n  # Set our model to training mode (as opposed to evaluation mode)\n  model.train()\n  \n  # Tracking variables\n  tr_loss = 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n  \n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n    # Forward pass\n    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    train_loss_set.append(loss.item())    \n    # Backward pass\n    loss.backward()\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    \n    \n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n\n  print(\"Train loss: {}\".format(tr_loss\/nb_tr_steps))\n    \n    \n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables \n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    \n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    \n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\n  print(\"Validation Accuracy: {}\".format(eval_accuracy\/nb_eval_steps))","3f2f14c0":"plt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()","c9fde675":"# Create sentence and label lists\nsentences = test.text.values\n\n# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\nsentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n#labels = df.label.values\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n\n\nMAX_LEN = 64\n\n# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n\n# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n  seq_mask = [float(i>0) for i in seq]\n  attention_masks.append(seq_mask) \n\nprediction_inputs = torch.tensor(input_ids)\nprediction_masks = torch.tensor(attention_masks)\n#prediction_labels = torch.tensor(labels)\n  \nbatch_size = 3264  \n\n\nprediction_data = TensorDataset(prediction_inputs, prediction_masks)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler)\n","1b75ef74":"# Prediction on test set\n\n# Put model in evaluation mode\nmodel.eval()\n\n# Tracking variables \npredictions = []\n\n# Predict \nfor batch in prediction_dataloader:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask = batch\n  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n  with torch.no_grad():\n    # Forward pass, calculate logit predictions\n    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  #label_ids = b_labels.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits)\n  #true_labels.append(label_ids)","57a40ed7":"l = []\nfor i in range(0,len(predictions)):\n    pred = np.argmax(predictions[i], axis=1).tolist()\n    l.extend(pred)\n","4a9fa109":"sub=pd.DataFrame({'id':sumbission['id'].values.tolist(),'target':l})","f8311490":"sub.head(10)","8fa812fb":"sub.to_csv('Submission.csv')","9946bf71":"# Model Training\n\nNow that our input data is properly formatted, it's time to fine tune the BERT model.\n\nFor this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n\nWe'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.","11d4a805":"We will use BERT tokenizer to convert our text into tokens that correspond to BERT's vocabulary.","c3858c63":"\nBERT requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n\n**Input ids**: a sequence of integers identifying each input token to its index number in the BERT tokenizer vocabulary\n\n**attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens \n\nNow, we havesenences with different lengths so we need to pad them to make the same length of all the sentence. Here, we are padding with 'post' this mean it would be padded with 0 in last. \n\nTo \"pad\" our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length.\n\nIf a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n\nI padded and truncated the sequences so that they all become of length MAX_LEN (\"post\" indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) .\n\npad_sequences is a utility function that we're borrowing from Keras. It simply handles the truncating and padding of Python lists.","5136265d":"# **Data Cleaning**","e7591359":"We can see the sentence length is almost same in both the tweets. Let's start Data cleaning process","4fc81102":"# Embedding\n\nAs per architecture of BERT we will add 'CLS' token at the starting of the sentence and 'SEP' at the end of the sentence.","c8188e69":"We can see that average number of words in both types of tweets are different, No_Disaster_Tweet is having average word length is in between 10-20 and that is diffrent in Dister_Tweet. \n\nNow we will check sentence length in each type of tweet. ","5a715dfa":"In this completition we have to check if the tweet is disaster or not with the help of ML or any Deep Learning techniques.\nHere, we will go with step by step. \n\n**1) EDA on Given data**\n\n**2) Data Cleaning**\n\n**3) Embedding**\n\n**4) Model Training**\n\nFirst of all let's start with importing all the required libraries for EDA","721748a7":"# **Import Train, Test and Submission CSV with pandas and EDA on that Data**","0e99116f":"We have train and test datasets but the test data is actually submission here. So we will split our data set as a tran and test from the training dataset only. ","4e891a26":"Now we will use our trained model to predict the test data according to problem statement. ","adc74e96":"We have to convert our data in to tensors now to train our model. ","4d0d615c":"While working on text data we can find that, data contains lots of unwanted data like punctuations, stopwords, html tags and emojis. So, we need to remove all thease things with data cleaning . ","7b257cb2":"For each pass in the training loop we have a training phase and a validation phase.\n\nAt each pass we need to:\n\n**Training loop:**\n\nTell the model to compute gradients by setting the model in train mode\nUnpack our data inputs and labels\nLoad data onto the GPU for acceleration\nClear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\nForward pass (feed input data through the network)\nBackward pass (backpropagation)\nTell the network to update parameters with optimizer.step()\nTrack variables for monitoring progress\n\n**Evalution loop:**\n\nTell the model not to compute gradients by setting th emodel in evaluation mode\nUnpack our data inputs and labels\nLoad data onto the GPU for acceleration\nForward pass (feed input data through the network)\nCompute loss on our validation data and track variables for monitoring progress","e7c6bc27":"We can see there are more tweets for Class 0 and less tweets for Class 1.\n\nNow Lets' do analysis on texts which are provided in tweets.\n\n**Number of words in tweets**"}}