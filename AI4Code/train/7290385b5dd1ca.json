{"cell_type":{"152f0289":"code","a4ffb1d4":"code","69cd3655":"code","286f5900":"code","c1d06031":"code","cdb48615":"code","d8b52610":"code","0bf46f49":"code","7fbde7e4":"markdown","10528282":"markdown","4f23cf05":"markdown","2b19b790":"markdown","8addb4cc":"markdown","719b014c":"markdown"},"source":{"152f0289":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4ffb1d4":"path = \"\/kaggle\/input\/jet-tagging-lhcb\/\"\nsignalData = pd.read_csv(path+\"bjet_train.csv\") # signal has mc_flavour = 5\nbackgroundData = pd.concat([pd.read_csv(path+\"cjet_train.csv\"), \n                            pd.read_csv(path+\"ljet_train.csv\")]) # background has mc_flavour != 5\nprint(\"First of {} signal rows\".format(signalData.shape[0]))\ndisplay(signalData.iloc[0])\nprint(\"First of {} background rows\".format(backgroundData.shape[0]))\ndisplay(backgroundData.iloc[0])","69cd3655":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplotCols = list(signalData.columns)\n\nfor i in range(len(plotCols)):\n    print(\"Plotting {}\".format(plotCols[i]))\n    plt.hist(signalData[plotCols[i]],label = \"Sig\")\n    plt.hist(backgroundData[plotCols[i]],label = \"Bkg\")\n    plt.legend()\n    plt.xlabel(plotCols[i])\n    plt.show()\n\n# Note if the plots do not display minimise then maximise the output area below (double arrow button to the top right)","286f5900":"# Try fdChi2 as log10, others as linear\nlogCol = ['fdChi2']\nlinCol = ['PT', 'ETA', 'drSvrJet', 'fdrMin', \n          'm', 'mCor', 'mCorErr', 'pt', 'ptSvrJet',\n          'tau', 'ipChi2Sum','fdChi2', 'nTrk', 'nTrkJet']\n# redefine columns as log10(col), so ranges are more similar between variables\nfor l in logCol:\n    signalData[l] = np.log10(signalData[l])\n    backgroundData[l] = np.log10(backgroundData[l])\n\nnTrainSig = signalData.shape[0]\/\/2 # half the rows for training, half for evaluation\nnTrainBkg = backgroundData.shape[0]\/\/2\n\n# first half as training\nx_data = np.concatenate([signalData[logCol+linCol][:nTrainSig].values,\n                         backgroundData[logCol+linCol][:nTrainBkg].values])\ny_data = np.concatenate([(signalData[\"mc_flavour\"][:nTrainSig]==5).values.astype(np.int),\n                         (backgroundData[\"mc_flavour\"][:nTrainBkg]==5).values.astype(np.int)])\n\n#second half as evaulation\nx_eval = np.concatenate([signalData[logCol+linCol][nTrainSig:].values,\n                         backgroundData[logCol+linCol][nTrainBkg:].values])\ny_eval = np.concatenate([(signalData[\"mc_flavour\"][nTrainSig:]==5).values.astype(np.int),\n                         (backgroundData[\"mc_flavour\"][nTrainBkg:]==5).values.astype(np.int)])","c1d06031":"# Simple 2 layer Keras network:\n# import Keras overall\nimport keras\n# a single NN layer of type \"Dense\" i.e. all inputs connected to all outputs\nfrom keras.layers import Dense\n# The input layer, takes x and starts NN processing\nfrom keras.layers import Input\n# Keras functional methods for defining a NN model\nfrom keras.models import Model\n\n# optimiser to use\nAdam = keras.optimizers.Adam() # defaults for optimiser\n\n# define a Functional keras NN model\n# define input layer\nnVal = x_data.shape[1]\ninputs = Input(shape=(nVal,)) \n# input->internal layer with nVal nodes\niLayer = Dense(nVal, activation='relu')(inputs) \n# prev layer -> output (1 node)\noutput = Dense(1, activation='sigmoid')(iLayer) \n# a model is created from connecting all the layers from input to output\nmodel = Model(inputs=inputs, outputs=output)\n# Compiling the model sets up how to optimise it\nmodel.compile(optimizer=Adam, \n              loss='binary_crossentropy', # define what is to be optimised\n              metrics=['accuracy']) # what to store at each step\n\n# run an evaluation before optimisation to see what the random initialisation\n# gave as an output\nscore = model.evaluate(x_eval, y_eval, verbose=1)\nprint('Initial loss:', score[0])\nprint('Inital accuracy:', score[1]) # note random get you to ~60% accuracy if the data are 60% true\n","cdb48615":"# Choose a batch size \nbatchSize = 4096\n# Rather than run a fixed number of rounds, stop when the output stops improving\nfrom keras.callbacks import EarlyStopping\n# stop training early if after 10 iterations the result has not improved\nearly_stopping = EarlyStopping(monitor=\"loss\", patience=10)\n# Now run the optimisation, taking events from the generator\n# Each epoch is one pass through the data, if not stopped do 20 epochs\nhistory=model.fit(x=x_data,y=y_data,batch_size=batchSize,\n                  verbose=1,\n                  epochs=25,\n                  shuffle=True, # needed as training is all true then all false\n                  callbacks=[early_stopping])\nprint(\"Stopped after \",history.epoch[-1],\" epochs\")","d8b52610":"# Now evaluate the model after training on the evaluation sample, should be better (but could be much improved still)\nscore = model.evaluate(x_eval, y_eval, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","0bf46f49":"testData = pd.read_csv(path+\"competitionData.csv\")\n# tidy up column names\n# apply log10 to columns that need it\nfor l in logCol:\n    testData[l] = np.log10(testData[l])\nx_test = testData[logCol+linCol].values\n\npredMCFloat = model.predict(x_test)\n# predMC is a float: need to convert to an int\npredMC = (predMCFloat>0.5).astype(np.int)\ntestData[\"mc_flavour\"] = predMC\n# solution to submit\noutput = testData[[\"Row\",\"mc_flavour\"]]\n# then format as required:\ndisplay(output[:5]) # display 5 rows\n# write to a csv file for submission\noutput.to_csv(\"submit.csv\",index=False) # Output a csv file for submission: see \/kaggle\/working to the right","7fbde7e4":"# Now load data for the competition","10528282":"## Do some input processing and put data into a numpy array for use in Keras","4f23cf05":"# Kaggle default setup for notebooks","2b19b790":"## Using Keras make a simple one hidden layer model","8addb4cc":"## Plot the data in each column","719b014c":"## Load the data"}}