{"cell_type":{"6b685335":"code","4af7c6db":"code","6c02740f":"code","84cebf1d":"code","91eb4dcd":"code","470c0155":"code","5cfc95fe":"code","77cdbf59":"code","d45c79a8":"code","f5b4fb65":"code","b7d4fe4f":"code","5df89fb0":"code","7a373cc1":"code","9b3a4abb":"code","5e591239":"code","c4432095":"code","e4a476d3":"code","18aa15ac":"code","84b5c4c4":"markdown","6f3734b5":"markdown","35c9ea37":"markdown","d5375414":"markdown","0d1ed173":"markdown","7982be3c":"markdown","2d175e37":"markdown","af6cdf7f":"markdown","7c6561da":"markdown","6bcbe840":"markdown","f498eb04":"markdown","39d4dda5":"markdown","dc7e161e":"markdown"},"source":{"6b685335":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)\n%matplotlib inline\npd.options.display.float_format = '{:,}'.format\n\n#reading raw data\ndata_raw = pd.read_csv('\/kaggle\/input\/student-performance-data-set\/student-por.csv')\ndisplay(data_raw.head())\n\n#creating catagorical columns list and numeric columns list\ncat_columns = ['school','sex','address','famsize','Pstatus','Mjob','Fjob','reason','guardian',\n               'schoolsup','famsup','paid','activities','nursery','higher','internet','romantic']\nnum_columns = ['Medu','Fedu','traveltime','studytime','famrel','freetime','goout','Dalc','Walc','health']\ncont_columns = ['age','failures','absences','G1','G2','G3']","4af7c6db":"from sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom category_encoders import LeaveOneOutEncoder\n\n#Creating different dataset copies for different label encoder\n#OneHotEncoding\ndummy_df = data_raw.copy()\n#target column\ntarget = dummy_df.pop('G3')\n#LeaveOneOutEncoding\nloo_df = dummy_df.copy()\n#LabelEncoder\nle_df = dummy_df.copy()\n\n#Creating Encoder\nloo = LeaveOneOutEncoder()\nle = LabelEncoder()\n#Encoding Catagorical Variables\nfor col in cat_columns:\n    loo_df[col] = loo.fit_transform(loo_df[col],target)\n    le_df[col] = le.fit_transform(le_df[col])\n    dummy_df = pd.get_dummies(dummy_df,columns=[col],drop_first=True)","6c02740f":"from sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom lightgbm import LGBMRegressor\n\nfrom catboost import CatBoostRegressor\n\nfrom xgboost import XGBRegressor","84cebf1d":"def ml(data,target,model,pr=False):\n    X_train,X_test,y_train,y_test = train_test_split(data, target, random_state=0)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    mae = metrics.mean_absolute_error(y_test, y_pred)\n    mse = metrics.mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n    r2_square = metrics.r2_score(y_test, y_pred)\n    if pr==True:\n        print('MAE:', mae)\n        print('MSE:', mse)\n        print('RMSE:', rmse)\n        print('R2 Square', r2_square)\n        print('__________________________________')\n    return model,mae,mse,rmse,r2_square\n\nMLA = [\n    LinearRegression(),\n    SVR(kernel='rbf'),\n    SVR(kernel='poly'),\n    LGBMRegressor(),\n    CatBoostRegressor(verbose=False),\n    XGBRegressor(),\n    GradientBoostingRegressor(),\n    RandomForestRegressor()\n]\n\nloo_models = {}\nle_models = {}\ndummy_models = {}","91eb4dcd":"%%time\n#Creating DataFrame to save model performance\nevaluation_metrics = ['MAE','MSE','RMSE','R2_Square']\nloo_df_performance = pd.DataFrame(columns=evaluation_metrics,)\nle_df_performance = pd.DataFrame(columns=evaluation_metrics)\ndummy_df_performance = pd.DataFrame(columns=evaluation_metrics)\n\nfor alg in MLA:\n    _name = alg.__class__.__name__\n    if _name == 'SVR':\n        _name = f'{_name}_{alg.kernel}'\n    \n    loo_model, loo_mae, loo_mse, loo_rmse, loo_r2_square = ml(loo_df,target,alg)\n    le_model, le_mae, le_mse, le_rmse, le_r2_square = ml(le_df,target,alg)\n    dummy_model, dummy_mae, dummy_mse, dummy_rmse, dummy_r2_square = ml(dummy_df,target,alg)\n    \n    loo_models[_name] = loo_model\n    le_models[_name] = le_model\n    dummy_models[_name] = dummy_model\n    \n    loo_df_performance = loo_df_performance.append(pd.Series({'MAE':loo_mae ,'MSE':loo_mse ,'RMSE':loo_r2_square ,'R2_Square':loo_r2_square},name=_name))\n    le_df_performance = le_df_performance.append(pd.Series({'MAE':le_mae ,'MSE':le_mse ,'RMSE':le_rmse ,'R2_Square':le_r2_square},name=_name))\n    dummy_df_performance = dummy_df_performance.append(pd.Series({'MAE':dummy_mae ,'MSE':dummy_mse ,'RMSE':dummy_rmse ,'R2_Square':dummy_r2_square},name=_name))","470c0155":"sortting = 'MAE'\nprint('Dummies Dataset')\ndisplay(dummy_df_performance.sort_values(sortting))\nprint('LabelEncoder Dataset')\ndisplay(le_df_performance.sort_values(sortting))\nprint('LeaveOneOut Encoder Dataset')\ndisplay(loo_df_performance.sort_values(sortting))","5cfc95fe":"#rfr,_,_,_,_ = ml(loo_df,target,RandomForestRegressor())\n#plot_data = zip(loo_df.columns,rfr.feature_importances_)\n#plot_data = sorted(plot_data, key=lambda x:x[1], reverse=True)\n#x = [col for col,val in plot_data[:5]]\n#y = [val for col,val in plot_data[:5]]\n#plt.figure(figsize=(6,4))\n#plt.bar(x,y)","77cdbf59":"import optuna as opt","d45c79a8":"def objective_xgbr(trial):\n    \n    xgb_params = {}\n    xgb_params['eval_metric'] = 'rmse'\n    xgb_params['eta'] = trial.suggest_uniform('eta', 0.05, 0.6)\n    xgb_params['max_depth'] = trial.suggest_int('max_depth', 2,15)\n    xgb_params['subsample'] = trial.suggest_uniform('subsample', 0.2,1)\n    \n    \n    \n    \n    X_train,X_test,y_train,y_test = train_test_split(loo_df, target, random_state=0)\n    model = XGBRegressor(**xgb_params)\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n    return rmse","f5b4fb65":"%%time\nstudy = opt.create_study( direction='minimize')\nstudy.optimize(objective_xgbr, n_trials=1000)","b7d4fe4f":"opt.visualization.plot_optimization_history(study)","5df89fb0":"print('Tuned Model')\n_=ml(loo_df,target,XGBRegressor(eta=0.27565560438172737,max_depth=4,subsample=0.2631374852738718),pr=True)\nprint('Defaul Model')\n_=ml(loo_df,target,XGBRegressor(),pr=True)","7a373cc1":"def objective_rfr(trial):\n    \n    rfr_params = {}\n    rfr_params['n_estimators'] = trial.suggest_int('n_estimators',50,800)\n    rfr_params['max_depth'] = trial.suggest_int('max_depth', 1, 100)\n    rfr_params['max_features'] = trial.suggest_int('max_features', 1, 20)\n    rfr_params['min_samples_leaf'] = trial.suggest_int('min_samples_leaf', 2, 50)\n    rfr_params['min_samples_split'] = trial.suggest_int('min_samples_split', 2, 50)\n    \n    \n    \n    X_train,X_test,y_train,y_test = train_test_split(loo_df, target, random_state=0)\n    model = RandomForestRegressor(**rfr_params)\n    \n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    rmse = metrics.mean_squared_error(y_test, y_pred, squared=False)\n    return rmse","9b3a4abb":"%%time\nstudy = opt.create_study( direction='minimize')\nstudy.optimize(objective_rfr, n_trials=1000)","5e591239":"opt.visualization.plot_optimization_history(study)","c4432095":"best_params = {'n_estimators': 68,\n                 'max_depth': 55,\n                 'max_features': 8,\n                 'min_samples_leaf': 3,\n                 'min_samples_split': 5}","e4a476d3":"print('Tuned Model')\n_=ml(loo_df,target,RandomForestRegressor(**best_params),pr=True)\nprint('Defaul Model')\n_=ml(loo_df,target,RandomForestRegressor(),pr=True)","18aa15ac":"train,test,train_y,test_y = train_test_split(loo_df,target,random_state=0,test_size=200)\n\nX_train,X_test,y_train,y_test = train_test_split(train,train_y,random_state=0)\n\nxgbr = XGBRegressor(eta=0.27565560438172737,\n                     max_depth=4,\n                     subsample=0.2631374852738718)\n\n\nxgbr.fit(X_train,y_train)\ny_pred = xgbr.predict(test)\nmae = metrics.mean_absolute_error(test_y, y_pred)\nmse = metrics.mean_squared_error(test_y, y_pred)\nrmse = np.sqrt(metrics.mean_squared_error(test_y, y_pred))\nr2_square = metrics.r2_score(test_y, y_pred)\nprint('MAE:', mae)\nprint('MSE:', mse)\nprint('RMSE:', rmse)\nprint('R2 Square', r2_square)\nprint('__________________________________')","84b5c4c4":"# Final performance test","6f3734b5":"# RandomForestRegressor","35c9ea37":"##### we can see a 0.01 increase after tuning random forest regressor","d5375414":"##### I'll take 200 entries from the data set to act as test dataset for our model and see how the model will perform with it","0d1ed173":"##### The r2 score drops to 0.97 but it is not that bad, and the model is pretty good for this dataset.\n\n##### Feel free to play around with the dataset and telling me what you think","7982be3c":"##### Most of the Machine learning algorithms can not handle categorical variables unless we convert them to numerical values. Many algorithm\u2019s performances vary based on how Categorical variables are encoded. [All about Categorical Variable Encoding](https:\/\/towardsdatascience.com\/all-about-categorical-variable-encoding-305f3361fd02)\n\n##### For this reason I'll be using 3 method for encoding catagorical variables\n* **One Hot Encoding**\n* **Label Encoder**\n* **Leave One Out Encoding**","2d175e37":"# Data Preprocessing","af6cdf7f":"# Hyperparameter tuning","7c6561da":"##### RandomForestRegressor comes at the top for each dataset with XGBRregressor perform the best for LeaveOneOut Encoder Dataset, I will choose both algorithems to try hyperparameter tuning with optuna","6bcbe840":"# Model Selection","f498eb04":"#### In this kernel I'll fit a regression model on the student-performance-data-set only, to understand the data and its columns refer to [Student Performance Data Visualization](https:\/\/www.kaggle.com\/mostafafathy4869\/student-performance-data-visualization)","39d4dda5":"##### We can see huge improving about 0.043 for tuning model which gives 0.995 R2 score","dc7e161e":"# XGBRegressor"}}