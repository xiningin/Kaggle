{"cell_type":{"073d49b3":"code","59199a0a":"code","e4d25254":"code","53119d85":"code","989b4a90":"code","f76a15b2":"code","928f20bc":"code","053359ed":"code","7e516f5a":"code","e9fa76ac":"code","a086b93e":"code","c7c1a0aa":"code","f33b21c8":"code","bc1f9dc0":"code","2c6ff65e":"code","60dbb2b1":"code","b877bd07":"code","7b3fe0cd":"code","f38b405f":"code","4efcc148":"code","26264184":"code","b74ae183":"code","11abdabe":"code","f1a877fa":"code","742f2797":"code","77852bc2":"code","f9399e0c":"code","b0f7f284":"markdown","4d68699d":"markdown","04760de9":"markdown","e7b848fb":"markdown","d3849537":"markdown","3251e484":"markdown","9a876eeb":"markdown","761cfd94":"markdown","77771a7a":"markdown","720706eb":"markdown","ff8840dc":"markdown","a146996c":"markdown","03c428ab":"markdown","b3089748":"markdown","83c6dfe4":"markdown","8dd00df4":"markdown","cbd6b5bf":"markdown","1963a01d":"markdown","308a3cfe":"markdown","fff47cea":"markdown","8b48dc64":"markdown","9863774b":"markdown","526ed8ab":"markdown","a1229561":"markdown","e85ee435":"markdown","868a5913":"markdown","8bd46ae0":"markdown","23a348e6":"markdown","e2b25cf0":"markdown","a83f7e8c":"markdown","db2dd8b7":"markdown","42d6d60b":"markdown","139420e9":"markdown","c3940467":"markdown","c04a0d18":"markdown","61034e78":"markdown","17785db9":"markdown","fa450397":"markdown","de76d0e2":"markdown","a27899ff":"markdown","bffaf35b":"markdown","a9ebdb74":"markdown","ffaecfb4":"markdown","c7d22fae":"markdown","fa4e8d69":"markdown","2849f956":"markdown","8abef516":"markdown","3ce55d75":"markdown","a8bf372b":"markdown","304cc0ed":"markdown"},"source":{"073d49b3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn import preprocessing\nfrom copy import deepcopy\nimport statistics as stat\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# own utility script\nimport roc_conf_matrix2 as conf_m\n\n# set charts size\nsns.set(rc={'figure.figsize':(9.36, 8.27)}) \n\n# iterations in Logistic regression - max\n# this should be at least around 200-300\nmax_iterations = 800\n\n# Paths\ntest_path = '..\/input\/tabular-playground-series-may-2021\/test.csv'\ntrain_path = '..\/input\/tabular-playground-series-may-2021\/train.csv'\nsample_submission_path = '..\/input\/tabular-playground-series-may-2021\/sample_submission.csv'\n\n# Read data\ntest_data = pd.read_csv(test_path)\ntrain_data = pd.read_csv(train_path)","59199a0a":"train_data.drop(columns=['id']).describe().T.style.background_gradient(cmap='Greens', axis=0)","e4d25254":"train_data.info()","53119d85":"# list of unique values for each column\nfor column in train_data:\n    print(column, train_data[column].unique())","989b4a90":"train_data.skew()","f76a15b2":"sns.countplot(train_data['target'], palette='winter')\nplt.show()","928f20bc":"train_data.groupby('target').id.count()","053359ed":"x_temp = train_data.drop(columns=['target', 'id'])\n\n# for dummy variables\nx_temp_dum = pd.get_dummies(deepcopy(x_temp), columns=x_temp.columns, prefix=x_temp.columns)\n\nscaler = preprocessing.StandardScaler().fit(x_temp)\nx_temp = pd.DataFrame(scaler.transform(x_temp), columns = x_temp.columns)\n\n#y = train_data['target'].replace(to_replace = 'Class_2', value = '')\ny_temp = pd.to_numeric(train_data['target'].str.replace('Class_', ''))\n\nx_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, train_size=0.8,\ntest_size=0.2)\n\n# create dummy variables\nx_train_dum, x_test_dum, y_train_dum, y_test_dum = train_test_split(x_temp_dum, y_temp, \n                                                                    train_size=0.8, test_size=0.2)","7e516f5a":"# provide function with no of observations for each class\ndef get_unbalanced_data(no_observations):\n    class1_data = train_data[train_data['target']=='Class_1']\n    class2_data = train_data[train_data['target']=='Class_2']\n    class3_data = train_data[train_data['target']=='Class_3']\n    class4_data = train_data[train_data['target']=='Class_4']\n\n    datasets_by_classes = [class1_data, class2_data, class3_data, class4_data]\n    \n    list_balanced_data = []\n    for count, dataset in enumerate(datasets_by_classes):\n        list_balanced_data.append(dataset.tail(no_observations[count]))\n\n    balanced_dataset = pd.concat(list_balanced_data)\n    x_balanced_temp = balanced_dataset.drop(columns=['id', 'target'])\n    y_balanced_temp = pd.to_numeric(balanced_dataset['target'].str.replace('Class_', ''))\n    return x_balanced_temp, y_balanced_temp\n\nx_balanced, y_balanced = get_unbalanced_data([8000, 8000, 8000, 8000])","e9fa76ac":"# Define the colormap\ncolors = sns.diverging_palette(150, 275, s=80, l=55, n=9, as_cmap=True)\n \n# Create heatmap of correlation between features\nsns.heatmap(train_data.corr(), center=0, cmap=colors, robust=True)\nplt.show()","a086b93e":"# Show correlation with target\ny_classes = pd.get_dummies(y_temp, prefix=\"class\")                                            \nplot_corr_data = train_data.join(y_classes)\n\nsns.heatmap(plot_corr_data.corr(), center=0, cmap=colors, robust=True)\nplt.show()","c7c1a0aa":"lin_reg = LinearRegression()\n\nlin_reg.fit(x_train_dum, y_train_dum)\npredictions_lin_reg = lin_reg.predict(x_test_dum)\n\nconf_m.conf_m_report_w_labels(y_test_dum, predictions_lin_reg, 'Linear Regression 1')","f33b21c8":"KNN_test_F1 = {}\nKNN_train_F1 = {}\n\nfor k in range(1, 14):\n    KNN = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n    KNN.fit(x_train_dum, y_train_dum)\n    knn_y_train_pred = KNN.predict(x_train_dum)\n    knn_y_test_pred = KNN.predict(x_test_dum)\n\n    # training avg macro F1\n    KNN_train_F1[k] = f1_score(y_train_dum, knn_y_train_pred, average='macro')\n    \n    # testing accuracy\n    KNN_test_F1[k]= f1_score(y_test_dum, knn_y_test_pred, average='macro')\n\nplt.plot(KNN_test_F1.keys(),KNN_test_F1.values(), label = 'Testing avg macro F1')\nplt.plot(KNN_train_F1.keys(),KNN_train_F1.values(), label = 'Training avg macro F1')\nplt.legend()\nplt.title('F1 (avg macro) vs. K Value')\nplt.xlabel('K')\nplt.ylabel('F1 (avg macro)')","bc1f9dc0":"KNN = KNeighborsClassifier(n_neighbors=4, algorithm='brute')\nKNN.fit(x_train_dum, y_train_dum)\npredictionsKNN1 = KNN.predict(x_test_dum)\n\nconf_m.conf_m_report_w_labels(y_test_dum, predictionsKNN1, 'KNN; model 1')","2c6ff65e":"x_train_CV, x_test_CV, y_train_CV, y_test_CV = train_test_split(x_temp, y_temp, \n                                                                train_size=0.4, test_size=0.6)","60dbb2b1":"kn = KNeighborsClassifier()\nparams = {\n    'n_neighbors' : list(range(1, 30, 6)), # max val among features is 66\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['ball_tree', 'kd_tree', 'brute']\n}\ngrid_kn = GridSearchCV(estimator = kn,\n                        param_grid = params,\n                        scoring = 'f1_macro', \n                        cv = 3, \n                        verbose = 1,\n                        n_jobs = -1)\ngrid_kn.fit(x_train_CV, y_train_CV)\n\n# best estimator\nprint(grid_kn.best_estimator_)\npreds_knn_grid = grid_kn.predict(x_train_CV)","b877bd07":"conf_m.conf_m_report_w_labels(y_train_CV, preds_knn_grid, 'KNN, auto tune, train data')","7b3fe0cd":"preds_knn_grid_test = grid_kn.predict(x_test_CV)\nconf_m.conf_m_report_w_labels(y_test_CV, preds_knn_grid_test, 'KNN, auto tune, test data')","f38b405f":"XGB1 = XGBRegressor()\nXGB1.fit(x_train_dum, y_train_dum)\npredictionsXGB1 = XGB1.predict(x_test_dum)\n\nconf_m.conf_m_report_no_labels(y_test_dum, predictionsXGB1, 'XGB; model 1')","4efcc148":"def get_model_lr(x,y, set_max_iter=max_iterations, set_class_weight='None', set_warm_start=False):\n    lr_balanced1 = LogisticRegression(solver=\"newton-cg\", C=1, multi_class=\"ovr\", max_iter=set_max_iter, n_jobs=-1,\n                                     warm_start=set_warm_start, class_weight=set_class_weight)\n    lr_balanced1.fit(x,y)\n    return lr_balanced1","26264184":"# create LR model where class 1 prevails:    (sample: [class1: 8000, class2: 5000, class3: 5000, class4: 5000] )\nx,y = get_unbalanced_data([8000, 5000, 5000, 5000])\nlr_bal_1 = get_model_lr(x, y)\n\nx,y = get_unbalanced_data([5000, 8000, 5000, 5000])\nlr_bal_2 = get_model_lr(x, y)\n\nx,y = get_unbalanced_data([5000, 5000, 8000, 5000])\nlr_bal_3 = get_model_lr(x, y)\n\nx,y = get_unbalanced_data([5000, 5000, 5000, 8000])\nlr_bal_4 = get_model_lr(x, y)","b74ae183":"models = [lr_bal_1, lr_bal_2, lr_bal_3, lr_bal_4]\nfinal_submission_lr = test_data['id'].to_frame()\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\ntest_data_to_pred = test_data.drop(columns=['id'])\n\nfor count, model in enumerate(models):\n    final_submission_lr[cols_submission[count]] = model.predict_proba(test_data_to_pred)[0:,count]","11abdabe":"final_submission_lr.to_csv('submission_lr_4.csv', index=False)","f1a877fa":"# Get model\nx,y = get_unbalanced_data([8490, 8490, 8490, 8490])\n\nlr_balanced_8490 = get_model_lr(x, y)\ntest_data_to_pred = test_data.drop(columns=['id'])\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\n\npreds_lr_8490 = lr_balanced_8490.predict_proba(test_data_to_pred)\n\npreds_lr_8490_df = pd.DataFrame(preds_lr_8490, columns=cols_submission)\nsubmission_lr_balanced_8490 = test_data['id'].to_frame().join(preds_lr_8490_df)\nsubmission_lr_balanced_8490.to_csv('submission_lr_balanced_8490.csv', index=False)","742f2797":"lr_whole_dataset_log = get_model_lr(np.log(x_temp+10), y_temp)\n\ntest_data_to_pred = test_data.drop(columns=['id'])\ntest_data_to_pred_normal = pd.DataFrame(scaler.transform(test_data_to_pred), \n                                        columns = test_data_to_pred.columns)\ntest_data_to_pred_log_norm = np.log(\n    test_data_to_pred_normal +10 )\n\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\npreds_lr_log = lr_whole_dataset_log.predict_proba(test_data_to_pred_log_norm)\npreds_lr_log_df = pd.DataFrame(preds_lr_log, columns=cols_submission)\nsubmission_lr_log_df = test_data['id'].to_frame().join(preds_lr_log_df)\n\nsubmission_lr_log_df.to_csv('submission_lr_log_df_normal.csv', index=False)","77852bc2":"x_test_data_temp = test_data.drop(columns=['id'])\n\n# dummy variables\nx_test_data_dum = pd.get_dummies(deepcopy(x_test_data_temp), \n                                 columns=x_test_data_temp.columns, \n                                 prefix=x_test_data_temp.columns)\n\nnew_submission_data = test_data['id'].to_frame()\n\n# for each column in data used to train models (I want to have same coulmns in the dataset for predictions)\nfor col_submission_data in x_temp_dum.columns:\n    #if the columns appears in submission data:\n    if col_submission_data in list(x_test_data_dum.columns):\n        new_submission_data[col_submission_data] = x_test_data_dum[col_submission_data].values\n    else:\n        new_submission_data[col_submission_data] = 0\n\nnew_submission_data_no_id = new_submission_data.drop(columns=['id'])","f9399e0c":"lr_dummies = get_model_lr(x_temp_dum, y_temp, 400)\npreds_lr_dummies = lr_dummies.predict_proba(new_submission_data_no_id)\n\ncols_submission = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\npreds_lr_dummies_df = pd.DataFrame(preds_lr_dummies, columns=cols_submission)\nsubmission_lr_dummies_df = test_data['id'].to_frame().join(preds_lr_dummies_df)\n\nsubmission_lr_dummies_df.to_csv('submission_lr_dummies_df.csv', index=False)","b0f7f284":"Model based on balanced sample (8490 samples of each class - this is the max possible number of samples to have them balanced).","4d68699d":"# Submissions\/Final models<hr>","04760de9":"# Models <hr>","e7b848fb":"Assumptions of my basic plan:<br><br>\n1. First I will treat **every variable as continuous**.<br>\n2. Alternatively I will try to create **dummy variables for features with 5 or less unique values** (to treat them as categorical ones). <br>\n3. Eventually I will create **a dataset with dummy variables for each feature**.\n\nIt turned out that the best solution was to treat the variables as categorical or continuous (getting to this point is not included in the notebook) and create dummy variables for each category.","d3849537":"4 models (for each class)<br>\nEach one fitted based on sample where the specific class prevailed<br>\nEach model used to predict the probability of one class (where samples from it dominated)","3251e484":"># Logistic Regression, balanced sample","9a876eeb":"I would expect better performance for predicting class 1 and class 4 but using balanced sample did not help. So other ways may be considered.\n<br><br>\n\nIt is also an option to delete outliers (e.g. using Least Trimmed Squares or quantile regression, though these models may not be fitted well) and verify if it helped.<br><br>","761cfd94":"Code to create models faster:","77771a7a":"Kaggle public score: 1.37537","720706eb":"Kaggle public score: 1.10090","ff8840dc":"# Setup Imports and Variables<hr>","a146996c":"# Kaggle ML Competition: Tabular Playground Series - May 2021<hr>","03c428ab":"># KNN with auto tuning","b3089748":"Comparing results within train and test dataset, we may suspect noticeable **overfitting**. Test conducted before proved that k=4 may minimize that effect. <br>So the model needs improvement but KNN did not perform well during submissions so I will not be developing models using KNN anymore.","83c6dfe4":"The linear correlation between classes and features is low. Which is why I will try to get dummies for each feature to build a better model.","8dd00df4":"Test dataset - confusion matrix:","cbd6b5bf":"># Overfitting \/ underfitting - KNN","1963a01d":"**Kaggle public score: 1.09789 <font color=\"green\">(BEST MODEL)<\/font>**","308a3cfe":"# Next steps<hr>","fff47cea":"# Data cleaning<hr>","8b48dc64":"Kaggle public score: 1.37538\n<br>So balancing the sample was not the thing.","9863774b":"# Exploratory Data Analysis (EDA)<hr>","526ed8ab":"* Setup Imports and Variables\n* Exploratory Data Analysis (EDA)\n* Data cleaning\n * Second dataset - balanced sample\n* Models\n * Linear regression\n * Overfitting \/ underfitting - KNN\n * KNN\n * KNN with auto tuning\n * XGBoost\n * Logistic Regression - automation\n* Submissions\/Final models\n * Logistic Regression - 4 models - unbalanced samples\n * Logistic Regression, balanced sample\n * Logistic Regression - whole dataset + logs of variables + normalized data\n * Logistic Regression - dummy variables for each feature \n* Next steps","a1229561":"># Logistic Regression - automation","e85ee435":"No NaN values (or other problematic cases) so **no imputation required**.","868a5913":"**Decrease train dataset size (especially since execution time of KNN does not grow in linear pace along with the size of the dataset ; )**","8bd46ae0":"Using original dataset (without changes)","23a348e6":"># KNN","e2b25cf0":"Imbalanced sample. That is why predictions are in favour of class 2 and class 3.","a83f7e8c":"Models were compared using F1 (harmonic mean of recall and precision) and kaggle score (log-loss, calculated after submission by kaggle)","db2dd8b7":"As seen above, K equal to 4 is the best option (minimizing the overfitting effect and maximizing F1 (I am trying to tune the model for better results for each class))","42d6d60b":"Train dataset - confusion matrix:","139420e9":"There should not be any problems with collinearity.","c3940467":"># XGBoost","c04a0d18":"Create dummy variables for each feature in whole training dataset and train a logistic regression model using it.","61034e78":"This notebook was prepared by Adrian Hajdukiewicz.","17785db9":"2. Train model, make predictions, upload results: <br>\n(be aware that logistic regression with so many features takes a lot of time to train; luckily making predictions is only a simple math)","fa450397":"1. Prepare data for final prediction (to make a submission)<br>\n(generate dummies, same as in the training set):","de76d0e2":"># Logistic Regression - whole dataset + logs of variables + normalized data","a27899ff":"As seen in the model results, Linear regression is not a good model for classification because predicted probabilities may go beyond the limits of the set. A way better solution is to use probit\/logit model if we want to have linear endogenous variables (but estimation is not linear in the latter case of course).<br><br>\n\nI will not conduct tests for heteroskedasticity (White test), functional form (RESET test), ... because the model is bad at the beginning. \n","bffaf35b":"<hr>","a9ebdb74":"Types of models used in this notebook:<br>\n1. Linear regression <font color=\"red\">(bad model)<\/font><br>\n2. Logistic regression<br>\n3. KNN<br>\n4. XGBoost<br>\n<br>","ffaecfb4":"Based on the table above, I have to **normalize the data if** I assume they are **continuous variables.** (due to different means, min and max values)","c7d22fae":"**Due to positive skewness**, I will **create an additional dataset with logs of variables** (assuming they are continuous variables).","fa4e8d69":"># Logistic Regression - dummy variables for each feature","2849f956":"KNN and XGB models were excluded because they provided me with not accurate enough probabilities.","8abef516":"In general, models have similar performance within test and train data (tests excluded from the notebook) so there is no need of tuning the models (based on test not included in the notebook).<br><br>\n**But in case of KNN we need to select proper number of neighbors.**","3ce55d75":"># Linear regression","a8bf372b":"># Logistic Regression - 4 models - unbalanced samples","304cc0ed":"># Second dataset - balanced sample"}}