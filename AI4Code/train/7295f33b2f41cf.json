{"cell_type":{"4d715af1":"code","2d8cb673":"code","8ab259f9":"code","ddba67fc":"code","00fbb2bb":"code","6a3c1cc5":"code","fb8b06d6":"code","dd3773c6":"code","81024a55":"code","f5d5e933":"code","967af7f0":"code","349644d5":"code","81cb3854":"code","43cdab87":"code","b56d4a92":"code","d079d5f9":"code","6ad67bec":"code","c7e0cd09":"code","c9b38275":"code","8a6214a6":"code","10f9bbb0":"code","2f5be08d":"markdown","bfb4f5fa":"markdown","2dceb9cf":"markdown","783f34f5":"markdown","96598cc4":"markdown","8d33923d":"markdown","5707c2c0":"markdown","384e4edc":"markdown","4c0b8926":"markdown","3c3899b7":"markdown","d9aca517":"markdown","d840e343":"markdown","f896195a":"markdown","a89dbf0d":"markdown","fd63ac19":"markdown","42ce3ef8":"markdown","c8f88250":"markdown","ad893d6a":"markdown","c23df8dd":"markdown","2de77714":"markdown","3036aa45":"markdown","4ce13629":"markdown","cfdbcc37":"markdown","149d0a53":"markdown","3d3605b4":"markdown","ec9c9109":"markdown","559c1861":"markdown","cbb09773":"markdown","3906804c":"markdown","f1698099":"markdown","e16fa11e":"markdown","4b8ef840":"markdown","a809c739":"markdown","340812c6":"markdown","d6a63cfa":"markdown","ef7f8aff":"markdown","3e0e5867":"markdown"},"source":{"4d715af1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2d8cb673":"# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Garbage Collector\nimport gc\nimport sys\n# Stats\nfrom scipy import stats\n# Text analysis\nfrom fuzzywuzzy import fuzz","8ab259f9":"# load training data\nj_df = pd.read_csv(\"..\/input\/train.csv\")","ddba67fc":"j_df.head()","00fbb2bb":"plt.hist(j_df['target'])\nplt.show()","6a3c1cc5":"plt.hist(j_df[j_df['target']!=0]['target'])\nplt.show()","fb8b06d6":"j_df[j_df['target']!=0]['target'].describe()","dd3773c6":"plt.hist(j_df[j_df['target']!=0]['target'], 15)\nplt.show()","81024a55":"plt.hist(j_df[j_df['target']!=0]['target'], 40)\nplt.show()","f5d5e933":"plt.hist(j_df[j_df['target']!=0]['target'], 100)\nplt.show()","967af7f0":"print(j_df[(j_df['target']!=0) & (j_df['target']>=0.10) & (j_df['target']<=0.20)]['target'].describe())\nprint(stats.mode(j_df[(j_df['target']!=0) & (j_df['target']>=0.10) & (j_df['target']<=0.20)]['target']))","349644d5":"j_df[(j_df['target']!=0) & (j_df['target']==1\/6)].head()","81cb3854":"print(\"Number of records where toxicity_annotator_count is 1: {}\".format(len(j_df[(j_df['toxicity_annotator_count']==1)])))\nprint(\"Most common # of annotators:\")\nprint(j_df['toxicity_annotator_count'].value_counts().head())\nprint(\"Most annotators: \", max(j_df['toxicity_annotator_count']))","43cdab87":"j_df[(j_df['target']!=0) & (j_df['target']==1.0)]['toxicity_annotator_count'].value_counts().head(1000)","b56d4a92":"j_df[(j_df['target']!=0) & (j_df['target']==1.0) & (j_df['toxicity_annotator_count']==4)].head()","d079d5f9":"pd.set_option('display.max_colwidth', -1)\nj_df[(j_df['target']!=0) & (j_df['target']==1.0) & (j_df['toxicity_annotator_count']==4)]['comment_text'].head(40)","6ad67bec":"j_df[(j_df['target']!=0) & (j_df['target']==1.0) & (j_df['toxicity_annotator_count']==4)]['comment_text'].iloc[33]","c7e0cd09":"print(\"Number of annotators:\")\nprint(j_df[(j_df['target']!=0) & (j_df['target']==1.0)]['toxicity_annotator_count'].iloc[16])\nprint(\"Comment:\")\nprint(j_df[(j_df['target']!=0) & (j_df['target']==1.0)]['comment_text'].iloc[16])","c9b38275":"my_string = j_df[(j_df['target']!=0) & (j_df['target']==1.0)]['comment_text'].iloc[16]\n\ndef my_str_compare(x):\n    return fuzz.token_sort_ratio(x,my_string)\n\njdf0 = j_df[j_df['target']==0]\njdf0['distance2mys1'] = jdf0['comment_text'].apply(my_str_compare)\njdf0.sort_values(by=['distance2mys1'], ascending=False).head()","8a6214a6":"my_string = j_df[(j_df['target']!=0) & (j_df['target']==1.0)]['comment_text'].iloc[0]\nprint(\"Toxic string we're going to compare: {}\".format(my_string))\n\ndef my_str_compare(x):\n    return fuzz.token_sort_ratio(x,my_string)\n\njdf0 = j_df[j_df['target']==0]\njdf0['distance2mys1'] = jdf0['comment_text'].apply(my_str_compare)\nprint(\"Showing top 5 similar non-toxic comments:\")\njdf0.sort_values(by=['distance2mys1'], ascending=False).head()","10f9bbb0":"my_string = j_df[(j_df['target']!=0) & (j_df['target']==1.0) & (j_df['toxicity_annotator_count']==53)]['comment_text'].iloc[0]\nprint(\"Toxic string we're going to compare: {}\".format(my_string))\n\ndef my_str_compare(x):\n    return fuzz.token_sort_ratio(x,my_string)\n\njdf0 = j_df[j_df['target']==0]\njdf0['distance2mys1'] = jdf0['comment_text'].apply(my_str_compare)\nprint(\"Showing top 5 similar non-toxic comments:\")\njdf0.sort_values(by=['distance2mys1'], ascending=False).head()","2f5be08d":"I would almost mark these as aggresively inquisitive.","bfb4f5fa":"**Some large takeaways here:**\n1.  We have 540,110 toxic comments\n2.  Our mean toxic comment value is 0.344251\n3.  Clearly right skewed\n4.  75% of our values are actually LESS than 0.50\n\nPretty interesting stuff off the bat.  I think point #4 is pretty interesting.  We have a pretty nice histogram, but let's increase our # of bins and see what happens, because the bin from 0.10 to 0.20 is very high.  Let's see if increasing the # of bins breaks up the 0.10 to 0.20 bin in hopes that we can better understand it.","2dceb9cf":"So seems like the most common value is 0.166667 which is equal to 1\/6.  Let's take a look at a sample where the target is 1\/6.","783f34f5":"So most have a low count which makes sense, and then we have some records with a HUGE number of annotators in agreement.\n\n**Seems like maybe we should we be weighting records by # of annotators?  Seems like # of annotators gives us a level of credibility here.**\n\nOkay, let's look at rows with toxic value of 1 and toxicity_annotator_count=4 ...","96598cc4":"## Excluding 0's","8d33923d":"clearly we have a bunch of 0's, as expected, but they're really distroying our view here, so let's get rid of them.","5707c2c0":"So according to the documentation *\"We recognize that toxicity and identity can be subjective, so we collect labels from up to 10 people per comment, to capture a range of opinions.\"*  Again, they say *\"**up to** 10 people per comment\"*.  I interpret this to mean that they might not always take 10, in some cases they might take 9 or 8 or 7 or 6 or 5, etc.  \n\nIn this case it seems there were 6 annotators and only 1 of them found the comment to be toxic.\n\nLet's learn a bit more about # of annotators...","384e4edc":"So what we want to do is take a toxic comment, and find the most similar non-toxic comment\n\nWe found an article that might be able to help us out here:  https:\/\/www.datacamp.com\/community\/tutorials\/fuzzy-string-python\n\nLet's pick a toxic string we want to use ...","4c0b8926":"So it looks like 6 annotators is very common, which makes sense.\n\nMakes me think... what does this look like for the extreme cases of toxic=1?","3c3899b7":"Wow, this is pretty damn cool.  Few things I take away here..\n1.  Notice that the toxic comment uses an expletive 'jerks' where the non-toxic comments don't include expletives, but the sentences are carzy similar.  I think you could use this as evidence to build a case regarding why just a logistic regression using TF-IDF works fairly well.\n2.  The most toxic part of our toxic comment is *'Long as a lawyer gets paid anything goes*' because it's taking a shot at lawyers.  But notice this non-toxic comment *'In this case there will be four sets of attorneys who will make and lose money. What a laugh.'*  This also takes a shot at lawyers.  How would the NN tell these apart if it wasn't for the word 'jerks'? \n\nI think this give me 3 thoughts:\n1.  Seems like using the additional tags to train some models for specific types of toxic comments and then finding a way to blend them could prove powerful, or even using all the tags in 1 model.\n2.  Including a \"bag of words\" type model in an ensemble could really prove helpful\n3.  TWIML has a great episode with David Ferrucci who goes over General AI and what it would take to get a G-AI.  I think there is something to be said here about teaching your AI system how to recognize different ideas and then how to combine those ideas.  This really just supports item #2, but what if you tried creating your own labels on ideas like sarcasm, or mockery, or taunting.  A lot of the additional tags revolve around social identities, but what about methods of communication?  Obviously you can't create at lot of your own tags, but maybe you could use BERT on your low number of tagged observations.  Or maybe you could try Google Drybell.  And, I think it's important to focus on concepts that seem like the NN would possibly choke on.  \n\n - https:\/\/twimlai.com\/twiml-talk-268-are-we-being-honest-about-how-difficult-ai-really-is-with-david-ferrucci\/\n - https:\/\/github.com\/google-research\/bert\n - https:\/\/ai.googleblog.com\/2019\/03\/harnessing-organizational-knowledge-for.html\n \n What are some other ways we can get our AI to recognize complex ideas and concepts?","d9aca517":"Another idea ..\n\n1.  What if you took each toxic comment as it's own concept?  So this comment actually captures 'aggresively inquisitive' pretty well.  You could exclude this observation from the dataset, then score the entire dataset using fuzz, then you could pick a threshold and 'if similarity>theshold then 1 else 0', then use that as a target for a new neural network.  You could create bunch of models using this technique and then use the StackNet k-fold method or just a hold-out set to create a meta learner.","d840e343":"# Data View","f896195a":"# Imports","a89dbf0d":"## Including 0's","fd63ac19":"Okay, we broke up the 0.10 to 0.20 bin a bit, but not very much.  Let's increase the # of bins a bit more (also note some interesting stuff happening with the rest of the bins.","42ce3ef8":"# Target Histogram","c8f88250":"Once I realized the comment we're comparing was \"You're a fucking twat.\" I honestly didn't think we'd find any compelling comments, but I think we did.  \"What are you smoking?\" seems pretty toxic and aggressive to me ... I think weighting by # of annotators is looking pretty compelling because \"What are you smoking?\".  Maybe if \"What are you smoking?\" had received more annotators it would have ended up with a higher toxicity score?  \n\nUnfortunately 1,185,412 comments have 4 annotators, which is a large chunk of the pie.  So weighting by # of annotators could help a bit, but I think trying to draw out more complex concepts could be more helpful in the end.","ad893d6a":"Let's try this one..","c23df8dd":"Okay, clearly some interesting stuff going on here, let's really bump up the # of bins.","2de77714":"## Excluding 0's, increasing bins","3036aa45":"let's try another one ...","4ce13629":"let's get a quick look at the data to familiarize ourselves","cfdbcc37":"This comment doesn't seem overly toxic to me, yet 4 people agreed that this was toxic.  The rest of the comments seem pretty toxic with a handful that I would made argue are more boarderline toxic vs. a hard 1.0.  \n\nThis does bring up an interesting idea though, what if we compared the 1.0's to the 0's and looked that the comments that were most similar to eachother?  I'm just wondering how some of our most toxic comments compare to our non-toxic comments ..","149d0a53":"1.  We have 540,110 toxic comments\n2.  Our mean toxic comment value is 0.344251\n3.  Clearly right skewed\n4.  75% of our values are actually LESS than 0.50\n5.  Including a 'bag of words' type model in an emsemble could give model lift\n6.  Maybe we should be weighting records by # of annotators?\n7.  Complex social concepts like mockery and being aggresively inquisitive could trip up your NN's\n8.  Model blending by building models on various concepts could prove helpful to separate toxic from non-toxic comments especially when the toxic and non-toxic comments are overly similar.  (Or if you can build all of this into 1 model, that's something too).\n9.  I think there is something to be said about the ideas on trying to capture many complex concepts, and the success of many public kernels using all targets (1 ex. https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm)","3d3605b4":"# Threshold EDA then Following the Rabbit Down the Rabbit Hole\n\nThe idea there is to start simple by reviewing the thresholds, and then just start following the data and some interesting finds to see where things lead ..\n\n(the title is in reference to Alice in Wonderland where alice follows the rabbit down the rabbit hole, in a sense we're going to follow the data and see where it leads us). \n\nLet's get going :-)","ec9c9109":"Please comment and let me know your ideas and thoughts\n* Does this entire thread seem like garbage?  **If so, why? ** \n* Is thie thread any good?  **If so, why?**\n* What could I do to improve this thread if anything?\n\nThanks everyone!","559c1861":"# Load Data","cbb09773":"# 0.10 to 0.20 Bin Deep Dive","3906804c":"let's try 1 more... a super confident toxic comment...","f1698099":"Let's start simple.  I think in a lot of ways everyone wants to dominate the world right off the bat, let's not do that, let's start simple and see where things lead.","e16fa11e":"This looks pretty nice, lets get some basic statistics","4b8ef840":"Hmm...let's look through some comments..","a809c739":"This one seems interesting ...","340812c6":"# Comparing 1.0's to 0's","d6a63cfa":"So we have some very common values, and they're distances apart from each other are very similar.  Let's dial in.  Let's pick a segment, and focus on the data within that segement, and hopefully this will lead to some insights as to why the data has this interesting structure.  Let's look at the 0.10 to 0.20 bin, since that's where most of the data lies.","ef7f8aff":"# Main Summary + Takeaways","3e0e5867":"# Final Comments"}}