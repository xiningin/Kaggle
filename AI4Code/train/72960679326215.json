{"cell_type":{"5680c02b":"code","d9137fff":"code","66163cd0":"code","26868418":"code","7ea921d2":"code","da744ffd":"code","79ca93fe":"code","492940ae":"code","7bb36fb7":"code","c987c4a0":"code","5d607c67":"code","6bfaf121":"code","58fc6428":"code","7671987d":"code","2dbace66":"code","84050dab":"code","e9f49dbe":"code","e44907ea":"code","459be4e6":"code","fc3a07e6":"code","d0bab990":"code","6df6c18d":"code","db7a41dc":"code","b177c14a":"code","ea361028":"code","7cc540a1":"code","babf7d23":"code","5a323b83":"code","7cf18d79":"code","9faf29f8":"code","e4e66a1d":"code","b7a83262":"code","a2895677":"code","949c765f":"code","b72f9b05":"code","34a55257":"code","af5193f4":"code","cc0a7acb":"code","c096d09e":"code","9f8f152e":"code","5ccc544c":"code","bad60116":"code","1fc6e3e4":"code","4f96c708":"code","c55852b9":"code","2b8d00b9":"code","fc40fc1d":"code","611c4090":"code","d16df982":"code","9b355bd1":"code","8eb98b9c":"code","50548a41":"code","934ae5d5":"code","7d0cd36b":"code","87b7ae6a":"code","e31caf33":"code","b781b4e1":"code","f0d9a656":"code","cfa90f65":"code","62bfb61f":"code","4371b3a0":"code","a345551d":"code","3c2e2ca9":"code","614c7679":"code","3b172403":"code","f5635d90":"code","de8ad82f":"code","4db976b2":"code","38ffc355":"code","35d2aee3":"code","81590a4a":"code","244ed8b0":"markdown","be495a74":"markdown","b09d65a6":"markdown","c9916ee4":"markdown","37dedbac":"markdown","983026d3":"markdown","1fdf4b33":"markdown","25365fe2":"markdown","19d67432":"markdown","84650573":"markdown","da47558d":"markdown","b7a4f216":"markdown","5f61a241":"markdown","4b1c64d1":"markdown","72172751":"markdown","b821c228":"markdown","881823bc":"markdown","da1493bb":"markdown","8c8bcd1e":"markdown","1e2e3764":"markdown","ac1520e8":"markdown","32705bea":"markdown","7b4199c5":"markdown","1b3477ec":"markdown","8e492fa9":"markdown","3c593d9d":"markdown","8aa2d457":"markdown","2e98cf2b":"markdown","4a394958":"markdown","68c35268":"markdown","d4065294":"markdown","199ba160":"markdown","cac34764":"markdown","359fb686":"markdown","f2dc5478":"markdown"},"source":{"5680c02b":"from IPython.display import YouTubeVideo\nYouTubeVideo(id='CR4mDpFJoXs', width=800, height=500)","d9137fff":"%%capture\n!pip install advertools","66163cd0":"import advertools as adv\nimport pandas as pd\npd.options.display.max_columns = None\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom dataset_utilities import value_counts_plus\n\nfor p in [adv, pd, widgets]:\n    print(f'{p.__name__:\u2013<15}v{p.__version__}')\n","26868418":"crawl_df = pd.read_csv('\/kaggle\/input\/seocrawldatasets\/django.csv',\n                       parse_dates=['crawl_time', 'resp_headers_last-modified', 'resp_headers_date', 'resp_headers_expires'],\n                       low_memory=False)\nprint(f'Shape: {crawl_df.shape}')\ncrawl_df.head(3)","7ea921d2":"import re\nregex = '20\\d\\d-\\d\\d-\\d\\d \\d\\d:\\d\\d:\\d\\d.*?Forbidden by robots\\.txt: <GET (.*?)>'\nblocked_urls = []\n\nwith open('\/kaggle\/input\/seocrawldatasets\/django.log') as file: \n    for line in file: \n        url = re.findall(regex, line) \n        if url: \n            blocked_urls.append(url[0]) \n\n\nprint(f'Number of blocked URLs: {len(blocked_urls):,}\\n\\nSample:')\nblocked_urls[:10]","da744ffd":"sitemaps_df = pd.read_csv('\/kaggle\/input\/seocrawldatasets\/django_sitemaps.csv')\nsitemaps_df.head()","79ca93fe":"sitemaps_df['sitemap'].value_counts().to_frame().style.background_gradient(cmap='cividis').format('{:,}')","492940ae":"mycolors = {'blue', 'green', 'red', 'yellow'}\n\nyourcolors = {'blue', 'green', 'orange', 'purple'}","7bb36fb7":"mycolors.intersection(yourcolors)","c987c4a0":"mycolors.union(yourcolors)","5d607c67":"in_sitemap_not_crawled = set(sitemaps_df['loc']).difference(crawl_df['url'])\ncrawled_not_in_sitemap = set(crawl_df['url']).difference(sitemaps_df['loc'])\n\nprint(f'In sitemap not crawled: {len(in_sitemap_not_crawled):>7,}')\nprint(f'Crawled not in sitemap: {len(crawled_not_in_sitemap):>7,}')","6bfaf121":"print('crawl_df Shape:', crawl_df.shape)\ncrawl_df.head(2)","58fc6428":"crawl_df['status'].value_counts()","7671987d":"duration = crawl_df['crawl_time'].max() - crawl_df['crawl_time'].min()\nprint(f'Crawl duration: {duration}')\nprint(f'Pages crawled per second: {len(crawl_df) \/ duration.seconds:.3f}')\n","2dbace66":"from IPython.display import YouTubeVideo\nYouTubeVideo('IZUBvDfDbqo', width=896, height=480)","84050dab":"def count_column_values(col_name, show_top=10, sort_others=False):\n    series = crawl_df[col_name].str.split('@@').explode()\n    print(f'Number of {series.name}s: {series.count():,}')\n    print(f'Number of pages: {len(crawl_df):,}')\n    print(f'{series.name}s per page: {series.count()\/len(crawl_df):.2f}')\n    display(value_counts_plus(series, sort_others=sort_others, dropna=False, show_top=show_top))\n\nwidgets.interact(count_column_values,\n                 col_name=crawl_df.select_dtypes('object').columns,\n                 show_top=widgets.IntSlider(min=1, max=50, value=10));","e9f49dbe":"example_urls = [\n    'https:\/\/www.example.com\/one\/two\/three?price=10&color=red',\n    \n    'http:\/\/example.com\/one\/two?color=blue&price=15&size=small'\n]\n\nadv.url_to_df(example_urls)","e44907ea":"url_df = adv.url_to_df(crawl_df['url'])\nurl_df.head()","459be4e6":"(url_df\n .replace('', pd.NA)\n .notna()\n .mean().round(2)\n .to_frame()\n .rename_axis('URL Element')\n .rename(columns={'index': 'URL element', \n                  0: 'URLs using it (%)'})\n .assign(count=url_df.replace('', pd.NA).notna().sum())\n .style\n .format({'URLs using it (%)': '{:.1%}',\n          'count': '{:,}'}))","fc3a07e6":"def count_column_values(col_name, show_top=10, sort_others=False):\n    series = url_df[col_name].str.split('@@').explode()\n    print(f'Number of {series.name}s: {series.count():,}')\n    print(f'Number of pages: {len(crawl_df):,}')\n    print(f'{series.name}s per page: {series.count()\/len(crawl_df):.2f}')\n    display(value_counts_plus(series, sort_others=sort_others, dropna=False, show_top=show_top))\n\nwidgets.interact(count_column_values,\n                 col_name=url_df.select_dtypes('object').columns,\n                 show_top=widgets.IntSlider(min=1, max=50, value=10));","d0bab990":"(crawl_df\n ['canonical']\n .str.split('@@')\n .str.len()\n .value_counts(dropna=False)\n)","6df6c18d":"crawl_df[['url', 'canonical']]","db7a41dc":"# Pointing to self\n\ncanonical_eq = crawl_df[crawl_df['url'].eq(crawl_df['canonical'])][['url']]\ncanonical_eq","b177c14a":"eq_url_df = adv.url_to_df(canonical_eq['url'])\neq_url_df","ea361028":"eq_url_df['netloc'].value_counts()","7cc540a1":"eq_url_df['dir_2'].value_counts()","babf7d23":"# pointing to another URL\n\ncanonical_ne = crawl_df[crawl_df['url'].ne(crawl_df['canonical'])][['url', 'canonical']].dropna()\ncanonical_ne","5a323b83":"df1 = pd.DataFrame({\n    'a': [1, 2, 3, 4, 5],\n    'b': [10, 20, 30, 40, 50],\n    'c': list('ABCDE')\n})\ndf1","7cf18d79":"df2 = pd.DataFrame({\n    'a': [1, 2, 3, 14, 15],\n    'b': [555, 666, 30, 40, pd.NA],\n    'c': list('ABCDE')\n})\ndf2","9faf29f8":"df_compare = df1.compare(df2, )\ndf_compare","e4e66a1d":"df_compare['a']","b7a83262":"df_compare['a'].dropna()","a2895677":"df_compare['b'].dropna(how='all')","949c765f":"canonical_diff = (adv.url_to_df(canonical_ne['url'])\n                  .compare(adv.url_to_df(canonical_ne['canonical'])\n                           .assign(query_from=pd.NA,\n                                   query_utm_medium=pd.NA,\n                                   query_utm_source=pd.NA)))\ncanonical_diff","b72f9b05":"canonical_diff['scheme'].value_counts().reset_index()","34a55257":"canonical_diff['fragment'].dropna(how='all').value_counts().to_frame()","af5193f4":"canonical_diff['dir_1'].dropna(how='all')","cc0a7acb":"canonical_diff.loc[canonical_diff['dir_1'].dropna(how='all').index]['url']","c096d09e":"canonical_diff['dir_2'].value_counts().to_frame()","9f8f152e":"canonical_diff[(canonical_diff['dir_2']=='2.1')['self']]['dir_3'].dropna()","5ccc544c":"for url, canonical in crawl_df[crawl_df['url'].str.contains('\/2.1\/')][['url', 'canonical']].sample(10).values:\n    print('\\n')\n    print('URL:      ', url)\n    print('Canonical:', canonical)","bad60116":"redirect_df = crawl_df.filter(regex='^url$|redirect').dropna()\nredirect_df","1fc6e3e4":"redirect_df['redirect_times'].value_counts()","4f96c708":"redirect_df['redirect_reasons'].value_counts()","c55852b9":"adv.url_to_df(redirect_df['url']).columns","2b8d00b9":"redirect_df_compare = (adv.url_to_df(\n                               redirect_df['redirect_urls']\n                               .str.split('@@')\n                               .str[0])\n                       .assign(query_next=pd.NA)\n                       .compare(\n                           adv.url_to_df(redirect_df['url'])))\nredirect_df_compare","fc40fc1d":"redirect_df_compare['scheme'].dropna().value_counts()","611c4090":"redirect_df_compare['netloc'].dropna().value_counts()","d16df982":"redirect_df_compare['dir_1'].dropna().value_counts()","9b355bd1":"redirect_df_compare['dir_2'].dropna().value_counts()","8eb98b9c":"crawl_df.filter(regex='links_').head()","50548a41":"from collections import defaultdict\n\ndef count_duplicates(lst):\n    dd = defaultdict(int)\n    times_duplicated = []\n    for num in lst:\n        times_duplicated.append(dd[num])\n        dd[num] += 1\n    return times_duplicated","934ae5d5":"links_df = (crawl_df[['url']]\n            .assign(links_url=crawl_df['links_url'].str.split('@@')).explode('links_url')\n            .assign(links_text=crawl_df['links_text'].str.split('@@').explode(),\n                    links_nofollow=crawl_df['links_nofollow'].str.split('@@').explode()))\nlinks_df","7d0cd36b":"pd.cut(links_df.groupby('url')['links_url'].count(), \n       [0, 25, 50, 60, 70, 80, 90, 95, 100, 200, 300, 400, 500, 9999999]).value_counts().sort_index()","87b7ae6a":"links_df_count_dup = (pd.concat([links_df,\n                      links_df.groupby('url').transform(lambda s: count_duplicates(s)).add_suffix('_num_dupes')],\n                                axis=1)\n                      .drop('links_nofollow_num_dupes', axis=1)\n                      .assign(internal=lambda df: df['links_url'].str.contains('djangoproject\\.com').astype(bool)))\nlinks_df_count_dup","e31caf33":"links_df_count_dup.loc[0].sort_values(['links_url', 'links_url_num_dupes'])","b781b4e1":"links_df_count_dup['internal'].mean() # proportion of internal links","f0d9a656":"links_df_count_dup.groupby('url')['internal'].mean().describe()","cfa90f65":"pd.cut(links_df_count_dup.groupby('url')['internal'].mean(), 5).value_counts().sort_index()","62bfb61f":"external_links_df = adv.url_to_df(links_df_count_dup[links_df_count_dup['internal']==False].drop_duplicates(subset=['links_url'])['links_url'])\nexternal_links_df","4371b3a0":"external_links_df['netloc'].value_counts()[:20]","a345551d":"external_links_df.notna().agg(['mean', 'sum'])","3c2e2ca9":"external_links_df[external_links_df['netloc']=='www.youtube.com']['path'].value_counts()","614c7679":"external_links_df[external_links_df['netloc']=='www.youtube.com']['query'].value_counts()","3b172403":"external_links_df[external_links_df['netloc']=='www.youtube.com']['query_v'].value_counts()","f5635d90":"links_df_count_dup[links_df_count_dup['links_url_num_dupes']>3]","de8ad82f":"X = pd.get_dummies(url_df[['scheme', 'netloc', 'dir_1','dir_2',\n                           'dir_3', 'dir_4','dir_5']], \n                   prefix_sep=': ')    \ny = crawl_df[['status']]\nprint(\"X Shape:\", X.shape)\nX.head()","4db976b2":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, y)","38ffc355":"import graphviz\ndot_data = tree.export_graphviz(clf, out_file=None, filled=True, rounded=True,\n                                class_names=[str(c) for c in clf.classes_],\n                                max_depth=4, \n                                feature_names=X.columns)\ngraph = graphviz.Source(dot_data)\ngraph","35d2aee3":"crawl_df[crawl_df['url'].str.contains('\/dev\/') & ~crawl_df['url'].str.contains('\/en\/')]['status'].value_counts()","81590a4a":"crawl_df['status'].value_counts()","244ed8b0":"# URLs Blocked by robots.txt:\nThe crawl logs file contains lines that indicate the URLs that were blocked, and therefore, no request\/response was made to them.  \n\nThe following code creates a regex pattern to find and extract those URLs, and adds them to the list `blocked_urls`.  \nThis is a example of such a line:  \n\n`2020-11-15 13:24:36 [scrapy.downloadermiddlewares.robotstxt] DEBUG: Forbidden by robots.txt: <GET https:\/\/docs.djangoproject.com\/zh-hans\/3.0\/internals\/mailing-lists\/>`","be495a74":"## Some questions that you might be interested in:\n\n* Do they have sub-domains? How many URLs each?\n* How deep is their directory structure? \/dir_1\/dir_2...\/dir__?\n* What are the different values, and the counts, of elements in `dir_1`, `dir_2`, etc.?\n* Under the \/products\/ `dir_1` directory, what is the split of categories `dir_2` for example?\n* In the English URLs `\/en\/` (`dir_1`), where `\/category\/` (`dir_3`) is \"bags\", what is the split of \"brands\" (`dir_4`) for example?\n* Do they use query parameters? If so, how many of each?\n* Filter the URLs that have `query_price` and sort them in descending order by the same column.\n* Which color does the website have most of (`query_color`), and how many products have a color?\n\nThe above are some examples and you can definitely think of others as well.","b09d65a6":"## Compare sitemaps and crawled URLs","c9916ee4":"# Dataset Structure\n\n* Each URL is independently and completely represented by a row\n* Each column contains information about one, and only one attribute of a URL\n* Multiple elements on a page, if available, are separated by two @ signs and live in one cell, for example `one@@two@@three`\n* Missing vs. empty elements: Having a missing value `NaN` means that that element does not exist on the page. On the other hand, an empty value, means that the element exists, but its text attribute is the empty string \"\". For example `<h2><\/h2>` is an h2 tag that exits on a page, yet empty (contains no text).\n\nExamples are always easier: \n\nurl | title | h1 | h2 | h2 split\n----|-------|----|----|---------\nhttps:\/\/example.com | Home Page | Welcome to the site | Sign up@@Subscribe to the newsletter | [\"Sign up\", \"Subscribe to the newsletter\"]\nhttps:\/\/example.com\/about | About Us | Learn more about us | Our Story@@How we Started@@What we do | [\"Our Story\", \"How we Started\", \"What we do\"]\nhttps:\/\/example.com\/product_1 | Product One | `NaN` | Specs@@Price@@Details | [\"Specs\", \"Price\", \"Details\"]\nhttps:\/\/example.com\/product_2 | Product Two | Welcome to the site | | [\"\"]\n\n\n<br>\n\nAll pages contain one h1 tag, with the exception of `\/product_1` , which does _not_ have an h1 tag.\n\nAll pages contain multiple h2 tags, (2, 3, 3, and 1) separated by the two @ signs, with the exception of `\/product_2`, it has one h2 tag, but it's empty. The column `h2 split` shows the actual values after splitting them.\n\n## Data Format\nBy default the crawl dataset is saved as a `jsonlines` file with the extension `.jl`. This format saves each row independently, and allows for flexibility in representing data, because not all pages contain the same data, and so they can be different (contain different columns). Once you import them as a DataFrame, columns are aligned automatically. \n\nI already saved to a CSV format by using the `pd.DataFrame.to_csv` method. \n","37dedbac":"There are 18,113 pages containing no canonical tags, and the remaining 17,350 contain one each.  \nWe can extract and split the URLs and canonicals based on whether or not, they point to themselves. We can do this by checking if `url` is (not) equal to `canonical`:","983026d3":"### The distribution of the number of links per page","1fdf4b33":"# Link Analysis","25365fe2":"# URL Analysis\nThe `url_to_df` function converts lists of URLs to a DataFrame, with each element in its respective column.\n\n`https:\/\/www.example.com\/dir_1\/dir_2\/...\/dir_n`\n\n`https:\/\/www.example.com\/dir_1\/dir_2\/...\/dir_n?one=1&two=2` ==> query_one=1, query_two=2","19d67432":"This shows that URLs with a fragment are pointing URLs without a fragment. These are probably pointing to the URL itself, but we have to double check that. Let's check the directories:","84650573":"### **You need to run the notebook for this to work, you can do so by forking it `Copy and Edit`, and hitting `Run All`**","da47558d":"## Why mechanically compareing URLs as strings might not work:\n\n* http:\/\/example.com?productid=1\n\n\n* https:\/\/example.com?productid=1\n\n* http:\/\/example.com?productid=2","b7a4f216":"# `crawl_df` Exploration","5f61a241":"There seems to be a clear pattern here in most cases. The values in `dir_2` show the version of the software, and almost all of them point to the 3.1 version URLs. \n\nWe can run a quick check. Get the colums where `dir_1` is equal to \"2.1\", then check what we have in `dir_3`, if we get an empty DataFrame, it means that all values in `dir_3` are the same in both URLs, which means the only difference is in `dir_2`:","4b1c64d1":"Recently, `pandas` introduced a new interesting method for comparing two DataFrames.  \nLet's quickly see how it works:","72172751":"It seems all URLs pointing to themselves are documentation URLs, and almost all of them are from the latest version 3.1 to itself.","b821c228":"# Counting occurrences of non-numeric values on pages\nFor each page element, the following questions can be interesting to explore: \n\n- How many occurrences of a certain element does this site have (especially for elements that occur multiple times on a page: h2, h3, links_url, links_text, etc.)?\n- How many pages do _not_ contain this element?\n- How many times is this element duplicated?\n- How many unique values form 70%, 80%, etc. of the total?\n\nThe below dropdown shows all available non-numeric columns in the `crawl_df` and allows you to answer the above questions interactively.\n\n## Column names:\n\n* `count`: The simple count of this value of this element across pages.\n* `cum_count`: The cumulative counts of elements up to this points.\n* `perc`: The percentage of the total for this element.\n* `cum_perc`: The cumulative percentage of of this element up to this point. This is where you can set priorities, knowing that the first seven elements (out of 500) form 75% of your values for example.\n* `Others:` This row contains all other values grouped in one row. You can use the slider to change how many rows you want to see, and this would change the count of `Others`. By default, it is displayed at the end of the table, but you can select `sort_others` to place this row in its sorted location in the table.\n\n\n## Counts Meaning and Interpretation\n\nThe counts would have different meanings in different situations, and based on which element you are looking at, for example:\n\n* It's not a problem if you have a page with a missing `h5` tag, but a page with a missing `title` tag probably is.\n* Having multiple `h3` tags on the same page is perfectly normal, but multiple canonicals defeats the purpose.\n* Three duplicated links on all pages linking to the home page, about us, and contact pages is perfectly normal, but having your `title` tag duplicatd across all pages is generally considered a problem.\n* In many cases you have to be the judge.\n\nHere's an example of how to use this interactively:","881823bc":"It seems there are 1,019 URLs where the difference is that the http version is pointing to the https version. This doesn't mean that it's the only difference, but it is at least one.  \nNote that we don't have a `netloc` column, which means that all canonicals are pointing to a URL under the same (sub)domain.  \n\nLet's take a look at fragments:","da1493bb":"## How \"full\/empty\" are those URL elements?","8c8bcd1e":"# Decision Tree to Locate 404s\n\nAre there certain pages, sub-domans, directories that have disproportionatly more 404s?\nOne way to check is to manually go through all possible options, and count them. \nAnother, is to use `sklearn`'s `DecisionTreeClassifier` for that.\n\n`X` is going to be the matrix of the most important elements in the URLs. Since its values are text values, we need to encode them using numbers. An easy way to do this is through the `pandas.get_dummies` function.  \n`y` is going to be the dependent variable, which is the `status` column in crawl_df in this case. We want to discover a set of rules to follow for finding where those 404's are hapening.","1e2e3764":"# SEO Crawl Dataset Exploration\n\n## A template for first steps in analyzing a crawl dataset\n\nThis dataset was generated using the unreleased version 0.11.0 of advertools. It is a crawl of the URLs of Django's website https:\/\/djangoproject.com. \nThe following function call starts crawling from the home page, follows links (`follow_links=True`), discovers and crawls all pages, and saves the output to the specified path `output_file=django1.jl`.  \nThe parameter `allowed_domains` restricts the crawler to those mentioned (sub)domains. In this case I wanted to check `docs` and `www`. There is a huge code.djangoproject.com domain for issues and tickets (~0.5M pages), which was not included.  \nThe crawler produces logs for every page crawled, and all the details can be saved to a file for later checking (errors, URLs blocked by robots.txt, etc). You simply have to specify a file path for that under `custom_settings`.\n\nFor large websites it's good to create a `JOBDIR` which allows you to pause and resume whenever you want, without having to worry about re-crawling and handling duplicates.\n\n\n`import advertools as adv`  \n\n`adv.crawl('https:\/\/djangoproject.com',  \n           output_file='django1.jl',     \n           follow_links=True,  \n           allowed_domains=['docs.djangoproject.com', 'www.djangoproject.com'],  \n           custom_settings={'LOG_FILE': 'django.log'  \n                            'JOBDIR': 'dj_job_1'})`","ac1520e8":"## External Links","32705bea":"Now, let's do the same to compare the crawled URLs with the canonicals that they point to (where they are not equal). \nWe first convert each set of URLs to a DataFrame using the `adv.url_to_df` function. To be able to compare DataFrames, they need to have the same columns, so I added three ones with missing values: `query_from`, `query_utm_medium`, and `query_utm_source`:","7b4199c5":"Missing values `NaN` mean that values are equal at those positions. Removing missing values, we end up with the rows that have differing values only:","1b3477ec":"First, note that since the column `c` is the same in both DataFrames, it was not included in `df_compare`. The columns where we have differences, are represented as MultiIndex columns, where the first level is the column name, and the second level, contains `self` and `other`.","8e492fa9":"## How long did the crawl take? \n\nEach row has a timestamp showing when it was crawled under `crawl_time`. Assuming the crawl happened in one job, we can simply subtract the minimum time from the maximum time, and the difference would be the answer.","3c593d9d":"# Redirects","8aa2d457":"It seems it is mainly in `dir_2` where we have differences. Here is a sample of URLs and their canonicals:","2e98cf2b":"$2,362 \u00f7 3,418 = 0.69$\n\nIt looks like we captured almost 70% of all the 404s just by following these two rules. The second one is still very small, adding about 200 samples to the final. If you also look at the number of samples in the blue boxes, you'll easily see how small the numbers are, meaning there isn't a clear pattern after the first few levels in the tree, which is normal.  \nIt's not clear why the `\/dev\/` URLs generate so many URLs, and it would be interesting to see. It could be that the documentation automatically links to certain pages in the latest version, and those pages haven't been generated yet (because it's the \"dev\" version)?  \nMaybe! ","4a394958":"## Possible Solution:\n* Only introduce new URLs if there is a change in a certain topic\n* If a certain page has content that is valide for several releases, mention it in the title (and on page)\n\nurl | title\n----|-------\ndocs.djangoproject.com\/topic_A | Topic A docs: versions 1, 2, 3\ndocs.djangoproject.com\/topic_A_feature_X | Topic A docs: versions         4, 5\ndocs.djangoproject.com\/topic_B | Topic B docs: versions 1, 2\ndocs.djangoproject.com\/topic_B_feature_Y | Topic A docs: versions      3, 4\ndocs.djangoproject.com\/topic_B_feature_Z | Topic A docs: versions           5","68c35268":"This is a very small set of URLs (54), where we have a language code, and those pages are pointing to URLs without anything in `dir_1`, probably pointing to the home page. \n\nWe can verify this by getting the index of the previous URLs, and using it to extract the pages in `url_diff` on those index values. \n\nIndeed, as you can see, those URLs are search pages, and they point to the home page of the docs. It would be better to link to the home page of the same language, because the home page defaults to English.","d4065294":"In this video, we go through and discuss this notebook, so you might be interested in following along:","199ba160":"# XML Sitemaps\n\nThe function `sitemap_to_df` crawls (multiple) sitemaps, and converts them to a DataFrame. It can take a regular sitemap, or a sitemap index. \n\n\n`import advertools as adv`  \n`django_sitemap = adv.sitemap_to_df('https:\/\/www.djangoproject.com\/sitemap.xml')`\n","cac34764":"According to he above tree, URLs where `dir_2` is equal to \"dev\" and where `dir_1` is **not** equal to \"en\", contain 2,362 404s. Let's check and compare that with the total:","359fb686":"### Get the video IDs of the YouTube vidoes (and their counts), linked to on the site","f2dc5478":"# Canonicals\n\nHow many canonical tags do we have per page?"}}