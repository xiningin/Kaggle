{"cell_type":{"60e3c5ba":"code","1862f718":"code","bb1c06d0":"code","41587573":"code","e9f53b7d":"code","ce92f581":"code","df3791ff":"code","852b0ae5":"code","0b121cce":"code","950f74c3":"code","500cd595":"code","2b05e0df":"code","f51c514f":"code","cb6042a2":"code","6f3c1c6d":"code","b2d07b4e":"code","68a79214":"code","a32dfc37":"code","20c26393":"code","b66289f3":"code","2644d186":"code","665c719e":"code","27f7b170":"code","02acf6ab":"code","5c33fce7":"code","ecf1ad45":"code","f4f2c92a":"code","4989e16d":"code","ddc52fe7":"code","043bc2ad":"code","4eed3c48":"code","fe85eb90":"code","88d93acf":"code","8ff8dfb8":"code","dc4ffff6":"code","e2b00a94":"code","f81909e2":"code","b04d86dc":"code","eaf41c15":"code","3ba0eaa3":"code","eceafd13":"code","d4943f58":"code","52f90c7d":"code","359d9c26":"code","2dc9f72f":"code","9582b695":"code","2c06ca06":"code","643e3ef7":"code","143f7cae":"code","d40c5f51":"code","850ced9a":"code","7b887184":"code","9124c3e8":"code","af9847b5":"code","57dab356":"code","99000cea":"markdown","7e10b396":"markdown","2cf3d03b":"markdown","2d1ee07d":"markdown","7c3a6354":"markdown","33ad48a1":"markdown","c7f7e647":"markdown","841a0912":"markdown","cf884546":"markdown","19962101":"markdown","f047a144":"markdown","fcd400a8":"markdown"},"source":{"60e3c5ba":"import pandas as pd\nimport numpy as np\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nimport xgboost as xgb\n","1862f718":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","bb1c06d0":"from sklearn.base import BaseEstimator, TransformerMixin","41587573":"import chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot, init_notebook_mode\n\ninit_notebook_mode(connected = True)\n# for offline plots\nimport cufflinks\ncufflinks.go_offline(connected = True)","e9f53b7d":"!ls ..\/input\/ericsson-ml-challenge-materialtype-prediction\/","ce92f581":"df_train = pd.read_csv(\"..\/input\/ericsson-ml-challenge-materialtype-prediction\/train_file.csv\")\ndf_test = pd.read_csv(\"..\/input\/ericsson-ml-challenge-materialtype-prediction\/test_file.csv\")","df3791ff":"df_train.shape, df_test.shape","852b0ae5":"df_train.columns","0b121cce":"df_train.head()","950f74c3":"df_train.info()","500cd595":"df_train.describe()","2b05e0df":"## UsageClass, CheckoutType, CheckoutYear, CheckoutMonth has unique entries. Hence that would not be useful for analysis\ndf_train.UsageClass.unique()","f51c514f":"df_train.CheckoutType.unique()","cb6042a2":"df_train.CheckoutYear.unique()","6f3c1c6d":"df_train.CheckoutMonth.unique()","b2d07b4e":"plt.figure(figsize = (15,5))\nplt.subplot(2,2,1)\nsns.countplot(x ='UsageClass', data = df_train)\n\nplt.subplot(2,2,2)\nsns.countplot(x ='CheckoutType', data = df_train)\n\nplt.subplot(2,2,3)\nsns.countplot(x= 'CheckoutYear', data = df_train)\n\nplt.subplot(2,2,4)\nsns.countplot(x ='CheckoutMonth', data = df_train)","68a79214":"## Checking the above 4 columns in test dataset and we have unique values in test set too\n\nplt.figure(figsize = (15,5))\nplt.subplot(2,2,1)\nsns.countplot(x ='UsageClass', data = df_test)\n\nplt.subplot(2,2,2)\nsns.countplot(x ='CheckoutType', data = df_test)\n\nplt.subplot(2,2,3)\nsns.countplot(x= 'CheckoutYear', data = df_test)\n\nplt.subplot(2,2,4)\nsns.countplot(x ='CheckoutMonth', data = df_test)","a32dfc37":"## function for plotting categorical variables count\n\ndef categorical_feature_distribution(feature_name, target_name, top_counts = None):\n    material_trace =[]\n    \n    for material in df_train[target_name].unique():\n        if not top_counts:\n            tmp_material = df_train[df_train[target_name]==material][feature_name].value_counts()\n        else:\n            tmp_material = df_train[df_train[target_name]==material][feature_name].value_counts()[:top_counts]\n        \n        tmp_trace = go.Bar(\n                        x = tmp_material.index,\n                        y = tmp_material.values,\n                        name = material\n                        )\n        material_trace.append(tmp_trace)\n        \n    layout = go.Layout(\n                barmode ='group',\n                title =feature_name + ' vs ' + target_name +' - Distribution',\n                yaxis = dict(title = 'Counts'),\n                xaxis = dict(title = feature_name)\n                    )\n    \n    fig = go.Figure(data = material_trace, layout = layout)\n    iplot(fig, filename ='grouped-bar')\n    ","20c26393":"import plotly\nplotly.offline.init_notebook_mode()\ndf_train['MaterialType'].iplot(kind='hist', xTitle='Material Types',\n                              yTitle ='Count', title ='Target Distribution')","b66289f3":"categorical_feature_distribution('Checkouts','MaterialType')","2644d186":"len(df_train['Creator'].unique()) # There are totally 6k creators, lets view 20 creators","665c719e":"categorical_feature_distribution('Creator','MaterialType',20)","27f7b170":"len(df_train['Publisher'].unique())","02acf6ab":"categorical_feature_distribution('Publisher','MaterialType',20) # Lets view only 20 publishers","5c33fce7":"len(df_train['Subjects'].unique())","ecf1ad45":"categorical_feature_distribution('Subjects','MaterialType',20)","f4f2c92a":"## Creating wordclouds\n\ndef create_wordcloud(material):\n    title = \" \".join(t for t in df_train[df_train['MaterialType']==material]['Title'])\n    print(\"There are totally {} words\".format(len(title)))\n    wordcloud = WordCloud(width = 1000, height = 400, margin = 0).generate(title)\n    \n    plt.figure(figsize=(10,5))\n    plt.imshow(wordcloud, interpolation ='bilinear')\n    plt.axis('off')\n    #plt.margins(x =0 , y=0)\n    plt.show()","4989e16d":"create_wordcloud('BOOK')","ddc52fe7":"create_wordcloud('SOUNDDISC')","043bc2ad":"create_wordcloud('VIDEODISC')","4eed3c48":"create_wordcloud('CR')","fe85eb90":"(df_train.isnull().sum() \/ df_train.shape[0])*100","88d93acf":"df_test.isnull().sum()\/df_test.shape[0] *100","8ff8dfb8":"## imputing as nopublisher, nocreater, nosubject\n\ndf_train['Publisher'].fillna('NoPublisher', inplace = True)\ndf_test['Publisher'].fillna('NoPublisher', inplace = True)\n\ndf_train['Subjects'].fillna('NoSubjects', inplace = True)\ndf_test['Subjects'].fillna('NoSubjects', inplace = True)\n\ndf_train['Creator'].fillna('NoCreator', inplace = True)\ndf_test['Creator'].fillna('NoCreator', inplace = True)\n","dc4ffff6":"df_train.isnull().sum(), df_test.isnull().sum()","e2b00a94":"df_train['info'] = df_train['Title'] +' '+ df_train['Subjects']+' '+df_train['Publisher']+' '+ df_train['Creator']\ndf_test['info'] = df_test['Title']+' '+df_test['Subjects']+' '+df_test['Publisher']+' '+df_train['Creator']","f81909e2":"target_encoder = LabelEncoder()\ny = target_encoder.fit_transform(df_train['MaterialType'])\n\ncategorical_features = ['Checkouts','Creator','Subjects','Publisher']\n\nfor col in categorical_features:\n    print(col)\n    le = LabelEncoder()\n    le.fit(list(df_train[col]) + list(df_test[col]))\n    df_train[col] = le.transform(df_train[col])\n    df_test[col] = le.transform(df_test[col])\n    ","b04d86dc":"stopwords = set(stopwords.words('english'))","eaf41c15":"def tokenize(text):\n    '''\n        Input: text\n        Returns: clean tokens\n        Desc:\n            Generates a clean token of text (words) by first getting words from the text.\n            Applies Lemmatization on the words.\n            Normalize the text by lowering it and removes the extra spaces.\n    '''\n    tokens = word_tokenize(text)\n    lemmatizer = WordNetLemmatizer()\n\n    clean_tokens = []\n    for tok in tokens:\n        #\n        if tok not in string.punctuation:# and tok not in stop_words:\n            clean_tok = tok.lower().strip()\n            #clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n            clean_tokens.append(clean_tok)\n\n    return clean_tokens","3ba0eaa3":"class TextLengthExtractor(BaseEstimator, TransformerMixin):\n    '''\n        Input: X\n        return: pandas series of length of text\n        TextLengthExtractor is a transformer , can be used in pipeline to extract the length of the text from a given input.\n        Input can be an array of text or pandas Series.\n    '''\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return pd.Series(X).apply(lambda x: len(x)).values.reshape(-1,1)\n","eceafd13":"class WordCountExtractor(BaseEstimator, TransformerMixin):\n    '''\n        Input: X\n        return: pandas series of word count\n        WordCountExtractor is a transformer , can be used in pipeline to extract the number of words of the text from a given input.\n        Input can be an array of text or pandas Series.\n    '''\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return pd.Series(X).apply(lambda x: len(x.split())).values.reshape(-1,1)\n    ","d4943f58":"class MessageExtractor(BaseEstimator, TransformerMixin):        \n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[\"info\"]\n    \n    ","52f90c7d":"class FeatureSelector( BaseEstimator, TransformerMixin ):\n    #Class Constructor     \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n        return X[ [ 'Checkouts','Creator','Publisher','Subjects'] ] ","359d9c26":"pipeline = Pipeline([\n        ('features', FeatureUnion([\n\n            ('text_pipeline', Pipeline([\n                ('text', MessageExtractor()),\n                ('tfidf', TfidfVectorizer(tokenizer=tokenize,\n                                         ngram_range=(1,2),\n                                         max_df=0.5,\n                                         max_features=5000,\n                                         use_idf=False)),\n            ])),\n            \n            ('categorical_features', Pipeline([\n                ('cat_features', FeatureSelector()),\n                \n            ])),\n            \n            ('text_length_pipeline', Pipeline([\n                ('text', MessageExtractor()),\n                ('text_len', TextLengthExtractor()),\n                \n            ])),\n\n            ('word_count_pipeline', Pipeline([\n                ('text', MessageExtractor()),\n                ('word_count', WordCountExtractor()),                \n            ])),\n\n        ])),\n        \n]) ","2dc9f72f":"pipeline.fit(df_train)","9582b695":"XTest_trans = pipeline.transform(df_test)\ndxtest = xgb.DMatrix(XTest_trans)","2c06ca06":"XTest_trans.todense()","643e3ef7":"param = {'objective':'multi:softprob',\n        'eta':0.1,\n        'max_depth':6,\n        'silent':1,\n        'nthread':4,\n        'num_class':8,\n        'eval_metric':['mlogloss'],\n        'seed':1}","143f7cae":"num_splits = 5\nskf = StratifiedKFold(n_splits = num_splits, shuffle = True, random_state=1)","d40c5f51":"y_test_pred = np.zeros((df_test.shape[0],8))\nprint(y_test_pred.shape)\ny_valid_scores =[]\n\nX = df_train\nfold_cnt = 1\nfor train_index, val_index in skf.split(X, y):\n    print(\"Fold....\", fold_cnt)\n    fold_cnt+=1\n    \n    X_train, X_valid = X.iloc[train_index], X.iloc[val_index]\n    y_train, y_valid = y[train_index], y[val_index]\n    \n    X_train_trans = pipeline.transform(X_train)\n    dtrain = xgb.DMatrix(X_train_trans, label = y_train)\n    \n    X_valid_trans = pipeline.transform(X_valid)\n    dvalid = xgb.DMatrix(X_valid_trans, label = y_valid)\n    \n    evallist = [(dtrain, 'train'), (dvalid,'valid')]\n    \n    num_round = 10000\n    bst = xgb.train(param, dtrain, num_round, evallist, early_stopping_rounds=10, verbose_eval = 50)\n    \n    #prediction -oof\n    y_pred_valid = bst.predict(dvalid, ntree_limit = bst.best_ntree_limit)\n    y_valid_scores.append(f1_score(y_valid, np.argmax(y_pred_valid, axis = 1), average = 'weighted'))\n    \n    #prediction -test set\n    \n    y_pred = bst.predict(dxtest, ntree_limit = bst.best_ntree_limit)\n    y_test_pred += y_pred\n    \n    ","850ced9a":"y_test_pred \/= num_splits","7b887184":"y_valid_scores","9124c3e8":"np.mean(y_valid_scores)","af9847b5":"pred_material = np.argmax(y_test_pred, axis = 1)\noutput = df_test[['ID']].copy()\noutput['MaterialType'] = target_encoder.inverse_transform(pred_material)","57dab356":"output.MaterialType.unique()","99000cea":"Numerical encode categorical features","7e10b396":"# Missing data handling","2cf3d03b":"# Pipeline","2d1ee07d":"# Load Dataset","7c3a6354":"# EDA","33ad48a1":"Add some functions and transformer class","c7f7e647":"# Output","841a0912":"Create info column as combination of - Title, Subjects, Publisher, Creator","cf884546":"# OOF\n\n![OOF](https:\/\/github.com\/asingleneuron\/Ericsson-Machine-Learning-Challenge\/blob\/master\/images\/OOF_PREDICTION.png?raw=True)\n\nReference: https:\/\/www.youtube.com\/watch?v=DW6gUvb8U8c&t=12s","19962101":"# XGB parameters","f047a144":"\n# Pipeline\n\n![Pipeline](https:\/\/github.com\/asingleneuron\/Ericsson-Machine-Learning-Challenge\/blob\/master\/images\/PIPELINE.jpg?raw=True)\n\nReference: https:\/\/www.youtube.com\/watch?v=DW6gUvb8U8c&t=12s","fcd400a8":"Reference:\nI would like to give complete credit to Shobhit upadhyaya for sharing his solution and dataset https:\/\/www.youtube.com\/watch?v=DW6gUvb8U8c&t=12s\n\n"}}