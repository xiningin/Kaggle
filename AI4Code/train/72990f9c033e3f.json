{"cell_type":{"58ea86c9":"code","cf442861":"code","625885a5":"code","a8679615":"code","04cec9ef":"code","3b9e4745":"code","9d29a681":"code","56a3df56":"code","1f461717":"code","684833d7":"code","02e796b9":"markdown","2197c8e4":"markdown","47b05cd1":"markdown","04bef188":"markdown","d44574f5":"markdown","07a8fb21":"markdown","6cd7e2d7":"markdown","87310b5a":"markdown"},"source":{"58ea86c9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom tensorflow.contrib.learn import preprocessing\nfrom keras.callbacks import ModelCheckpoint\nimport re\nfrom pickle import dump","cf442861":"train_df = pd.read_csv('..\/input\/train.csv')\nauthor = train_df[train_df['author'] == 'EAP'][\"text\"]\nauthor[:5]","625885a5":"max_words = 5000 # Max size of the dictionary\ntok = keras.preprocessing.text.Tokenizer(num_words=max_words)\ntok.fit_on_texts(author.values)\nsequences = tok.texts_to_sequences(author.values)\nprint(sequences[:5])","a8679615":"text = [item for sublist in sequences for item in sublist]\nlen(text)","04cec9ef":"sentence_len = 20\npred_len = 1\ntrain_len = sentence_len - pred_len\nseq = []\n# Sliding window to generate test and train data\nfor i in range(len(text)-sentence_len):\n    seq.append(text[i:i+sentence_len])\n# Reverse dictionary so as to decode tokenized sequences back to words and sentences\nreverse_word_map = dict(map(reversed, tok.word_index.items()))\ndump(tok, open('tokenizer.pkl', 'wb'))","3b9e4745":"trainX = []\ntrainy = []\nfor i in seq:\n    trainX.append(i[:train_len])\n    trainy.append(i[-1])\n#print(\"Training on : \",\" \".join(map(lambda x: reverse_word_map[x], trainX[0])),\"\\nTo predict : \",\" \".join(map(lambda x: reverse_word_map[x], trainy[0])))","9d29a681":"model = keras.Sequential()\nmodel.add(keras.layers.Embedding(max_words,100,input_length=train_len))\nmodel.add(keras.layers.LSTM(256, dropout=0.6, recurrent_dropout=0.2))\nmodel.add(keras.layers.Dense(1024,activation=\"relu\"))\nmodel.add(keras.layers.Dense(4999,activation=\"softmax\"))\nmodel.summary()","56a3df56":"model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nfilepath = \".\/weight_tr5.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\nhistory = model.fit(np.asarray(trainX),\n         pd.get_dummies(np.asarray(trainy)),\n         epochs = 500,\n         batch_size = 10240,\n         callbacks = callbacks_list,\n         verbose = 2)","1f461717":"def gen(seq,max_len = 20):\n    sent = tok.texts_to_sequences([seq])\n    #print(sent)\n    while len(sent[0]) < max_len:\n        sent2 = keras.preprocessing.sequence.pad_sequences(sent[-19:],maxlen=19)\n        op = model.predict(np.asarray(sent2).reshape(1,-1))\n        sent[0].append(op.argmax()+1)\n    return \" \".join(map(lambda x : reverse_word_map[x],sent[0]))","684833d7":"start = [(\"i am curious of\",26),(\"is this why he was \",32),\n         (\"he was scared of such \",24),(\"sea was blue like nothing else \",20),\n        (\"the last day i colud ever enjoy\",50),(\"could you stop doing all this you trouble me a lot\",600)]\n# Last one was Describe in 600 words\nfor i in range(len(start)):\n    print(\"<<-- Sentence %d -->>\\n\"%(i),gen(start[i][0],start[i][1]))","02e796b9":"## Generator\nHere this function iteratively predicts and generates sentences.<br>\nIt takes initial words as input and the total length of text to be generated. It padds the input and then predicts the next word. Then appends the predicted word to the input sentence and this continues iteratively till the specified sentence length is reached.","2197c8e4":"# Testing it up\n### Can it write a 600 word school literature answer ??","47b05cd1":"Now we will combine all the above generated tokens or well said as sequences to a single one. This is so that we can apply the sliding windows for training.","04bef188":"## Generating sequences for training data\nWe create sequencs using sliding window method. On every iteration we move the frame by 1 stride(distance), then we consider the n-1 evements of the frame for training and the nth evement is predicted. Here the value of n is 20.<br>\n<br>\n**Example: **<br>\n**Iteration 1**<br>\nsentence --> \"i am a author whose books don't get published\"<br>\ntrainX --> \"i am a author whose books don't get\"<br>\nY --> \"published\"<br>\n**Iteration 2**<br>\nsentence --> \"am a author whose books don't get published easily\"<br>\ntrainX --> \"am a author whose books don't get published\"<br>\nY --> \"easily\"<br>\n![img](https:\/\/eli.thegreenplace.net\/images\/2018\/markov-chain-window.png)","d44574f5":"## Preprocessing\nHere we choose text from a particular author. The LSTM will generate text and act like an author in himself.<br>\n**Why single author ??** *This is because the choice of words, styles and other things vary from person to person.*","07a8fb21":"# Generating text using LSTM","6cd7e2d7":"## Tokenization\nWe will convert all the text to sequences using Keras tokenizer. The padding of sequences is not needed here. We will see that in next cell","87310b5a":"Simple LSTM based text generator.\n## Flow\n1. **Preprocessing**  - Selecting texts of a specific author.<br>\n2. **Tokenization** - Converting texts to word tokens using keras tokenizer.<br>\n3. **Sliding Window** - Generating training sequences by combining all the text and making sliding windows<br>\n4. ** Model**  - Simple LSTM Keras model<br>\n5. ** Generator**  - Padds the user input to start generation, and generates text till set word limit is reached<br>"}}