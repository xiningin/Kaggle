{"cell_type":{"31b0c133":"code","331bb734":"code","50fb23c4":"code","5a68dd53":"code","12dead4a":"code","920f056d":"code","6fd2cd13":"code","11366d36":"code","d060c40d":"code","47fb3326":"code","3bb968e0":"code","b4a49098":"code","d3e55933":"code","9dc46c95":"code","37263743":"code","17bc790c":"code","dcbbf2b6":"code","5995fdfb":"code","5cb4ab8f":"markdown","68090994":"markdown","af9524b7":"markdown","3725fe70":"markdown","7b4ad666":"markdown","c7a0b259":"markdown","6f3635a5":"markdown","95e1162e":"markdown"},"source":{"31b0c133":"import numpy as np \nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier","331bb734":"data=pd.read_csv('..\/input\/Absenteeism_at_work.csv')\ndata.head()","50fb23c4":"data.describe()","5a68dd53":"print(\"Element wise count for Output class:\")\ndata['Absenteeism time in hours'].value_counts(sort = False)","12dead4a":"sns.pairplot(data)","920f056d":"fig, ax = plt.subplots(figsize=(20, 20)) \nsns.heatmap(data.corr(), annot = True, ax = ax)","6fd2cd13":"y = data['Absenteeism time in hours']\nX = data.drop(['Absenteeism time in hours'], axis=1)","11366d36":"# Feature Scaling\nfrom sklearn import preprocessing\nx = X.values\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nX = pd.DataFrame(x_scaled,columns=list(X.columns))\n\n# Splitting data into train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3)\nprint( \"\\nX_train:\\n\")\nprint(X_train.head())\nprint( X_train.shape)\nprint( \"\\nX_test:\\n\")\nprint(X_test.head())\nprint( X_test.shape)","d060c40d":"# Classification technique using KNN \nerror=[]\naccuracy=[]\nfor i in range(1, 40):    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    error.append(np.mean(y_pred != y_test))\n    accuracy.append(metrics.accuracy_score(y_test,y_pred))\n\nprint(\"Error Rate:\\n\",error)\nprint(\"Accuracy Score:\\n\",accuracy)\n","47fb3326":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')  \nplt.xlabel('K Value')  \nplt.ylabel('Mean Error') \n","3bb968e0":"k = np.argmin(error) + 1\nknn = KNeighborsClassifier(n_neighbors = k)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix  \nprint(\"Accuracy when k={}: \".format(k),metrics.accuracy_score(y_test, y_pred)) \nprint(\"Confusion matrix: \\n\",confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred)) ","b4a49098":"#Decision Tree Classifier with criterion gini index\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=50, min_samples_leaf=19)\nclf_gini.fit(X_train, y_train)","d3e55933":"#Decision Tree Classifier with criterion information gain\nclf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth=50, min_samples_leaf=19)\nclf_entropy.fit(X_train, y_train)","9dc46c95":"y_pred_gini = clf_gini.predict(X_test)\ny_pred_entropy = clf_gini.predict(X_test)\nprint(\"Accuracy using Gini Index: \",metrics.accuracy_score(y_test,y_pred_gini))\nprint(\"Confusion matrix using Gini Index: \\n\",confusion_matrix(y_test, y_pred_gini))  \nprint(classification_report(y_test, y_pred_gini)) \n\nprint(\"Accuracy using Information Gain: \",metrics.accuracy_score(y_test,y_pred_entropy))\nprint(\"Confusion matrix using Information gain: \\n\",confusion_matrix(y_test, y_pred_gini))  \nprint(classification_report(y_test, y_pred_entropy)) \n","37263743":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(n_estimators=50,random_state = 100,max_depth=50, min_samples_leaf=5)\nclf.fit(X_train,y_train)\n#y_pred=clf.predict(X_test)\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nfeature_imp = pd.Series(clf.feature_importances_,index=list(X.columns)).sort_values(ascending=False)\nfeature_imp","17bc790c":"sns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","dcbbf2b6":"# X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2)\nX_train = X_train.drop(['Education'], axis=1)\nX_test = X_test.drop(['Education'], axis=1)\n","5995fdfb":"clf=RandomForestClassifier(n_estimators=50, random_state = 100,max_depth=50, min_samples_leaf=5)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","5cb4ab8f":" # Assignment 6  \n ## Turing Machines\n\n**Team members :**  \n\n*Vikram G - 01FB16ECS484  *\n\n*Vinayaka R Kamath - 01FB16ECS445  *\n\n*Nikhil V Revankar - 01FB16ECS230*\n","68090994":"**Compared to other features 'Education' has significantly lower importance score. Hence it is safe to drop this feature before building the model. This will ensure that the model is comaparatively computationally inexpensive.**","af9524b7":"**Conclusion and Analysis:\n**\n\nKNN, Decision Trees and Random Forest Classifiers are compared on the same split of the data.\n\nAccuracy Score Standings:\n        Random Forest > Decision Trees > KNN\n\nAt First glance Random Forest performs better than the other two classifiers with higher precision and higher recall. The accuracy score is larger as well.\n\n*For accuracy*: random forest is almost always better.\n\n*For computational load*: A single decision tree will train much more quickly, and computing a prediction is also much quicker.\n\n*For comprehensibility*: A decision tree is more comprehensible.\n\n\n\n","3725fe70":"### Random Forest Classifier","7b4ad666":"### Decision Tree Classifier","c7a0b259":"**Min Max Scaler is used after an heuristic approach. It was choosen as it exhibited higher accuracy than it's counter parts.**","6f3635a5":"### **KNN Classifier**","95e1162e":"**For choosing the K in the KNN, an iterative approach was practised. The K value and the model  with the lowest error was choosen from 1 - 40 to determine the required output.**"}}