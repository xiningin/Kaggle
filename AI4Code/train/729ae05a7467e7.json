{"cell_type":{"fa59a695":"code","e5bce649":"code","df6c9324":"code","6515d938":"code","103b75d9":"code","ae46a689":"code","a4cc7b23":"code","e526a8dd":"code","2f3aa19e":"code","3c2870ef":"code","e3327e96":"code","efc236cd":"code","daafa967":"code","bc2ceff3":"code","419dce63":"code","3898e656":"code","3bb19a20":"code","16789247":"code","34eff085":"code","94ec983f":"code","748f32e9":"code","c40001cf":"code","7e13a310":"code","ecba8a22":"code","40427156":"code","dbfe3281":"code","dc28f499":"code","7060c7ff":"code","b77a1f01":"code","238c9a4f":"code","f0c85d6e":"code","b1a52c4a":"code","745c256b":"code","69b316a2":"code","644d2a6a":"code","41f3d30a":"markdown","775cc442":"markdown","fa7c14ed":"markdown","bafeec54":"markdown","25004a58":"markdown","4e434b85":"markdown","556bd691":"markdown","4467a722":"markdown","6cf9f3af":"markdown","448ea4d7":"markdown","2aa83265":"markdown","e209d26e":"markdown","1a16865a":"markdown","2d50d472":"markdown","3a6c6cad":"markdown","21e7e615":"markdown","64e7f4d8":"markdown","41bd39a7":"markdown","b2a19317":"markdown","afa9fd70":"markdown","bfd2e55c":"markdown","715e1ea4":"markdown","526dd450":"markdown","873b1aa5":"markdown","bff16d2b":"markdown","172bae14":"markdown","0f2f2fa2":"markdown","e1ba91d4":"markdown","1038fd6e":"markdown","e59ec1d7":"markdown","1d5145dc":"markdown","ae92e5fc":"markdown"},"source":{"fa59a695":"import warnings \nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport folium\n\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.decomposition import PCA\nfrom hmmlearn import hmm","e5bce649":"df = pd.read_csv('..\/input\/us-weather-events\/US_WeatherEvents_2016-2019.csv')\ndf.tail(3)","df6c9324":"datetimeFormat = '%Y-%m-%d %H:%M:%S'\ndf['End']=pd.to_datetime(df['EndTime(UTC)'], format=datetimeFormat)\ndf['Start']=pd.to_datetime(df['StartTime(UTC)'], format=datetimeFormat)\ndf['Duration']=df['End']-df['Start']\ndf['Duration'] = df['Duration'].dt.total_seconds()\ndf['Duration'] = df['Duration']\/(60*60) #in hours\ndf = df[(df['Duration']< 30*24) & (df['Duration'] != 0)] #remove obvious wrong data\ndf.tail(3)","6515d938":"print(\"Overall Duration Summary\")\nprint(\"--Count\", df['Duration'].size)\nprint(\"--%miss.\", sum(df['Duration'].isnull()))\nprint(\"--card.\",df['Duration'].unique().size)\nprint(\"--min\",df['Duration'].min())\nprint(\"--lowerBoundary.\",df['Duration'].median()-1.5*((df['Duration'].quantile(0.75))-df['Duration'].quantile(0.25)))\nprint(\"--1stQrt\",df['Duration'].quantile(0.25))\nprint(\"--mean\",df['Duration'].mean())\nprint(\"--median\",df['Duration'].median())\nprint(\"--3rdQrt\",df['Duration'].quantile(0.75))\nprint(\"--upperBoundary.\",df['Duration'].median()+1.5*((df['Duration'].quantile(0.75))-df['Duration'].quantile(0.25)))\nprint(\"--95%Boundary.\",df['Duration'].mean()+1.96*df['Duration'].std())\nprint(\"--max\",df['Duration'].max())\nprint(\"--Std.Dev\",df['Duration'].std())","103b75d9":"df = df[(df['Duration']< 10)]\ndf['Duration'].size","ae46a689":"df2 = df.groupby(['AirportCode','City','State', \n                  'LocationLat', 'LocationLng','Type']).agg({'Duration':['sum']}).reset_index()\ndf2.columns=pd.MultiIndex.from_tuples(((\"AirportCode\", \" \"),(\"City\", \" \"),\n                                       (\"State\", \" \"), (\"LocationLat\", \" \"),\n                                       (\"LocationLng\", \" \"), (\"Type\", \" \"), (\"Duration\", \" \")))\ndf2.columns = df2.columns.get_level_values(0)\ndf2['Duration'] = df2['Duration']\/(24*4*3.65) #yearly percentage  \ndf2 = df2.sort_values(by='Duration')\ndf2.tail(3)","a4cc7b23":"df_flat = df2.pivot(index='AirportCode', columns='Type', values=['Duration']).reset_index().fillna(0)\ndf_flat.columns=pd.MultiIndex.from_tuples(((' ', 'AirportCode'),(' ', 'Cold'),(' ', 'Fog'),\n            (' ',  'Hail'),(' ', 'Precipitation'),(' ', 'Rain'),(' ', 'Snow'),(' ', 'Storm')))\ndf_flat.columns = df_flat.columns.get_level_values(1)\n#df_flat().tail(3)\nuniqueKey = df2[['AirportCode', 'City', \n                 'State', 'LocationLat', 'LocationLng']].sort_values(by='AirportCode').drop_duplicates()\nweather = pd.merge(df_flat, uniqueKey, how='inner', on='AirportCode')\nweather.tail(3)","e526a8dd":"fig_sum = px.histogram(df2, x='Type', y= 'Duration',  histfunc = 'avg',\n                      title = 'fig 1. National wide weather events duration')\nfig_sum.update_xaxes(categoryorder='mean descending')\nfig_sum.update_yaxes(title_text='mean of duration% per year')\nfig_sum.update_layout(height=750, width=1000)\nfig_sum.show()","2f3aa19e":"fig_state=make_subplots(rows=7, cols=1, shared_yaxes=False, vertical_spacing=0.05)\n\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Rain'], name='Rain', histfunc ='avg'),1,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Fog'], name='Fog', histfunc ='avg'),2,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Snow'], name='Snow', histfunc ='avg'),3,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Cold'], name='Cold', histfunc ='avg'),4,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Precipitation'], name='Precipitation', histfunc ='avg'),5,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Storm'], name='Storm', histfunc ='avg'),6,1)\nfig_state.add_trace(go.Histogram(x=weather['State'], y=weather['Hail'], name='Hail', histfunc ='avg'),7,1)\n\nfig_state['layout']['xaxis7'].update(title=\"State\")\nfig_state['layout']['yaxis4'].update(title=\"duration% per year\")\nfig_state.update_xaxes(categoryorder='mean descending')\nfig_state.update_layout( title_text=\"fig 2. State wide weather events distribution\")\nfig_state.show()","3c2870ef":"fig_city = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      scope=\"usa\",\n                      title ='fig 3. Cities involved in this dataset')\n#fig_city.update_layout(height=750, width=1000)\nfig_city.show()","e3327e96":"fig_rain = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Rain\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 4. City wide rainy days percentage each year from 2016 to 2019')\n#fig_rain.update_layout(height=750, width=1000)\nfig_rain.show()","efc236cd":"fig_fog = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Fog\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      #range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 5. City wide foggy days percentage each year from 2016 to 2019')\n#fig_fog.update_layout(height=750, width=1000)\nfig_fog.show()","daafa967":"fig_snow = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Snow\",\n                      #size=\"Snow\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      #range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 6. City wide snow days percentage each year from 2016 to 2019')\n#fig_snow.update_layout(height=750, width=1000)\nfig_snow.show()","bc2ceff3":"fig_cold = px.scatter_geo(weather, lat='LocationLat', lon='LocationLng',\n                      color=\"Cold\",\n                      #size=\"Snow\",\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      color_continuous_scale='dense', \n                      #range_color = [0,16],\n                      scope=\"usa\",\n                      title ='fig 7. City wide cold days percentage each year from 2016 to 2019')\n#fig_cold.update_layout(height=750, width=1000)\nfig_cold.show()","419dce63":"X = df_flat.drop(['AirportCode','Cold', 'Hail'], axis=1)\nX.tail(3)","3898e656":"from sklearn.cluster import KMeans\ndistortions = []\n\nK = range(1,20)\nfor k in K:\n    kmean = KMeans(n_clusters=k, random_state=0, n_init = 50, max_iter = 500)\n    kmean.fit(X)\n    distortions.append(kmean.inertia_)\n\nfig_kmean = px.scatter(x=K, y=distortions, trendline=distortions, title='fig 8. The Elbow Method')\nfig_kmean.update_xaxes(title_text='k')\nfig_kmean.update_yaxes(title_text='distortion')\n#fig_kmean.update_layout(height=650, width=1000)\nfig_kmean.show()","3bb19a20":"kmeans = KMeans(n_clusters=4, random_state=0).fit(X)\n\ndf_flat['Cluster'] = (kmeans.labels_).astype(str)\ndf_cluster = pd.merge(df_flat[['AirportCode','Cluster']], weather.drop(['Cold','Hail'], axis=1), \n                      how='inner', on='AirportCode')\ndf_cluster.tail(3)","16789247":"fig_cluster = px.scatter_geo(df_cluster, lat='LocationLat', lon='LocationLng',\n                      hover_name=weather['City'] + ', ' + weather['State'],\n                      scope=\"usa\",\n                      color_discrete_sequence =['#AB63FA', '#EF553B', '#00CC96','#636EFA'],\n                      color = 'Cluster',\n                      title ='fig 9. City wide weather cluster distribution')\n#fig_cluster.update_layout(height=750, width=1000)\nfig_cluster.show()","34eff085":"df_cluster2 = df_cluster.groupby(['State','Cluster']).agg({'Cluster':['count']}).reset_index()\ndf_cluster2.columns=pd.MultiIndex.from_tuples(((\"State\", \" \"),(\"Cluster\", \" \"),(\"Count\", \" \")))\ndf_cluster2.columns = df_cluster2.columns.get_level_values(0)\n#df_cluster2.tail(3) #state with each cluster counts\n\ndf_loc = df_cluster[['State','Cluster','LocationLat', 'LocationLng']]\ndf_loc1 = df_loc.groupby(['State','Cluster']).agg({'LocationLat':'mean'}).reset_index()\ndf_loc2 = df_loc.groupby(['State','Cluster']).agg({'LocationLng':'mean'}).reset_index()\ndf_loc3 = pd.merge(df_loc1,df_loc2, how='inner', on=['State','Cluster'])\n#df_loc3.tail(3) #state with cluster and location\n\ndf_clusterS = pd.merge(df_loc3,df_cluster2, how='inner', on=['State','Cluster'])\ndf_clusterS.tail(3) #state with each cluster count location","94ec983f":"fig_clusterS = px.scatter_geo(df_clusterS, lat='LocationLat', lon='LocationLng', \n                     color='Cluster',\n                     size='Count',\n                     color_discrete_sequence=['#636EFA', '#AB63FA', '#EF553B','#00CC96'],\n                     hover_name='State',\n                     scope=\"usa\",\n                     title = 'fig 10. State wide weather cluster distribution')\n#fig_clusterS.update_layout(height=750, width=1000)\nfig_clusterS.show()","748f32e9":"prop = df_cluster[['Cluster', 'Fog',\n                   'Precipitation','Rain', 'Snow', 'Storm']].groupby(['Cluster']).mean().reset_index()\nprop2 = prop.transpose().reset_index()\nprop2 = prop2[(prop2['index'] !='Cluster')].sort_values(by=0)\nprop2","c40001cf":"#from plotly.subplots import make_subplots\nfig_prop=make_subplots(rows=1, cols=4, shared_yaxes=True,horizontal_spacing=0)\n\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[0], name='Cluster 0'), row=1, col=1)\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[1], name='Cluster 1'), row=1, col=2)\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[2], name='Cluster 2'), row=1, col=3)\nfig_prop.add_trace(go.Bar(x=prop2['index'], y=prop2[3], name='Cluster 3'), row=1, col=4)\n\nfig_prop.update_yaxes(title_text=\"duration%\/year\", row=1, col=1)\nfig_prop.update_layout(title_text=\"fig 11. Weather distribution in each cluster\")\n#fig_prop.update_layout(height=550, width=1000)\nfig_prop.show()","7e13a310":"df3 = weather[(weather['City']=='Seattle')| (weather['City']=='Detroit')|(weather['City']== 'Kansas City')|(weather['City']== 'Denver')]\n#df3\ndf4 = df3[['Storm', 'Precipitation','Snow', 'Fog','Rain', 'City']].groupby(['City']).mean().reset_index()\ndf4 = df4.transpose().reset_index()\ndf4.columns = df4.iloc[0]\ndf4 = df4[(df4['City'] !='City')]\ndf4","ecba8a22":"fig_city=make_subplots(rows=1, cols=4, shared_yaxes=True,horizontal_spacing=0)\n\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Kansas City'], name='Cluster0'), row=1, col=1)\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Denver'], name='Cluster1'), row=1, col=2)\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Detroit'], name='Cluster2'), row=1, col=3)\nfig_city.add_trace(go.Bar(x=df4['City'], y=df4['Seattle'], name='Cluster3'), row=1, col=4)\n\nfig_city['layout']['xaxis1'].update(title=\"Kansas City, MO\")\nfig_city['layout']['xaxis2'].update(title=\"Denver, CO\")\nfig_city['layout']['xaxis3'].update(title=\"Detroit, MI\")\nfig_city['layout']['xaxis4'].update(title=\"Seattle, WA\")\nfig_city.update_yaxes(title_text=\"duration%\/year\", row=1, col=1)\nfig_city.update_layout(title_text=\"fig 12. Representative cities in each cluster\")\n#fig_city.update_layout(height=550, width=1000)\nfig_city.show()","40427156":"pca = PCA().fit(X)\npcaX = pca.transform(X)\nc0 = []\nc1 = []\nc2 = []\nc3 = []\n\nfor i in range(len(pcaX)):\n    if kmeans.labels_[i] == 0:\n        c0.append(pcaX[i])\n    if kmeans.labels_[i] == 1:\n        c1.append(pcaX[i])\n    if kmeans.labels_[i] == 2:\n        c2.append(pcaX[i])\n    if kmeans.labels_[i] == 3:\n        c3.append(pcaX[i])\nc0 = np.array(c0)\nc1 = np.array(c1)\nc2 = np.array(c2)\nc3 = np.array(c3)\n\nfig_pca = make_subplots(rows=1, cols=2, column_widths=[0.3, 0.7], specs=[[{'type':'domain'}, {'type': 'mesh3d'}]])\n\nfig_pca.add_trace(go.Pie(values = pca.explained_variance_ratio_), row=1, col=1)\n\nfig_pca.add_trace(go.Scatter3d(x=c0[:,0], y=c0[:,1], z=c0[:,2],\n                               mode='markers', name='Cluster0'),  row=1, col=2)\nfig_pca.add_trace(go.Scatter3d(x=c1[:,0], y=c1[:,1], z=c1[:,2], \n                               mode='markers', name='Cluster1'),  row=1, col=2)\nfig_pca.add_trace(go.Scatter3d(x=c2[:,0], y=c2[:,1], z=c2[:,2], \n                               mode='markers', name='Cluster2'),  row=1, col=2)\nfig_pca.add_trace(go.Scatter3d(x=c3[:,0], y=c3[:,1], z=c3[:,2], \n                               mode='markers', name='Cluster3'),  row=1, col=2)\n\nfig_pca.update_layout(height=750, width=1000, title_text=\n          \"fig 13. PCA: explained variance%(left) and First 3 Component with mapped cluster(right)\")\n\nfig_pca.show()","dbfe3281":"atlanta = df[(df['City']== 'Atlanta')]\nprint(atlanta['LocationLat'].unique())\nprint(atlanta['LocationLng'].unique())","dc28f499":"import folium\nm = folium.Map(location=[33.755238, -84.388115])\nfolium.Marker(\n    location=[33.6301, -84.4418],\n).add_to(m)\nfolium.Marker(\n    location=[33.8784, -84.298],\n).add_to(m)\nfolium.Marker(\n    location=[33.7776, -84.5246],\n).add_to(m)\nm","7060c7ff":"atlanta = atlanta[(atlanta['LocationLng']== -84.4418)]\n\natlanta_m = atlanta.loc[(atlanta['Start'].dt.month==12) | (atlanta['Start'].dt.month==11)| (atlanta['Start'].dt.month==1)]\natlanta_m = atlanta_m.loc[(atlanta_m['Type']!='Hail')&(atlanta_m['Type']!='Precipitation')]\nprint(atlanta_m['Type'].unique())\nprint(atlanta_m['Severity'].unique())\nprint(atlanta_m['Type'].count())","b77a1f01":"X2 = atlanta_m[['Type', 'Severity']]\nX2_test = X2.tail(5)\nX2_train = X2.head(675)\nprint(X2_train.tail())\nprint(X2_test)\nprint(X2['Severity'].describe())","238c9a4f":"Type = {'Rain':0, 'Fog':1, 'Snow':2} \nX2.Type = [Type[item] for item in X2.Type] \n\nSeverity = {'Light':0, 'Moderate':1, 'Severe':2, 'Heavy':2}\nX2.Severity = [Severity[item] for item in X2.Severity] \nX2_test = X2.tail(5)\nX2_train = X2.head(675)\nprint(X2_train.tail())\nprint(X2_test)","f0c85d6e":"# A function that implements the Markov model to forecast the state. [3]\ndef transition_matrix(transitions):\n    n = 1+ max(transitions) #number of states\n\n    M = [[0]*n for _ in range(n)]\n\n    for (i,j) in zip(transitions,transitions[1:]):\n        M[i][j] += 1\n\n    #now convert to probabilities:\n    for row in M:\n        s = sum(row)\n        if s > 0:\n            row[:] = [f\/s for f in row]\n    return M\n\n#test:\nt = list(X2_train.Type)\nm = transition_matrix(t)\nfor row in m: print(' '.join('{0:.2f}'.format(x) for x in row))  ","b1a52c4a":"# The statespace\nstates = [\"Rain\",\"Fog\",\"Snow\"]\n\n# Possible sequences of events\ntransitionName = [[\"RR\",\"RF\",\"RS\"],[\"FR\",\"FF\",\"FS\"],[\"SR\",\"SF\",\"SS\"]]\n\n# Probabilities matrix (transition matrix)\ntransitionMatrix = [[0.89,0.10,0.02],[0.59,0.39,0.02],[0.33,0.07,0.60]]\n\n# A function that implements the Markov model to forecast the state. [4]\ndef activity_forecast(days):\n    # Choose the starting state\n    activityToday = \"Fog\"\n    print(\"Start state: \" + activityToday)\n    # Shall store the sequence of states taken. So, this only has the starting state for now.\n    activityList = [activityToday]\n    i = 0\n    # To calculate the probability of the activityList\n    prob = 1\n    while i != days:\n        if activityToday == \"Rain\":\n            change = np.random.choice(transitionName[0],replace=True,p=transitionMatrix[0])\n            if change == \"RR\":\n                prob = prob * 0.89\n                activityList.append(\"Rain\")\n                pass\n            elif change == \"RF\":\n                prob = prob * 0.1\n                activityToday = \"Fog\"\n                activityList.append(\"Fog\")\n            else:\n                prob = prob * 0.02\n                activityToday = \"Snow\"\n                activityList.append(\"Snow\")\n        elif activityToday == \"Fog\":\n            change = np.random.choice(transitionName[1],replace=True,p=transitionMatrix[1])\n            if change == \"FR\":\n                prob = prob * 0.59\n                activityList.append(\"Rain\")\n                pass\n            elif change == \"FF\":\n                prob = prob * 0.39\n                activityToday = \"Fog\"\n                activityList.append(\"Fog\")\n            else:\n                prob = prob * 0.02\n                activityToday = \"Snow\"\n                activityList.append(\"Snow\")\n        elif activityToday == \"Snow\":\n            change = np.random.choice(transitionName[2],replace=True,p=transitionMatrix[2])\n            if change == \"SR\":\n                prob = prob * 0.33\n                activityList.append(\"Rain\")\n                pass\n            elif change == \"SF\":\n                prob = prob * 0.07\n                activityToday = \"Fog\"\n                activityList.append(\"Fog\")\n            else:\n                prob = prob * 0.6\n                activityToday = \"Snow\"\n                activityList.append(\"Snow\")\n        i += 1  \n    print(\"Possible states: \" + str(activityList))\n    print(\"Probability of the possible sequence of states: \" + str(prob))\n    \n# Function that forecasts the possible state for the next 4 events\nactivity_forecast(4)\nprint('Acutal sequence of states: Rain, Fog, Rain, Rain, Rain')","745c256b":"activity_forecast(4)\nprint('Acutal sequence of states: Rain, Fog, Rain, Rain, Rain')","69b316a2":"from hmmlearn import hmm\nmodel = hmm.MultinomialHMM(n_components=3, n_iter=50, tol=0.001)\nmodel.startprob_=np.array([0.82, 0.132, 0.044])\nmodel.fit(np.array(X2_train))\n#model.fit(X2_train['Type'], X2_train['Severity'])\nprint('startprob', np.round(model.startprob_,3))\nprint('transmat', np.round(model.transmat_,3))\nprint('emissionprob', np.round(model.emissionprob_,3))\nprint('score', model.score(X2))","644d2a6a":"print ('Acutal sequence of states: 0, 2, 0, 0 ,0')\nseen = np.array(X2_test[['Type']])\nmodel.decode(seen, algorithm='viterbi')","41f3d30a":"# forecast Atlanta's weather events","775cc442":"# **K means clustering results**\n\n**city-wide cluster distribution**\n\nThe city-wide results of K-means clustering is shown in figure 9. It seems k=4 is a good number of clusters. And cluster 0 (blue), 2 (green) and 3 (purple) have clear boundary with each other. Cities in cluster 1 (red) widely spread in the United States.","fa7c14ed":"forecast weather using gained transition matrix","bafeec54":"library","25004a58":"# data cleaning","4e434b85":"**representive cities in each cluster**\n\nFour cities were selected in each cluster, Kansas City, MO is the representive city of clsuter 0; Dever, CO is the representive city of cluster 1; Detroit, MI is the representive city of cluster 2; Seattle, MA is the representive city of cluster 3. All the cities weather information are shown in figure 10, their weather properties are similar to their corresponding cluster as shown in figure 12.","556bd691":"few events were in severe and heavy, since HMMlearn cannot handle observation states (rain, fog, etc.) bigger than hidden states (light, moderate, etc), thus combine severe and heavy together.","4467a722":"# ** load dataset**","6cf9f3af":"This project is inspired by the idea of Lucas: Clustering and Visualisation using Folium Maps (https:\/\/www.kaggle.com\/lucaspcarlini\/clustering-and-visualisation-using-folium-maps)","448ea4d7":"# predict weather severity using HMM","2aa83265":"# Forecast weather events using Markov Chain\n\ntransition matrix","e209d26e":"this is a countrywide weather events dataset with more than 5 million events, which covers 49 states of the United States. Examples of weather events are rain, snow, storm, and freezing condition. The data is collected from January 2016 to December 2019, using historical weather reports that exist for airport-based weather stations across the country.\n\nWeather event is a spatiotemporal entity, where such an entity is associated with location and time. Following is the description of available weather event types in this dataset: Cold: The case of having extremely low temperature, with temperature below -23.7 degrees of Celsius.\n\nFog: The case where there is low visibility condition as a result of fog or haze.\n\nHail: The case of having solid precipitation including ice pellets and hail.\n\nRain: The case of having rain, ranging from light to heavy.\n\nSnow: The case of having snow, ranging from light to heavy.\n\nStorm: The extremely windy condition, where the wind speed is at least 60 km\/h.\n\nOther Precipitation: Any other type of precipitation which cannot be assigned to previously described event types.","1a16865a":"# **Fog**\n\nThe city-wide fog distribution is shown in figure 5. Like the distribution of rain, cities with long fog days are distributed at the west coast and east areas (dark blue markers), cities in the middle part of the United States has lower chance of fog (light blue area).","2d50d472":"# **Principle component analysis**\nA way for obtaining a better understanding of the data is to visualize it; however, it is not straightforward to visualize high-dimensional data. This was a problem, for example, we could only choose pairs of weather events and plot them.\n\nPrincipal Component Analysis (PCA) is a technique that is widely used for applications such as dimensionality reduction, visualization and lossy data compression. This project only focused on the visualization aspect. PCA can be de\ufb01ned as the orthogonal linear transformation of the data to a lower dimensional linear space, known as the principal subspace, such that the greatest variance by any projection of the data comes to lie on the \ufb01rst coordinate (called the \ufb01rst principal component), the second greatest variance on the second coordinate, and so on. Intuitively, PCA \ufb01nds a meaningful coordinate basis to express the dataset [2].\n\nIn order to visualize how are the clusters are related in the original high dimension space, PCA was applied to the original dataset. The result of PCA is shown in figure 11, the cumulative variance explained for the first three component is able to cover more than 95% of the original information, therefore, reducing the dimensionality of the dataset (figure 13 left). After mapping the cluster labels to the PCA dataset and visualize it in three dimensions (figure 13 right), it showed that four cluster is a reasonable to identify similar samples within the dataset.","3a6c6cad":"**1\/5 accuracy with 0.02 probability","21e7e615":"# **Exploratory data analysis**\n\n# **National wide weather events duration**\n\nThe national wide weather event duration information was shown in figure 1. Rain lasted around 4.5% period of a year, which is the most durable weather event. Fog and snow lasted around 1.5% time of a year. The duration of cold, precipitation, storm and hail were all below 0.5%.","64e7f4d8":"In this project, the duration of each weather event was the key feature used to cluster regions. It was calculated by using event end time minus start time, any single event that lasted more than 30 days was initially eliminated, then any events outside three standard deviation away from mean were also removed.","41bd39a7":"# **City wide weather events distribution**\n\n# **cities involved**\n\nAll the cities involved in this dataset are marked as blue dot in figure 3. Large amount of cities distribued in west coast and east part of the United States, few of them located in the middle.","b2a19317":"last five events selected as testing dataset ","afa9fd70":"# **Rain**\n\nThe city-wide rain distribution is shown in figure 4. United States cities with long rainy durations are distributed at the west coast and east areas (dark blue markers), cities in the middle part of the region has lower chance of raining (light blue area).","bfd2e55c":"Atlanta has three airports, in this project will focus on the largest airport : Hartsfield-Jackson Atlanta International Airport.\n\nThe last observation of the event was in Dec 2019, different seasons have different weather properties, thus only events from month Nov, Dec and Jan were considered.","715e1ea4":"**state-wide cluster distribution**\n\nTo better visulize each cluster's distribution in each state, clusters are grouped as state and location was calculated as the average of each cities latidude and logitude in that cluster. The state wide cluster distribution is shown in figure 10. Almost all states have mixtured types of clusters.","526dd450":"# Check events distribution","873b1aa5":"# **Cold**\n\nCold is defined as the case of having extremely low temperature, with temperature below -23.7 degrees of Celsius. The city-wide cold distribution is shown in figure 7. Cities in the north part of the United States have higher chance of cold, it is consistent with our common sense. However, there are large numbers of cities with high chance of cold distributed in the south east area, which is counter intuitive, thus the weather event cold may not be a good feature to cluster regions, and it was removed for the followed analysis.","bff16d2b":"# Reference\n\n[1] https:\/\/www.oreilly.com\/library\/view\/statistics-for machine\/9781788295758\/c71ea970-0f3c-4973-8d3a-b09a7a6553c1.xhtml\n\n[2] https:\/\/setosa.io\/ev\/principal-component-analysis\/\n\n[3] https:\/\/stackoverflow.com\/questions\/46657221\/generating-markov-transition-matrix-in-python\n\n[4] https:\/\/www.datacamp.com\/community\/tutorials\/markov-chains-python-tutorial","172bae14":"4\/5 accuracy with 0.08 probability","0f2f2fa2":"**property of each cluster**\n\nThe property of each cluster is summarized in figure 11. Compare to other clusters, cluster 3 (purple) has highest chance of rain, cluster 2 (green) has highest chance of snow, cluster 1 (red) has lowest chance of rain while highest chance of storm; cluster 0 (blue) is similar to cluster 3 while its\u2019 chance of rain is slightly lower than cluster 3.","e1ba91d4":"# **Snow**\n\nThe city-wide snow distribution is shown in figure 6. Cities in the north and middle part of the United States have higher chance of snow (dark blue markers), cities in the south east area have lower chance of snow (light blue area).","1038fd6e":"# **K means clustering**\n **algorithm**\n\nK-means algorithm is one of the most employed clustering algorithms. K-means aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean. The algorithm proceeds as follows:\n\nPick K random points as cluster center positions.\n\nAssign each point to the nearest center.\n\nRecompute each cluster mean as the mean of the vectors assigned to that cluster.\n\nIf centers moved, go to step 2.\n\n**Distance**\n\nThe algorithm requires a distance measure to be de\ufb01ned in the data space, and the Euclidean distance is often used. For example, the Euclidean distance between u = (u 1 , u 2 ) and v = (v 1 , v 2 ) is calculated by the following expression:\n\nr(u, v) = \u221a\uff08(u1-v1)2+(u2-v2)2\n\n**Pick K random points as cluster center position**\n\nThe elbow method is used to determine the optimal number of clusters. It plots the value of the cost function produced by different values of k. When k increases, the average distortion will increase, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters [1].\n\nFrom figure 8, the elbow happened when k = 3 or 4. Thus I will pick 4 as the number of clusters in this dataset.","e59ec1d7":"4\/5 accuracy with log(-4.6)=0.66 probability","1d5145dc":"**data normalization**\nFor better comparison of each weather events, all the duration time of them were normalized to the range 0 to 100, zero means that event never happened in the year and 100 means that event happened all the time during the year.","ae92e5fc":"# **State wide weather events distribution**\n\nThe statewide weather event duration information was shown in figure 2.\n\nThe top three states with long rainy days are OR, DE, MA; The top three states with long foggy days are NH, WA, CA; The top three states with long snow days are MI, VT, MT; The top three states with long cold days are ND, MN, NJ; The top three states with long precipitation days are NY, FL, LA; The top three states with long storm days are NH, CO, WY; The top three states with long hail days are CT, NY, DE.\n\nThe top three states with short rainy days are NV, UT, AZ; The top three states with short foggy days are AZ, NV, NM; The top three states with short snow days are FL, LA, TX; The top three states with short cold days are MO, WA, CT; The top three states with short precipitation days are UT, NV, CO; The top three states with short storm days are AL, GA, KY; The top three states with short hail days are FL, AZ, GA.\n\nAll of these information matched our common sense."}}