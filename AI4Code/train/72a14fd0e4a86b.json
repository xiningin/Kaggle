{"cell_type":{"5011b23a":"code","3102c0ed":"code","4df216b2":"code","62dfc299":"code","5c94ffac":"code","05703cc6":"code","29c685f9":"code","ceaba173":"code","fa8ad341":"code","28310343":"code","a45115a6":"code","8a41d280":"code","d96fe4e2":"code","573c968d":"code","884852f2":"code","8219bf98":"code","5b35d1b7":"code","2ee4f722":"code","adf8bc8a":"code","f41c03f4":"code","cea02372":"code","0adf3819":"code","d461e099":"code","a1a3c613":"code","bcef4beb":"code","0e9f2fb2":"code","ec2d0be4":"code","62ac9cd3":"code","0f39b2be":"code","98c549ff":"code","8be1741a":"code","44e1f9cb":"code","7744175f":"markdown","a7a16e1e":"markdown","5c38d63a":"markdown","3fd79ed0":"markdown","741b7df3":"markdown","f75985f2":"markdown","f2ea26a8":"markdown","0af90a82":"markdown","06494563":"markdown","fbadd25b":"markdown","227b888f":"markdown","4b580ae9":"markdown","7d0ca569":"markdown","c45d41dd":"markdown","f0ebaa52":"markdown","ad69f90b":"markdown","1bc51e28":"markdown","1cce09df":"markdown","4962be7a":"markdown","8d20b7e0":"markdown","51bae6ca":"markdown","440ad467":"markdown","3e0b6c77":"markdown","f4ae612f":"markdown","d8ee345e":"markdown","b0997ffb":"markdown","257366f6":"markdown","982b8e8e":"markdown","52fdfd01":"markdown","69dbe2ba":"markdown"},"source":{"5011b23a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\ndf=pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')","3102c0ed":"df.head()","4df216b2":"df.describe()","62dfc299":"df.shape","5c94ffac":"df.dropna()","05703cc6":"colnames_numerics_only = df.select_dtypes(include=np.number).columns.tolist()\ncolnames_numerics_only","29c685f9":"plt.figure(1,figsize=(15,6))\nn=0\nfor x in colnames_numerics_only:\n    n+=1\n    plt.subplot(1,4,n)\n    plt.subplots_adjust(hspace=0.5,wspace=0.5)\n    sns.distplot(df[x],bins=20)\n    plt.title('Distplot of {}'.format(x))\nplt.show()","ceaba173":"plt.figure(1,figsize=(15,10))\nn=0\nfor x in colnames_numerics_only:\n    for y in colnames_numerics_only:\n        n+=1\n        plt.subplot(4,4,n)\n        plt.subplots_adjust(hspace=0.5,wspace=0.5)\n        sns.regplot(x=x,y=y,data=df)\n        plt.ylabel(y.split()[0]+''+y.split()[1] if len(y.split())>1 else y)\nplt.show()","fa8ad341":"plt.figure(figsize=(10,8))\nplt.scatter(df[\"Spending Score (1-100)\"], df[\"Annual Income (k$)\"])","28310343":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=df, x=\"Spending Score (1-100)\", y=\"Annual Income (k$)\", hue=\"Gender\")","a45115a6":"plt.figure(figsize=(10,8))\nplt.scatter(df[\"Spending Score (1-100)\"], df[\"Age\"])","8a41d280":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=df, x=\"Spending Score (1-100)\", y=\"Age\", hue=\"Gender\")","d96fe4e2":"plt.figure(figsize=(7,5))\nax = sns.boxplot(x=\"Gender\", y=\"Spending Score (1-100)\", data=df)","573c968d":"df_income_score = df.iloc[:, [False, False, False, True, True]].values","884852f2":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_income_score_scaled=scaler.fit_transform(df_income_score)","8219bf98":"df_income_score_scaled","5b35d1b7":"distortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(df_income_score_scaled)\n    distortions.append(kmeanModel.inertia_)\n    \nplt.figure(figsize=(10,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","2ee4f722":"s_scores = []\nclusters = [2,3,4,5,6,7,8,9,10]\nclusters_inertia = []\n\nfor n in clusters:\n    KM_est = KMeans(n_clusters=n, init='k-means++').fit(df_income_score_scaled)\n    clusters_inertia.append(KM_est.inertia_)   \n    silhouette_avg = silhouette_score(df_income_score_scaled, KM_est.labels_)\n    s_scores.append(silhouette_avg)\n    \n    \nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(clusters, s_scores, marker='o', ax=ax)\n\nax.set_xlabel(\"Number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nplt.title('The Silhouette Method showing the optimal k')\nplt.grid()\nplt.show()","adf8bc8a":"from sklearn.mixture import GaussianMixture\n\ngm_bic= []\ngm_score=[]\nfor i in range(2,12):\n    gm = GaussianMixture(n_components=i,n_init=10,tol=1e-3,max_iter=1000).fit(df_income_score_scaled)\n    gm_bic.append(-gm.bic(df_income_score_scaled))\n    gm_score.append(gm.score(df_income_score_scaled))\n    \nplt.figure(figsize=(7,4))\nplt.title(\"The Gaussian Mixture model BIC\",fontsize=16)\nplt.scatter(x=[i for i in range(2,12)],y=np.log(gm_bic),s=150,edgecolor='k')\nplt.grid(True)\nplt.xlabel(\"Number of clusters\",fontsize=14)\nplt.ylabel(\"Log of Gaussian mixture BIC score\",fontsize=15)\nplt.xticks([i for i in range(2,12)],fontsize=14)\nplt.yticks(fontsize=15)\nplt.show()","f41c03f4":"kmeanModel = KMeans(n_clusters=5,init='k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeans= kmeanModel.fit_predict(df_income_score)\nplt.figure(figsize=(8,8))\nplt.scatter(df_income_score[y_kmeans == 0, 0], df_income_score[y_kmeans == 0, 1], s = 100, c = 'g', label = 'Cluster 1')\nplt.scatter(df_income_score[y_kmeans == 1, 0], df_income_score[y_kmeans == 1, 1], s = 100, c = 'b', label = 'Cluster 2')\nplt.scatter(df_income_score[y_kmeans == 2, 0], df_income_score[y_kmeans == 2, 1], s = 100, c = 'r', label = 'Cluster 3')\nplt.scatter(df_income_score[y_kmeans == 3, 0], df_income_score[y_kmeans == 3, 1], s = 100, c = 'burlywood', label = 'Cluster 4')\nplt.scatter(df_income_score[y_kmeans == 4, 0], df_income_score[y_kmeans == 4, 1], s = 100, c = 'green', label = 'Cluster 5')\nplt.scatter(kmeanModel.cluster_centers_[:, 0], kmeanModel.cluster_centers_[:, 1], s = 200, c = 'black', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","cea02372":"df_age_score = df.iloc[:, [False, False, True, False, True]].values\n\nscaler = MinMaxScaler()\ndf_age_score_scaled=scaler.fit_transform(df_age_score)","0adf3819":"distortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(df_age_score_scaled)\n    distortions.append(kmeanModel.inertia_)\n    \nplt.figure(figsize=(10,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","d461e099":"s_scores = []\nclusters = [2,3,4,5,6,7,8,9,10]\nclusters_inertia = []\n\nfor n in clusters:\n    KM_est = KMeans(n_clusters=n, init='k-means++').fit(df_age_score_scaled)\n    clusters_inertia.append(KM_est.inertia_)   \n    silhouette_avg = silhouette_score(df_age_score_scaled, KM_est.labels_)\n    s_scores.append(silhouette_avg)\n    \n    \nfig, ax = plt.subplots(figsize=(12,5))\nax = sns.lineplot(clusters, s_scores, marker='o', ax=ax)\n\nax.set_xlabel(\"Number of clusters\")\nax.set_ylabel(\"Silhouette score\")\nplt.title('The Silhouette Method showing the optimal k')\nplt.grid()\nplt.show()","a1a3c613":"kmeanModelAge = KMeans(n_clusters=6,init='k-means++',max_iter=300,n_init=10,random_state=0)\ny_kmeansAge= kmeanModelAge.fit_predict(df_age_score)\nplt.figure(figsize=(8,8))\nplt.scatter(df_age_score[y_kmeansAge == 0, 0], df_age_score[y_kmeansAge == 0, 1], s = 100, c = 'g', label = 'Cluster 1')\nplt.scatter(df_age_score[y_kmeansAge == 1, 0], df_age_score[y_kmeansAge == 1, 1], s = 100, c = 'b', label = 'Cluster 2')\nplt.scatter(df_age_score[y_kmeansAge == 2, 0], df_age_score[y_kmeansAge == 2, 1], s = 100, c = 'grey', label = 'Cluster 3')\nplt.scatter(df_age_score[y_kmeansAge == 3, 0], df_age_score[y_kmeansAge == 3, 1], s = 100, c = 'burlywood', label = 'Cluster 4')\nplt.scatter(df_age_score[y_kmeansAge == 4, 0], df_age_score[y_kmeansAge == 4, 1], s = 100, c = 'green', label = 'Cluster 5')\nplt.scatter(df_age_score[y_kmeansAge == 5, 0], df_age_score[y_kmeansAge == 5, 1], s = 100, c = 'red', label = 'Cluster 6')\nplt.scatter(kmeanModelAge.cluster_centers_[:, 0], kmeanModelAge.cluster_centers_[:, 1], s = 200, c = 'black', label = 'Centroids')\nplt.title('Clusters of customers')\nplt.xlabel('Age')\nplt.ylabel('Spending Score (1-100)')\nplt.legend()\nplt.show()","bcef4beb":"df_income_score = df.iloc[:, [False, False, False, True, True]]\ndf_norm = scaler.fit_transform(df_income_score)","0e9f2fb2":"df_income_score","ec2d0be4":"from sklearn.cluster import DBSCAN\n\n\nDBS_clustering = DBSCAN(eps=0.09, min_samples=5).fit(df_norm)\nDBSCAN_clustered = df_norm.copy()\nlabels = DBS_clustering.labels_\nlabels","62ac9cd3":"n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\nn_clusters_","0f39b2be":"\ncolors = ['g','r','b','y','burlywood','green', 'm', 'c']\nplt.figure(figsize=(8,8))\nfor i in range(0 ,n_clusters_ - 1):\n    plt.scatter(df_norm[labels == i, 0], df_norm[labels == i, 1], s = 100, c = colors[i], label = 'Cluster ' + str(i + 1))\nplt.scatter(df_norm[labels == -1, 0], df_norm[labels == -1, 1], s = 50, c = 'black', label = 'Outliers')    \nplt.legend()","98c549ff":"df_age_score = df.iloc[:, [False, False, True, False, True]]\ndf_norm_age = scaler.fit_transform(df_age_score)\n\nDBS_clustering = DBSCAN(eps=0.08, min_samples=5).fit(df_norm_age)\nDBSCAN_clustered = df_norm_age.copy()\nlabels = DBS_clustering.labels_\nlabels","8be1741a":"n_clusters_age = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\nn_clusters_age","44e1f9cb":"colors = ['g','r','b','y','burlywood','green', 'm', 'c']\nplt.figure(figsize=(8,8))\nfor i in range(0 ,n_clusters_age - 1):\n    plt.scatter(df_norm_age[labels == i, 0], df_norm_age[labels == i, 1], s = 100, c = colors[i], label = 'Cluster ' + str(i + 1))\nplt.scatter(df_norm_age[labels == -1, 0], df_norm_age[labels == -1, 1], s = 50, c = 'black', label = 'Outliers')    \nplt.legend()","7744175f":"<a id=\"subsection-three-two\"><\/a>\n## Spending score and age","a7a16e1e":"There are two parameters we need to set:\n* Eps, \u03b5 - distance,radius around each point\n\nThe higher eps is, more elements will be included in the particular group and the less density of this group will be.\n\n* MinPts \u2013 minimum number of data points that should be around that point within that radius\n\nThe more minPts is,the more outliers potentially could be, the more detached points will be exluded from clusters.\n\nIt should be said, selecting parameters isn't easy, it needs time and some iterations. The reason is that parameters are unique for each dataset and particular task. ","5c38d63a":"<a id=\"subsection-two-one\"><\/a>\n## Spending score and annual income","3fd79ed0":"Here again optimal value (peak) is 5. Great! Going straight to K-means model...","741b7df3":"\n\nElbow method helps to select the optimal number of clusters by fitting the model with a range of values for K.If the line chart resembles an arm, then the \u201celbow\u201d (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. \n","f75985f2":"Elbow method shows 5 is optimal. But what about silhouette method? By the way, the Elbow Method and the Silhouette Method are not like alternatives to each other for finding the optimal amount of clusters. Rather they are instruments for using together for a more confident decision.\n\nThe silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation). It's values aer within [-1;1]. Optimal value is a peak.","f2ea26a8":"Customer segmentation (or market segmentation) is the process of dividing customers into groups based on common characteristics so companies can market to each group effectively and appropriately. All customers share the common need of your product or service, but beyond that, there are distinct demographic differences (i.e., age, gender) and they tend to have additional socio-economic, lifestyle, or other behavioral differences that can be useful to the organization.\nIn this notebook we're going to split customers into segments according to their age and income.\n\nLet's go!","0af90a82":"* [Data overview and preparation](#section-one)\n* [EDA](#section-two)\n    - [Spending score and Annual income](#subsection-one)\n    - [Spending score and Age](#subsection-two)\n* [Customers segmentation with K-means](#section-three)\n    - [Spending score and Annual income](#subsection-two-one)\n    - [Spending score and Age](#subsection-two-two)\n* [Customers segmentation with DBCSAN](#section-four)\n    - [Spending score and Annual income](#subsection-three-one)\n    - [Spending score and Age](#subsection-three-two)\n* [Results](#section-five)","06494563":"Groups could be described as: \n* Blue - low annual income and high spending score (careless)\n* Red - high  income and high spending score (target)\n* Light green - medium indome and medium spending score (standart)\n* Dark green - high icome and low spending rate (careful)\n* Beige - low income and low spending score (sensible)","fbadd25b":"Finally, let's take a look at Bayesian information criterion (BIC)","227b888f":"<a id=\"section-two\"><\/a>\n# EDA","4b580ae9":"First of all, quick data overview for better understanding....","7d0ca569":"<a id=\"subsection-two\"><\/a>\n## Spending score and Age","c45d41dd":"<a id=\"section-one\"><\/a>\n# Data overview and preparation","f0ebaa52":"# Customer segmentation basics ","ad69f90b":"<a id=\"section-five\"><\/a>\n# Results","1bc51e28":"![pic](https:\/\/acquire.io\/wp-content\/uploads\/2016\/09\/25-Awesome-Customer-Service-Tips-You-Must-Employ-Updated%E2%80%A9.png)","1cce09df":"<a id=\"subsection-three-one\"><\/a>\n## Spending score and annual income","4962be7a":"<a id=\"subsection-two-two\"><\/a>\n## Spending and age","8d20b7e0":"Firstly, taking Spending score and Annual income.\nLooking at the scatter plot, it seems like 5 groups  of customes. However, we should prove it. There are two most used methods to determine optimal clusters amount - Elbow method and Silhouette method. BIC is also used in some cases.  Let's look at all of them!","51bae6ca":"<a id=\"section-three\"><\/a>\n# Customers segmentation with K-means","440ad467":"Here we have 5 clusters that look different from what we had using K-means. In terms of customer segmentation and marketing strategies, black outliers here should rather be interpeted as actual customers, but this is how the algorithm works ;-)","3e0b6c77":"All in all, we've successfuly found several groups that show the spending score of customers depending on their age or annual income.These groups could be applied in marketing in order to optimize the companies of attraction and retention as well as in strategic management and other business areas. Having the results of two algorithms it looks like K-means performs better for this need than  DBSCAN in this particular task. However, this theory could be proven only after application of our results and testing.  \n\n","f4ae612f":"<a id=\"subsection-one\"><\/a>\n## Spending score and Annual income","d8ee345e":"Groups could be described as:\n\n* Blue - middle age and medium spendings\n* Red - young and low spending\n* Light green - young and medium spending score\n* Dark green - the elderly with low spendings\n* Beige - the elderly with medium spendings \n* Grey - young that spend a lot (seems to be target group)","b0997ffb":"Here there are 6 clusters with some black outliers. Well, the result also differs from K-means one. However, cluster 1 and 4 look reasonable and logic. Probably, it'd be better to unite some of the clusters 2,3,5,6 because they look to small to represent whole category of customers. ","257366f6":"Density-Based Clustering refers to unsupervised learning methods that identify distinctive groups\/clusters in the data, based on the idea that a cluster in data space is a contiguous region of high point density, separated from other such clusters by contiguous regions of low point density. Simply,the  main idea of DBSCAN algorithm is to locate regions of high density that are separated from one another by regions of low density.","982b8e8e":"Optimal value here is 6. Let's try!","52fdfd01":"<a id=\"section-four\"><\/a>\n# Customer segmentation with DBCSAN ","69dbe2ba":"Here I've tried range of eps but selected 0.09 which gives adequate number of clusters. By the way, default value is 0.5 but here we have poinnts with much more higher density.\nAs for the min_samples, it seems logical to use approximately 10 or more as totally there are 200 customers in the dataset, however it leads to huge amount of poinnts considered as outliers and unadequate result. I decreased it slightly and decided to use the value of 5. "}}