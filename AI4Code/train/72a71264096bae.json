{"cell_type":{"90e82c51":"code","45c242f2":"code","8923de1e":"code","3353bd5f":"code","098e9c70":"code","0f4af4f6":"code","61f85ff2":"code","7c4d9fa1":"code","61202b83":"code","64bcc4b0":"code","0816c408":"code","804346ee":"code","9f34d53d":"code","ae780646":"code","684ce60a":"code","9be14767":"code","0161f907":"code","8bbec352":"markdown","392579e4":"markdown","38bc5653":"markdown","c3dc87f1":"markdown","49655e0a":"markdown","83afe81c":"markdown","cb70e81e":"markdown","1d8094db":"markdown","78896ec9":"markdown","10270294":"markdown","2d34a8b1":"markdown","95b48c5a":"markdown","ad772009":"markdown","b4f70529":"markdown","f47edecf":"markdown"},"source":{"90e82c51":"%matplotlib inline\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns\nimport skimage\nfrom skimage import io, transform\nfrom sklearn.metrics import confusion_matrix\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, models, transforms","45c242f2":"EPOCHS = 30\ndata_dir = \"..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\"\nTEST = 'test'\nTRAIN = 'train'\nVAL ='val'","8923de1e":"def data_transforms(phase):\n    if phase == TRAIN:\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n        \n    if phase == VAL:\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n    \n    if phase == TEST:\n        transform = transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])        \n        \n    return transform\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","3353bd5f":"image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms(x)) \n                  for x in [TRAIN, VAL, TEST]}\n\ndataloaders = {TRAIN: torch.utils.data.DataLoader(image_datasets[TRAIN], batch_size = 4, shuffle=True), \n               VAL: torch.utils.data.DataLoader(image_datasets[VAL], batch_size = 1, shuffle=True), \n               TEST: torch.utils.data.DataLoader(image_datasets[TEST], batch_size = 1, shuffle=True)}","098e9c70":"len(dataloaders[TRAIN])","0f4af4f6":"dataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL]}\nclasses = image_datasets[TRAIN].classes\nclass_names = image_datasets[TRAIN].classes","61f85ff2":"def imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  \n\n\ninputs, classes = next(iter(dataloaders[TRAIN]))\nout = torchvision.utils.make_grid(inputs)\nimshow(out, title=[class_names[x] for x in classes])","7c4d9fa1":"inputs, classes = next(iter(dataloaders[TRAIN]))","61202b83":"def train_model(model, criterion, optimizer, scheduler, num_epochs):\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(\"Epoch: {}\/{}\".format(epoch+1, num_epochs))\n        print(\"=\"*10)\n        \n        for phase in [TRAIN, VAL]:\n            if phase == TRAIN:\n                scheduler.step()\n                model.train()\n            else:\n                model.eval()\n            running_loss = 0.0\n            running_corrects = 0\n            for data in dataloaders[phase]:\n                inputs, labels = data\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase==TRAIN):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n    print('Best val Acc: {:4f}'.format(best_acc))\n    model.load_state_dict(best_model_wts)\n    return model","64bcc4b0":"model_pre = models.vgg16()\nmodel_pre.load_state_dict(torch.load(\"..\/input\/pytorch-pretrained-models\/vgg16-397923af.pth\"))","0816c408":"for param in model_pre.features.parameters():\n    param.required_grad = False\n\nnum_features = model_pre.classifier[6].in_features\nfeatures = list(model_pre.classifier.children())[:-1] \nfeatures.extend([nn.Linear(num_features, len(class_names))])\nmodel_pre.classifier = nn.Sequential(*features) \nprint(model_pre)","804346ee":"model_pre = model_pre.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model_pre.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)\n# Decay LR by a factor of 0.1 every 10 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","9f34d53d":"model_pre = train_model(model_pre, criterion, optimizer, exp_lr_scheduler, num_epochs=EPOCHS)","ae780646":"def test_model():\n    running_correct = 0.0\n    running_total = 0.0\n    true_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data in dataloaders[TEST]:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.append(labels.item())\n            outputs = model_pre(inputs)\n            _, preds = torch.max(outputs.data, 1)\n            pred_labels.append(preds.item())\n            running_total += labels.size(0)\n            running_correct += (preds == labels).sum().item()\n        acc = running_correct\/running_total\n    return (true_labels, pred_labels, running_correct, running_total, acc)","684ce60a":"true_labels, pred_labels, running_correct, running_total, acc = test_model()","9be14767":"print(\"Total Correct: {}, Total Test Images: {}\".format(running_correct, running_total))\nprint(\"Test Accuracy: \", acc)","0161f907":"cm = confusion_matrix(true_labels, pred_labels)\ntn, fp, fn, tp = cm.ravel()\nax = sns.heatmap(cm, annot=True, fmt=\"d\")","8bbec352":"# Visualize the Chest X-rays","392579e4":"# Train Phase","38bc5653":"# Load Dataset\nThe dataset respective to already classified category is divided into three sets:\n* test set\n* train set\n* validation set\n","c3dc87f1":"# INTRODUCTION\n> The traditional supervised learning paradigm breaks down when we do not have sufficient labeled data for the task or domain we care about to train a reliable model.\n> For example, if we want to train a model to detect pedestrians on night-time images, we could apply a model that has been trained on a similar domain, e.g. on day-time images. In practice, however, we often experience a deterioration or collapse in performance as the model has inherited the bias of its training data and does not know how to generalize to the new domain.\n> Transfer learning allows us to deal with these scenarios by leveraging the already existing labeled data of some related task or domain. \n\n> Here we will see one application of transfer learning on detecting pneumonia using chest x-rays.","49655e0a":"# *Please upvote the kernel if you find it insightful!*\n","83afe81c":"# Testing Phase","cb70e81e":"**Confusion Matrix, Presision and Recall**","1d8094db":"# Define Function for Training","78896ec9":"# Import Libraries","10270294":"# Define the Hyperparameters","2d34a8b1":"# Define Function for Testing","95b48c5a":"# Data Preprocessing and Augmentation\n> Deep learning models usually require a lot of data for training. In general, the more the data, the better the performance of the model.\n\n> Image Augmentation is the process of generating new images for training our deep learning model. These new images are generated using the existing training images and hence we don\u2019t have to collect them manually.","ad772009":"# Load the pretrained model from Pytorch","b4f70529":"# Results","f47edecf":"# VGG16  \n\n![VGG16 Architecture](https:\/\/packt-type-cloud.s3.amazonaws.com\/uploads\/sites\/3149\/2018\/11\/61d69a2a-f31c-477b-9197-b1764c2658b1.png)\n\n**Figure 1:** VGG16 architecture (from https:\/\/hub.packtpub.com\/how-to-leverage-transfer-learning-using-pretrained-cnn-models-tutorial\/)\n\n> ImageNet is a research project to develop a large database of images with annotations e.g. images and their labels.\nPretrained models like VGG-16 and VGG-19 are already trained on ImageNet which comprises of disparate categories of images. These models are built from scratch and trained by using high GPU\u2019s over millions of images consisting of thousands of image categories. \n\n> As the model is trained on huge dataset, it has learned a good representation of low level features like spatial, edges, rotation, lighting, shapes and these features can be shared across to enable the knowledge transfer and act as a feature extractor for new images in different computer vision problems. These new images might be of completely different categories from the source dataset, but the pretrained model should still be able to extract relevant features from these images based on the principles of transfer learning."}}