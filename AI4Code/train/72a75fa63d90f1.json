{"cell_type":{"be5b7ef4":"code","c0af2796":"code","235722f7":"code","ed5b209a":"code","221ba602":"code","8f8dd4fc":"code","6ac19097":"code","e7555968":"code","e7d2c297":"code","a5225d91":"code","0c6ca4ee":"code","da9980fc":"code","db80930e":"code","3a194d1d":"code","f3376af1":"code","e0a4aef6":"code","b95fd37e":"code","d86d8006":"code","e179ac63":"code","28e0d3a8":"code","2ce32422":"code","d8c8fc99":"code","0c283377":"code","67e027b0":"code","4c082cf5":"code","20a8dcad":"code","3c9b3dcc":"code","77d5fd8a":"code","f87c1dcf":"code","e4e90da0":"code","f9f49e68":"code","00e211f7":"code","c063b033":"code","9d53d2d3":"code","a66a617b":"code","cfe269de":"code","d1ce6a87":"code","150f842a":"code","a9ba574f":"code","c1448948":"code","5ef8d7ac":"code","6e664688":"code","968736cb":"code","80846b60":"code","f7ea22dd":"code","d2122f46":"code","214636c9":"markdown","de69034a":"markdown","1e99e5c2":"markdown","97bdee9a":"markdown","2d62917d":"markdown","3bdf0cda":"markdown","44e4f944":"markdown","2453f84f":"markdown","8da60f7b":"markdown","9688e0f4":"markdown","e9844182":"markdown","2da37fbc":"markdown","374bf150":"markdown","376e4ee1":"markdown","8743ffd8":"markdown","88a65bb9":"markdown","41bf06d7":"markdown","81c444ad":"markdown","e560ff50":"markdown","40935215":"markdown","7a70254f":"markdown","3cfc7321":"markdown","5cb6eded":"markdown","f3049aca":"markdown","e1336c84":"markdown","b32cc8f1":"markdown","25f20f0e":"markdown"},"source":{"be5b7ef4":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set_palette(sns.color_palette(\"viridis\"))","c0af2796":"train_df = pd.read_csv('\/kaggle\/input\/banking-dataset-marketing-targets\/train.csv', sep=';')\nprint(f\"Dataset shape - {train_df.shape}\")\ntrain_df.head()","235722f7":"train_df = train_df.rename(columns={'y': 'target'})\ntrain_df","ed5b209a":"df = train_df.copy()\ndf","221ba602":"binary_columns = []\nfor column in df.select_dtypes('object').columns:\n    if len(df[column].unique()) == 2:\n        binary_columns.append(column)\n    print(f\"Column - {column} \",df[column].unique(), end='\\n\\n')","8f8dd4fc":"for column in binary_columns:\n    df[column] = df[column].map({'yes': 1, 'no': 0})\n    \ndf.loc[:, binary_columns]","6ac19097":"plt.figure(figsize=(8,6))\nsns.histplot(data=df, x=\"target\", hue=\"target\", multiple=\"dodge\",binwidth=1.3)\nplt.show()","e7555968":"plt.figure(figsize=(8,6))\nsns.countplot(x = 'education',\n              data = df,\n              order = df['education'].value_counts().index)\nplt.title('Education')\nplt.show()","e7d2c297":"education_target = df.groupby('education', as_index=False)['target']\neducation_target_mean = education_target.mean().sort_values(by='target', ascending=False)\neducation_target_sum = education_target.sum().sort_values(by='target', ascending=False)","a5225d91":"plt.figure(figsize=(16,10))\nplt.subplot(1,3,1)\nsns.histplot(data=df, x=\"education\", hue=\"target\", multiple=\"dodge\",binwidth=1.1,shrink=.8)\nplt.title('Distribution of target value by education')\n\nplt.subplot(1,3,2)\nsns.barplot(data=education_target_mean, x='education', y='target')\nplt.xlabel(\"education\")\nplt.ylabel(\"Mean target\")\nplt.title(\"Mean target value by education\")\n\nplt.subplot(1,3,3)\nsns.barplot(data=education_target_sum, x='education', y='target')\nplt.xlabel(\"Education\")\nplt.ylabel(\"Sum target\")\nplt.title(\"Sum target value by education\")\n\nplt.show()","0c6ca4ee":"education_group = df.groupby('education', as_index=False)\neducation_loan_mean = education_group['loan'].mean().sort_values(by='loan', ascending=False)\neducation_loan_sum = education_group['loan'].sum().sort_values(by='loan', ascending=False)\n\neducation_default_mean = education_group['default'].mean().sort_values(by='default', ascending=False)\neducation_default_sum = education_group['default'].sum().sort_values(by='default', ascending=False)\n\neducation_loan_mean","da9980fc":"plt.figure(figsize=(16,12))\nplt.subplot(3,2,1)\n\nsns.histplot(data=df, x=\"education\", hue=\"loan\", multiple=\"dodge\",binwidth=1.1,shrink=.8)\nplt.title('Distribution of loan by education')\n\nplt.subplot(3,2,2)\nsns.histplot(data=df, x=\"education\", hue=\"default\", multiple=\"dodge\",binwidth=1.1,shrink=.8)\nplt.title('Distribution of default by education')\n\nplt.subplot(3,2,3)\nsns.barplot(data=education_loan_mean, x='education', y='loan',)\nplt.xlabel(\"education\")\nplt.ylabel(\"Mean loan\")\nplt.title(\"Mean loan by education\")\n\nplt.subplot(3,2,4)\nsns.barplot(data=education_loan_sum, x='education', y='loan')\nplt.xlabel(\"education\")\nplt.ylabel(\"Sum loan\")\nplt.title(\"Sum loan by education\")\n\nplt.subplot(3,2,5)\nsns.barplot(data=education_default_mean, x='education', y='default')\nplt.xlabel(\"education\")\nplt.ylabel(\"Mean default\")\nplt.title(\"Mean default by education\")\n\nplt.subplot(3,2,6)\nsns.barplot(data=education_default_sum, x='education', y='default')\nplt.xlabel(\"education\")\nplt.ylabel(\"Sum default\")\nplt.title(\"Sum default by education\")\nplt.tight_layout()\nplt.show()","db80930e":"plt.figure(figsize=(16,6))\nsns.countplot(x = 'job',\n              data = df,\n              order = df['job'].value_counts().index, palette=sns.color_palette('viridis'))\nplt.title('job')\nplt.show()","3a194d1d":"sns.set(rc={'figure.figsize':(16,18)})\nx,y = 'job', 'target'\n\ndf1 = df.groupby(x)[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\ng= sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\ng.figure.set_figheight(8)\ng.figure.set_figwidth(16)\ng.ax.set_ylim(0,100)\ng.ax.set_xlabel('Job')\ng.ax.set_ylabel('% of target variable')\ng.ax.set_title(\"% of target variable by job\")\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)\n\n\n","f3376af1":"plt.figure(figsize=(12,10))\ncorrelation_matrix = df.corr()\nsns.heatmap(correlation_matrix, annot =True)","e0a4aef6":"train_df.info()","b95fd37e":"pca_df = df.copy()\ncolumns_to_transform = ['age', 'balance', 'day', 'duration', 'pdays']\nobject_col = pca_df.select_dtypes('object').columns\nobject_col","d86d8006":"from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n\npca_df.loc[:, columns_to_transform] = StandardScaler().fit_transform(pca_df.loc[:, columns_to_transform].values)\npca_df.loc[:, object_col] = OrdinalEncoder().fit_transform(pca_df.loc[:, object_col].values)\npca_df","e179ac63":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\n\nx = pca_df.iloc[:, :-1]\ny = pca_df.iloc[:, -1]\n\nprincipalComponents = pca.fit_transform(x)","28e0d3a8":"principalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal_component_1', 'principal_component_2'])","2ce32422":"principalDf","d8c8fc99":"finalDf = pd.concat([principalDf, pca_df[['target']]], axis = 1)","0c283377":"sns.scatterplot(data=finalDf, x='principal_component_1', y='principal_component_2', hue='target')\nplt.title('2 component PCA')\nplt.show()","67e027b0":"df = train_df.copy()\ndf.shape","4c082cf5":"df.describe().T","20a8dcad":"df = pd.get_dummies(df,columns = ['job','marital','education','default','housing','month','loan','contact','poutcome'], drop_first = True)\ndf.head()","3c9b3dcc":"df['target'] = df['target'].map({'no': 0, 'yes':1})\ndf['target']","77d5fd8a":"df.target.value_counts()","f87c1dcf":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\ncolumns = df.columns[:-1]\n\ntarget = df['target']\ndf = df.drop('target',axis = 1)\n\ndf = scaler.fit_transform(df)\n\ndf = pd.DataFrame(df,columns=[columns])\ndf.head()","e4e90da0":"target.value_counts()","f9f49e68":"from sklearn.model_selection import train_test_split\n\n#Splitting the data into train and test data\nX_train, X_test, y_train, y_test = train_test_split(df,target,test_size = 0.25, random_state = 20)","00e211f7":"X_train.columns = X_train.columns.get_level_values(0)\nX_test.columns =  X_test.columns.get_level_values(0)","c063b033":"from imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\n\nkfold = KFold(shuffle=True , random_state=12)","9d53d2d3":"oversample = SMOTE()\n\nX_train_smote, y_train_smote = oversample.fit_resample(X_train,y_train)","a66a617b":"def plot_learning_curve(\n    estimator,\n    title,\n    X,\n    y,\n    axes=None,\n    ylim=None,\n    cv=None,\n    n_jobs=None,\n    train_sizes=np.linspace(0.1, 1.0, 5),\n):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n        estimator,\n        X,\n        y,\n        cv=cv,\n        n_jobs=n_jobs,\n        train_sizes=train_sizes,\n        return_times=True,\n    )\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(\n        train_sizes,\n        train_scores_mean - train_scores_std,\n        train_scores_mean + train_scores_std,\n        alpha=0.1,\n        color=\"r\",\n    )\n    axes[0].fill_between(\n        train_sizes,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.1,\n        color=\"g\",\n    )\n    axes[0].plot(\n        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n    )\n    axes[0].plot(\n        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n    )\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n    axes[1].fill_between(\n        train_sizes,\n        fit_times_mean - fit_times_std,\n        fit_times_mean + fit_times_std,\n        alpha=0.1,\n    )\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, \"o-\")\n    axes[2].fill_between(\n        fit_times_mean,\n        test_scores_mean - test_scores_std,\n        test_scores_mean + test_scores_std,\n        alpha=0.1,\n    )\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt","cfe269de":"from sklearn.model_selection import  learning_curve\nfrom sklearn.metrics import precision_recall_curve, roc_auc_score, f1_score","d1ce6a87":"classifier = RandomForestClassifier()\nclassifier.fit(X_train_smote, y_train_smote)\npredicted_proba = classifier.predict_proba(X_test)\nprint(f\"ROC AUC score: {roc_auc_score(y_test, predicted_proba[:, 1])}\")\npred = classifier.predict(X_test)\nprint(f\"F1 score : {f1_score(y_test, pred)}\")\nprint(classification_report(y_test, pred))","150f842a":"estimator = RandomForestClassifier()\nplot_learning_curve(estimator, \"Test\",X_train_smote, y_train_smote, cv=kfold, n_jobs=-1)","a9ba574f":"from lightgbm import LGBMClassifier\n\nestimator = LGBMClassifier()\nplot_learning_curve(estimator, \"Test\",X_train_smote, y_train_smote, cv=kfold, n_jobs=-1)\nestimator.fit(X_train_smote, y_train_smote)\ny_pred = estimator.predict(X_test)\nprint(classification_report(y_test, y_pred))","c1448948":"from sklearn.model_selection import GridSearchCV\nkfold= KFold(5, shuffle=True, random_state=47)\n\nparams = {\n    'boosting_type': ['gbdt', 'dart'],\n    'max_depth': range(2,15),\n}\n\nestimator = LGBMClassifier()\n\ngr_lgbm = GridSearchCV(estimator, param_grid=params, cv=kfold, n_jobs=-1, verbose=1, scoring='f1')\ngr_lgbm.fit(X_train_smote, y_train_smote)\nlgbm_depth = gr_lgbm.best_params_\nprint(f\"Best depth: {lgbm_depth['max_depth']}\")\nprint(f\"Best boosting_type: {lgbm_depth['boosting_type']}\")\nprint(f\"Best score: {gr_lgbm.best_score_}\")","5ef8d7ac":"kfold= KFold(5, shuffle=True, random_state=47)\n\nparams = {\n    \"num_leaves\": [2**x for x in range(1, 15)]\n}\n\nestimator = LGBMClassifier(max_depth=10)\n\ngr_lgbm = GridSearchCV(estimator, param_grid=params, cv=kfold, n_jobs=-1, verbose=1, scoring='f1')\ngr_lgbm.fit(X_train_smote, y_train_smote)\nlgbm_leaves = gr_lgbm.best_params_\nprint(f\"Best n_leaves: {lgbm_leaves['num_leaves']}\")\nprint(f\"Best score: {gr_lgbm.best_score_}\")\n","6e664688":"params = {\n    \"learning_rate\": np.linspace(0.009, 0.1)\n}\n\nestimator = LGBMClassifier(max_depth=10, num_leaves=256)\n\ngr_lgbm = GridSearchCV(estimator, param_grid=params, cv=kfold, n_jobs=-1, verbose=1, scoring='f1')\ngr_lgbm.fit(X_train_smote, y_train_smote)\nlgbm_rate = gr_lgbm.best_params_\nprint(f\"Best learning_rate: {lgbm_rate['learning_rate']}\")\nprint(f\"Best score: {gr_lgbm.best_score_}\")","968736cb":"estimator = LGBMClassifier(max_depth=10, num_leaves=256, learning_rate=0.1, n_estimators=1000)\nplot_learning_curve(estimator, \"Test\",X_train_smote, y_train_smote, cv=kfold, n_jobs=-1)","80846b60":"params = {'max_depth':10,'num_leaves':256, 'learning_rate':0.1, 'n_estimators':1000}\n\n\nestimator =LGBMClassifier(**params)\nestimator.fit(X_train_smote, y_train_smote, eval_set=(X_test, y_test), eval_metric='f1', early_stopping_rounds=100, verbose=1)","f7ea22dd":"pred = estimator.predict(X_test)\nprint(classification_report(y_test, pred))","d2122f46":"print(f\"Final F1 score: {f1_score(y_test, pred)}\")","214636c9":"Also, clients who have secondary education and loan in average subscribe a term deposit more\n\nThis can also impact the results of model","de69034a":"### Loading data\n\n#### We are not going to use test.csv data, because it consists of train.csv data, and this can lead to data leak","1e99e5c2":"But, as we can see, in average students and retired clients subscribed for deposit more","97bdee9a":"#### Function for testing models","2d62917d":"Even if we have more clients that have secondary education, by average clients who have tertiary have a positive target value.\n\nSo, in theory this can impact output of the model","3bdf0cda":"We need to balance classes before testing and learning.\n\nI will use SMOTE to oversample the minority class","44e4f944":"### Visualise variables with PCA","2453f84f":"### Target feature","8da60f7b":"### Testing models","9688e0f4":"### Job","e9844182":"Most of subscribed clients are in visible cloud\n\nBut it could be because we have imbalanced data","2da37fbc":"## Prepare data for visualizing'\n\nCopy training dataframe for manipulations during EDA","374bf150":"### Education","376e4ee1":"Most clients have secondary education\n\nLet's see how education impacts our target variable","8743ffd8":"## Encoding and Scaling Features","88a65bb9":"#### Transform binary category features to int","41bf06d7":"## Training_model","81c444ad":"Target variable is highly imbalanced, this will impact perfomance of a model, this can be fixed with SMOTE technique","e560ff50":"### Splitting data into train and test datasets","40935215":"#### Check RandomForest perfomance with learning curves","7a70254f":"#### LGBM have a better results, so i will use it","3cfc7321":"We have no high correlation between features","5cb6eded":"## Tunning LGBMClassifier","f3049aca":"##  EDA","e1336c84":"#### LGBM testing","b32cc8f1":"Most of the cliens have blue-collar, management or technician job","25f20f0e":"### Import libraries"}}