{"cell_type":{"8e00be3a":"code","982ea1e5":"code","5ee9238b":"code","88e28e19":"code","e03a1ab4":"code","e4ea1ab8":"code","5fc719de":"code","3e71dc43":"code","d4102c7e":"code","e8a45e7e":"code","c8a40ca5":"code","ac456cc6":"code","7496e3ca":"code","f1de7c8d":"code","9d518569":"code","a4d8c41d":"code","e8e5356f":"code","cffc8d1f":"code","96d90d66":"code","7aa9a69b":"code","9b034f19":"code","cbcbc3fb":"code","cd59d761":"code","5680434b":"code","92345655":"code","30a669c7":"code","8248831b":"code","fdcba88c":"code","9ad4f962":"code","ff6293a8":"code","1ce31c93":"code","28947097":"code","b0aff3cb":"code","571aa696":"code","3fc05445":"code","1884e46e":"code","068b90f2":"code","f37dfb68":"code","43904804":"code","4edceb5e":"code","71d8d819":"code","0b8a39f4":"code","94309d48":"code","f18647b3":"code","167e79dc":"code","c95536ab":"code","d7e0d513":"code","847701d7":"code","72e424fd":"code","b0c8bfbe":"code","e70280c0":"code","0e22ed98":"code","ce364a82":"code","b79cad7f":"code","a858e5b5":"code","454de2df":"code","cb622670":"code","1de951ae":"code","4e6474a7":"markdown","b0c1d9f2":"markdown","d86d0b9e":"markdown","483f87b6":"markdown","d1fe986c":"markdown","f6482d33":"markdown","0cc9d48c":"markdown","98a2466f":"markdown","6d433739":"markdown","4448b4fa":"markdown","758b4a8e":"markdown","6b9d2902":"markdown","76229d85":"markdown","6b96e1c1":"markdown","f0006afc":"markdown"},"source":{"8e00be3a":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\n\n# Ignore warnings\n\nwarnings.filterwarnings(\"ignore\")\npd.options.mode.chained_assignment = None\n\n# Show input data files\n\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","982ea1e5":"# Train set\n\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_train.shape","5ee9238b":"# Test set\n\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_test.shape","88e28e19":"# Combine features of test and train sets\n\ndf_both = df_train.loc[:, df_train.columns != \"Survived\"].append(df_test, ignore_index=True)\ndf_both.shape","e03a1ab4":"df_both.head()","e4ea1ab8":"df_both.describe()","5fc719de":"# Replace null values\n\ndef fill_na(df, val):\n    return df.fillna(val, inplace=True)","3e71dc43":"# Drop rows containing null values\n\ndef drop_na(df):\n    return df.dropna(axis=0, how=\"any\", thresh=None, subset=None, inplace=False)","d4102c7e":"import re\n\n# Check if a value exists in given columns\n\ndef val_exist(df, cols, val):\n    for col in cols:\n        if isinstance(val, str) and df[col].str.contains(val, na=False, flags=re.IGNORECASE).any():\n            return True\n        elif (isinstance(val, int) or isinstance(val, float)) and any(df[col] == val):\n            return True\n    return False","e8a45e7e":"# Get title info from passenger name\n\ndf_train[\"Title\"] = df_train[\"Name\"].apply(lambda x: x.split(\".\")[0].split(\",\")[1].strip())","c8a40ca5":"df_train[[\"Name\", \"Title\"]].head()","ac456cc6":"# Get family size\n\ndf_train[\"FamilySize\"] = df_train[\"SibSp\"] + df_train[\"Parch\"] + 1","7496e3ca":"df_train[[\"SibSp\", \"Parch\", \"FamilySize\"]].head()","f1de7c8d":"# Drop passenger name and id columns\n\ndf_train = df_train.drop([\"Name\", \"PassengerId\"], axis=1)","9d518569":"# Columns containing text values\nstr_cols = [\"Sex\", \"Ticket\", \"Cabin\", \"Embarked\", \"Title\"]\n\nval_exist(df_train, str_cols, \"Bilinmiyor\")","a4d8c41d":"# Replace null values with a unique value\n\nfor col in str_cols:\n    fill_na(df_train[col], \"Bilinmiyor\")","e8e5356f":"# Columns containing numeric values\nint_cols = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\"]\n\nfor col in int_cols:\n    print(val_exist(df_train, [col], df_train[col].mean(skipna=True)))","cffc8d1f":"# Replace null values with column mean\n\nfor col in int_cols:\n    fill_na(df_train[col], df_train[col].mean(skipna=True))","96d90d66":"import seaborn as sns\n\n# Visualization of survival by age\n\ngraph = sns.FacetGrid(df_train, col=\"Survived\") \ngraph = graph.map(sns.distplot, \"Age\")","7aa9a69b":"from sklearn.preprocessing import LabelBinarizer\n\n# Convert text values to binary lists\n\ndef binarize(df):\n    return LabelBinarizer().fit_transform(df).tolist()","9b034f19":"# Convert binary lists to base-2 numbers\n\ndef base_two(df):\n    return df.apply(lambda x: int(\"\".join(\"{0}\".format(i) for i in x), 2))","cbcbc3fb":"from sklearn.preprocessing import StandardScaler\n\n# Scale values column-wise\n\ndef scaler(df):\n    return StandardScaler().fit(df)","cd59d761":"# Replace outliers with null values\n\ndef outlier(df):\n    return df[np.abs(df - df.mean()) <= (3 * df.std())]","5680434b":"for col in str_cols:\n    df_train[col] = binarize(df_train[col])\n    df_train[col] = base_two(df_train[col])","92345655":"# Feature columns\n\ndf_feature = df_train.loc[:, df_train.columns != \"Survived\"]\n\n# Scale values column-wise\n\nstd_scaler = scaler(df_feature)\ndf_feature.iloc[:,:] = std_scaler.transform(df_feature)\n\n# Append labels\n\ndf_feature[\"Survived\"] = df_train[\"Survived\"]\n\ndf_feature.shape","30a669c7":"# Columns containing features\n\nall_cols = [\"Sex\", \"Ticket\", \"Cabin\", \"Embarked\", \"Title\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\"]\n\n# Detect outliers\n\nfor col in all_cols:\n    df_feature[col] = outlier(df_feature[col])\n    \n# Remove rows containing outliers\n\ndf_feature = drop_na(df_feature)\n\ndf_feature.shape","8248831b":"from sklearn.model_selection import train_test_split\n\n# Split train set into X and y\n\nX, y = df_feature.loc[:, df_feature.columns != \"Survived\"], df_feature[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train.shape","fdcba88c":"X_train.head()","9ad4f962":"from sklearn.model_selection import GridSearchCV\n\n# Find best parameters for given data and classifier\n\ndef best_params(classifier, parameters, X, y):\n    clf = GridSearchCV(classifier, parameters)\n    clf.fit(X, y)\n    return clf.best_params_","ff6293a8":"from sklearn.metrics import accuracy_score\n\n# Calculate accuracy of predictions by expected label set\n\ndef accuracy(name, y, predictions):\n    return \"Accuracy of {0}: {1}\".format(name, accuracy_score(y, predictions))","1ce31c93":"from sklearn.svm import SVC\n\nparams_svc = {\"C\":[1, 10], \"gamma\":(\"scale\", \"auto\"), \"kernel\":(\"linear\", \"rbf\")}\nbest_params(SVC(), params_svc, X_train, y_train)","28947097":"# Train SVC\n\nclf_svc = SVC(C=1, gamma=\"auto\", kernel=\"rbf\")\nclf_svc.fit(X_train, y_train)","b0aff3cb":"print(accuracy(\"SVC\", y_test, clf_svc.predict(X_test)))","571aa696":"from sklearn.neighbors import KNeighborsClassifier\n\nparams_kn = {\"algorithm\":(\"auto\", \"brute\", \"kd_tree\"), \"n_neighbors\":[1, 3, 5]}\nbest_params(KNeighborsClassifier(), params_kn, X_train, y_train)","3fc05445":"# Train K-Neighbors\n\nclf_kn = KNeighborsClassifier(algorithm=\"brute\", n_neighbors=5)\nclf_kn.fit(X_train, y_train)","1884e46e":"print(accuracy(\"K-Neighbors\", y_test, clf_kn.predict(X_test)))","068b90f2":"from sklearn.ensemble import RandomForestClassifier\n\nparams_rf = {\"class_weight\":(\"balanced\", \"balanced_subsample\"), \"max_features\":(\"auto\", \"log2\"), \"n_estimators\":[10, 100]}\nbest_params(RandomForestClassifier(), params_rf, X_train, y_train)","f37dfb68":"# Train Random Forest\n\nclf_rf = RandomForestClassifier(class_weight=\"balanced_subsample\", max_features=\"log2\", n_estimators=100)\nclf_rf.fit(X_train, y_train)","43904804":"print(accuracy(\"Random Forest\", y_test, clf_rf.predict(X_test)))","4edceb5e":"from sklearn.ensemble import GradientBoostingClassifier\n\nparams_gb = {\"criterion\":(\"friedman_mse\", \"mse\", \"mae\"), \"loss\":(\"deviance\", \"exponential\"), \"max_features\":(\"auto\", \"log2\")}\nbest_params(GradientBoostingClassifier(), params_gb, X_train, y_train)","71d8d819":"# Train Gradient Boosting\n\nclf_gb = GradientBoostingClassifier(criterion=\"friedman_mse\", loss=\"deviance\", max_features=\"auto\")\nclf_gb.fit(X_train, y_train)","0b8a39f4":"print(accuracy(\"Gradient Boosting\", y_test, clf_gb.predict(X_test)))","94309d48":"from sklearn.naive_bayes import GaussianNB\n\n# Train Gaussian Naive Bayes\n\nclf_nb = GaussianNB()\nclf_nb.fit(X_train, y_train)","f18647b3":"print(accuracy(\"Naive Bayes\", y_test, clf_nb.predict(X_test)))","167e79dc":"# Go on with the most accurate estimator\n\nclf = clf_kn","c95536ab":"df_test[\"Title\"] = df_test[\"Name\"].apply(lambda x: x.split(\".\")[0].split(\",\")[1].strip())","d7e0d513":"df_test[\"FamilySize\"] = df_test[\"SibSp\"] + df_test[\"Parch\"] + 1","847701d7":"for col in str_cols:\n    fill_na(df_test[col], \"Bilinmiyor\")","72e424fd":"for col in int_cols:\n    fill_na(df_test[col], df_test[col].mean(skipna=True))","b0c8bfbe":"for col in str_cols:\n    df_test[col] = binarize(df_test[col])\n    df_test[col] = base_two(df_test[col])","e70280c0":"data = df_test[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\", \"Title\", \"FamilySize\"]]\ndata.iloc[:,:] = std_scaler.transform(data)","0e22ed98":"data.shape","ce364a82":"data.head()","b79cad7f":"# Survival predictions by main estimator\n\npred = clf.predict(data)","a858e5b5":"# Match survival info with passenger id\n\nresult = {}\nfor i in range(len(pred)):\n    result[df_test.iloc[i][\"PassengerId\"]] = pred[i]","454de2df":"import csv\n\n# Export the result as csv file\n\nwith open(\"result.csv\", \"w\") as f:\n    writer = csv.DictWriter(f, fieldnames=[\"PassengerId\", \"Survived\"])\n    writer.writeheader()\n    for key, val in result.items():\n        writer.writerow({\"PassengerId\": key, \"Survived\": val})","cb622670":"df_result = pd.read_csv(\"result.csv\")\ndf_result.head()","1de951ae":"# Distribution of survival info\n\ndf_result.groupby([\"Survived\"]).count().plot(kind=\"bar\")","4e6474a7":"##### Classifiers","b0c1d9f2":"##### Methods","d86d0b9e":"### Processing","483f87b6":"##### Features","d1fe986c":"# RESULT","f6482d33":"##### Methods","0cc9d48c":"### Prediction","98a2466f":"##### Features","6d433739":"##### Features","4448b4fa":"# DATA","758b4a8e":"##### Predictions","6b9d2902":"# MODEL","76229d85":"##### Methods","6b96e1c1":"### Classification","f0006afc":"### Pre-processing"}}