{"cell_type":{"9fa9997e":"code","b607e60b":"code","d835768b":"code","02ad9b76":"code","cd86fd71":"code","6d252453":"code","3eac127e":"code","af96bfcd":"code","6aeb5271":"code","f32d200a":"code","a3df67c0":"code","a3675773":"code","0226cd90":"code","46a369cd":"code","5c092ca2":"code","5872e683":"code","859574d2":"code","d50e7a98":"code","768e3a99":"code","368a9cd6":"code","56a13eaa":"code","d2b1ed44":"code","41c56cc1":"code","b25af07e":"code","dc6fc904":"code","2216fd24":"code","cfc46328":"code","3e70272c":"code","c75a0ad2":"code","86b071b5":"code","ecd341a2":"code","7da3e18e":"code","d590f121":"code","99fcaee6":"code","1abe8274":"code","c96d29f2":"code","d480cc83":"code","803f70f9":"code","e26654c1":"code","fd1e39a3":"code","72d75a48":"code","a7d59f91":"code","e92d1507":"code","4dc78556":"code","46b64c05":"code","340d04b3":"code","2a6f836f":"code","e57e83b2":"code","05d17271":"code","077ff4a7":"code","7b1b4019":"code","4eb972bf":"code","a31da387":"code","349a0362":"markdown","bc265dd2":"markdown","79299a7d":"markdown"},"source":{"9fa9997e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly_express as px\nimport matplotlib.image as mpimg\nfrom tabulate import tabulate\nimport missingno as msno \nfrom IPython.display import display_html\nfrom PIL import Image\nimport gc\nimport cv2\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b607e60b":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","d835768b":"data.head(5)","02ad9b76":"data.info()","cd86fd71":"data.describe(include='all')","6d252453":"data.isnull().sum().sort_values(ascending=False)","3eac127e":"data.columns","af96bfcd":"f, ax = plt.subplots(figsize=(12, 5))\nsns.countplot('Class', data=data)","6aeb5271":"#The classes in the dataset are very skewed and very imbalanced \n# As you can see most of the transactions are non fraud if we should go ahead \n# and use this imbalanced dataset and make our predictions our results might be wrong.","f32d200a":"#Lets check the distribution of amount and some other features","a3df67c0":"f, ax = plt.subplots(figsize=(12, 5))\nsns.distplot(data['Amount'], color='r')","a3675773":"f, ax = plt.subplots(figsize=(12,5))\nsns.distplot(data.Time, color='g')","0226cd90":"# From the dataset we can observe all the features in the dataset are scaled \n#except for Amount and Time.\n\n#Since the dataset is also imbalanced we can  divide the dataset into subsamples \n# so we can have an equal amount of fraud and non fraud cases so as to help the model \n# understand the data more","46a369cd":"from sklearn.preprocessing import StandardScaler, RobustScaler","5c092ca2":"robust_scaler = RobustScaler()","5872e683":"data['scaled_amount'] = robust_scaler.fit_transform(data['Amount'].values.reshape(-1, 1))\ndata['Time'] = robust_scaler.fit_transform(data['Time'].values.reshape(-1, 1))","859574d2":"data.head(5)","d50e7a98":"data.drop(['Time','Amount'], axis=1, inplace=True)","768e3a99":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit","368a9cd6":"df = data.copy()","56a13eaa":"data = data.sample(frac = 1)","d2b1ed44":"fraud_data = data.loc[data['Class'] == 1]\nnon_fraud_data = data.loc[data['Class'] == 0][:492]","41c56cc1":"normally_distributed = pd.concat([fraud_data, non_fraud_data])","b25af07e":"new_data = normally_distributed.sample(frac=1, random_state=1)","dc6fc904":"new_data.head(5)","2216fd24":"colors = [\"#0101DF\", \"#DF0101\"]\nf, ax = plt.subplots(figsize=(12, 5))\nsns.countplot('Class', data=new_data, palette=colors)","cfc46328":"#correlation matrix helps us understand our data. It helps us to understand the \n#correlation between two vaariables. If there are features that heavily influence\n#whether a specific transaction is fraudulent or not","3e70272c":"#Below is a heatmap of the correlation of the normal data:\ncorrelation_matrix = data.corr()\nfig = plt.figure(figsize=(20,8))\nsns.heatmap(correlation_matrix, vmax=0.8, square=True)","c75a0ad2":"#Below is a heatmap of the correlation of the normal data:\ncorrelation_matrix = new_data.corr()\nfig = plt.figure(figsize=(20,8))\nsns.heatmap(correlation_matrix, vmax=0.8, square=True)","86b071b5":"correlation_matrix['Class'].sort_values(ascending=False)","ecd341a2":"#My main aim is to remove extreme outliers from features that have high correlation with our classes\n#we can remove these outliers by using interquartile range method","7da3e18e":"v10 = df['V10'].loc[df['Class'] == 1].values\nv12 = df['V12'].loc[df['Class'] == 1].values\nv14 = df['V14'].loc[df['Class'] == 1].values","d590f121":"f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nsns.distplot(v10, ax=ax1, color='g')\nax1.set_title('V10 Distribution Fraud Transactions)', fontsize=14)\n\n\nsns.distplot(v12, ax=ax2, color='r')\nax2.set_title('V12 Distribution Fraud Transactions)', fontsize=14)\n\nsns.distplot(v14, ax=ax3, color='b')\nax3.set_title('V14 Distribution Fraud Transactions)', fontsize=14)","99fcaee6":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = df['V14'].loc[df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_data = new_data.drop(new_data[(new_data['V14'] > v14_upper) | (new_data['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_data['V12'].loc[new_data['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_data = new_data.drop(new_data[(new_data['V12'] > v12_upper) | (new_data['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_data)))\nprint('----' * 44)\n\n\n# Removing outliers V10 Feature\nv10_fraud = new_data['V10'].loc[new_data['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_data = new_data.drop(new_data[(new_data['V10'] > v10_upper) | (new_data['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_data)))","1abe8274":"y = new_data['Class']\nX = new_data.drop(['Class'], axis=1)","c96d29f2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","d480cc83":"X_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","803f70f9":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score","e26654c1":"classifier = LogisticRegression()\nclassifier.fit(X_train, y_train)","fd1e39a3":"training_score = cross_val_score(classifier, X_train, y_train, cv=10)","72d75a48":"training_score","a7d59f91":"training_score.mean()","e92d1507":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n","4dc78556":"log_reg","46b64c05":"log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=10)","340d04b3":"log_reg_score","2a6f836f":"y_pred = log_reg.predict(X_test)","e57e83b2":"y_pred","05d17271":"from sklearn.metrics import confusion_matrix","077ff4a7":"cm = confusion_matrix(y_pred, y_test)","7b1b4019":"cm","4eb972bf":"from sklearn.metrics import classification_report","a31da387":"print('Logistic Regression:')\nprint(classification_report(y_test, y_pred))","349a0362":"PLEASE UPVOTE AFTER READING THIS. THANK YOU.","bc265dd2":"since we are dealing with an unbalanced dataset. i would love to split the data 50 \/ 50 with regards to the classes\n","79299a7d":"> correlation matrix helps us understand our data. It helps us to understand the \ncorrelation between two vaariables. If there are features that heavily influence\nwhether a specific transaction is fraudulent or not"}}