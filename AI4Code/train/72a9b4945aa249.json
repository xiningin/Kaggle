{"cell_type":{"34797d8b":"code","2c95eea4":"code","5b40c43e":"code","10811cb0":"code","b83c8b65":"code","f6f51c68":"code","d509bb42":"code","298339b4":"code","84fa89b3":"code","8533d40e":"code","2c89b748":"code","3443f688":"code","1aa5d8a7":"code","55e308a5":"code","5dec0379":"code","4ea7b474":"code","4febf556":"code","3c09ad8c":"code","854a8b22":"code","319f7796":"code","d60a16c6":"code","9d06399e":"code","e3d89116":"code","07d23dcf":"code","9c79ad2f":"code","3a66dcda":"code","91b91ca6":"code","01d0de5f":"code","18a646b4":"code","730d6217":"code","6dafb00c":"code","c973c011":"code","edb2bde7":"code","70044707":"code","93a0be00":"markdown","765bb44e":"markdown","200bcb47":"markdown","0127384e":"markdown","528e2c41":"markdown","3f337fdb":"markdown","182961bb":"markdown","9603fc81":"markdown","8263fff8":"markdown","52355876":"markdown","0b4b2107":"markdown","3469e809":"markdown","34aad90a":"markdown","05e685ae":"markdown","5f24f1f2":"markdown","c6c66055":"markdown","f985c603":"markdown","4f82bae3":"markdown","f2b18b42":"markdown","83283f41":"markdown","76b64f95":"markdown","c4247c1d":"markdown","af66e6a1":"markdown","6cf85615":"markdown","1ea43cae":"markdown","f5ecdad7":"markdown","47c07786":"markdown","89bcc597":"markdown","d85404ee":"markdown","e232e240":"markdown","3fbc2ab3":"markdown","87dbc6fa":"markdown","92a8c753":"markdown","be97f08b":"markdown","a747290e":"markdown"},"source":{"34797d8b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndata = pd.read_csv('..\/input\/train.csv')\ntarget = pd.read_csv('..\/input\/test.csv')\n\n# replace prices with logarithmic prices\ndata['logSalePrice'] = np.log10(data.SalePrice.values)\ndata.drop('SalePrice', axis=1, inplace=True)","2c95eea4":"print(open('..\/input\/data_description.txt', 'r').read())","5b40c43e":"continuous_features = [col for col in data.columns if data[col].dtype != 'O']\n\ncontinuous_features.remove('Id')\ncontinuous_features.remove('logSalePrice') # remove the target feature\n\nordinal_features = ['Street', 'Alley', 'LotShape', 'Utilities', 'LandSlope', 'ExterQual', \n                    'LandContour', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                    'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'CentralAir', \n                    'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType',\n                    'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', \n                    'Fence'] \n\ncategorical_features = ['LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n                        'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \n                        'MasVnrType', 'Foundation', 'Heating', 'MiscFeature',\n                        'SaleType', 'SaleCondition', 'MSZoning', 'BldgType']\n\n(len(set(continuous_features + ordinal_features + categorical_features)) == \n len(continuous_features + ordinal_features + categorical_features) == \n len(data.columns)-2)","10811cb0":"print('incomplete features in data:\\n' +\n      \"\\n\".join([\"  {}: {} useful data points\".format(col, data[col].dropna().count()) \n                 for col in data.columns if data[col].dropna().count() < 1460]), '\\n')\nprint('incomplete features in target:\\n' +\n      \"\\n\".join([\"  {}: {} useful data points\".format(col, target[col].dropna().count()) \n                 for col in target.columns if target[col].dropna().count() < 1459]))","b83c8b65":"from scipy.stats import linregress\n\n# linear regression on LotFrontage and sqrt(LotArea); cutoff at LotArea<20000 to minimize outliers\nslope, intercept, rvalue, pvalue, stderr = linregress(\n    np.sqrt(data.loc[(data.LotFrontage.notna()) & (data.LotArea < 20000), 'LotArea']),\n    data.loc[(data.LotFrontage.notna()) & (data.LotArea < 20000), 'LotFrontage'])\n\n# derive residuals for known LotFrontage\nresidual = data.LotFrontage - (slope*np.sqrt(data.LotArea)+intercept)\nprint('mean residual:', np.nanmean(residual), '\\nmedian residual:', \n      np.nanmedian(residual), '\\nresidual std:', np.nanstd(residual))\n\n# apply equation for missing data\ndata.loc[data.LotFrontage.isna(), 'LotFrontage'] = np.sqrt(data.LotArea)*slope+intercept\ntarget.loc[target.LotFrontage.isna(), 'LotFrontage'] = np.sqrt(target.LotArea)*slope+intercept","f6f51c68":"# fill nan with 'NA' for further processing (see below)\nfor col in ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n            'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', \n            'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']:\n    data.loc[:, col].fillna('NA', inplace=True)\n    target.loc[:, col].fillna('NA', inplace=True)    ","d509bb42":"for sample in [data, target]:\n    sample.fillna({col: sample.loc[:, col].median() for col in continuous_features}, inplace=True)\n    sample.fillna({col: sample.loc[:, col].value_counts().index[0] for col in categorical_features + ordinal_features}, \n                  inplace=True)","298339b4":"print('incomplete features in data:', \n      len([\"  {}: {} useful data points\".format(col, data[col].dropna().count()) \n           for col in data.columns if data[col].dropna().count() < 1460]), '\\n')\nprint('incomplete features in target:', \n      len([\"  {}: {} useful data points\".format(col, target[col].dropna().count()) \n           for col in target.columns if target[col].dropna().count() < 1459]))","84fa89b3":"ordinal_transform = {'Street':  {'Grvl': 1, 'Pave': 2}, \n                     'Alley': {'NA': 0, 'Grvl': 1, 'Pave': 2}, \n                     'LotShape': {'IR3': 1, 'IR2': 2, 'IR1': 3, 'Reg': 4}, \n                     'Utilities': {'ELO': 1, 'NoSeWa': 2, 'NoSewr': 3, 'AllPub': 4}, \n                     'LandSlope': {'Sev': 1, 'Mod': 2, 'Gtl': 3}, \n                     'LandContour': {'Low': 1, 'HLS': 1, 'Bnk': 2, 'Lvl': 3}, \n                     'ExterQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                     'ExterCond': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'BsmtQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'BsmtCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'BsmtExposure': {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}, \n                     'BsmtFinType1': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, \n                                      'GLQ': 6}, \n                     'BsmtFinType2': {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, \n                                      'GLQ': 6}, \n                     'HeatingQC': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5},\n                     'CentralAir': {'N': 1, 'Y': 2}, \n                     'Electrical': {'Mix': 1, 'FuseP': 2, 'FuseF': 3, 'FuseA': 4, 'SBrkr': 5}, \n                     'KitchenQual': {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'Functional': {'Sal': 1, 'Sev': 2, 'Maj2': 3, 'Maj1': 4, 'Mod': 5, 'Min2': 6, \n                                    'Min1': 7, 'Typ': 8}, \n                     'FireplaceQu': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'GarageType': {'NA': 0, 'Detchd': 1, 'CarPort': 2, 'BuiltIn': 3, \n                                    'Basment': 4, 'Attchd': 5, '2Types': 6},\n                     'GarageFinish': {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}, \n                     'GarageQual': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'GarageCond': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'PavedDrive': {'N': 1, 'P': 2, 'Y': 3}, \n                     'PoolQC': {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, \n                     'Fence': {'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}, \n                    }\n\n# apply transformations\nfor col in ordinal_features:\n    data.loc[:, col] = data.loc[:, col].map(ordinal_transform[col], na_action='ignore')\n    target.loc[:, col] = target.loc[:, col].map(ordinal_transform[col], na_action='ignore')\n    \n# move some features from continuous to ordinal feature list\nfor col in ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n            'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']:\n    continuous_features.remove(col)\n    ordinal_features.append(col)\n    \n# move one feature from continuous to categorial feature list\ncontinuous_features.remove('MSSubClass')\ncategorical_features.append('MSSubClass')","8533d40e":"from sklearn.preprocessing import MultiLabelBinarizer\n\nenc = MultiLabelBinarizer()\nenc.fit([data.Condition1, data.Condition2])\n\n# apply transformation to data sample\nbinarized_columns = pd.DataFrame(enc.transform(list(zip(data.Condition1, data.Condition2))), \n                                 columns=enc.classes_, index=data.Id)\ndata = pd.merge(data, binarized_columns, on='Id')\ndata.drop(['Condition1', 'Condition2'], axis=1, inplace=True)\n\n# apply transformation to target sample\nbinarized_columns = pd.DataFrame(enc.transform(list(zip(target.Condition1, target.Condition2))), \n                                 columns=enc.classes_, index=target.Id)\ntarget = pd.merge(target, binarized_columns, on='Id')\ntarget.drop(['Condition1', 'Condition2'], axis=1, inplace=True)\n\ncategorical_features.remove('Condition1')\ncategorical_features.remove('Condition2')","2c89b748":"from sklearn.preprocessing import LabelBinarizer\n\nenc = LabelBinarizer()\n\nordinalized = []\nbinary_features = []\nbinarize = ['Neighborhood', 'MSSubClass']\n\nfor col in categorical_features:\n    if col not in binarize:\n        # rank by median logSalePrice\n        sorted_labels = data[\n            [col, 'logSalePrice']].groupby(col).logSalePrice.median().sort_values()\n        data.loc[:, col] = data.loc[:, col].map(\n            {sorted_labels.index.values[i]:i for i in range(len(sorted_labels))})\n        target.loc[:, col] = target.loc[:, col].map(\n            {sorted_labels.index.values[i]:i for i in range(len(sorted_labels))})\n        ordinalized.append(col)\n    else:\n        # binarize\n        enc.fit(data[col])\n        binarized_columns = pd.DataFrame(enc.transform(data[col]), \n                                         columns=enc.classes_, index=data.Id)\n        data = pd.merge(data, binarized_columns, on='Id')\n        data.drop(col, axis=1, inplace=True)\n        binarized_columns = pd.DataFrame(enc.transform(target[col]), \n                                         columns=enc.classes_, index=target.Id)\n        target = pd.merge(target, binarized_columns, on='Id')\n        target.drop(col, axis=1, inplace=True)\n        binary_features += list(enc.classes_)\n\nfor col in binarize:        \n    categorical_features.remove(col)\n    \nfor col in ordinalized:\n    categorical_features.remove(col)\n    ordinal_features.append(col)","3443f688":"f, ax = plt.subplots(int(np.floor(len(ordinal_features)\/5)+1), 5, \n                     figsize=(15, (np.floor(len(ordinal_features)\/5)+1)*3),\n                     sharey=True)\nax = np.ravel(ax)\nplt.subplots_adjust(hspace=0.5)\n\nfor i in range(len(ordinal_features)):\n    data.boxplot(column='logSalePrice', by=ordinal_features[i], ax=ax[i])","1aa5d8a7":"f, ax = plt.subplots(int(np.floor(len(continuous_features)\/5))+1, 5, \n                     figsize=(15, int(np.floor(len(continuous_features)\/5)+1)*3),\n                     sharey=True)\nax = np.ravel(ax)\n\nplt.subplots_adjust(hspace=0.3)\n\nfor i in range(len(continuous_features)):\n    data.plot.scatter(x=continuous_features[i], y='logSalePrice',\n                      alpha=0.1, legend=True, s=5, ax=ax[i])","55e308a5":"data['totalyear'] = np.sqrt(4020.1-data.YearBuilt-data.YearRemodAdd)\ntarget['totalyear'] = np.sqrt(4020.1-target.YearBuilt-target.YearRemodAdd)\ndata.plot.scatter(x='totalyear', y='logSalePrice', alpha=0.1)","5dec0379":"data['basement'] = (data.BsmtFinType1 * data.BsmtFinSF1 + \n                    data.BsmtFinType2 * data.BsmtFinSF2)\ntarget['basement'] = (target.BsmtFinType1 * target.BsmtFinSF1 + \n                      target.BsmtFinType2 * target.BsmtFinSF2)\ndata.drop(['BsmtFinType1', 'BsmtFinType2', \n           'BsmtFinSF1', 'BsmtFinSF2'], axis=1, inplace=True)\nplt.scatter(data.basement, data.logSalePrice, alpha=0.1)","4ea7b474":"data['exterior'] = data.Exterior1st + data.Exterior2nd\ntarget['exterior'] = target.Exterior1st + target.Exterior2nd\ndata.drop(['Exterior1st', 'Exterior2nd'], axis=1, inplace=True)\nplt.scatter(data.exterior, data.logSalePrice, alpha=0.1)","4febf556":"data['squarefeet'] = (data.TotalBsmtSF + \n                      data['1stFlrSF'] + data['2ndFlrSF'])\ntarget['squarefeet'] = (target.TotalBsmtSF + target['1stFlrSF'] + \n                        target['2ndFlrSF'])\nplt.scatter(data.squarefeet, data.logSalePrice, alpha=0.1)","3c09ad8c":"data['baths'] = (3*data.FullBath + data.HalfBath + \n                 2*data.BsmtFullBath + data.BsmtHalfBath)\ntarget['baths'] = (3*target.FullBath + target.HalfBath + \n                   2*target.BsmtFullBath + target.BsmtHalfBath)\nplt.scatter(data.baths, data.logSalePrice, alpha=0.1)","854a8b22":"data['masonry'] = np.sqrt(data.MasVnrArea) * data.MasVnrType\ntarget['masonry'] = np.sqrt(target.MasVnrArea) * target.MasVnrType\ndata.drop(['MasVnrArea', 'MasVnrType'], axis=1, inplace=True)\nplt.scatter(data.masonry, data.logSalePrice, alpha=0.1)","319f7796":"data['kitchen'] = data.KitchenAbvGr+data.KitchenQual\ntarget['kitchen'] = target.KitchenAbvGr+target.KitchenQual\nplt.scatter(data.kitchen, data.logSalePrice, alpha=0.1)","d60a16c6":"data['fireplace'] = np.sqrt(data.Fireplaces*data.FireplaceQu)\ntarget['fireplace'] = np.sqrt(target.Fireplaces*target.FireplaceQu)\nplt.scatter(data.fireplace, data.logSalePrice, alpha=0.1)","9d06399e":"data['kitchenbaths'] = data.kitchen+data.baths\ntarget['kitchenbaths'] = target.kitchen+target.baths\nplt.scatter(data.kitchenbaths, data.logSalePrice, alpha=0.1)","e3d89116":"data['garage'] = data.GarageArea*(data.GarageQual+data.GarageCond+\n                                  2*data.GarageCars+2*data.GarageFinish)\ntarget['garage'] = target.GarageArea*(target.GarageQual+target.GarageCond+\n                                      2*target.GarageCars+2*target.GarageFinish)\nplt.scatter(data.garage, data.logSalePrice, alpha=0.1)","07d23dcf":"data['outdoorarea'] = (data.WoodDeckSF + data.OpenPorchSF + data.EnclosedPorch + \n                       data['3SsnPorch'] + data.ScreenPorch + data.PoolArea)\ntarget['outdoorarea'] = (target.WoodDeckSF + target.OpenPorchSF + target.EnclosedPorch + \n                         target['3SsnPorch'] + target.ScreenPorch + target.PoolArea)\ndata.plot.scatter(x='outdoorarea', y='logSalePrice', alpha=0.1)","9c79ad2f":"data['outdoor'] = 0\ntarget['outdoor'] = 0\nfor col in ['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n            'ScreenPorch', 'PoolArea']:\n    data.loc[data[col] > 0, 'outdoor'] = 1\n    target.loc[target[col] > 0, 'outdoor'] = 1\n    \ndata.groupby('outdoor').logSalePrice.plot.hist(alpha=0.5)","3a66dcda":"data['totalquality'] = (data.OverallQual + data.KitchenQual + data.BsmtQual + \n                        data.ExterQual + data.HeatingQC + data.FireplaceQu + \n                        data.GarageQual)\ntarget['totalquality'] = (target.OverallQual + target.KitchenQual + target.BsmtQual + \n                          target.ExterQual + target.HeatingQC + target.FireplaceQu + \n                          target.GarageQual)\nplt.scatter(data.totalquality, data.logSalePrice, alpha=0.1)","91b91ca6":"data['totalcond'] = (data.OverallCond + data.BsmtCond + data.ExterCond + \n                     data.GarageCond)\ntarget['totalcond'] = (target.OverallCond + target.BsmtCond + target.ExterCond + \n                       target.GarageCond)\nplt.scatter(data.totalcond, data.logSalePrice, alpha=0.1)","01d0de5f":"f, ax = plt.subplots(figsize=(20,6))\n\ndata.drop('logSalePrice', axis=1).corrwith(\n    data.logSalePrice).agg('square').plot.bar(ax=ax, alpha=0.5)\nax.plot(ax.get_xlim(), [0.6, 0.6], color='red')\nax.set_title('Coefficient of Determination')\nax.grid()","18a646b4":"from sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=True)\n\npoldata = poly.fit_transform(data[['squarefeet', 'kitchenbaths', \n                                   'totalquality']]).transpose()\nfor colid in range(poldata.shape[0]):\n    data['poly_{}'.format(colid)] = poldata[colid]\n    \npoldata = poly.fit_transform(target[['squarefeet', 'kitchenbaths', \n                                     'totalquality']]).transpose()\nfor colid in range(poldata.shape[0]):\n    target['poly_{}'.format(colid)] = poldata[colid]","730d6217":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nfeaturelist = data.drop('logSalePrice', axis=1).columns\n\npipe = Pipeline([('scaler', RobustScaler()), \n                 ('model', Ridge(fit_intercept=True, random_state=42))])\n\nparam_space = {'model__alpha': [1, 5, 10]}\n\ngrid = GridSearchCV(pipe, param_grid=param_space, cv=10, \n                    scoring='neg_mean_squared_error')\n\ngrid.fit(data[featurelist], data.logSalePrice)","6dafb00c":"grid.best_params_","c973c011":"10**np.sqrt(-grid.best_score_)","edb2bde7":"pred = pd.DataFrame({'SalePrice': 10**grid.predict(target[featurelist])}, index=target.Id)","70044707":"pred.to_csv('prediction.csv', index='Id')","93a0be00":"## Data Description","765bb44e":"We combine features describing the status of the finished basement area into a single feature `basement`.","200bcb47":"For the remaining categorical features, we either rank them based on the median `logSalePrice`, or we binarize them:","0127384e":"### Feature completeness\n\nComplete feature data in preparation for data exploration.","528e2c41":"The small number of houses with wood decks, porches, screened areas, and pools justifies that these features are turned into ordinal binary features that simply indicate the existence of these elements.","3f337fdb":"Another new feature that combines the `kitchen` and `baths` features.  ","182961bb":"We combine the total number of baths, utilizing an arbitrary weighting scheme, into a single feature `baths`.","9603fc81":"We combine features describing the status of the building's exterior into a single feature `exterior`.","8263fff8":"A new feature related to the overall garage status using an arbitrary weighting scheme.","52355876":"We combine information on the masonry performed into a single feature `masonry`.","0b4b2107":"### Transforming ordinal features\n\nThe following dictionary describes the ranking of those features that were previously classified as ordinal features from lowest rank (worst case, starting at `1` so that missing data can be ranked with zero where applicable) to highest rank (best case): ","3469e809":"## Data Preparation","34aad90a":"## Data Exploration","05e685ae":"We use the following approaches to fill in missing data:\n\n#### LotFrontage\n\nThis feature is clearly correlated with feature `LotArea` - we impute missing values based on a linear regression model.","5f24f1f2":"## Modeling\n\nWe train a simple Ridge Regression model in combination with a robust scaler using a grid search approach with cross-validation. MSE is used as loss function.","c6c66055":"# House Prices\n\nThis is my take on the house price prediction competition. These are the steps I am taking in this notebook:\n1. fill in missing values in each feature\n2. transform categorical and ordinal feature into a numerical form\n3. explore features and engineer new features where appropriate\n4. use a simple Ridge Regression model for prediction\n\nI chose a Ride Regression model on purpose as I wanted to see what can be achieved with a simple linear model for this rather complex data set.\n\nAny kind of comments are welcome!\n","f985c603":"#### ordinal\/categorical features with `NA`\n\nFor features with possible values `NA` based on the description above, fill in `NA` for missing values:","4f82bae3":"## Feature Engineering\n\nBased on the figures shown above, we engineer the following features:","f2b18b42":"## Feature Correlations","83283f41":"We combine the total livable area in a new feature `squarefeet`.","76b64f95":"### Ordinal Features","c4247c1d":"The resulting best score root-mean-square loss suggests that sales prices can be predicted within ~12%.","af66e6a1":"### Interpretation of categorical features\n\nWe take two different approaches in interpreting categorical features:\n* a few categorical features that are deemed more important than others (based on their descriptions) are binarized\n* the remaining categorical features are ranked: for each feature we derive the median `SalePrice` value and rank the classes for each feature based on the median `SalePrice` from low to high ","6cf85615":"`logSalePrice` seems to increase disproportionally for houses that were recently built or remodeled. We take advantage of this observation and create a feature that correlates somehwat linearly with `SalePrice`. ","1ea43cae":"The resulting regularization parameter shows a good generalization and a low probability of over-fitting.","f5ecdad7":"Based on this description, we sort the data into **continuous features** that use float and int as data types, string features that can be transformed into **ordinal features** based on some ranking scheme, and **categorical features** that cannot be ranked easily based on their description or for which any ranking might be ambiguous:","47c07786":"We create a new feature `fireplace` that is related to the number and quality of fireplaces available.","89bcc597":"#### Combining and binarizing `Condition`\n\nWe use a MultiLabelBinarizer to convert `Condition1` and `Condition2` into a number of binary features:","d85404ee":"#### other features\n\n* in the case of continuous features, fill in missing values with the median for across the entire feature\n* in the case of ordinal and categorical features, fill in missing values with the most common value across the feature","e232e240":"We combine all the different features that describe recreational areas outside the house into a single feature `outdoorarea`. ","3fbc2ab3":"We add two new features that combine all the individual quality and condition flags in them.","87dbc6fa":"## Final Adjustments\n\n\nWe pick those three features that show the strongest correlation with `logSalePrice` and generate a matrix of polynomial and interaction features from those.\n\nNote that we ignore `OverallQual` as it is a substantial of `totalquality`.","92a8c753":"## Target sample prediction","be97f08b":"### Continuous Features","a747290e":"We combine information related to kitchens into a single feature `kitchen`."}}