{"cell_type":{"9126f984":"code","a8498c07":"code","7f23b09d":"code","d93a345a":"code","db610371":"code","b10beb92":"code","85ac014f":"code","901654e9":"code","e869ff99":"code","7dc185ec":"code","281f23bc":"code","bc32e771":"code","9d8a9783":"code","fdf61ab7":"code","758fd232":"code","0d10d0b2":"code","7379c759":"code","2d28b299":"code","7a171968":"code","79c09d41":"code","43221256":"code","87e66aa4":"code","2a11876c":"code","0b64d887":"code","36356910":"code","f06c187d":"code","026b04ab":"code","4991eae0":"code","ba799a68":"code","36bffc09":"code","1609e6ad":"code","237e6769":"code","f05c7649":"code","858b6747":"code","ae04cc6c":"code","dcc50220":"code","22d6f38e":"code","6ade4c31":"code","a643d11c":"code","645f6def":"code","c159f0cd":"code","ff0055a3":"code","163754d3":"code","06daf033":"code","6a40022e":"code","a121fe79":"code","a4554edc":"code","07ab74c8":"markdown","1323f4d5":"markdown","ff332298":"markdown","9632bab0":"markdown","ba8cf734":"markdown","96593a13":"markdown","7cf588d2":"markdown","8ade5826":"markdown","32ff895d":"markdown","7665ef70":"markdown","28fecfa6":"markdown","3f3785fe":"markdown","ad75b646":"markdown","8205dbb6":"markdown","9a694500":"markdown","624ac552":"markdown","c031de05":"markdown","106905aa":"markdown","32048171":"markdown","83dcd11e":"markdown","35bcfe30":"markdown","8fbd2095":"markdown","ec96056c":"markdown","62f9dc9d":"markdown","bb27481f":"markdown","64f5c9cb":"markdown","894db1f0":"markdown","0ef6f99d":"markdown"},"source":{"9126f984":"# linear algebra\nimport numpy as np \n# data processing\nimport pandas as pd \n#visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a8498c07":"#Create dataframes for train and test set\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","7f23b09d":"#first look at data\ntrain.head()","d93a345a":"#shape of train and test set\nprint(train.shape, test.shape)","db610371":"#overview survival distribution\nsns.countplot(x=train[\"Survived\"])\nplt.show()","b10beb92":"#search for missing values\ntrain.info()\nprint(\"______________________________________\")\ntest.info()","85ac014f":"#distribution \"Age\"\nsns.set_style(\"darkgrid\")\nsns.displot(train[\"Age\"], bins=20)\nplt.show()","901654e9":"#create combined list of train and test set to operate adjustments as a batch\ncombine = [train, test]","e869ff99":"#applying mean to missing values\nfor dataset in combine:\n    dataset[\"Age\"].fillna(dataset[\"Age\"].mean(), inplace=True)","7dc185ec":"#overview \"Cabin\"-feature\ntrain[\"Cabin\"].describe()","281f23bc":"#overview \"Cabin\"-feature\ntrain[\"Cabin\"].values[:40]","bc32e771":"#create new label for \"Cabin\"-values with missing values\nfor dataset in combine:\n    dataset[\"Cabin\"].fillna(\"Regular\", inplace=True)","9d8a9783":"#fill missing \"Fare-values\ntest[\"Fare\"].fillna(test[\"Fare\"].median(), inplace=True)","fdf61ab7":"train.info()\nprint(\"______________________________________\")\ntest.info()","758fd232":"plt.figure(figsize=(12,8))\nsns.histplot(train[\"Age\"], bins=45, kde=True)\nplt.show()","0d10d0b2":"#overview \"Fare\"-distribution\nplt.figure(figsize=(12,8))\nsns.histplot(train[\"Fare\"], bins=45, kde=True)\nplt.show()","7379c759":"#apply log(1+x)\nfor dataset in combine:\n    dataset[\"Age\"] = np.log1p(train[\"Age\"])\n    dataset[\"Fare\"] = np.log1p(train[\"Fare\"])","2d28b299":"#check log-transformed distribution\nplt.figure(figsize=(12,8))\nsns.histplot(train[\"Age\"], bins=30, kde=True)\nplt.show()\nplt.figure(figsize=(12,8))\nsns.histplot(train[\"Fare\"], bins=30, kde=True)\nplt.show()","7a171968":"#merge to \"Familiy_Size\" feature\nfor dataset in combine:\n    dataset[\"Family_Size\"] = dataset[\"SibSp\"] + dataset[\"Parch\"]\n    dataset.drop([\"SibSp\", \"Parch\"], axis=1, inplace=True)","79c09d41":"#Survival distribution \"Familiy_Size\"\ntrain[[\"Family_Size\", 'Survived']].groupby([\"Family_Size\"], as_index=False).mean()","43221256":"#merge to \"Has_Family\" feature\nfor dataset in combine:\n    dataset[\"Has_Family\"] = dataset[\"Family_Size\"].replace([1,2,3,4,5,6,7,10],1)\n    dataset.drop([\"Family_Size\"], axis=1, inplace=True)","87e66aa4":"#Survival distribution \"Has_Family\"\ntrain[[\"Has_Family\",\"Survived\"]].groupby([\"Has_Family\"], as_index=False).mean()","2a11876c":"#Overview \"Name\" feature\ntrain[\"Name\"].values[:15]","0b64d887":"#Extract titles\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","36356910":"#values titles\ntrain.Title.value_counts()","f06c187d":"#sort into fewer categories\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset.drop([\"Name\"], axis=1, inplace=True)","026b04ab":"#survival rate per category\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","4991eae0":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"Embarked\",hue=\"Survived\",data=train)\nplt.show()","ba799a68":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"Pclass\",hue=\"Survived\",data=train)\nplt.show()","36bffc09":"#transform into categorical feature\nfor dataset in combine:\n    dataset[\"Pclass\"] = dataset.Pclass.astype(str).map( {\"1\": \"First\", \"2\": \"Second\", \"3\": \"Third\"} )","1609e6ad":"#unique values\ntrain.Ticket.describe()","237e6769":"#overview \"Ticket\" feature\ntrain.Ticket","f05c7649":"#remove \"Ticket\" feature\nfor dataset in combine:\n    dataset.drop([\"Ticket\"], inplace=True, axis=1)","858b6747":"#overview\ntrain.Cabin.describe()","ae04cc6c":"#unique \"Cabin\" values\ntrain.Cabin.unique()","dcc50220":"#extract capital letter\nfor dataset in combine:\n    dataset[\"Cabin\"] = dataset.Cabin.apply(lambda x: str(x)[0])","22d6f38e":"#map survival rate among cabin types\ntrain[['Cabin', 'Survived']].groupby(['Cabin'], as_index=False).mean().sort_values(by=\"Survived\")","6ade4c31":"#split into Special_Cabin (all cabins but R) and not Special_Cabin (R)\nfor dataset in combine:\n    dataset[\"Special_Cabin\"] = dataset.Cabin.map( {\"A\":1,\"B\":1,\"C\":1,\"D\":1,\"E\":1,\"F\":1,\"G\":1,\"T\":1,\"R\":0} )\n    dataset.drop(\"Cabin\", axis=1, inplace=True)","a643d11c":"#look on feature engineered data\ntrain","645f6def":"from sklearn.preprocessing import MinMaxScaler\n#scaling\nnorm = MinMaxScaler().fit(train[[\"Age\",\"Fare\"]])\ntrain[[\"Age\",\"Fare\"]] = norm.transform(train[[\"Age\",\"Fare\"]])\nnorm = MinMaxScaler().fit(test[[\"Age\",\"Fare\"]])\ntest[[\"Age\",\"Fare\"]] = norm.transform(test[[\"Age\",\"Fare\"]])","c159f0cd":"#dummies\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)","ff0055a3":"#create target (Survived or not)\ntarget = train.pop(\"Survived\")\n#remove \"Id\"-column as it does not matter for now\ntrain_Id = train.pop(\"PassengerId\")\ntest_Id = test.pop(\"PassengerId\")","163754d3":"from sklearn.model_selection import StratifiedKFold\nrandom_state = 2021\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)","06daf033":"xgb_train_preds = np.zeros(len(train.index), )\nxgb_test_preds = np.zeros(len(test.index), )\n\nrf_train_preds = np.zeros(len(train.index), )\nrf_test_preds = np.zeros(len(test.index), )\n\nsvc_train_preds = np.zeros(len(train.index), )\nsvc_test_preds = np.zeros(len(test.index), )\n\nknn_train_preds = np.zeros(len(train.index), )\nknn_test_preds = np.zeros(len(test.index), )","6a40022e":"from xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n    \n    x_train = pd.DataFrame(train.iloc[train_index])\n    x_valid = pd.DataFrame(train.iloc[test_index])\n    \n    rf_model = RandomForestClassifier(bootstrap= True, criterion= 'gini', max_depth= 15, max_features= 10, min_samples_leaf= 3, min_samples_split= 2, n_estimators= 550)\n    \n    rf_model.fit(x_train,y_train,)\n    \n    train_oof_preds = rf_model.predict(x_valid)\n    rf_test_preds = rf_model.predict(test)\n    rf_train_preds[test_index] = train_oof_preds\n    print(\": RF - Score = {}\".format(accuracy_score(y_valid, train_oof_preds)))\n    \n    svc_model = SVC(C= 1, gamma= 0.1, kernel= 'rbf')\n    \n    svc_model.fit(x_train,y_train)\n    \n    train_oof_preds = svc_model.predict(x_valid)\n    test_oof_preds = svc_model.predict(test)\n    svc_train_preds[test_index] = train_oof_preds\n    svc_test_preds = test_oof_preds\n    print(\": SVC - Score = {}\".format(accuracy_score(y_valid, train_oof_preds)))\n    \n    knn_model = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 9, p= 1, weights= 'uniform')\n    \n    knn_model.fit(x_train,y_train)\n    \n    train_oof_preds = knn_model.predict(x_valid)\n    knn_test_preds = knn_model.predict(test)\n    knn_train_preds[test_index] = train_oof_preds\n    print(\": KNN - Score = {}\".format(accuracy_score(y_valid, train_oof_preds)))\n    \n    \n    \nprint(\"--> Overall metrics\")\nprint(\": RF Score = {}\".format(accuracy_score(target, rf_train_preds)))\nprint(\": SVC Score = {}\".format(accuracy_score(target, svc_train_preds)))\nprint(\": KNN Score = {}\".format(accuracy_score(target, knn_train_preds)))\n","a121fe79":"from scipy.special import expit\n\n\nrandom_state = 2021\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pd.DataFrame(data={\n    \"rf\": rf_train_preds.tolist(),\n    \"svc\": svc_train_preds.tolist(),\n    \"knn\": knn_train_preds.tolist(),\n    \"target\": target.tolist()\n})\nl1_test = pd.DataFrame(data={\n    \"rf\": rf_test_preds.tolist(),\n    \"svc\": svc_test_preds.tolist(),\n    \"knn\": knn_test_preds.tolist(),\n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds = np.zeros(len(l1_test.index), )\nfeatures = [\"rf\", \"svc\", \"knn\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.075, max_depth=6, \n                             min_child_weight=1.5, n_estimators=10000,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =7, nthread = -1, eval_metric=\"logloss\", use_label_encoder=False)\n#    model = CalibratedClassifierCV(\n#       RidgeClassifier(random_state=random_state), \n#        cv=10\n#    )\n    model.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_valid, y_valid)], \n    verbose=0,\n    early_stopping_rounds=400\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    test_oof_preds = model.predict(l1_test[features])\n    train_preds[test_index] = train_oof_preds\n    test_preds = test_oof_preds\n    print(\": Stacked Model Score = {}\".format(accuracy_score(y_valid, train_oof_preds)))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": Stacked Model Score = {}\".format(accuracy_score(target, train_preds)))","a4554edc":"submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsubmission[\"Survived\"] = test_preds.tolist()\nsubmission.to_csv(\"submission.csv\", index=False)","07ab74c8":"Creating dummy variables for categroical features.","1323f4d5":"As we have alot of missing \"Cabin\" entries (~75%) we first need to have a look at the Cabin values.","ff332298":"For the missing \"Age\" entries we take the mean value in case age is normally distributed. Seems like it does.","9632bab0":"Split target from our dataset and little adjustments.","ba8cf734":"# 3.5 Pclass\nCheck survival distribution among the 3 diffrent class types. First Class survival rate is outstanding.","96593a13":"For the one missing Fare value in the test set we just take the median value. It should not impact our endresult too much.","7cf588d2":"# 3.6 Ticket\nWhen taking a look at our \"Ticket\" feature we realize that the data has alot of unique values ranging from pure number codes to a mixture of numbers and letters. As finding any correlating between ticket and survival would be too time consuming we elimiate this feature completely.","8ade5826":"# 3.1 Age and Fare\nFor the \"Age\" and \"Fare\" features we have a look at each distribution and log-transform them in case they are skewed. Our algorithms tend to be more accurate processing normal distributed data.","32ff895d":"# 3.7 Cabin\nAs the first letter of a cabin indicates its position we just extract the first letter and create a unique catagory for each letter. As we analyze the correlation between our cabin categories we distinguish between being in a\nregular cabin (R) or being in another cabin type (not R).","7665ef70":"Sclaling numerical features for further model optimazation.","28fecfa6":"# 5. Basic Model + Stacked Model Building\u2699\ufe0f","3f3785fe":"# 4. Data Preprocessing for Model\ud83d\udd27","ad75b646":"# 3.3 Name\nWe extract just the name title of each passanger and use these as a new feature.","8205dbb6":"# 1. First look and thoughts\ud83d\udd0d\n\nAt the beginning we just try to get an basic idea of what the given information looks like for each passanger.","9a694500":"## Titanic - Machine Learning from Disaster\ud83d\udea2\n\n![image.png](attachment:73e51e44-680c-485a-a4cb-cf0869b42ab1.png)\n\nIn this notebook we are going to predict the surivial outcome of each passanger of the catastrophic Titanic maiden voyage.\n\nBest results : 77,99 % accuracy.\n\n# Agenda \ud83d\udcc3\n1. First look\n2. Data Cleaning\n3. Data Exploration and Feature Engineering\n4. Data Preprocessing for Model\n5. Basic Model + Stacked Model Building\n6. Results","624ac552":"# 6. Tuned Model\u2699\ufe0f\nFor our tuned model we use XGBoost.","c031de05":"Before going into the data cleaning part we want to make assumptions about what factors are important for survival. In the course of this notebook we will confirm or disprove these assumptions. I have made the following assumptions:\n* Women first\n* Chilrden first\n* Rich people first\n* Men and crew last","106905aa":"# 2. Data Cleaning\ud83e\uddf9\n\nNow we impute missing data to avoid technical issues.","32048171":"Missing data in training set:\n* 177 \"Age\" entries\n* 687 \"Cabin\" entries\n\nMissing data in test set:\n* 86 \"Age\" entries\n* 1 \"Fare\" entry\n* 327 \"Cabin\" entries","83dcd11e":"At this point I decided to create an extra category for all missing cabin. I assume that the missing cabin values probably build a lower class cabin type.","35bcfe30":"# 3. Data Exploration and Feature Engineering\ud83d\udcca\ud83d\udcd0\nAs our next step we go through each given feature and prepare them for our algorithm the best way possible.\nThese are the features we go thorugh:\n1. Age & Fare\n2. SibSp and Parch\n4. Name\n5. Embarked\n6. Pclass\n7. Ticket\n8. Cabin\n","8fbd2095":"# 3.2 SibSp and Parch\nWe merge the parent and sibling features into one big \"Family Size\" feature which represents the total number of family aboard. Finally we transform the \"Family Size\" feature into a \"Has Family\" feature which indicates if a passanger has family aboard or not.","ec96056c":"Check for missing values after imputing the missing values","62f9dc9d":"# 3.4 Embarked\nCheck the distribution of the \"Embarked Feature\" in case we find any correlatioin. There is a significant increase in survival rate for passangers from Cherbourg (C).","bb27481f":"As both features are not normally distributed we apply log-transformation to both features. ","64f5c9cb":"Out of 204 Cabin values we have 147 unique values. It seems like every cabin has it's own number and letter combination.","894db1f0":"We crossvalidate via StrtifiedKfold with 10 folds in total. For our base line we use RandomForest, SupportVectorClassifier and Nearest-Neighbor-Classification.","0ef6f99d":"# 7. Results\ud83d\udd11\nMake our final submission"}}