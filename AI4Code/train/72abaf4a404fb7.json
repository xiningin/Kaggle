{"cell_type":{"1c3eb370":"code","fabdae53":"code","bfa7205f":"code","50facbd1":"code","4efb806a":"code","b149a97d":"code","913392d1":"code","8e5e9f46":"code","97519767":"code","a7db4220":"markdown","1b551bec":"markdown","7e330bbb":"markdown","8e22cdcf":"markdown","d7dd4597":"markdown","d1108856":"markdown","8dda2041":"markdown","9bab8157":"markdown","f181a0bb":"markdown","c26bae88":"markdown","7f646136":"markdown"},"source":{"1c3eb370":"from kaggle.competitions import twosigmanews\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport gc\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set(font_scale=1)\n\nimport warnings\nimport missingno as msno\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 100)\npd.options.mode.chained_assignment = None\n# dir(pd.options.display)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\nplt.style.use('ggplot')\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","fabdae53":"env = twosigmanews.make_env()","bfa7205f":"(market_train, news_train) = env.get_training_data()\ndel news_train\ngc.enable()\ngc.collect()","50facbd1":"ret = market_train.returnsOpenNextMktres10\nuniv = market_train.universe\nlabel = (ret > 0).astype(int)","4efb806a":"def ir(label, window):\n    global market_train, ret, univ\n    time_idx = market_train.time.factorize()[0]\n    # (label * 2 - 1) : perfect confidence value\n    x_t = (label * 2 - 1) * ret * univ\n    x_t_sum = x_t.groupby(time_idx).sum()\n    x_t_sum = x_t_sum[window:]\n    score = x_t_sum.mean() \/ x_t_sum.std()\n    return score","b149a97d":"ir_l = [ir(label, t) for t in range(0, market_train.time.nunique(), 10)]","913392d1":"trace = go.Scatter(\n    x = np.arange(0, market_train.time.nunique(), 10),\n    y = ir_l,\n    mode = 'lines+markers',\n    marker = dict(\n        size = 4,\n        color = 'lightblue'\n    ),\n    line = dict(\n        width = 1\n    )\n)\ndata = [trace]\nlayout = go.Layout(dict(\n    title = 'Eval Metric trend',\n    xaxis = dict(title = 'operational days passed ( window start point )'),\n    yaxis = dict(title = 'Evaluation metric'),\n    height = 400,\n    width = 750\n))\npy.iplot(dict(data=data, layout=layout), filename='IR trend')","8e5e9f46":"op = ['mean', 'std']\ndf = market_train[['time', 'returnsOpenPrevRaw1']].groupby('time').agg({\n    'returnsOpenPrevRaw1' : op,\n}).reset_index()\ndf.columns = ['time'] + [o + '_returnsOpenPrevRaw1' for o in op]","97519767":"trace = go.Scatter(\n    x = df.time,\n    y = df.std_returnsOpenPrevRaw1,\n    mode = 'lines+markers',\n    marker = dict(\n        size = 4,\n        color = 'pink'\n    ),\n    line = dict(\n        width = 1\n    )\n)\ndata = [trace]\nlayout = go.Layout(dict(\n    title = 'std of returnsOpenPrevRaw1',\n    xaxis = dict(title = 'date'),\n    yaxis = dict(title = 'std of returnsOpenPrevRaw1'),\n    height = 400,\n    width = 750\n))\npy.iplot(dict(data=data, layout=layout), filename='.')","a7db4220":"<font size=4>\nIn this competition, we evaluate our models with following metric.  \n<\/font>  \n  \n$$x_t = \\Sigma_{i} \\; \\hat{y}_{ti} r_{ti} u_{ti}$$  \n  \n$$\\mathrm{score} = \\frac{\\bar{x_t}}{\\sigma(x_t)}$$  \n  \n$r_{ti}$ is the 10-day market-adjusted leading return for day t for instrument i,  \nso this metric is a kind of [Information Ratio](https:\/\/www.investopedia.com\/terms\/i\/informationratio.asp) , I think.  \n  \nI did a naive experiment on this metric and got some insights about validation strategy in this competition.  ","1b551bec":"<font color=lightseagreen size=6><b> Naive Experiment on evaluation metric <\/b><\/font>\n<br>\n<font color=lightseagreen size=5><b> 2 sigma : Using News to Predict Stock Movements <\/b><\/font>","7e330bbb":"We will concentrate on market data.   \nLet's delete news data.","8e22cdcf":"# make 2sigma kaggle env","d7dd4597":"In @jannesklaas awesome [kernel](https:\/\/www.kaggle.com\/jannesklaas\/lb-0-63-xgboost-baseline),   \n> Stocks can only go up or down, if the stock is not going up, it must go down (at least a little bit). So if we know our model confidence in the stock going up, then our new confidence is:\n> $$\\hat{y}=up-(1-up)=2*up-1$$\n> \n> We are left with a \"simple\" binary classification problem, for which there are a number of good tool, here we use XGBoost, but pick your poison.  \n  \nI followed his setting.  \nAddition to it, I assumed that we managed to get the perfect model.  \nIt means that when stocks go up, our prediction is always 1, vice versa.","d1108856":"Define the function to calculate evalution metric.  \nThis function has `window` arg to limit the range of time.  \nWithin this function, we calculate the perfect confidence value.  ","8dda2041":"![hedge](https:\/\/cdn-ak.f.st-hatena.com\/images\/fotolife\/g\/greenwind120170\/20181002\/20181002214157.jpg)","9bab8157":"From above pitcure,   \nit is obvious that in the early stage of this data period ( 2007 ~ 2008 ), score is too low **due to its high volatility** .  \nBelow picture shows the standard deviation of `returnsOpenPrevRaw1` with time.  ","f181a0bb":"<font size=4 color=deeppink>\nMy conclusion is,  \n<\/font>\n<br>\n- Including data within 2007 ~ 2008 to bulid or evaluate model will not be good due to its high volatility.  \n<br>\n- The current financial market is similar to 2009 ~ 2017 rather than 2007 ~ 2008.  \n<br>\n- The possibility of the shock like Lehman or Pariba occured will be very low with the current situation.  \n    ( some rigid law like Basel III and Solvency II will protect the world to some extent. )","c26bae88":"Move by 10 operational days ( ~ 252days \/ year ), calculate scores.  ","7f646136":"# Load packages"}}