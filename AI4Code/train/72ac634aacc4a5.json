{"cell_type":{"25cdfa4e":"code","483a8ffd":"code","a88ec37e":"code","b955c66f":"code","e10ae47d":"code","b4f02e47":"code","9daefbc6":"code","d4970504":"code","a203983f":"code","ac2ab718":"code","096ecc1d":"code","a66b5a2e":"code","9be1d6f9":"code","4eb7b3dd":"code","09725656":"code","0504b509":"code","809dc886":"code","6fd12748":"code","44b7bb95":"code","0c09bb4d":"code","d020db89":"code","081b4fa2":"code","a4b53a41":"markdown","e39ed260":"markdown","8988f4d2":"markdown","43272bba":"markdown","457def28":"markdown","a86520e8":"markdown","81d62631":"markdown","fa424a05":"markdown","fdc3f15d":"markdown","2ab06759":"markdown","cef779ee":"markdown","5b2b87f2":"markdown","d84cd8e7":"markdown","2a9c66c8":"markdown","6437ed74":"markdown","5856ca66":"markdown","ab922347":"markdown"},"source":{"25cdfa4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","483a8ffd":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","a88ec37e":"new1 = train_data.Name.str.split(', ',n = 2, expand = True)\nnew2 = new1[1].str.split('. ',n = 2, expand = True)\nlast_name = new1[0]\ntitle = new2[0]\nname = new2[1]\n\ntrain_data['LastName'] = last_name\ntrain_data['Title'] = title\ntrain_data['Name'] = name\ntrain_data.head()\n\n# delete the rows of corresponding to passengers that do not have an Age entry\n#train_data.dropna(inplace=True)","b955c66f":"print(\"There are \" + str(train_data.Age.isnull().sum()) + \" number of missing age entries\")","e10ae47d":"AgeEstimate = train_data[[\"Age\",\"Title\",\"Sex\"]].groupby([\"Sex\",\"Title\"]).mean()\nprint(AgeEstimate)","b4f02e47":"def fill_age(row):\n    male_roles = {\"Capt\": 70, \"Col\": 58, \"Don\": 40, \"Dr\": 40.6, \"Jonkheer\": 38, \"Major\": 48.5, \"Master\": 4.574167, \"Mr\": 32.368090, \"Rev\": 43.166667, \"Sir\": 49}\n    female_roles = {\"Dr\": 49, \"Lady\": 48, \"Miss\": 21.773973, \"Mlle\": 24, \"Mme\": 24, \"Mrs\": 35.898148, \"Ms\": 28, \"the Countess\": 33}\n    if pd.isnull(row.Age):\n        if row.Sex == \"male\":\n            row.Age = male_roles[row.Title]\n        else:\n            row.Age = female_roles[row.Title]\n    return row\n\ntrain_data = train_data.apply(fill_age, axis = \"columns\")\n\nprint(\"There are now \" + str(train_data.Age.isnull().sum()) + \" number of missing ages.\")","9daefbc6":"train_data[\"FamilySize\"] = train_data.apply(lambda x: x.SibSp + x.Parch +1, axis = \"columns\")\ntrain_data.head()","d4970504":"print(train_data[[\"Sex\",\"Survived\"]].groupby([\"Sex\"], as_index = False).mean())\nprint('\\n')\n\nprint(train_data[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"], as_index = False).mean())\nprint('\\n')\n\nAgeSurvival = train_data[[\"Age\",\"Survived\"]].groupby([\"Age\"], as_index = False).mean()\nprint(\"The correlation between Age and Survival is \" + str(AgeSurvival.corr().Age.Survived))\nprint('\\n')\n\nprint(train_data[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"], as_index = False).mean())\nprint('\\n')\n\nprint(train_data[[\"Parch\",\"Survived\"]].groupby([\"Parch\"], as_index = False).mean())\nprint('\\n')\n\nprint(train_data[[\"FamilySize\",\"Survived\"]].groupby([\"FamilySize\"], as_index = False).mean())\nprint('\\n')\n\nFareSurvival = train_data.groupby([\"Fare\"], as_index = False).mean()\nprint(\"The correlation between Fare and Survival is \" + str(FareSurvival.corr().Fare.Survived))\nprint('\\n')\n\nTitleSurvival = train_data[[\"Title\",\"Survived\"]].groupby([\"Title\"], as_index = False).mean()\nTitleCount = train_data[[\"Title\",\"Survived\"]].groupby([\"Title\"], as_index = False).count().rename(columns={\"Survived\":\"Count\"})\nTitleSurvival = pd.merge(TitleSurvival,TitleCount, on = \"Title\")\nprint(TitleSurvival)","a203983f":"def AgeRange(age):\n    if age <= 10:\n        return \"<=10\"\n    elif age <= 16:\n        return \"(10,16]\"\n    elif age <= 20:        \n        return \"(16,20]\"\n    elif age <= 25:\n        return \"(20,25]\"        \n    elif age <= 30:\n        return \"(25,30]\"\n    elif age <= 40:\n        return \"(30,40]\"\n    elif age <= 50:\n        return \"(40,50]\"\n    elif age <= 60:\n        return \"(50,60]\"\n    else:\n        return \">60\"\n    \ntrain_data[\"AgeRange\"] = train_data.Age.map(AgeRange)\n\nprint(train_data[[\"AgeRange\",\"Survived\"]].groupby([\"AgeRange\"], as_index = False).mean())","ac2ab718":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeatures = [\"Pclass\", \"Sex\", \"Title\", \"Age\",\"FamilySize\",\"LastName\"]\ntrain_data_dummy = pd.get_dummies(train_data[features + [\"Survived\"]])\n\n# delete the rows of corresponding to passengers that have null entries\n#train_data.dropna(inplace=True)\n\nx_total = train_data_dummy.drop([\"Survived\"], axis=1)\ny_total = train_data_dummy[\"Survived\"]\n\nx_train, x_test, y_train, y_test = train_test_split(x_total, y_total, test_size=0.20, random_state=42)","096ecc1d":"def parametric_model(depth, n_estimators, x = x_test, y = y_test):\n    model = RandomForestClassifier(n_estimators = n_estimators, max_depth=depth, random_state=1)\n    model.fit(x_train, y_train)\n    predictions = model.predict(x)\n    return accuracy_score(predictions,y)\n\ndepths = range(1,40)\nn_estimate = range(70,200,10)\n\naccuracy_depth_test = [parametric_model(depth,100) for depth in depths]\naccuracy_depth_train = [parametric_model(depth,100, x_train, y_train) for depth in depths]\n\naccuracy_est_test = [parametric_model(10,estimate) for estimate in n_estimate]\naccuracy_est_train = [parametric_model(10,estimate, x_train, y_train) for estimate in n_estimate]\n\nfigure, (axis0,axis1) = plt.subplots(ncols=2)\nfigure.tight_layout(pad=1)\n\naxis0.plot(depths, accuracy_depth_test,color='blue')\naxis0.plot(depths, accuracy_depth_train,color='green')\naxis0.set_title(\"Accuracy as a function of depth\")\n\naxis1.plot(n_estimate, accuracy_est_test,color='blue')\naxis1.plot(n_estimate,accuracy_est_train,color='green')\naxis1.set_title(\"Accuracy as a function of n_estimate\")\nplt.show()\n","a66b5a2e":"RFCmodel = RandomForestClassifier(n_estimators = 100, max_depth=15, random_state=1)\nRFCmodel.fit(x_train, y_train)\nRFC_predictions = RFCmodel.predict(x_test)\nprint(\"The accuracy of this model is \"+ str(accuracy_score(RFC_predictions, y_test)))","9be1d6f9":"from sklearn.model_selection import GridSearchCV\n\nRandomForestGridSearch = GridSearchCV(estimator = RandomForestClassifier(random_state = 1), \n                                      param_grid = {'n_estimators':[20,200],'max_depth':[1,40]}, scoring='accuracy')\nRandomForestGridSearch.fit(x_train, y_train)\nRFC_predictions = RandomForestGridSearch.predict(x_test)\nprint(\"The accuracy score is \" + str(accuracy_score(RFC_predictions,y_test)))                        ","4eb7b3dd":"from sklearn.model_selection import cross_val_score\n\nprint(\"The accuracy score is \" + str(np.mean(cross_val_score(RandomForestGridSearch, x_total, y_total, scoring='accuracy'))))","09725656":"from sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nSVCModel = make_pipeline(StandardScaler(), SVC(random_state=1))\n\nSVCSearch = GridSearchCV(estimator = SVCModel, param_grid = \n                         {\"svc__C\":[10**x for x in range(-4,1)], \"svc__kernel\": [\"linear\", \"rbf\",\"poly\"], \"svc__gamma\": [\"scale\",\"auto\"]+[10**x for x in range(-4,3)]}, scoring='accuracy')\n\nprint(\"The accuracy score is \" + str(np.mean(cross_val_score(SVCSearch, x_total, y_total, scoring='accuracy'))))","0504b509":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","809dc886":"new1 = test_data.Name.str.split(', ',n = 2, expand = True)\nnew1.head()\nnew2 = new1[1].str.split('. ',n = 2, expand = True)\nlast_name = new1[0]\ntitle = new2[0]\nname = new2[1]\n\ntest_data['LastName'] = last_name\ntest_data['Title'] = title\ntest_data['Name'] = name\ntest_data.head()","6fd12748":"test_data = test_data.apply(fill_age, axis = \"columns\")\n\nprint(\"There are now \" + str(test_data.Age.isnull().sum()) + \" number of missing ages.\")","44b7bb95":"test_data[\"FamilySize\"] = test_data.apply(lambda x: x.SibSp + x.Parch +1, axis = \"columns\")\ntest_data.head()","0c09bb4d":"merged_set = pd.concat([test_data,train_data])\nmerged_set_dummy = pd.get_dummies(merged_set[features+[\"Survived\"]])\n\ntest_data_dummy = merged_set_dummy.loc[pd.isnull(merged_set_dummy.Survived),:]\ntrain_data_dummy = merged_set_dummy.loc[pd.notnull(merged_set_dummy.Survived),:]\n\n# Just to double check that the test_data are exactly those that has a null value for Survived, and similarly for the train_data\nprint(len(test_data_dummy) == len(test_data))\nprint(len(train_data_dummy) == len(train_data))","d020db89":"x_test = test_data_dummy.drop([\"Survived\"], axis=1)\nx_total = train_data_dummy.drop([\"Survived\"], axis=1)\ny_total = train_data_dummy[\"Survived\"]","081b4fa2":"RandomForestGridSearch.fit(x_total,y_total)\nRFCprediction = RandomForestGridSearch.predict(x_test)\nRFCprediction = RFCprediction.astype(int)\nRFCoutput = pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived': RFCprediction})\nRFCoutput.to_csv('RFCsubmission.csv', index=False)\n\nSVCSearch.fit(x_total,y_total)\nSVCprediction = SVCSearch.predict(x_test)\nSVCprediction = SVCprediction.astype(int)\nSVCoutput = pd.DataFrame({'PassengerId':test_data['PassengerId'],'Survived': RFCprediction})\nSVCoutput.to_csv('SVCsubmission.csv', index=False)","a4b53a41":"To be even more correct, we should be using cross validation to evaluate our model. This is so that we have a more accurate picture of our model regardless of how we picked our validation sets.","e39ed260":"In the above computation, we see that age actually has a low correlation to survival. However, as we know, children are much more likely to survive the Titanic. The problem with our analysis before is that we are treating each instance of age separately, when we should be grouping the ages to see a small trend. This is demonstrated in the following: ","8988f4d2":"We will first experiment with Random Forest Classifiers. To figure out what the best choices for the parameters depths and n_estimate, we first plot some examples, while being mindful of overfitting. ","43272bba":"In fact, we can make use of sklearn.model_selection.GridSearchCV","457def28":"**Filling in missing data**\n\nThere are a number of age entries missing, so we will first try to fill these in by guessing them from the rest of the data. ","a86520e8":"We will need to split the name and title column for test_data, as well as fill in the age and family size columns as we did above. ","81d62631":"Since the people in the test dataset and the train dataset may have distinct last names, we want to merge them first, create the dummy, then split them again. We can do the splitting, because the test dataset does not have the survival column, so their values in this column will be null in the merged set. ","fa424a05":"**Machine Learning**\n\nWe pick out the features we want to analyze, and break up our dataset into training sets and validation sets. We put last names in as a feature, because we saw that having more family members is correlated to survival. This means that if a given person has a family member that survived, there is a high that this given person does as well. ","fdc3f15d":"**Analyzing the Data**\n\nFirst we try to figure out which feature has the high correlations to survival. \n\nFrom the computation below, we see that sex, Pclass (the Ticket class) and Family Size have strong correlations to survival, so we will use these features in our training. Meanwhile, fare prices have a low correlation. ","2ab06759":"Now let's try a different model, SVM. But before we apply it, we will standardize the data first. ","cef779ee":"The experiment above suggests that depth = 15, and n_estimate = 100 are good choices. ","5b2b87f2":"**Importing the dataset**","d84cd8e7":"**Preparing the prediction for the test data set**","2a9c66c8":"Split the name column into Name (first and middle names), last name and title","6437ed74":"The guess for a person's age will be the average of the age of others with the same title and gender. ","5856ca66":"We will also create a new column FamilySize which keeps tracks of the number of family members of a particular person. ","ab922347":"It is now time to compute the predictions and save the files. We have to be mindful of storing the output as integers instead of as float. "}}