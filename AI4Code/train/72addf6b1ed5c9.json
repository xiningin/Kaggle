{"cell_type":{"de05ea5e":"code","f2d79103":"code","5ccedcf5":"code","211abf7d":"code","d93f2cca":"code","84f502a9":"code","7e26909d":"code","0aba534d":"code","f0b7cc2a":"code","aee52477":"code","899f8038":"code","87472480":"code","e85439da":"code","9bec94a5":"code","05ea0cec":"code","35521175":"code","1dd13b56":"code","42063ad1":"code","c4296dc0":"code","02cabf48":"code","44742014":"markdown","45d28576":"markdown","5abbb0ed":"markdown","01970db2":"markdown","738a2839":"markdown","3d3d576a":"markdown","f67a2a8c":"markdown","c9dcf547":"markdown","e92e8849":"markdown"},"source":{"de05ea5e":"!pip install -q transformers==3.0.2","f2d79103":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"\n\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom time import time, strftime, gmtime\n\nstart = time()\n#print(start)\n\nimport datetime\nprint(str(datetime.datetime.now()))\n\nprint(tf.version.VERSION)\nprint(transformers.__version__)","5ccedcf5":"train = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ntrain","211abf7d":"test = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')\ntest","d93f2cca":"sub = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')\nsub","84f502a9":"plt.figure(figsize = (10, 10))\nsns.countplot(train['label'])","7e26909d":"lbls, freqs = np.unique(train['language'].values, return_counts = True)\n#print(list(zip(lbls, freqs)))\n\nplt.figure(figsize = (10, 10))\nplt.title('Train')\nplt.pie(freqs, labels = lbls, autopct = '%1.1f%%', shadow = False, startangle = 90)\nplt.show()","0aba534d":"lbls, freqs = np.unique(test['language'].values, return_counts = True)\n#print(list(zip(lbls, freqs)))\n\nplt.figure(figsize = (10, 10))\nplt.title('Test')\nplt.pie(freqs, labels = lbls, autopct = '%1.1f%%', shadow = False, startangle = 90)\nplt.show()","f0b7cc2a":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n\nreplicas = strategy.num_replicas_in_sync\nbatch_size = 8 * replicas\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\nprint('Batch_size: ', batch_size)","aee52477":"model_name = 'jplu\/tf-xlm-roberta-large'\nepochs = 4\nmaxlen = 80\n\nAUTO = tf.data.experimental.AUTOTUNE","899f8038":"tokenizer = AutoTokenizer.from_pretrained(model_name)","87472480":"def display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize = (20, 15), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    #plt.show()","e85439da":"def get_training_dataset(idx, df = train, is_train = True):\n    text = df[['premise', 'hypothesis']].values[idx].tolist()\n    text_enc = tokenizer.batch_encode_plus(\n                            text,\n                            pad_to_max_length = True,\n                            max_length = maxlen\n                        )\n    dataset = tf.data.Dataset.from_tensor_slices((text_enc['input_ids'], df['label'][idx].values))\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2020)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_valid_dataset(idx, df = train, is_train = False):\n    text = df[['premise', 'hypothesis']].values[idx].tolist()\n    text_enc = tokenizer.batch_encode_plus(\n                            text,\n                            pad_to_max_length = True,\n                            max_length = maxlen\n                        )\n    dataset = tf.data.Dataset.from_tensor_slices((text_enc['input_ids'], df['label'][idx].values))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(df = test, is_train = False):\n    text = df[['premise', 'hypothesis']].values.tolist()\n    text_enc = tokenizer.batch_encode_plus(\n                            text,\n                            pad_to_max_length = True,\n                            max_length = maxlen\n                        )\n    dataset = tf.data.Dataset.from_tensor_slices(text_enc['input_ids'])\n    dataset = dataset.batch(batch_size)\n    return dataset","9bec94a5":"def build_model(maxlen, model_name):\n    with strategy.scope():\n        #Load Transformer model\n        base_model = TFAutoModel.from_pretrained(model_name)\n\n        input_word_ids = tf.keras.Input(shape = (maxlen, ), dtype = tf.int32, name = \"input_word_ids\")\n\n        #Encoding the input with the model\n        embedding = base_model(input_word_ids)[0]\n\n        #Extract the token used for classification, which is <s> and pass it to softmax (3 possible labels)\n        out_tokens = embedding[:, 0, :]\n\n        output = tf.keras.layers.Dense(3, activation = 'softmax')(out_tokens)\n\n        model = tf.keras.Model(inputs = input_word_ids, outputs = output)\n\n        model.compile(tf.keras.optimizers.Adam(lr = 1e-5), \n                      loss = 'sparse_categorical_crossentropy', \n                      metrics = ['accuracy'])\n    \n    return model","05ea0cec":"model = build_model(maxlen, model_name)\nmodel.summary()","35521175":"folds = 3\nkf = KFold(n_splits = folds, shuffle = True, random_state = 777)\nmodels = []\nhistories = []\npredictions = np.zeros((test.shape[0], 3))\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(np.arange(train['label'].shape[0]))):\n    print('\\n')\n    print('-'*50)\n    print(f'Training fold {fold + 1}')\n    train_dataset = get_training_dataset(trn_idx, df = train, is_train = True)\n    valid_dataset = get_valid_dataset(val_idx, df = train, is_train = False)\n    K.clear_session()\n    model = build_model(maxlen, model_name)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n                'XLM-R_fold-%i.h5'%fold, monitor = 'val_loss', verbose = 1, save_best_only = True,\n                save_weights_only = True, mode = 'min', save_freq = 'epoch'\n                )\n    print('Model Training.....')\n    STEPS_PER_EPOCH = len(trn_idx) \/\/ batch_size\n    history = model.fit(\n                train_dataset, epochs = epochs, verbose = 1, \n                steps_per_epoch = STEPS_PER_EPOCH,\n                batch_size = batch_size, \n                validation_data = valid_dataset\n            )\n    \n    display_training_curves(\n                history.history['loss'], \n                history.history['val_loss'], \n                'loss', 311\n                )\n    display_training_curves(\n                history.history['accuracy'], \n                history.history['val_accuracy'], \n                'accuracy', 312\n                )\n    histories.append(history)\n    models.append(model)\n    print('Prediting on test data..')\n    test_dataset = get_test_dataset(test, is_train = False)\n    pred = model.predict(test_dataset, verbose = 1)\n    \n    predictions += pred \/ folds\n    \n    del history, train_dataset, valid_dataset, model\n    gc.collect()\nprint('\\n')\nprint('-'*50)","1dd13b56":"sub['prediction'] = np.argmax(predictions, axis = 1)","42063ad1":"sub.to_csv('.\/submission.csv', index = False)\nsub","c4296dc0":"plt.figure(figsize = (10, 10))\nsns.countplot(sub['prediction'])","02cabf48":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","44742014":"__Competition Challenge__\n\nCreate an NLI model that assigns labels of 0, 1, or 2 (corresponding to entailment, neutral, and contradiction) to pairs of premises and hypotheses in the testset.","45d28576":"__TPU Config__","5abbb0ed":"__Competition Rules__\n\n- Notebook competition\n- CPU, GPU, or TPU all allowed\n- Submission file name: *sumbmission.csv*","01970db2":"__Language Distribution__","738a2839":"__Target Label Distribution__","3d3d576a":"__Competition Metric__\n\n*Accuracy*","f67a2a8c":"__Predicting on testset__","c9dcf547":"__Creating TF Dataset__","e92e8849":"__Encoding text data using tokenizer__"}}