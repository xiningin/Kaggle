{"cell_type":{"8bf8d583":"code","38ad5f66":"code","d2432cdc":"code","54f7f352":"code","ca3fb3de":"code","af1935c0":"code","cae64453":"code","398b1351":"code","e652c676":"code","6a530434":"code","1be30c1d":"code","6088cb26":"code","50d50f12":"code","642389ee":"code","596711cf":"code","8a54cfbb":"code","607304b9":"code","1284b669":"code","45b09915":"code","5e33c485":"code","e5aadd1e":"code","b59df2fe":"code","bf4f925f":"code","ce57e9ee":"code","c42f2123":"code","71e0b328":"code","a45cdccc":"code","66da13d5":"code","27f1e689":"code","d771f5e0":"code","05e50109":"code","ce2f020f":"code","210e6403":"code","9c24810f":"code","db5d6231":"code","2b85d78d":"code","4c5ecf38":"code","807380e3":"code","5f40ca34":"markdown","d9cb414d":"markdown","94e279e4":"markdown","84af9d1a":"markdown","59e5ce57":"markdown","148616b6":"markdown","28e18462":"markdown","ffb3716d":"markdown","d46c70ba":"markdown","4be7f365":"markdown","8d107c61":"markdown","49418ba2":"markdown","4058d0d1":"markdown","b81e8529":"markdown","9e9b47cb":"markdown","ec19b520":"markdown","19b18166":"markdown","cdd95258":"markdown","7916efda":"markdown","7094d2e1":"markdown","3a964856":"markdown","551f2542":"markdown","f87b2d44":"markdown","8cfe1141":"markdown","66ccb420":"markdown","b0ac4b7c":"markdown","0c9077fc":"markdown","ef1d9244":"markdown","cd8cce5c":"markdown"},"source":{"8bf8d583":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split \n\nimport tensorflow_addons as tfa\nfrom keras.utils import to_categorical, Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Activation\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import ResNet50\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","38ad5f66":"path = '\/kaggle\/input\/yoga-poses-dataset\/DATASET\/'\nos.listdir(path)","d2432cdc":"def plot_examples(category = 'downdog'):\n    \"\"\" Plot 5 images of a given category \"\"\"\n    \n    fig, axs = plt.subplots(1, 5, figsize=(25, 20))\n    fig.subplots_adjust(hspace = .1, wspace=.1)\n    axs = axs.ravel()\n    for i in range(5):\n        img = cv2.imread(path+'TRAIN\/'+category+'\/'+os.listdir(path+'TRAIN\/'+category+'\/')[i])\n        axs[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        axs[i].set_title('shape:'+str(img.shape))\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n    plt.show()","54f7f352":"print('Categories train:', os.listdir(path+'TRAIN'))\nprint('Categories test:', os.listdir(path+'TEST'))\n\ncategories = os.listdir(path+'TRAIN')","ca3fb3de":"len(os.listdir(path+'TRAIN\/'+'downdog\/'))\nimg = cv2.imread(path+'TRAIN\/'+'downdog\/'+os.listdir(path+'TRAIN\/'+'downdog\/')[0])\nprint('Image shape:', img.shape)","af1935c0":"fig, axs = plt.subplots(1, 1, figsize=(7, 7))\naxs.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\naxs.set_xticklabels([])\naxs.set_yticklabels([])\nplt.show()","cae64453":"plot_examples(category = 'downdog')","398b1351":"plot_examples(category = 'tree')","e652c676":"plot_examples(category = 'plank')","6a530434":"plot_examples(category = 'warrior2')","1be30c1d":"plot_examples(category = 'goddess')","6088cb26":"image = cv2.imread(path+'TRAIN\/'+'downdog\/'+os.listdir(path+'TRAIN\/'+'downdog\/')[4])\n#image = cv2.imread(path+'TRAIN\/'+'tree\/'+os.listdir(path+'TRAIN\/'+'tree\/')[0])","50d50f12":"fig, axs = plt.subplots(1, 1, figsize=(7, 7))\naxs.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\naxs.set_xticklabels([])\naxs.set_yticklabels([])\naxs.set_title('shape:'+str(image.shape))\nplt.show()","642389ee":"image_size = 345","596711cf":"image_compress = cv2.resize(image, (image_size, image_size))\nfig, axs = plt.subplots(1, 1, figsize=(7, 7))\naxs.imshow(cv2.cvtColor(image_compress, cv2.COLOR_BGR2RGB))\naxs.set_xticklabels([])\naxs.set_yticklabels([])\naxs.set_title('shape:'+str(image_compress.shape))\nplt.show()","8a54cfbb":"# Crop Image\nmid_row = int(image.shape[0]\/2)\nmid_col = int(image.shape[1]\/2)\nif image.shape[0]>image.shape[1]:\n    image_cropped = image[mid_row-mid_col:mid_row+mid_col,\n                          0:image.shape[1]]\nelse:\n    image_cropped = image[0:image.shape[0],\n                          mid_col-mid_row:mid_col+mid_row]\n    \n# Rescale Image\nimage_rescale = cv2.resize(image_cropped,\n                            dsize=(image_size, image_size),\n                            interpolation=cv2.INTER_AREA)","607304b9":"fig, axs = plt.subplots(1, 1, figsize=(7, 7))\naxs.imshow(cv2.cvtColor(image_rescale, cv2.COLOR_BGR2RGB))\naxs.set_xticklabels([])\naxs.set_yticklabels([])\naxs.set_title('shape:'+str(image_rescale.shape))\nplt.show()","1284b669":"if image.shape[0]>image.shape[1]:\n    image_filled = 255*np.ones([image.shape[0], image.shape[0], image.shape[2]], np.uint8)\n    start_col = int((image.shape[0]-image.shape[1])\/2)\n    image_filled[:,start_col:start_col+image.shape[1]] = image\nelse:\n    image_filled = 255*np.ones([image.shape[1], image.shape[1], image.shape[2]], np.uint8)\n    start_row = int((image.shape[1]-image.shape[0])\/2)\n    image_filled[start_row:start_row+image.shape[0]] = image\n\nimage_rescale = cv2.resize(image_filled,\n                            dsize=(image_size, image_size),\n                            interpolation=cv2.INTER_AREA)","45b09915":"fig, axs = plt.subplots(1, 1, figsize=(7, 7))\naxs.imshow(cv2.cvtColor(image_rescale, cv2.COLOR_BGR2RGB))\naxs.set_xticklabels([])\naxs.set_yticklabels([])\naxs.set_title('shape:'+str(image_rescale.shape))\nplt.show()","5e33c485":"train_data = pd.DataFrame()\ntest_data = pd.DataFrame()\n\nfor category in categories:\n    # Train Data\n    temp = pd.DataFrame(os.listdir(path+'TRAIN\/'+category+'\/'), columns=['image'])\n    temp['category'] = category\n    train_data = pd.concat([train_data, temp], ignore_index=True)\n    \n    # Test Data\n    temp = pd.DataFrame(os.listdir(path+'TEST\/'+category+'\/'), columns=['image'])\n    temp['category'] = category\n    test_data = pd.concat([test_data, temp], ignore_index=True)","e5aadd1e":"train_data['category'].value_counts()","b59df2fe":"test_data['category'].value_counts()","bf4f925f":"labels_dict = dict(zip(categories, range(5)))\nlabels_dict","ce57e9ee":"train_data = train_data.sample(frac = 1)\ntrain_data.index = range(len(train_data))","c42f2123":"train_data.head()","71e0b328":"batch_size = 32\nimg_size = 256\nimg_channel = 3\nnum_classes = 5","a45cdccc":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, labels, batch_size, img_size, img_channel, num_classes):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.img_channel = img_channel\n        self.num_classes = num_classes\n        self.indexes = np.arange(len(self.list_IDs))\n        \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_*self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        return X, y\n            \n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, self.img_size, self.img_size, self.img_channel))\n        y = np.zeros((self.batch_size, self.num_classes), dtype=int)\n        for i, ID in enumerate(list_IDs_temp):\n            row = self.list_IDs[self.list_IDs==ID].index[0]\n            img = cv2.imread(self.path+self.labels[row]+'\/'+ID)\n            # Compress the image\n            img = cv2.resize(img, (self.img_size, self.img_size))\n            X[i, ] = img\/255\n            y[i, ] = to_categorical(labels_dict[self.labels[row]], num_classes=self.num_classes)\n        return X, y","66da13d5":"train_generator = DataGenerator(path+'TRAIN\/', train_data['image'], train_data['category'],\n                                batch_size, img_size, img_channel, num_classes)\n\ntest_generator = DataGenerator(path+'TEST\/', test_data['image'], test_data['category'],\n                               batch_size, img_size, img_channel, num_classes)","27f1e689":"weights='..\/input\/models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\nconv_base = ResNet50(weights=weights,\n                     include_top=False,\n                     input_shape=(img_size, img_size, img_channel))\nconv_base.trainable = True","d771f5e0":"epochs = 10","05e50109":"model = Sequential()\nmodel.add(conv_base)\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='sigmoid'))\n\nmodel.compile(optimizer = RMSprop(lr=1e-5),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()","ce2f020f":"history = model.fit_generator(generator=train_generator,\n                              validation_data=test_generator,\n                              epochs = epochs)","210e6403":"fig, axs = plt.subplots(1, 2, figsize=(20, 6))\nfig.subplots_adjust(hspace = .2, wspace=.2)\naxs = axs.ravel()\nloss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\naxs[0].plot(epochs, loss, 'bo', label='loss_train')\naxs[0].plot(epochs, loss_val, 'ro', label='loss_val')\naxs[0].set_title('Value of the loss function')\naxs[0].set_xlabel('epochs')\naxs[0].set_ylabel('value of the loss function')\naxs[0].legend()\naxs[0].grid()\nacc = history.history['accuracy']\nacc_val = history.history['val_accuracy']\naxs[1].plot(epochs, acc, 'bo', label='accuracy_train')\naxs[1].plot(epochs, acc_val, 'ro', label='accuracy_val')\naxs[1].set_title('Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Value of accuracy')\naxs[1].legend()\naxs[1].grid()\nplt.show()","9c24810f":"predict = model.predict_generator(test_generator, verbose=1)","db5d6231":"test_data['predict'] = predict.argmax(axis=1)[0:len(test_data)]\ndict_rename = {v : k for k, v in labels_dict.items()}\ntest_data = test_data.replace({\"predict\": dict_rename})","2b85d78d":"test_data['correct'] = np.where(test_data['category']==test_data['predict'], 1, 0)","4c5ecf38":"100*test_data['correct'].sum()\/len(test_data)","807380e3":"test_data[test_data['correct']==0]['predict'].value_counts()","5f40ca34":"# Analyse Training","d9cb414d":"# Load Pretrained Model","94e279e4":"# Shuffle Data","84af9d1a":"Now we crop the image. We assume that the poses are in the center of the image.","59e5ce57":"The train data are not evenly distributed:","148616b6":"## Plank","28e18462":"## Tree","ffb3716d":"# Load Image\nFrist we demonstrate how to load a single image from folder.","d46c70ba":"This is a quadratic image of size 550 with 3 channels.","4be7f365":"# Path\nAs we can see, there are two subolders. One folder for the train images and one for testing.","8d107c61":"Use the generator to define train and test data:","49418ba2":"# Libraries","4058d0d1":"# Overview\nTo get a general overview we look on the data structure of the train and test folder. In both folders there are folder with the same names. These subfolderes, labeled with the name of the underlying category, contains the images. \nThe categories are:\n* [downdog](https:\/\/en.wikipedia.org\/wiki\/Downward_Dog_Pose)\n* [tree](https:\/\/en.wikipedia.org\/wiki\/Vriksasana)\n* [plank](https:\/\/en.wikipedia.org\/wiki\/Plank_(exercise))\n* [warrior2](https:\/\/en.wikipedia.org\/wiki\/Virabhadrasana)\n* [goddess](https:\/\/it.qiq.wiki\/wiki\/Utkatasana)\n\nIt might be helpful to study the backround of the data. Sometimes domain knowledge will help you to improve your predictions. Click on the categories to get more informations.","b81e8529":"## Downdog","9e9b47cb":"The target is to get a quadratic image of size 345.","ec19b520":"Finally we fill up the image with white pixel:","19b18166":"# Exploratory Data Analysis\nAs we can see on the example plots, the images doesn't have the same size and or format. Some of them are quadric, in landscape or in portrait format. Later we have to define a neural network based on a fixed image size and format. Therefore we have to resize all images. The question is how to do this?\n\nWe will use a quadratic format for our neural network. Now the question is how to deal with non-quadratic images. \n\nShould we:\n* stretch or compress the images?\n* crop the images?\n* fill up the images?\n\nOf course this decision might have an impact of the prediction score.\n\nLet us focus on the following example:","cdd95258":"## Goddess","7916efda":"# Data Generator\nTo load the data on demand we define da data generator.","7094d2e1":"# Functions\nWe define some helper functions. For example for visualization.","3a964856":"The test data are not evenly distributed:","551f2542":"# Intro\nWelcome to the [yoga poses](https:\/\/www.kaggle.com\/niharika41298\/yoga-poses-dataset) dataset.\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/920599\/1559111\/f7ae9af9c44cf6c946ba66e9b5beeef8\/dataset-cover.jpg)\n\nWe consider this notebook as a tutorial and focus stepwise the process from data to prediction results. \n\n<font size=\"4\"><span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span><\/font>","f87b2d44":"First we compress the image:","8cfe1141":"## Warrior2","66ccb420":"# Define Model","b0ac4b7c":"# Prepare Data\nWe create a list with all train and test data for a more comfortable processing.","0c9077fc":"# Predict Test","ef1d9244":"It is on you now to decide.","cd8cce5c":"# Plot Examples\nWe plot now 5 images of every category."}}