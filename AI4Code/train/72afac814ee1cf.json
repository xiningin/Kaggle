{"cell_type":{"5a4e1a80":"code","3953db64":"code","5f9ee89c":"code","dd078c19":"code","c5895797":"code","6f724fde":"code","55c81833":"code","2fb7b2b9":"code","12027554":"code","e928d278":"code","c44c6c5f":"code","35f9348a":"code","4073b694":"code","1c1eefe0":"code","387e95ce":"code","a6f753c7":"code","57585c00":"code","67d3815c":"code","0ee53a24":"code","56f4ed44":"markdown","0015116f":"markdown","877bd1d9":"markdown","f2af6097":"markdown","0fab820d":"markdown"},"source":{"5a4e1a80":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3953db64":"df=pd.read_csv('..\/input\/data.csv')\ndf.head()","5f9ee89c":"df.drop('date',axis=1,inplace=True)","dd078c19":"df.describe()","c5895797":"df.corr()","6f724fde":"correlation = df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')\n\nplt.title('Correlation between different fearures')","55c81833":"X = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values","2fb7b2b9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","12027554":"# Scale the data to be between -1 and 1\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX=scaler.fit_transform(X)\nX","e928d278":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit_transform(X)","c44c6c5f":"pca.get_covariance()","35f9348a":"explained_variance=pca.explained_variance_ratio_\nexplained_variance","4073b694":"pca=PCA(n_components=5)\nX_new=pca.fit_transform(X)\nX_new","1c1eefe0":"pca.get_covariance()","387e95ce":"explained_variance=pca.explained_variance_ratio_\nexplained_variance","a6f753c7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train.shape","57585c00":"# Establish model\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()","67d3815c":"estimators = np.arange(10, 200, 10)\nscores = []\nfor n in estimators:\n    model.set_params(n_estimators=n)\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\nprint(scores)    ","0ee53a24":"plt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","56f4ed44":"# Principal Component  Analysis","0015116f":"### Objectives of principal component analysis:-\n### \u2022 PCA reduces attribute space from a larger number of variables to a smaller number of factors and as such is a \"non-dependent\" procedure (that is, it does not assume a dependent variable is specified)\n### \u2022 PCA is a dimensionality reduction or data compression method. The goal is dimension reduction and there is no guarantee that the dimensions are interpretable (a fact often not appreciated by (amateur) statisticians).\n### \u2022To select a subset of variables from a larger set, based on which original variables have the highest correlations with the principal component.","877bd1d9":"### Introduction:-\n### \u2022  Principal Component Analysis (PCA) is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set.\n### \u2022 Principal component analysis (PCA) is a mathematical procedure that transforms a number of (possibly) correlated variables into a (smaller) number of uncorrelated variables called principal components. \n### \u2022 It is a method that uses simple matrix operations from linear algebra and statistics to calculate a projection of the original data into the same number or fewer dimensions.\n### Transforming the variables to a new set of variables, which are known as the principal components and are orthogonal, ordered such that the retention of variation present in the original variables decreases as we move down in the order.\n###  The 1st principal component retains maximum variation that was present in the original components\n### Importantly, the dataset on which PCA technique is to be used must be scaled. The results are also sensitive to the relative scaling.It is a method of summarizing data.","f2af6097":"### WORKING:- \n### PCA seeks a linear combination of variables such that the maximum variance is extracted from the variables. It then removes this variance and seeks a second linear combination which explains the maximum proportion of the remaining variance, and so on. This is called the principal axis method and results in orthogonal (uncorrelated) factors. PCA analyzes total (common and unique) variance.","0fab820d":"### IMPORTANT TERMINOLOGIES:-\n### Dimensionality: It is the number of random variables in a dataset or simply the number of features, or rather more simply, the number of columns present in your dataset.\n### Correlation: It shows how strongly two variable are related to each other. The value of the same ranges for -1 to +1. Positive indicates that when one variable increases, the other increases as well, while negative indicates the other decreases on increasing the former. And the modulus value of indicates the strength of relation.\n### Orthogonal: Uncorrelated to each other, i.e., correlation between any pair of variables is 0.\n### Covariance Matrix: This matrix consists of the covariances between the pairs of variables. The (i,j)th element is the covariance between i-th and j-th variable. A covariance matrix is a matrix whose element in the i, j position is the covariance between the i-th and j-th elements of a random vector. A random vector is a random variable with multiple dimensions"}}