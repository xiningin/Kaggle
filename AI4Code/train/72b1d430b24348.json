{"cell_type":{"1dc03974":"code","8a3ca415":"code","8951d736":"code","a249a482":"code","84e07c7f":"code","f7c373fd":"code","157c19b5":"code","47e8a6bf":"code","883e3151":"code","3ea31054":"code","6d84cc85":"code","4e2e3805":"code","73d99d77":"code","3a156327":"code","70bdf77b":"code","83e9a675":"code","63690036":"code","5210e277":"code","3ecb628d":"code","2d907280":"code","e05fc0b6":"code","389dc022":"code","83f66efb":"code","5b499bf4":"code","f0780455":"code","d522abb6":"code","ff1c06ac":"code","6abc641e":"code","d512cb3e":"code","61d92945":"code","5fe52f67":"code","8a77583c":"code","a5d6132b":"code","2f23a90e":"code","86f565d3":"code","55964be6":"code","ffda580c":"code","640e8666":"code","1f4cd339":"code","45b5e2f8":"code","d33fe5da":"code","fa1dead7":"code","188bc3c6":"code","8c2b1c89":"code","6368b3e6":"code","3c51887a":"code","0e99108f":"code","53a345d9":"code","0f0c6cf6":"code","47707ec4":"code","d7f72d6c":"code","6a2c1d23":"code","e71356e5":"code","fae7a03a":"code","b5b1b71f":"code","db858c10":"code","ccc4535b":"code","131d43fd":"code","7ea52440":"code","892cdd3a":"code","a0380320":"markdown","b22bf010":"markdown","027d7bb3":"markdown","332d1a26":"markdown","dccae747":"markdown","f6419597":"markdown","d7e399dc":"markdown","9d150dba":"markdown","f4a032d0":"markdown","7cdd52da":"markdown","a390ec8d":"markdown","c7175771":"markdown","50dda92b":"markdown","1af546c9":"markdown","ae98d859":"markdown","d7d31fe7":"markdown","8822846d":"markdown","7ef377e9":"markdown","1ee2a65a":"markdown","e377ac2d":"markdown","f1e3fda0":"markdown","578a7123":"markdown","8bc54511":"markdown","76df3c9a":"markdown","1a3c7475":"markdown","ed10d452":"markdown","14e2979f":"markdown","1524a630":"markdown","66f68983":"markdown","269d4150":"markdown","07c22226":"markdown","f3b6bef1":"markdown","b06295fd":"markdown"},"source":{"1dc03974":"#importing numpy and pandas\nimport numpy as np\nimport pandas as pd\n\n#importing for visualization\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nr_s=123456","8a3ca415":"#importing dataset\ndf = pd.read_csv('..\/input\/data.csv')","8951d736":"df.head()","a249a482":"df.shape","84e07c7f":"#Structural analysis\ndf.info()","f7c373fd":"#statistical analysis\ndf.describe()","157c19b5":"df.isnull().values.any()","47e8a6bf":"df.corr()","883e3151":"#Dropping the 'Unnamed: 0' column\ndf.drop('Unnamed: 0',inplace=True,axis=1)","3ea31054":"df.head()","6d84cc85":"#target variable\n#df['y'] = df['y'].apply(lambda x: 1 if x == 1 else 0)\ny=df['y']\n","4e2e3805":"y = y.apply(lambda x: 1 if x == 1 else 0)\ny.unique()\n","73d99d77":"#dropping the y column\nX=df.drop('y',axis=1)\nX.head()","3a156327":"#SCALING\nfrom sklearn.preprocessing import StandardScaler\nscale=StandardScaler().fit(X)\nx_scaled=scale.transform(X)\nx_scaled","70bdf77b":"#PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)  #two component\nprincipalComponents = pca.fit_transform(x_scaled)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])","83e9a675":"principalDf.head()","63690036":"#Concating Principal components and y varaible\nPCAdf= pd.concat([principalDf, y], axis = 1)\nPCAdf.head()\n","5210e277":"PCAdf.shape","3ecb628d":"from sklearn.manifold import TSNE\n\nTNSEdf = TSNE(random_state=r_s).fit_transform(x_scaled)\nTNSEdf.shape","2d907280":"tnsedf = pd.DataFrame(TNSEdf)","e05fc0b6":"tnsedf['y']=y","389dc022":"tnsedf.head()","83f66efb":"#K-Means\nfrom sklearn.cluster import KMeans\n\n# Number of clusters\nkmeans = KMeans(n_clusters=5)\n\n# Centroid values\n#centroids = kmeans.cluster_centers_","5b499bf4":"#implementing kmeans in pcadf\nkmeansPCA = kmeans.fit(principalDf)\n#Getting the cluster labels\nlabels = kmeans.predict(principalDf)","f0780455":"PK_df= PCAdf","d522abb6":"PK_df['kclust']=labels","ff1c06ac":"PK_df['kclust'].unique()","6abc641e":"#K-Means\nfrom sklearn.cluster import KMeans\n# Number of clusters\nkmeans = KMeans(n_clusters=5)\n#implementing kmeans in tnsedf\nkmeanstnse = kmeans.fit(TNSEdf)\n#Getting the cluster labels\nk_labels = kmeans.predict(TNSEdf)","d512cb3e":"TK_df = tnsedf","61d92945":"TK_df['cluster']=k_labels","5fe52f67":"TK_df.info()","8a77583c":"TK_df['cluster'].unique()","a5d6132b":"#splitting into x and y varialbles\n'''\np_x=PCAdf.loc[:,['principal component 1','principal component 2']]\np_y=PCAdf ['y']\n\nt_x=tnsedf.loc[:,[0,1]]\nt_y=tnsedf['y']\n'''\n\npk_x=PK_df.loc[:,['principal component 1','principal component 2','kclust']]\npk_y=PK_df['y']\n\ntk_x=TK_df.loc[:,[0,1,'cluster']]\ntk_y=TK_df['y']","2f23a90e":"from sklearn.model_selection import train_test_split\n'''\n#only PCA\npx_train, px_test, py_train, py_test = train_test_split(p_x,p_y,\n                                                    test_size = 0.2, \n                                                    random_state = 101)'''\n\n#PCA +k-means\npkx_train, pkx_test, pky_train, pky_test = train_test_split(pk_x,pk_y,\n                                                            test_size = 0.2, \n                                                            random_state = 101)\n\n'''\n#T-sne only\ntx_train, tx_test, ty_train, ty_test = train_test_split(t_x,t_y,\n                                                        test_size = 0.2, \n                                                        random_state = 101)'''\n\n#T-SNE + k-means\ntkx_train, tkx_test, tky_train, tky_test = train_test_split(tk_x,tk_y,\n                                                            test_size = 0.2, \n                                                            random_state = 101)","86f565d3":"#Confusion matrix and accuraccy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n#Cross validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","55964be6":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression() \n\nlogmodel.fit(pkx_train,pky_train)\npklogpred = logmodel.predict(pkx_test)\n\n\nlogmodel.fit(tkx_train,tky_train)\ntklogpred = logmodel.predict(tkx_test)","ffda580c":"print(confusion_matrix(pky_test, pklogpred))\nprint(round(accuracy_score(pky_test, pklogpred),2)*100)","640e8666":"print(confusion_matrix(tky_test, tklogpred))\nprint(round(accuracy_score(tky_test, tklogpred),2)*100)","1f4cd339":"#cross validation\nLOGCV = (cross_val_score(logmodel, tkx_train, tky_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","45b5e2f8":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\nknn.fit(pkx_train, pky_train)\npknnpred = knn.predict(pkx_test)\n\nknn.fit(tkx_train,tky_train)\ntknnpred = knn.predict(tkx_test)\n","d33fe5da":"print(confusion_matrix(pky_test, pknnpred))\nprint(round(accuracy_score(pky_test, pknnpred),2)*100)","fa1dead7":"print(confusion_matrix(tky_test, tknnpred))\nprint(round(accuracy_score(tky_test, tknnpred),2)*100)","188bc3c6":"KNNCV = (cross_val_score(knn, tkx_train, tky_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","8c2b1c89":"from sklearn.svm import SVC\nsvc= SVC(kernel = 'sigmoid')\n#There are various kernels linear,rbf\nsvc.fit(pkx_train, pky_train)\npspred = svc.predict(pkx_test)\n\nsvc.fit(tkx_train, tky_train)\ntspred = svc.predict(tkx_test)\n","6368b3e6":"\nprint(confusion_matrix(pky_test, pspred))\nprint(round(accuracy_score(pky_test, pspred),2)*100)","3c51887a":"print(confusion_matrix(tky_test, tspred))\nprint(round(accuracy_score(tky_test, tspred),2)*100)","0e99108f":"SVCCV = (cross_val_score(svc, pkx_train, pky_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","53a345d9":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 200)#criterion = entopy,gini\n\nrfc.fit(pkx_train, pky_train)\nprpred = rfc.predict(pkx_test)\n\nrfc.fit(tkx_train, tky_train)\ntrpred = rfc.predict(tkx_test)\n","0f0c6cf6":"print(confusion_matrix(pky_test, prpred))\nprint(round(accuracy_score(pky_test, prpred),2)*100)","47707ec4":"print(confusion_matrix(tky_test, trpred))\nprint(round(accuracy_score(tky_test, trpred),2)*100)","d7f72d6c":"#Cross validation\nRFCCV = (cross_val_score(rfc, tkx_train, tky_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","6a2c1d23":"from sklearn.naive_bayes import GaussianNB\ngaussiannb= GaussianNB()\n\ngaussiannb.fit(pkx_train, pky_train)\npgpred = gaussiannb.predict(pkx_test)\n\ngaussiannb.fit(tkx_train, tky_train)\ntgpred = gaussiannb.predict(tkx_test)","e71356e5":"print(confusion_matrix(pky_test, pgpred))\nprint(round(accuracy_score(pky_test, pgpred),2)*100)","fae7a03a":"print(confusion_matrix(tky_test, tgpred))\nprint(round(accuracy_score(tky_test, tgpred),2)*100)","b5b1b71f":"NBCV = (cross_val_score(gaussiannb, pkx_train, pky_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","db858c10":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nxgb = XGBClassifier()\n\nxgb.fit(pkx_train, pky_train)\npxpred = xgb.predict(pkx_test)\n\nxgb.fit(tkx_train, tky_train)\ntxpred = xgb.predict(tkx_test)","ccc4535b":"print(confusion_matrix(pky_test, pxpred))\nprint(round(accuracy_score(pky_test, pxpred),2)*100)","131d43fd":"print(confusion_matrix(tky_test, txpred))\nprint(round(accuracy_score(tky_test, txpred),2)*100)","7ea52440":"XGCV = (cross_val_score(xgb, pkx_train, pky_train, cv=k_fold, n_jobs=1, scoring = 'accuracy').mean())","892cdd3a":"models = pd.DataFrame({\n                'Models': ['Random Forest Classifier', 'Support Vector Machine',\n                           'K-Near Neighbors', 'Logistic Model', 'Gausian NB', 'XGBoost'],\n                'Score':  [RFCCV, SVCCV, KNNCV, LOGCV, NBCV, XGCV]})\n\nmodels.sort_values(by='Score', ascending=False)","a0380320":"From the above cross validation scores we get that random forest gives the high performance for this dataset.","b22bf010":"log(PCA + K-Means) gives the higher accuracy of 90.0","027d7bb3":"# Naive Bayes","332d1a26":"# Performance comparison","dccae747":"##http:\/\/archive.ics.uci.edu\/ml\/datasets\/Epileptic+Seizure+Recognition \nFor dataset : UCI link","f6419597":"SVM(PCA + k-means) gives the accuracy of 72.0","d7e399dc":"# XGboost","9d150dba":"for more information on this algorithm\nhttps:\/\/www.oreilly.com\/learning\/an-illustrated-introduction-to-the-t-sne-algorithm","f4a032d0":"# K-MEANS","7cdd52da":"# Dimensionality Reduction","a390ec8d":"There are several metrics can be used to tell how good the model performance is.(EX: ROC,AUC,SENSIVITY,MISCLASSIFICATION ERROR,R^2,ADJ R^2,ETC.,.)\n\nBut here we take only confusion matrix and Accuracy score.\n","c7175771":"KNN(T-sne + k-means) gives the higher accuracy of 94.0","50dda92b":"NB(PCA + k-means) gives as higher accuracy of 94.0","1af546c9":"# Implementing kmeans in pca","ae98d859":"# Splitting the data into train and test","d7d31fe7":"So, now we have four dataframes,\n\n1.PCAdf    => PCA only.\n\n2.tnsedf   => T-SNE only.\n\n3.PK_df    => PCA + KMeans.\n\n4.TK_df    => T-SNE + Kmeans\n\nbut for simplicity only two dataframes are used to compare the accuracies","8822846d":"XG(T-sne + k-means) gives as higher accuracy of 95.0","7ef377e9":"RF(T-sne + k-Means) gives the higher accuracy of  96.0","1ee2a65a":"# Analysis","e377ac2d":"This is only the conlusion for the models built here , still the performance can be increased by adjusting the parameters.","f1e3fda0":"We use k-Means clustering algorithm for clustering  pca and T-sne components","578a7123":"# Implementing kmeans in T-SNE\n","8bc54511":"As the data has the High number of variables \n\nUsing principal component analysis we are reducing the dimension of the data","76df3c9a":"T-sne is an another dimensionality reduction techinique \n\nThis algorithm reduces the variables into two components","1a3c7475":"From the analysis we know that the data\nRangeIndex: 11500 entries\ndtypes: int64(179), object(1)\nNA values : zero\nColumn 'Unnamed: 0' is object","ed10d452":"# Logistic regression","14e2979f":"# T-SNE","1524a630":"# SVM","66f68983":"# Random forest","269d4150":"Attribute Information:\n\nThe original dataset from the reference consists of 5 different folders, each with 100 files, with each file representing a single subject\/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time. So we have total 500 individuals with each has 4097 data points for 23.5 seconds.\n\nWe divided and shuffled every 4097 data points into 23 chunks, each chunk contains 178 data points for 1 second, and each data point is the value of the EEG recording at a different point in time. So now we have 23 x 500 = 11500 pieces of information(row), each information contains 178 data points for 1 second(column), the last column represents the label y {1,2,3,4,5}. \n\nThe response variable is y in column 179, the Explanatory variables X1, X2, ..., X178 \n\ny contains the category of the 178-dimensional input vector. Specifically y in {1, 2, 3, 4, 5}: \n\n5 - eyes open, means when they were recording the EEG signal of the brain the patient had their eyes open \n\n4 - eyes closed, means when they were recording the EEG signal the patient had their eyes closed \n\n3 - Yes they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area \n\n2 - They recorder the EEG from the area where the tumor was located \n\n1 - Recording of seizure activity \n\nAll subjects falling in classes 2, 3, 4, and 5 are subjects who did not have epileptic seizure. Only subjects in class 1 have epileptic seizure. Our motivation for creating this version of the data was to simplify access to the data via the creation of a .csv version of it. Although there are 5 classes most authors have done binary classification, namely class 1 (Epileptic seizure) against the rest.\n\n","07c22226":"# Preprocessing ","f3b6bef1":"# Metrics for Classification","b06295fd":"# KNN"}}