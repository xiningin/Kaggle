{"cell_type":{"d43f9bac":"code","0198b2db":"code","358cd9e4":"code","61e28404":"code","32eb7602":"code","5357da73":"code","aafae40d":"code","2ac5914d":"code","d762b442":"code","9abdbc98":"code","c1719036":"code","47cfb78e":"code","8760368c":"code","3a20add3":"code","154c7d7b":"code","88beac24":"code","76718483":"code","bf0424e2":"code","40a3cb19":"code","e9076d9d":"code","b745b3b7":"code","12221782":"code","0dcc4e13":"code","01e40a9d":"code","be98fc12":"code","cb147032":"code","cfb2b2e2":"code","073b18cd":"code","30ae4f4f":"code","4eb2fd17":"code","9fed2709":"code","0e217999":"code","aad0f710":"code","d54fd71c":"code","ef4f6de2":"code","7c2c9ce0":"code","aa074821":"code","fe8bcf29":"code","04b8be34":"code","2b2626ac":"code","08f4a703":"code","f866c0be":"code","d133f1d8":"code","e5647c45":"code","71798293":"code","80bcbb4a":"code","06f1fa72":"code","5128703a":"code","1ec4671a":"code","68792592":"code","ae384fcb":"code","c2127c66":"code","21e26fe0":"code","eb805e23":"code","e49f63a5":"code","5d156520":"code","ab068a71":"code","7f671ed8":"code","12387e7b":"code","13eeb088":"code","950f926c":"code","1c3c953c":"code","df30422d":"code","b75c730f":"code","758c478a":"code","e8d1c06c":"markdown","38954a8a":"markdown","b86f3da8":"markdown","fb1c95f7":"markdown","1353e61d":"markdown","bf46a3db":"markdown","0d518c78":"markdown","ae67dab0":"markdown","625900d8":"markdown","6ee41c47":"markdown","6d0dab62":"markdown","6337ae78":"markdown","a15675cf":"markdown","d9e2c33a":"markdown","8ed3c272":"markdown","60300dd5":"markdown","4bfe527a":"markdown","a42a5998":"markdown","f0222db6":"markdown","12d2d1cb":"markdown","8c798915":"markdown","e10ff736":"markdown","9e421410":"markdown","e5a3de30":"markdown","38e79f78":"markdown","733c3bfb":"markdown","a12a0410":"markdown","c68b2cda":"markdown","a247ca4b":"markdown","5e2185b3":"markdown","3ab72b8f":"markdown","64f784ce":"markdown","cb68f307":"markdown","871bae72":"markdown","60b49f4c":"markdown","319ab2b3":"markdown","3ff81934":"markdown","845bd866":"markdown","291a3b2c":"markdown","1903f29f":"markdown","cd99c761":"markdown","f2e3ce4a":"markdown","73179860":"markdown","028387ef":"markdown","d3bc5a54":"markdown","3e1565b4":"markdown","b5f472c5":"markdown","ef3984e4":"markdown","fdaa5736":"markdown","10142af9":"markdown","a987efea":"markdown"},"source":{"d43f9bac":"import gc\nimport itertools\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns","0198b2db":"def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n    \"\"\" \n    Iterates through all the columns of a dataframe and downcasts the data type\n     to reduce memory usage. Can also factorize categorical columns to integer dtype.\n    \"\"\"\n    def _downcast_numeric(series, allow_categorical=allow_categorical):\n        \"\"\"\n        Downcast a numeric series into either the smallest possible int dtype or a specified float dtype.\n        \"\"\"\n        if pd.api.types.is_sparse(series.dtype) is True:\n            return series\n        elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n            if pd.api.types.is_datetime64_any_dtype(series.dtype):\n                return series\n            else:\n                if allow_categorical:\n                    return series\n                else:\n                    codes, uniques = series.factorize()\n                    series = pd.Series(data=codes, index=series.index)\n                    series = _downcast_numeric(series)\n                    return series\n        else:\n            series = pd.to_numeric(series, downcast=\"integer\")\n        if pd.api.types.is_float_dtype(series.dtype):\n            series = series.astype(float_dtype)\n        return series\n\n    if silent is False:\n        start_mem = np.sum(df.memory_usage()) \/ 1024 ** 2\n        print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    if df.ndim == 1:\n        df = _downcast_numeric(df)\n    else:\n        for col in df.columns:\n            df.loc[:, col] = _downcast_numeric(df.loc[:,col])\n    if silent is False:\n        end_mem = np.sum(df.memory_usage()) \/ 1024 ** 2\n        print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n        print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\n\n\ndef shrink_mem_new_cols(matrix, oldcols=None, allow_categorical=False):\n    # Calls reduce_mem_usage on columns which have not yet been optimized\n    if oldcols is not None:\n        newcols = matrix.columns.difference(oldcols)\n    else:\n        newcols = matrix.columns\n    matrix.loc[:,newcols] = reduce_mem_usage(matrix.loc[:,newcols], allow_categorical=allow_categorical)\n    oldcols = matrix.columns  # This is used to track which columns have already been downcast\n    return matrix, oldcols\n\n\ndef list_if_not(s, dtype=str):\n    # Puts a variable in a list if it is not already a list\n    if type(s) not in (dtype, list):\n        raise TypeError\n    if (s != \"\") & (type(s) is not list):\n        s = [s]\n    return s","358cd9e4":"items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntrain = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","61e28404":"train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")","32eb7602":"# Merge some duplicate shops\ntrain[\"shop_id\"] = train[\"shop_id\"].replace({0: 57, 1: 58, 11: 10, 40: 39})\n# Keep only shops that are in the test set\ntrain = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]\n# Drop training items with extreme or negative prices or sales counts\ntrain = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]\ntrain = train[(train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]","5357da73":"def create_testlike_train(sales_train, test=None):\n    indexlist = []\n    for i in sales_train.date_block_num.unique():\n        x = itertools.product(\n            [i],\n            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n        )\n        indexlist.append(np.array(list(x)))\n    df = pd.DataFrame(\n        data=np.concatenate(indexlist, axis=0),\n        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    # Add revenue column to sales_train\n    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n    # Aggregate item_id \/ shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n    )\n\n    # Merge the grouped data with the index\n    df = df.merge(\n        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    if test is not None:\n        test[\"date_block_num\"] = 34\n        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n        test[\"item_id\"] = test.item_id.astype(np.int16)\n        test = test.drop(columns=\"ID\")\n\n        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n\n    # Fill empty item_cnt entries with 0\n    df.item_cnt_month = df.item_cnt_month.fillna(0)\n    df.item_revenue_month = df.item_revenue_month.fillna(0)\n\n    return df","aafae40d":"matrix = create_testlike_train(train, test)\ndel(test)","2ac5914d":"matrix = reduce_mem_usage(matrix, silent=False)\noldcols = matrix.columns","d762b442":"items.query(\"item_id>3564\").head(5)","9abdbc98":"import re\n\nfrom fuzzywuzzy import fuzz\n\n\ndef add_item_name_groups(matrix, train, items, sim_thresh, feature_name=\"item_name_group\"):\n    def partialmatchgroups(items, sim_thresh=sim_thresh):\n        def strip_brackets(string):\n            string = re.sub(r\"\\(.*?\\)\", \"\", string)\n            string = re.sub(r\"\\[.*?\\]\", \"\", string)\n            return string\n\n        items = items.copy()\n        items[\"nc\"] = items.item_name.apply(strip_brackets)\n        items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n\n        def partialcompare(s):\n            return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n\n        items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n        # Assign groups\n        grp = 0\n        for i in range(items.shape[0]):\n            items.loc[i, \"partialmatchgroup\"] = grp\n            if items.loc[i, \"partialmatch\"] < sim_thresh:\n                grp += 1\n        items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n        return items\n\n    items = partialmatchgroups(items)\n    items = items.rename(columns={\"partialmatchgroup\": feature_name})\n    items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n\n    items[feature_name] = items[feature_name].apply(str)\n    items[feature_name] = items[feature_name].factorize()[0]\n    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n    train = train.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n    return matrix, train\n\n\nmatrix, train = add_item_name_groups(matrix, train, items, 65)","c1719036":"from nltk.corpus import stopwords\n\n\ndef add_first_word_features(matrix, items=items, feature_name=\"artist_name_or_first_word\"):\n    # This extracts artist names for music categories and adds them as a feature.\n    def extract_artist(st):\n        st = st.strip()\n        if st.startswith(\"V\/A\"):\n            artist = \"V\/A\"\n        elif st.startswith(\"\u0421\u0411\"):\n            artist = \"\u0421\u0411\"\n        else:\n            # Retrieves artist names using the double space or all uppercase pattern\n            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n            match_dubspace = mus_artist_dubspace.match(st)\n            mus_artist_capsonly = re.compile(r\"^([^a-z\u0430-\u044f]+\\s)+\")\n            match_capsonly = mus_artist_capsonly.match(st)\n            candidates = [match_dubspace, match_capsonly]\n            candidates = [m[0] for m in candidates if m is not None]\n            # Sometimes one of the patterns catches some extra words so choose the shortest one\n            if len(candidates):\n                artist = min(candidates, key=len)\n            else:\n                # If neither of the previous patterns found something, use the dot-space pattern\n                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n                match = mus_artist_dotspace.match(st)\n                if match:\n                    artist = match[0]\n                else:\n                    artist = \"\"\n        artist = artist.upper()\n        artist = re.sub(r\"[^A-Z\u0410-\u042f ]||\\bTHE\\b\", \"\", artist)\n        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n        artist = artist.strip()\n        return artist\n\n    items = items.copy()\n    all_stopwords = stopwords.words(\"russian\")\n    all_stopwords = all_stopwords + stopwords.words(\"english\")\n\n    def first_word(string):\n        # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n        string = re.sub(r\"[^\\w\\s]\", \"\", string)\n        string = re.sub(r\"\\s{2,}\", \" \", string)\n        tokens = string.lower().split()\n        tokens = [t for t in tokens if t not in all_stopwords]\n        token = tokens[0] if len(tokens) > 0 else \"\"\n        return token\n\n    music_categories = [55, 56, 57, 58, 59, 60]\n    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[\n        items.item_category_id.isin(music_categories), \"item_name\"\n    ].apply(extract_artist)\n    items.loc[items[feature_name] == \"\", feature_name] = \"other music\"\n    items.loc[~items.item_category_id.isin(music_categories), feature_name] = items.loc[\n        ~items.item_category_id.isin(music_categories), \"item_name\"\n    ].apply(first_word)\n    items.loc[items[feature_name] == \"\", feature_name] = \"other non-music\"\n    items[feature_name] = items[feature_name].factorize()[0]\n    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\",)\n    return matrix\n\n\nmatrix = add_first_word_features(\n    matrix, items=items, feature_name=\"artist_name_or_first_word\"\n)","47cfb78e":"import re\ndef clean_item_name(string):\n    # Removes bracketed terms, special characters and extra whitespace\n    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n    string = re.sub(r\"[^A-Z\u0410-\u042fa-z\u0430-\u044f0-9 ]\", \"\", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    string = string.lower()\n    return string\n\nitems[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\nitems[\"item_name_length\"] = items[\"item_name\"].apply(len)\nmatrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')\nitems = items.drop(columns=['item_name_length', 'item_name_cleaned_length'])","8760368c":"print(\"Created name features\")\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","3a20add3":"def add_time_features(m, train, correct_item_cnt_day=False):\n    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n\n    def item_shop_age_months(m):\n        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(\n            lambda x: x - x.min()\n        )\n        # Sales tend to plateau after 12 months\n        m[\"new_item\"] = m[\"item_age\"] == 0\n        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n        m[\"shop_age\"] = (\n            m.groupby(\"shop_id\")[\"date_block_num\"]\n            .transform(lambda x: x - x.min())\n            .astype(\"int8\")\n        )\n        return m\n\n    # Add dummy values for the test month so that features are created correctly\n    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n    dummies = dummies.assign(\n        date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,\n    )\n    train = pd.concat([train, dummies])\n    del dummies\n\n    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n    month_last_day[~month_last_day.dt.is_month_end] = (\n        month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n    )\n    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n    month_first_day[~month_first_day.dt.is_month_start] = (\n        month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n    )\n    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n    first_shop_item_date = (\n        train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n    )\n    first_item_name_group_date = (\n        train.groupby(\"item_name_group\").date.min().rename(\"first_name_group_date\")\n    )\n\n    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n    m = m.merge(\n        first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\"\n    )\n    m = m.merge(\n        first_item_name_group_date, left_on=\"item_name_group\", right_index=True, how=\"left\"\n    )\n\n    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n    m[\"item_in_shop_days\"] = (\n        m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n    )\n    m = m.drop(columns=\"item_first_sale_days\")\n    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] \/ m[\"item_in_shop_days\"]\n    m[\"month_length\"] = m[\"month_length\"].dt.days\n\n    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n    m[\"first_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n    m[\"first_shop_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n    m[\"first_name_group_sale_days\"] = m[\"month_first_day\"] - m[\"first_name_group_date\"]\n    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n    m[\"first_item_sale_days\"] = (\n        m[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n    m[\"first_shop_item_sale_days\"] = (\n        m[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n    m[\"first_name_group_sale_days\"] = (\n        m[\"first_name_group_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n    )\n\n    # Add days since last sale\n    def last_sale_days(matrix):\n        last_shop_item_dates = []\n        for dbn in range(1, 35):\n            lsid_temp = (\n                train.query(f\"date_block_num<{dbn}\")\n                .groupby([\"shop_id\", \"item_id\"])\n                .date.max()\n                .rename(\"last_shop_item_sale_date\")\n                .reset_index()\n            )\n            lsid_temp[\"date_block_num\"] = dbn\n            last_shop_item_dates.append(lsid_temp)\n\n        last_shop_item_dates = pd.concat(last_shop_item_dates)\n        matrix = matrix.merge(\n            last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\"\n        )\n\n        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n            m.loc[m[feat_name] > 2000, feat_name] = missingval\n            m.loc[m[feat_name].isna(), feat_name] = missingval\n            return m\n\n        matrix = days_since_last_feat(\n            matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999\n        )\n\n        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\"])\n        return matrix\n\n    m = last_sale_days(m)\n    # Month id feature\n    m[\"month\"] = m[\"month_first_day\"].dt.month\n\n    m = m.drop(\n        columns=[\n            \"first_day\",\n            \"month_first_day\",\n            \"month_last_day\",\n            \"first_shop_date\",\n            \"first_item_date\",\n            \"first_name_group_date\",\n            \"item_in_shop_days\",\n            \"first_shop_item_date\",\n            \"month_length\",\n        ],\n        errors=\"ignore\",\n    )\n\n    m = item_shop_age_months(m)\n\n    if correct_item_cnt_day == True:\n        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n\n    return m","154c7d7b":"matrix = add_time_features(matrix, train, False)\nprint(\"Time features created\")","88beac24":"def add_price_features(matrix, train):\n    # Get mean prices per month from train dataframe\n    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n    price_features = pd.DataFrame(price_features)\n    price_features = price_features.reset_index()\n    # Calculate normalized differenced from mean category price per month\n    price_features = price_features.merge(\n        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n    )\n    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n        [\"date_block_num\", \"item_category_id\"]\n    )[\"item_price\"].transform(lambda x: (x - x.mean()) \/ x.mean())\n    # Retain only the necessary features\n    price_features = price_features[\n        [\n            \"date_block_num\",\n            \"item_id\",\n            \"item_price\",\n            \"norm_diff_cat_price\",\n        ]\n    ]\n\n    features = [\n        \"item_price\",\n        \"norm_diff_cat_price\",\n    ]\n    newnames = [\"last_\" + f for f in features]\n    aggs = {f: \"last\" for f in features}\n    renames = {f: \"last_\" + f for f in features}\n    features = []\n    for dbn in range(1, 35):\n        f_temp = (\n            price_features.query(f\"date_block_num<{dbn}\")\n            .groupby(\"item_id\")\n            .agg(aggs)\n            .rename(columns=renames)\n        )\n        f_temp[\"date_block_num\"] = dbn\n        features.append(f_temp)\n    features = pd.concat(features).reset_index()\n    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n    return matrix","76718483":"matrix = add_price_features(matrix, train)\ndel(train)","bf0424e2":"matrix = matrix.merge(items[['item_id', 'item_category_id']], on='item_id', how='left')\n\nplatform_map = {\n    0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 8, 10: 1, 11: 2,\n    12: 3, 13: 4, 14: 5, 15: 6, 16: 7, 17: 8, 18: 1, 19: 2, 20: 3, 21: 4, 22: 5,\n    23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 0, 29: 0, 30: 0, 31: 0, 32: 8, 33: 11,\n    34: 11, 35: 3, 36: 0, 37: 12, 38: 12, 39: 12, 40: 13, 41: 13, 42: 14, 43: 15,\n    44: 15, 45: 15, 46: 14, 47: 14, 48: 14, 49: 14, 50: 14, 51: 14, 52: 14, 53: 14,\n    54: 8, 55: 16, 56: 16, 57: 17, 58: 18, 59: 13, 60: 16, 61: 8, 62: 8, 63: 8, 64: 8,\n    65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 8, 73: 0, 74: 10, 75: 0,\n    76: 0, 77: 0, 78: 0, 79: 8, 80: 8, 81: 8, 82: 8, 83: 8,\n}\nmatrix['platform_id'] = matrix['item_category_id'].map(platform_map)\n\nsupercat_map = {\n    0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 1, 12: 1,\n    13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3,\n    24: 3, 25: 0, 26: 2, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 2, 33: 2, 34: 2,\n    35: 2, 36: 2, 37: 4, 38: 4, 39: 4, 40: 4, 41: 4, 42: 5, 43: 5, 44: 5, 45: 5,\n    46: 5, 47: 5, 48: 5, 49: 5, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 6, 56: 6,\n    57: 6, 58: 6, 59: 6, 60: 6, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0,\n    68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 7, 74: 7, 75: 7, 76: 7, 77: 7, 78: 7,\n    79: 2, 80: 2, 81: 0, 82: 0, 83: 0\n}\nmatrix['supercategory_id'] = matrix['item_category_id'].map(supercat_map)","40a3cb19":"def add_city_codes(matrix, shops):\n    shops.loc[\n        shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', \"shop_name\"\n    ] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n    shops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n    shop_labels = shops[[\"shop_id\", \"city_code\"]]\n    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n    return matrix\n\nmatrix = add_city_codes(matrix, shops)\ndel(shops)","e9076d9d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\n\ndef cluster_feature(matrix, target_feature, clust_feature, level_feature, n_components=4, n_clusters=5, aggfunc=\"mean\", exclude=None):\n    start_month = 20\n    end_month = 32\n    pt = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\n    if exclude is not None:\n        pt = matrix[~matrix[clust_feature].isin(exclude)]\n    pt = pt.pivot_table(values=target_feature, columns=clust_feature, index=level_feature, fill_value=0, aggfunc=aggfunc)\n    pt = pt.transpose()\n    pca = PCA(n_components=10)\n    components = pca.fit_transform(pt)\n    components = pd.DataFrame(components)\n    # Plot PCA explained variance\n    sns.set_theme()\n    features = list(range(pca.n_components_))\n    fig = plt.figure(figsize=(10,4))\n    ax = fig.add_subplot(121)\n#     ax.bar(features, pca.explained_variance_ratio_, color=\"black\")\n    sns.barplot(x=features, y=pca.explained_variance_ratio_, ax=ax)\n    plt.title(\"Variance by PCA components\")\n    plt.xlabel(\"component\")\n    plt.ylabel(\"explained variance\")\n    plt.xticks(features)\n\n    scorelist = []\n    nrange = range(2, 10)\n    for n in nrange:\n        clusterer = AgglomerativeClustering(n_clusters=n)\n        labels = clusterer.fit_predict(components)\n        silscore = silhouette_score(pt, labels)\n        scorelist.append(silscore)\n    ax = fig.add_subplot(122)\n    sns.lineplot(x=nrange, y=scorelist, ax=ax)\n    plt.title(\"Clustering quality by number of clusters\")\n    plt.xlabel(\"n clusters\")\n    plt.ylabel(\"silhouette score\")\n\n    pca = PCA(n_components=n_components)\n    components = pca.fit_transform(pt)\n    components = pd.DataFrame(components)\n    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n    labels = clusterer.fit_predict(components)\n    x = components[0]\n    y = components[1]\n    fig = plt.figure(figsize=(10, 4))\n    ax = fig.add_subplot(111)\n    sns.scatterplot(x=x, y=y, hue=labels, palette=sns.color_palette(\"hls\", n_clusters), ax=ax)\n    plt.title(\"Items by cluster\")\n    plt.xlabel(\"component 1 score\")\n    plt.ylabel(\"component 2 score\")\n    for i, txt in enumerate(pt.index.to_list()):\n        ax.annotate(str(txt), (x[i], y[i]))\n    groups = {}\n    for i, s in enumerate(pt.index):\n        groups[s] = labels[i]\n    return groups","b745b3b7":"category_group_dict = cluster_feature(matrix, 'item_cnt_month', 'item_category_id', 'date_block_num', n_components=2, n_clusters=4, aggfunc=\"mean\", exclude =[])\nmatrix['category_cluster'] = matrix['item_category_id'].map(category_group_dict)","12221782":"shop_group_dict = cluster_feature(matrix, 'item_cnt_month', 'shop_id', 'item_category_id', n_components=4, n_clusters=4, aggfunc=\"mean\", exclude=[36])\nshop_group_dict[36] = shop_group_dict[37]  # Shop36 added separately because it only has one month of data\nmatrix['shop_cluster'] = matrix['shop_id'].map(shop_group_dict)","0dcc4e13":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory","01e40a9d":"def uniques(matrix, groupers, name, limitation=None):\n    if limitation is not None:\n        s = (\n            matrix.query(limitation)\n            .groupby(groupers)\n            .item_id.nunique()\n            .rename(name)\n            .reset_index()\n        )\n    else:\n        s = matrix.groupby(groupers).item_id.nunique().rename(name).reset_index()\n    matrix = matrix.merge(s, on=groupers, how=\"left\")\n    matrix[name] = matrix[name].fillna(0)\n    return matrix\n\n\nmatrix = uniques(matrix, [\"date_block_num\"], \"unique_items_month\")\n\nmatrix = uniques(matrix, [\"date_block_num\", \"item_name_group\"], \"name_group_unique_month\")\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n    \"name_group_cat_unique_month\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_name_group\"],\n    \"name_group_new_unique_month\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n    \"name_group_new_cat_unique_month\",\n    limitation=\"new_item==True\",\n)\n\nmatrix = uniques(\n    matrix, [\"date_block_num\", \"artist_name_or_first_word\"], \"first_word_unique_month\"\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n    \"first_word_cat_unique_month\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"artist_name_or_first_word\"],\n    \"first_word_new_unique_month\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n    \"first_word_new_cat_unique_month\",\n    limitation=\"new_item==True\",\n)\n\nmatrix = uniques(matrix, [\"date_block_num\", \"item_category_id\"], \"unique_items_cat\")\nmatrix = uniques(\n    matrix,\n    [\"date_block_num\", \"item_category_id\"],\n    \"new_items_cat\",\n    limitation=\"new_item==True\",\n)\nmatrix = uniques(matrix, [\"date_block_num\"], \"new_items_month\", limitation=\"new_item==True\")\n\nmatrix[\"cat_items_proportion\"] = matrix[\"unique_items_cat\"] \/ matrix[\"unique_items_month\"]\nmatrix[\"name_group_new_proportion_month\"] = (\n    matrix[\"name_group_new_unique_month\"] \/ matrix[\"name_group_unique_month\"]\n)\n\nmatrix = matrix.drop(columns=[\"unique_items_month\", \"name_group_unique_month\"])","be98fc12":"matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","cb147032":"def add_pct_change(\n    matrix,\n    group_feats,\n    target=\"item_cnt_month\",\n    aggfunc=\"mean\",\n    periods=1,\n    lag=1,\n    clip_value=None,\n):\n    periods = list_if_not(periods, int)\n    group_feats = list_if_not(group_feats)\n    group_feats_full = [\"date_block_num\"] + group_feats\n    dat = matrix.pivot_table(\n        index=group_feats + [\"date_block_num\"],\n        values=target,\n        aggfunc=aggfunc,\n        fill_value=0,\n        dropna=False,\n    ).astype(\"float32\")\n    for g in group_feats:\n        firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n        dat = dat.merge(firsts, left_on=g, right_index=True, how=\"left\")\n        dat.loc[dat.index.get_level_values(\"date_block_num\") < dat[\"firsts\"], target] = float(\n            \"nan\"\n        )\n        del dat[\"firsts\"]\n    for period in periods:\n        feat_name = \"_\".join(\n            group_feats + [target] + [aggfunc] + [\"delta\"] + [str(period)] + [f\"lag_{lag}\"]\n        )\n        print(f\"Adding feature {feat_name}\")\n        dat = (\n            dat.groupby(group_feats)[target]\n            .transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\"))\n            .rename(feat_name)\n        )\n        if clip_value is not None:\n            dat = dat.clip(lower=-clip_value, upper=clip_value)\n    dat = dat.reset_index()\n    dat[\"date_block_num\"] += lag\n    matrix = matrix.merge(dat, on=[\"date_block_num\"] + group_feats, how=\"left\")\n    matrix[feat_name] = reduce_mem_usage(matrix[feat_name])\n    return matrix","cfb2b2e2":"matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\nmatrix = add_pct_change(matrix, [\"item_name_group\"], \"item_cnt_month\", clip_value=3)\n# Delta 1 feature lagged by 12 months, intended to capture seasonal trends\nmatrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\ngc.collect()","073b18cd":"matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","30ae4f4f":"shop_id = 16\nitem_id = 482\nim = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\nim['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\nim['expanding mean'] = im['item_cnt_month'].expanding().mean()\nim['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\nim = im.set_index('date_block_num')\nax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')","4eb2fd17":"def add_rolling_stats(\n    matrix,\n    features,\n    window=12,\n    kind=\"rolling\",\n    argfeat=\"item_cnt_month\",\n    aggfunc=\"mean\",\n    rolling_aggfunc=\"mean\",\n    dtype=\"float16\",\n    reshape_source=True,\n    lag_offset=0,\n):\n    def rolling_stat(\n        matrix,\n        source,\n        feats,\n        feat_name,\n        window=12,\n        argfeat=\"item_cnt_month\",\n        aggfunc=\"mean\",\n        dtype=dtype,\n        lag_offset=0,\n    ):\n        # Calculate a statistic on a windowed section of a source table,  grouping on specific features\n        store = []\n        for i in range(2 + lag_offset, 35 + lag_offset):\n            if len(feats) > 0:\n                mes = (\n                    source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n                    .groupby(feats)[argfeat]\n                    .agg(aggfunc)\n                    .astype(dtype)\n                    .rename(feat_name)\n                    .reset_index()\n                )\n            else:\n                mes = {}\n                mes[feat_name] = (\n                    source.loc[\n                        source.date_block_num.isin(range(max([i - window, 0]), i)), argfeat\n                    ]\n                    .agg(aggfunc)\n                    .astype(dtype)\n                )\n                mes = pd.DataFrame(data=mes, index=[i])\n            mes[\"date_block_num\"] = i - lag_offset\n            store.append(mes)\n        store = pd.concat(store)\n        matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n        return matrix\n\n    \"\"\" An issue when using windowed functions is that missing values from months when items recorded no sales are skipped rather than being correctly\n    treated as zeroes. Creating a pivot_table fills in the zeros.\"\"\"\n    if (reshape_source == True) or (kind == \"ewm\"):\n        source = matrix.pivot_table(\n            index=features + [\"date_block_num\"],\n            values=argfeat,\n            aggfunc=aggfunc,\n            fill_value=0,\n            dropna=False,\n        ).astype(dtype)\n        for g in features:\n            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n            # Set values before the items first appearance to nan so they are ignored rather than being treated as zero sales.\n            source.loc[\n                source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat\n            ] = float(\"nan\")\n            del source[\"firsts\"]\n        source = source.reset_index()\n    else:\n        source = matrix\n\n    if kind == \"rolling\":\n        feat_name = (\n            f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n        )\n        print(f'Creating feature \"{feat_name}\"')\n        return rolling_stat(\n            matrix,\n            source,\n            features,\n            feat_name,\n            window=window,\n            argfeat=argfeat,\n            aggfunc=rolling_aggfunc,\n            dtype=dtype,\n            lag_offset=lag_offset,\n        )\n    elif kind == \"expanding\":\n        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n        print(f'Creating feature \"{feat_name}\"')\n        return rolling_stat(\n            matrix,\n            source,\n            features,\n            feat_name,\n            window=100,\n            argfeat=argfeat,\n            aggfunc=aggfunc,\n            dtype=dtype,\n            lag_offset=lag_offset,\n        )\n    elif kind == \"ewm\":\n        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n        print(f'Creating feature \"{feat_name}\"')\n        source[feat_name] = (\n            source.groupby(features)[argfeat]\n            .ewm(halflife=window, min_periods=1)\n            .agg(rolling_aggfunc)\n            .to_numpy(dtype=dtype)\n        )\n        del source[argfeat]\n        #         source = source.reset_index()\n        source[\"date_block_num\"] += 1 - lag_offset\n        return matrix.merge(source, on=[\"date_block_num\"] + features, how=\"left\")","9fed2709":"matrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"category_cluster\"], window=12)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_category_id\", \"item_age\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_category_id\", \"new_item\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"shop_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], window=12)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_name_group\", \"item_category_id\", \"new_item\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_id\", \"supercategory_id\", \"new_item\"], window=12, reshape_source=False\n)\n\nmatrix = add_rolling_stats(matrix, [\"shop_cluster\", \"item_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_cluster\", \"item_category_id\", \"item_age\"],\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"shop_cluster\", \"item_name_group\", \"new_item\"], window=12, reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"category_cluster\", \"item_age\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix, [\"category_cluster\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(matrix, [\"item_id\"], window=12)\n\nmatrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix, [\"artist_name_or_first_word\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix, [\"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"item_category_id\", \"item_age\"], kind=\"expanding\", reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"item_category_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"item_category_id\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(\n    matrix, [\"item_category_id\", \"new_item\"], kind=\"expanding\", reshape_source=False\n)\n\nmatrix = add_rolling_stats(\n    matrix, [\"item_name_group\", \"item_age\"], window=12, reshape_source=False\n)\nmatrix = add_rolling_stats(matrix, [\"item_name_group\"], kind=\"ewm\", window=1)\nmatrix = add_rolling_stats(matrix, [\"item_name_group\"], window=12)\n\nmatrix = add_rolling_stats(matrix, [\"platform_id\"], window=12)\nmatrix = add_rolling_stats(matrix, [\"platform_id\"], kind=\"ewm\", window=1)","0e217999":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","aad0f710":"# Summed sales & accurate windowed mean sales per day features\nmatrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_id\"],\n    aggfunc=\"sum\",\n    rolling_aggfunc=\"sum\",\n    kind=\"rolling\",\n    window=12,\n    reshape_source=False,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"item_id\"],\n    aggfunc=\"sum\",\n    rolling_aggfunc=\"sum\",\n    kind=\"expanding\",\n    reshape_source=False,\n)\nmatrix[\"1year\"] = 365\nmatrix[\"item_id_day_mean_expanding\"] = matrix[\n    \"item_id_item_cnt_month_sum_expanding_sum\"\n] \/ matrix[[\"first_item_sale_days\"]].min(axis=1)\nmatrix[\"shop_id_item_id_day_mean_win_12\"] = matrix[\n    \"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"\n] \/ matrix[[\"first_item_sale_days\", \"shop_open_days\", \"1year\"]].min(axis=1)\nmatrix.loc[matrix.new_item == True, \"item_id_day_mean_expanding\",] = float(\"nan\")\nmatrix = matrix.drop(columns=[\"1year\", \"item_id_item_cnt_month_sum_expanding_sum\"])","d54fd71c":"matrix = add_rolling_stats(\n    matrix,\n    [\"shop_id\", \"item_name_group\"],\n    window=12,\n    argfeat=\"item_revenue_month\",\n    dtype=\"float32\",\n)","ef4f6de2":"matrix = add_rolling_stats(\n    matrix,\n    [\"item_category_id\"],\n    argfeat=\"new_items_cat\",\n    window=12,\n    reshape_source=True,\n    lag_offset=1,\n)\nmatrix = add_rolling_stats(\n    matrix,\n    [\"item_name_group\"],\n    argfeat=\"name_group_new_unique_month\",\n    window=12,\n    reshape_source=True,\n    lag_offset=1,\n)\n\nmatrix[\"new_items_cat_1_12_ratio\"] = (\n    matrix[\"new_items_cat\"]\n    \/ matrix[\"item_category_id_new_items_cat_mean_rolling_mean_win_12\"]\n)","7c2c9ce0":"gc.collect()\nmatrix, oldcols = shrink_mem_new_cols(matrix, oldcols)","aa074821":"def simple_lag_feature(matrix, lag_feature, lags):\n    for lag in lags:\n        newname = lag_feature + f\"_lag_{lag}\"\n        print(f\"Adding feature {newname}\")\n        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n        targetseries[\"date_block_num\"] += lag\n        targetseries = targetseries.rename(columns={lag_feature: newname})\n        matrix = matrix.merge(\n            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n        )\n        matrix.loc[\n            (matrix.item_age >= lag) & (matrix.shop_age >= lag) & (matrix[newname].isna()),\n            newname,\n        ] = 0\n    return matrix","fe8bcf29":"matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1,2,3])\nmatrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1, 2, 3])\nmatrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\ngc.collect()\nprint(\"Lag features created\")","04b8be34":"def create_apply_ME(\n    matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\", aggfunc=\"mean\"\n):\n    grouping_fields = list_if_not(grouping_fields)\n    for lag in lags:\n        newname = \"_\".join(grouping_fields + [target] + [aggfunc] + [f\"lag_{lag}\"])\n        print(f\"Adding feature {newname}\")\n        me_series = (\n            matrix.groupby([\"date_block_num\"] + grouping_fields)[target]\n            .agg(aggfunc)\n            .rename(newname)\n            .reset_index()\n        )\n        me_series[\"date_block_num\"] += lag\n        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n        del me_series\n        matrix[newname] = matrix[newname].fillna(0)\n        for g in grouping_fields:\n            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n            matrix = matrix.merge(firsts, left_on=g, right_index=True, how=\"left\")\n            matrix.loc[\n                matrix[\"date_block_num\"] < (matrix[\"firsts\"] + (lag)), newname\n            ] = float(\"nan\")\n            del matrix[\"firsts\"]\n        matrix[newname] = reduce_mem_usage(matrix[newname])\n    return matrix","2b2626ac":"matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\", aggfunc=\"sum\")\nmatrix = create_apply_ME(matrix, [\"item_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"item_id\"])\nmatrix = create_apply_ME(matrix, [\"platform_id\"])\nmatrix = create_apply_ME(matrix, [\"item_name_group\"])\nmatrix = create_apply_ME(matrix, [\"platform_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"supercategory_id\"])\nmatrix = create_apply_ME(matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_id\", \"item_category_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"], target=\"item_cnt_month\")\nmatrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"])\nmatrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"])\nmatrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"])","08f4a703":"matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n    matrix[\"item_id_item_cnt_month_mean_lag_1\"]\n    \/ matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n)\nmatrix[\"shop_id_item_id_item_cnt_1_12_ratio\"] = (\n    matrix[\"item_cnt_day_avg_lag_1\"] \/ matrix[\"shop_id_item_id_day_mean_win_12\"]\n)","f866c0be":"matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\nmatrix.to_pickle(\"matrixcheckpoint.pkl\")\nprint(\"Saved matrixcheckpoint\")\ngc.collect()\nprint(\"Mean encoding features created\")","d133f1d8":"surplus_columns = [\n    \"item_revenue_month\",\n    \"item_cnt_day_avg\",\n    \"item_name_group\",\n    \"artist_name_or_first_word\",\n    \"item_age\",\n    \"shop_open_days\",\n    \"shop_age\",\n    \"platform_id\",\n    \"supercategory_id\",\n    \"city_code\",\n    \"category_cluster\",\n    \"shop_cluster\",\n    \"new_items_cat\",\n    \"shop_id_item_id_day_mean_win_12\",\n    \"item_id_item_cnt_month_mean_rolling_mean_win_12\",\n]\nmatrix = matrix.drop(columns=surplus_columns)","e5647c45":"import re\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", module=\"sklearn\")\n\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectKBest, f_regression\n\n\ndef name_token_feats(matrix, items, k=50, item_n_threshold=5, target_month_start=33):\n    def name_correction(st):\n        st = re.sub(r\"[^\\w\\s]\", \"\", st)\n        st = re.sub(r\"\\s{2,}\", \" \", st)\n        st = st.lower().strip()\n        return st\n\n    items[\"item_name_clean\"] = items[\"item_name\"].apply(name_correction)\n\n    def create_item_id_bow_matrix(items):\n        all_stopwords = stopwords.words(\"russian\")\n        all_stopwords = all_stopwords + stopwords.words(\"english\")\n\n        vectorizer = CountVectorizer(stop_words=all_stopwords)\n        X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n        X = pd.DataFrame.sparse.from_spmatrix(X)\n        print(f\"{len(vectorizer.vocabulary_)} words found in all items\")\n        featuremap = {\n            col: \"word_\" + token\n            for col, token in zip(\n                range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names()\n            )\n        }\n        X = X.rename(columns=featuremap)\n        return X\n\n    items_bow = create_item_id_bow_matrix(items)\n    items_bow = items_bow.clip(0, 1)  # Made the word counts binary\n    common_word_mask = items_bow.sum(axis=0) > item_n_threshold\n    target_items = matrix.query(\n        f\"date_block_num>={target_month_start} & new_item==True\"\n    ).item_id.unique()\n    target_item_mask = items_bow.loc[target_items, :].sum(axis=0) > 1\n    items_bow = items_bow.loc[:, common_word_mask & target_item_mask]\n    print(f\"{items_bow.shape[1]} words of interest\")\n    mxbow = matrix[[\"date_block_num\", \"item_id\", \"item_cnt_month\"]].query(\"date_block_num<34\")\n    mxbow = mxbow.merge(items_bow, left_on=\"item_id\", right_index=True, how=\"left\")\n    X = mxbow.drop(columns=[\"date_block_num\", \"item_id\", \"item_cnt_month\"])\n    y = mxbow[\"item_cnt_month\"].clip(0, 20)\n    selektor = SelectKBest(f_regression, k=k)\n    selektor.fit(X, y)\n    tokencols = X.columns[selektor.get_support()]\n    print(f\"{k} word features selected\")\n    return items_bow[tokencols]","71798293":"items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nword_frame = name_token_feats(matrix, items, k=50, item_n_threshold=5)\nmatrix = matrix.merge(word_frame, left_on='item_id', right_index=True, how='left')\n# LightGBM didn't seem to work with sparse features in this case, so we'll convert them to int\nsparsecols = [c for c in matrix.columns if pd.api.types.is_sparse(matrix[c].dtype)]\nmatrix[sparsecols] = matrix[sparsecols].sparse.to_dense().astype('int8')","80bcbb4a":"gc.collect()\nmatrix.to_pickle(\"checkpoint_final.pkl\")\nprint(\"All features generated, dataframe saved\")","06f1fa72":"%reset -f","5128703a":"import pandas as pd","1ec4671a":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)","68792592":"import warnings\n\nwarnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n\nimport lightgbm as lgbm\n\n\ndef fit_booster(\n    X_train,\n    y_train,\n    X_test=None,\n    y_test=None,\n    params=None,\n    test_run=False,\n    categoricals=[],\n    early_stopping=True,\n):\n    if params is None:\n        params = {\"learning_rate\": 0.1, \"subsample_for_bin\": 300000, \"n_estimators\": 50}\n\n    early_stopping_rounds = None\n    if early_stopping == True:\n        early_stopping_rounds = 30\n\n    if test_run:\n        eval_set = [(X_train, y_train)]\n    else:\n        eval_set = [(X_train, y_train), (X_test, y_test)]\n\n    booster = lgbm.LGBMRegressor(**params)\n\n    categoricals = [c for c in categoricals if c in X_train.columns]\n\n    booster.fit(\n        X_train,\n        y_train,\n        eval_set=eval_set,\n        eval_metric=[\"rmse\"],\n        verbose=100,\n        categorical_feature=categoricals,\n        early_stopping_rounds=early_stopping_rounds,\n    )\n\n    return booster","ae384fcb":"keep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\ntest_month = 33\ndropcols = [\n    \"shop_id\",\n    \"item_id\",\n    \"new_item\",\n]  # The features are dropped to reduce overfitting\n\nvalid = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\ntrain = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\ntrain = train[train.date_block_num >= keep_from_month]\nX_train = train.drop(columns=\"item_cnt_month\")\ny_train = train.item_cnt_month\nX_valid = valid.drop(columns=\"item_cnt_month\")\ny_valid = valid.item_cnt_month\ndel(matrix, valid, train)","c2127c66":"params = {\n    \"num_leaves\": 966,\n    \"cat_smooth\": 45.01680827234465,\n    \"min_child_samples\": 27,\n    \"min_child_weight\": 0.021144950289224463,\n    \"max_bin\": 214,\n    \"learning_rate\": 0.01,\n    \"subsample_for_bin\": 300000,\n    \"min_data_in_bin\": 7,\n    \"colsample_bytree\": 0.8,\n    \"subsample\": 0.6,\n    \"subsample_freq\": 5,\n    \"n_estimators\": 8000,\n}","21e26fe0":"categoricals = [\n    \"item_category_id\",\n    \"month\",\n]  # These features will be set as categorical features by LightGBM and handled differently\n\nlgbooster = fit_booster(\n    X_train,\n    y_train,\n    X_valid,\n    y_valid,\n    params=params,\n    test_run=False,\n    categoricals=categoricals,\n)","eb805e23":"_ = lgbm.plot_importance(lgbooster, figsize=(10,50), height=0.7, importance_type=\"gain\", max_num_features=50)","e49f63a5":"import joblib\n_ = joblib.dump(lgbooster, \"trained_lgbooster.pkl\")\nprint(\"Saved single booster\")","5d156520":"%reset -f\nimport pandas as pd","ab068a71":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n# Downcast the float columns to reduce RAM usage\nfloatcols = [c for c in matrix.columns if matrix[c].dtype==\"float32\"]\nmatrix[floatcols] = matrix[floatcols].astype(\"float16\")\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\nkeep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\ntest_month = 34\ndropcols = [\n    \"shop_id\",\n    \"item_id\",\n    \"new_item\",\n]  # The features are dropped to reduce overfitting\ncategoricals = [\n    \"item_category_id\",\n    \"month\",\n]\nmatrix[categoricals] = matrix[categoricals].astype(\"category\") \ntest = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\ntrain = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\ntrain = train[train.date_block_num >= keep_from_month]\nX_train = train.drop(columns=\"item_cnt_month\")\ny_train = train.item_cnt_month\nX_test = test.drop(columns=\"item_cnt_month\")\ny_test = test.item_cnt_month\ndel(matrix, test, train, X_test, y_test)","7f671ed8":"best_params = [\n    {\n        \"num_leaves\": 966,\n        \"cat_smooth\": 45.01680827234465,\n        \"min_child_samples\": 27,\n        \"min_child_weight\": 0.021144950289224463,\n        \"max_bin\": 214,\n        \"n_estimators\": 500,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 940,\n        \"cat_smooth\": 43.418286701105615,\n        \"min_child_samples\": 29,\n        \"min_child_weight\": 0.003944267312494195,\n        \"max_bin\": 133,\n        \"n_estimators\": 572,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 971,\n        \"cat_smooth\": 40.103611531065525,\n        \"min_child_samples\": 30,\n        \"min_child_weight\": 0.03951287458923346,\n        \"max_bin\": 212,\n        \"n_estimators\": 828,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 965,\n        \"cat_smooth\": 40.05144976454027,\n        \"min_child_samples\": 27,\n        \"min_child_weight\": 0.029220951478909872,\n        \"max_bin\": 211,\n        \"n_estimators\": 870,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n    {\n        \"num_leaves\": 961,\n        \"cat_smooth\": 40.013529776221134,\n        \"min_child_samples\": 29,\n        \"min_child_weight\": 0.026526521644599493,\n        \"max_bin\": 210,\n        \"n_estimators\": 897,\n        \"subsample_for_bin\": 300000,\n        \"learning_rate\": 0.01,\n        \"force_col_wise\": True\n    },\n]","12387e7b":"from sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nregressors = []\nfor i, params in enumerate(best_params):\n    booster = LGBMRegressor(**params)\n    regressors.append((f\"lgbr_{i}\", booster))\nvr = VotingRegressor(regressors, verbose=True)\nprint(\"Fitting voting regressor\")\nvr = vr.fit(X_train, y_train)","13eeb088":"import joblib\n_ = joblib.dump(vr, \"trained_votingregressor.pkl\")\nprint(\"Voting regressor trained and saved\")","950f926c":"%reset -f","1c3c953c":"import pandas as pd\nimport joblib\nbooster = joblib.load(\"trained_votingregressor.pkl\")","df30422d":"matrix = pd.read_pickle(\"checkpoint_final.pkl\")\nkeep_from_month = 2\ntest_month = 34\ndropcols = [\n    \"shop_id\",\n    \"item_id\",\n    \"new_item\",\n]  # The features are dropped to reduce overfitting\ncategoricals = [\n    \"item_category_id\",\n    \"month\",\n]\nmatrix[categoricals] = matrix[categoricals].astype(\"category\")\ntest = matrix.loc[matrix.date_block_num == test_month, :]\nX_test = test.drop(columns=\"item_cnt_month\")\ny_test = test.item_cnt_month\ndel matrix","b75c730f":"X_test[\"item_cnt_month\"] = booster.predict(X_test.drop(columns=dropcols)).clip(0, 20)\n# Merge the predictions with the provided template\ntest_orig = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\ntest = test_orig.merge(\n    X_test[[\"shop_id\", \"item_id\", \"item_cnt_month\"]],\n    on=[\"shop_id\", \"item_id\"],\n    how=\"inner\",\n    copy=True,\n)\n# Verify that the indices of the submission match the original\nassert test_orig.equals(test[[\"ID\", \"shop_id\", \"item_id\"]])\ntest[[\"ID\", \"item_cnt_month\"]].to_csv(\"submission.csv\", index=False)","758c478a":"from os import remove\nremove(\"checkpoint_final.pkl\")\nremove(\"matrixcheckpoint.pkl\")\nprint(\"Finished everything!\")","e8d1c06c":"# Number of unique item features\n\nThese features count the number of unique items sharing the same value of a grouping feature or set of features as the current item in the current month, e.g. number of new items in the same category.  \n\nThis could considered to be a kind of data leakage feature, as the set of items in each month (and therefore the test set) is determined by whether each item recorded a sale or not in the month being predicted, which isn't known in advance.","38954a8a":"## Feature engineering  \nIn this section predictor feature columns are generated and added to the matrix","b86f3da8":"# Item name groups with fuzzywuzzy\n\nItems in the items table are ordered alphabetically according to the item_name field, so that similar items are generally listed next to each other. For example, the first two items in the table below are the same game \"Fuse\" for two different consoles, followed by two different licensing options for the same internet security program. This ordering can be used to help group related items together.  ","fb1c95f7":"## Data cleaning","1353e61d":"Load the provided data.","bf46a3db":"Create and fit the VotingRegressor ensemble on the training data. This takes a long time.","0d518c78":"# Introduction\n\nThis notebook constructs a prediction model for the Predict Future Sales competition that is the final project for the Coursera course \"[How to Win a Data Science Competition](http:\/\/www.coursera.org\/learn\/competitive-data-science\/home\/welcome)\". The task is to predict monthly sales for various items in different retail outlets of the Russian company 1C.  \n\nI spent several months on this as practice using pandas, so some parts are a bit more complicated than might be expected of a typical short project submission.\n\nThere are some other very good notebooks for this competition which are well worth looking at and taught me a lot:\nhttps:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost  \nhttps:\/\/www.kaggle.com\/gordotron85\/future-sales-xgboost-top-3  \nhttps:\/\/www.kaggle.com\/deepdivelm\/feature-engineering-lightgbm-exploring-performance  \n\nThis is the top-scoring public notebook at the time of writing (0.84325, place 51 on the public leaderboard), which is mainly because of two novel feature types which work well when combined together. First, there is an item name group feature that groups together items with very similar names that are likely to refer to different versions of the same item (e.g. different editions of the same game or music album). Second, the way the test set was generated was exploited to count how many items sold in the month being predicted were in the same group as the item being predicted (e.g. same category, same name group). This combines well with the item name group feature to detect new items which are part of large multi-format releases that are likely to sell well. Detecting high-selling new items is one of the hardest challenges for the model in this competition (and has to be performed manually to get a really high score, I think).\n\nI hope you find the notebook interesting, and I welcome feedback - suggestions for improvements, advice about parts that are unclear, etc","ae67dab0":"The following code block calculates windowed mean sales features with day resolution accuracy","625900d8":"These hyperparameters were found by using the hyperparameter optimization framework Optuna to optimize hyperparameters for the validation set.","6ee41c47":"# Shop city\n(from https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost)","6d0dab62":"# Lagged features and mean encodings  \nValues for the same shop-item combination from previous months","6337ae78":"# Music artist \/ first word of item name  \n\nThis function assigns music items into groups according to the artist name, which is extracted from the item name with regular expressions according to 3 common patterns used to indicate the artist name (all uppercase, separated from the release title by a doublespace, or separated by dot-space (. ).  \nFor non-music categories, the items are grouped according to the first word in the item name instead.","a15675cf":"A few utility functions used throughout the notebook.","d9e2c33a":"# Simple ensembling with VotingRegressor  \n\nHere we make a simple ensembling predictor with the scikit-learn [VotingRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingRegressor.html) predictor. This wraps a list of predictors and outputs a linear combination of their predictions. In principle any scikit-learn compatible regression model can be used, but here we just use LightGBM models, as no other model types were efficient enough to run in a Kaggle notebook without memory allocation errors.","8ed3c272":"# Percentage change in an aggregate feature  \nThis uses the pandas pct_change method to calculate the proportional change in mean sales count for a specific grouping for a specific time interval, e.g. increase \/ decrease in mean sales of an item between the last 2 months.","60300dd5":"# Shop and item category clustering\n\nShops and item categories are grouped into clusters according to their sales profiles. \nThe following function performs and plots the results of a principle component analysis decomposition and clustering of the shops and item categories.\n\nThe proportion of explained variance between items explained by each of the PCA dimensions is plotted, and the individual items are plotted according to their scores on the PCA dimensions and coloured according to their cluster assignment.\n\nThe silhouette score (a metric of clustering quality) for different values of the cluster number parameter is also plotted. These plots were used to decide the number of clusters.\n\nFor both shops and item categories, over 80% of differences occur on a single dimension, indicating that differences are mainly in magnitude rather than proportion. The item component score plots show that the clustering is mainly into a large cluster containing the large majority of items, and a few clusters containing outlier items.","4bfe527a":"# Data loading and preprocessing, utility function definition","a42a5998":"Revenue features","f0222db6":"# Item name length as a feature\nThe name of the item_name field is surprisingly predictive, presumably because similar items often have similar length names. This is recorded both from the raw item name and the name cleaned of special characters and bracketed terms, which often contain information about release formats that obscure similarities between items.","12d2d1cb":"Plot the feature importances ranked by error reduction on the training set.","8c798915":"Fit the booster using early stopping with the validation set","e10ff736":"# Price features  \n\nThe price of the item in the last month in which it was sold, and its price relative to other items in the same category.","9e421410":"The following cell groups similar items together by sequentially looping through items, measuring the similarity of the names of ajacent items using the string matching package fuzzywuzzy (https:\/\/github.com\/seatgeek\/fuzzywuzzy), and assigning items to the same group if their match value is above a threshold.","e5a3de30":"Serialize the trained regressor","38e79f78":"Convert the date column to the datetime dtype to enable datetime operations.","733c3bfb":"Item categories are clustered according to their mean sales in each month of the year. The principle component plot shows that 3 categories are outliers in this respect.","a12a0410":"The feature frame is loaded and the target is clipped to match the test items","c68b2cda":"Split train and validation sets from the feature matrix, month 33 used as validation set","a247ca4b":"The final feature frame is saved and the notebook kernel is reset to free up memory for LightGBM.","5e2185b3":"Define a function to fit and return a lightgbm regressor with or without early stopping","3ab72b8f":"# Item category features  \nIn addition to the item categories provided with the data, I also manually defined two additional category groupings - supercategory (e.g. \"games\", \"music\") and platform (e.g. \"PS4\", \"mp3\").","64f784ce":"Load and split the data for fitting. For the ensemble predictor the LightGBM models are fit on all the training data with a pre-set number of estimators.","cb68f307":"The training dataframe is cleaned with standard steps  \n","871bae72":"Save the trained model","60b49f4c":"## Imports and data loading","319ab2b3":"# Predictive words in item_name\n\nOne-hot features are made for words in the item_name field that are predictive of item sales.  \n\nTo select *k* word features from the 1000's of words found in item names, words are discarded if they are not in the names of a threshold number of items, or are not in the names of new items in the test or validation months. Remaining words are then selected by the scikit-learn SelectKBest function according to their correlation with the target.","3ff81934":"# Create the test submission  \nSplit the test items from the data matrix and use the trained voting regressor to predict the target.","845bd866":"The test data seems to be every possible combination (the cartesian product) of shops and items that registered a sale in the test month, with the target as the total month's sales made for each of these shop-item combinations. Here a training matrix is made that replicates this structure for every month in the training data period. The test items are concatenated to the end of the training data so that features can be generated for the test period.","291a3b2c":"Ratios between lag 1 sales and rolling 12 month means, to capture decreases from previous means","1903f29f":"Windowed mean unique item features and ratio of new items in category with mean over the previous year","cd99c761":"## Preprocessing","f2e3ce4a":"\\*\\*\\* Note! This notebook is a repost, I made the original private after some ensembling and postprocessing steps put it into the top 10 on the leaderboard. Kaggle doesn't allow specific versions of a notebook to be made private and I didn't think a top 10 solution should be shared \\*\\*\\*\n","73179860":"Different parameters are used for each of 5 LightGBM models used in the ensemble. These parameters were the highest scoring parameters found by Optuna when optimizing on the validation date block 33.","028387ef":"Shops are clustered by their summed sales of each item category. The principle component plots show that shops mainly differ in the magnitude of their sales, with shop 31 being an outlier due to the volume of its sales. Shops 12 and 55 are outliers on an orthogonal dimension because they sell different (online only) items.","d3bc5a54":"Create rolling mean features. The combinations of grouping features and window types here were selected by generating a large number of features and then pruning them with the scikit-learn RFECV function.","3e1565b4":"# Model fitting","b5f472c5":"The function reduce_mem_usage downcasts datatypes to reduce memory usage, which is necessary to prevent memory overflow errors in the Kaggle notebook.","ef3984e4":"# Windowed aggregates\n\nFeatures aggregated over a specific window to reduce noise. Available windows are expanding (i.e. all preceding timepoints), rolling (i.e. fixed number of equally weighted timepoints) and exponentially weighted mean.  \n\n\nA note about feature names: these are set automatically according to the pattern < grouping features > - < aggregated features > - < monthly aggregation function > - < window type > , where < window type > is either \"rolling - < window aggregation function > - win - < window length in months >\" for square rolling windows, \"expanding - < window aggregation function >\" for expanding windows, and \"ewm_hl - < decay rate in terms of half-life > for exponential weighted means, all connected by underscores.","fdaa5736":"# Time features\nDay and month-resolution time features are created from the training dataframe, e.g. number of days since the first and last sale of each item.\n\nThe time since the first sale of each items is also used to create a mean sales-per-day feature (\"item_cnt_day_avg\"), which is potentially useful to correct sales counts for items which are less than a month old and therefore were not available to buy for the entire preceding month.  ","10142af9":"Some columns that were used to generate other features can now be discarded.","a987efea":"## Mean encodings\nThe mean or sum value of a target feature for each level of a categorical feature or combination of categorical features, lagged"}}