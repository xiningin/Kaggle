{"cell_type":{"3663efaf":"code","6dcf9a0d":"code","7480d59c":"code","5d8ef402":"code","a8374b4f":"code","371bffee":"code","48728340":"code","50e6148e":"code","2ec3b224":"code","d594844d":"code","76b6b971":"code","13ddf260":"code","5ac402e6":"code","4fb66538":"code","65fea28c":"code","ce895cb2":"code","001416ef":"code","c02e3617":"code","d49a914c":"code","0a606389":"code","e392150b":"code","f6ff9eca":"code","768b29f5":"code","a45eb5e1":"code","0f12aa98":"code","88c12adb":"code","273975ad":"code","9d121499":"code","788161a5":"code","9ca0a870":"code","1e5a062a":"code","e5dd6835":"code","377f3334":"code","e2d31292":"code","20fad11a":"code","75027673":"code","9dd6d05d":"code","30289387":"code","e89e2d0e":"code","86c99ab9":"code","956e8977":"code","9f853171":"code","204523d7":"code","10df104a":"code","c2bd4464":"code","04b4e035":"code","cbb15ccf":"code","d6b45138":"code","de65f9dc":"code","ec055721":"code","a888bf24":"markdown","664d5a63":"markdown","592c2f88":"markdown","c68cb893":"markdown","4480bb68":"markdown","390560a5":"markdown","bc0d0b74":"markdown","c1bb4a0f":"markdown","b17e7b75":"markdown","2bd8b212":"markdown","807142e9":"markdown","726d1bc0":"markdown","64806522":"markdown","ee217d7e":"markdown","8811cf44":"markdown","86061bd6":"markdown","b0c0e056":"markdown","3d3033ea":"markdown","9b7f79fb":"markdown","d35988f8":"markdown","7ed64c56":"markdown","da0c6b7c":"markdown","a8f683d5":"markdown","c0d03952":"markdown","755b4080":"markdown"},"source":{"3663efaf":"#Import all the tools\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sbn\n\n\n#plots to appear inside the notebook\n%matplotlib inline\n\n#Models from scikit-Learn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve\n","6dcf9a0d":"heart_disease_data  =  pd.read_csv(\"..\/input\/heart-disease.csv\")\n#Size of data (Rows,Column)\nheart_disease_data.shape","7480d59c":"#Top 5 results\nheart_disease_data.head()","5d8ef402":"# 1 - > Having heart disease\nheart_disease_data[\"target\"].value_counts()","a8374b4f":"heart_disease_data[\"target\"].value_counts().plot(kind=\"bar\",color=[\"brown\",\"green\"]);","371bffee":"#heart_disease_data.info()\n#heart_disease_data.describe()\n#Check missing values\nheart_disease_data.isna().sum()","48728340":"heart_disease_data.sex.value_counts()","50e6148e":"# Compare target column with sex column\npd.crosstab(heart_disease_data.target,heart_disease_data.sex)\n","2ec3b224":"# Create a plot of crosstab\n\npd.crosstab(heart_disease_data.target,heart_disease_data.sex).plot(kind=\"bar\",figsize=(10,6),\n                    color=[\"brown\",\"green\"])\nplt.title(\"Heart disease frequency  for sex\")\nplt.xlabel(\"0 = No disease 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0)","d594844d":"#Graph b\/w Age and Max Heart Rate for Heart Disease\n\nplt.figure(figsize=(10,6))\n\n#sbn.set_style(\"darkgrid\")\n\n#plt.style.use(\"dark_background\")\n\n#Scatter with positive\nplt.scatter(heart_disease_data.age[heart_disease_data.target == 1],\n        heart_disease_data.thalach[heart_disease_data.target==1],\n           color=\"orange\")\n\n#Scatter with negative\nplt.scatter(heart_disease_data.age[heart_disease_data.target == 0],\n        heart_disease_data.thalach[heart_disease_data.target== 0],\n           color=\"blue\");\n\nplt.title(\"Hear disease scatter plot for Age vs Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max heart rate\")\nplt.legend([\"Disease\",\"No-Disease\"])","76b6b971":"#Check the distribution of age with histogram \n\nheart_disease_data.age.plot.hist()\n","13ddf260":"pd.crosstab(heart_disease_data.cp,heart_disease_data.target).plot(kind=\"bar\",color=[\"yellow\",\"blue\"],figsize=(10,6))\nplt.title(\"Hear disease frequency per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Disease\",\"No-Disease\"])\nplt.xticks(rotation=0)\n","5ac402e6":"pd.crosstab(heart_disease_data.cp,heart_disease_data.target)","4fb66538":"# Find the correlation between our independent variables\ncorr_heart_matrix = heart_disease_data.corr()\ncorr_heart_matrix\n\n#Negative Correlation = a relationship b\/w two varibles in which one variable increases  as the other decreases\n'''It also means that with the decrease of variable X, variable Y should increase instead. '''\ncorr_heart_matrix = heart_disease_data.corr()\nplt.figure(figsize=(15, 10))\n\nfig, ax = plt.subplots(figsize=(15,10))\nax = sbn.heatmap(corr_heart_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom+0.5,top-0.5)","65fea28c":"#Split data into X and y\nX = heart_disease_data.drop(\"target\",axis=1)\ny = heart_disease_data[\"target\"]","ce895cb2":"# Split data into train and test sets\nnp.random.seed(42)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n\n","001416ef":"# Create a new models in a dictionary\nmodels = {\"Logistic Regrssion\": LogisticRegression(),\n         \"KNN\":KNeighborsClassifier(),\n         \"Random Forest\":RandomForestClassifier()}\n\n#Function for Fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores\n","c02e3617":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","d49a914c":"model_compare_heart = pd.DataFrame(model_scores,index=[\"accuracy\"])\n#model_compare_heart.plot.bar()\nmodel_compare_heart.T.plot.bar()","0a606389":"# Create a list of train and test scores\ntrain_scores = []\ntest_scores = []\n\n# create a list of different values for N_neighbors\nneighbors= range(1,25)\n\nknn = KNeighborsClassifier()\n\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))\n    ","e392150b":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 25, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","f6ff9eca":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","768b29f5":"#Tune LogisticsRegression\nnp.random.seed(42)\n\n# cv - >cross validation 5+5+5+5+5+5=20\n#Setup random hyperparameter\nhyper_log_lgres = RandomizedSearchCV(LogisticRegression(),\n                                    param_distributions=log_reg_grid,\n                                    cv= 5,n_iter=20,verbose=True)\nhyper_log_lgres.fit(X_train,y_train)","a45eb5e1":"#Best Hyperparameter\n\nhyper_log_lgres.best_params_","0f12aa98":"#Evalauate the LogisticRegression model \nhyper_log_lgres.score(X_test,y_test)","88c12adb":"#Tune RandomForestClassifier\nnp.random.seed(42)\n\n# cv - >cross validation 5+5+5+5+5+5=20\n#Setup RandomForestClassifier\nhyper_log_rdmFostCls = RandomizedSearchCV(RandomForestClassifier(),\n                                    param_distributions=rf_grid,\n                                    cv= 5,n_iter=20,verbose=True)\nhyper_log_rdmFostCls.fit(X_train,y_train)","273975ad":"#Best Hyperparameter\nhyper_log_rdmFostCls.best_params_","9d121499":"#Evalauate the RandomForestClassifier model \nhyper_log_rdmFostCls.score(X_test,y_test)","788161a5":"#setup for LogisticRegression\nlog_reg_grid = {\"C\":np.logspace(-4,4,30),\n               \"solver\":[\"liblinear\"]}\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv= 5,\n                         verbose=True)\n#Fit grid hyperparameter search model\ngs_log_reg.fit(X_train,y_train)\n","9ca0a870":"#Check best hyperparameter\ngs_log_reg.best_params_","1e5a062a":"#Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test,y_test)","e5dd6835":"# Make preidctions on test data\ny_preds = gs_log_reg.predict(X_test)\ny_preds","377f3334":"# Plot ROC curve and calculate AUC metric\nfrom sklearn.metrics import plot_roc_curve\n\n\n#roc_curve(gs_log_reg, X_test)","e2d31292":"plot_roc_curve(gs_log_reg,X_test,y_test)","20fad11a":"#Confusion matrix\nprint(confusion_matrix(y_test,y_preds))","75027673":"# Import Seaborn\nimport seaborn as sbn\nsbn.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sbn.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, \n                     cbar=True)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","9dd6d05d":"#Classification report\nprint(classification_report(y_test,y_preds))","30289387":"#Check Best hyperparameter\ngs_log_reg.best_params_","e89e2d0e":"# New classifier with best parameter\nclf = LogisticRegression(C=0.20433597178569418,solver='liblinear')\nclf","86c99ab9":"#Crosss-validated accuracy\ncrss_acc = cross_val_score(clf,X,y,cv=5,scoring=\"accuracy\")\n#crss_acc\ncv_acc_mean = np.mean(crss_acc)\ncv_acc_mean","956e8977":"#Crosss-validated precision\ncrss_prec = cross_val_score(clf,X,y,cv=5,scoring=\"precision\")\ncv_prec_mean = np.mean(crss_prec)\ncv_prec_mean","9f853171":"#Crosss-validated recall\ncrss_rec = cross_val_score(clf,X,y,cv=5,scoring=\"recall\")\ncv_rec_mean = np.mean(crss_rec)\ncv_rec_mean","204523d7":"#Crosss-validated F1\ncrss_f1 = cross_val_score(clf,X,y,cv=5,scoring=\"f1\")\ncrss_f1_mean = np.mean(crss_f1)\ncrss_f1_mean","10df104a":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc_mean,\n                            \"Precision\": cv_prec_mean,\n                            \"Recall\": cv_rec_mean,\n                            \"F1\": crss_f1_mean},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);","c2bd4464":"#gs_log_reg.best_params_\n# Fit an instance of Logistic Regression\nclf  = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\nclf.fit(X_train,y_train)","04b4e035":"#Check coeff\nclf.coef_","cbb15ccf":"# Match features to columns\nfeatures_dict = dict(zip(heart_disease_data.columns, list(clf.coef_[0])))\nfeatures_dict","d6b45138":"# Visualize feature importance\nfeatures_heart_disease = pd.DataFrame(features_dict, index=[0])\nfeatures_heart_disease.T.plot.bar(title=\"Feature Importance\", legend=False, color=\"blue\");","de65f9dc":"pd.crosstab(heart_disease_data[\"sex\"], heart_disease_data[\"target\"])","ec055721":"### Slope (positive coefficient) with target\npd.crosstab(heart_disease_data[\"slope\"], heart_disease_data[\"target\"])\n\n","a888bf24":"### Calculate evaluation metrics using cross-validation\n* calculate accuracy,precision,recall and f1 score using cross-validation by `cross_val_score()` ","664d5a63":"# Predicting heart disease using Machine learning\n\n`Python-based machine learning model capable of predicting whether or not someone has heart disease based on their medical history.`","592c2f88":"#### Looking at these figures and this specific dataset, it seems if the patient is female, they're more likely to have heart disease.\n","c68cb893":"\n    0: Upsloping: better heart rate with excercise (uncommon)\n    1: Flatsloping: minimal change (typical healthy heart)\n    2: Downslopins: signs of unhealthy heart\n","4480bb68":"# 3. Evaluation\n\n  >  If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept.We will get success result.","390560a5":"# Tools Used\n > *** Pandas, Matplotlib and NumPy for data analysis and manipulation.***\n \n   * pandas for data analysis.\n   * NumPy for numerical operations.\n   * Matplotlib\/seaborn for plotting or data visualization.\n   * Scikit-Learn for machine learning modelling and evaluation.\n","bc0d0b74":" ***Hear disease frequency per Chest Pain Type***\n * cp - chest pain type\n* 0:  Typical angina: chest pain related decrease blood supply to the heart \n*  1: Atypical angina: chest pain not related to heart\n* 2: Non-anginal pain: typically esophageal spasms (non heart related)\n* 3: Asymptomatic: chest pain not showing signs of disease","c1bb4a0f":"## 6. Experimentation\n","b17e7b75":"### Evaluating a classification model, beyond accuracy\n\n   * ROC curve and AUC score - [plot_roc_curve()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.plot_roc_curve.html#sklearn.metrics.plot_roc_curve)\n   * Confusion matrix - [confusion_matrix()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html)\n   * Classification report - [classification_report()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html)\n   * Precision - [precision_score()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html)\n   * Recall - [recall_score()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html)\n   * F1-score - [f1_score()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html)\n","2bd8b212":"### Tuning model with GridSearchCv\n* The difference between RandomizedSearchCV and GridSearchCV is where RandomizedSearchCV searches over a grid of hyperparameters performing n_iter combinations, GridSearchCV will test every single possible combination.\n","807142e9":"# 1. Problem Definition\n\nIn a statement,\n\n> ***Given clinical parameters about a patient, can we predict whether or not they have heart disease?***\n","726d1bc0":"# 4. Features\n\n> This is where you'll get different information about each of the features in your data. You can do this via doing your own research (such as looking at the links above) or by talking to a subject matter expert (someone who knows about the dataset).\n\n> Create data dictionary\n\n `age - age in years\n    sex - (1 = male; 0 = female)\n    cp - chest pain type\n        0: Typical angina: chest pain related decrease blood supply to the heart\n        1: Atypical angina: chest pain not related to heart\n        2: Non-anginal pain: typically esophageal spasms (non heart related)\n        3: Asymptomatic: chest pain not showing signs of disease\n    trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n    chol - serum cholestoral in mg\/dl\n        serum = LDL + HDL + .2 * triglycerides\n        above 200 is cause for concern\n    fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n        '>126' mg\/dL signals diabetes\n    restecg - resting electrocardiographic results\n        0: Nothing to note\n        1: ST-T Wave abnormality\n            can range from mild symptoms to severe problems\n            signals non-normal heart beat\n        2: Possible or definite left ventricular hypertrophy\n            Enlarged heart's main pumping chamber\n    thalach - maximum heart rate achieved\n    exang - exercise induced angina (1 = yes; 0 = no)\n    oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n    slope - the slope of the peak exercise ST segment\n        0: Upsloping: better heart rate with excercise (uncommon)\n        1: Flatsloping: minimal change (typical healthy heart)\n        2: Downslopins: signs of unhealthy heart\n    ca - number of major vessels (0-3) colored by flourosopy\n        colored vessel means the doctor can see the blood passing through\n        the more blood movement the better (no clots)\n    thal - thalium stress result\n        1,3: normal\n        6: fixed defect: used to be defect but ok now\n        7: reversable defect: no proper blood movement when excercising\n    target - have disease or not (1=yes, 0=no) (= the predicted attribute)`\n","64806522":"### Hyperparameter Tuning","ee217d7e":"### Model Comparison","8811cf44":"\n# Data Exploration OR Exploratory data analysis(EDA)\n\n >  Goal here is to find out more about the data and become a subject matter export on the dataset.\n\n   1. What question(s) are you trying to solve?\n   2. What kind of data do we have and how do we treat different types?\n   3. What's missing from the data and how do you deal with it?\n   4. Where are the outliers and why should you care about them?\n   5. How can you add, change or remove features to get more out of your data?\n\n","86061bd6":"### Feature Importance\n* It means which feature contribute most to the outcomes of the model and How did they contribute?\n* Using Logistics Regression\n","b0c0e056":"\n***Model choices***\n\nNow we've got our data prepared, we can start to fit models. We'll be using the following and comparing their results.\n\n   1. Logistic Regression - LogisticRegression()\n   2. K-Nearest Neighbors - KNeighboursClassifier()\n   3.  RandomForest - RandomForestClassifier()\n\n* Scikit-Learn algorithm cheat sheet https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html","3d3033ea":"### Tuning models with with RandomizedSearchCV\n[Hyperparameter](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV)\n","9b7f79fb":"# Load data","d35988f8":">   If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursure this project.\n\n### Could you collect more data?\n\n* Could you try a better model? If you're working with structured data, you might want to look into [CatBoost](https:\/\/catboost.ai\/) or [XGBoost](https:\/\/xgboost.ai\/).\n\n* Could you improve the current models (beyond what we've done so far)?\n    If your model is good enough, how would you export it and share it with others? (Hint: check out [Scikit-Learn's documentation on model persistance](https:\/\/scikit-learn.org\/stable\/modules\/model_persistence.html))\n\n    #### The more you try, the more you figure out what doesn't work, the more you'll start to get a hang of what does.\n                                                                                     \n                    ","7ed64c56":"### Tune KNeighborsClassifier (K-Nearest Neighbors or KNN) by hand\n* There's one main hyperparameter we can tune for the K-Nearest Neighbors (KNN) algorithm, and that is number of neighbours. The default is 5 (n_neigbors=5).\n* Imagine all our different samples on one graph like the scatter graph we have above. KNN works by assuming dots which are closer together belong to the same class. If n_neighbors=5 then it assume a dot with the 5 closest dots around it are in the same class.\n","da0c6b7c":"#  2. Data\n\n> The original data came from the Cleavland data the UCI Machine Learning Repository. https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+Disease\n\n> Other version of it available on Kaggle. https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci","a8f683d5":"\n* Hyperparameter tuning - Each model you use has a series of dials you can turn to dictate how they perform. Changing these values may increase or decrease model performance.\n* Feature importance - If there are a large amount of features we're using to make predictions, do some have more importance than others? For example, for predicting heart disease, which is more important, sex or age?\n* [Confusion Matrix](https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/) - Compares the predicted values with the true values in a tabular way, if 100% correct, all values in the matrix will be top left to bottom right (diagnol line).\n* [Cross-Validation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html) - Splits your dataset into multiple parts and train and tests your model on each part and evaluates performance as an average.\n*    [Precision](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score) - Proportion of true positives over total number of samples. Higher precision leads to less false positives.\n*    [Recall](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score) - Proportion of true positives over total number of true positives and false negatives. Higher recall leads to less false negatives.\n*    [F1 score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score) - Combines precision and recall into one metric. 1 is best, 0 is worst.\n*    [Classification report](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html) - Sklearn has a built-in function called classification_report() which returns some of the main classification metrics such as precision, recall and f1-score.\n*    [ROC Curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_score.html) - Receiver Operating Characterisitc is a plot of true positive rate versus false positive rate.\n*    [Area Under Curve (AUC)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html) - The area underneath the ROC curve. A perfect model achieves a score of 1.0.\n","c0d03952":"# 5. Modelling","755b4080":"***My Approach :-***\n    \n    Problem definition\n    Data\n    Evaluation\n    Features\n    Modelling\n    Experimentation\n"}}