{"cell_type":{"a044260e":"code","b018b4c5":"code","b6e8bab2":"code","95b85e5f":"code","8e564fe5":"code","8a9dae39":"code","d9197621":"code","c854dbe0":"code","07c7578e":"code","7074ae65":"code","e021a7e8":"code","87c98aea":"code","2939f05d":"code","20c4a545":"code","dc55fca5":"code","bdb41990":"code","02bacf93":"code","e09139f3":"code","25d1e049":"code","37582294":"code","7a128ed9":"code","c0ffe6a8":"code","ac506cbb":"code","1a362e80":"code","a637ac69":"code","215ad3dc":"code","b756c83a":"code","03395863":"code","80248856":"code","a5e30bb5":"code","9c9d0c93":"code","da82d865":"code","5dac5b9e":"code","534f7971":"code","f9bd6ee6":"code","ba78c7be":"code","eb8d01d3":"code","df457a95":"code","26e5be91":"code","beb5a603":"code","c5a5bc86":"code","5305f65a":"code","8c62aaca":"code","05a582d4":"code","50afa800":"code","b8cc7b2d":"code","1a81848c":"code","cfad84a0":"code","9421352d":"markdown","a026e3e3":"markdown","cbd240d4":"markdown","c139d9c8":"markdown","23cb6686":"markdown","cdbaa0d0":"markdown","3749f994":"markdown","e24a0ce0":"markdown","98c944d6":"markdown"},"source":{"a044260e":"import numpy as np\nimport pandas as pd\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n","b018b4c5":"# Load sample data\n#df = pd.read_csv('..\/input\/EURUSD_Candlestick_15_M_BID_31.12.2016-23.08.2019.csv.csv')\ndf = pd.read_csv('..\/input\/eurusd1h\/EURUSD_Candlestick_1_Hour_BID_31.12.2009-30.08.2019.csv')","b6e8bab2":"df.head()\n","95b85e5f":"df.index.min(), df.index.max()","8e564fe5":"# FULL DATA (takes too long)\n# df = pd.read_csv('..\/input\/EURUSD_15m_BID_01.01.2010-31.12.2016.csv')","8a9dae39":"# Rename bid OHLC columns\nfrom datetime import datetime, timedelta\ndf.rename(columns={'Gmt time' : 'timestamp', 'Open' : 'open', 'Close' : 'close', \n                   'High' : 'high', 'Low' : 'low', 'Close' : 'close', 'Volume' : 'volume'}, inplace=True)\ndf['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)  - timedelta(hours=4,minutes=0,seconds=0)\n\ndf.set_index('timestamp', inplace=True)\ndf = df.astype(float)\ndf.head()\nfrom datetime import datetime, timedelta\ndf['hour'] = df.index.hour\ndf['day']  = df.index.weekday\ndf['week'] = df.index.week\ndf['momentum']  = df['volume'] * (df['open'] - df['close'])\n\ndf['avg_price'] = (df['low'] + df['high'])\/2\ndf['range']     = df['high'] - df['low']\ndf['ohlc_price'] = (df['low'] + df['high'] + df['open'] + df['close'])\/4\ndf['oc_diff']    = df['open'] - df['close']\ndf.head()\n","d9197621":"def heikin_ashi(df):\n    heikin_ashi_df = pd.DataFrame(index=df.index.values, columns=['open', 'high', 'low', 'close'])\n    \n    heikin_ashi_df['close'] = (df['open'] + df['high'] + df['low'] + df['close']) \/ 4\n    \n    for i in range(len(df)):\n        if i == 0:\n            heikin_ashi_df.iat[0, 0] = df['open'].iloc[0]\n        else:\n            heikin_ashi_df.iat[i, 0] = (heikin_ashi_df.iat[i-1, 0] + heikin_ashi_df.iat[i-1, 3]) \/ 2\n        \n    heikin_ashi_df['high'] = heikin_ashi_df.loc[:, ['open', 'close']].join(df['high']).max(axis=1)\n    \n    heikin_ashi_df['low'] = heikin_ashi_df.loc[:, ['open', 'close']].join(df['low']).min(axis=1)\n    \n    return heikin_ashi_df\n\ndf1=heikin_ashi(df)\n\ndf1['volume']=df['volume']\ndf1['ema8'] = pd.Series.ewm(df1['close'], span=8).mean()\ndf1['ema20'] = pd.Series.ewm(df1['close'], span=20).mean()\ndf1['ema50'] = pd.Series.ewm(df1['close'], span=50).mean()\ndf1['ema200'] = pd.Series.ewm(df1['close'], span=200).mean()\n\ndf1['hour'] = df1.index.hour\ndf1['day']  = df1.index.weekday\ndf1['week'] = df1.index.week\ndf1['momentum']  = df1['volume'] * (df1['open'] - df1['close'])\n\ndf1['avg_price'] = (df1['low'] + df1['high'])\/2\ndf1['range']     = df1['high'] - df1['low']\ndf1['ohlc_price'] = (df1['low'] + df1['high'] + df1['open'] + df1['close'])\/4\ndf1['oc_diff']    = df1['open'] - df1['close']\n\ndf1.tail(50)\ndf_test=df1[-30:]\ndf_test.head(30)\ndf1.dropna(how='any', inplace=True)\n\n","c854dbe0":"# Add PCA as a feature instead of for reducing the dimensionality. This improves the accuracy a bit.\nfrom sklearn.decomposition import PCA\n\ndataset = df.copy().values.astype('float32')\npca_features = df.columns.tolist()\n\npca = PCA(n_components=1)\ndf['pca'] = pca.fit_transform(dataset)","07c7578e":"import matplotlib.colors as colors\nimport matplotlib.cm as cm\nimport pylab\n\nplt.figure(figsize=(10,5))\nnorm = colors.Normalize(df['ohlc_price'].values.min(), df['ohlc_price'].values.max())\ncolor = cm.viridis(norm(df['ohlc_price'].values))\nplt.scatter(df['ohlc_price'].values, df['pca'].values, lw=0, c=color, cmap=pylab.cm.cool, alpha=0.3, s=1)\nplt.title('ohlc_price vs pca')\nplt.show()\n\nplt.figure(figsize=(10,5))\nnorm = colors.Normalize(df['volume'].values.min(), df['volume'].values.max())\ncolor = cm.viridis(norm(df['volume'].values))\nplt.scatter(df['volume'].values, df['pca'].values, lw=0, c=color, cmap=pylab.cm.cool, alpha=0.3, s=1)\nplt.title('volume vs pca')\nplt.show()\n\nplt.figure(figsize=(10,5))\nnorm = colors.Normalize(df['ohlc_price'].values.min(), df['ohlc_price'].values.max())\ncolor = cm.viridis(norm(df['ohlc_price'].values))\nplt.scatter(df['ohlc_price'].shift().values, df['pca'].values, lw=0, c=color, cmap=pylab.cm.cool, alpha=0.3, s=1)\nplt.title('ohlc_price - 15min future vs pca')\nplt.show()\n\nplt.figure(figsize=(10,5))\nnorm = colors.Normalize(df['volume'].values.min(), df['volume'].values.max())\ncolor = cm.viridis(norm(df['volume'].values))\nplt.scatter(df['volume'].shift().values, df['pca'].values, lw=0, c=color, cmap=pylab.cm.cool, alpha=0.3, s=1)\nplt.title('volume - 15min future vs pca')\nplt.show()","7074ae65":"df=df1\ndf.head(40)\ndf.to_pickle(\"eurusd.pkl\")\nimport os\nprint(os.getcwd())","e021a7e8":"os.listdir()","87c98aea":"def create_dataset(dataset, look_back=60):\n    dataX,dataY = [],[]\n    \n    for i in range(len(dataset)-look_back-1-7):\n        a = dataset[i:(i+look_back)]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back+7])\n    return np.array(dataX), np.array(dataY)","2939f05d":"colormap = plt.cm.inferno\nplt.figure(figsize=(15,15))\nplt.title('Pearson correlation of features', y=1.05, size=15)\nsns.heatmap(df.corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\nplt.show()\n\nplt.figure(figsize=(15,5))\ncorr = df.corr()\nsns.heatmap(corr[corr.index == 'close'], linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True);","20c4a545":"from sklearn.ensemble import RandomForestRegressor\n\n# Scale and create datasets\ntarget_index = df1.columns.tolist().index('close')\ndataset = df1.values.astype('float32')\n\n\n#   a = dataset[i:(i+look_back)\n#         dataX.append(a)\n#         dataY.append(dataset[i + look_back+8])\n# # Scale the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n# Set look_back to 20 which is 5 hours (15min*20)\nX, y = create_dataset(dataset, look_back=1)\ny = y[:,target_index]\nX = np.reshape(X, (X.shape[0], X.shape[2]))\n","dc55fca5":"forest = RandomForestRegressor(n_estimators = 100)\nforest = forest.fit(X, y)","bdb41990":"importances = forest.feature_importances_\nstd = np.std([forest.feature_importances_ for forest in forest.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\ncolumn_list = df.columns.tolist()\nprint(\"Feature ranking:\")\nfor f in range(X.shape[1]-1):\n    print(\"%d. %s %d (%f)\" % (f, column_list[indices[f]], indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure(figsize=(20,10))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"salmon\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","02bacf93":"# ax = df1.plot(x=df1.index, y='close', c='red', figsize=(40,10))\n# index = [str(item) for item in df1.index]\n# plt.fill_between(x=index, y1='low',y2='high', data=df1, alpha=0.4)\n# plt.show()\n# print(df1.index)\n\n\n\n# p = df[:200].copy()\n# ax = p.plot(x=p.index, y='close', c='red', figsize=(40,10))\n# index = [str(item) for item in p.index]\n# plt.fill_between(x=index, y1='low', y2='high', data=p, alpha=0.4)\n# plt.title('zoomed, first 200')\n# plt.show()","e09139f3":"%matplotlib inline\ndf1.head()","25d1e049":"df1['ema20'].plot(figsize=(100,8))","37582294":"# ax = df.plot(x=df.index, y='close', c='red', figsize=(40,10))\n# index = [str(item) for item in df.index]\n# plt.fill_between(x=index, y1='low',y2='high', data=df, alpha=0.4)\n# plt.show()\n\n# p = df[:200].copy()\n# ax = p.plot(x=p.index, y='close', c='red', figsize=(40,10))\n# index = [str(item) for item in p.index]\n# plt.fill_between(x=index, y1='low', y2='high', data=p, alpha=0.4)\n# plt.title('zoomed, first 200')\n# plt.show()","7a128ed9":"# Scale and create datasets\ntarget_index = df.columns.tolist().index('close')\nhigh_index = df.columns.tolist().index('high')\nlow_index = df.columns.tolist().index('low')\ndataset = df.values.astype('float32')\n\n# Scale the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n# Create y_scaler to inverse it later\ny_scaler = MinMaxScaler(feature_range=(0, 1))\nt_y = df['close'].values.astype('float32')\nt_y = np.reshape(t_y, (-1, 1))\ny_scaler = y_scaler.fit(t_y)\n\n# Set look_back to 20 which is 5 hours (15min*20)\nX, y = create_dataset(dataset, look_back=20)\ny = y[:,target_index]\nX.shape[2]\nX.shape[1]\n","c0ffe6a8":"# Set training data size\n# We have a large enough dataset. So divid into 98% training \/ 1%  development \/ 1% test sets\ntrain_size = int(len(X) * 0.95)\nprint(train_size)\nprint(len(X))\ntrainX = X[:train_size]\ntrainY = y[:train_size]\n\ntestX = X[train_size:]\ntestY = y[train_size:]\nprint(len(testX))\nprint(len(testY))\nX.shape[2]\n\n\n\n","ac506cbb":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Input, LSTM, Dense\n\n\n# create a small LSTM network\nmodel = Sequential()\nmodel.add(LSTM(290, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel.add(LSTM(49, return_sequences=True))\nmodel.add(LSTM(49, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(20, return_sequences=False))\nmodel.add(Dense(14, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='relu'))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\nprint(model.summary())","1a362e80":"# Save the best weight during training.\nfrom keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(\"EURUSD_7.best.hdf5\", monitor='val_mean_squared_error', verbose=1, save_best_only=True, mode='max')\n\n# Fit\ncallbacks_list = [checkpoint]\nhistory = model.fit(trainX, trainY, epochs=20, batch_size=100, verbose=1, callbacks=callbacks_list, validation_split=0.1)","a637ac69":"epoch = len(history.history['loss'])\nfor k in list(history.history.keys()):\n    if 'val' not in k:\n        plt.figure(figsize=(40,10))\n        plt.plot(history.history[k])\n        plt.plot(history.history['val_' + k])\n        plt.title(k)\n        plt.ylabel(k)\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()","215ad3dc":"min(history.history['val_mean_absolute_error'])","b756c83a":"# Baby the model a bit\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Input, LSTM, Dense\n\n\n# create a small LSTM network\nmodel = Sequential()\nmodel.add(LSTM(290, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\nmodel.add(LSTM(49, return_sequences=True))\nmodel.add(LSTM(49, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(20, return_sequences=False))\nmodel.add(Dense(14, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dense(1, kernel_initializer='uniform', activation='relu'))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\nprint(model.summary())\n# Load the weight that worked the best\nmodel.save(\"EURUSD_20.best.hdf5\")\nmodel.load_weights(\"EURUSD_20.best.hdf5\")\n\n# Train again with decaying learning rate\nfrom keras.callbacks import LearningRateScheduler\nimport keras.backend as K\n\ndef scheduler(epoch):\n    if epoch%10==0 and epoch!=0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr*.9)\n        print(\"lr changed to {}\".format(lr*.9))\n    return K.get_value(model.optimizer.lr)\nlr_decay = LearningRateScheduler(scheduler)\n\ncallbacks_list = [checkpoint, lr_decay]\nhistory = model.fit(trainX, trainY, epochs=int(1005), batch_size=32, verbose=1, callbacks=callbacks_list, validation_split=0.1)","03395863":"min(history.history['val_mean_absolute_error'])","80248856":"epoch = len(history.history['loss'])\nfor k in list(history.history.keys()):\n    if 'val' not in k:\n        plt.figure(figsize=(40,10))\n        plt.plot(history.history[k])\n        plt.plot(history.history['val_' + k])\n        plt.title(k)\n        plt.ylabel(k)\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()","a5e30bb5":"min(history.history['val_mean_absolute_error'])","9c9d0c93":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Benchmark\nmodel.load_weights(\"EURUSD_20.best.hdf5\")\n\npred = model.predict(testX)\n\npredictions = pd.DataFrame()\npredictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\npredictions['actual'] = testY\npredictions = predictions.astype(float)\n\npredictions.plot(figsize=(20,10))\nplt.show()\n\npredictions['diff'] = predictions['predicted'] - predictions['actual']\nplt.figure(figsize=(10,10))\nsns.distplot(predictions['diff']);\nplt.title('Distribution of differences between actual and prediction')\nplt.show()\n\nprint(\"MSE : \", mean_squared_error(predictions['predicted'].values, predictions['actual'].values))\nprint(\"MAE : \", mean_absolute_error(predictions['predicted'].values, predictions['actual'].values))\npredictions['diff'].describe()","da82d865":"predictions = pd.DataFrame()\npredictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\npredictions['actual'] = testY\npredictions = predictions.astype(float)\nimport plotly.express as px\n\nimport pandas as pd\n\nfig = px.line(predictions, )\nfig.show()\n","5dac5b9e":"print (testX\n      )","534f7971":"pred = model.predict(testX)\npred = y_scaler.inverse_transform(pred)\nclose = y_scaler.inverse_transform(np.reshape(testY, (testY.shape[0], 1)))\n\npredictions = pd.DataFrame()\npredictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\npredictions['close'] = pd.Series(np.reshape(close, (close.shape[0])))\nprint (predictions['predicted'] )\npredictions.tail()\n\n\np = df1[-pred.shape[0]:].copy()\npredictions.index = p.index\npredictions = predictions.astype(float)\npredictions.tail()\n\n# predictions = predictions.merge(p[['low', 'high']], right_index=True, left_index=True)\n# predictions.tail()\n# ax = predictions.plot(x=predictions.index, y='close', c='red', figsize=(40,10))\n# ax = predictions.plot(x=predictions.index, y='predicted', c='blue', figsize=(40,10), ax=ax)\n# index = [str(item) for item in predictions.index]\n# plt.fill_between(x=index, y1='low', y2='high', data=p, alpha=0.4)\n# plt.title('Prediction vs Actual (low and high as blue region)')\n# plt.show()\n\n# predictions['diff'] = predictions['predicted'] - predictions['close']\n# plt.figure(figsize=(10,10))\n# sns.distplot(predictions['diff']);\n# plt.title('Distribution of differences between actual and prediction ')\n# plt.show()\n\n# g = sns.jointplot(\"diff\", \"predicted\", data=predictions, kind=\"kde\", space=0)\n# plt.title('Distributtion of error and price')\n# plt.show()\n\n# predictions['correct'] = (predictions['predicted'] <= predictions['high']) & (predictions['predicted'] >= predictions['low'])\n# sns.factorplot(data=predictions, x='correct', kind='count')\n\n# print(\"MSE : \", mean_squared_error(predictions['predicted'].values, predictions['close'].values))\n# print(\"MAE : \", mean_absolute_error(predictions['predicted'].values, predictions['close'].values))\n# predictions['diff'].describe()","f9bd6ee6":"predictions.head()","ba78c7be":"df_test.info()","eb8d01d3":"dataset[0:20]","df457a95":"len(df_test)","26e5be91":"\ndef create_dataset_test(dataset, look_back=20):\n    dataX = []\n    for i in range(11):\n        a = dataset[i:(i+look_back)]\n        dataX.append(a)\n    return np.array(dataX)\n\n# Scale and create datasets\ntarget_index = df_test.columns.tolist().index('close')\nhigh_index = df_test.columns.tolist().index('high')\nlow_index = df_test.columns.tolist().index('low')\ndataset = df_test.values.astype('float32')\n\n\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n# Create y_scaler to inverse it later\n# y_scaler = MinMaxScaler(feature_range=(0, 1))\n# t_y = df_test['close'].values.astype('float32')\n# t_y = np.reshape(t_y, (-1, 1))\n# y_scaler = y_scaler.fit(t_y)\n\npred = y_scaler.inverse_transform(pred)\n\n# Set look_back to 20 which is 5 hours (15min*20)\nX_predict= create_dataset_test(dataset, look_back=20)\n\nX.shape[0]\n\n","beb5a603":"close = y_scaler.inverse_transform(np.reshape(testY, (testY.shape[0], 1)))\n","c5a5bc86":"df_test.head(30)","5305f65a":"# Scale and create datasets\ntarget_index = df_test.columns.tolist().index('close')\nhigh_index = df_test.columns.tolist().index('high')\nlow_index = df_test.columns.tolist().index('low')\ndataset = df_test.values.astype('float32')\n\n# Scale the data\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n\n# Create y_scaler to inverse it later\ny_scaler = MinMaxScaler(feature_range=(0, 1))\nt_y = df_test['close'].values.astype('float32')\nt_y = np.reshape(t_y, (-1, 1))\ny_scaler = y_scaler.fit(t_y)\n\n# Set look_back to 20 which is 5 hours (15min*20)\nX= create_dataset_test(dataset, look_back=20)\nX.shape","8c62aaca":"df_test.tail(30)","05a582d4":"for i in range(30-20-1):\n    print (i)\n    ","50afa800":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Benchmark\nmodel.load_weights(\"EURUSD.best.hdf5\")\n\npred = model.predict(X_predict)\nprint(pred)\npred = y_scaler.inverse_transform(pred)\nprint(pred)\na=pd.DataFrame(pred)\na.plot()\n\n\n# predictions = pd.DataFrame()\n# predictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\n# predictions['actual'] = testY\n# predictions = predictions.astype(float)\n\n# predictions.plot(figsize=(20,10))\n# plt.show()\n\n# predictions['diff'] = predictions['predicted'] - predictions['actual']\n# plt.figure(figsize=(10,10))\n# sns.distplot(predictions['diff']);\n# plt.title('Distribution of differences between actual and prediction')\n# plt.show()\n\n# print(\"MSE : \", mean_squared_error(predictions['predicted'].values, predictions['actual'].values))\n# print(\"MAE : \", mean_absolute_error(predictions['predicted'].values, predictions['actual'].values))\n# predictions['diff'].describe()","b8cc7b2d":"dtest=df_test['close']\ndtest = dtest.values.astype('float32')\nprint(dtest)","1a81848c":"X=[]\nfor i in range(30):\n    X.append(dtest[i])","cfad84a0":"print(X)","9421352d":"As seen from the above, the model seems to have converged nicely, but the mean absolute error on the development data remains at ~0.003X which means the model is unusable in practice. Ideally, we want to get ~0.0005. Let's go back to the best weight, and decay the learning rate while retraining the model","a026e3e3":"testX = X[train_size:]\ntestY = y[train_size:]","cbd240d4":"The variance should have improved slightly. However, unless the mean absolute error is not small enough. The model is still not an usable model in practice. This is mainly due to only using the sample data for training and limiting epoch to a few hundreds.","c139d9c8":"# Exploration","23cb6686":"# Visually compare the delta between the prediction and actual (scaled values)","cdbaa0d0":"# Introduction\n\nThis notebook trains a LSTM model that predicts the bid price of **EURUSD** 15 minutes in the future by looking at last five hours of data. While there is no requirement for the input to be contiguous, it's been empirically observed that having the contiguous input does improve the accuracy of the model. I suspect that having *day of the week* and *hour of the day* as the features mitigates some of the seasonality and contiguousness problems.\n\n**Disclaimer**: This exercise has been carried out using a small sample data which only contains 14880 samples (2015-12-29 00:00:00 to 2016-05-31 23:45:00) and lacks ASK prices. Which restricts the ability for the model to approach a better accuracy. \n\n**Improvements**\n* To tune the model further, I would recommend having at least 5 years worth of data, have ASK price (so that you can compute the spread), and increasing the epoch to 3000.\n* Adding more cross-axial features. Such as *spread*.\n* If you are looking into classification approach (PASS, BUY, SELL), consider adding some technical indicators that is more sensitive to more recent data. \n* Consider adding non-numerical data, e.g. news, Tweets. The catch is that you have to get the data under one minute for trading, otherwise the news will be reflected before you even make a trade. If anybody knows how to get the news streamed really fast, please let me know.\n\n**Credits** : Dave Y. Kim, Mahmoud Elsaftawy,","3749f994":"As observed above, using PCA shows data seperability that somehwat clusters the data into different price groups.","e24a0ce0":"# Doing a bit of features analysis","98c944d6":"# Compare the unscaled values and see if the prediction falls within the Low and High"}}