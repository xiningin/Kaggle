{"cell_type":{"09c7b40c":"code","d41cfab1":"code","130fc386":"code","311c430d":"code","1358d23c":"code","af6115d1":"code","50521b13":"code","92a5f180":"code","33115953":"code","ac4daf3b":"code","482db22c":"code","8ec5266d":"code","c8454551":"code","24b207d0":"code","9c463fc8":"code","d65b9c39":"code","bbf480f0":"code","0080c31c":"code","1ed30707":"code","c62b636c":"code","bdda1174":"code","20bfc6bc":"code","07ae7acf":"code","cc59df43":"code","aa4529df":"code","5df3d892":"code","5ef4d17a":"code","a885d760":"code","06f4daf5":"code","38d68cb3":"code","859d1744":"code","5fb13a86":"code","5596461d":"code","d2c10efb":"code","38083292":"code","79e203aa":"code","8734b3ec":"code","ff09edae":"code","2961f850":"code","82d28fe5":"code","06ff486d":"code","daccb9b3":"code","9c6245fb":"code","64461773":"code","72e4d93b":"code","d0f17d2d":"code","1e04eb58":"code","641a1c22":"code","358caf29":"code","677f2560":"code","b2e6c129":"code","b70d9c25":"code","69e4071a":"markdown","1e75f21e":"markdown","45f65eea":"markdown","bf097601":"markdown","0178576e":"markdown","8452f8e6":"markdown","c2b80c36":"markdown"},"source":{"09c7b40c":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master\/')\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport os\nfrom matplotlib import pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nsns.set_style('ticks')\nsns.set_context(\"poster\")\nsns.set_palette('colorblind')\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.modules.loss import _WeightedLoss\nfrom torch.utils.data import Dataset, DataLoader\nimport warnings\nwarnings.filterwarnings('ignore')\n# os.listdir('..\/input\/lish-moa')","d41cfab1":"plt.rcParams['figure.figsize'] = (20.0, 10.0)","130fc386":"device = ('cuda' if torch.cuda.is_available() else 'cpu')","311c430d":"params = {'device': device,\n          'n_comp_g': 450, \n          'n_comp_c': 45, \n          'var_thresh': 0.67,\n          'epochs': 25,\n          'batch_size': 128,\n          'lr': 1e-3,\n          'weight_decay': 1e-5, \n          'n_folds': 7, \n          'early_stopping_steps': 10,\n          'early_stop': False,\n          'in_size': None,\n          'out_size': None,\n          'hidden_size': 1500}","1358d23c":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv') # ..\/input\/lish-moa\/\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv') # ..\/input\/lish-moa\/\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv') # ..\/input\/lish-moa\/\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv') # ..\/input\/lish-moa\/","af6115d1":"train_features.shape","50521b13":"test_features.shape","92a5f180":"sample_submission.shape","33115953":"g_features = [col for col in train_features.columns if col.startswith('g-')]\nc_features = [col for col in train_features.columns if col.startswith('c-')]\n\ng_c_features = g_features + c_features","ac4daf3b":"transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")","482db22c":"trans_train_features = transformer.fit_transform(train_features[g_c_features])\ntrans_test_features = transformer.transform(test_features[g_c_features])\n\ntrans_train_df = pd.DataFrame(trans_train_features, columns = g_c_features)\ntrans_test_df = pd.DataFrame(trans_test_features, columns = g_c_features)\n\ntrain_features = pd.concat([train_features.drop(columns=g_c_features), trans_train_df], axis=1)\ntest_features = pd.concat([test_features.drop(columns=g_c_features), trans_test_df], axis=1)","8ec5266d":"g_sample = random.sample(g_features, 3)\nc_sample = random.sample(c_features, 3)","c8454551":"colors = ['navy', 'r', 'g']\nfor col, color in zip(g_sample, colors):\n    plt.hist(test_features[col], bins=50, alpha=0.5, label=col)\n    plt.axvline(np.median(test_features[col]), linewidth=3, color=color, label='median_{}'.format(col))\nplt.xlim(-7, 7)\nplt.legend();","24b207d0":"colors = ['navy', 'r', 'g']\nfor col, color in zip(c_sample, colors):\n    plt.hist(test_features[col], bins=50, alpha=0.5, label=col)\n    plt.axvline(np.median(test_features[col]), linewidth=3, color=color, label='median_{}'.format(col))\nplt.xlim(-7, 7)\nplt.legend();","9c463fc8":"def transfrom_all_data(transformer, train, test, feature_list):\n    \n    data = pd.concat([train[feature_list], test[feature_list]], axis=0).reset_index(drop=True)\n    n = train.shape[0]\n    \n    data_trans = transformer.fit_transform(data)\n    train_trans = data_trans[:n, :]\n    test_trans = data_trans[n:, :]\n    return train_trans, test_trans","d65b9c39":"def make_pca_features(n_comp, train, test, feature_list, name, normalize=False, scaler=None):\n    \n    pca = PCA(n_comp)\n    \n    train_pca, test_pca = transfrom_all_data(pca, train, test, feature_list)\n    \n    if normalize and scaler is not None:\n        train_pca = scaler.fit_transform(train_pca)\n        test_pca = scaler.transform(test_pca)\n    \n    for i in range(n_comp):\n        train['{0}_{1}'.format(name, i)] = train_pca[:, i]\n        test['{0}_{1}'.format(name, i)] = test_pca[:, i]\n        \n    return train, test","bbf480f0":"def preprocess(data):\n    data['cp_time'] = data['cp_time'].map({24:0, 48:1, 72:2})\n    data['cp_dose'] = data['cp_dose'].map({'D1':0, 'D2':1})\n    return data","0080c31c":"train_features, test_features = make_pca_features(params['n_comp_g'], train_features, test_features, g_features, 'g_pca')","1ed30707":"train_features, test_features = make_pca_features(params['n_comp_c'], train_features, test_features, c_features, 'c_pca')","c62b636c":"var_thresh = VarianceThreshold(params['var_thresh'])\nto_thresh = train_features.columns[4:]\ncat_features = train_features.columns[:4]","bdda1174":"train_thresh, test_thresh = transfrom_all_data(var_thresh, train_features, test_features, to_thresh)","20bfc6bc":"train_features = pd.concat([train_features[cat_features], pd.DataFrame(train_thresh)], axis=1)\ntest_features = pd.concat([test_features[cat_features], pd.DataFrame(test_thresh)], axis=1)","07ae7acf":"train_features.shape","cc59df43":"test_features.shape","aa4529df":"train_mask = train_features['cp_type'] != 'ctl_vehicle'\ntrain_sig_ids = train_features.loc[train_mask]['sig_id']\ntrain = train_features.loc[train_mask].reset_index(drop=True)\n\ntest_mask = test_features['cp_type'] != 'ctl_vehicle'\ntest_sig_ids = test_features.loc[test_mask]['sig_id']\ntest = test_features.loc[test_mask].reset_index(drop=True)\n\ntrain_target_sigids = train_targets[['sig_id']]\ny_true  = train_targets.copy()\n\ntrain_targets = train_targets[train_targets['sig_id'].isin(train_sig_ids)].reset_index(drop=True)\ntrain_targets.drop(columns=['sig_id'], inplace=True)\ntrain_targets.reset_index(drop=True, inplace=True)","5df3d892":"train.shape","5ef4d17a":"test.shape","a885d760":"y_true.shape","06f4daf5":"train_targets.shape","38d68cb3":"train_target_sigids","859d1744":"params['in_size'] = train.shape[1] - 2\nparams['out_size'] = train_targets.shape[1]","5fb13a86":"params['out_size']","5596461d":"params['in_size']","d2c10efb":"train.head()","38083292":"mskf = MultilabelStratifiedKFold(n_splits=params['n_folds'])","79e203aa":"folds = train.copy()\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train_targets)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)","8734b3ec":"folds.head()","ff09edae":"class TabularDataset:\n    \n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.tensor(self.X[i, :], dtype=torch.float)\n        y_i = torch.tensor(self.y[i, :], dtype=torch.float)\n        \n        return X_i, y_i\n    \n    \n\nclass TabularDatasetTest:\n    \n    def __init__(self, X):\n        self.X = X\n    \n    def __len__(self):\n        return(self.X.shape[0])\n    \n    def __getitem__(self, i):\n        \n        X_i = torch.tensor(self.X[i, :], dtype=torch.float)        \n        return X_i","2961f850":"def train_func(model, optimizer, scheduler, loss_func, dataloader, device):\n    \n    train_loss = 0\n    \n    model.train()  \n    for inputs, labels in dataloader:        \n        optimizer.zero_grad()\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        train_loss += loss.item()\n        \n    train_loss \/= len(dataloader)\n    \n    return train_loss","82d28fe5":"def valid_func(model, loss_func, dataloader, device):\n    \n    model.eval()\n    \n    valid_loss = 0\n    valid_preds = []\n    \n    for inputs, labels in dataloader:   \n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = loss_func(outputs, labels)\n        \n        valid_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    valid_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return valid_loss, valid_preds","06ff486d":"def inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data.to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","daccb9b3":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","9c6245fb":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.25)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","64461773":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = preprocess(folds.drop(columns = ['sig_id', 'cp_type']))\n    \n    train_mask = train['kfold'] != fold\n    valid_idc = train.loc[~train_mask].index\n    \n    X_train = train.loc[train_mask].reset_index(drop=True)\n    y_train = train_targets.loc[train_mask].reset_index(drop=True)\n\n    \n    X_val = train.loc[~train_mask].reset_index(drop=True)\n    y_val = train_targets.loc[~train_mask].reset_index(drop=True)\n    \n    X_train.drop(columns=['kfold'], inplace=True)\n    X_val.drop(columns=['kfold'], inplace=True)\n    \n    test_ = preprocess(test.drop(columns = ['sig_id', 'cp_type']))\n\n    \n    train_ds = TabularDataset(X_train.values, y_train.values)\n    valid_ds = TabularDataset(X_val.values, y_val.values)\n    test_ds = TabularDatasetTest(test_.values)\n    \n    train_dl = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n    valid_dl = DataLoader(valid_ds, batch_size=params['batch_size'], shuffle=False)\n    test_dl = DataLoader(test_ds, batch_size=params['batch_size'], shuffle=False)\n    \n    \n    model = Model(num_features=params['in_size'], num_targets=params['out_size'], \n                  hidden_size=params['hidden_size'] )\n    \n    model.to(params['device'])\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=params['epochs'], steps_per_epoch=len(train_dl))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing=0.001)\n    \n    early_stopping_steps = params['early_stopping_steps']\n    early_step = 0\n   \n    oof = np.zeros((train.shape[0], params['out_size']))\n    best_loss = np.inf\n    \n    for epoch in range(params['epochs']):\n        \n        train_loss = train_func(model, optimizer,scheduler, loss_tr, train_dl, params['device'])\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_func(model, loss_fn, valid_dl, params['device'])\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[valid_idc] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(params['early_stop'] == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n\n    \n    model = Model(num_features=params['in_size'], num_targets=params['out_size'], \n                  hidden_size=params['hidden_size'] )\n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(params['device'])\n    \n    \n    predictions = np.zeros((test.shape[0], params['out_size']))\n    predictions = inference_fn(model, test_dl, params['device'])\n    \n    return oof, predictions\n","72e4d93b":"def run_k_fold(n_folds, seed):\n    oof = np.zeros((train.shape[0], params['out_size']))\n    predictions = np.zeros((test.shape[0], params['out_size']))\n    \n    for fold in range(n_folds):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ n_folds\n        oof += oof_\n        \n    return oof, predictions","d0f17d2d":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","1e04eb58":"# Averaging on multiple SEEDS\n\nseeds = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((train.shape[0], params['out_size']))\npredictions = np.zeros((test.shape[0], params['out_size']))\n\nfor seed in seeds:\n    \n    oof_, predictions_ = run_k_fold(params['n_folds'], seed)\n    oof += oof_ \/ len(seeds)\n    predictions += predictions_ \/ len(seeds)","641a1c22":"valid_results = pd.concat([train_target_sigids[train_target_sigids['sig_id'].isin(train_sig_ids)].reset_index(drop=True), pd.DataFrame(oof)], axis=1)","358caf29":"test_results = pd.concat([test[['sig_id']], pd.DataFrame(predictions, columns = sample_submission.columns[1:])], axis=1)","677f2560":"valid_full = train_target_sigids.merge(valid_results, on='sig_id', how='left').fillna(0)","b2e6c129":"y_true = y_true.drop(columns=['sig_id']).values\ny_pred = valid_full.drop(columns=['sig_id']).values\n\nscore = 0\nfor i in range(y_true.shape[1]):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ y_true.shape[1]\n    \nprint(\"CV log_loss: \", score)    ","b70d9c25":"sub = sample_submission[['sig_id']].merge(test_results, on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","69e4071a":"## 1. Data preparation","1e75f21e":"### 1.2. PCA","45f65eea":"### 1.5. Cross validation split","bf097601":"### 1.3. Variance threshold","0178576e":"### 1.1. Quantile transform","8452f8e6":"## 2. Model","c2b80c36":"### 1.4. Control group removal"}}