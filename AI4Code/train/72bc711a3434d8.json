{"cell_type":{"75a03385":"code","9e952f6c":"code","426a1074":"code","4762d691":"code","82c289af":"code","29f116fe":"code","c982cf5f":"code","19463a0a":"code","8d1dbbdf":"code","08ff9e59":"code","358147e8":"code","9f8a28ee":"markdown","83f84dbc":"markdown"},"source":{"75a03385":"import numpy as np \nimport pandas as pd  \nimport cv2\nimport random\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt","9e952f6c":"#blending algorithms\n\n\n# 1. simple color transfer by rgb normalisation\n#https:\/\/github.com\/chia56028\/Color-Transfer-between-Images\/blob\/master\/color_transfer.py\n\ndef norm_color_transfer(src, dst):\n\n    def get_mean_and_std(x):\n        x_mean, x_std = cv2.meanStdDev(x)\n        x_mean = np.hstack(np.around(x_mean,2)).reshape(1,1,3)\n        x_std = np.hstack(np.around(x_std,2)).reshape(1,1,3)\n        return x_mean, x_std\n\n    s = cv2.cvtColor(src,cv2.COLOR_BGR2LAB)\n    t = cv2.cvtColor(dst,cv2.COLOR_BGR2LAB)\n    s_mean, s_std = get_mean_and_std(s)\n    t_mean, t_std = get_mean_and_std(t)\n    \n    # print(s_mean, s_std, t_mean, t_std)\n\n    m = (s-s_mean)*(t_std\/s_std)+t_mean\n    m = np.round(m)\n    m = np.clip(m,0,255).astype(np.uint8)\n\n    m = cv2.cvtColor(m,cv2.COLOR_LAB2BGR)\n    return m\n\n\n\n\n# 2. deep blending (in progress)\n# https:\/\/github.com\/owenzlz\/DeepImageBlending\n\n\n\n\n# 3. piosson editing  \n# https:\/\/github.com\/PPPW\/poisson-image-editing\nimport scipy.sparse\nfrom scipy.sparse.linalg import spsolve\n\n\ndef laplacian_matrix(n, m):\n    \"\"\"Generate the Poisson matrix.\n    Refer to:\n    https:\/\/en.wikipedia.org\/wiki\/Discrete_Poisson_equation\n    Note: it's the transpose of the wiki's matrix\n    \"\"\"\n    mat_D = scipy.sparse.lil_matrix((m, m))\n    mat_D.setdiag(-1, -1)\n    mat_D.setdiag(4)\n    mat_D.setdiag(-1, 1)\n\n    mat_A = scipy.sparse.block_diag([mat_D] * n).tolil()\n\n    mat_A.setdiag(-1, 1*m)\n    mat_A.setdiag(-1, -1*m)\n\n    return mat_A\n\n\ndef poisson_edit(source, target, mask, offset=(0,0)):\n    \"\"\"The poisson blending function.\n    Refer to:\n    Perez et. al., \"Poisson Image Editing\", 2003.\n    \"\"\"\n\n    # Assume:\n    # target is not smaller than source.\n    # shape of mask is same as shape of target.\n    y_max, x_max = target.shape[:-1]\n    y_min, x_min = 0, 0\n\n    x_range = x_max - x_min\n    y_range = y_max - y_min\n\n    M = np.float32([[1,0,offset[0]],[0,1,offset[1]]])\n    source = cv2.warpAffine(source,M,(x_range,y_range))\n\n    mask = mask[y_min:y_max, x_min:x_max]\n    mask[mask != 0] = 1\n    #mask = cv2.threshold(mask, 127, 1, cv2.THRESH_BINARY)\n\n    mat_A = laplacian_matrix(y_range, x_range)\n\n    # for \\Delta g\n    laplacian = mat_A.tocsc()\n\n    # set the region outside the mask to identity\n    for y in range(1, y_range - 1):\n        for x in range(1, x_range - 1):\n            if mask[y, x] == 0:\n                k = x + y * x_range\n                mat_A[k, k] = 1\n                mat_A[k, k + 1] = 0\n                mat_A[k, k - 1] = 0\n                mat_A[k, k + x_range] = 0\n                mat_A[k, k - x_range] = 0\n\n    # corners\n    # mask[0, 0]\n    # mask[0, y_range-1]\n    # mask[x_range-1, 0]\n    # mask[x_range-1, y_range-1]\n\n    mat_A = mat_A.tocsc()\n\n    mask_flat = mask.flatten()\n    for channel in range(source.shape[2]):\n        source_flat = source[y_min:y_max, x_min:x_max, channel].flatten()\n        target_flat = target[y_min:y_max, x_min:x_max, channel].flatten()\n\n        #concat = source_flat*mask_flat + target_flat*(1-mask_flat)\n\n        # inside the mask:\n        # \\Delta f = div v = \\Delta g\n        alpha = 1\n        mat_b = laplacian.dot(source_flat)*alpha\n\n        # outside the mask:\n        # f = t\n        mat_b[mask_flat==0] = target_flat[mask_flat==0]\n\n        x = spsolve(mat_A, mat_b)\n        #print(x.shape)\n        x = x.reshape((y_range, x_range))\n        #print(x.shape)\n        x[x > 255] = 255\n        x[x < 0] = 0\n        x = x.astype('uint8')\n        #x = cv2.normalize(x, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n        #print(x.shape)\n\n        target[y_min:y_max, x_min:x_max, channel] = x\n    return target","426a1074":"#helper\ndef make_blend_mask(size, object_box):\n    x,y,w,h = object_box\n    x0=x\n    x1=x+w\n    y0=y\n    y1=y+h\n\n\n    w,h = size\n    mask = np.ones((h,w,3),np.float32)\n\n    for i in range(0,y0):\n        mask[i]=i\/(y0)\n    for i in range(y1,h):\n        mask[i]=(h-i)\/(h-y1+1)\n    for i in range(0,x0):\n        mask[:,i]=np.minimum(mask[:,i],i\/(x0))\n    for i in range(x1,w):\n        mask[:,i]=np.minimum(mask[:,i],(w-i)\/(w-x1+1))\n\n    return mask\n\n\ndef insert_object(mix, box, crop, mask):\n    x,y,w,h = box\n    crop = cv2.resize(crop, dsize=(w,h), interpolation=cv2.INTER_AREA)\n    mask = cv2.resize(mask, dsize=(w,h), interpolation=cv2.INTER_AREA)\n\n    mix_crop = mix[y:y+h,x:x+w]\n    crop = norm_color_transfer(crop, mix_crop)\n    mix[y:y+h,x:x+w] = mask*crop +(1-mask)*mix_crop\n    return mix","4762d691":"import os\nfrom tqdm import tqdm\n\nfns = os.listdir('\/kaggle\/input\/cots-masks\/images')\n\ntotal_objects = []\n\nfor fn in tqdm(fns):\n    image = cv2.imread('..\/input\/cots-masks\/images\/' + fn, cv2.IMREAD_COLOR)\n    mask =  cv2.imread('..\/input\/cots-masks\/masks\/' + fn[:-3] + 'png', cv2.IMREAD_COLOR)\n\n    total_objects.append([image, mask])","82c289af":"df = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')\ndf = df[df['annotations'] == '[]']\n\ndef get_rand_background():\n    global df\n    \n    row = df.sample()\n    \n    image = cv2.imread(f'..\/input\/tensorflow-great-barrier-reef\/train_images\/video_{row.video_id.values[0]}\/{row.video_frame.values[0]}.jpg', cv2.IMREAD_COLOR)\n#     print(f'..\/input\/tensorflow-great-barrier-reef\/train_images\/video_{row.video_id.values[0]}\/{row.video_frame.values[0]}.jpg')\n\n    if random.random() > .5:\n        image = cv2.flip(image, 0)\n        \n    if random.random() > .5:\n        image = cv2.flip(image, 1)\n    \n    return image\n    ","29f116fe":"def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n    # initialize the dimensions of the image to be resized and\n    # grab the image size\n    dim = None\n    (h, w) = image.shape[:2]\n\n    # if both the width and height are None, then return the\n    # original image\n    if width is None and height is None:\n        return image\n\n    # check to see if the width is None\n    if width is None:\n        # calculate the ratio of the height and construct the\n        # dimensions\n        r = height \/ float(h)\n        dim = (int(w * r), height)\n\n    # otherwise, the height is None\n    else:\n        # calculate the ratio of the width and construct the\n        # dimensions\n        r = width \/ float(w)\n        dim = (width, int(h * r))\n\n    # resize the image\n    resized = cv2.resize(image, dim, interpolation = inter)\n\n    # return the resized image\n    return resized","c982cf5f":"!rm -rf labels\n!rm -rf images\n\n!mkdir images\n!mkdir labels","19463a0a":"kernel = np.ones((5,5),np.float32)\/25\n\n# cv2.rectangle(mix1, (x-50,y-50), (x+w+50,y+h+50), (255,255,255), 2)\n\n# mix = background.copy()\n\n\nfor im_id in tqdm(range(10000)):\n    mix = get_rand_background()\n    bboxes = []\n    for i in range(5):\n        crop, mask = total_objects[i]\n\n        x, y = random.randint(int(1280 * .1), int(1280 * .9)), random.randint(int(720 * .1), int(720 * .9))\n\n        # random resize\n        hi = random.randint(15, 100)\n        crop = image_resize(crop, height=hi)\n        mask = image_resize(mask, height=hi)\n\n\n        # w = random.randint(20, 100)\n        # h = max(20, w - random.randint(20, 30))\n\n    #     crop = cv2.resize(crop, dsize=(h,w), interpolation=cv2.INTER_AREA)\n    #     mask = cv2.resize(mask, dsize=(h,w), interpolation=cv2.INTER_AREA)\n\n\n        mask = cv2.filter2D(mask,-1,kernel)\n\n        # augmentations\n        if random.random() > .5:\n            crop = cv2.flip(crop, 0)\n            mask = cv2.flip(mask, 0)\n\n        if random.random() > .5:\n            crop = cv2.flip(crop, 1)\n            mask = cv2.flip(mask, 1)\n\n        if random.random() > .5:\n            crop = cv2.rotate(crop, cv2.ROTATE_90_COUNTERCLOCKWISE)\n            mask = cv2.rotate(mask, cv2.ROTATE_90_COUNTERCLOCKWISE)\n\n        if random.random() > .5:\n            crop = cv2.rotate(crop, cv2.ROTATE_180)\n            mask = cv2.rotate(mask, cv2.ROTATE_180)\n\n        if random.random() > .5:\n            crop = cv2.rotate(crop, cv2.ROTATE_90_CLOCKWISE)\n            mask = cv2.rotate(mask, cv2.ROTATE_90_CLOCKWISE)\n\n        h, w, _ = mask.shape\n\n    #     mask = np.float32(mask)\n        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n\n        nmask = np.zeros((h, w, 3))\n        nmask[:, :, 0] = mask\n        nmask[:, :, 1] = mask\n        nmask[:, :, 2] = mask\n\n    #     print(h,w)\n\n        nmask = (nmask\/255) * random.uniform(.7, .95)\n        # nmask =nmask\n\n        mix_crop = mix[y:y+h,x:x+w]\n        crop = norm_color_transfer(crop, mix_crop) \n    #     crop = poisson_edit(crop, mix_crop, (mask).astype(np.float32), offset=(0,0))\n\n        mix[y:y+h,x:x+w] = nmask*crop +(1-nmask)*mix_crop\n        \n        # blur top of it \n        # mix[y-int(h * .10):y+int(h * .10),x:x+w] = cv2.filter2D(mix[y-int(h * .10):y+int(h * .10),x:x+w],-1,kernel)\n\n        bboxes.append([x, y, w, h])\n\n        # cv2.rectangle(mix, (x-20,y-20), (x+w+20,y+h+20), (255,255,255), 2)\n    # Save image\n    cv2.imwrite(f'.\/images\/{im_id}.jpg', mix)\n    \n    with open(f'.\/labels\/{im_id}.txt', 'w') as f:\n        for x, y, w, h in bboxes:\n            f.write(f'0 {x \/ 1280} {y \/ 720} {w \/ 1280} {h \/ 720}\\n')\n\n    ","8d1dbbdf":"import shutil\nshutil.make_archive('labels', 'zip', 'labels')\n\nimport shutil\nshutil.make_archive('images', 'zip', 'images')","08ff9e59":"!rm -rf images\n!rm -rf labels","358147e8":"plt.figure()\nplt.imshow(mask[...,::-1])\n\nplt.figure(figsize=(25,25))\nplt.imshow(mix[...,::-1])","9f8a28ee":"# Augmentation using image blending\n\n- The images used for illustration purposes only. I just downloaded some creative commons license images from google image search. I haven't checked their licenses from the source website yet ... you are advised to **use your own images ** after checking their license !!!!\n\n- Alternatively, you can just use kaggle train images and transfer COTS objects from one frame to another frame\n","83f84dbc":"# This is a fork of:\nhttps:\/\/www.kaggle.com\/hengck23\/augmentation-using-image-blending\nand uses cots-masks from \nhttps:\/\/www.kaggle.com\/alexandrecc\/cots-masks\n\nSome changes are needed like style transfer to get best results. "}}