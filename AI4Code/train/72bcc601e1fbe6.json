{"cell_type":{"2e01381e":"code","fb0a3cbb":"code","a1bce8af":"code","8f84e826":"code","f0950a47":"code","7252f839":"code","95155dd7":"code","01dd4383":"code","54a56536":"code","2a95fd8f":"code","6f64b986":"code","b395824c":"code","ae552c18":"code","3bca68f2":"code","450ca0e2":"code","67634321":"code","3f6d9a77":"code","6f1f33fd":"code","26d70248":"code","7836f737":"code","e3045324":"code","d59be469":"code","27dca381":"code","9bb3ccae":"code","a1f4b148":"code","94c26b2c":"code","8656a2fe":"code","e3fa95f7":"code","7a104b03":"code","9588c62b":"code","de65a53e":"code","942e7228":"code","ca92dc4f":"code","e8619ee1":"code","bfb23c58":"code","02a6048b":"code","37656fe0":"code","e55cf2fa":"code","04c323c4":"code","a24aa6ea":"code","48902658":"code","b15bf374":"code","c9669f34":"code","be50e380":"markdown","59b54747":"markdown","97556c1a":"markdown","10a50d39":"markdown","8632d946":"markdown","1a1660d0":"markdown","742e365d":"markdown","1876a8e6":"markdown","9220e80b":"markdown","949a835b":"markdown","1666bb34":"markdown","4113b2d2":"markdown","8c7f9db3":"markdown","db72f460":"markdown","b89cfd61":"markdown","fd2f5025":"markdown","59e3df04":"markdown","8ce46bec":"markdown","df36f6b7":"markdown","050867ec":"markdown","3f53d905":"markdown","692d313c":"markdown","15e1bfd4":"markdown","696b4e5f":"markdown","5561fc6b":"markdown","b3e9566b":"markdown","a8061a93":"markdown","6b117261":"markdown","d18fecf6":"markdown","fb846ea1":"markdown","b12ad8e3":"markdown","10172623":"markdown","823209b6":"markdown","c5d00572":"markdown","891a894f":"markdown","cc3df3cd":"markdown","5ad5a18a":"markdown"},"source":{"2e01381e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.lines import Line2D\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\n\n\n\nimport sklearn\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","fb0a3cbb":"train_set = pd.read_csv('..\/input\/bike-sharing-demand\/train.csv')\nfinal_test = pd.read_csv('..\/input\/bike-sharing-demand\/test.csv')","a1bce8af":"print(train_set.columns)\nprint(final_test.columns)","8f84e826":"train_set.head()","f0950a47":"train_set.info()","7252f839":"train_set.isnull().sum()","95155dd7":"# extracting month and hour \ntemp = pd.DataFrame()\ntemp[\"month\"] = train_set.datetime.apply(lambda x : x.split()[0].split('-')[1]).astype('int')\ntemp[\"hour\"] = train_set.datetime.apply(lambda x : x.split()[1].split(':')[0]).astype('int')\ntrain_set = pd.concat([temp , train_set] , axis = 1)\ntrain_set.sample(10)","01dd4383":"train_set.drop(labels = 'datetime' , axis = 1 , inplace = True)\ntrain_set.info()","54a56536":"train_set.drop(labels = ['casual','registered'] , axis = 1 , inplace = True)\ntrain_set.head()","2a95fd8f":"corr = train_set.corr()\nplt.figure(figsize = (9,5))\nsns.heatmap(corr , cmap = 'coolwarm' , annot=True , fmt = '.2f')","6f64b986":"sns.catplot(data = train_set.replace({'season':{1:'spring' , 2:'summer' , 3:'fall' , 4:'winter'}} , inplace = False)\\\n           , x = 'season' , y = 'count' , kind = 'bar' , ci = None ,  alpha = 0.8)\nplt.title('Comparison between season' , fontdict = {'family':'serif' , 'size':16})\nprint( \"mean rentals in fall :\", train_set[train_set['season'] == 3]['count'].mean())","b395824c":"# mrw = mean rentals on working day\n# mrf = mean rentals on free day\n\nmrw = train_set[train_set['workingday'] == 1]['count'].mean()\nmrf = train_set[train_set['workingday'] == 0]['count'].mean()\n\nplt.pie([ mrw , mrf ] , labels = ['working day' , 'free day'] , explode = [0.1,0] , startangle = 10 , shadow = True)\nplt.text(-0.4,0.5,'%f'%(mrw))\nplt.text(-0.3,-0.5,'%f'%(mrf))\nplt.show()","ae552c18":"mean_per_month = pd.DataFrame()\nmeans = []\nmean_per_month['month'] = ['January','February','March','April','May','June','July','August','September','October'\\\n                                   ,'November','December']\nfor i in range(1,13):\n    means.append(train_set[train_set['month'] == i]['count'].mean())\n    \nmean_per_month['mean'] = means\nmean_per_month.sort_values('mean',ascending = False , inplace = True)\nprint(\"mean rentals per month in descending order :\")\nmean_per_month","3bca68f2":"ex = [0.2,0,0,0,0,0,0,0,0,0,0,0]\nplt.pie(mean_per_month['mean'] , labels = mean_per_month['month'] , shadow = True , explode = ex )\nplt.title('Mean per month')\nplt.show()","450ca0e2":"legend_dict = {'spring':'tab:green' , 'summer':'tab:red' , 'fall':'gold' , 'winter':'tab:blue'}\nline_color = ['tab:green','tab:red','gold','tab:blue']\n\n\nfor i in range(1,5): # we have 4 seasons from 1 to 4\n    y = []\n    for j in range(24): # also 24 hours from 0 to 23\n        y.append(train_set[(train_set['hour'] == j)&(train_set['season'] == i)]['count'].mean())\n    plt.plot(np.arange(24) , y , alpha = 0.9 , linestyle = '--' , linewidth = 2.2 , color = line_color[i-1])\n\n    \nhandles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=c, label=l, markersize=8) for l,c in legend_dict.items()]\nplt.legend(title='Seasons', handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.plot([8,8] , [0,600] , linestyle = ':' , color = \"k\")\nplt.plot([17,17] , [0,600] , linestyle = ':', color = \"k\")\n\nplt_font = {'family':'serif' , 'size':14}\nplt.xlabel('Hour' , fontdict = plt_font)\nplt.ylabel('Mean rentals' , fontdict = plt_font)\nplt.text(8.5,580,'x = 8')\nplt.text(17.5,580,'x = 17')","67634321":"x_set = train_set.iloc[:,:-1].values\ny_set = train_set.iloc[:,-1]\n\nx_train , x_test , y_train , y_test = train_test_split(x_set , y_set , test_size = 0.2 \n                                                       , random_state = 1)\n\nprint(\"x_train ->\" , x_train.shape)\nprint(\"y_train ->\" , y_train.shape)\nprint(\"x_test ->\" , x_test.shape)\nprint(\"y_test ->\" , y_test.shape)","3f6d9a77":"sc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","6f1f33fd":"# by simple, i mean i just use one input feature\n# remember that 'hour' has the highest correlation with 'count'\n\nslr = LinearRegression()\nslr.fit(np.reshape(x_train[:,1] , (-1 , 1)) , y_train)\n\nslr_pred_train = slr.predict(np.reshape(x_train[:,1] , (-1 , 1)))\nslr_pred_test = slr.predict(np.reshape(x_test[:,1] , (-1 , 1)))","26d70248":"slr_r2 = metrics.r2_score(y_test , slr_pred_test)\nprint(\"r2_score =\" , slr_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , slr_pred_test))","7836f737":"# lets use all the input features!\nmlr = LinearRegression()\nmlr.fit(x_train , y_train)\n\nmlr_pred_train = mlr.predict(x_train)\nmlr_pred_test = mlr.predict(x_test)","e3045324":"mlr_r2 = metrics.r2_score(y_test , mlr_pred_test)\nprint(\"r2 score =\" , mlr_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , mlr_pred_test))","d59be469":"features_deg2 = PolynomialFeatures(degree = 2)\n\nx_train_deg2 = features_deg2.fit_transform(x_train)\nx_test_deg2 = features_deg2.fit_transform(x_test)","27dca381":"# pr2 = polynomial regression degree 2\npr2 = LinearRegression()\npr2.fit(x_train_deg2 , y_train)\n\npr2_pred_train = pr2.predict(x_train_deg2) \npr2_pred_test = pr2.predict(x_test_deg2)","9bb3ccae":"pr2_r2 = metrics.r2_score(y_test , pr2_pred_test)\nprint(\"r2 score =\" , pr2_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , pr2_pred_test))","a1f4b148":"features_deg3 = PolynomialFeatures(degree = 3)\n\nx_train_deg3 = features_deg3.fit_transform(x_train)\nx_test_deg3 = features_deg3.fit_transform(x_test)","94c26b2c":"pr3 = LinearRegression()\npr3.fit(x_train_deg3 , y_train)\n\npr3_pred_train = pr3.predict(x_train_deg3) \npr3_pred_test = pr3.predict(x_test_deg3)","8656a2fe":"pr3_r2 = metrics.r2_score(y_test , pr3_pred_test)\nprint(\"r2 score =\" , pr3_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , pr3_pred_test))","e3fa95f7":"features_deg4 = PolynomialFeatures(degree = 4)\n\nx_train_deg4 = features_deg4.fit_transform(x_train) \nx_test_deg4 = features_deg4.fit_transform(x_test)","7a104b03":"pr4 = LinearRegression()\npr4.fit(x_train_deg4 , y_train)\n\npr4_pred_train = pr4.predict(x_train_deg4)\npr4_pred_test = pr4.predict(x_test_deg4)","9588c62b":"pr4_r2 = metrics.r2_score(y_test , pr4_pred_test)\nprint(\"r2 score =\" , pr4_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , pr4_pred_test))","de65a53e":"# lets do this with trial and error for finding the best k\nacc = [] # accuracy per k (by accuracy i mean r2_score)\nmax_r2 = 0\nbest_k = 0\nfor k in range(1,31):\n    knr = KNeighborsRegressor(n_neighbors = k)\n    knr.fit(x_train , y_train)\n    knr_pred = knr.predict(x_test)\n    r2 = metrics.r2_score(y_test , knr_pred)\n    acc.append(r2)\n    if r2 > max_r2:\n        max_r2 = r2\n        best_k = k\n\nprint(\"best k:\",best_k , \" with r2_score = \" , max_r2)\nplt.plot(np.arange(1,31) , acc)","942e7228":"# best k is 4\nknr = KNeighborsRegressor(n_neighbors = 4)\nknr.fit(x_train , y_train)\n\nknr_pred_train = knr.predict(x_train)\nknr_pred_test = knr.predict(x_test)\n\n# overfitting check :\nknr_r2_train = metrics.r2_score(y_train , knr_pred_train)\nknr_r2_test = metrics.r2_score(y_test , knr_pred_test)\nprint(\"%f not to bigger than %f , so its not much overfitted !\"%(knr_r2_train , knr_r2_test))","ca92dc4f":"knr_r2 = knr_r2_test\nprint(\"r2 score =\" , knr_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , knr_pred_test))","e8619ee1":"svr = SVR(kernel = 'rbf')\nsvr.fit(x_train , y_train)\n\nsvr_pred_train = svr.predict(x_train)\nsvr_pred_test = svr.predict(x_test)","bfb23c58":"svr_r2 = metrics.r2_score(y_test , svr_pred_test)\nprint(\"r2 score =\" , svr_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , svr_pred_test))","02a6048b":"# lets find the best min_samples_leaf with trial and error\nacc_test = [] # accuracy per k (by accuracy i mean r2_score)\nacc_train = []\nmax_r2_test = 0\nbest_k_test = 0\nfor k in range(1,201):\n    rt = DecisionTreeRegressor(min_samples_leaf = k , random_state = 1)\n    rt.fit(x_train , y_train)\n    rt_pred_train = rt.predict(x_train)\n    rt_pred_test = rt.predict(x_test)\n    r2_train = metrics.r2_score(y_train , rt_pred_train)\n    r2_test = metrics.r2_score(y_test , rt_pred_test)\n    acc_test.append(r2_test)\n    acc_train.append(r2_train)\n    if r2_test > max_r2_test:\n        max_r2_test = r2_test\n        best_k_test = k\n\nprint(\"best min_samples_leaf:\",best_k_test , \" with r2_score = \" , max_r2_test)\nplt.plot(np.arange(1,201) , acc_train , color = 'tab:blue')\nplt.plot(np.arange(1,201) , acc_test , color = 'tab:red')","37656fe0":"rt = DecisionTreeRegressor(min_samples_leaf = 8 , random_state = 1)\nrt.fit(x_train , y_train)\n\nrt_pred_train = rt.predict(x_train)\nrt_pred_test = rt.predict(x_test)","e55cf2fa":"rt_r2 = metrics.r2_score(y_test , rt_pred_test)\nprint(\"r2 score =\" , rt_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , rt_pred_test))","04c323c4":"rfr = RandomForestRegressor(random_state = 1)\nrfr.fit(x_train , y_train)\n\nrfr_pred_train = rfr.predict(x_train) \nrfr_pred_test = rfr.predict(x_test)","a24aa6ea":"rfr_r2 = metrics.r2_score(y_test , rfr_pred_test)\nprint(\"r2 score =\" , rfr_r2)\nprint(\"mean squared error =\" , metrics.mean_squared_error(y_test , rfr_pred_test))","48902658":"model_names = ['SimpleLinearRegression' , 'MultipleLinearRegression' , 'PolyRegressionD2' , 'PolyRegressionD3',\n              'PolyRegressionD4' , '4NearestNeighbors' , 'SupportVectorRegressor' , 'RegressionTree',\n              'RandomForestRegressor']\nmodel_r2_scores = [slr_r2 , mlr_r2 , pr2_r2 , pr3_r2 , pr4_r2 , knr_r2 , svr_r2 , rt_r2 , rfr_r2]\n\nplt.figure(figsize = (20,7))\nsns.barplot(x = model_names , y = model_r2_scores)\nplt.title(\"Model comaprison\" , fontdict = {'family':'serif' , 'size':14})\nplt.ylabel(\"R2 score\" , fontdict = {'family':'serif' , 'size':14})","b15bf374":"temp2 = pd.DataFrame()\ntemp2[\"month\"] = final_test.datetime.apply(lambda x : x.split()[0].split('-')[1]).astype('int')\ntemp2[\"hour\"] = final_test.datetime.apply(lambda x : x.split()[1].split(':')[0]).astype('int')\nfinal_test = pd.concat([temp2 , final_test] , axis = 1)\n\ninput_data = final_test.drop(labels = 'datetime' , axis = 1 , inplace = False)","c9669f34":"standard_scaling = StandardScaler()\ninput_data = standard_scaling.fit_transform(input_data)\n\nfinal_predictions = rfr.predict(input_data)\nfinal_test['count'] = final_predictions\nfinal_test = final_test[['datetime' , 'count']]\nfinal_test.to_csv('.\/submission.csv' , index = False)","be50e380":"<a id = \"5\"><\/a>\n## 5. Strory telling - Visualization","59b54747":"<a id = \"6_6\"><\/a>\n- <h3 style = color:tomato>K_Nearest Neighbors(KNN)<\/h3>","97556c1a":"<a id = \"5_2\"><\/a>","10a50d39":"<a id = \"4_4\"><\/a>\n- <h2 style = color:tomato>Scaling the data:<\/h2>","8632d946":"<a id = \"3\"><\/a>\n## 3. EDA","1a1660d0":"<a id = \"6_8\"><\/a>\n- <h3 style = color:tomato>Regression Tree<\/h3>","742e365d":"<a id = \"5_5\"><\/a>","1876a8e6":"<a id = \"6_9\"><\/a>\n- <h3 style = color:tomato>Random Forest Regressor<\/h3>","9220e80b":"<a id = \"6_5\"><\/a>\n- <h3 style = color:tomato>Polynomial regression(degree = 4)<\/h3>","949a835b":"<h1 style=\"color:tomato;\">Note :<\/h1>\n\n- [x] its obvious that number of casual + number of registered = count\n- [x] and in test.csv we dont have casual and registered columns <b>because if we have them, there is no need for prediction and we can easily add them together to find the total count !<\/b>\n\n<h3 style=\"color:#7bc043\">so we should drop casual and registered columns<\/h3>","1666bb34":"- <h3 style = color:#005b96>i dont continue increasing the degree of features because its going to be costly with respect of time<\/h3>","4113b2d2":"<a id = \"6_2\"><\/a>\n- <h3 style = color:tomato>Multiple linear regression<\/h3>","8c7f9db3":"<h2 style=\"color:#005b96\">Bar plot results :<\/h2>\n\n- <h3>Between seasons, fall has the most mean of rentals.<\/h3>\n- <h3>Also spring has the least mean of rentals.<\/h3>\n- <h3>So we can say that people use rental bicycles more, in fall !<\/h3>","db72f460":"<h3 style=\"color:#7bc043\">i will do scaling at the end of the visualization phase<\/h3>","b89cfd61":"<h2 style=\"color:#005b96\">line plot results :<\/h2>\n\n- <h4>Each colored line represents mean of rental bicycles in a specific season during day hours.<\/h4>\n- <h4>The plot shows that seasons have approximately same behaviors.(due to their maximums and minimums)<\/h4>\n- <h4>x = 7 is the global maximum and we can see a local maximum at x = 8<\/h4>\n- <h4>It means at 8:00 and 17:00 we have the most number of rental bicycles(people are going to work and returning from work)<\/h4>\n- <h4>Also the global minimum occurs around 4:00 <\/h4>\n- <h4>From 4:00 to 8:00 there is an increasing behavior same in 15:00 to 17:00<\/h4>","fd2f5025":"<a id = \"2\"><\/a>\n## 2. Load and Prepare Data","59e3df04":"<a id = \"6_7\"><\/a>\n- <h3 style = color:tomato>Support Vector Regression(SVR)<\/h3>","8ce46bec":"<a id = \"6_4\"><\/a>\n- <h3 style = color:tomato>Polynomial regression(degree = 3)<\/h3>","df36f6b7":"<h1 style=\"color:tomato;\">Note :<\/h1>\n\n-  as you see casual and registered columns are not in the test.csv\n-  but why? (i will answer to this question in the preprocessing phase)","050867ec":"- <h4>The above plot is showing that with min_samples_leaf = 1 the model is not learning, it just memorize the data, as you see the r2 for train is 1 but for test r2 is not good!<\/h4>\n- <h4>So when we increase the min_samples_leaf, r2 for train is decreasing and r2 for test is increasing, because model is learning not memorizing<\/h4>\n- <h4>The best state happens when min_samples_leaf = 8<\/h4>","3f53d905":"<a id = \"7\"><\/a>\n- <h2 style = color:tomato>Models comparison<\/h2>","692d313c":"<a id = \"4\"><\/a>\n## 4. Data Preprocessing","15e1bfd4":"<a id = \"5_3\"><\/a>","696b4e5f":"<a id = \"6_1\"><\/a>\n- <h3 style = color:tomato>Simple linear regression<\/h3>","5561fc6b":"<h2 style = color:tomato>Thanks for reading<\/h2>\n<h2 style = color:tomato>Hope to be helpful<\/h2>","b3e9566b":"<h2 style=\"color:#005b96\">pie plot results :<\/h2>\n\n- <h3>From the pie plot starting with January if you turn clockwise, mean of rentals is increasing<\/h3>\n- <h3>People use rental bicycles in June more than other months<\/h3>\n- <h3>In January we have the minimum usage of rental bicycles !<\/h3>","a8061a93":"<a id = \"1\"><\/a>\n## 1. Importing the libraries","6b117261":"<a id = \"4_2\"><\/a>\n<h1 style=\"color:tomato;\">From the datetime some usefull features could be extracted!<\/h1>\n<h2 style=\"color:#005b96\">But :<\/h2>\n\n- <h3> date in train.csv starts from 1th and ends in 19th for each month <\/h3>\n- <h3> and date in test.csv starts from 20th to the end<\/h3>\n- <h3> so because of the different range for these files, i will not extract day from datetime<\/h3>\n- <h3 style=\"color:#7bc043\"> i will extract <u>month<\/u> and <u>hour<\/u> from the datetime column <\/h3>","d18fecf6":"<a id = \"6_3\"><\/a>\n- <h3 style = color:tomato>Polynomial regression(degree = 2)<\/h3>","fb846ea1":"<h2 style=\"color:#005b96\">Pie plot results :<\/h2>\n\n- <h3>There is not a big difference in rentals between working days and free days<\/h3>\n- <h3>But we can say that people use rental bicycles in working days more than free days !<\/h3>","b12ad8e3":"<h2 style=\"color:#005b96\">Heat map results :<\/h2>\n\n- <h3>Between input features 'hour' has the strongest correlation with 'count'<\/h3>\n- <h3>So we will use 'hour' for simple linear regression<\/h3>","10172623":"<h1 style=\"color:tomato;\">In this notebook i will do :<\/h1>\n\n---\n- 1.<a href=\"#1\">Importing the libraries<\/a>\n\n\n- 2.<a href=\"#2\">Load the data<\/a>\n\n\n- 3.<a href=\"#3\">EDA<\/a>\n\n\n- 4.<a href=\"#4\">Preprocessing<\/a>\n- - 4-1.<a href=\"#4_1\">Missing values<\/a>\n- - 4-2.<a href=\"#4_2\">Extract month and hour from the datetime<\/a>\n- - 4-3.<a href=\"#4_3\">Train test split<\/a>\n- - 4-4.<a href=\"#4_4\">Scaling<\/a>\n\n\n\n- 5.<a href=\"#5\">Strory telling - Visualization<\/a>\n- - 5-1.<a href=\"#5_1\">Linear correlations<\/a>\n- - 5-2.<a href=\"#5_2\">Seasons comparison<\/a>\n- - 5-3.<a href=\"#5_3\">Work day VS free day<\/a>\n- - 5-4.<a href=\"#5_4\">Mean rentals per month<\/a>\n- - 5-5.<a href=\"#5_5\">Mean rentals per hour<\/a>\n\n\n\n- 6.<a href=\"#6\">Models :<\/a>\n- - 6-1.<a href=\"#6_1\">Simple linear regression<\/a>\n- - 6_2.<a href=\"#6_2\">Multiple linear regression<\/a>\n- - 6_3.<a href=\"#6_3\">Polynomial regression(degree = 2)<\/a>\n- - 6_4.<a href=\"#6_4\">Polynomial regression(degree = 3)<\/a>\n- - 6_5.<a href=\"#6_5\">Polynomial regression(degree = 4)<\/a>\n- - 6_6.<a href=\"#6_6\">K_Nearest Neighbors<\/a>\n- - 6_7.<a href=\"#6_7\">Support Vector Regression<\/a>\n- - 6_8.<a href=\"#6_8\">Regression Tree<\/a>\n- - 6_9.<a href=\"#6_9\">Random forest<\/a>\n\n- 7.<a href=\"#7\">Models comparison<\/a>","823209b6":"<a id = \"4_1\"><\/a>\n<h3 style=\"color:#005b96\">no missing values -->> no need to dropna or impute<\/h3>","c5d00572":"<a id = \"5_4\"><\/a>","891a894f":"<a id = \"6\"><\/a>\n## 6. Train the model","cc3df3cd":"<a id = \"4_3\"><\/a>\n- <h2 style = color:tomato>Train test split:<\/h2>","5ad5a18a":"<a id = \"5_1\"><\/a>\n<h2 style=\"color:tomato;\">correlation between attributes :<\/h2>"}}