{"cell_type":{"31efe730":"code","666aa330":"code","33521dde":"code","a08c7ee7":"code","b2e0279a":"code","629b12d3":"code","69cb00ed":"code","61feabf0":"code","d56b849b":"code","24062299":"code","5e7b2c98":"code","2c02f1ab":"code","01982e69":"code","5b89da36":"code","dca4cd35":"code","3f97e03e":"markdown","6172f7c7":"markdown","79e52a20":"markdown","66065744":"markdown"},"source":{"31efe730":"import argparse\nimport json\nimport os\nimport random\nimport re\nimport shutil\nfrom collections import OrderedDict, defaultdict\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport tqdm\n\n\nfrom sklearn.model_selection import GroupKFold\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom transformers import RobertaConfig, RobertaModel, RobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer\nfrom transformers.optimization import (AdamW, get_cosine_schedule_with_warmup,\n                                       get_linear_schedule_with_warmup,\n                                       get_cosine_with_hard_restarts_schedule_with_warmup)","666aa330":"from shutil import copyfile\ncopyfile(src = \"..\/input\/utils-v10\/utilsv10.py\", dst = \"..\/working\/utilsv10.py\")\ncopyfile(src = \"..\/input\/utils-v10\/dataset10.py\", dst = \"..\/working\/dataset10.py\")","33521dde":"from utilsv10 import (binary_focal_loss, get_learning_rate, jaccard_list, get_best_pred, ensemble, ensemble_words,get_char_prob,\n                   load_model, save_model, set_seed, write_event, evaluate, get_predicts_from_token_logits)","a08c7ee7":"from dataset10 import TrainDataset, MyCollator","b2e0279a":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","629b12d3":"tokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-base\/', do_lower_case=False)","69cb00ed":"class Args:\n    post = True\n    tokenizer = tokenizer\n    offset = 4\n    batch_size = 32\n    workers = 1\nargs = Args()","61feabf0":"collator = MyCollator()\ntest_set = TrainDataset(test, None, tokenizer=tokenizer, mode='test', offset=args.offset)\ntest_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, collate_fn=collator,\n                                 num_workers=args.workers)","d56b849b":"class TweetModel(nn.Module):\n\n    def __init__(self, pretrain_path=None, dropout=0.2, config=None):\n        super(TweetModel, self).__init__()\n        if config is not None:\n            self.bert = AutoModel.from_config(config)\n        else:\n            config = AutoConfig.from_pretrained(pretrain_path, output_hidden_states=True)\n            self.bert = AutoModel.from_pretrained(\n                pretrain_path, cache_dir=None, config=config)\n        \n        self.cnn =  nn.Conv1d(self.bert.config.hidden_size*3, self.bert.config.hidden_size, 3, padding=1)\n\n        # self.rnn = nn.LSTM(self.bert.config.hidden_size, self.bert.config.hidden_size\/\/2, num_layers=2,\n        #                     batch_first=True, bidirectional=True)\n        self.gelu = nn.GELU()\n\n        self.whole_head = nn.Sequential(OrderedDict([\n            ('dropout', nn.Dropout(0.1)),\n            ('l1', nn.Linear(self.bert.config.hidden_size*3, 256)),\n            ('act1', nn.GELU()),\n            ('dropout', nn.Dropout(0.1)),\n            ('l2', nn.Linear(256, 2))\n        ]))\n        self.se_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.inst_head = nn.Linear(self.bert.config.hidden_size, 2)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, inputs, masks, token_type_ids=None, input_emb=None):\n        _, pooled_output, hs = self.bert(\n            inputs, masks, token_type_ids=token_type_ids, inputs_embeds=input_emb)\n\n        seq_output = torch.cat([hs[-1],hs[-2],hs[-3]], dim=-1)\n\n        # seq_output = hs[-1]\n\n        avg_output = torch.sum(seq_output*masks.unsqueeze(-1), dim=1, keepdim=False)\n        avg_output = avg_output\/torch.sum(masks, dim=-1, keepdim=True)\n        # +max_output\n        whole_out = self.whole_head(avg_output)\n\n        seq_output = self.gelu(self.cnn(seq_output.permute(0,2,1)).permute(0,2,1))\n        \n        se_out = self.se_head(self.dropout(seq_output))  #()\n        inst_out = self.inst_head(self.dropout(seq_output))\n        return whole_out, se_out[:, :, 0], se_out[:, :, 1], inst_out","24062299":"def predict(model: nn.Module, valid_df, valid_loader, args, progress=False) -> Dict[str, float]:\n    # run_root = Path('..\/experiments\/' + args.run_root)\n    model.eval()\n    all_end_pred, all_whole_pred, all_start_pred, all_inst_out = [], [], [], []\n    if progress:\n        tq = tqdm.tqdm(total=len(valid_df))\n    with torch.no_grad():\n        for tokens, types, masks, _, _, _, _, _, _, _ in valid_loader:\n            if progress:\n                batch_size = tokens.size(0)\n                tq.update(batch_size)\n            masks = masks.cuda()\n            tokens = tokens.cuda()\n            types = types.cuda()\n            whole_out, start_out, end_out, inst_out = model(tokens, masks, types)\n            \n            all_whole_pred.append(torch.softmax(whole_out, dim=-1)[:,1].cpu().numpy())\n            inst_out = torch.softmax(inst_out, dim=-1)\n            for idx in range(len(start_out)):\n                length = torch.sum(masks[idx,:]).item()-1 # -1 for last token\n                all_start_pred.append(torch.softmax(start_out[idx, args.offset:length], axis=-1).cpu())\n                all_end_pred.append(torch.softmax(end_out[idx, args.offset:length], axis=-1).cpu())\n                all_inst_out.append(inst_out[idx,:,1].cpu())\n            assert all_start_pred[-1].dim()==1\n\n    all_whole_pred = np.concatenate(all_whole_pred)\n    \n    if progress:\n        tq.close()\n    return all_whole_pred, all_start_pred, all_end_pred, all_inst_out","5e7b2c98":"config = RobertaConfig.from_pretrained('..\/input\/roberta-base', output_hidden_states=True)\nmodel = TweetModel(config=config)\n","2c02f1ab":"all_whole_preds, all_start_preds, all_end_preds, all_inst_preds = [], [], [], []\n\n    \nfor fold in range(10):\n    load_model(model, '..\/input\/roberta-v10-10\/best-model-%d.pt' % fold)\n    model.cuda()\n    fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds = predict(model, test, test_loader, args, progress=True)\n\n    all_whole_preds.append(fold_whole_preds)\n    all_start_preds.append(fold_start_preds)\n    all_end_preds.append(fold_end_preds)\n    all_inst_preds.append(fold_inst_preds)\n\n\nall_whole_preds, all_start_preds, all_end_preds, all_inst_preds = ensemble(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test)\nword_preds, inst_word_preds, scores = get_predicts_from_token_logits(all_whole_preds, all_start_preds, all_end_preds, all_inst_preds, test, args)\n# word_preds, inst_word_preds, scores = get_predicts_from_token_logits(fold_whole_preds, fold_start_preds, fold_end_preds, fold_inst_preds, test, args)\nstart_char_prob, end_char_prob = get_char_prob(all_start_preds, all_end_preds, test, args)\n","01982e69":"test['start_char_prob'] = start_char_prob\ntest['end_char_prob'] = end_char_prob","5b89da36":"test['selected_text'] = word_preds\n# test.loc[replace_idx, 'selected_text'] = test.loc[replace_idx, 'text']\ndef f(selected):\n    return \" \".join(set(selected.lower().split()))\n# test.selected_text = test.selected_text.map(f)\ntest[['textID','selected_text']].to_csv('submission.csv', index=False)","dca4cd35":"test.head(20)","3f97e03e":"## model","6172f7c7":"## dataset","79e52a20":"## predict","66065744":"## Parse data"}}