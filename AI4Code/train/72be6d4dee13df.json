{"cell_type":{"7c5653e2":"code","18954a4a":"code","91cf0a54":"code","0f4ab584":"code","1862ef2e":"code","a7f6fb3f":"code","c67c700b":"code","9eebc112":"code","c8b7f1b7":"code","4c9bf6fb":"code","28acde63":"code","11d4e90d":"code","31a142ca":"code","1f85f353":"code","6f34be44":"code","93b188dd":"code","d36710da":"code","374da108":"code","497f1f6e":"code","779c289d":"code","f366a9da":"code","5b7d706d":"code","e0271755":"code","99428bed":"code","b097e994":"code","80acd9a2":"code","fb3fe1ca":"code","4eb42d4b":"code","7a3a8407":"code","36f95c0a":"code","e6f6d061":"code","2529804c":"code","00c23375":"code","268783e5":"code","7a09ded8":"code","25f64fdd":"code","557624c2":"code","7ae87bbd":"code","02e55923":"code","2b590946":"code","6bc66df7":"code","2ad1e953":"code","26ed361f":"code","cedbd2b2":"code","78f0a879":"code","41f3a68c":"code","1111282a":"code","24ce4a0c":"code","371cdc8f":"code","889ec8f7":"code","e3c02a5a":"code","1a389719":"code","deeab3e1":"code","e8631a0a":"markdown","d00355e9":"markdown","ba41e4b6":"markdown","fab7ee99":"markdown","a57ca77e":"markdown","9e70e137":"markdown","f29fd171":"markdown","c8f64537":"markdown","8ccf9c97":"markdown","a67c21bd":"markdown","b8e699f8":"markdown","f3f00d46":"markdown","15e2161b":"markdown","2e3115e7":"markdown","f429d94b":"markdown","3d3490eb":"markdown","aa81c73d":"markdown","1883500d":"markdown","0ca27757":"markdown","4e981a89":"markdown","820edbcd":"markdown","2f17e3f7":"markdown","c95ac0f3":"markdown","1b6a2b70":"markdown","a0ba3d1f":"markdown","489491b5":"markdown","568d0af9":"markdown","15a91f0b":"markdown","5e1e3ef9":"markdown","bffbab4d":"markdown","8d55bcdc":"markdown","36b0d846":"markdown","87096ec1":"markdown","311d53f2":"markdown","8b8c0b99":"markdown","dc55901c":"markdown","8a404f74":"markdown","6f8f775f":"markdown","ed5515c4":"markdown","d2ed19f1":"markdown","4859e5c2":"markdown","e677412b":"markdown","ab2d68b4":"markdown","20a5854d":"markdown","06f0b750":"markdown","83442374":"markdown","25e930f9":"markdown","8874a834":"markdown"},"source":{"7c5653e2":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torch.utils.data as data\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom sklearn import decomposition\nfrom sklearn import manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time\n\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","18954a4a":"class VGG(nn.Module):\n    def __init__(self, features, output_dim):\n        super().__init__()\n        \n        self.features = features\n        \n        self.avgpool = nn.AdaptiveAvgPool2d(7)\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace = True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace = True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, output_dim),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.classifier(h)\n        return x, h","91cf0a54":"vgg11_config = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\nvgg13_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\nvgg16_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, \n                512, 'M']\n\nvgg19_config = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n                512, 512, 512, 512, 'M']","0f4ab584":"def get_vgg_layers(config, batch_norm):\n    \n    layers = []\n    in_channels = 3\n    \n    for c in config:\n        assert c == 'M' or isinstance(c, int)\n        if c == 'M':\n            layers += [nn.MaxPool2d(kernel_size = 2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, c, kernel_size = 3, padding = 1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace = True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace = True)]\n            in_channels = c\n            \n    return nn.Sequential(*layers)","1862ef2e":"vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)\nprint(vgg11_layers)","a7f6fb3f":"OUTPUT_DIM = 10\n\nmodel = VGG(vgg11_layers, OUTPUT_DIM)\n\nprint(model)","c67c700b":"import torchvision.models as models\n\npretrained_model = models.vgg11_bn(pretrained = True)\n\nprint(pretrained_model)","9eebc112":"pretrained_model.classifier[-1]","c8b7f1b7":"\nIN_FEATURES = pretrained_model.classifier[-1].in_features \n\nfinal_fc = nn.Linear(IN_FEATURES, OUTPUT_DIM)","4c9bf6fb":"pretrained_model.classifier[-1] = final_fc","28acde63":"print(pretrained_model.classifier)","11d4e90d":"model.load_state_dict(pretrained_model.state_dict())","31a142ca":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","1f85f353":"pretrained_size = 224\npretrained_means = [0.485, 0.456, 0.406]\npretrained_stds= [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n                           transforms.Resize(pretrained_size),\n                           transforms.RandomRotation(5),\n                           transforms.RandomHorizontalFlip(0.5),\n                           transforms.RandomCrop(pretrained_size, padding = 10),\n                           transforms.ToTensor(),\n                           transforms.Normalize(mean = pretrained_means, \n                                                std = pretrained_stds)\n                       ])\n\ntest_transforms = transforms.Compose([\n                           transforms.Resize(pretrained_size),\n                           transforms.ToTensor(),\n                           transforms.Normalize(mean = pretrained_means, \n                                                std = pretrained_stds)\n                       ])","6f34be44":"ROOT = '.data'\n\ntrain_data = datasets.CIFAR10(ROOT, \n                              train = True, \n                              download = True, \n                              transform = train_transforms)\n\ntest_data = datasets.CIFAR10(ROOT, \n                             train = False, \n                             download = True, \n                             transform = test_transforms)","93b188dd":"VALID_RATIO = 0.9\n\nn_train_examples = int(len(train_data) * VALID_RATIO)\nn_valid_examples = len(train_data) - n_train_examples\n\ntrain_data, valid_data = data.random_split(train_data, \n                                           [n_train_examples, n_valid_examples])","d36710da":"valid_data = copy.deepcopy(valid_data)\nvalid_data.dataset.transform = test_transforms","374da108":"print(f'Number of training examples: {len(train_data)}')\nprint(f'Number of validation examples: {len(valid_data)}')\nprint(f'Number of testing examples: {len(test_data)}')","497f1f6e":"def normalize_image(image):\n    image_min = image.min()\n    image_max = image.max()\n    image.clamp_(min = image_min, max = image_max)\n    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n    return image\n\ndef plot_images(images, labels, classes, normalize = True):\n\n    n_images = len(images)\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize = (10, 10))\n\n    for i in range(rows*cols):\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        \n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n        ax.set_title(classes[labels[i]])\n        ax.axis('off')","779c289d":"N_IMAGES = 25\n\nimages, labels = zip(*[(image, label) for image, label in \n                           [train_data[i] for i in range(N_IMAGES)]])\n\nclasses = test_data.classes\n\nplot_images(images, labels, classes)","f366a9da":"BATCH_SIZE = 128\n\ntrain_iterator = data.DataLoader(train_data, \n                                 shuffle = True, \n                                 batch_size = BATCH_SIZE)\n\nvalid_iterator = data.DataLoader(valid_data, \n                                 batch_size = BATCH_SIZE)\n\ntest_iterator = data.DataLoader(test_data, \n                                batch_size = BATCH_SIZE)","5b7d706d":"START_LR = 1e-7\n\noptimizer = optim.Adam(model.parameters(), lr = START_LR)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","e0271755":"class LRFinder:\n    def __init__(self, model, optimizer, criterion, device):\n        \n        self.optimizer = optimizer\n        self.model = model\n        self.criterion = criterion\n        self.device = device\n        \n        torch.save(model.state_dict(), 'init_params.pt')\n\n    def range_test(self, iterator, end_lr = 10, num_iter = 100, \n                   smooth_f = 0.05, diverge_th = 5):\n        \n        lrs = []\n        losses = []\n        best_loss = float('inf')\n\n        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n        \n        iterator = IteratorWrapper(iterator)\n        \n        for iteration in range(num_iter):\n\n            loss = self._train_batch(iterator)\n\n            lrs.append(lr_scheduler.get_last_lr()[0])\n\n            #update lr\n            lr_scheduler.step()\n            \n            if iteration > 0:\n                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n                \n            if loss < best_loss:\n                best_loss = loss\n\n            losses.append(loss)\n            \n            if loss > diverge_th * best_loss:\n                print(\"Stopping early, the loss has diverged\")\n                break\n                       \n        #reset model to initial parameters\n        model.load_state_dict(torch.load('init_params.pt'))\n\n                    \n        return lrs, losses\n\n    def _train_batch(self, iterator):\n        \n        self.model.train()\n        \n        self.optimizer.zero_grad()\n        \n        x, y = iterator.get_batch()\n        \n        x = x.to(self.device)\n        y = y.to(self.device)\n        \n        y_pred, _ = self.model(x)\n                \n        loss = self.criterion(y_pred, y)\n        \n        loss.backward()\n        \n        self.optimizer.step()\n        \n        return loss.item()\n\nclass ExponentialLR(_LRScheduler):\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n        self.num_iter = num_iter\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        curr_iter = self.last_epoch\n        r = curr_iter \/ self.num_iter\n        return [base_lr * (self.end_lr \/ base_lr) ** r for base_lr in self.base_lrs]\n\nclass IteratorWrapper:\n    def __init__(self, iterator):\n        self.iterator = iterator\n        self._iterator = iter(iterator)\n\n    def __next__(self):\n        try:\n            inputs, labels = next(self._iterator)\n        except StopIteration:\n            self._iterator = iter(self.iterator)\n            inputs, labels, *_ = next(self._iterator)\n\n        return inputs, labels\n\n    def get_batch(self):\n        return next(self)","99428bed":"END_LR = 10\nNUM_ITER = 100\n\nlr_finder = LRFinder(model, optimizer, criterion, device)\nlrs, losses = lr_finder.range_test(train_iterator, END_LR, NUM_ITER)","b097e994":"def plot_lr_finder(lrs, losses, skip_start = 5, skip_end = 5):\n    \n    if skip_end == 0:\n        lrs = lrs[skip_start:]\n        losses = losses[skip_start:]\n    else:\n        lrs = lrs[skip_start:-skip_end]\n        losses = losses[skip_start:-skip_end]\n    \n    fig = plt.figure(figsize = (16,8))\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(lrs, losses)\n    ax.set_xscale('log')\n    ax.set_xlabel('Learning rate')\n    ax.set_ylabel('Loss')\n    ax.grid(True, 'both', 'x')\n    plt.show()","80acd9a2":"plot_lr_finder(lrs, losses, skip_start = 10, skip_end = 20)","fb3fe1ca":"FOUND_LR = 5e-4\n\nparams = [\n          {'params': model.features.parameters(), 'lr': FOUND_LR \/ 10},\n          {'params': model.classifier.parameters()}\n         ]\n\noptimizer = optim.Adam(params, lr = FOUND_LR)","4eb42d4b":"def calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim = True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() \/ y.shape[0]\n    return acc","7a3a8407":"def train(model, iterator, optimizer, criterion, device):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for (x, y) in iterator:\n        \n        x = x.to(device)\n        y = y.to(device)\n        \n        optimizer.zero_grad()\n                \n        y_pred, _ = model(x)\n        \n        loss = criterion(y_pred, y)\n        \n        acc = calculate_accuracy(y_pred, y)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","36f95c0a":"def evaluate(model, iterator, criterion, device):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        \n        for (x, y) in iterator:\n\n            x = x.to(device)\n            y = y.to(device)\n\n            y_pred, _ = model(x)\n\n            loss = criterion(y_pred, y)\n\n            acc = calculate_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","e6f6d061":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","2529804c":"EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n        \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut4-model.pt')\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","00c23375":"model.load_state_dict(torch.load('tut4-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","268783e5":"def get_predictions(model, iterator):\n\n    model.eval()\n\n    images = []\n    labels = []\n    probs = []\n\n    with torch.no_grad():\n\n        for (x, y) in iterator:\n\n            x = x.to(device)\n\n            y_pred, _ = model(x)\n\n            y_prob = F.softmax(y_pred, dim = -1)\n            top_pred = y_prob.argmax(1, keepdim = True)\n\n            images.append(x.cpu())\n            labels.append(y.cpu())\n            probs.append(y_prob.cpu())\n\n    images = torch.cat(images, dim = 0)\n    labels = torch.cat(labels, dim = 0)\n    probs = torch.cat(probs, dim = 0)\n\n    return images, labels, probs","7a09ded8":"images, labels, probs = get_predictions(model, test_iterator)","25f64fdd":"pred_labels = torch.argmax(probs, 1)","557624c2":"def plot_confusion_matrix(labels, pred_labels, classes):\n    \n    fig = plt.figure(figsize = (10, 10));\n    ax = fig.add_subplot(1, 1, 1);\n    cm = confusion_matrix(labels, pred_labels);\n    cm = ConfusionMatrixDisplay(cm, classes);\n    cm.plot(values_format = 'd', cmap = 'Blues', ax = ax)\n    plt.xticks(rotation = 20)","7ae87bbd":"plot_confusion_matrix(labels, pred_labels, classes)","02e55923":"corrects = torch.eq(labels, pred_labels)","2b590946":"incorrect_examples = []\n\nfor image, label, prob, correct in zip(images, labels, probs, corrects):\n    if not correct:\n        incorrect_examples.append((image, label, prob))\n\nincorrect_examples.sort(reverse = True, key = lambda x: torch.max(x[2], dim = 0).values)","6bc66df7":"def plot_most_incorrect(incorrect, classes, n_images, normalize = True):\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize = (25, 20))\n\n    for i in range(rows*cols):\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        \n        image, true_label, probs = incorrect[i]\n        image = image.permute(1, 2, 0)\n        true_prob = probs[true_label]\n        incorrect_prob, incorrect_label = torch.max(probs, dim = 0)\n        true_class = classes[true_label]\n        incorrect_class = classes[incorrect_label]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax.imshow(image.cpu().numpy())\n        ax.set_title(f'true label: {true_class} ({true_prob:.3f})\\n' \\\n                     f'pred label: {incorrect_class} ({incorrect_prob:.3f})')\n        ax.axis('off')\n        \n    fig.subplots_adjust(hspace = 0.4)","2ad1e953":"N_IMAGES = 36\n\nplot_most_incorrect(incorrect_examples, classes, N_IMAGES)","26ed361f":"def get_representations(model, iterator):\n\n    model.eval()\n\n    outputs = []\n    intermediates = []\n    labels = []\n\n    with torch.no_grad():\n        \n        for (x, y) in iterator:\n\n            x = x.to(device)\n\n            y_pred, _ = model(x)\n\n            outputs.append(y_pred.cpu())\n            labels.append(y)\n        \n    outputs = torch.cat(outputs, dim = 0)\n    labels = torch.cat(labels, dim = 0)\n\n    return outputs, labels","cedbd2b2":"outputs, labels = get_representations(model, train_iterator)","78f0a879":"def get_pca(data, n_components = 2):\n    pca = decomposition.PCA()\n    pca.n_components = n_components\n    pca_data = pca.fit_transform(data)\n    return pca_data","41f3a68c":"def plot_representations(data, labels, classes, n_images = None):\n    \n    if n_images is not None:\n        data = data[:n_images]\n        labels = labels[:n_images]\n        \n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.add_subplot(111)\n    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'tab10')\n    handles, labels = scatter.legend_elements()\n    legend = ax.legend(handles = handles, labels = classes)","1111282a":"output_pca_data = get_pca(outputs)\nplot_representations(output_pca_data, labels, classes)","24ce4a0c":"def get_tsne(data, n_components = 2, n_images = None):\n    \n    if n_images is not None:\n        data = data[:n_images]\n        \n    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n    tsne_data = tsne.fit_transform(data)\n    return tsne_data","371cdc8f":"N_IMAGES = 5_000\n\noutput_tsne_data = get_tsne(outputs, n_images = N_IMAGES)\nplot_representations(output_tsne_data, labels, classes, n_images = N_IMAGES)","889ec8f7":"def plot_filtered_images(images, filters, n_filters = None, normalize = True):\n\n    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n    filters = filters.cpu()\n\n    if n_filters is not None:\n        filters = filters[:n_filters]\n\n    n_images = images.shape[0]\n    n_filters = filters.shape[0]\n\n    filtered_images = F.conv2d(images, filters)\n\n    fig = plt.figure(figsize = (30, 30))\n\n    for i in range(n_images):\n\n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters))\n        ax.imshow(image.permute(1,2,0).numpy())\n        ax.set_title('Original')\n        ax.axis('off')\n\n        for j in range(n_filters):\n            image = filtered_images[i][j]\n\n            if normalize:\n                image = normalize_image(image)\n\n            ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters)+j+1)\n            ax.imshow(image.numpy(), cmap = 'bone')\n            ax.set_title(f'Filter {j+1}')\n            ax.axis('off');\n\n    fig.subplots_adjust(hspace = -0.7)","e3c02a5a":"N_IMAGES = 5\nN_FILTERS = 7\n\nimages = [image for image, label in [test_data[i] for i in range(N_IMAGES)]]\nfilters = model.features[0].weight.data\n\nplot_filtered_images(images, filters, N_FILTERS)","1a389719":"def plot_filters(filters, normalize = True):\n\n    filters = filters.cpu()\n\n    n_filters = filters.shape[0]\n\n    rows = int(np.sqrt(n_filters))\n    cols = int(np.sqrt(n_filters))\n\n    fig = plt.figure(figsize = (20, 10))\n\n    for i in range(rows*cols):\n\n        image = filters[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        ax.imshow(image.permute(1, 2, 0))\n        ax.axis('off')\n\n    fig.subplots_adjust(wspace = -0.9)","deeab3e1":"plot_filters(filters)","e8631a0a":"\nWe can now define a function which takes in a configuration list and returns a nn.Sequential with the respective layers.\n\nget_vgg_layers iterates over the configuration list and appends each layer to layers, then it converts them to a nn.Sequential as it returns.\n\nWe can see that we always use the same size filter (2x2) and stride (2) in all of our max pool layers. As mentioned in previous tutorials, the default stride for pooling layers is equal to the kernel size.\n\nFor the convolutional layers we always the same filter size (3x3) and padding (1). As a reminder, padding adds padding pixels with values of zero around each side of the image in each channel before the filter is applied. Each convolutional layer is followed by a ReLU non-linearity. We then set the in_channels to be equal to the number of filters in the convolutional layer so the next convolutional layer has the correct in_channels.\n\nAnother new layer introduced here is batch normalization (BN) defined with BatchNorm2d and only used if batch_norm = True. As mentioned in previous tutorials, normalization is when we try to ensure that some data has a mean of zero and a standard deviation (std) of one as this improves learning - both stability and convergence speed - in machine learning models. Previously we have normalized our input data using PyTorch transforms and tried to ensure our data remains as normalized as possible by using weight initialization schemes - such as Glorot or Kaiming. However, as our model begins to train and the parameters change then the mean and std output by each layer will also change. A change in mean and std of the output of one layer will cause a change in mean and std for all following layers. Our model has no way to correct these changes in means and stds once training has begun.\n\nBN is a layer with learnable parameters - two per filter - denoted $\\gamma$ and $\\beta$. The layer normalizes, scales and then shifts across the channel dimension of the input. The output of a BN layer is calculated by:\n![image.png](attachment:image.png)","d00355e9":"# Training the Model\n\nWe'll use the learning rate finder as used in previous notebooks. Generally when using a pre-trained model the learning rate used will be considerably lower.\n\nFirst, we'll set up the optimizer with the initial learning rate that is much lower than we expect to use. Then we define the device to place our model on our GPU, if we have one. Next we define the criterion (loss function) and place the model and criterion on our device.","ba41e4b6":"Next, we define a function to perform PCA on the representations and plot the PCA output.","fab7ee99":"...then get the predicted labels for each image...","a57ca77e":"Again, the frog that was incorrectly labelled as a cat is there with a lot of automobile\/truck confusion.","9e70e137":"...create a function that performs an evaluation loop...","f29fd171":"We can then print out the classifier of our model to ensure the final linear layer now has an output dimension of 10.","c8f64537":"# Examining the Model\n\nWe'll do the same examinations on the model as we have done on the previous notebooks.\n\nFirst, we'll get the predictions for each of the examples in the test set...","8ccf9c97":"In this notebook we will be implementing one of the VGG model variants. VGG is a neural network model that uses convolutional neural network (CNN) layers and was designed for the ImageNet challenge, which it won in 2014.\n\nVGG is not a single model, but a family of models that are all similar but have different configurations. Each configuration specifies the number of layers and the size of each layer. The configurations are listed in table 1 of the VGG paper and denoted by a letter, although recently they are just referred to as the number of layers with weights in the model, e.g. configuration \"A\" has 11 layers with weights so is known as VGG11.\n<br>\n![image.png](attachment:image.png)\n<br>","a67c21bd":"Below is the architecture of configuration \"D\", also known as VGG16, for a 224x224 color image.\n<br>\n![image.png](attachment:image.png)\nThe other commonly used VGG variants are VGG11, VGG13 and VGG19, which correspond to configurations \"A\", \"B\", and \"E\". Configurations \"A-LRN\" and \"C\" - which is the same as \"D\" but with smaller filter sizes in some convolutional layers - are rarely used.\n\nAs in the previous notebook, we will use the CIFAR10 dataset and the learning rate finder introduced here. However we will be making use of pre-trained models.\n\nUsually we will initialize our weights randomly - following some weight initialization scheme - and then train our model. Using a pre-trained model means some - potentially all - of our model's weights are not initialized randomly, but instead taken from a copy of our model that has already been trained on some task. The task the model has been pre-trained on does not necessarily have to match the \"downstream task\" - the task we want to use the pre-trained model on. For example, a model that has been trained to classify images can then be used to detect objects within an image.\n\nThe theory is that these pre-trained models have already learned high level features within images that will be useful for our task. This means we don't have to learn them from scratch when using the pre-trained model for our task, causing our model to converge earlier. We can also think of the pre-trained model as being a very good set of weights to initialize our model from, and using pre-trained models usually leads to a performance improvement compared to initializing our weights randomly.\n\nThe act of using a pre-trained model is generally known as transfer learning, as we are learning to transfer knowledge from one task to another. It is also referred to as fine-tuning, as we fine-tune our parameters trained on one task to the new, downstream task. The terms transfer learning and fine-tuning are used interchangably in machine learning.\n\nWe are also going to look into a technique called discriminative fine-tuning, initially introduced to improve transfer learning for text classification but has been used for computer vision tasks too.","b8e699f8":"This model has considerably more parameters than the previous model, AlexNet - 128M compared to 23M.","f3f00d46":"We'll then find out which predictions were incorrect and then sort these incorrect predictions by how confident our model was.","15e2161b":"As we can see, the pre-trained model loaded is exactly the same as the one we have defined with one exception - the output of the final linear layer.\n\nAll of torchvision's pre-trained models are trained as image classification models on the ImageNet dataset. A dataset of 224x224 color images with 1000 classes, therefore the final layer will have a 1000 dimensional output.\n\nWe can get the last layer specifically by indexing into the classifier layer of the pre-trained model.","2e3115e7":"...and plot the loss achieved per batch for each learning rate value tested.","f429d94b":"Next up is plotting the representations of the model with PCA and then t-SNE.\n\nWe are only going to plot the output representations and not the intermediate ones here. This is because the output dimensions are ten dimensional but the intermediate representations have over 25,000 dimensions and storing these in memory takes a considerably amount of RAM.","3d3490eb":"Next, we run the learning rate finder...","aa81c73d":"We'll then get the filters learned by our model and plot some images with the filters applied to them","1883500d":"Why do we scale and shift? Why not leave the outputs with a mean of zero and a std of one? Perhaps there is a better mean and std for our task instead of zero and one. If this is the case then our model can learn this whilst training as $\\gamma$ and $\\beta$ are learnable parameters. However, to bias our model to start off with the idea that a mean of zero and a std of one is a good idea by default $\\gamma$ and $\\beta$ are initialized to one and zero.\n\nThere are a few things to consider with batch normalization during inference time (validation or testing). The first is that we don't want to calculate the mean and variance of our data to normalize it during inference. This is because an image in the validation or test set might be drastically different to an image in the training set, and we do not want to remove that information via normalization. The second is what to do with a batch size of one, common when deploying models. Calculating the mean and variance across a single example (a batch size of one) doesn't make sense.\n\nLuckily, there is a solution to both these problems. Instead of using the actual mean and variance of a batch, we use an exponentially weighted moving average which we update every batch. Then, when using inference (with any batch size, including one) we use the saved weighted average of the means and variances.\n\nThis video has a good explanation of batch normalization and other types of normalization layers. For another explanation on why batch normalization helps, check out this paper.\n\nOne last thing to mention on batch normalization is that, in theory, it should be used after the activation function. Why would you normalize the output of a layer only to just ruin the normalization effect with an activation function? However, in the original VGG architecture they use batch normalization before the activation function, so we do too.","0ca27757":"# Data Processing\nAs always, we'll import the modules we need. No new imports yet.","4e981a89":"As we can see, the loss remains almost constant until around $1x10^{-4}$, it then decreases rapidly before diverging.\n\nA good learning rate from this is the middle of the steep decline which is around $5x10^{-4}$.","820edbcd":"\nAs our dataset, CIFAR10, only has 10 classes then we want the last layer to have a 10 dimensional output.\n\nWe'll define a new final linear layer which has to have an input size equal to that of the layer we are replacing - as it's input will be the 4096 dimensional output from the previous linear layer in the classifier. The output of this linear layer will be 10 dimensions - as our dataset has 10 classes.","2f17e3f7":"...then create the validation split...","c95ac0f3":"...and a helper function to tell us how long an epoch takes.","1b6a2b70":"\nNow, let's get the features for the VGG11 architecture, with batch normalization.\n\nThe original VGG paper did not use batch normalization, but it is now common to use it in VGG models.","a0ba3d1f":"# Thankyou For Reading!!!","489491b5":"# Pre-trained Models\nIn this notebook we aren't actually going to use a VGG model with parameters that have been randomly initialized VGG model. We are going to be using a VGG model with pre-trained parameters. Using a pre-trained model involves initializing our model with parameters that have already been trained for a certain task - usually not the exact same task we are trying to do ourselves.\n\nTorchvision has ways to easily download a pre-trained model. We simply import the torchvision.models package, specify which model we want to use and then pass the argument pretrained = True. We can see a list of all available pre-trained models provided by torchvision here.\n\nLet's import a pre-trained VGG11 with batch normalization. The first time this code is run the pre-trained parameters will be downloaded and are around 140MB for VGG11 with batch normalization.","568d0af9":"We can see that the filters learned by the model do different types of edge detection, color inversion and blurring.","15a91f0b":"First, we gets the output representations...","5e1e3ef9":"\nWe can then pass these features to our base VGG module to get our VGG11 model.","bffbab4d":"Finally, we train our model.\n\nAs our images have been resized to be significantly larger and our model has significantly more parameters training takes considerably longer. However, when performing transfer learning we usually train for significantly less epochs and are still able to achieve much higher accuracy than before.","8d55bcdc":"...and then use these predictions to create a confusion matrix.","36b0d846":"We can then plot the PCA representations.","87096ec1":"We can directly overwrite the previous linear layer with our new linear layer.\n\nNote that our final_fc will be initialized randomly. It is the only part of our model with its parameters not pre-trained.","311d53f2":"We can now create an optimizer with our found learning rate and using discriminative fine-tuning.\n\nThe concept behind descriminative fine-tuning is that we use different learning rates for different layers in our models. The hypothesis is that early layers in a neural network learn to extract more general features, whilst later layers learn to extract more task specific features. If this is true, then the general features extracted by the early layers should be useful for any task, and we should change the pre-trained weights of them by a very small amount - if at all.\n\n**Note**: discriminative fine-tuning should only be used when performing transfer learning from a pre-trained model. It is typically not necessary to use it when training a model from randomly initialized weights.\n\nPyTorch allows us to set different learning rate values per parameter in our model. This is done by passing a list of dictionaries to the optimizer. Each dictionary should state the parameters ('params') and also any other arguments that will override those given directly to the optimizer.\n\nHere, instead of using a different learning rate for every single layer, we have split the parameters into two \"groups\": features, which contains all of the convolutional layers; and classifier, which contains all of the linear layers. classifier will be using the FOUND_LR given directly to the optimizer and features will be using FOUND_LR \/ 10, as specified in the first dictionary. Thus, our convolutional layers have a learning rate 10x less than the linear layers.","8b8c0b99":"...create a function that implements a training loop...","dc55901c":"# Defining the Model\nUsually the next thing we'd do is load our data, however this is quite different when using a pre-trained model, so we'll first introduce the VGG architecture and then show how to load a pre-trained VGG model.\n\nBelow is the general VGG model. This is the part of VGG that is common for all VGG configurations. The only part that depends the configuration is the features, which we will pass as an argument when we construct the VGG model.\n\nThe only new feature introduced here in the AdaptiveAvgPool2d. As well as the standard AvgPool and MaxPool layers, PyTorch has \"adaptive\" versions of those layers. In adaptive pooling layers we specify the desired output size of the pooling layer instead of the size of the filter used by the pooling layer. Here, we want an output size of 7x7. We know that all VGG configurations end with a convolutional layer that has 512 filters, thus if our features layer for each configuration always has a size of 7x7 we do not have to change the classifier for each VGG configuration. The advantage of using adaptive layers is that it allows us to apply our model to images of different sizes - down to a minimum size, which is 32x32 in VGG models.\n\nNote: even though VGG net can handle images as small as 32x32 it is designed to give optimal performance for larger images. We handle this later on in this tutorial.\n\nHow do the adaptive pooling layers calculate the size of their filters? For each dimension, i.e. height and width, we calculate:\n\nfilter_size = (input_size + desired_size - 1) \/\/ desired_size\n\/\/ means we round down to the nearest integer. So, if we wanted to filter a 32x32 image to 7x7 we would have a 6x6 filter. When the filter is applied to the image it will need to overlap at some points, i.e. some pixels will be covered by the filter twice. To calculate the positions of the filter we split the 32x32 image into desired_size evenly spaced points.\n\nThis can be calculated with np.linspace(0, input_size - desired_size, desired_size) which for our example gives us: [0, 4.16666667, 8.33333333, 12.5, 16.66666667, 20.83333333, 25]. We then round each of these points down to the nearest integer as we cannot have a filter cover a fraction of a pixel, which gives us: [0, 4, 8, 12, 16, 20, 25]. Thus, the 6x6 filter covers pixels: 0-5, 4-10, 8-14, 12-18, 16-22, 20-26 and 25-31 in each dimension.\n\nLet's define the VGG base architecture.","8a404f74":"Next up is calculating the features for each VGG configuration.\n\nTypically the VGG configurations are defined as lists. Each item in the list is either 'M', which denotes a max pooling layer, or an integer, which denotes a convolutional layer with that many filters.\n\nBelow are the configurations for VGG11, VGG13, VGG16 and VGG19. Otherwise known as configurations \"A\", \"B\", \"D\" and \"E\", respectively.","6f8f775f":"Now all of the set-up is done, the rest of the notebook is pretty standard from here on out.\n\nWe create a function to calculate accuracy...","ed5515c4":"We can also plot out the actual filter weights learned by the model.","d2ed19f1":"Next up is calculating the t-SNE representations.","4859e5c2":"As we can see, the larger images mean we can get away with larger amounts of rotation and cropping with the images still looking reasonable.","e677412b":"The batch $\\mathcal{B}$ has $m$ examples, $x_i,\\dots,x_m$. We first calculate the mean and variance across each channel dimension of the batch, $\\mu_\\mathcal{B}$ and $\\sigma^2_\\mathcal{B}$. Then normalize the batch by subtracting the channel means and dividing by the channel stds (the square root of the variance plus a small epsilon term to avoid division by zero) across each channel. We then scale and shift each channel of this normalized batch of inputs, $\\hat{x}_i$ using $\\gamma$ and $\\beta$.\n![image.png](attachment:image.png)","ab2d68b4":"Even though these aren't as interpretable as the pre-trained AlexNet filters shown in the previous notebook, they are more interesting than the AlexNet filters learned from scratch.\n\nInterestingly there are multiple filters that are completely black, implying they all have filters weights that are basically zero","20a5854d":"# Index\n\n- loading Torchvision datasets\n- loading transforms to augment and normalize our data\n- defining a CNN (VGG)\n- adaptive pooling\n- batch normalization\n- loading a pre-trained model\n- loading pre-trained model parameters into a defined model\n- how to freeze weights of our model\n- how to use the learning rate finder\n- how to use discriminative fine-tuning\n- fine-tuning a pre-trained model to achieve >94% accuracy\n- viewing our model's mistakes\n- visualizing our data in lower dimensions with PCA and t-SNE\n- viewing the learned weights of our model.","06f0b750":"Instead of training all of the parameters we have loaded from a pre-trained model, we could instead only learn some of them and leave some \"frozen\" at their pre-trained values. As our model will then have less trainable parameters it will usually train faster and we can usually fit it on smaller GPUs.\n\nWe aren't going to freeze any parameters in this notebook, but if we wanted to freeze the features layer then we could do that with:\n\nfor parameter in model.features.parameters():\n    parameter.requires_grad = False\nWe could also freeze the classifier layer, however we always want to train the last layer as this is what we have initialized randomly and needs to be trained. Freezing all but the last layer in the classifier can be done with:\n\nfor parameter in model.classifier[:-1].parameters():\n    parameter.requires_grad = False\n# Data Processing\nThere's a few things we need to consider in regards to data processing when using pre-trained models.\n\nAs mentioned in the torchvision models page:\n\nAll pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n\nThus, we need to resize our 32x32 images to 224x224 and also normalize them with the given means and stds instead of calculating them from the CIFAR10 dataset.\n\nEven though the VGG models can handle images as small as 32x32 and convolutional layers are translation invariant, our images still need to be resized as the pre-trained classifier layer is expecting certain features to appear in certain places within the flattened 512x7x7 output of the features layer after the adaptive average pooling. Using a different image size than was used to pre-train the model causes features sent to the classifier to be in different places than expected, and thus leads to poor performance when using the pre-trained model.\n\nWe need to use the same means and stds to make the colors of the images fed to the model with the pre-trained parameters be the same as they were to train the pre-trained model. Let's say the original dataset had lots of dark green images and the mean for the green channel was be relatively low, say 0.2, and the dataset we are going to use had lots of light green images with the mean for the green channel being around 0.8. The pre-trained model was trained with dark green pixels normalized to zero (subtracting the mean). If we incorrectly used the means and stds from our dataset we want to apply our model on, then light green pixels will be normalized to zero, thus our pre-trained model will think a given light green image is actually a dark green image. This will confuse our model and lead to poor performance.\n\nWe handle the resizing with the Resize transform, passing in the desired size. As the images are larger we can also get away with slightly higher rotations and crops within the RandomRotation and RandomCrop transforms. We pass the pre-trained mean and stds the same way we would pass our calculated means and transforms.","83442374":"...and ensure the validation data uses the test transforms.","25e930f9":"We could go ahead and use the pretrained_model module from here on out, however it only returns the final output layer and not the intermediate representation as our model does. We can see that in the model's definition here.\n\nWe can load the parameters of the pretrained_model into our model by loading the parameters (state_dict) from the pretrained_model into our model in the same way we loaded the \"best\" saved parameters in the previous notebooks - by using load_state_dict. However, this time we load directly from the pretrained_model instead of using torch.load on the path of the saved model parameters.\n\nThis is only possible as our model has the exact same layers (order and shape) as the pretrained_model with the final linear layer replaced with our 10 dimensional output linear layer.","8874a834":"We'll also plot out a few images to ensure the transformations look sensible - making sure to re-normalize our data so we can view it with the correct colors."}}