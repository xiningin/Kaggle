{"cell_type":{"8fc32926":"code","a100143c":"code","fc49a326":"code","c76a5f80":"code","3380863e":"code","905b3a62":"code","9f03ea80":"code","193de039":"code","6c7c60c1":"code","58ce5d2f":"code","70e541b0":"code","90b5c7a1":"code","d10fe17a":"code","54ace5e0":"code","0c93ff3b":"code","24f4e6f4":"code","b04703dc":"code","4c7b376f":"code","8c056308":"code","18e4b5ba":"code","7efaa3fa":"code","8eb2486a":"code","f7633fec":"code","f8cd0539":"code","3a358b6c":"code","4d6f8169":"code","ae9fa46c":"code","397f293c":"code","0528c4eb":"code","555c0d09":"code","37df8f8c":"code","4637109c":"code","26baf3d9":"code","09607996":"code","7b601161":"code","02983481":"code","a4c2c4c5":"code","5e25595b":"code","9f3d8694":"code","7cbb55c0":"code","ed499fb9":"code","6939fbd9":"code","34152a74":"code","b56fbc33":"code","edc72d9f":"code","727d1078":"code","1e3080a9":"code","eef66e5f":"code","a26dc012":"code","efcfb872":"code","80b07dbd":"code","8b073aea":"code","4bbe2de3":"code","afeff56d":"code","9f64966e":"code","de4627e4":"code","b73d2312":"code","151f65d0":"code","1a657679":"code","8b83316f":"code","ffa51895":"code","1e5d82f5":"markdown","1519cb0c":"markdown","3d12b1c2":"markdown","e332e96a":"markdown","215c92bb":"markdown","360f80b8":"markdown","177a7a3e":"markdown","010a5b42":"markdown","cfb63cd1":"markdown","218808ca":"markdown","ce394bef":"markdown"},"source":{"8fc32926":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a100143c":"import contextlib2\nimport io\nimport IPython\nimport json\nimport numpy as np\nimport os\nimport pathlib\nimport pandas as pd\nimport sys\nimport tensorflow as tf\nimport time\nimport pandas as pd\nimport numpy as np\nimport argparse\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt","fc49a326":"!git clone https:\/\/github.com\/tensorflow\/models    \n# Check out a certain commit to ensure that future changes in the TF ODT API codebase won't affect this notebook.\n# !cd models && git checkout ac8d06519\n","c76a5f80":"!cd models","3380863e":"!pip install tensorflow-gpu==2.6.0","905b3a62":"%%bash\ncd models\/research\n\n# Compile protos.\nprotoc object_detection\/protos\/*.proto --python_out=.\n\n# Install TensorFlow Object Detection API.\n# Note: I fixed the version of some dependencies to make it work on Kaggle notebook. In particular:\n# * scipy==1.6.3 to avoid the missing GLIBCXX_3.4.26 error\n# * tensorflow to 2.6.0 to make it compatible with the CUDA version preinstalled on Kaggle.\n# When Kaggle notebook upgrade to TF 2.7, you can use the default setup.py script:\n# cp object_detection\/packages\/tf2\/setup.py .\n\nwget https:\/\/storage.googleapis.com\/odml-dataset\/others\/setup.py\npip install -q --user .\n\n# Test if the Object Dectection API is working correctly\npython object_detection\/builders\/model_builder_tf2_test.py","9f03ea80":"from object_detection.utils import visualization_utils as viz_utils","193de039":"print(tf.__version__)\nprint(tf.test.is_gpu_available())\nprint(tf.config.list_physical_devices('GPU'))\nprint(tf.debugging.set_log_device_placement(True))","6c7c60c1":"%%writefile cross_validation_setup.py\n\"\"\"\nSplits a dataframe to a specified cross_validation framework\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport argparse\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n\nparser = argparse.ArgumentParser(\n    description=\"Cross Validation Setup\")\nparser.add_argument(\"-in\",\n                    \"--input_dir\",\n                    help=\"The filepath to the input csv.\", type=str)\nparser.add_argument(\"-out\",\n                    \"--output_dir\",\n                    help=\"The filepath to the output csv.\", type=str)\nparser.add_argument(\"-type\",\n                    \"--type\",\n                    help=\"The type of cross_validation to use.\", type=str)\nparser.add_argument(\"-on\",\n                    \"--on\",\n                    help=\"The column to stratisfy on.\", type=str)\nparser.add_argument(\"-folds\",\n                    \"--folds\",\n                    help=\"The number of folds to create.\", type=int)\nparser.add_argument(\"-hold\",\n                    \"--holdout\",\n                    help=\"The fold to holdout.\", type=int)\n\nargs = parser.parse_args()\n\ndef cross_validation():\n    df = pd.read_csv(args.input_dir)\n    df = df[df.annotations!='[]']\n    df = df.reset_index(drop=True)\n    \n    if args.type=='kfold':\n        kf = KFold(n_splits=args.folds, shuffle=True, random_state=42)\n        for f, (t_, v_) in enumerate(kf.split(X=df)):\n            df.loc[v_, 'fold'] = f\n            \n    elif args.type=='skfold' and args.on=='video_id':\n        # uses skf to get even distrubution of data from different videos\n        kf = StratifiedKFold(n_splits=args.folds, shuffle=True, random_state=42)\n        for f, (t_, v_) in enumerate(kf.split(X=df, y=df.video_id)): \n            df.loc[v_, 'fold'] = f\n            \n    elif args.type=='gkfold' and args.on=='sequence':\n        kf = GroupKFold(n_splits=args.folds)\n        for f, (t_, v_) in enumerate(kf.split(X=df, y=df.video_id, groups=df.sequence)): \n            df.loc[v_, 'fold'] = f\n            \n    else:\n        raise Exception('Not Implemented')\n        \n    df.to_csv(args.output_dir, index=False)\n    \nif __name__ == '__main__':\n    cross_validation()","58ce5d2f":"%%writefile generate_tfrecords.py\n\nimport os\nfrom os.path import exists\nimport glob\nimport pandas as pd\nimport io\nimport json\nimport xml.etree.ElementTree as ET\nimport contextlib2\nimport argparse\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\nimport tensorflow.compat.v1 as tf\nfrom PIL import Image\nfrom object_detection.utils import dataset_util, label_map_util\nfrom object_detection.dataset_tools import tf_record_creation_util\nfrom collections import namedtuple\n\n# Initiate argument parser\nparser = argparse.ArgumentParser(\n    description=\"TensorFlow TFRecord Generator\")\nparser.add_argument(\"-c\",\n                    \"--csv_path\",\n                    help=\"Path to the train.csv file.\", type=str)\nparser.add_argument(\"-o\",\n                    \"--output_path\",\n                    help=\"Path of output TFRecord (.record) file.\", type=str)\nparser.add_argument(\"-i\",\n                    \"--image_dir\",\n                    help=\"Path to the folder where the input image files are stored.\", type=str)\nparser.add_argument(\"-t\",\n                    \"--train\",\n                    help=\"True if this is a training dataset, false if it is a validation dataset.\", type=str)\nparser.add_argument(\"-s\",\n                    \"--shards\",\n                    help=\"The number of shards for the dataset\", type=int)\nparser.add_argument(\"-f\",\n                    \"--holdout_fold\",\n                    help=\"The fold to holdout.\", type=int)\n\nargs = parser.parse_args()\n\ndef create_tf_example(data_df: pd.DataFrame, video_id: int, video_frame: int):\n    \"\"\"\n    Create a tf.Example entry for a given training image.\n    \"\"\"\n    full_path = os.path.join(args.image_dir, os.path.join(f'video_{video_id}', f'{video_frame}.jpg'))\n    with tf.io.gfile.GFile(full_path, 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    if image.format != 'JPEG':\n        raise ValueError('Image format not JPEG')\n\n    height = image.size[1] # Image height\n    width = image.size[0] # Image width\n    filename = f'{video_id}:{video_frame}'.encode('utf8') # Unique id of the image.\n    encoded_image_data = None # Encoded image bytes\n    image_format = 'jpeg'.encode('utf8') # b'jpeg' or b'png'\n\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box# (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n\n    rows = data_df[(data_df.video_id == video_id) & (data_df.video_frame == video_frame)]\n    for _, row in rows.iterrows():\n        annotations = json.loads(row.annotations.replace(\"'\", '\"'))\n        for annotation in annotations:\n            xmins.append(annotation['x'] \/ width) \n            xmaxs.append((annotation['x'] + annotation['width']) \/ width) \n            ymins.append(annotation['y'] \/ height) \n            ymaxs.append((annotation['y'] + annotation['height']) \/ height) \n\n            classes_text.append('COTS'.encode('utf8'))\n            classes.append(1)\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image\/height': dataset_util.int64_feature(height),\n      'image\/width': dataset_util.int64_feature(width),\n      'image\/filename': dataset_util.bytes_feature(filename),\n      'image\/source_id': dataset_util.bytes_feature(filename),\n      'image\/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image\/format': dataset_util.bytes_feature(image_format),\n      'image\/object\/bbox\/xmin': dataset_util.float_list_feature(xmins),\n      'image\/object\/bbox\/xmax': dataset_util.float_list_feature(xmaxs),\n      'image\/object\/bbox\/ymin': dataset_util.float_list_feature(ymins),\n      'image\/object\/bbox\/ymax': dataset_util.float_list_feature(ymaxs),\n      'image\/object\/class\/text': dataset_util.bytes_list_feature(classes_text),\n      'image\/object\/class\/label': dataset_util.int64_list_feature(classes),\n    }))\n\n    return tf_example\n\ndef create_labels_file():\n    label_map_str = \"\"\"\n    item {\n        id: 1\n        name: 'COTS'\n        }\n                    \"\"\"\n\n    if exists('dataset\/label_map.pbtxt') is False:\n        with open('dataset\/label_map.pbtxt', 'w') as f:\n            f.write(label_map_str)\n        print('Successfully created label_map.pbtxt file')\n\nif __name__ == '__main__':\n\n    # label file\n    create_labels_file()\n    #writer = tf.python_io.TFRecordWriter(args.output_path)\n    \n    # setup df\n    data_df = pd.read_csv(args.csv_path)\n    if args.train =='train':\n        data_df = data_df[data_df.fold != args.holdout_fold].reset_index(drop=True)\n    else:\n        data_df = data_df[data_df.fold == args.holdout_fold].reset_index(drop=True)\n    \n    # make records\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n            tf_record_close_stack, args.output_path, args.shards)\n        \n        for index, row in data_df.iterrows():\n            tf_example = create_tf_example(data_df, row.video_id, row.video_frame)\n            output_shard_index = index % args.shards\n            output_tfrecords[output_shard_index].write(tf_example.SerializeToString())\n    #writer.close()\n    print('Successfully created the TFRecord file: {}'.format(args.output_path))","70e541b0":"# %%bash\n# !mkdir dataset\nos.mkdir('dataset')","90b5c7a1":"# %%bash\n\n# python cross_validation_setup.py \\\n#     -in ..\/input\/tensorflow-great-barrier-reef\/train.csv \\\n#     -out train.csv \\\n#     -type skfold \\\n#     -on video_id \\\n#     -folds 10 \\\n#     -hold 0\n\n# mkdir dataset\nfrom os.path import exists\n\ndef create_labels_file():\n    label_map_str = \"\"\"\n    item {\n        id: 1\n        name: 'COTS'\n        }\n                    \"\"\"\n    if exists('dataset\/label_map.pbtxt') is False:\n        with open('dataset\/label_map.pbtxt', 'w') as f:\n            f.write(label_map_str)\n        print('Successfully created label_map.pbtxt file')\n\ncreate_labels_file()\n\n# python generate_tfrecords.py \\\n#     -c train.csv \\\n#     -o dataset\/cots_train \\\n#     -i ..\/input\/tensorflow-great-barrier-reef\/train_images \\\n#     -t train \\\n#     -s 4 \\\n#     -f 0\n\n# python generate_tfrecords.py \\\n#     -c train.csv \\\n#     -o dataset\/cots_val \\\n#     -i ..\/input\/tensorflow-great-barrier-reef\/train_images \\\n#     -t valid \\\n#     -s 4 \\\n#     -f 0","d10fe17a":"!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n!tar -xvzf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n!rm ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz","54ace5e0":"!wget http:\/\/download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\n!tar -xvzf faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\n!rm faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz","0c93ff3b":"# # Faster R-CNN with Resnet-50 (v1), configuration for MSCOCO Dataset.\n# # Users should configure the fine_tune_checkpoint field in the train config as\n# # well as the label_map_path and input_path fields in the train_input_reader and\n# # eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n# # should be configured.\n\n# model {\n#   faster_rcnn {\n#     num_classes: 90\n#     image_resizer {\n#       keep_aspect_ratio_resizer {\n#         min_dimension: 600\n#         max_dimension: 1024\n#       }\n#     }\n#     feature_extractor {\n#       type: 'faster_rcnn_resnet50'\n#       first_stage_features_stride: 16\n#     }\n#     first_stage_anchor_generator {\n#       grid_anchor_generator {\n#         scales: [0.25, 0.5, 1.0, 2.0]\n#         aspect_ratios: [0.5, 1.0, 2.0]\n#         height_stride: 16\n#         width_stride: 16\n#       }\n#     }\n#     first_stage_box_predictor_conv_hyperparams {\n#       op: CONV\n#       regularizer {\n#         l2_regularizer {\n#           weight: 0.0\n#         }\n#       }\n#       initializer {\n#         truncated_normal_initializer {\n#           stddev: 0.01\n#         }\n#       }\n#     }\n#     first_stage_nms_score_threshold: 0.0\n#     first_stage_nms_iou_threshold: 0.7\n#     first_stage_max_proposals: 300\n#     first_stage_localization_loss_weight: 2.0\n#     first_stage_objectness_loss_weight: 1.0\n#     initial_crop_size: 14\n#     maxpool_kernel_size: 2\n#     maxpool_stride: 2\n#     second_stage_box_predictor {\n#       mask_rcnn_box_predictor {\n#         use_dropout: false\n#         dropout_keep_probability: 1.0\n#         fc_hyperparams {\n#           op: FC\n#           regularizer {\n#             l2_regularizer {\n#               weight: 0.0\n#             }\n#           }\n#           initializer {\n#             variance_scaling_initializer {\n#               factor: 1.0\n#               uniform: true\n#               mode: FAN_AVG\n#             }\n#           }\n#         }\n#       }\n#     }\n#     second_stage_post_processing {\n#       batch_non_max_suppression {\n#         score_threshold: 0.0\n#         iou_threshold: 0.6\n#         max_detections_per_class: 100\n#         max_total_detections: 300\n#       }\n#       score_converter: SOFTMAX\n#     }\n#     second_stage_localization_loss_weight: 2.0\n#     second_stage_classification_loss_weight: 1.0\n#   }\n# }\n\n# train_config: {\n#   batch_size: 1\n#   optimizer {\n#     momentum_optimizer: {\n#       learning_rate: {\n#         manual_step_learning_rate {\n#           initial_learning_rate: 0.0003\n#           schedule {\n#             step: 0\n#             learning_rate: .0003\n#           }\n#           schedule {\n#             step: 900000\n#             learning_rate: .00003\n#           }\n#           schedule {\n#             step: 1200000\n#             learning_rate: .000003\n#           }\n#         }\n#       }\n#       momentum_optimizer_value: 0.9\n#     }\n#     use_moving_average: false\n#   }\n#   gradient_clipping_by_norm: 10.0\n#   fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\/model.ckpt\"\n#   from_detection_checkpoint: true\n#   # Note: The below line limits the training process to 200K steps, which we\n#   # empirically found to be sufficient enough to train the pets dataset. This\n#   # effectively bypasses the learning rate schedule (the learning rate will\n#   # never decay). Remove the below line to train indefinitely.\n#   num_steps: 200000\n#   data_augmentation_options {\n#     random_horizontal_flip {\n#     }\n#   }\n# }\n\n# train_input_reader: {\n#   tf_record_input_reader {\n#     input_path: \"PATH_TO_BE_CONFIGURED\/mscoco_train.record\"\n#   }\n#   label_map_path: \"PATH_TO_BE_CONFIGURED\/mscoco_label_map.pbtxt\"\n# }\n\n# eval_config: {\n#   num_examples: 8000\n#   # Note: The below line limits the evaluation process to 10 evaluations.\n#   # Remove the below line to evaluate indefinitely.\n#   max_evals: 10\n# }\n\n# eval_input_reader: {\n#   tf_record_input_reader {\n#     input_path: \"PATH_TO_BE_CONFIGURED\/mscoco_val.record\"\n#   }\n#   label_map_path: \"PATH_TO_BE_CONFIGURED\/mscoco_label_map.pbtxt\"\n#   shuffle: false\n#   num_readers: 1\n# }","24f4e6f4":"from string import Template\n\nconfig_file_template = \"\"\"\n# SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal\n# loss (a.k.a Retinanet).\n# See Lin et al, https:\/\/arxiv.org\/abs\/1708.02002\n# Trained on COCO, initialized from Imagenet classification checkpoint\n# Train on TPU-8\n#\n# Achieves 38.3 mAP on COCO17 Val\n\nmodel {\n  ssd {\n    inplace_batchnorm_update: true\n    freeze_batchnorm: false\n    num_classes: 1\n    box_coder {\n      faster_rcnn_box_coder {\n        y_scale: 10.0\n        x_scale: 10.0\n        height_scale: 5.0\n        width_scale: 5.0\n      }\n    }\n    matcher {\n      argmax_matcher {\n        matched_threshold: 0.5\n        unmatched_threshold: 0.5\n        ignore_thresholds: false\n        negatives_lower_than_unmatched: true\n        force_match_for_each_row: true\n        use_matmul_gather: true\n      }\n    }\n    similarity_calculator {\n      iou_similarity {\n      }\n    }\n    encode_background_as_zeros: true\n    anchor_generator {\n      multiscale_anchor_generator {\n        min_level: 3\n        max_level: 7\n        anchor_scale: 4.0\n        aspect_ratios: [1.0, 2.0, 0.5]\n        scales_per_octave: 2\n      }\n    }\n    image_resizer {\n      fixed_shape_resizer {\n        height: 640\n        width: 640\n      }\n    }\n    box_predictor {\n      weight_shared_convolutional_box_predictor {\n        depth: 256\n        class_prediction_bias_init: -4.6\n        conv_hyperparams {\n          activation: RELU_6,\n          regularizer {\n            l2_regularizer {\n              weight: 0.0004\n            }\n          }\n          initializer {\n            random_normal_initializer {\n              stddev: 0.01\n              mean: 0.0\n            }\n          }\n          batch_norm {\n            scale: true,\n            decay: 0.997,\n            epsilon: 0.001,\n          }\n        }\n        num_layers_before_predictor: 4\n        kernel_size: 3\n      }\n    }\n    feature_extractor {\n      type: 'ssd_resnet50_v1_fpn_keras'\n      fpn {\n        min_level: 3\n        max_level: 7\n      }\n      min_depth: 16\n      depth_multiplier: 1.0\n      conv_hyperparams {\n        activation: RELU_6,\n        regularizer {\n          l2_regularizer {\n            weight: 0.0004\n          }\n        }\n        initializer {\n          truncated_normal_initializer {\n            stddev: 0.03\n            mean: 0.0\n          }\n        }\n        batch_norm {\n          scale: true,\n          decay: 0.997,\n          epsilon: 0.001,\n        }\n      }\n      override_base_feature_extractor_hyperparams: true\n    }\n    loss {\n      classification_loss {\n        weighted_sigmoid_focal {\n          alpha: 0.25\n          gamma: 2.0\n        }\n      }\n      localization_loss {\n        weighted_smooth_l1 {\n        }\n      }\n      classification_weight: 1.0\n      localization_weight: 1.0\n    }\n    normalize_loss_by_num_matches: true\n    normalize_loc_loss_by_codesize: true\n    post_processing {\n      batch_non_max_suppression {\n        score_threshold: 1e-8\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 100\n      }\n      score_converter: SIGMOID\n    }\n  }\n}\n\ntrain_config: {\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint: \"ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\/checkpoint\/ckpt-0\"\n  fine_tune_checkpoint_type: \"detection\"\n  batch_size: 64\n  sync_replicas: true\n  startup_delay_steps: 0\n  replicas_to_aggregate: 8\n  use_bfloat16: true\n  num_steps: $training_steps\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n  data_augmentation_options {\n    random_crop_image {\n      min_object_covered: 0.0\n      min_aspect_ratio: 0.75\n      max_aspect_ratio: 3.0\n      min_area: 0.75\n      max_area: 1.0\n      overlap_thresh: 0.0\n    }\n  }\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: 5e-3\n          total_steps: 2500\n          warmup_learning_rate: 5e-4\n          warmup_steps: $warmup_steps\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n}\ntrain_input_reader: {\n  label_map_path: \"dataset\/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"..\/input\/cotsdataset\/dataset\/cots_train-?????-of-00004\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 2;\n}\n\neval_input_reader: {\n  label_map_path: \"dataset\/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"..\/input\/cotsdataset\/dataset\/cots_val-?????-of-00004\"\n  }\n}\n\n\n\"\"\"","b04703dc":"# Define the training pipeline\n\nTRAINING_STEPS = 100\nWARMUP_STEPS = 20\nPIPELINE_CONFIG_PATH='dataset\/pipeline.config'\n\npipeline = Template(config_file_template).substitute(\n    training_steps=TRAINING_STEPS, warmup_steps=WARMUP_STEPS)\n\nwith open(PIPELINE_CONFIG_PATH, 'w') as f:\n    f.write(pipeline)","4c7b376f":"MODEL_DIR='.\/cots_faster_rcnn_resnet50'\nos.mkdir(MODEL_DIR)\n# !mkdir {MODEL_DIR}","8c056308":"# pip install tensorflow-gpu==1.15","18e4b5ba":"!python models\/research\/object_detection\/model_main_tf2.py \\\n    --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n    --model_dir={MODEL_DIR} \\\n    --alsologtostderr","7efaa3fa":"# # !python models\/research\/object_detection\/model_main_tf2.py \\\n# #     --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n# #     --model_dir={MODEL_DIR} \\\n# #     --checkpoint_dir={MODEL_DIR} \\\n# #     --eval_timeout=0 \\\n# #     --alsologtostderr\n# for i in os.listdir():\n#     path = i\n#     isdir = os.path.isdir(path) \n#     if isdir: \n#         print(i,\" : \", os.listdir(i))\n# file1='faster_rcnn_resnet50_v1_640x640_coco17_tpu-8'\n# file2='cots_faster_rcnn_resnet50'","8eb2486a":"!python models\/research\/object_detection\/exporter_main_v2.py \\\n    --input_type image_tensor \\\n    --pipeline_config_path={'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\/pipeline.config'} \\\n    --trained_checkpoint_dir={'faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\/checkpoint'} \\\n    --output_directory={MODEL_DIR}\/output","f7633fec":"# !mkdir {MODEL_DIR}\/output\n# os.mkdir(MODEL_DIR+\"\/output\")\n# os.mkdir('.\/fine_tuned_model')","f8cd0539":"# output_directory = '.\/fine_tuned_model'\n\n# # lst = os.listdir('..\/input\/output\/output')\n# # lst = [l for l in lst if 'ckpt' in l and '.meta' in l]\n# # steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n# # last_model = lst[steps.argmax()].replace('.meta', '')\n\n# # last_model_path = os.path.join(model_dir, last_model)\n# # print(last_model_path)\n# !python models\/research\/object_detection\/export_inference_graph.py \\\n#     --input_type=image_tensor \\\n#     --pipeline_config_path={'..\/input\/output\/output\/pipeline.config'} \\\n#     --output_directory={output_directory} \\\n#     --trained_checkpoint_prefix={'..\/input\/output\/output'}","3a358b6c":"model=tf.keras.models.load_model(\"cots_faster_rcnn_resnet50\/output\/saved_model\")\nos.listdir('cots_faster_rcnn_resnet101')\n","4d6f8169":"# import matplotlib.pyplot as plt\n# BASE_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef'\n# training_data = pd.read_csv(f'{BASE_DIR}\/train.csv')\n# testing_data = pd.read_csv(f'{BASE_DIR}\/test.csv')\n# training_data.info()","ae9fa46c":"# detect_fn1 = tf.saved_model.load('..\/input\/output\/output')\n# detect_fn2 = tf.saved_model.load('cots_faster_rcnn_resnet50\/output\/saved_model')","397f293c":"# testing_data.iloc[0]","0528c4eb":"# xy=[]\n# example = testing_data.iloc[9332, :]\n# video_id = example['video_id']\n# video_frame = example['video_frame']\n# sequence = example['sequence']\n# sequence_frame = example['sequence_frame']\n# image_id = example['image_id']\n# annotations = eval(example['annotations'])\n# print(annotations)\n# print(f'Image ID: {image_id}')\n# for annotations in annotations:\n#     xy.append([annotations['x'], annotations['y'], annotations['x'] + annotations['width'], annotations['y'] + annotations['height']])\n# with Image.open(f'{BASE_DIR}\/train_images\/video_{video_id}\/{video_frame}.jpg') as im:\n#     for xy in xy:\n#         draw = ImageDraw.Draw(im)\n#         draw.rectangle(xy, outline='red', width=2)\n#         im.save('fig1.jpg')\n# Image.open('fig1.jpg')\n# # print(video_frame)","555c0d09":"# import torchvision as tv\n# print(f'Number of images: {len(training_data)}')\n# with Image.open(f'{BASE_DIR}\/train_images\/video_{video_id}\/{video_frame}.jpg') as im:\n#     transform = tv.transforms.PILToTensor()\n#     tensor = transform(im.copy())\n# print(f'Channels: {tensor.shape[0]}\\nHeight: {tensor.shape[1]}\\nWidth: {tensor.shape[2]}')\n\n# def load_image_into_numpy_array(image):\n#     (im_width, im_height) = image.size\n#     return np.array(image.getdata()).reshape(\n#         (im_height, im_width, 3)).astype(np.uint8)\n\n# image_path='..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/1910.jpg'\n# image = Image.open(image_path)\n# # the array based representation of the image will be used later in order to prepare the\n# # result image with boxes and labels on it.\n# image_np = load_image_into_numpy_array(image)\n# # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n# image_np_expanded = np.expand_dims(image_np, axis=0)\n# # image_np_expanded\n# plt.imshow(image)\n\npred = predict(image_np)\npredictions=format_output(image_np,pred)\nprediction_str = ' '.join(predictions)\n\n# detect_fn(image_np_expanded)","37df8f8c":"# import matplotlib.pyplot as plt\n# from object_detection.utils import visualization_utils as viz_utils\n# detections1 = detect_fn1(image_np_expanded)\n# detections2 = detect_fn2(image_np_expanded)\n\n# print(detections)\n# category_index = {\n#     1: {'id': 1, 'name': 'COTS'}}\n# # end_time = time.time()\n# #   elapsed.append(end_time - start_time)\n# image_np = load_image_into_numpy_array(image)\n# plt.rcParams['figure.figsize'] = [42, 21]\n# label_id_offset = 1\n# image_np_with_detections1 = image_np.copy()\n# viz_utils.visualize_boxes_and_labels_on_image_array(\n#         image_np_with_detections1,\n#         detections1['detection_boxes'][0].numpy(),\n#         detections1['detection_classes'][0].numpy().astype(np.int32),\n#         detections1['detection_scores'][0].numpy(),\n#         category_index,\n#         use_normalized_coordinates=True,\n#         max_boxes_to_draw=200,\n#         min_score_thresh=.40,\n#         agnostic_mode=False)\n# plt.subplot(2, 1, 1)\n# plt.imshow(image_np_with_detections1)","4637109c":"# # %matplotlib inline\n# image_np_with_detections2 = image_np.copy()\n# viz_utils.visualize_boxes_and_labels_on_image_array(\n#         image_np_with_detections2,\n#         detections2['detection_boxes'][0].numpy(),\n#         detections2['detection_classes'][0].numpy().astype(np.int32),\n#         detections2['detection_scores'][0].numpy(),\n#         category_index,\n#         use_normalized_coordinates=True,\n#         max_boxes_to_draw=200,\n#         min_score_thresh=.40,\n#         agnostic_mode=False)\n# plt.subplot(2, 1, 1)\n# plt.imshow(image_np_with_detections2)\n# # plt.imshow(image_np_with_detections)\n\n\n# # mean_elapsed = sum(elapsed) \/ float(len(elapsed))","26baf3d9":"# Predict function\ndetect_fn1 = tf.saved_model.load('..\/input\/output\/output')","09607996":"def load_image_into_numpy_array(image):\n    (im_width, im_height) = image.size\n    return np.array(image.getdata()).reshape(\n        (im_height, im_width, 3)).astype(np.uint8)\n\n# \ndef predict(image):\n#     image_np = load_image_into_numpy_array(image)\n    image_np_expanded = np.expand_dims(image, axis=0)\n    detections1 = detect_fn1(image_np_expanded)\n    return detections1\n    ","7b601161":"#visualise\n%matplotlib inline\n\ndef show_pred(detections,image):\n    category_index = {\n    1: {'id': 1, 'name': 'COTS'}}\n    image_np = load_image_into_numpy_array(image)\n    plt.rcParams['figure.figsize'] = [42, 21]\n    label_id_offset = 1\n    image_np_with_detections1 = image_np.copy()\n    viz_utils.visualize_boxes_and_labels_on_image_array(\n            image_np_with_detections1,\n            detections['detection_boxes'][0].numpy(),\n            detections['detection_classes'][0].numpy().astype(np.int32),\n            detections['detection_scores'][0].numpy(),\n            category_index,\n            use_normalized_coordinates=True,\n            max_boxes_to_draw=200,\n            min_score_thresh=.25,\n            agnostic_mode=False)\n    plt.subplot(2, 1, 1)\n    plt.imshow(image_np_with_detections1)","02983481":"# show_pred(pred,image)","a4c2c4c5":"# pred=predict(image)","5e25595b":"# for i in pred.keys():\n#     print(i,\" : \",pred[i])\n# pred['detection_boxes'][0][0]","9f3d8694":"# !rm -rf dataset\/\n# !rm -rf faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\/\n# !rm -rf models\/","7cbb55c0":"# # Remove the dataset files to save space.\n# !rm -rf dataset\n# !rm -rf train_images\n# !rm tensorflow-great-barrier-reef.zip\n\n# # Remove other data downloaded during training.\n# !rm -rf models\n# !rm faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz","ed499fb9":"# print(os.listdir(\".\/cots_faster_rcnn_resnet50\/train\/events.out.tfevents.1642756374.6caee2b3a66d.443.0.v2\"),end=\"\\n\")","6939fbd9":"# %cd \/kaggle\/working\n# from IPython.display import FileLink \n# FileLink(r'.\/cots_faster_rcnn_resnet101\/output')","34152a74":"# FileLink(r'.\/cots_faster_rcnn_resnet101\/output')","b56fbc33":"print(\"hi\")","edc72d9f":"# !python3 models\/research\/object_detection\/export_saved_model.py \\\n#   --export_path=\/tmp\/movinet\/ \\\n#   --model_id=a0 \\\n#   --causal=True \\\n#   --conv_type=\"3d\" \\\n#   --num_classes=600 \\\n#   --use_positional_encoding=False \\\n#   --checkpoint_path=\"\"","727d1078":"# # os.listdir(\"models\/research\/object_detection\/predictors\")\n# for i in os.listdir('dataset'):\n#     s=\"<a href='.\/dataset\/\"+i+\"'> Download File \"+i+\"<\/a>\"\n#     print(s,end=\"\\n\")","1e3080a9":"# import re\n# import numpy as np\n\n# output_directory = '.\/fine_tuned_model'\n\n# lst = os.listdir(MODEL_DIR)\n# lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n# steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n# last_model = lst[steps.argmax()].replace('.meta', '')\n\n# last_model_path = os.path.join(model_dir, last_model)\n# print(last_model_path)\n# !python \/content\/models\/research\/object_detection\/export_inference_graph.py \\\n#     --input_type=image_tensor \\\n#     --pipeline_config_path={pipeline_fname} \\\n#     --output_directory={output_directory} \\\n#     --trained_checkpoint_prefix={last_model_path}","eef66e5f":"import greatbarrierreef\nenv = greatbarrierreef.make_env()\niter_test = env.iter_test() ","a26dc012":"# raw_detection_boxes\n# detection_boxes\n# detection_scores\n# detection_anchor_indices\n# num_detections\n# detection_multiclass_scores\n# detection_classes\n# raw_detection_scores\n# format_prediction(pred['detection_boxes'],pred['detection_scores'])\n\n# Format output\ndef format_output(image,pred):\n#     image_np = load_image_into_numpy_array(image)\n    annot = []\n    for i in range(300):\n        if pred['detection_scores'][0][i].numpy()>0.25:\n    #         print(\"Confidence :\",pred['detection_scores'][0][i].numpy(),\"Co-ordinates : \",pred['detection_boxes'][0][i].numpy())\n            ls=pred['detection_boxes'][0][i].numpy()\n            height,width,chal=image_np.shape\n            xmin=ls[1]*width\n            x2=ls[3]*width\n            ymin=ls[0]*height\n            y2=ls[2]*height\n            height_img=x2-xmin\n            width_img=y2-ymin\n            arr={'x':xmin, 'y':ymin, 'width':width_img , 'height':height_img}\n            annot.append('{:.2f} {} {} {} {}'.format(pred['detection_scores'][0][i], int(xmin), int(ymin), int(width_img), int(height_img)))\n\n    print(annot)\n    return annot\n\n# pred['detection_scores'][0][i].numpy()","efcfb872":"# format_output(image,pred)\n# image_np = load_image_into_numpy_array(image)\n# height,width,chal=image_np.shape\n# height,width","80b07dbd":"type(iter_test) # iterate through all test set images\n     \n#     df_pred['annotations'] = predict(detections1, pixel_array)\n#     env.predict(df_pred)","8b073aea":"# env = greatbarrierreef.make_env()# initialize the environment\n# iter_test = env.iter_test()\n# net = load_model(conf=0.25, iou=0.40)\n# from tqdm.notebook import tqdm\n# tqdm.pandas()\n\n# annot = format_prediction(preds, img)\n# pred_df=pd.dataFrame()\n# pred_df['annotations'] = format_output(image,pred)\n# env.predict()","4bbe2de3":"# for i in iter_test:\n#     print(i)\n#     break","afeff56d":"for (image_np, sample_prediction_df) in iter_test:\n    pred = predict(image_np)\n    predictions=format_output(image_np,pred)\n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n    print('Prediction:', prediction_str)","9f64966e":"# def find(image_np):\n#     pred = predict(image_np)\n#     anot=format_output(image_np,pred)\n#     print(anot)\n# find(image)","de4627e4":"image_path='..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/1910.jpg'\nimage = Image.open(image_path)\n# the array based representation of the image will be used later in order to prepare the\n# result image with boxes and labels on it.\nimage_np = load_image_into_numpy_array(image)\n# Expand dimensions since the model expects images to have shape: [1, None, None, 3]\nimage_np_expanded = np.expand_dims(image_np, axis=0)\n# image_np_expanded\n# plt.imshow(image)\npred = predict(image_np)\n\n\n# predictions=format_output(image_np,pred)\n# prediction_str = ' '.join(predictions)","b73d2312":"\nshow_pred(pred,image)","151f65d0":"print(prediction_str)","1a657679":"output=format_output(image,pred)","8b83316f":"prediction_str = ' '.join(output)\nprediction_str","ffa51895":"# Quiz","1e5d82f5":"### Edit Config","1519cb0c":"<a href=\".\/cots_faster_rcnn_resnet50\/output\/checkpoint\/checkpoint\"> Download File <\/a>","3d12b1c2":"### Clean up","e332e96a":"\n<a href='models\/research\/object_detection\/configs\/tf2\/ssd_resnet50_v1_fpn_1024x1024_coco17_tpu-8.config'> Download File cots_train-00000-of-00004<\/a>\n<!-- <a href='.\/dataset\/cots_train-00001-of-00004'> Download File cots_train-00001-of-00004<\/a>\n<a href='.\/dataset\/label_map.pbtxt'> Download File label_map.pbtxt<\/a>\n<a href='.\/dataset\/cots_train-00002-of-00004'> Download File cots_train-00002-of-00004<\/a>\n<a href='.\/dataset\/cots_val-00001-of-00004'> Download File cots_val-00001-of-00004<\/a>\n<a href='.\/dataset\/cots_val-00000-of-00004'> Download File cots_val-00000-of-00004<\/a>\n<a href='.\/dataset\/cots_val-00003-of-00004'> Download File cots_val-00003-of-00004<\/a>\n<a href='.\/dataset\/cots_train-00003-of-00004'> Download File cots_train-00003-of-00004<\/a>\n<a href='.\/dataset\/cots_val-00002-of-00004'> Download File cots_val-00002-of-00004<\/a> -->","215c92bb":"### Train","360f80b8":"<a href=\".\/cots_faster_rcnn_resnet101\/output\/saved_model\/keras_metadata.pb\"> Download File <\/a>","177a7a3e":"### Stratified KFold and Create Dataset","010a5b42":"### Download Pretrained Model","cfb63cd1":"### Install Object Detectin API","218808ca":"#### Reference Notebook - https:\/\/www.kaggle.com\/ravishah1\/cots-faster-rcnn-training-w-tf-2-0-od-api-0-474","ce394bef":"### Export"}}