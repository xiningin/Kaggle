{"cell_type":{"a43439ea":"code","d0fb6e38":"code","35af152c":"code","93d9d924":"code","51d6e953":"code","442d9af0":"code","885f84bd":"code","b06f2a4e":"code","e3685d27":"code","7180ee4f":"code","8c079729":"markdown","65fe50e8":"markdown","6e7792ac":"markdown","d86f4444":"markdown","3455db80":"markdown"},"source":{"a43439ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport cv2\nimport glob\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nfrom operator import itemgetter\nimport os\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0fb6e38":"train_path = '\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'","35af152c":"def _dicom2array(path, voi_lut=True, fix_monochrome=True, resize=False):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    #Normalize the data: subtract off the minimum, divide by the maximum, convert to 256 uint8\n    data = data - np.min(data)\n    data = data\/np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    #Resize images to target value\n    if resize:\n        data = cv2.resize(data, (256, 256))\n    return data","93d9d924":"def _circumscriber(img: np.array) -> np.array:\n    #First is vertical, second is horizontal, third is slices\n    vmin = 0\n    vlimit = img.shape[0]\n    hmin = 0\n    hlimit = img.shape[1]\n    \n    for i in range(vlimit):\n        if np.max(img[i, :, :]) == 0:\n            vmin += 1\n        else:\n            break\n    vmax = vmin + 1\n    for i in range(vmin+1, vlimit):\n        if np.max(img[i, :, :]) > 0:\n            vmax += 1\n        else:\n            break\n    \n    for j in range(hlimit):\n        if np.max(img[:, j, :]) == 0:\n            hmin += 1\n        else:\n            break\n    hmax = hmin + 1\n    for j in range(hmin+1, hlimit):\n        if np.max(img[:, j, :]) > 0:\n            hmax += 1\n        else:\n            break\n    return img[vmin:vmax, hmin:hmax, :]","51d6e953":"def load_FULL_brain(scan_id, split = 'train', modality='FLAIR'):\n    \"\"\"\n    send all of the images in the chosen modality, in order, as a single 3D np array\n    \"\"\"\n    if split != \"train\" and split != \"test\":\n        print('Please request a valid split: train or test.  Defaulting to train.')\n        split = \"train\"\n        \n    if modality != 'FLAIR' and modality != 'T1w' and modality != 'T1wCE' and modality != 'T2w':\n        print('Please select an appropriate modality: FLAIR, T1w, T1wCE, or T2w')\n        print('Defaulting to FLAIR')\n        modality = 'FLAIR'\n        \n    image = sorted(glob.glob(f'{train_path}\/{split}\/{scan_id}\/{modality}\/*.dcm'))\n    image_slice_locs = [pydicom.dcmread(im)[('0020', '1041')].value for im in image]\n    image_pairs = list(zip(image, image_slice_locs))\n    ordered_IP = sorted(image_pairs, key=itemgetter(1))\n    images = [f[0] for f in ordered_IP]\n    real_images = [_dicom2array(f) for f in images]\n    good_images = np.array([im for im in real_images if np.max(im) > 0]).T\n    final_image = _circumscriber(good_images)\n    return final_image","442d9af0":"def plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(512,512)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i in range(4):\n        img = imgs[:,:,i]\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","885f84bd":"boxtest = load_FULL_brain('00000', split='train', modality='FLAIR')","b06f2a4e":"plot_imgs(boxtest[:,:,100:104])","e3685d27":"class BrainLoader(Dataset):\n    def __init__(self, label_file, path, split, modality, val_split=0.25):\n        train_data = pd.read_csv(os.path.join(path, label_file))\n        self.labels = {}\n        self.path = path\n        brats = list(train_data['BraTS21ID'])\n        mgmt = list(train_data['MGMT_value'])\n        for b, m in zip(brats, mgmt):\n            self.labels[str(b).zfill(5)] = m\n            \n        self.split = split\n        self.modality = modality\n        \n        self.ids = [a.split('\/')[-1] for a in sorted(glob.glob(path + f'\/{split}\/*'))]\n        stop = int(len(self.ids) * (1 - val_split))\n        if split == 'train':\n            self.ids = self.ids[:stop]\n        elif split == 'val':\n            self.ids = self.ids[stop:]\n            \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, idx):\n        p_id = self.ids[idx]\n        imgs = load_FULL_brain(p_id, split=self.split, modality=self.modality)\n        transform = transforms.Compose([transforms.ToTensor()])\n        imgs = transform(imgs)\n        \n        if self.split != 'test':\n            label = torch.tensor(self.labels[p_id], dtype=torch.long)\n            return torch.tensor(imgs, dtype=torch.float32), label\n        return torch.tensor(imgs, dtype=torch.float32)","7180ee4f":"train_bs = 1\ntrain_dataset = BrainLoader('train_labels.csv', train_path, split='train', modality='T1w')\n\ntrain_loader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n\nfor img, label in train_loader:\n    print('Iteration')\n    print(img.shape)\n    print(img.min())\n    print(img.mean())\n    print(img.max())\n    print(label.shape)\n    break","8c079729":"This should give anyone intending to use 3D CNNs in PyTorch a head-start.","65fe50e8":"This is a simple notebook showing how to load the full Brain in PyTorch.  This means, reading in all of the images of a particular modality for a particular patient, putting them in order, and producing a single 3D tensor to be fed into a PyTorch model.  In order for this to work, all images of a particular modality for a particular patient must have the same shape.  This is the case for this dataset, but you may want to investigate it for yourself.\n\nSeveral of these helper functions came from or were inspired by this notebook: https:\/\/www.kaggle.com\/furcifer\/no-baseline-pytorch-cnn-for-mri?scriptVersionId=68186710\n","6e7792ac":"Let's have a look at four slices from the middle of patient 00000's FLAIR brain image.","d86f4444":"If one looks at the raw images, one sees lots of blank space around the actually brain, which is not useful for classification.  The following function finds the edges of the brain and eliminates the surrounding blank space.  Note: this is circumscribing it, not eliminating *all* the blank space, there is still blank space in the corners because PyTorch needs a cuboid input.","3455db80":"Finally, let's test out BrainLoader."}}