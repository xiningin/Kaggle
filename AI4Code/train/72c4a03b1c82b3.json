{"cell_type":{"8a23f907":"code","b3c97003":"code","12c4824b":"code","f9aa0ff8":"code","9b50f500":"code","872be5f3":"code","1736f908":"code","08c58908":"code","790590f8":"code","3befa752":"code","b281d4fd":"code","fc4b7d8f":"code","c29de776":"code","9511eb7c":"code","6c40b2e4":"code","aca878c8":"code","596ef9d2":"code","c82b75e4":"code","91434dd4":"code","6e82d27a":"code","c7d38a80":"code","e1905be6":"code","51e00929":"code","6e0b2ffe":"code","da1ef50f":"code","5a55bf74":"code","03cd2bc9":"code","18901350":"code","e29f5657":"code","82850a0e":"code","2274fe0c":"code","f4002a22":"code","f4fcedcf":"code","93b209e0":"code","5479756c":"code","8b24ff06":"code","f96772a5":"code","b9ae075e":"code","256f2f9e":"code","136f70ce":"code","aa4d0988":"code","61ff31c8":"code","391c8036":"code","afdf832a":"code","ee02b7d5":"code","b41e55fd":"code","c8793da8":"code","76b1e11b":"code","346d8ec2":"code","8624ee16":"code","e803a19c":"code","1aac509c":"code","0ba261b7":"code","0905aa01":"code","e8be3494":"code","b3869054":"code","d1110959":"code","ccccef58":"code","82a53eaf":"code","17ace3b2":"code","5eee6a5f":"code","0e696c5f":"code","bd9527c4":"code","7b7b0039":"code","b0156d34":"code","39ad5211":"code","7637b57c":"code","9d31ba31":"code","dd72c20c":"code","2063ace0":"code","d9dae3e2":"code","d5b2cd1b":"markdown","355f027c":"markdown","0e3617b1":"markdown","bc939a5a":"markdown","42b1220a":"markdown","c7759e4c":"markdown","dafba962":"markdown","1bac7281":"markdown","5d28d4ff":"markdown","8aad4065":"markdown","fbc6a67c":"markdown","f07fefba":"markdown","af37261e":"markdown","f085c64e":"markdown","cec68c58":"markdown","59246eee":"markdown","d695600b":"markdown","72b59831":"markdown","d3708854":"markdown","36154b37":"markdown"},"source":{"8a23f907":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b3c97003":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","12c4824b":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(train.shape)\ntrain.head()","f9aa0ff8":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(test.shape)\ntest.head()","9b50f500":"sample_sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nprint(sample_sub.shape)\nsample_sub.head()","872be5f3":"df = pd.concat([train, test], sort=False).reset_index(drop=True)\nprint(df.shape)\ndf.head()","1736f908":"df.tail()","08c58908":"features = df.columns[1:-1]\nprint(len(features))\nfeatures","790590f8":"num_features = train.select_dtypes(include='number').columns[1:-1]\ncat_features = train.select_dtypes(exclude='number').columns","3befa752":"import pandas_profiling","b281d4fd":"pandas_profiling.ProfileReport(df)","fc4b7d8f":"target = train['SalePrice']\ntarget.head(10)","c29de776":"target.describe()","9511eb7c":"%matplotlib inline\nplt.figure(figsize=[20, 10])\ntarget.hist(bins=100)","6c40b2e4":"corr_mat = train.loc[:, num_features].corr()\nplt.figure(figsize=[15, 15])\nsns.heatmap(corr_mat, square=True)","aca878c8":"fig = plt.figure(figsize=[30, 30])\nplt.tight_layout()\n\nfor i, feature in enumerate(num_features):\n    ax = fig.add_subplot(6, 6, i+1)\n    sns.regplot(x=train.loc[:, feature],\n                y=train.loc[:, 'SalePrice'])","596ef9d2":"fig = plt.figure(figsize=[30, 40])\nplt.tight_layout()\n\nfor i, feature in enumerate(cat_features):\n    ax = fig.add_subplot(9, 5, i+1)\n    sns.violinplot(x=df.loc[:, feature],\n                   y=df.loc[:, 'SalePrice'])","c82b75e4":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","91434dd4":"for col in cat_features:\n    df[col] = df[col].fillna('NULL')\n    df[col+'_le'] = le.fit_transform(df[col])","6e82d27a":"df = df.drop(cat_features, axis=1)","c7d38a80":"df.head()","e1905be6":"le_features = []\nfor feat in cat_features:\n    le_features.append(feat+'_le')","51e00929":"len(le_features)","6e0b2ffe":"for feat in num_features:\n    df[feat] = df[feat].fillna(-1)","da1ef50f":"train = df[df['Id'].isin(train['Id'])]\ntest = df[df['Id'].isin(test['Id'])]","5a55bf74":"X_train = train.drop(['Id', 'SalePrice'], axis=1)\ny_train = train['SalePrice']\n\nX_test = test.drop(['Id', 'SalePrice'], axis=1)","03cd2bc9":"from sklearn.model_selection import train_test_split","18901350":"X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)","e29f5657":"from sklearn.linear_model import Ridge","82850a0e":"reg = Ridge(alpha=0.1, random_state=42)","2274fe0c":"reg.fit(X_train_, y_train_)","f4002a22":"from sklearn.metrics import mean_squared_error\ndef metric(y_true, y_pred):\n    return mean_squared_error(np.log(y_true), np.log(y_pred)) ** 0.5","f4fcedcf":"pred_train = reg.predict(X_train_)\nrmse_train = mean_squared_error(np.log(y_train_), np.log(pred_train))**0.5\nrmse_train","93b209e0":"pred_train[:5]","5479756c":"y_train_.head()","8b24ff06":"pred_val = reg.predict(X_val)\nrmse_val = mean_squared_error(np.log(y_val), np.log(pred_val))**0.5\nrmse_val","f96772a5":"pred_test = reg.predict(X_test)\nprint(pred_test.shape)\npred_test[:5]","b9ae075e":"sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nprint(sub.shape)\nsub.head()","256f2f9e":"sub['SalePrice'] = pred_test\nsub.head()","136f70ce":"sub.to_csv('submission_ridge_regression.csv', index=False)","aa4d0988":"from sklearn.model_selection import KFold","61ff31c8":"def cv(reg, X_train, y_train, X_test):\n    kf = KFold(n_splits=5, random_state=42)\n    pred_test_mean = np.zeros(sub['SalePrice'].shape)\n    for train_index, val_index in kf.split(X_train):\n        X_train_train = X_train.iloc[train_index]\n        y_train_train = y_train.iloc[train_index]\n\n        X_train_val = X_train.iloc[val_index]\n        y_train_val = y_train.iloc[val_index]\n\n        reg.fit(X_train_train, y_train_train)\n        pred_train = reg.predict(X_train_train)\n        metric_train = metric(y_train_train, pred_train)\n        print('train metric: ', metric_train)\n\n        pred_val = reg.predict(X_train_val)\n        metric_val = metric(y_train_val, pred_val)\n        print('val metric:   ', metric_val)\n        print()\n\n        pred_test = reg.predict(X_test)\n        pred_test_mean += pred_test \/ kf.get_n_splits()\n        \n    return pred_test_mean","391c8036":"reg = Ridge(alpha=0.3, random_state=42)\npred_test_mean = cv(reg, X_train, y_train, X_test)","afdf832a":"sub['SalePrice'] = pred_test_mean\nsub.head()","ee02b7d5":"sub.to_csv('submission_ridge_regression_5f_CV.csv', index=False)","b41e55fd":"y_train_log = np.log(y_train)\nplt.figure(figsize=[20, 10])\nplt.hist(y_train_log, bins=50);","c8793da8":"reg = Ridge(alpha=0.3, random_state=42)\npred_test_mean = cv(reg, X_train, y_train_log, X_test)","76b1e11b":"sub['SalePrice'] = np.exp(pred_test_mean)\nsub.to_csv('submission_ridge_regression_cv_target_log.csv', index=False)\nsub.head()","346d8ec2":"from sklearn.preprocessing import StandardScaler","8624ee16":"scaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\nX_train_scaled.head()","e803a19c":"X_test_scaled = pd.DataFrame(scaler.fit_transform(X_test))\nX_test_scaled.head()","1aac509c":"reg = Ridge(alpha=0.3, random_state=42)\npred_test = cv(reg, X_train_scaled, y_train_log, X_test_scaled)","0ba261b7":"sub['SalePrice'] = np.exp(pred_test)\nsub.to_csv('submission_ridge_regression_cv_target_log_scaled_feature.csv', index=False)\nsub.head()","0905aa01":"from sklearn.ensemble import RandomForestRegressor","e8be3494":"reg = RandomForestRegressor(n_estimators=1000, random_state=42)\npred_test = cv(reg, X_train, y_train_log, X_test)","b3869054":"sub['SalePrice'] = np.exp(pred_test)\nsub.to_csv('submission_random_forest_cv_target_log.csv', index=False)\nsub.head()","d1110959":"reg.fit(X_train, y_train_log)","ccccef58":"feature_importances = reg.feature_importances_\nfeature_importances","82a53eaf":"feature_importances = pd.DataFrame([X_train.columns, feature_importances]).T\nfeature_importances = feature_importances.sort_values(by=1, ascending=False)","17ace3b2":"plt.figure(figsize=[20, 20])\nsns.barplot(x=feature_importances.iloc[:, 1],\n            y=feature_importances.iloc[:, 0], orient='h')\nplt.tight_layout()\nplt.show()","5eee6a5f":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint(\"Train Shape: \" + str(train.shape))\nprint(\"Test Shape: \" + str(test.shape))","0e696c5f":"y = train['SalePrice']\ntrain.drop(\"SalePrice\", axis = 1, inplace = True)","bd9527c4":"from sklearn import preprocessing\n#from sklearn.preprocessing import LabelEncoder\nimport datetime\n\nntrain = train.shape[0]\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nprint(\"all_data size is : {}\".format(all_data.shape))\n\ndef ordinal_encode(df, col, order_list):\n    df[col] = df[col].astype('category', ordered=True, categories=order_list).cat.codes\n    return df\n\ndef label_encode(df, col):\n    for c in col:\n        #print(str(c))\n        encoder = preprocessing.LabelEncoder()\n        df[c] = encoder.fit_transform(df[c].astype(str))\n    return df \n\ndef split_all_data(all_data, ntrain):\n    print(('Split all_data back to train and test: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())))\n    train_df = all_data[:ntrain]\n    test_df = all_data[ntrain:]\n    return train_df, test_df\n\n\"\"\"\nNOW START ENCODING 1. ORDINALS\n\"\"\"\nprint(('Ordinal Encoding: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())))\norder_list = ['Ex', 'Gd', 'TA', 'Fa', 'Po'] #This applies to a few different columns\ncols = ['KitchenQual', 'ExterQual', 'ExterCond', 'HeatingQC']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n\norder_list = ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NA'] #This applies to a few different columns\ncols = ['BsmtQual', 'BsmtCond']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n\norder_list = ['Gd', 'Av', 'Mn', 'No', 'NA']\ncols = ['BsmtExposure', 'FireplaceQu', 'GarageQual', 'GarageCond']\n\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n    \norder_list = ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal']\ncols = ['Functional']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n    \norder_list = ['Fin', 'RFn', 'Unf', 'NA']\ncols = ['GarageFinish']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n    \norder_list = ['Ex', 'Gd', 'TA', 'Fa', 'NA'] \ncols = ['PoolQC']\nfor col in cols:\n    all_data = ordinal_encode(all_data, col, order_list)\n\n\"\"\"\nENCODE 2. NON-ORDINAL LABELS\n\"\"\"\nprint(('Label Encoding: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now())))\ncols_to_label_encode = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', \n                       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'CentralAir', \n                       'Electrical', 'GarageType', 'PavedDrive', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\nall_data = label_encode(all_data, cols_to_label_encode)\n\ntrain, test = split_all_data(all_data, ntrain)\n\nprint(\"Train Shape: \" + str(train.shape))\nprint(\"Test Shape: \" + str(test.shape))","7b7b0039":"def score_transformer(y):\n    y = np.log(y)\n    \n    return y\n\ny = score_transformer(y)","b0156d34":"all_data = pd.concat((train, test)).reset_index(drop=True)\n\nall_data['fe.sum.GrLivArea_BsmtFinSF1_BsmtFinSF2'] = all_data['GrLivArea'] + all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] \nall_data['fe.sum.OverallQual_Overall_Cond'] = all_data['OverallQual'] + all_data['OverallCond']\nall_data['fe.mult.OverallQual_Overall_Cond'] = all_data['OverallQual'] * all_data['OverallCond']\nall_data['fe.sum.KitchenQual_ExterQual'] = all_data['KitchenQual'] + all_data['ExterQual']\nall_data['fe.mult.OverallQual_Overall_Cond'] = all_data['OverallQual'] * all_data['OverallCond']\nall_data['fe.ratio.1stFlrSF_2ndFlrSF'] = all_data['1stFlrSF'] \/ all_data['2ndFlrSF']\nall_data['fe.ratio.BedroomAbvGr_GrLivArea'] = all_data['BedroomAbvGr'] \/ all_data['GrLivArea']","39ad5211":"train_features = list(all_data)\n#Id should be removed for modelling\ntrain_features = [e for e in train_features if e not in ('ExterQual', 'Condition2', 'GarageCond', 'Street', 'Alley', 'PoolArea', 'PoolQC', 'Utilities', \n                                                         'GarageQual', 'MiscVal', 'MiscFeature')]\n\ntrain, test = split_all_data(all_data, ntrain)\n\ntrain_features.remove('Id')","7637b57c":"from sklearn.model_selection import KFold\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error\n\nnfolds=5\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=37) #33 originally\ny_valid_pred = 0*y\ny_valid_pred_cat = 0*y\nfold_scores = [0] * nfolds\nfold_scores_cat = [0] * nfolds\n\nimportances = pd.DataFrame()\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nsub_reg_preds_cat = np.zeros(test.shape[0])","9d31ba31":"for fold_, (train_index, val_index) in enumerate(kf.split(train, y)):\n    trn_x, trn_y = train[train_features].iloc[train_index], y.iloc[train_index]\n    val_x, val_y = train[train_features].iloc[val_index], y.iloc[val_index]\n    \n    reg = LGBMRegressor(\n        num_leaves=10,\n        max_depth=3,\n        min_child_weight=50,\n        learning_rate=0.1,\n        n_estimators=1000,\n        #min_split_gain=0.01,\n        #gamma=100,\n        reg_alpha=0.01,\n        reg_lambda=5,\n        subsample=1,\n        colsample_bytree=0.5,\n        random_state=2\n    )\n    \n    reg.fit(\n        trn_x, trn_y,\n        eval_set=[(val_x, val_y)],\n        early_stopping_rounds=20,\n        verbose=100,\n        eval_metric='rmse'\n    )    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n\n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n\n    y_valid_pred.iloc[val_index] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    y_valid_pred[y_valid_pred < 0] = 0\n    fold_score = reg.best_score_['valid_0']['rmse']\n    fold_scores[fold_] = fold_score\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += _preds \/ nfolds\n    \nprint(\"LightGBM CV RMSE: \" + str(mean_squared_error(y, y_valid_pred) ** .5))\nprint(\"LightGBM CV standard deviation: \" + str(np.std(fold_scores)))","dd72c20c":"import seaborn as sns\nimport warnings\n#cat_rgr.fit(X_train, y_train, eval_set=(X_valid, y_valid), logging_level='Verbose', plot=False)\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(10, 14))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","2063ace0":"sub_reg_preds = np.exp(sub_reg_preds)\ntest.is_copy = False\ntest.loc[:,'SalePrice'] = sub_reg_preds\ntest[['Id', 'SalePrice']].to_csv(\"lightgbm_fold5.csv\", index=False)","d9dae3e2":"import pandas as pd\n\nmodel1 = pd.read_csv(\"..\/input\/predicted-data\/lightgbm_fold5.csv\")\nmodel2 = pd.read_csv(\"..\/input\/predicted-data\/submission_random_forest_cv_target_log.csv\")\nmodel3 = pd.read_csv(\"..\/input\/predicted-data\/submission_ridge_regression_5f_CV.csv\")\n\nblend_model = (model1[\"SalePrice\"] + model2[\"SalePrice\"] + model3[\"SalePrice\"]) \/3\n\nsub['SalePrice'] = blend_model\n\nsub.to_csv(\"blend_model.csv\", index = False)","d5b2cd1b":"---\n# Load data","355f027c":"---\n# Submission","0e3617b1":"---\n# 5-fold CV\n## Model: Ridge Regression","bc939a5a":"---\n# Distribution of target","42b1220a":"---\n# Data fields\nHere's a brief version of what you'll find in the data description file.\n\n- **SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.**\n- MSSubClass: The building class\n- MSZoning: The general zoning classification\n- LotFrontage: Linear feet of street connected to property\n- LotArea: Lot size in square feet\n- Street: Type of road access\n- Alley: Type of alley access\n- LotShape: General shape of property\n- LandContour: Flatness of the property\n- Utilities: Type of utilities available\n- LotConfig: Lot configuration\n- LandSlope: Slope of property\n- Neighborhood: Physical locations within Ames city limits\n- Condition1: Proximity to main road or railroad\n- Condition2: Proximity to main road or railroad (if a second is present)\n- BldgType: Type of dwelling\n- HouseStyle: Style of dwelling\n- OverallQual: Overall material and finish quality\n- OverallCond: Overall condition rating\n- YearBuilt: Original construction date\n- YearRemodAdd: Remodel date\n- RoofStyle: Type of roof\n- RoofMatl: Roof material\n- Exterior1st: Exterior covering on house\n- Exterior2nd: Exterior covering on house (if more than one material)\n- MasVnrType: Masonry veneer type\n- MasVnrArea: Masonry veneer area in square feet\n- ExterQual: Exterior material quality\n- ExterCond: Present condition of the material on the exterior\n- Foundation: Type of foundation\n- BsmtQual: Height of the basement\n- BsmtCond: General condition of the basement\n- BsmtExposure: Walkout or garden level basement walls\n- BsmtFinType1: Quality of basement finished area\n- BsmtFinSF1: Type 1 finished square feet\n- BsmtFinType2: Quality of second finished area (if present)\n- BsmtFinSF2: Type 2 finished square feet\n- BsmtUnfSF: Unfinished square feet of basement area\n- TotalBsmtSF: Total square feet of basement area\n- Heating: Type of heating\n- HeatingQC: Heating quality and condition\n- CentralAir: Central air conditioning\n- Electrical: Electrical system\n- 1stFlrSF: First Floor square feet\n- 2ndFlrSF: Second floor square feet\n- LowQualFinSF: Low quality finished square feet (all floors)\n- GrLivArea: Above grade (ground) living area square feet\n- BsmtFullBath: Basement full bathrooms\n- BsmtHalfBath: Basement half bathrooms\n- FullBath: Full bathrooms above grade\n- HalfBath: Half baths above grade\n- Bedroom: Number of bedrooms above basement level\n- Kitchen: Number of kitchens\n- KitchenQual: Kitchen quality\n- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n- Functional: Home functionality rating\n- Fireplaces: Number of fireplaces\n- FireplaceQu: Fireplace quality\n- GarageType: Garage location\n- GarageYrBlt: Year garage was built\n- GarageFinish: Interior finish of the garage\n- GarageCars: Size of garage in car capacity\n- GarageArea: Size of garage in square feet\n- GarageQual: Garage quality\n- GarageCond: Garage condition\n- PavedDrive: Paved driveway\n- WoodDeckSF: Wood deck area in square feet\n- OpenPorchSF: Open porch area in square feet\n- EnclosedPorch: Enclosed porch area in square feet\n- 3SsnPorch: Three season porch area in square feet\n- ScreenPorch: Screen porch area in square feet\n- PoolArea: Pool area in square feet\n- PoolQC: Pool quality\n- Fence: Fence quality\n- MiscFeature: Miscellaneous feature not covered in other categories\n- MiscVal: $Value of miscellaneous feature\n- MoSold: Month Sold\n- YrSold: Year Sold\n- SaleType: Type of sale\n- SaleCondition: Condition of sale","c7759e4c":"---\n# Data split","dafba962":"# Feature Importance","1bac7281":"---\n# Fill NaN","5d28d4ff":"---\n# Label Encoding","8aad4065":"## Model: Random Forest","fbc6a67c":"---\n# Hold-out validation\n## Model: Ridge Regression","f07fefba":"---\n# Plot feature importances","af37261e":"# AIMS Kaggle Seminar 2nd Day","f085c64e":"# Model\uff1aLightGBM","cec68c58":"---\n# Target scaling","59246eee":"---\n# Correlation of feature and target","d695600b":"# Model Blending","72b59831":"# AIMS Kaggle Seminar 1st Day","d3708854":"---\n# Correlation of features","36154b37":"# Features scaling"}}