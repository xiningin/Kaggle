{"cell_type":{"f01ff325":"code","65da1dcf":"code","ba2b83a3":"code","1ab69cb7":"code","3a807caf":"code","a5beedab":"code","411ace1b":"code","6eaf69f1":"code","4c58da2f":"code","87f8c684":"code","21cdc280":"code","6f888e41":"code","c4251c7d":"code","8e57bbe7":"code","b55a20b5":"code","f3292aef":"code","a1f4d954":"code","33d2d6b5":"code","e88ca47b":"code","e2740da8":"code","f235e965":"code","b56a7039":"code","867a58db":"code","e87782d1":"markdown","88cffedd":"markdown","4fca93d9":"markdown","17c8e9c4":"markdown","c7c78a7f":"markdown","11a8e053":"markdown","293272f7":"markdown","71fa1c98":"markdown","67e259d4":"markdown","ca419217":"markdown","fbaf9cb4":"markdown","2b9c3381":"markdown"},"source":{"f01ff325":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom subprocess import check_output\nfrom datetime import time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","65da1dcf":"df = pd.read_csv('..\/input\/crowdedness-at-the-campus-gym\/data.csv')","ba2b83a3":"df.head()","1ab69cb7":"df.describe()","3a807caf":"correlation = df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')","a5beedab":"sns.distplot(df['temperature'], kde=True, rug=True)","411ace1b":"sns.distplot(df['number_people'], kde=True, rug=True)","6eaf69f1":"def time_to_seconds(time):\n    return time.hour * 3600 + time.minute * 60 + time.second","4c58da2f":"df = df.drop(\"date\", axis=1)\nnoon = time_to_seconds(time(12, 0, 0))\ndf.timestamp = df.timestamp.apply(lambda t: abs(noon - t))\n# one hot encoding\ncolumns = [\"day_of_week\", \"month\", \"hour\"]\ndf = pd.get_dummies(df, columns=columns)\ndf.head()","87f8c684":"data = df.values\nX = data[:, 1:]  # all rows, no label\ny = data[:, 0]  # all rows, label only\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","21cdc280":"scaler1 = StandardScaler()\nscaler1.fit(X_train[:, 0:1])\nX_train[:, 0:1] = scaler1.transform(X_train[:, 0:1])\nX_test[:, 0:1] = scaler1.transform(X_test[:, 0:1])\nscaler2 = StandardScaler()\nscaler2.fit(X_train[:, 3:4])\nX_train[:, 3:4] = scaler2.transform(X_train[:, 3:4])\nX_test[:, 3:4] = scaler2.transform(X_test[:, 3:4])","6f888e41":"print(X_train)","c4251c7d":"model = RandomForestRegressor(n_jobs=-1)","8e57bbe7":"estimators = np.arange(40, 160, 20)\nscores = []\nfor n in estimators:\n    model.set_params(n_estimators=n)\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n    print('score = ', model.score(X_test, y_test))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","b55a20b5":"import sklearn.metrics as metrics","f3292aef":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true + 1 - y_pred) \/ (y_true + 1)) * 100)\ndef regression_results(y_true, y_pred):\n    # Regression metrics\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n\n    print('explained_variance: ', round(explained_variance,4))    \n    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n    print('R^2: ', r2)\n    print('MAE: ', mean_absolute_error)\n    print('MSE: ', mse)\n    print('RMSE: ', np.sqrt(mse))\n    print('MAPE: ', mean_absolute_percentage_error(y_true, y_pred), '%')","a1f4d954":"regression_results(y_test, model.predict(X_test))","33d2d6b5":"from keras.models import Sequential \nfrom keras.layers import Dense \nfrom keras.optimizers import Adam \nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","e88ca47b":"print(X_train.shape)\nprint(y_train.shape)","e2740da8":"def create_network():\n    model = Sequential()\n    model.add(Dense(64, input_shape=(49,), activation='relu')) \n    model.add(Dense(1, activation='relu')) \n    #model.add(Dense(128, activation='relu'))\n        \n    model.compile(loss='mse', optimizer=Adam(), metrics=['accuracy']) \n    return model","f235e965":"earlyStopping = EarlyStopping(patience=30, verbose=1)\nmcp_save = ModelCheckpoint('gym_model.h5', verbose=0, save_best_only=True, save_weights_only=True)\nreduce_lr_loss = ReduceLROnPlateau(factor=0.1, patience=15, min_lr=0.000001, verbose=0)\ncallbacks = [earlyStopping, mcp_save, reduce_lr_loss]","b56a7039":"model = create_network()\nresults = model.fit(X_train, y_train, epochs = 500, batch_size = 64, \n                    validation_data = (X_test, y_test), verbose = 0)","867a58db":"model.evaluate(X_test, y_test)","e87782d1":"Evaluate Regression model","88cffedd":"Encoding and Scale Data","4fca93d9":"Correlation Matrix between different fearures and Heatmap","17c8e9c4":"**Data Pre-Processing**","c7c78a7f":"**Exploratory data analysis**","11a8e053":"**Convert time format from 24h to 12h**","293272f7":"Split dataframe to Train set and Test set","71fa1c98":"Now, make a neural network by Keras model for regression problem. The loss function will be used is MSE and activations will be used for non-linear regression","67e259d4":"## A simple Neural Network by Keras","ca419217":"Implement scaler on timestamp and Temperature","fbaf9cb4":"## RANDOM FOREST REGRESSOR MODEL","2b9c3381":"# Regression with RF and NN "}}