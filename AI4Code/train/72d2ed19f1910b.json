{"cell_type":{"07864403":"code","232731f5":"code","522bebc8":"code","ebca8907":"code","a6563d5b":"code","5f484f88":"code","25c56f7a":"code","e011e1e5":"code","bf58965f":"code","c6d3bbd7":"code","fcce48e8":"code","1422c436":"code","f7dba637":"code","e64dbe28":"code","7ba423ba":"code","5f7275f9":"code","6558bf63":"code","685c7901":"code","8b25a4dd":"code","9a3b8d04":"code","c92a1aa1":"code","a57b1036":"markdown","abd63966":"markdown","2387f90b":"markdown","39805f2c":"markdown","b1777193":"markdown","54fcd38a":"markdown","6c583910":"markdown","ba34f6ca":"markdown","635b39c3":"markdown","a858fa64":"markdown","5a1b20ad":"markdown","4c126e39":"markdown","d985b918":"markdown","186d3d5a":"markdown","3f91d3d6":"markdown","192f3aea":"markdown","239540fb":"markdown"},"source":{"07864403":"\nimport os\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nimport subprocess\nimport pickle\nimport numpy as np\nimport io\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pprint\nimport matplotlib.pyplot as plt\nimport pickle as pkl\n!pip install biobert-embedding\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom scipy.spatial import distance\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = set(stopwords.words('english'))\ndef preprocess_sentence(text):\n    text = text.replace('\/', ' \/ ')\n    text = text.replace('.-', ' .- ')\n    text = text.replace('.', ' . ')\n    text = text.replace('\\'', ' \\' ')\n    text = text.lower()\n\n    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n\n    return ' '.join(tokens)\nfrom nltk import tokenize\n","232731f5":"from biobert_embedding.embedding import BiobertEmbedding\nimport os\nimport torch\nimport logging\nimport tensorflow as tf\nfrom pathlib import Path\nfrom biobert_embedding import downloader\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\nlogging.basicConfig(filename='app.log', filemode='w',format='%(asctime)s %(message)s', level=logging.INFO)\n\nlogger = logging.getLogger(__name__)\nclass BiobertEmbedding(object):\n    \"\"\"\n    Encoding from BioBERT model (BERT finetuned on PubMed articles).\n    Parameters\n    ----------\n    model : str, default Biobert.\n            pre-trained BERT model\n    \"\"\"\n\n    def __init__(self, model_path=None):\n\n        if model_path is not None:\n            self.model_path = model_path\n        else:\n            self.model_path = downloader.get_BioBert(\"google drive\")\n\n        self.tokens = \"\"\n        self.sentence_tokens = \"\"\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_path)\n        # Load pre-trained model (weights)\n        self.model = BertModel.from_pretrained(self.model_path)\n        self.model.to(device)\n        logger.info(\"Initialization Done !!\")\n\n    def process_text(self, text):\n\n        marked_text = \"[CLS] \" + text + \" [SEP]\"\n        # Tokenize our sentence with the BERT tokenizer.\n        tokenized_text = self.tokenizer.tokenize(marked_text)\n        return tokenized_text\n\n\n    def handle_oov(self, tokenized_text, word_embeddings):\n        embeddings = []\n        tokens = []\n        oov_len = 1\n        for token,word_embedding in zip(tokenized_text, word_embeddings):\n            if token.startswith('##'):\n                token = token[2:]\n                tokens[-1] += token\n                oov_len += 1\n                embeddings[-1] += word_embedding\n            else:\n                if oov_len > 1:\n                    embeddings[-1] \/= oov_len\n                tokens.append(token)\n                embeddings.append(word_embedding)\n        return tokens,embeddings\n\n\n    def eval_fwdprop_biobert(self, tokenized_text):\n\n        # Mark each of the tokens as belonging to sentence \"1\".\n        segments_ids = [1] * len(tokenized_text)\n        # Map the token strings to their vocabulary indeces.\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n\n        # Convert inputs to PyTorch tensors\n        tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n        segments_tensors = torch.tensor([segments_ids]).to(device)\n\n        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n        self.model.eval()\n        # Predict hidden states features for each layer\n        with torch.no_grad():\n            encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n\n        return encoded_layers\n\n\n    def word_vector(self, text, handle_oov=True, filter_extra_tokens=True):\n\n        tokenized_text = self.process_text(text)\n\n        encoded_layers = self.eval_fwdprop_biobert(tokenized_text)\n\n        # Concatenate the tensors for all layers. We use `stack` here to\n        # create a new dimension in the tensor.\n        token_embeddings = torch.stack(encoded_layers, dim=0)\n        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n        # Swap dimensions 0 and 1.\n        token_embeddings = token_embeddings.permute(1,0,2)\n\n        # Stores the token vectors, with shape [22 x 768]\n        word_embeddings = []\n        logger.info(\"Summing last 4 layers for each token\")\n        # For each token in the sentence...\n        for token in token_embeddings:\n\n            # `token` is a [12 x 768] tensor\n            # Sum the vectors from the last four layers.\n            sum_vec = torch.sum(token[-4:], dim=0)\n\n            # Use `sum_vec` to represent `token`.\n            word_embeddings.append(sum_vec)\n\n        self.tokens = tokenized_text\n        if filter_extra_tokens:\n            # filter_spec_tokens: filter [CLS], [SEP] tokens.\n            word_embeddings = word_embeddings[1:-1]\n            self.tokens = tokenized_text[1:-1]\n\n        if handle_oov:\n            self.tokens, word_embeddings = self.handle_oov(self.tokens,word_embeddings)\n        logger.info(self.tokens)\n        logger.info(\"Shape of Word Embeddings = %s\",str(len(word_embeddings)))\n        return word_embeddings\n\n\n\n    def sentence_vector(self,text):\n\n        logger.info(\"Taking last layer embedding of each word.\")\n        logger.info(\"Mean of all words for sentence embedding.\")\n        tokenized_text = self.process_text(text)\n        self.sentence_tokens = tokenized_text\n        encoded_layers = self.eval_fwdprop_biobert(tokenized_text)\n\n        # `encoded_layers` has shape [12 x 1 x 22 x 768]\n        # `token_vecs` is a tensor with shape [22 x 768]\n        token_vecs = encoded_layers[11][0]\n\n        # Calculate the average of all 22 token vectors.\n        sentence_embedding = torch.mean(token_vecs, dim=0)\n        logger.info(\"Shape of Sentence Embeddings = %s\",str(len(sentence_embedding)))\n        return sentence_embedding\n","522bebc8":"device.type","ebca8907":"pip install -U sentence-transformers","a6563d5b":"model_path = downloader.get_BioBert(\"google drive\")","5f484f88":"text = \"Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.\"\\\n\n# Class Initialization (You can set default 'model_path=None' as your finetuned BERT model path while Initialization)\nbiobert = BiobertEmbedding(model_path)\n\nword_embeddings = biobert.word_vector(text)\nsentence_embedding = biobert.sentence_vector(text)\n\nprint(\"Text Tokens: \", biobert.tokens)\n# Text Tokens:  ['breast', 'cancers', 'with', 'her2', 'amplification', 'have', 'a', 'higher', 'risk', 'of', 'cns', 'metastasis', 'and', 'poorer', 'prognosis', '.']\n\nprint ('Shape of Word Embeddings: %d x %d' % (len(word_embeddings), len(word_embeddings[0])))\n# Shape of Word Embeddings: 16 x 768\n\nprint(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n# Shape of Sentence Embedding =  768","25c56f7a":"# Use BERT for mapping tokens to embeddings\nfrom sentence_transformers import models\nfrom sentence_transformers import SentenceTransformer\nword_embedding_model = models.BERT('\/kaggle\/working\/'+model_path.name)\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=True,\n                               pooling_mode_max_tokens=True)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","e011e1e5":"from sentence_transformers import SentenceTransformer\n\nsentence_embeddings = model.encode([text])\nprint(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n","bf58965f":"data=pd.read_excel(\"\/kaggle\/input\/task3covid\/task1_results_summary.xlsx\",index=False).dropna()\n","c6d3bbd7":"meta_df_title_abstract=data\nlen1=meta_df_title_abstract.shape[0]\nlist1=list(range(len1))\nmeta_df_title_abstract['pid']=list1\nmeta_df_title_abstract.head()\nmeta_df_title_abstract['summary_preprocessed']=meta_df_title_abstract['Text'].apply(lambda x:tokenize.sent_tokenize(x))\nnew_data_sent=meta_df_title_abstract['summary_preprocessed'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index', 'value']].set_index('index')\nnew_data_sent=new_data_sent.merge(meta_df_title_abstract[['cord_uid', 'lsid', 'gsid', 'Name', 'Text', 'Subtype', 'summary', 'pid']],right_index=True,left_index=True,how='left')\nnew_data_sent['wrd_cnt']=new_data_sent['value'].str.split().str.len()\nnew_data_sent_strip=new_data_sent[new_data_sent['wrd_cnt']>6]\nprint(\"wrd cnt > 6 \" + str(new_data_sent.shape))\nnew_data_sent_strip=new_data_sent_strip[new_data_sent_strip['wrd_cnt']<100]\nprint(\"wrd cnt < 200 \" + str(new_data_sent_strip.shape))\nnew_data_sent_strip['value_edit']=new_data_sent_strip['value'].apply(lambda x:preprocess_sentence(x))","fcce48e8":"def get_bert_embedding(wr):\n    #try :\n    return biobert.sentence_vector(wr).cpu()\n\ndef get_sent_bert_embedding(wr):\n    #try :\n    return model.encode([wr],show_progress_bar=False)\n\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\ndef process_on_set(files, num_workers,function):\n    def process_file(i):\n        filename_2 = files[i]\n\n        y_pred = function(filename_2)\n        return y_pred\n\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        predictions = ex.map(process_file, range(len(files)))\n\n    return list(predictions)\n\n#xt=new_data_sent_strip['value_edit'].head().apply(lambda x:chk_len(x))","1422c436":"\n%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].head(1000).apply(lambda x:get_sent_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].head(1000).values,4,get_bert_embedding)","f7dba637":"%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].head(1000).apply(lambda x:get_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].head(1000).values,4,get_bert_embedding)","e64dbe28":"%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].apply(lambda x:get_sent_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].values,4,get_bert_embedding)\nnew_data_sent_strip['Embedding']=xt\nimport pickle\nwith open('\/kaggle\/working\/embeddings37912.pickle', 'wb') as handle:\n    pickle.dump(new_data_sent_strip, handle)\nnew_data_sent_strip.to_csv(\"new_data_sent_strip.csv\",index=False)","7ba423ba":"def prepare_dataset(df,query,query_id=0,val=0.9):\n    #print(\"Query is \")\n    #print(query_text[query_id])\n    query_embedding_sent_df['match']=df['Embedding'].apply(lambda x:cosine_similarity(x[0].reshape(1,x[0].shape[0]),query[query_id].reshape(1,query[query_id].shape[0]))[0][0])\n    shape_val=0\n    while shape_val<100:\n        query_embedding_sent_df_subset=query_embedding_sent_df[query_embedding_sent_df['match']>val]\n        val=val-0.02\n        shape_val=query_embedding_sent_df_subset.shape[0]\n    original_sentences=query_embedding_sent_df_subset.value.values\n    sentence_vectors=query_embedding_sent_df_subset.Embedding.values\n    cord_uid=query_embedding_sent_df_subset.cord_uid.values\n    #print('Total Sentence := ' + str (sentence_vectors.shape[0]))\n    sentence_vectors_all=[sentence_vectors[k][0] for k in range(sentence_vectors.shape[0])]\n    sim_matrix = cosine_similarity(np.array(sentence_vectors_all))\n    #print(np.mean(sim_matrix))\n    sim_matrix_thresh = np.where(sim_matrix > np.mean(sim_matrix), sim_matrix, 0)\n    return sim_matrix_thresh,original_sentences,cord_uid\n\n\ndef print_diffmethod(function,name,original_sentences,cord_uid):\n#     print(str(name))\n    page_rank_result = pd.DataFrame({'sentence_index':list(function.keys()), 'score':list(function.values()), \n                                    'original_sentence':original_sentences,'cord_uid':cord_uid})\n    page_rank_result['key']=page_rank_result.original_sentence.str.replace(\" \",\"\")\n    page_rank_result=page_rank_result.drop_duplicates(['key'])\n#     page_rank_result.nlargest(10, 'score')\n#     for s in page_rank_result.nlargest(7, 'score')['original_sentence']:\n#         pprint.pprint(s)\n#         pprint.pprint('-------------------------------------------------------------------------------------')\n#     pprint.pprint(\"-\"*40)\n    return page_rank_result[['score','original_sentence','cord_uid']]\n\n","5f7275f9":"query_text=['What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?',\n'Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.',\n'Prevalence of asymptomatic shedding and transmission',\n'Seasonality of transmission of covid corona virus.',\n'Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding)',\n'Disease models, including animal models for infection, disease and transmission',\n'Immune response and immunity',\n'Role of the environment in transmission',\n'Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings',\n'Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings',\n'Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).']\nimport pickle\nquery_embedding=model.encode(query_text,show_progress_bar=False)\nwith open('\/kaggle\/working\/query_embedding_sent.pickle', 'wb') as handle:\n    pickle.dump(query_embedding, handle)\nquery_embedding[0].shape","6558bf63":"# query=pd.read_pickle('\/kaggle\/input\/covid19-task3\/query_embedding_sent.pickle')\n# query_embedding_sent_df=pd.read_pickle('\/kaggle\/input\/covid19-task3\/embeddings37912.pickle')\n# Comp_reserch_data=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\nquery=query_embedding\nquery_embedding_sent_df=new_data_sent_strip.copy()\nComp_reserch_data=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","685c7901":"from IPython.display import display, HTML\n# print_diffmethod(nx.degree_centrality(G),'degree_centrality')\n# print_diffmethod(nx.betweenness_centrality(G),'betweenness_centrality')\n# print_diffmethod(nx.eigenvector_centrality(G),'eigenvector_centrality')\n\nfor query_id in range(len(query_text)):\n    sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query,query_id=query_id,val=0.9)\n    G = nx.Graph(sim_matrix_thresh)\n    nx.pagerank(G)\n    #nx.draw_networkx(G)\n    t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n    all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n    display(HTML('<font size=\"5\" color=\"blue\"> <b> Query Searched : <\/b> <\/font><p> <font size=\"4\">'+query_text[query_id]+'<\/font><p>'))\n\n    #display(HTML(all_search.to_html()))\n    display(HTML(all_search.rename({'original_sentence':'Summary'},axis=1).style.set_properties(subset=['Summary'], \\\n                                             **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                             [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'skyblue'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n\n    display(HTML(\"-------End-----\"*15))","8b25a4dd":"dict_val={'Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.':[\t\t'What is known about incubation period of the virus',\n\t\t'What is known about incubation period humans of the virus',\n\t\t'What is known about incubation period range of the virus',\n\t\t'What is known about incubation period human range of the virus'],\n          'Prevalence of asymptomatic shedding and transmission':[\t\t'What is known about transmission range of the virus',\n\t\t'What is known about transmission humans of the virus',\n\t\t'What is known about transmission children of the virus'],\n          'Seasonality of transmission of covid corona virus':[\t\t'What is known about transmission seasonality of the virus'],\n          'Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding)':\n          [\t\t'What is known about coronavirus survival of the virus'],\n          'Disease models, including animal models for infection, disease and transmission':[\t\t'What is known about animal model of infection of the virus'],\n          'Immune response and immunity':[\t\t'What is known about immunity humans of the virus',\n\t\t'What is known about immune response  of the virus',\n\t\t'What is known about immunity by age of the virus'],\n          'Role of the environment in transmission':[\t\t'What is known about environment ideal for spread  of the virus',\n\t\t'What is known about coronavirus environment of the virus',\n\t\t'What is known about environmental transmission of the virus'],\n          'Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings':[\t\t'What is known about transmission PPE of the virus',\n\t\t'What is known about personal protective equipment transmission of the virus'],\n          'Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings':\n          [\t\t'What is known about movement control strategy of the virus'],\n'Disease models, including animal models for infection, disease and transmission':[\t\t'What is known about disease model of the virus'],\n          'Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).':[\t\t'What is known about surface persistence  of the virus',\n\t\t'What is known about material persistence  of the virus',\n\t\t'What is known about environmental survival  of the virus']}","9a3b8d04":"dict_val.keys()","c92a1aa1":"from IPython.display import display, HTML\n# print_diffmethod(nx.degree_centrality(G),'degree_centrality')\n# print_diffmethod(nx.betweenness_centrality(G),'betweenness_centrality')\n# print_diffmethod(nx.eigenvector_centrality(G),'eigenvector_centrality')\n\nfor query_id_list in dict_val.keys():\n    display(HTML('<font size=\"6\" color=\"black\"> <b> Subtask Searched : <\/b> <\/font><p> <font size=\"4\">'+query_id_list+'<\/font><p>'))\n    query_i_embed=model.encode([query_id_list],show_progress_bar=False)\n    sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query_i_embed,query_id=0,val=0.9)\n    G = nx.Graph(sim_matrix_thresh)\n    nx.pagerank(G)\n    #nx.draw_networkx(G)\n    t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n    all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n    \n    display(HTML('<font size=\"3\" color=\"green\"> <b> Subtask search Results<\/b> <\/font><p> <font size=\"4\">'+'<\/font><p>'))\n\n    #display(HTML(all_search.to_html()))\n    display(HTML(all_search.rename({'original_sentence':'Summary'},axis=1).style.set_properties(subset=['Summary'], \\\n                                             **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                             [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'skyblue'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n    display(HTML(\"-------End-----\"*15))\n    for query_i_text in dict_val[query_id_list]:\n        \n        query_i_embed=model.encode([query_i_text],show_progress_bar=False)\n        \n        sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query_i_embed,query_id=0,val=0.9)\n        G = nx.Graph(sim_matrix_thresh)\n        nx.pagerank(G)\n        #nx.draw_networkx(G)\n        t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n        all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n        display(HTML('<font size=\"6\" color=\"black\"> <b> Subtask Searched : <\/b> <\/font><p> <font size=\"3\">'+query_id_list+'<\/font><p>'))\n        display(HTML('<font size=\"4\" color=\"blue\"> <b> Specific question subtask Query Searched : <\/b> <\/font><p> <font size=\"4\">'+query_i_text+'<\/font><p>'))\n\n        #display(HTML(all_search.to_html()))\n        display(HTML(all_search.rename({'original_sentence':'Summary'},axis=1).style.set_properties(subset=['Summary'], \\\n                                             **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                             [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'pink'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n\n        display(HTML(\"-------End-----\"*15))","a57b1036":"# Working using sentence transformer for speed and polling selection","abd63966":"# Data cleaning","2387f90b":"# Import package and change code to run on GPU\n\n\n","39805f2c":"# Testing Speed","b1777193":"# Code for Search and page rank","54fcd38a":"# Subtast queries result","6c583910":"# embedding function for Biobert on CPU and GPU","ba34f6ca":"## Create embedding for sub task","635b39c3":"# Download BioBert","a858fa64":"# Testing BiobertEmbedding","5a1b20ad":"# Read papers Selected for Task3 ","4c126e39":"# Testing SentenceTransformer","d985b918":"# Sentence transformer","186d3d5a":"# Sentence Search\n** Find best sentence for query**","3f91d3d6":"## Installing and testing sentence embedding extraction modules. Testing For GPU and setting up device. \n### Using Pytorch based Bio-Bert download via biobert-embedding\n\n### **Testing below package to generate embedding** \n[Biobert Reference](https:\/\/github.com\/Overfitter\/biobert_embedding)\n**This package main code is modified to run it on GPU**\n\n[sentence-transformers](https:\/\/github.com\/UKPLab\/sentence-transformers)\n\n#### Reserch paper for the search is selected based on Clustering Via Citation and other networks \n\n[Links to be added](http:\/\/)\n\n#### *This will select best sentence via Network X graph. Currently using Pagerank method* Other methods can also me used like degree_centrality betweenness_centrality eigenvector_centrality","192f3aea":"## Generate embedding for each line\n#### Automatic use parallel processing if GPU is unavailable","239540fb":"# Check GPU"}}