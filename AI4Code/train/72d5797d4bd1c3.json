{"cell_type":{"e30e14eb":"code","f44449df":"code","1b8450c2":"code","f729508d":"code","6b76db12":"code","13b722aa":"code","2ad141a8":"code","2bc12030":"code","32c06bb8":"code","f69660f0":"code","d89a23c4":"code","65f5ac3e":"code","7cb5a357":"code","ca69013b":"code","ba16becb":"code","322b4141":"code","49d7e253":"code","3d77a7c0":"code","837071e6":"code","08786768":"code","d15dcf8c":"markdown","e961876c":"markdown","57a241db":"markdown","87546230":"markdown","13b9686f":"markdown","820b38fc":"markdown","61a00feb":"markdown","6f6775ea":"markdown","73697e8d":"markdown","eee528b2":"markdown","5f6bcb8b":"markdown","af23a078":"markdown","2f0ab956":"markdown"},"source":{"e30e14eb":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport numpy as np \nimport pandas as pd\nimport os\nimport librosa\nimport librosa.display\n\nimport IPython.display as ipd\n\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"..\/input\"))\n\n\n\nimport torchaudio\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils import data\nfrom torchvision import datasets, models, transforms\nimport torch.optim as optim\n\ntrain_on_gpu=torch.cuda.is_available()\n\n# Any results you write to the current directory are saved as output.","f44449df":"import zipfile\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"..\/input\/\"+\"train_curated\"+\".zip\",\"r\") as z:\n    z.extractall(\".\")\n    \n    \n\n","1b8450c2":"\nLabels = pd.read_csv(\"..\/input\/train_curated.csv\")\nLabels.head()\nWavPath = '\/kaggle\/working\/'\nFils = os.listdir(WavPath)\nsound, sample_rate = torchaudio.load(WavPath+Fils[2])\nipd.Audio(data=sound[0,:],rate=sample_rate) # load a local WAV file\n","f729508d":"x, sr = librosa.load(WavPath+Fils[2])\n\nplt.figure(figsize=(14, 5))\n\n\n\nlibrosa.display.waveplot(x, sr=sr)\nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(14, 5))\n#librosa.display.specshow(Xdb, sr=sample_rate, x_axis='time', y_axis='hz')\nXdb.shape\n\nS = librosa.feature.melspectrogram(x, sr=sample_rate, n_mels=128)\nlog_S = librosa.power_to_db(S, ref=np.max)\nMFCC = librosa.feature.mfcc(S=log_S, n_mfcc=23)\ndelta2_mfcc = librosa.feature.delta(MFCC, order=2)\n\n#MFCC = librosa.feature.mfcc(y=x, sr=sample_rate,n_mfcc=23,dct_type=2)\nlibrosa.display.specshow(log_S)\n#print(np.max(MFCC),np.min(MFCC))\n#MFCC = (MFCC+200)\/500\n#print(np.max(MFCC),np.min(MFCC))\nplt.colorbar()\nplt.tight_layout()","6b76db12":"FilesS = np.zeros(len(Fils))\nfor i,File in enumerate(Fils):\n    FilesS[i] = os.path.getsize(WavPath+File)\n\nplt.figure(figsize=(20,8))\nplt.hist(FilesS,bins=50)","13b722aa":"Fils_2 = Labels['fname']\nFils_2\n\nClass =set(Labels['labels'])\nAll_class= [] \nfor i in Class:\n    for j  in i.split(','):\n        All_class.append(j)\n\nAll_class = set(All_class)\n\nNumClasses = len(All_class)\nOneHot_All = np.zeros((len(Fils_2),NumClasses))\n\nfor  i,file in enumerate(Labels['labels']):\n    for j,clas in enumerate(All_class):\n        OneHot_All[i,j] = np.int(clas in file)\n\n","2ad141a8":"np.mean(log_S\/10+4)","2bc12030":"# Encode classes\n#ClassDict = dict(enumerate(set(Labels['labels'])))\n#Class2int = {ch: ii for ii, ch in ClassDict.items()}\n#encoded = np.array([Class2int[ch] for ch in Labels['labels']])\n\n#NumClasses = len(Class2int) \nprint(NumClasses)\n## split data into training, validation, and test data (features and labels, x and y)\nsplit_frac = 0.79\nbatch_size = 32\n\nsplit_idx = int(len(Fils)*split_frac)\nsplit_idx1 = int(batch_size*np.floor(split_idx\/batch_size))\nsplit_idx2 = int(batch_size*np.floor( (len(Fils) - split_idx1)\/batch_size ))\ntrain_x, val_x = Fils_2[:split_idx1], Fils_2[split_idx1:split_idx1+split_idx2]\ntrain_y, val_y = OneHot_All[:split_idx1,:], OneHot_All[split_idx1:split_idx1+split_idx2,:]\nprint(len(train_x)\/batch_size, len(val_x)\/batch_size )\n\nfrom sklearn.model_selection import train_test_split\ntrain_x, val_x, train_y, val_y = train_test_split( Fils_2, OneHot_All, test_size=1-split_frac, random_state=42)\nprint(train_x.shape,val_x.shape,train_y.shape,val_y.shape)\n","32c06bb8":"\nfrom scipy.io import wavfile\nfrom librosa.feature import mfcc\nclass Dataset(data.Dataset):\n    def __init__(self, list_IDs, labels,DataPath,RecLen,DecNum=5,fft_Samp= 256,Im_3D= False):\n        'Initialization'\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.DataPath = DataPath\n        self.RecLen = RecLen # length of most records\n        self.fft_Samp = fft_Samp \n        self.Im_3D = Im_3D\n        \n        self.NFCC_Num = 128\n        self.TimeSamp = 128\n    def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.list_IDs)\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        ID = self.list_IDs[index]\n\n        #y, sr = librosa.load(self.DataPath + ID)\n        data,fs =  librosa.load(self.DataPath + ID)\n        data = np.float32(data)\n        S = librosa.feature.melspectrogram(data, sr=sample_rate, n_mels=128)\n        Mel = librosa.power_to_db(S, ref=np.max)\/10+4\n        LabelOut = torch.from_numpy(self.labels[ID]).double()\n        \n        \n        Im = torch.zeros((self.NFCC_Num,self.TimeSamp)).type(torch.FloatTensor)\n        Ssum = np.sum(Mel,axis=0)\n        MaxE = np.argmax(Ssum)\n        if MaxE > Mel.shape[1]-64 : \n            MaxE = Mel.shape[1]-65\n        if MaxE< 64 :\n            MaxE = 64\n        if Mel.shape[1] > self.TimeSamp :\n            Im = torch.from_numpy(Mel[:,MaxE-64:MaxE+64])\n        else: \n            Im[:,:Mel.shape[1]  ] = torch.from_numpy(Mel)\n        \n        \n\n        Im = Im.double()\n        return Im, LabelOut,ID","f69660f0":"class CnnAudioNet(nn.Module):\n    def __init__(self,NumClasses):\n        super(CnnAudioNet,self).__init__()\n        self.NumClasses = NumClasses\n        self.Fc_features = 128\n        self.C1 = nn.Conv2d(1,32,5,padding=1)\n        self.C2 = nn.Conv2d(32,32,5,padding=1)\n        self.C3 = nn.Conv2d(32,64,5,padding=1)\n        self.C4 = nn.Conv2d(64,64,5,padding=1)\n        \n        self.BN1 = nn.BatchNorm2d(32)\n        self.BN2 = nn.BatchNorm2d(64)\n        self.BN3 = nn.BatchNorm2d(64)\n        self.maxpool1 = nn.MaxPool2d(2,2)\n        self.maxpool2 = nn.MaxPool2d((1,2),(1,2))\n        \n        \n        self.fc1 = nn.Linear(64*8*8,128)\n        self.fc2 = nn.Linear(128,self.NumClasses )\n        self.dropout = nn.Dropout(0.25)\n        self.Bat1 = nn.BatchNorm1d(128)\n\n        \n        \n    def forward(self,x):\n        # add sequence of convolutional and max pooling layers\n        x = F.relu(self.BN1(self.C1(x)))\n        x = self.maxpool1(F.relu(self.BN1(self.C2(x))))\n        x = F.relu(self.BN2(self.C3(x)))\n        x = self.maxpool1(F.relu(self.BN2(self.C4(x))))\n        x = F.relu(self.BN2(self.C4(x)))\n        x = self.maxpool1(F.relu(self.BN2(self.C4(x))))\n        x = F.relu(self.BN2(self.C4(x)))\n        x = F.relu(self.BN3(self.C4(x)))\n        # flatten image input\n        x = self.dropout(x.view(-1,64*8*8))\n        # add dropout layer\n        x =  self.dropout(self.fc1(x))\n        # add 1st hidden layer, with relu activation function\n        # add dropout layer\n        # add 2nd hidden layer, with relu activation function\n        #x = torch.sigmoid(self.fc2(x))\n        x = self.fc2(x)\n        return x\n        ","d89a23c4":"from torchvision import datasets, models, transforms\n\n\n# Freeze training for all layers\n\n\nclass CnnTransferNet(nn.Module):\n    def __init__(self):\n        super(CnnTransferNet,self).__init__()\n        \n        self.vgg =  models.vgg16_bn().cuda()\n        for param in self.vgg.features.parameters():\n            param.require_grad = False\n\n        \n        self.fc1 = nn.Linear(1000,128)\n        self.fc2 = nn.Linear(128,NumClasses)\n        self.dropout = nn.Dropout(0.25)\n\n        \n        \n    def forward(self,x):\n        # add sequence of convolutional and max pooling layers\n        Features = self.dropout(self.vgg(x))\n        # flatten image input\n        # add 1st hidden layer, with relu activation function\n        Features = F.relu(self.fc1(Features))\n        # add dropout layer\n        # add 2nd hidden layer, with relu activation function\n        Features = self.fc2(Features)\n        return Features","65f5ac3e":"model = CnnAudioNet(NumClasses)\nif train_on_gpu:\n    model.cuda()\nprint(model)\n# specify loss function (MSE)\n\n#criterion = nn.MSELoss()\n#criterion = nn.BCELoss()\ncriterion = nn.BCEWithLogitsLoss()\n#criterion = nn.MultiLabelSoftMarginLoss()\n\noptimizer = optim.Adam(params=model.parameters(), lr=0.001)# specify optimizer\n#optimizer = optim.Adam(model.parameters(), lr=0.005)\n\n\na = train_x.tolist()\n","7cb5a357":"#abelsDict = dict(zip(Fils,one_hot))\nlabelsDict_train = dict(zip(train_x,train_y))\nlabelsDict_val = dict(zip(val_x,val_y))\n\nparams = {'batch_size': batch_size,\n          'shuffle': True,\n          'num_workers': 9}\nparams_v = {'batch_size': batch_size,\n          'shuffle': False,\n          'num_workers': 3}\nRecLen = 176400\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\ntraining_set = Dataset(train_x.tolist(), labelsDict_train,WavPath,RecLen,transforms.Compose(normalize))\ntraining_generator = data.DataLoader(training_set, **params)\n\nval_set = Dataset(val_x.tolist(),labelsDict_val,WavPath,RecLen,transforms.Compose(normalize))\nval_generator = data.DataLoader(val_set, **params_v)\n\n","ca69013b":"\nimport time\nstart_time = time.time()\n#Warnings.filterwarnings('ignore')\n\n# number of epochs to train the model\nn_epochs = 1\n\nvalid_loss_min = np.Inf # track change in validation loss\nprint(\"Start training:\")\nidx = 0 \nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    TotMSE = 0 \n    TotEl = 0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n\n    for dataBatch, target,_ in training_generator:\n        \n        idx+=1\n\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            dataBatch, target = dataBatch.unsqueeze(1).float().cuda(), target.cuda()\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(dataBatch)\n        # calculate the batch loss\n        #loss = criterion(output, torch.squeeze(torch.argmax(target,dim=-1)))\n        loss = criterion(output,target.float())\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*dataBatch.size(0)\n        #print(loss.item())\n        #print('Finish batch')\n        _,pred = torch.max(output,1)\n        \n        #Correct = torch.sum(torch.pow(output-target.float(),2))#\n        ErrorS = torch.sum(torch.pow(torch.sigmoid(output)-target.float(),2))#\n        TotMSE += ErrorS\n        TotEl += output.numel()\n        Correct =torch.sum(pred ==torch.squeeze(torch.argmax(target,dim=-1)))\n        #print('Train batch loss: {:.6f},  Error: {:.4f},  Sum Correct: {} out of {}'.format(loss,ErrorS,Correct,output.shape[0]))\n    print('Epoch: {} \\t  Train batch loss: {:.6f} '.format(epoch,loss))\n\n        \n    ######################    \n    # validate the model #\n    ######################\n    with torch.no_grad():\n        model.eval()\n        TotEl_v = 0\n        valid_loss = 0 \n        TotMSE_v = 0\n        for dataBatch_v, target ,_ in val_generator  :\n\n        # move tensors to GPU if CUDA is available\n            if train_on_gpu:\n                dataBatch_v, target = dataBatch_v.unsqueeze(1).float().cuda(),target.cuda()\n        # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(dataBatch_v)\n        # calculate the batch loss\n            loss = criterion(output,target.float())\n\n            #loss = criterion(output, torch.squeeze(torch.argmax(target,dim=-1)))\n        # update average validation loss \n            output.shape\n            _,pred = torch.max(output,1)\n            Correct = torch.sum(pred ==torch.squeeze(torch.argmax(target,dim=-1)))\n            #SumCorrectVal += Correct\n            valid_loss += loss.item()*dataBatch.size(0)\n            #print(TotVal)\n\n            ErrorS = torch.sum(torch.pow(torch.sigmoid(output)-target.float(),2))#\n            TotMSE_v += ErrorS\n            TotEl_v += output.numel()\n        # calculate average losses\n        train_lossM = train_loss\/len(training_generator.dataset)\n        valid_lossM = valid_loss\/len(val_generator.dataset)\n        MSE = TotMSE\/TotEl\n        MSE_V = TotMSE_v\/TotEl_v\n\n        # print training\/validation statistics \n        print('Epoch: {} \\t Training Loss: {:.6f}, Train MSE: {:.4f} \\tValidation Loss: {:.6f},  Val MSE: {:.4f} '.format(\n            epoch, train_lossM,MSE, valid_lossM,MSE_V))\n        print(\"--- %s seconds ---\" % (time.time() - start_time))","ba16becb":"# data,target ,_= next(iter(val_generator))\n# data = data.unsqueeze(1).float().cuda()\n# output = model(dataBatch_v)\n","322b4141":"# plt.figure(figsize=(20,20))\n# for i in range(16):\n#     plt.subplot(4,4,i+1)\n#     plt.plot(target[i,:].detach().cpu().numpy())\n#     plt.plot(torch.sigmoid(output[i,:]).detach().cpu().numpy())\nfrom glob import glob\nF1 = glob('.\/*wav*')\nfor file in F1:\n    os.remove(file)","49d7e253":"# # Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"..\/input\/\"+\"test\"+\".zip\",\"r\") as z:\n    z.extractall(\".\/test\/\")","3d77a7c0":"WavPath_test =  '.\/test\/'\n\nFils_test = os.listdir(WavPath_test)\n\n\none_hot_test = np.zeros((len(Fils_test),NumClasses))\n\nlabelsDict = dict(zip(Fils_test,one_hot_test))\n\nparams = {'batch_size': 4,\n          'shuffle': True,\n          'num_workers': 4}\n\ntest_set = Dataset(Fils_test, labelsDict,WavPath_test,RecLen,transforms.Compose(normalize))\ntest_generator = data.DataLoader(test_set, **params)","837071e6":"model.eval()\nSoftM = torch.nn.Softmax()\nOutput_all = [] \nBatchRecs_all = [] \nwith torch.no_grad():                   # operations inside don't track history\n\n    for dataBatch, Lab,BatchRecs in test_generator:\n        if train_on_gpu:\n            dataBatch, Lab = dataBatch.unsqueeze(1).float().cuda(), Lab.cuda()\n        output = model(dataBatch)\n        outP = torch.sigmoid(output)\n        #outP = output\n        Output_all.append(outP)\n        BatchRecs_all.append(BatchRecs)\n\n  ","08786768":"Dataout = np.zeros((4*len(Output_all)-3,80))\nNames = []\nfor i in range(len(Output_all)):\n    Dataout[i*4:(i+1)*4,:] = Output_all[i].cpu().detach().numpy()\n    Names.append(BatchRecs_all[i][0])\n    if i<840:\n        Names.append(BatchRecs_all[i][1])\n        Names.append(BatchRecs_all[i][2])\n        Names.append(BatchRecs_all[i][3])\n    \nCl = list(All_class)\n#Cl.append('fname')\n\nOutput_all_DF =pd.DataFrame(columns=Cl,data = Dataout)\nOutput_all_DF['fname'] = Names\nOutput_all_DF.to_csv('submission.csv', index=False)\nOutput_all_DF.head()\n","d15dcf8c":"14. Test prediction","e961876c":"10. Create Dataset:","57a241db":"15. Submission:","87546230":"6. Data set pytorch loader:","13b9686f":"13. Test data loader:","820b38fc":"\nThis notebook convert sound to 2D CNN images, train using Pytorch and predict test set.\nI will make it more orgnize, main structure is:\n\n1. Load and listen to Recs\n2. Create spectrum image\n3. Histogram of Recs lengthes\n4. Labeling the data using one( or more then one) hot encoded:\n5. Train and validetion split\n6. Data set orgenize pytorch loader\n7. Design CNN neural net\n8. Transfer learning VGG net (currently not in use)\n9. Init model\n10. Create Dataset\n11. Learning\n12. Test data loader\n13. Test prediction\n14. Submission","61a00feb":"8. Transfer learning VGG net (currently not in use)","6f6775ea":" 5. Train and validetion split","73697e8d":"11. Learning:","eee528b2":"7. Design CNN neural net","5f6bcb8b":"9. Init model:","af23a078":"4. Labeling the data using one( or more then one) hot encoded:","2f0ab956":"2. Create spectrum image:"}}