{"cell_type":{"964b6e71":"code","6bee54bd":"code","e9431f1d":"code","7a06ac25":"code","4aad6cf4":"code","86c1524b":"code","2aed99f0":"code","f72080ba":"code","2a6579b9":"code","41749053":"code","8cfe4fb3":"code","3b674909":"markdown","3dcfb724":"markdown","db0e0bc6":"markdown","4a8f48c2":"markdown","18a37337":"markdown","8ec0fd6a":"markdown"},"source":{"964b6e71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n","6bee54bd":"data_dir =\"..\/input\/car_data\/car_data\"\ntrain_dir= data_dir +'\/train'\ntest_dir = data_dir +'\/test'\n\n# number of subprocesses to use for data loading\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 20\n# percentage of training set to use as validation\nvalid_size = 0.2\n\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\ntest_transforms = transforms.Compose([transforms.Resize(256),\n                                       transforms.CenterCrop(224),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                         [0.229, 0.224, 0.225])])\ntrain_data = datasets.ImageFolder(train_dir, transform=train_transforms)\ntest_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n\n# obtain training indices that will be used for validation\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]                                                            \n                                                            \n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders (combine dataset and sampler)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n    sampler=valid_sampler, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n    num_workers=num_workers)\n\nprint(\"Training data: {}\".format(len(train_data)))\nprint(\"Test data: {}\".format(len(test_data)))","e9431f1d":"# helper function to un-normalize and display an image\ndef imshow(img):\n    img = img \/ 2 + 0.5  # unnormalize\n    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image","7a06ac25":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\nprint(\"labels:\",labels)\n# display 20 images\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])","4aad6cf4":"#check if GPU  is available to use\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","86c1524b":"#defining model\nmodel = models.densenet121(pretrained=True)\nprint(model)","2aed99f0":"\n# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n    \nmodel.classifier = nn.Sequential(nn.Linear(1024, 512),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.2),\n                                 nn.Linear(512, 196),\n                                 nn.LogSoftmax(dim=1))\n\ncriterion = nn.NLLLoss()\n\n# Only train the classifier parameters, feature parameters are frozen\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)\n\nmodel.to(device)","f72080ba":"n_epochs = 30\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nfor epoch in range(1, n_epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        if train_on_gpu:\n            data, target = data.cuda(), target.cuda()\n        output = model(data)\n        loss = criterion(output, target)\n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss\/len(train_loader.sampler)\n    valid_loss = valid_loss\/len(valid_loader.sampler)\n \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_car.pt')\n        valid_loss_min = valid_loss","2a6579b9":"model.load_state_dict(torch.load('model_car.pt'))","41749053":"data=pd.read_csv(\"..\/input\/names.csv\")\ndata.head()","8cfe4fb3":"prediction=[]\n# track test loss\ntest_loss = 0.0\naccuracy = 0\nmodel.eval()\nfor data, target in test_loader:\n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        data, target = data.cuda(), target.cuda()\n    output = model(data)\n    loss = criterion(output, target)\n    # update test loss \n    test_loss += loss.item()*data.size(0)\n    ps = torch.exp(output)\n    top_p, top_class = ps.topk(1, dim=1)\n    equals = top_class == target.view(*top_class.shape)\n    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n# average test loss\ntest_loss = test_loss\/len(test_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))","3b674909":"**Using pretrained: Densenet121 model**","3dcfb724":"> Testing model and calculating accuracy","db0e0bc6":"Train the network","4a8f48c2":"Step0: Import dependencies","18a37337":"Step 1:  \na)Transform the data and normalize it   \nb) Load training and test dataset","8ec0fd6a":"Approach:\nI used pretrained modeL: densenet121()\nThe main reason I used it because I have previous exposure with this trained model\noptimizer: Adam optimizer because it works quite well. \nlearning rate:0.0001\ncriterion: NLLLoss()"}}