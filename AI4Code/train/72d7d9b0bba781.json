{"cell_type":{"fd614591":"code","86026732":"code","dd314aab":"code","e52385e9":"code","31507962":"code","fc7fcec4":"code","8bee72b1":"code","29df673d":"code","cf115c63":"code","3b23bd69":"code","1bda3a0c":"code","26adcc80":"code","9af4e0c7":"code","52d84317":"code","7269bacd":"code","f4b5dd04":"code","3bd71e7e":"code","8325899b":"code","d7c20abb":"code","7841f426":"code","1b2871a2":"code","fe6288fd":"code","a9054a7c":"markdown","6c68548c":"markdown","91b1ca5c":"markdown","194c0f6d":"markdown","f50e0e2f":"markdown","b352826a":"markdown","911809d7":"markdown","d895113b":"markdown","6a11459c":"markdown","a5e0ab89":"markdown","6da75485":"markdown","bf18a1c1":"markdown"},"source":{"fd614591":"!pip install transformers","86026732":"import requests \nimport pandas as pd\nfrom itertools import compress\n\n# This will return the data for all the cards available to scryfall.\nr = requests.get('https:\/\/c2.scryfall.com\/file\/scryfall-bulk\/all-cards\/all-cards-20200831091816.json')\ndata = r.json()","dd314aab":"# we'll start parsing by removing any cards with no flavor text\ncontains_x = []\nfor i in data:\n    contains_x.append('flavor_text' in i.keys())\n\ndata_filtered = list(compress(data, contains_x))\n\n# next we'll remove any cards in a language other than english\ncontains_y = []\nfor i in data_filtered:\n    contains_y.append('lang' in i.keys() and 'en' == i['lang'])\n\ndata_filtered = list(compress(data_filtered, contains_y))\n\n# Now we'll create a list to iterate through.\ncardValues = []\nfor i in data_filtered:\n    cardValues.append(i['flavor_text'])\n\n# I'll convert this to a data frame to visualize a few rows nicely\n# mostly just a sanity check.\ndf = pd.DataFrame(\n    cardValues,\n    columns=['data']\n    )\n\ncards = df.data.copy()\ncards.head()","e52385e9":"cards.shape","31507962":"from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2', \n                                          bos_token='<|startoftext|>', \n                                          eos_token='<|endoftext|>', \n                                          pad_token='<|pad|>')\n\n\ntokenizer.encode(\"sample text\")","fc7fcec4":"print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\nprint(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\nprint(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\nprint(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))","8bee72b1":"max_flavour = max([len(tokenizer.encode(card)) for card in cards])\n\nprint(f'The longest flavour text is {max_flavour} tokens long.')","29df673d":"!nvidia-smi","cf115c63":"bs = 32","3b23bd69":"import torch\ntorch.manual_seed(42)\nfrom torch.utils.data import Dataset # this is the pytorch class import\n\nclass MTGDataset(Dataset):\n    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=max_flavour):\n        self.tokenizer = tokenizer # the gpt2 tokenizer we instantiated\n        self.input_ids = []\n        self.attn_masks = []\n\n        for txt in txt_list:\n            \"\"\"\n            This loop will iterate through each entry in the flavour text corpus.\n            For each bit of text it will prepend it with the start of text token,\n            then append the end of text token and pad to the maximum length with the \n            pad token. \n            \"\"\"\n\n            encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', \n                                       truncation=True, \n                                       max_length=max_length, \n                                       padding=\"max_length\")\n      \n            \"\"\"\n            Each iteration then appends either the encoded tensor to a list,\n            or the attention mask for that encoding to a list. The attention mask is\n            binary list of 1's or 0's which determine whether the langauge model\n            should take that token into consideration or not. \n            \"\"\"\n            \n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n    \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx] ","1bda3a0c":"from torch.utils.data import random_split\n\ndataset = MTGDataset(cards, tokenizer, max_length=max_flavour)\n\n# Split into training and validation sets\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nf'There are {train_size} samples for training, and {val_size} samples for validation testing'","26adcc80":"dataset[0]","9af4e0c7":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ntrain_dataloader = DataLoader(\n            train_dataset,  \n            sampler = RandomSampler(train_dataset), # Sampling for training is random\n            batch_size = bs\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, \n            sampler = SequentialSampler(val_dataset), # Sampling for validation is sequential as the order doesn't matter.\n            batch_size = bs \n        )","52d84317":"import random\nfrom transformers import GPT2LMHeadModel, GPT2Config\nimport numpy as np\n\n# Loading the model configuration and setting it to the GPT2 standard settings.\nconfiguration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n\n# Create the instance of the model and set the token size embedding length\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Tell pytorch to run this model on the GPU.\ndevice = torch.device(\"cuda\")\nmodel.cuda()\n\n# This step is optional but will enable reproducible runs.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","7269bacd":"# We wil create a few variables to define the training parameters of the model\n# epochs are the training rounds\n# the warmup steps are steps at the start of training that are ignored\n# every x steps we will sample the model to test the output\n\nepochs = 4\nwarmup_steps = 1e2\nsample_every = 100","f4b5dd04":"from transformers import AdamW\n# AdamW is a class from the huggingface library, it is the optimizer we will be using, and \n# we will only be instantiating it with the default parameters. \n\noptimizer = AdamW(model.parameters(),\n                  lr = 5e-4,\n                  eps = 1e-8\n                )","3bd71e7e":"from transformers import get_linear_schedule_with_warmup\n\n\"\"\"\nTotal training steps is the number of data points, times the number of epochs. \nEssentially, epochs are training cycles, how many times each point will be seen by the model. \n\"\"\"\n\ntotal_steps = len(train_dataloader) * epochs\n\n\"\"\"\nWe can set a variable learning rate which will help scan larger areas of the \nproblem space at higher LR earlier, then fine tune to find the exact model minima \nat lower LR later in training.\n\"\"\"\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = warmup_steps, \n                                            num_training_steps = total_steps)","8325899b":"import random\nimport time\nimport datetime\n\ndef format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n\ntotal_t0 = time.time()\n\ntraining_stats = []\n\nmodel = model.to(device)\n\nfor epoch_i in range(0, epochs):\n\n    print(f'Beginning epoch {epoch_i + 1} of {epochs}')\n\n    t0 = time.time()\n\n    total_train_loss = 0\n\n    model.train()\n    \n    for step, batch in enumerate(train_dataloader):\n\n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n\n        model.zero_grad()        \n\n        outputs = model(  b_input_ids,\n                          labels=b_labels, \n                          attention_mask = b_masks,\n                          token_type_ids=None\n                        )\n\n        loss = outputs[0]  \n\n        batch_loss = loss.item()\n        total_train_loss += batch_loss\n        \n        # Get sample every 100 batches.\n        if step % sample_every == 0 and not step == 0:\n\n            elapsed = format_time(time.time() - t0)\n            print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')\n\n            model.eval()\n\n            sample_outputs = model.generate(\n                                    bos_token_id=random.randint(1,30000),\n                                    do_sample=True,   \n                                    top_k=50, \n                                    max_length = 200,\n                                    top_p=0.95, \n                                    num_return_sequences=1\n                                )\n            for i, sample_output in enumerate(sample_outputs):\n                  print(f'Example output: {tokenizer.decode(sample_output, skip_special_tokens=True)}')\n            \n            model.train()\n            \n        loss.backward()\n\n        optimizer.step()\n\n        scheduler.step()\n        \n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss \/ len(train_dataloader)       \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n\n    t0 = time.time()\n\n    model.eval()\n\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    \n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n        \n        with torch.no_grad():        \n\n            outputs  = model(b_input_ids,  \n                             attention_mask = b_masks,\n                             labels=b_labels)\n          \n            loss = outputs[0]  \n            \n        batch_loss = loss.item()\n        total_eval_loss += batch_loss\n    \n    avg_val_loss = total_eval_loss \/ len(validation_dataloader)\n    \n    validation_time = format_time(time.time() - t0)    \n\n    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(f'Total training took {format_time(time.time()-total_t0)}')","d7c20abb":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('precision', 2)\ndf_stats = pd.DataFrame(data=training_stats)\ndf_stats = df_stats.set_index('epoch')\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()","7841f426":"import os\n\noutput_dir = '.\/'\n\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\nmodel_to_save = model.module if hasattr(model, 'module') else model\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Good practice: save your training arguments together with the trained model\n# torch.save(args, os.path.join(output_dir, 'training_args.bin'))","1b2871a2":"model.eval()\n\nprompt = \"<|startoftext|>\"\n\ngenerated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\ngenerated = generated.to(device)\n\nsample_outputs = model.generate(\n                                generated, \n                                do_sample=True,   \n                                top_k=50, \n                                max_length = 300,\n                                top_p=0.95, \n                                num_return_sequences=3\n                                )\n\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","fe6288fd":"# model = GPT2LMHeadModel.from_pretrained(output_dir)\n# tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n# model.to(device)","a9054a7c":"# Generate Text","6c68548c":"# GPT2 Tokenizer","91b1ca5c":"# Saving & Loading Fine-Tuned Model\n\nNext it is a good idea to move the model we have trained and the associated weights, biases and model parameters out of the Colab space and into our own google drives. The cell below will mount your drive for you, then you can save the model using the prebuilt Hugging Face and PyTorch functionalities.","194c0f6d":"load the fine tuned GPT2 model and tokenizer","f50e0e2f":"# Create Training Set","b352826a":"# PyTorch Datasets & Dataloaders","911809d7":"AdamW is the optimizer of choice for training many models, we will be using Hugging Face's implementation and all of it's defaults, we will also set the number of epochs here, again as we are fine tuning, not retraining, we don't need to run very long models.","d895113b":"Finally to illustrate what an entry in this dataset looks like below is a print out of the first encoded string. You can see that for every encoded word the model pays attention to we have a 1, then for the padding encodings (50258) we have a 0.","6a11459c":"# Finetune GPT2 Language Model","a5e0ab89":"The next thing to do is to create a custom dataloader for our corpus, we will follow the PyTorch documentation on this to create MTGDataset.","6da75485":"First a brief description of tokenization straight from the source, the Hugging Face [Tokenizers](https:\/\/github.com\/huggingface\/tokenizers) github page:\n\n\nWe will be using the GPT-2 tokenizer to tokenize our flavor text data. The defaults of this function set the bos (beginning of sentence) eos (end of sentence) to '<|endoftext|>' but we can specifically set them differently to differentiate and also assign a non-default pad token that will take care of white space for differently sized text. The next cell will instantiate our tokenizer and provide an example encoding.","bf18a1c1":"From the example above we can see that the example string is encoded by the GPT2 tokenizer to a list of numerical values that represent the string, in this case one value per word. These values are easier to train the neural network model on than the string representation. We now have a corpus of flavour text we can iterate through, and a tokenizer, we should quickly inspect it to see what the longest string is, this will be useful later when we need to know how long to pad our sentences out to."}}