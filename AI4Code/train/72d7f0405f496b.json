{"cell_type":{"caf47c65":"code","0c6fc5a4":"code","93f4e48f":"code","148a906b":"code","c1fecb03":"code","4d0dc77a":"code","2cab2965":"code","5e89626d":"code","c18252b7":"code","95615e2b":"code","088456c2":"markdown","ad61074d":"markdown","658d8f54":"markdown"},"source":{"caf47c65":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ndf.head()","0c6fc5a4":"print(df.shape)\nprint(\"-----------------------\")\nprint(df['sentiment'].value_counts())","93f4e48f":"df.isnull().sum()","148a906b":"#map positive to 1 and negative to 0\ndf.sentiment = df.sentiment.apply(lambda x: 1 if x == 'positive' else 0)","c1fecb03":"#create a new column called kfold and fill it with -1\ndf['kfold'] = -1\n#randomize rows of the data\ndf = df.sample(frac = 1).reset_index(drop = True)\ny = df.sentiment.values #labels","4d0dc77a":"#clean text\nimport re\nimport string\nfrom nltk.corpus import stopwords\nimport xgboost as xgb\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\ndef clean_text(text):\n    #lowercase every letter\n    text = text.split() #split by all white spaces\n    \n    #join tokens by single space, this will remove all kinds of weird spaces\n    text = \" \".join(text)\n    #removes all punctuation using regex and string module\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n    \n    \n    return text\n\ndf.loc[:,'review'] = df.review.apply(clean_text)","2cab2965":"df.head()","5e89626d":"kf = model_selection.StratifiedKFold(n_splits = 5)\n\nfor feature, (train, validation) in enumerate(kf.split(X = df, y=y)):\n    df.loc[validation, 'kfold'] = feature\n    ","c18252b7":"from sklearn.feature_extraction.text import TfidfVectorizer\nfor fold_ in range(5):\n        #temporary dataframes for train and test\n        train_df = df[df.kfold != fold_].reset_index(drop = True)\n        test_df = df[df.kfold == fold_].reset_index(drop = True)\n        \n        #initialize TF-IDF vectorizer\n        vec = TfidfVectorizer(tokenizer = word_tokenize, token_pattern = None)\n        \n        #fit the count on trainig data review\n        vec.fit(train_df.review)\n        #transform\n        X_train = vec.transform(train_df.review)\n        X_test = vec.transform(test_df.review)\n        \n        #initialize XGBClassifier \n        ##Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n#Beware that XGBoost aggressively consumes memory when training a deep tree.\n        model = xgb.XGBClassifier(max_depth = 8, eta = 0.7, objective= 'binary:logistic', n_estimators = 200, \n                                  use_label_encoder=False, eval_metric = 'auc')\n        #fit the model\n        model.fit(X_train, train_df.sentiment)\n        preds = model.predict(X_test)\n        \n        #calculate accuracy\n        accuracy = metrics.accuracy_score(test_df.sentiment, preds)\n        \n        print(f\"Fold: {fold_}\")\n        print(f\"Accuracy: {accuracy}\")\n        print(\"\")","95615e2b":"print(f\"The mean accuracy is: {accuracy.mean()}\")","088456c2":"86.2% accuracy!","ad61074d":"# This notebook is written as simple as possible to target mainly beginners in NLP.\n# I am a firm believer of simplicity and low code works, I have explained my codes here, feel free to ask if you don't understand something, happy to help. Explanations are inside comments, so don't overlook them.\n# This notebook uses XGBoost for the classification with Stratified k fold cross validation.","658d8f54":"# Accuracy can be increased by setting the eta to a much lower value and then increasing estimators, 90% accuracy can easily be achieved if you are willing to wait, because with only 100 estimators it took 1 hour to train.\n## So feel free to play with the parameters.\n## Upvote if you like it or fork it, this gives us motivation to produce more notebooks for the community."}}