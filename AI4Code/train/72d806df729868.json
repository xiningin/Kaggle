{"cell_type":{"d9582b85":"code","43f62444":"code","c164bc01":"code","e3b31458":"code","9ab0e78b":"code","ecd23dad":"code","101fcce3":"code","7f7e9b1f":"code","76ea7002":"code","4df02a3e":"code","b19a764d":"code","eb8e7d36":"code","7faeb448":"code","9855332a":"code","8979a950":"code","82ee35db":"code","361b148f":"code","f1477628":"code","6d8bba47":"code","070831c6":"code","a04262ca":"code","f964eb75":"code","81bb8d0c":"code","d325a5d2":"code","d0598782":"code","40ad621a":"code","c68481d0":"code","1b25bc5b":"code","87e0e36f":"code","bd176168":"code","ed83f89c":"code","f3aa7959":"code","6b33268a":"code","ff683a02":"code","b9fba5eb":"code","c20aba1b":"code","7bc3e1ac":"code","67d2f88b":"code","c7236aa7":"code","603b116b":"code","ad3a04db":"code","aaaea874":"code","eadc20fa":"code","a69a7f1c":"code","8fc0210f":"code","e5648006":"code","b42ad3ce":"code","ea8af111":"code","2bfc4841":"code","91db5c5e":"code","965bf3f7":"code","55c9560a":"code","b0e2604b":"code","cb415c9a":"code","da5d503c":"code","288b8979":"code","8cdd2e9f":"code","49c18f2c":"code","f37a56f1":"code","d54db3f7":"code","9ae5aae1":"code","9186c631":"code","4854fa76":"code","762bf2e4":"code","f29e4da3":"code","2648ec3a":"code","798b8321":"code","25b83ee5":"code","ac15da3f":"code","385cd092":"code","ed10cf84":"code","12636873":"code","0290025e":"code","4a854089":"code","db491994":"code","e9432a29":"code","f34c3ba4":"code","e72036cb":"code","d7c3bfc3":"code","aa5a3f50":"code","73ad093b":"code","c9541677":"code","c52c3b44":"code","cce274e5":"code","62ac453e":"code","909ce951":"code","208433ca":"code","69ba8441":"code","6ffd1b9d":"code","a975d29a":"code","7c9f8b0d":"code","7fe00822":"code","a723dd98":"code","d8b3605e":"code","eed028bf":"code","883ebc60":"code","87517674":"code","f1dc49cb":"code","23896040":"code","34f7ddef":"code","362a0e56":"code","292f13c4":"code","cd914345":"code","77e5ced5":"markdown","ae1d1ccb":"markdown","dd53ebed":"markdown","5eb225d9":"markdown","9cf14921":"markdown","bc57c641":"markdown","ffe5db58":"markdown","cdaf6ffd":"markdown","d0404702":"markdown","674649d0":"markdown","a134c856":"markdown","ab3af49d":"markdown","9368c76b":"markdown","09c0932a":"markdown","e87d5cad":"markdown","4cd8dab3":"markdown","87fde9a9":"markdown","2e6dc8f2":"markdown","a9e4f751":"markdown","2d5327b1":"markdown","ea8728f4":"markdown","052b0569":"markdown"},"source":{"d9582b85":"#Importing fundamental libraries for data science\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","43f62444":"#Reading CSV file with Pandas Library\ndados = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","c164bc01":"dados.head()","e3b31458":"dados = dados.drop(['id','Unnamed: 32'],axis=1)","9ab0e78b":"dados.head()","ecd23dad":"num_colunas = dados.shape[1]","101fcce3":"sns.countplot(x='diagnosis',data=dados)\nplt.xlabel('Diagn\u00f3stico')\nplt.ylabel('Count')\nplt.title('Kind of diagnostic')","7f7e9b1f":"corr = dados.corr()\nf,ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr, annot=True, linewidths=.5, fmt= '.1f',ax=ax)","76ea7002":"colunas = dados.columns\nprint(\"Number of columns = {}\".format(len(colunas)))","4df02a3e":"fig,ax=plt.subplots(6,5,figsize=(12,15))\nsns.boxplot(y=dados[colunas[1]],x=dados['diagnosis'],ax=ax[0][0])\nsns.boxplot(y=dados[colunas[2]],x=dados['diagnosis'],ax=ax[0][1])\nsns.boxplot(y=dados[colunas[3]],x=dados['diagnosis'],ax=ax[0][2])\nsns.boxplot(y=dados[colunas[4]],x=dados['diagnosis'],ax=ax[0][3])\nsns.boxplot(y=dados[colunas[5]],x=dados['diagnosis'],ax=ax[0][4])\n\nsns.boxplot(y=dados[colunas[6]],x=dados['diagnosis'],ax=ax[1][0])\nsns.boxplot(y=dados[colunas[7]],x=dados['diagnosis'],ax=ax[1][1])\nsns.boxplot(y=dados[colunas[8]],x=dados['diagnosis'],ax=ax[1][2])\nsns.boxplot(y=dados[colunas[9]],x=dados['diagnosis'],ax=ax[1][3])\nsns.boxplot(y=dados[colunas[10]],x=dados['diagnosis'],ax=ax[1][4])\n\nsns.boxplot(y=dados[colunas[11]],x=dados['diagnosis'],ax=ax[2][0])\nsns.boxplot(y=dados[colunas[12]],x=dados['diagnosis'],ax=ax[2][1])\nsns.boxplot(y=dados[colunas[13]],x=dados['diagnosis'],ax=ax[2][2])\nsns.boxplot(y=dados[colunas[14]],x=dados['diagnosis'],ax=ax[2][3])\nsns.boxplot(y=dados[colunas[15]],x=dados['diagnosis'],ax=ax[2][4])\n\nsns.boxplot(y=dados[colunas[16]],x=dados['diagnosis'],ax=ax[3][0])\nsns.boxplot(y=dados[colunas[17]],x=dados['diagnosis'],ax=ax[3][1])\nsns.boxplot(y=dados[colunas[18]],x=dados['diagnosis'],ax=ax[3][2])\nsns.boxplot(y=dados[colunas[19]],x=dados['diagnosis'],ax=ax[3][3])\nsns.boxplot(y=dados[colunas[20]],x=dados['diagnosis'],ax=ax[3][4])\n\nsns.boxplot(y=dados[colunas[21]],x=dados['diagnosis'],ax=ax[4][0])\nsns.boxplot(y=dados[colunas[22]],x=dados['diagnosis'],ax=ax[4][1])\nsns.boxplot(y=dados[colunas[23]],x=dados['diagnosis'],ax=ax[4][2])\nsns.boxplot(y=dados[colunas[24]],x=dados['diagnosis'],ax=ax[4][3])\nsns.boxplot(y=dados[colunas[25]],x=dados['diagnosis'],ax=ax[4][4])\n\nsns.boxplot(y=dados[colunas[26]],x=dados['diagnosis'],ax=ax[5][0])\nsns.boxplot(y=dados[colunas[27]],x=dados['diagnosis'],ax=ax[5][1])\nsns.boxplot(y=dados[colunas[28]],x=dados['diagnosis'],ax=ax[5][2])\nsns.boxplot(y=dados[colunas[29]],x=dados['diagnosis'],ax=ax[5][3])\nsns.boxplot(y=dados[colunas[30]],x=dados['diagnosis'],ax=ax[5][4])\n\nplt.tight_layout()","b19a764d":"colunas_normalizar = colunas.drop('diagnosis')\ndados_norm = dados.copy()","eb8e7d36":"from sklearn.model_selection import train_test_split, cross_val_score,KFold,StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,LabelEncoder","7faeb448":"enconder = LabelEncoder()\ndados_norm['diagnosis'] = enconder.fit_transform(dados_norm['diagnosis'])","9855332a":"dados_norm.head()","8979a950":"scaler = RobustScaler()\nfor col in colunas_normalizar:\n    dados_norm[col] = scaler.fit_transform(dados_norm[col].values.reshape(-1,1))","82ee35db":"dados.head()","361b148f":"X = dados_norm.drop(['diagnosis'],axis=1)\nY = dados_norm['diagnosis']","f1477628":"strat_kfold = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor indice_treino, indice_teste in strat_kfold.split(X, Y):\n    #print(\"Treino:\", indice_treino, \"Teste:\", indice_teste)\n    X_treino, X_teste = X.iloc[indice_treino], X.iloc[indice_teste]\n    Y_treino, Y_teste = Y.iloc[indice_treino], Y.iloc[indice_teste]","6d8bba47":"from sklearn.metrics import confusion_matrix,accuracy_score,f1_score,average_precision_score,classification_report,recall_score","070831c6":"from sklearn.model_selection import GridSearchCV","a04262ca":"nome_modelo = []\nresultados = []","f964eb75":"accuracy = []\nprecision =[]\nrecall = []\nf1 = []","81bb8d0c":"from sklearn.linear_model import LogisticRegression","d325a5d2":"print(\"Logistic Regression\")\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000,10000,100000], \n                  'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\ngrid_log_reg = GridSearchCV(LogisticRegression(max_iter=2000), log_reg_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\ngrid_log_reg.fit(X_treino, Y_treino)\nlogreg = grid_log_reg.best_estimator_\nlog_reg_score = cross_val_score(logreg, X_treino, Y_treino, cv=10,scoring='recall')\nprint(\"Best Estimator\")\nprint(logreg)\nprint('Score Regressao Logistica Validacao Cruzada: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')","d0598782":"nome_modelo.append(\"Logistic Regression\")\nresultados.append(log_reg_score)","40ad621a":"logreg.fit(X_treino,Y_treino)\nY_pred_logreg = logreg.predict(X_teste)\ncm_logreg = confusion_matrix(Y_teste,Y_pred_logreg)\nacc_score_logreg = accuracy_score(Y_teste,Y_pred_logreg)\nf1_score_logreg = f1_score(Y_teste,Y_pred_logreg)\nprecisao_logreg = average_precision_score(Y_teste,Y_pred_logreg)\nrecall_logreg = recall_score(Y_teste,Y_pred_logreg)\nprint('Acuracia Regress\u00e3o Logistica ',round(acc_score_logreg*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Regress\u00e3o Logistica ',round(precisao_logreg*100,2).astype(str)+'%')\nprint('F1 Regress\u00e3o Logistica ',round(f1_score_logreg*100,2).astype(str)+'%')\nprint('Recall Regress\u00e3o Logistica ',round(recall_logreg*100,2).astype(str)+'%')","c68481d0":"accuracy.append(acc_score_logreg)\nprecision.append(precisao_logreg)\nrecall.append(recall_logreg)\nf1.append(f1_score_logreg)","1b25bc5b":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_logreg, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Regress\u00e3o Logistica \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","87e0e36f":"from sklearn.neighbors import KNeighborsClassifier","bd176168":"print(\"KNN\")\nknears_params = {\"n_neighbors\": list(range(5,40,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                'leaf_size' : list(range(3,40,1))}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\ngrid_knears.fit(X_treino, Y_treino)\nknn = grid_knears.best_estimator_\nknears_score = cross_val_score(knn, X_treino, Y_treino, cv=10,scoring='recall')\nprint(\"Best Estimator\")\nprint(knn)\nprint('Score KNN Validacao Cruzada: ', round(knears_score.mean() * 100, 2).astype(str) + '%')","ed83f89c":"nome_modelo.append(\"KNN\")\nresultados.append(knears_score)","f3aa7959":"knn.fit(X_treino,Y_treino)\nY_pred_knn = knn.predict(X_teste)\ncm_knn = confusion_matrix(Y_teste,Y_pred_knn)\nacc_score_knn = accuracy_score(Y_teste,Y_pred_knn)\nf1_score_knn = f1_score(Y_teste,Y_pred_knn)\nprecisao_knn = average_precision_score(Y_teste,Y_pred_knn)\nrecall_knn = recall_score(Y_teste,Y_pred_knn)\nprint('Acuracia KNN ',round(acc_score_knn*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia KNN ',round(precisao_knn*100,2).astype(str)+'%')\nprint('F1 KNN ',round(f1_score_knn*100,2).astype(str)+'%')\nprint('Recall KNN ',round(recall_knn*100,2).astype(str)+'%')","6b33268a":"accuracy.append(acc_score_knn)\nprecision.append(precisao_knn)\nrecall.append(recall_knn)\nf1.append(f1_score_knn)","ff683a02":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_knn, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"KNN \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","b9fba5eb":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier","c20aba1b":"print(\"Ada Boost Classifier\")\nada_params = {'n_estimators' : [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80], 'learning_rate' : [0.001,0.01,0.1,1.0], 'algorithm' : ['SAMME','SAMME.R']}\ngrid_ada = GridSearchCV(AdaBoostClassifier(), ada_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\ngrid_ada.fit(X_treino, Y_treino)\nada = grid_ada.best_estimator_\nprint(\"Best Estimator\")\nprint(ada)\nada_score = cross_val_score(ada, X_treino, Y_treino, cv=10,scoring='recall')\nprint('Score AdaBoost Validacao Cruzada: ', round(ada_score.mean() * 100, 2).astype(str) + '%')","7bc3e1ac":"nome_modelo.append(\"AdaBoost\")\nresultados.append(ada_score)","67d2f88b":"ada.fit(X_treino,Y_treino)\nY_pred_ada = ada.predict(X_teste)\ncm_ada = confusion_matrix(Y_teste,Y_pred_ada)\nacc_score_ada = accuracy_score(Y_teste,Y_pred_ada)\nf1_score_ada = f1_score(Y_teste,Y_pred_ada)\nprecisao_ada = average_precision_score(Y_teste,Y_pred_ada)\nrecall_ada = recall_score(Y_teste,Y_pred_ada)\nprint('Acuracia ADA Boost ',round(acc_score_ada*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Ada Boost ',round(precisao_ada*100,2).astype(str)+'%')\nprint('F1 Ada Boost ',round(f1_score_ada*100,2).astype(str)+'%')\nprint('Recall Ada Boost ',round(recall_ada*100,2).astype(str)+'%')","c7236aa7":"accuracy.append(acc_score_ada)\nprecision.append(precisao_ada)\nrecall.append(recall_ada)\nf1.append(f1_score_ada)","603b116b":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_ada, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Ada Boost \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","ad3a04db":"print(\"Random Forest Classifier\")\nforest_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,20,1)), \n              \"min_samples_leaf\": list(range(3,20,1)), 'max_features' : ['auto','sqrt','log2']}\nforest = GridSearchCV(RandomForestClassifier(), forest_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\nforest.fit(X_treino, Y_treino)\nrandom_forest = forest.best_estimator_\nprint(\"Best Estimator\")\nprint(random_forest)\nforest_score = cross_val_score(random_forest, X_treino, Y_treino, cv=10,scoring='recall')\nprint('Score RFC Validacao Cruzada: ', round(forest_score.mean() * 100, 2).astype(str) + '%')","aaaea874":"nome_modelo.append(\"RFC\")\nresultados.append(forest_score)","eadc20fa":"random_forest.fit(X_treino,Y_treino)\nY_pred_rf = random_forest.predict(X_teste)\ncm_rf = confusion_matrix(Y_teste,Y_pred_rf)\nacc_score_rf = accuracy_score(Y_teste,Y_pred_rf)\nf1_score_rf = f1_score(Y_teste,Y_pred_rf)\nprecisao_rf = average_precision_score(Y_teste,Y_pred_rf)\nrecall_rf = recall_score(Y_teste,Y_pred_rf)\nprint('Acuracia Random Forest ',round(acc_score_rf*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Random Forest ',round(precisao_rf*100,2).astype(str)+'%')\nprint('F1 Random Forest ',round(f1_score_rf*100,2).astype(str)+'%')\nprint('Recall Random Forest ',round(recall_rf*100,2).astype(str)+'%')","a69a7f1c":"accuracy.append(acc_score_rf)\nprecision.append(precisao_rf)\nrecall.append(recall_rf)\nf1.append(f1_score_rf)","8fc0210f":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_rf, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Random Forest \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","e5648006":"print(\"Gradient Boost Classifier\")\ngrad_params = {'n_estimators' : [30,35,40,45,50,55,60,65,70], 'learning_rate' : [0.001,0.01,0.1,1.0], 'loss' : ['deviance','exponential'],\n              'max_depth' : [3,4,5,6,7], 'max_features' : ['auto','sqrt','log2'], 'min_samples_leaf' : [2,3,4,5,6]}\ngrad = GridSearchCV(GradientBoostingClassifier(), grad_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\ngrad.fit(X_treino, Y_treino)\ngrad_boost = grad.best_estimator_\nprint(\"Best Estimator\")\nprint(grad_boost)\ngrad_score = cross_val_score(grad_boost, X_treino, Y_treino, cv=10,scoring='recall')\nprint('Score GradBoost Validacao Cruzada: ', round(grad_score.mean() * 100, 2).astype(str) + '%')","b42ad3ce":"nome_modelo.append(\"GradBoost\")\nresultados.append(grad_score)","ea8af111":"grad_boost.fit(X_treino,Y_treino)\nY_pred_gb = grad_boost.predict(X_teste)\ncm_gb = confusion_matrix(Y_teste,Y_pred_gb)\nacc_score_gb = accuracy_score(Y_teste,Y_pred_gb)\nf1_score_gb = f1_score(Y_teste,Y_pred_gb)\nprecisao_gb = average_precision_score(Y_teste,Y_pred_gb)\nrecall_gb = recall_score(Y_teste,Y_pred_gb)\nprint('Acuracia Gradient Boosting ',round(acc_score_gb*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Gradient Boosting  ',round(precisao_gb*100,2).astype(str)+'%')\nprint('F1 Gradient Boosting  ',round(f1_score_gb*100,2).astype(str)+'%')\nprint('Recall Gradient Boosting  ',round(recall_gb*100,2).astype(str)+'%')","2bfc4841":"accuracy.append(acc_score_gb)\nprecision.append(precisao_gb)\nrecall.append(recall_gb)\nf1.append(f1_score_gb)","91db5c5e":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_gb, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Gradient Boosting  \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","965bf3f7":"fig,ax=plt.subplots(figsize=(10,7))\nplt.boxplot(resultados)\nax.set_xticklabels(nome_modelo)\nplt.tight_layout()","55c9560a":"from keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense,Dropout\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.callbacks import ReduceLROnPlateau,EarlyStopping","b0e2604b":"n_inputs = X_treino.shape[1]","cb415c9a":"modelo = Sequential()\nmodelo.add(Dense(32, input_shape=(n_inputs, ), activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo.add(Dropout(0.5))\nmodelo.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo.add(Dense(32, activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo.add(Dropout(0.5))\nmodelo.add(Dense(2, activation='softmax', kernel_initializer='glorot_uniform',bias_initializer='zeros'))","da5d503c":"reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, mode='auto', min_delta=0.0001)","288b8979":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)","8cdd2e9f":"callbacks_list = [reduce_lr,es]","49c18f2c":"modelo.compile(Adam(lr=0.01), loss='sparse_categorical_crossentropy', metrics=['top_k_categorical_accuracy'])\nmodelo.fit(X_treino, Y_treino, batch_size=20, epochs=200, verbose=2, validation_data=(X_teste,Y_teste),callbacks=callbacks_list)","f37a56f1":"Y_pred_keras = modelo.predict_classes(X_teste, batch_size=50, verbose=0)","d54db3f7":"cm_keras = confusion_matrix(Y_teste,Y_pred_keras)\nacc_score_keras = accuracy_score(Y_teste,Y_pred_keras)\nf1_score_keras = f1_score(Y_teste,Y_pred_keras)\nprecisao_keras = average_precision_score(Y_teste,Y_pred_keras)\nrecall_keras = recall_score(Y_teste,Y_pred_keras)\nprint('Acuracia Keras ',round(acc_score_keras*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Keras  ',round(precisao_keras*100,2).astype(str)+'%')\nprint('F1 Gradient Boosting  ',round(f1_score_keras*100,2).astype(str)+'%')\nprint('Recall Keras  ',round(recall_keras*100,2).astype(str)+'%')","9ae5aae1":"nome_modelo.append(\"Keras\")\naccuracy.append(acc_score_keras)\nprecision.append(precisao_keras)\nrecall.append(recall_keras)\nf1.append(f1_score_keras)","9186c631":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_keras, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Keras  \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","4854fa76":"from sklearn.feature_selection import SelectKBest,chi2","762bf2e4":"dados['diagnosis'] = enconder.fit_transform(dados['diagnosis'])","f29e4da3":"X = dados.drop(['diagnosis'],axis=1)\nY = dados['diagnosis']","2648ec3a":"def model_params(model):\n    if(model == 'logistic_regression'):\n        modelo = LogisticRegression(max_iter=2000)\n        modelo_params = {\"penalty\": ['l1', 'l2'], 'C': [0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000,10000,100000], \n                  'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n    \n    elif(model == 'KNN'):\n        modelo = KNeighborsClassifier()\n        modelo_params = {\"n_neighbors\": list(range(5,40,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                'leaf_size' : list(range(2,40,1))}\n    \n    elif(model == 'AdaBoost'):\n        modelo = AdaBoostClassifier()\n        modelo_params = {'n_estimators' : list(range(5,81)), 'learning_rate' : [0.001,0.01,0.1,1.0], 'algorithm' : ['SAMME','SAMME.R']}\n        \n    elif(model == 'RFC'):\n        modelo = RandomForestClassifier()\n        modelo_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,20,1)), \n              \"min_samples_leaf\": list(range(3,20,1)), 'max_features' : ['auto','sqrt','log2']}\n        \n    elif(model == 'GradBoost'):\n        modelo = GradientBoostingClassifier()\n        modelo_params = {'n_estimators' : [30,35,40,45,50,55,60,65,70], 'learning_rate' : [0.001,0.01,0.1,1.0], 'loss' : ['deviance','exponential'],\n              'max_depth' : [3,4,5,6,7], 'max_features' : ['auto','sqrt','log2'], 'min_samples_leaf' : [2,3,4,5,6]}\n    \n    return modelo,modelo_params","798b8321":"def find_best_features(modelo,X,Y,n):\n    X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size=0.3, random_state=42)\n    best_features = SelectKBest(chi2, k=n).fit(X_treino, Y_treino)\n    X_treino = best_features.transform(X_treino)\n    X_teste = best_features.transform(X_teste)\n    acc,precision,recall,f1 = best_model(modelo,X_treino,Y_treino,X_teste,Y_teste)\n    \n    return acc, precision, recall, f1","25b83ee5":"def best_model(model,X_treino_best,Y_treino_best,X_teste,Y_teste):\n    \n    modelo, parametros = model_params(model)\n    grid_log_reg = GridSearchCV(modelo, parametros,n_jobs=8,cv=10,scoring=['recall','f1'],refit='f1')\n    grid_log_reg.fit(X_treino_best, Y_treino_best)\n    logreg = grid_log_reg.best_estimator_\n    logreg.fit(X_treino_best,Y_treino_best)\n    Y_pred_Kbest = logreg.predict(X_teste)\n    acc_kest = accuracy_score(Y_teste,Y_pred_Kbest)\n    f1_kbest = f1_score(Y_teste,Y_pred_Kbest)\n    precisao_kbest = average_precision_score(Y_teste,Y_pred_Kbest)\n    recall_kbest = recall_score(Y_teste,Y_pred_Kbest)\n    \n    return acc_kest,precisao_kbest,recall_kbest,f1_kbest","ac15da3f":"def find_bestn(modelo,X,Y,number):\n\n    acc_findbest = []\n    rec_findbest = []\n    prec_findbest = []\n    f1s_findbest= []\n    n_idex = []\n\n    for n in range(5,number):\n        acuraciax,precisaox,recallx,f1x = find_best_features(modelo,X,Y,n)\n        acc_findbest.append(acuraciax)\n        rec_findbest.append(recallx)\n        prec_findbest.append(precisaox)\n        f1s_findbest.append(f1x)\n        n_idex.append(n)\n        print(\"N = \",n,\"Acc = \",acuraciax, \"Prec = \",precisaox, \"Rec = \",recallx, \"F1 = \",f1x)\n    \n    dic_kbest = {\"N\" : n_idex, \"Acuracia\" : acc_findbest, \"Recall\" : rec_findbest, \"Precision\" : prec_findbest, \"F1\" : f1s_findbest}\n\n    dataframe_kbest = pd.DataFrame(dic_kbest)\n    \n    dataframe_kbest = dataframe_kbest.sort_values(by=['Acuracia','Recall','F1','Precision'],ascending=False).reset_index()\n    \n    best_n = int(dataframe_kbest.iloc[0]['N'])\n    \n    return best_n","385cd092":"modelos = ['logistic_regression']","ed10cf84":"dic_bestn = {}\n\nfor models in modelos:\n    bestn = find_bestn(models,X,Y,num_colunas-1)\n    dic_bestn[models] = bestn\n    print(\"Modelo = \",models,\" \",\"N = \",bestn)","12636873":"X_treino_best, X_teste_best, Y_treino_best, Y_teste_best = train_test_split(X, Y, test_size=0.3, random_state=42)","0290025e":"best_n = dic_bestn['logistic_regression']","4a854089":"modelo_kbest = SelectKBest(chi2, k=int(best_n)).fit(X_treino_best, Y_treino_best)\nX_treino_best = modelo_kbest.transform(X_treino_best)#.values\nX_teste_best = modelo_kbest.transform(X_teste_best)#.values\nY_treino_best = Y_treino_best.values\nY_teste_best = Y_teste_best.values","db491994":"acc_kbest = []\nprecison_kbest =[]\nrecall_kbest = []\nf1_kbest = []","e9432a29":"log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1e3,1e4], \n                  'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\ngrid_log_reg = GridSearchCV(LogisticRegression(max_iter=2000), log_reg_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='f1')\ngrid_log_reg.fit(X_treino_best, Y_treino_best)\nlogreg = grid_log_reg.best_estimator_\nlogreg.fit(X_treino_best,Y_treino_best)\nY_pred_best = logreg.predict(X_teste_best)\ncm_best = confusion_matrix(Y_teste_best,Y_pred_best)","f34c3ba4":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_best, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Regress\u00e3o logistica  \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","e72036cb":"acc_score_logreg_best = accuracy_score(Y_teste_best,Y_pred_best)\nf1_score_logreg_best = f1_score(Y_teste_best,Y_pred_best)\nprecisao_logreg_best = average_precision_score(Y_teste_best,Y_pred_best)\nrecall_logreg_best = recall_score(Y_teste_best,Y_pred_best)\nprint('Acuracia Regress\u00e3o Logistica ',round(acc_score_logreg_best*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Regress\u00e3o Logistica ',round(precisao_logreg_best*100,2).astype(str)+'%')\nprint('F1 Regress\u00e3o Logistica ',round(f1_score_logreg_best*100,2).astype(str)+'%')\nprint('Recall Regress\u00e3o Logistica ',round(recall_logreg_best*100,2).astype(str)+'%')","d7c3bfc3":"acc_kbest.append(acc_score_logreg_best)\nprecison_kbest.append(precisao_logreg_best)\nrecall_kbest.append(recall_logreg_best)\nf1_kbest.append(f1_score_logreg_best)","aa5a3f50":"knears_params = {\"n_neighbors\": list(range(5,40,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                'leaf_size' : list(range(2,40,1))}\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\ngrid_knears.fit(X_treino_best, Y_treino_best)\nknn = grid_knears.best_estimator_\nknn.fit(X_treino_best,Y_treino_best)\nY_pred_best_knn = knn.predict(X_teste_best)\ncm_best_knn = confusion_matrix(Y_teste_best,Y_pred_best_knn)","73ad093b":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_best_knn, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"KNN  \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","c9541677":"acc_score_knn = accuracy_score(Y_teste_best,Y_pred_best_knn)\nf1_score_knn = f1_score(Y_teste_best,Y_pred_best_knn)\nprecisao_knn = average_precision_score(Y_teste_best,Y_pred_best_knn)\nrecall_knn = recall_score(Y_teste_best,Y_pred_best_knn)\nprint('Acuracia KNN ',round(acc_score_knn*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia KNN ',round(precisao_knn*100,2).astype(str)+'%')\nprint('F1 KNN ',round(f1_score_knn*100,2).astype(str)+'%')\nprint('Recall KNN ',round(recall_knn*100,2).astype(str)+'%')","c52c3b44":"acc_kbest.append(acc_score_knn)\nprecison_kbest.append(precisao_knn)\nrecall_kbest.append(recall_knn)\nf1_kbest.append(f1_score_knn)","cce274e5":"ada_params = {'n_estimators' : list(range(5,81)), 'learning_rate' : [0.001,0.01,0.1,1.0], 'algorithm' : ['SAMME','SAMME.R']}\ngrid_ada = GridSearchCV(AdaBoostClassifier(), ada_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='f1')\ngrid_ada.fit(X_treino_best, Y_treino_best)\nada = grid_ada.best_estimator_\nada.fit(X_treino_best,Y_treino_best)\nY_pred_best_ada = ada.predict(X_teste_best)\ncm_best_ada = confusion_matrix(Y_teste_best,Y_pred_best_ada)","62ac453e":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_best_ada, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Ada Boost  \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","909ce951":"acc_score_ada_best = accuracy_score(Y_teste_best,Y_pred_best_ada)\nf1_score_ada_best = f1_score(Y_teste_best,Y_pred_best_ada)\nprecisao_ada_best = average_precision_score(Y_teste_best,Y_pred_best_ada)\nrecall_ada_best = recall_score(Y_teste_best,Y_pred_best_ada)\nprint('Acuracia Ada Boost ',round(acc_score_ada_best*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Ada Boost ',round(precisao_ada_best*100,2).astype(str)+'%')\nprint('F1 Ada Boost ',round(f1_score_ada_best*100,2).astype(str)+'%')\nprint('Recall Ada Boost ',round(recall_ada_best*100,2).astype(str)+'%')","208433ca":"acc_kbest.append(acc_score_ada_best)\nprecison_kbest.append(precisao_ada_best)\nrecall_kbest.append(recall_ada_best)\nf1_kbest.append(f1_score_ada_best)","69ba8441":"forest_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,20,1)), \n              \"min_samples_leaf\": list(range(3,20,1)), 'max_features' : ['auto','sqrt','log2']}\nforest = GridSearchCV(RandomForestClassifier(), forest_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\nforest.fit(X_treino_best, Y_treino_best)\nrandom_forest = forest.best_estimator_\nrandom_forest.fit(X_treino_best,Y_treino_best)\nY_pred_best_rf = random_forest.predict(X_teste_best)\ncm_best_rf = confusion_matrix(Y_teste_best,Y_pred_best_rf)","6ffd1b9d":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_best_rf, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Random Forest  \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","a975d29a":"acc_score_rf_best = accuracy_score(Y_teste_best,Y_pred_best_rf)\nf1_score_rf_best = f1_score(Y_teste_best,Y_pred_best_rf)\nprecisao_rf_best = average_precision_score(Y_teste_best,Y_pred_best_rf)\nrecall_rf_best = recall_score(Y_teste_best,Y_pred_best_rf)\nprint('Acuracia Random Forest ',round(acc_score_rf_best*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Random Forest ',round(precisao_rf_best*100,2).astype(str)+'%')\nprint('F1 Random Forest ',round(f1_score_rf_best*100,2).astype(str)+'%')\nprint('Recall Random Forest ',round(recall_rf_best*100,2).astype(str)+'%')","7c9f8b0d":"acc_kbest.append(acc_score_rf_best)\nprecison_kbest.append(precisao_rf_best)\nrecall_kbest.append(recall_rf_best)\nf1_kbest.append(f1_score_rf_best)","7fe00822":"grad_params = {'n_estimators' : [30,35,40,45,50,55,60,65,70], 'learning_rate' : [0.001,0.01,0.1,1.0], 'loss' : ['deviance','exponential'],\n              'max_depth' : [3,4,5,6,7], 'max_features' : ['auto','sqrt','log2'], 'min_samples_leaf' : [2,3,4,5,6]}\ngrad = GridSearchCV(GradientBoostingClassifier(), grad_params,n_jobs=8,cv=10,scoring=['recall','f1'],refit='recall')\ngrad.fit(X_treino_best, Y_treino_best)\ngrad_boost = grad.best_estimator_\ngrad_boost.fit(X_treino_best,Y_treino_best)\nY_pred_best_grad = grad_boost.predict(X_teste_best)\ncm_best_grad = confusion_matrix(Y_teste_best,Y_pred_best_grad)","a723dd98":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(cm_best_grad, ax=ax, annot=True, cmap=plt.cm.copper)\nax.set_title(\"Gradient Boosting  \\n Matriz de Confus\u00e3o\", fontsize=14)\nax.set_xticklabels(['B', 'M'], fontsize=14, rotation=0)\nax.set_yticklabels(['B', 'M'], fontsize=14, rotation=360)","d8b3605e":"acc_score_grad_best = accuracy_score(Y_teste_best,Y_pred_best_grad)\nf1_score_grad_best = f1_score(Y_teste_best,Y_pred_best_grad)\nprecisao_grad_best = average_precision_score(Y_teste_best,Y_pred_best_grad)\nrecall_grad_best = recall_score(Y_teste_best,Y_pred_best_grad)\nprint('Acuracia Random Forest ',round(acc_score_grad_best*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Random Forest ',round(precisao_grad_best*100,2).astype(str)+'%')\nprint('F1 Random Forest ',round(f1_score_grad_best*100,2).astype(str)+'%')\nprint('Recall Random Forest ',round(recall_grad_best*100,2).astype(str)+'%')","eed028bf":"acc_kbest.append(acc_score_grad_best)\nprecison_kbest.append(precisao_grad_best)\nrecall_kbest.append(recall_grad_best)\nf1_kbest.append(f1_score_grad_best)","883ebc60":"n_inputs = X_treino_best.shape[1]","87517674":"modelo2 = Sequential()\nmodelo2.add(Dense(32, input_shape=(n_inputs, ), activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo2.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo2.add(Dropout(0.5))\nmodelo2.add(Dense(64, activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo2.add(Dense(32, activation='relu', kernel_initializer='glorot_uniform',bias_initializer='zeros'))\nmodelo2.add(Dropout(0.5))\nmodelo2.add(Dense(2, activation='softmax', kernel_initializer='glorot_uniform',bias_initializer='zeros'))","f1dc49cb":"modelo2.compile(Adam(lr=0.01), loss='sparse_categorical_crossentropy', metrics=['top_k_categorical_accuracy'])\nmodelo2.fit(X_treino_best, Y_treino_best, batch_size=20, epochs=200, verbose=2, validation_data=(X_teste_best,Y_teste_best),callbacks=callbacks_list)","23896040":"Y_pred_keras = modelo2.predict_classes(X_teste_best, batch_size=50, verbose=0)","34f7ddef":"cm_keras = confusion_matrix(Y_teste_best,Y_pred_keras)\nacc_score_keras = accuracy_score(Y_teste_best,Y_pred_keras)\nf1_score_keras = f1_score(Y_teste_best,Y_pred_keras)\nprecisao_keras = average_precision_score(Y_teste_best,Y_pred_keras)\nrecall_keras = recall_score(Y_teste_best,Y_pred_keras)\nprint('Acuracia Keras ',round(acc_score_keras*100,2).astype(str)+'%')\nprint('Preci\u00e3o m\u00e9dia Keras  ',round(precisao_keras*100,2).astype(str)+'%')\nprint('F1 Gradient Boosting  ',round(f1_score_keras*100,2).astype(str)+'%')\nprint('Recall Keras  ',round(recall_keras*100,2).astype(str)+'%')","362a0e56":"acc_kbest.append(acc_score_keras)\nprecison_kbest.append(precisao_keras)\nrecall_kbest.append(recall_keras)\nf1_kbest.append(f1_score_keras)","292f13c4":"dic_metrics = {'Model' : nome_modelo, 'Accuracy' : accuracy, 'Precision' : precision, 'Recall' : recall, 'F1' : f1, \n              'Accuracy_Kbest' : acc_kbest, 'Precision_Kbest' : precison_kbest, 'Recall_Kbest' : recall_kbest,\n              'F1_Kbest' : f1_kbest}\n\ndataframe = pd.DataFrame(dic_metrics)","cd914345":"dataframe","77e5ced5":"Up to this point logistic regression and AdaBoost are the best models","ae1d1ccb":"Keras had similar behavior compared to the other models. Deep learning models are not required up to this point to categorize the cancer type","dd53ebed":"Normalizing columns with RobustScaler to take into account the effects due to outliers","5eb225d9":"Using GridSearchCV to find the best inputs for Logistic Regression, KNN, SVC, Decision Tree, Random Forest, Ada Boost and Gradient Boosting","9cf14921":"From the 30 features from this dataset only 12 of them are really important. We will from this point choose only these 14 features and see how the models are improved","bc57c641":"Dropping columns id and Unnamed: 32 sicne they are unimportant","ffe5db58":"Boxplots to check the distribution of each variable","cdaf6ffd":"Except Keras, all other models have reached better accuracies demonstranting that remove the not necessary features plays an important role in my models","d0404702":"Splitting sample into training and testing","674649d0":"Countplot to check the amount of each type of cancer","a134c856":"Converting categorical variable diagnosis to numeric","ab3af49d":"Importing libraries to compute metrics","9368c76b":"Checking the first five lines","09c0932a":"Importing libraries to normalize the data and split into train and test samples","e87d5cad":"Splitting into X and Y variables","4cd8dab3":"Boxplot of the average score of each model","87fde9a9":"Up to this point logistic regression and AdaBoost are the best models","2e6dc8f2":"Choosing the K best features in order to best the performance of each model and remove features that are causing troubles to the models to get the best result","a9e4f751":"Finding the correlation among each of the variables","2d5327b1":"There are features like Compactness_mean, concavity_mean and concave points_mean that are correlated with each other","ea8728f4":"Normalization usually improves model performance. With expection of the column diagnosis, all remaining columns will be normalized","052b0569":"Each feature has a good distribuition with few outliers that will not be removed up to this point"}}