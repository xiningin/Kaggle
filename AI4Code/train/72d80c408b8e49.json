{"cell_type":{"7c7559f1":"code","4548b2ea":"code","54db069f":"code","fc0c298c":"code","37cf7649":"code","d417ff7d":"code","c581dff8":"code","f1951ab9":"code","42f99178":"code","c6be152c":"code","18103bdf":"code","f6ea5cb5":"code","00172ee8":"code","c96d5340":"code","5defc864":"code","f7ca2098":"code","188134e2":"code","3901fe3e":"code","88d62de7":"code","0e6c4c7b":"code","96263767":"code","7676c442":"code","b7336bd0":"code","7f83cdc2":"code","edd71dc9":"code","b69a9c96":"code","6c70938a":"code","b64b9e82":"code","908b7936":"code","6cc79cd2":"code","02c7e646":"code","62c48e55":"code","8834ba3f":"code","66d30e88":"code","a21b7cb0":"code","153f4216":"code","c3d9259d":"code","a0c328e4":"code","d4920a63":"code","20c7642e":"code","1e606783":"code","c45073b0":"code","8f58010e":"code","16747805":"code","02b74947":"code","af4323d2":"markdown","98bf126a":"markdown","a1fb7e85":"markdown","30bc748d":"markdown","297a62aa":"markdown","d4feef4a":"markdown","391b2033":"markdown","06c0db33":"markdown","afe093e0":"markdown","e0a3f305":"markdown","236e4732":"markdown","546faa9d":"markdown","dcd2de36":"markdown","ddd2ec08":"markdown","2610075f":"markdown","3900e04f":"markdown","040860f8":"markdown","bc17943a":"markdown","43a8a3eb":"markdown","530572c5":"markdown","8a40e4b7":"markdown","141b607e":"markdown","1b7c7c0c":"markdown","59f2374a":"markdown","c39c6e4c":"markdown","ca20805b":"markdown","62c2b8e0":"markdown","a189a5a1":"markdown","f901e2d7":"markdown","d8c4b1a1":"markdown","03266e2b":"markdown"},"source":{"7c7559f1":"# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import cross_validation\nfrom sklearn.grid_search import GridSearchCV\nfrom time import time\nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\n\n# Stop deprecation warnings from being printed\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","4548b2ea":"# Load training and test data into pandas dataframes\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n# merge training and test sets into one dataframe\nfull = pd.concat([train, test])","54db069f":"# Get size of dataframes\nfor dataframe in [train, test, full]:\n    print(dataframe.shape)","fc0c298c":"full.head(2)","37cf7649":"#return a formatted percentage from a fraction\ndef percentage(numerator, denomenator):\n    \n    if type(numerator) == pd.core.series.Series:\n        return (numerator\/denomenator*100).map('{:.1f}%'.format)\n    \n    elif type(numerator) == int or type(numerator) == float:\n        return '{:.1f}%'.format(float(numerator)\/float(denomenator)*100) \n    \n    else:\n        print(\"check type\")","d417ff7d":"#Get percentage by variable of values which are not NaN\npercentage(full.count()-1, full.shape[0]-1)","c581dff8":"# Get cabin #'s in list\ncabin_numbers = full[full.Cabin.notnull()]['Cabin'].tolist()\ncabin_numbers[:10]","f1951ab9":"# Number of passengers w\/ cabin numbers by class\nfull[full.Cabin.notnull()].groupby('Pclass')['Pclass'].count()","42f99178":"# Percentage of passengers w\/ cabin numbers by class\npercentage(full[full.Cabin.notnull()].groupby('Pclass')['Pclass'].count(),\n           full.groupby('Pclass')['Pclass'].count()\n          )","c6be152c":"# Number of passengers w\/ cabin numbers by class AND survival\ntrain[train.Cabin.notnull()].groupby(['Pclass', 'Survived'])['Cabin'].count()","18103bdf":"# How classes were distributed by deck (decks were labeled A-G from top to bottom of ship)\nfull['Deck'] = full.Cabin.str.extract(\"([a-zA-Z])\", expand=False)\nfull[full.Cabin.notnull()].groupby(['Deck', 'Pclass'])['Deck'].count()","f6ea5cb5":"# Remove 'Cabin' column from dataset\nfull = full.drop(['Cabin', 'Deck'], 1);","00172ee8":"# find entries where port of embarkation is null\nfull[full.Embarked.isnull()]","c96d5340":"# Extract first three characters from ticket number and look for partterns from port of embarkation\nfull['ticket_header'] = full.Ticket.str.extract(\"([a-zA-Z0-9]{3})\", expand=False)\nfull.groupby(['ticket_header', 'Embarked'])['Ticket'].count().head(12)","5defc864":"full.groupby(['Sex', 'Pclass', 'Embarked'])['Embarked'].count().head(3)","f7ca2098":"full.set_value(61, 'Embarked', 'S');\nfull.set_value(829, 'Embarked', 'S');","188134e2":"full[full.Fare.isnull()]","3901fe3e":"# look at fare statistics for passengers traveling alone or with 1 spouse\/sibling \n# and with ticket  beginning with '370'\nfull[full.ticket_header == '370'].groupby(['Parch', 'SibSp'])['Fare'].describe().head(8)","88d62de7":"full.set_value(152, 'Fare', 7.75);","0e6c4c7b":"# remove 'ticket_header' and 'ticket' columns from data frame\nfull = full.drop(['ticket_header', 'Ticket'], 1);","96263767":"# Extract title from name column using RE and put in new column labeled 'Title'\nfull['Title']= full.Name.str.extract(\"(.*, )([^\\.]+)\", expand=False)[1]\nfull.groupby('Title')['Name'].count()","7676c442":"# Convert French titles like Mlle and Mme to English equivalent and convert all other titles to 'Rare'\nfull.loc[full.Title == 'Mlle', 'Title'] = 'Miss'\nfull.loc[full.Title == 'Mme', 'Title'] = 'Mrs'\nfull.loc[~full.Title.isin(['Master', 'Mr', 'Mrs', 'Miss']), 'Title'] = 'Rare'","b7336bd0":"full.groupby('Title')['Name'].count()","7f83cdc2":"# Look at survivorship numbers by title\nfull[full.Survived.notnull()].groupby(['Title', 'Survived'])['Name'].count()","edd71dc9":"# Look at median age by title\nfull.groupby(['Title'])['Age'].median()","b69a9c96":"# Look at standard deviation of age by title\nfull.groupby(['Title'])['Age'].std()","6c70938a":"# Look at median age by title and class\nfull.groupby(['Title', 'Pclass'])['Age'].median()","b64b9e82":"full_ver2 = full.copy()","908b7936":"# create dataframe with median ages by class and title\nage_summary = full.groupby(['Title', 'Pclass'])['Age'].median().to_frame()\nage_summary = age_summary.reset_index()\n\nfor index in full_ver2[full_ver2.Age.isnull()].index:\n    median = age_summary[(age_summary.Title == full_ver2.iloc[index]['Title'])& \\\n                         (age_summary.Pclass == full_ver2.iloc[index]['Pclass'])]['Age'].values[0]\n    full_ver2.set_value(index, 'Age', median)","6cc79cd2":"# format and split dataset for RF fitting.\ndef train_test_split(dataframe):\n    try:\n        # change gender labels to '0' or '1'\n        dataframe[\"Sex\"] = dataframe[\"Sex\"].apply(lambda sex: 0 if sex == \"male\" else 1)\n        \n        convert = LabelEncoder()\n        \n        # change embarkation to numerical labels\n        embarked = convert.fit_transform(dataframe.Embarked.tolist())\n        dataframe['Embarked'] = embarked\n        \n        # change title to numerical labels\n        title = convert.fit_transform(dataframe.Title.tolist())\n        dataframe['Title'] = title    \n        \n    except:\n        \"dataframe is not correctly formatted\"\n    \n    # split into training and test sets and move survival labels to list\n    return dataframe[0:891].drop('Survived', 1), \\\n           dataframe[891:].drop('Survived', 1), \\\n           dataframe[0:891]['Survived'].tolist()","02c7e646":"train_ver2, test_ver2, labels = train_test_split(full_ver2)","62c48e55":"X_train, X_test, y_train, y_test = cross_validation.train_test_split(train_ver2, labels, test_size=0.3, random_state=42);","8834ba3f":"# calculate the time to run a GridSearchCV for multiple numbers of parameter permutations.  \ngrid_times = {}\nclf = RandomForestClassifier(random_state = 84)\n\nfeatures = X_train.columns.drop(['Name', 'PassengerId'], 1)\n\n# I commented this out after running once locally since this block of code takes a long time to run\n'''\nfor number in np.arange(2, 600, 50):\n    \n    param = np.arange(1,number,10)\n    param_grid = {\"n_estimators\": param,\n                  \"criterion\": [\"gini\", \"entropy\"]}\n    \n    grid_search = GridSearchCV(clf, param_grid = param_grid)\n    \n    t0 = time()\n    grid_search.fit(X_train[features], y_train)\n    compute_time = time() - t0\n    grid_times[len(grid_search.grid_scores_)] = time() - t0\n    \ngrid_times = pd.DataFrame.from_dict(grid_times, orient = 'index')\n'''\n\n# hard-coded values were found by running code above\ngrid_times = {0: { 2: 0.034411907196044922,\n                  12: 1.5366179943084717,\n                  22: 5.0431020259857178,\n                  32: 11.378448963165283,\n                  42: 20.211128950119019,\n                  52: 30.040457010269165,\n                  62: 39.442277908325195,\n                  72: 56.834053993225098,\n                  82: 67.847633838653564,\n                  92: 91.005517959594727,\n                  102: 111.2420859336853,\n                  112: 135.75759792327881}}","66d30e88":"final = pd.DataFrame.from_dict(grid_times)\nfinal = final.sort_index()\nplt.plot(final.index.values, final[0])\nplt.xlabel('Number of Parameter Permutations')\nplt.ylabel('Time (sec)')\nplt.title('Time vs. Number of Parameter Permutations of GridSearchCV')","a21b7cb0":"# function takes a RF parameter and a ranger and produces a plot and dataframe of CV scores for parameter values\ndef evaluate_param(parameter, num_range, index):\n    grid_search = GridSearchCV(clf, param_grid = {parameter: num_range})\n    grid_search.fit(X_train[features], y_train)\n    \n    df = {}\n    for i, score in enumerate(grid_search.grid_scores_):\n        df[score[0][parameter]] = score[1]\n       \n    \n    df = pd.DataFrame.from_dict(df, orient='index')\n    df.reset_index(level=0, inplace=True)\n    df = df.sort_values(by='index')\n \n    plt.subplot(3,2,index)\n    plot = plt.plot(df['index'], df[0])\n    plt.title(parameter)\n    return plot, df","153f4216":"# parameters and ranges to plot\nparam_grid = {\"n_estimators\": np.arange(2, 300, 2),\n              \"max_depth\": np.arange(1, 28, 1),\n              \"min_samples_split\": np.arange(1,150,1),\n              \"min_samples_leaf\": np.arange(1,60,1),\n              \"max_leaf_nodes\": np.arange(2,60,1),\n              \"min_weight_fraction_leaf\": np.arange(0.1,0.4, 0.1)}","c3d9259d":"index = 1\nplt.figure(figsize=(16,12))\nfor parameter, param_range in dict.items(param_grid):   \n    evaluate_param(parameter, param_range, index)\n    index += 1","a0c328e4":"from operator import itemgetter\n\n# Utility function to report best scores\ndef report(grid_scores, n_top):\n    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n    for i, score in enumerate(top_scores):\n        print(\"Model with rank: {0}\".format(i + 1))\n        print(\"Mean validation score: {0:.4f})\".format(\n              score.mean_validation_score,\n              np.std(score.cv_validation_scores)))\n        print(\"Parameters: {0}\".format(score.parameters))\n        print(\"\")","d4920a63":"# parameters for GridSearchCV\nparam_grid2 = {\"n_estimators\": [10, 18, 22],\n              \"max_depth\": [3, 5],\n              \"min_samples_split\": [15, 20],\n              \"min_samples_leaf\": [5, 10, 20],\n              \"max_leaf_nodes\": [20, 40],\n              \"min_weight_fraction_leaf\": [0.1]}","20c7642e":"grid_search = GridSearchCV(clf, param_grid=param_grid2)\ngrid_search.fit(X_train[features], y_train)\n\nreport(grid_search.grid_scores_, 4)","1e606783":"param_grid3 = {\"n_estimators\": [5, 40, 42],\n              \"max_depth\": [5, 6],\n              \"min_samples_split\": [5, 10],\n              \"min_samples_leaf\": [3, 5],\n              \"max_leaf_nodes\": [14, 15]}","c45073b0":"grid_search = GridSearchCV(clf, param_grid=param_grid3)\ngrid_search.fit(X_train[features], y_train)\n\nreport(grid_search.grid_scores_, 4)","8f58010e":"clf = RandomForestClassifier(min_samples_split = 40, \n                             max_leaf_nodes = 15, \n                             n_estimators = 40, \n                             max_depth = 5,\n                             min_samples_leaf = 3)","16747805":"clf.fit(train_ver2[features], labels);","02b74947":"prediction = clf.predict(test_ver2[features])\n\noutput = pd.DataFrame(test_ver2['PassengerId'])\noutput['Survived'] = prediction\noutput.to_csv('prediction.csv')","af4323d2":"So simply having a cabin number recorded gives you a survival advantage.  There are obviously non-random reasons for the distribution of cabin numbers, like [survivorship bias](https:\/\/en.wikipedia.org\/wiki\/Survivorship_bias).  We could fill in NaN cabin values with an arbitrary value, say '0', and predict that anyone with a cabin number `!= 0` would survive.  The problem with this is that missingness of this variable is non-random, and this prediction would be based on a peculariaty of the dataset and not on an underlying relationship between survival and cabin number.","98bf126a":"I can narrow the parameters for `GridSearchCV` using the plots above.  For example, max_leaf_nodes has steadily increasing from 1 to 20 and then it plateaus.  Therefore I will restrict my `GridSearchCV` to the range [18, 22].  Similarly, min_weight_fraction_leaf decreases monotonically, so I won't even look at other values besides the lowest value of 0.1.  This helps restrict the number of permutations I need to run.","a1fb7e85":"The letter in each cabin number refers to the deck the cabin was on.  From [Encyclopedia Titanica](https:\/\/www.encyclopedia-titanica.org\/titanic-deckplans\/), *Titanic's* decks were labeled A to F from top to bottom.","30bc748d":"<a name=\"Formatting\"><\/a>\n## Formatting Data for Sklearn and Examining RF Algorithm Parameters","297a62aa":"<a name=\"Handling\"><\/a>\n## 2 Handling Missing Values \nWhen using a random forest classifier(RCF), missing values [are usually handled](http:\/\/nerds.airbnb.com\/overcoming-missing-values-in-a-rfc\/) by either dropping a row or column or by filling in missing values using the average.  There are more sophisticated imputation techniques like k-nearest neighbors but I'm only going to use simple techniques here.","d4feef4a":"So the dataset has been split approximately 70\/30% for training\/testing.  Here's a description of some of the more cryptic variable names:\n\n **Variable** | **Description** \n-------- | -----------\n Embarked | Port of embarkation (S = Southhampton, C = Chersbourg, Q = Queenstown) \n Parch | Number of parents\/children of passenger on board \n Pclass | Passenger's class (1st, 2nd, or 3rd) \n SibSp | Number of siblings or spouses on board ","391b2033":"<a name=\"Filling\"><\/a>\n### 2.2 Filling in NaN's for variables with high coverage\nBoth embarked and fare variables have >99% coverage.  This is such a low number of NaN's that we can look at each entry individually.  I'll look at embarked first.","06c0db33":"<a name=\"Removing\"><\/a>\n### 2.1 Removing variables with too little coverage\nCabin number has by far the least coverage (less than 1\/4 of passengers have a recorded cabin number), so I'll look at the data in this column first.","afe093e0":"<a name=\"Tuning\"><\/a>\n### 3.2 Tuning RF parameters (somewhat methodically)\nThere are many more RF parameter configurations than can be practically be computed.  The plot below illustrates this,","e0a3f305":"# First Kaggle Script: Tuning Random Forest Parameters\n### *Daniel Haden*\n*August 7th, 2016*","236e4732":"[1 Introduction](#Introduction)\n* [1.1 Loading and Checking Data](#Loading)\n* [1.2 Looking for NaNs](#Looking)\n\n[2 Handling Missing Values](#Handling)\n* [2.1 Removing variables with too little coverage](#Removing)\n* [2.2 Filling in NaN's for variables with high coverage](#Filling)\n* [2.3 Imputing Missing Age Values](#Imputing)\n\n[3 Formatting Data for Sklearn and Examining RF Algorithm Parameters](#Formatting)\n* [3.1 Converting categorical variable labels](#Converting)\n* [3.2 Tuning RF parameters (somewhat methodically)](#Tuning)\n\n[4 Training Final Classifier](#Training)\n\n[5 Conclusion](#Conclusion)\n\n<a name=\"Introduction\"><\/a>\n## 1 Introduction\n\nThis is my first attempt at a Kaggle dataset - I was inspired by some very good notebooks by [Megan Risdal](https:\/\/www.kaggle.com\/mrisdal\/titanic\/exploring-survival-on-the-titanic\/notebook) and [Sadat Nazrul](https:\/\/www.kaggle.com\/creepykoala\/titanic\/study-of-tree-and-forest-algorithms\/comments) and thought I'd give it a try myself.  The data exploration portion is pretty standard - I tried to focus on how to tune parameters for the RF classifier.  My goal was  to find a semi-automated way of tuning algorithm parameters. ","546faa9d":"The vast majority of recorded cabin numbers came from first class passengers.  And by percentage,","dcd2de36":"<a name=\"Training\"><\/a>\n## Training Final Classifier","ddd2ec08":"So 3rd class passengers skewed younger than 2nd class passengers, who skewed younger than 1st class passengers.  This pattern repeats for each title (except 'Master').\n\nMissing age values will be filled in with the mode of the passengers title and class subgroup.","2610075f":"Unfortunately, it does not look like tickets were issued in this manner.  The ticket header `113` that we are interested in was assigned to Southampton *and* Chersbourg passengers, though there are large majority of Southhampton passengers for this ticket header.  While I was working on the problem, I stumbled on [this](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html), which gives the port of embarkation(PoE) as Southhampton for both these passengers.\n\nHow could this value be imputed if there weren't this additional source?  I think using the mode to impute this categorical variable would be the best option.  In this example, using the mode for ticket_prefix would land us the right value of 'S'.  Of course, other subgroup averages are possible.  For example, we could use the mode for first class female passengers,","3900e04f":"For roughly 100 parameter configurations, it takes about a minute to compute the optimum configuration.  Given that this rate appears to increase at a superlinear rate, the 1000's of configurations we may want to consider cannot be practically computed.  Below I plot the score vs. parameter value graphs for six variables.  From this plot, I can estimate a reasonable range for each parameter to try in `GridSearchCV`.  This method will cut down on the time calculating optimum parameters.  Of course, the downside is that the optimum parameters may still very well fall outside of these ranges.","040860f8":"<a name=\"Imputing\"><\/a>\n### 2.3 Imputing Missing Age Values\nBefore we impute age values, I want to see if any information about a passenger's age can be gleaned from their title.","bc17943a":"I'll tweak certain parameters one-by-one and repeat this process, looking for an increase in mean validation score.  Setting the random_state is useful here to prevent random fluctuations in mean validation score when comparing different parameter combinations.","43a8a3eb":"<a name=\"Conclusion\"><\/a>\n## Conclusion\nMy prediction gave a `0.799` precision rate, which is okay, though I was hoping for a little higher.  I think the parameter tuning method I used above could be tweaked to work better than it did in this case.","530572c5":"I'm setting the random state variable to prevent random fluctuations appearing significant.  I won't use it in the final classifier.","8a40e4b7":"Clearly titles like 'Master' applied to small children but all titles, have a distinct and quite different median.  We can divide these subgroups further,","141b607e":"<a name=\"Looking\"><\/a>\n### 1.2 Looking for NaN's\n\nNext, let's look at how complete the dataset is.","1b7c7c0c":"<a name=\"Converting\"><\/a>\n### 3.1 Converting categorical variable labels\nThe `RandomForestClassifier` in `sklearn` does not accept string labels for categorical variables.  The `LabelEncoder` module can be used to convert categorical names into numerical names.  Below, I use `LabelEncoder` and also split the training set further into a 70\/30% training\/test for assessing algorithm parameters using `GridSearchCV`.","59f2374a":"<a name=\"Loading\"><\/a>\n### 1.1 Loading and Checking Data","c39c6e4c":"I wonder if the ticket number corresponds to port of embarkation.  Since the ticket number is a 6 digit number, perhaps the tickets were issued sequentially by port. (i.e. lower ticket numbers correspond to passengers who embarked at Southhampton (first port) while higher ticket numbers correspond to Queenstown passengers).","ca20805b":"So most first class passengers had their cabin numbers recorded in the dataset, yet only a small fraction of 2nd and 3rd class passengers had theirs recorded.  If we consider survival also,","62c2b8e0":"However, this mode would assign the NaN values the wrong value of 'C'.  It's not clear which subgroup mode to use but I would say that groups like ticket number and fare are better, in this case, for imputing PoE since these variables are related directly, whereas *passenger characteristics* like class and gender are only circumstantially related.","a189a5a1":"So all tickets starting with '370' were priced at 7.75 for passenger's traveling alone.  I will set the passengers fare as 7.75,","f901e2d7":"Let's examine fare NaN's now,","d8c4b1a1":"So there are four variables with NaN values: *age, cabin, embarked*, and *fare*.","03266e2b":"As a side note, it's interesting that deck E contained 1st, 2nd, and 3rd class cabins.  There's definitely more that can be gleaned from these data, but that's as far as I want to go into it here.  I'll remove this column from the set,"}}