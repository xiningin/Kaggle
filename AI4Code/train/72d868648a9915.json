{"cell_type":{"15334648":"code","f9ca74b9":"code","c3698c51":"code","83b713e4":"code","51af4d15":"code","4eef5c25":"code","ddb8023c":"code","0b60f750":"code","9ef1e491":"code","f4951844":"code","aaca1925":"code","f52c8d4c":"code","a2709274":"code","c49167cc":"code","67a42198":"code","c0304acc":"code","8f7f8a1e":"code","4e64db74":"code","9141165e":"code","f62e4d34":"code","b90bf52d":"code","e37872a0":"code","6d0abeae":"markdown","66228b60":"markdown","11237c25":"markdown","29ebd2c7":"markdown","4ac39daa":"markdown","cfdde4ac":"markdown","a60a2839":"markdown","f438e3ee":"markdown","34199fe5":"markdown","51fbde34":"markdown","f4168119":"markdown","e69f1f95":"markdown","762cab4b":"markdown","a45a8bb1":"markdown","693cc6aa":"markdown","1762353b":"markdown","ec0c64fa":"markdown","e02ca8a1":"markdown","34bfb359":"markdown","b53b85d9":"markdown","0ca792f1":"markdown","2a6207f2":"markdown","46a001d6":"markdown","ffad5c09":"markdown","d58e80fe":"markdown","4db6a5b9":"markdown"},"source":{"15334648":"import os\n\n# Keras functions\nimport tensorflow as tf\n\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Conv2D, Conv2DTranspose, Reshape, Flatten\nfrom keras.layers import Dropout, LeakyReLU, BatchNormalization\nfrom keras.layers import Activation, ZeroPadding2D, UpSampling2D\nfrom keras.layers import Input, Reshape\nfrom matplotlib import pyplot\nfrom IPython.display import clear_output\n\n# Numpy functions\nimport numpy\nimport numpy as np\nfrom numpy import expand_dims\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import vstack\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import asarray\n\n#Torchvision for fast and easy loading and resizing\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\n","f9ca74b9":"dataset_path = \"\/kaggle\/input\/best-artworks-of-all-time\/resized\/\"\nsaved_path = '\/kaggle\/working\/training_data.npy'\n\nWIDTH = 64\nHEIGHT = 64\nIMG_SIZE = (WIDTH,HEIGHT)\nEPOCHS = 250\n\n#Look for saved file to save time loading and processing images between runs\nprint(\"Looking for previously saved file...\")\n\nif not os.path.isfile(saved_path):\n    print(\"\\n File not found, loading new data...\\n\")\n    dataset = []\n    transform_ds = transforms.Compose([transforms.Resize(IMG_SIZE),]) #define transformation\n    \n    image_folder = torchvision.datasets.ImageFolder(root=dataset_path,\n                                     transform=transform_ds)\n\n    print('Number of artworks found: ',len(image_folder))\n    \n    \n    print(\"Converting images, this will take a few minutes\")\n    for i in range (len(image_folder)):\n        image_array = numpy.array(image_folder[i][0])\n        dataset.append(image_array)\n        if (i%500 == 0):\n            print(\"Pictures processed: \", i)\n            \n    print(\"Saving dataset binary file...\")\n    dataset = np.array(dataset, dtype=np.float32)\n    dataset = (dataset - 127.5) \/ 127.5 #Normalize to [-1 , 1]\n    numpy.save(saved_path, dataset)  #Save processed images as npy file\n\nelse:\n    print(\"Data found, loading..\")\n    dataset = np.load(saved_path) \n\nprint(\"Dataset length: \", len(dataset))","c3698c51":"dataset[3].shape\n","83b713e4":"#Use a TensorFlow Dataset to manage the images for easy shuffling, dividing etc\nBATCH_SIZE = 32\n\ntraining_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(9000).batch(BATCH_SIZE)","51af4d15":"import pandas as pd\n\nartists = pd.read_csv('\/kaggle\/input\/best-artworks-of-all-time\/artists.csv')\nartists","4eef5c25":"artists[['name','genre','paintings']]","ddb8023c":"top_genres = artists['genre'].value_counts()\ntop_genres[:5]","0b60f750":"no_of_paintings = artists[['name','paintings','genre']].sort_values(ascending=False, by='paintings')\nno_of_paintings","9ef1e491":"import matplotlib.pyplot as plt\nimport random\n\nnp.random.shuffle(dataset) #Shuffle the images\n\nfig = plt.figure(figsize=(12,12))\nfor i in range(1,37):\n    fig.add_subplot(6,6,i)\n    plt.imshow(dataset[i])\n    plt.axis('off')","f4951844":"def build_generator(seed_size, channels):\n    \n    model = Sequential()\n\n    model.add(Dense(64*64,activation=\"relu\",input_dim=seed_size)) #64x64 units\n    model.add(Reshape((4,4,256)))\n\n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    \n    model.add(UpSampling2D())\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    \n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n   \n    # Optional additional upsampling goes here\n    model.add(UpSampling2D(size = (2,2))) #4,4 for 128x128, 2,2 for 64x64\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    \n    model.add(UpSampling2D(size=(2,2)))\n    model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n\n    model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n    model.add(Activation(\"tanh\"))\n    \n    return model\n\n","aaca1925":"def build_discriminator(image_shape):\n    model = Sequential()\n\n    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n                     padding=\"same\"))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Conv2D(512, kernel_size=3, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(LeakyReLU(alpha=0.2))\n\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model","f52c8d4c":"SEED_SIZE = 100\nIMAGE_CHANNELS = 3\n\ngenerator = build_generator(SEED_SIZE,IMAGE_CHANNELS)\n\nnoise = tf.random.normal([1,SEED_SIZE])\ngenerated_image = generator(noise,training=False)\n\nplt.imshow(generated_image[0, :, :, 0])\n\n","a2709274":"image_shape = (HEIGHT, HEIGHT, IMAGE_CHANNELS)\n\ndiscriminator = build_discriminator(image_shape)\ndecision = discriminator(generated_image)\nprint(decision)","c49167cc":"cross_entropy = tf.keras.losses.BinaryCrossentropy()\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n","67a42198":"generator_optimizer = tf.keras.optimizers.Adam(1.2e-4, 0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4, 0.5)","c0304acc":"GENERATED_ROWS = 7\nGENERATED_COLS = 7\nMARGIN = 2 # Give the images a little frame\n\ndef save_images(cnt, noise):\n    #Define the \"base\" of the saved image as a big black canvas\n    image_array = np.full(( \n      MARGIN + (GENERATED_ROWS * (WIDTH + MARGIN)), \n      MARGIN + (GENERATED_COLS * (HEIGHT + MARGIN)), 3), \n      0, dtype=np.uint8)\n    \n    generated_images =  generator.predict(noise)\n    \n    image_count = 0\n    for row in range(GENERATED_ROWS):\n        for col in range(GENERATED_COLS):\n            r = row * (WIDTH + 2) + MARGIN\n            c = col * (HEIGHT + 2) + MARGIN\n            image_array[r:r+WIDTH , c:c+HEIGHT] = generated_images[image_count] * 127.5 + 127.5\n            image_count += 1\n            \n    output_path  = os.path.join(\"\/kaggle\/working\/generated2\/\")\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n        \n    filename = os.path.join(output_path,f\"train-{cnt}.png\")\n    im = Image.fromarray(image_array)\n    im.save(filename)","8f7f8a1e":"# This goes a bit deeper \"behind the scenes\" and manually sets the gradients\n# instead of letting Keras and TensorFlow set it automatically. This is because\n# the two models need to be trained separately\n@tf.function\ndef train_step(images):\n    seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(seed, training = True)\n        \n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n        \n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n        \n        \n        gradients_of_generator = gen_tape.gradient(\n            gen_loss,\n            generator.trainable_variables\n        )\n        gradients_of_discriminator = disc_tape.gradient(\n            disc_loss, \n            discriminator.trainable_variables\n        )\n        \n        generator_optimizer.apply_gradients(zip(gradients_of_generator,\n                                                generator.trainable_variables))\n        \n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, \n                                                    discriminator.trainable_variables))\n        \n        return gen_loss, disc_loss","4e64db74":"\ndef train(dataset, epochs):\n    # Use a fixed seed for the saved images so we can watch their development\n    fixed_seed = np.random.normal(0, 1, (GENERATED_ROWS * GENERATED_COLS, SEED_SIZE))\n    \n    for epoch in range(epochs):\n        gen_loss_list = []\n        disc_loss_list = []\n        \n        for image_batch in dataset:\n            t = train_step(image_batch)\n            gen_loss_list.append(t[0])\n            disc_loss_list.append(t[1])\n            \n        g_loss = sum(gen_loss_list) \/ len(gen_loss_list) #calculate losses\n        d_loss = sum(disc_loss_list) \/ len(disc_loss_list)\n        \n        print(f'Epoch {epoch+1}, gen loss = {g_loss}, disc loss = {d_loss}')\n        \n        save_images(epoch, fixed_seed)\n        ","9141165e":"train(training_dataset, EPOCHS)","f62e4d34":"!zip -r images.zip \"\/kaggle\/working\/generated2\/\"","b90bf52d":"from IPython.display import FileLink\nFileLink(r'images.zip')","e37872a0":"seed2 = tf.random.normal([BATCH_SIZE, SEED_SIZE])\ngenerated_images2 = generator(seed2, training = True)\n\ngenerated_images2 =0.5 * generated_images2 + 0.5\n\nfig = plt.figure(figsize=(10,10))\nfor i in range(1,21):\n    fig.add_subplot(5,5,i)\n    plt.imshow(generated_images2[i])\n    plt.axis('off')\n","6d0abeae":"**Beautiful!**\nThere's some distortion from the rescaling and they look all weird normalized, this is due to plt.imshow only displaying ranges [0,1] and these are normalized to [-1,1] . If we were to not normalize them at the beginning they would look normal.","66228b60":"Here we can check out the top 5 genres and artists","11237c25":"Here, we'll check out the dataset and what it looks like","29ebd2c7":"Van Gogh has been busy I see! Lets try printing some of the paintings to see what we're dealing with.","4ac39daa":"The generator will take in a random seed and generate an image from that seed, upsampling as much as needed. Note that increasing resolution will require retuning of the models as well as increase training time by a lot. I found that the generator needs to have quite a lot of capacity in order to keep up with the discriminator at later stages of training.\n\narchitecture based on the findings and guidelines proposed in the paper:\n\n* Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n* Use batchnorm in both the generator and the discriminator.\n* Remove fully connected hidden layers for deeper architectures\n* Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n* Use LeakyReLU activation in the discriminator for all layers.\n\nFurther increasing conv layers will lead to mode collapse from what I've tried","cfdde4ac":"Classify this random image with the untrained discriminator, the prediction will likely be 50%","a60a2839":"# Import required libraries","f438e3ee":"# Build the models","34199fe5":"Here we use the untrained generator to generate an image (which will be very random)","51fbde34":"# Art generating GAN\n\nA Generative Adversarial Network for generating new art","f4168119":"# Load and pre-process the data","e69f1f95":"There's some data that we dont really need for this task, like their nationality, wikipedia page and bio. Below, we filter the data to more clearly see the artists along with their corresponding genre and number of paintings in a more compact manner. As you can see, we have quite the mix of genres which will hopefully give our model a good base to generate from.","762cab4b":"**Author: Isak \u00c5man Larsson**\n\nCode for training based on DCGAN tutorial at \u201cDeep Convolutional Generative Adversarial Network | TensorFlow Core.\u201d TensorFlow, https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan\n\nAnd \"Unsupervised representation learning\nwith deep convolutional\ngenerative adversarial networks\", Radford. A, Metz. L, Facebook AI Research, 2016 (https:\/\/arxiv.org\/abs\/1511.06434)\n\n","a45a8bb1":"# Define loss functions for the models","693cc6aa":"Confirm that the images are the correct shape.","1762353b":"**The discriminators loss** is based on its ability to distinguish real images from fakes. It compares its predictions on real images to an array of ones (remember 1 being real) and its predictions on fake images to an array of zeros (0 being fake). The goal is to classify all real images as 1 and all fakes as 0. The total loss is then these two losses added together.\n\n**The generators loss** is a measurement of how good it performed at fooling the discriminator. If the discriminator classifies the fake images as 1, the generator did a good job.\n","ec0c64fa":"# Train the models\n\nThe training begins by providing a random seed to the generator, which is then used to generate an image. The discriminator then classifies images from both the fake and real dataset. The loss is calculated separately for each model and the gradients are updated.\n\nBased on the TensorFlow DCGAN tutorial","e02ca8a1":"Train the models, ideally we want to see the losses hovering at around the same values, indicating that they are both improving at similar rates","34bfb359":"****Note: These genres are only sorted from the number of artists belonging to that genre, not the number of paintings.****","b53b85d9":"# Bonus: printing a random set of pictures generated by the trained generator\n\nJust for playing with it to see what we find, generates a new seed everytime","0ca792f1":"The discriminator is simply a CNN based image classifier which will output values of what it thinks is fake or real with 0 being fake and 1 being real","2a6207f2":"# Explore the data","46a001d6":"# Zip images for download","ffad5c09":"We'll sort the list of artists by number of paintings instead to get a better overview of which artist has the most painings in the dataset.","d58e80fe":"The two models optimizers are separated because we train them separately. I found a slightly lower generator LR to be beneficial.\n\nBeta value of 0.5 generated more stable models as per the findings in the paper \"Unsupervised representation learning with deep convolutional generative adversarial networks\"","4db6a5b9":"# Function for saving the generated images"}}