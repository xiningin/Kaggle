{"cell_type":{"02088db2":"code","1c12a0fe":"code","74d2e087":"code","1c0c5b4f":"code","cd68f041":"code","0d05b52d":"code","99b79699":"code","c133a460":"code","b0c3d63a":"code","c4f230c1":"code","7fc2f95e":"code","b5af08a0":"code","f9d829be":"code","1b33e790":"code","f43e1e8f":"code","c2418809":"code","c46169aa":"code","8db6a52d":"code","f06df8d3":"code","0a1c62ea":"code","c41c8441":"code","2a5d733a":"code","e17e6f7c":"code","60cf9726":"code","8ac83869":"code","af4dc0f0":"code","f002a9ac":"code","68220612":"code","412adf1b":"code","6ffe7944":"code","5f597564":"code","f06889f5":"code","86f8de23":"code","c153c170":"code","2873792d":"code","4cfbe652":"code","b5b201a1":"code","5466d718":"code","d8daf552":"markdown","61dbc77b":"markdown","b4e6a9aa":"markdown","bc0cfffa":"markdown","e04f5d21":"markdown","5744c86b":"markdown","22ee2023":"markdown","a06c3e3f":"markdown","53a5daad":"markdown","8e461f47":"markdown","40247b11":"markdown","ed5621c2":"markdown","01ebaabb":"markdown","f39996b6":"markdown","3145421c":"markdown","da3bfad2":"markdown","95e2103c":"markdown","ab6a6519":"markdown","d436bebd":"markdown","d3918e06":"markdown","de7ac971":"markdown","1c51775e":"markdown","d83b34ab":"markdown","78d500e0":"markdown","8da34bd9":"markdown","12070c89":"markdown","61455d47":"markdown","2772dc36":"markdown","94557bdc":"markdown","a14a4a5d":"markdown","cf5c1a1e":"markdown","0a485fa8":"markdown","67826c9f":"markdown","14f31d2b":"markdown","70d7e449":"markdown","f1bd7ca3":"markdown","f1fa10a1":"markdown","ae0f662a":"markdown","0b183a86":"markdown","a7052d9c":"markdown","7f00f39a":"markdown","1abd28ab":"markdown"},"source":{"02088db2":"import numpy as np\nimport pandas as pd \nimport matplotlib\nimport seaborn as sns\nimport holoviews as hv\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d.axes3d as p3\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.utils import resample,shuffle\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.preprocessing import minmax_scaling\nfrom sklearn.decomposition import PCA,SparsePCA,KernelPCA,NMF\nfrom holoviews import opts\nfrom sklearn import metrics, mixture, cluster, datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,roc_auc_score, roc_curve, auc\nhv.extension('bokeh')\n\nprint('Libraries correctly loaded')","1c12a0fe":"df_path = \"..\/input\/health-insurance-cross-sell-prediction\/train.csv\"\ndf_test_path = \"..\/input\/health-insurance-cross-sell-prediction\/train.csv\"\ndf = pd.read_csv(df_path)\ndf_test = pd.read_csv(df_test_path)\n\nprint('Number of rows: '+ format(df.shape[0]) +', number of features: '+ format(df.shape[1]))","74d2e087":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nMissing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\nprint(\"The number of missing entries before cleaning: \" + str(round(Missing_Percentage,5)) + \" %\")","1c0c5b4f":"CategoricalVariables","cd68f041":"df.Vehicle_Age.unique()","0d05b52d":"Vehicle_Age_map  = {'< 1 Year':0,'1-2 Year':1,'> 2 Years':2}\n\ndf['Vehicle_Age'] = df['Vehicle_Age'].map(Vehicle_Age_map)\ndf=df.set_index(\"id\")\n\nC = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\ndf.head()","99b79699":"def highlight_cols(s, coldict):\n    if s.name in coldict.keys():\n        return ['background-color: {}'.format(coldict[s.name])] * len(s)\n    return [''] * len(s)\n\ndef ExtractColumn(lst,j): \n    return [item[j] for item in lst] ","c133a460":"coldict = {'Gender':'lightcoral','Age':'lightcoral', 'Driving_License':'lightsalmon', 'Region_Code':'lightsalmon', 'Previously_Insured':'lightsalmon'\n           , 'Vehicle_Age':'lightsalmon', 'Vehicle_Damage':'lightsalmon', 'Annual_Premium':'lightsalmon', 'Policy_Sales_Channel':'tomato'\n           ,'Vintage':'tomato','Response':'darksalmon'}\ndf.iloc[0:5].style.apply(highlight_cols, coldict=coldict)","b0c3d63a":"df_dummy = pd.get_dummies(df[CategoricalVariables], columns=CategoricalVariables)\ndf_numeric = df[NumericVariables]\ndf_final = pd.merge(df_numeric,df_dummy,on='id')\n\nresponse = ['Response']\nVariablesNoTarget = [x for x in df_final.columns if x not in response]\nprint(\"Dummy transformation was successful\")","c4f230c1":"coldict_dummy = {'Gender_Female':'lightcoral','Gender_Male':'lightcoral','Age':'lightcoral', 'Driving_License':'lightsalmon' \n                   ,'Region_Code':'lightsalmon', 'Previously_Insured':'lightsalmon'\n                   , 'Vehicle_Age':'lightsalmon', 'Vehicle_Damage_No':'lightsalmon', 'Vehicle_Damage_Yes':'lightsalmon', 'Annual_Premium':'lightsalmon'\n                   , 'Policy_Sales_Channel':'tomato','Vintage':'tomato','Response':'darksalmon'}\n\ndf_final = df_final[['Age','Gender_Female','Gender_Male','Driving_License','Previously_Insured','Vehicle_Age','Region_Code','Vehicle_Damage_No'\n               ,'Vehicle_Damage_Yes','Annual_Premium','Policy_Sales_Channel','Vintage','Response']]\n\ndf_final[VariablesNoTarget] = minmax_scaling(df_final, columns=VariablesNoTarget)\ndf_final.iloc[0:5].style.apply(highlight_cols, coldict=coldict_dummy)","7fc2f95e":"SpearmanCorr = df_final.corr(method=\"spearman\")\nmatplotlib.pyplot.figure(figsize=(10,10))\nsns.heatmap(SpearmanCorr, vmax=.9, square=True, annot=True, linewidths=.3, cmap=\"YlGnBu\", fmt='.1f')","b5af08a0":"age = hv.Distribution(df['Age'],label=\"Age\").opts(color=\"red\")\nreg = hv.Distribution(df['Region_Code'],label=\"Region_Code\").opts(color=\"green\")\nprem = hv.Distribution(df['Annual_Premium'],label=\"Annual_Premium\").opts(color=\"yellow\")\nchan = hv.Distribution(df['Policy_Sales_Channel'],label=\"Policy_Sales_Channel\").opts(color=\"blue\")\nvehage = hv.Distribution(df['Vehicle_Age'],label=\"Vehicle_Age\").opts(color=\"purple\")\nvin = hv.Distribution(df['Vintage'],label=\"Vintage\").opts(color=\"pink\")\n\n(age + reg + prem + chan + vehage + vin).opts(opts.Distribution(xlabel=\"Values\", ylabel=\"Density\", width=400, height=300,tools=['hover'],show_grid=True)).cols(3)","f9d829be":"pca = PCA().fit(df_final[VariablesNoTarget])\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5), dpi=70, facecolor='w', edgecolor='k')\nax0, ax1 = axes.flatten()\n\nsns.set('talk', palette='colorblind')\n\nfont = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\n\nax0.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\nax0.set_xlabel('Number of components')\nax0.set_ylabel('Cumulative explained variance');\n\nax1.bar(range(df_final[VariablesNoTarget].shape[1]),pca.explained_variance_)\nax1.set_xlabel('Number of components')\nax1.set_ylabel('Explained variance');\n\nplt.tight_layout()\nplt.show()","1b33e790":"n_PCA_90 = np.size(np.cumsum(pca.explained_variance_ratio_)>0.9) - np.count_nonzero(np.cumsum(pca.explained_variance_ratio_)>0.9)\nprint(\"Already: \" + format(n_PCA_90) + \" components cover 90% of variance.\")","f43e1e8f":"#KPCA = KernelPCA(n_components = df_final[VariablesNoTarget].shape[1], kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n#KPCA_fit = KPCA.fit(df_final[VariablesNoTarget])\n#X_KPCA = KPCA.fit_transform(df_final[VariablesNoTarget])\n#X_KPCA_back = KPCA.inverse_transform(X_KPCA)","c2418809":"pca = PCA(4).fit((df_final[VariablesNoTarget]))\n\nX_pca=pca.transform((df_final[VariablesNoTarget])) \n\nplt.matshow(pca.components_,cmap='viridis')\nplt.yticks([0,1,2,3,4],['1st Comp','2nd Comp','3rd Comp','4th Comp'],fontsize=10)\nplt.colorbar()\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11],VariablesNoTarget,fontsize=10,rotation=30)\nplt.tight_layout()\nplt.show()","c46169aa":"PCA_vars = [0]*len(VariablesNoTarget)\n\nfor i, feature in zip(range(len(VariablesNoTarget)),VariablesNoTarget):\n    x = ExtractColumn(pca.components_,i)\n    if ((max(x) > 0.4) | (min(x) < -0.4)):\n        if abs(max(x)) > abs(min(x)):\n            PCA_vars[i] = max(x)\n        else:\n            PCA_vars[i] = min(x)                 \n    else:\n        PCA_vars[i] = 0\n\nPCA_vars = pd.DataFrame(list(zip(VariablesNoTarget,PCA_vars)),columns=('Name','Max absolute contribution'),index=range(1,13,1))      \nPCA_vars = PCA_vars[(PCA_vars['Max absolute contribution']!=0)]\nPCA_vars","8db6a52d":"df_business = df_final[['Region_Code','Annual_Premium','Policy_Sales_Channel','Vintage']]\nX = df_business.values\n\nGM_n_components = np.arange(1, 8)\nGM_models = [mixture.GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in GM_n_components]\n\nplt.figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='r')\nplt.plot(GM_n_components, [m.aic(X) for m in GM_models], label='AIC')\nplt.tight_layout()\nplt.legend(loc='best')\nplt.xlabel('n_components');","f06df8d3":"GM_n_classes = 2\n\nGMcluster = mixture.GaussianMixture(n_components=GM_n_classes, covariance_type='full',random_state = 0)\nGMcluster_fit = GMcluster.fit(df_business)\nGMlabels = GMcluster_fit.predict(df_business)\n\nprint('Number of clusters: ' + format(len(np.unique(GMlabels))))","0a1c62ea":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), facecolor='w', edgecolor='k')\nax = p3.Axes3D(fig)\nax.view_init(10, 40)\nfor l in np.unique(GMlabels):\n    ax.scatter(X[GMlabels == l, 0], X[GMlabels == l, 1], X[GMlabels == l, 2],color=plt.cm.jet(float(l) \/ np.max(GMlabels + 1)),s=20, edgecolor='k')\nplt.title('Expectation-maximization algorithm for business features clustering' )\n\nplt.show()","c41c8441":"df_final[['Business_Cluster']] = list(GMlabels)\n\ncoldict_cluster_1 = {'Gender_Female':'lightcoral','Gender_Male':'lightcoral','Age':'lightcoral', 'Driving_License':'lightsalmon' \n                   ,'Region_Code':'lightsalmon', 'Previously_Insured':'lightsalmon'\n                   , 'Vehicle_Age':'lightsalmon', 'Vehicle_Damage_No':'lightsalmon', 'Vehicle_Damage_Yes':'lightsalmon', 'Annual_Premium':'lightsalmon'\n                   , 'Policy_Sales_Channel':'tomato','Vintage':'tomato','Response':'darksalmon','Business_Cluster':'pink'}\n\ndf_final.iloc[0:5].style.apply(highlight_cols, coldict=coldict_cluster_1)","2a5d733a":"VariablesClient = [x for x in df_final[VariablesNoTarget] if x not in df_business.columns]\n\ndf_client = df_final[VariablesClient]\nX = df_client.values\n\nGMcluster_fit = GMcluster.fit(df_client)\nGMlabels = GMcluster_fit.predict(df_client)\n\ndf_final[['Client_Cluster']] = list(GMlabels)\n\ncoldict_cluster_2 = {'Gender_Female':'lightcoral','Gender_Male':'lightcoral','Age':'lightcoral', 'Driving_License':'lightsalmon' \n                   ,'Region_Code':'lightsalmon', 'Previously_Insured':'lightsalmon'\n                   , 'Vehicle_Age':'lightsalmon', 'Vehicle_Damage_No':'lightsalmon', 'Vehicle_Damage_Yes':'lightsalmon', 'Annual_Premium':'lightsalmon'\n                   , 'Policy_Sales_Channel':'tomato','Vintage':'tomato','Response':'darksalmon','Business_Cluster':'pink','Client_Cluster':'pink'}\n\ndf_final = df_final[['Age','Gender_Female','Gender_Male','Driving_License','Previously_Insured','Vehicle_Age','Region_Code','Vehicle_Damage_No'\n               ,'Vehicle_Damage_Yes','Annual_Premium','Policy_Sales_Channel','Vintage','Business_Cluster','Client_Cluster','Response']]\n\ndf_final.iloc[0:5].style.apply(highlight_cols, coldict=coldict_cluster_2)","e17e6f7c":"Target= df_final['Response']\ndf_final_ = df_final.drop(['Response'],axis=1)\n\nx_train,x_test,y_train,y_test = train_test_split(df_final_,Target,test_size=0.2,random_state=0)","60cf9726":"ModelAverage = y_train.mean()\nprint(str(round(ModelAverage,5)))","8ac83869":"GLM = LogisticRegression(solver='liblinear', random_state=0)\nGLM_fit = GLM.fit(x_train, y_train)\nGLM_probability = pd.DataFrame(GLM_fit.predict_proba(x_test))\nGLM_probability.mean()","af4dc0f0":"print(\"We expect: \" +format(round((float(GLM_probability[1].mean() * x_test.shape[0]))))+ \" 1's.\")","f002a9ac":"GLM_clas = pd.DataFrame(GLM_fit.predict(x_test))\nprint(\"The rate is very low: \"+ format(float(round(GLM_clas.mean(),5))) + \" and translates to just: \" + format(float(GLM_clas.mean() * x_test.shape[0])) + \" records with 1's.\")","68220612":"GLM_Ret = hv.Distribution(GLM_probability[1],label=\"Probability of retention\").opts(color=\"blue\")\n(GLM_Ret).opts(opts.Distribution(xlabel=\"Values\", ylabel=\"Density\", width=600, height=400,tools=['hover'],show_grid=True))","412adf1b":"df_majority = df_final[df_final['Response']==0]\ndf_minority = df_final[df_final['Response']==1]\ndf_minority_upsampled = resample(df_minority,replace=True,n_samples=334399,random_state = 0)\nbalanced_df = pd.concat([df_minority_upsampled,df_majority])\nbalanced_df = shuffle(balanced_df)\nbalanced_df.Response.value_counts()","6ffe7944":"Target= balanced_df['Response']\ndf_final_ = balanced_df.drop(['Response'],axis=1)\n\nx_train,x_test,y_train,y_test = train_test_split(df_final_,Target,test_size=0.2,random_state=0)\n\nGLM = LogisticRegression(solver='liblinear', random_state=0)\nGLM_fit = GLM.fit(x_train, y_train)\nGLM_clas = pd.DataFrame(GLM_fit.predict(x_test))\nGLM_probability = pd.DataFrame(GLM_fit.predict_proba(x_test))","5f597564":"fpr, tpr, _ = roc_curve(y_test, GLM_fit.predict_proba(x_test)[:,1])\n\nplt.title('Logistic regression ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ' ,format(round(auc(fpr,tpr),5)))","f06889f5":"df_evaluation = pd.DataFrame(y_test)\ndf_evaluation[['GLM']] = list(GLM_fit.predict(x_test))","86f8de23":"fpr, tpr, _ = roc_curve(y_test, x_test['Client_Cluster'])\n\nplt.title('Client Cluster ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', format(round(auc(fpr,tpr),5)))","c153c170":"InvertClientCluster = x_test['Client_Cluster'].replace(0,-1)\nInvertClientCluster = InvertClientCluster.replace(1,0)\nInvertClientCluster = InvertClientCluster.replace(-1,1)\nInvertClientCluster\n\nfpr, tpr, _ = roc_curve(y_test, x_test['Business_Cluster'])\n\nplt.title('Business Cluster ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', format(round(auc(fpr,tpr),5)))","2873792d":"space={ 'max_depth': hp.quniform(\"max_depth\", 3,18,1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 300,\n        'seed': 0}\n\ndef objective(space):\n    clf=XGBClassifier(n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                      reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                      colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( x_train, y_train), ( x_test, y_test)]\n    \n    clf.fit(x_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n    pred = clf.predict(x_test)\n    y_score = clf.predict_proba(x_test)[:,1]\n    accuracy = accuracy_score(y_test, pred>0.5)\n    Roc_Auc_Score = roc_auc_score(y_test, y_score)\n    print (\"ROC-AUC Score: \",Roc_Auc_Score)\n    print (\"SCORE: \", accuracy)\n    return {'loss': -Roc_Auc_Score, 'status': STATUS_OK }\n\n#trials = Trials()\n\n#best_hyperparams = fmin(fn = objective,space = space,algo = tpe.suggest,max_evals = 100,trials = trials)\n\n#print(\"The best hyperparameters are : \",\"\\n\")\n#print(best_hyperparams)","4cfbe652":"XGB_=XGBClassifier(n_estimators = 300, max_depth = 13, gamma = 2.3408807913619945, reg_lambda = 0.3770436657913232,\n                            reg_alpha = 40.0, min_child_weight=7.0,colsample_bytree = 0.5786479102658189 ,random_state = 0)\nXGB_fit = XGB_.fit(x_train, y_train)\nXGB_probability = XGB_fit.predict_proba(x_test)[:,1]\nXGB_class = pd.DataFrame(XGB_fit.predict(x_test))","b5b201a1":"df_evaluation[['XGB']] = list(XGB_fit.predict(x_test))","5466d718":"fpr, tpr, _ = roc_curve(y_test, XGB_probability)\n\nplt.title('XGBoost ROC curve')\nplt.xlabel('FPR (Precision)')\nplt.ylabel('TPR (Recall)')\n\nplt.plot(fpr,tpr)\nplt.plot((0,1), ls='dashed',color='black')\nplt.show()\nprint ('Area under curve (AUC): ', format(round(auc(fpr,tpr),5)))","d8daf552":"Precisely it means that 12% of customers are interested in this corss-selled product. Alright, we have benchmark.","61dbc77b":"# 3.Data analysis\n\nFirst, I would like to granularly describe what I observe in the data and how I will approach it:","b4e6a9aa":"Three of them vastly dominante the picture. However, looking into the table, they are distributed well between 5 clusters. I merge this cluster into data:","bc0cfffa":"**Factors distributions**\n\nThe distribution of factors is relevant for real understanding underlying data. Actuaries tend also to fit statistical distributions ([more](https:\/\/en.wikipedia.org\/wiki\/Probability_density_function)) to particular factors.","e04f5d21":"# Introduction\n\nThe notebook is devoted to cross-selling investigation with granular actuarial background. Techncially, I aim on testing some different methods by use of logistic link for the binary response.\n\n**Majority of the code was hidden, click at right to uncover it.**\n\n**If you are interested only in actuarial sciences, skip the code. If you aim only on understanding modeling, skip actuarial background passages.**","5744c86b":"In this chapter we enriched the data set by defining two clusters which operate on the different subsets and aim on capturing different relations. We will see whether these clusters appear to be useful in further analysis.","22ee2023":"I fit the model with the best parameters:","a06c3e3f":"I hope you enjoyed the notebook. If so, please upvote.","53a5daad":"![image.png](attachment:image.png)","8e461f47":"# 4.Basic models estimation\n\nThis part I would like to start from repeating one message: in Principial Component part we have already predefined candidates for the best performance. All models which don't select variables automatically can be fed by this list. Furthermore, to start with the topic, I will define the simplest model in the world: 'average', as the benchmark of further tools' quality.\n\nFirst, I split the train data set into two parts:\n* Train\n* Validation","40247b11":"To break it down in even more readable way:","ed5621c2":"# 1.Actuarial background","01ebaabb":"Our 'Business cluster' joined the data. I would like to create same cluster for policyholder, and call it 'Client cluster'. I repeat the procedure.","f39996b6":"AUC at this level is quite satisfying. Obviously, extreme boosting overperformed logistic regression. \n\n**Actuarially speaking: it's of course relevant to have models with high predictive power, however every insurer's tariff has to be transparent and possible to be explained to country's financial regulator. Hence, what is usually done is trends \/ importance detection by use of ML\/DL methods, but the outputs are not directly applied to insurer's model but rather insight is used for GLM modeling.**\n\nWhat would be done in this case: we would export importance strcuture from extreme boosting \/ GBM and applied findings to Generalized Linear Model \/ Logistic regression manually.","3145421c":"Definitely, 'vehicle age' has to be transformed to numeric structure.","da3bfad2":"![image.png](attachment:image.png)","95e2103c":"**Logistic regression**\n\nI will fit first real model, namely logistic regression looking and probability predicted by the model:","ab6a6519":"Interesting results. Whereas business cluster has compeltely neutral performance (it is perfectly random), client cluster is already quite powerful.","d436bebd":"**Correlation check**\n\nI use Spearman correlation as its rank nature counter-acts for linearity assumption ([more info in my correlation notebook](https:\/\/www.kaggle.com\/jjmewtw\/yt-pearson-spearman-distance-corr-rv-coef)).","d3918e06":"To sum up:\n* 'age' contributes to at leat two components\n* 'previously insured' contributes to at least three components\n* 'veh age' performs same as 'age'\n* 'vintage' dominates one and only one component 4th\n* 'sales channel' contributes to at leat two components\n* both 'gender' and 'vehicle damage contribute to three components\n\nAlready some information what can be useful further.","de7ac971":"**Extreme boosting**\n\nI apply very popular but also really powerful algorithm called extreme boosting for this task as well. My cross validation was inspired by [this notebook](https:\/\/www.kaggle.com\/yashvi\/vehicle-insurance-eda-and-boosting-models). Please uncomment if you would like to run (approximately 40 minutes):","1c51775e":"Alright, let me approach it differently, as the data is unbalanced I will resample it ([inspiration](https:\/\/www.kaggle.com\/ayushikaushik\/eda-oversampling-classification-roc-auc-score-93)):","d83b34ab":"**Clustering**\n\nThe aim of this part is to create the cluster variable which will group policyholders in reasonable way. First, I aim to create so-called 'business cluster' which will cover the trends from frour variables: <Region_Code,Annual_Premium,Policy_Sales_Channel,Vintage>. For this I will employ powerful tool called \"Expectation-maximization algorithm\", which belongs to clustering methods' family.","78d500e0":"I apply 2 as number of clusters to my data and look at it in 3D:","8da34bd9":"This graph works like elbow rule, it enables us to see how many clusters are needed to lower variability in the cluster, AIC ensures that the difference is relevant statistically. Regardless the results, I want to employ this unsupervised method for classification task, so 2 clusters.","12070c89":"In this analysis, we have in our portfolio approximately 400.000 policyholders with health insurance. The policyholders in this data set are then already our clients. What every insurance company aims to do is to broaden its portfolio by selling its customers another product from their offer. \n\n**As the product is sold between segments, we call it 'cross-selling' as noted in the name of the data.**\n\nFurthermore, we usually divide our protfolio into:\n* Renewals business - policyholders who just renew policy\n* New business - policyholders who will have policy from us first time\n\nIn this cross-selling investigation, we don not have any information about our exisiting vehicle portfolio. Hence, this is purely new business analysis. \n\nAnother important point is that new business modeling is always connected by so-called \"competitors' models\". Why? Our company is always one out of many present in the insurance market, so we can't treat it as lonely island. What is usually done is fitting tariff of our competitors and comparing our tariff against their ones. Hence, in this case we won't produce perfectly usable strcuture regardless the level of prediction. However, this great data enables us anyway to perform very absorbing task.","61455d47":"I look at means of both binary cases, on average they should be consistent with the data ones. I like the results at this stage. Next, I will investigate how predictied probabilities are translated to classification.","2772dc36":"The most relevant is the relation between \"Response\" and the rest of the variables. In this case I see some correlation betweenResponse and \"Vehicle_Damage\", furthermore there is a negative relation with binary \"Previously_Insured\" variable. Another interesting relations:\n* age is strongly correalted with age of car. This is well-known in actuarial world relation that young people drive old cars\n* age with sales channel, old people tend to use brokers and agents, young ones use internet\n* previously insured is correlated with age and vehicle age. Young people tend to change insurer often\n* previously insured with vehicle damage, indeed a lot of polocyholders change the insurer while they correspond to big risk\n* **vehicle age and policy sales channel - this is interesting spurious effect. The real underlying effect is relation between age and sales channel, indirectly influencing numbers for age of vehicle**","94557bdc":"What does it say to us? We face imbalanced data where the majority of observations fall into one out of binary levels. Solutions can be re-balancing or using cost-sensitive algorithm. Let's look at the probability distribution:","a14a4a5d":"Let me repeat all the steps and check the effect:","cf5c1a1e":"Alright, this works much better. Let's continue with this approach.","0a485fa8":"**Clustering**\n\nIn the chapter '3' I defined 2 different clusters: 'business cluster' and 'client cluster'. This method can be used in to ways:\n* I can directly employ clusters to my data as interesting interation-type variables\n* Alternatively, I can say that binary cluster should point at some pecularities differentiating between clients who would like to have policy and these reluctant ones","67826c9f":"# 2.Data cleaning\n\nI will perform classic set of operations starting from libraries loading, data loading, finishing at compact cleaning. I add simple comments at the end of code stages to keep it self-explonatory.","14f31d2b":"I apply regular PCA with 4 components and check what variables contribute the most:","70d7e449":"The data is ready for some checks and analysis. Some functions for the future:","f1bd7ca3":"**Dummy transformation**\n\nI have only two vategorical variables: Gender and Vehicle_Damage, each of them has only 2 levels. Transforming them I will receive only 4 new variables isntead of 2 which are currently present.","f1fa10a1":"**Principial Component Analysis**\n\nIt is relevant to understand the variability in the data set. First we will apply linear method: PCA ([more information](https:\/\/www.kaggle.com\/jjmewtw\/total-analysis-of-pca-sparse-pca-nmf-kernel-pca)) which allows for assessment how many components suffice to cover the majority of variance in the data set. For this I look at cumulative variance explained:","ae0f662a":"1. General policyholder variables regarding any insurance type\n2. Variables for car insurance portfolio\n3. Features regarding selling methodology\n4. The binary response variable informing whether the health policyholder is interested in our car product or not","0b183a86":"Hence, no NA's corrections are needed. Next, I check my categorical variables:","a7052d9c":"Age has not regular shape for non-life portfolio. It is highly skewed towards younger ages. It is hard for me to understand why it is the case, but it can be certain trend characteristic for developing countries where new generation approaches finance & health in mdoern way whereas older generations are still reluctant. Region code, sales channel and vehicle age are dispersed around some dominating points. Annyal premium has reasonable shape. Vintage is uniformly distributed, hence proably useless.","7f00f39a":"This data set is especially interesting, as it enables us to perform the analysis considered to be quite advanced in actuarial pricing world. The investigation which we aim to perfom belongs to non-life part of the insurance business, and the department responsible for this operation is Pricing Team in insurance company. Lines of insurance business belonging to this segment:","1abd28ab":"The above operation allows us to detect how many components are needed to capture the variability at satisfying level. The shortcoming of PCA is linearity assumption, to facilitate this we can employ Kernel PCA transformation for this. By applying Hilbert space transformation, we will achieve data set where this would be not a problem anymore. However, as algorithm is computationally very heavy, I left it out:"}}