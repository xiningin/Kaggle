{"cell_type":{"4ce72416":"code","0cef76ae":"code","45cf39b5":"code","4f6d0896":"code","20ecedce":"code","9ec9f4b3":"code","51f5674b":"code","b66f23c2":"code","fbabc4a6":"code","e2ec4d15":"code","d24f7042":"code","0687854c":"code","e90603be":"code","4e4b2796":"code","d8eee255":"code","5de96539":"markdown","3ef5dca2":"markdown","6113771f":"markdown","a4b3548d":"markdown","c2e50628":"markdown","4e7c6c0c":"markdown","26864191":"markdown","dad9a622":"markdown","0011cbd3":"markdown","fac87f8f":"markdown","eb746b1f":"markdown","d68e5f0f":"markdown","c8102ee7":"markdown","bcf32a2b":"markdown","79083b56":"markdown","b48c4c95":"markdown","18029a16":"markdown","2caff47a":"markdown","f6909695":"markdown","4db4da1f":"markdown"},"source":{"4ce72416":"# univariate lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequence)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the sequence\n                if end_ix > len(sequence)-1:\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","0cef76ae":"# univariate stacked lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a univariate sequence\ndef split_sequence(sequence, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequence)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the sequence\n                if end_ix > len(sequence)-1:\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","45cf39b5":"# univariate bidirectional lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Bidirectional\n\n# split a univariate sequence\ndef split_sequence(sequence, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequence)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the sequence\n                if end_ix > len(sequence)-1:\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# define model\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","4f6d0896":"# univariate cnn lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequence)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the sequence\n                if end_ix > len(sequence)-1:\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 4\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\nn_features = 1\nn_seq = 2\nn_steps = 2\nX = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n# define model\nmodel = Sequential()\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\nmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=500, verbose=0)\n# demonstrate prediction\nx_input = array([60, 70, 80, 90])\nx_input = x_input.reshape((1, n_seq, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","20ecedce":"# multivariate data preparation\nfrom numpy import array\nfrom numpy import hstack\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the dataset\n                if end_ix > len(sequences):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\nprint(dataset)\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n        print(X[i], y[i])","9ec9f4b3":"# multivariate lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the dataset\n                if end_ix > len(sequences):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\nprint(dataset)\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([[80, 85], [90, 95], [100, 105]])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","51f5674b":"# multivariate output data prep\nfrom numpy import array\nfrom numpy import hstack\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the dataset\n                if end_ix > len(sequences)-1:\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\nprint(dataset)\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n        print(X[i], y[i])","b66f23c2":"# multivariate output stacked lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps\n                # check if we are beyond the dataset\n                if end_ix > len(sequences)-1:\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\nprint(dataset)\n# choose a number of time steps\nn_steps = 3\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=400, verbose=0)\n# demonstrate prediction\nx_input = array([[70,75,145], [80,85,165], [90,95,185]])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","fbabc4a6":"# multi-step data preparation\nfrom numpy import array\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n        X, y = list(), list()\n        for i in range(len(sequence)):\n                # find the end of this pattern\n                end_ix = i + n_steps_in\n                out_end_ix = end_ix + n_steps_out\n                # check if we are beyond the sequence\n                if out_end_ix > len(sequence):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# summarize the data\nfor i in range(len(X)):\n        print(X[i], y[i])","e2ec4d15":"# univariate multi-step vector-output stacked lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n        X, y = list(), list()\n        for i in range(len(sequence)):\n                # find the end of this pattern\n                end_ix = i + n_steps_in\n                out_end_ix = end_ix + n_steps_out\n                # check if we are beyond the sequence\n                if out_end_ix > len(sequence):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\nprint(X)\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=50, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","d24f7042":"# univariate multi-step encoder-decoder lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n        X, y = list(), list()\n        for i in range(len(sequence)):\n                # find the end of this pattern\n                end_ix = i + n_steps_in\n                out_end_ix = end_ix + n_steps_out\n                # check if we are beyond the sequence\n                if out_end_ix > len(sequence):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\nprint(X)\ny = y.reshape((y.shape[0], y.shape[1], n_features))\nprint(y)\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=100, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","0687854c":"# multivariate multi-step data preparation\nfrom numpy import array\nfrom numpy import hstack\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps_in\n                out_end_ix = end_ix + n_steps_out-1\n                # check if we are beyond the dataset\n                if out_end_ix > len(sequences):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n        print(X[i], y[i])","e90603be":"# multivariate multi-step stacked lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps_in\n                out_end_ix = end_ix + n_steps_out-1\n                # check if we are beyond the dataset\n                if out_end_ix > len(sequences):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([[70, 75], [80, 85], [90, 95]])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","4e4b2796":"# multivariate multi-step data preparation\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps_in\n                out_end_ix = end_ix + n_steps_out\n                # check if we are beyond the dataset\n                if out_end_ix > len(sequences):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\nprint(X.shape, y.shape)\n# summarize the data\nfor i in range(len(X)):\n        print(X[i], y[i])","d8eee255":"# multivariate multi-step encoder-decoder lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n        X, y = list(), list()\n        for i in range(len(sequences)):\n                # find the end of this pattern\n                end_ix = i + n_steps_in\n                out_end_ix = end_ix + n_steps_out\n                # check if we are beyond the dataset\n                if out_end_ix > len(sequences):\n                        break\n                # gather input and output parts of the pattern\n                seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n                X.append(seq_x)\n                y.append(seq_y)\n        return array(X), array(y)\n\n# define input sequence\nin_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\nin_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\nout_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n# convert to [rows, columns] structure\nin_seq1 = in_seq1.reshape((len(in_seq1), 1))\nin_seq2 = in_seq2.reshape((len(in_seq2), 1))\nout_seq = out_seq.reshape((len(out_seq), 1))\n# horizontally stack columns\ndataset = hstack((in_seq1, in_seq2, out_seq))\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# covert into input\/output\nX, y = split_sequences(dataset, n_steps_in, n_steps_out)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]\n# define model\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=300, verbose=0)\n# demonstrate prediction\nx_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","5de96539":"## Multi-Step LSTM Models\n\nA time series forecasting problem that requires a prediction of multiple time steps into the future can be referred to as multi-step time series forecasting.\n\nSpecifically, these are problems where the forecast horizon or interval is more than one time step.\n\nThere are two main types of LSTM models that can be used for multi-step forecasting; they are:\n* Vector Output Model\n* Encoder-Decoder Model","3ef5dca2":"## Multivariate Multi-Step LSTM Models\nIn the previous sections, we have looked at univariate, multivariate, and multi-step time series forecasting.\n\nIt is possible to mix and match the different types of LSTM models presented so far for the different problems. This too applies to time series forecasting problems that involve multivariate and multi-step forecasting, but it may be a little more challenging.\n\nIn this section, we will provide short examples of data preparation and modeling for multivariate multi-step time series forecasting as a template to ease this challenge, specifically:\n\n* Multiple Input Multi-Step Output.\n* Multiple Parallel Input and Multi-Step Output.","6113771f":"## Encoder-Decoder Model\nA model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM.\n\nThe model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another.\n\nThis model can be used for multi-step time series forecasting.\n\nAs its name suggests, the model is comprised of two sub-models: the encoder and the decoder.\n\nThe encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model\u2019s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models.","a4b3548d":"We can use either the Vector Output or Encoder-Decoder LSTM to model this problem. In this case, we will use the Encoder-Decoder model, as shown below:","c2e50628":"## Vector Output Model\n\nLike other types of neural network models, the LSTM can output a vector directly that can be interpreted as a multi-step forecast.\n\nThe LSTM expects data to have a three-dimensional structure of [samples, timesteps, features], and in this case, we only have one feature so the reshape is straightforward.\nAny of the presented LSTM model types could be used, such as Vanilla, Stacked, Bidirectional, CNN-LSTM, or ConvLSTM. Below defines a Stacked LSTM for multi-step forecasting.","4e7c6c0c":"## Stacked LSTM with the above data","26864191":"# Long Short-Term Memory networks\n\nLong Short-Term Memory networks, or LSTMs for short, can be applied to time series forecasting.\n\nThere are many types of LSTM models that can be used for each specific type of time series forecasting problem.\n\nIn this tutorial, you will discover how to develop a suite of LSTM models for a range of standard time series forecasting problems.\n\nThe objective of this tutorial is to provide standalone examples of each model on each type of time series problem as a template that you can copy and adapt for your specific time series forecasting problem.\n\nAfter completing this tutorial, you will know:\n\n* How to develop LSTM models for univariate time series forecasting.\n* How to develop LSTM models for multivariate time series forecasting.\n* How to develop LSTM models for multi-step time series forecasting.\n\nThis tutorial is divided into four parts; they are:\n\n* Univariate LSTM Models\n* Multivariate LSTM Models\n* Multi-Step LSTM Models\n* Multivariate Multi-Step LSTM Models","dad9a622":"## Multiple Parallel Input and Multi-Step Output\nA problem with parallel time series may require the prediction of multiple time steps of each time series.","0011cbd3":"## Stacked LSTM\n\nMultiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.\n\nAn LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\n\nWe can address this by having the LSTM output a value for each time step in the input data by setting the *return_sequences=True* argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next.","fac87f8f":"## Bidirectional LSTM\n\nBidirectional LSTMs learn the input sequence both forward and backwards and concatenate both interpretations.\n\nWe can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.","eb746b1f":"## Multiple Input Series","d68e5f0f":"## Multiple Parallel Series\n\nAn alternate time series problem is the case where there are multiple parallel time series and a value must be predicted for each.","c8102ee7":"# Multivariate LSTM Models\n\nMultivariate time series data means data where there is more than one observation for each time step.\n\nThere are two main models that we may require with multivariate time series data; they are:\n\n* Multiple Input Series.\n* Multiple Parallel Series.","bcf32a2b":"## CNN LSTM\n\nA **convolutional neural network**, or **CNN** for short, is a type of neural network developed for working with two-dimensional image data.\n\nThe CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.\n\nA CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.\n\nThe first step is to split the input sequences into subsequences that can be processed by the CNN model. For example, we can first split our univariate time series data into input\/output samples with four steps as input and one as output. Each sample can then be split into two sub-samples, each with two time steps. The CNN can interpret each subsequence of two time steps and provide a time series of interpretations of the subsequences to the LSTM model to process as input.\n\nWe can parameterize this and define the number of subsequences as n_seq and the number of time steps per subsequence as n_steps.","79083b56":"### Data Preparation","b48c4c95":"## Vanilla LSTM\n\nVanilla LSTM has a single hidden layer of LSTM units, and an output layer used to make a prediction.","18029a16":"## Multivariate data preparation","2caff47a":"### Multiple Input Multi-Step Output\nThere are those multivariate time series forecasting problems where the output series is separate but dependent upon the input time series, and multiple time steps are required for the output series.","f6909695":"We can now develop an LSTM model for multi-step predictions.\n\nA vector output or an encoder-decoder model could be used. In this case, we will demonstrate a vector output with a Stacked LSTM.","4db4da1f":"## Multivariate LSTM example"}}