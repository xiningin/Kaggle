{"cell_type":{"d3c85fb3":"code","d79cde01":"code","dc578542":"code","91b8df9f":"code","6214e5eb":"code","0a3060dc":"code","ca50ebc3":"code","b9e71f58":"code","e0876331":"code","e3df043e":"code","2c0f0832":"code","83f1cc86":"code","5c1fec5a":"code","e357bd77":"code","e092738a":"code","3c567bb3":"code","80a5162c":"markdown","969f3188":"markdown","9753dc24":"markdown","e6ff8d7f":"markdown","84f36fd4":"markdown","8a6a5d6f":"markdown","7c83cd5c":"markdown","ac560615":"markdown","ee95a861":"markdown","69bac40c":"markdown","2994352c":"markdown","06143945":"markdown","773f0399":"markdown","0893b825":"markdown","10f3cf7b":"markdown","2048f91a":"markdown"},"source":{"d3c85fb3":"import tempfile\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.estimator import Estimator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow_estimator.python.estimator.mode_keys import ModeKeys","d79cde01":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path = (os.path.join(dirname, filename))\n\ndata = pd.read_csv(path, header=None)\ndata.head()","dc578542":"labels = np.unique(data.iloc[:, -1])\nprint('Class labels:', labels)","91b8df9f":"featuer = []\nfor i in range(len(data.columns)-1):\n  featuer.append(str(i))\n\ndf = pd.DataFrame(data)\n# Replace the header with String value \ncol_rename = {i:j for i,j in zip(df.columns, featuer)}\ndf = df.rename(columns=col_rename, inplace=False)\ndf.head()","6214e5eb":"X = df.iloc[:, :-1]\ny = df.iloc[:, -1]\ndftrain, dfeval, y_train, y_eval = train_test_split(\n    X, y, test_size=0.3, random_state=2)","0a3060dc":"dftrain = dftrain.astype(\"int64\")\ny_train = y_train.astype(\"int64\")\ndfeval = dfeval.astype(\"int64\")\ny_eval = y_eval.astype(\"int64\")","ca50ebc3":"NUM_EXAMPLES = len(y_train)\n\n\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\n    def input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n        if shuffle:\n            dataset = dataset.shuffle(NUM_EXAMPLES)\n\n        dataset = dataset.repeat(n_epochs)\n\n        dataset = dataset.batch(NUM_EXAMPLES)\n        return dataset\n    return input_fn\n\n\n# Training and evaluation input functions.\ntrain_input_fn = make_input_fn(dftrain, y_train)\neval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\n\n\nNUMERIC_COLUMNS = featuer\n\n\nfeature_columns = []\n\n\nfor feature_name in NUMERIC_COLUMNS:\n    feature_columns.append(tf.feature_column.numeric_column(feature_name,\n                                                            dtype=tf.float32))\n\n\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                          n_batches_per_layer=1,\n                                          n_classes=len(labels),\n                                          n_trees=30,\n                                          max_depth=10)\n\nest.train(train_input_fn, max_steps=20)\n#You may increase the tree depth to reach a better accuracy and model performance.","b9e71f58":"history = est.evaluate(eval_input_fn)\nprint(history)","e0876331":"items = list(history.items())\narray = np.array(items)\nprint('Accuracy:', '{0:.0f}%'.format(array[0, 1].astype('float64') * 100))","e3df043e":"est.model_fn","2c0f0832":"def serving_input_receiver_fn(X):\n    features = []\n    for i in range(X.shape[1]):\n        features.append(tf.feature_column.numeric_column(str(i)))\n    serving_input_receiver_fn = (\n        tf.estimator.export.build_parsing_serving_input_receiver_fn(\n            tf.feature_column.make_parse_example_spec(features)))\n    return serving_input_receiver_fn\n\ndef export_saved_model(path, X):\n    export_dir = est.export_saved_model(\n        path, serving_input_receiver_fn(X))\n    return export_dir\n\npath = ('.\/')\nexport = export_saved_model(path, X)","83f1cc86":"loaded = tf.saved_model.load(\".\/1642887781\")","5c1fec5a":"infer = loaded.signatures[\"predict\"]\nprint(infer.structured_outputs)","e357bd77":"infer = loaded.signatures[\"classification\"]\nprint(infer.structured_outputs)","e092738a":"def load_pb(path_to_pb):\n    with tf.io.gfile.GFile(path_to_pb, \"rb\") as f:\n        graph_def = tf.compat.v1.GraphDef()\n        graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(graph_def, name='')\n        return graph","3c567bb3":"infer = loaded.signatures[\"serving_default\"]\nprint(infer.structured_outputs)","80a5162c":"**Changing datatype to be compatible with the model**","969f3188":"# Importing the libraries","9753dc24":"# Preparing DataFrame","e6ff8d7f":"## Returning the model_fn","84f36fd4":"<h4> Exports inference graph as a SavedModel into the given dir. <\/h4>\n<br>\n\nYou can find the reference [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/estimator\/BoostedTreesClassifier).\n\nYou could use the saved models to have the same outputs.","8a6a5d6f":"Replacing the header type to string to be compatible with feature selection process ","7c83cd5c":"# Model evaluation","ac560615":"**Splitting dataset for training the model**","ee95a861":"# Export the saved models","69bac40c":"# Loading the saved models\n## Pre_trained TFBT model","2994352c":"### Author: [Seyedsaman Emami](https:\/\/github.com\/samanemami)\n\nIf you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (copy and Edit Kernel).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\n<hr>\n\n##### You can find some of my developments [here](https:\/\/github.com\/samanemami?tab=repositories).\n\n<hr>","06143945":"# Build the model","773f0399":"# Input data files are available in the read-only","0893b825":"> One could change\/add hyper-parameters to tune the model. ","10f3cf7b":"# Print class labels","2048f91a":"# About this NoteBook\nI the following notebook, I run the **Gradient Boosting Classifier** from the **TensorFlow** package over the vowel dataset and study different aspects of the model.\n\nBeside the training part, one additional method in this notebook is that I saved the trained model so that in the future we can use it as a pre_trained model.\n<hr>\n\nYou may find some modifications on TFBT [here](https:\/\/github.com\/samanemami\/TFBoostedTree)."}}