{"cell_type":{"1a944c2e":"code","b5582e61":"code","ef963c9d":"code","86970757":"code","49d3f027":"code","31ee7271":"code","8178264c":"code","782a0a5a":"code","963c5e4a":"code","0767e768":"code","9d381275":"code","8e3ae0f8":"code","af601b89":"code","4392d50d":"code","9f77a0d6":"code","765aa442":"code","df96cdbe":"code","0a47047d":"code","3218c1cf":"code","d3a5e04e":"code","7c117669":"code","5e35bf6d":"code","84b8a9f0":"code","605c7875":"code","35a5d55c":"code","b2479b49":"code","be311699":"code","efb8b3dc":"markdown","30ce81fc":"markdown","4333b593":"markdown","1a431908":"markdown","876e4afb":"markdown","94241d02":"markdown","2e24bc89":"markdown","8d884f2a":"markdown","5c6b26b3":"markdown","57473f90":"markdown","db3bb153":"markdown","1dc4712d":"markdown","2b23e3b7":"markdown","b60944ed":"markdown","76552365":"markdown","ec649608":"markdown","df14d242":"markdown","c801d0b3":"markdown","aa172a90":"markdown","d23c6bb9":"markdown","8bbb0ba9":"markdown","674bc296":"markdown","6df4e612":"markdown","e6d07205":"markdown","ad703954":"markdown","ed93c571":"markdown","8b50f77a":"markdown","143a8177":"markdown","7c9269b8":"markdown"},"source":{"1a944c2e":"from keras.layers import Input, Lambda, Dense, Flatten\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt","b5582e61":"train_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/train\/'\nval_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/val\/'\ntest_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/test\/'","ef963c9d":"# re-size all the images to a size VGG-16 expects.\nIMAGE_SIZE = [224, 224]\n\n# Set the batch size\nBATCH_SIZE = 32  # try reducing batch size or freeze more layers if your GPU runs out of memory\nNUM_EPOCHS = 5\nLEARNING_RATE = 0.0001\nNUM_CLASSES = 2 # We are aware of it.","86970757":"import os\nCLASSES = os.listdir(train_path)\nNUM_CLASSES = len(CLASSES)","49d3f027":"print(\"Class --> {} \\n and the length is : {}\".format(CLASSES, NUM_CLASSES))","31ee7271":"# Image Data Augmentation\n\ntrain_datagen = ImageDataGenerator(\n    rescale = 1.\/255,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True\n)","8178264c":"# Import the images from the train dataset.\n# Make sure to provide the same target size as initialied for the image size\ntraining_set = train_datagen.flow_from_directory(\n    directory = train_path,\n    target_size = (224, 224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical'\n)","782a0a5a":"test_datagen = ImageDataGenerator(rescale = 1.\/255)","963c5e4a":"# Import the images from the test dataset.\n\ntest_set = test_datagen.flow_from_directory(\n    directory = test_path,\n    target_size = (224, 224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical'\n)","0767e768":"# Import the VGG 16 library as shown below and add preprocessing layer to the front of VGG\n# Here we will be using imagenet weights\n\nvgg = VGG16(input_shape = IMAGE_SIZE + [3], weights='imagenet', include_top=False)","9d381275":"# don't train existing weights\nfor layer in vgg.layers:\n    layer.trainable = False","8e3ae0f8":"### Sample... for adding Pooling (optional)\n# global_average_layer = GlobalAveragePooling2D()\n\n# prediction = Dense(NUM_CLASSES,activation='softmax')","af601b89":"# our layers - you can add more if you want\nx = Flatten()(vgg.output)\n\nprediction = Dense(NUM_CLASSES, activation='softmax')(x)","4392d50d":"# create a model object\nmodel = Model(inputs=vgg.input, outputs=prediction)","9f77a0d6":"model.summary()","765aa442":"# tell the model what cost and optimization method to use\nmodel.compile(\n  loss='categorical_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy']\n)","df96cdbe":"# fit the model\n\n# history = model.fit_generator(\n#   training_set,\n#   validation_data=test_set,\n#   epochs=5,\n#   steps_per_epoch=len(training_set),\n#   validation_steps=len(test_set)\n# )\n\nhistory = model.fit(\n  training_set,\n  validation_data=test_set,\n  epochs=5,\n  steps_per_epoch=len(training_set),\n  validation_steps=len(test_set)\n)","0a47047d":"# Generate Validation set.\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255)\n\nvalidation_set = validation_datagen.flow_from_directory(\n    directory = val_path,\n    target_size = (224, 224),\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical'\n)","3218c1cf":"validation_steps = 20\n\nloss0,accuracy0 = model.evaluate(validation_set, steps = validation_steps)\n\nprint(\"loss: {:.2f}\".format(loss0))\nprint(\"accuracy: {:.2f}\".format(accuracy0))","d3a5e04e":"# Generate Validation set.\nvalidation_set2 = validation_datagen.flow_from_directory(\n    directory = val_path,\n    target_size = (224, 224),\n    batch_size = 1,\n    shuffle=False, \n    seed=42, \n    class_mode=\"binary\"\n)\n\n# validation_set2.reset()","7c117669":"# just capture the loss and accuray into val variable... unlike in pervious code to capture into loss0 and accuracy0. Just to showcase alternate way.\n\nval = model.evaluate(validation_set, steps = validation_steps)\n\nprint(\"loss: {:.2f}\".format(val[0]))\nprint(\"accuracy: {:.2f}\".format(val[1]))","5e35bf6d":"# summarize history for loss\n\nplt.plot(history.history['loss'], label='Train loss')\nplt.plot(history.history['val_loss'], label='Validation (Test) loss')\nplt.title('summarize history for loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","84b8a9f0":"# summarize history for accuracy\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('summarize history for accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","605c7875":"# get sample image to test.\nimg_normal = image.load_img('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/NORMAL\/NORMAL2-IM-1430-0001.jpeg', target_size = (224, 224))\nimg_pneumonia = image.load_img('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\/person1947_bacteria_4876.jpeg', target_size = (224, 224))","35a5d55c":"def model_predict(img, actual):\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis = 0)\n    x_processed = preprocess_input(x)\n    result = model.predict(x_processed)\n    if(result[0][0]<.50):\n        result=\"normal\"\n    else:\n        result=\"pneumonia\"\n        \n    plt.figure()\n    plt.imshow(img)\n    plt.title('Actual : {} --> Predicted  : {}'.format(actual, result))\n    \n#     return result","b2479b49":"pred_normal = model_predict(img_normal, \"normal\")\npred_pneumonia = model_predict(img_pneumonia, \"pneumonia\")","be311699":"img = image.load_img('..\/input\/chest-xray-pneumonia\/chest_xray\/val\/PNEUMONIA\/person1954_bacteria_4886.jpeg', target_size = (224, 224))\n\npred = model_predict(img, \"pneumonia\")","efb8b3dc":"We could see that the training dataset contains 624 images with 2 classes.","30ce81fc":"On Validation set, model did not perform well, and the accuracy was 81%. \n\nSo there is a lot of room for improvement. We can play around with various parameters and or adding layers and fie-tune the VGG.","4333b593":"Hello Kagglers,\n\nThis is just a learning of myself and sharing the same with others. Please do add comments if have better approach or have other suggestions. Eagerly to learn.\n\nFirst of all thanks to @Paul Mooney for sharing this dataset, which can be found from https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia\n\nHere I will be trying to focus only on training the model using VGG-16, \n* for X-Ray image analysis please refer to my other Notebook ...\n* for understanding on the Medical Imagine (dicom) and how to view them refer to https:\/\/www.kaggle.com\/dskagglemt\/dicom-image-reading-and-visualize","1a431908":"It\u2019s important to freeze the convolutional based before you compile and train the model. By freezing or setting `layer.trainable = False`, you prevent the weights in a given layer from being updated during training.","876e4afb":"As we already done all the analysis in the Notebook .... I am directly jumping to model.","94241d02":"## For Training dataset","2e24bc89":"# Learning curves\nLet\u2019s take a look at the learning curves of the training and test accuracy\/loss when using the VGG16 base model.","8d884f2a":"Just changing the class mode to \"binary\" and using shuffle = False.","5c6b26b3":"Apply a `Dense` layer to convert these features into a single prediction per image. \n\nNote : This is the place where we can add more layers such as Pooling layer; Flatten and Dense layers etc.\n\nFor now I am just adding a Flatten followed by the Dense layer.\n\n","57473f90":"Till here we are ready with our train and test dataset. Lets now build our pre-trained model.","db3bb153":"Now stack the feature extractor `vgg.input`, and these two layers `prediction`.","1dc4712d":"# Load the Data \/ Images","2b23e3b7":"We could check the summary of our model using the below code.","b60944ed":"# Compile the model\n\nWe need to compile the model before training it. Since there are 2 classes, use a `categorical_crossentropy`.\n\nNote if there are more than 2 classes, we could you `sparse_categorical_crossentropy` or other.\n\nAlso we tell the model to use `adam` optimizer. ","76552365":"# Import the VGG-16 library \/ Create Base-Model\nWe will create a base model from the VGG16 model. This is pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.\n\nThe very last classification layer is not very useful. Instead, we will follow the common practice to instead depend on the very last layer before the flatten operation. This layer is called the \u201cbottleneck layer\u201d. The bottleneck features retain many generalities as compared to the final\/top layer.\n\nFirst, instantiate a VGG16 model pre-loaded with weights trained on ImageNet. By specifying the `include_top=False` argument, you load a network that doesn\u2019t include the classification layers.\n\nVGG model weights are freely available and can be loaded and used in your own models and applications. This allowed other researchers and developers to use a state-of-the-art image classification model in their own work and programs.\n\nHere we will be using `imagenet` weights.","ec649608":"Alternate and Automate way to get Number of Classes.\n\nUseful for getting number of output classes\n\n* Option 1 : Using listdir. Used this in below cell.\n* Option 2 : Using glob, code-snippet is shown below\n        folders = glob(train_path +'\/*')\n        folders, len(folders)**","df14d242":"# Train the model\n\n`keras.fit()` in Python is deep learning libraries which can be used to train our machine learning and deep learning models. \n\nIf you are using tensorflow 2.1.0 or above then `fit_generator` is deprecated starting from tensorflow 2.1.0. You can find the documentation for tf-2.1.0-rc1 here: https:\/\/www.tensorflow.org\/versions\/r2.1\/api_docs\/python\/tf\/keras\/Model#fit\n\nAnd if using tensorflow 2.1.0 or below, then could use `fit_generator`. Will leave the code for both below.\n\nAs you can see the first argument of the `.fit` can take a generator so just pass it your generator.\n\n* `.fit` now supports generators, so there is no longer any need to use `.fit_generator`.","c801d0b3":"# Get the Path for Images","aa172a90":"# Import Required Library","d23c6bb9":"In tthis notebook we are going to build a deep learning model with pre-trained model VGG-16 to classify the patient either (s)he has Pneumonia or not using Chest X-ray.\n\nBefore getting into the Classification lets just get to know what is pre-trained model.\n\n# What is a Pre-trained Model?\nA pre-trained model is a model that has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. \n\nLearned features are often transferable to different data. \n\nFor example, a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.\n\n# Why use a Pre-trained Model?\nPre-trained models are beneficial to us for many reasons. By using a pre-trained model we are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it.\n\n# What is VGG-16?\n**VGG16** is a convolutional neural network model for Large-Scale Image Recognition, and is trained on ImageNet dataset which has over 14 million images belonging to 1000 classes.\n\nFull form of VGG is **Visual Geometry Group**.\n\nVGG-16 makes the improvement over **AlexNet** by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3\u00d73 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU\u2019s.\n\nRefer to https:\/\/neurohive.io\/en\/popular-networks\/vgg16\/ for understand more on VGG-16.","8bbb0ba9":"So from the above it looks like our model did well on train and test set.\n\nOn train set the accuracy was 96% where as on test set it was 89%.","674bc296":"Soon will be adding lerning rate and performing model tuning.\n\nPlease add comments what else we could do to increase the performance and accuracy.\n\nHappy Learning... Stay Safe, Stay Healthy.","6df4e612":"# Define Constants","e6d07205":"# Evaluate the Model","ad703954":"We could see that the training dataset contains 5216 images with 2 classes.","ed93c571":"## For Test Dataset","8b50f77a":"The performance of deep learning neural networks often improves with the amount of data available.\n\nData augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.\n\nImage data augmentation is perhaps the most well-known type of data augmentation and involves creating transformed versions of images in the training dataset that belong to the same class as the original image.\n\nTransforms include a range of operations from the field of image manipulation, such as shifts, flips, zooms, and much more.\n\n* Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.\n* Image data augmentation is supported in the Keras deep learning library via the ImageDataGenerator class.\n\nThe choice of the specific data augmentation techniques used for a training dataset must be chosen carefully and within the context of the training dataset and knowledge of the problem domain. In addition, it can be useful to experiment with data augmentation methods in isolation and in concert to see if they result in a measurable improvement to model performance, perhaps with a small prototype dataset, model, and training run.\n\nHere will be using `ImageDataGenerator()` as below.","143a8177":"# Predict","7c9269b8":"Cool... so by now we have done configuring the VGG model.\n\nNow its time to train the model."}}