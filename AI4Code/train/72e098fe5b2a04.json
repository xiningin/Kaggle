{"cell_type":{"1a48dbd0":"code","67bbec6d":"code","8cca79f5":"code","dc156748":"code","6c039545":"code","f30cafde":"code","261891c9":"code","8e5f5f86":"code","485f5517":"code","fe4272fb":"code","a71b2c82":"code","6578f231":"code","f5221f47":"code","5de9f42d":"code","e3954d9f":"code","90a44754":"code","04768c69":"code","5ac470dd":"code","118a52fd":"code","d8001a5a":"code","9fd4b38d":"code","12df81aa":"code","b9f43aa9":"code","f6a29034":"code","5761e04a":"code","f3c0e2c8":"code","523836ea":"code","20b31aa0":"markdown","aeffdc06":"markdown","bb12492c":"markdown","0cfe82a6":"markdown","c027eac6":"markdown","7fe96562":"markdown","e7531367":"markdown","719116b8":"markdown"},"source":{"1a48dbd0":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold\n\nimport gc\ngc.enable()","67bbec6d":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nTOKENIZER_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","8cca79f5":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","dc156748":"train_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\n\n# Remove incomplete entries if any.\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","6c039545":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","f30cafde":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","261891c9":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","8e5f5f86":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","485f5517":"test_dataset = LitDataset(test_df, inference_only=True)","fe4272fb":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(5):            \n    model_path = f\"..\/input\/verbatim467\/model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()","a71b2c82":"roberta1 = all_predictions.mean(axis=0)","6578f231":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-large\"\nTOKENIZER_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-large\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","f5221f47":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","5de9f42d":"class LitModel(nn.Module):  #for large hidden size 1024\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        x = torch.stack([roberta_output.hidden_states[-1],roberta_output.hidden_states[-2],roberta_output.hidden_states[-3],roberta_output.hidden_states[-4]])\n        last_layer_hidden_states =  torch.mean(x, 0)  #roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","e3954d9f":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(5):            \n    model_path = f\"..\/input\/rl4lnoptcombo\/model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()\n    \nroberta3 = all_predictions.mean(axis=0)","90a44754":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/d\/kpriyanshu256\/debertalarge\"\nTOKENIZER_PATH = \"..\/input\/d\/kpriyanshu256\/debertalarge\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","04768c69":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","5ac470dd":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(5):            \n    model_path = f\"..\/input\/debertalarge4layer\/model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()\n    \ndeberta3 = all_predictions.mean(axis=0)","118a52fd":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/myclrprobertalarge\"\nTOKENIZER_PATH = \"..\/input\/myclrprobertalarge\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","d8001a5a":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","9fd4b38d":"class LitModel(nn.Module):  #for large hidden size 1024\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n    ","12df81aa":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(5):            \n    model_path = f\"..\/input\/robertav2large4838\/model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()\n    \nroberta2 = all_predictions.mean(axis=0)    ","b9f43aa9":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/electra\/large-discriminator\"\nTOKENIZER_PATH = \"..\/input\/electra\/large-discriminator\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","f6a29034":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","5761e04a":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor index in range(5):            \n    #model_path = f\"..\/input\/electralarge\/model_{index + 1}.pth\"\n    model_path = f\"..\/input\/electralargenopt\/model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()","f3c0e2c8":"electra1 = all_predictions.mean(axis=0)","523836ea":"sub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n#roberta1 = 465\n#roberta2 = 462\n#roberta3 = 460 noPT 4 layer\n#deberta3 = 460 noPT 4 layer\n#electra =  462 noPT\n\nsub['target'] = (roberta1+roberta2+roberta3+electra1+deberta3)\/5\nsub.to_csv('submission.csv', index=False)\nsub","20b31aa0":"# Model\nThe model is inspired by the one from [Maunish](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm).","aeffdc06":"# Overview\nTrained mainly using this kernel [Lightweight Roberta solution in PyTorch](https:\/\/www.kaggle.com\/andretugan\/lightweight-roberta-solution-in-pytorch)\n\nLB score is the following:\n- roberta1 = 465 (roberta base)\n- roberta2 = 462\n- roberta3 = 460 noPT 4 layer\n- deberta3 = 460 noPT 4 layer\n- electra =  462 noPT\n\n* noPT : no pretraining\n* 4 layer : sum the last 4 hidden layer ","bb12492c":"# electra nopt 462","0cfe82a6":"# inference roberta large 462","c027eac6":"# Inference Roberta base 465","7fe96562":"# Dataset","e7531367":"# roberta large no pt 4 layer 461","719116b8":"# 4 layer deberta 460"}}