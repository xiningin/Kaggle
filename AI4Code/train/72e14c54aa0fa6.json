{"cell_type":{"e8ee2e29":"code","ca1727a9":"code","47021582":"code","3aace6b1":"code","a2161838":"code","bbc594d3":"code","b6378096":"code","63b09c03":"code","5951c2d1":"code","87552abd":"code","d8a6db88":"code","dec6327b":"code","f1304649":"code","cb708e8a":"code","a954ea9f":"code","1d639767":"code","f4a045d7":"code","6365414f":"code","e790175d":"code","9b66d2ce":"code","ac714547":"code","7494b48d":"code","cc7628cf":"code","e1bd9e87":"code","25f0acf5":"code","013d2bd6":"code","61226d69":"code","89f59b48":"code","6eb18755":"code","9c425dac":"code","0d15142b":"code","4f5c9744":"code","494b1a31":"code","006f19e7":"code","aa354527":"code","d3018e73":"code","69b17dd6":"code","1418200b":"code","056f9b73":"code","68affe85":"code","83569fe8":"code","bc49549e":"code","41b47870":"code","35e1daa1":"code","317a3bf1":"code","40c340b7":"code","61797a38":"code","604ceece":"code","16a0a736":"code","07901130":"code","d44e3071":"code","7aa60cce":"code","6cbbbdef":"code","7b260efd":"code","523e8d5c":"code","960f98c0":"code","13fa56cf":"code","41a26b40":"code","6b3a010b":"code","929a2904":"code","0461b225":"code","8170dc63":"code","44486cb0":"code","87a508bc":"code","ef518055":"code","bcb69def":"code","6c812a31":"code","b55297f1":"markdown","ba75eae6":"markdown","b0e83900":"markdown","64a3f9d6":"markdown","8da5bb88":"markdown","2a010151":"markdown","3b6ceb2c":"markdown","b8d37433":"markdown","24e3fb23":"markdown","02398c61":"markdown","951c8c7b":"markdown","c89a5735":"markdown","914b1844":"markdown","9d48c2aa":"markdown","0556e51c":"markdown","7027a210":"markdown","6189e8cc":"markdown","3ca19505":"markdown","ef2c7e77":"markdown","492d5c78":"markdown","6cc68e1b":"markdown","0383d4e3":"markdown","d10ab50c":"markdown","b917db49":"markdown","bf0d8cd6":"markdown"},"source":{"e8ee2e29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca1727a9":"import numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","47021582":"dataset = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndataset.head()\ndataset.count()","3aace6b1":"df = dataset.copy()\ndf = df.dropna() #Eksik g\u00f6zlem ortadan kald\u0131r\u0131l\u0131r.\ndf.head()","a2161838":"df.info()","bbc594d3":"corr_data = df.corr() # Datada bulunan numerik de\u011ferler aras\u0131ndaki korelasyona bakar.\nsns.clustermap(corr_data,annot= True,fmt = '.2f')\n#annot grafik \u00fcst\u00fcndeki say\u0131sal de\u011ferleri g\u00f6steririken fmt ise virg\u00fclden sonra ka\u00e7 basamak g\u00f6sterilecek bunu belirler.\nplt.title('Correlation Between Features')\nplt.show();","b6378096":"df[\"stroke\"].value_counts() ","63b09c03":"df[\"stroke\"].value_counts().plot.barh(); #Ba\u011f\u0131ml\u0131 de\u011fi\u015fkenin incelenmesi","5951c2d1":"df.describe().T","87552abd":"#Pre-processing for training & test dataset \ny = df[\"stroke\"] \nX = df.drop([\"stroke\"], axis=1)","d8a6db88":"X['gender'] = [1 if i.strip() == 'Male' else 0 for i in X.gender]\nX['ever_married'] = [1 if i.strip() == 'Yes' else 0 for i in X.ever_married]\nX['Residence_type'] = [1 if i.strip() == 'Urban' else 0 for i in X.Residence_type]\n\nX = X.drop([\"work_type\"], axis=1)\nX = X.drop([\"smoking_status\"], axis=1)\nX = X.drop([\"id\"], axis=1)","dec6327b":"X.head()","f1304649":"from sklearn.linear_model import LogisticRegression\nloj = LogisticRegression(solver = \"liblinear\")\nloj_model = loj.fit(X,y)\nloj_model","cb708e8a":"loj_model.intercept_","a954ea9f":"loj_model.coef_","1d639767":"y_pred = loj_model.predict(X)\nconfusion_matrix(y, y_pred)","f4a045d7":"accuracy_score(y, y_pred) ","6365414f":"print(classification_report(y, y_pred))","e790175d":"accuracy_score(y, y_pred)","9b66d2ce":"loj_model.predict_proba(X)[:,1][0:5]\n\nlogit_roc_auc = roc_auc_score(y, loj_model.predict(X))\n\nfpr, tpr, thresholds = roc_curve(y, loj_model.predict_proba(X)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","ac714547":"X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.30, \n                                                    random_state = 42)\n","7494b48d":"loj = LogisticRegression(solver = \"liblinear\")\nloj_model = loj.fit(X_train,y_train)\nloj_model\n\ntestscore_lr =accuracy_score(y_test, loj_model.predict(X_test))\naccuracy_score(y_test, loj_model.predict(X_test)) #Do\u011frulama \u00f6ncesi hata score","cc7628cf":"crosscore_lr =cross_val_score(loj_model, X_test, y_test, cv = 10).mean() #CV sonras\u0131 hata\ncross_val_score(loj_model, X_test, y_test, cv = 10).mean()","e1bd9e87":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb_model = nb.fit(X_train, y_train)\nnb_model\n\ny_pred = nb_model.predict(X_test)\ntestscore_nb =   accuracy_score(y_test, y_pred) # First Test acc\naccuracy_score(y_test, y_pred) # First Test acc\n","25f0acf5":"crosscore_nb=cross_val_score(nb_model, X_test, y_test, cv = 10).mean() #Test acc after cross val.\ncross_val_score(nb_model, X_test, y_test, cv = 10).mean() #Test acc after cross val.","013d2bd6":"knn = KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\nknn_model\n\ny_pred = knn_model.predict(X_test)\n\ntestscore_knn =accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","61226d69":"knn_params = {\"n_neighbors\": np.arange(1,50)}\n\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(X_train, y_train)","89f59b48":"print(\"En iyi skor:\" + str(knn_cv.best_score_))\nprint(\"En iyi parametreler: \" + str(knn_cv.best_params_)) #Bu skor train setindeki accdir","6eb18755":"knn = KNeighborsClassifier(12)\nknn_tuned = knn.fit(X_train, y_train)\nknn_tuned.score(X_test, y_test)#Test setindeki valide edilmi\u015f acc","9c425dac":"y_pred = knn_tuned.predict(X_test)\ncrosscore_knn=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","0d15142b":"svm_model = SVC(kernel = \"linear\").fit(X_train, y_train)\ny_pred = svm_model.predict(X_test)\n\ntestscore_svm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred) #test acc","4f5c9744":"svc_params = {\"C\": np.arange(1,5)}\n\nsvc = SVC(kernel = \"linear\")\n\nsvc_cv_model = GridSearchCV(svc,svc_params, \n                            cv = 10, \n                            n_jobs = -1, \n                            verbose = 2 )\n\nsvc_cv_model.fit(X_train, y_train)","494b1a31":"print(\"En iyi parametreler: \" + str(svc_cv_model.best_params_))","006f19e7":"svc_tuned = SVC(kernel = \"linear\", C = 1).fit(X_train, y_train)","aa354527":"y_pred = svc_tuned.predict(X_test)\ncrosscore_svm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","d3018e73":"svc_model = SVC(kernel = \"rbf\").fit(X_train, y_train)\ny_pred = svc_model.predict(X_test)\ntestscore_nonsvm=accuracy_score(y_test, y_pred) \naccuracy_score(y_test, y_pred) ","69b17dd6":"svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\n\nsvc = SVC()\nsvc_cv_model = GridSearchCV(svc, svc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nsvc_cv_model.fit(X_train, y_train)","1418200b":"print(\"En iyi parametreler: \" + str(svc_cv_model.best_params_))","056f9b73":"svc_tuned = SVC(C = 0.0001, gamma = 0.0001).fit(X_train, y_train)\ny_pred = svc_tuned.predict(X_test)\ncrosscore_nonsvm=accuracy_score(y_test, y_pred) #Tuning acc\naccuracy_score(y_test, y_pred) #Tuning acc","68affe85":"\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler() # it needs standardization for ANN\n\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","83569fe8":"from sklearn.neural_network import MLPClassifier\nmlpc = MLPClassifier().fit(X_train_scaled, y_train)\n\ny_pred = mlpc.predict(X_test_scaled)\n\ntestscore_mlpc=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","bc49549e":"### Tuning YSA\n\nmlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100),\n                                     (3,5), \n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n              \"activation\": [\"relu\",\"logistic\"]}\n\nmlpc = MLPClassifier()\nmlpc_cv_model = GridSearchCV(mlpc, mlpc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nmlpc_cv_model.fit(X_train_scaled, y_train)","41b47870":"print(\"En iyi parametreler: \" + str(mlpc_cv_model.best_params_))","35e1daa1":"mlpc_tuned = MLPClassifier(activation = \"relu\", \n                           alpha = 0.1, \n                           hidden_layer_sizes = (5, 3),\n                          solver = \"sgd\")","317a3bf1":"mlpc_tuned.fit(X_train_scaled, y_train)\ny_pred = mlpc_tuned.predict(X_test_scaled)\ncrosscore_mlpc=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","40c340b7":"from sklearn.tree import DecisionTreeClassifier\ncart = DecisionTreeClassifier()\ncart_model = cart.fit(X_train, y_train)","61797a38":"y_pred = cart_model.predict(X_test)\ntestscore_cart=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","604ceece":"#Model Tunning\n\ncart_grid = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50)) }\ncart = tree.DecisionTreeClassifier()\ncart_cv = GridSearchCV(cart, cart_grid, cv = 10, n_jobs = -1, verbose = 2)\ncart_cv_model = cart_cv.fit(X_train, y_train)\n\nprint(\"En iyi parametreler: \" + str(cart_cv_model.best_params_))","16a0a736":"cart = tree.DecisionTreeClassifier(max_depth = 5, min_samples_split = 19)\ncart_tuned = cart.fit(X_train, y_train)\n\ny_pred = cart_tuned.predict(X_test)\ncrosscore_cart=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","07901130":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier().fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\ntestscore_rf=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","d44e3071":"#Model Tuniing\n\nrf_params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}\n\nrf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) \n\nrf_cv_model.fit(X_train, y_train)\nprint(\"En iyi parametreler: \" + str(rf_cv_model.best_params_))","7aa60cce":"rf_tuned = RandomForestClassifier(max_depth = 5, \n                                  max_features = 2, \n                                  min_samples_split = 5,\n                                  n_estimators = 5)\n\nrf_tuned.fit(X_train, y_train)","6cbbbdef":"y_pred = rf_tuned.predict(X_test)\ncrosscore_rf=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","7b260efd":"Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Importance Level of Features \")","523e8d5c":"from sklearn.ensemble import GradientBoostingClassifier\ngbm_model = GradientBoostingClassifier().fit(X_train, y_train)\n\ny_pred = gbm_model.predict(X_test)\ntestscore_gbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","960f98c0":"### Model Tuning\n\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\ngbm = GradientBoostingClassifier()\n\ngbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv.fit(X_train, y_train)\n\nprint(\"The Best Parameters: \" + str(gbm_cv.best_params_))","13fa56cf":"gbm = GradientBoostingClassifier(learning_rate = 0.001, \n                                 max_depth = 3,\n                                min_samples_split = 2,\n                                n_estimators = 100)\n\ngbm_tuned =  gbm.fit(X_train,y_train)\ny_pred = gbm_tuned.predict(X_test)\ncrosscore_gbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","41a26b40":"#!pip install xgboost\nfrom xgboost import XGBClassifier\nxgb_model = XGBClassifier().fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\ntestscore_xgb=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","6b3a010b":"\n\nxgb_params = {\n        'n_estimators': [100, 500],\n        'subsample': [0.6, 0.8],\n        'max_depth': [3, 4,],\n        'learning_rate': [0.1,0.01,]}\nxgb = XGBClassifier()\n\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 5, n_jobs = -1, verbose = 2)\n\nxgb_cv_model.fit(X_train, y_train)","929a2904":"xgb_cv_model.best_params_","0461b225":"xgb = XGBClassifier(learning_rate = 0.01, \n                    max_depth = 3,\n                    n_estimators = 500,\n                    subsample = 0.6)\nxgb_tuned =  xgb.fit(X_train,y_train)\ny_pred = xgb_tuned.predict(X_test)\ncrosscore_xgb=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","8170dc63":"#!conda install -c conda-forge lightgbm\nfrom lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier().fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\ntestscore_lgbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","44486cb0":"\nlgbm_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'learning_rate': [0.1,0.01,0.02,0.05],\n        \"min_child_samples\": [5,10,20]}\n\nlgbm = LGBMClassifier()\n\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose = 2)\n\nlgbm_cv_model.fit(X_train, y_train)\n","87a508bc":"lgbm_cv_model.best_params_","ef518055":"lgbm = LGBMClassifier(learning_rate = 0.01, \n                       max_depth = 3,\n                       subsample = 0.6,\n                       n_estimators = 100,\n                       min_child_samples = 5)\n\nlgbm_tuned = lgbm.fit(X_train,y_train)\ny_pred = lgbm_tuned.predict(X_test)\ncrosscore_lgbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","bcb69def":"modeller = [\n    knn_tuned,\n    loj_model,\n    svc_tuned,\n    nb_model,\n    mlpc_tuned,\n    cart_tuned,\n    rf_tuned,\n    gbm_tuned,\n    lgbm_tuned,\n    xgb_tuned\n    \n]\n\n\nfor model in modeller:\n    isimler = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    dogruluk = accuracy_score(y_test, y_pred)\n    print(\"-\"*28)\n    print(isimler + \":\" )\n    print(\"Accuracy: {:.4%}\".format(dogruluk))","6c812a31":"modeller_name=['LR','NB','KNN', 'SVC', 'Non-SVC','ANN','CART', 'RF','GradientBoosting','XGBC','LGBM']\nmodel_Test=[testscore_lr,testscore_nb,testscore_knn,testscore_svm,testscore_nonsvm,testscore_mlpc,testscore_cart,testscore_rf,testscore_gbm,testscore_xgb,testscore_lgbm]\nmodel_Croos=[crosscore_lr,crosscore_nb,crosscore_knn,crosscore_svm,crosscore_nonsvm,crosscore_mlpc,crosscore_cart,crosscore_rf,crosscore_gbm,crosscore_xgb,crosscore_lgbm]\n#creating line1\n\nline1= go.Scatter(\n    x = modeller_name, # x axis\n    y = model_Test, # y axis\n    mode = \"markers\", #type of plot\n    name = \"Test Scores\", # name of the plots\n    marker = dict(color = 'rgba(167,150,55,0.8)'), #color + opacity\n    text = modeller_name # hover text\n)\n\n#cretaing line2\nline2= go.Scatter(\n    x = modeller_name,\n    y = model_Croos,\n    mode = 'lines+markers',\n    name = \"Cross Valid Scores\",\n    marker = dict(color = 'rgba(95,26,80,0.8)'),\n    text = modeller_name\n) \n\ndata = [line1,line2]\n\nlayout= dict(title= 'Comparison of Test & Cross Validation Scores',              \n             xaxis= dict(title= 'ML Methods',ticklen= 5,zeroline= False)\n)\nfig = dict(data = data, layout = layout)\niplot(fig)","b55297f1":"## Librires","ba75eae6":"## Model Tuning For KNN","b0e83900":"## Random Forest","64a3f9d6":"## XGB Classifier","8da5bb88":"## Light GBM","2a010151":"## Model Tuning ","3b6ceb2c":"## CART","b8d37433":"## Naive Bayes","24e3fb23":"## Correlation Matris","02398c61":"### Dependent Variable","951c8c7b":"## Logistic Regression ","c89a5735":"## Results","914b1844":"## Model Tuning","9d48c2aa":"# If you like this kernel, Please Upvote :) Thanks\n\n![image.gif](https:\/\/yyb7613mjr31k8vxr4692ftk-wpengine.netdna-ssl.com\/wp-content\/uploads\/2020\/09\/d6aa5176-statistics-for-data-science-courses.gif)","0556e51c":"# Prediction of Stroke in People with ML & Dataset Visualization\n\n### Dataset: Stroke Prediction Dataset\n\n* 11 clinical features por predicting stroke events\n\n### Features\n\nAttribute Information\n* id: unique identifier\n* gender: \"Male\", \"Female\" or \"Other\"\n* age: age of the patient\n* hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n* heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n* ever_married: \"No\" or \"Yes\"\n* work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n* Residence_type: \"Rural\" or \"Urban\"\n* avg_glucose_level: average glucose level in blood\n* bmi: body mass index\n* smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n* stroke: 1 if the patient had a stroke or 0 if not\n\n![image.png](attachment:image.png)","7027a210":"### Dataset Information \n*  stroke: 1 if the patient had a stroke or 0 if not","6189e8cc":"### Model Tuning\n","3ca19505":"## Gradient Boosting Machines","ef2c7e77":"## Train-Test Split","492d5c78":"### Dataset Pre-processing","6cc68e1b":"## KNN","0383d4e3":"### Non-Linear SVM & Radial Basis Function","d10ab50c":"### Artificial Neural Networks","b917db49":"## Support Vector Machine","bf0d8cd6":"## ROC Curve"}}