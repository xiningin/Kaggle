{"cell_type":{"2e11a287":"code","82129561":"code","83f2b325":"code","62cfd2dd":"code","9debb8d8":"code","3245b178":"code","7414ad69":"code","ce36392f":"code","b4e19c10":"code","a8ba5955":"code","194a069d":"code","7d173ccc":"code","0ed1337c":"code","2c73e06a":"code","9c69c09f":"code","46f070a8":"code","a94ab386":"code","d1670847":"code","8c21c46f":"code","cab0ca6e":"code","06ea4ef5":"code","031bd7a8":"code","6497a9d6":"code","71f24897":"code","5cec184b":"code","514adf73":"code","e96310a6":"code","63686d73":"code","f58437cf":"code","d52f9f1f":"code","97474ce1":"code","dac44972":"code","b02e6ef2":"code","6e1aaf0d":"code","f19adc56":"code","1e640a55":"code","c6a7a358":"markdown","338f52dc":"markdown","3174f34f":"markdown","63287326":"markdown"},"source":{"2e11a287":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))","82129561":"import random\nseed = 2357\nrandom.seed(seed)","83f2b325":"#Load data\ntrain_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')","62cfd2dd":"index_f_data = np.load('..\/input\/split-test-dataset\/index_of_fake_data.npy')\nindex_r_data = np.load('..\/input\/split-test-dataset\/index_of_real_data.npy')","9debb8d8":"fold_information = pd.read_csv(\"..\/input\/fold-information\/10_fold_information.csv\")","3245b178":"splits=[]\nfor i in fold_information.columns:\n    a = np.array(list(set(train_df.index)-set(list(fold_information[i].dropna()))))\n    a.sort()\n    splits.append((np.array(fold_information[i].dropna().astype(int)),a))","7414ad69":"del fold_information\ndel a\ngc.collect()","ce36392f":"train_features = train_df.drop(['target','ID_code'], axis = 1)\ntest_features = test_df.drop(['ID_code'],axis = 1)\ntrain_target = train_df['target']","b4e19c10":"fake_data = test_features.iloc[index_f_data]\nreal_data = test_features.iloc[index_r_data]","a8ba5955":"train_all_real = pd.concat([train_features,real_data], axis = 0)","194a069d":"for f in train_all_real.columns[0:200]:\n    train_all_real[f+'duplicate'] = train_all_real.duplicated(f,False).astype(int)","7d173ccc":"for f in train_all_real.columns[0:200]:\n    train_all_real[f+'duplicate_count'] = train_all_real.groupby([f])[f].transform('count')","0ed1337c":"for f in train_all_real.columns[0:200]:\n    train_all_real[f + '_duplicate_value_1'] = train_all_real[f]* (train_all_real[f + 'duplicate_count']>1).astype(int)\n    train_all_real[f + '_duplicate_value_2'] = train_all_real[f]* (train_all_real[f + 'duplicate_count']>2).astype(int)\n    train_all_real[f + '_duplicate_value_3'] = train_all_real[f]* (train_all_real[f + 'duplicate_count']>3).astype(int)","2c73e06a":"#for f in train_all_real.columns[0:200]:\n#    train_all_real[f+'duplicate_value'] = train_all_real[f]*train_all_real[f+'duplicate']","9c69c09f":"train_all_real = train_all_real.drop(train_all_real.columns[400:600],axis = 1)","46f070a8":"train_features_real = train_all_real.iloc[:len(train_target)]\nreal_data = train_all_real.iloc[len(train_target):len(train_all_real)]","a94ab386":"train_features_real.shape, real_data.shape","d1670847":"del train_all_real\ngc.collect()","8c21c46f":"#train_all_fake = pd.concat([train_features,fake_data], axis = 0)","cab0ca6e":"\"\"\"\nfor f in train_all_fake.columns[0:200]:\n    train_all_fake[f+'duplicate'] = train_all_fake.duplicated(f,False).astype(int)\n\"\"\"","06ea4ef5":"\"\"\"\nfor f in train_all_fake.columns[0:200]:\n    train_all_fake[f+'duplicate_value'] = train_all_fake[f]*train_all_fake[f+'duplicate']\n\"\"\"","031bd7a8":"#train_features_fake = train_all_fake.iloc[:len(train_target)]\n#fake_data = train_all_fake.iloc[len(train_target):len(train_all_fake)]","6497a9d6":"#del train_all_fake\n#gc.collect()","71f24897":"for f in train_features_real.columns[0:200]:\n    train_features_real[f+'distance_of_mean'] = train_features_real[f]-train_features_real[f].mean()\n    real_data[f+'distance_of_mean'] = real_data[f]-real_data[f].mean()\n    #train_features_real[f+'plus_of_mean'] = train_features_real[f]+train_features_real[f].mean()\n    #real_data[f+'plus_of_mean'] = real_data[f]+real_data[f].mean()\n    train_features_real[f+'distance_of_mean'] = train_features_real[f+'distance_of_mean']*train_features_real[f+'duplicate']\n    real_data[f+'distance_of_mean'] = real_data[f+'distance_of_mean']*real_data[f+'duplicate']\n    #train_features_real[f+'plus_of_mean'] = train_features_real[f+'plus_of_mean']*train_features_real[f+'duplicate']\n    #real_data[f+'plus_of_mean'] = real_data[f+'plus_of_mean']*real_data[f+'duplicate']","5cec184b":"train_features_real.shape,real_data.shape,train_target.shape","514adf73":"del train_features\ndel test_features\ngc.collect()","e96310a6":"n_splits = 10# Number of K-fold Splits","63686d73":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.33,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.0085,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 12,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1\n}","f58437cf":"\"\"\"\noof = np.zeros(len(train_features_real))\npredictions = np.zeros(len(test_df))\n#feature_importance_df = pd.DataFrame()\n#features = [c for c in train_features.columns if c not in ['ID_code', 'target']]\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    print(f'Fold {i + 1}')\n    x_train = np.array(train_features_fake)\n    y_train = np.array(train_target)\n    trn_data = lgb.Dataset(x_train[train_idx.astype(int)], label=y_train[train_idx.astype(int)])\n    val_data = lgb.Dataset(x_train[valid_idx.astype(int)], label=y_train[valid_idx.astype(int)])\n    \n    num_round = 100000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n    oof[valid_idx] = clf.predict(x_train[valid_idx], num_iteration=clf.best_iteration)\n    \n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = features\n    #fold_importance_df[\"importance\"] = clf.feature_importance()\n    #fold_importance_df[\"fold\"] = i + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    predictions[fake_data.index] += clf.predict(fake_data, num_iteration=clf.best_iteration) \/ n_splits\n    #predictions += clf.predict(test_features, num_iteration=clf.best_iteration) \/ n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof)))\n\"\"\"","d52f9f1f":"oof = np.zeros(len(train_features_real))\npredictions = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()\nfeatures = [c for c in train_features_real.columns if c not in ['ID_code', 'target']]\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    print(f'Fold {i + 1}')\n    x_train = np.array(train_features_real)\n    y_train = np.array(train_target)\n    trn_data = lgb.Dataset(x_train[train_idx.astype(int)], label=y_train[train_idx.astype(int)])\n    val_data = lgb.Dataset(x_train[valid_idx.astype(int)], label=y_train[valid_idx.astype(int)])\n    \n    num_round = 100000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n    oof[valid_idx] = clf.predict(x_train[valid_idx], num_iteration=clf.best_iteration)\n    \n    \n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = features\n    #fold_importance_df[\"importance\"] = clf.feature_importance()\n    f#old_importance_df[\"fold\"] = i + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions[real_data.index] += clf.predict(real_data, num_iteration=clf.best_iteration) \/ n_splits\n    #predictions += clf.predict(test_features, num_iteration=clf.best_iteration) \/ n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof)))","97474ce1":"predictions[fake_data.index] = 0.5","dac44972":"\"\"\"\nimport seaborn as sns\ncols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\n\"\"\"","b02e6ef2":"#esemble_lgbm_cat = 0.5*oof_cb+0.5*oof\nprint('LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, oof)))\n#print('catboost auc = {:<8.5f}'.format(roc_auc_score(train_target, oof_cb)))\n#print('LightBGM+catboost auc = {:<8.5f}'.format(roc_auc_score(train_target, esemble_lgbm_cat)))","6e1aaf0d":"id_code_test = test_df['ID_code']\nid_code_train = train_df['ID_code']","f19adc56":"my_submission_lbgm = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : predictions})\nmy_submission_train = pd.DataFrame({\"ID_code\" : id_code_train, \"target\" : oof})\n#my_submission_esemble_lgbm_cat = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : esemble_pred_lgbm_cat})","1e640a55":"my_submission_lbgm.to_csv('submission_lbgm.csv', index = False, header = True)\nmy_submission_train.to_csv('submission_lbgm_train.csv', index = False, header = True)\n#my_submission_esemble_lgbm_cat.to_csv('my_submission_esemble_lgbm_cat.csv', index = False, header = True)","c6a7a358":"## Ensemble two model (NN+ LGBM)\n* NN model accuracy is too low, ensemble looks don't work.","338f52dc":"## Create submit file","3174f34f":"## Load Data","63287326":"## Pytorch to implement simple feed-forward NN model (0.89+)\n\n* As below discussion, NN model can get lB 0.89+\n* https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82499#latest-483679\n* Add Cycling learning rate , K-fold cross validation (0.85 to 0.86)\n* Add flatten layer as below discussion (0.86 to 0.897)\n* https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82863\n\n## LightGBM (LB 0.899)\n\n* Fine tune parameters (0.898 to 0.899)\n* Reference this kernel : https:\/\/www.kaggle.com\/chocozzz\/santander-lightgbm-baseline-lb-0-899\n\n\n## Plan to do\n* Modify model structure on NN model\n* Focal loss\n* Feature engineering\n* Tune parameters oof LightGBM"}}