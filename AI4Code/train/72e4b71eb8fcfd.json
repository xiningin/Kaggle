{"cell_type":{"34db872b":"code","69b3206a":"code","1b8a80cb":"code","9897dccb":"code","aa80a458":"code","bfff0e92":"code","5f14d9bd":"code","440bbf85":"code","bcd28c8e":"code","372ae894":"code","e8a0c639":"code","bc2390cb":"code","f9a72880":"code","53ca4316":"code","5d5987dd":"code","b286b470":"code","5150070d":"code","19f9bf76":"code","17b0fd7b":"code","2e029e29":"code","f31f1c2a":"code","cd8aa79f":"code","804291d2":"code","159512ab":"markdown","4b51c46a":"markdown","c0558be0":"markdown","8a83b5b4":"markdown","ec1f435d":"markdown","c4dd089e":"markdown","71610857":"markdown","0f53fb20":"markdown","9bcf1997":"markdown","00e36a78":"markdown","99b085fa":"markdown","37100069":"markdown","0d3e31df":"markdown","7fd2c0fc":"markdown"},"source":{"34db872b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sea","69b3206a":"data_set  = pd.read_csv('..\/input\/sample-data\/Data.csv')\ndata_set.head()","1b8a80cb":"X = data_set.iloc[: ,:-1].values\ny = data_set.iloc[:,-1].values","9897dccb":"y","aa80a458":"X","bfff0e92":"data_set.isnull()","5f14d9bd":"data_set.isnull().sum()","440bbf85":"from sklearn.impute import SimpleImputer\n\nimputer  = SimpleImputer(missing_values = np.nan , strategy = 'mean')\n\nimputer.fit(X[:,1:3])\nX[:,1:3] = imputer.transform(X[:,1:3])","bcd28c8e":"print(X)","372ae894":"from sklearn.compose import ColumnTransformer\n\nfrom sklearn.preprocessing import OneHotEncoder\n","e8a0c639":"col_trans = ColumnTransformer(transformers=[('encoder',OneHotEncoder(), [0] )] , remainder='passthrough')\nX = np.array(col_trans.fit_transform(X))\nX","bc2390cb":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","f9a72880":"print(y)","53ca4316":"X","5d5987dd":"from sklearn.model_selection import train_test_split\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , train_size=0.8 ,   random_state = 1 )","b286b470":"print(X_train)","5150070d":"print(X_test)","19f9bf76":"print(y_train)","17b0fd7b":"print(y_test)","2e029e29":"print(X_train.size)\nprint(X_test.size)\nprint(y_train.size)\nprint(y_test.size)","f31f1c2a":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train[:,3:] = sc.fit_transform(X_train[:,3:])\nX_test[: , 3:] = sc.transform(X_test[: , 3:])\n","cd8aa79f":"print(X_train)","804291d2":"print(X_test)","159512ab":"fit() --> all the column with numerical value \/ not text values","4b51c46a":"2. import dataset","c0558be0":"\nTypeError: LabelEncoder() takes no arguments","8a83b5b4":"for X_test only tranform method bcz , we need same scaler \nsince this data use in production , feathure need apply same Scaler  , Scaler which applied in X_train , we need to applied to X_test","ec1f435d":"1. Importing libraries","c4dd089e":"Encodig Independent Feature","71610857":"# 5.Encoding Categorical Data","0f53fb20":"#  6.Splitting the dataset into Training set and Test set ","9bcf1997":"we can replace missing or fill the missing value with avg \/ median \/ most frequent value ","00e36a78":"# 4. Taking Care of Missing data","99b085fa":"    do we have to apply feature scalling to dummies variable  -- No\n\n    bcz  , the variable are in 0 or 1 , already ","37100069":"3. create matrix of features \/ dependent and independent variable ","0d3e31df":"# Encoding Dependent Variable","7fd2c0fc":"# 7.Feature Scalling\n\n    Standardisation --[-3 ,3]   , will work all the time \n    \n    Normalisation -- [0,1] when we have noraml distributation in most of feature"}}