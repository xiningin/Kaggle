{"cell_type":{"2e221712":"code","1a67a3cf":"code","cc9e6884":"code","c30b146a":"code","eb9c20eb":"code","92734dc4":"code","9816b2ad":"code","8dbac33b":"code","f3507a31":"code","bf838c93":"code","cb193664":"code","f9b1ce30":"code","9ef29e2d":"code","fd45ee26":"code","40d0112d":"code","6cbefc15":"code","cb73e511":"code","e3d3c6e4":"code","578fcc0f":"code","67db1065":"code","a19f7d89":"code","94322d62":"code","f97bc6f5":"code","af79f536":"code","d3224509":"code","57246f61":"code","2531632a":"code","b6c71449":"code","e37c9f07":"code","f05b7579":"code","19c1691a":"code","3d162a35":"code","8c45be82":"code","16d6c6ed":"code","67c91af0":"code","0ae4fb2b":"code","fe5d3d06":"code","fbdbd08b":"code","a2f769e6":"code","af88e62e":"code","1b278607":"code","84c8bff5":"code","92901349":"code","815fd44d":"code","bbaee92d":"code","12ef6f97":"code","91489e59":"code","b6d26311":"code","16142a93":"code","316149da":"code","4d9ec95e":"code","8bd19bff":"code","ca14a1b3":"markdown","05e142af":"markdown","ba1c07a6":"markdown","2eebb658":"markdown","f8750bc4":"markdown","6f9cb5fe":"markdown","fb6da3d9":"markdown","0f0082cb":"markdown","4f0f7ac6":"markdown","3b66986b":"markdown","a9b4fec0":"markdown","44985cda":"markdown","3f6f310d":"markdown","77299cbf":"markdown","a4f6e8eb":"markdown","38f60d7a":"markdown","afa4229c":"markdown","c2ea5610":"markdown","04eec874":"markdown","6f3a450a":"markdown","1e61a85c":"markdown","eee7944c":"markdown","e34e8f17":"markdown","1333d9bd":"markdown","ec1fd5ca":"markdown","f6c3cf45":"markdown","03c7fd33":"markdown","9ef09362":"markdown","d1dd8a19":"markdown","1d307638":"markdown","7d74d8eb":"markdown","982dde97":"markdown","906a9b9c":"markdown","45183ff1":"markdown","2056fb80":"markdown"},"source":{"2e221712":"# import PyTorch\nimport torch\n# import torchvision\nimport torchvision\n\n# get PyTorch version\nprint(torch.__version__)\n# get torchvision version\nprint(torchvision.__version__)","1a67a3cf":"# checking if cuda is available\ntorch.cuda.is_available()","cc9e6884":"# get number of cuda\/gpu devices\ntorch.cuda.device_count()","c30b146a":"# get cuda\/gpu device id\ntorch.cuda.current_device()","eb9c20eb":"# get cuda\/gpu device name\ntorch.cuda.get_device_name(0)","92734dc4":"x = torch.rand(2,3)\nprint(x)","9816b2ad":"# Define a tensor with a default data type:\nx = torch.ones(2,3)\nprint(x)\nprint(x.dtype)","8dbac33b":"# define a tensor with specific data type\nx = torch.ones(2,3,dtype=torch.int16)\nprint(x)\nprint(x.dtype)","f3507a31":"x = torch.ones(3,3,dtype=torch.int8)\nprint(x.dtype)\n# Change the tensor datatype\nx = x.type(torch.float32)\nprint(x.dtype)","bf838c93":"s_val = torch.full((3,4),3.1416)\nprint(s_val)","cb193664":"e_val = torch.empty((3,4))\nprint(e_val)","f9b1ce30":"r_val = torch.randn((4,5))\nprint(r_val)","9ef29e2d":"rng_val = torch.randint(10,20,(3,4))\nprint(rng_val)","fd45ee26":"# Define a tensor\nx = torch.rand(2,3)\nprint(x)\nprint(x.dtype)\n# convert tensor into numpy array\ny = x.numpy()\nprint(y)\nprint(y.dtype)","40d0112d":"import numpy as np\n# define a numpy array\nx = np.ones((2,3),dtype=np.float32)\nprint(x)\nprint(x.dtype)\n# convert to pytorch tensor\ny = torch.from_numpy(x)\nprint(y)\nprint(y.dtype)","6cbefc15":"# Define a tensor in cpu\nx = torch.tensor([2.3,5.8])\nprint(x)\nprint(x.device)\n\n# Define a CUDA device\nif torch.cuda.is_available():\n    device= torch.device(\"cuda:0\")\n    \n# Move the tensor onto CUDA device\nx = x.to(device)\nprint(x)\nprint(x.device)","cb73e511":"# Similarly, we can move tensors to CPU:\n# define a cpu device\ndevice = torch.device(\"cpu\")\nx = x.to(device) \nprint(x)\nprint(x.device)","e3d3c6e4":"# We can also directly create a tensor on any device:\n# define a tensor on device\ndevice = torch.device(\"cuda:0\")\nx = torch.ones(2,2, device=device) \nprint(x)","578fcc0f":"from torchvision import datasets\n# path to store data and\/or load from\ndata_path=\".\/data\"\n# loading training data\ntrain_data=datasets.MNIST(data_path, train=True, download=True)","67db1065":"# extract data and targets\nx_train, y_train=train_data.data,train_data.targets\nprint(x_train.shape)\nprint(y_train.shape)","a19f7d89":"# loading validation data\nval_data=datasets.MNIST(data_path, train=False, download=True)","94322d62":"# extract data and targets\nx_val,y_val=val_data.data, val_data.targets\nprint(x_val.shape)\nprint(y_val.shape)","f97bc6f5":"# add a dimension to tensor to become B*C*H*W\nif len(x_train.shape)==3:\n    x_train=x_train.unsqueeze(1)\nprint(x_train.shape)\n\nif len(x_val.shape)==3:\n    x_val=x_val.unsqueeze(1)\nprint(x_val.shape)","af79f536":"from torchvision import utils\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# define a helper function to display tensors as images\ndef show(img):\n    # convert tensor to numpy array\n    npimg = img.numpy()\n    # Convert to H*W*C shape\n    npimg_tr=np.transpose(npimg, (1,2,0))\n    plt.imshow(npimg_tr,interpolation='nearest')\n\n    \n# make a grid of 40 images, 8 images per row\nx_grid=utils.make_grid(x_train[:40], nrow=8, padding=2)\nprint(x_grid.shape)\n# call helper function\nshow(x_grid)","d3224509":"from torchvision import transforms\n# define transformations\ndata_transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=1),\n        transforms.RandomVerticalFlip(p=1),\n        transforms.ToTensor(),\n    ])","57246f61":"# get a sample image from training dataset\nimg = train_data[0][0]\n\n# transform sample image\nimg_tr=data_transform(img)\n\n# convert tensor to numpy array\nimg_tr_np=img_tr.numpy()\n\n# show original and transformed images\nplt.subplot(1,2,1)\nplt.imshow(img,cmap=\"gray\")\nplt.title(\"original\")\nplt.subplot(1,2,2)\nplt.imshow(img_tr_np[0],cmap=\"gray\");\nplt.title(\"transformed\")","2531632a":"# define transformations\n# data_transform = transforms.Compose([\n#         transforms.RandomHorizontalFlip(1),\n#         transforms.RandomVerticalFlip(1),\n#         transforms.ToTensor(),\n#     ])\n\n# Loading MNIST training data with on-the-fly transformations\n# train_data=datasets.MNIST(path2data, train=True, download=True, transform=data_transform )","b6c71449":"from torch.utils.data import TensorDataset\n\n# wrap tensors into a dataset\ntrain_ds = TensorDataset(x_train, y_train)\nval_ds = TensorDataset(x_val, y_val)\n\nfor x,y in train_ds:\n    print(x.shape,y.item())\n    break","e37c9f07":"from torch.utils.data import DataLoader\n\n# create a data loader from dataset\ntrain_dl = DataLoader(train_ds, batch_size=8)\nval_dl = DataLoader(val_ds, batch_size=8)\n\n# iterate over batches\nfor xb,yb in train_dl:\n    print(xb.shape)\n    print(yb.shape)\n    break","f05b7579":"from torch import nn\n\n# input tensor dimension 64*1000\ninput_tensor = torch.randn(64, 1000) \n# linear layer with 1000 inputs and 10 outputs\nlinear_layer = nn.Linear(1000, 10) \n# output of the linear layer\noutput = linear_layer(input_tensor) \nprint(output.size())","19c1691a":"# implement and print the model using nn.Sequential\nfrom torch import nn\n\n# define a two-layer model\nmodel = nn.Sequential(\n    nn.Linear(4, 5),\n    nn.ReLU(), \n    nn.Linear(5, 1),\n)\nprint(model)","3d162a35":"import torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n    \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n#     Net.__init__ = __init__\n#     Net.forward = forward","8c45be82":"model = Net()    \nprint(model)","16d6c6ed":"print(next(model.parameters()).device)","67c91af0":"device = torch.device(\"cuda:0\")\nmodel.to(device)\nprint(next(model.parameters()).device)","0ae4fb2b":"!pip install torchsummary","fe5d3d06":"# model summary using torchsummary\nfrom torchsummary import summary\nsummary(model, input_size=(1, 28, 28))","fbdbd08b":"from torch import nn\nloss_func = nn.NLLLoss(reduction=\"sum\")","a2f769e6":"for xb, yb in train_dl:\n    # move batch to cuda device\n    xb=xb.type(torch.float).to(device)\n    yb=yb.to(device)\n    # get model output\n    out=model(xb)\n    # calculate loss value\n    loss = loss_func(out, yb)\n    print (loss.item())\n    break","af88e62e":"# define the Adam optimizer\nfrom torch import optim\nopt = optim.Adam(model.parameters(), lr=1e-4)","1b278607":"# update model parameters\nopt.step()","84c8bff5":"# set gradients to zero\nopt.zero_grad()","92901349":"#  helper function to compute the loss value per mini-batch\ndef loss_batch(loss_func, xb, yb,yb_h, opt=None):\n    # obtain loss\n    loss = loss_func(yb_h, yb)\n    \n    # obtain performance metric\n    metric_b = metrics_batch(yb,yb_h)\n    \n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), metric_b","815fd44d":"# helper function to compute the accuracy per mini-batch\ndef metrics_batch(target, output):\n    # obtain output class\n    pred = output.argmax(dim=1, keepdim=True)\n    \n    # compare output class with target class\n    corrects=pred.eq(target.view_as(pred)).sum().item()\n    return corrects","bbaee92d":"# helper function to compute the loss and metric values for a dataset\ndef loss_epoch(model,loss_func,dataset_dl,opt=None):\n    loss=0.0\n    metric=0.0\n    len_data=len(dataset_dl.dataset)\n    for xb, yb in dataset_dl:\n        xb=xb.type(torch.float).to(device)\n        yb=yb.to(device)\n        \n        # obtain model output\n        yb_h=model(xb)\n\n        loss_b,metric_b=loss_batch(loss_func, xb, yb,yb_h, opt)\n        loss+=loss_b\n        if metric_b is not None:\n            metric+=metric_b\n    loss\/=len_data\n    metric\/=len_data\n    return loss, metric","12ef6f97":"def train_val(epochs, model, loss_func, opt, train_dl, val_dl):\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_metric=loss_epoch(model,loss_func,train_dl,opt)\n  \n        model.eval()\n        with torch.no_grad():\n            val_loss, val_metric=loss_epoch(model,loss_func,val_dl)\n        \n        accuracy=100*val_metric\n\n        print(\"epoch: %d, train loss: %.6f, val loss: %.6f, accuracy: %.2f\" %(epoch, train_loss,val_loss,accuracy))","91489e59":"# call train_val function\nnum_epochs=5\ntrain_val(num_epochs, model, loss_func, opt, train_dl, val_dl)","b6d26311":"# define path to weights\nweights_path =\"weights.pt\"\n \n# store state_dict to file\ntorch.save(model.state_dict(), weights_path)","16142a93":"# define model: weights are randomly initiated\n_model = Net()\nweights=torch.load(weights_path)\n_model.load_state_dict(weights)\n_model.to(device)","316149da":"n=100\nx= x_val[n]\ny=y_val[n]\nprint(x.shape)\nplt.imshow(x.numpy()[0],cmap=\"gray\")","4d9ec95e":"# we use unsqueeze to expand dimensions to 1*C*H*W\nx= x.unsqueeze(0)\n\n# convert to torch.float32\nx=x.type(torch.float)\n\n# move to cuda device\nx=x.to(device)","8bd19bff":"# get model output\noutput=_model(x)\n\n# get predicted class\npred = output.argmax(dim=1, keepdim=True)\nprint (pred.item(),y.item())","ca14a1b3":"**The negative log-likelihood loss:**","05e142af":"## Training and evaluation","ba1c07a6":"**Moving the model to a CUDA device**","2eebb658":"## What is a neural network?\n\nIn short, a neural network is an algorithm that learns about the relationship between the input variables and their associated target variables.","f8750bc4":"### Converting tensors into NumPy arrays\nWe can easily convert PyTorch tensors into NumPy arrays using `.numpy` method.","6f9cb5fe":"### Create a tensor filled with a specific value","fb6da3d9":"## What is Machine Learning?\nML is a subfield of AI that uses algorithms and statistical techniques to perform a task without the use of any explicit instructions. Instead, it relies on underlying statistical patterns in the data.","0f0082cb":"### Verifying the installation\n\nLet's make sure that the installation is correct by importing PyTorch into Python:","4f0f7ac6":"## Loading and processing data\nIn most cases, we receive data in three groups: training, validation, and test. We use the training dataset to train the model. The validation dataset is used to track the model's performance during training and test dataset used for the final evaluation of the model. The target values of the test dataset are usually hidden from us. We need at least one training dataset and one validation dataset to be able to develop and train a model.","3b66986b":"## Overview\nDeep learning is a field of machine learning utilizing massive neural networks, massive datasets, and accelerated computing on GPUs. Many of the advancements we've seen in AI recently are due to the power of deep learning. This revolution is impacting a wide range of industries already with applications such as personal voice assistants, medical imaging, automated vehicles, video game AI, and more.\n\nIn this notebook, I'll be covering the concepts behind deep learning and how to build deep learning models using PyTorch. By the end of the notebook, you'll be defining and training your own state-of-the-art deep learning models.","a9b4fec0":"### Create an empty tensor","44985cda":"### Working with PyTorch tensors\nPyTorch is built on tensors. A PyTorch tensor is an n-dimensional array, similar to NumPy arrays.\n\nIf you are familiar with NumPy, you will see a similarity in the syntax when working with tensors, as shown in the following table:\n\n|NumPy Arrays |\tPyTorch tensors |\tDescription|\n|-----------------|-----------------|-------------------|\n|numpy.ones(.) |\ttorch.ones(.) |\tCreate an array of ones|\n|numpy.zeros(.) |\ttorch.zeros(.) |\tCreate an array of zeros|\n|numpy.random.rand(.) |\ttorch.rand(.) |\tCreate a random array|\n|numpy.array(.) |\ttorch.tensor(.) |\tCreate an array from given values|\n|x.shape |\tx.shape or x.size() |\tGet an array shape|","3f6f310d":"Tensors are data containers and are a generalized representation of vectors and matrices. A vector is a first-order tensor since it only has one axis and would look like [x1, x2, x3,..]. A matrix is a second-order tensor that has two axes and looks like [ [x11, x12, x13..] , [x21, x22, x23..] ]. On the other hand, a scalar is a zero-order tensor that only contains a single element, such as x1. ","77299cbf":"### Creating data loaders\n\nTo easily iterate over the data during training, we can create a data loader using the `DataLoader` class","a4f6e8eb":"Create a linear layer and print out its output size","38f60d7a":"### Defining the tensor data type\nThe default tensor data type is `torch.float32`. This is the most used data type for tensor operations.","afa4229c":"## Building Models\nA model is a collection of connected layers that process the inputs to generate the outputs. You can use the nn package to define models. The nn package is a collection of modules that provide common deep learning layers. A module or layer of nn receives input tensors, computes output tensors, and holds the weights, if any. There are two methods we can use to define models in PyTorch: nn.Sequential and nn.Module.","c2ea5610":"## Loss function and Optimizer","04eec874":"## Contents\n- Getting Started with PyTorch\n- Loading and processing data\n- Building Models\n- Loss function and optimizer\n- Training and evaluation","6f3a450a":"## PyTorch\n[PyTorch](https:\/\/pytorch.org) is an open-source Python framework from the [Facebook AI Research team](https:\/\/ai.facebook.com\/) used for developing deep neural networks. PyTorch as an extension of Numpy that has some convenience classes for defining neural networks and accelerated computations using GPUs. PyTorch is designed with a Python-first philosophy, it follows Python conventions and idioms, and works perfectly alongside popular Python packages.","1e61a85c":"## Getting Started with PyTorch","eee7944c":"### Deploying the model","e34e8f17":"### Create a tensor from given range of values","1333d9bd":"### Data transformation\nImage transformation\/augmentation is an effective technique that's used to improve a model's performance. The `torchvision` package provides common image transformations through the `transforms` class","ec1fd5ca":"### Changing the tensor's data type\nWe can change a tensor's data type using the `.type` method:","f6c3cf45":"### Loading a dataset\nThe PyTorch `torchvision` package provides multiple popular datasets. Let's load the `MNIST` dataset from `torchvision`:","03c7fd33":"\n## What is CUDA?\n\nCUDA is a framework developed by NVIDIA that allows us to use General Purpose Computing on Graphics Processing Units (GPGPU). It is a widely used framework written in C++ that allows us to write general-purpose programs that run on GPUs. Almost all deep learning frameworks leverage CUDA to execute instructions on GPUs","9ef09362":"### Create a tensor with mean 0 and variance 1","d1dd8a19":"### Defining models using nn.Module\n\n Another way of defining models in PyTorch is by subclassing the nn.Module class. In this method, we specify the layers in the __init__ method of the class. Then, in the forward method, we apply the layers to inputs. This method provides better flexibility for building customized models.","1d307638":"We can also pass the transformer function to the dataset class:","7d74d8eb":"### Storing and loading models","982dde97":"### Moving tensors between devices\n\nBy default, PyTorch tensors are stored on the CPU. PyTorch tensors can be utilized on a GPU to speed up computing. This is the main advantage of tensors compared to NumPy arrays. To get this advantage, we need to move the tensors to the CUDA device. We can move tensors onto any device using the `.to` method:","906a9b9c":"### Converting NumPy arrays into tensors\nWe can also convert NumPy arrays into PyTorch tensors using `.from_numpy` method.","45183ff1":"Let's display a few sample images.","2056fb80":"**create a dataset from tensors.**"}}