{"cell_type":{"21c03732":"code","bccfa665":"code","5a3b4190":"code","44e4e713":"code","649a9724":"code","1f4c8a80":"code","a76b7c73":"code","1339ff5e":"code","65fa597d":"code","b9862838":"code","e24b03d1":"code","3aa05b30":"code","e7a2d198":"code","d010b2d2":"code","8a2161c9":"markdown","a3dd26ea":"markdown","83a3dd11":"markdown","fa1323cb":"markdown","a599ca05":"markdown","099c610f":"markdown","33f2ec37":"markdown","45312f5f":"markdown"},"source":{"21c03732":"import torch\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt","bccfa665":"\nclass tinyUNet(torch.nn.Module):\n    def __init__(self,in_channels=3,\n                 filters1=64,filters2 = 128,filters3=256,\n                 out_classes=5,\n                 filter_size=3,Pools=4):\n        super(tinyUNet,self).__init__()\n        self.Conv1 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(in_channels),\n            torch.nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Down1 = torch.nn.MaxPool2d(Pools)\n        self.Conv2 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters1),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Down2 = torch.nn.MaxPool2d(Pools)\n        self.Conv3 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters2),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters3,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters3,\n                out_channels=filters3,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Up1 = torch.nn.ConvTranspose2d(\n            in_channels=filters3,\n            out_channels=filters2,\n            kernel_size=Pools,\n            stride=Pools,\n            padding=0)\n        self.Conv4 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters2+filters2),\n            torch.nn.Conv2d(\n                in_channels=filters2+filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters2,\n                out_channels=filters2,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Up2 = torch.nn.ConvTranspose2d(\n            in_channels=filters2,\n            out_channels=filters1,\n            kernel_size=Pools,\n            stride=Pools,\n            padding=0)\n        self.Conv5 = torch.nn.Sequential(\n            torch.nn.BatchNorm2d(filters1+filters1),\n            torch.nn.Conv2d(\n                in_channels=filters1+filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(\n                in_channels=filters1,\n                out_channels=filters1,\n                kernel_size = filter_size,\n                padding=(filter_size-1)\/\/2,\n                stride=1),\n            torch.nn.ReLU())\n        self.Out = torch.nn.Conv2d(in_channels=filters1,\n                                   out_channels=out_classes,\n                                   kernel_size=1,\n                                   padding=0,\n                                   stride=1)\n    \n    def forward(self,Input):\n        self.Conv1Out = self.Conv1((Input))\n        self.Down1Out = self.Down1(self.Conv1Out)\n        self.Conv2Out = self.Conv2(self.Down1Out)\n        self.Down2Out = self.Down2(self.Conv2Out)\n        self.Conv3Out = self.Conv3(self.Down2Out)\n        self.Up1Out = self.Up1(self.Conv3Out)\n        self.Conv4Out = self.Conv4(torch.cat((self.Conv2Out,self.Up1Out),dim=1))\n        self.Up2Out = self.Up2(self.Conv4Out)\n        self.Conv5Out = self.Conv5(torch.cat((self.Conv1Out,self.Up2Out),dim=1))\n        self.Logit = self.Out(self.Conv5Out)\n        return self.Logit\n  ","5a3b4190":"Model = tinyUNet().to('cuda')","44e4e713":"Model.load_state_dict(torch.load('\/kaggle\/input\/sss-v2-model-training-data-augmentation-no-reg\/miniUNet8'))","649a9724":"Model.eval()\nModel","1f4c8a80":"! ls \/kaggle\/input\/severstal-steel-defect-detection\/ -al","a76b7c73":"# Thanks @rakhlin for sharing!\n# https:\/\/www.kaggle.com\/rakhlin\/fast-run-length-encoding-python\n\ndef rle_encoding(x):\n    '''\n    x: numpy array of shape (height, width), 1 - mask, 0 - background\n    Returns run length as list\n    '''\n    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b+1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return run_lengths","1339ff5e":"!mkdir \/kaggle\/test_masks","65fa597d":"for dirname,_,filenames in os.walk('\/kaggle\/input\/severstal-steel-defect-detection\/test_images\/'):\n    files = len(filenames)\n    for i in range(0,files):\n        Istack = []\n        Istack.append(plt.imread(dirname+filenames[i]))\n        testTensor = torch.cuda.FloatTensor(\n            np.swapaxes(\n                np.swapaxes(\n                    np.stack(\n                        Istack,axis=0),1,3),2,3))\n        pred = Model(testTensor).detach().cpu().numpy()\n        np.save('\/kaggle\/test_masks\/'+str(filenames[i]),pred)\n        print(\"Completion: {}%\".format(100*(i+1)\/files),end='\\r')","b9862838":"Predictions = []\n\nfor dirname,_,filenames in os.walk('\/kaggle\/test_masks\/'):\n    for idx,filename in enumerate(filenames):\n        Masks = np.load(dirname+filename)\n        MasksHWC = np.squeeze(np.argmax(Masks,axis=1))\n        for i in range(4):\n            RLE = str(rle_encoding(MasksHWC==i)).replace(',','').replace('[','').replace(']','')\n            Predictions.append([filename[:-4]+'_'+str(i+1),RLE])\n        print(\"Completion : {}%\".format(100*(idx+1)\/len(filenames)),end='\\r')\n","e24b03d1":"PDF = pd.DataFrame(Predictions,columns = ['ImageId_ClassId','EncodedPixels'])","3aa05b30":"PDF.head()","e7a2d198":"PDF.to_csv('submission.csv',index=False)","d010b2d2":"pd.read_csv('submission.csv').head(40)","8a2161c9":"### Instantiate the model","a3dd26ea":"### Let's define a semantic segmentation framework with the UNet architecture with just two downsampling and two upsampling portions. Downsampling and Upsampling by a factor of four","83a3dd11":"# Let's get all the output masks and save them to a directory for run length encoding later on","fa1323cb":"## Create a DataFrame with the data and save to a csv","a599ca05":"## A Check for sanity reasons","099c610f":"### Load up the weights from a model that was trained on the entire dataset without any regularization or data augmentation","33f2ec37":"## Change the format of the output to NHWC since pytorch use NCHW format, and then get the run length encodings for each mask except the last one. We don't need the mask that defines when it's not a part of any class","45312f5f":"## The model outputs Masks for each class so we need to convert this in to Run Length Encodings"}}