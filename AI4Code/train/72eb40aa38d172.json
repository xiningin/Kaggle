{"cell_type":{"4120d39d":"code","705fc1a5":"code","ed95999f":"code","d3a5fe9e":"code","c2c75bd1":"code","ab0ac188":"code","2bf9ca79":"code","cb2fc377":"code","8781c2f5":"code","e7b5148d":"code","a1fe1652":"code","2de57381":"code","bdd55a08":"code","b9fff79b":"code","48467a3c":"code","151db3fa":"code","ac7922ae":"code","c3675a87":"code","e1930876":"code","ea61c1e9":"code","3cb85b78":"code","fb4594bf":"code","84b9e354":"code","37edf04b":"code","bcea26b6":"code","e8a987f0":"code","5a0dae69":"code","db8bd275":"code","c5fdd939":"code","950b363e":"code","48359c97":"code","c3e5d978":"code","9aede245":"markdown"},"source":{"4120d39d":"import numpy as np\nimport pandas as pd\nfrom os.path import join\n\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nfrom pytorch_pretrained_bert import BertTokenizer","705fc1a5":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\nfrom fastai.callbacks import *","ed95999f":"import math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] \/ (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:            \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss","d3a5fe9e":"config = {\n    'testing':False,\n    'bert_model_name':\"bert-base-uncased\",\n    'max_lr':3e-5,\n    'epochs':6,\n    'use_fp16':True,\n    'bs':32,\n    'discriminative':False,\n    'max_seq_len':256,\n}\ndata_path = '..\/input'","c2c75bd1":"bert_tok = BertTokenizer.from_pretrained(config['bert_model_name'])","ab0ac188":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","2bf9ca79":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config['max_seq_len']), pre_rules=[], post_rules=[])\nfastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","cb2fc377":"train_df = pd.read_csv(join(data_path, 'train.csv'))\nval_df = pd.read_csv(join(data_path, 'valid.csv'))\ntest_df = pd.read_csv(join(data_path, 'test.csv'))\n\nif config['testing']:\n    train_df = train_df.head(1024)\n    val_df = val_df.head(1024)\n    test_df = test_df.head(1024)","8781c2f5":"train_df.head()","e7b5148d":"print(sorted(train_df['label'].unique()))\nprint(sorted(val_df['label'].unique()))","a1fe1652":"label_cols = list(pd.get_dummies(train_df['label']).columns)\nprint(label_cols)","2de57381":"databunch = TextDataBunch.from_df(\".\", train_df, val_df, test_df,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=\"text\",\n                  label_cols='label',\n                  bs=config['bs'],\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","bdd55a08":"# databunch.save('databunch')","b9fff79b":"# databunch = load_data('.', 'databunch');","48467a3c":"databunch.show_batch()","151db3fa":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\nbert_model = BertForSequenceClassification.from_pretrained(config['bert_model_name'], num_labels=len(label_cols))","ac7922ae":"loss_func = nn.CrossEntropyLoss()","c3675a87":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch, bert_model,\n    loss_func=loss_func,\n    opt_func=RAdam,\n)\nif config['use_fp16']: \n    learner = learner.to_fp16()","e1930876":"learner.lr_find()","ea61c1e9":"learner.recorder.plot()","3cb85b78":"learner.freeze_to(-1)","fb4594bf":"learner.fit_one_cycle(1, max_lr=config['max_lr'])","84b9e354":"learner.unfreeze()","37edf04b":"learner.fit_one_cycle(config['epochs'], max_lr=config['max_lr'], \n                      callbacks=[SaveModelCallback(learner, name='best',\n                                                 every='improvement', monitor='valid_loss')])","bcea26b6":"learner.load('best');","e8a987f0":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","5a0dae69":"test_preds = get_preds_as_nparray(DatasetType.Test)\ntest_preds","db8bd275":"test_preds = np.argmax(test_preds, axis=1)\ntest_preds","c5fdd939":"label_dict = {i: label for i, label in enumerate(learner.data.classes)}\nlabel_dict","950b363e":"sub_df = pd.read_csv(join(data_path, 'sample_submission.csv'), index_col='id')\nsub_df['label'] = test_preds\nsub_df['label'] = sub_df['label'].apply(lambda x: label_dict[x])","48359c97":"sub_df.head()","c3e5d978":"sub_df.to_csv('submission.csv')","9aede245":"This notebook is based on this tutorial https:\/\/mlexplained.com\/2019\/05\/13\/a-tutorial-to-fine-tuning-bert-with-fast-ai\/"}}