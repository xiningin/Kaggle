{"cell_type":{"6bed24a0":"code","768333c8":"code","a57c313d":"code","061ea9ab":"code","d87cd262":"code","3e7be8e2":"code","31143b6c":"code","44311c6c":"code","4e03c5a2":"code","fdff3a09":"code","0cf68705":"code","08a63773":"code","b939c11b":"code","c6820b2a":"code","9c570361":"code","52af73c4":"code","ad66a37a":"code","31b51823":"code","23a4cff8":"code","6fdb861b":"code","7cce84cc":"code","a0869885":"code","35dbc96a":"code","5d2cbc16":"markdown","9e678d3a":"markdown","beb7bb8d":"markdown","5d01a0f3":"markdown","156673de":"markdown","59a3f596":"markdown","ce51bc12":"markdown","fdb71736":"markdown","3c3551b2":"markdown","cc094983":"markdown","1715b2d4":"markdown","8fe89c2c":"markdown","f04d1259":"markdown","94fb326d":"markdown","ba1b41b5":"markdown","97fd56b0":"markdown","2545f560":"markdown","c2fe7a8d":"markdown","ffbd1bd1":"markdown"},"source":{"6bed24a0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","768333c8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","a57c313d":"data = pd.read_csv(\"\/kaggle\/input\/concrete-compressive-strength-data-set\/compresive_strength_concrete.csv\")","061ea9ab":"len(data)","d87cd262":"data.head()","3e7be8e2":"req_col_names = [\"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\", \"Superplasticizer\",\n                 \"CoarseAggregate\", \"FineAggregare\", \"Age\", \"CC_Strength\"]\ncurr_col_names = list(data.columns)\n\nmapper = {}\nfor i, name in enumerate(curr_col_names):\n    mapper[name] = req_col_names[i]\n\ndata = data.rename(columns=mapper)","31143b6c":"data.head()","44311c6c":"data.isna().sum()","4e03c5a2":"sns.pairplot(data)\nplt.show()","fdff3a09":"corr = data.corr()\n\nsns.heatmap(corr, annot=True, cmap='Blues')\nb, t = plt.ylim()\nplt.ylim(b+0.5, t-0.5)\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","0cf68705":"ax = sns.distplot(data.CC_Strength)\nax.set_title(\"Compressive Strength Distribution\")","08a63773":"fig, ax = plt.subplots(figsize=(10,7))\nsns.scatterplot(y=\"CC_Strength\", x=\"Cement\", hue=\"Water\", size=\"Age\", data=data, ax=ax, sizes=(20, 200))\nax.set_title(\"CC Strength vs (Cement, Age, Water)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","b939c11b":"X = data.iloc[:,:-1]         # Features - All columns but last\ny = data.iloc[:,-1]          # Target - Last Column","c6820b2a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)","9c570361":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","52af73c4":"# Importing models\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\n\n# Linear Regression\nlr = LinearRegression()\n# Lasso Regression\nlasso = Lasso()\n# Ridge Regression\nridge = Ridge()\n\n# Fitting models on Training data \nlr.fit(X_train, y_train)\nlasso.fit(X_train, y_train)\nridge.fit(X_train, y_train)\n\n# Making predictions on Test data\ny_pred_lr = lr.predict(X_test)\ny_pred_lasso = lasso.predict(X_test)\ny_pred_ridge = ridge.predict(X_test)","ad66a37a":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nprint(\"Model\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"LinearRegression \\t {:.2f} \\t\\t {:.2f} \\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_lr)),mean_squared_error(y_test, y_pred_lr),\n            mean_absolute_error(y_test, y_pred_lr), r2_score(y_test, y_pred_lr)))\nprint(\"\"\"LassoRegression \\t {:.2f} \\t\\t {:.2f} \\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_lasso)),mean_squared_error(y_test, y_pred_lasso),\n            mean_absolute_error(y_test, y_pred_lasso), r2_score(y_test, y_pred_lasso)))\nprint(\"\"\"RidgeRegression \\t {:.2f} \\t\\t {:.2f} \\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_ridge)),mean_squared_error(y_test, y_pred_ridge),\n            mean_absolute_error(y_test, y_pred_ridge), r2_score(y_test, y_pred_ridge)))","31b51823":"coeff_lr = lr.coef_\ncoeff_lasso = lasso.coef_\ncoeff_ridge = ridge.coef_\n\nlabels = req_col_names[:-1]\n\nx = np.arange(len(labels)) \nwidth = 0.3\n\nfig, ax = plt.subplots(figsize=(10,6))\nrects1 = ax.bar(x - 2*(width\/2), coeff_lr, width, label='LR')\nrects2 = ax.bar(x, coeff_lasso, width, label='Lasso')\nrects3 = ax.bar(x + 2*(width\/2), coeff_ridge, width, label='Ridge')\n\nax.set_ylabel('Coefficient')\nax.set_xlabel('Features')\nax.set_title('Feature Coefficients')\nax.set_xticks(x)\nax.set_xticklabels(labels, rotation=45)\nax.legend()\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{:.2f}'.format(height), xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\nautolabel(rects1)\nautolabel(rects2)\nautolabel(rects3)\n\nfig.tight_layout()\nplt.show()","23a4cff8":"fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,4))\n\nax1.scatter(y_pred_lr, y_test, s=20)\nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nax1.set_ylabel(\"True\")\nax1.set_xlabel(\"Predicted\")\nax1.set_title(\"Linear Regression\")\n\nax2.scatter(y_pred_lasso, y_test, s=20)\nax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nax2.set_ylabel(\"True\")\nax2.set_xlabel(\"Predicted\")\nax2.set_title(\"Lasso Regression\")\n\nax3.scatter(y_pred_ridge, y_test, s=20)\nax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nax3.set_ylabel(\"True\")\nax3.set_xlabel(\"Predicted\")\nax3.set_title(\"Ridge Regression\")\n\nfig.suptitle(\"True vs Predicted\")\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","6fdb861b":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor()\n\ndtr.fit(X_train, y_train)\n\ny_pred_dtr = dtr.predict(X_test)\n\nprint(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Decision Tree Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_dtr)),mean_squared_error(y_test, y_pred_dtr),\n            mean_absolute_error(y_test, y_pred_dtr), r2_score(y_test, y_pred_dtr)))\n\nplt.scatter(y_test, y_pred_dtr)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Decision Tree Regressor\")\nplt.show()","7cce84cc":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\n\nrfr.fit(X_train, y_train)\n\ny_pred_rfr = rfr.predict(X_test)\n\nprint(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Random Forest Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_rfr)),mean_squared_error(y_test, y_pred_rfr),\n            mean_absolute_error(y_test, y_pred_rfr), r2_score(y_test, y_pred_rfr)))\n\nplt.scatter(y_test, y_pred_rfr)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Decision Tree Regressor\")\nplt.show()","a0869885":"from sklearn.neural_network import MLPRegressor\n\nmlp = MLPRegressor(hidden_layer_sizes=(100,50), max_iter=1000)\n\nmlp.fit(X_train, y_train)\n\ny_pred_mlp = rfr.predict(X_test)\n\nprint(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Multi Layer Perceptron \\t\\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_mlp)),mean_squared_error(y_test, y_pred_mlp),\n            mean_absolute_error(y_test, y_pred_mlp), r2_score(y_test, y_pred_mlp)))\n\nplt.scatter(y_test, y_pred_mlp)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Decision Tree Regressor\")\nplt.show()","35dbc96a":"models = [lr, lasso, ridge, dtr, rfr, mlp]\nnames = [\"Linear Regression\", \"Lasso Regression\", \"Ridge Regression\", \n         \"Decision Tree Regressor\", \"Random Forest Regressor\", \"Multi Layer Perceptron\"]\nrmses = []\n\nfor model in models:\n    rmses.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n\nx = np.arange(len(names)) \nwidth = 0.3\n\nfig, ax = plt.subplots(figsize=(10,7))\nrects = ax.bar(x, rmses, width)\nax.set_ylabel('RMSE')\nax.set_xlabel('Models')\nax.set_title('RMSE with Different Algorithms')\nax.set_xticks(x)\nax.set_xticklabels(names, rotation=45)\nautolabel(rects)\nfig.tight_layout()\nplt.show()","5d2cbc16":"Lasso Regression, reduces the complexity of the model by keeping the coefficients as low as possible. Also, Coefficients with Linear and Ridge are almost same.\n\n##### Plotting predictions","9e678d3a":"There are'nt any high correlations, except between Cement and Compressive Strength of Concrete. Which should be the case for strength.\n\n### EDA","beb7bb8d":"### Conclusion\nRandom Forest Regressor is the best choice for this problem.","5d01a0f3":"#### Splitting data into Training and Test splits","156673de":"There seems to be no high correlation between independant variables (features). This can be further confirmed by plotting the Pearson Correlation coefficients between the features.","59a3f596":"#### Evaluation\nComparing the Root Mean Squared Error (RMSE), Mean Squared Error (MSE), Mean Absolute Error(MAE) and R2 Score.","ce51bc12":"The performance seem to be similar with all the three methods.\n\n##### Plotting the coefficients","fdb71736":"### Model Building\nTraining Machine Learning Algorithms on the training data and making predictions on Test data.\n\n#### 1. Linear Regression\n* The Go-to method for Regression problems.\n* The Algorithm assigns coefficients to each input feature to form a linear relation between input features and target variable, so as to minimize an objective function.\n* The objective function used in this case is Mean Squared Error.\n* There are three versions of Linear Regression\n    - Linear Regression - No regularisation\n    - Lasso Regression - L1 regularisation (Tries to push coefficients to zero)\n    - Ridge Regression - L2 regularisation (Tries to keep coefficients as low as possible)\ncomparing these three algorithms","3c3551b2":"#### Loading Data","cc094983":"#### Scaling\nStandardizing the data i.e. to rescale the features to have a mean of zero and standard deviation of 1.","1715b2d4":"As expected, the performance did not improve.\n\n### Comparision\n\nFinally, lets compare the results of all the algorithms.","8fe89c2c":"\n### Data Preprocessing\nSeparating Input Features and Target Variable.","f04d1259":"The Root Mean Squared Error (RMSE) has come down from 10.29 to 7.31, so the Decision Tree Regressor has improved the performance by a significant amount. This can be observed in the plot as well as more points are on the line.\n\n#### 3. Random Forest Regressor\n\nSince Using a Decision Tree Regressor has improved our performance, we can further improve the performance by ensembling more trees. Random Forest Regressor trains randomly initialized trees with random subsets of data sampled from the training data, this will make our model more robust","94fb326d":"##### Checking for 'null' values","ba1b41b5":"The RMSE with Random Forest Regressor is now 5.11, we have reduced the error by ensembling multiple trees.\n\n#### 4. Multi Layer Perceptron\n\nA Multi Layer Perceptron or a Neural Network is capable of learning complex non linear functions mapping input features to target variable, which a linear model like a linear regression cannot do. A Decision tree is still a non linear representation of the data but it can fail to model minute relations between the features and targets. So, the Neural Networks will almost everytime give a better performance compared to the algorithms we have used in this notebook before.\n\nWe will use the sklearn MLPRegressor in this case, since the dataset is small there might not be a significant improvment in performance.","97fd56b0":"Looking at the graphs between predicted and true values of the target variable, we can conclude that Linear and Ridge Regression perform well as the predictions are closer to the actual values. While Lasso Regression reduces the complexity at the cost of loosing performance in this case. (The closer the points are to the black line, the less the error is.)\n\n\n#### 2. Decision Trees\n\nAnother algorithm that would give better performance in this case would be Decision Trees, since we have a lot of zeros in some of the input features as seen from their distributions in the pair plot above. This would help the decision trees build trees based on some conditions on features which can further improve performance.","2545f560":"Simplifying Column names, since they appear to be too lengthy.","c2fe7a8d":"#### Conclusions from Strength vs (Cement, Age, Water)\n* Compressive strength increases with amount of cement\n* Compressive strength increases with age\n* Cement with low age requires more cement for higher strength\n* The older the cement is the more water it requires\n* Concrete strength increases when less water is used in preparing it","ffbd1bd1":"There are no null values in the data.\n\n##### Checking the pairwise relations of Features."}}