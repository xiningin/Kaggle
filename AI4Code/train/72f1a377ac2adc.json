{"cell_type":{"5fff5f23":"code","3bc710b1":"code","4d76f663":"code","38aa4753":"code","66874ad4":"code","c3cf76c8":"code","f4689205":"code","dacf28f9":"code","136ec0d2":"code","cc292f22":"code","767c37fb":"code","011057da":"code","046ed521":"code","572d34a6":"code","3e8ee1aa":"code","7c0aabed":"code","03c3f84e":"code","9cbbeaa2":"code","d032cd27":"code","a61111d1":"code","dad81118":"code","ac1fcbd7":"code","cb1cadfb":"code","d488b321":"code","8d9b8c20":"code","9a947a46":"code","e1625915":"code","3307ee4f":"code","d04e17d1":"markdown","9858731f":"markdown","7a7ca898":"markdown","e4f40372":"markdown","4cb9c480":"markdown","974e6277":"markdown","152d8238":"markdown","efb44330":"markdown","8de329c2":"markdown","1413b975":"markdown","d77dc5ad":"markdown","b4286f57":"markdown","6a0f815f":"markdown","52055428":"markdown","43f9269b":"markdown","ce682134":"markdown","30600aeb":"markdown","d6bc9221":"markdown","26e0e332":"markdown","80853d32":"markdown","134c43f4":"markdown","a40a652e":"markdown"},"source":{"5fff5f23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport time\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3bc710b1":"# Import Toyota dataset\n# We want to look at how the data looks like before generalising with other datasets\nallToyota = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv')\n\n# Check out head()\nprint(allToyota.head())\n\n# Check shape of dataset\nprint('Shape of dataset: ', allToyota.shape)\nprint('\\n')\n\n# Check for basic info about dataset i.e. any missing or null values\nprint(allToyota.info())","4d76f663":"# use str.strip()\nfeatures = ['model', 'transmission', 'fuelType']\nfor feature in features:\n    allToyota[feature] = allToyota[feature].apply(lambda x: x.strip())","38aa4753":"sns.set_style(\"whitegrid\")\n\nfig, axes = plt.subplots(2,2, figsize=(22,15))\nfeatures = ['price','mileage','tax','year']\nfor i, ax in zip(range(len(features)), axes.flat):\n    sns.distplot(allToyota[features[i]], bins=20, kde=False, ax=ax)\n    plt.title('Distribution of ' + features[i])\n    plt.xlabel(str(features[i]))\n","66874ad4":"plt.subplots(figsize=(20,10))\n# Price is our dependent variable; year is our independent variable\nsns.scatterplot(x = 'year', y = 'price', hue = 'model', data = allToyota, s = 100, alpha = 0.7)\nplt.title('Price of cars vs year')\nplt.xlabel('Year')\nplt.ylabel('Price of Car')\nplt.show()","c3cf76c8":"plt.subplots(figsize=(20,10))\n# Price is our dependent variable; year is our independent variable\nsns.boxplot(x = 'year', y = 'price', data = allToyota)\nplt.title('Price of cars vs year')\nplt.xlabel('Year')\nplt.ylabel('Price of Car')\nplt.show()","f4689205":"plt.subplots(figsize=(20,10))\n# Price is our dependent variable; year is our independent variable\nsns.scatterplot(x = 'mileage', y = 'price', hue = 'model', data = allToyota, s = 100, alpha = 0.7)\nplt.title('Price of cars vs year')\nplt.xlabel('Mileage')\nplt.ylabel('Price of Car')\nplt.show()","dacf28f9":"plt.subplots(figsize=(20,10))\n# Price is our dependent variable; year is our independent variable\nsns.boxplot(x = 'model', y = 'price', data = allToyota)\nplt.title('Variation of prices for each model')\nplt.xlabel('Model')\nplt.ylabel('Price of Car')\nplt.show()","136ec0d2":"supraAndIQ = allToyota.loc[allToyota['model'].isin(['Supra', 'IQ'])]\nprint('Shape of supraAndIQ is:', supraAndIQ.shape)\n\nplt.subplots(figsize=(13,6))\n# Price is our dependent variable; year is our independent variable\nsns.scatterplot(x = 'year', y = 'price', hue = 'model', data = supraAndIQ, s = 100, alpha = 0.7)\nplt.title('Price vs Year')\nplt.xlabel('Registration Year')\nplt.ylabel('Price')\nplt.show()","cc292f22":"plt.subplots(figsize=(13,6))\nsns.scatterplot(x = 'mileage', y = 'price', hue = 'transmission', data = supraAndIQ[supraAndIQ['model'] == 'Supra'], s = 100, alpha = 0.7)\nplt.title('Price vs Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Price')\nplt.show()","767c37fb":"plt.subplots(figsize=(13,6))\nsns.scatterplot(x = 'mileage', y = 'price', hue = 'transmission', data = supraAndIQ[supraAndIQ['model'] == 'IQ'], s = 100, alpha = 0.7)\nplt.title('Price vs Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Price')\nplt.show()","011057da":"plt.subplots(figsize=(13,6))\nsns.scatterplot(x = 'mileage', y = 'price', hue = 'transmission', data = allToyota[allToyota['model'] == 'Land Cruiser'], s = 100, alpha = 0.7)\nplt.title('Price vs Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Price')\nplt.show()","046ed521":"# Need to groupby to know how many cars per model\nallToyota.head()\n\n# modelGroupby = allToyota.groupby(['model'])['price'].count().reset_index()\nmodelGroupby = allToyota.groupby(['model'])['price'].count().reset_index().sort_values(['price'], ascending=False)\n\nmodelGroupby = modelGroupby.rename(columns={'price':'count'})\n\nplt.subplots(figsize=(20,10))\n# Price is our dependent variable; year is our independent variable\nsns.barplot(x = 'model', y = 'count', data = modelGroupby)\nplt.title('Sale for Each Model')\nplt.xlabel('Model')\nplt.ylabel('No. of Cars Sold')\nplt.show()","572d34a6":"# What are the categorical columns?\ncategorical_cols = allToyota.select_dtypes('object').columns\nprint(categorical_cols)\n\ndef encode_categoricals(df, cols):\n    # df - input dataframe\n    # cols - categorical column names\n    \n    # Steps to dummy encode\n    # 1. Create dummy_encode dataframe\n    # 2. Concat to main dataframe\n    # 3. Drop original columns\n    \n    for col in cols:\n        df_dummy = pd.get_dummies(df[col], prefix = col)\n        df = pd.concat([df, df_dummy], axis = 1)\n        df.drop([col], axis = 1, inplace = True)\n        \n    return df\n\nallToyotaEncoded = encode_categoricals(allToyota, categorical_cols)\nallToyotaEncoded.head()","3e8ee1aa":"from sklearn.model_selection import train_test_split\n\nlabel = allToyotaEncoded['price']\nfeatures = allToyotaEncoded.drop(columns=['price'], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, label, test_size = 0.1, random_state = 123)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7c0aabed":"from sklearn.linear_model import ElasticNet, LinearRegression, Lasso, BayesianRidge, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","03c3f84e":"# Define a generic model to calculate the R2_score between the true and predicted value\ndef kfold_cv_scoring(model, scoring, folds, x_train, y_train):\n    kf_cv = KFold(folds, shuffle=True, random_state = 123).get_n_splits(x_train.values)\n#     score = np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring = scoring, cv = kf_cv))\n    score = cross_val_score(model, x_train.values, y_train, scoring = scoring, cv = kf_cv)\n\n    return score\n\n# Define a function to plot the results of the k-fold cross validation training:\ndef plot_kfold_cv_results(model, folds, x_train, y_train, model_name, scoring, result_table):\n    # model - model class\n    # model_name - name of model\n    # result_time - to hold the summary of results\n    \n    start = time.time()\n    model_scores = kfold_cv_scoring(model, scoring, folds, x_train, y_train)\n    \n    end = time.time()\n    duration = (end-start)\/60\n    print('{} took {:.3f} minutes to complete training'.format(model_name, duration))\n    \n    # Plot the graph of RMSE for different folds\n    plt.plot(model_scores)\n    plt.xlabel('ith interation of Kfolds')\n    plt.ylabel(str(scoring))\n    plt.title(str(scoring) + ' for different iteration of K-folds')\n    plt.show()\n    \n    # Print out the mean and standard deviation of the scoring values\n    scoring_mean = model_scores.mean()\n    scoring_std = model_scores.std()\n    print('Mean ' + str(scoring) + ' is {:.4f}'.format(scoring_mean))\n    print('Std ' + str(scoring) + ' is {:.4f}'.format(scoring_std))\n    print('\\n')\n    \n    # Append results to result_table\n    new_row = [model_name, scoring_mean, scoring_std]\n    result_table.loc[len(result_table)] = new_row\n#     result_table.sort_values(by = [str(scoring)])\n    print(result_table)\n    \n    return None","9cbbeaa2":"### Initialize the 'Result_Table' first\nresult_table = pd.DataFrame(columns = ['Model', 'Mean', 'Std'])\n\n# Define all the models here\nlm = LinearRegression()\nkernelridge = KernelRidge(alpha = 0.6, kernel='polynomial', degree=2, coef0=2.5)\ngradboost = GradientBoostingRegressor(\n    n_estimators=500,\n    learning_rate=0.001,\n    max_depth=4,\n    max_features='sqrt',\n    loss='huber',\n    random_state=123,\n    criterion='friedman_mse'\n)\nelasticnet = make_pipeline(RobustScaler(), ElasticNet(max_iter=1e7, alpha=0.0005, l1_ratio=0.9, random_state=123))\nxgb = XGBRegressor(\n    colsample_bytree=0.2,\n    gamma=0.0,\n    learning_rate=0.01,\n    max_depth=5, \n    min_child_weight=1.5,\n    n_estimators=4000,\n    reg_alpha=0.9,\n    reg_lambda=0.6,\n    subsample=0.8,\n    verbosity=0,\n    random_state = 7,\n    objective='reg:squarederror',\n    n_jobs = -1\n)\nlightGBM = lgb.LGBMRegressor(\n    objective='regression',\n    num_leaves=10,\n    learning_rate=0.05,\n    n_estimators=1000,\n    max_bin = 55,\n    bagging_fraction = 0.8,\n    bagging_freq = 5,\n    feature_fraction = 0.2319,\n    feature_fraction_seed=9,\n    bagging_seed=9,\n    min_data_in_leaf =10,\n    min_sum_hessian_in_leaf = 11)\nrf = RandomForestRegressor(n_estimators=500, max_depth=2, random_state=123)\n\nmodels = [lm, kernelridge, gradboost, elasticnet, xgb, lightGBM, rf]\n# Use for-loop to train all models defined above:\nfor model in models:\n    print('Begin training for ', model)\n    plot_kfold_cv_results(\n        model=model,\n        folds=5,\n        x_train=X_train,\n        y_train=y_train,\n        model_name=str(model),\n        scoring='r2',\n        result_table=result_table)","d032cd27":"# In summary, these are the baseline results\nresult_table.sort_values(by='Mean', ascending=False)","a61111d1":"from sklearn.feature_selection import SelectFromModel\nfrom xgboost import plot_importance\nfrom numpy import sort\n\n# Define our XGBRegressor model again\nxgb = XGBRegressor(\n    colsample_bytree=0.2,\n    gamma=0.0,\n    learning_rate=0.01,\n    max_depth=5, \n    min_child_weight=1.5,\n    n_estimators=4000,\n    reg_alpha=0.9,\n    reg_lambda=0.6,\n    subsample=0.8,\n    verbosity=0,\n    random_state = 7,\n    objective='reg:squarederror',\n    n_jobs = -1\n)\n\n# Fit XGBR on all training data - X_train\nxgb.fit(X_train, y_train)\n\n# Plot features ranked according to their importances\nfig, ax = plt.subplots(figsize=(20,15))\nplot_importance(xgb, max_num_features=50, height=0.8, ax=ax)\nplt.show()","dad81118":"print('Current number of features in X_train:', len(X_train.columns))","ac1fcbd7":"# Fit XGBR using each importance as a threshold\nthresholds = sort(xgb.feature_importances_)\nprint('thresholds:', thresholds)\n\n# Do a quick plot to see the feature importance\nplt.subplots(figsize=(11, 8))\nplt.scatter(x = range(0, len(thresholds)), y = thresholds, s=10)\nplt.xlabel('n-th feature')\nplt.ylabel('Feature_Importance')\nplt.show()","cb1cadfb":"# Store the threshold, no. of features, and corresponding R2 for purpose of visualisation\nr2_feat_importance = pd.DataFrame(columns = ['thresholds', 'no_features', 'threshold_r2'])\n\n# Define function to calculate R2:\ndef model_r2(y, y_pred):\n    return r2_score(y, y_pred)\n\nstart = time.time()\n\n# For thresh values in interval of 2 units (depends on how many features and how long you want to spend iterating on)\nfor i in range(0, len(thresholds)):\n    if i % 2 == 0: # multiples of 2\n        print('Current index is:', i)\n        \n        thresh = thresholds[i]\n        # For thresh values in interval of i units:\n        # select features using threshold\n        selection = SelectFromModel(xgb, threshold = thresh, prefit = True)\n        select_X_train = selection.transform(X_train)\n            \n        # Define model again\n        selection_model = XGBRegressor(\n            colsample_bytree=0.2,\n            gamma=0.0,\n            learning_rate=0.01,\n            max_depth=5, \n            min_child_weight=1.5,\n            n_estimators=4000,\n            reg_alpha=0.9,\n            reg_lambda=0.6,\n            subsample=0.8,\n            verbosity=0,\n            random_state = 7,\n            objective='reg:squarederror',\n            n_jobs = -1\n        )\n        \n        # Train model\n        selection_model.fit(select_X_train, y_train)\n\n        # Eval model - select same features as in select_X_train as well in select_X_test\n        select_X_test = selection.transform(X_test)\n        y_pred = selection_model.predict(select_X_test)\n        selection_model_r2 = model_r2(y_test, y_pred)\n        print(\"Thresh = {:.7f}, n = {}, R2 = {:.5f}\".format(thresh, select_X_train.shape[1], selection_model_r2))\n\n        # Append the results to a r2_feat_importance for consolidation            \n        new_entry = [thresh, select_X_train.shape[1], selection_model_r2]\n        r2_feat_importance.loc[len(r2_feat_importance)] = new_entry\n    else:\n        continue\n                \nend = time.time()\nprint('Time taken to run:', (end-start)\/60)\n\n# Show final 'r2_feat_importance' table\nprint(r2_feat_importance)","d488b321":"# Plot a graph to see the performance of XGB for different number of features\nplt.subplots(figsize=(15, 10))\nplt.scatter(x = r2_feat_importance['no_features'], y = r2_feat_importance['threshold_r2'], s = 5)\nplt.xlabel('No. of Features in XGB')\nplt.ylabel('R2 - Performance of XGB')\nplt.show()","8d9b8c20":"row_max_r2 = r2_feat_importance[r2_feat_importance['threshold_r2'] == r2_feat_importance['threshold_r2'].max()]\nprint(row_max_r2)\n\n# Number of features for min rmse\nno_features_max_r2 = row_max_r2['no_features'].values[0]\nprint('No. of features for max. R2 score:', no_features_max_r2)\n\n# Corresponding threshold\nthreshold_max_r2 = row_max_r2['thresholds'].values[0]\nprint('Threshold value for max. R2 score: {:.6f}'.format(threshold_max_r2))","9a947a46":"# We use k-fold cross validation\nk_folds = 10\n\n### Retrain the XGB model using X features only on FULL TRAINING DATA\n\n# Modify the function for calculating mean RMSE\ndef r2_model_feat_impt(model):\n    kf_cv = KFold(k_folds, shuffle = True, random_state = 123).get_n_splits(select_train)\n    r2 = cross_val_score(\n            model,\n            select_train,\n            y_train,\n            scoring = \"r2\",\n            cv = kf_cv\n    )\n    return(r2)\n\nstart = time.time()\nselection = SelectFromModel(xgb, threshold = threshold_max_r2, prefit = True)\nselect_train = selection.transform(X_train)\nselect_test = selection.transform(X_test)\n            \n# Define model again\nselection_model = XGBRegressor(\n    colsample_bytree=0.2,\n    gamma=0.0,\n    learning_rate=0.01,\n    max_depth=5, \n    min_child_weight=1.5,\n    n_estimators=4000,\n    reg_alpha=0.9,\n    reg_lambda=0.6,\n    subsample=0.8,\n    verbosity=0,\n    random_state = 7,\n    objective='reg:squarederror',\n    n_jobs = -1\n)\n\n# KFolds CV:\nCV_r2 = r2_model_feat_impt(selection_model)\n\n# Print result\nprint('Mean R2 of XGB training using {} features is {:.6f}'.format(no_features_min_r2, CV_r2.mean()))\n\nend = time.time()\nprint('Time taken to complete {:.2f} mins'.format((end-start)\/60))","e1625915":"start = time.time()\ngrid_params = {\n    'learning_rate':[0.001, 0.01],\n    'n_estimators':[4000, 1000],\n    'reg_alpha': [0.3, 0.6, 0.9],\n    'reg_lambda': [0.3, 0.6, 0.9],\n    'max_depth': [3, 5, 7],\n    'subsample': [0.6, 0.8]\n}\n\nGS_xgb = GridSearchCV(\n    estimator = XGBRegressor(\n        min_child_weight=1.5,\n        colsample_bytree=0.2,\n        gamma=0.0,\n        verbosity=0,\n        random_state = 7,\n        objective='reg:squarederror',\n        n_jobs = -1\n    ),\n    param_grid = grid_params,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1,\n    iid=False,\n    cv=5,\n    verbose = 1\n    )\n\n# Select_train is the full training set with just 21 feature columns\nGS_xgb.fit(select_train,y_train)\n\n#GS_gradboost_1.grid_scores_,\nbestparam = GS_xgb.best_params_\nbestscore = GS_xgb.best_score_\n\nprint('bestscore is:', bestscore)\nprint('bestparam is:', bestparam)\nend = time.time()\nprint('Time taken to run is:', end - start)","3307ee4f":"### Retrain model again with best parameters from GridSearchCV\n\nfinal_xgb = XGBRegressor(\n    learning_rate=bestparam['learning_rate'],\n    max_depth=bestparam['max_depth'], \n    subsample=bestparam['subsample'],\n    n_estimators=bestparam['n_estimators'],\n    reg_alpha=bestparam['reg_alpha'],\n    reg_lambda=bestparam['reg_lambda'],\n    min_child_weight=1.5,\n    colsample_bytree=0.2,\n    gamma=0.0,\n    verbosity=0,\n    random_state = 7,\n    objective='reg:squarederror',\n    n_jobs = -1\n)\n\nfinal_xgb.fit(select_train, y_train)\n\n# Churn out the predictions based on this final model\nfinal_xgb_pred = final_xgb.predict(select_test)\n\n# Calculate RMSE \nfinal_xgb_r2 = model_r2(y_test, final_xgb_pred)\nprint('R2 of final XGBRegressor is: {:.6f}'.format(final_xgb_r2))","d04e17d1":"### Model Sales","9858731f":"### Price vs Year for All Models - Boxplot\nWith boxplot we can immediately see what are the outliers for each year","7a7ca898":"### Comparing the most and least expensive models\nBased on the Price Variation for Each Model chart, we can tell the range of price that each model. Let's compare the Supra (most expensive) and IQ (least expensive).\nWe can probably see what are the relationships between the price and other features for each model.\n\n#### Registration Year","e4f40372":"### Price vs Year for All Models - Scatterplot","4cb9c480":"The distribution plots show that:\n1. The average selling price is around 10,000 pounds\n2. Most of the cars have mileage below 37,500 miles\n3. The road tax on these cars is below 50 pounds or between 100 and 175 pounds\n4. Most of the cars were registered in the past 5 years ","974e6277":"### K-Fold Cross-Validation","152d8238":"For the IQ, there is a clearer trend where mileage is inversely related to the selling price if we exclude the 2 outliers (70,000 miles at 5000 pounds, and 27,000 miles at 4000 pounds)\n\n#### Land Cruiser\nLet's also examine a model with a huge range of selling prices.","efb44330":"Besides using transmission as 'hue' to add more information to our chart, we have also tried with engineSize and tax. But engineSize and tax for these Supras are almost the same.\nExcluding the outlier of 60,000 pounds for a Supra with a mileage of nearly 10,000 miles, there seems to be no clear trend between the price and mileage for Supras. We ought to expect that cars with lower mileage should fetch higher prices, but this appears not to be the case with some of these Supras - 2 cars with less than 500 miles of mileage sold for less than cars with more than 2000 miles of mileage.\n\n#### IQ\nLet's examine the same thing for the IQ","8de329c2":"## Establishing a Baseline Model\nBefore we dive into feature engineering, we should get a benchmark first.\n\nWe need to encode the categorical features for our models:","1413b975":"The chart for the Land Cruiser clearly shows a trend where greater mileages would sell for lower prices as expected.","d77dc5ad":"### Price Variation for Each Model","b4286f57":"## Simple Cleaning Up\nRemove whitespaces in front and behind of each value in columns with string. i.e. model, transmission, fuelType, ","6a0f815f":"#### Some improvements with using just 21 features\n1. Before feature selection - R2 = 0.966465\n2. After feature selection - R2 = 0.966757\n\nAn improvement of only 0.03% ","52055428":"#### Hyperparamter Tuning using GridSearchCV for XGBoostRegressor\nFor this section, we will perform hyperparameter tuning using GridSearch to try and optimize the models.","43f9269b":"## Import Datasets","ce682134":"It shows that it is better to use the top 21 features to train our model, in order to arrive at the maximum R2 score.\nWe will use KFolds CV to retrain the XGB with full training set and with X number of features selected based on the threshold value. Observe if there is any improvement to the R2 score.","30600aeb":"## Data Visualization\nSince there are no null values, we can dive straight into visualising the data.\nNote that the year in the dataset represents the registration year of the car, not the year of sale.\n\n### Distribution of Features\nLet's look at the distribution of the following numerical features:\n1. Mileage\n2. Tax\n3. Price\n4. Year of Registration","d6bc9221":"### Price vs Mileage - Scatterplot","26e0e332":"#### Train Test Split","80853d32":"#### Method: SelectFromModel\nLet's try to use SelectFromModel to select the number of features to consider based on the threshold level.\n\n1. The feature importance of each feature in the XGB are obtained from xgb.featureimportances\n2. SelectFromModel will take a pre-trained model - e.g. a model that is trained on the entire training dataset - and then use a Threshold to decide which features to select\n3. Each feature has a relative importance as stored in xgb.featureimportances. For a certain threshold value, there will be X number of features once a features' relative importance passes the threshold value.\n4. This threshold is used when the transform() method is called to select the same features on the training and test datatsets.\n\nWe first train and then evaluate an XGBoost model on the entire training dataset and test datasets respectively.\n\nUsing the feature importances calculated from the training dataset, we then wrap the model in a SelectFromModel instance. We use this to select features on the training dataset, train a model from the selected subset of features, then evaluate the model on the testset, subject to the same feature selection scheme.","134c43f4":"We notice 2 interesting patterns from the above chart. For the Supra, the more expensive model, the registration year is 2019 when the model was first released. Most of the selling prices are around 50,000 pounds.\nFor the IQ, the range of selling price seems to be around 5000 pounds, even though the registration years of the car ranges from 2009 to 2013. \n\nThis shows that there are other factors at play that determine the selling price of a car.\n\n#### Supra","a40a652e":"It seems that our best baseline model is XGBoostRegressor.\nNow, we can do feature engineering and subsequently hyperparameter tuning to improve the model.\n\n### Feature Importance and Selection on XGBoost"}}