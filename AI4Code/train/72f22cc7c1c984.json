{"cell_type":{"eb6b8a3f":"code","059418dc":"code","73296f31":"code","555ebaa8":"code","fde9e4a0":"code","4fdcf4ce":"code","b362e2ba":"code","72bfcb74":"code","edc2fe62":"code","1dab33ad":"code","fbf62d82":"code","14736a7e":"markdown","b080e563":"markdown","b6f91fa9":"markdown","6d388525":"markdown","910738d2":"markdown","c9b2170e":"markdown","8084996c":"markdown","78235770":"markdown","91e2578f":"markdown","0cda3b83":"markdown","afe44be4":"markdown"},"source":{"eb6b8a3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","059418dc":"import time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble        import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier \nfrom sklearn.neighbors       import KNeighborsClassifier\nfrom sklearn.svm             import LinearSVC\nfrom sklearn.naive_bayes     import GaussianNB\nfrom sklearn.linear_model    import LogisticRegression\nfrom sklearn.neural_network  import MLPClassifier\nfrom sklearn.tree            import DecisionTreeClassifier\nfrom sklearn.svm             import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model    import LogisticRegression\nfrom lightgbm                import LGBMClassifier\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.calibration     import calibration_curve\nfrom sklearn.pipeline        import make_pipeline\nfrom sklearn.metrics         import plot_confusion_matrix\nfrom sklearn.metrics         import classification_report, confusion_matrix, accuracy_score\n%matplotlib inline","73296f31":"N_SPLITS   = 2         # Number of Stratified K Folds\nDATA_PERC  = 1         # Percentage of the training set (execution speed parameter during dev & test)\nSEED       = 1         # Random seed","555ebaa8":"all_train = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\nall_test = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")","fde9e4a0":"print(\"+-------------------- train ----------------------------+ +-------------------- test ----------------------------+\")\nrf = lambda df :  [ \"{:>10s} - Count: {:4d} - Nan: {:4d} - type:{:10s}\".format(col,df[col].count(),\n                                                                 df[col].isna().sum(),\n                                                                 df.dtypes[col].name) for col in df.columns]\nprint(\"\\n\")\n_ = [print(tr + tr) for (tr,te) in zip(rf(all_train),rf(all_test))]\n","4fdcf4ce":"all_train.describe().T.style.background_gradient(cmap='YlOrRd',vmin=0,vmax=1,subset=pd.IndexSlice[:,'mean':'max'])","b362e2ba":"all_test.describe().T.style.background_gradient(cmap='YlOrRd',vmin=0,vmax=1,subset=pd.IndexSlice[:,'mean':'max'])","72bfcb74":"def build_feature(no_hot_array, hot_array,dataset) :\n        tmp = dataset[no_hot_array]\n        for a_hot in hot_array :\n             tmp = pd.concat([tmp, pd.get_dummies(dataset[a_hot],prefix=a_hot)], axis=1);   \n        return tmp\n    \ncont_columns = [col for col in all_train.columns if 'cont' in col]\ncat_columns  = [col for col in all_train.columns if 'cat' in col]\n\nall_data = pd.concat([all_train,all_test]).reset_index(drop=True)\nall_data_encoded  = build_feature(cont_columns, cat_columns,all_data)\nall_train_encoded = all_data_encoded[:all_train.shape[0]]\nall_test_encoded  = all_data_encoded[all_train.shape[0]:]\nprint(\"all_data.shape          : \",all_data.shape,\" - all_data_encoded.shape : \",all_data_encoded.shape, \" \")\nprint(\"all_train_encoded.shape : \",all_train_encoded.shape,\" - all_test_encoded.shape : \",all_test_encoded.shape, \" \")\n","edc2fe62":"np.random.seed(SEED)\nmask = np.random.rand(all_train_encoded.shape[0]) <= DATA_PERC\ntrain_encoded = all_train_encoded[mask]","1dab33ad":"classifiers = [\n    LGBMClassifier(),\n#         KNeighborsClassifier(4),\n#         SVC(probability=True),\n#         DecisionTreeClassifier(),\n    RandomForestClassifier (n_estimators=20, random_state=0),\n#         AdaBoostClassifier(),\n#         GradientBoostingClassifier(),\n    GaussianNB(),\n    MLPClassifier( solver='adam', alpha=0.314, random_state=1, max_iter=4000,\n                       early_stopping=True, hidden_layer_sizes=[40, 40, 40], ),\n    LinearSVC(C=1.0),\n    LinearDiscriminantAnalysis(),\n#         QuadraticDiscriminantAnalysis(),\n#         LogisticRegression(max_iter=4000)\n]\n\nX = train_encoded.values\ny = all_train[mask].loc[:,\"target\"].values\nlog_res  = []\nfor clf in classifiers :\n        print(\"\\n----------------------------------------------------------\" )\n        print(\"Classifier: {:20s} - # fold: {:2d}\".format(clf.__class__.__name__,N_SPLITS) )\n        skf = StratifiedKFold(n_splits=N_SPLITS)\n        # skf.get_n_splits(X, y)\n        fold_no = 1\n        accuracy = []\n        for train_index, test_index in skf.split(X, y):\n            tic = time.perf_counter()\n            print(\"   fold: {:2d} -\".format(fold_no), end=\" \")\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n#             print(confusion_matrix(y_test,y_pred))\n#             print(classification_report(y_test,y_pred))\n            acc = accuracy_score(y_test, y_pred)\n            toc = time.perf_counter()\n            print(\"accuracy: {:2.2f} time: {:0.1f} sec\".format(100*acc,toc - tic))\n            accuracy.append(acc)\n            fold_no = fold_no + 1\n        log_entry = [clf, np.mean(accuracy) ,toc - tic,sum(y_train)\/len(y_train)]\n        log_res.append(log_entry)\n\nlog_cols = [\"Classifier\", \"Accuracy\",\"Time\",\"target distrib. %\"]\nlog \t = pd.DataFrame(log_res, columns=log_cols)\nprint(log.sort_values(['Accuracy'],ascending=False))\n\n# Get best classifier \n\nbest_clf = log.loc[0,\"Classifier\"] \nprint(\"\\nBest classifier :\",best_clf)\n","fbf62d82":"if DATA_PERC == 1 : \n    # Train best_clf on the entire train set\n    best_clf.fit(X,y)\n    y_train_pred = best_clf.predict(all_train_encoded)\n    acc = accuracy_score(y_train_pred, y)\n    print(\" Accuracy against the all train set : {:.4f}\".format(acc))\n\n    # Test prediction\n\n    y_test_pred = best_clf.predict(all_test_encoded)\n    print(log)\n\n    # Saving the file\n    sub = pd.DataFrame({'id': all_test['id'].values, 'target': y_test_pred})\n    sub.to_csv('sub.csv', index=False)\nelse :\n    print(\"Set DATA_PERC to 1 to save submission\")","14736a7e":"# Read data file","b080e563":"# Review data","b6f91fa9":"# Review each classifiers","6d388525":"# Import","910738d2":"# Setting parameter","c9b2170e":"## Translate categorical feature\nAcross the all data set (train and test) to ensure encoded feature consistency","8084996c":"## Review NaN impact","78235770":"# Prepare submission\nUse the best performing classifier","91e2578f":"## Test data","0cda3b83":"## Select a subset of train data set for shorter runtime, during notebook developement","afe44be4":"## Training data"}}