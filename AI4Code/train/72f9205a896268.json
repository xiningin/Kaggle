{"cell_type":{"9a343c19":"code","2cecf624":"code","b687656b":"code","32320a00":"code","3e3ae4d7":"code","b2c268b4":"code","793753a0":"code","923bb263":"code","79b6709d":"code","e9a981a2":"code","d1dcda4a":"code","2c842bd8":"code","3cc5e0bb":"code","b6176db0":"code","b33d63cf":"code","49dfa1c5":"code","aa408eb9":"code","e4f596f6":"code","385d26d1":"code","9cfce7b5":"code","528f5aa6":"code","af0d67b4":"code","1e239742":"code","d3907359":"code","9ee78bd5":"code","af662854":"code","9b341537":"code","87f09af3":"code","2c80f62f":"code","cde77426":"markdown","5d107be5":"markdown","e5b1f97d":"markdown","3a6418cb":"markdown","9377b466":"markdown","9c379365":"markdown","932c9c2a":"markdown","d484f41d":"markdown","cca7669e":"markdown","d901fb8c":"markdown","22fa2bb6":"markdown","ae5881bb":"markdown","477ce6bb":"markdown","97114310":"markdown","87c01320":"markdown","a372638a":"markdown","223c8ec0":"markdown","0488155b":"markdown","701ad4df":"markdown","6bd17975":"markdown","87216095":"markdown","24f7f806":"markdown","cf7a251c":"markdown","067e494c":"markdown","a4cbace0":"markdown","b16ed215":"markdown","eff54bf5":"markdown","c9f4e30a":"markdown","85aae20b":"markdown","ef9a5266":"markdown","0f66b7d2":"markdown","5a06bc6b":"markdown","ef579440":"markdown","0dee7fc1":"markdown"},"source":{"9a343c19":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2cecf624":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_palette('husl')\nimport matplotlib.pyplot as plt\n%matplotlib inline","b687656b":"#Load Data\niris = pd.read_csv('..\/input\/Iris.csv')","32320a00":"iris.shape","3e3ae4d7":"# View Top 5 records\niris.head()","b2c268b4":"# View bottom 5 records\niris.tail()","793753a0":"iris.info()","923bb263":"iris.describe()","79b6709d":"iris['Species'].value_counts()","e9a981a2":"iris1 = iris.drop('Id', axis=1)\ng = sns.pairplot(iris1, hue='Species', markers='+')\nplt.show()","d1dcda4a":"fig = iris1[iris1.Species=='Iris-setosa'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='orange', label='Setosa')\niris1[iris1.Species=='Iris-versicolor'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='blue', label='versicolor',ax=fig)\niris1[iris1.Species=='Iris-virginica'].plot(kind='scatter',x='SepalLengthCm',y='SepalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Sepal Length\")\nfig.set_ylabel(\"Sepal Width\")\nfig.set_title(\"Sepal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.show()","2c842bd8":"fig = iris1[iris1.Species=='Iris-setosa'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='orange', label='Setosa')\niris1[iris1.Species=='Iris-versicolor'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='blue', label='versicolor',ax=fig)\niris1[iris1.Species=='Iris-virginica'].plot.scatter(x='PetalLengthCm',y='PetalWidthCm',color='green', label='virginica', ax=fig)\nfig.set_xlabel(\"Petal Length\")\nfig.set_ylabel(\"Petal Width\")\nfig.set_title(\" Petal Length VS Width\")\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.show()","3cc5e0bb":"iris1.hist(edgecolor='black')\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","b6176db0":"g = sns.violinplot(y='Species', x='SepalLengthCm', data=iris1, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='SepalWidthCm', data=iris1, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalLengthCm', data=iris1, inner='quartile')\nplt.show()\ng = sns.violinplot(y='Species', x='PetalWidthCm', data=iris1, inner='quartile')\nplt.show()","b33d63cf":"plt.figure(figsize=(10,8)) \nsns.heatmap(iris1.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","49dfa1c5":"X = iris.drop(['Id', 'Species'], axis=1)\ny = iris['Species']\n# print(X.head())\nprint(X.shape)\n# print(y.head())\nprint(y.shape)","aa408eb9":"from sklearn.model_selection import train_test_split  #to split the dataset for training and testing","e4f596f6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","385d26d1":"X_train.head()","9cfce7b5":"X_test.head()","528f5aa6":"y_train.head()","af0d67b4":"y_test.head()","1e239742":"# Importing alll the necessary packages to use the various classification algorithms\n\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier  # for K nearest neighbours\nfrom sklearn import metrics #for checking the model accuracy","d3907359":"logr = LogisticRegression()\nlogr.fit(X_train,y_train)\ny_pred = logr.predict(X_test)\nacc_log = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the Logistic Regression is', acc_log)","9ee78bd5":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\nacc_dt = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the Decision Tree is', acc_dt)","af662854":"sv = svm.SVC() #select the algorithm\nsv.fit(X_train,y_train) # we train the algorithm with the training data and the training output\ny_pred = sv.predict(X_test) #now we pass the testing data to the trained algorithm\nacc_svm = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the SVM is:', acc_svm)","9b341537":"knc = KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nknc.fit(X_train,y_train)\ny_pred = knc.predict(X_test)\nacc_knn = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the KNN is', acc_knn)","87f09af3":"a_index = list(range(1,11))\na = pd.Series()\nx = [1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    kcs = KNeighborsClassifier(n_neighbors=i) \n    kcs.fit(X_train,y_train)\n    y_pred = kcs.predict(X_test)\n    a=a.append(pd.Series(metrics.accuracy_score(y_pred,y_test)))\nplt.plot(a_index, a)\nplt.xticks(x)","2c80f62f":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree', 'Support Vector Machines',\n              'K-Nearest Neighbours'],\n    'Score': [acc_log, acc_dt, acc_svm, acc_knn]})\nmodels.sort_values(by='Score', ascending=False)","cde77426":"### Import necessary libraries","5d107be5":"As we see above, there are no null values in the dataset, so the data can be processed","e5b1f97d":"### Logistic Regression","3a6418cb":"#### Classification Algorithms: which we used with this IRIS (structured) dataset\n\n1. Logistic Regression\n2. Decision Tree\n3. Support Vector Machine (SVM)\n4. K-Nearest Neighbours","9377b466":"#### Steps To Be followed When Applying an Algorithm\n**Step 1:** Split the dataset into training and testing dataset. The testing dataset is generally smaller than training one as it will help in training the model better.\n\n**Step2:** Select any algorithm based on the problem (classification or regression) whatever you feel may be good.\n\n**Step3:** Then pass the training dataset to the algorithm to train it. We use the **.fit()** method\n\n**Step4:** Then pass the testing data to the trained algorithm to predict the outcome. We use the .predict() method.\n\n**Step5:** We then check the accuracy by passing the predicted outcome and the actual output to the model.","9c379365":"#### Let's check, If there is any inconsistency in the dataset","932c9c2a":"As we all know that the given problem is a classification problem. Thus we will be using the classification algorithms to build a model.\n\nBefore we start, we need to clear some ML notations.\n\n**Attributes-->** An attribute is a property of an instance that may be used to determine its classification. In the following dataset, the attributes are the petal and sepal length and width. It is also known as Features.\n\n**Target Variable-->**, In the machine learning context is the variable that is or should be the output. Here the target variables are the 3 flower species.","d484f41d":"#### Now let us see how the length and width vary according to the species","cca7669e":"Here, We have implemented some of the common Machine Learning classification algorithms. Since given dataset is small with very few features, I didn't cover some concepts as they would be relevant when we have many features.\n\nI hope this kernal is useful to you to learn machine learning from the scratch with IRIS dataset.\n\nIf find this notebook help you to learn, **Please Upvote.**\n\n##### Thank You!!","d901fb8c":"#### Visualization1 : Sepal Length VS Width\nThis graph shows relationship between the **sepal length** and **sepal width**.","22fa2bb6":"### Exploratory Data Analysis (EDA)","ae5881bb":"### Decision Tree","477ce6bb":"Let's check the Train and Test Dataset","97114310":"Let's check the accuracy for various values of n for K-Nearest nerighbours","87c01320":"* After graphing the features in a pair plot, it is clear that the relationship between pairs of features of a iris-setosa (in pink) is distinctly different from those of the other two species.\n* There is some overlap in the pairwise relationships of the other two species, iris-versicolor (brown) and iris-virginica (green).","a372638a":"#### Data Visualization","223c8ec0":"As we can see that the Petal Features are giving a better cluster division compared to the Sepal features. This is an indication that the Petals can help in better and accurate Predictions over the Sepal. We will check that later.","0488155b":"#### Visualization2 : Sepal Length VS Width\n> This graph shows relationship between the **petal length** and **petal width**.","701ad4df":"## The Iris classification Problem\nImagine you are a botanist seeking an automated way to categorize each Iris flower you find. Machine learning provides many algorithms to classify flowers statistically. For instance, a sophisticated machine learning program could classify flowers based on photographs. Our ambitions are more modest\u2014we're going to classify Iris flowers based on the length and width measurements of their sepals and petals.\n\nThe Iris genus entails about 300 species, but our program will only classify the following three:\n\n* Iris setosa\n* Iris virginica\n* Iris versicolor\n","6bd17975":"#### Advantages\n* By splitting the dataset pseudo-randomly into a two separate sets, we can train using one set and test using another.\n* This ensures that we won't use the same observations in both sets.\nMore flexible and faster than creating a model using all of the dataset for training.\n\n#### Disadvantages\n* The accuracy scores for the testing set can vary depending on what observations are in the set.\n* This disadvantage can be countered using k-fold cross-validation.\n\n#### Notes\n* The accuracy score of the models depends on the observations in the testing set, which is determined by the seed of the pseudo-random number generator (random_state parameter).\n* As a model's complexity increases, the training accuracy (accuracy you get when you train and test the model on the same data) increases.\n* If a model is too complex or not complex enough, the testing accuracy is lower.\n* For KNN models, the value of k determines the level of complexity. A lower value of k means that the model is more complex.","87216095":"![](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/Machine+Learning+R\/iris-machinelearning.png)","24f7f806":"### Import or upload dataset","cf7a251c":"### Support Vector Machine (SVM)","067e494c":"# The Iris classification Problem\n![](http:\/\/wildadirondacks.org\/images\/Adirondack-Wildflowers-Blue-Flag-Iris-Iris-versicolor-Cemetery-Road-Wetlands-12-June-2018-71.jpg)\n\n#### Hello Dear Kagglers !!\n\nThis is a very basic tutorial to learn Machine Learning from scratch using the Iris Dataset. Here you learn how to implement a machine learning to a given dataset by following this notebook. I try to explain everything related to the implement ML in detail. Hope you find it useful learning material.\n\nFor a more advanced notebook that covers some more detailed concepts, have a look at this notebook.\n\nIf this notebook to be useful, **Please Upvote**!!!","a4cbace0":"#### Let's check some statistical facts","b16ed215":"### Spilit the training and test dataset","eff54bf5":"* There are 150 observations with 4 features each (sepal length, sepal width, petal length, petal width).\n* There are no null values, so we don't have to worry about that.\n* There are 50 observations of each species (setosa, versicolor, virginica).","c9f4e30a":"### K-Nearest Neighbours","85aae20b":"Now, when we train any algorithm, the number of features and their correlation plays an important role. If there are features and many of the features are highly correlated, then training an algorithm with all the featues will reduce the accuracy. Thus features selection should be done carefully. This dataset has less featues but still we will see the correlation.","ef9a5266":"#### Preview of Data","0f66b7d2":"## Build Model with Scikit-learn","5a06bc6b":"#### Now let us see how are the length and width are distributed","ef579440":"### Let's view top 5 and bottom 5 records from dataset","0dee7fc1":"The Sepal Width and Length are not correlated The Petal Width and Length are highly correlated\n\nWe will use all the features for training the algorithm and check the accuracy.\n\nThen we will use 1 Petal Feature and 1 Sepal Feature to check the accuracy of the algorithm as we are using only 2 features that are not correlated. Thus we can have a variance in the dataset which may help in better accuracy. We will check it later."}}