{"cell_type":{"ab7c8f5c":"code","6028d923":"code","c59260c3":"code","d91f2507":"code","0750f644":"code","51c5788a":"code","4979dec8":"code","2f536ca7":"code","d7ad0036":"code","e3f49db5":"code","6761ee08":"code","e5790c8c":"code","b38781a5":"code","b86e824a":"code","966a055d":"code","049e1aa3":"code","d79e7483":"code","b66b1315":"code","24b2ff07":"code","b2383a77":"code","39d9b33d":"code","a79333da":"markdown"},"source":{"ab7c8f5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nwarnings.filterwarnings('ignore')","6028d923":"# Import Test and Train Data\ntrain_data=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","c59260c3":"# Display first ten data\npd.set_option('display.max_columns', 500)\ntrain_data.head(10)","d91f2507":"#Combining Train and Test data \ntrain_data.describe().T","0750f644":"#temp=train_data.pop('Id')\ncorrmat = train_data.corr()\nplt.subplots(figsize=(16,8))\nsns.heatmap(corrmat,cmap=\"YlGnBu\")","51c5788a":"# Checking correlation with Sales Price\n\nCorr_Column = list(train_data.corr()[\"SalePrice\"][(train_data.corr()[\"SalePrice\"]>0.50) | (train_data.corr()[\"SalePrice\"]<-0.50)].index)\n\n#print(Corr_Column)\n\nsns.pairplot(train_data[Corr_Column])","4979dec8":"# Target variable:\nsns.distplot(train_data['SalePrice'])\nplt.show()","2f536ca7":"# Check Missing Values\ncombined_data = pd.concat((train_data, test_data)).reset_index(drop=True)\ncombined_data.drop(['SalePrice'],axis=1,inplace=True)\n\n\nmissing_details=combined_data.isnull().sum()*100\/train_data.shape[0]\n\nmissing_details=missing_details.drop(missing_details[missing_details==0].index).sort_values(ascending=False)\n\nmissing_data=pd.DataFrame({'Missing_Ratio':missing_details})\nprint(missing_data.head(33))\nprint(missing_data.shape[0])\n","d7ad0036":"#Missing Value Imputation\ndef fill_NA(df,feature):\n    df[feature]=df[feature].fillna(feature+'_NA')\n    \nfor col in ('PoolQC','MiscFeature','MasVnrType','Alley','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual',\n            'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    fill_NA(combined_data,col)    \n\ncombined_data['LotFrontage']=combined_data['LotFrontage'].fillna(train_data['LotFrontage'].median())\ncombined_data['GarageYrBlt']=combined_data['GarageYrBlt'].fillna(0)\ncombined_data['GarageArea']=combined_data['GarageArea'].fillna(0)\ncombined_data['GarageCars']=combined_data['GarageCars'].fillna(0)\ncombined_data['MasVnrArea']=combined_data['MasVnrArea'].fillna(0)\ncombined_data['MSZoning']=combined_data['MSZoning'].fillna(train_data['MSZoning'].mode()[0])\ncombined_data['Electrical']=combined_data['Electrical'].fillna(train_data['Electrical'].mode()[0])\ncombined_data['KitchenQual']=combined_data['KitchenQual'].fillna(train_data['KitchenQual'].mode()[0])\ncombined_data['Exterior2nd']=combined_data['Exterior2nd'].fillna(train_data['Exterior2nd'].mode()[0])\ncombined_data['Exterior1st']=combined_data['Exterior1st'].fillna(train_data['Exterior1st'].mode()[0])\ncombined_data['SaleType']=combined_data['SaleType'].fillna(train_data['SaleType'].mode()[0])\n\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    combined_data[col] = combined_data[col].fillna(0)\n    \ncombined_data=combined_data.drop(['Utilities'],axis=1)\ncombined_data[\"Functional\"] = combined_data[\"Functional\"].fillna(\"Typ\")   \n\ncombined_data.head()","e3f49db5":"# Create_Dummy variables for Train and Test Data\nntrain = train_data.shape[0]\nntest = test_data.shape[0]\ny = train_data.SalePrice.values\n\n\ncombined_categorical = combined_data.select_dtypes(include=['object'])\ncombined_dummies = pd.get_dummies(combined_categorical, drop_first=True)\ncombined = combined_data.drop(list(combined_categorical.columns), axis=1)\ncombined = pd.concat([combined, combined_dummies], axis=1)\ncombined.head()","6761ee08":"# Modeling\n\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet,Lasso,Ridge\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n","e5790c8c":"X_train = combined[:ntrain]\nX_test = combined[ntrain:]\n\n\nx_train, x_test, y_train, y_test = train_test_split(X_train, y, test_size = 0.3, random_state = 0)\nprint(\"X_train : \" + str(x_train.shape))\nprint(\"X_test : \" + str(x_test.shape))\nprint(\"y_train : \" + str(y_train.shape))\nprint(\"y_test : \" + str(y_test.shape))","b38781a5":"#Evaluation Function\nn_folds=5\ndef rmsle_cv_train(model):\n    kf=KFold(n_folds,shuffle=True,random_state=100).get_n_splits(x_train.values)\n    rmse=np.sqrt(-cross_val_score(model,x_train.values,y_train,scoring=\"neg_mean_squared_error\",cv=kf))\n    return rmse\n\ndef rmsle_cv_test(model):\n    kf=KFold(n_folds,shuffle=True,random_state=100).get_n_splits(x_test.values)\n    rmse=np.sqrt(-cross_val_score(model,x_test.values,y_test,scoring=\"neg_mean_squared_error\",cv=kf))\n    return rmse\n\n\ndef model_eval(model,title):\n    print(\"RMSE on Training set :\", rmsle_cv_train(model).mean())\n    print(\"RMSE on Test set :\", rmsle_cv_test(model).mean())\n    y_train_pred = model.predict(x_train)\n    y_test_pred = model.predict(x_test)\n    \n    \n    #Residual Plot\n    plt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(title)\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Residuals\")\n    plt.legend(loc = \"upper left\")\n    plt.show()\n    \n    \n    # Prediction Plot\n    plt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(title)\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Real values\")\n    plt.legend(loc = \"upper left\")\n    plt.show()","b86e824a":"\n#Base Model Linear Regression\nlr=LinearRegression()\nlr.fit(x_train,y_train)\nmodel_eval(lr,'Linear_Regression')","966a055d":"# Model_2 Ridge Regression\nridge=Ridge()\nridge.fit(x_train,y_train)\n\nmodel_eval(ridge,'Ridge_Regression')","049e1aa3":"# Model_3 Lasso Regression\nlasso=Lasso()\nlasso.fit(x_train,y_train)\n\nmodel_eval(lasso,'Lasso_Regression')","d79e7483":"#Elastic Net\n\nelastic=ElasticNet()\nelastic.fit(x_train,y_train)\n\nmodel_eval(elastic,'Elastic_Regression')","b66b1315":"# Support Vector Regression\nsvr=SVR(C=100000)\nsvr.fit(x_train,y_train)\n\nmodel_eval(svr,\"Support Vector Regression\")\n","24b2ff07":"# Random Forest Regression\nRFR=RandomForestRegressor(n_estimators=110)\nRFR.fit(x_train,y_train)\n\nmodel_eval(RFR,\"Random Forest Regression\")\n","b2383a77":"#XG Boost Regressor\nxgb=XGBRegressor(n_estimators=110,learning_rate=0.1)\nxgb.fit(x_train,y_train)\n\nmodel_eval(xgb,\"XGBOOST\")","39d9b33d":"sub = pd.DataFrame()\nsub['Id'] = test_data['Id']\nsub['SalePrice'] = RFR.predict(X_test)\nsub.to_csv('submission.csv',index=False)","a79333da":"# Here Random Forest is the better estimator"}}