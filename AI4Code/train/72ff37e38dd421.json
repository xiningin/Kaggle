{"cell_type":{"cba2ec27":"code","0c0149e4":"code","a9dd18ca":"code","ea019d5c":"code","1db3063f":"code","642a9a42":"code","b38a73e4":"code","5b0df780":"code","ef8f6cc8":"code","608f3f22":"code","96d2a412":"code","33147178":"code","94311c0c":"code","3cc4dad2":"code","b6213cc9":"code","34df9062":"code","630a4bbe":"code","a6ae7099":"code","9bff9b0c":"code","f65a4cb9":"code","cb468641":"code","f728cc16":"code","f3e7b3e9":"code","d1056a9a":"markdown","8a5ae6ed":"markdown","bb9dae0a":"markdown","b32f3bbe":"markdown","4da8e87b":"markdown","deb41f12":"markdown","c42363c6":"markdown","08a8b7c6":"markdown"},"source":{"cba2ec27":"# Import libraries\nimport seaborn as sns\n\n#Set Seaborn Style\n\nsns.set(style=\"whitegrid\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","0c0149e4":"# Bring training and test data into environment\ndf_train = pd.read_csv(\"..\/input\/proxymeanstest-cr\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/proxymeanstest-cr\/test.csv\")","a9dd18ca":"# Let's look at the shape of our dataframes to confirm the number of columns and rows\nprint(df_train.shape)\nprint(df_test.shape)","ea019d5c":"#Let's display the heads of both dataframes to make sure they imported correctly\npd.set_option('display.max_columns', 200)\ndf_train.head()","1db3063f":"df_test.head()","642a9a42":"# Summary Statistics\ndf_train.describe()","b38a73e4":"df_test.describe()","5b0df780":"# In order to continue, we should understand how many unique households are in the data set vs. the number of individuals\nprint(df_train.idhogar.nunique())\nprint(df_test.idhogar.nunique())","ef8f6cc8":"# Creating smaller dataframe that removes missing values so that we can use seaborn\ndf_1 = df_train[pd.notnull(df_train['v2a1'])]\n\nsns.distplot(df_1.v2a1)","608f3f22":"#The outliers cause the scaling to function strangely. I'd like to have a clean look at what's going on at the lower incomes.\ndf_2 = df_train[df_train.v2a1 < 750000]\n\n#Boxplot for income by target group with upper outliers removed\nsns.boxplot(y=\"v2a1\", x=\"Target\", data=df_2)","96d2a412":"escuela_1 = sns.countplot(x=\"escolari\", data = df_train)","33147178":"counts_tar = sns.countplot(x=\"Target\", data = df_train)","94311c0c":"# Import libraries\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import RandomForestClassifier  \nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\n#We'll need this to ignore the warnings due to the df manipulation\nimport warnings\nwarnings.filterwarnings('ignore')\n","3cc4dad2":"# Remove object type columns\ndf_train_1 = df_train.select_dtypes(exclude=['object'])\n\n#These columns contain NA values\n# In order to understand data collection issues, I should make sure I understand where presumably \nlisto = df_train_1.columns[df_train_1.isna().any()].tolist()","b6213cc9":"# By imputing, we can regress data anomalies to the mean\n# What does this \"assumtpion\" mean?\n\n# It means that we assume missing values come from a place of collection shortfalls.\nimp=SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n\nfor i in listo:\n    imp.fit(df_train_1[[i]])\n    df_train_1[[i]] = imp.fit_transform(df_train_1[[i]])\n    \n#Perform the same for the test data\nfor i in listo:\n    imp.fit(df_test[[i]])\n    df_test[[i]] = imp.fit_transform(df_test[[i]])","34df9062":"# divide into attributes and labels (sklearn syntax-ism)\nX = df_train_1.drop('Target', axis=1)  \ny = df_train_1['Target']\n\n#80\/20 split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","630a4bbe":"# Fit our classifier using a random state to allow duplication of results\nclassifier = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=321)  \nclassifier.fit(X_train, y_train)","a6ae7099":"# Evaluating our model\ny_pred = classifier.predict(X_test)\nprint(\"Accuracy is \", accuracy_score(y_test,y_pred)*100)\nprint(\"Macro F1 Score is \", f1_score(y_test,y_pred, average ='macro'))\nprint(\"Weighted F1 Score is \", f1_score(y_test,y_pred, average ='weighted'))\n\n# Casting feature importances to a dataframe for easy sorting and \nfeature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': classifier.feature_importances_})\nfeature_importances.head()","9bff9b0c":"# Defining a function so that we can visualize feature importances\n# Why might feature importances interest us?\ndef plot_feature_importances(df, n = 10, threshold = None):\n\n    plt.style.use('fivethirtyeight')\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{n} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df\n\nnorm_fi = plot_feature_importances(feature_importances, threshold=0.95)","f65a4cb9":"#Precision, recall, and other metrics.\n# What do all of these metrics mean?\n# What do they tell us about the performance of the model?\nprint(metrics.classification_report(y_pred, y_test))","cb468641":"#Visualizing the confusion matrix for our model\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)\nplt.xlabel('true label')\nplt.ylabel('predicted label');","f728cc16":"df_train_2 = df_train_1.drop(['Target'], axis=1)\nfeatures = df_train_2.columns","f3e7b3e9":"#Let's make some predictions\n\nsample_pred = classifier.predict(df_test[features])\n\ndf = pd.DataFrame(sample_pred)\ndf_test['Target'] = df\ndf_final = df_test[['ID', 'Target']]\ndf_final.to_csv(\"\/kaggle\/working\/predictions_starter.csv\", index = False)\ndf_final.head()","d1056a9a":"\nAs shown above, there are significantly more individuals belonging to the \"4\" group than to any other `Target` group. By understanding this, we can avoid the pitfalls that might come from overreliance on accuracy. Think about this, if the data were 90% comprised of `4 = Target` individuals and our model randomly guessed \"4\" for every prediction, we would have a 90% accuracy. Even though this sounds like a reasonably successful model, _it isn't_. Using this visualization, we can also see that we might do well to measure our model's weighted F1 score, due to the skewwing towards class \"4\" (even though the contest makers decided to use macro F1 as the primary performance indicator).\n\n\nIn all, the training and test sets are reasonably similar. That said, if they weren't, would we necesarily have an issue that makes the data sets unusable? The present author's opinion is that differences in the characteristics of training and test data do not necessarily preclude them for usefulness in model-building. \n\nWhy? \n\nIt is arguably more important for a model's training data to match the _collection_ context and for the test data to match the model's _usage_ context. In most model-building, the training and test data are randomly split at the 80\/20 level; therefore, there is generally less concern for collection contexts of training and test data to be different from one another. However, in some cases, broad economic studies use data derived from the general population, rather than from a model's proposed usage context. While this is less of an issue when describing the state of affairs concerning the general population, it can prevent targeted models from attaining the best results in their intended scopes.\n\n\n___ \n## Feature Engineering\n\nOf all the variables included in the present data, there were few that related composite details concerning an individual's household. For instance, the variable `qmobilephone` indicates the number of mobile phones in a given individual's household. However, feature engineering is required to run a model that considers the proportion of mobile phones per household to a household size (`phone_per_person_household`). The following illustrates how the author engineered the `qmobilephone`\/`hhsize` feature for modeling:\n\n```python\ndf_train['phone_per_person_household'] = df_train['qmobilephone']\/df_train['hhsize']\n```\n\nAs it turns out, the `phone_per_person_household` variable ended up being one of the most important variables in the models in which it was employed. By engineering custom features, we can focus modeling efforts to deal with unique variable combinations. In addition to `phone_per_person_household`, the author engineered 40 other variables. \n\nMost of the engineered variables were built to clarify relationships throughout households. By clarifying the experience an individual has within their immediate familial context, we can better understand our data.\n\nThe author also built variables that consolidated some of the underrepresented groups under the assumption that the dimensionality of certain groups do not effectively add insight to the data. For example, the author combined the `techozinc`, `techoentrepiso`, `techocane`, and `techootro` variables into `roof_waste_material` because the materials noted in those \"techo...\" variables are waste materials. It may be the case that the impacts each of the roofing variables has on `Target` is similiar in characteristic, meaning that the most salient fact about all the people who fall into those groups is that they have roofs made of _waste_ materials. \n\n___\n\n## Model Results\n| Model Type        | Validation Categorical Accuracy | Validation Macro F1 Score  | Test Performance (CA)  |\n| ------------- |:-------------:|:-------------:| :-----:|\n| Sample Model (built in this thread)      | 91.31% | 87.17 | 66.05% |\n| Current Best Model (Ensemble RF and XGB) | 97.12%      |   94.21 | 95.63% |\n| Random Forest (soft) | 87% | 91.23 | 86.5% |\n| XBG (soft) | 92% | 93.2 | 91.1% |","8a5ae6ed":"# Costa Rican Household Poverty Level Prediction\nA starter kernel for the [Inter-American Development Bank's 2018 Costa Rican Household Poverty Prediction Challenge](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction)\n\n#### Author: Evan Gibson\n\n___ \n## Table of Contents\n- Summary\n- Exploratory Analysis\n- Feature Engineering\n- Model Results\n\n## Summary\n\nSocial programs have a hard time making sure the right people are given enough aid. It makes sense for these programs to establish thresholds for aid \"worthiness.\" Income\/expense measurements provide the standard baseline for credit worthiness in most parts of the world. But how can aid programs develop practical criteria when members of their target populations frequently lack \"acceptable\" income\/expense verification? The least affluent members of any society often lack robust income\/expense records; therefore, those individuals have a hard time proving that they qualify for certain aid programs. \n\nMicrofinance organizations around the world have spent years developing \"just right\" methods to determine how their funds should be allocated. To make things difficult for those organizations, the poorest segment of any population experiences especially acute outlier circumstances. From income level to the number of mobile phones, the variability of descriptive characteristics among those who occupy the poorest level of society is staggering. In Latin America, one popular method utilizes a unique algorithm to verify income qualification using measurable characteristics outside of income\/expense records. It\u2019s called the Proxy Means Test (or PMT). With PMT, agencies use a model that considers a family\u2019s observable household attributes like the material of their walls and ceiling, or the assets found in the home to classify them and predict their level of need.\n\nEven with the nuanced approach of the PMT, accuracy (among other metrics) remains a problem. As the Latin American population grows and poverty declines, reorientation of the PMT is necessary. The [Inter-American Development Bank](https:\/\/www.iadb.org) (IDB) has launched a public Kaggle competition to improve the PMT. The IDB is branching away from traditional econometrics in an effort to attain better, more reliable predictions. \n\nBeyond Costa Rica, many countries face this same problem of inaccurately assessing social need. \n\nThe present kernel will address the following:\n  - Detail the author's exploration of the given data.\n  - Attempts to tune an alternative PMT model.\n  - Investigate the PMT as a concept for the betterment of development practices.\n\nThe `Target` variable is classified (ordinally) as follows:\n- 1 = extreme poverty \n- 2 = moderate poverty \n- 3 = vulnerable households \n- 4 = non-vulnerable households\n\n___ \n## Exploratory Analysis\n\nWithout understanding the data we will use, we cannot hope to derive meaningful, scalable predictions. \n\nThe following code cells detail some of the present author's exploration steps:\n","bb9dae0a":"It may appear like the high income outliers in this data will skew our model's results. However, if one inspects the data more carefully, it is clear that the high income outliers lie in the \"4\" bracket of `Target`. These outliers, if handled non-linearly, present the opportunity for our model to display income thresholds that greatly, and perhaps appropriately, impact `Target`. The following graphic displays income as it occurs throughout `Target` groups (in the training data):  \n\n","b32f3bbe":"The training data contains 143 \"unique\" variables. For comparison, the test data contains 142 variables. The only difference between the variables in the two files are in the presence of the `Target` column. The following details some of the most \"important\" characteristics of both sets:\n\n| Data Set        | Train           | Test  |\n| ------------- |:-------------:| :-----:|\n| Participants      | 8,801 | 756 |\n| Households      | 2,749      |   239 |\n| Mean Income `v2a1` |   165,027    |    167,296 |\n| Refrigerator Ratio `refrig` | 0.96 | 0.96 |\n| Mean Education `meaneduc`| 9.22      | 9.35 |\n| Gender Proportion  `male`\/total |   0.48    |    0.48 |\n| Mean Household Size `hhsize` |   4.00    |    3.96 |\n| Mean Age `age` |   34.3    |    34.3 |\n\nIncome lies at the heart of the PMT. Many organizations use income to determine an individual's worthiness for social programs, interest rates, and credit limits. If our model disregarded income, it would undoubtedly mischaracterize its target. The following is the income distribution for the training set:","4da8e87b":"Where can we go from here?\n\n_Good luck!_","deb41f12":"\n\nOn a related note, during exploration of the income, it becomes obvious that no one who owns a fully paid house (`tipovivi1`) reported income. While this fact does not preclude those who own a house from being considered by our model, it does illuminate some of the data collection context. Why is it that those who own their houses do not report income? It could be the case that the income for the other individuals was self reported as part of an application process. It could be the case that the data for those who own their houses were collected under significantly different circumstances than those who do not own their houses. Ultimately, the answer is unclear. That said, it will be important to understand that there is a possibility that the present data were consolidated from different sources under different methodologies.  \n\n\nVisual exploratory analysis is an important aspect of most statistical methodlogy. By visually representing data, we can identify non-linear patterns that could impact our findings. For example, a simple countplot of the `escolari` variable shows us that there are two \"spikes\" in the amount of schooling our population receives (at 6 and at 11 years):\n\n","c42363c6":"## Sample model\nThe is a sample submission to get you started. It will defeat the benchmark standard.","08a8b7c6":"While not enough to paint the whole picture, this graph displays prevalent aspects of the data. If education ends up being at the nexus of the model's most important features, a researcher's understanding of the societal impactors that have driven these spikes might be invaluable to deriving meaningful insight.\n\nIn addition to helping researchers understand the characteristics of the data, visual analysis can help researchers determine metrics for model success. The following graph displays the count of individuals belonging to each of the `Target` groups:\n\n"}}