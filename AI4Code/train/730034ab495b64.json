{"cell_type":{"21df4d3c":"code","b51b7115":"code","ef31bc79":"code","b03d3be3":"code","dc058b85":"code","6f3dbd3c":"code","127ecaf2":"code","539fd0b3":"code","86534a47":"code","ecfe98e6":"code","38373841":"code","5da4d124":"code","56a302ba":"code","b4a264b4":"code","06ba9138":"code","62a145d2":"code","02ddfe49":"code","68597249":"code","e1251d2a":"code","7ae1b304":"markdown","dfa1df47":"markdown","bf72c972":"markdown","b26c1074":"markdown","81608c49":"markdown","09f701ea":"markdown","a295ad40":"markdown","34e16ebf":"markdown","ef1766c2":"markdown","c9509608":"markdown","62928e4d":"markdown","096f6d84":"markdown"},"source":{"21df4d3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b51b7115":"#importing libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score,classification_report,confusion_matrix\nfrom sklearn.model_selection import train_test_split","ef31bc79":"df=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head(2)","b03d3be3":"df.shape","dc058b85":"#checking null values\ndf.isnull().sum()","6f3dbd3c":"#dividing dataframes into two dataframes on the bases of fraud cases and non fraud cases.\nfraud=df[df['Class']==1]\nno_fraud=df[df['Class']==0]\n\n#plotting Amount attribute as scatterplot\nsns.scatterplot(data=no_fraud['Amount'],color='blue')\nsns.scatterplot(data=fraud['Amount'],color='red')\nplt.legend(['no_fraud','fraud'])","127ecaf2":"#Plotting time attrbute to check if there's some relation.\nsns.scatterplot(data=no_fraud.Time,color='blue')\nsns.scatterplot(data=fraud.Time,color='red')","539fd0b3":"#Plotting heatmap to show correlation between attributes\nsns.heatmap(df.corr())","86534a47":"#Plotting heatmap to show correlation between attributes for fraud cases only\nsns.heatmap(fraud.corr())","ecfe98e6":"#Countplot to check counts of number of fraud cases and non-fraud cases in our dataset\nsns.countplot(df['Class'])","38373841":"#Percentage proportion of fraud class and non-fraud class\nprint(\"Fraud Cases in our data set : \",len(fraud)*100\/len(df),\" %\")\nprint(\"Non Fraud Cases in our data set : \",len(no_fraud)*100\/len(df),\" %\")","5da4d124":"#Dividing into feature set and target set.\nx=df.drop('Class',axis=1)\ny=df['Class']\n\n#importing SMOTE\nfrom imblearn.over_sampling import SMOTE\nsmote=SMOTE(random_state=60)\nx_smote,y_smote=smote.fit_resample(x,y)","56a302ba":"sns.countplot(y_smote)","b4a264b4":"#splitting data we get after applying SMOTE\nx_train,x_test,y_train,y_test=train_test_split(x_smote,y_smote,test_size=0.3,random_state=1)","06ba9138":"from sklearn.pipeline import make_pipeline\nrf=make_pipeline(StandardScaler(),RandomForestClassifier())\nrf.fit(x_train,y_train)","62a145d2":"yhat=rf.predict(x_test)","02ddfe49":"print(classification_report(yhat,y_test))","68597249":"#plotting roc_auc_curve\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, rf.predict_proba(x_test)[:,1])\nplt.plot(fpr,tpr,marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nprint(\"roc_auc_score : \",roc_auc_score(yhat,y_test))","e1251d2a":"#plotting precision_recall_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import precision_score,recall_score\nprecision,recall,_ = precision_recall_curve(y_test, rf.predict_proba(x_test)[:,1])\nplt.plot(recall,precision,marker='.')\nplt.xlabel('recall')\nplt.ylabel('precision')\nprint(\"Precision score : \",precision_score(yhat,y_test,average='binary'))\nprint(\"Recall score : \",recall_score(yhat,y_test,average='binary'))","7ae1b304":"Dataset is highly unbalanced. It contains 99.8% non-fraud cases and only 0.2% of fraud cases. Using this dataset only to train our model would be highly bias towards non-fraud cases.<br>\nTo deal with this we can use over sampling methods or under sampling methods.<br>\nHere, I am using SMOTE which stands for Synthetic Minority Oversampling Technique.\n(one can also use ADASYN or Random Over Sampling depending on the dataset and choice)","dfa1df47":"linear combinations v1,v2,...,v28 are not highly correlated to one another.\nthere's still good correlation between class and several LC namely v7,v20,etc.","bf72c972":"Now we can see after applying SMOTE, data is no more bias towards any case. Both are 50-50.","b26c1074":"There's no specific period which shows most fraud cases.","81608c49":"The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the \u2018signal\u2019 from the \u2018noise\u2019.<br>\nThe Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.<br>\nThe higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.<br>\nWe can see that AUC is approx to its max, which shows that the performance of this model is great. Model classifies with minimum False positive rate and max true positive rate\n\n","09f701ea":"We have build a Classifier using Random Forest Classifier which could detect credit card fraud case with Precision and recall 1 and with roc_auc_score 99% .<br>\nLet me know if I could improve it or if it contains any mistakes.","a295ad40":"Data does not contain any missing value.<br>\nlet's proceed to some visualization","34e16ebf":"It is surprising that the attributes have good correlations in fraud cases.<br>\nWe are provided with only linear combinations to hide the real attributes because of privacy policy, ow we could have get some great relation between them. We could still have that but it's not gonna add something as we don't know what they are.","ef1766c2":"Thank You!","c9509608":"It is clearly seen that all the fraud transactions are of below 5000 unit.<br>","62928e4d":"Data contains 31 attributes which are linear combination of several variables, which aren't provided because of privacy terms. And it contains 284807 records.\n","096f6d84":"Precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n\nIn this model, the area under the precision-recall curve tends to its highest, which shows that our classifying with accurately results with majority of positive results."}}