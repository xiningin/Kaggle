{"cell_type":{"e6f22c22":"code","e17efbaa":"code","d6d60ba0":"code","ce256a81":"code","567e72bc":"code","2a34c717":"code","a3ffef91":"code","a43f5b74":"code","8e44e617":"code","c800bd97":"code","2850ea4f":"code","bab0eee4":"code","27f0e3fe":"code","395ab233":"code","f5381bcd":"code","0ebc503f":"code","94fae846":"code","df6627f1":"code","ddf2d0b9":"markdown","b24967f2":"markdown","b77e2e2d":"markdown","4dd67399":"markdown","3aaef0a3":"markdown","c431e8c1":"markdown","31a20e14":"markdown"},"source":{"e6f22c22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e17efbaa":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore')\nimport seaborn as sns\nimport statsmodels.api as sm\nimport lightgbm as lgb\n\n### Feature selection modules from sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso","d6d60ba0":"def unique_num(data, key):\n    return len(set(data[key]))\n\n## day of the week and time of day\ndef add_feature( data, time_key):\n    data['day'] = [int(x.split(' ')[0].split('-')[2])%7 for x in  data[time_key]]\n    data['hour'] = [float(x.split(' ')[1].split(':')[0]) for x in data[time_key]]\n    return data","ce256a81":"\ntrain_data = pd.read_csv('..\/input\/wns2019contest\/train.csv')\nitem_data = pd.read_csv('..\/input\/wns2019contest\/item_data.csv')\nview_log = pd.read_csv('..\/input\/wns2019contest\/view_log.csv')\n\ntest_data = pd.read_csv('..\/input\/wns2019contest\/test.csv')\n#sample_sub = pd.read_csv('.\/input\/wns2019contest\/sample_submission.csv')\n","567e72bc":"display(train_data.head(2))\nn_click_train = len(train_data[train_data['is_click']==1])\nn_noclick_train = len(train_data[train_data['is_click']==0])\nprint ('Number of unique users :- %s'%(len(np.unique(train_data['user_id']))))\nprint ('Number of clicks :- %s'%(n_click_train))\nprint ('Number of no click :- %s'%(n_noclick_train))\nprint ('----> Percentage of clicks to noclick in train data :- %f'%(n_click_train*100\/n_noclick_train))\nprint ('---------------------------------')\nprint ('---------------------------------')\n\nchars = [ch for ch in train_data['impression_id'][0] ]\nprint ('Length of impression id characters %s is same as length of data'%(len (chars)))\nprint (unique_num(train_data, 'impression_id'))\nprint ('----> each impression id encodes entire information of user_id, impression time, os version, is_4G and clicking data')\n\nprint ('---------------------------------')\nprint ('---------------------------------')\nprint ('impression times range from 15 nov to 13 dec which means ')\ntime_key = ['impression_time', 'server_time']\ntrain_data = add_feature(train_data, time_key[0])\ntest_data = add_feature(test_data, time_key[0])\nview_log = add_feature(view_log, time_key[1])\n\n\n#print (unique_num(train_data, 'impression_time_int'))\ntrain_data['os_version_num'] = np.zeros(len(train_data))\ntrain_data['os_version_num'][train_data['os_version'] == 'intermediate'] = int(2)\ntrain_data['os_version_num'][train_data['os_version'] == 'old'] = int(1)\ntrain_data['os_version_num'][train_data['os_version'] == 'latest'] = int(3)\n\ndisplay(train_data.head(2))\n","2a34c717":"## identifying the features for modeling\ndf = pd.DataFrame(train_data)\nx_y = df.drop('impression_id',  1)\nx_y = x_y.drop ('impression_time', 1)\n\nprint (x_y.columns)\n#x_y['']\nplt.show()\ndisplay(x_y.head(2))\nplt.figure(figsize=(12,10))\ncor = x_y.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.viridis)\nplt.show()\n","a3ffef91":"#Correlation with output variable\ncor_target = abs(cor[\"is_click\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.001]\nprint (relevant_features)\nprint ('=============')\nprint (x_y[['os_version_num', 'is_4G']].corr())\n\nprint ('os version and is_4G are correlated makes sense because normally latest version are more likely to have 4G connection')\n\n\nprint ('==========================')\nprint (x_y[['os_version_num', 'app_code']].corr() )\nprint (x_y[['app_code', 'hour']].corr() )\nprint (x_y[['os_version_num', 'hour']].corr() )\n\n\nprint ('None of these features are correlated with each other,\\\n\\nTherefore I believe the relevant features for traning should be :- ', 'app_code', 'hour', 'os_version_int' )\nprint ('we can get rid of 2 features user_id, and is_4G.... may be more testin required before gettin rid of these')","a43f5b74":"### AN attempt to understand user profiles using view_log\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndisplay(view_log.head(2))\n\n\n\nprint ('=======================')\nunique_devices = np.unique(view_log['device_type'])\nfor i in range(3):\n    \n    print (len(view_log[view_log['device_type'] == unique_devices[i]]), '%s users'%(unique_devices[i]))\nprint ('device type does not matter')\nprint ('==========')\n\n","8e44e617":"\ndf_view = pd.DataFrame(view_log)\ndf_view = df_view.drop('server_time',1)\nplt.figure(figsize=(12,10))\ncor = df_view.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.viridis)\nplt.show()\n","c800bd97":"display(item_data.head(2))\n\ndf_item = pd.DataFrame(item_data)\n\nplt.figure(figsize=(12,10))\ncor = df_item.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.viridis)\nplt.show()\n","2850ea4f":"train_data['hour'] = train_data.impression_time.apply(lambda x: x.split(\" \")[1].split(':')[0])\ntrain_data.groupby('hour').agg({'is_click':'sum'}).plot(figsize=(12,6))\nplt.ylabel('# clicks')\nplt.title('Trends of clicks by hour of day');","bab0eee4":"display(train_data.head())\ntrain = train_data\nX_train = train.loc[:, ['user_id', 'is_4G', 'day',  'app_code', 'os_version_num']]# train.columns != ['is_click', 'impression_time']]\ny_target = train.is_click.values\n#create lightgbm dataset\nmsk = np.random.rand(len(X_train)) < 0.8\nlgb_train = lgb.Dataset(X_train[msk], y_target[msk])\nlgb_eval = lgb.Dataset(X_train[~msk], y_target[~msk], reference=lgb_train)\n\n# specify your configurations as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': { 'auc'},\n    'num_leaves': 31, # defauly leaves(31) amount for each tree\n    'learning_rate': 0.08,\n    'feature_fraction': 0.7, # will select 70% features before training each tree\n    'bagging_fraction': 0.3, #feature_fraction, but this will random select part of data\n    'bagging_freq': 5, #  perform bagging at every 5 iteration\n    'verbose': 1\n}\n\nprint('Start training...')\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=4000,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=1500)","27f0e3fe":"import matplotlib.pylab as pylab\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (12, 5),\n         'axes.labelsize': 'x-large',\n         'axes.titlesize':'x-large',\n         'xtick.labelsize':'x-large',\n         'ytick.labelsize':'x-large', 'axes.linewidth' :3}\npylab.rcParams.update(params)","395ab233":"#print (unique_num(train_data, 'impression_time_int'))\ntest_data['os_version_num'] = np.zeros(len(test_data))\ntest_data['os_version_num'][test_data['os_version'] == 'intermediate'] = int(2)\ntest_data['os_version_num'][test_data['os_version'] == 'old'] = int(1)\ntest_data['os_version_num'][test_data['os_version'] == 'latest'] = int(3)\n\npredictions_lightgbm = gbm.predict(test_data.drop(['impression_time', 'os_version', 'impression_id'], axis =1))\nplt.hist(predictions_lightgbm, label = 'lightgbm')\n\ndf_kag = pd.read_csv('.\/DeepFM_submission_copy.csv')\nplt.hist(df_kag['is_click'], alpha = 0.7,color = 'g', label ='kaggle')\nplt.legend()\nplt.show()","f5381bcd":"from operator import itemgetter\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\ndef run_default_test(train, test, features, target, random_state=0):\n    eta = 0.1\n    max_depth = 6\n    subsample = 0.8\n    colsample_bytree = 0.8\n    print('XGBoost params. ETA: {}, MAX_DEPTH: {}, SUBSAMPLE: {}, COLSAMPLE_BY_TREE: {}'.format(eta, max_depth, subsample, colsample_bytree))\n    params = {\n        \"objective\": \"binary:logistic\",\n        \"booster\" : \"gbtree\",\n        \"eval_metric\": \"auc\",\n        \"eta\": eta,\n        \"max_depth\": max_depth,\n        \"subsample\": subsample,\n        \"colsample_bytree\": colsample_bytree,\n        \"silent\": 0,\n        \"seed\": random_state\n    }\n    num_boost_round = 260\n    early_stopping_rounds = 20\n    test_size = 0.2\n\n    X_train, X_valid = train_test_split(train, test_size=test_size, random_state=random_state)\n    y_train = X_train[target]\n    y_valid = X_valid[target]\n    dtrain = xgb.DMatrix(X_train[features], y_train)\n    dvalid = xgb.DMatrix(X_valid[features], y_valid)\n    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n    gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n    \n    features = [ 'user_id', 'app_code', 'is_4G', 'day', 'os_version_num']\n    return gbm\nfeatures = [ 'user_id', 'app_code', 'is_4G', 'day', 'os_version_num']\ny_target = train_data['is_click']\nxgb_out = run_default_test(train, y_target, features, 'is_click')","0ebc503f":"predictions_xgboost = xgb_out.predict(xgb.DMatrix(test_data.drop(['impression_time', 'os_version', 'impression_id', 'hour'], axis =1)))#, ntree_limit=clf.best_ntree_limit)\/folds.n_splits\n","94fae846":"plt.hist(predictions_lightgbm, label = 'lightgbm')\nplt.hist(predictions_xgboost, label = 'xgboost', color ='r', alpha = 0.5)\n\n\ndf_kag = pd.read_csv('..\/input\/deepfm-test\/DeepFM_submission_copy.csv')\nplt.hist(df_kag['is_click'], alpha = 0.7,color = 'g', label ='kaggle')\nplt.legend()\nplt.show()","df6627f1":"### predictions_lightgbm and predictions_xgboost are the output from the normal feature enginnering.","ddf2d0b9":"## lightgbm","b24967f2":"### Imports","b77e2e2d":"# This notebook can be a starting point in WNS data challange\n\n*** This kernel reaches top 70 ranks in the WNS competition with total 6456 participants.\n\n* This kernel finds the features correlation between various features\n* It identifies relevant features for fitting\n* Performs **lightgbm** and **xgboost** for fitting\n    * Early stopping, best iteration is:\n    \n        [100]\tvalid_0's auc: 0.685143\n    * the AUC scores on validation and trianing sets for xgboost are:-\n\n        [218]\ttrain-auc:0.780506\teval-auc:0.705122\n\n### references:-\nhttps:\/\/towardsdatascience.com\/mobile-ads-click-through-rate-ctr-prediction-44fdac40c6ff\n\nhttps:\/\/towardsdatascience.com\/feature-selection-with-pandas-e3690ad8504b\n\nhttps:\/\/datahack.analyticsvidhya.com\/contest\/wns-analytics-wizard-2019\/","4dd67399":"# xgboost ","3aaef0a3":"### Reading all data ","c431e8c1":"So, only 4.8% of all impressions result in a click","31a20e14":"extracting basic information from data"}}