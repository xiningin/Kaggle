{"cell_type":{"78c3bc9c":"code","2fee0bd3":"code","e9fabf87":"code","c9237372":"code","834d73c6":"code","379e3546":"code","773d9d16":"code","25d531be":"code","3b48888c":"code","0aec205f":"code","245a48f4":"code","2da0ff45":"markdown","3367a39e":"markdown","d7443e8f":"markdown","98bd6e8e":"markdown","668203e9":"markdown","036eb4b2":"markdown","add7dfd2":"markdown"},"source":{"78c3bc9c":"import numpy as np\n'''\n--------------------\nx:\u5165\u529b\u30d9\u30af\u30c8\u30eb(\u30b7\u30b0\u30ca\u30eb)\nw:\u30d5\u30a3\u30eb\u30bf(\u30ab\u30fc\u30cd\u30eb)\np:\u30d1\u30c7\u30a3\u30f3\u30b0\u30b5\u30a4\u30ba\ns:\u30b9\u30c8\u30e9\u30a4\u30c9\n--------------------\n'''\n\ndef conv1d(x, w, p = 0, s = 1):\n    # \u30d5\u30a3\u30eb\u30bf\u3092\u56de\u8ee2\n    w_rot = np.array(w[::-1])\n    # \u5165\u529b\u30d9\u30af\u30c8\u30eb\n    x_padded = np.array(x)\n    # \u30d1\u30c6\u30a3\u30f3\u30b0\u30b5\u30a4\u30ba\u304c0\u4ee5\u4e0a\u306e\u5834\u5408\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0(0\u3067\u57cb\u3081\u308b)\n    if p > 0:\n        # \u30d1\u30c7\u30a3\u30f3\u30b0\u7528\u306endarray\u3092\u7528\u610f\n        zero_pad = np.zeros(shape = p)\n        # ndarray\u3092\u9023\u7d50\n        x_padded = np.concatenate([zero_pad, x_padded, zero_pad])\n    res = []\n    for i in range(0, int((len(x_padded) - len(w_rot)) \/ s) + 1, s):\n        # \u7573\u307f\u8fbc\u307f\u3092\u8a08\u7b97\n        res.append(np.sum(x_padded[i:i + w_rot.shape[0]] * w_rot))\n    return np.array(res)","2fee0bd3":"x = [1, 3, 2, 4, 5, 6, 1, 3]\nw = [1, 0, 3, 1, 2]\n\nprint('Conv1d Imprementation', conv1d(x, w, p = 2, s = 1))","e9fabf87":"print('Numpy Results:', np.convolve(x, w, mode = 'same'))","c9237372":"'''\n--------------------\nX:\u5165\u529b\u30d9\u30af\u30c8\u30eb\nW:\u30d5\u30a3\u30eb\u30bf(\u30ab\u30fc\u30cd\u30eb)\np:\u30d1\u30c7\u30a3\u30f3\u30b0\u30b5\u30a4\u30ba\ns:\u30b9\u30c8\u30e9\u30a4\u30c9\n--------------------\n'''\n\ndef conv2d(X, W, p = (0,0), s = (1, 1)):\n    # \u30d5\u30a3\u30eb\u30bf\u3092\u56de\u8ee2\n    W_rot = np.array(W)[::-1, ::-1]\n    # \u5165\u529b\u30d9\u30af\u30c8\u30eb\n    X_orig = np.array(X)\n    # \u30d1\u30c7\u30a3\u30f3\u30b0\u7528\u306eshape\n    n1 = X_orig.shape[0] + 2 * p[0]\n    n2 = X_orig.shape[1] + 2 * p[1]\n    # \u30d1\u30c7\u30a3\u30f3\u30b0\u7528\u306e\u8981\u7d200\u306endarray\n    X_padded = np.zeros(shape = (n1, n2))\n    # \u4e2d\u306b\u5165\u529b\u3092\u5165\u308c\u8fbc\u3080(\u30d1\u30c7\u30a3\u30f3\u30b0\u90e8\u5206\u306f0)\n    X_padded[p[0]:p[0] + X_orig.shape[0], p[1]:p[1] + X_orig.shape[1]] = X_orig\n    \n    res = []\n    # \u7573\u307f\u8fbc\u307f\n    for i in range(0, int((X_padded.shape[0] - W_rot.shape[0]) \/ s[0]) + 1, s[0]):\n        res.append([])\n        for j in range(0, int((X_padded.shape[1] - W_rot.shape[1]) \/ s[1]) + 1, s[1]):\n            X_sub = X_padded[i:i + W_rot.shape[0], j:j+W_rot.shape[1]]\n            res[-1].append(np.sum(X_sub * W_rot))\n    return (np.array(res))","834d73c6":"X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\nW = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]\nprint('Conv2d Implementation:\\n', conv2d(X, W, p = (1, 1), s = (1, 1)))","379e3546":"import scipy.signal\nprint('Scipy Results:\\n', scipy.signal.convolve2d(X, W, mode='same'))","773d9d16":"import tensorflow_datasets as tfds\nimport tensorflow as tf\nmnist_bldr = tfds.builder('mnist')\n\nmnist_bldr.download_and_prepare()\n# \u8a13\u7df4\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4e2d\u304b\u3089\u691c\u8a3c\u7528\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u5206\u5272\u3059\u308b\u305f\u3081\u306b\u30b7\u30e3\u30c3\u30d5\u30eb\u3092\u3057\u306a\u3044\ndatasets = mnist_bldr.as_dataset(shuffle_files = False)\n\n# \u8a13\u7df4\u30c7\u30fc\u30bf\nmnist_train_orig = datasets['train']\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\nmnist_test_orig = datasets['test']\n\nBUFFER_SIZE = 10000\nBATCH_SIZE = 64\nNUM_EPOCHS = 20\nmnist_train = mnist_train_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32) \/ 255.0,\n                  tf.cast(item['label'], tf.int32)))\nmnist_test = mnist_test_orig.map(\n    lambda item: (tf.cast(item['image'], tf.float32) \/ 255.0,\n                  tf.cast(item['label'], tf.int32)))\ntf.random.set_seed(1)\nmnist_train = mnist_train.shuffle(buffer_size = BUFFER_SIZE,\n                                  reshuffle_each_iteration = False)\nmnist_valid = mnist_train.take(10000).batch(BATCH_SIZE)\nmnist_train = mnist_train.skip(10000).batch(BATCH_SIZE)","25d531be":"model = tf.keras.Sequential()\n# \u7573\u307f\u8fbc\u307f\u5c641\nmodel.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = (5, 5), strides = (1, 1),\n                                 padding = 'same', data_format = 'channels_last',\n                                 name = 'conv_1', activation = 'relu'))\n# \u30d7\u30fc\u30ea\u30f3\u30b0\u5c641\nmodel.add(tf.keras.layers.MaxPool2D(pool_size = (2, 2), name = 'pool_1'))\n# \u7573\u307f\u8fbc\u307f\u5c642\nmodel.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = (5, 5), strides = (1, 1),\n                                 padding = 'same', data_format = 'channels_last',\n                                 name = 'conv_2', activation = 'relu'))\n# \u30d7\u30fc\u30ea\u30f3\u30b0\u5c641\nmodel.add(tf.keras.layers.MaxPool2D(pool_size = (2, 2), name = 'pool_2'))\nmodel.compute_output_shape(input_shape = (16, 28, 28, 1))\n# \u5168\u7d50\u5408\u5c64\u306b\u5bfe\u3059\u308b\u5165\u529b\u306f\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u00d7\u5165\u529b\u30e6\u30cb\u30c3\u30c8\u3067\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u306e\u3067Flatten\u5c64\u3092\u5165\u308c\u308b\nmodel.add(tf.keras.layers.Flatten())\nmodel.compute_output_shape(input_shape = (16, 28, 28, 1))\n# \u5168\u7d50\u5408\u5c641\nmodel.add(tf.keras.layers.Dense(units = 1024, name = 'fc_1', activation = 'relu'))\n# \u30c9\u30ed\u30c3\u30d7\u30a2\u30a6\u30c8\u5c64\nmodel.add(tf.keras.layers.Dropout(rate=0.5))\n# \u5168\u7d50\u5408\u5c642(0~9\u306e10\u500b\u306e\u30af\u30e9\u30b9\u30e9\u30d9\u30eb\u306b\u76f8\u5f53)\nmodel.add(tf.keras.layers.Dense(units = 10, name = 'fc_2', activation = 'softmax'))\n\ntf.random.set_seed(1)\nmodel.build(input_shape = (16, 28, 28, 1))\nmodel.compile(optimizer = tf.keras.optimizers.Adam(),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = ['accuracy'])\n\nhistory = model.fit(mnist_train,\n                    epochs = NUM_EPOCHS,\n                    validation_data = mnist_valid,\n                    shuffle = True)\n","3b48888c":"import matplotlib.pyplot as plt\n\nhist = history.history\nx_arr = np.arange(len(hist['loss'])) + 1\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(1, 2, 1)\n\nax.plot(x_arr, hist['loss'], '-o', label = 'Train loss')\nax.plot(x_arr, hist['val_loss'], '--<', label = 'Validation loss')\n\nax.set_xlabel('Epoch', size = 15)\nax.set_ylabel('Loss', size = 15)\n\nax.legend(fontsize=15)\n\nax = fig.add_subplot(1, 2, 2)\nax.plot(x_arr, hist['accuracy'], '-o', label = 'Train acc')\nax.plot(x_arr, hist['val_accuracy'], '--<', label = 'Validation acc')\nax.set_xlabel('Epoch', size = 15)\nax.set_ylabel('Accuracy', size = 15)\nax.legend(fontsize=15)\nplt.savefig(\".\/result.png\")\nplt.show()","0aec205f":"test_results = model.evaluate(mnist_test.batch(20))\nprint('Test Acc.: {:.2f}%'.format(test_results[1] * 100))","245a48f4":"batch_test = next(iter(mnist_test.batch(12)))\npreds = model(batch_test[0])\ntf.print(preds.shape)\n\npreds = tf.argmax(preds, axis = 1)\nprint(preds)\n\nfig = plt.figure(figsize = (12, 4))\nfor i in range(12):\n    ax = fig.add_subplot(2, 6, i+1)\n    ax.set_xticks([]);ax.set_yticks([])\n    img = batch_test[0][i, :, :, 0]\n    ax.imshow(img, cmap = 'gray_r')\n    ax.text(0.9, 0.1, '{}'.format(preds[i]), size = 15, color = 'blue',\n            horizontalalignment = 'center', verticalalignment = 'center',\n            transform=ax.transAxes)\n\nplt.savefig(\".\/classification.png\")\nplt.show()","2da0ff45":"## STEP3 \u30c7\u30a3\u30fc\u30d7\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\n\n***\n\n\u624b\u66f8\u304d\u6570\u5b57\u306e\u5206\u985e\u5668\u3092\u69cb\u6210\u3059\u308b\uff0e\n\u5404\u5c64\u306e\u30c6\u30f3\u30bd\u30eb\u306e\u6b21\u5143\u306f\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u306a\u308b\n\n- \u5165\u529b\u5c64 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba \\* 28 \\* 28 \\* 1\n- \u7573\u307f\u8fbc\u307f\u5c641 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba \\* 28 \\* 28 \\* 32\n- \u30d7\u30fc\u30ea\u30f3\u30b0\u5c641 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba \\* 14 \\* 14 \\* 32\n- \u7573\u307f\u8fbc\u307f\u5c642 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba \\* 14 \\* 14 \\* 64\n- \u30d7\u30fc\u30ea\u30f3\u30b0\u5c642 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba \\* 7 \\* 7 \\* 64\n- \u5168\u7d50\u5408\u5c641 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba \\* 1024\n- \u5168\u7d50\u5408\u5c642 \u30d0\u30c3\u30c1\u30b5\u30a4\u30ba \\* 10","3367a39e":"numpy\u306econvolve\u95a2\u6570\u3068\u6bd4\u8f03","d7443e8f":"# \u753b\u50cf\u306e\u5206\u985e-CNN\n\n***\n\n\u4eca\u56de\u306e\u6388\u696d\u3067\u306f\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306e\u4e2d\u3067\u3082\u753b\u50cf\u5206\u985e\u306b\u3064\u3044\u3066\u5b66\u3076\uff0e\n\n\u753b\u50cf\u5206\u985e\u306b\u306f\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(Convolutional Neural Network:CNN)\u304c\u3088\u304f\u7528\u3044\u3089\u308c\u308b\uff0e\n\u307e\u305aCNN\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u3092\u8aac\u660e\u3059\u308b\uff0e\u6b21\u306bCNN\u306e\u69cb\u9020\u306e\u8a73\u7d30\u306b\u8e0f\u307f\u8fbc\u307f\uff0cCNN\u3092Tensorflow\u3067\u5b9f\u88c5\u3059\u308b\u65b9\u6cd5\u3092\u78ba\u8a8d\u3059\u308b\uff0e\n\n\u672c\u7ae0\u306f\u4ee5\u4e0b\u306e\u9805\u76ee\u3067\u69cb\u6210\u3055\u308c\u308b\uff0e\n\n- 1\u6b21\u5143\u30682\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\u6f14\u7b97\n- CNN\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u69cb\u6210\u8981\u7d20\n- TensorFlow\u3067\u306e\u30c7\u30a3\u30fc\u30d7\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af(DCNN)\u306e\u5b9f\u88c5\n","98bd6e8e":"### STEP2 2\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\n\n***","668203e9":"## 1\u6b21\u5143\u30682\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\u6f14\u7b97\n\n***\n\n\u6559\u79d1\u66f8(p448~p459)\n\n\u7573\u307f\u8fbc\u307f\u306e\u6982\u5ff5\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u7528\u610f\u3055\u308c\u305f\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u95a2\u6570\u7b49\u3092\u7528\u3044\u305a\u306b1\u6b21\u5143\u304a\u3088\u30732\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\u6f14\u7b97\u3092\u5b9f\u88c5\u3057\uff0c\u305d\u306e\u7d50\u679c\u3092\u7528\u610f\u3055\u308c\u305f\u95a2\u6570\u3068\u6bd4\u8f03\u3057\u6b63\u3057\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3059\u308b\uff0e\n\n\n### STEP1 1\u6b21\u5143\u306e\u7573\u307f\u8fbc\u307f\n\n***","036eb4b2":"\u4e88\u6e2c\u5024\u3092\u30af\u30e9\u30b9\u6240\u5c5e\u78ba\u7387\u3068\u3057\u3066\u53d6\u5f97\uff0c\u6700\u5927\u78ba\u7387\u3092\u6301\u3064\u8981\u7d20\u3092\u7279\u5b9a\u3057\u30e9\u30d9\u30eb\u306b\u5909\u63db\uff0e\n\u5165\u529b\u5024\u3068\u5b9f\u969b\u306b\u4e88\u6e2c\u3057\u305f\u30e9\u30d9\u30eb\u3092\u53ef\u8996\u5316\u3059\u308b\uff0e","add7dfd2":"\u8a13\u7df4\u3057\u305f\u30e2\u30c7\u30eb\u3092\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u8a55\u4fa1"}}