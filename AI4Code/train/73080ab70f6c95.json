{"cell_type":{"5bdc0fb3":"code","3b6fdbcd":"code","a15e8d1a":"code","5489b0d1":"code","6c843d34":"code","0316a339":"code","a905c719":"code","ed12973d":"code","f48eb860":"code","1f682118":"code","8cdbc1ed":"code","e09169e2":"code","6beb316b":"code","cd6790f0":"code","aa4a502e":"code","03266319":"code","c71ac593":"code","fabb2b02":"code","bf35cbe8":"code","8ebb9765":"code","59e15c10":"code","7d4cdac8":"code","0a1711a7":"code","32e8e86f":"code","c5bae3f2":"code","ec09dd3b":"code","47b4a3e2":"code","6678a64c":"code","dc8e5b7e":"code","8571194c":"code","0c6b1b36":"code","a144b0fc":"code","e8610400":"code","eac8e96c":"code","48e5e3fd":"code","0b6b69ae":"code","c505e4bd":"code","0bbeab71":"code","cc520ef7":"code","c8b984c7":"code","6b26b9ec":"code","49029bc5":"code","69ed8d90":"code","8392cd28":"code","4140a320":"code","723e5535":"code","b702fdf9":"code","34c7bb39":"code","2e97c646":"code","4b1e18d6":"code","c898bf01":"code","c0318e31":"code","d372083e":"code","cce1fe5a":"code","89f1575d":"code","775824b9":"code","4fd19b5a":"code","1c955f13":"code","f4ccd265":"code","b3ae93aa":"code","624e7351":"markdown","b90732c6":"markdown","8d9a57b4":"markdown","2ce9cc49":"markdown","3601e26a":"markdown","b49cabb5":"markdown","889ae95a":"markdown","ec45d469":"markdown","d312a5f4":"markdown"},"source":{"5bdc0fb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b6fdbcd":"import time\nimport datatable as dt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport catboost\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cluster import KMeans\nimport optuna\n# import imblearn\n#from imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.metrics import geometric_mean_score\nfrom imblearn.metrics import make_index_balanced_accuracy\nimport seaborn as sns","a15e8d1a":"RANDOM_STATE = 42\nN_SPLITS = 20\nCLIP_FEATURES = False\nCLUSTER_DISTANCE = False","5489b0d1":"%%time\ntrain_file_name = \"\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv\"\ntest_file_name = \"\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv\"\nsubmission_file_name = \"\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\"\ntrain_dt = dt.fread(train_file_name)\ntest_dt = dt.fread(test_file_name)\nsubmission = pd.read_csv(submission_file_name)    ","6c843d34":"def initialize():\n    train_file_name = \"\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv\"\n    test_file_name = \"\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv\"\n    submission_file_name = \"\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\"\n    train_dt = dt.fread(train_file_name)\n    test_dt = dt.fread(test_file_name)\n    submission = pd.read_csv(submission_file_name)\n    \n    X = train_dt.to_pandas()\n    X_test = test_dt.to_pandas()\n    \n    X = reduce_mem_usage(X)\n    X_test = reduce_mem_usage(X_test)\n    return X, X_test","0316a339":"#Setting up options\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)\npd.options.display.float_format = \"{:,.3f}\".format","a905c719":"X = train_dt.to_pandas()\nX = train_dt.to_pandas()\nX_test = test_dt.to_pandas()\ntarget = 'Cover_Type'\n\nX_id = X.Id\nX_test_id = X_test.Id","ed12973d":"num_features = [col for col in X.columns if X[col].dtype in ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']]\ncat_features = [col for col in X.columns if X[col].dtype in ['bool']]","f48eb860":"# will do 83.9% reduction im memory usage\ndef reduce_mem_usage(df, verbose=True):#from santander ml comp\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    for col in df.columns :\n        if df[col].dtype=='bool' :\n            df[col] = df[col].astype(int)\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n                    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","1f682118":"X = reduce_mem_usage(X)\nX_test = reduce_mem_usage(X_test)","8cdbc1ed":"%%time\nX, X_test = initialize()","e09169e2":"def feature_engineering(X, X_test, clip_features=CLIP_FEATURES, cluster_distance=CLUSTER_DISTANCE):\n    \"\"\"\n    X : train data set\n    X_test : test data set\n    clip_features : bool : clip features to naturale scale or handel them by other way : abs or rescaling\n    cluster_distance : bool : True if you want to add cluster_distance features else false\n    (do not improuve the score on kaggle comp with catboost. See with other models...)\n    return : X (modified, target droped), y : target, X_test (modified)\n    \n    \"\"\"\n    \n    # drop the only one sample with Cover_Type == 5 :\n    outlier=X[X.Cover_Type==5].index\n    X.drop(index=outlier, axis=1, inplace=True)\n    X.reset_index(inplace=True, drop=True)\n    print('Cover_Type 5 droped : just one sample')\n    \n    y = X.Cover_Type.astype(int)\n    X_test.drop(['Id'], axis=1, inplace=True)\n    X.drop(['Cover_Type', 'Id'], axis=1, inplace=True)\n    print('Target feature y created')\n    print('Features \"Id\" droped in train and test set. \"Cover_type\" droped in train set')\n    \n    \n    \n    # drop 'Soil_Type7' and 'Soil_Type15' : no sample\n    X.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\n    X_test.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\n    print('\"Soil_Type7\" and \"Soil_Type15\" features droped in train and test set : no samples')\n    \n    \n    # sum of soil type\n    soil_features=[f'Soil_Type{i}' for i in range(1,41) if not i in (7, 15) ]\n    X['sum_soil']=X[soil_features].sum(axis=1)\n    X_test['sum_soil']=X_test[soil_features].sum(axis=1)\n    print('feature \"sum_soil\" added')\n    \n    # Handle distance features, Aspect and Hillshade_features, wich are out of natural range : clip or else\n    # Aspect is supposed to be between 0 and 360\n    # Hillshade is supposed to be between 0 and 255 \n    # --> http:\/\/www.geography.hunter.cuny.edu\/~jochen\/gtech361\/lectures\/lecture11\/concepts\/hillshade.htm\n    # distance features are supposed to be positive\n    \n    distance_features = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', \n                             'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points']\n    Hillshade_features = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \n    if clip_features:\n        # clip values for \"Aspect\" between 0\u00b0 and 360\u00b0\n        X['Aspect'].clip(0, 360, inplace=True)\n        X_test['Aspect'].clip(0, 360, inplace=True)\n        print('Feature \"Aspect\" cliped between 0 and 360 in train and test set')\n    \n        # clip values for \"Hillshade_features\" between 0 and 255\n        for feature in Hillshade_features:\n            X[feature].clip(0, 255, inplace=True)\n            X_test[feature].clip(0, 255, inplace=True)\n        print('Features \"Hillshade\" cliped positive in train and test set')\n            \n        # clip values for distances >0\n        for feature in distance_features:\n            X[feature].clip(0, inplace=True)\n            X_test[feature].clip(0, inplace=True)\n        print('Distance features cliped positive in train and test set')\n        \n        X['Slope'].clip(0, inplace=True)\n        X_test['Slope'].clip(0, inplace=True)\n        print('\"Slope\" features cliped positive in train and test set')\n        \n    else :\n        \n        #X[\"Aspect\"][X[\"Aspect\"] < 0] += 360\n        #X[\"Aspect\"][X[\"Aspect\"] > 359] -= 360\n        X[\"Aspect\"] = X[\"Aspect\"].map(lambda x : x + 360 if x < 0 else (x- 360 if x> 359 else x))\n        \n        #X_test[\"Aspect\"][X_test[\"Aspect\"] < 0] += 360\n        #X_test[\"Aspect\"][X_test[\"Aspect\"] > 359] -= 360\n        X_test[\"Aspect\"] = X_test[\"Aspect\"].map(lambda x : x + 360 if x < 0 else (x- 360 if x> 359 else x))\n        print('Features \"Aspect\" rescaled between 0 and 360 in train and test set')\n        \n        for feature in distance_features:\n            X[feature] = np.abs(X[feature])\n            X_test[feature] = np.abs(X_test[feature])\n        print('Distance features rescaled positive in train and test set')\n        \n        X[\"Slope\"] = np.abs(X[\"Slope\"])\n        X_test[\"Slope\"] = np.abs(X_test[\"Slope\"])\n        print('\"Slope\" feature rescaled positive in train and test set')\n        \n        for feature in Hillshade_features:\n            X[feature] = X[feature].map(lambda x : -x if x < 0 else (x - (x - 255) if x> 255 else x))\n            X_test[feature] = X_test[feature].map(lambda x : -x if x < 0 else (x - (x - 255) if x> 255 else x))\n            \n        print('Features \"Hillshade\" rescaled in [0, 255] in train and test set')\n        \n    \n    # sum of Wilderness_Area features :\n    Wilderness_Area_features = [f'Wilderness_Area{i}' for i in range(1,5)]\n    X['sum_W_A']=X[Wilderness_Area_features].sum(axis=1)\n    X_test['sum_W_A']=X_test[Wilderness_Area_features].sum(axis=1)\n    print('feature \"sum_W_A\" added in train and test set')\n    \n    # Hillshade_features : mean and amplitude :\n    X['mean_Hillshade']=X[Hillshade_features].mean(axis=1)\n    X_test['mean_Hillshade']=X_test[Hillshade_features].mean(axis=1)\n    X['amp_Hillshade']=X[Hillshade_features].max(axis=1)-X[Hillshade_features].min(axis=1)\n    X_test['amp_Hillshade']=X_test[Hillshade_features].max(axis=1) - X_test[Hillshade_features].min(axis=1)\n    print('feature \"amp_Hillshade\" added in train and test set')\n    \n    # Euclidean distance to hydrology\n    X['dist_water']=np.sqrt(\n        np.square(X['Horizontal_Distance_To_Hydrology'].astype('int64')) + np.square(X['Vertical_Distance_To_Hydrology'].astype('int64'))\n            )\n    X_test['dist_water']=np.sqrt(\n        np.square(X_test['Horizontal_Distance_To_Hydrology'].astype('int64')) + np.square(X_test['Vertical_Distance_To_Hydrology'].astype('int64'))\n            )\n    print('feature \"dist_water\" added in train and test set')\n    \n    # Add cluster distances features : n_cluster =3 (best results after optuna tuning)\n    if cluster_distance :\n        # Add \"cluster distance to centroids\" features\n        n_clusters = 3\n        kmeans=KMeans(n_clusters=n_clusters)\n    \n        cluster_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n           'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n           'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n           'Horizontal_Distance_To_Fire_Points', 'mean_Hillshade', 'amp_Hillshade', 'dist_water']\n        X_test_scaled = X_test.loc[:, cluster_features]\n        X_test_scaled = (X_test_scaled - X_test_scaled.mean(axis=0)) \/ X_test_scaled.std(axis=0)\n        X_scaled = X.loc[:, cluster_features]\n        X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n        X_cd = kmeans.fit_transform(X_scaled)\n        X_test_cd = kmeans.transform(X_test_scaled)\n\n        X_cd = pd.DataFrame(X_cd, index =X.index,  columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\n        X = X.join(X_cd)\n        X_test_cd = pd.DataFrame(X_test_cd, index =X_test.index, columns=[f\"Centroid_{i}\" for i in range(X_test_cd.shape[1])])\n        X_test = X_test.join(X_test_cd)\n        print('Feature cluster distance added in train and test set')\n    print('Feature engineering done')\n    \n    return X, y, X_test","6beb316b":"X, y, X_test = feature_engineering(X, X_test, clip_features=CLIP_FEATURES, cluster_distance=CLUSTER_DISTANCE)","cd6790f0":"X.head(5)","aa4a502e":"X.describe()","03266319":"feature='Horizontal_Distance_To_Roadways'\n# Horizontal_Distance_To_Fire_Points\nsns.displot(X[feature], kde=True)\nplt.show()","c71ac593":"feature='Horizontal_Distance_To_Fire_Points'\nsns.displot(X[feature], kde=True)\nplt.show()","fabb2b02":"feature='Slope'\n# Horizontal_Distance_To_Fire_Points\nsns.displot(X[feature], kde=True)\nplt.show()","bf35cbe8":"from scipy.stats import chi2_contingency\ntableW_A = pd.crosstab(X['sum_W_A'], y)\ntestW_A=chi2_contingency(tableW_A)\ntestW_A","8ebb9765":"tableSoil = pd.crosstab(X['sum_soil'], y)\ntestSoil=chi2_contingency(tableSoil)\ntestSoil","59e15c10":"def make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n","7d4cdac8":"discrete_features = [col for col in X.columns if X.loc(axis=1)[col].dtypes != 'float64']\ndiscrete_features = [col in discrete_features for col in X.columns]","0a1711a7":"# discrete_features","32e8e86f":"# mi_scores = make_mi_scores(X, y, discrete_features)\n","c5bae3f2":"# mi_scores  \n# show a few features with their MI scores","ec09dd3b":"# Wilderness_Area_features :\nfor i in range(1,5) :\n    tableWilderness = pd.crosstab(X[f'Wilderness_Area{i}'], y)\n    print(f'Wilderness_Area{i} :\\n')\n    display(tableWilderness)","47b4a3e2":"X_test.head(5)","6678a64c":"# Can't be done : too long\n#%%time\n#sm = SMOTE(random_state=RANDOM_STATE, n_jobs=-1)\n#X, y = sm.fit_resample(X, y)","dc8e5b7e":"X.shape","8571194c":"X.to_pickle('..\/input\/tabular-playground-series-dec-2021\/X_resample.pkl')\ny.to_pickle('..\/input\/tabular-playground-series-dec-2021\/y_resample.pkl')\n# df = pd.read_pickle(file_name)","0c6b1b36":"test_preds=[]\ncount=0   \n\nalpha = 0.1\ngeo_mean = make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n\ndef objective(trial):\n    \n    kf = StratifiedShuffleSplit(n_splits=N_SPLITS, test_size=0.2, random_state=RANDOM_STATE)\n    score=[]\n    score_geo=[]\n    global X_test\n    global count\n    idx=0\n    \n    \n    n_clusters = trial.suggest_int('n_clusters', 2, 10)\n    kmeans=KMeans(n_clusters=n_clusters)\n    \n    cluster_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n           'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n           'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n           'Horizontal_Distance_To_Fire_Points', 'mean_Hillshade', 'amp_Hillshade', 'dist_water']\n        \n    \n    idx=0\n    \n    for train_index, test_index in kf.split(X, y) :\n        \n        start = time.time()\n        \n        \n        X_train,  y_train = X.iloc[train_index], y.iloc[train_index]\n        X_valid, y_valid = X.iloc[test_index], y.iloc[test_index]\n        print(\"smote done\")       \n       \n        # Standardize\n        X_test_in = X_test.copy()\n        X_train_scaled = X_train.loc[:, cluster_features]\n        X_train_scaled = (X_train_scaled - X_train_scaled.mean(axis=0)) \/ X_train_scaled.std(axis=0)\n        X_valid_scaled = X_valid.loc[:, cluster_features]\n        X_valid_scaled = (X_valid_scaled - X_valid_scaled.mean(axis=0)) \/ X_valid_scaled.std(axis=0)\n        X_test_scaled = X_test_in.loc[:, cluster_features]\n        X_test_scaled = (X_test_scaled - X_test_scaled.mean(axis=0)) \/ X_test_scaled.std(axis=0)\n        #X_test_scaled = X_test_in.loc[:, cluster_features]\n        #X_test_scaled = (X_test_scaled - X_test_scaled.mean(axis=0)) \/ X_test_scaled.std(axis=0)\n        \n        \n        X_train_cd = kmeans.fit_transform(X_train_scaled)\n        X_valid_cd = kmeans.transform(X_valid_scaled)\n        # X_test_cd = kmeans.transform(X_test_scaled)\n        \n        # Label features and join to dataset\n        X_train_cd = pd.DataFrame(X_train_cd, index =X_train.index,  columns=[f\"Centroid_{i}\" for i in range(X_train_cd.shape[1])])\n        X_train = X_train.join(X_train_cd)\n        X_valid_cd = pd.DataFrame(X_valid_cd, index =X_valid.index, columns=[f\"Centroid_{i}\" for i in range(X_valid_cd.shape[1])])\n        X_valid = X_valid.join(X_valid_cd)\n        X_test_cd = kmeans.transform(X_test_scaled)\n        X_test_cd = pd.DataFrame(X_test_cd, index =X_test_in.index, columns=[f\"Centroid_{i}\" for i in range(X_test_cd.shape[1])])\n        X_test_in = X_test_in.join(X_test_cd)\n        \n        # cat_clf = make_pipeline(SMOTE(random_state=RANDOM_STATE), \n        #                            CatBoostClassifier(iterations=1000, task_type=\"GPU\")) \n        cat_clf = catboost.CatBoostClassifier(iterations=2000, task_type=\"GPU\")\n        cat_clf.fit(X_train, y_train, verbose=0) #eval_set=[X_valid, y_valid]\n        cat_preds = cat_clf.predict(X_valid)\n        cat_test_preds = cat_clf.predict(X_test_in)\n        test_preds.append(cat_test_preds)\n        score.append(accuracy_score(cat_preds, y_valid))\n        score_geo.append(geo_mean(cat_preds, y_valid))\n        end = time.time()\n        idx+=1\n        print(f'Finished round{count}, fold {idx} done in {end-start} with accuracy {accuracy_score(cat_preds, y_valid)} and geo_score {geo_mean(cat_preds, y_valid)}  ')\n    count+=1    \n    print(f'Round {count} finished with score {np.mean(score)} and geo_score {np.mean(score_geo)}')\n    return np.mean(score_geo)\n        \n","a144b0fc":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=20)","e8610400":"# Add \"cluster distance to centroids\" features\nn_clusters = 3\nkmeans=KMeans(n_clusters=n_clusters)\n    \ncluster_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n           'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n           'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n           'Horizontal_Distance_To_Fire_Points', 'mean_Hillshade', 'amp_Hillshade', 'dist_water']\nX_test_scaled = X_test.loc[:, cluster_features]\nX_test_scaled = (X_test_scaled - X_test_scaled.mean(axis=0)) \/ X_test_scaled.std(axis=0)\nX_scaled = X.loc[:, cluster_features]\nX_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\nX_cd = kmeans.fit_transform(X_scaled)\nX_test_cd = kmeans.transform(X_test_scaled)\n\nX_cd = pd.DataFrame(X_cd, index =X.index,  columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])])\nX = X.join(X_cd)\nX_test_cd = pd.DataFrame(X_test_cd, index =X_test.index, columns=[f\"Centroid_{i}\" for i in range(X_test_cd.shape[1])])\nX_test = X_test.join(X_test_cd)\n\n    \n ","eac8e96c":"X.head(5)","48e5e3fd":"X_test.head(5)","0b6b69ae":"print(study.best_trial)\nprint(study.best_params)","c505e4bd":"'''\nkf = StratifiedShuffleSplit(n_splits=N_SPLITS, test_size=0.2, random_state=RANDOM_STATE)\nX1 = pd.concat([X, X.iloc[y[y==5].index]], axis=0)\ny1 = pd.concat([y, y[y==5]], axis=0)\nfor train_index, test_index in kf.split(X1, y1.astype(int)) :\n    X_train = X1.iloc[train_index]\n    y_train = y1.iloc[train_index]\n    X_valid = X1.iloc[test_index]\n    y_valid = y1.iloc[test_index]\n    \n    n_clusters = 4\n        \n    cluster_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n           'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n           'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n           'Horizontal_Distance_To_Fire_Points', 'mean_Hillshade', 'amp_Hillshade', 'dist_water']\n        \n    kmeans=KMeans(n_clusters=n_clusters)\n    # Standardize\n    X_train_scaled = X_train.loc[:, cluster_features]\n    X_train_scaled = (X_train_scaled - X_train_scaled.mean(axis=0)) \/ X_train_scaled.std(axis=0)\n    X_valid_scaled = X_valid.loc[:, cluster_features]\n    X_valid_scaled = (X_valid_scaled - X_valid_scaled.mean(axis=0)) \/ X_valid_scaled.std(axis=0)\n    \n    X_train_cd = kmeans.fit_transform(X_train_scaled)\n    X_valid_cd = kmeans.transform(X_valid_scaled)\n    print('X_train_cd :')\n    display(X_train_cd[:5])\n    print('X_train_cd.shape :', X_train_cd.shape)\n    \n    print('----------------')\n    print('X_valid_cd :')\n    display(X_valid_cd[:5])\n    print('X_valid_cd.shape :', X_valid_cd.shape)\n    \n    # Label features and join to dataset\n    X_train_cd = pd.DataFrame(X_train_cd, index =X_train.index,  columns=[f\"Centroid_{i}\" for i in range(X_train_cd.shape[1])])\n    X_train = X_train.join(X_train_cd)\n    X_valid_cd = pd.DataFrame(X_valid_cd, index =X_valid.index, columns=[f\"Centroid_{i}\" for i in range(X_valid_cd.shape[1])])\n    display(X_valid_cd.head(5))\n    X_valid = X_valid.join(X_valid_cd)\n    print('--------------------------')\n    print('X_train :')\n    display(X_train.head(5))\n    print('--------------------------')\n    print('X_valid :')\n    display(X_valid.head(5))\n'''","0bbeab71":"X.isnull().sum()","cc520ef7":"cat_clf = catboost.CatBoostClassifier(iterations=2000, task_type=\"GPU\")","c8b984c7":"final_test_predictions = [] # list of predictions for X_test on every split\nfinal_valid_predictions = {} # key : index of validation set, values : predictions for X_valid on every split (probabilities)\nscores = []\nscore_geo=[]\n\nkf = StratifiedShuffleSplit(n_splits=N_SPLITS, test_size=0.2, random_state=RANDOM_STATE)\nalpha = 0.1\ngeo_mean = make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n    start = time.time()\n    print(10*\"=\", f\"Fold={fold}\", 10*\"=\")\n\n    x_train = X.loc[train_idx, :]\n    x_valid = X.loc[valid_idx, :]\n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    cat_clf = catboost.CatBoostClassifier(iterations=2000, task_type=\"GPU\")\n    \n    cat_clf.fit(x_train, y_train, verbose=0)\n\n    preds_valid = cat_clf.predict(x_valid)\n    final_valid_predictions.update(dict(zip(valid_idx, preds_valid)))\n    \n    accur = accuracy_score(y_valid, preds_valid)\n    geo_score = geo_mean(y_valid, preds_valid)\n    scores.append(accur)\n    score_geo.append(geo_score)\n    #loss = log_loss(y_valid, preds_valid)\n    print('accuracy: ', accur)\n    print('geo_score', geo_score)\n    \n    \n    \n    test_preds = cat_clf.predict(X_test)\n    final_test_predictions.append(test_preds)\n    end = time.time()\n    print(f\"Finished fold {fold} in {end - start}\")\n ","6b26b9ec":"print(f\"scores -> mean: {np.mean(scores)}, std: {np.std(scores)}\")\nprint(f\"scores_geo -> mean: {np.mean(score_geo)}, std: {np.std(score_geo)}\")","49029bc5":"# dict values : rows of the df\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)","69ed8d90":"df = pd.DataFrame(np.column_stack(final_test_predictions))\n\ndf.head(15)","8392cd28":"# mod = df.mode(axis=1)","4140a320":"# mod2 = mod.loc[:,0].astype(int) ","723e5535":"# mod2.head(10)","b702fdf9":"#col=mod.loc[:5,0].astype(int)\n#col","34c7bb39":"#df['mode']=mod","2e97c646":"#df_test=df.loc[70:80]\n#df_test","4b1e18d6":"mod2=df_test.mode(axis=1)\ndf_test['mode']=mod2\ndf_test.head(5)","c898bf01":"df.head(100)","c0318e31":"submission['Cover_Type'] = mod2\n\nsubmission.to_csv(\"test_pred_1.csv\",index=None)\nsubmission.to_csv(\"catboost_cv_FE.csv\",index=None)\nsubmission[70:79]","d372083e":"submission[70:79]","cce1fe5a":"print('done')","89f1575d":"import shap\nfrom catboost import *\nexplainer = shap.TreeExplainer(cat_clf)\nshap_values = explainer.shap_values(X)","775824b9":"# visualize the first prediction's explanation\nshap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])","4fd19b5a":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X)","1c955f13":"cat_preds = cat_clf.predict(X_test)","f4ccd265":"submission['Cover_Type'] = cat_preds\nsubmission.head(5)","b3ae93aa":"submission.to_csv(\"submission.csv\", index=None)","624e7351":"## Reduce memory","b90732c6":"### DATA FIELDS :  \n\n(from https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/overview)\n\nElevation - Elevation in meters\n\nAspect - Aspect in degrees azimuth\n\nSlope - Slope in degrees\n\nHorizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n\nVertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n\nHorizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n\nHillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n\nHillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n\nHillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n\nHorizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n\nWilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area\ndesignation\n\nSoil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n\nCover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n\n\n\nThe wilderness areas are:\n\n1 - Rawah Wilderness Area\n\n2 - Neota Wilderness Area\n\n3 - Comanche Peak Wilderness Area\n\n4 - Cache la Poudre Wilderness Area\n\n\n\nThe soil types are:\n\n1 Cathedral family - Rock outcrop complex, extremely stony.\n\n2 Vanet - Ratake families complex, very stony.\n\n3 Haploborolis - Rock outcrop complex, rubbly.\n\n4 Ratake family - Rock outcrop complex, rubbly.\n\n5 Vanet family - Rock outcrop complex complex, rubbly.\n\n6 Vanet - Wetmore families - Rock outcrop complex, stony.\n\n7 Gothic family.\n\n8 Supervisor - Limber families complex.\n\n9 Troutville family, very stony.\n\n10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n\n11 Bullwark - Catamount families - Rock land complex, rubbly.\n\n12 Legault family - Rock land complex, stony.\n\n13 Catamount family - Rock land - Bullwark family complex, rubbly.\n\n14 Pachic Argiborolis - Aquolis complex.\n\n15 unspecified in the USFS Soil and ELU Survey.\n\n16 Cryaquolis - Cryoborolis complex.\n\n17 Gateview family - Cryaquolis complex.\n\n18 Rogert family, very stony.\n\n19 Typic Cryaquolis - Borohemists complex.\n\n20 Typic Cryaquepts - Typic Cryaquolls complex.\n\n21 Typic Cryaquolls - Leighcan family, till substratum complex.\n\n22 Leighcan family, till substratum, extremely bouldery.\n\n23 Leighcan family, till substratum - Typic Cryaquolls complex.\n\n24 Leighcan family, extremely stony.\n\n25 Leighcan family, warm, extremely stony.\n\n26 Granile - Catamount families complex, very stony.\n\n27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n\n28 Leighcan family - Rock outcrop complex, extremely stony.\n\n29 Como - Legault families complex, extremely stony.\n\n30 Como family - Rock land - Legault family complex, extremely stony.\n\n31 Leighcan - Catamount families complex, extremely stony.\n\n32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n\n33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n\n34 Cryorthents - Rock land complex, extremely stony.\n\n35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n\n36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n\n37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n\n38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n\n39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n\n40 Moran family - Cryorthents - Rock land complex, extremely stony.","8d9a57b4":"## Feature engineering :","2ce9cc49":"## Modeling :","3601e26a":"## Import data :","b49cabb5":"FrozenTrial(number=1, values=[0.9606684999999999], datetime_start=datetime.datetime(2021, 12, 5, 10, 39, 14, 808230), datetime_complete=datetime.datetime(2021, 12, 5, 10, 46, 21, 260684), params={'n_clusters': 3}, distributions={'n_clusters': IntUniformDistribution(high=10, low=2, step=1)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=1, state=TrialState.COMPLETE, value=None)\n{'n_clusters': 3}","889ae95a":"## Import modules :","ec45d469":"#### MI Scores : \nElevation                            0.618  \nWilderness_Area4                     0.066  \nHorizontal_Distance_To_Roadways      0.033  \nHorizontal_Distance_To_Fire_Points   0.022  \nWilderness_Area1                     0.019  \nsum_soil                             0.013  \nsum_W_A                              0.011  \nWilderness_Area3                     0.011  \nSoil_Type39                          0.006  \nSoil_Type10                          0.006  \nSoil_Type38                          0.004  \nVertical_Distance_To_Hydrology       0.004  \nSoil_Type6                           0.004  \nSoil_Type3                           0.003  \nSoil_Type40                          0.003  \nWilderness_Area2                     0.003  \nSoil_Type2                           0.003  \nSoil_Type23                          0.002  \ndist_water                           0.002  \nSoil_Type32                          0.002  \nmean_Hillshade                       0.002  \nHorizontal_Distance_To_Hydrology     0.002  \nSoil_Type33                          0.002  \nSoil_Type22                          0.001  \nSoil_Type29                          0.001  \nSlope                                0.001  \nSoil_Type30                          0.001  \nSoil_Type35                          0.001  \nSoil_Type4                           0.001  \nSoil_Type31                          0.001  \nSoil_Type13                          0.001  \nSoil_Type24                          0.001  \nSoil_Type37                          0.001  \nSoil_Type11                          0.000  \nSoil_Type17                          0.000  \nHillshade_3pm                        0.000  \nSoil_Type36                          0.000  \namp_Hillshade                        0.000  \nAspect                               0.000  \nSoil_Type12                          0.000  \nSoil_Type1                           0.000  \nHillshade_9am                        0.000  \nHillshade_Noon                       0.000  \nSoil_Type20                          0.000  \nSoil_Type19                          0.000  \nSoil_Type34                          0.000  \nSoil_Type14                          0.000  \nSoil_Type18                          0.000  \nSoil_Type16                          0.000  \nSoil_Type26                          0.000  \nSoil_Type21                          0.000  \nSoil_Type27                          0.000  \nSoil_Type28                          0.000  \nSoil_Type9                           0.000  \nSoil_Type25                          0.000  \nSoil_Type5                           0.000  \nSoil_Type8                           0.000","d312a5f4":"V1 : without FE:\nKaggle score : 0.95310\n\nV2 : with FE : \nKaggle score : 0.95460\n\nV3 : with cv, FE and cluster distance : 0.95466\n\nV4 : with cv, FE without cluster distance : 0.95484\n\nV5 : with cv no FE : 0.95397\n"}}