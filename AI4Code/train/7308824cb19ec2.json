{"cell_type":{"c3580b4c":"code","1bf1dd16":"code","7d128dcb":"code","33c0db5b":"code","ab2c1076":"code","2e2177f1":"code","39f93c74":"code","81973b51":"code","ba6817d4":"code","4604cb55":"code","fb66c0b8":"code","e29732d5":"code","3d7b406f":"code","a4c4f3f2":"code","08e4b586":"code","b4520080":"code","07489235":"code","5deb84b7":"code","9a713694":"code","090950bc":"code","d69f2007":"code","93447ef2":"code","143f75a3":"markdown","668b47ff":"markdown","3ec54113":"markdown","464f6688":"markdown","de3d039e":"markdown","129a9b28":"markdown","3721700d":"markdown","f749cfb0":"markdown","ee669065":"markdown","330f3c9b":"markdown"},"source":{"c3580b4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom scipy.stats import randint\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. \nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.model_selection import KFold # use for cross validation\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline # pipeline making\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nfrom sklearn.metrics import mean_squared_error,r2_score\n\n## for Deep-learing:\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout\n\n# Any results you write to the current directory are saved as output.","1bf1dd16":"df = pd.read_csv('..\/input\/household_power_consumption.txt',sep=';')","7d128dcb":"display(df.head())\ndisplay(df.isnull().sum())","33c0db5b":"#merging two calloumns data and time \n#missing values \ndf = pd.read_csv('..\/input\/household_power_consumption.txt', sep=';', \n                 parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n                 low_memory=False, na_values=['nan','?'], index_col='dt')","ab2c1076":"df.head()","2e2177f1":"display(df.dtypes)\ndisplay(df.info())\ndisplay(df.describe())","39f93c74":"droping_list_all=[]\nfor j in range(0,7):\n    if not df.iloc[:, j].notnull().all():\n        droping_list_all.append(j)        \n        #print(df.iloc[:,j].unique())\ndroping_list_all","81973b51":"\nfor j in range(0,7):        \n        df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())","ba6817d4":"df.isnull().sum()","4604cb55":"df.Global_active_power.resample('D').sum().plot(title='Global_active_power resampled over day for sum') \n\nplt.tight_layout()\nplt.show()   \n\ndf.Global_active_power.resample('D').mean().plot(title='Global_active_power resampled over day for mean', color='red') \nplt.tight_layout()\nplt.show()","fb66c0b8":"r = df.Global_intensity.resample('D').agg(['mean', 'std'])\nr.plot(subplots = True, title='Global_intensity resampled over day')\nplt.show()","e29732d5":"r2 = df.Global_reactive_power.resample('D').agg(['mean', 'std'])\nr2.plot(subplots = True, title='Global_reactive_power resampled over day', color='red')\nplt.show()","3d7b406f":"# Correlations among columns\nplt.matshow(df.corr(method='spearman'),vmax=1,vmin=-1,cmap='PRGn')\nplt.title('without resampling', size=15)\nplt.colorbar()\nplt.show()","a4c4f3f2":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdff = pd.DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(dff.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(dff.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = pd.concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg\n ","08e4b586":"df_resample = df.resample('h').mean() \ndf_resample.shape\nvalues = df_resample.values \n\n\n## full data without resampling\n#values = df.values\n\n# integer encode direction\n# ensure all data is float\n#values = values.astype('float32')\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[8,9,10,11,12,13]], axis=1, inplace=True)\nprint(reframed.head())","b4520080":"# split into train and test sets\nvalues = reframed.values\n\nn_train_time = 365*24\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\n##test = values[n_train_time:n_test_time, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :-1], train[:, -1]\ntest_X, test_y = test[:, :-1], test[:, -1]\n# reshape input to be 3D [samples, timesteps, features]\ntrain_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape) \n# We reshaped the input into the 3D format as expected by LSTMs, namely [samples, timesteps, features].","07489235":"model = Sequential()\nmodel.add(LSTM(100, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1))\n","5deb84b7":"model.compile(loss='mean_squared_error', optimizer='adam')\n","9a713694":"model.summary()","090950bc":"# fit network\nhistory = model.fit(train_X, train_y, epochs=20, batch_size=70, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n","d69f2007":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\n# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], 7))\n# invert scaling for forecast\ninv_yhat = np.concatenate((yhat, test_X[:, -6:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_X[:, -6:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","93447ef2":"aa=[x for x in range(200)]\nplt.plot(aa, inv_y[:200], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:200], 'r', label=\"prediction\")\nplt.ylabel('Global_active_power', size=15)\nplt.xlabel('Time step', size=15)\nplt.legend(fontsize=15)\nplt.show()","143f75a3":"## Mean and std of 'Global_intensity'","668b47ff":"# EDA","3ec54113":"# Dealing with Missing values","464f6688":"# **Data Ingestion **","de3d039e":"## Correlation ","129a9b28":"##  Mean and std of Global_active_power","3721700d":"## Mean and std of Global_reactive_power","f749cfb0":"## Splitting Train and Test data","ee669065":"# LSTM Network ","330f3c9b":"## Resample the data"}}