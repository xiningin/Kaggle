{"cell_type":{"e649d963":"code","1459038b":"code","91a2942a":"code","f89f65b0":"code","a1111296":"code","690a5c63":"code","fc2ada35":"code","9287b653":"code","8f89ffc5":"code","2ff5b9b9":"code","eaeaaf85":"code","1b50969d":"code","162fe522":"code","ed2bcfd6":"code","da31efe3":"code","0a7d8fc8":"code","6f0a0f8f":"code","da619174":"code","00aa315f":"code","017ac8b9":"markdown","8afbd1f9":"markdown","6c792554":"markdown","564d9e97":"markdown","fe8de020":"markdown","91f5db1a":"markdown","ebc768c7":"markdown","09741339":"markdown","cfc6ca94":"markdown","1133f847":"markdown"},"source":{"e649d963":"!pip install transformers","1459038b":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm, trange\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertPreTrainedModel, BertModel, get_linear_schedule_with_warmup, AdamW\n\nfrom transformers import AutoConfig, AutoTokenizer\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","91a2942a":"MODEL_OUT_DIR = '\/kaggle\/working\/my_model'\n\nDATA = pd.read_csv('..\/input\/google-app-review-dataset\/reviews.csv')\n\n## Model Configurations\nMAX_LEN_TRAIN = 160\nMAX_LEN_VALID = 160\nBATCH_SIZE = 16\nLR = 2e-5\nNUM_EPOCHS = 3 #10 ####################################################################################### Change it to 10\nNUM_THREADS = 4  ## Number of threads for collecting dataset\nMODEL_NAME = 'bert-base-uncased'\n\n\nif not os.path.isdir(MODEL_OUT_DIR):\n    os.makedirs(MODEL_OUT_DIR)\n\n\ndef to_sentiment(rating):\n  rating = int(rating)\n  if rating <= 2:\n    return 0\n  elif rating == 3:\n    return 1\n  else:\n    return 2\nDATA['sentiment'] = DATA.score.apply(to_sentiment)\nsns.countplot(DATA.sentiment)\nplt.xlabel('review score');\n","f89f65b0":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(DATA, test_size=0.1, random_state=RANDOM_SEED)\nvalid_df, test_df = train_test_split(DATA, test_size=0.5, random_state=RANDOM_SEED)","a1111296":"train_df.reset_index(inplace=True)\nvalid_df.reset_index(inplace=True)\ntest_df.reset_index(inplace=True)","690a5c63":"train_df.head()","fc2ada35":"class GoogleDataset(Dataset):\n\n    def __init__(self, file, maxlen, tokenizer): \n        #Store the contents of the file in a pandas dataframe\n        # self.df = pd.read_csv(filename)\n        self.df = file\n        #Initialize the tokenizer for the desired transformer model\n        self.tokenizer = tokenizer\n        #Maximum length of the tokens list to keep all the sequences of fixed size\n        self.maxlen = maxlen\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):    \n        #Select the sentence and label at the specified index in the data frame\n        sentence = self.df.loc[index, 'content']\n        label = self.df.loc[index, 'sentiment']\n        #Preprocess the text to be suitable for the transformer\n        tokens = self.tokenizer.tokenize((sentence)) \n        encoding = self.tokenizer.encode_plus(\n          sentence,\n          add_special_tokens=True,\n          max_length=self.maxlen,\n          return_token_type_ids=False,\n          pad_to_max_length=True,\n          return_attention_mask=True,\n          truncation = True,\n          return_tensors='pt',\n        )\n        input_ids = encoding['input_ids'].flatten()\n        attention_mask = encoding['attention_mask'].flatten()\n        label = torch.tensor(label, dtype=torch.long)\n        \n        return input_ids, attention_mask, label","9287b653":"## Tokenizer loaded from AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n## Training Dataset\ntrain_set = GoogleDataset(file=train_df, maxlen=MAX_LEN_TRAIN, tokenizer=tokenizer)\nvalid_set = GoogleDataset(file=valid_df, maxlen=MAX_LEN_VALID, tokenizer=tokenizer)\ntest_set = GoogleDataset(file=test_df, maxlen=MAX_LEN_VALID, tokenizer=tokenizer)","8f89ffc5":"## Data Loaders\ntrain_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\nvalid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\ntest_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n\nprint(len(train_loader))\n","2ff5b9b9":"class BertForSentimentClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(p=0.3)\n        #The classification layer that takes the [CLS] representation and outputs the logit\n        self.cls_layer = nn.Linear(config.hidden_size, 3)\n\n    def forward(self, input_ids, attention_mask):\n        #Feed the input to Bert model to obtain contextualized representations\n        reps, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        #Obtain the representations of [CLS] heads\n        cls_reps = reps[:, 0]\n        cls_reps = self.dropout(cls_reps)\n        logits = self.cls_layer(cls_reps)\n        return logits\n","eaeaaf85":"## Configuration loaded from AutoConfig \nconfig = AutoConfig.from_pretrained(MODEL_NAME)\n## Creating the model from the desired transformer model\nmodel = BertForSentimentClassification.from_pretrained(MODEL_NAME, config=config)\n## GPU or CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n## Putting model to device\nmodel = model.to(device)","1b50969d":"## Experimenting\ninput_ids, attention_mask, label = next(iter(train_loader))\ninput_ids, attention_mask, label = input_ids.to(device), attention_mask.to(device), label.to(device)\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length\noutput = model(input_ids, attention_mask)\nF.softmax(model(input_ids, attention_mask), dim=1)","162fe522":"criterion = nn.CrossEntropyLoss().to(device)\n## Optimizer\noptimizer = AdamW(model.parameters(), lr=LR, correct_bias=False)\ntotal_steps = len(train_loader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)","ed2bcfd6":"def train(model, criterion, optimizer, train_loader, val_loader, epochs, scheduler):\n    best_acc = 0\n    for epoch in trange(epochs, desc=\"Epoch\"):\n        model.train()\n        train_acc = 0\n        for i, (input_ids, attention_mask, labels) in enumerate(tqdm(iterable=train_loader, desc=\"Training\")):\n            # optimizer.zero_grad()  \n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n            loss = criterion(logits, labels)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            train_acc += get_accuracy_from_logits(logits, labels)\n        print(f\"Training accuracy is {train_acc\/len(train_loader)}\")\n        val_acc, val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n        print(\"Epoch {} complete! Validation Accuracy : {}, Validation Loss : {}\".format(epoch, val_acc, val_loss))\n        if val_acc > best_acc:\n            print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_acc, val_acc))\n            best_acc = val_acc\n            model.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            config.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n            tokenizer.save_pretrained(save_directory=MODEL_OUT_DIR + '\/')\n\n## Accuracy Function\ndef get_accuracy_from_logits(logits, labels):\n    probs = F.softmax(logits, dim=1)\n    output = torch.argmax(probs, dim=1)\n    acc = (output == labels).float().mean()\n    return acc","da31efe3":"def evaluate(model, criterion, dataloader, device, test=False):\n    model.eval()\n    mean_acc, mean_loss, count = 0, 0, 0\n    total_acc = 0\n    with torch.no_grad():\n        for input_ids, attention_mask, labels in tqdm(dataloader, desc=\"Evaluating\"):\n        # for input_ids, attention_mask, labels in (dataloader):\n            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n            logits = model(input_ids, attention_mask)\n            mean_loss += criterion(logits.squeeze(-1), labels).item()\n            mean_acc += get_accuracy_from_logits(logits, labels)\n            count += 1\n            if test == True:\n              total_acc += mean_acc\/count\n        if test == True:\n          return total_acc\/count\n    return mean_acc \/ count, mean_loss \/ count","0a7d8fc8":"train(model=model, \n      criterion=criterion,\n      optimizer=optimizer, \n      train_loader=train_loader,\n      val_loader=valid_loader,\n      epochs = NUM_EPOCHS,\n      scheduler = scheduler\n      )","6f0a0f8f":"evaluate(model=model, criterion=criterion, dataloader=test_loader, device=device, test=True)","da619174":"def classify_sentiment(sentence):\n  with torch.no_grad():  \n    tokens = tokenizer.tokenize(sentence)\n    tokens = ['[CLS]'] + tokens + ['[SEP]']\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = torch.tensor(input_ids).to(device)\n    input_ids = input_ids.unsqueeze(0)\n    attention_mask = (input_ids != 0).long()\n    attention_mask = attention_mask.to(device)\n    if len(tokens) < MAX_LEN_VALID:\n        tokens = tokens + ['[PAD]' for _ in range(MAX_LEN_VALID - len(tokens))] \n    else:\n        tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n\n    print(input_ids.shape)\n    logit = model(input_ids=input_ids, attention_mask=attention_mask)\n    prob = F.softmax(logit, dim=1)\n    output = torch.argmax(prob)\n    prob = prob[0][output]\n    if output == 0:\n        print('Negative {}'.format(int(prob*100)))\n    elif output == 1:\n        print('Neutral {}'.format(int(prob*100)))\n    elif output == 2:\n        print('Positive {}'.format(int(prob*100)))","00aa315f":"sentence = \"Hope this notebook was informative and useful\"\nclassify_sentiment(sentence)","017ac8b9":"## Train Function","8afbd1f9":"## Downloads","6c792554":"## Creating Dataset","564d9e97":"## Building Model","fe8de020":"## Imports and Libraries","91f5db1a":"## Create Analysis","ebc768c7":"## Configurations","09741339":"## Evaluation with test","cfc6ca94":"# Google Review","1133f847":"\n## Evaluate Function"}}