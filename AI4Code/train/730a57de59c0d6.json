{"cell_type":{"b8042eae":"code","b237a3cb":"code","61c200d4":"code","1f8f9273":"code","5a5bc124":"code","b7531416":"code","6ae1bbde":"code","caefbfce":"code","3427d44b":"code","cc34a164":"code","64141abe":"code","bee036fb":"code","59d33cb9":"code","cc866f2a":"code","2e8a10e5":"code","f57817be":"code","9c14c04f":"code","1d9a8bf7":"code","b6fe9801":"code","8e4a9fef":"code","c2c39ca1":"code","abc6525b":"code","fcc76e26":"code","b773348e":"code","7452b724":"code","a7c6a9a1":"code","8a672eea":"code","14e06205":"code","f955d8d8":"code","53db94b0":"code","2479060d":"code","c4352f43":"code","e9340bee":"code","c22d4579":"code","971d94a7":"code","fd501095":"code","36347da5":"code","a5881364":"code","d50414ae":"code","c5b4f5ef":"code","1466f151":"code","a869ef87":"code","b4eb4bbe":"code","f1cc46a0":"code","d5661336":"code","09cc6d33":"code","cf76bbf8":"code","c3e6b36b":"code","241f5fc1":"code","a4196312":"code","c18a58e6":"code","30331f4a":"code","aeeb282b":"code","a897a502":"code","1b3e4e45":"code","65476474":"code","31ad939c":"code","5b22c7a7":"code","a9b4e0a4":"code","33555fb2":"code","848896a1":"code","89a173fc":"code","f4eb751b":"code","7cc1e9bd":"code","ee94bab5":"code","060991b2":"code","550f34c8":"code","5b4d429b":"code","4b16950c":"code","e91d5778":"code","9afbd33b":"code","ff7b6439":"code","450bdd0c":"code","eb50a082":"markdown","c9480d28":"markdown","c7f97397":"markdown","9614362d":"markdown","3efbb346":"markdown","a4193680":"markdown","d59f2cae":"markdown","d477d0e2":"markdown","f650b587":"markdown","aac1cab4":"markdown","672dba59":"markdown","339baef1":"markdown","c0d93ed1":"markdown","63b10760":"markdown","1b0e5f8b":"markdown","6e41d149":"markdown","5812fca3":"markdown","ec6bd449":"markdown","6320f5fb":"markdown","8b93280a":"markdown","38f62932":"markdown","cec47437":"markdown","d31d1cb2":"markdown","155a1b51":"markdown","ddeda3a0":"markdown","eff0c86e":"markdown","a934a822":"markdown","4b4de5c7":"markdown","1364b376":"markdown","3d9c94e8":"markdown","4b3fe62f":"markdown","853b9961":"markdown","87e54a8b":"markdown","96d2bc65":"markdown","69e594bf":"markdown","930989f9":"markdown","74a26264":"markdown","d033f368":"markdown","be5ded67":"markdown","696c3586":"markdown","c9234745":"markdown","8fb15ce1":"markdown","ce166d80":"markdown","315065d1":"markdown","c366bb45":"markdown","7eff899e":"markdown","370aeb54":"markdown","b1a4994a":"markdown","53a8343a":"markdown","13623dfc":"markdown"},"source":{"b8042eae":"import os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', 5000)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#os.mkdir('\/kaggle\/working\/individual_charts\/')\nimport matplotlib.pyplot as plt\n# Load the data\n#Will come in handy to wrap the lengthy texts\nimport textwrap\n#useful libraries and functions\n#import sidetable as stb\nfrom itertools import repeat\n#Libraries that give a different visual possibilities\nfrom pandas import option_context \nfrom plotly.subplots import make_subplots\n#Importing Market Basket Analysis libraries\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\nfrom wordcloud import WordCloud\nfrom geopy.geocoders import Nominatim\n\nimport gensim\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\ndef long_sentences_seperate(sentence, width=30):\n    try:\n        splittext = textwrap.wrap(sentence,width)\n        text = '<br>'.join(splittext)#whitespace is removed, and the sentence is joined\n        return text\n    except:\n        return sentence\n\ndef load_csv(base_dir,file_name):\n    \"\"\"Loads a CSV file into a Pandas DataFrame\"\"\"\n    file_path = os.path.join(base_dir,file_name)\n    df = pd.read_csv(file_path,low_memory=False)\n    return df    \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b237a3cb":"#Supporting Functions that are used at various locations in the notebook\n\n#Function to reduce the names to just abbreviations\ndef shrnk_name(company):\n    lngt = company.split(' ')\n    temp = str()\n    if len(lngt) > 1:\n        for x in lngt:\n            temp = temp + x[0]\n        return temp\n    else:\n        return company\n\n#Function that converts the strings that needs to be numbers. This function grew, as I started finding issues\n#Issues like finding \"M\",\",\" and \".\" in the sales, profit, MV and assets values \ndef convert_cost(x):\n    if ',' in x: #checking if there is ',' in the string\n        temp = x.replace(',','')\n        if '.'in temp:\n            return float(temp)\n        else:\n            return int(temp)\n\n    else:\n        return int(x)","61c200d4":"base_dir = '..\/input\/zomato-bangalore-restaurants'\nfile_name = 'zomato.csv'\nmain_df = load_csv(base_dir,file_name)","1f8f9273":"#Taking the important columns further for effective EDA\nanlys_df = main_df[['name', 'online_order', 'book_table', 'rate', 'votes',\n                    'location', 'rest_type', 'dish_liked', 'cuisines',\n                    'approx_cost(for two people)', 'reviews_list']]","5a5bc124":"#Doing some cleanUPs, general fillna() functions are not applicable here.\nanlys_df.loc[anlys_df.dish_liked.isna(),'dish_liked'] = 'None_Liked'\nanlys_df.loc[anlys_df.location.isna(),'location'] = 'not_provided'\nanlys_df.loc[anlys_df.rest_type.isna(),'rest_type'] = 'Unknown'\nanlys_df.loc[anlys_df.cuisines.isna(),'cuisines'] = 'Unknown'\n\n#I am assuming the value here, will be changed, or even can be used as prediction set\nanlys_df.loc[anlys_df['approx_cost(for two people)'].isna(),'approx_cost(for two people)'] = '0'\nanlys_df.loc[anlys_df.rate.isna(),'rate'] = '0\/5'\nanlys_df.loc[anlys_df.rate == 'NEW','rate'] = '0\/5'\nanlys_df.loc[anlys_df.rate == '-','rate'] = '0\/5'","b7531416":"#dish_liked column is string type with multiple items\nanlys_df['tot_dish_liked'] = anlys_df.dish_liked.apply(lambda x : len(x.split(',')))\nanlys_df['tot_cuisines'] = anlys_df.cuisines.apply(lambda x : len(x.split(',')))\n\n#There are multiple reviews in the list, so creating the seperate column\nanlys_df['tot_reviews'] = anlys_df.reviews_list.apply(lambda x: len(x))\n#Can think about running NLP sentiment analysis on there reviews later \n\n#Getting the review rate of each restaurants\nanlys_df['review_rate'] = anlys_df.rate.apply(lambda x: float(x.split('\/')[0]))\nanlys_df.drop('rate',axis=1,inplace=True)\n#converting the votes column to integer for easy manipulation\nanlys_df.votes = anlys_df.votes.astype(int)\n\n#calculating votes in favour\nanlys_df['temp'] = anlys_df.votes * anlys_df.review_rate\nanlys_df['in_fav'] = anlys_df.temp.apply(lambda x: round(x\/5,0))\nanlys_df.drop('temp',axis=1,inplace=True)\n\n#Converting approx cost to int\nanlys_df['approx_cost_per_pair'] = anlys_df['approx_cost(for two people)'].apply(lambda x: convert_cost(x))\nanlys_df.drop('approx_cost(for two people)',axis=1,inplace = True)","6ae1bbde":"#There are so many types of restaurants. To help visualisation creating this columns\nanlys_df['dine_type'] = anlys_df.rest_type.apply(lambda x: x.split(',')[0])\nanlys_df.loc[anlys_df.online_order == 'Yes','online_order'] = 'online'\nanlys_df.loc[anlys_df.online_order == 'No','online_order'] = 'offline'\nanlys_df.loc[anlys_df.book_table == 'Yes','book_table'] = 'booking_allowed'\nanlys_df.loc[anlys_df.book_table == 'No','book_table'] = 'no_booking'","caefbfce":"anlys_df.cuisines = anlys_df.cuisines.apply(lambda x: x.replace(' ','').split(','))\nanlys_df.dish_liked = anlys_df.dish_liked.apply(lambda x: x.replace(' ','').split(','))\n\n\n#convert the lists into seperate dataframe\ncusines = pd.DataFrame(anlys_df.cuisines.to_list(),columns=['C1','C2','C3','C4','C5','C6','C7','C8'])\ndishes = pd.DataFrame(anlys_df.dish_liked.to_list(),columns=['D1','D2','D3','D4','D5','D6','D7'])\n\n#merge the dataframes on to the main_df\nanlys_df = pd.merge(left=anlys_df,right=cusines,how='left',left_index=True,right_index=True)\nanlys_df = pd.merge(left=anlys_df,right=dishes,how='left',left_index=True,right_index=True)\n\n#Need a string values in place of 'None'\nanlys_df.fillna('NA',inplace=True)","3427d44b":"# Idea is to locate the unique dishes in all the restaurant cuisines, and dishes liked\ncuisine_set = set()\n\nfor cols in anlys_df.columns[16:23]:\n    #print(cols)\n    for cuisi in anlys_df[cols].apply(lambda x: x.replace(' ','')):\n        cuisine_set.add(cuisi)\n        \ndishes_set = set()\n\nfor cols in anlys_df.columns[24:]:\n    #print(cols)\n    for cuisi in anlys_df[cols].apply(lambda x: x.replace(' ','')):\n        dishes_set.add(cuisi)\n        \nprint('There are total {} unique cuisines sold in Zomato'.format(len(cuisine_set)))\nprint('There are total {} unique dishes sold in Zomato'.format(len(dishes_set)))","cc34a164":"#value_counts() gives pandas Series which can be concatenated to pandas dataframe directly.\n#market_basket algorithms can be used to find relationships between the dishes liked, and the cuisines served\n\n#cuisines Dataframe\ncusines_df = pd.DataFrame()\nfor cols in anlys_df.columns[16:24]:\n    cusines_df = pd.concat([cusines_df,anlys_df[cols].value_counts()],axis = 1,ignore_index=True)\n    \ncusines_df.columns =['First','second','third','fourth','fifth','sixth','seventh','eigth']\ncusines_df.fillna(0,inplace=True)\ncusines_df['total_served']= cusines_df.sum(axis=1)\n\n#dishes Dataframe\ndishes_df = pd.DataFrame()\nfor cols in anlys_df.columns[24:]:\n    dishes_df = pd.concat([dishes_df,anlys_df[cols].value_counts()],axis = 1,ignore_index=True)\n    \ndishes_df.columns =['First','second','third','fourth','fifth','sixth','seventh']\ndishes_df.fillna(0,inplace=True)\n\ndishes_df['total_liked']= dishes_df.sum(axis=1)\nprint(cusines_df.head(1))\nprint(dishes_df.head(1))","64141abe":"# Making the Rows to Columns\ncuis_transpose = cusines_df.T\ndish_transpose = dishes_df.T\n\n#making the cells with values greater than 1 as simply 1. The Market Basket algo requirement\nfor cols in cuis_transpose.columns: \n    cuis_transpose[cols] = cuis_transpose[cols].apply(lambda x : 1 if x>0 else 0)\n\nfor cols in dish_transpose.columns: \n    dish_transpose[cols] = dish_transpose[cols].apply(lambda x : 1 if x>0 else 0)","bee036fb":"#Collecting garbage memory\nimport gc\ngc.collect()","59d33cb9":"grp_14 = anlys_df.groupby(['location','tot_dish_liked','tot_cuisines','online_order', 'book_table', \n                           'tot_reviews', 'review_rate','approx_cost_per_pair','dine_type'])['name'].count().reset_index()\nvis_14 = px.treemap(data_frame=grp_14,\n                    path=['location','dine_type','tot_dish_liked','tot_cuisines'],\n                    names='approx_cost_per_pair',\n                    values = 'name',\n                    color='review_rate',\n                    title='Restaurants Distribution on various factors')\nvis_14.update_layout(height = 1000)\n#del grp_14. Not deleting since used in vis 17 and 18\nvis_14.show()","cc866f2a":"grp_1 = anlys_df.groupby(['book_table','online_order'])['name'].count().reset_index()\nvis_1 = px.bar(data_frame=grp_1,x='book_table',y='name',color='online_order')\ndel grp_1\nvis_1.show()","2e8a10e5":"grp_2 = anlys_df.groupby(['location','online_order'])['name'].count().reset_index()\nvis_2 = px.bar(data_frame=grp_2,y='location',x='name',color='online_order',orientation='h')\nvis_2.update_layout(yaxis={'categoryorder':'total ascending'},height = 1000)\ndel grp_2\nvis_2.show()","f57817be":"grp_3 = anlys_df.groupby(['location','book_table'])['name'].count().reset_index()\nvis_3 = px.bar(data_frame=grp_3,y='location',x='name',color='book_table',orientation='h')\nvis_3.update_layout(yaxis={'categoryorder':'total ascending'},height = 1000)\ndel grp_3\nvis_3.show()","9c14c04f":"grp_4 = anlys_df.groupby(['dine_type','location','book_table'])['name'].count().reset_index()\nvis_4 = px.bar(data_frame=grp_4,y='location',x='name',color='dine_type',title = 'Restaurant Types and Locales')\nvis_4.update_layout(yaxis={'categoryorder':'total ascending'},height = 1200)\ndel grp_4\nvis_4.show()","1d9a8bf7":"grp_5 = anlys_df.groupby('dine_type')['name'].count().reset_index()\nvis_5 = px.bar(data_frame=grp_5,y='dine_type',x='name',\n               title = 'Restaurant Types and Locales')\nvis_5.update_layout(yaxis={'categoryorder':'total ascending'},height = 1200)\ndel grp_5\nvis_5.show()","b6fe9801":"vis_6 = px.scatter(data_frame=anlys_df,x='review_rate',y='votes',animation_frame='dine_type',\n                   color='location',title='Review Rating and Votes')\nvis_6.update_layout(height = 1200)\nvis_6.show()","8e4a9fef":"grp_7 = anlys_df.groupby(['tot_dish_liked','dine_type'])['name'].count().reset_index()\nvis_7 = px.bar(data_frame=grp_7,x='tot_dish_liked',y='name',color='dine_type',\n               title='Where People liked more dishes')\nvis_7.update_layout(xaxis={'categoryorder':'total ascending'},height = 1200)\ndel grp_7\nvis_7.show()","c2c39ca1":"grp_8 = anlys_df.groupby(['tot_cuisines','dine_type'])['name'].count().reset_index()\ngrp_8.tot_cuisines = grp_8.tot_cuisines.astype('category')\nvis_8 = px.bar(data_frame=grp_8,y='dine_type',x='name',color='tot_cuisines',\n               title='Where variety of cuisine is more?',orientation='h')\nvis_8.update_layout(yaxis={'categoryorder':'total ascending'},height = 1200)\ndel grp_8\nvis_8.show()","abc6525b":"grp_9 = anlys_df.groupby(['approx_cost_per_pair','dine_type'])['name'].count().reset_index()\ngrp_9.approx_cost_per_pair = grp_9.approx_cost_per_pair.astype('category')\nvis_9 = px.bar(data_frame=grp_9,y='dine_type',x='name',color='approx_cost_per_pair',\n               title='Which type of restaurant is pocket friendly?',orientation='h')\nvis_9.update_layout(yaxis={'categoryorder':'total ascending'},height = 1200)\ndel grp_9\nvis_9.show()","fcc76e26":"grp_10 = anlys_df.groupby(['approx_cost_per_pair','location'])['name'].count().reset_index()\ngrp_10.approx_cost_per_pair = grp_10.approx_cost_per_pair.astype('category')\nvis_10 = px.bar(data_frame=grp_10,y='location',x='name',color='approx_cost_per_pair',\n               title='Where are pocket friendly restaurants?',orientation='h')\nvis_10.update_layout(yaxis={'categoryorder':'total ascending'},height = 1200)\ndel grp_10\nvis_10.show()","b773348e":"grp_11 = anlys_df.groupby(['approx_cost_per_pair','review_rate'])['name'].count().reset_index()\ngrp_11.approx_cost_per_pair = grp_11.approx_cost_per_pair.astype('category')\nvis_11 = px.bar(data_frame=grp_11,y='review_rate',x='name',color='approx_cost_per_pair',\n               title='Do costly restaurants have better rating?',orientation='h')\nvis_11.update_layout(yaxis={'categoryorder':'total ascending'},height = 800)\ndel grp_11\nvis_11.show()","7452b724":"grp_12 = anlys_df.groupby(['dine_type','review_rate'])['name'].count().reset_index()\ngrp_12.dine_type = grp_12.dine_type.astype('category')\nvis_12 = px.bar(data_frame=grp_12,y='review_rate',x='name',color='dine_type',\n               title='Which type of restaurant have better rating?',orientation='h')\nvis_12.update_layout(yaxis={'categoryorder':'total ascending'},height = 800)\ndel grp_12\nvis_12.show()","a7c6a9a1":"grp_13 = anlys_df.groupby(['location','review_rate'])['name'].count().reset_index()\nvis_13 = px.bar(data_frame=grp_13,y='location',x='name',color='review_rate',\n               title='Which location have better rating?',orientation='h')\nvis_13.update_layout(yaxis={'categoryorder':'total ascending'},height = 800)\ndel grp_13\nvis_13.show()","8a672eea":"# changing all the not applicable values to 0 \ncusines_df.loc[cusines_df.index == 'NA',:] = 0\ndishes_df.loc[dishes_df.index == 'NA',:] = 0","14e06205":"#considering only the top 25 dishes, and cuisine types to show the visualisation.\ncuis_cons = cusines_df[:25]\ndish_cons = dishes_df[1:25]","f955d8d8":"vis_15 = make_subplots(rows=8,cols=1)\n\nx = 1\nfor colum in cuis_cons.columns[:-1]:\n    vis_15.add_trace(go.Bar(orientation='h',x=cuis_cons[colum],y=cuis_cons.index,name=colum),row=x,col=1)\n    x = x + 1\n    vis_15.update_yaxes(categoryorder=\"total descending\")\n\nvis_15.update_layout(height = 1500,title='Top cuisines offered at Restuarants')\nvis_15.show()","53db94b0":"vis_16 = make_subplots(rows=7,cols=1)\n\nx = 1\nfor colum in dish_cons.columns[:-1]:\n    vis_16.add_trace(go.Bar(orientation='h',x=dish_cons[colum],y=dish_cons.index,name=colum),row=x,col=1)\n    x = x + 1\n    vis_16.update_yaxes(categoryorder=\"total descending\") #This option is great find\n\nvis_16.update_layout(height = 1500,title='Dishes liked the most by Bangaloreans')\nvis_16.show()","2479060d":"vis_17 = px.treemap(data_frame=grp_14,\n                    path=['location','book_table','online_order'],\n                    names='review_rate',\n                    values = 'name',\n                    color='approx_cost_per_pair',\n                    title='What it will cost you when ordering?')\nvis_17.update_layout(height = 1000)\nvis_17.show()","c4352f43":"vis_18 = px.treemap(data_frame=grp_14,\n                    path=['location','dine_type','book_table','online_order'],\n                    values = 'approx_cost_per_pair',\n                    color='dine_type',\n                    title='What will be cost in certain type of restaurant')\nvis_18.update_layout(height = 1000)\n#del grp_14. Not deleting since used in vis 17 and 18\nvis_18.show()","e9340bee":"import gc\ngc.collect()","c22d4579":"#NLP libraries\nfrom textblob import TextBlob\nimport spacy\nfrom tqdm import tqdm #library to show the progress bar\nimport re\nimport nltk\nimport warnings\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\nimport csv\n\nfrom nltk import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nfrom gensim.models import word2vec\n\nfrom sklearn.model_selection import train_test_split\n\n#Run the command python -m spacy download en_core_web_sm to download this\n#https:\/\/spacy.io\/models\nimport en_core_web_lg\nnlp = en_core_web_lg.load()\n\n#Libraries for processing the news headlines\nfrom lxml import etree\nimport json\nfrom io import StringIO\nfrom os import listdir\nfrom os.path import isfile, join\nfrom pandas.tseries.offsets import BDay\nfrom scipy.stats.mstats import winsorize\nfrom copy import copy\n\n# Libraries for Classification for modeling the sentiments\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Keras package for the deep learning model for the sentiment prediction. \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n\n# Load libraries\nimport seaborn as sns\nimport datetime\nfrom datetime import date\nimport matplotlib.pyplot as plt\n\n#Additional Libraries \nimport json  \nimport zipfile\nimport os.path\nimport sys","971d94a7":"#Creating subset of the anlys dataframe to start our NLP sentiment analysis(main_df has missing values!!!)\nnlp_df = anlys_df[['name','tot_reviews','reviews_list','review_rate','votes','in_fav']]\nnlp_df.info()","fd501095":"#Loading all the reviews in the model building is unnecessary at this moment\npart_df = anlys_df[:100]","36347da5":"#Getting the rating and the reviews stripped \npart_ratings = []\n\nfor name,ratings in tqdm(zip(part_df['name'],part_df['reviews_list'])):\n    ratings = eval(ratings)\n    for score, doc in ratings:\n        if score:\n            score = score.strip(\"Rated\").strip()\n            doc = doc.strip('RATED').strip()\n            score = float(score)\n            part_ratings.append([name,score, doc])","a5881364":"sample_rating_df = pd.DataFrame(part_ratings,columns=['name','rating','review'])\nsample_rating_df.head()","d50414ae":"#The nlp library that has been instantiated has many tokens of the english vocabs. Each token is reperesented\n#using 300 variables.\n#Below phrase converts the text in the reviews to the representation in the nlp library.\npart_vectors = pd.np.array([pd.np.array([token.vector for token in nlp(s)]).mean(axis=0)*pd.np.ones((300)) \\\n                           for s in sample_rating_df['review']])","c5b4f5ef":"#The vectors are created from the dictionary of already existing library, so the below sentence becomes a 300\n#element array which is in turn created by individual words that the sentence makes. This concept is later\n#useful in understanding other modeling methods.\nvec = [token.vector for token in nlp('Restaurant location was very calm and futuristic')]\nvec[0].shape\n\n#Each word creates 300 element array, and then sentence is converted to 300 element array.","1466f151":"#There will be total 300 columns of numbers, and one column of existing review ratings. Now the language\n#modeling problem is simply a machine learning problem that will be solved using the traditional \n\nprint('The shape of the part vector is {}'.format(part_vectors.shape))\nprint('The number of reviews that were collected from 1st 1000 restaurant is {}'.format(sample_rating_df.shape[0]))","a869ef87":"# split out validation dataset for the end\nY= sample_rating_df[\"review\"].values\nX = part_vectors\n\n#Check if there are infinites in the array\n#print('Are there infinite values: {}'.format(np.all(np.isfinite(X))))\n#Check if there are null values in the array. It seems there is\n#print('Are there Null values: {}'.format(np.any(np.isnan(X))))\nX = np.nan_to_num(X) #Runnning this function to replacing null values\nnp.any(np.isnan(X))\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nvalidation_size = 0.3\nseed = 7\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n# test options for classification\nnum_folds = 10\nseed = 7\nscoring = 'accuracy'\n\n# spot check the algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear')))#default solver doesnt work\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))\n#Neural Network\nmodels.append(('NN', MLPClassifier()))\n#Ensable Models \nmodels.append(('RF', RandomForestClassifier()))","b4eb4bbe":"results = []\nnames = []\nkfold_results = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    #msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    #print(msg)\n   # Full Training period\n    res = model.fit(X_train, Y_train)\n    train_result = accuracy_score(res.predict(X_train), Y_train)\n    train_results.append(train_result)\n    \n    # Test results\n    test_result = accuracy_score(res.predict(X_test), Y_test)\n    test_results.append(test_result)    \n    \n    msg = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), cv_results.std(), train_result, test_result)\n    print(msg)\n    print(confusion_matrix(res.predict(X_test), Y_test))\n    #print(classification_report(res.predict(X_test), Y_test))","f1cc46a0":"# compare algorithms\nfrom matplotlib import pyplot\nfig = pyplot.figure()\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.bar(ind - width\/2, train_results,  width=width, label='Train Error')\npyplot.bar(ind + width\/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(15,8)\npyplot.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\npyplot.show()","d5661336":"all_ratings = []\n\nfor name,ratings in tqdm(zip(anlys_df['name'],anlys_df['reviews_list'])):\n    ratings = eval(ratings)\n    for score, doc in ratings:\n        if score:\n            score = score.strip(\"Rated\").strip()\n            doc = doc.strip('RATED').strip()\n            score = float(score)\n            all_ratings.append([name,score, doc])","09cc6d33":"rating_df=pd.DataFrame(all_ratings,columns=['name','rating','review'])\nrating_df.to_csv('ratings.csv')\nrating_df.head()","cf76bbf8":"rest=anlys_df['name'].value_counts()[:9].index\ndef produce_wordcloud(rest):\n    \n    plt.figure(figsize=(20,30))\n    for i,r in enumerate(rest):\n        plt.subplot(3,3,i+1)\n        corpus=rating_df[rating_df['name']==r]['review'].values.tolist()\n        corpus=' '.join(x  for x in corpus)\n        wordcloud = WordCloud(max_font_size=None, background_color='white', collocations=False,\n                      width=800, height=800).generate(corpus)\n        plt.imshow(wordcloud)\n        plt.title(r)\n        plt.axis(\"off\")\n\n        \nproduce_wordcloud(rest)","c3e6b36b":"#Converting the ratings to either 0 or 1\nrating_df['sent']=rating_df['rating'].apply(lambda x: 1 if int(x)>2.5 else 0)","241f5fc1":"sample_rating_df['sentiment_textblob'] = [TextBlob(s).sentiment.polarity for s in sample_rating_df['review']] \nsample_rating_df.head(10)","a4196312":"vis_19 = go.Figure()\n\nvis_19.add_trace(go.Scatter(x=rating_df['rating'],y=rating_df['sentiment_textblob']))\nvis_19.update_xaxes(title='Actual Rating')\nvis_19.update_yaxes(title='Sentiment by TB')\nvis_19.update_layout(title='Comparing the actual rating with TB rating')\nvis_19.show()","c18a58e6":"stops=stopwords.words('english')\n\nlem=WordNetLemmatizer()\n#creating corpus for the positive sentiment reviews.\ncorpus_positive =' '.join(lem.lemmatize(x) for x in rating_df[rating_df['sent']==1]['review'][:3000] if x not in stops)\npositive_tokens=word_tokenize(corpus_positive)","30331f4a":"vect=TfidfVectorizer()\nvect_fit_pos=vect.fit(positive_tokens)","aeeb282b":"#Latend Drichlet Model\nid_map=dict((v,k) for k,v in vect.vocabulary_.items()) #Changes the items and keys\nvectorized_data=vect_fit_pos.transform(tokens)\ngensim_corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(gensim_corpus,id2word=id_map,num_topics=5,random_state=34,passes=25)","a897a502":"counter=Counter(corpus_positive)","1b3e4e45":"import matplotlib.colors as mcolors\n\nout=[]\ntopics=ldamodel.show_topics(formatted=False)\nfor i,topic in topics:\n    for word,weight in topic:\n        out.append([word,i,weight,counter[word]])\n\ndataframe = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(8,6), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.3, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=8)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(dataframe.loc[dataframe.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=8, y=1.05)    \nplt.show()","65476474":"stops=stopwords.words('english')\nlem=WordNetLemmatizer()\n#Building the model with negative reviews\n\ncorpus=' '.join(lem.lemmatize(x) for x in rating_df[rating_df['sent']==0]['review'][:3000] if x not in stops)\ntokens=word_tokenize(corpus)","31ad939c":"vect=TfidfVectorizer()\nvect_fit=vect.fit(tokens)\n\nid_map=dict((v,k) for k,v in vect.vocabulary_.items())\nvectorized_data=vect_fit.transform(tokens)\n\ngensim_corpus=gensim.matutils.Sparse2Corpus(vectorized_data,documents_columns=False)\nldamodel = gensim.models.ldamodel.LdaModel(gensim_corpus,id2word=id_map,num_topics=5,random_state=34,passes=25)\n","5b22c7a7":"counter=Counter(corpus)","a9b4e0a4":"out=[]\ntopics=ldamodel.show_topics(formatted=False)\nfor i,topic in topics:\n    for word,weight in topic:\n        out.append([word,i,weight,counter[word]])\n\ndataframe = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(8,6), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.3, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=dataframe.loc[dataframe.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    #ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=8)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(dataframe.loc[dataframe.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=8, y=1.05)    \nplt.show()","33555fb2":"stops=set(stopwords.words('english'))\nlem=WordNetLemmatizer()\ncorpus=[]\nfor review in tqdm(rating_df['review'][:10000]):\n    words=[]\n    for x in word_tokenize(review):\n        x=lem.lemmatize(x.lower())\n        if x not in stops:\n            words.append(x)\n            \n    corpus.append(words)","848896a1":"model = word2vec.Word2Vec(corpus, vector_size=100, window=20, min_count=200, workers=4)","89a173fc":"from sklearn.manifold import TSNE\ndef tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.key_to_index:\n        tokens.append(model.wv[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(10, 10)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","f4eb751b":"tsne_plot(model)","7cc1e9bd":"postive=rating_df[rating_df['rating']>3]['review'][:2000]\nnegative=rating_df[rating_df['rating']<2.5]['review'][:2000]\n\ndef return_corpus(df):\n    corpus=[]\n    for review in df:\n        tagged=nltk.pos_tag(word_tokenize(review))\n        adj=[]\n        for x in tagged:\n            if x[1]=='JJ':\n                adj.append(x[0])\n        corpus.append(adj)\n    return corpus","ee94bab5":"corpus=return_corpus(postive)\nmodel = word2vec.Word2Vec(corpus, vector_size=100, min_count=10,window=20, workers=4)\ntsne_plot(model)","060991b2":"corpus=return_corpus(negative)\nmodel = word2vec.Word2Vec(corpus, vector_size=100, min_count=10,window=20, workers=4)\ntsne_plot(model)","550f34c8":"rating_df['sent']=rating_df['rating'].apply(lambda x: 1 if int(x)>2.5 else 0)","5b4d429b":"max_features=3000\ntokenizer=Tokenizer(num_words=max_features,split=' ')\ntokenizer.fit_on_texts(rating_df['review'].values)\nX = tokenizer.texts_to_sequences(rating_df['review'].values)\nX = pad_sequences(X)","4b16950c":"embed_dim = 32\nlstm_out = 32\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n#model.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","e91d5778":"Y = pd.get_dummies(rating_df['sent'].astype(int)).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","9afbd33b":"batch_size = 3200\nmodel.fit(X_train, Y_train, epochs = 5, batch_size=batch_size)","ff7b6439":"validation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","450bdd0c":"Y_train","eb50a082":"Further analysis will be continued, as new questions arise. This dataset is treasure trove to be dug into more insights and machine learning experimentation.\n\n1) Review ratings predictions using the NLP sentiment analysis\n\n2) Classification of the restaurant type based on multiple factors, and providing the probability\n\n3) Choropleth and Scatter Geo plot rendering, by merging the location coordinates of the restaurants\n\n4) Association rules can be generated to show the \"Food you may like\" by using the food someone has liked. \n\n5) The population of the location can be predicted based on the number of restaurants\n\nWant to review any other chart, go to the contents using this [link](#go_up)","c9480d28":"[Go To Contents](#go_up)","c7f97397":"import pkg_resources\nimport pip\ninstalledPackages = {pkg.key for pkg in pkg_resources.working_set}\nrequired = {'nltk', 'spacy', 'textblob', 'backtrader'}\nmissing = required - installedPackages\nif missing:\n    !pip install nltk==3.4\n    !pip install textblob==0.15.3\n    !pip install -U SpaCy==2.2.0\n    !python -m spacy download en_core_web_lg\n    !pip install backtrader==1.9.74.123    ","9614362d":"### <a id='vis-3'> Visual_3 Location that are having Table Booking Restaurants<\/a>","3efbb346":"### <a id='vis-18'> Visual_18 What it will cost you in certain type of restaurant?<\/a>","a4193680":"### Purpose\n\nLet the data come alive with the efficient usage of the library. \n\nTo do EDA with simple transformations that can be implemented by others with minimal modifications.\n\nUse the Restaurant Reviews to understand the NLP modeling with multiple heuristics\n\n### What to Expect\n\nSlew of transformation, in order to clean and get the data ready for visualisations. Especially, the cuisines and dishes liked columns posed interesting [challenge](#chal-1). Attempting to change the data types and clean the dataset took some [imagination.](#chal-2) These challenges are hidden from the viewers looking for the visualisations, just unhide the code.\n\n### Sneek Peek\n\nThe [North Star](#vis-17) in the dark sky of restaurants jumps out. Plot shines with vibrancy once the [effort](#vis-14) to prepare the data is complete. The ease with which plots were [rendered](#vis-5) with the simple commands were possible after the transformations mentioned above. The visuals provide [interfaces](#vis-6) to check each category, and make it interactive. Take a look and see if there is story somewhere, that is missed....  ","d59f2cae":"### <a id='vis-4'> Visual_4 Restaurants Types and their locales <\/a>","d477d0e2":"### <a id='vis-13'> Visual_13 Which location has the best or worst rated hotels?<\/a>","f650b587":"[Go To Contents](#go_up)","aac1cab4":"### <a id='vis-9'> Visual_9 Which type of restaurant is pocket friendly<\/a>","672dba59":"### <a id='vis-15'> Visual_15 Cuisines types served in Bangalore Restaurants?<\/a>","339baef1":"[Go To Contents](#go_up)","c0d93ed1":"### <a id='vis-17'> Visual_17 What it will cost if you?<\/a>\n\nThis question was raised by my dad. Below treemap goes a step more and show which area has more costly restaurants, and highlights like a \"North Star\" in a dead of a ocean","63b10760":"[Go To Contents](#go_up)","1b0e5f8b":"### <a id='vis-12'> Visual_12 Which type of restaurants have best or worst rating?<\/a>","6e41d149":"[Go To Contents](#go_up)","5812fca3":"### <a id='startM'> Modeling Start with the 1st 1000 restaurant reviews <\/a>","ec6bd449":"### <a id='vis-11'> Visual_11 Do costly restaurants have best or worst rating?<\/a>","6320f5fb":"[Go To Contents](#go_up)","8b93280a":"### <a id='datpre'> Data preparation n Dataframe creation<\/a>\n\nThe process of rooting out the null-values, renaming the columns and creating new columns for better dataset rendering is carried out in next couple of cells, which has been hidden. Those curious to learn, here is somethings you will find.\n\n1) Creation of set to get the unique values of the cuisines and dishes\n\n2) Using merge and concat operations on the dataframes\n\n3) Creation of dataframes from the list of cusines and dishes, to find the frequency they are liked, or served. \n\n4) Data manipulation using the split and replace functions \n\nFeel free to explore. The pythonic grammar used is kept to those most of us are familiar with. \n","38f62932":"[Go To Contents](#go_up)","cec47437":"[Go To Contents](#go_up)","d31d1cb2":"### <a id='vis-1'> Visual_1 Restaurants with reservation table and online ordering<\/a>","155a1b51":"[Go To Contents](#go_up)","ddeda3a0":"### <a id='vis-14'> Visual_14 Restaurant distribution on multiple factor?<\/a>\n\nEvery dataset has mainly two types of data, continous and discrete data. The mixture of these two data happens vibrantly with Treemap chart. The contours of the colors created by the continous variable, and the neat demarcation of the discrete categorical variables can be mesmerising, and informative. The idea struck as I was nearing my EDA. Then I realized, multifactor EDA has not been done. \n\nThe underlying algorithm in treemap takes care of many details which would be a challenge for us to set. For example, in the below tree map the review rate is a continous variable, that is used for color. The location variable is discrete category. \n\nThe color of the entire location is averaged based on the underlying components. This is inbuit and not explicit either through surprise, or continous research and experimentation.\n","eff0c86e":"### Using existing library called Textblob","a934a822":"### <a id='vis-5'> Visual_5 Dine Types <\/a>","4b4de5c7":"[Go To Contents](#go_up)","1364b376":"### <a id='Run'> Executing the multiple models <\/a>","3d9c94e8":"#### <a id='chal-1'> Idea is to convert the list of cuisines and dish liked into seperate columns","4b3fe62f":"### <a id='vis-2'> Visual_2 Locations that are online_ordering friendly<\/a>","853b9961":"### <a id='vis-7'> Visual_7 Where people Liked more dishes Vs Ratings<\/a>","87e54a8b":"[Go To Contents](#go_up)","96d2bc65":"[Go To Contents](#go_up)","69e594bf":"[Go To Contents](#go_up)","930989f9":"[Go To Contents](#go_up)","74a26264":"[Go To Contents](#go_up)","d033f368":"<a id='go_up'>PS: Purpose, What to Expect and A sneek Peek are hidden in above cell, unhide to see the same.The blue colored words are links that take you to the relevant location or the chart in the notebook<\/a>\n\n## Contents:\n\n[Starting the datapreparation and Dataframe creations](#datpre)\n\n[Visual_14 Restaurant distribution on multiple factory?](#vis-14)\n\nThinking Why Visual 14 is on the top? The idea of this visual came late in the EDA. But this visual embodies the major part of the dataset.\n\n[Visual_1 Restaurants with reservation table and online ordering](#vis-1)\n\n[Visual_2 Locations that are online_ordering friendly](#vis-2)\n\n[Visual_3 Location that are having Table Booking Restaurants](#vis-3)\n\n[Visual_4 Restaurants Types and their locales](#vis-4)\n\n[Visual_5 Dine Types](#vis-5)\n\n[Visual_6 Review ratings and Votes](#vis-6)\n\n[Visual_7 Where people Liked more dishes Vs Ratings](#vis-7)\n\n[Visual_8 Which type of restaurant variety of food is served](#vis-8)\n\n[Visual_9 Which type of restaurant is pocket friendly](#vis-9)\n\n[Visual_10 Where are pocket friendly restaurants](#vis-10)\n\n[Visual_11 Do costly restaurants have best or worst rating?](#vis-11)\n\n[Visual_12 Which type of restaurants have best or worst rating?](#vis-12)\n\n[Visual_13 Which location has the best or worst rated hotels?](#vis-13)\n\nWe have explored the restaurants, location and its review ratings. The food is having its own dimension. The types of foods served, and the dishes liked is having some insights under its sleeve. \n\n[Visual_15 Which location has the best or worst rated hotels?](#vis-15)\n\n[Visual_16 Dishes liked the most by Bangaloreans?](#vis-16)\n\n[Visual_17 What it will cost if you?](#vis-17)\n\n[Visual_18 What it will cost in certain type of restaurant?](#vis-18)\n\n[NLP Sentiment Analyis](#NLP)\n","be5ded67":"### <a id='vis-6'> Visual_6 Review ratings and Votes <\/a>","696c3586":"[Go To Contents](#go_up)","c9234745":"#### <a id='chal-2'> Imagining the way to transform the data","8fb15ce1":"### <a id='vis-8'> Visual_8 Which type of restaurant variety of food is served<\/a>","ce166d80":"### <a id='vis-10'> Visual_10 Where are pocket friendly restaurants<\/a>","315065d1":"[Go To Contents](#go_up)","c366bb45":"[Go To Contents](#go_up)","7eff899e":"### <a id='NLP'> NLP Sentiment Analyis <\/a>","370aeb54":"[Go To Contents](#go_up)","b1a4994a":"#### [Sentimental Analysis]()<a id=\"sentimental\" ><\/a><br>","53a8343a":"### <a id='vis-16'> Visual_16 Dishes liked the most by Bangaloreans?<\/a>","13623dfc":"[Go To Contents](#go_up)"}}