{"cell_type":{"1b60197a":"code","98ac508d":"code","1c41bbb0":"code","e5ca607a":"code","d9e840ad":"code","cc3960a5":"code","af553af6":"code","fab5012c":"code","63994cf6":"code","d6cc2653":"code","015a9cab":"code","6a2941d7":"code","d923c147":"code","01e8282d":"code","6fbb4c3b":"code","8d72cc8e":"code","a8280479":"code","f4767e58":"code","a4b4fa9a":"code","1a68e86e":"code","be00f109":"markdown","0f2e4b1c":"markdown","c962358d":"markdown","b37e9e2f":"markdown","6f3a2ccd":"markdown","6af1fb95":"markdown","3298c833":"markdown","65304e22":"markdown","723d1d65":"markdown","23b86019":"markdown","24eafec2":"markdown","d91d8f74":"markdown","ddc4fd4c":"markdown","7de00824":"markdown","247bb21a":"markdown","c163287f":"markdown","439fb399":"markdown","8e0d237e":"markdown","1a4a1915":"markdown"},"source":{"1b60197a":"import pandas as pd \nimport numpy as np\nimport torch\nimport math\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\n","98ac508d":"df = pd.read_csv('\/kaggle\/input\/indian-liver-patient-records\/indian_liver_patient.csv')\nprint (df['Dataset'])\ndf.dropna(inplace=True)\n\ndf.Gender.replace({'Male': 1, 'Female': 2}, inplace=True)\ndf.Dataset.replace({2: 0}, inplace=True)","1c41bbb0":"# Create a minimum and maximum processor object\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n# Create an object to transform the data to fit minmax processor\ndf_scaled = min_max_scaler.fit_transform(df)\n\n# Run the normalizer on the dataframe\ndf_normalized = pd.DataFrame(df_scaled)","e5ca607a":"train, test = train_test_split(df_normalized, test_size=0.2, random_state=1)\n\nlabels = train[10]\ntrain = train.drop(10, axis=1)\n\nlabelsTest = test[10]\ntest = test.drop(10, axis=1)","d9e840ad":"x = torch.tensor(train.values.astype(np.float32))\ny = torch.tensor(labels.values.astype(np.float32))\n\nx_test = torch.tensor(test.values.astype(np.float32))\ny_test = torch.tensor(labelsTest.values.astype(np.float32))","cc3960a5":"torch.manual_seed(123)","af553af6":"from imblearn.under_sampling import NearMiss\nnr = NearMiss()\nx_near, y_near = nr.fit_resample(x, y)\n\nx_nm = torch.from_numpy(x_near)\ny_nm = torch.from_numpy(y_near)","fab5012c":"negative_imbalanced=0\npositive_imbalanced=0\nfor i in range(y_nm.shape[0]):\n    if y_nm[i] == 0:\n        negative_imbalanced += 1\n    if y_nm[i] == 1:\n        positive_imbalanced += 1\nprint ('Number of negative samples after balance: ', negative_imbalanced)\nprint ('Number of positive samples after balance: ', positive_imbalanced)","63994cf6":"model = torch.nn.Sequential(\n    torch.nn.Linear(10, 50, bias=True),\n    torch.nn.Dropout(0.4),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20, bias=True),\n    torch.nn.Dropout(0.3),\n    torch.nn.LeakyReLU(),\n    torch.nn.Linear(20, 1, bias=True),\n    torch.nn.Sigmoid()\n)","d6cc2653":"criterion = torch.nn.BCELoss()\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","015a9cab":"Losses = []\nepochs = 500\n\nfor epoch in range(epochs):\n    y_pred = model(x_nm)    \n    y_nm = torch.reshape(y_nm, (-1, 1))\n    loss = criterion(y_pred, y_nm)\n    #collecting the losses... \n    Losses.append(loss)\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","6a2941d7":"import matplotlib.pyplot as plt\nplt.plot(Losses)\nplt.title('Test loss trend')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()","d923c147":"# Get the model predictions for the testset:\ntest_pred = model(x_test)\n\n# Lets round the test prediction to yes\/no. (This means: 1 or 0).\ny_out = torch.round(test_pred)\n\n# mach the two vectors dimantion:\ny_test = torch.reshape(y_test, (-1, 1))\n\n# By the following vector we can extract F\/P, F\/N, T\/P, T\/N. \nF_positive, T_positive, F_negative, T_negative = 0, 0, 0, 0\nfor i in range(y_test.shape[0]):\n    if y_out[i] == 0:\n        if y_test[i] == 0:\n            T_negative += 1\n        else:\n            F_negative += 1\n    if y_out[i] == 1:\n        if y_test[i] == 1:\n            T_positive += 1\n        if y_test[i] == 0:\n            F_positive += 1\n\nrecall = T_positive \/ (F_negative + T_positive)\nprint('The percentage of recall is: ', int(100*recall))","01e8282d":"from sklearn.metrics import classification_report\n\nLabel = y_test.detach().numpy()\nPrediction = y_out.detach().numpy()\n\nprint('Classification report for NearMiss: \\n', classification_report(Label, Prediction))\n","6fbb4c3b":"epsilon = 10**-10\ndef my_loss(output, target, alpha):\n    loss = alpha * target * torch.abs(torch.log(output + epsilon)) + torch.abs((1 - target) * torch.log(1 - output + epsilon))\n    loss = torch.mean(loss)\n    return loss","8d72cc8e":"def run(x, y, x_test, y_test, alpha, epochs):\n    Losses = []\n    for epoch in range(epochs):\n        y_pred = model(x)    \n        y = torch.reshape(y, (-1, 1))\n        loss = my_loss(y_pred, y, alpha)\n        #collecting the losses... \n        Losses.append(loss)\n        # Zero gradients, perform a backward pass, and update the weights.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Get the model predictions for the testset:\n    test_pred = model(x_test)\n\n    # Lets round the test prediction to yes\/no. (This means: 1 or 0).\n    y_out = torch.round(test_pred)\n\n    # mach the two vectors dimantion:\n    y_test = torch.reshape(y_test, (-1, 1))\n\n    plt.plot(Losses)\n    plt.title('Test loss trend')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()\n\n    Label = y_test.detach().numpy()\n    Prediction = y_out.detach().numpy()\n    print('Classification report for alpha =', alpha, ': \\n', classification_report(Label, Prediction))","a8280479":"run(x_nm, y_nm, x_test, y_test, 0.5, 500)","f4767e58":"run(x_nm, y_nm, x_test, y_test, 30, 500)","a4b4fa9a":"run(x, y, x_test, y_test,1.4, 1000)","1a68e86e":"run(x, y, x_test, y_test, 1, 500)","be00f109":"### **SKlearn** is surely much more elegant **!!**","0f2e4b1c":"### Let's see the loss","c962358d":"## **NearMiss**\n### To deal with this small and imbalnced dataset, I will start with NearMiss.\n### By using the NearMiss method, we undersample the frequent class. In NearMiss method, the chosen samples have similar characteristics to those of the rare class. We assume that the clasifier will need to \"work\" harder to find good generalization princepeles, and it will be easier to clasify the droped samples.","b37e9e2f":"#### Let's set aside 20% of the data for **validation**.","6f3a2ccd":"#### Lets **normalize** the data: ","6af1fb95":"## Some data preparation... ","3298c833":"#### Shifting to pytorch","65304e22":"### The model: ","723d1d65":"### Adding a 4th layer to the model didn't give me a signicent improvement.\n### I also tryied to add epochs.\n#### (Pytorch-lightning has an early stopping, but it did not work properly at the time I made this kernal). ","23b86019":"### Let's verify that the classes are balanced","24eafec2":"### Alpha=0.5 compensating the imbalance of the data, and give greater precision. But the recall is poor.. ### Let's see the recall for higher alpha.","d91d8f74":"## I'm not sure why imbalanced loss function gave better results compare to NearMiss.  \n## Please let me know if you have some insights and\/or improvement sugestions. \n### *I assume it relates to the {small} size of this dataset.* \n### Thanks for reading,\n### Ziv","ddc4fd4c":"### The precision is better then other notbooks I saw here, but this result does not match **my main goal**: high recall for a Liver patients prediction. \n### To reach this goal, I will try to **manipulate the loss function** a little bit: ","7de00824":"### Because the dataset is so tiny, it suffers from the Bias\u2013variance tradeoff. \n### Therefore, it will be hard to do **hyperparameters tuning**. For this purpose I use seed:","247bb21a":"### let's see the recall","c163287f":"### Lets run!","439fb399":"### Redefine the model. \n##### (I define a function for quick reuse.)","8e0d237e":"### I will use conventional configuration (Binary Cross Entropy, Adam, ReLU)","1a4a1915":"#  Liver patients prediction\n### A prediction \/classification model should take into account at least three factors: \n### 1. Purpose 2. Dataset characteristics. 3. Computanional & run time resurces. \n\n#### Starting with point #1, the goal I aim at is to get the **highest recall for positive diagnosis**, for not missing any patient. Nevertheless, I will also try to get a good **precision for healthy people**. \n#### As for point #2, the Liver patients dataset is **small and imbalnced**.\n\n#### Sorry, I don't like oversampling, such as **smote** method. I suspect that the model will learn the smote pattern, and not the real generalization that I expect from a good ML algorithm.  \n#### Because it is a small Dataset, I tried **NearMiss** undersampling method.\n#### Additional improvments I got with 1. a **costum loss** function, 2. not using NearMiss and 3. increasing number of epochs. \n"}}