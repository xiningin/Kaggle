{"cell_type":{"0d8416f0":"code","ca031eed":"code","93456e41":"code","7a8fa5de":"code","f3bd646f":"code","ec578a1a":"code","746123fa":"code","e59fc2af":"code","931e4151":"code","35720196":"code","802d845c":"code","c2fa1859":"code","aca6125d":"code","ddbaf209":"code","9063e62e":"code","493e44a0":"code","b48f2eef":"code","bc08f1cb":"code","08fdcb35":"code","cf6dd4b0":"code","69be894e":"code","382688b8":"code","6a4e59b5":"code","9c7b2d4f":"code","d1c9ba28":"code","82d826ca":"code","7c1a3885":"code","6746facf":"code","10408ab8":"code","951d3c5a":"code","5ae91b15":"code","f7cc01c3":"code","71e20dbc":"code","1f61b428":"code","51fb77cd":"code","04f21ac6":"code","8e38aee3":"code","39a44db5":"code","0f5775c5":"code","439b2e05":"code","dec7e24c":"code","82881273":"code","83c7ea7c":"code","cb1073bf":"code","6a3ea932":"markdown","fc51de6c":"markdown","7d9d23f1":"markdown","1494dc21":"markdown","3d0c10bb":"markdown","7ea9250b":"markdown","e4f81a92":"markdown","99b80ae5":"markdown","65cea6cd":"markdown","16c3c135":"markdown","7a1c4b62":"markdown","10acf488":"markdown","37469188":"markdown","7a855dc3":"markdown","e4312d96":"markdown","600d7e67":"markdown","d9fb57b2":"markdown","cea0f13a":"markdown","9ffc8f97":"markdown","142bfb29":"markdown","fced7a11":"markdown","b775fddb":"markdown","2fc9a999":"markdown","71e3a83e":"markdown","773a010c":"markdown","a5b253ec":"markdown","ca2a6b7b":"markdown"},"source":{"0d8416f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca031eed":"#Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","93456e41":"# loading the dataset to a pandas DataFrame\n# Read .csv file into dataframe\ndata = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndata.head()","7a8fa5de":"#Shape of data \nprint(data.shape)\n#dtypes of data \nprint(data.dtypes)","f3bd646f":"# Info of data\ndata.info()","ec578a1a":"# value_counts\ndata[\"quality\"].value_counts()","746123fa":"# mean value ofred wine\ndata.groupby(\"quality\").mean()","e59fc2af":"# describe the data\ndata.describe()","931e4151":"# missing_values\ndata.isnull().sum()","35720196":"# number of values for each quality\nsns.catplot(x='quality', data = data, kind = 'count')","802d845c":"# volatile acidity vs Quality\nplot = plt.figure(figsize=(5,5))\nsns.barplot(x='quality', y = 'volatile acidity', data = data)","c2fa1859":"# citric acid vs Quality\nplot = plt.figure(figsize=(5,5))\nsns.barplot(x='quality', y = 'citric acid', data = data)","aca6125d":"correlation = data.corr()\n# constructing a heatmap to understand the correlation between the columns\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, cbar=True, square=True, fmt = '.1f', annot = True, annot_kws={'size':8}, cmap = 'Blues')","ddbaf209":"#label binarization\ntransform = data['quality'].apply(lambda y_value: 1 if y_value >= 7 else 0)\ntransform.head()","9063e62e":"# separating the data and label\nX = data.drop(['quality'], axis=1)\ny = transform\nprint(\"The shape of X is \" ,X.shape)\nprint(\"The shape of Y is \" ,y.shape)","493e44a0":"# Checking value counts again\ny.value_counts()","b48f2eef":"# train_test_spilt\nX_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y , test_size=0.2, random_state=42)\nprint(\"The shape of X_train is\", X_train.shape )\nprint(\"The shape of X_test is\", X_test.shape)\nprint(\"The shape of y_train is\", y_train.shape)\nprint(\"The shape of y_test is\", y_test.shape)","bc08f1cb":"# Checking again for value_counts\ny_train.value_counts()","08fdcb35":"# Checking again for value_counts\ny_test.value_counts()","cf6dd4b0":"from sklearn import svm\nclassifier_model = svm.SVC(kernel='linear')\nclassifier_model.fit(X_train,y_train)","69be894e":"from sklearn.linear_model import LogisticRegression\nlogistic_model = LogisticRegression(max_iter=700)\nlogistic_model.fit(X_train,y_train)","382688b8":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_X_train = sc_X.fit_transform(X_train)\nsc_X_test = sc_X.transform(X_test)","6a4e59b5":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree_model = DecisionTreeClassifier(random_state = 0)\ndecision_tree_model.fit(sc_X_train,y_train)","9c7b2d4f":"from sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'min_samples_split' : range(2, 10, 1),\n    'min_samples_leaf' : range(2, 10, 1)\n}\n\ngrid_search = GridSearchCV(decision_tree_model, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(sc_X_train, y_train)","d1c9ba28":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","82d826ca":"from sklearn.neighbors import KNeighborsClassifier\nk_model = KNeighborsClassifier(n_neighbors=35)\nkfitModel = k_model.fit(sc_X_train, y_train)\nprint(kfitModel)","7c1a3885":"# finding optimal values for k\nfrom sklearn.model_selection import cross_val_score\ncross_valid_scores = []\nfor k in range(1, 100):\n  knn = KNeighborsClassifier(n_neighbors = k)\n  scores = cross_val_score(knn,X, y, cv = 10, scoring = 'accuracy')\n  cross_valid_scores.append(scores.mean())    \n\nprint(\"Optimal k with cross-validation: \\t\",np.argmax(cross_valid_scores))","6746facf":"from sklearn.ensemble import RandomForestClassifier\nmodelRF = RandomForestClassifier()\nmodelRF.fit(sc_X_train,y_train)","10408ab8":"# accuracy score on training data\n\nX_train_prediction = classifier_model.predict(X_train)\ntraining_data_accuray = accuracy_score(X_train_prediction,y_train)\nprint('Accuracy of SVM model on training data : ', training_data_accuray)\n\n# accuracy score on testing data\n\nX_test_prediction = classifier_model.predict(X_test)\nsvm_test_data_accuray = accuracy_score(X_test_prediction,y_test)\nprint('Accuracy of SVM model on test data    : ', svm_test_data_accuray)","951d3c5a":"# accuracy score on training data\n\nX_train_prediction = logistic_model.predict(sc_X_train)\ntraining_data_accuray = accuracy_score(X_train_prediction,y_train)\nprint('Accuracy of LGR model on training data  : ', training_data_accuray)\n\n# accuracy score on testing data\nX_test_prediction = logistic_model.predict(sc_X_test)\nlgr_test_data_accuray = accuracy_score(X_test_prediction,y_test)\nprint('Accuracy of LGR model on test data      : ', lgr_test_data_accuray)","5ae91b15":"dtc = grid_search.best_estimator_\ny_pred = dtc.predict(sc_X_test)\ndtc_train_acc = accuracy_score(y_train, dtc.predict(sc_X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decesion Tree Model  is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decesion Tree Model      is {dtc_test_acc}\")","f7cc01c3":"from sklearn import tree\nplt.figure(figsize=(15,10))\ntree.plot_tree(dtc,filled=True)","71e20dbc":"# accuracy on test data\nX_test_prediction = modelRF.predict(X_test)\nkr_test_data_accuracy = accuracy_score(X_test_prediction, y_test)\nprint('Accuracy : ', kr_test_data_accuracy)","1f61b428":"kX_train_prediction = kfitModel.predict(sc_X_train)\ntraining_data_accuray = accuracy_score(kX_train_prediction,y_train)\nprint('Accuracy on training data  : ', training_data_accuray)\n\n# accuracy score on testing data\nkX_test_prediction = kfitModel.predict(sc_X_test)\nkx_lgr_test_data_accuray = accuracy_score(kX_test_prediction,y_test)\nprint('Accuracy on test data      : ', kx_lgr_test_data_accuray)","51fb77cd":"#Ada Boost Classifie\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nparameters = {\n    'n_estimators' : [50, 70, 90, 120, 180, 200],\n    'learning_rate' : [0.001, 0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, parameters, n_jobs = -1, cv = 5, verbose = 1)\ngrid_search.fit(X_train, y_train)","04f21ac6":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","8e38aee3":"ada = AdaBoostClassifier(base_estimator = dtc, algorithm = 'SAMME', learning_rate = 1, n_estimators = 50)\nada.fit(sc_X_train, y_train)\n\nada_train_acc = accuracy_score(y_train, ada.predict(sc_X_train))\nada_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Ada Boost Model is {ada_train_acc}\")\nprint(f\"Test Accuracy of Ada Boost Model is {ada_test_acc}\")","39a44db5":"# confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_test, y_pred)","0f5775c5":"# classification report\nprint(classification_report(y_test, y_pred))","439b2e05":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gblinear', learning_rate = 1, max_depth = 3, n_estimators = 10)\nxgb.fit(sc_X_train, y_train)\n\ny_pred = xgb.predict(sc_X_test)\n\nxgb_train_acc = accuracy_score(y_train, xgb.predict(sc_X_train))\nxgb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of XGB Model is {xgb_train_acc}\")\nprint(f\"Test Accuracy of XGB Model is {xgb_test_acc}\")","dec7e24c":"models = ['xg Boost','Ada Boost Classifier','Logistic Regression','KNN','SVC', 'Decision Tree', 'Random Forest']\nscores = [xgb_test_acc,ada_test_acc,lgr_test_data_accuray,kx_lgr_test_data_accuray, svm_test_data_accuray, dtc_test_acc, kr_test_data_accuracy]\nmodels = pd.DataFrame({'Model' : models, 'Score' : scores})\nmodels","82881273":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (18, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.show()","83c7ea7c":"input_data = (7.5,0.5,0.36,6.1,0.071,17.0,102.0,0.9978,3.35,0.8,10.5)\n\n# changing the input data to a numpy array\ninput_data_as_numpy_array = np.asarray(input_data)\n\n# reshape the data as we are predicting the label for only one instance\ninput_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n\nprediction = kfitModel.predict(input_data_reshaped)\nprint(prediction)\n\nif (prediction[0]==1):\n  print('Good Quality Wine')\nelse:\n  print('Bad Quality Wine')","cb1073bf":"input_data = (7.5,0.5,0.36,6.1,0.071,17.0,102.0,0.9978,3.35,0.8,10.5)\n\n# changing the input data to a numpy array\ninput_data_as_numpy_array = np.asarray(input_data)\n\n# reshape the data as we are predicting the label for only one instance\ninput_data_reshaped = input_data_as_numpy_array.reshape(1,-1)\n\nprediction = decision_tree_model.predict(input_data_reshaped)\nprint(prediction)\n\nif (prediction[0]==1):\n  print('Good Quality Wine')\nelse:\n  print('Bad Quality Wine')","6a3ea932":"# **Model Evaluation of DTR after hypertuning**","fc51de6c":"## **Logistic Regression Model**","7d9d23f1":"# **Model Training**\n\nWe will train different model after the evaluation of model we will select out best model for production.\n\n1.   SVM Model\n2.   Logistic Regression\n3.   Decision Tree\n4.   Random Forest Regressor\n5.   KNeighborsClassifier\n6.   AdaBoost Classifier\n7.   Xgb Boost Classifier","1494dc21":"# **Hyper Parameter Tuning For DTC**\n","3d0c10bb":"# **Getting Started**\n\nTitle : Loan Status Prediction\n\nLoan Status :\n\n0 -- > Low Quality Wine\n\n1 -- > Good Quality Wine ","7ea9250b":"# **Pridictive System for DTC.**","e4f81a92":"#### **After stratify we have almost equal number of y_train & y_test values.**","99b80ae5":"### **Model Evaluation of LGR**","65cea6cd":"# **Xg Boost**","16c3c135":"## **SVM model**","7a1c4b62":"## **Random Forest model**","10acf488":"# **Feature Scaling for Decision tree, Random Forest & boosting**","37469188":"# **Data Transformation**","7a855dc3":"# **Model Preparation**\n","e4312d96":"## **Exploratory data analysis**","600d7e67":"**Correlation**\n\n1.0   -->  Positive Correlation\n\n-0.0  --> Negative Correlation","d9fb57b2":"### **If you find this notebook usefull please upvote.** \u2764\ufe0f","cea0f13a":"# **Visualization for DTR trees**","9ffc8f97":"### ***Xg Boost Ada boost classifier have almost same value but DTC & KNN give us the best result we will use KNN for production. lets visualize best score more.***","142bfb29":"# **Model Evaluation of Random Forest**","fced7a11":"# **Boosting**","b775fddb":"# **Models Best Scores : -**","2fc9a999":"# **Model Evaluation of KNN**","71e3a83e":"# **KNeighborsClassifier**","773a010c":"# **Model Evaluation**\n### **Model Evaluation Of SVM**","a5b253ec":"# **Decision Tree**","ca2a6b7b":"# **Pridictive System for KNN.**"}}