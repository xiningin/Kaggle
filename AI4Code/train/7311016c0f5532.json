{"cell_type":{"c7642d55":"code","5e061572":"code","34773606":"code","3b9650cb":"code","41baef21":"code","13975259":"code","4c5b704f":"code","e79628a4":"code","41bd6ab4":"code","a1f2306f":"code","bec0d304":"code","988a16d5":"code","b896af0e":"code","23682fbb":"code","681a59bd":"code","d6395810":"code","2f8f401c":"code","9c0c318e":"code","65081222":"code","d8f065ba":"code","34687de6":"code","80f45889":"code","e3a08f87":"code","e6675209":"code","b1c69e07":"code","7e8fd8c1":"markdown","4f62432e":"markdown","3d47ff5c":"markdown","f02dcc36":"markdown"},"source":{"c7642d55":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sklearn.metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, log_loss, classification_report\n\n#import plotly.offline as py\n#py.init_notebook_mode(connected=True)\n#import plotly.graph_objs as go\n#import plotly.tools as tls\n\n# Import and suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5e061572":"data = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata.head()","34773606":"data.isnull().any()","3b9650cb":"numerical = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction',\n       u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction',\n       u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked',\n       u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction',\n       u'StockOptionLevel', u'TotalWorkingYears',\n       u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany',\n       u'YearsInCurrentRole', u'YearsSinceLastPromotion',\n       u'YearsWithCurrManager']\ndata1=data[numerical]\na4_dims = (11.7, 8.27)\nfig, ax = plt.subplots(figsize=a4_dims)\nsns.heatmap(data1.corr())","41baef21":"data.dtypes","13975259":"obj_data = data.select_dtypes(include=['object']).copy()\nobj_data.head()","4c5b704f":"\ncategorical = []\nfor col, value in data.iteritems():\n    if value.dtype == 'object':\n        categorical.append(col)\n\n# Store the numerical columns in a list numerical\nnumerical = data.columns.difference(categorical)","e79628a4":"attrition_cat = data[categorical]\nattrition_cat = attrition_cat.drop(['Attrition'], axis=1) # Dropping the target column","41bd6ab4":"attrition_cat = pd.get_dummies(attrition_cat)\nattrition_cat.head(3)","a1f2306f":"# Store the numerical features to a dataframe attrition_num\nattrition_num = data[numerical]","bec0d304":"data_final = pd.concat([attrition_num, attrition_cat], axis=1)","988a16d5":"target_map = {'Yes':1, 'No':0}\n# Use the pandas apply method to numerically encode our attrition target variable\ntarget = data[\"Attrition\"].apply(lambda x: target_map[x])\ntarget.head(3)","b896af0e":"train, test, target_train, target_val = train_test_split(data_final, target, train_size= 0.75,random_state=0);","23682fbb":"from sklearn import tree\n\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 5)\ndecision_tree.fit(train, target_train)\n\n# Predicting results for test dataset\ny_pred1 = decision_tree.predict(test)\n\n\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(decision_tree, out_file=dot_data,feature_names=list(train),class_names=['0','1'],\n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","681a59bd":"accuracy_tree = sklearn.metrics.accuracy_score(target_val, y_pred1)\nprint(accuracy_tree)","d6395810":"sklearn.metrics.confusion_matrix(target_val, y_pred1,sample_weight=None)\nreport = classification_report(target_val, y_pred1)\nprint(report)","2f8f401c":"kfold = model_selection.KFold(n_splits=10, random_state=None)\ncv_results = model_selection.cross_val_score(decision_tree, train, target_train, cv=kfold, scoring='accuracy')\ncv_results.mean()","9c0c318e":"from sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn import feature_selection\nfrom sklearn import preprocessing\n\nfrom sklearn.linear_model import LogisticRegression\n","65081222":"model = LogisticRegression()\n\n# Fit Recursive Feature Extraction with our model to the data\n# This intuitively tries combinations of 26 features that maximizes our accuracy\nselector = feature_selection.RFE(model, 26, step=1) # I did an exhaustive search of all the number of features to try. 26 features is just the best\nselector = selector.fit(train, target_train)\n\n\n\ny_pred2 = selector.predict(test)\n\n# Get Accuracy\naccuracy_logistic = metrics.accuracy_score(target_val, y_pred2)\nprint(accuracy_logistic)","d8f065ba":"sklearn.metrics.confusion_matrix(target_val, y_pred2,sample_weight=None)\nreport = classification_report(target_val, y_pred2)\nprint(report)","34687de6":"kfold = model_selection.KFold(n_splits=10, random_state=None)\ncv_results = model_selection.cross_val_score(selector, train, target_train, cv=kfold, scoring='accuracy')\ncv_results.mean()","80f45889":"from sklearn import linear_model\n\n# Create linear regression object\nlinear = linear_model.LinearRegression()\n# Train the model using the training sets and check score\nlinear.fit(train, target_train)\nlinear.score(train, target_train)\n#Equation coefficient and Intercept\nprint('Coefficient: \\n', linear.coef_)\nprint('Intercept: \\n', linear.intercept_)\n#Predict Output\ny_pred3= linear.predict(test)","e3a08f87":"y_pred3_new=[]\nfor i in y_pred3:\n    if i<0.7:\n        i=0\n        y_pred3_new.append(i)\n        \n    else:\n        i=1\n        y_pred3_new.append(i)","e6675209":"\naccuracy_linear = metrics.accuracy_score(target_val, y_pred3_new)\nprint(accuracy_linear)","b1c69e07":"sklearn.metrics.confusion_matrix(target_val, y_pred3_new,sample_weight=None)\nreport = classification_report(target_val, y_pred2)\nprint(report)","7e8fd8c1":"**As we can see Logistic Regression performs the best in the above case, with the highest accuracy. Let me know if yu have any suggestions or ideas. This is my first submission on Kaggle.**","4f62432e":"## Linear Regression","3d47ff5c":"# Logistic Regression","f02dcc36":"# Decision tree"}}