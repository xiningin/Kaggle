{"cell_type":{"bf1295fa":"code","bcd9b421":"code","dd76b571":"code","c9393421":"code","a15013c8":"code","f3ba218e":"code","fc63e7fb":"code","74747878":"code","b153ba09":"code","1391b521":"code","916857e3":"code","3c9dd5c4":"code","46f4b3d9":"code","bb0363e1":"code","6a35bdc1":"code","1f3ed81c":"code","34d1a35a":"code","405a5246":"code","de2552b5":"code","1b229320":"code","22d90d05":"code","ff43fa36":"code","156f09c8":"code","17ee2fc3":"code","cf90150b":"code","4c4affa5":"code","3fcc5e60":"code","8201090c":"code","18994019":"code","bae0d884":"code","f0bca3ac":"code","e0bf9c31":"code","709e0272":"code","3d13ae5c":"code","cc2f905d":"code","84ad861a":"code","f099cd3b":"code","bb7d4c16":"code","3b616ae1":"code","1be5ac1e":"code","09475fdf":"code","07ee426b":"code","d51a3a7a":"code","dc677a41":"code","d5b0c4c2":"code","1699726a":"code","a092403c":"code","c05f7fa0":"code","c0b89df8":"code","93125be6":"code","707d949c":"code","f3bd2275":"code","d3249c73":"code","47aa47fc":"code","8c3a20c0":"code","ddd8849c":"code","0d185a1b":"code","f1bd2113":"markdown","d8ba2420":"markdown","4d12e84a":"markdown"},"source":{"bf1295fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bcd9b421":"import pandas as pd\nimport numpy as np\nimport string\nfrom string import digits\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport re\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.layers import Input, LSTM, Embedding, Dense, Bidirectional, Concatenate, Dot, Activation, TimeDistributed\nfrom keras.models import Model\nfrom keras.utils import plot_model","dd76b571":"\nbatch_size=64\nepochs=100\nlatent_dim=256\nnum_samples=10000\n\ndata_path='..\/input\/fra-eng\/fra.txt'","c9393421":"with open(data_path, 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\n","a15013c8":"# Vectorize the data.\ninput_texts = []\ntarget_bef = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\nwith open(data_path, 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text = line.split('\\t')\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_bef.append(target_text)\n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)","f3ba218e":"target_texts","fc63e7fb":"#char level\nline = pd.DataFrame({'input':input_texts, 'target':target_texts})","74747878":"line","b153ba09":"lines = pd.DataFrame({'input':input_texts, 'target':target_bef})","1391b521":"lines","916857e3":"def cleanup(lines):\n    \n      # Since we work on word level, if we normalize the text to lower case, this will reduce the vocabulary. It's easy to recover the case later. \n    lines.input=lines.input.apply(lambda x: x.lower())\n    lines.target=lines.target.apply(lambda x: x.lower())\n\n    # To help the model capture the word separations, mark the comma with special token:\n    lines.input=lines.input.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n    lines.target=lines.target.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n\n    # Clean up punctuations and digits. Such special chars are common to both domains, and can just be copied with no error.\n    exclude = set(string.punctuation)\n    lines.input=lines.input.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n    lines.target=lines.target.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n\n    remove_digits = str.maketrans('', '', digits)\n    lines.input=lines.input.apply(lambda x: x.translate(remove_digits))\n    lines.target=lines.target.apply(lambda x: x.translate(remove_digits))\n\n      #return lines\n","3c9dd5c4":"st_tok = 'START_'\nend_tok = '_END'\ndef data_prep(lines):\n    cleanup(lines)\n    lines.target = lines.target.apply(lambda x : st_tok + ' ' + x + ' ' + end_tok)","46f4b3d9":"data_prep(lines)","bb0363e1":"lines.head()","6a35bdc1":"#word_leve","1f3ed81c":"def tok_split_word2word(data):\n    return data.split()","34d1a35a":"tok_split_fn = tok_split_word2word","405a5246":"def data_stats(lines, input_tok_split_fn, target_tok_split_fn):\n    input_tokens=set()\n    for line in lines.input:\n        for tok in input_tok_split_fn(line):\n            if tok not in input_tokens:\n                input_tokens.add(tok)\n      \n    target_tokens=set()\n    for line in lines.target:\n        for tok in target_tok_split_fn(line):\n            if tok not in target_tokens:\n                target_tokens.add(tok)\n    input_tokens = sorted(list(input_tokens))\n    target_tokens = sorted(list(target_tokens))\n\n    num_encoder_tokens = len(input_tokens)\n    num_decoder_tokens = len(target_tokens)\n    max_encoder_seq_length = np.max([len(input_tok_split_fn(l)) for l in lines.input])\n    max_decoder_seq_length = np.max([len(target_tok_split_fn(l)) for l in lines.target])\n\n    return input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n\n","de2552b5":"input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length = data_stats(lines, input_tok_split_fn=tok_split_fn, target_tok_split_fn=tok_split_fn)\nprint('Number of samples:', len(lines))\nprint('Number of unique input tokens:', num_encoder_tokens)\nprint('Number of unique output tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)","1b229320":"input_tokens","22d90d05":"target_tokens","ff43fa36":"pad_tok = 'PAD'\nsep_tok = ' '\nspecial_tokens = [pad_tok, sep_tok, st_tok, end_tok] \nnum_encoder_tokens += len(special_tokens)\nnum_decoder_tokens += len(special_tokens)","156f09c8":"#adding the nwew value of thr\ndef vocab(input_tokens, target_tokens):\n    \n    input_token_index = {}\n    target_token_index = {}\n    for i,tok in enumerate(special_tokens):\n\n        input_token_index[tok] = i\n        target_token_index[tok] = i \n\n    offset = len(special_tokens)\n    for i, tok in enumerate(input_tokens):\n        input_token_index[tok] = i+offset\n\n    for i, tok in enumerate(target_tokens):\n        target_token_index[tok] = i+offset\n    # Reverse-lookup token index to decode sequences back to something readable.\n    reverse_input_tok_index = dict(\n        (i, tok) for tok, i in input_token_index.items())\n    reverse_target_tok_index = dict(\n        (i, tok) for tok, i in target_token_index.items())\n    return input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index","17ee2fc3":"input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index = vocab(input_tokens, target_tokens)","cf90150b":"max_encoder_seq_length = 16\nmax_decoder_seq_length = 16","4c4affa5":"def init_model_inputs(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n    encoder_input_data = np.zeros(\n        (len(lines.input), max_encoder_seq_length),\n        dtype='float32')\n    decoder_input_data = np.zeros(\n        (len(lines.target), max_decoder_seq_length),\n        dtype='float32')\n    decoder_target_data = np.zeros(\n        (len(lines.target), max_decoder_seq_length, num_decoder_tokens),\n        dtype='float32')\n  \n    return encoder_input_data, decoder_input_data, decoder_target_data","3fcc5e60":"def vectorize(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens, input_tok_split_fn, target_tok_split_fn):\n    encoder_input_data, decoder_input_data, decoder_target_data = init_model_inputs(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)\n    for i, (input_text, target_text) in enumerate(zip(lines.input, lines.target)):\n        for t, tok in enumerate(input_tok_split_fn(input_text)):\n            encoder_input_data[i, t] = input_token_index[tok]\n        encoder_input_data[i, t+1:] = input_token_index[pad_tok]\n        for t, tok in enumerate(target_tok_split_fn(target_text)):\n            # decoder_target_data is ahead of decoder_input_data by one timestep\n            decoder_input_data[i, t] = target_token_index[tok]         \n            if t > 0:\n                # decoder_target_data will be ahead by one timestep\n                # and will not include the start character.\n                decoder_target_data[i, t - 1, target_token_index[tok]] = 1.\n        decoder_input_data[i, t+1:] = target_token_index[pad_tok] \n        decoder_target_data[i, t:, target_token_index[pad_tok]] = 1.          \n              \n    return encoder_input_data, decoder_input_data, decoder_target_data              ","8201090c":"encoder_input_data, decoder_input_data, decoder_target_data  = vectorize(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens, input_tok_split_fn=tok_split_fn, target_tok_split_fn=tok_split_fn)","18994019":"def seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, lstm_sz):\n    encoder_inputs = Input(shape=(None,))\n    en_x=  Embedding(num_encoder_tokens, emb_sz)(encoder_inputs)\n    encoder = LSTM(lstm_sz, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(en_x)\n    # We discard `encoder_outputs` and only keep the states.\n    encoder_states = [state_h, state_c]\n  \n    # Encoder model\n    encoder_model = Model(encoder_inputs, encoder_states)\n  \n   \n    # Set up the decoder, using `encoder_states` as initial state.\n    decoder_inputs = Input(shape=(None,))\n \n    dex=  Embedding(num_decoder_tokens, emb_sz)\n\n    final_dex= dex(decoder_inputs)\n  \n  \n    decoder_lstm = LSTM(lstm_sz, return_sequences=True, return_state=True)\n\n    decoder_outputs, _, _ = decoder_lstm(final_dex,\n                                      initial_state=encoder_states)\n  \n    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n\n    decoder_outputs = decoder_dense(decoder_outputs)\n\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n\n\n  \n    # Decoder model: Re-build based on explicit state inputs. Needed for step-by-step inference:\n    decoder_state_input_h = Input(shape=(lstm_sz,))\n    decoder_state_input_c = Input(shape=(lstm_sz,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n    decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex, initial_state=decoder_states_inputs)\n    decoder_states2 = [state_h2, state_c2]\n    decoder_outputs2 = decoder_dense(decoder_outputs2)\n    decoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + decoder_states2)  \n\n    return model, encoder_model, decoder_model\n","bae0d884":"emb_sz=50","f0bca3ac":"model, encoder_model, decoder_model = seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, emb_sz)\nprint(model.summary())\nplot_model(model, show_shapes=True, show_layer_names=True)","e0bf9c31":"model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=64,\n          epochs=30,\n          validation_split=0.05)","709e0272":"def seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, lstm_sz):\n  encoder_inputs = Input(shape=(None,))\n  en_x=  Embedding(num_encoder_tokens, emb_sz, mask_zero=True)(encoder_inputs)\n  encoder = LSTM(lstm_sz, return_state=True)\n  encoder_outputs, state_h, state_c = encoder(en_x)\n  # We discard `encoder_outputs` and only keep the states.\n  encoder_states = [state_h, state_c]\n  \n  # Encoder model\n  encoder_model = Model(encoder_inputs, encoder_states)\n  \n  \n  # Set up the decoder, using `encoder_states` as initial state.\n  decoder_inputs = Input(shape=(None,))\n\n  dex=  Embedding(num_decoder_tokens, emb_sz, mask_zero=True)\n\n  final_dex= dex(decoder_inputs)\n\n\n  decoder_lstm = LSTM(lstm_sz, return_sequences=True, return_state=True)\n\n  decoder_outputs, _, _ = decoder_lstm(final_dex,\n                                      initial_state=encoder_states)\n\n  decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n\n  decoder_outputs = decoder_dense(decoder_outputs)\n\n  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n  model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n\n\n  \n  # Decoder model: Re-build based on explicit state inputs. Needed for step-by-step inference:\n  decoder_state_input_h = Input(shape=(lstm_sz,))\n  decoder_state_input_c = Input(shape=(lstm_sz,))\n  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n  decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex, initial_state=decoder_states_inputs)\n  decoder_states2 = [state_h2, state_c2]\n  decoder_outputs2 = decoder_dense(decoder_outputs2)\n  decoder_model = Model(\n  [decoder_inputs] + decoder_states_inputs,\n  [decoder_outputs2] + decoder_states2)  \n\n  return model, encoder_model, decoder_model\nemb_sz = 256\nmodel, encoder_model, decoder_model = seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, emb_sz)\nprint(model.summary())\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=64,\n          epochs=10,\n          validation_split=0.2)","3d13ae5c":"#using the last decoder which has been traind on the GT ","cc2f905d":"def decode_sequence(input_seq, sep=' '):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = target_token_index[st_tok]\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_tok = reverse_target_tok_index[sampled_token_index]\n        decoded_sentence += sep + sampled_tok\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_tok == end_tok or\n           len(decoded_sentence) > 52):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence","84ad861a":"for seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', lines.input[seq_index: seq_index + 1])\n    print('Decoded sentence:', decoded_sentence)","f099cd3b":"def seq2seq_attention(num_encoder_tokens, num_decoder_tokens, emb_sz, latent_dim):\n    # Define an input sequence and process it.\n    encoder_inputs = Input(shape=(None,), dtype='float32')\n    encoder_inputs_ = Embedding(num_encoder_tokens, emb_sz, mask_zero=True)(encoder_inputs)    \n    \n    encoder = Bidirectional(LSTM(latent_dim, return_state=True, return_sequences=True)) # Bi LSTM\n    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(encoder_inputs_)# Bi LSTM\n    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n\n    # We discard `encoder_outputs` and only keep the states.\n    encoder_states = [state_h, state_c]# Bi GRU, LSTM, BHi LSTM\n    print(encoder_states)\n    \n    decoder_inputs = Input(shape=(None,))\n    decoder_inputs_ = Embedding(num_decoder_tokens, emb_sz, mask_zero=True)(decoder_inputs)    \n    # We set up our decoder to return full output sequences,\n    # and to return internal states as well. We don't use the\n    # return states in the training model, but we will use them in inference.\n    decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)# Bi LSTM\n    \n    decoder_outputs, _, _ = decoder_lstm(decoder_inputs_, initial_state=encoder_states)\n\n    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n    print(decoder_outputs)\n    print(encoder_outputs)\n    att_dot = Dot(axes=[2, 2])\n    attention = att_dot([decoder_outputs, encoder_outputs])\n    att_activation = Activation('softmax', name='attention')\n    attention = att_activation(attention)\n    print('attention', attention)\n    context_dot = Dot(axes=[2,1])\n    context = context_dot([attention, encoder_outputs])\n    att_context_concat = Concatenate()\n    decoder_combined_context = att_context_concat([context, decoder_outputs])\n\n    # Has another weight + tanh layer as described in equation (5) of the paper\n\n    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n    #decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_outputs = decoder_dense(decoder_combined_context)\n\n    # Define the model that will turn\n    # `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n\n    print('encoder-decoder  model:')\n    print(model.summary()) \n    \n    print(encoder_inputs)\n    print(encoder_outputs)\n    print(encoder_states)\n    encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs] + encoder_states)\n\n    decoder_encoder_inputs = Input(shape=(None, latent_dim*2,))\n    decoder_state_input_h = Input(shape=(latent_dim*2,))# Bi LSTM\n    decoder_state_input_c = Input(shape=(latent_dim*2,)) # Bi LSTM\n    \n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_, initial_state=decoder_states_inputs)\n\n    \n    decoder_states = [state_h, state_c]\n    \n    # Equation (7) with 'dot' score from Section 3.1 in the paper.\n    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n    \n    attention = att_dot([decoder_outputs, decoder_encoder_inputs])\n    \n    attention = att_activation(attention)\n    \n    context = context_dot([attention, decoder_encoder_inputs])\n    \n    \n    \n    decoder_combined_context = att_context_concat([context, decoder_outputs])\n    \n    # Has another weight + tanh layer as described in equation (5) of the paper\n    \n    decoder_outputs = decoder_dense(decoder_combined_context)\n    \n    decoder_model = Model(\n        [decoder_inputs, decoder_encoder_inputs] + decoder_states_inputs,\n        [decoder_outputs, attention] + decoder_states)\n    \n    return model, encoder_model, decoder_model","bb7d4c16":"model, encoder_model, decoder_model = seq2seq_attention(num_encoder_tokens, num_decoder_tokens, emb_sz=emb_sz, latent_dim=emb_sz)\nprint(model.summary())\nplot_model(model, show_shapes=True, show_layer_names=True)","3b616ae1":"\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=128,\n          epochs=20,\n          validation_split=0.05)","1be5ac1e":"def decode_sequence_attention(input_seq, sep=' '):\n    # Encode the input as state vectors.\n    encoder_outputs, h, c = encoder_model.predict(input_seq)\n    states_value = [h,c]\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = target_token_index[st_tok]\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    attention_density = []\n    while not stop_condition:\n        output_tokens, attention, h, c  = decoder_model.predict(\n            [target_seq, encoder_outputs] + states_value)\n        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_tok = reverse_target_tok_index[sampled_token_index]\n        decoded_sentence += sep + sampled_tok\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_tok == end_tok or\n           len(decoded_sentence) > 52):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n    attention_density = np.array(attention_density)\n    return decoded_sentence, attention_density","09475fdf":"word_decoded_sents = []\nfor seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence, attention = decode_sequence_attention(input_seq)\n    print('-')\n    print('Input sentence:', lines.input[seq_index: seq_index + 1])\n    print('Decoded sentence:', decoded_sentence)\n    word_decoded_sents.append(decoded_sentence)","07ee426b":"lines = pd.DataFrame({'input':input_texts, 'target':target_bef})\nnum_samples = 10000\nlines = lines[:num_samples]","d51a3a7a":"st_tok = '\\t'\nend_tok = '\\n'\ndef data_prep(lines):\n  #cleanup(lines)\n  lines.target = lines.target.apply(lambda x : st_tok  + x  + end_tok)\n  ","dc677a41":"data_prep(lines)\n","d5b0c4c2":"lines","1699726a":"#using chracter level\nemb_sz = 256","a092403c":"def tok_split_char2char(data):\n    return data\n  \ntok_split_fn = tok_split_char2char","c05f7fa0":"input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length = data_stats(lines, input_tok_split_fn=tok_split_fn, target_tok_split_fn=tok_split_fn)\nprint('Number of samples:', len(lines))\nprint('Number of unique input tokens:', num_encoder_tokens)\nprint('Number of unique output tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)","c0b89df8":"pad_tok = 'PAD'\nsep_tok = ' '\nspecial_tokens = [pad_tok, sep_tok, st_tok, end_tok] \nnum_encoder_tokens += len(special_tokens)\nnum_decoder_tokens += len(special_tokens)\n","93125be6":"input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index = vocab(input_tokens, target_tokens)","707d949c":"encoder_input_data, decoder_input_data, decoder_target_data  = vectorize(lines, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens, input_tok_split_fn=tok_split_fn, target_tok_split_fn=tok_split_fn)","f3bd2275":"emb_sz = 256\nmodel, encoder_model, decoder_model = seq2seq(num_decoder_tokens, num_encoder_tokens, emb_sz, emb_sz)\nprint(model.summary())\nplot_model(model, show_shapes=True, show_layer_names=True)","d3249c73":"model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=64,\n          epochs=10,\n          validation_split=0.2)","47aa47fc":"for seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq, sep='')\n    print('-')\n    print('Input sentence:', lines.input[seq_index: seq_index + 1])\n    print('Decoded sentence:', decoded_sentence)","8c3a20c0":"model, encoder_model, decoder_model = seq2seq_attention(num_encoder_tokens, num_decoder_tokens, emb_sz=emb_sz, latent_dim=emb_sz)\nprint(model.summary())\nplot_model(model)","ddd8849c":"model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=128,\n          epochs=20,\n          validation_split=0.05)","0d185a1b":"char_decoded_sents = []\ntarget_sents = []\nfor seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence, attention = decode_sequence_attention(input_seq, sep='')\n    print('-')\n    print('Input sentence:', lines.input[seq_index: seq_index + 1])\n    print('GT sentence:', lines.target[seq_index: seq_index + 1][1:-1])\n    print('Decoded sentence:', decoded_sentence)\n    char_decoded_sents.append(decoded_sentence)\n    target_sents.append(np.array(lines.target[seq_index: seq_index + 1]))","f1bd2113":"#infrence (decoding and generating seq)","d8ba2420":"**seq2seq by using atteention**","4d12e84a":"adding the length of tokens to the input and output token(encoder and decoder) "}}