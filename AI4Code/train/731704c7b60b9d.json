{"cell_type":{"36983209":"code","4f55bc07":"code","d82e03b1":"code","c71defdd":"code","dd9ea40e":"code","4745736a":"code","4795c524":"code","40773957":"code","6fa62420":"code","00099963":"code","92c2f43d":"code","9304279d":"code","e5c32776":"code","84a4f4b3":"code","5ca5208a":"code","9c0ea611":"code","6e941650":"code","80200898":"code","bee9519a":"code","78c3f3a2":"code","ede44472":"code","a71175e9":"code","da031de1":"code","e1cb5ba0":"code","e5604df8":"code","66d926b2":"code","340d77a2":"code","8202f607":"code","8e63bda2":"code","afcc2d57":"code","2dcb63e0":"markdown","5e8f1a1d":"markdown","32c92bed":"markdown","3cb4ec0c":"markdown","25ed5bab":"markdown","5a65e10c":"markdown","672707b9":"markdown","d7ad54c4":"markdown","78f2a861":"markdown","dba53913":"markdown","cb2a340a":"markdown","bf070a7e":"markdown","1095a81c":"markdown","0f6a0d51":"markdown","18f621aa":"markdown","440973b1":"markdown","3aed30ad":"markdown","35024742":"markdown"},"source":{"36983209":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4f55bc07":"data = pd.read_csv('..\/input\/data.csv')","d82e03b1":"# information of our data (lenght, how many NaN objects we have etc.)\ndata.info()","c71defdd":"# to see features and target variable of the first 5 rows\ndata.head()","dd9ea40e":"# to see features and target variable of the last 5 rows\ndata.tail()","4745736a":"data = data.drop(['Unnamed: 32', 'id'], axis=1)\ndata.head()","4795c524":"data['diagnosis'].value_counts()","40773957":"import seaborn as sns\nsns.countplot(data['diagnosis'])","6fa62420":"data['diagnosis'] = [1 if x=='M' else 0 for x in  data['diagnosis']]","00099963":"data.head()","92c2f43d":"data.describe()","9304279d":"color_list = ['red' if i==1.0 else 'green' for i in data.loc[:,'diagnosis']]\npd.plotting.scatter_matrix(data.iloc[:, 7:13],\n                                       c=color_list,\n                                       figsize= [10,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","e5c32776":"x = data.iloc[:,1:]\ny = data['diagnosis']","84a4f4b3":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1) # 0.3 means 30% of data is splitted for testing. Remaining 70% is used to train our data","5ca5208a":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train) # to train our data\npredicted_values = knn.predict(x_test)\ncorrect_values = np.array(y_test) # just to make them array","9c0ea611":"predicted_values","6e941650":"correct_values","80200898":"print('KNN (with k=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","bee9519a":"neig = np.arange(1,25)\ntest_accuracy = []\n# Loop over different values of k\nfor k in neig:\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.legend()\nplt.title('k-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","78c3f3a2":"data1 = data[data['diagnosis'] == 1.0]\nx = np.array(data1.loc[:,'fractal_dimension_mean']).reshape(-1,1)\ny = np.array(data1.loc[:,'symmetry_mean']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('fractal_dimension_mean')\nplt.ylabel('symmetry_mean')\nplt.show()","ede44472":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nLR = LinearRegression()\n# data to be predicted\ntest_data = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nLR.fit(x,y)\n# Predict\npredicted = LR.predict(test_data)\n# score\nprint('score: ',LR.score(x, y))\n# Plot regression line and scatter\nplt.figure(figsize=(10,10))\nplt.plot(test_data, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('fractal_dimension_mean')\nplt.ylabel('symmetry_mean')\nplt.show()","a71175e9":"# CV\nfrom sklearn.model_selection import cross_val_score\nreg = LinearRegression()\nk = 5 # 5 times split train and predict \ncv_result = cross_val_score(reg,x,y,cv=k) # uses R^2 as score \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)\/k)","da031de1":"from sklearn.ensemble import RandomForestClassifier\nx = data.iloc[:,1:]\ny = data['diagnosis']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1) \nRF = RandomForestClassifier(random_state = 4)\nRF.fit(x_train,y_train)\ny_predicted = RF.predict(x_test)","e1cb5ba0":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_predicted)\nprint('Confusion matrix: \\n',cm)","e5604df8":"from sklearn.metrics import classification_report\nprint('Classification report: \\n',classification_report(y_test,y_predicted))","66d926b2":"# visualize with seaborn library\nsns.heatmap(cm,annot=True,fmt=\"d\") \nplt.show()","340d77a2":"x = data.iloc[:,1:]\ny = data['diagnosis']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred = logreg.predict(x_test)","8202f607":"print('logistic regression score: ', logreg.score(x_test, y_test))","8e63bda2":"# CV\nfrom sklearn.model_selection import cross_val_score\nlogreg = LogisticRegression()\nk = 5 # 5 times split train and predict \ncv_result = cross_val_score(logreg,x,y,cv=k) # uses R^2 as score \nprint('CV Scores: ',cv_result)\nprint('CV scores average: ',np.sum(cv_result)\/k)","afcc2d57":"# grid search cross validation with 1 hyperparameter (KNN)\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","2dcb63e0":"**Linear Regression**\n1. y = ax + b where y = target, x = feature and a = parameter of model\n2. We choose parameter of model(a) according to minimum error function that is lost function","5e8f1a1d":"**Random Forest Regression**","32c92bed":"**k value**\n1. k value is as hyperparameter\n2. but is this selected k value (k=3) optimum one?","3cb4ec0c":"* **how many types of diagnosis do we have?**","25ed5bab":"*** change diagnosis column to integers**","5a65e10c":"**CROSS VALIDATION**\n* In KNN method if we change train-test data,  the score can change. But if the score changes according to our data to which score values should I rely on?\n* the answer is I dont know. But if I use cross validation, I can find a reasonable accuracy.","672707b9":"**K-NEAREST NEIGHBORS (KNN)**\n1. k is a selected number (for example k=3)\n2. we select k nearest neighbors of a particle that we want to predict\n3. in this selected set, if we have more M diagnosis than B diagnosis, we predict this particle as M. Or vise versa  ","d7ad54c4":"**Split our data into train data and test data**","78f2a861":"**KNN Prediction Score**","dba53913":"**Confusion matrix with random forest**","cb2a340a":"**HYPERPARAMETER TUNING**\n\nGridSearchCV is utilized for finding the hyperparameters (KNN)","bf070a7e":"* **drop unnecessary columns**","1095a81c":"we see that for the diagnosis 0 we have predicted 106 of them correctly (2 false prediction) (98% accuracy for diagnosis 0)\n\non the other hand, \n\nwe see that for the diagnosis 1 we have predicted 56 of them correctly (7 false prediction) (89% accuracy for diagnosis 1)","0f6a0d51":"**Classification Report**","18f621aa":"**Cross Validation for logistic regression**","440973b1":"**Choosing x and y values**\n1. x is our features except diagnosis (classification columns)\n2. y is diagnosis","3aed30ad":"**Logistic Regression**","35024742":"**REGRESSION**"}}