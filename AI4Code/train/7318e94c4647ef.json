{"cell_type":{"2290bb2d":"code","c5ff6aba":"code","0caa0cc9":"code","3c5e2b01":"code","0480b37b":"code","0dfe0fd3":"code","4220aaa0":"code","c85f383a":"code","ed43a094":"code","57eac5aa":"code","a78970ba":"code","15837479":"code","0594acd2":"code","8fca979a":"code","d99ef301":"code","8a5b53ee":"code","8451d83a":"code","c825e18f":"code","462bcc19":"code","dca9eda2":"code","a9fcf340":"code","7a7cbba4":"code","bf9dbcb7":"code","d5a837a1":"code","1f014a81":"code","09d6cc9c":"code","9b2a8321":"code","d5193a4d":"markdown","4ebc1db9":"markdown","97fb5887":"markdown","66584631":"markdown","84392246":"markdown","5d9c3285":"markdown","5b947b06":"markdown","ffe14bdc":"markdown","73cb0591":"markdown","f4cc9b55":"markdown","85375580":"markdown","0cf32a20":"markdown","8120ee94":"markdown","e05141a0":"markdown","5310bb1c":"markdown","24ea843c":"markdown","a68d9b98":"markdown","754e522b":"markdown","aaddabf4":"markdown","70fbf9c4":"markdown","ba2f1bcd":"markdown"},"source":{"2290bb2d":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score, cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom argparse import Namespace\nimport seaborn as sns \n\nfrom sklearn import preprocessing\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nsns.set(rc={'figure.figsize':(10,8)})","c5ff6aba":"args = Namespace(\n    target=\"expenses\",\n    data_file=\"0_dataset.csv\",\n    cv = 10\n)","0caa0cc9":"premium =  pd.read_csv(\"..\/input\/insurance-premium-prediction\/insurance.csv\")\ndf = premium.copy()","3c5e2b01":"df.head(10)\n","0480b37b":"df.info()","0dfe0fd3":"df.isna().sum()","4220aaa0":"df[\"age\"].unique()","c85f383a":"df[\"sex\"].unique()","ed43a094":"df[\"bmi\"].unique()","57eac5aa":"df[\"children\"].unique()","a78970ba":"df[\"smoker\"].unique()","15837479":"df[\"region\"].unique()","0594acd2":"df[\"expenses\"].unique()","8fca979a":"df.sample(30)","d99ef301":"df.describe()","8a5b53ee":"le = preprocessing.LabelEncoder()","8451d83a":"df[\"sex\"] = le.fit_transform(df[\"sex\"])  # 0 is female, 1 is male\ndf.head()","c825e18f":"df[\"smoker\"] = le.fit_transform(df[\"smoker\"])  # 1 is yes, 0 is no\ndf.head()","462bcc19":"df_region = pd.get_dummies(df[\"region\"])\ndf_region.head()","dca9eda2":"df_region.columns = ['Bursa', 'Ankara', 'Istanbul', 'Izmir']\ndf_region.head()","a9fcf340":"df = pd.concat([df, df_region], axis = 1)\ndf.drop([\"Bursa\", \"region\"], axis = 1, inplace = True)\ndf.head()","7a7cbba4":"df.head()","bf9dbcb7":"age = df[['age']].values.astype(float)\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nage_scaled = min_max_scaler.fit_transform(age)\ndf['age_scaled'] = pd.DataFrame(age_scaled)","d5a837a1":"bmi = df[['bmi']].values.astype(float) \n\nmin_max_scaler = preprocessing.MinMaxScaler() \nbmi_scaled = min_max_scaler.fit_transform(bmi) \ndf['bmi_scaled'] = pd.DataFrame(bmi_scaled) ","1f014a81":"df.head()","09d6cc9c":"df.drop([\"age\", \"bmi\"], axis = 1, inplace = True)\ndf.head()","9b2a8321":"export_csv = df.to_csv (r'1_preprocessed_data.csv', index = None, header = True)","d5193a4d":"> \u0130lk 10 kayda bakarak veya siz rastgele 10 kayda (df.sample(10)) bakarak da \u00f6znitelikleri inceleyebilirsiniz.\n;","4ebc1db9":"> Expenses da g\u00f6zlemlerimizi tamam\u0131n\u0131 g\u00f6remiyoruz o y\u00fczden df.sample(30) komutunu g\u00f6rmek istiyorum.","97fb5887":"> \u0130nfo komutumuzu da \u00e7al\u0131\u015ft\u0131r\u0131p verimizdeki \u00f6zniteliklerde null de\u011ferimiz var m\u0131 varsa ka\u00e7 tane var ayr\u0131ca de\u011fi\u015fken t\u00fcrlerimizi ve her \u00f6znitelikde ka\u00e7 adet g\u00f6zlem var g\u00f6rmek istiyorum.","66584631":"# Premium Insurance Preprocessing","84392246":"> 2 yeni \u00f6znitelik ekledik bu durumda \"Age\" ve \"Bmi\" de\u011ferlerimizle i\u015fimiz kalmad\u0131 bu y\u00fczden drop edip preprocessing i\u015flemimize son verece\u011fiz.","5d9c3285":"#### Label Encoding (Binary)","5b947b06":"> Yukar\u0131da gerekli k\u00fct\u00fcphaneleri import ettik. \n> \u00d6zellikle ileriki sat\u0131rlar\u0131m\u0131zda g\u00f6rece\u011fimiz age scale ve bmi scale i\u015flemlerinde kullanmak i\u00e7in \"from sklearn.preprocessing import scale\" ifadesini import ediyoruz buraya dikkat \u00e7ekmek istiyorum.\n> K\u0131saca transform i\u015flemimizi ger\u00e7ekle\u015ftirmemiz i\u00e7in gerekli ifadeler oldu\u011funu s\u00f6yleyebilirim.\n> ","ffe14bdc":"> \u00d6n i\u015fleme a\u015famas\u0131nda label encoding i\u015flemimizi ger\u00e7ekle\u015ftirece\u011fim.\n> \"Sex\" ve \"Smoker\" \u00f6zniteliklerimizde label encoding i\u015flemlerimizi uygulayaca\u011f\u0131m.\n> \u00c7\u00fcnk\u00fc makine \u00f6\u011frenmemiz de kullanmam\u0131z i\u00e7in 0 1 formuna transform etmemiz gerekiyor.","73cb0591":"### \u00d6n \u0130\u015fleme","f4cc9b55":"**Art\u0131k \u00d6zel Sigorta data analizimizde export etti\u011fimiz Data y\u0131 kullanaca\u011f\u0131z.**","85375580":"> Ve bu ingilizce y\u00f6nler yerine bize daha tan\u0131d\u0131k gelen \u015fehir isimlerini kullanmak istiyorum.","0cf32a20":"### Gerekli K\u00fct\u00fcphaneleri  ve Veri Setini Y\u00fcklemek","8120ee94":"> Tekrar df.head() komutu ile datam\u0131n yeni \u015feklini g\u00f6rd\u00fcm.","e05141a0":"> Evet Burada da birbirinden \u00fcst\u00fcnl\u00fc\u011f\u00fc olmayan ve 2 den fazla unique de\u011fere sahip \u00f6zniteliklerimizle bir i\u015flem gercekle\u015ftirip onlar\u0131da say\u0131sal olarak verime eklemek istiyorum .\n> Komutumuzu uygulad\u0131\u011f\u0131m\u0131z \u00f6zniteli\u011fin t\u00fcm unique de\u011ferleri i\u00e7in birer \u00f6znitelik alan\u0131 ekleyece\u011fim ve g\u00f6zlemimizde \u00f6nceki \u00f6zniteli\u011fe ait olan de\u011feri yeni \u00f6zniteli\u011finde 1 de\u011ferini alacak ","5310bb1c":"#### Dummy Variable Extraction (Multi)","24ea843c":"***Bir Sonraki Analizimizde G\u00f6r\u00fc\u015fmek Dile\u011fiyle Veriyle Kal\u0131n...***","a68d9b98":"> Burda \u00f6nemli bir de\u011fi\u015fiklik ger\u00e7ekle\u015ftirdim , ekledi\u011fm \u00f6zniteliklerden birisini drop ettim.\n> Bunun ana sebebi \"BURSA\" olmas\u0131 de\u011fildi tabiki ,e\u011fer bir g\u00f6zlemimizde \"ANKARA\" \"\u0130stanbul\" ve \"\u0130zmir\" g\u00f6zlemleri 0 ise demekki \"BURSA\"dan oldu\u011funu anlayaca\u011f\u0131z.Bunun i\u00e7in ekstra bir \u00f6znitelik tutmam\u0131za gerek yok idi bu y\u00fczden drop i\u015flemini rastgele 4 \u00fcnden birini se\u00e7erek ger\u00e7ekle\u015ftirdim.","754e522b":"> Burada g\u00fczel bir bilgiyi elde ediyoruz, null bilgimizin olmad\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz.\n> Fakat kesin emin olmak amac\u0131yla herbir \u00f6zniteli\u011fimize unique komutunu uygulay\u0131p bilgimizi desteklemek istiyorum .\n> \u00c7\u00fcnk\u00fc bazen g\u00f6zlemi bo\u015f b\u0131rakmak yerine \"\" veya bo\u015f gibi ifadelerde yer alabiliyor.\n>  ","aaddabf4":"> Birka\u00e7 defa \u00e7al\u0131\u015ft\u0131rmama ra\u011fmen null de\u011fer g\u00f6remiyorum .Null de\u011fer olmad\u0131\u011f\u0131n\u0131 sayarak devam ediyorum.\n> Bunu farkl\u0131 bi yol ile , 0-100 100-200 200-300 300-400 ... aras\u0131 de\u011ferleri g\u00f6zlemleyerek de kesinle\u015ftirebliriz","70fbf9c4":"> Datam\u0131zdak\u015f \"Age\" ve \"Bmi\"(v\u00fccut kitle indeksi) de\u011ferlerini 0 - 1 \u00f6lce\u011finde de\u011ferlendirmek istiyorum bunu 0= minumum de\u011fer ve 1 = maximum de\u011fer olarak oranl\u0131yor.","ba2f1bcd":"## Normalize Data"}}