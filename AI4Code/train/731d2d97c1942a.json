{"cell_type":{"e308310c":"code","59fa9542":"code","0ff03911":"code","d74ea129":"code","6b10e831":"code","eb113521":"code","5588931a":"code","43caf924":"code","fda4fcfe":"code","de851ef1":"code","b864a935":"code","968ddd48":"code","9ea41f63":"code","f1db1b84":"code","8eb8febd":"code","7aaebff3":"code","5288cf6c":"code","cc72ee29":"code","a3cf8563":"code","1fc83b03":"code","9b026066":"code","3c2cd493":"code","fd99ea2b":"code","fc007181":"code","43e9c3c3":"code","a6694c98":"code","8022bb9e":"code","f79bf8f1":"code","04d86274":"code","fbc525b7":"code","5594c9a0":"code","c296f369":"code","f1ae2800":"code","7b6376af":"code","d3ce5c54":"code","a90457d8":"code","3e60e02d":"code","708ec210":"code","7cfaf691":"code","7ef4873d":"code","42029304":"code","bdbf2de9":"code","661d2864":"code","999d970a":"code","50fb55a3":"code","c1df4173":"markdown","490c9c1b":"markdown","d813f30c":"markdown","476c5da4":"markdown","7028d90f":"markdown"},"source":{"e308310c":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())  ","59fa9542":"!nvidia-smi","0ff03911":"import sys\nsys.path.append('..\/input\/detectron-data\/mysitepackages')","d74ea129":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport pandas as pd \n\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nfrom datetime import datetime\nimport time\nimport matplotlib.pyplot as plt\n\nimport os, json, cv2, random\nimport skimage.io as io\nimport copy\nfrom pathlib import Path\nfrom typing import Optional\nimport json\nimport matplotlib.pyplot as plt\n\nimport ast\n\n\nfrom tqdm import tqdm\nimport itertools\n\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom glob import glob\nimport numba\nfrom numba import jit\n\nfrom pycocotools.coco import COCO\n# detectron2\nfrom detectron2.structures import BoxMode\nfrom detectron2 import model_zoo\nfrom detectron2.config import get_cfg\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer, launch\nfrom detectron2.evaluation import COCOEvaluator\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.utils.visualizer import Visualizer\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.data import detection_utils as utils\n\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.data import detection_utils as utils\nimport detectron2.data.transforms as T\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\n\n\n\nfrom detectron2.evaluation.evaluator import DatasetEvaluator\nimport pycocotools.mask as mask_util\nfrom detectron2.engine import BestCheckpointer\nfrom detectron2.checkpoint import DetectionCheckpointer\n\nsetup_logger()","6b10e831":"# !python -m pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git' --target=\/kaggle\/working\/mysitepackages","eb113521":"# import sys\n# sys.path.append('..\/input\/detectron-data\/mysitepackages')","5588931a":"# Read training dataset\ntrain_df = pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/test.csv\")","43caf924":"# train_df","fda4fcfe":"# eval(train_df[\"annotations\"][100])[0]","de851ef1":"# Create a \"path\" column containing full path to the frames\nbase_folder = \"..\/input\/tensorflow-great-barrier-reef\/train_images\"\n\ntrain_df[\"path\"] = base_folder + \"\/video_\" + \\\n                    train_df['video_id'].astype(str) + \"\/\" +\\\n                    train_df['video_frame'].astype(str) +\".jpg\"\ntrain_df[\"Width\"]=1280\ntrain_df[\"Height\"]=720","b864a935":"train_df[\"NumBBox\"]=train_df['annotations'].apply(lambda x: str.count(x, 'x'))\ntrain_df.head(5)","968ddd48":"print(train_df[\"NumBBox\"].unique())\n","9ea41f63":"df_train=train_df[train_df[\"NumBBox\"]>0]\ndf_train.reset_index(drop=True, inplace=True)\ndf_train","f1db1b84":"df_train.shape[0]","8eb8febd":"df_train[\"image_id\"][0]","7aaebff3":"print(df_train[\"NumBBox\"].unique())\n","5288cf6c":"def load_dataset(train_df) : \n    base_folder = \"..\/input\/tensorflow-great-barrier-reef\/train_images\"\n    dataset_dict = []\n    for index in np.arange(train_df.shape[0]) : \n        record = {}\n        record[\"image_id\"] = train_df[\"image_id\"][index]\n        record[\"file_name\"] = train_df[\"path\"][index]\n        #record[\"file_name\"] = f'..\/input\/tensorflow-great-barrier-reef\/train_images\/video_{train_df[\"video_id\"][index]}\/{train_df[\"video_frame\"][index]}.jpg'\n        #..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\/0.jpg\n        record[\"width\"] = train_df[\"Width\"][index]\n        record[\"height\"] = train_df[\"Height\"][index]\n        class_id = 0\n        obj = []\n        \n        for box in np.arange(len(eval(train_df.annotations[index]))) : \n            annot = {}\n            annot[\"bbox\"] = [eval(train_df.annotations[index])[box]['x'],\n                            eval(train_df.annotations[index])[box]['y'],\n                            eval(train_df.annotations[index])[box]['width'],\n                            eval(train_df.annotations[index])[box]['height']]\n            annot[\"bbox_mode\"] =  BoxMode.XYWH_ABS\n            annot[\"category_id\"] = class_id\n            obj.append(annot)\n        record[\"annotations\"] = obj\n        dataset_dict.append(record)\n    return dataset_dict \n    ","cc72ee29":"# train_dataset =  load_dataset(df_train)\n","a3cf8563":"# train_dataset","1fc83b03":"# len(train_dataset)","9b026066":"# train_dataset[90:100]","3c2cd493":"# train_dataset[100]","fd99ea2b":"DatasetCatalog.register(\"train_dataset\" ,lambda d=df_train: load_dataset(d)[:])\n#atasetCatalog.register(\"test_dataset\" ,lambda d=df_train: load_dataset(d)[4500:])\n\n# #DatasetCatalog.register(\"test_dataset\" ,lambda d=train_dataset[90:100]: load_dataset(d))\n","fc007181":"from detectron2.engine import DefaultTrainer\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\"))\ncfg.DATASETS.TRAIN = (\"train_dataset\",)\ncfg.DATASETS.TEST = ()\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\ncfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\ncfg.SOLVER.STEPS = []        # do not decay learning rate\n#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets)\n# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()","43e9c3c3":"# from detectron2.engine import DefaultTrainer\n\n# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_dataset\",)\n# cfg.DATASETS.TEST = ()\n# #cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 \n# cfg.DATALOADER.NUM_WORKERS = 2\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n# cfg.SOLVER.IMS_PER_BATCH = 2\n# cfg.SOLVER.BASE_LR = 0.001 # pick a good LR\n# cfg.SOLVER.MAX_ITER = 40000  # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n# cfg.SOLVER.GAMMA = 0.1\n# # The iteration number to decrease learning rate by GAMMA.\n# cfg.SOLVER.STEPS = (10000,)\n# #cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 2   # faster, and good enough for this toy dataset (default: 512)\n# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1 # only has one class (ballon). (see https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets)\n# # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = DefaultTrainer(cfg) \n# trainer.resume_or_load(resume=False)\n# trainer.train()","a6694c98":"# from detectron2.engine import DefaultTrainer\n\n# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\"))\n# cfg.DATASETS.TRAIN = (\"train_dataset\")\n# #cfg.DATASETS.TEST = (\"test_dataset\")\n# cfg.DATALOADER.NUM_WORKERS = 2\n# #cfg.OUTPUT_DIR = '..\/input\/detectron-data\/output'\n\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n# #cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n# cfg.SOLVER.IMS_PER_BATCH = 2\n# cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n# cfg.SOLVER.MAX_ITER = 100    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n# cfg.SOLVER.STEPS = []        # do not decay learning rate\n# #cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n# cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets)\n# # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n# os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n# trainer = DefaultTrainer(cfg) \n# trainer.resume_or_load(resume=False)\n# trainer.train()\n","8022bb9e":"# im = cv2.imread(train_df['path'][100])\n","f79bf8f1":"# train_dataset[100]","04d86274":"# # Inference should use the config with parameters that are used in training\n# # cfg now already contains everything we've set previously. We changed it a little bit for inference:\nfrom detectron2.engine import DefaultTrainer\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\"))\ncfg.OUTPUT_DIR = '..\/input\/detectron-data\/output'\n\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3# set a custom testing threshold\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n\npredictor = DefaultPredictor(cfg)\n# outputs = predictor(array[0])\n# outputs","fbc525b7":"# array = np.load('..\/input\/tensorflow-great-barrier-reef\/example_test.npy')","5594c9a0":"# array.shape","c296f369":"# train_dataset[100]","f1ae2800":"# outputs = predictor(im)\n\n# outputs","7b6376af":"# train_df['annotations'][100]","d3ce5c54":"# test = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/example_sample_submission.csv')","a90457d8":"# test","3e60e02d":"# def pred(array): \n#     for im,i in zip(array,np.arange(array.shape[0])) : \n#         outputs = predictor(im)\n#         pred_boxes = outputs[\"instances\"].pred_boxes.tensor.cpu().numpy()\n#         scores = outputs[\"instances\"].scores.cpu().numpy()\n#         test.row_num[0] =  i\n#         annotations = []\n#         for annot in np.arange(len(scores)): \n#             annotations.append(np.concatenate((scores[annot],pred_boxes[annot]),axis=None))","708ec210":"# from detectron2.modeling import build_model\n# cfg = get_cfg()\n# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\"))\n# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n\n# model = build_model(cfg)\n# DetectionCheckpointer(model).load('..\/input\/detectron-data\/output\/model_final.pth')  # load a file, usually from cfg.MODEL.WEIGHTS\n# model.eval()\n","7cfaf691":"def pred(im): \n    outputs = predictor(im)\n    array = np.array([])\n    pred_boxes = outputs[\"instances\"].pred_boxes.tensor.cpu().numpy()\n    scores = outputs[\"instances\"].scores.cpu().numpy()\n    annotations = []\n    for annot in np.arange(len(scores)): \n        array = np.concatenate((array,scores[annot],pred_boxes[annot][0],pred_boxes[annot][1],pred_boxes[annot][2]-pred_boxes[annot][0],\n                               pred_boxes[annot][3]-pred_boxes[annot][1]),axis=None)\n    return array","7ef4873d":"# \" \".join(list(pred(im).astype(str)))","42029304":"# print(outputs)","bdbf2de9":"# # look at the outputs. See https:\/\/detectron2.readthedocs.io\/tutorials\/models.html#model-output-format for specification\n# print(outputs[\"instances\"].pred_boxes.tensor.cpu().numpy())\n# print(outputs[\"instances\"].scores.cpu().numpy())","661d2864":"# # We can use `Visualizer` to draw the predictions on the image.\n# v = Visualizer(im, MetadataCatalog.get(cfg.DATASETS.TRAIN[0]))\n# out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n# plt.imshow(out.get_image()[:, :, ::-1])","999d970a":"# for d in random.sample(train_dataset, 1):    \n#     im = cv2.imread(d[\"file_name\"])\n#     outputs = predictor(im)  # format is documented at https:\/\/detectron2.readthedocs.io\/tutorials\/models.html#model-output-format\n#     # look at the outputs. See https:\/\/detectron2.readthedocs.io\/tutorials\/models.html#model-output-format for specification\n#     print(outputs[\"instances\"].pred_classes)\n#     print(outputs[\"instances\"].pred_boxes)\n#     v = Visualizer(im[:, :, ::-1],\n                    \n#                    scale=0.5, \n#     )\n#     out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n# plt.imshow(out.get_image()[:, :, ::-1])","50fb55a3":"import greatbarrierreef\nenv = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (pixel_array, sample_prediction_df) in iter_test:\n    sample_prediction_df['annotations'] = \" \".join(list(pred(pixel_array).astype(str)))  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","c1df4173":"<center><img src=\"https:\/\/cdn.britannica.com\/64\/155864-050-34FBD7A2\/view-Great-Barrier-Reef-Australia-coast.jpg\" width=700><\/center>\n\n<center><h1>Great Barrier Reef - YOLO5 and Detectron2 Full Understanding<\/h1><\/center>\n\n","490c9c1b":"<center><img src=\"https:\/\/debuggercafe.com\/wp-content\/uploads\/2021\/03\/Object-Detection-using-PyTorch-YOLOv5-e1617643548833.jpg\"><\/center>","d813f30c":"* YOLO an acronym for 'You only look once', is an object detection algorithm that divides images into a grid system. Each cell in the grid is responsible for detecting objects within itself.\n\n* YOLO is one of the most famous object detection algorithms due to its speed and accuracy.\n","476c5da4":"## Introduction ","7028d90f":"## YOLO5 Architecture "}}