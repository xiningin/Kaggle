{"cell_type":{"a68cb947":"code","75d07453":"code","1598bbb4":"code","1cd1dce7":"code","5216eacc":"code","ac361e92":"code","8095b8a5":"code","a52ae24c":"code","87a6b98c":"code","50fc466d":"code","df7a89fb":"code","e7db76df":"code","9481fda4":"code","990f447e":"code","7c053d90":"code","0c353f29":"code","4256a2a6":"code","8518bc45":"code","e088728f":"code","0cc9d7ea":"code","dbc4e04f":"code","41eb9218":"code","68753b7b":"code","cfedc4ce":"code","8c7176e1":"code","a19102b2":"code","ddb6d6c1":"code","bc39751d":"code","6e61b57a":"code","5a8bf46f":"code","09259a0b":"code","ec66f757":"code","611f0471":"code","091ba6f8":"code","07b75ad0":"code","792b1c3a":"code","db02e9e0":"code","5c92a77f":"code","742e921d":"code","5d0bdcb6":"code","f98e7688":"code","e915bcf9":"code","a961e2b6":"code","1cd597d9":"code","3c2bbc1a":"code","5c5a63af":"code","24f677ef":"markdown","d4d0289b":"markdown","98421e04":"markdown","eb3b8374":"markdown","a8d17848":"markdown","96be09e0":"markdown","d0692af4":"markdown","f63d5a96":"markdown","770a63cf":"markdown","325c8955":"markdown","537ef966":"markdown","c31cc661":"markdown","21e09143":"markdown","5f97ec3e":"markdown","2d448e0f":"markdown","9fcc1955":"markdown","18a1e413":"markdown","7774539d":"markdown","16cf4f37":"markdown","d5f7ce0d":"markdown","6fa31c9b":"markdown","752ed572":"markdown","dd184b57":"markdown","627dbd99":"markdown","1a8a2263":"markdown","fb6de710":"markdown","b44aef36":"markdown","6280632a":"markdown","a614c33b":"markdown","262e367e":"markdown","1f949f59":"markdown","bbf4870d":"markdown","763dfd30":"markdown","2b90db34":"markdown","c93136c8":"markdown","9d9245e6":"markdown","a7fb2305":"markdown","ac0b1c3d":"markdown","7b00fe0c":"markdown","7071da7b":"markdown","ad6070d7":"markdown","c79b658a":"markdown","71916c90":"markdown","a3d20f77":"markdown","c10b86ef":"markdown","dd370b35":"markdown","539397af":"markdown","e408183d":"markdown","5ee6fa5d":"markdown","c23ec571":"markdown","c84903e9":"markdown","8f71f8ab":"markdown","eb0e8558":"markdown","90b859cb":"markdown","1f1aeba3":"markdown","5223982a":"markdown","eed685db":"markdown","fd7211a6":"markdown","59381438":"markdown","d46da186":"markdown","10776c84":"markdown","f54bf3e8":"markdown","2e597a16":"markdown","2747d24b":"markdown","74abcdb7":"markdown","4608c7fd":"markdown","f3a8a684":"markdown","5bee1ac9":"markdown","2fc09ef5":"markdown","cf3c4632":"markdown","a275cfa2":"markdown","261be879":"markdown","a3f28d52":"markdown"},"source":{"a68cb947":"# Third party\nfrom sklearn.base import clone\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\n\n# Local application\nimport md_grupos_practica2_utils as utils","75d07453":"random_state = 265","1598bbb4":"\ntarget = \"Outcome\"\n\ndata = utils.load_data('..\/input\/pima-indians-diabetes-database\/diabetes.csv', target, index=None)","1cd1dce7":"data.sample(5, random_state=random_state)","5216eacc":"data.shape","ac361e92":"(X, y) = utils.divide_dataset(data, target)","8095b8a5":"X.sample(5, random_state=random_state)","a52ae24c":"y.sample(5, random_state=random_state)","87a6b98c":"(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      train_size=0.7,\n                                                      random_state=random_state)","50fc466d":"X_train.sample(5, random_state=random_state)","df7a89fb":"y_train.sample(5, random_state=random_state)","e7db76df":"X_test.sample(5, random_state=random_state)","9481fda4":"y_test.sample(5, random_state=random_state)","990f447e":"n_splits = 10\nn_repeats = 5\n\ncv = RepeatedStratifiedKFold(n_splits=n_splits,\n                             n_repeats=n_repeats,\n                             random_state=random_state)","7c053d90":"\ncolumn_transformer = make_column_transformer((\"drop\",[\"SkinThickness\",\"Insulin\"]),(SimpleImputer(missing_values = 0, strategy=\"mean\"), [\"Glucose\",\"BloodPressure\",\"BMI\"]), remainder=\"passthrough\")\n\npipeline = make_pipeline(column_transformer, KNeighborsClassifier())\n\nweights = [\"uniform\", \"distance\"]\nmetrics = [\"euclidean\", \"manhattan\", \"minkowski\"]\nn_neighbors = [5, 7, 9, 11, 13, 15, 17, 19, 21, 23]\n\nk_neighbors_clf = utils.optimize_params(pipeline,\n                                        X_train, y_train, cv,\n                                        kneighborsclassifier__weights = weights,\n                                        kneighborsclassifier__metric = metrics,\n                                        kneighborsclassifier__n_neighbors=n_neighbors, scoring=['recall', 'roc_auc'], refit='roc_auc')","0c353f29":"X_train.shape","4256a2a6":"pipeline = make_pipeline(column_transformer, DecisionTreeClassifier(random_state=random_state))\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [ 2, 3, 4, 5, 6]\nmin_samples_split = [5, 10, 15, 20, 30] #afecta al max depth\nccp_alpha = [0.0, 0.1, 0.2, 0.3, 0.4]\n\ndecision_tree_clf = utils.optimize_params(pipeline,\n                                          X_train, y_train, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__min_samples_split=min_samples_split, #no aparece en el grid params, pero si en el display final\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha, \n                                          scoring=['recall', 'roc_auc'], \n                                          refit='roc_auc')","8518bc45":"pipeline = make_pipeline(column_transformer, AdaBoostClassifier(random_state=random_state))\n\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nn_estimators = [ 40, 50, 60, 70]\nlearning_rate = [0.1, 0.3, 0.9, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\n\nadaboost_clf = utils.optimize_params(pipeline,\n                                     X_train, y_train, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__n_estimators=n_estimators,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     scoring=['recall', 'roc_auc'], \n                                     refit='roc_auc')","e088728f":"pipeline = make_pipeline(column_transformer, BaggingClassifier(random_state=random_state))\n\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nn_estimators = [ 10, 50, 75, 100 ]\nmax_samples = [ 0.2, 1.0 , 1.2 ]\nbootstrap = [ True, False ]\ncriterion = [ \"gini\", \"entropy\" ]\n\nbagging_clf = utils.optimize_params(pipeline,\n                                    X_train, y_train, cv,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__max_samples=max_samples,\n                                    baggingclassifier__bootstrap=bootstrap,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    scoring=['recall', 'roc_auc'], \n                                    refit='roc_auc')","0cc9d7ea":"pipeline = make_pipeline(column_transformer, RandomForestClassifier(random_state=random_state))\n\nn_estimators = [ 50, 100, 150 ]\nmax_features = [\"sqrt\", \"log2\"]\nmax_depth = [ None, 5, 10]\ncriterion = [\"gini\", \"entropy\"]\n\nrandom_forest_clf = utils.optimize_params(pipeline,\n                                          X_train, y_train, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__max_features=max_features,\n                                          randomforestclassifier__max_depth=max_depth,\n                                          scoring=['recall', 'roc_auc'], \n                                          refit='roc_auc')","dbc4e04f":"pipeline = make_pipeline(column_transformer, GradientBoostingClassifier(random_state=random_state))\n\nn_estimators = [ 50, 100, 150 ]\nlearning_rate = [0.01, 0.05, 0.1]\nsubsample = [0.2, 0.8, 1.0]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\n\ngradient_boosting_clf = utils.optimize_params(pipeline,\n                                              X_train, y_train, cv,\n                                              gradientboostingclassifier__n_estimators=n_estimators,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__subsample=subsample,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              scoring=['recall', 'roc_auc'], \n                                              refit='roc_auc')","41eb9218":"pipeline = make_pipeline(column_transformer, HistGradientBoostingClassifier(random_state=random_state))\n\nloss = [\"binary_crossentropy\", \"categorical_crossentropy\"]\nlearning_rate = [0.01, 0.05, 0.1]\nmax_iter = [50, 100, 130]\nmin_samples_leaf = [15, 25, 35]\n\nhist_gradient_boosting_clf = utils.optimize_params(pipeline,\n                                                   X_train, y_train, cv,\n                                                   histgradientboostingclassifier__loss=loss,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_iter=max_iter,\n                                                   histgradientboostingclassifier__min_samples_leaf=min_samples_leaf,\n                                                   scoring=['recall', 'roc_auc'], \n                                                   refit='roc_auc')","68753b7b":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","cfedc4ce":"utils.evaluate_estimators(estimators, X_test, y_test)\n","8c7176e1":"# filepath = \"..\/input\/wisconsin.csv\"\nindex = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\", target, index)","a19102b2":"data.sample(5, random_state=random_state)","ddb6d6c1":"del data[\"Unnamed: 32\"]\ndata.sample(5, random_state=random_state)","bc39751d":"(X,y) = utils.divide_dataset(data, target = target)","6e61b57a":"X.sample(5, random_state=random_state)","5a8bf46f":"y.sample(5, random_state=random_state)","09259a0b":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=random_state,\n                                                      train_size=train_size)","ec66f757":"X_train.sample(5, random_state=random_state)","611f0471":"y_train.sample(5, random_state=random_state)","091ba6f8":"X_test.sample(5, random_state=random_state)","07b75ad0":"y_test.sample(5, random_state=random_state)","792b1c3a":"column_transformer = make_column_transformer((\"drop\",[\"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \n                                                      \"area_worst\", \"perimeter_worst\", \"concavity_mean\", \"compactness_mean\", \n                                                      \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\"]), \n                                             remainder=\"passthrough\")\n\ndiscretizer = KBinsDiscretizer(n_bins=2, strategy=\"uniform\")\n\npipeline = make_pipeline(column_transformer, discretizer, KNeighborsClassifier())\n\nweights = [\"uniform\", \"distance\"]\nmetrics = [\"euclidean\", \"manhattan\", \"minkowski\"]\nn_neighbors = [3, 5, 7, 9, 11, 13, 15, 17, 19]\n\nk_neighbors_clf = utils.optimize_params(pipeline,\n                                        X_train, y_train, cv,\n                                        kneighborsclassifier__weights = weights,\n                                        kneighborsclassifier__metric=metrics,\n                                        kneighborsclassifier__n_neighbors=n_neighbors, scoring=['recall', 'roc_auc'], refit='roc_auc')","db02e9e0":"X_train.shape","5c92a77f":"pipeline = make_pipeline(column_transformer, discretizer, DecisionTreeClassifier(random_state=random_state))\n\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [ 3, 4, 5, 6, 7]\nmin_samples_split = [10, 15, 20, 30, 40] #afecta al max depth\nccp_alpha = [0.0, 0.1, 0.2]\n\ndecision_tree_clf = utils.optimize_params(pipeline,\n                                          X_train, y_train, cv,\n                                          decisiontreeclassifier__criterion=criterion,\n                                          decisiontreeclassifier__max_depth=max_depth,\n                                          decisiontreeclassifier__min_samples_split=min_samples_split,\n                                          decisiontreeclassifier__ccp_alpha=ccp_alpha, \n                                          scoring=['recall', 'roc_auc'], \n                                          refit='roc_auc')","742e921d":"pipeline = make_pipeline(column_transformer, discretizer, AdaBoostClassifier(random_state=random_state))\n\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nn_estimators = [40, 50, 60, 70]\nlearning_rate = [0.1, 0.3, 0.9, 1.0]\ncriterion = [\"gini\", \"entropy\"]\nmax_depth = [1, 2, 3]\n\nadaboost_clf = utils.optimize_params(pipeline,\n                                     X_train, y_train, cv,\n                                     adaboostclassifier__base_estimator=base_estimator,\n                                     adaboostclassifier__learning_rate=learning_rate,\n                                     adaboostclassifier__n_estimators=n_estimators,\n                                     adaboostclassifier__base_estimator__criterion=criterion,\n                                     adaboostclassifier__base_estimator__max_depth=max_depth,\n                                     scoring=['recall', 'roc_auc'], \n                                     refit='roc_auc')","5d0bdcb6":"pipeline = make_pipeline(column_transformer, discretizer, BaggingClassifier(random_state=random_state))\n\nbase_estimator = [DecisionTreeClassifier(random_state=random_state)]\nn_estimators = [ 10, 50, 75, 100 ]\nmax_samples = [ 0.2, 1.0 , 1.2 ]\nbootstrap = [ True, False ]\ncriterion = [ \"gini\", \"entropy\" ]\n\nbagging_clf = utils.optimize_params(pipeline,\n                                    X_train, y_train, cv,\n                                    baggingclassifier__base_estimator=base_estimator,\n                                    baggingclassifier__n_estimators=n_estimators,\n                                    baggingclassifier__max_samples=max_samples,\n                                    baggingclassifier__bootstrap=bootstrap,\n                                    baggingclassifier__base_estimator__criterion=criterion,\n                                    scoring=['recall', 'roc_auc'], \n                                    refit='roc_auc')","f98e7688":"pipeline = make_pipeline(column_transformer, discretizer,RandomForestClassifier(random_state=random_state))\n\nn_estimators = [ 50, 100, 150 ]\nmax_features = [\"sqrt\", \"log2\"]\nmax_depth = [ None, 5, 10]\ncriterion = [\"gini\", \"entropy\"]\n\nrandom_forest_clf = utils.optimize_params(pipeline,\n                                          X_train, y_train, cv,\n                                          randomforestclassifier__criterion=criterion,\n                                          randomforestclassifier__n_estimators=n_estimators,\n                                          randomforestclassifier__max_features=max_features,\n                                          randomforestclassifier__max_depth=max_depth,\n                                          scoring=['recall', 'roc_auc'], \n                                          refit='roc_auc')","e915bcf9":"pipeline = make_pipeline(column_transformer, discretizer, GradientBoostingClassifier(random_state=random_state))\n\nn_estimators = [ 50, 100, 150 ]\nlearning_rate = [0.01, 0.05, 0.1]\nsubsample = [0.2, 0.8, 1.0]\ncriterion = [\"friedman_mse\", \"mse\"]\nmax_depth = [1, 2, 3]\n\ngradient_boosting_clf = utils.optimize_params(pipeline,\n                                              X_train, y_train, cv,\n                                              gradientboostingclassifier__n_estimators=n_estimators,\n                                              gradientboostingclassifier__learning_rate=learning_rate,\n                                              gradientboostingclassifier__subsample=subsample,\n                                              gradientboostingclassifier__criterion=criterion,\n                                              gradientboostingclassifier__max_depth=max_depth,\n                                              scoring=['recall', 'roc_auc'], \n                                              refit='roc_auc')","a961e2b6":"from mlxtend.preprocessing import DenseTransformer\nto_dense = DenseTransformer()","1cd597d9":"pipeline = make_pipeline(column_transformer, discretizer, to_dense, HistGradientBoostingClassifier(random_state=random_state))\n\nloss = [\"binary_crossentropy\", \"categorical_crossentropy\"]\nlearning_rate = [0.01, 0.05, 0.1]\nmax_iter = [50, 100, 130]\nmin_samples_leaf = [15, 25, 35]\n\nhist_gradient_boosting_clf = utils.optimize_params(pipeline,\n                                                   X_train, y_train, cv,\n                                                   histgradientboostingclassifier__loss=loss,\n                                                   histgradientboostingclassifier__learning_rate=learning_rate,\n                                                   histgradientboostingclassifier__max_iter=max_iter,\n                                                   histgradientboostingclassifier__min_samples_leaf=min_samples_leaf,\n                                                   scoring=['recall', 'roc_auc'], \n                                                   refit='roc_auc')","3c2bbc1a":"estimators = {\n    \"Nearest neighbors\": k_neighbors_clf,\n    \"Decision tree\": decision_tree_clf,\n    \"AdaBoost\": adaboost_clf,\n    \"Bagging\": bagging_clf,\n    \"Random Forests\": random_forest_clf,\n    \"Gradient Boosting\": gradient_boosting_clf,\n    \"Histogram Gradient Boosting\": hist_gradient_boosting_clf\n}","5c5a63af":"utils.evaluate_estimators(estimators, X_test, y_test)\n","24f677ef":"# Pr\u00e1ctica 2 Aprendizaje y Selecci\u00f3n de Modelos Miner\u00eda de Datos 2020\/21\nHecha por: **Cristian Stanimirov Petrov** y **Nikola Svetlozarov Dyulgerov**","d4d0289b":"Al igual que para el anterior ensemble, tan solo usaremos los \u00e1rboles de decisi\u00f3n como estimadores, aunque en este caso tambi\u00e9n podemos pasar otros como los vecinos m\u00e1s cercanos. ","98421e04":"Con todo esto, mirando los resultados obtenidos con el `grid search` podemos ver que los clasificadores utilizados tienen profundidad m\u00ednima, un solo nivel, y que en este caso ha funcionado la t\u00e9cnica de la tasa de aprendizaje baja con un n\u00famero de estimadores razonable (el que viene por defecto). Por \u00faltimo comentar que de nuevo las puntuaciones con las dos m\u00e9tricas giran entorno a los valores visto con los dos algoritmos anteriores.","eb3b8374":"De nuevo nos encontramos que debemos fijar el **n\u00famero de estimadores** y al igual que el resto de ensembles la regla general (no necesariamente siempre funciona) es que cuantos m\u00e1s mejor, pero ya sabemos que esto tiene un coste de tiempo y memoria considerables. Otro hiperpar\u00e1metro importante para la creaci\u00f3n de los \u00e1rboles es la **cantidad de variables** que usar\u00e1n, para ello se puede emplear como m\u00e1ximo la ra\u00edz cuadrada  o el logaritmo del total  de variables que componen la base de datos. Aparte de estos dos hiperpar\u00e1metros que digamos son los m\u00e1s relevantes del ensemble, hay otros \"secundarios\" que afectan a c\u00f3mo se construyen los \u00e1rboles de decisi\u00f3n. En nuestro caso, nos hemos decantado solamente por ver qu\u00e9 ocurre si controlamos la profundidad de estos, aunque no es una pr\u00e1ctica com\u00fan ya que normalmente se deja que produzca todas las ramificaciones a pesar de sobreajustar al conjunto de datos. Sabiendo esto, la regla de cuantos m\u00e1s clasificadores mejor ha funcionado a la perfecci\u00f3n. Como algo interesante ser\u00eda ver en qu\u00e9 momento la puntuaci\u00f3n se estabiliza y tambi\u00e9n hacer un balance si merece asumir el coste computacional que conlleva la utilizaci\u00f3n de tantos \u00e1rboles. Lo que nos ha sorprendido es que el mejor modelo resultante tiene relativamente poca profundidad, esto se puede deber a que al limitar el n\u00famero de variables de los \u00e1rboles y al trabajar con una base de datos no muy grande, no nos resulte de provecho ramificar hasta el final, ya que solamente producir\u00edamos un sobre-ajuste innecesario. De ah\u00ed que con \u00e1rboles m\u00e1s peque\u00f1os se extrae mayor informaci\u00f3n.","a8d17848":"#### Histogram Gradient Boosting","96be09e0":"### \u00c1rboles de decisi\u00f3n","d0692af4":"#### Random Forest","f63d5a96":"# Pima Diabetes\n","770a63cf":"El mejor modelo de \u00e1rboles de decisi\u00f3n ha usado como criterio de divisi\u00f3n de nodos la ganancia de informaci\u00f3n y ha conseguido una profundidad de seis niveles, esto puede deberse a que tenemos un mayor n\u00famero de variables de mayor relevancia y conseguie ramificar m\u00e1s que en el anterior problema. Por otro lado, en este caso hemos obtenido un `min_samples_split` algo m\u00e1s grande que en el **Diabetes**, pero, al igual que este, hemos obtenido un `ccp_alpha` nulo, por lo que no se ha realizado demasiadas podas.","325c8955":"## Construcci\u00f3n y validaci\u00f3n del modelo final\n\nUna vez hemos optimizado los hiperpar\u00e1metros de todos los modelos resultantes procedemos al igual que antes a la parte final de encontrar el modelo final en base a su medida de rendimiento. De nuevo en caso de empate nos guiaremos por la navaja de Ockham.","537ef966":"Cargamos nuestro conjunto de datos, en este caso el de `wisconsin` especificando que variable corresponde al identificador de las instancias del conjunto de datos y cual corresponde a la variable clase.","c31cc661":"## Construcci\u00f3n y validaci\u00f3n del modelo final\nUna vez hemos optimizado los hiperpar\u00e1metros de todos los modelos resultantes de los algoritmos estudiados, procedemos a encontrar el modelo final en base a su medida de rendimiento al generalizar y en caso de empates nos guiaremos por la navaja de Ockham mirando el coste computacional o la complejidad de interpretaci\u00f3n\/construcci\u00f3n. Para medir el rendimiento usaremos el conjunto de datos de prueba que hemos aislado de la validaci\u00f3n cruzada al principio del proceso. ","21e09143":"### Gradient Boosting","5f97ec3e":"Creamos el nuevo **pipeline** y probamos las diferentes combinaciones.","2d448e0f":"De nuevo nos encontramos con **n\u00famero de estimadores** con lo que seguiremos la misma l\u00f3gica hasta ahora con los ensembles, siempre teniendo cuidado con el coste temporal que implica crear muchos \u00e1rboles. Tenemos tambi\u00e9n la **tasa de aprendizaje** que para este caso concreto hay una norma general y es que siempre se suelen fijar valores por debajo de `0.1`. Esto se debe a que si damos valores m\u00e1s altos que este, los \u00e1rboles que se van a\u00f1adiendo al modelo hacen menos correcciones y crecen r\u00e1pidamente lo que acaba resultando en un sobreajuste final. Por lo que nuestra intenci\u00f3n es relantizar ese crecimiento y llegar a un punto de balance con el n\u00famero de estimadores. Por otro lado, jugamos con el tama\u00f1o de los conjuntos de datos con los que se construyen los \u00e1rboles, algo similar a los `bootstrap sets`, por lo que hemos encontrado es que suelen funcionar mejor conjuntos ligeramente inferiores respecto al tama\u00f1o de la base de datos con la que se trabaja. Y ya los hiperpar\u00e1metros que quedan son los encargados de darles caracter\u00edsticas a los \u00e1rboles de decisi\u00f3n. Como siempre nos quedamos con el **criterio** con el que se contruyen, ahora es diferente al \u00edndice gini y la entrop\u00eda dado que no clasificamos sino que calculamos residuos o errores, y tambi\u00e9n su profundidad. Para esta \u00faltimo, suelen darse valores peque\u00f1os, ya que si los ramificamos enteramente al irlos agregando al modelo final r\u00e1pidamente producirr\u00eda sobre ajuste y lo que queremos es que se \"fijen\" en las variables m\u00e1s relevantes.","9fcc1955":"Viendo los resultados obtenidos, la teor\u00eda ha dado en el clavo. El mejor modelo se compone del m\u00e1ximo n\u00famero de estimadores que se ha asignado, el `subsample` es ligeramente inferior al n\u00famero de instancias totales, los \u00e1rboles que se han ido agregando al modelo son simples (un nivel solamente) y ha triunfado una tasa de aprendizaje en el l\u00edmite de lo que se suele establecer. Un siguiente paso que se podr\u00eda dar es encontrar ese balance entre `n_estimators` y `learning_rate` del que hemos hablado, pero para ello debemos asumir el coste computacional que conlleva incrementar un hiperpar\u00e1metro y rebajar el otro respectivamente. En nuestro caso, hemos intentado limitar un poco estos, ya que a partir de 150 estimadores el tiempo aumentaba considerablemente, pero sabemos que podr\u00eda mejorar aun m\u00e1s la puntuaci\u00f3n.","18a1e413":"Y de nuevo hacemos las comprobaciones necesarias antes de pasar a trabajar con los conjuntos de datos","7774539d":"### Adaptive Boosting (AdaBoost)","16cf4f37":"Veamos un ejemplo de muestra aleatoria con la funci\u00f3n `sample` para ver que se ha cargado correctamente.","d5f7ce0d":"Para llevar a cabo esta tarea haremos uso del algoritmo de b\u00fasqueda en *grid* que proporciona `scikit-learn`. B\u00e1sicamente en lo que consiste es en un m\u00e9todo por fuerza bruta. Realiza una b\u00fasqueda exhaustiva mediante validaci\u00f3n cruzada probando todas las combinaciones de hiperpar\u00e1metros que se le hayan proporcionado previamente.","6fa31c9b":"Se trata de un modelo experimental que son m\u00e1s r\u00e1pidos que el **Gradient Boosting** anterior cuando el n\u00famero de instancias ronda un magnitud mayor a los diez mil. En otras palabras, est\u00e1 optimizado para bases de datos enormes comparadas con la nuestra.","752ed572":"#### Adaptive Boosting (AdaBoost)","dd184b57":"Volvemos a crear el **pipeline**, pero esta vez le pasamos como paso final el \u00e1rbol de clasificaci\u00f3n.","627dbd99":"#### Arboles de decisi\u00f3n\n","1a8a2263":"### Boosting Aggregation (Bagging)","fb6de710":"### Random Forest","b44aef36":"Mirando los resultados del `grid-search` nos damos cuenta la funci\u00f3n de p\u00e9rdida que mejor ha funcionado es la `binary_crossentropy` y esto se debe principalmente a que en esta base de datos el n\u00famero de clases es dos as\u00ed que era un tanto evidente que dar\u00eda mejor puntuaci\u00f3n que la categ\u00f3rica que se emplea para problemas multiclase. La tasa de aprendizaje es la m\u00ednima que hemos establecido, as\u00ed que cumple con lo esperado. Por \u00faltimo, el valor m\u00ednimo de instancias en las hojas de los \u00e1rboles ronda entorno al valor por defecto. Una cosa que llama la atenci\u00f3n es que con valores por debajo de estos aparecen `NaN`, algo que no entendemos del todo porque puede ocurrir.","6280632a":"Si nos vamos a la p\u00e1gina de `sci-kit` sobre los \u00e1rboles de decisi\u00f3n veremos que hay muchos hiperpar\u00e1metros que se pueden ajustar. Todos tienen que ver con el control de la ramificaci\u00f3n de los \u00e1rboles, ya que estos a pesar de ser muy f\u00e1ciles de entender una vez construidos, tienden a sobreajustar f\u00e1cilmente sobre el conjunto de datos de entrenamiento. Por nuestra parte, hemos decidido incluir un hiperpar\u00e1metro m\u00e1s de los que se nos dan en la pr\u00e1ctica. Con `min_samples_split` se controla la creaci\u00f3n de nodos internos, es decir, es el m\u00ednimo n\u00famero de ejemplos que debe haber en un nodo interno para que pueda ser divido. Esto impl\u00edcitamente afectar\u00e1 a otros hiperpar\u00e1metros como `max_depth`. En cuanto a la elecci\u00f3n de valores de los hiperpar\u00e1metros nos guiamos por lo que ya sabemos. Un valor de `max_depth` muy grande sobreajustar\u00e1, as\u00ed que lo mantenemos en valores que consideremos peque\u00f1os acorde a nuestras instancias y variables de la base de datos. Para `min_samples_split` suele rondar los valores hasta `40`, pero nos ha parecido un n\u00famero elevado dado que el n\u00famero de instancias no es demasiado alto. Tiene el mismo problema que otros hiperpar\u00e1metros de los \u00e1rboles de clasificaci\u00f3n. Si su valor es peque\u00f1o, nos lleva a un sobre ajuste (muchas hojas con pocos ejemplos), pero si es un valor muy grande podemos provocar que el \u00e1rbol no aprenda, es decir, hojas muy \"generalistas\" que se podr\u00edan dividir en hojas m\u00e1s espec\u00edficas de las que extraer informaci\u00f3n relevante.","a614c33b":"A diferencia del boosting, en el **bagging** se deja a los \u00e1rboles ramificar hasta el final. Es por eso, por lo que no controlamos su profundidad sino que los dejamos crecer hasta el final. Tan solo jugamos con el criterio con el que se construyen. En cuanto a los hiperpar\u00e1metros \"directos\" de este ensemble tenemos el **n\u00famero de estimadores** y el tama\u00f1o de los conjuntos de datos (boostraps) que usan los \u00e1rboles para entrenar. Por lo general, a mayor n\u00famero de clasificadores, mejores resultados se obtienen. La intuici\u00f3n nos dice que esto puede sugerir un sobre-ajuste, pero no es el caso ya que reducen las varianza al computar m\u00e1s votos entre los distintos clasificadores que se crean. El hiperpar\u00e1metro que controla el tama\u00f1o de los bootstraps es `max_samples` que se expresa en porcentajes acorde al tama\u00f1o de la base de datos con la que se trata. Por defecto est\u00e1 fijado a uno lo que representa que estos conjuntos tienen el mismo tama\u00f1o que la base de datos. Normalmente se deja as\u00ed, pero el uso de tama\u00f1os m\u00e1s peque\u00f1os puede suponer el incremento de la varianza de los \u00e1rboles de decisi\u00f3n resultantes, lo cual se supone como una mejora en el rendimiento global. Por \u00faltimo, estos conjuntos de datos se pueden hacer con reemplazo o no, de ah\u00ed que probemos las dos alternativas para ver cual resulta ser mejor en este caso, aunque por lo general siempre se utiliza la  t\u00e9cnida del reemplazo. Dicho esto, podemos comprobar que nuestras hip\u00f3tesis se cumplen dado que los conjuntos de datos tiene un tama\u00f1o de 20 % de 537 = 107.4 instancias lo cual es una porci\u00f3n significativa inferior al total del conjunto de entrenamiento y que se han construido con el m\u00e9todo de reemplazo. Por otro lado tenemos que cuantos m\u00e1s \u00e1rboles se crean mejor, se podr\u00eda ver en qu\u00e9 n\u00famero empieza a estabilizarse la puntuaci\u00f3n para poder elegir el n\u00famero perfecto de estos para este problema en concreto. De momento nos quedaremos con los 100 que hemos establecido.","262e367e":" Los hiperp\u00e1rametros obtenidos son totalmente iguales a los resultantes derivados del problema de **Diabetes**, exceptuando que en vez de 100 estimadores emplea solamente 75. Esto es un claro indicador de que la teor\u00eda nos da un apoyo s\u00f3lido en cuanto a la asignaci\u00f3n de valores para los hiperpar\u00e1metros por probar, independientemente del tipo de problema. Eso s\u00ed, es muy importante saber qu\u00e9 cosas espec\u00edficas retocar para cada caso con el fin de obtener mejores resultados.","1f949f59":"#### Bootstrap Aggregating (Bagging)\n","bbf4870d":"## Carga de datos\nEmpezaremos por la base de datos de **Pima-Indians Diabetes** que utilizamos en la anterior pr\u00e1ctica.","763dfd30":"Y comprobamos que se haya realizado completamente. Primero, las variables predictoras:","2b90db34":"## Selecci\u00f3n de modelos\nEn esta secci\u00f3n lo que haremos ser\u00e1 una evaluaci\u00f3n individual de los modelos que genera cada algoritmo propuesto. Intentaremos encontrar los hiperpar\u00e1metros que mejor se amoldan con el fin de seleccionar el modelo que m\u00e1s nos interesa. Adem\u00e1s de ello comentaremos el funcionamiento y resultados de cada algoritmo, as\u00ed como las salidas que vayamos obteniendo a lo largo del proceso.","c93136c8":"#### Vecinos m\u00e1s cercano\nEmpezamos con el algoritmo de los vecinos m\u00e1s cercanos. Primeramente creamos el **pipeline** con el procesamiento de datos que ya hicimos en la pr\u00e1ctica pasada, solo que ahora a\u00f1adimos como paso final el algoritmo para poder as\u00ed evaluar los diferentes modelos que crea en base a los hiperpar\u00e1metros. Estos \u00faltimos los establecemos acorde a nuestro conocimiento de la teor\u00eda, as\u00ed como por las conclusiones sacadas del an\u00e1lisis exploratorio, ya que no tiene sentido perder el tiempo probando combinaciones absurdas. Una vez hecho todo esto invocaremos la funci\u00f3n de b\u00fasqueda en grid que se nos proporciona con los par\u00e1metros correspondientes. En este \u00faltimo paso, es muy importante fijar las m\u00e9tricas de evaluaci\u00f3n apropiadas para el problema dado,  as\u00ed como indicar los hiperpar\u00e1metros que debe usar el algoritmo.","9d9245e6":"## 2. Acceso y almacenamiento de datos","a7fb2305":"Como esta version del **Gradient Boosting** realiza un discretizado interno, no permite el uso de discretizadores en pasos previos del **pipeline** por lo que hay dos opciones, o bien eliminar nuestros discretizador, o bien la opci\u00f3n que hemos elegido para este caso que implica el uso de **DenseTransformer**. B\u00e1sicamente lo que hace es convertir `sparse_matrix` (matriz llena de ceros) en `dense_matrix` para que pueda ejecutarse correctamente.","ac0b1c3d":"Todo correcto. Pasemos a realizar la divisi\u00f3n de conjunto de entrenamiento y prueba con la t\u00e9cnica de holdout estratificado","7b00fe0c":"Como podemos observar en la muestra, tenemos una columna sin nombre (`Unnamed: 32`) que no tiene datos para niguna de las instancias, por lo tanto la eliminaremos puesto que no nos aporta nada.","7071da7b":"Comprobamos que se han separado correctamente:","ad6070d7":"Y por otro la variable clase","c79b658a":"Vamos a separar nuestro conjunto de datos en dos subconjuntos, por un lado las variables predictoras y por otro lado la variable objetivo.","71916c90":"Comenzamos con los ensemebles. El procedimiento con el **pipeline** es exactamente el mismo que antes. Algo importante a tener en cuenta es el `base_estimator` que vamos a utilizar. Lo normal para este algoritmo es construir \u00e1rboles sencillos, pero tambi\u00e9n podemos pasar otro tipo \"learners\". Sin embargo, como no los conocemos bien y se escapan de la envergadura de la pr\u00e1ctica, tan solo usaremos los \u00e1rboles de decisi\u00f3n como clasificadores que van a componer el ensemble.","a3d20f77":"## Selecci\u00f3n de modelos","c10b86ef":"Al igual que en **Diabetes**, la teor\u00eda vuelve a dar en el clavo. Obtenemos el m\u00e1ximo de estimadores que se ha asignado, un subsample ligeramente inferior al n\u00famero de instancias, los \u00e1rboles esta vez tienen una profundidad de `2` y tenemos una tasa de aprendizaje en el l\u00edmite de lo que suele establecer: `0.1`. Con todo esto podemos volvemos a recalcar la importancia de saber c\u00f3mo funcionan los algoritmos por dentro con el fin de tener una intuici\u00f3n base sobre los resultados a obtener.","dd370b35":"# Wisconsin","539397af":"#### Gradient Boosting","e408183d":"Comprobamos las dos particiones ","5ee6fa5d":"Como hiperpar\u00e1metros principales o \"directos\" tenemos el **n\u00famero de estimadores** que se utiliza para la construcci\u00f3n del ensemble y la **tasa de aprendizaje** que determina la contribuci\u00f3n de los clasificadores a la combinaci\u00f3n final. Tenemos que tener en cuenta la compensaci\u00f3n que existe entre estos dos hiperpar\u00e1metros. Por lo general cuando se usa un alto n\u00famero de estimadores, se emplea una tasa muy baja. Esto no siempre implica buenos resultados, pero s\u00ed aumenta el coste computacional. Es muy com\u00fan usar valores entre cero y uno para la tasa de aprendizaje, ya que de la otra manera (mayor que uno) suele dar problemas de sobre-ajuste. Hemos experimentado asignando varios valores bastante bajos, menores que `0.3`, y hemos obtenido mejores modelos, pero el tiempo de ejecuci\u00f3n de la b\u00fasqueda en grid se disparaba mucho, as\u00ed que hemos dejado los dos posibles extremos para ver el efecto que tiene y poder comparar los resultados obtenidos en un tiempo razonable. Por otro lado, est\u00e1n los hiperpar\u00e1metros \"indirectos\" que vienen a ser los que necesita el `base_estimator`, es decir, los \u00e1rboles de decisi\u00f3n que conforman el ensemble. As\u00ed que, como ya sabemos la teor\u00eda, la intenci\u00f3n principal de este tipo de ensemble es unir clasificadores muy simples por lo que la profundidad de los \u00e1rboles no debe ser muy grande. Y ya por otro lado podemos jugar con el criterio que se sigue para su construcci\u00f3n. No hemos incluido m\u00e1s par\u00e1metros para el `base_estimator` dado que el principal es la profundidad y a\u00f1adir m\u00e1s combinaciones de hiperpar\u00e1metros implica un incremento considerable del tiempo de ejecuci\u00f3n. Queremos encontrar modelos representativos, pero siempre pensando tambi\u00e9n en el coste de estos adem\u00e1s de su complejidad. ","c23ec571":"### Histogram Gradient Boosting","c84903e9":"**\u00a1Aclaraci\u00f3n!**\nAntes, cuando hemos mencionado la creaci\u00f3n de modelos, nos referimos en general y no a este primer algoritmo, ya que este en concreto no crea ninguno sino que utiliza toda la base de datos para hacer sus predicciones.","8f71f8ab":"Para el algoritmo de los vecinos cercanos tenemos \u00fanicamente dos hiperpar\u00e1metros que ajustar: **funci\u00f3n distancia** y **n\u00famero de vecinos**. Por defecto en el paquete de `sci-kit` est\u00e1n fijados a `minkowski` y `5`, respectivamente. De la teor\u00eda sabemos que la forma de medir la distancia tiene un impacto sobre los resultados obtenidos, en este caso la mejor soluci\u00f3n encontrada es a trav\u00e9s de `manhattan` que es de las maneras m\u00e1s simples de calcular la distancia entre dos puntos. Por otro lado, tenemos lo que se conoce como pesos, esto se refiere a la a como se realiza la predicci\u00f3n, hay dos posibilidades `uniform` todos los vecinos cercanos tienen la misma importancia o `distance` lo que da un peso proporcional a la inversa de la distancia desde el punto que se busca clasificar. En nuestro caso, la mejor opci\u00f3n es la de asignar el mismo peso a todos. Por otro lado, tenemos el ajuste del par\u00e1metro $k$ y para ello nos valemos de la opci\u00f3n secuencial, es decir, vamos probando valores en secuencia hasta un m\u00e1ximo de la ra\u00edz cuadrada de los ejemplos totales de nuestra base de datos $\\sqrt{537} = 23.17$ Los valores que damos son impares a prop\u00f3sito de evitar posibles confusiones en situaciones de empate. En este caso el mejor valor encontrado es `19` que es un poco menor que el m\u00e1ximo propuesto. Sabemos de la teor\u00eda que un n\u00famero peque\u00f1o de $k$ hace al clasificador muy sensible al ruido y tambi\u00e9n lo que ocurre es lo que conocemos como **underfitting**, es decir, se trata de un modelo muy simple e insuficiente para captar la informaci\u00f3n que nos dan las variables. Por eso directamente hemos saltado esos valores peque\u00f1os por debajo de 5 porque sabemos de antemano que por lo general dan malos resultados. Otra ser\u00eda la situaci\u00f3n si la base de datos fuese realmente peque\u00f1a, pero no es el caso. En cambio, un valor de $k$ muy alto suprime ese ruido diluyendo las fronteras de decisi\u00f3n, pero el problema que surge es el **sobre-ajuste** al conjunto de entrenamiento, de ah\u00ed que limitemos el incremento de su valor. ","eb0e8558":"Comprobamos que se ha cargado correctamente con una muestra","90b859cb":"De un plumazo podemos observar que las puntuaciones para este problemas son mucho m\u00e1s altas que en el anterior caso. Como ya hemos comentado, la m\u00e9trica predefinida es `roc_auc_score`as\u00ed que ser\u00e1 en la cual nos fijemos primordialmente. Dicho esto, el claro vencedor es el modelo de los **vecinos m\u00e1s cercanos** que gana con cierta ventaja a los restantes, no s\u00f3lo en puntuaci\u00f3n sino en su simplicidad. Algo que nos ha sorprendido totalmente, dado que sabemos el potencial ofrecido por los ensembles para cualquier tipo de problema. Ahora bien, si nos fijamos en la puntuaci\u00f3n obtenida con la m\u00e9trica del `recall` vemos que la cosa no est\u00e1 del todo clara, ya que hay m\u00faltiples empates. Eso s\u00ed, el rendimiento de todos sigue estando ligeramente por debajo que con nuestra m\u00e9trica establecida. En ese caso nos quedar\u00edamos con el **\u00e1rbol de decisi\u00f3n** ya que es un modelo mucho m\u00e1s f\u00e1cil de entender y de explicar que un ensemble.","1f1aeba3":"Los modelos resultantes del algoritmo de los vecinos cercanos mantenemos los mismos valores para los hiperpar\u00e1metros a excepci\u00f3n del n\u00famero de vecinos a buscar, es decir **k**. Debemos dar unos valores acordes a este nuevo conjunto de datos, usamos de nuevo una secuencia de valores impares, pero esta vez con m\u00e1ximo de `19` que se obtiene de $\\sqrt{398} = 19.95$.  \n\nIgual que el anterior caso, la mejor opci\u00f3n es asignar el mismo peso a todos: `uniform`. En cambio, la manera de medir la distancia que mejor nos conviene es la `euclidean` por lo que vemos efectivamente es que la funci\u00f3n de distancia tiene diferente impacto dependiendo del problema del que se trata y debe elegirse cuidadosamente.","5223982a":"El mejor modelo ha usado como criterio para la divisi\u00f3n de nodos la entrop\u00eda y no ha conseguido una profundidad muy grande, sino que se ha quedado en tres niveles. Esto quiere decir que el \u00e1rbol no ha crecido en complejidad y tampoco ha sobreajustado si nos fijamos en la puntuaci\u00f3n obtenida para el entrenamiento. Una de las razones por las que puede no haber creado otro nivel es que `min_sample_split` lo ha impedido. Otra cosa que notamos es que su valor tampoco es muy alto, cumpliendo nuesta hip\u00f3tesis de no asignarle valores grandes. Por otro lado, tampoco se ha realizado demasiadas podas dado que el mejor valor de `ccp_alpha` es literalmente cero, lo cual indica que la complejidad del \u00e1rbol creado no es elevada. Por \u00faltimo, nos queda comentar que de nuevo con la m\u00e9trica de `recall` se obtienen puntuaciones bastante malas, en cambio con `roc_auc` rondamos los mismos valores que con el anterior algoritmo.","eed685db":"Y despu\u00e9s el conjunto de datos de prueba:","fd7211a6":"\nComo podemos observar para elegir la cantidad de variables que usar\u00e1n los \u00e1rboles se usa la ra\u00edz del total de variables, lo cual es lo m\u00e1s t\u00edpico. La profundidad obtenida es de `5` lo cual es relativamente poco profundo en comparaci\u00f3n a un \u00e1rbol totalmente ramificado. Y por \u00faltimo, volvemos a obtener un n\u00famero alto de estimadores cumpli\u00e9ndose una vez m\u00e1s la regla de oro de los ensembles.","59381438":"Al igual que antes, creamos el **pipeline**, pero ahora como paso final lo que pasamos es el \u00e1rbol de clasificaci\u00f3n de `sci-kit`. Es lo mismo que haremos con los dem\u00e1s algoritmos. Muy importante establecer el `random_state` para que no haya variaciones en los resultados obtenidos.","d46da186":"Con un r\u00e1pido vistazo se puede ver que los mejores clasificadores han sido los ensembles con cierta ventaja respecto los dem\u00e1s. Al igual que antes usamos diferentes m\u00e9tricas de evaluaci\u00f3n del rendimiento de los clasificadores. En este caso, el **Gradient Boosting** es el que mejor ha funcionado si nos fijamos en el `roc_auc_score` que es la m\u00e9trica que hemos elegido como predeterminada en este problema. Sin embargo, si nos fijamos en el `recall_score` el vencedor ser\u00eda su variante optimizada **Histogram Gradient Boosting**, pero con esta m\u00e9trica los resultados parecen no ser del todo buenos, ya que aun hay margen significativo de mejora para todos los clasificadores. Al no haber empate, nos quedamos directamente con el claro vencedor, **Gradient Boosting** como modelo final.","10776c84":"Como siempre, dividimos en variables predictoras (`X`) y variable clase (`y`)","f54bf3e8":"Antes de empezar con la b\u00fasqueda en grid, debemos especificar el tipo de validaci\u00f3n cruzada que vamos a emplear. En nuestro caso nos hemos decantado por una $5 x 10 - cv$ estratificada.","2e597a16":"Utilizaremos la validaci\u00f3n cruzada usada anteriormente.","2747d24b":"Realizamos el holdout para poder tener un conjunto de entrenamiento y por otro lado uno de prueba. Este \u00faltimo solo lo usaremos al final del proceso, mientras tanto el aprendizaje de modelo se har\u00e1 con el de entrenamiento.","74abcdb7":"### Vecinos m\u00e1s cercanos","4608c7fd":"Analizando los resultados obtenidos podemos ver que el ensemble utiliza \u00e1rboles de la profundidad m\u00ednima, un solo nivel, lo cual corrobora la teor\u00eda de usar estimadores muy simples para la construcci\u00f3n del modelo final. Por otro lado, se ha encontrado un balance entre la tasa de aprendizaje y el n\u00famero de estimadores de `0.1` y `50`, respectivamente. Este ejemplo se sale de lo normal en cuanto al n\u00famero de estimadores, ya que rompe la regla de cuantos m\u00e1s, mejor, y esto puede ser causa directa del valor asignado a la tasa de aprendizaje.","f3a8a684":"No hay mucho escrito aun sobre este ensemble, as\u00ed que nos hemos guiado principalmente por lo que hay en la p\u00e1gina de `sci-kit`. Se trata de una versi\u00f3n optimizada del **Gradient Boosting** para bases de datos muy grandes, es por eso que muchos de sus hiperpar\u00e1metros se mantienen, en cambio otros no y aparecen otros nuevos. La **tasa de aprendizaje** sigue estando, as\u00ed que seguiremos la misma estrategia que antes de asignar valores menores a `0.1`. El **n\u00famero de estimadores** se ha convertido en `max_iter` y viene a controlar las iteraciones que realiza el proceso de boosting. Su objetivo es en parte mantener en unos m\u00e1rgenes razonables el coste computacional que conlleva su ejecuci\u00f3n. Hay m\u00e1s hiperpar\u00e1metros como este que tienen la funci\u00f3n de controlar la condici\u00f3n de parada del ensemble, pero solo nos centraremos en este en concreto. Y ya lo que quedar\u00edan son los hiperpar\u00e1metros encargados de controlar las caracer\u00edsticas de los \u00e1rboles que se van construyendo. Antes usabamos `max_depth` como par\u00e1metro principal, pero esta vez emplearemos `min_samples_leaf`, ya que en la documentaci\u00f3n expresa expl\u00edcitamente que para bases de datos peque\u00f1as como la nuestra es necesario controlarlo para evitar que se creen \u00e1rboles poco profundos. El valor por defecto es 20, as\u00ed que hemos dado valores en un rango entorno a este valor.","5bee1ac9":"# Comentarios de un kernel\nHemos decidido separar esta parte de la pr\u00e1ctica para recortar la longitud del cuaderno y ganar un poco de claridad en cuanto a las explicaci\u00f3n. Este enlace te lleva a la reproducci\u00f3n del [kernel](https:\/\/www.kaggle.com\/nikoladyulgerov\/titanic-getting-better-eda-top-14-4932eb) que hemos elegido.\n\n","2fc09ef5":"Como es l\u00f3gico, la funci\u00f3n de p\u00e9rdida que mejor funciona es la `binary_crossentropy` ya que este problema tiene solo dos clases. De nuevo el \"n\u00famero de estimadores\" es el m\u00e1s alto posible, sin embargo la tasa de aprendizaje baja respecto al **Gradient Boosting**. En cuanto a la partici\u00f3n de nodos de los \u00e1rboles de decisi\u00f3n el mejor valor obtenido es ligeramente m\u00e1s alto de lo esperado. Esto implica de forma un poco m\u00e1s indirecta que si miramos por ejemplo `max_depth` de que los \u00e1rboles construidos \nson m\u00e1s simples dado que sus nodos acaparan mas ejemplos.","cf3c4632":"## Preliminares \nComo siempre lo primero de todo ser\u00e1 cargar los m\u00f3dulos que vamos a usar:","a275cfa2":"Como comentamos en el ejemplo de **Pima-Indians Diabetes** creamos el pipeline con el procesamiento de datos al cu\u00e1l le a\u00f1adiremos como paso final el algoritmo para evaluar los diferentes modelos en base a los hiperpar\u00e1metros. En este caso el **pipeline** tiene m\u00e1s transformadores que ejecutar, pero la met\u00f3dica de trabajo es exactamente la misma.","261be879":"Al igual que hemos realizado para *Pima-Indians Diabetes* en esta secci\u00f3n haremos una evaluaci\u00f3n individual de los modelos buscando la mejor combinaci\u00f3n de hiperpar\u00e1metros para cada modelo.","a3f28d52":"En cuanto al m\u00e9todo de evaluaci\u00f3n nos hemos decantado por la m\u00e9trica `roc_auc`, ya que es la que usamos en la pr\u00e1ctica anterior. Cuanto m\u00e1s cerca sea la puntuaci\u00f3n a uno, mejor es el modelo. Para contrastar esta parte y tener una visi\u00f3n m\u00e1s global, usaremos tambi\u00e9n el `recall` como otra alternativa que en este caso los resultados obtenido evaluados con esta m\u00e9trica son bastante malos, ya que al igual que la otra m\u00e9trica cuanto m\u00e1s se acerque el valor a uno mejor es y con esta giran entorno al `0.56` lo que est\u00e1 bastante lejos."}}