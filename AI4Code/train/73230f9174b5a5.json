{"cell_type":{"a7536009":"code","732333a2":"code","7a3d7747":"code","32b8dafd":"code","9647c7ee":"code","ca3b26cb":"code","41e0bc1a":"code","ce014ae2":"code","162299db":"code","c8b87291":"code","f09847e7":"code","5cbc4262":"code","4ab53641":"code","c70683a6":"code","45758fd7":"code","f986ccd5":"code","33f1d9cf":"code","a4e8c618":"code","2a09e515":"code","6dd52353":"code","c573b4d6":"code","d2235bbf":"code","d92c5fae":"code","87e803d9":"code","7b2c68bd":"code","6bf5a41c":"code","d760e34f":"code","4fd2be7a":"code","a40fb66c":"code","0337f942":"code","7c75d8ad":"code","1e8000c4":"code","3999508a":"code","1627d03e":"code","351dfa9a":"code","f4860082":"code","67b6ddba":"code","257edb58":"code","b29708d3":"code","e77d5702":"code","bc3ffc7e":"markdown","901e0d1c":"markdown","09ce083f":"markdown","5eb938c8":"markdown","9235489e":"markdown","231abc3e":"markdown"},"source":{"a7536009":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #plot graph\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","732333a2":"#hidden_layer is node in hidden layer\n#hidden_layer sample [5,5,X] X for clasification = number of class\ndef neural_network_create_weight_bias(feature_list, hidden_layer):\n    #create empty weight list\n    weight = []\n    #create empty bias list\n    bias = []\n    #for in layer of hidden layer\n    for i in range(len(hidden_layer)):\n        \n        #create bias_i \n        #bias appear in hidden_layer only\n        #create bias_i row = 1,column = number of node of  hidden_layer[i]\n        bias_i_column = hidden_layer[i]\n        #create bias_i\n        bias_i = np.random.randn(1, bias_i_column)\n        \n        #create weigth_i\n        if i <= 0:\n            #first weigth_i\n            #create weigth_i row = column of feature\n            weight_i_row = feature_list.shape[1]\n            \n        else:\n            #other weigth_i\n            #create weigth_i row = number of node of hidden_layer[i-1]\n            weight_i_row = hidden_layer[i-1]\n        \n        #create weigth_i column = number of node of  hidden_layer[i]\n        weight_i_column = hidden_layer[i]            \n        #create weigth_i\n        weight_i = np.random.randn(weight_i_row, weight_i_column)\n        \n        #improve weight and bias\n        #improve bias_i using devine by square root of number of node in hidden_layer[i]\n        bias_i = bias_i\/np.sqrt(hidden_layer[i])\n        #improve weight_i using devine by square root of number of node in hidden_layer[0]\n        weight_i = weight_i\/np.sqrt(hidden_layer[0])\n        \n        #add list\n        #add bias_i to bias list\n        bias.append(bias_i)\n        #add weight_i to weight list\n        weight.append(weight_i)\n        \n    return weight, bias","7a3d7747":"def neural_network_forward(feature_list, weight, bias, activate_function):\n    #create empty list of output\n    output = []\n    #create empty list of activated_output\n    activated_output = []\n    \n    #for activate_function\n    for i in range(len(activate_function)):\n        \n        if i <= 0:\n            #first layer calculate output_i from feature_list\n            output_i = np.dot(feature_list, weight[i]) + bias[i]\n        else:\n            #first layer calculate output_i from last activated_output\n            output_i = np.dot(activated_output[-1], weight[i]) + bias[i]\n        \n        #compute_activated_output by activate_function[i]\n        activated_output_i = neural_network_compute_activated_output(output_i, activate_function[i])\n                \n        #add output_i and activated_output_i to list\n        output.append(output_i)\n        activated_output.append(activated_output_i)\n        \n    return output,activated_output","32b8dafd":"def neural_network_compute_activated_output(output_i, activate_function):\n    if type(activate_function) == str:\n        \n        if activate_function == 'sigmoid':\n            #sigmoid : activated_output_i = 1\/(1+e^(-output_i))\n            activated_output_i = 1\/(1 + np.exp(-output_i))\n            \n        elif activate_function == 'tanh':\n            #hyperbolic tangent : activated_output_i = ((e^output_i)-(e^-output_i))\/((e^output_i)+(e^-output_i))\n            activated_output_i = (np.exp(output_i) - np.exp(-output_i))\/(np.exp(output_i) + np.exp(-output_i))\n            \n        elif activate_function == 'ReLU':\n            #rectified linear unit : activated_output_i = if output_i <= 0 : 0, if output_i > 0 : output_i\n            activated_output_i = output_i * (output_i > 0)\n            \n        elif activate_function == 'softmax':\n            #softmax : e^output_i\/sum(e^output_i)\n            activated_output_i = np.exp(output_i)\/np.exp(output_i).sum(axis=1, keepdims = True)\n            \n    elif type(activate_function) == list:\n        \n        if activate_function[0] == 'PReLU':\n            #parametric rectified linear unit : activated_output_i = if output_i <= 0 : output_i * new_slope , if output_i > 0 : output_i \n            #remark slope != 1\n            activated_output_i = output_i * (output_i > 0) + activate_function[1] * output_i * (output_i <= 0)\n            \n    return activated_output_i","9647c7ee":"def neural_network_compute_different(output_i, activated_output_i, activate_function_i):\n    \n    if type(activate_function_i) == str:\n        \n        if activate_function_i == 'sigmoid':\n            different_i = activated_output_i * (1 - activated_output_i)\n            \n        elif activate_function_i == 'tanh':\n            different_i = 1 - activated_output_i**2\n            \n        elif activate_function_i == 'ReLU':\n            different_i = (output_i > 0)\n            \n    elif type(activate_function_i) == list:\n        \n        if activate_function_i[0] == 'PReLU':\n            different_i = (output_i > 0) + activate_function_i[1] * (output_i <= 0)\n            \n    return different_i","ca3b26cb":"def neural_network_compute_error(delta_i, different_i):\n    error_i = delta_i * different_i\n    return error_i","41e0bc1a":"def find_error(target_list, output, error_type):\n    \n    if error_type == 'SSE':\n        error = find_sum_square_error(target_list, output)\n        \n    elif error_type == 'MSE':\n        error = find_mean_square_error(target_list, output)\n        \n    elif error_type == 'MAE':\n        error = find_mean_absolute_error(target_list, output)\n        \n    elif error_type == 'MAPE':\n        error = find_mean_absolute_percentage_error(target_list, output)\n        \n    elif error_type == 'Entropy':\n        error = find_entropy_error(target_list, output)\n        \n    elif error_type == 'Binary':\n        error = find_binary_class_error(target_list, output)\n        \n    elif error_type == 'Multiclass':\n        error = find_multi_class_error(target_list, output)\n        \n    return error","ce014ae2":"def find_sum_square_error(target_list, output):\n    sum_square_error = ((target_list - output)**2).sum()\n    return sum_square_error","162299db":"def find_mean_square_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    sum_square_error = ((target_list - output)**2).sum()\n    mean_square_error = sum_square_error\/number_of_sample\n    return mean_square_error","c8b87291":"def find_mean_absolute_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    mean_absolute_error = (np.abs(target_list - output)).sum()\/number_of_sample\n    return mean_absolute_error","f09847e7":"def find_mean_absolute_percentage_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    mean_absolute_percentage_error = np.abs((target_list - output)\/target_list).sum()*100\/number_of_sample\n    return mean_absolute_percentage_error","5cbc4262":"def find_entropy_error(target_list, output):\n    log_output = np.log(output)\n    entropy_error = (-target_list*log_output).sum()\n    return entropy_error","4ab53641":"def find_binary_class_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    _target_list = np.round(target_list, 0)\n    _output = np.round(output, 0)\n    binary_class_error = 100*(_target_list != _output).sum()\/number_of_sample\n    return binary_class_error","c70683a6":"def find_multi_class_error(target_list, output):\n    number_of_sample = target_list.shape[0]\n    argmax_of_target_list = np.argmax(target_list, axis=1)\n    argmax_of_output = np.argmax(output, axis=1)\n    multi_class_error = 100*(argmax_of_target_list != argmax_of_output).sum()\/number_of_sample\n    return multi_class_error","45758fd7":"def create_onehot_target(label):\n    \n    #define unique_label for column\n    unique_label = len(np.unique(label))\n    \n    #define number_of_label for row\n    number_of_label = label.shape[0]\n    \n    #create zeros metrix column = number_of_label, row = unique_label\n    onehot = np.zeros([number_of_label, unique_label])\n    \n    for i in range(number_of_label):\n        #add 1 at label type for each row in zeros metrix\n        onehot[i, label[i]] = 1\n        \n    return onehot","f986ccd5":"def neural_network_classification_find_weight_bias(feature_list, target_list, hidden_layer, activate_function, weight = [], bias = [], epoch = 1000, learning_rate = 0.01, lambda1 = 0, lambda2 = 0, dropout = False, prob_drop = [0]):\n   \n    #number_of_layer = length of hidden_layer\n    number_of_layer = len(hidden_layer)\n    #number_of_trianing_data = row of feature_list\n    number_of_trianing = feature_list.shape[0]\n    \n    #if weight == [] create a new one\n    if not weight:        \n        weight, bias = neural_network_create_weight_bias(feature_list, hidden_layer)\n        \n    #create empty error_list\n    error_list = []\n    percent = 0\n    for i in range(epoch):\n        \n        #print progress\n        new_percent = int(i*100\/epoch)\n        if(new_percent > percent):\n            percent = new_percent\n            print(percent)\n            \n        #calculate output and activated output for each hidden layer\n        output, activated_output = neural_network_forward(feature_list, weight, bias, activate_function)\n        \n        #find error using output of last layer\n        #find error by entropy error using \"Entropy\"\n        error = find_error(target_list, activated_output[-1], 'Entropy')\n         #collact error in error_list for error trend\n        error_list.append(error)\n        \n        #calculate slope of weight and bias by backpropagation\n        slope_of_weight, slope_of_bias = neural_network_classification_backpropagation(feature_list, weight, bias, output, activated_output, target_list, activate_function, dropout, prob_drop)\n        \n        #loop = number of layer for create new weight,bias using gradient descent\n        for layer_i in range(number_of_layer):\n            #gradient descent weight_new = weight - (learning_rate * (1\/n) * slope_of_weight)\n            #add L1 and L2 regularization using lambda1 and lambda2\n            weight[layer_i] = weight[layer_i] + (learning_rate * (1\/number_of_trianing) * slope_of_weight[layer_i]) - lambda1*np.sign(weight[layer_i]) - lambda2*weight[layer_i]\n            bias[layer_i] = bias[layer_i] + (learning_rate * (1\/number_of_trianing) * slope_of_bias[layer_i]) - lambda1*np.sign(bias[layer_i]) - lambda2*bias[layer_i]\n    return weight, bias, error_list","33f1d9cf":"def neural_network_classification_backpropagation(feature_list, weight, bias, output, activated_output, target_list, activate_function, dropout, prob_drop):\n    \n    #number_of_layer = length of activate_function\n    number_of_layer = len(activate_function)\n    \n    #create empty list\n    slope_weight = []\n    slope_bias = []\n    \n    #for begin in last layer\n    index_of_last_layer = number_of_layer - 1\n    #step -1, stop at 0    \n    #range(start,stop,step)\n    for i in range(index_of_last_layer, -1, -1):\n        \n        if i >= number_of_layer - 1:\n            #last layer delta = target_list - activated_output of last layer \n            delta_i = target_list - activated_output[i]\n            different_i = 1\n        else:\n            #other layer delta_i = error_i of layer [i+1] . transpose(weight of layer [i+1])\n            #remark use error_i = error_i of layer [i+1] because error_i is collected by previous round of loop\n            delta_i = np.dot(error_i, weight[i+1].T)\n            different_i = neural_network_compute_different(output[i], activated_output[i], activate_function[i])\n            \n        #collect error_i to error_i for calculate previous layer\n        error_i = neural_network_compute_error(delta_i, different_i)\n        \n        if i <= 0:\n            #at first layer slope_weight_i = transpose(feature_list) . error_i\n            slope_weight_i = np.dot(feature_list.T, error_i)\n        else:\n            #other layer slope_weight_i = transpose(activated_output of [i-1] layer) . error_i\n            slope_weight_i = np.dot(activated_output[i-1].T, error_i)\n        \n        if dropout == False:\n            if i <= 0:\n                #at first layer slope_weight_i = transpose(feature_list) . error_i\n                slope_weight_i = np.dot(feature_list.T, error_i)\n            else:\n                #other layer slope_weight_i = transpose(activated_output of [i-1] layer) . error_i\n                slope_weight_i = np.dot(activated_output[i-1].T, error_i)\n\n                #slope_bias_i = sum(error_i)\n                slope_bias_i = error_i.sum(axis=0)\n        else:\n            #create drop out node list for each layer\n            dropout_node_list = neural_network_random_dropout_node(hidden_layer, prob_drop)\n            \n            if i > 0:\n                #at other layer drop\n                slope_weight_i = np.dot((activated_output[i-1]*dropout_node_list[i-1]).T, error_i)\n            else:\n                #at first layer not drop\n                slope_weight_i = np.dot(feature_list.T, error_i)\n                \n            if i >= number_of_layer - 1:\n                #at last layer not drop\n                slope_bias_i = error_i.sum(axis=0)\n            else:\n                #at other layer drop\n                slope_bias_i = (error_i*dropout_node_list[i]).sum(axis=0)\n        \n        \n        #slope_bias_i = sum(error_i)\n        slope_bias_i = error_i.sum(axis=0)\n        \n        #add slope_weight_i,slope_bias_i to list\n        slope_weight.append(slope_weight_i)\n        slope_bias.append(slope_bias_i)\n        \n    #convert [slope_weight_3,slope_weight_2,slope_weight_1] to [slope_weight_1,slope_weight_2,slope_weight_3] \n    slope_weight =  slope_weight[::-1]\n    slope_bias =  slope_bias[::-1]\n    \n    return slope_weight, slope_bias","a4e8c618":"def neural_network_random_dropout_node(hidden_layer, prob_drop):\n    dropout_node_list = []\n    for i in range(len(hidden_layer)):\n        dropout_node = np.random.choice([False, True], [1, hidden_layer[i]], p = [prob_drop[i], 1 - prob_drop[i]])\n        dropout_node_list.append(dropout_node)\n    return dropout_node_list","2a09e515":"def train_test_split(feature_list, target_list, train_size_percent = 80):\n    \n    #define N\n    number_of_data = feature_list.shape[0]\n    \n    #random for split\n    arr_rand = np.random.rand(number_of_data)\n    \n    #split random array using train_size_percent \n    split = arr_rand < np.percentile(arr_rand, train_size_percent)\n    \n    #split\n    feature_list_train = feature_list[split]\n    target_list_train = target_list[split]\n    feature_list_test =  feature_list[~split]\n    target_list_test = target_list[~split]\n    \n    return feature_list_train,target_list_train,feature_list_test,target_list_test","6dd52353":"def standardization(data, mean_norm, std_norm):\n    data_norm = (data - mean_norm)\/std_norm\n    return data_norm","c573b4d6":"def std_for_norm(data):\n    _std = data.std(axis=0)\n    return _std.reshape(1, -1)","d2235bbf":"def mean_for_norm(data):\n    _mean = data.mean(axis=0)\n    return _mean.reshape(1, -1)","d92c5fae":"def de_standardization(data_norm, mean_norm, std_norm):\n    data = data_norm*std_norm + mean_norm\n    return data","87e803d9":"#read data values\nraw_csv = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nraw_data = raw_csv.values\nfeature = raw_data[:,:-1]\ntarget = raw_csv['DEATH_EVENT'].values","7b2c68bd":"raw_csv.keys()","6bf5a41c":"#split train-test data\nfeature_list_train,target_list_train,feature_list_test,target_list_test = train_test_split(feature,target)","d760e34f":"#find standard deviation\nstd_feature_list_train = std_for_norm(feature_list_train)\n\n#find mean\nmean_feature_list_train = mean_for_norm(feature_list_train)\n\n#normalize with standardization\nfeature_list_train_norm = standardization(feature_list_train, mean_feature_list_train, std_feature_list_train)\nfeature_list_test_norm = standardization(feature_list_test, mean_feature_list_train, std_feature_list_train)","4fd2be7a":"#create one hot matrix for classification\ntarget_list_train_onehot = create_onehot_target(target_list_train)\ntarget_list_test_onehot = create_onehot_target(target_list_test)","a40fb66c":"#last layer of clasification = unique target\nunique_label = len(np.unique(target_list_train))\n\n#defind node in hidden layer\nhidden_layer = [20,20,unique_label]","0337f942":"#define activated for each layer \n#for clasification use softmax in last layer\nactivated_function = ['tanh',['PReLU',0.1],'softmax']","7c75d8ad":"#random initial weight, bias for train model\nweight, bias = neural_network_create_weight_bias(feature_list_train_norm, hidden_layer)","1e8000c4":"#train model\nweight, bias, error_list = neural_network_classification_find_weight_bias(feature_list_train_norm, target_list_train_onehot, hidden_layer, activated_function, weight = weight, bias = bias, epoch = 10000, learning_rate = 0.1,lambda1 = 0,lambda2 = 0,dropout = True, prob_drop = [0.5,0.5,0])","3999508a":"#plot error list\nplt.plot(error_list)","1627d03e":"error_list[-1]","351dfa9a":"#predict Yhat_train\nZhat_train, Yhat_train = neural_network_forward(feature_list_train_norm, weight, bias, activated_function)\n#find Yhat_train muticlass error\nerror_train = find_error(target_list_train_onehot, Yhat_train[-1], 'Multiclass')\n#print\nprint(error_train)","f4860082":"#predict Yhat_test\nZhat_test, Yhat_test = neural_network_forward(feature_list_test_norm, weight, bias, activated_function)\n#find Yhat_test muticlass error\nerror_test = find_error(target_list_test_onehot, Yhat_test[-1], 'Multiclass')\n#print\nprint(error_test)","67b6ddba":"#view error prediction image\nnot_match_count = 0\n#for in feature_list_test\nfor i in range(len(feature_list_test)):\n        #argmax predicted value\n        predicted = np.argmax(Yhat_test[-1], axis=1)[i]\n        #label value\n        label = np.argmax(target_list_test_onehot, axis=1)[i]\n        #check predicted\n        if(predicted != label):\n            #count not_match]\n            not_match_count += 1","257edb58":"#view percent error and not match count\nprint(\"not match count : {0}\".format(not_match_count))\nprint(\"feature list test count : {0}\".format(len(feature_list_test)))\nprint(\"accuracy : {:.4f}%\".format((1-(not_match_count\/len(feature_list_test)))*100))","b29708d3":"# data_to_submit = pd.DataFrame(weight)\n# data_to_submit.to_csv('.\/weight.csv', index = False)","e77d5702":"# data_to_submit = pd.DataFrame(bias)\n# data_to_submit.to_csv('.\/bias.csv', index = False)","bc3ffc7e":"# Normalization","901e0d1c":"# Import Library","09ce083f":"# Create Model","5eb938c8":"# Read Data & Prepare Data","9235489e":"# Prediction","231abc3e":"# Write Function"}}