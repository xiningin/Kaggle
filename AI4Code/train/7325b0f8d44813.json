{"cell_type":{"be39d8c3":"code","6fe56418":"code","9d2bd4b1":"code","7aa773e9":"code","b9699cf7":"code","5d3206c3":"code","6d5fc7cb":"code","aeb4ce92":"code","989ffdb4":"code","7efcc491":"code","53285d6f":"code","7d822953":"code","cc8ff05a":"markdown","11eccd3f":"markdown","45e4b595":"markdown","0d76fd57":"markdown","f067bce7":"markdown","65984b21":"markdown","bac09405":"markdown","14336d92":"markdown","bdc8dfa3":"markdown","d7bb8a4f":"markdown","e3648389":"markdown","4b837c20":"markdown","8d616d0c":"markdown"},"source":{"be39d8c3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix","6fe56418":"df_iris=pd.read_csv('..\/input\/iris\/Iris.csv')\ndf_iris.head()","9d2bd4b1":"setosa=df_iris.iloc[:50,:]\nversicolor=df_iris.iloc[50:100,:]\nvirginica=df_iris.iloc[100:150,:]\n\nsns.distplot(a=setosa['PetalLengthCm'], label=\"Iris-setosa\")\nsns.distplot(a=versicolor['PetalLengthCm'], label=\"Iris-versicolor\" )\nsns.distplot(a=virginica['PetalLengthCm'], label=\"Iris-virginica\")\n\n# Add title\nplt.title(\"Histogram of Petal Lengths, by Species\")\n\n# Force legend to appear\nplt.legend()","7aa773e9":"df=df_iris.iloc[:100,2:4]","b9699cf7":"g=sns.jointplot(x=df_iris['SepalWidthCm'], y=df_iris['PetalLengthCm'],kind=\"kde\", color=\"m\")\ng.plot_joint(plt.scatter, c=\"w\", s=5, linewidth=1)\ng.ax_joint.collections[0].set_alpha(0)\ng.set_axis_labels(\"$Sepal$ $Width$ $(cm)$\", \"$Petal$ $Length$ $(cm)$\");","5d3206c3":"X=df.to_numpy()","6d5fc7cb":"y=df_iris.iloc[:100,-1]\ny = LabelEncoder().fit_transform(y)","aeb4ce92":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)\n\nsvc = LinearSVC()\nmodel_fit=svc.fit(X_train, y_train)","989ffdb4":"decision_function = model_fit.decision_function(X_train)\nsupport_vector_indices = np.where((2 * y_train - 1) * decision_function <= 1)[0]\nsupport_vectors = X_train[support_vector_indices]","7efcc491":"plt.figure()\nplt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='winter')\nax=plt.gca()\nxlim=ax.get_xlim()\nw = svc.coef_[0]\na = -w[0] \/ w[1]\nxx = np.linspace(xlim[0], xlim[1])\nyy = a * xx - svc.intercept_[0] \/ w[1]\nplt.plot(xx, yy)\nyy = a * xx - (svc.intercept_[0] - 1) \/ w[1]\nplt.plot(xx, yy, 'k--')\nyy = a * xx - (svc.intercept_[0] + 1) \/ w[1]\nplt.plot(xx, yy, 'k--')\nplt.xlabel('Sepal Width (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.title('Train vectors and support vectors')\nplt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100,\n                linewidth=1, facecolors='none', edgecolors='r')","53285d6f":"plt.figure()\nplt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap='winter')\nax=plt.gca()\nxlim=ax.get_xlim()\nw = svc.coef_[0]\na = -w[0] \/ w[1]\nxx = np.linspace(xlim[0], xlim[1])\nyy = a * xx - svc.intercept_[0] \/ w[1]\nplt.plot(xx, yy)\nyy = a * xx - (svc.intercept_[0] - 1) \/ w[1]\nplt.plot(xx, yy, 'k--')\nyy = a * xx - (svc.intercept_[0] + 1) \/ w[1]\nplt.plot(xx, yy, 'k--')\nplt.xlabel('Sepal Width (cm)')\nplt.ylabel('Petal Length (cm)')\nplt.title('Test vectors')","7d822953":"y_pred = svc.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\n","cc8ff05a":"The following plot shows the training vectors, the decision boundaries and the hyperplane. The support vectors are encircled in red.","11eccd3f":"The confusion Matrix:","45e4b595":"Histograms of petal lengths by species","0d76fd57":"Finding the support vectors:","f067bce7":"In order to find the optimal hyperplane (that is, best positioned so it can help classify efficiently by determining the best decision boundaries), it is necessary to maximise the distance between the hyperplane and the support vectors, a maximum margin, as shown in the Figure above. A test point is then classified by comparing its relative position with respect to the hyperplane.","65984b21":"From 25 test vectors, all of them were correctly classified. So our simple linear SVM model works great! The SVM method becomes evidently powerful when facing large datasets with a large number of features.","bac09405":"The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray), but not dataframes. The array X containing the features is created.","14336d92":"Converting string value to int type for labels\nSetosa = 0\nVersicolor = 1","bdc8dfa3":"![](http:\/\/)![optimal-hyperplane.png](attachment:optimal-hyperplane.png)","d7bb8a4f":"For simplicity, we will onlt discriminate between **Iris Setosa** and **Iris Versicolor**, so a dataframe with this data only is created. Also, only two features will be considered: Sepal Width and Petal Length. So the feature space is 2-dimensional, and the hyperplane will be a line. In the case of an N-Dimensional feature space, the hyperplane will be (N-1)-Dimensional. ","e3648389":"A plot to see the distribution of **Iris Setosa** and **Iris Versicolor**","4b837c20":"# Iris species classification with Linear Support Vector Machines\n\nA support vector machine (SVM) is a **non-probabilistic** supervised machine learning model that uses classification algorithms for two-group classification problems. The non-probabilistic aspect is its key strength, in contrast with a probabilistic classifier, such as The Naive Bayes. In essence, an SVM separates data across decision boundaries, determined by only a small subset of the feature vectors (data or ponts in the feature space). The data subset that supports the decision boundary are called **support vectors**. The decision boundaries are found by determining a hyperplane placed in the middle of the decision boundaries. See the Figure below:\n\n","8d616d0c":"The model is trained by splitting the data into train and test subsets:"}}