{"cell_type":{"e642d447":"code","418eef55":"code","24eed42f":"code","3cfe6ae5":"code","450dd796":"code","0a2279aa":"code","c0f9af24":"code","fed3b66e":"code","f4eebf82":"code","7871895a":"markdown","4cebed00":"markdown","c7a6e1b2":"markdown","f45ff910":"markdown"},"source":{"e642d447":"### importing some libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # data visualizations\nfrom sklearn.preprocessing import LabelEncoder  # label encoder\nfrom sklearn.model_selection import train_test_split  # Splitter\n\n### Working on te Data ###\n\ndf = pd.read_csv(\"..\/input\/mushrooms.csv\") # reading\n\nlabel_encoder = LabelEncoder()  \ndf = df.apply(label_encoder.fit_transform) # label encoding\n\n#\n\ny = df[\"class\"].values   # our labels.. okay to eat or poison.\n\ndf.drop([\"class\"],axis=1,inplace=True)  # dropping the lables from the data\ndf.drop([\"veil-color\"],axis=1,inplace=True)\ndf.drop([\"veil-type\"],axis=1,inplace=True)\n\nx_data = df  # our features..\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values # normalization\n\n#\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)   # 20% would be enough\n\nx_train = x_train.values.T\nx_test = x_test.values.T\n\ny_train = y_train.reshape(-1,1).T\ny_test = y_test.reshape(-1,1).T","418eef55":"def initialize_param(layers_with_nodes,in_data,out_data):\n    \n    # we start with initializing the input layer's values.\n    \n    layer_amount = len(layers_with_nodes)\n    \n    parameters = {}\n    parameters[\"weight1\"] = np.random.randn(layers_with_nodes[0],in_data.shape[0]) * 0.1\n    parameters[\"bias1\"] = np.zeros((layers_with_nodes[0],1))\n    \n    print(\"\" + str(layer_amount+2) + \" layers. (\" + str(layer_amount) + \" hidden layer)\")\n\n    # then we initialize the hidden layer's values.\n    \n    for i in range(layer_amount-1):\n        #print(i+2)\n        w = \"weight\" + str(i+2)\n        b = \"bias\" + str(i+2)\n        parameters[w] = np.random.randn(layers_with_nodes[i+1],layers_with_nodes[i]) * 0.1\n        parameters[b] = np.zeros((layers_with_nodes[i+1],1))\n    \n    # and lastly output layer's values.\n    \n    lastw = \"weight\" + str(layer_amount+1)\n    lastb = \"bias\" + str(layer_amount+1)\n    parameters[lastw] = np.random.randn(out_data.shape[0],layers_with_nodes[layer_amount-1]) * 0.1\n    parameters[lastb] = np.zeros((out_data.shape[0],1))\n    \n    return parameters, (layer_amount+1)","24eed42f":"def sigmoid(x):\n    y_head = 1\/(1 + np.exp(-x))   # basic sigmoid for output layer\n    return y_head;\n\ndef compute_cost(A_f, y_train):\n    logprobs = np.multiply(np.log(A_f),y_train)\n    cost = -np.sum(logprobs)\/y_train.shape[1]    # computing the loss value, so we can improve our success more in the future.\n    return cost","3cfe6ae5":"def forw_prop_NN(in_data, parameters, connection_amount):\n    \n    cache = {}\n    a0 = in_data\n    \n    # forward propagation until last layer is same because we use tanh for all except output layer.\n    \n    for i in range(connection_amount-1):\n        w = \"weight\" + str(i+1)   \n        b = \"bias\" + str(i+1) \n        \n        z = \"Z\" + str(i+1)\n        a = \"A\" + str(i+1)\n        \n        cache[z] = np.dot(parameters[w],a0) + parameters[b]\n        cache[a] = np.tanh(cache[z])\n        a0 = cache[a]\n        \n        \n    # then on the output(last) layer we use sigmoid.\n    \n    z_fin = \"Z\" + str(connection_amount)\n    a_fin = \"A\" + str(connection_amount)\n    \n    w = \"weight\" + str(connection_amount)   \n    b = \"bias\" + str(connection_amount)\n    \n    a0 = \"A\" + str(connection_amount-1)\n    \n    cache[z_fin] = np.dot(parameters[w],cache[a0]) + parameters[b]\n    cache[a_fin] = sigmoid(cache[z_fin])\n    A = cache[a_fin]\n    \n    return A, cache\n        ","450dd796":"def backw_prop_NN(parameters,cache,x_train,y_train,connection_amount):\n    \n    dz = {}\n    grads = {}\n    \n    # since we are going backwards on back propagation, we update values from last to first\n    # so firstly we will start with output layer.\n    \n    a_fin = \"A\" + str(connection_amount)\n    a_pre_fin = \"A\" + str(connection_amount-1)\n    w_fin = \"dweight\" + str(connection_amount)\n    b_fin = \"dbias\" + str(connection_amount)\n    \n    dz[connection_amount] = cache[a_fin] - y_train\n    grads[w_fin] = np.dot(dz[connection_amount],cache[a_pre_fin].T)\/x_train.shape[1]\n    grads[b_fin] = np.sum(dz[connection_amount], axis=1, keepdims=True)\/x_train.shape[1]\n    \n    # then continue with the rest of layers.\n    \n    for i in range((connection_amount-1),1,-1):\n        a = \"A\" + str(i)\n        a_pre = \"A\" + str(i-1)\n        dw = \"dweight\" + str(i)\n        db = \"dbias\" + str(i)\n        w = \"weight\" + str(i+1)\n        \n        dz[i] = np.dot(parameters[w].T,dz[i+1])*(1 - np.power(cache[a], 2))\n        grads[dw] = np.dot(dz[i],cache[a_pre].T)\/x_train.shape[1]\n        grads[db] = np.sum(dz[i], axis=1,keepdims=True)\/x_train.shape[1]\n    \n    dz[1] = np.dot(parameters[\"weight2\"].T,dz[2])*(1 - np.power(cache[\"A1\"], 2))\n    grads[\"dweight1\"] = np.dot(dz[1],x_train.T)\/x_train.shape[1]\n    grads[\"dbias1\"] = np.sum(dz[1],axis =1,keepdims=True)\/x_train.shape[1]\n    \n    return grads","0a2279aa":"def update_param(parameters, grads, connection_amount, lr=0.01):\n    for i in range(connection_amount):\n        w = \"weight\" + str(i+1)\n        b = \"bias\" + str(i+1)\n        dw = \"dweight\" + str(i+1)\n        db = \"dbias\" + str(i+1)\n        \n        parameters[w] = parameters[w] - lr*grads[dw]\n        parameters[b] = parameters[b] - lr*grads[db]\n        \n    return parameters                 # basic update for all layers...","c0f9af24":"def predict(A,parameters,x_test,connection_amount):\n    # x_test is a input for forward propagation\n    A, cache = forw_prop_NN(x_test,parameters,connection_amount)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n \n    for i in range(A.shape[1]):\n        if A[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction             # basic binary prediction...","fed3b66e":"def multi_layer_model(layers_with_nodes,x_train,y_train,x_test,y_test, num_iter, lr = 0.01):    # only learning rate is pre-defined\n    \n    cost_list = []     \n    index_list = []\n    \n    parameters, connection_amount = initialize_param(layers_with_nodes, x_train, y_train)   # starting with initializing for only once\n    \n    # then do the following part for each iteration..\n    \n    for i in range(0, num_iter):\n         # forward propagation\n        A, cache = forw_prop_NN(x_train,parameters,connection_amount)\n        # compute cost\n        cost = compute_cost(A, y_train)\n         # backward propagation\n        grads = backw_prop_NN(parameters,cache,x_train,y_train,connection_amount)\n         # update parameters\n        parameters = update_param(parameters, grads, connection_amount, lr)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n   \n    y_prediction_train = predict(A, parameters, x_train, connection_amount)\n    y_prediction_test = predict(A, parameters, x_test, connection_amount)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n","f4eebf82":"multi_layer_model([20,4],x_train,y_train,x_test,y_test,num_iter=3001,lr=0.03)\n\n# the first array is where we desing our net for hidden layers. \n# for example layers_with_nodes=[4] means one hidden layer with 4 nodes, or layers_with_nodes=[10,5] means two hidden layers: first layer with 10 and second with 5 nodes\n\n# lr is learning rate as usual\n\n# num_iter is number of iterations that we want our model to train.","7871895a":"**THIS IS WHERE REAL STUFF STARTS**","4cebed00":"**DATA PREPROCESS**","c7a6e1b2":"**INTRODUCTION**\n\nAs a deep learning lover, I always used libraries like Keras to build deep neural networks.\nBut recently, I was looking to [this](https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners) kernel provided by DATAI and thinking about improving the Artificial Neural Network part to make it more customizable.\n\nOn this Kernel you will find a simple, primitive approach to make a deep neural network :) Have a fun reading.","f45ff910":"**CONCLUSION**\n\nAs we see above, it is actually not that hard to create a neural network that we can configure and go as deep as we want :) (or as wide as we want)\n\nI hope you enjoyed my approach.\n\nif you find it helpful, please leave a comment or like. So, I will contribute more :)\n\nreference for inspiration: https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners"}}