{"cell_type":{"df821d4f":"code","998a88de":"code","5a2bb31b":"code","726b960f":"code","a73c8c6b":"code","5e475bb5":"code","9abc8ee8":"code","83bd287d":"code","38af4c12":"code","de3e80e2":"code","cf7e3aa1":"code","770d80ca":"code","88dd7801":"code","53ffa3d8":"code","37bff218":"code","f9ee5c43":"code","2eb1b156":"code","813daa66":"code","f237c76c":"code","fe0b3938":"code","593deccf":"code","bbb84755":"code","37976fa9":"code","5c29bb5c":"code","e4068218":"code","77055917":"code","80715f33":"code","71cdaa71":"code","7b09f134":"code","f1508142":"code","ac9329fd":"code","6b29b959":"code","8a263bc0":"code","3f40eacb":"code","b1749f6b":"code","065691f2":"code","0e671367":"code","ca90858f":"code","73d107d5":"code","06d19424":"code","d21a431d":"code","ca164b8b":"code","caf37d38":"code","092c3d08":"code","f2099095":"code","07f6de31":"code","b79105c8":"code","b6b7ac26":"code","0849d244":"markdown","8017b0a6":"markdown","feea5b35":"markdown","9181ceaf":"markdown","4773e50a":"markdown","75aac1ad":"markdown","c9fc5a3f":"markdown","b4a9f878":"markdown","85c90059":"markdown","d38b146d":"markdown","7d488a83":"markdown","ab39d23a":"markdown","77825c1e":"markdown","73d3ad98":"markdown","411d47a6":"markdown","5c1c5cb0":"markdown","86a2197a":"markdown","58238edf":"markdown","b6e9ba16":"markdown"},"source":{"df821d4f":"!pip install dataprep","998a88de":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\n\nfrom dataprep.eda import create_report\nfrom dataprep.eda import plot_missing\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, roc_curve\nfrom sklearn.model_selection import learning_curve, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","5a2bb31b":"data = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf = data.copy()\npd.set_option('display.max_row',df.shape[0])\npd.set_option('display.max_column',df.shape[1]) \ndf.head()","726b960f":"code = {\n        'Unknown':'NaN'\n        }\n\ndf = df.replace({'smoking_status':code})","a73c8c6b":"plot_missing(df)","5e475bb5":"print('There is' , df.shape[0] , 'rows')\nprint('There is' , df.shape[1] , 'columns')","9abc8ee8":"df.duplicated().sum() # There are no duplicates","83bd287d":"# Visualising target balance\ndf['stroke'].value_counts(normalize=True) #Classes d\u00e9s\u00e9quilibr\u00e9es","38af4c12":"# Qualitative features\nfor col in df.select_dtypes(\"object\"):\n    print(f'{col :-<50} {df[col].unique()}')","de3e80e2":"# Quantitative features\nfor col in df.select_dtypes(include=['float64','int64']):\n    plt.figure()\n    sns.displot(df[col],kind='kde',height=3)\n    plt.show()","cf7e3aa1":"def encoding(df):\n    code = {\n        'Male':0,\n        'Female':1,\n        'Other':2,\n        'Yes':1,\n        'No':0,\n        'Private':0,\n        'Self-employed':1,\n        'Govt_job':2,\n        'children':3,\n        'Never_worked':4,\n        'Urban':0,\n        'Rural':1,\n        'formerly smoked':0,\n        'never smoked':1,\n        'smokes':2\n           }\n    for col in df.select_dtypes('object'):\n        df.loc[:,col]=df[col].map(code)\n        \n    return df\n\ndef imputation(df):\n    \n    df = df.dropna(axis=0)\n    \n    return df\n\ndef feature_engineering(df):\n    useless_columns = ['id'] # Let's consider we want to use all the features\n    df = df.drop(useless_columns,axis=1)\n    return df","770d80ca":"def preprocessing(df):\n    df = encoding(df)\n    df = feature_engineering(df)\n    df = imputation(df)\n    \n    X = df.drop('stroke',axis=1)\n    y = df['stroke']    \n      \n    return df,X,y","88dd7801":"df = data.copy()\ndf,X,y = preprocessing(df)","53ffa3d8":"riskyDF = df[y == 1]\nsafeDF = df[y == 0]","37bff218":"plt.figure(figsize=(4,4))\nsns.pairplot(df,height=1.5)\nplt.show()","f9ee5c43":"corr = df.corr(method='pearson').abs()\n\nfig = plt.figure(figsize=(8,6))\nsns.heatmap(corr, annot=True, cmap='tab10', vmin=0, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\nprint (df.corr()['stroke'].abs().sort_values())","2eb1b156":"for col in df.select_dtypes(include=['float64','int64']):\n    plt.figure(figsize=(4,4))\n    sns.distplot(riskyDF[col],label='High Risk')\n    sns.distplot(safeDF[col],label='Low Risk')\n    plt.legend()\n    plt.show()","813daa66":"for col in X.select_dtypes(include=['float64','int64']):\n    plt.figure(figsize=(4,4))\n    sns.lmplot(x='age', y=col, hue='stroke', data=df)","f237c76c":"create_report(df)","fe0b3938":"df = data.copy()\ndf,X,y=preprocessing(df)\nprint(df['stroke'].value_counts())","593deccf":"# Class count\ncount_class_0, count_class_1 = df['stroke'].value_counts()\n\n# Divide by class\ndf_class_0 = df[df['stroke'] == 0]\ndf_class_1 = df[df['stroke'] == 1]","bbb84755":"df_class_0_under = df_class_0.sample(count_class_1)\ndf_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_under['stroke'].value_counts())\n\ndf_under['stroke'].value_counts().plot(kind='bar', title='Count (target)');","37976fa9":"trainset, testset = train_test_split(df_under, test_size=0.2, random_state=0)\nprint(trainset['stroke'].value_counts())\nprint(testset['stroke'].value_counts())","5c29bb5c":"X_train = trainset.drop(['stroke'],axis=1)\ny_train = trainset['stroke']\nX_test = testset.drop(['stroke'],axis=1)\ny_test = testset['stroke']","e4068218":"preprocessor = make_pipeline(MinMaxScaler())\n\nPCAPipeline = make_pipeline(StandardScaler(), PCA(n_components=2,random_state=0))\n\nRandomPipeline = make_pipeline(preprocessor,RandomForestClassifier(random_state=0))\nAdaPipeline = make_pipeline(preprocessor,AdaBoostClassifier(random_state=0))\nSVMPipeline = make_pipeline(preprocessor,SVC(random_state=0,probability=True))\nKNNPipeline = make_pipeline(preprocessor,KNeighborsClassifier())\nLRPipeline = make_pipeline(preprocessor,LogisticRegression(solver='sag'))","77055917":"PCA_df = pd.DataFrame(PCAPipeline.fit_transform(X))\nPCA_df = pd.concat([PCA_df, y], axis=1)\nPCA_df.head()","80715f33":"plt.figure(figsize=(8,8))\nsns.scatterplot(PCA_df[0],PCA_df[1],hue=PCA_df['stroke'],palette=sns.color_palette(\"tab10\", 2))\nplt.show()","71cdaa71":"dict_of_models = {'RandomForest': RandomPipeline,\n'AdaBoost': AdaPipeline,\n'SVM': SVMPipeline,\n'KNN': KNNPipeline,\n'LR': LRPipeline}","7b09f134":"def evaluation(model):\n    model.fit(X_train, y_train)\n    # calculating the probabilities\n    y_pred_proba = model.predict_proba(X_test)\n\n    # finding the predicted valued\n    y_pred = np.argmax(y_pred_proba,axis=1)\n    print('Accuracy = ', accuracy_score(y_test, y_pred))\n    print('-')\n    print(confusion_matrix(y_test,y_pred))\n    print('-')\n    print(classification_report(y_test,y_pred))\n    print('-')\n    \n    N, train_score, val_score = learning_curve(model, X_train, y_train, \n                                               cv=4, scoring='f1', \n                                               train_sizes=np.linspace(0.1,1,10))\n    plt.figure(figsize=(12,8))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()","f1508142":"for name, model in dict_of_models.items():\n    print('---------------------------------')\n    print(name)\n    evaluation(model)","ac9329fd":"from sklearn.model_selection import RandomizedSearchCV\nRandomPipeline.get_params().keys()","6b29b959":"hyper_params = {\n    'randomforestclassifier__n_estimators':[10,100,150,250,400,600],\n    'randomforestclassifier__criterion':['gini','entropy'],\n    'randomforestclassifier__min_samples_split':[2,6,12],\n    'randomforestclassifier__min_samples_leaf':[1,4,6,10],\n    'randomforestclassifier__max_features':['auto','srqt','log2',int,float],\n    'randomforestclassifier__n_jobs':[-1],\n    'randomforestclassifier__verbose':[0,1,2],\n    'randomforestclassifier__class_weight':['balanced','balanced_subsample'],\n    'randomforestclassifier__n_jobs':[-1],\n    'randomforestclassifier__n_jobs':[-1],\n}","8a263bc0":"grid = RandomizedSearchCV(RandomPipeline,hyper_params,scoring='accuracy',n_iter=40)","3f40eacb":"grid.fit(X_train,y_train)","b1749f6b":"print(grid.best_params_)\nprint(grid.best_score_)","065691f2":"ypred = grid.predict(X_test)\nprint(classification_report(y_test,ypred))","0e671367":"SVMPipeline.fit(X_train, y_train)\ny_proba = SVMPipeline.predict_proba(X_test)\ny_pred = np.argmax(y_proba,axis=1)\n\nprint(\"SVM : \", accuracy_score(y_test, y_pred))","ca90858f":"y_pred_prob = SVMPipeline.predict_proba(X_test)[:,1]\n\nfpr,tpr,threshols=roc_curve(y_test,y_pred_prob)\n\nplt.plot(fpr,tpr,label='SVM ROC Curve')\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"SVM ROC Curve\")\nplt.show()","73d107d5":"SVMPipeline.get_params().keys()","06d19424":"from sklearn.utils.fixes import loguniform\n\n\nhyper_params = {\n    'svc__C': loguniform(1e0, 1e3),\n    'svc__gamma': loguniform(1e-4, 1e-3),\n    'svc__kernel': ['rbf'],\n    'svc__class_weight':['balanced', None]\n               }","d21a431d":"grid = RandomizedSearchCV(SVMPipeline,hyper_params,scoring='accuracy',n_iter=40)","ca164b8b":"grid.fit(X_train,y_train)","caf37d38":"print(grid.best_params_)\nprint(grid.best_score_)","092c3d08":"ypred = grid.predict(X_test)\nprint(classification_report(y_test,ypred))","f2099095":"import xgboost as xgb\ngbm = xgb.XGBClassifier(\n learning_rate = 0.05,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n eval_metric = 'logloss',\n nthread= -1,\n scale_pos_weight=1).fit(X_train, y_train)\nevaluation (gbm)","07f6de31":"best_classifier = RandomPipeline","b79105c8":"predictions = best_classifier.predict(X_test)\npredictions_proba = best_classifier.predict_proba(X_test)\nprint(predictions_proba)","b6b7ac26":"thresholds = [0.3,0.4,0.5,0.6,0.7,0.8]\nbest_t = 0.3\nbest_acc = 0\nfor t in thresholds:\n    y_pred = (best_classifier.predict_proba(X_test)[:,1] >= t).astype(int)\n    acc = accuracy_score(y_test, y_pred)\n    if acc > best_acc:\n        best_acc=acc\n        best_t=t\nprint('Accuracy on test set :',round(best_acc*100),\"%\")\nprint('Best threshold :',best_t)","0849d244":"## RandomForest Optimization","8017b0a6":"## XGBoost","feea5b35":"## Dataset Analysis","9181ceaf":"## Detailed Analysis","4773e50a":"# Exploratory Data Analysis\n\n## Aim :\n- Understand the data (\"A small step forward is better than a big one backwards\")\n- Begin to develop a modelling strategy\n\n## Features\n\n1) id: unique identifier\n\n2) gender: \"Male\", \"Female\" or \"Other\"\n\n3) age: age of the patient\n\n4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6) ever_married: \"No\" or \"Yes\"\n\n7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8) Residence_type: \"Rural\" or \"Urban\"\n\n9) avg_glucose_level: average glucose level in blood\n\n10) bmi: body mass index\n\n11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12) stroke: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\n## Base Checklist\n#### Shape Analysis :\n- **target feature** : stroke\n- **rows and columns** : 5110, 12\n- **features types** : qualitatives :  5, quantitatives : 7\n- **NaN analysis** :\n    - NaN (0 % of NaN)\n\n#### Columns Analysis :\n- **Target Analysis** :\n    - Balanced (Yes\/No) : No\n    - Percentages : 95% No stroke","75aac1ad":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)","c9fc5a3f":"# Tuning estimators","b4a9f878":"### Comments\n\nIt looks like we have some very useful features here, with a correlation > 0.1.\nThe following features seems promising for predicting wether a patient will have a heart attack or not :\n- **heart_disease**\n- **avg_glucose_level**\n- **hypertension**\n- **age**\n\nWe can also notice that **ever_married** and **age** looks correlated, let's find out !","85c90059":"## PCA Analysis","d38b146d":"<h2 id=\"t4\" style=\"margin-bottom: 18px\">Resampling<\/h2>\n\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).","7d488a83":"## Visualising Target and Features","ab39d23a":"# If you like please upvote !\n## Also check my other notebooks :\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83d\udc01Mice Trisomy (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83c\udf97\ufe0fBreast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### \ud83c\udf26\ud83c\udf21 Weather Forecasting \ud83d\udcc8 (98% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/weather-forecasting-98-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Heart Attack \ud83e\ude7a\ud83d\udc93 (90% Acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Mobile price (95.5% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83e\udde0 Stroke (74% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-stroke-74-acc","77825c1e":"# Modelling","73d3ad98":"## Classification problem","411d47a6":"# A bit of data engineering ...","5c1c5cb0":"## SVM Optimization","86a2197a":"# Best Estimator","58238edf":"<h1><center>\ud83e\udde0Stroke Data Analysis\ud83d\udd0e<\/center><\/h1>\n<h3><center>\ud83e\ude7a(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1120859\/1882037\/04da2fb7763e553bdf251d5adf6f88d9\/data-original.jpg?t=2021-01-26-19-57-05\" alt =\"Titanic\" style='width: 400px;'><\/center>\n\n<h3>Overview<\/h3>\n<p>\nA stroke occurs when the blood supply to part of your brain is interrupted or reduced, preventing brain tissue from getting oxygen and nutrients. Brain cells begin to die in minutes.\n\nA stroke is a medical emergency, and prompt treatment is crucial. Early action can reduce brain damage and other complications.\n\nThe good news is that many fewer Americans die of stroke now than in the past. Effective treatments can also help prevent disability from stroke.\n<\/p>\n\n<h3>Symptoms<\/h3>\n<p>\n\nIf you or someone you're with may be having a stroke, pay particular attention to the time the symptoms began. Some treatment options are most effective when given soon after a stroke begins.\n\nSigns and symptoms of stroke include:\n\n* Trouble speaking and understanding what others are saying. You may experience confusion, slur your words or have difficulty understanding speech.\n    \n* Paralysis or numbness of the face, arm or leg. You may develop sudden numbness, weakness or paralysis in your face, arm or leg. This often affects just one side of your body. Try to raise both your arms over your head at the same time. If one arm begins to fall, you may be having a stroke. Also, one side of your mouth may droop when you try to smile.\n    \n* Problems seeing in one or both eyes. You may suddenly have blurred or blackened vision in one or both eyes, or you may see double.\n    \n* Headache. A sudden, severe headache, which may be accompanied by vomiting, dizziness or altered consciousness, may indicate that you're having a stroke.\n    \n* Trouble walking. You may stumble or lose your balance. You may also have sudden dizziness or a loss of coordination.\n    \n<\/p>","b6e9ba16":"Despite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.\n\nLet's implement a basic example, which uses the <code>DataFrame.sample<\/code> method to get random samples each class:"}}