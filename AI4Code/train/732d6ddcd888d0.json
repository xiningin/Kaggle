{"cell_type":{"32868b2f":"code","cc818e49":"code","79550d63":"code","aa036cf0":"code","4510c8e7":"code","621355c4":"code","29d7d3e3":"code","73a50f68":"code","58d2202a":"code","bdef2796":"code","4956663b":"code","905aa53a":"code","beb2d213":"code","bf329c93":"code","85cd1b0d":"code","1fb77cdc":"code","faa91640":"code","eda77019":"code","3ecc5b7e":"code","014f114c":"code","ffe4f531":"code","eff08d52":"code","263401ca":"code","49865506":"code","777d0b7e":"code","53ffc500":"code","2a49ebfc":"code","3dc45cdb":"code","bf351ebc":"code","aa117e7b":"code","480444e5":"code","90ae1ff3":"code","077a32d0":"code","f98b7deb":"code","1885b606":"code","9157fa88":"code","1a2b05bc":"code","d82bee50":"code","1869286a":"code","ff7cc199":"code","bf06e8e4":"code","d45eec16":"markdown","c9cc1b77":"markdown","9f242152":"markdown","4aa68c2e":"markdown","34c5af77":"markdown","4b7eb835":"markdown","a59e2000":"markdown","7b4c0186":"markdown","83453244":"markdown","bb700955":"markdown","e2f9a787":"markdown","f4e166e2":"markdown"},"source":{"32868b2f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")","cc818e49":"import os\nfor dirname, _, filenames in os.walk('..\/input\/hepatitis-c-dataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv(\"..\/input\/hepatitis-c-dataset\/HepatitisCdata.csv\")","79550d63":"df.head()","aa036cf0":"df.tail()","4510c8e7":"df.describe()","621355c4":"df.info()","29d7d3e3":"df.dtypes","73a50f68":"df.describe()","58d2202a":"for i in df.columns:\n    values = np.unique(df[i])\n    print(i)\n    print(\"\")\n    print(values)","bdef2796":"#Checking for Duplicated Rows\ndf_duplicated = df[df.duplicated()]\nprint(df_duplicated.shape)","4956663b":"#Dropping Id Column\ndf.drop('Unnamed: 0', axis = 1, inplace=True)","905aa53a":"#Converting Column Names to lowercase\ndf.columns = df.columns.str.lower()","beb2d213":"df.head()","bf329c93":"df.isnull().sum()\/len(df)*100","85cd1b0d":"df.fillna(df.median(),inplace= True)","1fb77cdc":"df.isnull().sum()","faa91640":"numerical_columns = df.select_dtypes(['float64','int64'])\ncategorical_columns = df.select_dtypes(['object'])","eda77019":"numerical_columns.head()","3ecc5b7e":"categorical_columns.head()","014f114c":"#Distplot of Numerical Columns\nfig = plt.figure(figsize = (20,25))\nfor i in range(len(numerical_columns.columns)):\n    fig.add_subplot(3, 4, i+1)\n    sns.distplot(numerical_columns.iloc[:,i])\n    plt.xlabel(numerical_columns.columns[i])\nplt.show()","ffe4f531":"#Catplot of category and hue\nsns.catplot(y=\"category\", hue=\"sex\", kind=\"count\",\n            palette=\"Purples_r\", edgecolor=\".6\",\n            data=df)","eff08d52":"sns.set_style(\"dark\")\nsns.pairplot(df,hue=\"category\");\nplt.show()","263401ca":"# Though there are many categories, we are breaking it into binary: healthy and unhealthy.\ndf['category'] = df['category'].map({'0=Blood Donor':0, '0s=Suspect Blood Donor':0, '1=Hepatisis':1,'2=Fibrosis':1,\n                                     '3=Cirrhosis':1,'3=Cirrhosis':1})","49865506":"print('Suspected Patients:', df.category.value_counts()[0])\nprint('Healthy Patients:',df.category.value_counts()[1])","777d0b7e":"labels = df['category'].value_counts(sort = True).index\nsizes = df['category'].value_counts(sort = True)\ncolors = [\"pink\",\"purple\"]\nplt.figure(figsize = (7,7))\nplt.pie(sizes,labels=labels, colors=colors, autopct='%1.2f%%',startangle=90)\nplt.title('Category Pie')\nplt.show()","53ffc500":"df.corr()","2a49ebfc":"#Correlation Heatmap\nfig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,cmap =\"Blues_r\")","3dc45cdb":"ordinal_df = df[['category']]\nordinal_df.head()","bf351ebc":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nordinal_df = ordinal_df.apply(LabelEncoder().fit_transform)\nordinal_df.head()","aa117e7b":"nominal_df = df[['sex']]\nnominal_df.head()","480444e5":"nominal_df = pd.get_dummies(data=nominal_df)\nnominal_df.head()","90ae1ff3":"#Adding Nominal_df and Ordinal_df\ndf.drop(ordinal_df.columns, axis=1, inplace=True)\ndf = df.join(ordinal_df)\ndf.head()","077a32d0":"df.drop(['sex'], axis = 1, inplace = True)\ndf = df.join(nominal_df)\ndf.head()","f98b7deb":"from sklearn.preprocessing import StandardScaler\nx = df.loc[:, df.columns != 'category'].values\nx= StandardScaler().fit_transform(df) # normalizing the features","1885b606":"normalised_df = pd.DataFrame(x)\nnormalised_df.head()","9157fa88":"X = normalised_df\nY = df['category']","1a2b05bc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y,test_size = 0.2,random_state=42)","d82bee50":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndef hyper_tuning(name, model, model_params, X_train, y_train, X_test, y_test, cv):  \n    grid_cv = GridSearchCV(model, model_params, n_jobs=-1, cv=cv)\n    grid_cv_model = grid_cv.fit(X_train, y_train)\n    print()\n    print(name)\n    print('Best Parameters: ', grid_cv_model.best_params_)\n    print('Best Model Score = {}'.format(grid_cv_model.best_score_))\n    #print('Train score:   ', grid_cv_model.score(X_train, y_train))\n    #print('Test score:    ', grid_cv_model.score(X_test, y_test))\n    print()\n    return grid_cv_model\ndef model_metrics(model, X_train, y_train, X_test, y_test):   \n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    results = cross_val_score(model, X_train, y_train, cv = kfold)\n    print(\"CV scores: \", results); print(\"CV Standard Deviation: \", results.std()); \n    print();\n    print('CV Mean score: ', results.mean()); \n    model.fit(X_train, y_train)\n    print('Train score:   ', model.score(X_train, y_train))\n    print('Test score:    ', model.score(X_test, y_test))\n    \n    pred = model.predict(X_test)\n    print()\n    print('Confusion Matrix: ')\n    print(confusion_matrix(y_test, pred))\n    print()\n    print('Classification Report:  ')\n    print(classification_report(y_test, pred))\n    train_score =  model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    test_pred = model.predict(X_test)\n    print()\n    return results.mean(), results.std()","1869286a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nresults = []\nnames = []\n\n### KNN ###\n\nknn_params = { \"n_neighbors\" : [5,15,25,30,35,40,100],\n    \"weights\" : [\"uniform\" , \"distance\"]\n    }\n\nknn_name = 'K Neighbors Classifier Model:'\nknn_model = KNeighborsClassifier()\nfinal_model = hyper_tuning(knn_name, knn_model, knn_params, X_train, y_train, X_test, y_test,4)\nresult_mean, result_std = model_metrics(final_model, X_train, y_train, X_test, y_test)\nnames.append(knn_name)\nresults.append([result_std, result_mean, result_std])\n\n### Logistic Regression ###\n\nlr_params = {\n    \"penalty\": [\"l1\", \"l2\"],\n    \"C\": np.logspace(-2,4,100)\n    }\n\nlr_name = 'Logistic Regression Model:'\nlr_model = LogisticRegression(random_state=42)\nfinal_model = hyper_tuning(lr_name, lr_model, lr_params, X_train, y_train, X_test, y_test,4)\nresult_mean, result_std = model_metrics(final_model, X_train, y_train, X_test, y_test)\nnames.append(lr_name)\nresults.append([result_std, result_mean, result_std])\n\n### Random Forest Classifier ###\n\nrf_params = {\n    #'n_estimators': [10, 50, 100, 150, 200, 250],\n    'max_features':[2, 3, 5, 7, 8],\n    #'max_depth': [1, 2, 3, 4, 5, 8],\n    }\n\nrf_name = 'Random Forest Classifier Model:' \nrf_model = RandomForestClassifier(n_estimators=100)\nfinal_model = hyper_tuning(rf_name, rf_model, rf_params, X_train, y_train, X_test, y_test,5)\nresult_mean, result_std = model_metrics(final_model, X_train, y_train, X_test, y_test)\nnames.append(rf_name)\nresults.append([result_std, result_mean, result_std])\n\n### Decision Tree Classifier ###\n\ndt_params = {\n    'max_depth': [1, 2, 3, 4, 5, 8],\n    }\n\ndt_name = 'Decision Tree Classifier Model:' \ndt_model = DecisionTreeClassifier(random_state=42)\nfinal_model = hyper_tuning(dt_name, dt_model, dt_params, X_train, y_train, X_test, y_test,4)\nresult_mean, result_std = model_metrics(final_model, X_train, y_train, X_test, y_test)\nnames.append(dt_name)\nresults.append([result_std, result_mean, result_std])\n\n### Extra Tree Classifier ###\n\net_params = {\n    'n_estimators': [10, 50, 100],\n    'max_depth': [1, 2, 3, 4, 5, 8],\n    }\n\net_name = 'Extra Trees Classifier Model:' \net_model = ExtraTreesClassifier(random_state=42)\nfinal_model = hyper_tuning(et_name, et_model, et_params, X_train, y_train, X_test, y_test,5)\nresult_mean, result_std = model_metrics(final_model, X_train, y_train, X_test, y_test)\nnames.append(et_name)\nresults.append([result_std, result_mean, result_std])\n\n\n### AdaBoost Classifier ###\n\nada_params = {\n    'n_estimators': [10, 50, 100],\n    #'max_depth': [1, 2, 3, 4, 5, 8],\n    'learning_rate' : [0.25, 0.50]\n    }\n\nada_name = 'AdaBoost Classifier Model:'\nada_model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=8),random_state=42)\nfinal_model = hyper_tuning(ada_name, ada_model, ada_params, X_train, y_train, X_test, y_test,4)\nresult_mean, result_std = model_metrics(final_model, X_train, y_train, X_test, y_test)\nnames.append(ada_name)\nresults.append([result_std, result_mean, result_std])","ff7cc199":"names = ['KNN', 'Logistic', 'Random Forest', 'Decision Tree', 'Extra Trees', 'AdaBoost']","bf06e8e4":"fig = plt.figure(figsize=(10,10))\nfig.suptitle('Comparison of Algorithms')\nnew_results = []\nfor i in range(len(results)):\n    new_results.append(results[i][1])\nplt.bar(names, new_results)\nplt.show()","d45eec16":"**Correlation**","c9cc1b77":"**Dealing With Missing Values**","9f242152":"**LABEL ENCODING**","4aa68c2e":"**Feature Scaling**","34c5af77":"**ENCODING**","4b7eb835":"**Data Cleaning**","a59e2000":"**MODELS**","7b4c0186":"**One-Hot Encoding**","83453244":"**Importing Required Libraries**","bb700955":"**Train Test Split**","e2f9a787":"**Loading Dataset**","f4e166e2":"**EDA**"}}