{"cell_type":{"5cbcea30":"code","a87d0baa":"code","5aec5636":"code","ffe518b6":"code","7f584417":"code","a97bf4da":"code","f4b103a1":"code","96dc9ee6":"code","6e1f088f":"code","cb8be655":"code","7ba710ed":"code","f3470777":"code","71c5a3c7":"code","b7ea2388":"code","302ef6b5":"code","6584081b":"code","7b93aacf":"code","6059816e":"code","275f8d6e":"markdown","6da2ef60":"markdown","ff88b411":"markdown"},"source":{"5cbcea30":"from fastai.tabular.all import *\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc, mean_absolute_error, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBClassifier, XGBRFRegressor\nimport lightgbm as lgb\nimport seaborn as sns\nimport optuna","a87d0baa":"df_train = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/test.csv')","5aec5636":"df_train.drop('id', axis=1,inplace=True)\ndf_test.drop('id', axis=1,inplace=True)\n","ffe518b6":"df_train.hist(bins=10, figsize=(20,20));","7f584417":"df_train.instrumentalness = np.log(df_train.instrumentalness)\ndf_test.instrumentalness = np.log(df_test.instrumentalness)\n\ndf_train.speechiness = np.log(df_train.speechiness)\ndf_test.speechiness = np.log(df_test.speechiness)","a97bf4da":"df_train.hist(bins=10, figsize=(20,20));","f4b103a1":"# heat = df_train.corr().round(5)\n\n# # Mask to hide upper-right part of plot as it is a duplicate\n# mask = np.zeros_like(heat)\n# mask[np.triu_indices_from(mask)] = True\n\n# # Making a plot\n# plt.figure(figsize=(16,16))\n# ax = sns.heatmap(heat, annot=False, mask=mask, cmap=\"RdYlGn\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\n# ax.set_title(\"Feature correlation heatmap\", fontsize=17)\n# plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n#          rotation_mode=\"anchor\", weight=\"normal\")\n# plt.setp(ax.get_yticklabels(), weight=\"normal\",\n#          rotation_mode=\"anchor\", rotation=0, ha=\"right\")\n# plt.show();","96dc9ee6":"# heat.song_popularity","6e1f088f":"null_features = ['loudness', \n                 'energy', \n                 'danceability', \n                 'acousticness', \n                 'key', \n                 'liveness',\n                 'instrumentalness', \n                 'song_duration_ms']\n## Based on the probability of predictions","cb8be655":"fix_features = ['audio_mode',\n 'speechiness',\n 'tempo',\n 'time_signature',\n 'audio_valence',]","7ba710ed":"# heat.song_popularity[null_features].sort_values()","f3470777":"def objective(trial,data,target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n        'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 20000, 500),\n        'max_depth': trial.suggest_int('max_depth', 6, 15),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4),\n    }\n    \n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","71c5a3c7":"def get_models(df, ind_features, dep_feature, nb_fold=5, xgb_params={}):\n    idx = list(range(len(df)))\n    models = []\n    reduces = []\n    for i in range(nb_fold):\n        random.shuffle(idx)\n        train_idx = idx[:int(len(df)*0.8)]\n        valid_idx = idx[int(len(df)*0.8):]\n        x = df[ind_features]\n        y = df[dep_feature]\n\n\n        x_train = x.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        x_valid = x.iloc[valid_idx]\n        y_valid = y.iloc[valid_idx]\n        \n#         if (i == 0):\n#             study = optuna.create_study(direction='minimize')\n#             func = lambda trial: objective(trial, x, y)\n#             study.optimize(func, n_trials=5)\n#             print('Number of finished trials:', len(study.trials))\n#             print('Best trial:', study.best_trial.params)\n        \n#         model= XGBRFRegressor(**study.best_trial.params, \n#                               n_jobs=-1, \n#                               booster= 'gbtree',\n#                               tree_method= 'gpu_hist',\n#                               predictor=\"gpu_predictor\",)\n        model= XGBRFRegressor( \n                              n_jobs=-1, \n                              booster= 'gbtree',\n                              tree_method= 'gpu_hist',\n                              predictor=\"gpu_predictor\",)\n        model.fit(x_train,y_train)\n#         model.fit(x_train,y_train,early_stopping_rounds=100,eval_set=[(x_valid,y_valid)],verbose=False)\n        predictions = model.predict(x_valid)\n        \n        new_error = mean_absolute_error(predictions, y_valid)\n        old_error = mean_absolute_error([y_valid.mean()]*len(y_valid), y_valid)\n        reduce = 1-new_error\/old_error\n\n        models.append(model)\n        reduces.append(reduce)\n        \n    print(f'Number of fold: {nb_fold}')\n    print(f'mean of reduce by %: {np.mean(np.array(reduces))}')\n    return models\n    ","b7ea2388":"def predictions_mean(models, x):\n    return np.mean(np.array([model.predict(x) for model in models]), axis = 0)","302ef6b5":"len(df_train), len(df_test)","6584081b":"df_full = pd.concat([df_train, df_test])\ndf_full.reset_index(drop=True, inplace=True)","7b93aacf":"new_fix_features = fix_features.copy()\nfor null_feature in null_features:\n    print(f'----dep features:  {null_feature}  ------')\n    print(f'----indep features: {new_fix_features}')\n    \n    idx_notnull_full = df_full[df_full[null_feature].notnull()].index.tolist()\n    idx_null_full = df_full[df_full[null_feature].isnull()].index.tolist()\n    \n    df_full_notnull = df_full.iloc[idx_notnull_full][fix_features + [null_feature]]\n    \n    models = get_models(df_full_notnull, fix_features, null_feature, nb_fold=30)\n    \n    \n    idx_null_train = df_train[df_train[null_feature].isnull()].index.tolist()\n    df_null = df_train.iloc[idx_null_train][fix_features + [null_feature]]\n    x_test = df_null[fix_features]\n    y_test = predictions_mean(models, x_test)\n    \n    df_train.loc[idx_null_train,null_feature] = y_test\n    \n    idx_null_new = df_test[df_test[null_feature].isnull()].index.tolist()\n    df_null_new = df_test.iloc[idx_null_new][fix_features + [null_feature]]\n    x_new = df_null_new[fix_features]\n    y_new = predictions_mean(models, x_new)\n    \n    df_test.loc[idx_null_new,null_feature] = y_new\n    \n    new_fix_features.append(null_feature)","6059816e":"df_train.to_csv('train_full.csv', index=False)\ndf_test.to_csv('test_full.csv', index=False)","275f8d6e":"## Save new csv","6da2ef60":"My experiment to integently impute the missing values (Before I saw the tutorial from Rob cover the Iterative Imputer which uses the similar concept https:\/\/www.youtube.com\/watch?v=EYySNJU8qR0)\n\nThe final .csv you can find in this dataset here: https:\/\/www.kaggle.com\/dienhoa\/csv-fiiling-missing-values-song-popularity","ff88b411":"## Replace Nan"}}