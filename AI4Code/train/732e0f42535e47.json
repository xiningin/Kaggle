{"cell_type":{"4363b71d":"code","61013fec":"code","ecbe5965":"code","59c1913b":"code","ecd76639":"code","2cec5aec":"code","60752eb2":"code","023bfaf6":"code","527c81d6":"code","bcc6e989":"code","e11922aa":"code","868a7128":"code","fe7d66ec":"code","8fdc5c5a":"code","4dc4c055":"code","3621bce8":"code","780c290e":"code","083ce7c8":"code","009fe552":"code","f9f118c3":"code","c805ba4d":"code","9c0c0c34":"markdown","ec07e152":"markdown","6bfeea1d":"markdown","23be89a7":"markdown","c73830e4":"markdown","dda05cf0":"markdown","a0f0935a":"markdown","cd186e9b":"markdown"},"source":{"4363b71d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport sqlite3\nimport nltk \nimport string\n\n#from sklearn.feature_extraction.text import TfidTransformer\n#from sklearn.feature_extraction.text import TfidVectorizer\n#from sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom nltk.stem.porter import PorterStemmer","61013fec":"con = sqlite3.connect(\"..\/input\/amazon-fine-food-reviews\/database.sqlite\")","ecbe5965":"#selecting those reviews where score is not equal to 3\nfilt_data = pd.read_sql_query(\"select * from Reviews where score != 3 \", con)\n\ndef partition(x):\n    if x < 3:\n        return 'negative'\n    return 'positive'\n\nactual_score = filt_data['Score']\nposneg = actual_score.map(partition)\nfilt_data['Score'] = posneg","59c1913b":"filt_data.head(10)","ecd76639":"#sorting data according to ProductId\nsorted_data = filt_data.sort_values('ProductId' , axis = 0 , ascending = True)","2cec5aec":"#deduplication of data\nfinal = sorted_data.drop_duplicates(subset = {\"UserId\" , \"ProfileName\" , \"Text\" , \"Time\" } , keep = 'first' , inplace = False)\nfinal.shape","60752eb2":"#finding how much % data still remains\n(final['Id'].size * 1.0) \/ (filt_data['Id'].size * 1.0) * 100","023bfaf6":"# we observe that Helpfullness numerator should alwyas be greater than helpfullness denominator\n# the products or tuples that d not follw this rule need to be removed\n\nfinal = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]\nfinal.shape","527c81d6":"plt.figure(figsize = (10,5))\nsns.countplot(final['Score'] , palette = 'gist_rainbow')\nplt.xlabel(\"Reviews\")\nplt.ylabel(\"No. of Reviews\")\nplt.show()\n\nprint(final['Score'].value_counts())","bcc6e989":"import re\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score , precision_score , recall_score\nfrom nltk.metrics.scores import (precision , recall , f_measure)","e11922aa":"# Removing stop words , stemming and lemmetization\n\n#set of stop words\nstop = set(stopwords.words('english'))\nwords_to_keep = set(('not'))\nstop -= words_to_keep\n\n#initialising snowball stemmer\nsno = nltk.stem.SnowballStemmer('english')\n\n\n#removing html tags\ndef cleanhtml(sentence):    #function cleans word of html tags\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr , ' ', sentence)\n    return cleantext\n\n#removing punctuation or special characters\ndef cleanpunc(sentence):    #function cleans words with these symbols\n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return cleaned\n\n# text processing code\ni = 0\nstr1 = ' '\nfinal_string = []\nall_positive_words = []\nall_negative_words = []\nw = ''\n\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    for s in sent.split():\n        for cleaned_words in cleanpunc(s).split():\n            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n                if(cleaned_words.lower() not in stop):\n                    w = (sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(w)\n                    if(final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(w)\n                    if(final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(w)\n                else:\n                    continue\n            else:\n                continue\n                \n    str1 = b\" \".join(filtered_sentence)\n\n    final_string.append(str1)\n    i += 1","868a7128":"#adding an extra column\nfinal['CleanedText'] = pd.Series(final_string)\nfinal['CleanedText'] = final['CleanedText'].str.decode(\"utf-8\")","fe7d66ec":"final.head(10)","8fdc5c5a":"final.isnull().sum()","4dc4c055":"#removing all the rows with null reviews after pre processing\ncleaned_final = final\ncleaned_final.dropna()","3621bce8":"print(final.shape)\nprint(cleaned_final.shape)\ncleaned_final.isnull().sum()","780c290e":"cleaned_final['CleanedText'].replace('', np.nan, inplace=True)\ncleaned_final.dropna(subset=['CleanedText'], inplace=True)\ncleaned_final.shape","083ce7c8":"#sorting the data according to time in asc\ntimesortdata = cleaned_final.sort_values('Time' , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last')\nx = cleaned_final['CleanedText'].values\ny = cleaned_final['Score']\n\nX_train ,X_test ,Y_train ,Y_test = train_test_split(x ,y ,test_size = 0.3, random_state = 0)","009fe552":"countVector = CountVectorizer(min_df = 1000)\nxTrainVector = countVector.fit_transform(X_train)\nxTestVector = countVector.transform(X_test)\n\nstdScale = StandardScaler(with_mean = False)\nxTrVecStd = stdScale.fit_transform(xTrainVector)\nxTsVecStd = stdScale.transform(xTestVector)","f9f118c3":"Depths = [3,4,5,6,7,8,9,10,11,12,13,14,15]\nparamGrid = {'max_depth':Depths}\n\nmodel = GridSearchCV(DecisionTreeClassifier() , paramGrid , scoring = 'accuracy' , cv = 3 , n_jobs = -1 , pre_dispatch = 2)\nmodel.fit(xTrVecStd,Y_train)\n\nprint(\"Accuracy : \", model.score(xTsVecStd , Y_test))\n\n#cross validation\ncv = [1-i for i in model.cv_results_['mean_test_score']]\n\n#optimum depth\nopt_depth = model.best_estimator_.max_depth\nprint(\"Optimal depth : \", opt_depth)\n\n#Decision Tree Classifier with optimal depth\nDT = DecisionTreeClassifier(max_depth = opt_depth)\nDT.fit(xTrVecStd , Y_train)\npredictions = DT.predict(xTsVecStd)","c805ba4d":"precision = precision_score(Y_test, predictions, pos_label = 'positive')\nprint(\"Precision = \" , precision)\n\nrecall = recall_score(Y_test, predictions , pos_label = 'positive')\nprint(\"\\nRecall = \", recall)\n\nf1 = f1_score(Y_test, predictions , pos_label = 'positive')\nprint(\"\\nf1 Score = \", f1)\n","9c0c0c34":"# Building Decision Tree","ec07e152":"# Performance Metrices","6bfeea1d":"If you are new to Text processing, this is for you. Feel free to fork it and upvote if you find it helpful.","23be89a7":"While Pre processing, we'll do:-\n* Removing STOP words.\n* Stemming\n* Lemmetization","c73830e4":"#  Exploratory Data Analysis","dda05cf0":"# Text Processing","a0f0935a":"# Data Cleaning : Remove Error data","cd186e9b":"# Data Cleaning : Deduplication"}}