{"cell_type":{"d53f2d53":"code","1313efca":"code","31fb41bb":"code","4bc91990":"code","48d641e3":"code","a722c3df":"code","a4b881c1":"code","efd94592":"code","78b537b0":"code","0d71c03a":"code","47e9e53c":"code","eba7eaae":"code","45c7ce2a":"code","e878cb58":"code","cca460df":"code","bab22083":"code","6d7138eb":"code","015a4047":"code","55032b45":"code","d56a0ea0":"code","899fe7e1":"code","5194a695":"code","dde01de5":"code","72c0023c":"code","31fdcb17":"code","d948f680":"code","3c7fee73":"code","cc4e90c7":"code","de8c0a45":"code","42d4efb5":"markdown","98dfa7e4":"markdown","507cdf76":"markdown"},"source":{"d53f2d53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1313efca":"# Ignore warning\n\nimport warnings\nwarnings.filterwarnings('ignore')","31fb41bb":"FILEPATH = '\/kaggle\/input\/creditcardfraud\/creditcard.csv'","4bc91990":"df = pd.read_csv(FILEPATH)","48d641e3":"df.info()","a722c3df":"df.head()","a4b881c1":"df.isnull().sum()","efd94592":"import missingno as miss\n\nmiss.matrix(df)","78b537b0":"import seaborn as sns\n\nsns.heatmap(df.corr())","0d71c03a":"X = df.iloc[:, :-1]\ny = df.iloc[:, -1]","47e9e53c":"# Show X Columns\n\nX.columns","eba7eaae":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 56, stratify=y)","45c7ce2a":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","e878cb58":"import matplotlib.pyplot as plt\n\ndef show_confusion_matrix(_model_cm, title = None):\n    \n    f, ax = plt.subplots(figsize = (5, 5))\n    \n    sns.heatmap(_model_cm, annot = True, linewidth = 0, linecolor = 'red', fmt = 'g', ax = ax, cmap = 'Greens')\n    \n    # cmap colors:\n    # YlGnBu, Blues, BuPu, Greens\n    \n    plt.title(title + ' Confusion Matrix')\n    plt.xlabel('y Predict')\n    plt.ylabel('y test')\n    \n    plt.show()","cca460df":"def predict_with_model(model):\n    \n    model = model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return y_pred, accuracy","bab22083":"def show_metrics(model_cm):\n\n    total = sum(sum(model_cm))\n    \n    accuracy = (model_cm[0, 0] + model_cm[1, 1]) \/ total\n    accuracy = float(\"{:.2f}\".format(accuracy))\n\n    sensitivity = model_cm[0, 0] \/ (model_cm[0, 0] + model_cm[0, 1])\n    sensitivity = float(\"{:.2f}\".format(sensitivity))\n\n    specificity = model_cm[1, 1]\/(model_cm[1, 0] + model_cm[1, 1])\n    specificity = float(\"{:.2f}\".format(specificity))\n    \n    print(f'accuracy : {accuracy}, sensitivity : {sensitivity}, specificity : {specificity}')","6d7138eb":"best_model_accuracy = 0\nbest_model = None\n\nmodels = [\n#     MLPClassifier(),\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    LogisticRegression(solver = \"liblinear\"),\n    DecisionTreeClassifier(),\n    GaussianNB()\n]\n\nresults = pd.DataFrame(columns = ['Accuracy'])\n\nfor model in models:\n    \n    model_name = model.__class__.__name__\n\n    y_pred, accuracy = predict_with_model(model)\n    \n    print(\"-\" * 30)\n    print(model_name + \": \" )\n    \n    current_model_cm = confusion_matrix(y_test, y_pred)\n    show_metrics(current_model_cm)\n    \n    results.loc[model_name] = accuracy\n    \n    if(accuracy > best_model_accuracy):\n        best_model_accuracy = accuracy\n        best_model = model_name\n    \n    print(\"Accuracy: {:.2%}\".format(accuracy))\n    \n    show_confusion_matrix(current_model_cm, model_name)","015a4047":"print(\"Best Model : {}\".format(best_model))\nprint(\"Best Model Accuracy : {:.2%}\".format(best_model_accuracy))","55032b45":"results","d56a0ea0":"error_rate_list = []\nrange_min = 1\nrange_max = 10\n\nbest_k_value = 0\nbest_k_error_rate = 100\n\nfor i in range(range_min, range_max):\n    \n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    \n    pred_i = knn.predict(X_test)\n    current_error_rate = np.mean(pred_i != y_test)\n    error_rate_list.append(current_error_rate)\n    \n    if(best_k_error_rate > current_error_rate):\n        best_k_error_rate = current_error_rate\n        best_k_value = i","899fe7e1":"current_error_rate, best_k_value","5194a695":"plt.figure(figsize = (16, 6))\nplt.plot(range(range_min,range_max), error_rate_list, color='green', linestyle = 'dotted', marker = 'o',\n         markerfacecolor = 'green', markersize = 10)\nplt.title('Error Rate vs K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","dde01de5":"knn = KNeighborsClassifier(n_neighbors = i)\ny_pred, accuracy = predict_with_model(knn)\n\ncurrent_model_cm = confusion_matrix(y_test, y_pred)\nshow_metrics(current_model_cm)\n\nresults.loc[model_name] = accuracy","72c0023c":"results","31fdcb17":"!pip install python-vivid","d948f680":"from vivid.utils import timer, get_logger\nlogger = get_logger(__name__)","3c7fee73":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score","cc4e90c7":"def calculate_score(y_true, y_pred):\n    \n    _functions = [\n        accuracy_score,\n        f1_score,\n        precision_score,\n        recall_score\n    ]\n    \n    score_map = {}\n    \n    for func in _functions:\n        score_map[func.__name__] = func(y_true, y_pred)\n        \n    return score_map\n","de8c0a45":"with timer(logger, prefix = '\\ntime taken for svc : '):\n    \n    svc_model = SVC()\n    \n    svc_model.fit(X_train, y_train)\n    y_pred = svc_model.predict(X_test)\n    \n    score = calculate_score(y_test, y_pred)\n    \n    logger.info(score)","42d4efb5":"<font color=\"blue\" size=+1.5><b>Check out my other kernels<\/b><\/font>\n\n<table style=\"font-family: 'Trebuchet MS', Arial, Helvetica, sans-serif;border-collapse: collapse;width: 100%;\">\n  <tr>\n    <th style=\"border: 1px solid #ddd;padding: 8px; padding-top: 12px;padding-bottom: 12px;text-align: left;background-color: #2987E7;color: white;\">Notebook<\/th>\n    <th style=\"border: 1px solid #ddd;padding: 8px; padding-top: 12px;padding-bottom: 12px;text-align: left;background-color: #2987E7;color: white;\">Tags<\/th>\n  <\/tr>\n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/sof-questions-eda-and-visual\">SOF Questions - EDA and Visual<\/a> <\/td>\n    <td style=\"text-align: left\">Data Visual, Plotly<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/netflix-visualization-plotly-plots-treemap\">Netflix - Visualization, Plotly, Plots, and Treemap<\/a> <\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Data Visual, Data Cleaning, Plotly<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/prediction-with-various-algorithms\">Prediction with various Algorithms<\/a> <\/td>\n    <td style=\"text-align: left\">Random Forest, Logistic Regression<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/eda-and-visualization\">EDA and Visualization<\/a> <\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Data Cleaning, Data Visual<\/td>\n  <\/tr>\n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/job-analysis-eda-visual\">Job Analysis - EDA and Visual<\/a> <\/td>\n    <td style=\"text-align: left\">Data Visual, EDA, Plotly<\/td>\n  <\/tr>   \n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/estonia-disaster-visualization\">Estonia Disaster - Visualization<\/a> <\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Data Visual, EDA, Data Cleaning<\/td>\n  <\/tr>\n    \n\n  <tr>\n    <td style=\"background-color: #f2f2f2;text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/prediction-with-various-algorithms\">Credit Card Fraud - Prediction with various algorithms<\/a><\/td>\n    <td style=\"background-color: #f2f2f2;text-align: left\">Various ML Algorithms<\/td>\n  <\/tr>  \n  <tr>\n    <td style=\"text-align: left\"><a href=\"https:\/\/www.kaggle.com\/rajacsp\/linear-equations-real-time\">Linear Equations - Real Time<\/a> <\/td>\n    <td style=\"text-align: left\">Linear Equation<\/td>\n  <\/tr>  \n<\/table>\n","98dfa7e4":"### Improve K Neighbors by Hyper Parameter Tuning","507cdf76":"### Use python-vivid model as an experiment"}}