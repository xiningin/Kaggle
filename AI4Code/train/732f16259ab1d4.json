{"cell_type":{"4af3385a":"code","96adc996":"code","eaf5981e":"code","db8c6d79":"code","49628593":"code","3a1ce8f0":"code","ac10c427":"code","c309ab0a":"code","7f364179":"code","f363bcf2":"code","93682708":"code","664c0c16":"code","c215644a":"code","096772b1":"code","25794023":"code","57289744":"code","06e39463":"code","ca4c1b0a":"code","261874dd":"code","2276b9bc":"code","4ebd0a66":"code","6d3497d9":"code","e69d86bb":"code","25eec7ec":"code","7126dc71":"code","9f4205e3":"code","aea35579":"code","eeb799b3":"code","a8dcf384":"code","ad6ffaa7":"code","b0be6dd2":"code","906666e4":"code","7cbe1389":"code","d244583d":"code","98748f86":"code","ba48a169":"code","21be1922":"code","81c7401d":"code","b96d3927":"code","159f893e":"code","a9322ca9":"code","673e44b2":"code","d0b59317":"code","2500bb7d":"code","c654f5a7":"code","e98f44fe":"code","2e84df81":"code","85497812":"code","fe8910ba":"code","8d008d9b":"code","fdbd971c":"code","15c7495f":"code","fc0338ef":"code","adce5f11":"code","f1645053":"code","7ab23ef0":"code","d49913ac":"code","8c7e0e63":"code","b57878e7":"code","dc057021":"code","b7187a29":"code","7db8be0d":"code","6985fc20":"code","a98ecdc9":"code","058e0f90":"code","4acaec3d":"code","afd2162a":"code","b8b7f040":"code","5f554c38":"code","5541e261":"code","4f1d3dd3":"code","46bf1034":"code","e2309a74":"code","a737ac52":"code","f74bf97c":"code","336949ed":"code","f9d331df":"code","17339761":"code","d699030e":"code","4a46fbeb":"markdown","9b62a830":"markdown","90fb80e2":"markdown","e6e0e970":"markdown","7b8ebafc":"markdown","09bec1b6":"markdown","839e2b11":"markdown","b65d6108":"markdown","aa5b98ee":"markdown","f37cfb52":"markdown","bb642b52":"markdown","1f35d932":"markdown","9ddb2461":"markdown","2c9a1e35":"markdown","022d274a":"markdown","3691a91e":"markdown","f7c99d17":"markdown","eab9f7bd":"markdown","c7e908e7":"markdown","15f47107":"markdown","9f3a4b51":"markdown","5f97884e":"markdown","a9451997":"markdown","4b199b6a":"markdown","2bdc2603":"markdown","1d1398b3":"markdown","3a311c1d":"markdown","baa5ff11":"markdown","2c44bdb2":"markdown","27645de1":"markdown","27076b7d":"markdown","01bf5f16":"markdown","51a873b8":"markdown","0593a6ec":"markdown","53d6a086":"markdown","d022c646":"markdown","c27e1815":"markdown","42439edb":"markdown","91705a65":"markdown","1e41a5b2":"markdown","5c8a1cd4":"markdown","4add6283":"markdown","013e918c":"markdown","1e8b12f4":"markdown","3738ecb6":"markdown","c77ed9fc":"markdown","eed2cbae":"markdown","d1f0a5ce":"markdown","7e22381c":"markdown","01477266":"markdown","084da484":"markdown","a0227040":"markdown","8e728e0a":"markdown","23f16e94":"markdown","7dd5e81e":"markdown","7ccdcb45":"markdown","d81668ce":"markdown","4651199d":"markdown","9ec4f871":"markdown","3a439063":"markdown","a8283b17":"markdown","64940aa7":"markdown","528a5abe":"markdown","02bb1f6d":"markdown","037b6948":"markdown","21690ba6":"markdown","11e07899":"markdown","faeb1501":"markdown","e26a1541":"markdown","29d57cd7":"markdown","cf1ad367":"markdown","7e17b80d":"markdown","29ffc8cc":"markdown","224f6f36":"markdown","bf7b1243":"markdown","3e6d4360":"markdown","1ddd0573":"markdown","e3fd291f":"markdown","bc46ddc0":"markdown","02d9a8ef":"markdown","5f457cc0":"markdown","eccf7609":"markdown","2c6e7f70":"markdown","7d402a2c":"markdown","4d6abf00":"markdown","cf6560d9":"markdown","057dbbb8":"markdown","4e0f2837":"markdown","9f328f07":"markdown","029097ef":"markdown","4c8ee25b":"markdown","8e7d50c9":"markdown","d181d472":"markdown","7b59e4ec":"markdown","5383a84f":"markdown"},"source":{"4af3385a":"import matplotlib.pyplot as plt\nfrom os import listdir\nfrom os.path import isfile, join\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport math, os, random, cv2\nimport matplotlib.animation as animation\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torchvision.utils import save_image\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim.optimizer import Optimizer, required\nfrom torch import Tensor\nfrom torch.nn import Parameter\nfrom IPython.display import HTML\nimport time\nimport matplotlib.image as mpimg\n\nos.makedirs(\"datas\/faces_64\")\nos.makedirs(\"datas\/faces_128\")\n\noriginal_resize=1000\nimg_size=64\nnew_faces_path=\".\/datas\/faces_\"+str(img_size)","96adc996":"face_cascade = cv2.CascadeClassifier('..\/input\/haar-cascades-for-face-detection\/haarcascade_frontalface_alt2.xml')","eaf5981e":"\"\"\"\noriginal_images_path='..\/input\/wikiart-gangogh-creating-art-gan\/portrait'\nonlyfiles = [f for f in listdir(original_images_path) if isfile(join(original_images_path, f))]\n\"\"\"","db8c6d79":"\"\"\"\ntotal_faces_count=0\nface_per_image=[]\ni=0\nverbose=False\n\nfor file in tqdm(onlyfiles[i:]):\n    i+=1\n    # Read the input image\n    img = cv2.imread(original_images_path+\"\/\"+file)\n\n    if max(img.shape[0],img.shape[1])>original_resize:\n        if verbose:\n            print(\"Resizing\")\n        img = cv2.resize(img, (original_resize,int((original_resize\/img.shape[1])*img.shape[0])), interpolation = cv2.INTER_AREA)\n        \n    \n    # Convert into grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Detect faces\n    faces_detect = face_cascade.detectMultiScale(gray, 1.1, 4)\n    face_per_image.append(len(faces_detect))\n    \n    if len(faces_detect)>0:\n        for (x, y, w, h) in faces_detect:\n            total_faces_count+=1\n            borders=int(min(img.shape[0],img.shape[1])\/7)\n            \n            faces = img[max(y-borders,0):min(y + h + borders,img.shape[0]), max(x-borders,0):min(x + w + borders,img.shape[1])]\n            \n            faces = cv2.resize(faces, (img_size,img_size), interpolation = cv2.INTER_AREA)\n            cv2.imwrite(new_faces_path+\"\/\"+\"face_\"+str(i)+\".jpg\", faces)\n            \nfaces_per_image_count=pd.Series(face_per_image).value_counts()   \n\nprint(\"original_resize {0} \\n\".format(original_resize))\nprint(\"img_size {0} \\n\".format(img_size))\nprint(\"original_images_path %s \\n\"%(original_images_path))\nprint(\"new_faces_path %s \\n\"%(new_faces_path))\n\nprint(\"=========================\")\nfor i in np.sort(faces_per_image_count.index):\n    print(\"Images with \"+str(i)+\" faces : \"+str(faces_per_image_count[i])+\" \/ \"+str(round(faces_per_image_count[i]\/sum(faces_per_image_count),2)))\n\nprint(\"Total number of images : \"+str(faces_per_image_count.sum()))\n\nprint(\"Total number of faces : \"+str(total_faces_count))\n\"\"\"","49628593":"#Alternatively faces from preprocessed faces-64 or faces-128 can be used\nfrom distutils.dir_util import copy_tree\ncopy_tree(\"..\/input\/portrait-faces-\"+str(img_size)+\"\/faces_\"+str(img_size),\"datas\/faces_\"+str(img_size)+\"\/faces\")","3a1ce8f0":"# Number of workers for dataloader\nworkers = 2\n\n# Numbers of GPUs\nngpu = 1\n\n# Batch size during training\nbatch_size = 64","ac10c427":"mydir = \"datas\/faces_\"+str(img_size)+\"\/faces\"\n\nfilelist = [ f for f in os.listdir(mydir)]\nfor f in filelist:\n    if random.random()>0.5:\n        os.remove(os.path.join(mydir, f))","c309ab0a":"root_path=\".\/datas\/faces_\"+str(img_size)\n\n#Create the dataset\ndataset = dset.ImageFolder(root=root_path,\n                           transform=transforms.Compose([\n                               transforms.Resize(img_size),\n                               transforms.CenterCrop(img_size),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n                                         shuffle=True, num_workers=workers,drop_last=True)\n\n#Define the devide to be used\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")","7f364179":"real_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","f363bcf2":"# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 60\n\n# Starting learning rate for optimizers\nlr = 0.0006\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available\nngpu = 1\n\n# Establish convention for real and fake labels during training (set one sided label smoothing eventually by modifying real_label)\nreal_label = 1.0\nfake_label = 0.\n\n#Options to use or not for training\nuse_feature_matching=False\nuse_minibatch_discrimination=False\n\n#Save models or not during training\nsave_models=True\n\n#Path to the saved models\nos.makedirs(\"models\/netG\")\nos.makedirs(\"models\/netD\")","93682708":"! pip3 install torchgan","664c0c16":"from torchgan.layers import MinibatchDiscrimination1d","c215644a":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","096772b1":"class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            \n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            \n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            \n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            \n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)","25794023":"DCGAN_netG = Generator(ngpu).to(device)\nDCGAN_netG.apply(weights_init)\n\nprint(DCGAN_netG)","57289744":"if use_minibatch_discrimination:\n    main2_size=4096+128\nelse:\n    main2_size=4096","06e39463":"class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            #nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False)\n            nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 16),\n            # state size. (ndf*16) x 2 x 2\n        )\n        self.main2 = nn.Sequential(\n            nn.Linear(main2_size,1),\n            nn.Sigmoid()\n        )\n        self.minibatchdis=nn.Sequential(\n            MinibatchDiscrimination1d(in_features=4096,out_features=128,intermediate_features=16),\n        )\n        \n    def forward(self, input,matching = False):\n        output = self.main(input)\n        feature = output.view(-1,4096)\n        #output = self.main2(feature)\n        \n        output = output.view(-1,4096)\n        if use_minibatch_discrimination:\n            output=self.minibatchdis(output)\n            \n        output=self.main2(output)\n        \n        if matching == True:\n            return feature,output\n        else:\n            return output #batch_size x 1 x 1 x 1 => batch_size","ca4c1b0a":"DCGAN_netD = Discriminator(ngpu).to(device)\nDCGAN_netD.apply(weights_init)\n\nprint(DCGAN_netD)","261874dd":"# Initialize Loss functions\ncriterion = nn.BCELoss()\ncriterionG = nn.MSELoss()\n\n# Create batch of latent vectors that we will use to visualize the progression of the generator\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(DCGAN_netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(DCGAN_netG.parameters(), lr=lr, betas=(beta1, 0.999))\n\n#Setup schedulers for learning rate decrease\nschedulerD = optim.lr_scheduler.StepLR(optimizerD, step_size=int(num_epochs\/4), gamma=0.5)\nschedulerG = optim.lr_scheduler.StepLR(optimizerG, step_size=int(num_epochs\/4), gamma=0.5)","2276b9bc":"# Training Loop\nstarting_time=time.time()\n\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\nD_G_z1s=[]\nD_G_z2s=[]\nD_xs=[]\niters = 0\n\nprint(\"Starting Training Loop...\")\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        ############################\n        # (1) TRAIN DISCRIMINATOR\n        ###########################\n        ## Train with all-real batch\n        DCGAN_netD.zero_grad()\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        \n        #Fill the label with the \"real_label\" using one_sided smoothing\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        \n        # Forward pass real batch through D\n        output = DCGAN_netD(real_cpu).view(-1)\n        errD_real = criterion(output, label)\n        \n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        \n        # Calculate the mean value of D(x) with x real images\n        D_x = output.mean().item()\n\n              \n        ## Train with all-fake batch\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        fake = DCGAN_netG(noise) #generating the fake images\n        label.fill_(fake_label) #replace the content of the previous label variable with the fake label (no smoothing)\n        \n        # Forward pass fake batch through D\n        output = DCGAN_netD(fake.detach()).view(-1)\n        errD_fake = criterion(output, label)\n        \n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        \n        #Calculate the mean value of D(G(z)) with G(z) generated image\n        D_G_z1 = output.mean().item()\n        \n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        \n        # Update D optimizer\n        optimizerD.step()\n        \n        \n        ############################\n        # (2) TRAIN GENERATOR\n        ###########################\n        DCGAN_netG.zero_grad() #reset G gradient\n        label.fill_(real_label)  # all labels are set to real (and used with generated images to get the generator loss through the Discriminator)\n        \n        if use_feature_matching==True:\n            ##Feature matching\n            feature_real,_=netD(real_cpu,matching=True)\n            feature_fake,output=netD(fake,matching=True)\n            feature_real=torch.mean(feature_real,0)\n            feature_fake=torch.mean(feature_fake,0)\n            feature_matching_error=criterionG(feature_fake,feature_real.detach())\n        else:\n            # Perform another forward pass of all-fake batch through D\n            feature_matching_error=0\n            output = DCGAN_netD(fake).view(-1)\n\n        # Add the BCE loss to feature matching (if activated)\n        errG = 1*feature_matching_error+criterion(output.view(-1), label)\n        \n        # Calculate gradients for G\n        errG.backward()\n        \n        #Calculate the mean value of D(G(z)) with G(z) generated image\n        output = DCGAN_netD(fake).view(-1)        \n        D_G_z2 = output.mean().item()\n\n        # Update G\n        optimizerG.step()\n\n        \n        ############################\n        # TRAINING STATS\n        ###########################\n        if i % 1 == 0:\n            print('[%d\/%d][%d\/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\t(inc feature matching loss: %.4f)\\tD(x): %.4f\\tD(G(z)): %.4f \/ %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(),feature_matching_error, D_x, D_G_z1, D_G_z2))\n\n        # Save Losses\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Save mean values of D(G(z)) and D(x)\n        D_G_z1s.append(D_G_z1)\n        D_G_z2s.append(D_G_z2)\n        D_xs.append(D_x)      \n        \n        # Save the image produced by the generator at various stages using the same noise input\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = DCGAN_netG(fixed_noise).detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n            \n        iters += 1\n        \n    if ((epoch+1)%5)==0 and save_models:\n        #Save models at various stages\n        torch.save(DCGAN_netD.state_dict(), \"models\/netD\/DCGAN_netD_\"+str(epoch+1)+\".pt\")\n        torch.save(DCGAN_netG.state_dict(), \"models\/netG\/DCGAN_netG_\"+str(epoch+1)+\".pt\")\n    \n    #Update schedulers\n    schedulerG.step()\n    schedulerD.step()\n\ntraining_time=time.time()-starting_time","4ebd0a66":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training (training time %4.f s.)\"%(training_time))\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","6d3497d9":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training (MA=165)\")\nplt.plot(np.convolve(G_losses, np.ones(165)\/165, mode='valid'),label=\"G\")\nplt.plot(np.convolve(D_losses, np.ones(165)\/165, mode='valid'),label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","e69d86bb":"plt.figure(figsize=(10,5))\nplt.title(\"Disciminator mean values\")\nplt.plot(D_G_z1s,label=\"D(G(z)) mean values (1)\")\nplt.plot(D_G_z2s,label=\"D(G(z)) mean values (2)\")\nplt.plot(D_xs,label=\"D(x) mean values\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","25eec7ec":"plt.figure(figsize=(10,5))\nplt.title(\"Disciminator mean values (MA=165)\")\nplt.plot(np.convolve(D_G_z1s, np.ones(165)\/165, mode='valid'),label=\"D(G(z)) mean values (1)\")\nplt.plot(np.convolve(D_G_z2s, np.ones(165)\/165, mode='valid'),label=\"D(G(z)) mean values (2)\")\nplt.plot(np.convolve(D_xs, np.ones(165)\/165, mode='valid'),label=\"D(x) mean values\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","7126dc71":"fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","9f4205e3":"real_batch = next(iter(dataloader))\n\n# Plot real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","aea35579":"def tensor2var(x, grad=False):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x, requires_grad=grad)\n\ndef var2tensor(x):\n    return x.data.cpu()\n\ndef var2numpy(x):\n    return x.data.cpu().numpy()\n\ndef denorm(x):\n    out = (x + 1) \/ 2\n    return out.clamp_(0, 1)\n\ndef l2normalize(v, eps=1e-12):\n    return v \/ (v.norm() + eps)","eeb799b3":"class SpectralNorm(nn.Module):\n    def __init__(self, module, name='weight', power_iterations=1):\n        super(SpectralNorm, self).__init__()\n        self.module = module\n        self.name = name\n        self.power_iterations = power_iterations\n        if not self._made_params():\n            self._make_params()\n\n    def _update_u_v(self):\n        u = getattr(self.module, self.name + \"_u\")\n        v = getattr(self.module, self.name + \"_v\")\n        w = getattr(self.module, self.name + \"_bar\")\n\n        height = w.data.shape[0]\n        for _ in range(self.power_iterations):\n            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n\n        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n        sigma = u.dot(w.view(height, -1).mv(v))\n        setattr(self.module, self.name, w \/ sigma.expand_as(w))\n\n    def _made_params(self):\n        try:\n            u = getattr(self.module, self.name + \"_u\")\n            v = getattr(self.module, self.name + \"_v\")\n            w = getattr(self.module, self.name + \"_bar\")\n            return True\n        except AttributeError:\n            return False\n\n\n    def _make_params(self):\n        w = getattr(self.module, self.name)\n\n        height = w.data.shape[0]\n        width = w.view(height, -1).data.shape[1]\n\n        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n        u.data = l2normalize(u.data)\n        v.data = l2normalize(v.data)\n        w_bar = Parameter(w.data)\n\n        del self.module._parameters[self.name]\n\n        self.module.register_parameter(self.name + \"_u\", u)\n        self.module.register_parameter(self.name + \"_v\", v)\n        self.module.register_parameter(self.name + \"_bar\", w_bar)\n\n\n    def forward(self, *args):\n        self._update_u_v()\n        return self.module.forward(*args)","a8dcf384":"gammas=[]","ad6ffaa7":"class Self_Attn(nn.Module):\n    \"\"\" Self attention Layer\"\"\"\n    def __init__(self,in_dim,activation):\n        super(Self_Attn,self).__init__()\n        self.chanel_in = in_dim\n        self.activation = activation\n        \n        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim\/\/8 , kernel_size= 1)\n        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim\/\/8 , kernel_size= 1)\n        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax  = nn.Softmax(dim=-1) #\n    def forward(self,x):\n        \"\"\"\n            inputs :\n                x : input feature maps( B X C X W X H)\n            returns :\n                out : self attention value + input feature \n                attention: B X N X N (N is Width*Height)\n        \"\"\"\n        m_batchsize,C,width ,height = x.size()\n        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\n        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n        energy =  torch.bmm(proj_query,proj_key) # transpose check\n        attention = self.softmax(energy) # BX (N) X (N) \n        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n\n        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n        out = out.view(m_batchsize,C,width,height)\n        \n        gammas.append(self.gamma.item()) #evolution of the learnable parameter\n        \n        out = self.gamma*out + x\n        return out,attention","b0be6dd2":"class Generator(nn.Module):\n    \"\"\"Generator.\"\"\"\n\n    def __init__(self, batch_size, image_size=64, z_dim=100, conv_dim=64):\n        super(Generator, self).__init__()\n        self.imsize = image_size\n        layer1 = []\n        layer2 = []\n        layer3 = []\n        last = []\n\n        repeat_num = int(np.log2(self.imsize)) - 3\n        mult = 2 ** repeat_num # 8\n        layer1.append(SpectralNorm(nn.ConvTranspose2d(z_dim, conv_dim * mult, 4)))\n        layer1.append(nn.BatchNorm2d(conv_dim * mult))\n        layer1.append(nn.ReLU())\n\n        curr_dim = conv_dim * mult\n\n        layer2.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim \/ 2), 4, 2, 1)))\n        layer2.append(nn.BatchNorm2d(int(curr_dim \/ 2)))\n        layer2.append(nn.ReLU())\n\n        curr_dim = int(curr_dim \/ 2)\n\n        layer3.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim \/ 2), 4, 2, 1)))\n        layer3.append(nn.BatchNorm2d(int(curr_dim \/ 2)))\n        layer3.append(nn.ReLU())\n\n        if self.imsize == 64:\n            layer4 = []\n            curr_dim = int(curr_dim \/ 2)\n            layer4.append(SpectralNorm(nn.ConvTranspose2d(curr_dim, int(curr_dim \/ 2), 4, 2, 1)))\n            layer4.append(nn.BatchNorm2d(int(curr_dim \/ 2)))\n            layer4.append(nn.ReLU())\n            self.l4 = nn.Sequential(*layer4)\n            curr_dim = int(curr_dim \/ 2)\n\n        self.l1 = nn.Sequential(*layer1)\n        self.l2 = nn.Sequential(*layer2)\n        self.l3 = nn.Sequential(*layer3)\n\n        last.append(nn.ConvTranspose2d(curr_dim, 3, 4, 2, 1))\n        last.append(nn.Tanh())\n        self.last = nn.Sequential(*last)\n\n        self.attn1 = Self_Attn( 128, 'relu')\n        self.attn2 = Self_Attn( 64,  'relu')\n\n    def forward(self, z):\n        z = z.view(z.size(0), z.size(1), 1, 1)\n        out=self.l1(z)\n        out=self.l2(out)\n        out=self.l3(out)\n        out,p1 = self.attn1(out)\n        out=self.l4(out)\n        \n        out,p2 = self.attn2(out)\n\n        out=self.last(out)\n\n        return out, p1, p2","906666e4":"class Discriminator(nn.Module):\n    \"\"\"Discriminator, Auxiliary Classifier.\"\"\"\n\n    def __init__(self, batch_size=64, image_size=64, conv_dim=64):\n        super(Discriminator, self).__init__()\n        self.imsize = image_size\n        layer1 = []\n        layer2 = []\n        layer3 = []\n        last = []\n\n        layer1.append(SpectralNorm(nn.Conv2d(3, conv_dim, 4, 2, 1)))\n        layer1.append(nn.LeakyReLU(0.1))\n\n        curr_dim = conv_dim\n\n        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n        layer2.append(nn.LeakyReLU(0.1))\n        curr_dim = curr_dim * 2\n\n        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n        layer3.append(nn.LeakyReLU(0.1))\n        curr_dim = curr_dim * 2\n\n        if self.imsize == 64:\n            layer4 = []\n            layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n            layer4.append(nn.LeakyReLU(0.1))\n            self.l4 = nn.Sequential(*layer4)\n            curr_dim = curr_dim*2\n        self.l1 = nn.Sequential(*layer1)\n        self.l2 = nn.Sequential(*layer2)\n        self.l3 = nn.Sequential(*layer3)\n\n        last.append(nn.Conv2d(curr_dim, 1, 4))\n        self.last = nn.Sequential(*last)\n\n        self.attn1 = Self_Attn(256, 'relu')\n        self.attn2 = Self_Attn(512, 'relu')\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.l2(out)\n        out = self.l3(out)\n        out,p1 = self.attn1(out)\n        out=self.l4(out)\n        out,p2 = self.attn2(out)\n        out=self.last(out)\n        \n        return out.squeeze(), p1, p2","7cbe1389":"adv_loss=\"wgan-gp\"\nnz=128\nngf=64\nndf=64\nlambda_gp=10\ng_lr=0.0001\nd_lr=0.0004\nlr_decay=0.95\nbeta1=0.0\nbeta2=0.9\nnum_epochs=60\nsave_models=True","d244583d":"SAGAN_netG = Generator(batch_size,img_size, nz, ngf).cuda()\nSAGAN_netD = Discriminator(batch_size,img_size, ndf).cuda()\nprint(SAGAN_netG)\nprint(SAGAN_netD)\n\n# Loss and optimizer\n# self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\ng_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, SAGAN_netG.parameters()), g_lr, [beta1, beta2])\nd_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, SAGAN_netD.parameters()), d_lr, [beta1, beta2])\n\nc_loss = torch.nn.CrossEntropyLoss()","98748f86":"#Evolution measures lists\nimg_list = []\ngf2s_1024_list=[]\ngf2s_528_list=[]\n\nd_losses=[]\nd_losses_gp=[]\ng_losses=[]\n\nD_G_z1s=[]\nD_G_z2s=[]\nD_xs=[]\n\n\n# Fixed input\nfixed_z = tensor2var(torch.randn(batch_size, nz))\n\n# Start time\nstart_time = time.time()\niters=0","ba48a169":"print(\"Start of the training\")\nfor epoch in range(num_epochs):\n    for i,data in enumerate(dataloader):   \n        SAGAN_netD.train() \n        SAGAN_netG.train()\n        \n        ############################\n        # (1) TRAIN DISCRIMINATOR\n        ###########################\n        \n        #With real images\n        real_images=data[0]\n        real_images = tensor2var(real_images)\n        d_out_real,dr1,dr2 = SAGAN_netD(real_images)\n        \n        if adv_loss == 'wgan-gp':\n            d_loss_real = - torch.mean(d_out_real)\n        elif adv_loss == 'hinge':\n            d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()    \n        \n        D_x=d_loss_real\n        D_xs.append(D_x)\n        \n        #With fake images\n        z = tensor2var(torch.randn(real_images.size(0), nz))\n        fake_images,gf1,gf2 = SAGAN_netG(z)\n        d_out_fake,df1,df2 = SAGAN_netD(fake_images)\n\n        if adv_loss == 'wgan-gp':\n            d_loss_fake = d_out_fake.mean()\n        elif adv_loss == 'hinge':\n            d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\n        \n        D_G_z1=d_loss_fake\n        D_G_z1s.append(D_G_z1)\n        #Backward pass\n        d_loss = d_loss_real + d_loss_fake\n        \n        d_optimizer.zero_grad()\n        g_optimizer.zero_grad()\n        d_loss.backward()\n        d_optimizer.step()\n        \n\n        if adv_loss == 'wgan-gp':\n            # Gradient penalty\n            alpha = torch.rand(real_images.size(0), 1, 1, 1).cuda().expand_as(real_images)\n            interpolated = Variable(alpha * real_images.data + (1 - alpha) * fake_images.data, requires_grad=True)\n            out,_,_ = SAGAN_netD(interpolated)\n\n            grad = torch.autograd.grad(outputs=out,\n                                       inputs=interpolated,\n                                       grad_outputs=torch.ones(out.size()).cuda(),\n                                       retain_graph=True,\n                                       create_graph=True,\n                                       only_inputs=True)[0]\n\n            grad = grad.view(grad.size(0), -1)\n            grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n            d_loss_gp = torch.mean((grad_l2norm - 1) ** 2)\n\n            # Backward pass\n            d_loss_gp = lambda_gp * d_loss_gp\n\n            d_optimizer.zero_grad()\n            g_optimizer.zero_grad()\n            d_loss_gp.backward()\n            d_optimizer.step()\n            \n            \n        ############################\n        # (2) TRAIN GENERATOR\n        ###########################\n        z = tensor2var(torch.randn(real_images.size(0), nz))\n        fake_images,_,_ = SAGAN_netG(z)\n\n        # With fake images\n        g_out_fake,_,_ = SAGAN_netD(fake_images)  # batch x n\n        if adv_loss == 'wgan-gp':\n            g_loss_fake = - g_out_fake.mean()\n        elif adv_loss == 'hinge':\n            g_loss_fake = - g_out_fake.mean()\n\n        D_G_z2=g_loss_fake\n        D_G_z2s.append(D_G_z2)\n\n        d_optimizer.zero_grad()\n        g_optimizer.zero_grad()\n        g_loss_fake.backward()\n        g_optimizer.step()      \n\n        \n        ############################\n        # TRAINING STATS\n        ###########################\n        if i % 1 == 0:\n            print('[%d\/%d][%d\/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\t(inc gp loss: %.4f)\\tD(x): %.4f\\tD(G(z)): %.4f \/ %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     d_loss.item(), g_loss_fake.item(),d_loss_gp, D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        d_losses.append(d_loss.item())\n        d_losses_gp.append(d_loss_gp.item())\n        g_losses.append(g_loss_fake.item())\n\n        D_G_z1s.append(D_G_z1)\n        D_G_z2s.append(D_G_z2)\n        D_xs.append(D_x)      \n        \n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake,gf1,gf2 = SAGAN_netG(fixed_z)\n                fake=fake.detach().cpu()\n            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n            gf2s_528_list.append(vutils.make_grid(gf2[:,:,527].view(64,32,32).detach().unsqueeze(0).permute(1,0,2,3).to(device)[:64], padding=5, normalize=True,pad_value=1).cpu())\n            gf2s_1024_list.append(vutils.make_grid(gf2[:,:,1023].view(64,32,32).detach().unsqueeze(0).permute(1,0,2,3).to(device)[:64], padding=5, normalize=True,pad_value=1).cpu())\n        \n        iters += 1\n        \n    if ((epoch+1)%5)==0 and save_models:\n        torch.save(SAGAN_netD.state_dict(), \"models\/netD\/SAGAN_netD_\"+str(epoch+1)+\".pt\")\n        torch.save(SAGAN_netG.state_dict(), \"models\/netG\/SAGAN_netG_\"+str(epoch+1)+\".pt\")\n\ntraining_time=time.time()-starting_time","21be1922":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training (training time %4.f s.)\"%(training_time))\nplt.plot(g_losses,label=\"Generator\")\nplt.plot(d_losses_gp,label=\"Gradient Penalty Loss\")\nplt.plot(d_losses,label=\"Discriminator\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","81c7401d":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training (MA=165)\")\nplt.plot(np.convolve(g_losses, np.ones(165)\/165, mode='valid'),label=\"G\")\nplt.plot(np.convolve(d_losses_gp, np.ones(165)\/165, mode='valid'),label=\"Gradient Penalty\")\nplt.plot(np.convolve(d_losses, np.ones(165)\/165, mode='valid'),label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","b96d3927":"plt.figure(figsize=(10,5))\nplt.title(\"Gamma parameter (attention) evolution\")\nplt.plot(np.abs(gammas),label=\"|Gamma|\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","159f893e":"fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","a9322ca9":"real_batch = next(iter(dataloader))\n\n# Plot real images\nplt.figure(figsize=(15,15))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","673e44b2":"fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in gf2s_528_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","d0b59317":"fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in gf2s_1024_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","2500bb7d":"import torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.utils as vutils\nimport torch.nn as nn\nfrom tqdm import tqdm","c654f5a7":"#Create the dataset\ndataset = dset.ImageFolder(root=\"..\/input\/portrait-faces-64\",\n                           transform=transforms.Compose([\n                               transforms.Resize(64),\n                               transforms.CenterCrop(64),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=64)\n\n#Define the devide to be used\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")","e98f44fe":"real_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Reference real images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","2e84df81":"!pip install pytorch-fid-wrapper","85497812":"import  pytorch_fid_wrapper as pfw","fe8910ba":"pfw.set_config(batch_size=64, device=\"cuda\")","8d008d9b":"def evaluate_FID(path,plot=False,n_iters=1,verbose=True,latent_dim=100,generator_type=\"dcgan\"):\n    if generator_type==\"sagan\":\n        SAGAN_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        SAGAN_netG.eval()\n    elif generator_type==\"dcgan\":\n        DCGAN_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        DCGAN_netG.eval()\n    elif generator_type==\"dcgan128\":\n        DCGAN128_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        DCGAN128_netG.eval()\n        \n    result_list=[]\n    for i in tqdm(range(n_iters)):\n        real_images=next(iter(dataloader))[0]\n        real_m, real_s = pfw.get_stats(real_images)\n        noise = torch.randn(64, latent_dim, 1, 1, device=\"cuda\")\n        if generator_type==\"sagan\":\n            fake_images = SAGAN_netG(noise)[0].detach().cpu()\n        elif generator_type==\"dcgan\":\n            fake_images = DCGAN_netG(noise).detach().cpu()\n        elif generator_type==\"dcgan128\":\n            fake_images = DCGAN128_netG(noise).detach().cpu()\n            \n        val_fid = pfw.fid(fake_images, real_m=real_m, real_s=real_s)\n        if verbose:\n            print(\"FID : %.4f\"%(val_fid))\n        result_list.append(val_fid)\n        \n    if plot:\n        # Plot real images\n        plt.figure(figsize=(15,15))\n        plt.subplot(1,2,1)\n        plt.axis(\"off\")\n        plt.title(\"Real Images\")\n        plt.imshow(np.transpose(vutils.make_grid(real_images.to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n        #Plot the fake images\n        plt.subplot(1,2,2)\n        plt.axis(\"off\")\n        plt.title(\"Fake Images (FID = %.4f)\"%(val_fid))\n        plt.imshow(np.transpose(vutils.make_grid(fake_images.to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n        plt.show()\n    \n    mean=np.mean(result_list)\n    std=np.std(result_list)\n    print(\"Mean FID %.4f +- %.4f\"%(mean,std))\n    \n    return(result_list)","fdbd971c":"from PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torchvision\n\n# Gaussian blur kernel\ndef get_gaussian_kernel(device=\"cpu\"):\n    kernel = np.array([\n        [1, 4, 6, 4, 1],\n        [4, 16, 24, 16, 4],\n        [6, 24, 36, 24, 6],\n        [4, 16, 24, 16, 4],\n        [1, 4, 6, 4, 1]], np.float32) \/ 256.0\n    gaussian_k = torch.as_tensor(kernel.reshape(1, 1, 5, 5)).to(device)\n    return gaussian_k\n\ndef pyramid_down(image, device=\"cpu\"):\n    gaussian_k = get_gaussian_kernel(device=device)        \n    # channel-wise conv(important)\n    multiband = [F.conv2d(image[:, i:i + 1,:,:], gaussian_k, padding=2, stride=2) for i in range(3)]\n    down_image = torch.cat(multiband, dim=1)\n    return down_image\n\ndef pyramid_up(image, device=\"cpu\"):\n    gaussian_k = get_gaussian_kernel(device=device)\n    upsample = F.interpolate(image, scale_factor=2)\n    multiband = [F.conv2d(upsample[:, i:i + 1,:,:], gaussian_k, padding=2) for i in range(3)]\n    up_image = torch.cat(multiband, dim=1)\n    return up_image\n\ndef gaussian_pyramid(original, n_pyramids, device=\"cpu\"):\n    x = original\n    # pyramid down\n    pyramids = [original]\n    for i in range(n_pyramids):\n        x = pyramid_down(x, device=device)\n        pyramids.append(x)\n    return pyramids\n\ndef laplacian_pyramid(original, n_pyramids, device=\"cpu\"):\n    # create gaussian pyramid\n    pyramids = gaussian_pyramid(original, n_pyramids, device=device)\n\n    # pyramid up - diff\n    laplacian = []\n    for i in range(len(pyramids) - 1):\n        diff = pyramids[i] - pyramid_up(pyramids[i + 1], device=device)\n        laplacian.append(diff)\n    # Add last gaussian pyramid\n    laplacian.append(pyramids[len(pyramids) - 1])        \n    return laplacian\n\ndef minibatch_laplacian_pyramid(image, n_pyramids, batch_size, device=\"cpu\"):\n    n = image.size(0) \/\/ batch_size + np.sign(image.size(0) % batch_size)\n    pyramids = []\n    for i in range(n):\n        x = image[i * batch_size:(i + 1) * batch_size]\n        p = laplacian_pyramid(x.to(device), n_pyramids, device=device)\n        p = [x.cpu() for x in p]\n        pyramids.append(p)\n    del x\n    result = []\n    for i in range(n_pyramids + 1):\n        x = []\n        for j in range(n):\n            x.append(pyramids[j][i])\n        result.append(torch.cat(x, dim=0))\n    return result\n\ndef extract_patches(pyramid_layer, slice_indices,\n                    slice_size=7, unfold_batch_size=128, device=\"cpu\"):\n    assert pyramid_layer.ndim == 4\n    n = pyramid_layer.size(0) \/\/ unfold_batch_size + np.sign(pyramid_layer.size(0) % unfold_batch_size)\n    # random slice 7x7\n    p_slice = []\n    for i in range(n):\n        # [unfold_batch_size, ch, n_slices, slice_size, slice_size]\n        ind_start = i * unfold_batch_size\n        ind_end = min((i + 1) * unfold_batch_size, pyramid_layer.size(0))\n        x = pyramid_layer[ind_start:ind_end].unfold(\n                2, slice_size, 1).unfold(3, slice_size, 1).reshape(\n                ind_end - ind_start, pyramid_layer.size(1), -1, slice_size, slice_size)\n        # [unfold_batch_size, ch, n_descriptors, slice_size, slice_size]\n        x = x[:,:, slice_indices,:,:]\n        # [unfold_batch_size, n_descriptors, ch, slice_size, slice_size]\n        p_slice.append(x.permute([0, 2, 1, 3, 4]))\n    # sliced tensor per layer [batch, n_descriptors, ch, slice_size, slice_size]\n    x = torch.cat(p_slice, dim=0)\n    # normalize along ch\n    std, mean = torch.std_mean(x, dim=(0, 1, 3, 4), keepdim=True)\n    x = (x - mean) \/ (std + 1e-8)\n    # reshape to 2rank\n    x = x.reshape(-1, 3 * slice_size * slice_size)\n    return x\n        \ndef swd(image1, image2, \n        n_pyramids=None, slice_size=7, n_descriptors=128,\n        n_repeat_projection=128, proj_per_repeat=4, device=\"cpu\", return_by_resolution=False,\n        pyramid_batchsize=128):\n    # n_repeat_projectton * proj_per_repeat = 512\n    # Please change these values according to memory usage.\n    # original = n_repeat_projection=4, proj_per_repeat=128    \n    assert image1.size() == image2.size()\n    assert image1.ndim == 4 and image2.ndim == 4\n\n    if n_pyramids is None:\n        n_pyramids = int(np.rint(np.log2(image1.size(2) \/\/ 16)))\n    with torch.no_grad():\n        # minibatch laplacian pyramid for cuda memory reasons\n        pyramid1 = minibatch_laplacian_pyramid(image1, n_pyramids, pyramid_batchsize, device=device)\n        pyramid2 = minibatch_laplacian_pyramid(image2, n_pyramids, pyramid_batchsize, device=device)\n        result = []\n\n        for i_pyramid in range(n_pyramids + 1):\n            # indices\n            n = (pyramid1[i_pyramid].size(2) - 6) * (pyramid1[i_pyramid].size(3) - 6)\n            indices = torch.randperm(n)[:n_descriptors]\n\n            # extract patches on CPU\n            # patch : 2rank (n_image*n_descriptors, slice_size**2*ch)\n            p1 = extract_patches(pyramid1[i_pyramid], indices,\n                            slice_size=slice_size, device=\"cpu\")\n            p2 = extract_patches(pyramid2[i_pyramid], indices,\n                            slice_size=slice_size, device=\"cpu\")\n\n            p1, p2 = p1.to(device), p2.to(device)\n\n            distances = []\n            for j in range(n_repeat_projection):\n                # random\n                rand = torch.randn(p1.size(1), proj_per_repeat).to(device)  # (slice_size**2*ch)\n                rand = rand \/ torch.std(rand, dim=0, keepdim=True)  # noramlize\n                # projection\n                proj1 = torch.matmul(p1, rand)\n                proj2 = torch.matmul(p2, rand)\n                proj1, _ = torch.sort(proj1, dim=0)\n                proj2, _ = torch.sort(proj2, dim=0)\n                d = torch.abs(proj1 - proj2)\n                distances.append(torch.mean(d))\n\n            # swd\n            result.append(torch.mean(torch.stack(distances)))\n        \n        # average over resolution\n        result = torch.stack(result) * 1e3\n        if return_by_resolution:\n            return result.cpu()\n        else:\n            return torch.mean(result).cpu()","15c7495f":"def evaluate_SWD(path,plot=False,n_iters=1,verbose=True,latent_dim=100,generator_type=\"dcgan\"):\n    if generator_type==\"sagan\":\n        SAGAN_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        SAGAN_netG.eval()\n    elif generator_type==\"dcgan\":\n        DCGAN_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        DCGAN_netG.eval()\n    elif generator_type==\"dcgan128\":\n        DCGAN128_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        DCGAN128_netG.eval()\n        \n    result_list=[]\n    for i in tqdm(range(n_iters)):\n        real_images=next(iter(dataloader))[0]\n        noise = torch.randn(64, latent_dim, 1, 1, device=\"cuda\")\n        if generator_type==\"sagan\":\n            fake_images = SAGAN_netG(noise)[0].detach().cpu()\n        elif generator_type==\"dcgan\":\n            fake_images = DCGAN_netG(noise).detach().cpu()\n        elif generator_type==\"dcgan128\":\n            fake_images = DCGAN128_netG(noise).detach().cpu()    \n\n        val_swd = swd(real_images, fake_images, device=\"cpu\")\n\n        \n        if verbose:\n            print(\"SWD : %.4f\"%(val_swd))\n        result_list.append(val_swd)\n        \n    if plot:\n        # Plot real images\n        plt.figure(figsize=(15,15))\n        plt.subplot(1,2,1)\n        plt.axis(\"off\")\n        plt.title(\"Real Images\")\n        plt.imshow(np.transpose(vutils.make_grid(real_images.to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n        #Plot the fake images\n        plt.subplot(1,2,2)\n        plt.axis(\"off\")\n        plt.title(\"Fake Images (SWD = %.4f)\"%(val_swd))\n        plt.imshow(np.transpose(vutils.make_grid(fake_images.to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n        plt.show()\n    \n    mean=np.mean(result_list)\n    std=np.std(result_list)\n    print(\"Mean SWD %.4f +- %.4f\"%(mean,std))\n    \n    return(result_list)","fc0338ef":"import glob\nimport numpy as np\nimport scipy.misc\nfrom scipy.spatial.distance import minkowski\nfrom scipy.stats import ks_2samp\nimport time,imageio, os\nimport torch\nfrom tqdm import tqdm\n\n# input images\ndef get_image_vector(filename):\n    im = imageio.imread(filename, pilmode = 'RGB')\n    \n    return np.float32(np.ndarray.flatten(im))\/255\n\n\n#####################  LS CPU ver. ##################{\n\ndef dists(data):  # compute ICD\n    num = data.shape[0]\n    data = data.reshape((num, -1))\n    dist = []\n    for i in range(0,num-1):\n        for j in range(i+1,num):\n            dist.append(minkowski(data[i],data[j]))\n            \n    return np.array(dist)\n\ndef dist_btw(a,b):  # compute BCD\n    a = a.reshape((a.shape[0], -1))\n    b = b.reshape((b.shape[0], -1))\n    dist = []\n    for i in range(a.shape[0]):\n        for j in range(b.shape[0]):\n            dist.append(minkowski(a[i],b[j]))\n            \n    return np.array(dist)\n\n\ndef LS(real,gen):  # KS distance btw ICD and BCD\n    dist_real = dists(real)  # ICD 1\n    dist_gen = dists(gen)  # ICD 2\n    distbtw = dist_btw(real, gen)  # BCD\n    \n    D_Sep_1, _ = ks_2samp(dist_real, distbtw)\n    D_Sep_2, _ = ks_2samp(dist_gen, distbtw)\n\n    return 1- np.max([D_Sep_1, D_Sep_2])  # LS=1-DSI\n","adce5f11":"def evaluate_LS(path,plot=False,n_iters=1,verbose=True,latent_dim=100,generator_type=\"dcgan\"):\n    if generator_type==\"sagan\":\n        SAGAN_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        SAGAN_netG.eval()\n    elif generator_type==\"dcgan\":\n        DCGAN_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        DCGAN_netG.eval()\n    elif generator_type==\"dcgan128\":\n        DCGAN128_netG.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n        DCGAN128_netG.eval()\n        \n    result_list=[]\n    for i in tqdm(range(n_iters)):\n        real_images=next(iter(dataloader))[0]\n        noise = torch.randn(64, latent_dim, 1, 1, device=\"cuda\")\n        if generator_type==\"sagan\":\n            fake_images = SAGAN_netG(noise)[0].detach().cpu()\n        elif generator_type==\"dcgan\":\n            fake_images = DCGAN_netG(noise).detach().cpu()\n        elif generator_type==\"dcgan128\":\n            fake_images = DCGAN128_netG(noise).detach().cpu()\n            \n        val_ls = LS(real_images.cpu(), fake_images)\n        if verbose:\n            print(\"LD : %.4f\"%(val_ls))\n        result_list.append(val_ls)\n        \n    if plot:\n        # Plot real images\n        plt.figure(figsize=(15,15))\n        plt.subplot(1,2,1)\n        plt.axis(\"off\")\n        plt.title(\"Real Images\")\n        plt.imshow(np.transpose(vutils.make_grid(real_images.to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n        #Plot the fake images\n        plt.subplot(1,2,2)\n        plt.axis(\"off\")\n        plt.title(\"Fake Images (LS = %.4f)\"%(val_ls))\n        plt.imshow(np.transpose(vutils.make_grid(fake_images.to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n        plt.show()\n    \n    mean=np.mean(result_list)\n    std=np.std(result_list)\n    print(\"Mean LS %.4f +- %.4f\"%(mean,std))\n    \n    return(result_list)\n\n","f1645053":"stages=[5,10,15,20,25,30,35,40,45,50,55,60]\ndcgan_means_fid=[]\ndcgan_std_fid=[]\nsagan_means_fid=[]\nsagan_std_fid=[]\n\ndcgan_means_swd=[]\ndcgan_std_swd=[]\nsagan_means_swd=[]\nsagan_std_swd=[]\n\ndcgan_means_ls=[]\ndcgan_std_ls=[]\nsagan_means_ls=[]\nsagan_std_ls=[]","7ab23ef0":"for epoch in stages:\n    print(\"Evaluating models at %2.f epochs\"%(epoch))\n    #SAGAN evaluation\n    model_generator_path=\".\/models\/netG\/SAGAN_netG_\"+str(epoch)+\".pt\"\n    fids=evaluate_FID(model_generator_path,n_iters=5,generator_type=\"sagan\",latent_dim=128)\n    sagan_means_fid.append(np.mean(fids))\n    sagan_std_fid.append(np.std(fids))\n    \n    swds=evaluate_SWD(model_generator_path,n_iters=10,generator_type=\"sagan\",latent_dim=128)\n    sagan_means_swd.append(np.mean(swds))\n    sagan_std_swd.append(np.std(swds))  \n\n    lss=evaluate_LS(model_generator_path,n_iters=10,generator_type=\"sagan\",latent_dim=128)\n    sagan_means_ls.append(np.mean(lss))\n    sagan_std_ls.append(np.std(lss)) \n    \n    #DCGAN evaluation\n    model_generator_path=\".\/models\/netG\/DCGAN_netG_\"+str(epoch)+\".pt\"\n    fids=evaluate_FID(model_generator_path,n_iters=5,generator_type=\"dcgan\",latent_dim=100)\n    dcgan_means_fid.append(np.mean(fids))\n    dcgan_std_fid.append(np.std(fids))\n    \n    swds=evaluate_SWD(model_generator_path,n_iters=10,generator_type=\"dcgan\",latent_dim=100)\n    dcgan_means_swd.append(np.mean(swds))\n    dcgan_std_swd.append(np.std(swds))\n    \n    lss=evaluate_LS(model_generator_path,n_iters=10,generator_type=\"dcgan\",latent_dim=100)\n    dcgan_means_ls.append(np.mean(lss))\n    dcgan_std_ls.append(np.std(lss))\n    ","d49913ac":"sagan_means_fid=np.array(sagan_means_fid)\nsagan_std_fid=np.array(sagan_std_fid)\ndcgan_means_fid=np.array(dcgan_means_fid)\ndcgan_std_fid=np.array(dcgan_std_fid)\n\nsagan_means_swd=np.array(sagan_means_swd)\nsagan_std_swd=np.array(sagan_std_swd)\ndcgan_means_swd=np.array(dcgan_means_swd)\ndcgan_std_swd=np.array(dcgan_std_swd)\n\nsagan_means_ls=np.array(sagan_means_ls)\nsagan_std_ls=np.array(sagan_std_ls)\ndcgan_means_ls=np.array(dcgan_means_ls)\ndcgan_std_ls=np.array(dcgan_std_ls)","8c7e0e63":"plt.figure(figsize=(10,10))\nplt.plot(stages,sagan_means_fid,label=\"SAGAN\")\nplt.fill_between(stages,sagan_means_fid-sagan_std_fid,sagan_means_fid+sagan_std_fid,alpha=.1)\nplt.plot(stages,dcgan_means_fid,label=\"DCGAN\")\nplt.fill_between(stages,dcgan_means_fid-dcgan_std_fid,dcgan_means_fid+dcgan_std_fid,alpha=.1)\nplt.legend()\nplt.title(\"FID comparision\")","b57878e7":"plt.figure(figsize=(10,10))\nplt.plot(stages,sagan_means_swd,label=\"SAGAN\")\nplt.fill_between(stages,sagan_means_swd-sagan_std_swd,sagan_means_swd+sagan_std_swd,alpha=.1)\nplt.plot(stages,dcgan_means_swd,label=\"DCGAN\")\nplt.fill_between(stages,dcgan_means_swd-dcgan_std_swd,dcgan_means_swd+dcgan_std_swd,alpha=.1)\nplt.legend()\nplt.title(\"SWD comparision\")","dc057021":"plt.figure(figsize=(10,10))\nplt.plot(stages,sagan_means_ls,label=\"SAGAN\")\nplt.fill_between(stages,sagan_means_ls-sagan_std_ls,sagan_means_ls+sagan_std_ls,alpha=.1)\nplt.plot(stages,dcgan_means_ls,label=\"DCGAN\")\nplt.fill_between(stages,dcgan_means_ls-dcgan_std_ls,dcgan_means_ls+dcgan_std_ls,alpha=.1)\nplt.legend()\nplt.title(\"LS comparision (higher score means better performance)\")","b7187a29":"model_generator_path=\".\/models\/netG\/DCGAN_netG_60.pt\"\nevaluate_LS(model_generator_path,n_iters=1,generator_type=\"dcgan\",latent_dim=100,plot=True)","7db8be0d":"model_generator_path=\".\/models\/netG\/SAGAN_netG_60.pt\"\nevaluate_LS(model_generator_path,n_iters=1,generator_type=\"sagan\",latent_dim=128,plot=True)","6985fc20":"\"\"\"\n#Create the dataset\ndataset = dset.ImageFolder(root=\"..\/input\/portrait-faces-128\",\n                           transform=transforms.Compose([\n                               transforms.Resize(128),\n                               transforms.CenterCrop(128),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                           ]))\n\n# Create the dataloader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=64)\n\n#Define the devide to be used\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n\"\"\"","a98ecdc9":"\"\"\"\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Reference real images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n\"\"\"","058e0f90":"\"\"\"\nreal_images=real_batch[0]\n\"\"\"","4acaec3d":"\"\"\"\nclass Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d( nz, ngf * 16, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 16),\n            nn.ReLU(True),\n            # state size. (ngf*16) x 4 x 4\n            \n            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 8 x 8\n            \n            nn.ConvTranspose2d( ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 16 x 16\n            \n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 32 x 32\n            \n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            #state size. (ngf) x 64 x 64\n            \n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 128 x 128\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\"\"\"","afd2162a":"\"\"\"\nnz = 100\nngf = 32\nndf = 32\nnc = 3\n\n# Create the generator\nDCGAN128_netG = Generator(ngpu=1).to(\"cuda\")\n# Print the model\nprint(DCGAN128_netG)\n\"\"\"","b8b7f040":"\"\"\"\npfw.set_config(batch_size=64, device=\"cuda\")\nreal_m, real_s = pfw.get_stats(real_images)\n\"\"\"","5f554c38":"\"\"\"\nmodels = [f for f in listdir(\"..\/input\/saved-dcgan128-models\") if isfile(join(\"..\/input\/saved-dcgan128-models\", f))]\nmodels_mean_fid=[]\nmodels_mean_swd=[]\nmodels_mean_ls=[]\nmodels_std_fid=[]\nmodels_std_swd=[]\nmodels_std_ls=[]\n\"\"\"","5541e261":"\"\"\"\nfor model in models:\n    model_generator_path=\"..\/input\/saved-dcgan128-models\/\"+str(model)\n    print(\"Evaluating %s\"%(model))\n    fid_list=evaluate_FID(model_generator_path,n_iters=10,generator_type=\"dcgan128\",latent_dim=100)\n    swd_list=evaluate_SWD(model_generator_path,n_iters=10,generator_type=\"dcgan128\",latent_dim=100)    \n    ls_list=evaluate_LS(model_generator_path,n_iters=10,generator_type=\"dcgan128\",latent_dim=100)\n    \n    models_mean_fid.append(np.mean(fid_list))\n    models_mean_swd.append(np.mean(swd_list))\n    models_mean_ls.append(np.mean(ls_list))\n    \n    models_std_fid.append(np.std(fid_list))\n    models_std_swd.append(np.std(swd_list))\n    models_std_ls.append(np.std(ls_list))\n\"\"\"","4f1d3dd3":"\"\"\"\ndcgan_results=pd.DataFrame(models,columns=[\"name\"])\ndcgan_results[\"fid\"]=models_mean_fid\ndcgan_results[\"fid_std\"]=models_std_fid\ndcgan_results[\"swd\"]=models_mean_swd\ndcgan_results[\"swd_std\"]=models_std_swd\ndcgan_results[\"ls\"]=models_mean_ls\ndcgan_results[\"ls_std\"]=models_std_ls\n\"\"\"","46bf1034":"\"\"\"\nimport re\nepochs_used=[]\nhas_FM=[]\nhas_OSLB=[]\nhas_MINIBATCHDIS=[]\nlabels=[]\nfor name in dcgan_results[\"name\"]:\n    split=re.split(\"_|\\.\",name)\n    epochs_used.append(int(split[1]))\n    if \"FM\" in split:\n        has_FM.append(True)\n    else:\n        has_FM.append(False)\n\n    if \"OSLB\" in split:\n        has_OSLB.append(True)\n    else:\n        has_OSLB.append(False)\n\n    if \"MINIBATCHDIS\" in split:\n        has_MINIBATCHDIS.append(True)\n    else:\n        has_MINIBATCHDIS.append(False)\n        \n    split=split[:-1]\n    if len(split)==3:\n        label=\"None\"\n    else:\n        label=\"\"\n        for f in split[3:]:\n            label=label+\" \"+str(f)\n            \n    labels.append(label)\n\"\"\"","e2309a74":"\"\"\"\ndcgan_results[\"epochs_used\"]=epochs_used\ndcgan_results[\"FM\"]=has_FM\ndcgan_results[\"OSLB\"]=has_OSLB\ndcgan_results[\"MINIBATCHDIS\"]=has_MINIBATCHDIS\ndcgan_results[\"label\"]=labels\ndcgan_results_30=dcgan_results[dcgan_results[\"epochs_used\"]==30]\ndcgan_results_60=dcgan_results[dcgan_results[\"epochs_used\"]==60]\ndcgan_results_30=dcgan_results_30.sort_values(by=\"label\")\ndcgan_results_60=dcgan_results_60.sort_values(by=\"label\")\n\"\"\"","a737ac52":"\"\"\"\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nplt.bar(np.arange(len(dcgan_results_30)), dcgan_results_30[\"fid\"], width = 0.5, edgecolor = 'black', yerr=dcgan_results_30[\"fid_std\"], capsize=7, label='30')\nplt.xticks(np.arange(len(dcgan_results_30)), dcgan_results_30[\"label\"],rotation=90)\nplt.ylim((185,245))\nplt.ylabel('FID')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.bar(np.arange(len(dcgan_results_60)), dcgan_results_60[\"fid\"], width = 0.5, edgecolor = 'black', yerr=dcgan_results_60[\"fid_std\"], capsize=7, label='60')\nplt.xticks(np.arange(len(dcgan_results_60)), dcgan_results_60[\"label\"],rotation=90)\nplt.ylim((185,245))\nplt.legend()\n\nplt.show()\n\"\"\"","f74bf97c":"\"\"\"\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nplt.bar(np.arange(len(dcgan_results_30)), dcgan_results_30[\"swd\"], width = 0.5, edgecolor = 'black', yerr=dcgan_results_30[\"swd_std\"], capsize=7, label='30')\nplt.xticks(np.arange(len(dcgan_results_30)), dcgan_results_30[\"label\"],rotation=90)\nplt.ylim((500,850))\nplt.ylabel('SWD')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.bar(np.arange(len(dcgan_results_60)), dcgan_results_60[\"swd\"], width = 0.5, edgecolor = 'black', yerr=dcgan_results_60[\"swd_std\"], capsize=7, label='60')\nplt.xticks(np.arange(len(dcgan_results_60)), dcgan_results_60[\"label\"],rotation=90)\nplt.ylim((500,850))\nplt.legend()\n\nplt.show()\n\"\"\"","336949ed":"\"\"\"\nplt.figure(figsize=(15,8))\nplt.subplot(1,2,1)\nplt.bar(np.arange(len(dcgan_results_30)), dcgan_results_30[\"ls\"], width = 0.5, edgecolor = 'black',color=\"orange\", yerr=dcgan_results_30[\"ls_std\"], capsize=7, label='30')\nplt.xticks(np.arange(len(dcgan_results_30)), dcgan_results_30[\"label\"],rotation=90)\nplt.ylim((0.85,1))\nplt.ylabel('LS')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.bar(np.arange(len(dcgan_results_60)), dcgan_results_60[\"ls\"], width = 0.5, edgecolor = 'black',color=\"orange\", yerr=dcgan_results_60[\"ls_std\"], capsize=7, label='60')\nplt.xticks(np.arange(len(dcgan_results_60)), dcgan_results_60[\"label\"],rotation=90)\nplt.ylim((0.85,1))\nplt.legend()\n\nplt.show()\n\"\"\"","f9d331df":"\"\"\"\nDCGAN128_netG.load_state_dict(torch.load(\"..\/input\/saved-dcgan128-models\/netG_60_DCGAN_OSLB.pt\",map_location=torch.device('cpu')))\nDCGAN128_netG.eval()\nnoise = torch.randn(64, 100, 1, 1, device=\"cuda\")\nfake_images = DCGAN128_netG(noise).detach().cpu()\n#Plot the fake images\nplt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(vutils.make_grid(fake_images.to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\nplt.show()\n\"\"\"","17339761":"\"\"\"\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(vutils.make_grid(fake_images.to(device)[i], padding=0, normalize=True).cpu(),(1,2,0)), animated=True)] for i in range(64)]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())\n\"\"\"","d699030e":"mydir = \"datas\/faces_64\/faces\"\n\nfilelist = [ f for f in os.listdir(mydir)]\nfor f in filelist:\n    os.remove(os.path.join(mydir, f))","4a46fbeb":"## Discriminator","9b62a830":"## Processing","90fb80e2":"We used the `torchgan.layers` library to implement Minibatch Discrimination (a manual implementation is possible and was made in https:\/\/www.kaggle.com\/raphallorenzo\/portrait-generation-with-dcgan, version 25, but it was not optimal and ran very slowly for a few epochs). \nThe minibatch discrimination takes as an input the result of a intermediate layer as features : we choose the last layer before the linear one, which provides 8192 \"features\" corresponding to the flattening of the `main` block output of the discriminator (4096 for the 64 $\\times$ 64 images), we kept the 16 default intermidate features ($C$) and chose 128 output features that are directly fed into the last block (fully connected Linear layer then Sigmoid activation)","e6e0e970":"## Utils functions and spectral normalization function","7b8ebafc":"Unsurprisingly the losses are both reduced very quickly at the beggining of the training, but then the discriminator loss keeps droping slowly to 0 whereas the generator loss seems to converge to a higher level. \nIt is explained by the use of the discriminator to compute the generator loss : at each iterations, the discriminator is \"improved\" thus despite the improvement of the generator the computed error may not be reduced. \n\nThe discriminator's loss also relies on the generator (for the `errD_fake` calculation) thus the improvement of the generator might affect its loss, but it is only partially related since the discriminator loss also relies on a ground truth error. \n\nThe GANs theory lead us to expect a theoritical convergence of both Generator and Discriminator, and thus of respective their losses, we seem to achieve it with a variance that can be explained by the adjustment for each different batch along training.","09bec1b6":"## Self attention layers","839e2b11":"These 128 features are calculated separately for real and fake samples, and there is no \"matching\" operation made between the real and fake samples, but so as in the Feature Matching, the use of features in the Discriminator is also a way to create visually appealing images in a few epochs (as stated in Salimans et al.).","b65d6108":"## Imports and settings","aa5b98ee":"## Sliced Wassertein Distance (SWD)","f37cfb52":"We used the implementation of FID in the `pytorch-fid-wrapper` (Seitzer, https:\/\/github.com\/mseitzer\/pytorch-fid) that allows us to use the Inception net on GPU to improve computationnal efficiency (but it still is a quite expensive method compared to the two others).","bb642b52":"## Remove images from the output folder","1f35d932":"## Evaluation metrics using saved weights for generators","9ddb2461":"The Frechet Inception Distance relies on a pretrained model (the Inception Net CNN here) to capture features given by a specififc layer of the pretrained model. These features are computed for both real and fake images and are assumed to corresponds to a multivariate Gaussian distribution (which is a strong hypothesis that is not always verified especially when features are extracted from layers that result in high dimensional outputs). Then the means of both real and fake samples of these features are calculated $\\mu_r$ and $\\mu_g$ as well as their covariance matrix $\\Sigma_r$ and $\\Sigma_g$. The FID is the Fr\u00e9chet Distance between these distribution : $FID(r,g)=||\\mu_g-\\mu_r||_2^2+Tr(\\Sigma_r + \\Sigma_g -2(\\Sigma_r \\Sigma_g)^{1\/2})$","2c9a1e35":"## Minibatch Discrimination","022d274a":"Deep Convolutional Generative Adversarial Networks are a class of GANs introduced in 2016 by Radford, Metz & Chintala in *Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks* (https:\/\/arxiv.org\/pdf\/1511.06434.pdf)\nThe idea behind DCGANs is to use the architechure and the development made in CNNs to extend them to new applications (image generation but also feature extraction that can later be used for supervised tasks such as image classification).\n\nOur implementation of DCGAN is based on the dedicated Pytorch Tutorial https:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html","3691a91e":"Specificities of the DCGAN's discriminator includes :\n* The use of almost only strided convolutions layers to map the input to an output (except for the last one). In comparision to the original approach where fully connected linear layers were used to downsample the images to its output, other approach used spatially deterministic functions such as max pooling, this approach with strided convolution allows the discriminator to learn a structured downsampling from the original image to the point of a $1024 \\times 2 \\times 2$ \"image\" from which it is consistent to extract features. \n* The use of Batch Normalization that improves training problems that could be due to the random initialization, and which in the absence of Minibatch Normalization, adress a major problem of GAN's that is the risk of \"collapse\" from the generator to a state where it only emits a single point, unable to create diverse images (this problem is also adressed by Minibatch Normalization and mentionned below in this notebook)","f7c99d17":"The Generator structure is very similar to the DCGAN Generator, with blocks of strided Transpose convolutions, Batch Normalization and ReLu, then a Tanh activation before output. It has two additions : \n* The use of Spectral Normalization, which is a regularization techniques on the weight of the convolutions\n* The use of Self Attention, described above, on the the last two blocks before the final one. The Self Attention is then used in quite \"high resolution\" images ($16\\times 16$ and $32\\times 32$)","eab9f7bd":"For each `nn.ConvTranspose2d` the arguments are respectively nn.ConvTranspose2d(`in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `bias`)\n\nThe actual padding size is `dilation` $\\times ($ `kernel_size` $-1) - $ `padding` with default `dilation=1`. So the implicit default padding size is `kernel_size-1`,thus in our example there is an implicit padding for ` nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False) ` of $(3\\times 3)$\n\nFor this example, considering we have square images and `int` parameters only (so that they have the same size or effect on width and height), an input of size of size ($X \\times X\\times$`in_channels`) will have an output of size :\n\n<center> $(O,O,\\mathrm{out\\_channels})$ with $O=(X-1)\\times \\mathrm{stride} - 2 \\times \\mathrm{padding} + \\mathrm{dilation} \\times (\\mathrm{kernel\\_size} - 1) +1$ <\/center>","c7e908e7":"# Evaluation metrics","15f47107":"## Create the dataset for the batch of real images in dimension 128$\\times$128","9f3a4b51":"## Visual result","5f97884e":"We set the parameters of the following Deep Convolutional GAN Structure. These parameters are those suggested by Radford et al. (2016) in the original paper, except for the learning rate (the same for both the generator and discriminator) which is higher (0.0006 instead of 0.0002) because we added a scheduler that reduces it to (0.0003, 0.00015 and 0.000075) by step every 15 epochs.\nWe also added the Minibatch Discrimination and Feature matching that can be activated or not.\n\nAs suggested by the authors we uses the Adam Optimizer on which the $\\beta_1$ parameter is modified because the default 0.9 value leads to unstable training (experiments we made also shows that reducing $\\beta_1$ more to 0.2 or 0.1 leads to unefficient training and poor results too)","a9451997":"As we could expect from the previous remark the values of $D(G(z))$ are not converging to 1, although they seems to rise along the training, they are regularly \"abruptly\" reduced (the Discriminator learns a particularly discriminating features to distinguish images and the generator is thus made inefficient in its task).\n\nThis shows a \"better\" training for the discriminator, because in a situation of perfect disciminator both $D(x)$ and $D(G(z))$ would converge to $1\/2$.","4b199b6a":"This measure works by assuming how possible it is to separate real and fake images. It actually works on the distribution on various sets of distances :\n* The Intra Class Distance (ICD) of real images and the ICD of fake images noted $\\{d_r\\}=\\{||x_i-x_j||_2,x_i,x_j\\in R,x_i \\ne x_j\\}$ for real images, $\\{d_g\\}=\\{||x_i-x_j||_2,x_i,x_j\\in G,x_i \\ne x_j\\}$ for generated images\n* The Between Class Distance (BCD) : $\\{d_{g,r}\\}=\\{||x_i-y_j||_2,x_i\\in R,y_j\\in G\\}$\n\nThe Theorem 1 of Guan and Loew states that when the number of distances in each set ($|\\{d_r\\}|$ and $|\\{d_g\\}|$) gets infinitely high, $P_R=P_G$ if and only if the distributions of ICD and BCD sets are identical.\n\nWe then have $DSI(R,G)=\\max\\{KS(\\{d_r\\},\\{d_{r,g}\\}),KS(\\{d_g\\},\\{d_{r,g}\\})\\}$ and $LS=1-DSI$, with $KS(A,B)$ the Kolmogorov-Smirnov distance between two samples (the maximum difference of their cumulative repartition function).","2bdc2603":"This training techniques consist in adding a new objective for the generator. Instead of only maximizing the output of the discriminator for the generated images it also tries to match features from the original datas. It is meant to create more visually appealing results and to adresses a situation where the generator is overtrained to fool the discriminator but not to match the distribution (and thus the \"realness\") of the original datas. ","1d1398b3":"![image.png](attachment:0eb29346-255c-4e1f-ab23-2426262976e3.png)","3a311c1d":"## Fr\u00e9chet Inception Distance (FID)","baa5ff11":"The Distance Based Separability index (which leads directly to the Likenes Score) is a method proposed by Guan and Loew (2021, https:\/\/arxiv.org\/pdf\/2002.12345.pdf). It adresses issues raised by FID and SWD : \n* These two measures are meant to capture similarities or dissimilarities in the distribution of datas thus a good Discriminator leads to $P_g \\sim P_r$ and low FID ans SWD, but they do not adress the issue of creativity of the Generator, an overfitted Generator would thus lead to very satisfying FID and SWD.\n* The Gaussian assumption in the FID is not always verified (and may never be for small samples and high dimensional features)","2c44bdb2":"## Distance-based Separability Index (DSI) and Likeness Score (LS)","27645de1":"We use (as first approach) a Binary Cross Entropy Loss:\n* Then, the Discriminator aim is to maximize the value of $\\log\u2061(D(x))$ knowing $x$ comes from a real data distribution and to minimize $\\log\u2061(D(G(z)))$ knowing $G(z)$ comes from the fake data distribution (at the moment) i.e. to maximize $\\log\u2061(1-D(G(z)))$, we get the problem : $\\max_D[\\mathbb{E}_{x\\sim P_{data}}[\\log(D(x))]+\\mathbb{E}_{z\\sim P_{generator}}[\\log(1-D(G(z)))]]$\n* Then, the Generator aim is to fool the discriminator and maximize the value of $D(G(z))$ i.e. minimize $\\log\u2061(1-D(G(z)))$, we get the problem : $\\min_G[\\mathbb{E}_{z\\sim P_{generator}}[\\log(1-D(G(z)))]]$","27076b7d":"SAGAN stands for Self Attention Generative Adversarial Networks, this architecture was proposed in H. Zhang, I. Goodfellow, D. Metaxas, A. Odena (2019, https:\/\/arxiv.org\/pdf\/1805.08318.pdf). \nIt consists in using a Deep Convolutionnal GAN architecture but adding at some stages of both the Discriminator and Generator a Self Attention layer. Self Attention is a mechanism (described later) that helps capture dependencies between features far away from each other in the images, it came from research made for Natural Langage Processing such as those of *Attention Is All You Need* (Vaswani et al., 2017, https:\/\/arxiv.org\/pdf\/1706.03762.pdf)\n\nAnother addition of Zhang et al. is the use of Spectral Normalization which is supposed to improve training stability, we also used another regularization technique that is Gradient Penalty (included in Generator Loss), as proposed by Gulrajani et al. (2017, https:\/\/arxiv.org\/pdf\/1704.00028v3.pdf).","01bf5f16":"# Extract faces and create datasets","51a873b8":"Self Attention is meant to adress the issue brought by the use of convolutions all along the treatement made by both the Generator and the Discriminator : it works locally. Dependencies between features spread on the images might be learned but only through many layers and thus training might not be able to capture these dependencies.","0593a6ec":"## DCGAN generator for 128$\\times$128 images","53d6a086":"Evaluating GANs is a complex task since Generators do not a have a real objective function and are trained indirectly through an ever changing disciminator. One of the main drawbacks of GANs is the absence of an evaluation technique  that is widely adopted.\nAs mentionned in Salimans et al. (2016), one of the way to assess a model quality when it comes to generate visually appealing is the use of the human annotators, of course this method is subjective (but it might not be such a problem when it comes to produce images for humans) and way too much time-consumming.\nIn *Pros and Cons of GAN Evaluation Measures*, Borji (2018, https:\/\/arxiv.org\/pdf\/1802.03446.pdf) made a survey of existing measures, we chose two of them and also implemented a more recent method (Likeness Score).\n\nWe want our methods to match human judgement on visually appealing results (that ressembles the original images) but also to favor models who can genereate diverse images, and of creative models (who generate new images, new faces)","d022c646":"The idea underlying One Sided Label Smoothing is Label Smoothing Regularization, which is actually a Two Sided Smoothing (meaning it would be modifying both `real_label` and `fake_label`). It was described in *Rethinking the Inception Architecture for Computer Vision* (Szegedy et al. 2015, https:\/\/arxiv.org\/pdf\/1512.00567.pdf),it was meant to improve a classification performances of a network and it proved to be efficient over the ImageNet dataset. \nIt is meant to avoid that classifying network become too \"confident\" and classify with a too high probability (Logits, Szegedy article) dominating all other. In our case we only have two outputs but we also want to avoid a overfitted Discriminator regarding our ground truth, for two main reasons : \n* It would lead to a too conservative discriminator that would reject even visually appealing images produced by the generator\n* Our training dataset is, as we have seen before, not perfect because of the errors of the Cascade Classifier used to create it, leading to having a few non-faces images inside some training batches\n\nThe original optimal discriminator, to which, the GANs is theoretically converging (as proved in the original paper of Goodfellow, 2014) is : $D(x)=\\frac{p_{data}(x)}{p_{data}(x)+p_{generator}(x)}$ with $p_{data}$ the distribution of the original images and $p_{generator}$ the distribution of images produced by the generator at a given moment (thus with a perfect generator it yields a constant $1\/2$ output)\n\nBy using a two sided label smoothing (replacing `true_labels` with $\\alpha$ and `fake_labels` with $\\beta$) we obtain a different optimal discriminator : $D(x)=\\frac{\\alpha p_{data}(x)+\\beta p_{generator}}{p_{data}(x)+p_{generator}(x)}$ but Salimans suggest that having $p_{model}$ in the numerator would be a problem when $p_{data}(x)$ is near $0$ because the discriminator is used to compute the loss of the generator and in that case for erroneous sample produced could still be classified as real and thus not be moving nearer to the ground truth.","c27e1815":"Our first piece of work is to extract faces from the portrait images of the Wiki-Art Encyplopedia Dataset. This dataset (available here : https:\/\/www.kaggle.com\/ipythonx\/wikiart-gangogh-creating-art-gan) contains around 100000 high resolution images, including 15000 portraits. \n\nWe used a Haar Cascade Classifier to extract faces from the images, crop them and resize them to the desired size for our training dataset. This classifier from P. Viola and M. Jones (Rapid Object Detection using a Boosted Cascade of Simple Features, 2001 [1]) is pretrained with positive and negative samples of images containing or not containing faces. It relies on a very important number of simple features on which the Adaboost algorithm is applied, for each of the features a threshold and accuracy over the training set is computed and the more relevant features with the associated threshold are kept forming weak classifiers whose weighted sum defines a strong classifier. Even though most relevant features are selected, to reduce computation time, the concept of cascade is used : weak classifiers are organised by stages, with a limited number of classifier for the first stages. If a subwindow does not \"pass\" a stage, it is classified as negative (not containing a face) and the faetures of the next stages are not computed.\n\nThis classifier is unperfect, especially with false negatives : out of the 15000 portrait images, 29% of them do not contain any face according to the Haar Cascade Classifier. This false negative issue is also explained by the fact that some portraits are particularly \"abstract art\" and may not contains the features of a standard face, also the classifier is meant to detect front faces and side faces might not be detected. \n\nMore recent face detectors, that could handle more diversity of faces could have been used, but the Haar Cascade Classifier is fast even with high resolution images and on CPU and can handle various image sizes, also it is impletended in `open-cv` which we used for the images croping and resizing.","42439edb":"Using \"images\" of size $x \\in \\mathbb{R}^{C\\times W\\times H}$ with $W\\times H=N$ at certain stage of the Discriminator or the Generator, we remap them as $x\\in \\mathbb{R}^{C\\times N}$ so they are represented by $C$ channels and $N$ features (corresponding to the positions of each pixels) the Self Attention layer produces two feature spaces (by using $1\\times 1$ convolutions with learnable parameters) : \n* $f(x)=W_f x \\in \\mathbb{R}^{\\bar{C}\\times N}$ (the query)\n* $g(x)=W_g x \\in \\mathbb{R}^{\\bar{C}\\times N} $ (the key)","91705a65":"## Training process stats","1e41a5b2":"Because of memory issues DCGAN and SAGAN can not be run on the same kernel with 60 epochs, so we gathered the saved weights for the generators from other notebooks at various stage (10, 20, 30, 40, 50 and 60 epochs)","5c8a1cd4":"**Remark** : This implementation is slightly different than the one in the paper which have a $h(x)\\in \\mathbb{R}^{\\bar{C}\\times N}$ and a last $1\\times 1$ convolution $v(x)=W_v x \\in \\mathbb{R}^{C\\times N} $. Other implementation proposed it but we did not modified it considering it would add more parameters and memory is already an important issue with SAGAN. ","4add6283":"For each of the selected metrics we create a `evalutate_XXX` function that is meant to execute the process of computing the measure, eventually multiples times (with various batches of real and fake images, to then compute the mean of the measures for a given model) for a specific model, and specific parameters saved along training.","013e918c":"As for the Generator, the Discriminator structure is very similar to the DCGANs one, with blocks of strided convolutions and Leaky ReLu. \nIt has two additions : \n* The use of Spectral Normalization\n* The use of Self Attention at the end of the convolutions (thus on quite \"low\" resolution) images\n\nThe Discriminator although has a difference in the main structure with the DCGAN : it does not uses Batch Normalization. The other differneces are in the output at the end of the downsampling, instead of using a Linear layer it uses a Conv2d and has no Sigmoid activation, thus it does not yield an output in $[0,1]$, and the output is not handled with BCE in the training to create both Discriminator and Generator Loss.","1e8b12f4":"Visualize the first batch of the dataloader","3738ecb6":"Specifities of the DCGAN Generators includes :\n* The use of strided convolutions (instead of fully connected Linear layers), that leads the generator to \"learn\" gradually its upsampling from latent space to image shape\n* The use of ReLU for all layers except the last one, where the `tanh` activation provides a bounded output, which allow the generator to learn more quickly the color space of the training distribution than it would with some unbounded outputs for some color channels.","c77ed9fc":"## Training","eed2cbae":"# Other training techniques for a DCGAN with images of size 128x128 and comparision of the results","d1f0a5ce":"# Introduction : principle of GANs","7e22381c":"## Attention layers evolution","01477266":"It is shown that ICD Real, ICD generated, and BCD distributions are highly separable in 3 cases : \n* The generated data lacks diversity\n* The generated data lacks inheritance (similar distribution to the the original datas)\n* The generated data lacks creativity","084da484":"# DCGAN","a0227040":"One main addition in the structure of the training loop is the use of the `wgan-gp` loss. This criterion's interest and behaviour has been explained in *Improved Training of Wasserstein GANs* (Gulrajani et al., 2017, https:\/\/arxiv.org\/pdf\/1704.00028v3.pdf), the use of Wassertein-1 distance for training is meant to adress the problem of Generators training, for which the standard minimax game of GANs using BCE might result in non continuous functions (regarding to Generators parameters) and training difficulties. The Wassertein GAN uses the Wassertein-1 distance instead of BCE and it lead to the following global problem : $\\min_G \\max_{D\\in \\mathcal{D}} \u2061S(D,G)=\\mathbb{E}_{x\\sim P_r}[D(x)]-\\mathbb{E}_{G(z)\\sim P_g}[D(G(z))]$ with $\\mathcal{D}$ the set of 1-Lipschitz functions. To enforce this constraint of having $D\\in\\mathcal{D}$ it is possible to use weight clipping but it lead to a problem of  gradient explosion.\n\nThus, as suggested, we add a regularization penalty on the gradient value for a random sample $\\hat{x}$ (random mix of real and fake images datas). This yield a new loss function for the discriminator (called *critic* in the paper,since it is not a classifier) : $\\mathbb{E}_{G(z)\\sim P_g}[D(G(z))]-\\mathbb{E}_{x\\sim P_r}[D(x)]+\\lambda_{gp}\\mathbb{E}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_2-1)^2]$ an thus a new global problem : $\\min_G \\max_D \u2061S(D,G)=\\mathbb{E}_{x\\sim P_r}[D(x)]-\\mathbb{E}_{G(z)\\sim P_g}[D(G(z))]-\\lambda_{gp}\\mathbb{E}[(||\\nabla_{\\hat{x}}D(\\hat{x})||_2-1)^2] $","8e728e0a":"## Discriminator","23f16e94":"Finally, we get the \u201ctwo player minimax game\u201d with function $V(D,G)$ (Goodfellow, 2014) :\n$\\min_G \\max_D \u2061V(D,G) = \\mathbb{E}_{x\\sim P_{data}}[\\log(D(x))]+\\mathbb{E}_{z\\sim P_{generator}}[\\log(1-D(G(z)))] $","7dd5e81e":"The results are quite appealing although two kind of errors remains : \n* Completely erroneous images, that does not seems to represent a face at all. These kind of error might occur because of the erroneous images in the training dataset, thus globally the batch may match the expected distribution.\n* Faces whose features does not seems to match the features of real faces, this might occur because we did not use the feature matching but also because of the low resolution of the training images where features might not be well distinguishable ","7ccdcb45":"# Dataloader","d81668ce":"Our implementation of the SAGAN network is based on the PyTorch implementation of https:\/\/github.com\/heykeetae\/Self-Attention-GAN","4651199d":"We then determine the output `out` by using the another feature space which is the value, also obtained by a $1\\times 1$ convolution from $x$ : $h(x)=W_h x \\in \\mathbb{R}^{C\\times N}$. And we get `out`$=o=(o_1,...,o_N)$ with $o_j=\\sum_i \\beta_{ji}h(x_i)$. We have $o\\in \\mathbb{R}^{C\\times N}$ which is then remapped to the original $x$ size $\\mathbb{R}^{C\\times W\\times H}$. The real output of the Self Attention layer is the sum of the $x$ we used (output of the previous layer) and of the `out` of the Self Attention module, this sum is weighted with a $\\gamma$ parameter that is learnable, and we actually have (it can be seen in the results) a low $|\\gamma|$ at the beginning that gets bigger along the training. It means the Self Attention is progressively more important in the Generator and the discriminator process along the training.","9ec4f871":"The Sliced Wassertein Distance relies on the Laplacian Pyramid of the real and generated images. Each image is downsampled then up sampled and blured (with a $5\\times 5$ Gaussian Kernel) and components (*slices*) of the Laplacian Pyramid are extracted. Patches are selected and compared at various stages. This method is non deterministic (it uses random projections of the extracted patches to compare real and generated images) so it works with multiples iterations, however it remains quite efficient in computation times even on CPU.","3a439063":"We encountered memory issues so we reduce the size of the dataset for this notebook","a8283b17":"![image.png](attachment:fc78b849-1c18-47b6-876f-6fc3408d94a8.png)","64940aa7":"We uses a custom weight initialization with a $\\mathcal{N}(0,0.02)$ distribution of weights weight (as suggested in Radford, 2016 - https:\/\/arxiv.org\/pdf\/1511.06434.pdf - 4. DETAILS OF ADVERSARIAL TRAINING) for convolutionnal layers, a  $\\mathcal{N}(1,0.02)$ distribution of weights for BatchNorm Layers and 0 BatchNorm bias.","528a5abe":"This discriminator is a supervised classifier that works with labeled real samples (with label 1) and labeled fake samples (obtained with $G(Z)$, with label 0) and aims to classify each image as real or fake. \nThus, the discriminator is trained directly using the labels of datas, and the generator is trained indirectly with its ability to \u201cfool\u201d the discriminator (i.e. having $D(G(Z))$ as close to 1 as possible). ","02bb1f6d":"## One Sided Label Smoothing","037b6948":"## Imports","21690ba6":"# Visual results","11e07899":"We worked with higher resolution images to obtain results more visually appealing. We also used some techniques that are meant to stabilize and improve GANs training. These techniques are describes in (Improved Techniques for Training GANs, Salimans et al., 2016, https:\/\/arxiv.org\/pdf\/1606.03498.pdf). The resulting weights of generators were saved using another notebook (https:\/\/www.kaggle.com\/raphallorenzo\/fork-of-portrait-generation-with-dcgan\/notebook) and are now used at two stages : 30 and 60 epochs, we comapre performances of the generators depending how they were trainied using the aforementionned evaluation measures. ","faeb1501":"## Set a real batch of images","e26a1541":"## Feature Matching","29d57cd7":"The concept Generative Adversarial Networks has been introduced in 2014 by Ian Goodfellow et al (Goodfellow, 2014). What characterized this class of networks is the simultaneous use of two Neural Networks. \nThe aim of GANs is originally to generate an illimited number of samples from a limited number of original examples. \nThat is why the first of the two competing networks is a Generator ($\\mathcal{G}$), it creates images (in our case) from a random seed ($z\\sim P_z$) using transpose convolution as we will be detailing in the Generator section. \nInstead of comparing the distribution of datas directly to obtain: $P_{G(z)} \\sim P_R$ (where $R$ is the reference sample), a Discriminator ($\\mathcal{D}$) is introduced. ","cf1ad367":"## Training process stats","7e17b80d":"The 3 improved techniques we implemented can also be used in this notebook with 64$\\times$64 images by modifiying the settings. We impleted : One Sided Label Smoothing (that can be activated by setting the `real_label` to 0.85, or another value below 1), Minibatch Discrimination (that can be activated by setting `use_minibatch_discrimination=True`), and Feature Matching  (that can be activated by setting `use_feature_matching=True`).","29ffc8cc":"We apply a matrix multiplication and get $s_{ij}= f(x_i)^\\top g(x_j)$ then a softmax on each row and get the attention $\\beta_{ji} = \\frac{\\exp(s_{ij})}{\\sum_i \\exp(s_{ij})}$ that represents the the attention of the model on the region $i$ when synthetizing the region $j$ with $i,j\\in \\{1,...,N\\}$","224f6f36":"We initialize the elements of our training loop :\n* The criterions : used to calculate losses\n* The fixed noise to follow the evolution of the generator outputs\n* The optimizers : we uses Adam optimizers with a specifi $\\beta_1$ as mentionned in the parameter section\n* The schedulers : we added schedulers to allow us to have a greater learning rate at the beggining and reduce the number of epochs required to obtain a quite appealing result at the first stages (we used a StepLR scheduler that divides the learning rate by 2 every 15 epochs)","bf7b1243":"## Weight initialization","3e6d4360":"The discriminator is mostly made symmetrically to the Generator with 5 blocks of 2D-Convolution with`LeakyReLu` (which allows negative outputs, with a $0.2$ slope), except for the final convolution, and `BatchNorm2d`, except for the first convolution. \n\nIt takes images of size $3\\times 64 \\times 64$ as inputs that are convoluted to $1024 \\times 2 \\times 2$ entries.\nThen the output is flatten and used a features for the feature matching training and for the Minibatch Discrimination features (128 features obtained from the 4096 flattened features that are added through a dedicated block) if theses options are activated. The resulting features (4096 or 4096$+$128) are fed into a linear layer and yields a real output where a `Sigmoid` activation is applied.","1ddd0573":"The FID is meant to perform well to match human judgement and it is claimed that it performs better than the commonly used Incepetion Score which also relies on the Inception Net pretrained network, but that is less performant to detect a generator collapse (a situation where the generator would emit a single point). ","e3fd291f":"We used the functions of the PyTorch implementation proposed in https:\/\/github.com\/koshian2\/swd-pytorch","bc46ddc0":"Minibatch Discrimination is meant to prevent a \"collapse\" in the generator. An issue with GANs can be a state where the generator tends to always generate the same output. Once again, we act on the discriminator to prevenet it from happenning.\nThis problem comes from the fact that there is no coordination between the gradients calculated for each image of the batch by the discriminator. The gradients of the discriminator for each images lie in the same direction with inputs that are getting more and more similar and gives a positive output (real image), thus the generator also tends to produce more and more similar results. Afterwards the discriminator learns that the collapsing point is an image produced by the generator (fake image), but the same inputs producing the same gradients the problem is repeated with no \"diversity\" in the production of the generator.\n\nThe solution proposed is to establish a link between the inputs of batch used in the discriminator. Actually, any form of coordination would adress this problem of generator collapsing and the use of Batch Normalization in the DCGAN (Radford, 2016) that we implemented prevented this (that is the reason why even without Minibatch Discrimination we didn't face this problem of collapsing to a single mode in our experiments). ","02d9a8ef":"The features we used are extracted from the output of an intermediate layer of the discriminator : $f(x)\\in \\mathbb{R}^F$, we chose the features from the last convolutionnal block, with $F=4096$ (8192 for 128 $\\times$ 128 images). At each batch of each epoch of the training, the features are calculated for the real batch and compared to those calculated for a generated batch. We compute the Mean Squared Error and add it to generator loss (which also accounts for the Binary Cross Entropy calculated from the comparision of the predicted labels of generated images to the alleged \"real\" labels), the generator has know a double objective including minimizing $||\\mathbb{E}_{x\\sim p_{data}}[f(x)]-\\mathbb{E}_{z\\sim p_{generator}}[f(G(z))]||_2^2$. We can set a coefficient to increase or reduce the influence of Feature Matching in the generator loss (we kept it at 1 in our examples, experiments does not show a significative difference in the training depending on the composition of the loss)","5f457cc0":"Evolution of the per-pixel attention (on the last batch at the end of each epoch) extracted from the the second Self Attention layer of the Generator. The first figure below represents the evolution of the attention on the $32\\times 32$ image after 4 blocks of Transpose convolution for the synthesis of pixel number 528 (in the center of the image), the second reprensents the areas of attention for the synthesis of the pixel number 1024 (bottom right).","eccf7609":"## Generator","2c6e7f70":"## Utils","7d402a2c":"As mentionned in Zhang et al., $\\bar{C}$ can be chosen $\\bar{C}=C\/8$ without affecting the performance and it saves memory by reducing the number of parameters in the convolutions.","4d6abf00":"The training of the SAGAN network is globally the standard structure of GANs training : \n* The Discriminator is trained on both real and generated samples\n* The Generator is trained by using the Discriminator at its given state of training\n\nThe hyperparameters used are the one suggested in the original paper. We did not add a scheduler to the learning rate in this case (experiments with higher learning rate later reduced led to poor results).","cf6560d9":"## Training","057dbbb8":"## Visual result","4e0f2837":"# Common settings (for DCGAN and SAGAN)","9f328f07":"# SAGAN","029097ef":"The generator is made of 5 blocks of `ConvTranspose2d`, with `BatchNorm2d` and `ReLu` layers (except for the last one where the activation layer is `Tanh`)\n\nThe input data is a random noise (seen later) of size $1\\times 1$ with `nz` channels. \nWith the size ($4\\times 4$) of the kernel and the implicit (or controled) padding in the first `ConvTranspose2d` layer, then with the stride (an reduced padding) the image is upsampled to the $64\\times 64$ or `ngf` size with 3 channels.","4c8ee25b":"Minibatch Discrimination takes as an input the output of a intermediate layer : $f(x_i)$ (with $(x_1,\\dots,x_N)$ being the components of the batch of size $N$, with $f(x_i)\\in \\mathbb{R}^A$ (to follow the notation of Salimans et al., the number of `in_features`), performs a multiplication with a Tensor of size $A\\times B \\times C$ where $B$ is the number of `out_features` and $C$ is an intermediate dimension. We get a $B\\times C$ matrix : $M_i=T\\times f(x_i)$ then for each line of the matrix we compute : $c(x_i)_b=\\sum_{j=1}^{N} \\mathrm{exp}(-||M_{i,b}-M_{j,b}||_1)$ this is where the lines of the matrix for every other $x_j$ intervenes. The output for each $x_i$ is $o(x_i)=[o(x_i)_1,\\dots,o(x_i)_B]\\in \\mathbb{R}^B$.","8e7d50c9":"## Generator","d181d472":"We will be using the Binary Cross Entroy Loss function with its default parameters on the discriminator outputs 3 separate times (for each input of each batch it computes $l=-[y.\\mathrm{log}(O) + (1-y).\\mathrm{log}(1-O)]$ with $O$ the ouput of the considered network, and then the mean across the batch) :\n1. On the discriminator for `errD_real` with the output $O=D(x)$ of the discriminator for $x$ real image and with the label $y=1$ thus $l=-\\mathrm{log}(D(x))$\n2. On the discriminator for `errD_fake` with the output $O=D(G(z))$ with $G$ the generator, and $z$ a random noise used as input for the generator, and with the label $y=0$ thus $l=-\\mathrm{log}(1-D(G(z)))$\n3. On the generator for `errG` with the output $O=D(G(z))$ and with the label $y=1$ thus $l=-\\mathrm{log}(D(G(z)))+l_{fm}$\n\nWhen feature matching is activated we also uses the `criterionG` loss, which is the Mean Squared Error for the feature matching accross the determined features (with the discriminator) of the real and the generated images.\nThus `errG` also accounts for the feature matching error : $l_{fm}=\\sum_{i=1}^{4096}(f_i^{R}-f_i^{G})^2$","7b59e4ec":"# Comparision of DCGAN and SAGAN","5383a84f":"## Alternative (preprocessed)"}}