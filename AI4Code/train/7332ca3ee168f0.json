{"cell_type":{"5f8c905e":"code","bd777060":"code","5bb72011":"code","b3e5ed09":"code","5ab1e1c7":"code","80999ff2":"code","5496dea7":"code","ab941c40":"code","796069b2":"code","730a4e33":"code","80d4403c":"code","426c6e8a":"code","419f7d87":"code","a9d9c996":"code","4da5056b":"code","fbf87974":"code","450b4ef2":"code","a01867ee":"code","df55001d":"markdown","02edfeb0":"markdown","4121459c":"markdown","a7fdfd32":"markdown","6e1b2207":"markdown","f75d2340":"markdown","ded0169d":"markdown","8aad9350":"markdown","bcf70493":"markdown","75551f60":"markdown","689e75d3":"markdown","d131773c":"markdown","215ff133":"markdown","4ed885e5":"markdown","e6de5c96":"markdown"},"source":{"5f8c905e":"!pip install --quiet efficientnet\nprint(\"VolkanOzdemirDL\")\nimport math, os, re, warnings, random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras import optimizers, applications, Sequential, losses\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nimport efficientnet.tfkeras as efn\nfrom IPython.display import SVG\n\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings(\"ignore\")","bd777060":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","5bb72011":"BATCH_SIZE = 4 * REPLICAS\nWARMUP_EPOCHS = 3\nWARMUP_LEARNING_RATE = 1e-4 * REPLICAS\nEPOCHS =30\nLEARNING_RATE = 3e-5 * REPLICAS\nHEIGHT = 512\nWIDTH = 512\nCHANNELS = 3\nN_CLASSES = 104\nES_PATIENCE = 5\n\nmodel_path = f'model_{HEIGHT}x{WIDTH}.h5'\n\nGCS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started') + f'\/tfrecords-jpeg-{HEIGHT}x{WIDTH}'\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/train\/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/val\/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test\/*.tfrec')\n\n# Bunu tek tek yazman\u0131za gerek yok ama inputu tf records olarak verdiklerinden \u00f6n i\u015fleme uzun s\u00fcr\u00fcyor\nCLASSES = [\n    'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', \n    'wild geranium', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', \n    'globe thistle', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', \n    'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', \n    'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', \n    'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', \n    'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', \n    'carnation', 'garden phlox', 'love in the mist', 'cosmos',  'alpine sea holly', \n    'ruby-lipped cattleya', 'cape flower', 'great masterwort',  'siam tulip', \n    'lenten rose', 'barberton daisy', 'daffodil',  'sword lily', 'poinsettia', \n    'bolero deep blue',  'wallflower', 'marigold', 'buttercup', 'daisy', \n    'common dandelion', 'petunia', 'wild pansy', 'primula',  'sunflower', \n    'lilac hibiscus', 'bishop of llandaff', 'gaura',  'geranium', 'orange dahlia', \n    'pink-yellow dahlia', 'cautleya spicata',  'japanese anemone', \n    'black-eyed susan', 'silverbush', 'californian poppy',  'osteospermum', \n    'spring crocus', 'iris', 'windflower',  'tree poppy', 'gazania', 'azalea', \n    'water lily',  'rose', 'thorn apple', 'morning glory', 'passion flower',  \n    'lotus', 'toad lily', 'anthurium', 'frangipani',  'clematis', 'hibiscus', \n    'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen ', \n    'watercress',  'canna lily', 'hippeastrum ', 'bee balm', 'pink quill',  \n    'foxglove', 'bougainvillea', 'camellia', 'mallow',  'mexican petunia',  \n    'bromelia', 'blanket flower', 'trumpet creeper',  'blackberry lily', \n    'common tulip', 'wild rose']","b3e5ed09":"\nAUTO = tf.data.experimental.AUTOTUNE # instructs the API to read from multiple files if available.\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.reshape(image, [HEIGHT, WIDTH, 3])\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef data_augment(image, label):\n    crop_size = tf.random.uniform([], int(HEIGHT*.75), HEIGHT, dtype=tf.int32)\n        \n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_saturation(image, lower=0, upper=2)\n    image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    image = tf.image.resize(image, size=[HEIGHT, WIDTH])\n    image = tf.image.rot90(image)\n    return image, label\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_training_dataset_preview(ordered=True):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","5ab1e1c7":"\nnp.set_printoptions(threshold=15, linewidth=80)\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is the case for test data)\n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize\/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize\/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n    \n# Visualize model predictions\ndef dataset_to_numpy_util(dataset, N):\n    dataset = dataset.unbatch().batch(N)\n    for images, labels in dataset:\n        numpy_images = images.numpy()\n        numpy_labels = labels.numpy()\n        break;  \n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    label = np.argmax(label, axis=-1)\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower_eval(image, title, subplot, red=False):\n    plt.subplot(subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(title, fontsize=14, color='red' if red else 'black')\n    return subplot+1\n\ndef display_9_images_with_predictions(images, predictions, labels):\n    subplot=331\n    plt.figure(figsize=(13,13))\n    for i, image in enumerate(images):\n        title, correct = title_from_label_and_target(predictions[i], labels[i])\n        subplot = display_one_flower_eval(image, title, subplot, not correct)\n        if i >= 8:\n            break;\n              \n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()","80999ff2":"\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\ntrain_dataset = get_training_dataset_preview(ordered=True)\ny_train = next(iter(train_dataset.unbatch().map(lambda image, label: label).batch(NUM_TRAINING_IMAGES))).numpy()\nprint(f'Number of training images {NUM_TRAINING_IMAGES}')\n\n\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nvalid_dataset = get_validation_dataset(ordered=True)\ny_valid = next(iter(valid_dataset.unbatch().map(lambda image, label: label).batch(NUM_VALIDATION_IMAGES))).numpy()\nprint(f'Number of validation images {NUM_VALIDATION_IMAGES}')\n\n\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint(f'Number of test images {NUM_TEST_IMAGES}')\ntest_dataset = get_test_dataset(ordered=True)","5496dea7":"display_batch_of_images(next(iter(train_dataset.unbatch().batch(16))))\ntrain_agg = np.asarray([[label, (y_train == index).sum()] for index, label in enumerate(CLASSES)])\nvalid_agg = np.asarray([[label, (y_valid == index).sum()] for index, label in enumerate(CLASSES)])\n\n\n\n\n","ab941c40":"def create_model(input_shape, N_CLASSES):\n    base_model = efn.EfficientNetB6(weights='imagenet', \n                                    include_top=False,\n                                    input_shape=input_shape)\n\n    base_model.trainable = False # Freeze layers\n    model = tf.keras.Sequential([\n        base_model,\n        L.GlobalAveragePooling2D(),\n        L.Dense(N_CLASSES, activation='softmax')])\n    \n    return model","796069b2":"with strategy.scope():\n    model = create_model((None, None, CHANNELS), N_CLASSES)\n    \nmetric_list = ['sparse_categorical_accuracy']\n\noptimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)\nmodel.compile(optimizer=optimizer, \n              loss=losses.SparseCategoricalCrossentropy(), \n              metrics=metric_list)\nmodel.summary()","730a4e33":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nwarmup_history = model.fit(x=get_training_dataset(), \n                           steps_per_epoch=STEPS_PER_EPOCH, \n                           validation_data=get_validation_dataset(),\n                           epochs=WARMUP_EPOCHS, \n                           verbose=1).history\n","80d4403c":"\nLR_START = 0.00000001\nLR_MIN = 0.000001\nLR_MAX = LEARNING_RATE\nLR_RAMPUP_EPOCHS = 4\nLR_SUSTAIN_EPOCHS = 1\nLR_EXP_DECAY = .81\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\n\nsns.set(style='whitegrid')\nfig, ax = plt.subplots(figsize=(20, 7))\nplt.plot(rng, y)\n\nprint(f'{EPOCHS} total epochs and {NUM_TRAINING_IMAGES\/\/BATCH_SIZE} steps per epoch')\nprint(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')","426c6e8a":"for layer in model.layers:\n    layer.trainable = True # Unfreeze layers\n\ncheckpoint = ModelCheckpoint(model_path, monitor='val_loss', mode='min', save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, \n                   restore_best_weights=True, verbose=1)\nlr_callback = LearningRateScheduler(lrfn, verbose=0)\n\ncallback_list = [checkpoint, es, lr_callback]\n\noptimizer = optimizers.Adam(lr=LEARNING_RATE)\nmodel.compile(optimizer=optimizer, \n              loss='sparse_categorical_crossentropy', \n              metrics=metric_list)\nmodel.summary()\n\nSVG(tf.keras.utils.model_to_dot(model, dpi=70).create(prog='dot', format='svg'))","419f7d87":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n\nhistory = model.fit(x=get_training_dataset(), \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data=get_validation_dataset(), \n                    callbacks=callback_list, \n                    epochs=30, \n                    verbose=1).history","a9d9c996":"def plot_metrics(history, metric_list):\n    fig, axes = plt.subplots(len(metric_list), 1, sharex='col', figsize=(24, 12))\n    axes = axes.flatten()\n    \n    for index, metric in enumerate(metric_list):\n        axes[index].plot(history[metric], label=f'Train {metric}')\n        axes[index].plot(history[f'val_{metric}'], label=f'Validation {metric}')\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric)\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()\n\nplot_metrics(history, metric_list=['loss', 'sparse_categorical_accuracy'])","4da5056b":"x_test = test_dataset.map(lambda image, idnum: image)\ntest_preds = model.predict(x_test)\ntest_preds = np.argmax(test_preds, axis=-1)\n\ntest_ids_ds = test_dataset.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n\nsubmission = pd.DataFrame(test_ids, columns=['id'])\nsubmission['label'] = test_preds\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head(12))","fbf87974":"from tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.applications import DenseNet201\nimport cv2\nfrom IPython.display import SVG\n\n\n\n\n\ndef create_model2(input_shape, N_CLASSES):\n    model2 = tf.keras.models.Sequential()\n\n    model2.add(DenseNet201(\n                     input_shape=(512,512,3),\n                     weights = 'imagenet',\n                     include_top=False))\n\n    model2.add(tf.keras.layers.GlobalAveragePooling2D())\n\n    model2.add(tf.keras.layers.Dense(N_CLASSES,\n                                   activation='softmax'))\n\n\n    \n    return model2\n\nwith strategy.scope():\n    model2 = create_model2((None, None, CHANNELS), N_CLASSES)\n    \n\n\ncheckpoint = ModelCheckpoint(model_path, monitor='val_loss', mode='min', save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, \n                   restore_best_weights=True, verbose=1)\nlr_callback = LearningRateScheduler(lrfn, verbose=0)\n\ncallback_list = [checkpoint, es, lr_callback]\n\noptimizer = optimizers.Adam(lr=LEARNING_RATE)\nmodel2.compile(optimizer=optimizer, \n              loss='sparse_categorical_crossentropy', \n              metrics=metric_list)\nmodel2.summary()\n\n\n\n    \nSVG(tf.keras.utils.model_to_dot(model2, dpi=70).create(prog='dot', format='svg'))\n                                   ","450b4ef2":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nhistory = model2.fit(x=get_training_dataset(), \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data=get_validation_dataset(), \n                    callbacks=callback_list, \n                    epochs=30, \n                    verbose=1).history","a01867ee":"print('Generating submission.csv file...')\n\n# Get image ids from test set and convert to unicode\ntest_ids_ds = test_dataset.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n\n# Write the submission file\nnp.savetxt(\n    'submission.csv',\n    np.rec.fromarrays([test_ids,test_preds]),\n    fmt=['%s', '%d'],\n    delimiter=',',\n    header='id,label',\n    comments='',\n)\n\n# Look at the first few predictions\n!head submission.csv","df55001d":"# Test Sonu\u00e7lar\u0131","02edfeb0":"Learning rate'i dinamik vermek m\u00fcmk\u00fcnd\u00fcr. Burada farkl\u0131 stratejiler g\u00fcd\u00fclebilir. Ama temel trend d\u00fc\u015f\u00fck bir learning rate'den ba\u015flayarak \u00f6nce biraz art\u0131rmak sonra azaltmakt\u0131r. Bu i\u015fin temel mant\u0131\u011f\u0131 optimizasyon teorisine kadar gider. yapay sinir a\u011flar\u0131 haf\u0131za bak\u0131m\u0131ndan en iyi gradient descent tarz\u0131 basit optimizasyon modelleri ile verimli \u00e7al\u0131\u015ft\u0131\u011f\u0131ndan dolay\u0131, learning rate ana parametredir ( ilk ba\u015flang\u0131\u00e7 noktas\u0131 da \u00f6nemlidir, onu pre-trained model ile sa\u011fl\u0131yoruz ). Local minimalara tak\u0131lmadan gitmek i\u00e7in bu tarz stratejileri \u00f6neririm. Bu stratejiyi daha sonra model'i fit ederken callsback_learningrate olarak tan\u0131mlayaca\u011f\u0131z.","4121459c":"Dondurdu\u011fumuz layerlar\u0131 a\u00e7t\u0131k. T\u00fcm modeli yazd\u0131k.\n\noptimizer https:\/\/keras.io\/api\/optimizers\/ 'dan se\u00e7ilebilir. Nadam, Adam, SGD, RMSprop optimizerlar\u0131 aras\u0131ndan en iyi perform verecek optimizer cross-validation ile se\u00e7ilebilir.","a7fdfd32":"# DenseNet201 - Optional\n\nDenseNet102 ve DenseNet201 de g\u00fczel modellerdir. Dense yap\u0131s\u0131 ile ba\u015far\u0131m g\u00f6stermektedirler. Efficient Net \u00e7o\u011fu durumda DenseNEt'i outperform etse dahi dataya g\u00f6re farkl\u0131 \u00f6odeller denemek gerekir.","6e1b2207":"* Decode fonksiyonu\n* Etiketli datay\u0131 tf record'dan okuma fonksiyonu\n* Etiketlenmemi\u015f datay\u0131 tf record'dan okuma fonksiyonu\n* Data seti y\u00fckleme\n* Data augmentation --> sa\u011f-sol aynalama, yukar\u0131 a\u015fa\u011f\u0131 aynalama, 90 derece d\u00f6nd\u00fcrme, rasgele kesme, resize\n* Di\u011fer data ve data g\u00f6rselle\u015ftirme fonksiyonlar\u0131","f75d2340":"EfficientNetB6 modeli i\u00e7in \u00f6n \u0131s\u0131nma turu \"warm up\" konulabilir. Burada 3 epoch layerlar\u0131 dondurulmu\u015f modeli g\u00f6rmekteyiz, \u00e7ok bir katk\u0131s\u0131 olmayabilir. Ama genelde b\u00fcy\u00fck modeller \u00fcst\u00fcnde transfer learning yaparken belirli bir epcoh say\u0131s\u0131nda ilk layerlar\u0131 dondurarak daha \u00e7ok son layerlar\u0131 e\u011fitiriz. Bunun sebebi ilk layerlar\u0131n temel geometrik \u015fekilleri ve anlamlar\u0131 \u00f6\u011frenmesidir. ","ded0169d":"# $\\color{orange}{\\text{TPU Nedir? }}$\n \n\n\nTPU: \"Tensor Processimg Unit\", T\u00fcrk\u00e7e olarak ise tensor i\u015flem birimi b\u00fcy\u00fck data y\u0131\u011f\u0131nlar\u0131 ve konvalosyonel sinir a\u011flar\u0131 ( CNN) i\u00e7in son derece optimize edilmi\u015f \u00e7ok y\u00fcksek e\u011fitim verimine sahip donan\u0131mlard\u0131r. \n\nDaha \u00f6nce data science veya derin \u00f6\u011frenme ile u\u011fra\u015fm\u0131\u015fsan\u0131z illaki bu algoritmalar\u0131n GPU'larda CPU'lara g\u00f6re ne kadar h\u0131zl\u0131 oldu\u011funu duymu\u015fsunuzdur. \u00dcstelik tek fark da bu de\u011fil, CPU'lar kapasite gere\u011fi b\u00fcy\u00fck batch-size kullanamamaktad\u0131r ve ba\u015far\u0131m\u0131 y\u00fckseltmek i\u00e7in gerekli olan giri\u015f b\u00fcy\u00fckl\u00fc\u011f\u00fcne (input size) ula\u015famamaktad\u0131r. CPU'nun 1 ila 8 \u00e7ekirde\u011fi veya daha fazlas\u0131 vard\u0131r. Bir GPU'da ise y\u00fczlerce vard\u0131r. Bu i\u015flem birimlerinin fazla olmas\u0131 yapay sinir a\u011flar\u0131 gibi toplama ve \u00e7arpma gibi temel i\u015flemler \u00fczerine olu\u015fturulmu\u015f ( feed-forward yap\u0131lar i\u00e7in wX+b ve aktivasyon fonksiyonlar\u0131 ve back-propogation i\u00e7in gradient descent ) sistemler i\u00e7in \u00e7ok h\u0131zl\u0131 i\u015flem yapabilmektedir. GPU ve TPU ise \u00e7ok benzer teknolojidir. Tek fark,  GPU \u00fcreticilerinin kimseye satmad\u0131klar\u0131 tescilli GPU yongalar\u0131n\u0131 kullanan bir bulut hizmeti olmas\u0131d\u0131r. \u015eu an en \u00fcnl\u00fc TPU payla\u015f\u0131mc\u0131s\u0131 Google'd\u0131r.\n\n\nCPU, GPU ve TPU'nun fark\u0131n\u0131 anlamak i\u00e7in asl\u0131nda temel algebra'dan yararlanmak gerekmektedir. CPU i\u015fleme birimini skaler bir de\u011fer olarak d\u00fc\u015f\u00fcnecek olur isek, GPU bir vekt\u00f6r gibi, TPU ise bir matris ya da \u00e7ok boyutlu matris olan tensor'ler ile ifade edilebilir. Tensor yap\u0131lar\u0131  \u00e7ok boyutlu matrisler olarak d\u00fc\u015f\u00fcn\u00fclmelidir ki CNN'lerin temelinde 3 boyutlu bir RGB resim giri\u015fi N boyutlu tensorlere d\u00f6nmektedir.\n\nGiri\u015f data boyutu: \n* CPU: 1 X 1 data birimi\n* GPU: 1 X N data birimi\n* TPU: N X N data birimi\n\nPerformans olarak:\n\n\n* CPU d\u00f6ng\u00fc ba\u015f\u0131na onlarca i\u015flemi idare edebilir.\n* GPU, d\u00f6ng\u00fc ba\u015f\u0131na on binlerce i\u015flemi kald\u0131rabilir.\n* TPU, d\u00f6ng\u00fc ba\u015f\u0131na 128.000'e kadar i\u015flemi ger\u00e7ekle\u015ftirebilir.\n \nTabi ki, TPU'nun kullan\u0131m\u0131 uygun olarak ayarlamak ve kullan\u0131ma haz\u0131r hale getirmek gerekmektedir.\n\n\n\nTek bir Cloud TPU cihaz\u0131, her biri iki TPU \u00e7ekirde\u011fine sahip d\u00f6rt yongadan olu\u015fur. Yine de Cloud TPU'nun verimli kullan\u0131m\u0131 i\u00e7in, bir algoritma program\u0131 sekiz \u00e7ekirde\u011fin her birini kullanmal\u0131d\u0131r. \"TPUEstimator\" \u00e7o\u011falt\u0131lm\u0131\u015f bir hesaplama olu\u015fturmak ve \u00e7al\u0131\u015ft\u0131rmak i\u00e7in bir grafik operat\u00f6r\u00fc sa\u011flar. Her Replica, esasen, her bir \u00e7ekirdek \u00fczerinde \u00e7al\u0131\u015ft\u0131r\u0131lan ve toplam parti boyutunun 1 \/ 8'ini i\u00e7eren bir mini parti (mini batch) e\u011fiten, e\u011fitim grafi\u011finin bir kopyas\u0131d\u0131r. Yani e\u011fitimimiz i\u00e7in \u00e7al\u0131\u015fan ufak minik grafik hesaplay\u0131c\u0131lard\u0131r. Replica say\u0131s\u0131 ile bir \u00e7ok hyper-parametreyi ayarlayaca\u011f\u0131z.\n\n\n# \u00d6n \u00c7al\u0131\u015fma ve \u0130\u015fleme \n\n* TPU Donan\u0131m varl\u0131\u011f\u0131 test edilmi\u015ftir.\n\n> tpu = tf.distribute.cluster_resolver.TPUClusterResolver()   # -> TPU k\u00fcmesi hakk\u0131nda bilgi sa\u011flar\n\n> tf.config.experimental_connect_to_cluster(tpu)\n\n> tf.tpu.experimental.initialize_tpu_system(tpu)\n\n> strategy = tf.distribute.experimental.TPUStrategy(tpu)  # ->  TPU configi yap\u0131l\u0131yor, \u00f6n de\u011ferleniyor ve strategy tan\u0131mlan\u0131yor\n\n\n> REPLICAS = strategy.num_replicas_in_sync   # -> Gradyan ge\u00e7i\u015flerinin topland\u0131\u011f\u0131 kopya (Replicas) say\u0131s\u0131n\u0131 d\u00f6nd\u00fcr\u00fcr.\n","8aad9350":"Loss grafi\u011fi","bcf70493":"Model i\u00e7in Adam optimizer kullan\u0131l\u0131ld\u0131. Burada farkl\u0131 \u015feyler kullanmak m\u00fcmk\u00fcn, deneme yan\u0131lma ile se\u00e7mek gerekecek. Deneme yan\u0131lma ile se\u00e7ti\u011fimiz parametrelere \"hyper parametre\" deriz. Bu parametreler Cross-validation dedi\u011fimiz \u00e7apraz ger\u00e7ekleme ile belirlenmelidir.\n\nloss i\u00e7in ise categorical corss entropy kullan\u0131labilece\u011fi gibi, Sparse Categorical Crossentropy de kullan\u0131labilir.\nbknz: https:\/\/keras.io\/api\/optimizers\/","75551f60":"# Modeller\n\n\u0130lk Model olu\u015fturuldu: Resnet, EfficientNetB6, EfficientNetB7, DenseNet102, DenseNet201 kullan\u0131labilir.Deneyerek performans\u0131n\u0131 g\u00f6rmek gerekir\n\n\"import efficientnet.tfkeras as efn\" ile efficientnet haz\u0131r modeli keras'tan \u00e7a\u011fr\u0131labilir.\n\n* Son layer N_CLASSES (s\u0131n\u0131f say\u0131s\u0131=104) kadar n\u00f6ron i\u00e7eren softmax olacakt\u0131r.\n* Giri\u015f 512,512,3\n* Transfer learning bu case i\u00e7in \u00e7ok faydal\u0131 olacakt\u0131r. noisy-student veya imagenet i\u00e7erisinde \u00e7ok fazla \u00e7i\u00e7ek foto\u011fraf\u0131 oldu\u011fundan dolay\u0131, pre-trained model kullanarak e\u011fitime ba\u015flamak avantajl\u0131d\u0131r. \n\nPre-trained model kullan\u0131rken e\u011fitim yapt\u0131\u011f\u0131n\u0131z konu \u00e7ok \u00f6nemlidir, e\u011fer herhangi bir pre-trained modelini i\u00e7erisinde olmayan \u00f6rnekler \u00fczerine \u00e7al\u0131\u015f\u0131yorsan\u0131z pre-trained model avantaj yaratmayacakt\u0131r.","689e75d3":"# DATA\n\n* Data Train - Validation - Test olarak ayr\u0131lm\u0131\u015ft\u0131r. \n\ntraining : 12753\nvalidation : 3712\ntest : 7382","d131773c":"Model Parametreleri girilmi\u015ftir. TPU'da GPU'da e\u011fitimden farkl\u0131 olarak strategy tan\u0131mlanmakta ve REPLICAS ile parametreler parametrize edilmektedir. \u00d6rne\u011fin Batch size 4 * REPLICAS ( bizim durumumuzda 4 * 8=32 olarak) ifade edilmi\u015ftir. Burada batch size art\u0131r\u0131labilir ve art\u0131r\u0131lmas\u0131 local minimumlara tak\u0131lmay\u0131 engeller fakat yeterli haf\u0131za olmamas\u0131 durumunda kodunuz \u00e7al\u0131\u015fmaz. E\u011fer kodunuz memory hatas\u0131 veriyorsa, el mahkum, bu say\u0131y\u0131 k\u00fc\u00e7\u00fcltmek gerekecektir.\n\nYar\u0131\u015fma konusu s\u0131n\u0131falar da teker teker yaz\u0131lm\u0131\u015ft\u0131r. Bu b\u00f6lgedeki kodlar i\u00e7in Martin G\u00f6rner, https:\/\/www.kaggle.com\/mgornergoogle 'in \u00e7al\u0131\u015fmas\u0131ndan yard\u0131m al\u0131nm\u0131\u015ft\u0131r ve kendi algoritmam\u0131za g\u00f6re d\u00fczenlenmi\u015ftir.","215ff133":"\n# **$\\color{orange}{\\text{Flower Classification on TPU  }}$**\n\n\nKaggle'a TPU se\u00e7ene\u011fi gelmi\u015f, ho\u015fgelmi\u015f. GPU'dan ziyade TPU'lar \u00e7ok daha h\u0131zl\u0131 i\u015flem yapan birimlerdir. Biz de TPU \u00fczerinde temel CNN algoritmalar\u0131n\u0131 deneyerek, TPU kullan\u0131m\u0131n\u0131 test etmi\u015f olaca\u011f\u0131z. Temel fonksiyonlar i\u00e7in Martin G\u00f6rner'e te\u015fekk\u00fcr ederim. \u00d6nc\u00fc ve y\u00f6nlendirici \u00e7al\u0131\u015fmas\u0131ndan esinlenerek t\u00fcrk\u00e7e bir kaynak olu\u015fturmak istedim. Amac\u0131m, ingilizce kaynaklar\u0131 takip edemeyen T\u00fcrk geli\u015ftiriciler i\u00e7in yard\u0131mc\u0131 bir d\u00f6k\u00fcman olu\u015fturmak.\n\n\u00c7al\u0131\u015fma i\u00e7erisinde:\n* TPU Mimarisi ve GPU ve CPU'lara g\u00f6re avantajlar\u0131\n* TPU kullan\u0131m\u0131 ve haz\u0131rlanmas\u0131\n* \u00c7i\u00e7ek s\u0131n\u0131fland\u0131rmas\u0131 \u00fczerinde CNN algoritmalar\u0131 ile pre-trained modeller ile denemeler yap\u0131lmas\u0131\n\nBulunmaktad\u0131r.\n\n\n**EfficientNetB6, EfficientNetB7 ve DenseNet201 kullan\u0131lm\u0131\u015ft\u0131r. 20 epoch ile tamamlanm\u0131\u015ft\u0131r.**\n\n****K\u0131sa \u00d6zet: 9 epoch'da model performanslar\u0131****\n\n\n**$\\color{orange}{\\text{9 Epoch'da EfficientNetB6 i\u00e7in : }}$**\n\n* sparse_categorical_accuracy: 0.9394 - val_loss: 0.4078 - val_sparse_categorical_accuracy: 0.9108\n\n\n\n**$\\color{orange}{\\text{9 Epoch'da DenseNet201 i\u00e7in : }}$**\n\n* sparse_categorical_accuracy: 0.9458 - val_loss: 0.4519 - val_sparse_categorical_accuracy: 0.8898\n\n\n","4ed885e5":"\n\n# **$\\color{pink}{\\text{\u00d6n \u00c7al\u0131\u015fma ve \u0130\u015fleme  }}$**\n\n* Gerekli K\u00fct\u00fcphaneler y\u00fcklenmi\u015ftir. -> tensorflow ve keras modelleri kullan\u0131lm\u0131\u015ft\u0131r\n\n* Seed \u00e7ekirde\u011fi atanm\u0131\u015ft\u0131r. -> Random atanan de\u011ferler i\u00e7in ayn\u0131 k\u00f6kten (tohumdan) almas\u0131 sa\u011flan\u0131r.","e6de5c96":"Modeli \u00e7al\u0131\u015ft\u0131rd\u0131rk."}}