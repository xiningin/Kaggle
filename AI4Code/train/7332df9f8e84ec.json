{"cell_type":{"b614db35":"code","59055896":"code","5c2279a1":"code","f16f5bf5":"code","6be0c44e":"code","1c110f90":"code","c0fdb96e":"code","989ecb1a":"code","209aa381":"code","f525eee7":"code","2a1ce247":"code","86525c3d":"code","fe8631b8":"code","002762f1":"code","bf18264c":"code","03da342d":"code","97bfd2b1":"code","32ced7f5":"code","7a18f953":"code","d30e725e":"code","abdba3bc":"code","dcc3e368":"code","e86be1a1":"code","cc1ffd0a":"code","7794539e":"code","d38cab84":"code","602bfd82":"code","94f874ee":"code","83929dd3":"code","19cac748":"code","2c987f55":"code","29147595":"code","44921ef2":"code","6b277e66":"code","34faf26f":"code","707bb411":"code","8f5a5e75":"code","a7207abf":"code","82e3f4b6":"code","7110f4bb":"code","06f30b7b":"code","33c9e6e1":"code","170a43f2":"code","5559f3ea":"code","83ae7897":"code","b2da0863":"code","78d8cb13":"code","0c441428":"code","938c573b":"code","426d9b7b":"code","79659f62":"code","6be42aca":"markdown","3fdde7bc":"markdown","317cc2a6":"markdown","9cac67cf":"markdown","8b8f18b2":"markdown","5b0bc63f":"markdown","df15df72":"markdown","1487f1b0":"markdown","4d7ca36a":"markdown","1f1db757":"markdown","38c178be":"markdown","2960f805":"markdown","3d09f1bc":"markdown","57a44363":"markdown","faa5cfc5":"markdown","bb8c4cca":"markdown","e1c75426":"markdown","f8f68785":"markdown","376ac929":"markdown","ef996a1d":"markdown","626a77cb":"markdown","28ec6b62":"markdown","72ee0328":"markdown","50298e10":"markdown","482d0b4a":"markdown","a84f4ab0":"markdown","699e1d3f":"markdown","ddc5f24e":"markdown","1f2d6d77":"markdown","1a2b3030":"markdown","e269537a":"markdown","bd2fbf17":"markdown","8f9a48a7":"markdown","2ad4ba6b":"markdown","4ef6b743":"markdown","d66e0fcb":"markdown","47b3b78c":"markdown"},"source":{"b614db35":"!pip install langdetect","59055896":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nfrom time import time\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport re\nimport glob\nfrom langdetect import detect\nimport pickle\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer \n\nimport gensim\nfrom gensim.summarization import summarize\nfrom gensim.models.doc2vec import Doc2Vec\n\n\nimport spacy\nimport scipy\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.manifold import TSNE\nfrom sklearn import metrics\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.decomposition import PCA\n\nimport umap.umap_ as umap\n\nimport wordcloud\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport plotly.express as px\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5c2279a1":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    if authors==[]:\n        return None\n    \n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    if body_text is None:\n        return 'Not Provided'\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n        \n    if len(body) <= 11:\n        body += 'Not Provided'\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)","f16f5bf5":"class PaperReader:\n    def __init__(self, root_path):\n        self.root_path = root_path\n        self.filenames = glob.glob('{}\/**\/*.json'.format(root_path), recursive=True)\n        self.filenames = self.filenames[:15000] #this line was added to save on memory, comment this line to load all the papers\n        print(str(len(self.filenames))+' files were found')\n        \n    def load_files(self):\n        raw_files = []\n\n        for filename in tqdm(self.filenames):\n            file = json.load(open(filename, 'rb'))\n            raw_files.append(file)\n\n        return raw_files\n    \n    def retrieve(self, file , field):\n        try: \n            field = file[field]\n        except KeyError:\n            field = None\n        return field\n    \n    def generate_clean_df(self):\n        \n        raw_files = self.load_files()\n        cleaned_files = []\n\n        for file in tqdm(raw_files):\n            paper_id = self.retrieve(file , 'paper_id')\n            title = self.retrieve(file['metadata'],'title')\n            authors = self.retrieve(file['metadata'],'authors')\n            abstract = self.retrieve(file, 'abstract')\n            text = self.retrieve(file, 'body_text')\n            \n            features = [\n                paper_id,\n                title,\n                format_authors(authors),\n#                 format_authors(file['metadata']['authors'], \n#                                with_affiliation=True),\n                format_body(abstract),\n                format_body(text)\n#                 format_bib(file['bib_entries']),\n#                 file['metadata']['authors'],\n#                 file['bib_entries']    for now we can comment these parts as we do not need the bibliography or affiliation \n                                        # so we can speed up the process. If you wish to include these fields make sure to add \n                                        # the correlated column name\n            ]\n\n            cleaned_files.append(features)\n\n        col_names = ['paper_id', 'title' , 'authors', \n                    'abstract', 'text', \n                     ]\n\n        clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n        clean_df.head()\n\n        return clean_df","6be0c44e":"\nroot_path = '\/kaggle\/input'\n# all_file_path = glob.glob('{}\/**\/*.json'.format(root_path), recursive=True)\n# print(str(len(all_file_path))+' papers were found')","1c110f90":"PR = PaperReader(root_path)","c0fdb96e":"papers_df = PR.generate_clean_df()","989ecb1a":"papers_df.info()","209aa381":"class Preprocessor:\n    \n    def __init__(self, df):\n        self.df = df\n        self.text = df['text']\n        self.abstract = df['abstract']\n        \n    def drop_duplicate(self):\n        \n        self.df.drop_duplicates(['abstract', 'text'], inplace = True)\n    \n    def dropna(self):\n        \n        self.df.dropna(inplace = True)\n    \n    def lower_case(self):\n        \n        self.df['text'] = self.df['text'].str.lower()\n        self.df['abstract'] = self.df['abstract'].str.lower()\n        #self.df['title'] = self.df['title'].str.lower()\n        \n        return self.df\n        \n        \n    def filter_language(self , language = 'en'):\n        \n        def detect_language(text):\n        \n            portion=text[1000:2000]\n            try:\n                lang=detect(portion)\n            except Exception:\n                lang=None\n\n            return lang\n\n        self.df['lang'] = self.df['text'].apply(detect_language)\n        self.df = self.df[self.df['lang'] == language]  \n        \n           \n        \n        \n    def clean_string(self):\n        \n        def cleaner(text_col):\n        \n            #remove newline characters\n#             clean_text = text_col.str.replace('\\n', ' ')\n            #remove strings with length <= 2\n            clean_text =text_col.str.replace(r'\\b\\w{1,2}\\b', '', regex=True)\n    \n            #separate words having dashline such as non-vegeterian --> non vegeterian\n            clean_text = clean_text.str.replace('-', ' ')\n            #remove punctuations and digits\n#           clean_text = clean_text.str.replace(r'[^A-Za-z\\s]', '', regex=True)\n             \n            #remove web links\n            clean_text = clean_text.str.replace(r'(http|https)[\\w\\d]+\\b','')\n            clean_text = clean_text.str.replace(r'www.[\\w\\d]+.[\\w\\d]+','')\n            #remove refrences\n            clean_text = clean_text.str.replace(r'\\[[\\d ]+\\]','')\n            #remove digits\n            clean_text = clean_text.str.replace(r'\\d','')\n\n            return clean_text\n        \n        self.df['text'] = cleaner(self.df['text'])\n        self.df['abstract'] = cleaner(self.df['abstract'])\n    \n    def get_df(self):\n        \n        return self.df\n\n","f525eee7":"PP = Preprocessor(papers_df)\n\nPP.dropna()\nPP.drop_duplicate()\nPP.filter_language()\nPP.clean_string()\nPP.lower_case()\n\npreprocessed_df = PP.get_df()","2a1ce247":"#preprocessed_df.to_csv('\/kaggle\/clean_dataset\/preprocessed_data.csv', index=False)","86525c3d":"#preprocessed_df = pd.read_csv('clean_dataset\/preprocessed_data.csv')","fe8631b8":"customize_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'introduction', 'section', 'abstract', 'summary',\n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'fig', 'fig.', 'al.', 'al'\n    'di', 'la', 'il', 'del', 'le', 'della', 'dei', 'delle', 'una', 'da',  'dell',  'non', 'si','also', 'too',\n    'que', 'qui', 'le', 'la', 'les', 'un', 'une', 'si','de', 'des', 'est', 'sont', 'plus', 'dans', 'par', 'ici',\n    'para', 'por', 'lo', 'sera', 'viru', 'caso', 'entre', 'avec', 'sur', 'ont', 'pour', 'pa', 'ce', 'ca', 'ces', 'cehz', 'son', 'moi', 'toi'\n]\n\nstop_words = list(stopwords.words('english')) \nstop_words = stop_words + customize_stop_words ","002762f1":"def lemmatizer(text):\n    global i\n    # Lemmatize every word.\n    # Init the Wordnet Lemmatizer\n    tokens = word_tokenize(text)\n    WNL = WordNetLemmatizer()\n    return ' '.join([WNL.lemmatize(y) for y in tokens if y not in stop_words])\n\nlemmas = preprocessed_df['text'].apply(lemmatizer)\n","bf18264c":"#lemmas.to_csv('clean_dataset\/tokens.csv', index=False)","03da342d":"#lemmas = pd.read_csv('clean_dataset\/tokens.csv')","97bfd2b1":"lemmas = lemmas.tolist()\n#tokens = [doc.split(\" \") for doc in lemmas]     # uncomment only if you need the tokens","32ced7f5":"n_samples = len(lemmas) \nn_features = 4000\nn_topics = 9\nn_top_words = 50","7a18f953":"class lda_tools:\n    \n    def __init__(self , text_list , n_topics = None):\n        self.text_list = text_list\n        self.words = [doc.split(\" \") for doc in text_list]\n        self.tf = None\n        self.tf_vec = None\n        self.lda = None\n        self.tf_feature_names = None\n        self.lda_out_vec = None\n        self.n_topics = n_topics\n        self.tsne_embedding = None\n    \n    \n    def count_vectorizer(self , max_df=0.95, min_df=2, max_features = 4000):\n        count_vectorizer = CountVectorizer(max_df = max_df, min_df = min_df , max_features = max_features)\n        t0 = time()\n        self.tf = count_vectorizer.fit(self.text_list)\n        self.tf_vec = count_vectorizer.transform(self.text_list)\n        self.tf_feature_names = count_vectorizer.get_feature_names()\n        print(\"Extracting tf features for LDA done in %0.3fs.\" % (time() - t0))\n        \n        return self.tf_vec\n        \n        \n    def lda_model(self,  n_components= 15, max_iter=5, learning_method='online', learning_offset = 50):\n        self.n_topics = n_components\n        self.lda = LatentDirichletAllocation(n_components = n_components, max_iter = max_iter, learning_method = learning_method , learning_offset = learning_offset , random_state=0)\n        t0 = time()\n        self.lda.fit(self.tf_vec)\n        print(\"Fitting LDA models with tf features, \"\n        \"n_samples=%d and n_features=%d done in %0.3fs.\" % (n_samples, n_features, time() - t0))\n        \n        return self.lda\n    \n    def lda_out(self):\n        if self.lda_out_vec is not None:\n            return self.lda_out_vec\n        else:\n            model = self.lda\n            tf_vec = self.tf_vec\n            self.lda_out_vec = model.transform(tf_vec)\n\n        \n        return self.lda_out_vec\n\n    def word_cloud_top_words(self, n_top_words = 15 , number_of_column = 3):\n        model = self.lda\n        feature_names = self.tf_feature_names\n        num_topics=model.components_.shape[0]\n        f, axarr = plt.subplots(int(np.ceil(num_topics\/number_of_column)),number_of_column,figsize=(16, 16))\n        Dict={}\n        for topic_idx, topic in enumerate(model.components_):\n            for i in topic.argsort()[:-n_top_words - 1:-1]:\n                Dict[feature_names[i]] = topic[i]\n            cloud = wordcloud.WordCloud(width=900, height=500,background_color='white').generate_from_frequencies(Dict)\n            x=int(np.floor(topic_idx\/number_of_column))\n            r=topic_idx%number_of_column\n            axarr[x,r].imshow(cloud)\n            axarr[x,r].axis('off')\n            topic_idx=topic_idx+1\n            axarr[x,r].set_title(\"Topic %d \" % topic_idx)\n            Dict={}\n\n    def print_top_words(self, n_top_words = 15):\n        feature_names = self.tf_feature_names\n        model = self.lda\n        for topic_idx, topic in enumerate(model.components_):\n            message = \"Topic #%d: \" % topic_idx\n            message += \" \".join([feature_names[i]\n                                 for i in topic.argsort()[:-n_top_words - 1:-1]])\n            print(message +\"\\n\\n\")\n\n            \n    def get_tSNE_embedding(self, n_components = 2):\n        if self.tsne_embedding is None:\n            lda_out = self.lda_out()\n            X_embedded = TSNE(n_components = n_components).fit_transform(lda_out)\n            self.tsne_embedding = X_embedded\n        else:\n            X_embedded = self.tsne_embedding\n        return X_embedded \n    \n    def get_y(self):\n        lda_out = self.lda_out()\n        y_pred = np.argmax(lda_out, axis=1)\n        return y_pred\n    \n    def tSNE_plot(self, n_components = 2):\n        X_embedded = self.get_tSNE_embedding(n_components = n_components)\n        y_pred = self.get_y()\n        fig = plt.gcf()\n        fig.set_size_inches(18.5, 10.5)\n        # colors\n        scatter=plt.scatter(X_embedded[:,0], X_embedded[:,1], c=y_pred , cmap=\"Paired\", s=50, alpha=1)\n        plt.title(\"t-SNE plot related to Non-negative Matrix Factorization on Covid-19 Articles\")\n        \n    #Silhouette refers to a method of interpretation and validation of consistency within clusters of data.\n    def calculate_silhouette(self):\n        X_embedded = self.get_tSNE_embedding(n_components = 2)\n        y_pred = self.get_y()\n        silhouette_score = metrics.silhouette_score(X_embedded, y_pred, metric='euclidean')\n        print (\"Silhouette_score: \")\n        print (silhouette_score)\n        \n    def topic_probability_scores(self , lda_out_vec = None, num_docs = None):\n        if num_docs is None:\n            num_docs = len(self.words)\n        if lda_out_vec is None:\n            lda_out_vec = np.matrix(self.lda_out())\n            \n        lda = self.lda\n        if not self.n_topics:\n            self.n_topics = 15\n        n_components = self.n_topics\n        # column names\n        topicnames = [\"Topic\" + str(i) for i in range(n_components)]\n\n        # index names\n        docnames = [\"Doc\" + str(i) for i in range(num_docs)]\n\n        # Make the pandas dataframe\n        df_document_topic = pd.DataFrame(np.round(lda_out_vec, 2), columns=topicnames, index=docnames)\n\n        # Get dominant topic for each document\n        dominant_topic = np.argmax(df_document_topic.values, axis=1)\n        df_document_topic['dominant_topic'] = dominant_topic\n\n        return df_document_topic\n        \n    def get_topic_keywords(self):\n        # Topic-Keyword Matrix\n        df_topic_keywords = pd.DataFrame(self.lda.components_)\n        \n        \n        # Assign Column and Index\n        df_topic_keywords.columns = self.tf_feature_names\n        df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n        \n        return df_topic_keywords\n    \n    # Show top n keywords for each topic\n    def show_topics(self, n_words=20):\n        feature_names = self.tf_feature_names\n        model = self.lda\n        keywords = np.array(feature_names)\n        topic_keywords = []\n        \n        for topic_weights in model.components_:\n            top_keyword_locs = (-topic_weights).argsort()[:n_words]\n            topic_keywords.append(keywords.take(top_keyword_locs))\n        \n        df_topic_keywords = pd.DataFrame(topic_keywords)\n        df_topic_keywords.columns = ['Keyword '+str(i) for i in range(df_topic_keywords.shape[1])]\n        df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n        \n        return topic_keywords  \n\n\n    def get_topic_distribution(self):\n        df_document_topics = self.topic_probability_scores()\n        df_topic_distribution = df_document_topics['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n        df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n        return df_topic_distribution\n    \n    def predict_topic(self,new_text_list):\n        \n        num_new_text = len(new_text_list)\n        test_tf_vec = self.tf.transform(new_text_list)\n        test_lda_out = self.lda.transform(test_tf_vec) \n        df_topic_keywords = self.get_topic_keywords()\n        #topics = df_topic_keywords.iloc[np.argmax(test_lda_out), :].values.tolist()\n        \n        topic_probability_score = self.topic_probability_scores(test_lda_out, num_new_text)\n        \n        return  test_lda_out , topic_probability_score\n    \n    def get_similar_documents(self, df, lda_out_test, top_n_similar, verbose=False):\n        \n        lda_out_vec = self.lda_out()\n        dists = euclidean_distances(lda_out_test.reshape(1, -1), self.lda_out_vec)[0]\n        \n        try:\n            doc_ids = np.argsort(dists)[:top_n_similar]\n            \n        except Exception:\n            doc_ids = np.argsort(dists)\n            \n        print('doc_ids = \\n{}'.format(doc_ids))\n\n        return doc_ids, df.iloc[doc_ids, :]\n    ","d30e725e":"LT = lda_tools(lemmas)\nLT.count_vectorizer(max_df=0.95, min_df=2, max_features = 6000)","abdba3bc":"LT.lda_model(n_components= n_topics, max_iter=5, learning_method='online', learning_offset = 50)","dcc3e368":"# filehandler = open('clean_dataset\/lda_object.txt', 'wb') \n# pickle.dump(LT, filehandler)","e86be1a1":"# filehandler = open('clean_dataset\/lda_object.txt', 'rb') \n# LT = pickle.load(filehandler)","cc1ffd0a":"LT.print_top_words(n_top_words = 15)","7794539e":"LT.word_cloud_top_words(n_top_words = 30)","d38cab84":"LT.tSNE_plot(n_components = 2)","602bfd82":"LT.calculate_silhouette()","94f874ee":"df_document_topics_probability = LT.topic_probability_scores()\ndf_document_topics_probability","83929dd3":"topics_keywords_df = LT.get_topic_keywords()\ntopics_keywords_df.head(n_topics)","19cac748":"topic_distribution = LT.get_topic_distribution()\nax = topic_distribution.plot.bar(x='Topic Num', y='Num Documents', colormap='Paired')\nax.set_ylabel('Num Documents')","2c987f55":"new_text_list = lemmas[15:20]\n\n# or  a question = ['how we can provide urgent help poor countries to have access to sustainable education system while the virus is spraeding like e.g support online schools, distance learning and etc. ?']\n\nld_out , df_document_topics_probability = LT.predict_topic(new_text_list)\n\npredicted_topic = df_document_topics_probability['dominant_topic']\n\ndf_document_topics_probability\n#print(\"the topic of this document is: \"+ str(predicted_topic))","29147595":"doc_ids, docs = LT.get_similar_documents(preprocessed_df, ld_out[0], top_n_similar = 10, verbose=False)\n\ndocs","44921ef2":"def read_corpus(df, column, tokens_only=False):\n    \"\"\"\n    Arguments\n    ---------\n        df: pd.DataFrame\n        column: str \n            text column name\n        tokens_only: bool\n            wether to add tags or not\n    \"\"\"\n    for i, line in enumerate(df[column]):\n        \n        tokens = gensim.parsing.preprocess_string(line)\n        if tokens_only:\n            yield tokens\n        else:\n            # For training data, add tags\n            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])","6b277e66":"frac_of_articles = 1\ntrain_df  = preprocessed_df.sample(frac=frac_of_articles, random_state=42)\ntrain_corpus = (list(read_corpus(train_df, 'abstract'))) ","34faf26f":"# using distributed memory model\nmodel = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=300, min_count=2, epochs=20, seed=42, workers=3)\nmodel.build_vocab(train_corpus)\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)","707bb411":"# model_name = \"Ethics_Doc2Vec_model.model\"\n# model.save(model_name)","8f5a5e75":"# model_name = \"Ethics_Doc2Vec_model.model\"\n# model = Doc2Vec.load(model_name)","a7207abf":"abstract_vectors = model.docvecs.vectors_docs\ntrain_df['abstract_vector'] = [vec for vec in abstract_vectors]","82e3f4b6":"def get_doc_vector(doc):\n    tokens = gensim.parsing.preprocess_string(doc)\n    vector = model.infer_vector(tokens)\n    return vector","7110f4bb":"question_0 = \"What has been published concerning ethical considerations for research?\"\nquestion_1 = \"What has been published concerning social sciences at the outbreak response?\"\nquestion_2 = \"Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\"\nquestion_3 = \"Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\"\nquestion_4 = \"Efforts to support sustained education, access, and capacity building in the area of ethics\"\nquestion_5 = \"Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\"\nquestion_6 = \"Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\"\nquestion_7 = \"Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\"\nquestion_8 = \"Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"\nquestion_other=\"Other\"","06f30b7b":"list_of_tasks = [question_0, question_1, question_2, question_3, question_4, question_5, question_6, question_7, question_8, question_other]","33c9e6e1":"import ipywidgets as widgets\nfrom ipywidgets import interactive, Layout\nfrom IPython.display import clear_output\n\nouts = widgets.Output()\n\nuserInstLbl = widgets.Label('Please select from the following list of questions')\ndisplay(userInstLbl)\n\nquestions_widget = widgets.Dropdown(\n        options=list(list_of_tasks),\n        value=None,\n        description='Questions:'\n    )\n\ndisplay(questions_widget)\n\ndisplay(outs)\n\nmyVar = ''\n\n#Display the textbox if user selects \"Other\" from the dropdown\nsearchTxt=widgets.Text(placeholder='Please Type in Your Own Question')\n#display(searchTxt)\n#searchTxt.layout.visibility = 'hidden'\n\n\ndef dropdown_eventhandler(change):\n    global myVar\n    with outs:\n        clear_output()\n        myVar = change['new']\n        #print('Value Changed to = '+ myVar)\n        if myVar == 'Other':\n            display(searchTxt)\n            searchTxt.layout.visibility = 'visible'\n        else:\n            searchTxt.layout.visibility = 'hidden'\n            searchTxt.value = ''\n\nquestions_widget.observe(dropdown_eventhandler, names='value')\n\nbtn = widgets.Button(description='Search', button_style='primary',  width='auto')\n#btn.style.button_color = 'lightblue'\ndisplay(btn)\n\ndef search(b):\n    #print('INSIDE Search')\n    myVar=searchTxt.value\n    #print(searchTxt.value)\n\nbtn.on_click(search)","170a43f2":"if myVar == 'Other':\n    qsn = searchTxt.value\nelse:\n    qsn = myVar","5559f3ea":"user_question = [qsn]\narray_of_tasks = [get_doc_vector(task) for task in user_question]","83ae7897":"train_df = train_df[train_df['abstract'].apply(lambda x: len(x)) > 30]\ntrain_array = train_df['abstract_vector'].values.tolist()\nball_tree = NearestNeighbors(algorithm='ball_tree', leaf_size=30).fit(train_array)","b2da0863":"distances, indices = ball_tree.kneighbors(array_of_tasks, n_neighbors=100)","78d8cb13":"normalized_distances = (distances-np.min(distances))\/(np.max(distances)-np.min(distances))\nnormalized_scores = 1 - normalized_distances\nnormalized_scores = np.round(normalized_scores, 2)","0c441428":"summarizer_ratio = 0.02","938c573b":"for i, info in enumerate(user_question):\n    print(\"=\"*100, f\"\\n\\nQuestion {str(i+1)} = {info}\\n\", )\n    df =  train_df.iloc[indices[i]]\n    abstracts = df['abstract']\n    titles = df['title']\n    authors =  df['authors']\n    texts = df['text']\n    dist = normalized_scores[i]","426d9b7b":"import re\nfrom IPython.display import HTML\n\n#\n# Function to take an URL string and text string and generate a href for embedding\n#\ndef href(val1,val2):\n    return '<a href=\"{}\" target=\"_blank\">{}<\/a>'.format(val1,val2)\n\n#\n# Function to set the caption\n#\n\n\ndef setCaption(html, caption):\n    html = re.sub('mb-0\">', 'mb-0\"><caption>' + caption + '<\/caption>', html)\n    return html\n\n#\n# Function to add column width for a particular column within the HTML string\n#\ndef setColWidth(html, col, width):\n    html = re.sub('<th>'+ col,  '<th width=\"'+ width+ '%\">'+col, html)\n    return html\n\n\n\n#\n#Function to generate HTML table string that can be displayed\n#\ndef createHtmlTable(df,userinput):\n    # CSS string to justify text to the left\n    #css_str ='<style>.dataframe thead tr:only-child th {text-align: left;}.dataframe thead th {text-align: left;}.dataframe tbody tr th:only-of-type {vertical-align: middle;}.dataframe tbody tr th {vertical-align: top;}.dataframe tbody tr td {text-align: left;}.dataframe caption {display: table-caption;text-align: left;font-size: 15px;color: black;}<\/style>'\n    css_str ='<style>.dataframe thead tr:only-child th {text-align: left;}.dataframe thead th {text-align: left;}.dataframe tbody tr th:only-of-type {vertical-align: middle;}.dataframe tbody tr th {vertical-align: top;}.dataframe tbody tr td {text-align: left;}.dataframe caption {display: table-caption;text-align: left;font-size: 15px;color: black;font-weight: bold;}<\/style>'\n        \n    # Generate HTML table string\n    html_str = df[['Relevancy Score', 'Title', 'Authors','Summary']].to_html(render_links=True, index=False,  classes=\"table table-bordered table-striped mb-0\")\n    \n    # Set table caption\n    html_str = setCaption (html_str, userinput)\n    \n    # Perform a few adjustments on the HTML string to make it even better\n    html_str = re.sub('&lt;', '<', html_str)\n    html_str = re.sub('&gt;', '>', html_str)\n    html_str = setColWidth(html_str, 'Relevancy Score', '15')\n    html_str = setColWidth(html_str, 'Title', '20')\n    html_str = setColWidth(html_str, 'Authors', '20')     \n    html_str = setColWidth(html_str, 'Summary', '45')\n    \n    # Return the final HTML table string for display\n    return css_str + html_str","79659f62":"from IPython.display import Markdown, display\n\nfinal_df = pd.DataFrame(columns=['Relevancy Score', 'Title', 'Authors', 'Summary'])\nfinalDf = pd.DataFrame()\n\nfor l in range(len(dist)):\n    text_summary = \"\"\n    text_summary = summarize((train_df['abstract'].iloc[l]+train_df['text'].iloc[l]), ratio=summarizer_ratio)\n    final_df = final_df.append({'Relevancy Score': round(dist[l],2) , 'Title': titles.iloc[l] , 'Authors': authors.iloc[l] , 'Summary' : text_summary}, ignore_index=True)\n    \n#pd.option_context('display.colheader_justify','left')\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n#pd.set_option('display.max_colwidth', None)\npd.set_option(\"colheader_justify\", \"left\")\npd.set_option('max_seq_item', None)\n\n\nHTML(createHtmlTable(final_df,qsn))","6be42aca":"The silhouette metrics is a measure of performance of the clustering. The silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If the score is near 0, it means either we have too many clusters or too few of them. ","3fdde7bc":"**Run the following 2 cells only one time!**","317cc2a6":"## Results visualization\nPrint out the the most relevant abstracts, as well as, the summary","9cac67cf":"**The following cell will take some time to run**","8b8f18b2":"We can save and reuse the trained model.","5b0bc63f":"## Other processing steps","df15df72":"The abstracts of all articles are used to train the Doc2Vec model","1487f1b0":"Calculate the normalized relevancy score","4d7ca36a":"Ericsson, the world\u2019s leading telecommunications company, cares about doing good. This task was completed as part of our Ericsson for Good program, which allows our 90,000+ employees to contribute to their communities.\n\n\u00a9 This Notebook has been released under the OSI-approved GNU LGPLv2.1 license. ","1f1db757":"## Load Files","38c178be":"To get the topic probability for a new document:","2960f805":"## Cleaning the Data","3d09f1bc":"These functions were defined to standardize the format of different information fields. \n\n* `format_name(author)`\n* `format_affiliation(affiliation)`\n* `format_authors(authors, with_affiliation=False)`\n* `format_body(body_text)`\n* `format_bib(bibs)`","57a44363":"## Take the input question\n\n### Please provide your desired question.\nWe then calculate its vectors with the Doc2Vec model in order to find the most relevant documents, accordingly.","faa5cfc5":"for loading an existing LT object for the following sections, run the cell below: (make sure you have run the cell containing the class first)","bb8c4cca":"This class loads the papers and retreives the formatted information from the JSON format, and creates a DataFrame. To save on memory, only a subset of papers were used in this kernel. In the code below, you will have the option to remove a line mentioned with a comment, and use the whole set of data. ","e1c75426":"We can save the LDA model to use later with the visualization tools, as it takes time to retrain the model. ","f8f68785":"## LDA Topic Modeling","376ac929":"Transform 1D DataFrame into 1D list where each index is an article (instance), so that we can work with words from each instance:","ef996a1d":"**It takes some time to run the lda_model **","626a77cb":"## Load the pre-trained model","28ec6b62":"## FEATURES OF THIS NOTEBOOK\n\nThe rest of this notebook contains the following components:\n\n* ### Methodology\n\n* ### Pros and Cons\n\n* ### Input\/Output incl. Data Sources\n\n* ### Code and Results\n\n* ### Acknowledgement and Licensing\n\n    Some parts of this notebook are excerpted from the notebooks below (Thanks to @xhlulu and @LuisBlanche): <br>\n     > https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n      \n     > https:\/\/www.kaggle.com\/luisblanche\/cord-19-use-doc2vec-to-match-articles-to-tasks\n","72ee0328":"## Class PaperReader","50298e10":"# <center> COVID-19 CHALLENGE \n# <center> TASK 5\n![image.png](attachment:image.png)\n\n## THE CRITICAL CHALLENGE\n\nHow do healthcare communities, institutions, and nations confront the Novel Coronavirus Disease 2019 (COVID-19) pandemic while facing difficult and serious questions about their ethical responsibilities towards human societies and their governments?\n    \n## EXECUTIVE SUMMARY\n* The Ericsson Task 5 team has developed a **Question-Answering Machine (intelligent search engine)**, using data science and data mining techniques, to assist the medical community in easily finding the most relevant answers to ethical and social considerations.\n\n    \n* This notebook brings together all available data sources together to answer the question of what has been published about ethical and social science considerations as it relates to COVID-19.\n\n    \n* The CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection of more than 30,000 scientific articles available for data mining to date. However, the rapid increase in the volume and type of coronavirus literature makes it difficult for the medical community to keep up. \n  \n    \n* There is a growing urgency to develop text and data mining approaches to help the medical community to find answers to questions within, and connect insights across, this content in support of the ongoing COVID-19 response efforts worldwide.\n\n    \n\n\n## OVERVIEW VIDEO\nThis **5-minute video summary** lays out the challenge this work addresses and shows the user how to make rapid use of this specially created solution for Task 5.   It summarizes the approach, with screen capture annotations that make the code easy to read and reuse. Data visualizations that communicate the findings are explained.\n>### https:\/\/web.microsoftstream.com\/video\/e14e3f49-982e-4c68-9cf3-b86913e13081\n    ","482d0b4a":"The class *preprocessor* contains functions for preprocessing steps. ","a84f4ab0":"Get similar documents to the first test document:","699e1d3f":"### Find the most relevant documents to the given question via searching through the nearest neighbors ","ddc5f24e":"## Topic Modelling","1f2d6d77":"## Helper Functions ","1a2b3030":"## Train gensim Doc2Vec model","e269537a":"Define the parameters for countvectorizer. Use tf (raw term count) features for LDA. <br>\nSet number of topics equivalent to the number of ethics-related questions.","bd2fbf17":"### Text summarization has been done using Gensim TextRank graph-based ranking algorithm\nYou can adjust the length of text that summarizer outputs via the \"summarizer_ratio\" parameter.","8f9a48a7":"You can continue analysing the LDA results. ","2ad4ba6b":"Some words appear repetitively in the papers, but they are not informative about the topic, such as *doi*. These words were added to the customized stopwords set.","4ef6b743":"## METHODOLOGY\n\nThe workflow in this figure illustrates the approach taken and includes both the design and operational phases. \n![image.png](attachment:image.png)\n\n* In the **design phase**, the CORD-19 dataset is used to create the knowledge base, including the features extracted from its contents (abstracts and texts of the published articles), using pre-processing, model training and retrieval techniques. \n\n\n* The resulting **trained model** is then used to retrieve the most relevant answers (selected parts of the articles) to the questions entered by the user during the operational phase. \n\n\nThe 3 main steps of the approach are as follows:\n\n\n1.\t**Preprocessing**: An important technique, data preprocessing transforms raw data into a format required for the modeling step. It results in retaining the most important keywords which helps to improve the efficiency of the retrieval process, as well as the quality of the results. Activities involved in this step are: <br>\n\n        a.\tDiscard non-English articles (less than 5%) <br>\n        b.\tChanging words to lower case (Lower_case) <br>\n        c.\tRemove stop_words <br>\n        d.\tLemmatization to group together inflected forms of a word so they can be analyzed as a single item, identified by the word's lemma (dictionary form) <br>\n        e.\tStemming to reduce inflected words to their word stem, base or root form\u2014generally a written word form <br>\n\n\n2.\t**Model Training**: Data modeling in general is the process of producing a descriptive diagram of relationships between various types of information that are to be stored in a database. One of the goals of data modeling is to create the most efficient method of storing information while still providing for complete access and reporting. The modeling process as explained below is divided into multiple activities including topic modeling, Doc2Vec training and Ball-tree nearest neighbor research:\n    \n      a.\t**Topic Modeling**:\n      \n      * **LDA topic modeling**: a type of statistical unsupervised modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents. Latent Dirichlet Allocation (LDA) is used to classify text in a document to a specific topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions. We have applied LDA to select several topics with regard to the pertinent questions about our task. It can provide some insights to the user about the global overview of the entire dataset using the most frequent keywords in each topic.  \n           \n      * **Topic clustering**: topic clusters are a collection of interlinked articles or pages around one umbrella topic. They ultimately allow you to provide greater visibility for search engines to identify your content. A topic cluster starts with the main topic, which is also known as your \u201cpillar\u201d page.    \n          \n      * **Topic prediction and ranking**: this helps predict the similarity of the question to different topics that may help the user to either refine the question or limit the search space to find the most relevant answers using the dominant topic.\n\n      b.\t**Doc2Vec training**: Doc2Vec approach is employed to convert the text into feature vector, where it maintains the spatiotemporal relation of the sentences within the document. Since the abstract of each document is the most informative section, it is considered as the **training data**. It may help to improve the retrieval performance (less computational cost) and training a lighter model.\n\n      c.\t**Ball-tree**: This is the unsupervised version of KNN classification method, where it finds the nearest neighbors (vectors of articles) to the given question. The distance calculated using Ball-tree can be used as the relevancy score of retrieved articles to the questions.\n\n\n3.\t**Retrieval**: During the operation, we apply the trained model to extract a feature vector out of the question and calculate its distance to all the documents to obtain the text similarity. The distances will then be normalized as the relevancy scores and the articles will be ranked according to these scores. Finally, the _Gensim TextRank\u00a9_ method is used to summarize the abstract and the text of the retrieved documents in order to provide the most valuable sentences.\n\n\n## PROS AND CONS OF THIS METHODOLOGY\n\nPros:\n* In contrast to Word2Vec, the Doc2Vec model considers the matrix of paragraphs on top of the word windows during the training, where it helps to extract higher-level concepts than low-level features of Word2Vec.\n* The model can extract low-dimensional features, yet meaningful from the multiple consecutive sentences.\n* The model can be trained in the distributed fashion, where it can return the trained model in a reasonable time.\n\nCons:\n* It may fail to extract appropriate features over the short sentences with limited number of words.\n* It does not consider attention mechanism to apply more weights to the important combination of keywords and sentences.\n\n\n\n## DATA SOURCE\n\n1.\tmetadata.csv\n2.\tjson_schema.txt\n3.\tbiorxiv_medrxiv\n4.\tcomm_use_subset\n5.\tCOVID.DATA.LIC.AGMT.pdf\n6.\tcustom_license\n7.\tmetadata.readme\n8.\tnoncomm_use_subset\n\n","d66e0fcb":"We can save this data.","47b3b78c":"Load the cleaned dataset and apply processing steps.\nSteps in processing the text:\n\n- Tokenization\n- Removing the stopwords\n- Lemmatization"}}