{"cell_type":{"b2f90419":"code","2721ed4e":"code","5d8869b0":"code","37df6356":"code","5029f7e5":"code","89e56196":"code","b1bf84a2":"code","ebb52627":"code","906958ba":"code","d5f6c8cd":"code","8eca486e":"markdown"},"source":{"b2f90419":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import evaluate, make","2721ed4e":"!pip install stable-baselines3","5d8869b0":"import matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common import logger\nfrom stable_baselines3.common.callbacks import EvalCallback\n\nfrom shutil import copyfile\nimport os\n\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.monitor import Monitor\n\ndef transform_observation(obs, config):\n    my_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    their_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    food_board = np.zeros((config.columns * config.rows * 1), dtype = np.uint8)\n    for goose in obs[0].observation.geese[0]:\n        my_board[goose] = 255\n    my_board = my_board.reshape((config.rows, config.columns, 1))\n    \n    for goose in obs[0].observation.geese[1]:\n        their_board[goose] = 255\n    their_board = their_board.reshape((config.rows, config.columns, 1))\n    \n    for goose in obs[0].observation.food:\n        food_board[goose] = 255\n    food_board = food_board.reshape((config.rows, config.columns, 1))\n    board = np.concatenate([my_board, their_board, food_board], axis = -1)\n    return board","37df6356":"def transform_actions(actions):\n    if actions == 0:\n        return \"NORTH\"\n    if actions == 1:\n        return \"EAST\"\n    if actions == 2:\n        return \"WEST\"\n    if actions == 3:\n        return \"SOUTH\"","5029f7e5":"geese_env = make(\"hungry_geese\")","89e56196":"REWARD_LOST = -1\nREWARD_WON = 1\nclass GeeseGym(gym.Env):\n    def __init__(self, debug = False):     \n        self.geese_env = make(\"hungry_geese\", debug = debug)\n        self.config = self.geese_env.configuration\n        self.action_space = spaces.Discrete(4)\n        \n        self.observation_space = spaces.Box(low=0, high=255, \n                                            shape=(self.config.rows, \n                                                   self.config.columns, \n                                                   3), \n                                            dtype=np.uint8)\n        \n        \n        self.reward_range = (-1, 1000)\n    def reset(self):\n        self.obs = self.geese_env.reset(num_agents = 2)\n        x_obs = transform_observation(self.obs, self.config)\n        return x_obs\n    \n    \n    def step(self, action):\n        my_actions = transform_actions(action)\n        opponent_action = transform_actions(0)\n        self.obs = self.geese_env.step([my_actions, opponent_action])        \n        x_obs = transform_observation(self.obs, self.config)\n        x_reward = self.obs[0].reward\n        done = (self.obs[0][\"status\"] != \"ACTIVE\")\n        info = self.obs[0][\"info\"]\n        return x_obs, x_reward, done, info","b1bf84a2":"# Settings\nSEED = 17\nNUM_TIMESTEPS = int(1e7)\nEVAL_FREQ = int(1e4)\nEVAL_EPISODES = int(1e2)\nBEST_THRESHOLD = 0.01 # must achieve a mean score above this to replace prev best self\n\nLOGDIR = \"ppo1_selfplay\"\n\nclass GeeseSelfPlayEnv(GeeseGym):\n  # wrapper over the normal single player env, but loads the best self play model\n    def __init__(self):\n        super(GeeseSelfPlayEnv, self).__init__()\n        self.policy = self\n        self.best_model = None\n        self.best_model_filename = None\n    def predict(self, obs): # the policy\n        if self.best_model is None:\n            return self.action_space.sample() # return a random action\n        else:\n            action, _ = self.best_model.predict(obs)\n        return action\n    def reset(self):\n        # load model if it's there\n        modellist = [f for f in os.listdir(LOGDIR) if f.startswith(\"history\")]\n        modellist.sort()\n        if len(modellist) > 0:\n            filename = os.path.join(LOGDIR, modellist[-1]) # the latest best model\n            if filename != self.best_model_filename:\n                print(\"loading model: \", filename)\n                self.best_model_filename = filename\n                if self.best_model is not None:\n                    del self.best_model\n                self.best_model = PPO.load(filename, env=self)\n        return super(GeeseSelfPlayEnv, self).reset()\n\nclass SelfPlayCallback(EvalCallback):\n  # hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n  # after saving model, resets the best score to be BEST_THRESHOLD\n    def __init__(self, *args, **kwargs):\n        super(SelfPlayCallback, self).__init__(*args, **kwargs)\n        self.best_mean_reward = BEST_THRESHOLD\n        self.generation = 0\n    def _on_step(self) -> bool:\n        result = super(SelfPlayCallback, self)._on_step()\n        if result and self.best_mean_reward > BEST_THRESHOLD:\n            self.generation += 1\n            print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n            print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)\n            source_file = os.path.join(LOGDIR, \"best_model.zip\")\n            backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n            copyfile(source_file, backup_file)\n            self.best_mean_reward = BEST_THRESHOLD\n        return result\n\ndef rollout(env, policy):\n    obs = env.reset()\n\n    done = False\n    total_reward = 0\n\n    while not done:\n        action, _states = policy.predict(obs)\n        obs, reward, done, _ = env.step(action)\n        total_reward += reward\n\n    return total_reward","ebb52627":"def make_env(rank=0):\n    def _init():\n        env = GeeseSelfPlayEnv()\n        log_file = os.path.join(LOGDIR, str(rank))\n        return env\n","906958ba":"logger.configure(folder=LOGDIR)\n# env = GeeseSelfPlayEnv()\n# env = SubprocVecEnv([GeeseGym() for i in range(4)])\nmodel = PPO('MlpPolicy', GeeseGym(), verbose = 1, n_steps = 2048*16, batch_size = 128, n_epochs = 50, learning_rate = .01)\n# eval_callback = SelfPlayCallback(env,\n#     best_model_save_path=LOGDIR,\n#     log_path=LOGDIR,\n#     eval_freq=EVAL_FREQ,\n#     n_eval_episodes=EVAL_EPISODES,\n#     deterministic=False)\nmodel.learn(total_timesteps=NUM_TIMESTEPS)\nmodel.save(os.path.join(LOGDIR, \"final_model\")) # probably never get to this point.","d5f6c8cd":"def run_test(model):\n    env = GeeseGym(debug = True)\n    obs = env.reset()\n    done = False\n    while not done:\n        actions = model.predict(obs)[0]\n        obs, reward, done, info = env.step(actions)\n        print(reward)\n#         plt.imshow(obs)\n#         plt.show()\nrun_test(model)","8eca486e":"Started up working on a stable_baselines3 self-play implementation for snake. Still lots of work to do to get it fully functional but figured I would open source it so others can work off it\/give me tips if they see anything blatantly misconfigured since I am relatively new to applying RL. \n\nNotebook is a blend of https:\/\/github.com\/hardmaru\/slimevolleygym\/blob\/master\/training_scripts\/train_ppo_selfplay.py and https:\/\/www.kaggle.com\/kwabenantim\/stable-baselines-starter"}}