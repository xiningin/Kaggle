{"cell_type":{"fa3004f5":"code","b2a49e20":"code","b0498d6e":"code","5498be59":"code","0938ca37":"code","89ca1692":"code","12a1a027":"code","e84bd7c1":"code","ec5b61ce":"code","da6beed0":"code","569c2931":"code","68a4fe7c":"code","ac23905e":"code","9a95e207":"code","58aaf772":"code","5d97610b":"code","b64ab800":"code","a3bef648":"code","6ee75ef5":"code","a4d9e9ae":"markdown","234929aa":"markdown","9b3774f9":"markdown","7aa8baf3":"markdown"},"source":{"fa3004f5":"import numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\ndef rmlse(preds,train_data):\n    labels = train_data.get_label()\n    rmlse = sqrt( mean_squared_error( np.log(labels), np.log(preds) )  ) \n    return 'rmlse',rmlse, False \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndef label_encoding(train: pd.DataFrame, test: pd.DataFrame, col_definition: dict):\n        \"\"\"\n        col_definition: encode_col\n        \"\"\"\n        n_train = len(train)\n        train = pd.concat([train, test], sort=False)\n        for f in col_definition['encode_col']:\n            try:\n                lbl = preprocessing.LabelEncoder()\n                train[f] = lbl.fit_transform(list(train[f].values))\n            except:\n                print(f)\n        test = train[n_train:]\n        train = train[:n_train]\n        return train, test","b2a49e20":"train = pd.read_csv(os.path.join(dirname, \"train.csv\"))\ntest = pd.read_csv(os.path.join(dirname, \"test.csv\"))\nprint(\"Dim_train:\",train.shape)\nprint(\"Dim_test:\",test.shape)","b0498d6e":"y_train = train[[\"SalePrice\",\"Id\"]].copy().set_index(\"Id\")\ntrain   = train.drop(\"SalePrice\",axis=1).reset_index().set_index(\"Id\")\ntest    = test.reset_index().set_index(\"Id\")","5498be59":"y_train.describe()","0938ca37":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsb.distplot(y_train, rug=True,ax=ax[0])\nax[0].set_title('Empirical distribution')\nax[0].set_xlabel('SalePrice')\nsb.boxplot(y_train,ax=ax[1])\nax[1].set_title('BoxPlot')\nplt.show()\n","89ca1692":"train.info()","12a1a027":"train.describe()","e84bd7c1":"pd.DataFrame(train.isnull().sum()\/len(train)*100,columns=[\"% of missing train\"]).join(\n    pd.DataFrame(test.isnull().sum()\/len(test)*100,columns=[\"% of missing test\"])).sort_values(by=\"% of missing train\", ascending=False)","ec5b61ce":"f, ax = plt.subplots(figsize=(9, 9))\nsb.heatmap( train.corr(), \n            vmax=.8, square=True)","da6beed0":"test = test.loc[:,train.isnull().sum()\/len(train)*100<30]\ntrain = train.loc[:,train.isnull().sum()\/len(train)*100<30]","569c2931":"non_categorical_missing_train = train.loc[:,(train.dtypes!=\"object\") & (train.isnull().sum()\/len(train)*100!=0)].columns\nnon_categorical_missing_test  = test.loc[:,(test.dtypes!=\"object\") & (test.isnull().sum()\/len(test)*100!=0)].columns\n\nfor i in non_categorical_missing_test:\n     test[i]=test[i].fillna(train[i].median())\n\nfor i in non_categorical_missing_train:\n     train[i]=train[i].fillna(train[i].median())\n        \ntrain = train.fillna(\"none\")\ntest  = test.fillna(\"none\")","68a4fe7c":"pd.DataFrame(train.isnull().sum()\/len(train)*100,columns=[\"% of missing train\"]).join(\n    pd.DataFrame(test.isnull().sum()\/len(test)*100,columns=[\"% of missing test\"])).sort_values(by=\"% of missing train\", ascending=False)","ac23905e":"cols = ['OverallQual','GrLivArea',\n       'GarageCars', 'GarageArea', 'TotalBsmtSF',\n       '1stFlrSF', 'FullBath', 'TotRmsAbvGrd',\n       'YearBuilt']\nsb.pairplot(train[cols].join(y_train), size =5)","9a95e207":"categorical_variables =  train.loc[:,(train.dtypes==\"object\")].columns.tolist()","58aaf772":"train, test = label_encoding(train, test, col_definition={'encode_col': categorical_variables})","5d97610b":"kf = KFold(n_splits=5,shuffle=True,random_state=123)\ndrop = categorical_variables\ntest_pred  = []\ntrain_pred = []\n\nfor i,(a,b) in enumerate(kf.split(train,y_train.loc[train.index, ])) :\n    Xt = train.iloc[a,:]\n    yt = np.log(y_train.iloc[a, ])\n   # Xt = Xt.drop(drop,axis=1)\n    \n    Xv = train.iloc[b,:]\n    yv = np.log(y_train.iloc[b, ])\n    #Xv = Xv.drop(drop,axis=1)\n    \n    train_data=lgb.Dataset(Xt, yt, categorical_feature=list(categorical_variables))\n    test_data=lgb.Dataset(Xv, yv, reference=train_data, categorical_feature=list(categorical_variables))\n    \n    print('---------- Training fold N\u00ba {} ----------'.format(i+1))\n    \n   \n    params = {'num_leaves': 40,  'min_data_in_leaf': 45, 'objective': 'regression_l1', 'max_depth': 4, 'learning_rate': 0.05, 'boosting': 'gbdt',\n         'random_state': 123, 'metric': ['rmse'], 'verbosity': -1,'type':'gamma'}\n    \n    model1 = lgb.train(params,train_data,valid_sets=[train_data, test_data],verbose_eval=10,num_boost_round=3000 , early_stopping_rounds=200)\n                       #, feval=rmlse)\n\n    \n    train_pred.append(pd.Series(model1.predict(Xv),\n                                index=Xv.index, name=\"pred\"+ str(i)))\n    test_pred.append(pd.Series(model1.predict(test),\n                                index=test.index, name=\"fold_\" + str(i)  ))\n      \ntest_pred = np.exp(pd.concat(test_pred, axis=1).mean(axis=1))\ntrain_pred = pd.concat(train_pred, axis=1)","b64ab800":"print(f'CV: {np.sqrt(mean_squared_error(np.log(y_train), train_pred.mean(axis=1) ))}')\nnp.log(y_train).describe()","a3bef648":"train_pred.mean(axis=1).describe()","6ee75ef5":"test_pred = pd.DataFrame(test_pred.rename(\"SalePrice\"))\ntest_pred.to_csv(\"lightgbm_baseline_func:l1.csv\", header=True)","a4d9e9ae":"# Simplest imputation\n\nFor this first experiment the simplest imputation is done. Because we have explanatory variables with a lot of missing data points, some of them will not be considered with 30% as the cut off .\n","234929aa":"# **The explanatory variables**\n\nIn this section, the set of explanatory variables is analyzed. \n1. Descriptive statistic  \n2. How many variables are lossing? What is the %?\n3. Correlation?","9b3774f9":"# **Lightgbm with cross validation**\n\nBecause the metric is the RMSE between the logarithm of the predicted value and the logarithm of the observed sales price, the new target is set as the logarithm of the saleprice.","7aa8baf3":"# **The distribution of SalePrice**\n> The *SalePrice* has an asymmetric behavior as it can see in the empirical distribution. So, there are differents ways to fit a model, can be used an asymmetric distributions like the gamma, log-normal, log t-Student, etc. Because the support of the response variable is the R+, a distribution with this support must be required.\n\n\n"}}