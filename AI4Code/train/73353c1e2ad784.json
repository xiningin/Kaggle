{"cell_type":{"00bcf08a":"code","54e78010":"code","612c407d":"code","d776a101":"code","1ce3456f":"code","a19ba09e":"code","8fa6613e":"code","4f7de1d6":"code","8e8a165a":"code","d73ce1d9":"code","59635550":"code","e1241f26":"code","fa0f7559":"code","f88d2dec":"markdown","d9eabc50":"markdown","34cb65fe":"markdown","61daaeab":"markdown"},"source":{"00bcf08a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","54e78010":"!pip install pyspark","612c407d":"from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as f\nspark = SparkSession.builder.appName(\"NLP\")\\\n.config(\"spark.yarn.maxAppAttempts\",\"2\")\\\n.config(\"spark.num.executors\",\"50\")\\\n.config(\"spark.executor.memory\",\"12g\")\\\n.config(\"spark.driver.memory\", \"12g\")\\\n.config(\"spark.memory.offHeap.enabled\",True)\\\n.config(\"spark.memory.offHeap.size\",\"12g\")\\\n.getOrCreate()\nspark.sql(\"set spark.sql.legacy.timeParserPolicy\")","d776a101":"trains = spark.read.options(inferSchema = True,header = True).csv(\"..\/input\/uit-spring-2021-ds200-assignment-9\/train.csv\").cache()\ntest = spark.read.options(inferSchema = True,header = True).csv(\"..\/input\/uit-spring-2021-ds200-assignment-9\/test.csv\").cache()\n#trains.printSchema()\n#trains.show(5,False)","1ce3456f":"import html\nfrom pyspark.sql.functions import udf\n@udf\ndef html_unescape(s: str):\n    return html.unescape(s)\ndef Processing(df):\n    user_regex = \"@\\w{1,}\"\n    email_regex = r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n    hastag_regex = \"#\\w{1,}\"\n    url_regex = r\"((https?):((\/\/)|(\\\\\\\\))+([\\w\\d:#@%\/;$()~_?\\+-=\\\\\\.&](#!)?)*)\"\n    \n    text = df.withColumnRenamed(\"text\",\"original_text\")\\\n    .withColumn(\"text\",f.regexp_replace(f.col(\"original_text\"),user_regex,\" \"))\\\n    .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),email_regex,\" \"))\\\n    .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),hastag_regex,\" \"))\\\n    .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),url_regex,\" \"))\\\n    .withColumn(\"text\",html_unescape(f.col(\"text\")))\\\n    .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),\"[^a-zA-z']\",\" \"))\\\n    .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),\" +\",\" \"))\\\n    .withColumn(\"text\",f.trim(f.col(\"text\")))\\\n    .withColumn(\"text\",f.regexp_replace(f.col(\"text\"),\"^o{2,}|$o{2,}\",\"o\"))\\\n    .filter(\"text != ''\")\n    return text","a19ba09e":"trains = Processing(trains)","8fa6613e":"from pyspark.ml.feature import (StopWordsRemover,Tokenizer,HashingTF,IDF)\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline","4f7de1d6":"%%time\n#train,dev = trains.randomSplit([0.3,0.7],seed = 202120)\ntokenizer = Tokenizer(inputCol = \"text\",outputCol = \"token1\")\nstopwords_remover = StopWordsRemover(inputCol = \"token1\",\n                                     outputCol = \"token2\",\n                                     stopWords = StopWordsRemover.loadDefaultStopWords(\"english\") )\nhashing_tf = HashingTF(inputCol = \"token2\",\n                       outputCol = \"term_frequency\")\nidf = IDF(inputCol = \"term_frequency\",\n          outputCol = \"features\",\n          minDocFreq = 10) \n\nlr = LogisticRegression(labelCol = \"polarity\",maxIter = 100, regParam = 0.044,elasticNetParam=0.0479)\npl = Pipeline(stages = [tokenizer,stopwords_remover,hashing_tf,idf,lr])\nmodel = pl.fit(trains)","8e8a165a":"lg_model = model.stages[4]\nlg_model.save(\"sample-model\")","d73ce1d9":"from pyspark.ml.functions import vector_to_array","59635550":"test_text = Processing(test)\ntest_pred = model.transform(test_text)\ntest_pred = test_pred.select(\"id\",\"probability\")\\\n.withColumn(\"probability\",vector_to_array(f.col(\"probability\")))\\\n.withColumn(\"probability\",f.element_at(f.col(\"probability\"),-1))","e1241f26":"final = test_pred.withColumn(\"polarity\",\n                     f.when(f.col(\"probability\") < 0.5,0)\\\n                     .when((f.col(\"probability\") >= 0.5) & (f.col(\"probability\") <= 0.61),2)\n                     .when(f.col(\"probability\") > 0.61,4)\\\n                     .otherwise(f.col(\"probability\")))\nstatic = final.groupby(\"polarity\").agg((f.count(f.col(\"polarity\"))\/test_pred.count()).alias(\"frequency\")).sort(\"polarity\")\nstatic.show(5,False)\nfinal = final.withColumn(\"polarity\",f.col(\"polarity\").cast(\"int\")).select(\"id\",\"polarity\")","fa0f7559":"final.toPandas().to_csv(\"max_acc.csv\",index = 0,header = True)","f88d2dec":"# Test Prediction","d9eabc50":"# Model","34cb65fe":"# Text Processing","61daaeab":"Team 11<br>\n18520832 - L\u00e2m Gia Huy<br>\n18520997 - Tr\u1ea7n Quang Linh<br>\n18520505 - D\u01b0\u01a1ng V\u0103n B\u00ecnh<br>"}}