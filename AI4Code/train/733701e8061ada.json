{"cell_type":{"87d4e081":"code","14b0336d":"code","f2f07c03":"code","8ddf32b8":"code","4a42ebb0":"code","486ababa":"code","3742bb52":"code","e0adfbc1":"code","83d9859b":"code","aea178ac":"code","b22eda32":"code","421d4536":"code","9fb0f5e8":"code","639619b2":"code","f67d34df":"code","cdeb1020":"code","20266918":"code","c7b84fdf":"code","306d0302":"code","6c9fbe0a":"code","77789841":"code","947b24ff":"code","3cd80dce":"code","014d6189":"code","f7610f08":"code","1a03731a":"code","f817959b":"code","0a23b59a":"code","61544023":"code","30a2efbe":"code","9c5722d9":"code","7b900373":"code","dd6b0cf6":"code","bdde326a":"code","42632eec":"code","878791f2":"code","45604120":"code","644d283f":"code","4a5dae18":"code","7031b288":"code","30781d48":"code","d7e70abe":"code","e9381a75":"code","7d9b19fb":"code","d485b57d":"code","2e5ac74f":"code","9e70fd92":"code","25f4bb79":"code","0eb2bff3":"code","753b8f4d":"code","9f72eb89":"markdown","782d8a3d":"markdown","c21339e0":"markdown","5bf934ac":"markdown","8ad5d69f":"markdown","21f3f492":"markdown","6be8993f":"markdown","fb649ae8":"markdown","a28319e9":"markdown","14b80481":"markdown","b143e6dd":"markdown","ddf1d6c0":"markdown","8ad39731":"markdown","a39540f0":"markdown","069d95e6":"markdown","5dcb0d88":"markdown","ef869932":"markdown","c0affe4f":"markdown","d9f7c05e":"markdown","176ff7d9":"markdown","6ff0a0c9":"markdown","f149eeec":"markdown","4b2dd89b":"markdown"},"source":{"87d4e081":"# Load packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom pathlib import Path\nfrom transformers import BertTokenizer, TFBertModel\nfrom urllib.request import urlretrieve\n\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.optimizers import Adam","14b0336d":"SNIPS_DATA_BASE_URL = (\n    \"https:\/\/github.com\/ogrisel\/slot_filling_and_intent_detection_of_SLU\/blob\/\"\n    \"master\/data\/snips\/\"\n)\nfor filename in [\"train\", \"valid\", \"test\", \"vocab.intent\", \"vocab.slot\"]:\n    path = Path(filename)\n    if not path.exists():\n        print(f\"Downloading {filename}...\")\n        urlretrieve(SNIPS_DATA_BASE_URL + filename + \"?raw=true\", path)","f2f07c03":"lines_train = Path('train').read_text('utf-8').strip().splitlines()","8ddf32b8":"print(f'First line of training set: {lines_train[0]}.')","4a42ebb0":"def parse_line(line):\n    utterance_data, intent_label = line.split(\" <=> \")\n    items = utterance_data.split()\n    words = [item.rsplit(':', 1)[0] for item in items]\n    word_labels = [item.rsplit(':', 1)[1] for item in items]\n    return {\n        'intent_label': intent_label,\n        'words': \" \".join(words),\n        'words_label': \" \".join(word_labels),\n        'length': len(words)\n    }","486ababa":"parse_line(lines_train[0])","3742bb52":"print(Path('vocab.intent').read_text('utf-8'))","e0adfbc1":"print(Path('vocab.slot').read_text('utf-8'))","83d9859b":"parsed = [parse_line(line) for line in lines_train]\ndf_train = pd.DataFrame([p for p in parsed if p is not None])","aea178ac":"# Print some lines of the training set\ndf_train.head(5)","b22eda32":"# Count the number of lines by intent label\ndf_train.intent_label.value_counts()","421d4536":"# Histogram of sentence lengths\ndf_train.hist('length', bins=30)","9fb0f5e8":"# Get validation and test set\nlines_validation = Path('valid').read_text('utf-8').strip().splitlines()\nlines_test = Path('test').read_text('utf-8').strip().splitlines()\n\ndf_validation = pd.DataFrame([parse_line(line) for line in lines_validation])\ndf_test = pd.DataFrame([parse_line(line) for line in lines_test])","639619b2":"model_name = 'bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","f67d34df":"first_sentence = df_train.iloc[0]['words']\nprint(first_sentence)","cdeb1020":"tokenizer.tokenize(first_sentence)","20266918":"# Encode sentence to id\ntokenizer.encode(first_sentence)","c7b84fdf":"# Do the inverse operation\ntokenizer.decode(tokenizer.encode(first_sentence))","306d0302":"train_sequence_lengths = [len(tokenizer.encode(text))\n                          for text in df_train['words']]\nplt.hist(train_sequence_lengths, bins=30)\nplt.title(f'Max sequence length: {max(train_sequence_lengths)}')\nplt.xlabel('Length')\nplt.ylabel('Count')\nplt.show()","6c9fbe0a":"print(f'Vocabulary size: {tokenizer.vocab_size} words.')","77789841":"# Get the items in BERT\nbert_vocab_items = list(tokenizer.vocab.items())","947b24ff":"# Print some examples of items\nbert_vocab_items[250:260]","3cd80dce":"def encode_dataset(tokenizer, text_sequences, max_length):\n    token_ids = np.zeros(shape=(len(text_sequences), max_length),\n                         dtype=np.int32)\n    for i, text_sequence in enumerate(text_sequences):\n        encoded = tokenizer.encode(text_sequence)\n        token_ids[i, 0:len(encoded)] = encoded\n    attention_masks = (token_ids != 0).astype(np.int32)\n    \n    return {'input_ids': token_ids, 'attention_masks': attention_masks}","014d6189":"encoded_train = encode_dataset(tokenizer, df_train['words'], 45)\nencoded_validation = encode_dataset(tokenizer, df_validation['words'], 45)\nencoded_test = encode_dataset(tokenizer, df_test['words'], 45)","f7610f08":"encoded_train['input_ids']","1a03731a":"encoded_train['attention_masks']","f817959b":"intent_names = Path('vocab.intent').read_text('utf-8').split()\nintent_map = dict((label, idx) for idx, label in enumerate(intent_names))","0a23b59a":"intent_map","61544023":"intent_train = df_train['intent_label'].map(intent_map).values\nintent_validation = df_validation['intent_label'].map(intent_map).values\nintent_test = df_test['intent_label'].map(intent_map).values","30a2efbe":"base_bert_model = TFBertModel.from_pretrained('bert-base-cased')\nbase_bert_model.summary()","9c5722d9":"outputs = base_bert_model(encoded_validation)","7b900373":"print(f'Shape of the first output of the BERT model: {outputs[0].shape}.')","dd6b0cf6":"print(f'Shape of the second output of the BERT model: {outputs[1].shape}.')","bdde326a":"# Define IntentClassification model\nclass IntentClassificationModel(tf.keras.Model):\n    def __init__(self, intent_num_labels=None,\n                 model_name='bert-base-cased',\n                 dropout_prob=0.1):\n        super().__init__(name='joint_intent_slot')\n        # Let's preload the pretrained model BERT in the constructor\n        # of our classifier model.\n        self.bert = TFBertModel.from_pretrained(model_name)\n        self.dropout = Dropout(dropout_prob)\n        \n        # Define a (Dense) classification layer to compute for each\n        # sequence in a batch of samples. The number of output classes\n        # is given by the intent_num_labels parameter.\n        # Use the default linear activation (no softmax) to compute\n        # logits. The softmax normalization will be computed in the\n        # loss function instead of the model itself.\n        self.intent_classifier = Dense(intent_num_labels)\n        \n    def call(self, inputs, **kwargs):\n        # Use the pretrained model to extract features from our\n        # encoded inputs.\n        sequence_output, pooled_output = self.bert(inputs, **kwargs)\n        \n        # The second output of the main BERT layer has shape:\n        # (batch_size, output_dim) and gives a \"pooled\" representation\n        # for the full sequence from the hidden state that corresponds\n        # to the \"[CLS]\" token.\n        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n        \n        # Use the classifier layer to compute the logits from the\n        # pooled features.\n        intent_logits = self.intent_classifier(pooled_output)\n        return intent_logits","42632eec":"# Build the model\nintent_model = IntentClassificationModel(intent_num_labels=len(intent_map))\n\nintent_model.compile(optimizer=Adam(learning_rate=3e-5, epsilon=1e-08),\n                     loss=SparseCategoricalCrossentropy(from_logits=True),\n                     metrics=[SparseCategoricalAccuracy('accuracy')])","878791f2":"# Train the model\nhistory = intent_model.fit(encoded_train, intent_train,\n                           epochs=2, batch_size=32,\n                           validation_data=(encoded_validation, intent_validation))","45604120":"def classify(text, tokenizerzer, model, intent_names):\n    inputs = tf.constant(tokenizer.encode(text))[None, :] # Batch size = 1\n    class_id = model(inputs).numpy().argmax(axis=1)[0]\n    return intent_names[class_id]","644d283f":"# Example of classification\nclassify('Will it snow tomorrow in Paris?',\n         tokenizer, intent_model, intent_names)","4a5dae18":"slot_names = [\"[PAD]\"]\nslot_names += Path('vocab.slot').read_text('utf-8').strip().splitlines()\n\nslot_map = {}\nfor label in slot_names:\n    slot_map[label] = len(slot_map)","7031b288":"def encode_token_labels(text_sequences, slot_names, tokenizer, slot_map, max_length):\n    encoded = np.zeros(shape=(len(text_sequences), max_length), dtype=np.int32)\n    for i, (text_sequence, word_labels) in enumerate(\n            zip(text_sequences, slot_names)):\n        encoded_labels = []\n        for word, word_label in zip(text_sequence.split(), word_labels.split()):\n            tokens = tokenizer.tokenize(word)\n            encoded_labels.append(slot_map[word_label])\n            expand_label = word_label.replace(\"B-\", \"I-\")\n            if not expand_label in slot_map:\n                expand_label = word_label\n            encoded_labels.extend([slot_map[expand_label]] * (len(tokens) - 1))\n        encoded[i, 1:len(encoded_labels) + 1] = encoded_labels\n    return encoded","30781d48":"slot_train = encode_token_labels(df_train['words'], df_train['words_label'], tokenizer, slot_map, 45)\nslot_validation = encode_token_labels(df_validation['words'], df_validation['words_label'], tokenizer, slot_map, 45)\nslot_test = encode_token_labels(df_test['words'], df_test['words_label'], tokenizer, slot_map, 45)","d7e70abe":"# Define JointIntentAndSlotFilling model\nclass JointIntentAndSlotFillingModel(tf.keras.Model):\n\n    def __init__(self, intent_num_labels=None, slot_num_labels=None,\n                 model_name=\"bert-base-cased\", dropout_prob=0.1):\n        super().__init__(name=\"joint_intent_slot\")\n        self.bert = TFBertModel.from_pretrained(model_name)\n        self.dropout = Dropout(dropout_prob)\n        self.intent_classifier = Dense(intent_num_labels,\n                                       name=\"intent_classifier\")\n        self.slot_classifier = Dense(slot_num_labels,\n                                     name=\"slot_classifier\")\n\n    def call(self, inputs, **kwargs):\n        sequence_output, pooled_output = self.bert(inputs, **kwargs)\n\n        # The first output of the main BERT layer has shape:\n        # (batch_size, max_length, output_dim)\n        sequence_output = self.dropout(sequence_output,\n                                       training=kwargs.get(\"training\", False))\n        slot_logits = self.slot_classifier(sequence_output)\n\n        # The second output of the main BERT layer has shape:\n        # (batch_size, output_dim)\n        # and gives a \"pooled\" representation for the full sequence from the\n        # hidden state that corresponds to the \"[CLS]\" token.\n        pooled_output = self.dropout(pooled_output,\n                                     training=kwargs.get(\"training\", False))\n        intent_logits = self.intent_classifier(pooled_output)\n\n        return slot_logits, intent_logits","e9381a75":"joint_model = JointIntentAndSlotFillingModel(\n    intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))\n\n# Define one classification loss for each output:\nopt = Adam(learning_rate=3e-5, epsilon=1e-08)\nlosses = [SparseCategoricalCrossentropy(from_logits=True),\n          SparseCategoricalCrossentropy(from_logits=True)]\nmetrics = [SparseCategoricalAccuracy('accuracy')]\njoint_model.compile(optimizer=opt, loss=losses, metrics=metrics)","7d9b19fb":"history = joint_model.fit(\n    encoded_train, (slot_train, intent_train),\n    validation_data=(encoded_validation, (slot_validation, intent_validation)),\n    epochs=2, batch_size=32)","d485b57d":"def show_predictions(text, tokenizer, model, intent_names, slot_names):\n    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n    outputs = model(inputs)\n    slot_logits, intent_logits = outputs\n    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n    print(\"## Intent:\", intent_names[intent_id])\n    print(\"## Slots:\")\n    for token, slot_id in zip(tokenizer.tokenize(text), slot_ids):\n        print(f\"{token:>10} : {slot_names[slot_id]}\")","2e5ac74f":"# Example of classification\nshow_predictions('Will it snow tomorrow in Paris?',\n                 tokenizer, joint_model, intent_names, slot_names)","9e70fd92":"def decode_predictions(text, tokenizer, intent_names, slot_names,\n                       intent_id, slot_ids):\n    info = {\"intent\": intent_names[intent_id]}\n    collected_slots = {}\n    active_slot_words = []\n    active_slot_name = None\n    for word in text.split():\n        tokens = tokenizer.tokenize(word)\n        current_word_slot_ids = slot_ids[:len(tokens)]\n        slot_ids = slot_ids[len(tokens):]\n        current_word_slot_name = slot_names[current_word_slot_ids[0]]\n        if current_word_slot_name == \"O\":\n            if active_slot_name:\n                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n                active_slot_words = []\n                active_slot_name = None\n        else:\n            # Naive BIO: handling: treat B- and I- the same...\n            new_slot_name = current_word_slot_name[2:]\n            if active_slot_name is None:\n                active_slot_words.append(word)\n                active_slot_name = new_slot_name\n            elif new_slot_name == active_slot_name:\n                active_slot_words.append(word)\n            else:\n                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n                active_slot_words = [word]\n                active_slot_name = new_slot_name\n    if active_slot_name:\n        collected_slots[active_slot_name] = \" \".join(active_slot_words)\n    info[\"slots\"] = collected_slots\n    return info","25f4bb79":"def nlu(text, tokenizer, model, intent_names, slot_names):\n    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n    outputs = model(inputs)\n    slot_logits, intent_logits = outputs\n    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n\n    return decode_predictions(text, tokenizer, intent_names, slot_names,\n                              intent_id, slot_ids)","0eb2bff3":"nlu('Will it snow tomorrow in Paris?',\n                 tokenizer, joint_model, intent_names, slot_names)","753b8f4d":"nlu('I would like to listen to Wake me up by Avicii.',\n    tokenizer, joint_model, intent_names, slot_names)","9f72eb89":"The following function generates token-aligned integer labels from the BIO word-level annotations. In particular, if a specific word is too long to be represented as a single token, we expand its label for all the tokens of that word while taking care of using \"B-\" labels only for the first token and then use \"I-\" for the matching slot type for subsequent tokens of the same word.","782d8a3d":"This utterance is a voice command of type \"AddToPlayist\" with annotations:\n* an entity-name: \"Don and Sherri\",\n* a playlist: \"Medidate to Sounds of Nature\".\n\nThe goal of this project is to build a baseline Natural Understanding model to analyse such voice commands and predict:\n* the intent of the speaker: the sentence level class label (\"AddToPlaylist\");\n* extract the interesting slots (typed named entities) from the sentence by performing word level classification using the B-I-O tags as target classes. This second task is often referred to as \"NER\" (Named Entity Recognition) in the NLP litterature. Alternatively, this is also known as \"slot filling\" when we expect a fixed set of named entity per sentence of a given class.\n\nThe list of possible classes for the sentence level and the word level classification problems are given as:","c21339e0":"## Join intent classification and slot filling\n\nLet's now refine our natural language understanding system by trying to retrieve the important structured elements of each voice command. To do so, we will perform word level (or token level) classification of the BIO labels. Since we have word level tags but BERT uses a wordpiece tokenizer, we need to align the BIO labels with the BERT tokens. Let's load the list of possible word token labels and augment it with an additional padding label to be able to ignore special tokens.","5bf934ac":"The second output of the BERT model is a tensor with shape `(batch_size, output_dim)` which is the vector representation of the special token `[CLS]`. This vector is typically used as a pooled representation for the sequence as a whole. This will be used as the features of our latent classifier.","8ad5d69f":"### Encoding the sequence classification targets\n\nTo do so, we build a simple mapping from the auxiliary files.","21f3f492":"Remarks:\n* The first token `[CLS]` is used by the pre-training task for sequence classification.\n* The last token `[SEP]` is a separator for the pre-training task that classifies if a pair of sentences are consecutive in a corpus or not (next sentence prediction).\n* Here, we want to use BERT to compute a representation of a single voice command at a time.\n* We could reuse the representation of the `[CLS]` token for sequence classification.\n* Alternatively, we can pool the representations of all the tokens of the voice command (*e.g.* global average) and use that as the input of the final sequence classification layer.","6be8993f":"To perform transfer learning, we will need to work with padded sequences. So, they all have the same sizes. The above histograms, shows that after tokenization, $43$ tokens are enough to represent all the voice commands in the training set.\n\nThe mapping can be introspected in the `tokenizer.vocab` attribute.","fb649ae8":"Notive that BERT uses subword tokens. So the length of the tokenized sentence is likely to be larger than the number of words in the sentence. It is particularly interesting to use subword tokenization sentence for general purpose language models such as BERT because it should be possible to generalize the model and then to fine-tuned it to be a specialized one.\n\nEach token string is mapped to a unique integer id that makes it fast to lookup the right column in the input layer token embedding.","a28319e9":"Note that the special tokens such as \"[PAD]\" and \"[SEP]\" and all padded positions receive a $0$ label.","14b80481":"## Decoding predictions into structured knowledge\n\nFor completeness, here a minimal functional to naively decode the predicted BIO slot ids and convert it into a structured representation for the detected slots as a Python dictionaries.","b143e6dd":"The classification model outputs logits instead of probabilities. The final softmax normalization layer is implicit, that is, included in the loss function instead of the model directly. We need to configure the loss function `SparseCategoricalCrossentropy(from_logits=True)` accordingly.","ddf1d6c0":"The first output of the BERT model is a tensor with shape: `(batch_size, seq_len, output_dim)` which computes features for each token in the input sequence.","8ad39731":"Let's have a look at the first lines from the training set.","a39540f0":"## Intent classification (sentence level)\n\nLet's ignore the slot filling task for now and let's try to build a sentence level classifier by fine-tuning a pre-trained Transformer-based model using the `huggingface\/transformers` package that provides both Tensorflow\/Keras and Pytorch APIs.\n\n### The BERT tokenizer\n\nFirst let's load a pre-trained tokenizer and test it on a test sentence from the training set.","069d95e6":"## The data\n\nWe will use a speed command dataset collected, annotated and published by French startup snips.ai (bought in 2019 by Audio device manufacturer Sonos). The original dataset comes in [YAML format with inline markdown annotations](https:\/\/snips-nlu.readthedocs.io\/en\/latest\/dataset.html). Instead, we will use a preprocessed variant with token level B-I-O annotations closer to the representation our model will predict. This variant of the snips dataset was prepared by [Su Zhu](https:\/\/github.com\/sz128).","5dcb0d88":"## Limitations\n\n### Language\n\nBERT is pretrained primarily on English content. It can therefore only extract meaningful features on text written in English.\n\nNote that there exists alternative pretrained model that use a mix of different languages (e.g. [XLM](https:\/\/github.com\/facebookresearch\/XLM\/) and others that have been trained on other languages. For instance [CamemBERT](https:\/\/camembert-model.fr\/) is pretrained on French text. Both kinds of models are available in the transformers package:\n\nhttps:\/\/github.com\/huggingface\/transformers#model-architectures\n\nThe public snips.ai dataset used for fine-tuning is English only. To build a model for another language we would need to collect and annotate a similar corpus with tens of thousands of diverse, representative samples.\n\n### Biases embedded in the pre-trained model\n\nThe original data used to pre-trained BERT was collected from the Internet and contains all kinds of data, including offensive and hateful speech.\n\nWhile using BERT for or voice command understanding system is quite unlikely to be significantly impacted by those biases, it could be a serious problem for other kinds of applications such as Machine Translation for instance.\n\nIt is therefore strongly recommended to spend time auditing the biases that are embedded in such pre-trained models before deciding to deploy system that derive from them.\n\n### Computational ressources\n\nThe original BERT model has many parameters which uses a lot of memory and can be prohibitive to deploy on small devices such as mobile phones. It is also very computationally intensive and typically requires powerful GPUs or TPUs to process text data at a reasonable speed (both for training and at inference time).\n\nDesigning alternative architectures with fewer parameters or more efficient training and inference procedures is still a very active area of research.\n\nDepending of on the problems, it might be the case that simpler architectures based on convolutional neural networks and LSTMs might offer a better speed \/ accuracy trade-off.","ef869932":"### Loading and feeding a pretrained BERT model\n\nLet's load a pretrained BERT model using the [huggingface transformers](https:\/\/github.com\/huggingface\/transformers) package.","c0affe4f":"\"POI\" stands for \"Point of Interest\". Let's parse all the lines and store the results in a Pandas dataframes.","d9f7c05e":"The following function uses our trained model to make prediction on a single text sequence and display both the sequence-wise and the token-wise class labels.","176ff7d9":"Let's build an train a sequece classification model using to predict the intent class. We will use the `self.bert` pretrained model in the `call` method and only consider the pooled features (ignore the token-wise features for now).","6ff0a0c9":"Some remarks:\n* The class label for the voice command appears at the end of each line (after the \"<=>\" marker).\n* Each word-level token is annotated with B-I-O labels using the \".\" separator.\n* B-I-O stands for Beginning-Inside-Outside\n* \"Add:O\" means that the token \"Add\" is \"Outside\" of any annotation span.\n* \"Don:B-entity_name\" means that \"Don\" is the \"Beginning\" of an annotation of type \"entity_name\".\n* \"and:I-entity_name\" means that \"and\" is \"inside\" the previously started annotation of type \"entity_name\".\n\nLet's write a parsing function and test it on the first line.","f149eeec":"# Joint Intent Classification and Slot Filliing with Transformers\n\nThis notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found [here](https:\/\/github.com\/m2dsupsdlclass\/lectures-labs).\n\n**Goal**\n* Fine-tune a pretrained transformer-based neural network model to convert a user qeury expressed in English into a representation that is structured enough to be processed by an automated service.\n\nHere is an example of interpretation computed by such a Natural Language Understanding system:\n    \n    >>> nlu('Book a table for two at Le Ritz for Friday night\",\n            tokenizer, joint_model, intent_names, slot_names)\n    {\n        'intent': 'BookRestaurant',\n        'slots': {\n            'party_size_number': 'two',\n            'restaurant_name': 'Le Ritz',\n            'timeRange': 'Friday night'\n        }\n    }\n    \nIntent classification is a simple classification problem. The trick is to treat the structured knowledge extraction part (\"Slot Filling\") as a token-level classification problem using BIO-annotations:\n\n    >>> show_predictions('Book a table for two at Le Ritz for Friday night',\n                         tokenizer, joint_model, intent_names, slot_names)\n    ## Intent: BookRestaurant\n    ## Slots:\n      Book : O\n         a : O\n     table : O\n       for : O\n       two : B-party_size_number\n        at : O\n        Le : B-restaurant_name\n         R : I-restaurant_name\n     ##itz : I-restaurant_name\n       for : O\n    Friday : B-timeRange\n     night : I-timeRange\n     \nWe will show hhow to train a such \"sequence classification\" and \"token classification\" joint model on a voice command dataset published by snips.ai. This notebook is a partial reproduction of some of the results presented in this paper: BERT for Joint Intent Classification and Shot Filling, Qian Chen, Zhu Zhuo, Wen Wang [link](https:\/\/arxiv.org\/abs\/1902.10909).","4b2dd89b":"Remarks:\n* $30000$ is a reasonable vocabulary size and is small enough to be used in a softmax output layer.\n* It can represent multi-lingual sentences, including non-western alphabets.\n* Subword tokenization makes it possible to deal with typos and morphological variations with a small vocabulary size and without any language-specific preprocessing.\n* Subword tokenization makes it unlikely to use the `[UNK]` special token as rare words can often be represented as a sequence of frequent enough short subwords in a meaningful way.\n\n### Encoding the dataset with the tokenizer\n\nLet's now encode the full train\/validation and test sets with our tokenizer to get a padded integer numpy arrays."}}