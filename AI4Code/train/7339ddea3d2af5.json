{"cell_type":{"321ca39a":"code","b21a299c":"code","51cd8e3f":"code","365d3c96":"code","f0a0ab94":"code","58943bea":"code","324b1a85":"code","3eefb0b3":"code","f0c5074f":"code","062b7bc3":"code","39435ce2":"code","c60b9b2b":"code","f02970cb":"code","dd4f1a20":"code","ca72fcb1":"code","3c8d51b8":"code","3bb3c26a":"code","209ea07c":"code","bf1acc3d":"code","54285e5e":"code","b87cecf4":"code","d5cf3f96":"code","999af242":"code","183b09c4":"code","49b5c15a":"code","98a766ef":"code","01d57063":"code","34db1c49":"code","85162c70":"code","f9098d44":"code","8dc6be5f":"code","82d42426":"code","6c1e607b":"code","c6f65b34":"code","821a3490":"code","3fdef3fd":"code","ba3860b0":"code","f7442dc9":"code","daeace5a":"code","b2c519dc":"code","8cbd3005":"code","88663147":"code","2a582ecf":"code","abba52dc":"code","a860e21d":"code","452913e8":"markdown","901cef6e":"markdown","bde4b362":"markdown","36c8f328":"markdown","eec6b767":"markdown","b179ed44":"markdown","61d3b57b":"markdown","fc7e6c56":"markdown","84fe3f10":"markdown","7ab3a7e0":"markdown","e7abb7ca":"markdown","a5c88b5d":"markdown"},"source":{"321ca39a":"# Load Libaries Needed\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","b21a299c":"# Read the data\ndata_path='\/kaggle\/input\/videogamesales\/vgsales.csv'\ndf=pd.read_csv(data_path)\n# Print the first 5 rows\ndf.head()","51cd8e3f":"# Look at all the variables in the columns\ndf.columns","365d3c96":"#Explore the data type of each column\ndf.info()","f0a0ab94":"# Get a snapshot of the data's central tendencies \ndf.describe()","58943bea":"# Let's see how the Global Sales are distritubed for all the data \nf,ax=plt.subplots(figsize=(10,5))\nsns.distplot(df['Global_Sales'])\nplt.title(\"Distribution of Global Sales for Video Games\")\n","324b1a85":"# Calculate Skewness and Kurtosis \nprint (\"Skewness: \" + str(df['Global_Sales'].skew()))\nprint (\"Kurtosis: \"+str(df['Global_Sales'].kurtosis()))","3eefb0b3":"#Numerical Variables \nnum= ['Year','NA_Sales','EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']\ndf_num=df[num]\nfor var in df_num:\n    if var!='Global_Sales':\n        f,ax=plt.subplots(figsize=(5,5))\n        sns.distplot(df_num[var])\n","f0c5074f":"#Calculate Skewness and Kurtosis for each numerical variable\nfor var in df_num: \n    if var != \"Global_Sales\":\n        print ('Skewness for '+ var + ' is: '+ str(df[var].skew()))\n        print ('Kurtosis for '+ var + ' is: '+ str(df[var].kurtosis()))","062b7bc3":"#Calculate Correlation between numeric variables\nsns.heatmap(df_num.corr(),annot=True,cmap=\"YlGnBu\")","39435ce2":"#Categorical Variables\ncat=['Platform','Genre', 'Publisher']\ndf_cat=df[cat]\nfor var in df_cat:\n    f,ax=plt.subplots(figsize=(16,8))\n    sns.barplot(df_cat[var].value_counts().index,df_cat[var].value_counts()).set_title('Distrubution of ' + var)\n    plt.xlabel(var)\n    plt.ylabel('Frequency')\n    plt.show()","c60b9b2b":"# Sales across different platforms:\nf,ax=plt.subplots(figsize=(18,6))\nsns.boxplot(df['Platform'],df['Global_Sales'])\nplt.show()\n\n#Calculate average sale price for each platform:\ndf2=df.groupby(['Platform'])['Platform','Global_Sales'].mean().sort_values(by='Global_Sales', ascending=False).reset_index()\ndf2","f02970cb":"# Sales over the years \ndf3 =df.groupby(['Year'])['Global_Sales'].mean().sort_values(ascending=False).reset_index()\nprint(df3)\n\n# Scatter Plot\nf,ax=plt.subplots(figsize=(18,6))\nplt.scatter(df['Year'],df['Global_Sales'])\n","dd4f1a20":"# Best Selling Genres:\ndf4 =df.groupby(['Genre'])['Genre','Global_Sales'].mean().sort_values(by='Global_Sales', ascending=False).reset_index()\nprint(df4)\n\n# Boxplot comparison between genres\nf,ax=plt.subplots(figsize=(18,6))\nsns.boxplot(df['Genre'],df['Global_Sales'])\nplt.ylim([0,10])","ca72fcb1":"# Most Successful Publishers:\ndf5 =df.groupby(['Publisher'])['Publisher','Global_Sales'].mean().sort_values(by='Global_Sales', ascending=False).reset_index()\nprint(df5.head(10))\n\n# Boxplot comparing Publishers:\nf,ax=plt.subplots(figsize=(18,6))\nsns.boxplot(df['Publisher'],df['Global_Sales'])","3c8d51b8":"# Comparing Sales of Major Game Series\nseries=['Fifa','Mario','Call of Duty','Grand Theft Auto', 'Pokemon','Halo','Wii','NBA']\n\nfor ser in series:\n    M = df[df['Name'].str.contains(ser, regex=False, case=False, na=False)].copy()\n    f,ax=plt.subplots(figsize=(16,5))\n    plt.scatter(M['Year'],M['Global_Sales'])\n    plt.title(ser+' Sales Over Time')\n    plt.xlabel('Year')\n    plt.ylabel('Global Sales')\n    plt.show()\n\n","3bb3c26a":"# Compare average sales for each major game series\nseries=['Fifa','Mario','Call of Duty','Grand Theft Auto', 'Pokemon','Halo','Wii','NBA']\nfor ser in series:\n    game_series={}\n    M = df[df['Name'].str.contains(ser, regex=False, case=False, na=False)].copy()\n    average= round(M['Global_Sales'].median(),2)\n    correlation=M['Global_Sales'].corr(M['Year'])\n    rounded_corr=round(correlation,2)\n    print('The Median For {} Series is {}, its Correlation with Years is {}'.format(ser,average,rounded_corr))","209ea07c":"# Compare Type of Device Game Sales \nprint(df['Platform'].unique())\n# 3 types: Portable, PC and Console\nPC=['PC']\nPortable = ['GB','DS','GBA','3DS','PSP','PSV','WS','GG']\ndf_portable=df[(df.Platform =='GB')|(df.Platform =='DS')|(df.Platform =='GBA')|(df.Platform =='3DS')|(df.Platform =='PSP')|(df.Platform =='PSV')\n              |(df.Platform =='WS')|(df.Platform =='GG')]\ndf_PC=df[(df.Platform=='PC')]\n\ndf_console= df[(df.Platform =='Wii')|(df.Platform =='NES')|(df.Platform =='X360')|(df.Platform =='PS3')|(df.Platform =='PS2')|(df.Platform =='SNES')\n              |(df.Platform =='PS4')|(df.Platform =='N64')|(df.Platform =='PS')|(df.Platform =='XB')|(df.Platform =='2600')|(df.Platform =='XOne')\n             |(df.Platform =='GC')|(df.Platform =='WiiU')|(df.Platform =='GEN')|(df.Platform =='DC')\n            |(df.Platform =='SAT')|(df.Platform =='SCD')|(df.Platform =='NG')|(df.Platform =='TG16')|(df.Platform =='3DO')\n            |(df.Platform =='PCFX')]\nd={'Portable':df_portable,'PC': df_PC,'Console':df_console}\nfor name,n in d.items():\n    f,ax=plt.subplots(figsize=(10,6))\n    plt.scatter(n['Year'],n['Global_Sales'])\n    plt.title(name + ' Device Sales Over Time')\n    plt.xlabel('Year')\n    plt.ylabel('Global Sales')\n    plt.show()\n    average= n['Global_Sales'].median()\n    print (\"The Median Sales for {} devices is {}\".format(name,average))\n\n\n","bf1acc3d":"len(df['Platform'])","54285e5e":"#Console, Portable or PC\nDevice=[]\nfor i in range(0,16598):\n    if df['Platform'][i] in Portable:\n        Device.append('Portable')\n    elif df['Platform'][i] in PC:\n        Device.append('PC')\n    else:\n        Device.append('Console')\ndf['Device']=Device\ndf.head()\n        ","b87cecf4":"# Adding Feature of Game Series\nseries=['FIFA','Mario','Call of Duty','Grand Theft Auto', 'Pokemon','Halo','Wii','NBA']\nfor ser in series:\n    df[ser]=df['Name'].str.contains(ser, regex=False, case=False, na=False)\ndf.head(10)","d5cf3f96":"# Percentage of Total Sales Per Region\nregions_sales=['NA_Sales','EU_Sales','JP_Sales','Other_Sales']\nfor reg in regions_sales:\n    A=df[reg]\n    B=df['Global_Sales']\n    df[reg+' As a percentage of Total']=round(100*(A\/B),2)\ndf","999af242":"# Splitting Data into input and output variables\nfeatures= ['Year', 'Genre', 'Publisher','Device',\n       'FIFA', 'Mario', 'Call of Duty', 'Grand Theft Auto', 'Pokemon', 'Halo',\n       'Wii', 'NBA', 'NA_Sales As a percentage of Total',\n       'EU_Sales As a percentage of Total',\n       'JP_Sales As a percentage of Total',\n       'Other_Sales As a percentage of Total']\nX=df[features]\nX\ny=df['Global_Sales']","183b09c4":"# Checking for Missing values\nprint(X.isnull().sum())","49b5c15a":"# Investigating which rows have missing values\ndf[df.isnull().any(axis=1)]\n# For those with Publishers as NaN we will fill them with unknown\nX.Publisher.fillna('Unknown',inplace=True)\nX.Publisher.isnull().sum()","98a766ef":"# Dealing with missing year value:\ndf[df.Year.isnull()]\n# From look at this dataset there does not seem to be a pattern for missing years. We will therefore fill it by its median since \n# this will be a better measure of center than mean due to the left skewed distrbution of Years in this dataset\nf=df.Year.median()\nX.Year.fillna(f,inplace=True)\n# We should indicate in which records we have imputed years column with the median\nX['Year was Missing']=df.Year.isnull()\nX[X['Year was Missing']==True]\n# Confirm all missing values are filled\nX.isnull().sum()","01d57063":"# Now let's move on to the categorical variables\n# For Series Type we have already encoded it using OneHotEncoding. The only other two variables left are Genre, Publisher and Device Type\n# Check the cardinality for each of these variables\nfor col in ['Genre','Publisher','Device']:\n    print('The cardinality of {} is {}'.format(col,X[col].nunique()))","34db1c49":"# For Device Type we can use One-Hot Encoding \nfrom sklearn.preprocessing import OneHotEncoder\nOH_Encoder=OneHotEncoder(sparse=False,handle_unknown='ignore')\nlow_card_col=['Device']\nOH_cols_X=pd.DataFrame(OH_Encoder.fit_transform(X[low_card_col]))\nOH_cols_X.index=X.index\nother=X.drop(low_card_col,axis=1)\nX_processed=pd.concat([other,OH_cols_X], axis=1)","85162c70":"# For the high cardinality columns we shall use label encoding to minimise the data transformations needed to take place\nhigh_cardinality =['Genre','Publisher']\nlabel_X=X_processed.copy()\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder= LabelEncoder()\nfor col in high_cardinality:\n    label_X[col]=label_encoder.fit_transform(X[col])","f9098d44":"# Winsorization\nfrom scipy.stats.mstats import winsorize\ny_w=winsorize(y,limits=0.0001)\ny_w","8dc6be5f":"# Linear Regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nlin=LinearRegression()\ncv=-1*cross_val_score(lin,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint(cv)\nprint(cv.mean())","82d42426":"# Decision Tree Regressor\nfrom sklearn import tree\ndt=tree.DecisionTreeRegressor(max_leaf_nodes=950,random_state=1)\ncv=-1*cross_val_score(dt,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint(cv)\nprint(cv.mean())","6c1e607b":"# Random Forest Regressor\nfrom sklearn import ensemble\nrf=ensemble.RandomForestRegressor(n_estimators=100,random_state=1)\ncv=-1*cross_val_score(rf,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint(cv)\nprint(cv.mean())","c6f65b34":"# Bagging  Regressor\nbag=ensemble.BaggingRegressor(random_state=1)\ncv=-1*cross_val_score(bag,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint(cv)\nprint(cv.mean())","821a3490":"# Extra Trees Regressor\netr=ensemble.ExtraTreesRegressor(random_state=1,n_estimators=100)\ncv=-1*cross_val_score(etr,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint(cv)\nprint(cv.mean())","3fdef3fd":"# Gradient Boosting Regressor\ngb=ensemble.GradientBoostingRegressor(random_state=1)\ncv=-1*cross_val_score(gb,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint(cv)\nprint(cv.mean())","ba3860b0":"#Now let's try combining all the models with a average MAE of lower than 0.57 in a Voting Regressor:\nfrom sklearn.ensemble import VotingRegressor\nvoting_reg=VotingRegressor(estimators=[('dt',dt),('rf',rf),('gb',gb),('bag',bag),('etr',etr)])\ncv=-1*cross_val_score(voting_reg,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint(cv)\nprint(cv.mean())","f7442dc9":"# Model Tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndef model_performance(model,name):\n    print(name)\n    print('Best Score: {}'.format(model.best_score_))\n    print('Best Parameters: {}'.format(model.best_params_))","daeace5a":"# Random Forest Random Search\nrf\nparam={\n    'n_estimators':[100,500,1000]}\nreg_rf_rs=GridSearchCV(rf, param_grid=param, cv = 5, verbose = True, n_jobs = -1)\nbest_reg_rf_rnd = reg_rf_rs.fit(label_X,y_w)\nmodel_performance(best_reg_rf_rnd,'Random Forest')","b2c519dc":"# Most Important Features for Random Forest\nbest_rf = best_reg_rf_rnd.best_estimator_.fit(label_X,y_w)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=label_X.columns)\nfeat_importances.plot(kind='barh')","8cbd3005":"# Best RF Model\nbest_rf=best_reg_rf_rnd.best_estimator_\n\n#Developing Voting Model\nvoting_reg_1 = VotingRegressor(estimators=[('dt',dt),('rf',best_rf),('gb',gb),('bag',bag),('etr',etr)])\ncv=-1*cross_val_score(voting_reg_1,label_X,y_w,cv=5,scoring='neg_mean_absolute_error')\nprint('Cross Val Score: {}'.format(cv))\nprint('Average Cross Val Score: {}'.format(cv.mean()))","88663147":"# Try and improve weights for the best voting model:\nparams = {'weights' : [[1,1,1,1,1],[1,2,1,1,1],[1,1,2,1,1],[1,2,2,1,1]]}\n\nvote_weight = GridSearchCV(voting_reg_1, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\nbest_reg_weight = vote_weight.fit(label_X,y_w)\nmodel_performance(best_reg_weight,'VR Weights')\n\nbest_vote=best_reg_weight.best_estimator_","2a582ecf":"# Split Data into Train and Test\nfrom sklearn.model_selection import train_test_split\ntrain_X,test_X,train_y,test_y=train_test_split(label_X,y_w,random_state=1,test_size=1000)\n\n# Voting Regressor\n# Train Model and Make predictions\nbest_vote.fit(train_X,train_y)\npred=best_vote.predict(test_X)\n\n# Compute MAE and R2\nfrom sklearn import metrics\nmae=metrics.mean_absolute_error(test_y,pred)\nr2=metrics.r2_score(test_y,pred)\nprint('The Mean Absolute Error for our final model is {}'.format(mae))\nprint('The R-squared score for our final model is {}'.format(r2))","abba52dc":"feature_name=(test_X.columns.tolist())\nfeature_name[-3]='Console'\nfeature_name[-2]='PC'\nfeature_name[-1]='Portable'","a860e21d":"# Explore feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(best_vote,random_state=1).fit(test_X, test_y)\neli5.show_weights(perm,feature_names = feature_name)","452913e8":"# Final Model Building\n\nWe will now build our final model using the tuned **voting regressor model** and test it on 1000 rows of the original dataset as our final test data in order to get key model metrics such as MAE and R2.\n\n## Summary of Final Model Buliding Results\n* MAE:\n* R2: \n\n","901cef6e":"# **Exploring the breakdown of all the numerical and categorical variables**\n\n\n## Numerical Variables:\n\n* We will create distribution plots to see how each of these are spread out in our dataset.\n* We will analyse the correlation between these variables \n\n## Categorical Variables:\n* We will create bar charts to see the distrbution of the data broken down by different variables ","bde4b362":"# Data Pre-processing for Model \n\nNow that we have engineered 3 features that better represent our variables better. Let's start pre-processing the data so that we can apply our model to it:\n\n**1. Splitting data into input and output variables**:\n\nWe will ignore the Video Game's Name and Platform from our dataset as we have feature engineered these variables into more useful ones.\n\nWe will also ignore the gross number of region sales and instead look at each as a percentage of the total. This is because the gross region sales are highly correlated with the global sales since they actually make it up. Therefore, if left this in our model since it would be a data leakage for our target variable. Despite this, it would be interesting to see if the distrbution of sales by region impacts the actual Global Sales number. Thus, we will look at this through expressing it as a percentage of global sales.   \n\n\n**2. Dealing with missing values and cateogrical variables**\n\n**3. Winsorization for outliers**\n\nIn our data set we can see that there is a significant outliers for global sales. We can see this in the distrbution plot of global sales where the mean is significantly lower than the maximum. The next closest global sales value is more than half of the maximum. In order to combat the impact that outliers will have on our model we will use winsorization to reduce its effect.    \n\n","36c8f328":"We can see in the table above that the top 3 features\/predictors of a video game's global sales is:\n1. Percentage of Total Sales in Japan\n2. Percentage of Total Sales in North America\n3. Mario Games\n\nThis provides really interesting insights into where video games should be targeted if a company wants to maxmise its sales. \nI will discussed these insights and my interpretations further in [my blog post](https:\/\/mirpurishail.wixsite.com\/portfolio\/data-blogs). \n","eec6b767":"# Let's Finally Start Model Building!\n\nUsing cross validation scores we will start to evaluate different models performance on our dataset\n\n## Baseline Model Results\n\n1. Linear Regression: 0.670\n2. Decision Tree Regressor: 0.569\n3. Random Forest Regressor: 0.570\n4. Bagging Regressor: 0.572\n5. Extra Trees Regressor: 0.569\n6. Gradient Boosting: 0.566\n7. **Voting Regressor #1: 0.561**  - where we will combine the best baseline models and make them vote on the final predicted value. \n\n","b179ed44":"# Exploratory Data Analysis\n\nWe shall explore some key themes in this dataset:\n\n1. Sales across different platforms \n2. Sales over the years\n3. Genres that are the best selling\n4. Most successful publishers\n5. Comparing the sales data for games in the total series (FIFA, Call of Duty, GTA, etc.)\n6. Console vs. Portable Device Video Game Sales","61d3b57b":"We can see from the plot above that the Global Sales for video games is significantly right skewed with a heavy tail. This may indicate that most video games are quite relatively unsuccessful, however there are few that really break out and have a ton of success. \n\nIn addition to this, the large value for Kurtosis indicates the presence of outliers. This is something we will need to consider later when making our model.","fc7e6c56":"Some of the categorical variables are quite messy. Therefore, we may want to do some feature engineering to get a better representation of this in our model.","84fe3f10":"# References\n\n[The Dataset](https:\/\/www.kaggle.com\/gregorut\/videogamesales)\n\n[Winsorization](https:\/\/www.youtube.com\/watch?v=WLFQxoXn_uw) \n\n[Model Building and Tuning](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example)\n\n[Permutation Importance](https:\/\/www.kaggle.com\/nitindatta\/fifa-in-depth-analysis-with-linear-regression)\n\n[SK Learn Documentation](https:\/\/scikit-learn.org\/stable\/index.html)","7ab3a7e0":"# Predicting Video Game Sales \n\n\n## Overview\n1. Understanding the shape and distribution of the data \n2. Exploratory Data Analysis\n3. Feature Engineering\n4. Data Pre-processing\n5. Model Building and Tuning\n6. Final Model","e7abb7ca":"# **Feature Engineering**\n\n1. Whether it is released on a Console, Portable device or PC\n2. The game series it is apart of (We consider the ones we performed EDA on)\n3. Percentage of Total Sales Per Region\n","a5c88b5d":"# Model Tuning\n\nWe will tune the following model to see if we want improve its performance:\n* Random Forest"}}