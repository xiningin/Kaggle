{"cell_type":{"35b481ea":"code","5bd6d533":"code","4f3095ee":"code","22be7a6f":"code","5b8b4a5c":"code","612e3ef1":"code","bf3cfaf6":"code","91b3a5fc":"code","a0f0ca8b":"code","d6ac5135":"code","78c35e74":"code","ab74765f":"code","7da51e17":"code","939cffe0":"code","898e7479":"code","9f69ab54":"code","338bbc89":"code","e6bfcae1":"code","3a197039":"code","c58de129":"code","f815660b":"code","e50c7a01":"code","b3f6eac9":"code","75f5957c":"code","6489e3c2":"markdown","4f70b84a":"markdown","b7a0ff3d":"markdown","5f8cb565":"markdown","a0f63007":"markdown","b449ba19":"markdown","8f67b756":"markdown","c1a98ef9":"markdown","e4627362":"markdown","9775d324":"markdown","0e9e58c7":"markdown","4875ccc8":"markdown","e3ee007e":"markdown","881ac2df":"markdown","0739af43":"markdown","9c995d7c":"markdown","c3e4d9b2":"markdown","d8f16a7c":"markdown","3d76e55e":"markdown","c7120236":"markdown","bf9a0a3c":"markdown","2c8ab480":"markdown","29c1174e":"markdown","cbfe1ed1":"markdown"},"source":{"35b481ea":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom joblib import Parallel, delayed\nimport pyarrow.parquet as pq\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.decomposition import PCA","5bd6d533":"N_THREADS = 4\nFEATURES_DIM = 1000\nCLUSTER_DIM = 100\nN_SAMPLES = 3\nRANDOM_SEED = 2019\n\nnp.random.seed(RANDOM_SEED)","4f3095ee":"def load_signal(signal_id):\n    if signal_id <= 8711:\n        signal = pd.read_parquet('..\/input\/train.parquet', columns=[str(signal_id)])\n    else:   \n        signal = pd.read_parquet('..\/input\/test.parquet', columns=[str(signal_id)])\n    return np.squeeze(signal.values)\n\ndef plot_grid(id_measurements=None, signal_id=None, frame_size=(5,3), preprocessing=None):\n    meta_df = pd.read_csv('..\/input\/metadata_train.csv')\n    meta_df2 = pd.read_csv('..\/input\/metadata_test.csv')\n    meta_df = pd.concat((meta_df, meta_df2), axis=0, ignore_index=True, sort=False)\n    if signal_id is not None:\n        id_measurements = meta_df.loc[meta_df['signal_id'].isin(signal_id), 'id_measurement'].unique()\n    n_imgs = len(id_measurements)\n    fig = plt.figure(figsize=(frame_size[0]*3, frame_size[1]*n_imgs))\n    img_cursor = 1\n    for i in id_measurements:\n        signal_ids = meta_df.loc[meta_df['id_measurement']==i, 'signal_id'].values\n        targets = meta_df.loc[meta_df['id_measurement']==i, 'target'].values\n        for s,t in zip(signal_ids, targets):\n            color = '#007fff'\n            if signal_id is not None and s not in signal_id:\n                color = '#c0c0c0'\n            ax = fig.add_subplot(n_imgs, 3, img_cursor)\n            signal = load_signal(s)\n            if preprocessing is not None: \n                ax.plot(signal, color)\n                ax.plot(preprocessing(signal))\n            else:\n                ax.plot(signal, color) \n            ax.ticklabel_format(style='sci',scilimits=(-3,4),axis='x')\n            ax.grid(True)\n            ax.set_title('signal_id = {} , target = {}'.format(str(s), t))\n            img_cursor += 1\n    plt.tight_layout()\n    \ndef cluster_features(signals, dataset='train'):\n    if dataset == 'train':\n        data = pq.read_pandas('..\/input\/train.parquet', columns=[str(s) for s in signals]).to_pandas().values\n    else:\n        data = pq.read_pandas('..\/input\/test.parquet', columns=[str(s) for s in signals]).to_pandas().values\n    features = np.zeros((data.shape[1], FEATURES_DIM))\n    for i, signal in enumerate(data.T):\n        fft = np.fft.rfft(signal)\n        fft = np.abs(fft)\n        fft = np.array_split(fft, FEATURES_DIM)\n        features[i] = [np.median(d) for d in fft]\n    return features","22be7a6f":"train_meta = pd.read_csv('..\/input\/metadata_train.csv')\ntrain_meta = train_meta.loc[train_meta['phase'] == 0]\ntrain_sig_ids = train_meta['signal_id'].values\ntrain_feat = Parallel(n_jobs=N_THREADS, verbose=2)(delayed(cluster_features)(s, 'train') for s in np.array_split(train_sig_ids, 3*N_THREADS))\ntrain_feat = np.concatenate(train_feat, axis=0)","5b8b4a5c":"test_meta = pd.read_csv('..\/input\/metadata_test.csv')\ntest_meta = test_meta.loc[test_meta['phase'] == 0]\ntest_sig_ids = np.asarray([str(s) for s in test_meta['signal_id'].values])\ntest_feat = Parallel(n_jobs=N_THREADS, verbose=2)(delayed(cluster_features)(s, 'test') for s in np.array_split(test_sig_ids, 3*N_THREADS))\ntest_feat = np.concatenate(test_feat, axis=0)","612e3ef1":"features_idx = np.random.choice(np.arange(CLUSTER_DIM), 10)\nnorm_feat = MinMaxScaler().fit_transform(train_feat)\npca = PCA(n_components=CLUSTER_DIM).fit(norm_feat)\nnorm_feat = pca.transform(norm_feat)\nprint('PCA Explained Variance: {:.2f}%'.format(pca.explained_variance_ratio_.sum()*100))\nfig = plt.figure(figsize=(12,6))\nax = fig.add_subplot('111')\nax.set_title ('Random Subset of Features')\n_ = sns.boxplot(data=norm_feat[:, features_idx], ax=ax)","bf3cfaf6":"n_clusters = np.arange(2, 10)\n\ntrain_norm_feat = MinMaxScaler().fit_transform(train_feat)\npca = PCA(n_components=CLUSTER_DIM).fit(train_norm_feat)\ntrain_norm_feat = pca.transform(train_norm_feat)\nprint('PCA Explained Variance: {:.2f}%'.format(pca.explained_variance_ratio_.sum()*100))\ndef get_silhouette(n):\n    clt = KMeans(n_clusters=n, random_state=RANDOM_SEED).fit(train_norm_feat)\n    #clt = DBSCAN(0.5*n).fit(train_norm_feat)\n    try:\n        result = silhouette_score(train_norm_feat, clt.labels_)\n    except:\n        result = -1\n    return result\nsilhouette = Parallel(n_jobs=N_THREADS)(delayed(get_silhouette)(n) for n in n_clusters)\nplt.plot(np.arange(2, len(silhouette)+2), silhouette)\nplt.xlabel('N_Clusters')\nplt.ylabel('Silhoutte Score')\nplt.grid(True)\n_ = plt.title('Clustering Score - Training Set')","91b3a5fc":"best_n_clusters = 5\nclt = KMeans(n_clusters=best_n_clusters, random_state=RANDOM_SEED).fit(train_norm_feat)\ntrain_clusters = pd.DataFrame(clt.labels_, columns=['cluster'], index=pd.Index([int(x) for x in train_sig_ids], name='signal_id'))\ntrain_clusters['target'] = train_meta.loc[train_meta['phase']==0, 'target'].values\nstats_df = train_clusters.groupby('cluster')['target'].agg(['count','sum'])\nstats_df.columns = ['count', 'pos_count']\nstats_df['pos_rate'] = stats_df['pos_count']\/stats_df['count']\nstats_df['pos_rate'] = stats_df['pos_rate'].apply(lambda x: '{:.1f} %'.format(x*100))\ndisplay(stats_df)","a0f0ca8b":"plot_ids = train_clusters.loc[train_clusters['cluster']==0].sample(N_SAMPLES).index\nplot_grid(signal_id=plot_ids)","d6ac5135":"plot_ids = train_clusters.loc[train_clusters['cluster']==1].sample(N_SAMPLES).index\nplot_grid(signal_id=plot_ids)","78c35e74":"plot_ids = train_clusters.loc[train_clusters['cluster']==2].sample(N_SAMPLES).index\nplot_grid(signal_id=plot_ids)","ab74765f":"plot_ids = train_clusters.loc[train_clusters['cluster']==3].sample(N_SAMPLES).index\nplot_grid(signal_id=plot_ids)","7da51e17":"plot_ids = train_clusters.loc[train_clusters['cluster']==4].sample(N_SAMPLES).index\nplot_grid(signal_id=plot_ids)","939cffe0":"test_norm_feat = MinMaxScaler().fit_transform(test_feat)\npca = PCA(n_components=CLUSTER_DIM).fit(test_norm_feat)\ntest_norm_feat = pca.transform(test_norm_feat)\nprint('PCA Explained Variance: {:.2f}%'.format(pca.explained_variance_ratio_.sum()*100))\ndef get_silhouette(n):\n    clt = KMeans(n_clusters=n, random_state=RANDOM_SEED).fit(test_norm_feat)\n    return silhouette_score(test_norm_feat, clt.labels_)\nsilhouette = Parallel(n_jobs=N_THREADS)(delayed(get_silhouette)(n) for n in n_clusters)\nplt.plot(np.arange(2, len(silhouette)+2), silhouette)\nplt.xlabel('N_Clusters')\nplt.ylabel('Silhoutte Score')\nplt.grid(True)\n_ = plt.title('Clustering Score - Test Set')","898e7479":"best_n_clusters = 5\nnorm_feat = MinMaxScaler().fit_transform(test_feat)\nclt = KMeans(n_clusters=best_n_clusters, random_state=RANDOM_SEED).fit(norm_feat)\ntest_clusters = pd.DataFrame(clt.labels_, columns=['cluster'], index=test_meta['signal_id'])\nstats_df = test_clusters.reset_index().groupby('cluster').count()\nstats_df.columns = ['count']\ndisplay(stats_df)","9f69ab54":"norm_feat = MinMaxScaler().fit_transform(np.concatenate((train_feat, test_feat), axis=0))\npca = PCA(n_components=CLUSTER_DIM).fit(norm_feat)\nnorm_feat = pca.transform(norm_feat)\nprint('PCA Explained Variance: {:.2f} %'.format(pca.explained_variance_ratio_.sum()*100))\n\ndef get_silhouette(n):\n    clt = KMeans(n_clusters=n, random_state=RANDOM_SEED).fit(norm_feat)\n    return silhouette_score(norm_feat, clt.labels_)\nsilhouette = Parallel(n_jobs=N_THREADS)(delayed(get_silhouette)(n) for n in n_clusters)\nplt.plot(np.arange(2, len(silhouette)+2), silhouette)\nplt.xlabel('N_Clusters')\nplt.ylabel('Silhoutte Score')\nplt.grid(True)\n_ = plt.title('Clustering Score - Test Set')","338bbc89":"best_n_clusters = 6\nnorm_feat = MinMaxScaler().fit_transform(np.concatenate((train_feat, test_feat), axis=0))\npca = PCA(n_components=CLUSTER_DIM).fit(norm_feat)\nnorm_feat = pca.transform(norm_feat)\nclt = KMeans(n_clusters=best_n_clusters, random_state=RANDOM_SEED).fit(norm_feat)\nclusters = pd.DataFrame(clt.labels_, columns=['cluster'], index=pd.Index([int(x) for x in np.concatenate((train_sig_ids, test_sig_ids), 0)], name='signal_id'))\nclusters['dataset'] = 0\nclusters.iloc[train_sig_ids.shape[0]:, 1] = 1\nclusters['target'] = 0\nclusters.iloc[:train_sig_ids.shape[0], -1] = train_meta['target'].values\nstats_df = pd.DataFrame()\nstats_df['count'] = clusters['cluster'].value_counts()\nstats_df['train'] = clusters.groupby('cluster')['dataset'].apply(lambda x: (x==0).sum())\nstats_df['test'] = clusters.groupby('cluster')['dataset'].apply(lambda x: (x==1).sum())\nstats_df['pos_count'] = clusters.groupby('cluster')['target'].sum()\nstats_df['pos_rate'] = clusters.groupby('cluster')['target'].sum()\/stats_df['train']\nstats_df['pos_rate'] = stats_df['pos_rate'].apply(lambda x: '{:.2f} %'.format(x*100))\ndisplay(stats_df)","e6bfcae1":"plot_grid(signal_id=clusters.loc[clusters['cluster']==0].sample(N_SAMPLES).index)","3a197039":"plot_grid(signal_id=clusters.loc[clusters['cluster']==1].sample(N_SAMPLES).index)","c58de129":"plot_grid(signal_id=clusters.loc[clusters['cluster']==2].index[:6])","f815660b":"plot_grid(signal_id=clusters.loc[clusters['cluster']==3].sample(N_SAMPLES).index)","e50c7a01":"plot_grid(signal_id=clusters.loc[clusters['cluster']==4].sample(N_SAMPLES).index)","b3f6eac9":"plot_grid(signal_id=clusters.loc[clusters['cluster']==5].index[-6:])","75f5957c":"train_clusters.to_csv('train_clusters.csv')\ntest_clusters.to_csv('test_clusters.csv')\nclusters.to_csv('clusters.csv')","6489e3c2":"### **Cluster 0**","4f70b84a":"**Good! We've got five cluster again.**  \n**The number of samples in each cluster is the following: **","b7a0ff3d":"**Now, we are going to select the number of clusters based on the [Silhouette Score](https:\/\/en.wikipedia.org\/wiki\/Silhouette_%28clustering%29). For now, just the training data is been used.**","5f8cb565":"**It's clear that five cluster work well in our training data. Now we can see how the `target` are spread across these clusters: **","a0f63007":"### **Cluster 5 (Problematic)**","b449ba19":"### **Cluster 1**","8f67b756":"### **Cluster 3**","c1a98ef9":"**Interesting to note the clusters 4 and 2. The former virtually has no positive samples while tha latter has almost 50% of positive samples.**  \n**Let's plot a small sample of the signals in each cluster to get a feel of how they look like.**","e4627362":"**Let's straight to the clusters using the whole dataset (training + test). We hope to find the same clusters in both dataset.**","9775d324":"**Let's start by defining the features that will be used to cluster the signals.  \nWe are going to use a simple median of the slices of the frequency spectrum from a discrete Fourrier transform. It's a very simple feature space to start the analysis, but let's Keep It Sweet & Simple!  \nWe can expect a very skewed distribution of the data in the spectrum slices, but we are not going to transform the data this time.  \nLet's stop to write and start to coding. Here is the function to extract the features of a list of `signal_id`'s**\n\n    def cluster_features(signals, dataset='train'):\n        if dataset == 'train':\n            data = pq.read_pandas('..\/input\/train.parquet', columns=[str(s) for s in signals]).to_pandas().values\n        else:\n            data = pq.read_pandas('..\/input\/test.parquet', columns=[str(s) for s in signals]).to_pandas().values\n        features = np.zeros((data.shape[1], FEATURES_DIM))\n        for i, signal in enumerate(data.T):\n            fft = np.fft.rfft(signal)\n            fft = np.abs(fft)\n            fft = np.array_split(fft, FEATURES_DIM)\n            features[i] = [np.median(d) for d in fft]\n    return features\n    \n**In this case, the spectrum is been splitted in ranges of 20 KHz. Later, we will use PCA to reduce the dimension of the feature space.**","0e9e58c7":"**The number of components in the PCA was enough to explain almost 99% of the variance of the training data and, as we were expecting, the features present a lot of outliers.**","4875ccc8":"### **Cluster 2 (Problematic)**","e3ee007e":"**Now, it seems the data have 6 clusters! We have some cluster in a dataset that don't matches a cluster in the other one**\n**Getting some statistics in each cluster: **","881ac2df":"# Simple Clustering Analysis in the Frequency Domain\n\n**In this kernel, we are going to present some simple observations based on clusters created based on the frequency spectrum of the signals.  \nThis competition provides a good challenge (and a lot of fun!) regarding the stability of the training and the differences beetwen results in the local CV and LB score. While the discussions on adversarial validation can help with the issues, maybe the information in the clusters provide some complementary insights.**","0739af43":"**We have a couple of interesting obsevations in the above dataframe:**\n1. The cluster 5 matches the cluster 2 in the training analysis and it has no proper equivalent in the test set\n2. The cluster 2 matches the cluster 1 in the test analysis and it has no proper equivalent in the training set\n3. The proportion of training\/test samples in cluster 0 matches the proportion of training\/test samples in the dataset => Can we expect the same positive rate in the test set - cluster 0 ?  \n\n**Let's take a look in the clusters: **","9c995d7c":"### **Cluster 2**","c3e4d9b2":"# Final Notes\n\n**This analysis still have a wide margin to improvements. Note the clusters are imperfect, e.g the test examples in the cluster 5 seems more similar to the cluster 4 and the clusters 2 and 3 are similar except the number of peaks.  \nYet, it can maybe help in a post processing phase. As an example, all positive predictions in my current solution are placed in the cluster 0. It can also be used to stratify the folds in the training **\n\n### **That's all Folks!**\n### **Please, leave your suggestions, comments or feedback**","d8f16a7c":"### **Cluster 3**","3d76e55e":"### **Cluster 1**","c7120236":"**Now, the clusters will be created using just the test set.**","bf9a0a3c":"### **Cluster 4**","2c8ab480":"### **Cluster 0**","29c1174e":"### **Cluster 4**","cbfe1ed1":"**Let's load the features for the training and test datasets (s2 joblib!)**"}}