{"cell_type":{"c11579d1":"code","398b2c59":"code","451503f3":"code","f923efeb":"code","8881be32":"code","c189dedf":"code","288df9ba":"code","a92b1e19":"code","8da3d0da":"code","40aa2b2a":"code","7dacff6b":"code","2002aa8f":"code","440aba72":"code","b778a498":"code","6d6620ab":"code","d383b4b2":"code","a762c25a":"code","57140293":"code","e9ca6c85":"code","8de1e3ca":"code","13fcf82a":"code","4627ac59":"code","8a6beb03":"code","a15743b4":"code","93b5bfd2":"code","5c339197":"code","89497e8b":"code","0a9cfb89":"code","5e24d157":"code","16d96d2c":"code","8d1856ec":"code","e1540efb":"code","d2ea8c5d":"code","78688e10":"code","85f654d3":"code","28013627":"markdown","49b399eb":"markdown","696cc12f":"markdown","265b1a59":"markdown","723235aa":"markdown","7e57988d":"markdown","33d60d16":"markdown","15708d75":"markdown","d193b719":"markdown","5b241b82":"markdown","5774da60":"markdown","620b2cbf":"markdown","c0424df1":"markdown","b5befdd2":"markdown","476d9d72":"markdown","0e26bf9f":"markdown","8210bee3":"markdown","88aedc4b":"markdown","cdb7b66b":"markdown","39c10004":"markdown","74f9cd08":"markdown","49243cfe":"markdown","de157d21":"markdown","3c9cee3a":"markdown","869d8b7d":"markdown","9cc85c0b":"markdown","db64df48":"markdown"},"source":{"c11579d1":"import pandas as pd\n\nmain_file_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ndata = pd.read_csv(main_file_path)","398b2c59":"data.columns = [col.lower() for col in data.columns]","451503f3":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBRegressor","f923efeb":"data.head()","8881be32":"(data.groupby('yrsold')\n    .saleprice\n    .mean()\n    .sort_values(ascending=False))","c189dedf":"(data.assign(x=0)\n     .groupby('yrsold')\n     .x\n     .count()\n     .sort_values(ascending=False)\n)","288df9ba":"X = data.drop(['saleprice'], axis=1).select_dtypes(exclude='object').fillna(0)\ny = data.saleprice","a92b1e19":"data_model = DecisionTreeRegressor()\ndata_model.fit(X, y)\n","8da3d0da":"print('Predicting the next houses prices')\nprint(X.head())\nprint('The predictions are: ')\nprint(data_model.predict(X.head()))","40aa2b2a":"prediction = data_model.predict(X)\nmean_absolute_error(y, prediction)","7dacff6b":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\nvalid_model = DecisionTreeRegressor()\nvalid_model.fit(train_X, train_y)\nvalid_model_prediction = valid_model.predict(val_X)\nmean_absolute_error(val_y, valid_model_prediction)","2002aa8f":"def get_mae(max_leaf_nodes, predictor_train, predictor_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=36)\n    model.fit(predictor_train, targ_train)\n    pred_val = model.predict(predictor_val)\n    mae = mean_absolute_error(targ_val, pred_val)\n    return mae","440aba72":"for max_leaf_nodes in [5, 50, 500, 5000, None]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print('Max leaf nodes: {} \\t\\t Mean Absolute Error: {}'.format(max_leaf_nodes, my_mae))","b778a498":"rand_model = RandomForestRegressor(random_state=500)\nrand_model.fit(train_X, train_y)\nrand_model_pred = rand_model.predict(val_X)\nmean_absolute_error(val_y, rand_model_pred)\n","6d6620ab":"def get_forest_mae(max_leaf_nodes, predictor_train, predictor_val, targ_train, targ_val):\n    model = RandomForestRegressor(max_leaf_nodes=max_leaf_nodes, random_state=500)\n    model.fit(predictor_train, targ_train)\n    pred_val = model.predict(predictor_val)\n    mae = mean_absolute_error(targ_val, pred_val)\n    return mae\nfor max_leaf_nodes in [5, 50, 500, 5000, None]:\n    my_mae = get_forest_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print('Max leaf nodes: {} \\t\\t Mean Absolute Error: {}'.format(max_leaf_nodes, my_mae))","d383b4b2":"melb = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\nmelb.columns = [col.lower() for col in melb.columns]\nmelb.head()","a762c25a":"X2 = melb.drop(['price'], axis=1).select_dtypes(exclude='object')\ny2 = melb.price","57140293":"def score_dataset(train_X, val_X, train_y, val_y):\n    model = RandomForestRegressor(n_jobs=4)\n    model.fit(train_X, train_y)\n    model_predictions = model.predict(val_X)\n    mae = mean_absolute_error(val_y, model_predictions)\n    return mae","e9ca6c85":"train_X2, val_X2, train_y2, val_y2 = train_test_split(X2, y2, random_state=0)\ncol_miss = [col for col in train_X2.columns\n                       if train_X2[col].isnull().any()]\nreduced_train_X2 = train_X2.drop(col_miss, axis=1)\nreduced_val_X2 = val_X2.drop(col_miss, axis=1)\nprint('Mean Absolute Error: {}'.format(score_dataset(reduced_train_X2, reduced_val_X2, train_y2, val_y2)))","8de1e3ca":"impute = SimpleImputer()\nimp_train_X2 = impute.fit_transform(train_X2)\nimp_val_X2 = impute.transform(val_X2)\nprint('Mean Absolute Error: {}'.format(score_dataset(imp_train_X2, imp_val_X2, train_y2, val_y2)))","13fcf82a":"impute_train_X = train_X2.copy()\nimpute_val_X = val_X2.copy()\n\nfor col in col_miss:\n    impute_train_X[col + ' was missing'] = impute_train_X[col].isnull()\n    impute_val_X[col + ' was missing'] = impute_val_X[col].isnull()\nimpute_train_X2 = impute.fit_transform(impute_train_X)\nimpute_val_X2 = impute.transform(impute_val_X)\nprint('Mean Absolute Error: {}'.format(score_dataset(impute_train_X2, impute_val_X2, train_y2, val_y2)))","4627ac59":"melbX = pd.get_dummies(melb.drop(['price'], axis=1))\nmelby = melb.price","8a6beb03":"melbX.shape","a15743b4":"melbX_train, melbX_val, melby_train, melby_val = train_test_split(melbX, melby, test_size=0.25)\nimpute_melbX_train = impute.fit_transform(melbX_train)\nimpute_melbX_val = impute.transform(melbX_val)","93b5bfd2":"temp_X = melb.drop(['price'], axis=1) \nenc = pd.get_dummies(temp_X)\nenc_colmiss = [col for col in enc.columns if enc[col].isnull().any()]\nencoded = enc.drop(enc_colmiss, axis=1)\nnoenc_colmiss = [col for col in X2.columns if X2[col].isnull().any()]\nno_encoded = X2.drop(noenc_colmiss, axis=1)","5c339197":"def scorer(X, y):\n    return -1 * cross_val_score(RandomForestRegressor(50, n_jobs=4), X, y, scoring='neg_mean_absolute_error').mean()\n\nscore_encoded = scorer(encoded, y2)\nscore_no_encoded = scorer(no_encoded, y2)\nprint('Score with One Hot Encoder: {} \\n Score without One Hot Encoder: {}'.format(str(score_encoded), str(score_no_encoded)))","89497e8b":"plotX = impute.fit_transform(X2)\ngrad_model = GradientBoostingRegressor()\ngrad_model.fit(plotX, y2)\nplots = plot_partial_dependence(grad_model, \n                                features=[1, 6], \n                                X=plotX, \n                                feature_names=['rooms', 'distance', 'postcode', 'bedroom2', 'bathroom', 'car',\n     'landsize', 'buildingarea', 'yearbuilt', 'lattitude', 'longtitude', 'propertycount'], \n                                grid_resolution=15, \n                                n_jobs=4)","0a9cfb89":"iX = data.drop(['saleprice'], axis=1).select_dtypes(exclude='object')\niy = data.saleprice\ntrain_iX, val_iX, train_iy, val_iy = train_test_split(iX, iy, random_state=0)\ni_train_iX = impute.fit_transform(train_iX)\ni_val_iX = impute.transform(val_iX)\nprint('Mean Absolute Error: {}'.format(score_dataset(i_train_iX, i_val_iX, train_iy, val_iy)))","5e24d157":"impute_train_iX = train_iX.copy()\nimpute_val_iX = val_iX.copy()\n\nfor col in iX.columns:\n    impute_train_iX[col + ' was missing'] = impute_train_iX[col].isnull()\n    impute_val_iX[col + ' was missing'] = impute_val_iX[col].isnull()\nimpute_train_iX_plus = impute.fit_transform(impute_train_iX)\nimpute_val_iX_plus = impute.transform(impute_val_iX)\nprint('Mean Absolute Error: {}'.format(score_dataset(impute_train_iX_plus, impute_val_iX_plus, train_iy, val_iy)))","16d96d2c":"pip = make_pipeline(SimpleImputer(), RandomForestRegressor(n_jobs=4))\ncross_scores = cross_val_score(pip, iX, iy, scoring='neg_mean_absolute_error')\nprint('Mean Absolute Error: {}'.format(cross_scores.mean() * -1))","8d1856ec":"encX = pd.get_dummies(data.drop(['saleprice'], axis=1))\nmissed = [col for col in encX.columns if encX[col].isnull().any()]\nencodedX = encX.drop(missed, axis=1)\nmis = [col for col in iX.columns if iX[col].isnull().any()]\nnoencodedX = iX.drop(mis, axis=1)","e1540efb":"iowa_encoded = scorer(encodedX, iy)\niowa_noencoded = scorer(noencodedX, iy)\nprint('Score with One Hot Encoder: {} \\n Score without One Hot Encoder: {}'.format(iowa_encoded, iowa_noencoded))","d2ea8c5d":"Xboost = pd.get_dummies(data.drop(['saleprice'], axis=1))\nyboost = data.saleprice","78688e10":"def model_pipeline(transform, model, X, y):\n    pip = make_pipeline(transform, model)\n    cross_scores = cross_val_score(pip, X, y, scoring='neg_mean_absolute_error')\n    return print('Mean Absolute Error: {}'.format(cross_scores.mean() * -1))","85f654d3":"model_pipeline(SimpleImputer(), XGBRegressor(n_jobs=4), Xboost, yboost)\nmodel_pipeline(SimpleImputer(), XGBRegressor(n_estimators=1000, n_jobs=4), Xboost, yboost)\nmodel_pipeline(SimpleImputer(), XGBRegressor(n_estimators=100, learning_rate=0.05, n_jobs=4), Xboost, yboost)","28013627":"Now that we can create more accurate models with impute techniques, let's go one step further and incorporate categorical variables to our models. To acomplish that we will need to encode this categoricals. One way to do that is with One Hot Encoder:\n\n**One Hot Encoder:** It is an encoding technique that creates new columns to every categorical and assign 1 for rows that contains that categorical and 0 to the ones that not. It is commonly used in datasets that doesn't have more than 10 different categoricals per column. Pandas has a simple function that implements this technique, pd.get_dummies(). Web page:  https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.get_dummies.html ","49b399eb":"In this particular data, as the model goes deeper in nested classificators the accuracy is lower. We can assume that as it goes deeper it learns the patterns of the training data which are not useful to predict unseen data.\n\n**ENSEMBLE MODELS**\n\nThese models consists in running two or more related models with the same data to synthesize the result in one score, which is usually more accurated.\n\n**Random Forest Regressor** is an ensemble model that run many random Decision Tree Regressor models and synthesize their results. Web page: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html\n","696cc12f":"# Predicting the price of houses in Iowa and Melbourne\n**This tutorial covers both, level 1 and level 2 of the machine learning track**\n\nIn order to start we first need to load the data:","265b1a59":"Now we will use some techniques to replace NaN values, more complicated than replace all NaN's with zero. And in order to evaluate their efficience we will create a function that returns the Mean Absolute Error:","723235aa":"**Our model is perfect!!!**\n\nWell perfectly wrong, what we seeing here is a case of overfitting, our model perfectly learned the patterns of the training data but It is useless in the real world. Our mission is to create a model that can give us useful information for new data, data that was not use in the training. So we can use train_test_split to separate the data and prove the model with unseen data:","7e57988d":"By reusing the above function score_dataset() we can see how the accuracy improve, I will leave the code below in case you want to run it yourself:\n\n> score_dataset(impute_melbX_train, impute_melbX_val, melby_train, melby_val)\n\nOn the other hand, if we want to get a better evaluation for a model, we should use cross validation.\n\n**Cross Validation:** Consist in separating the train data from the test data in an iterative process which always pick different samples in every run. Scikit-learn's function is cross_val_score. Web page: http:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n\nThis process should be used in relatively small datasets so we are going to reduce our dataframes dropping the columns with NaN values.","33d60d16":"**3)** Highlighting the rows we are going to replace with the imputer:","15708d75":"We can see that our predictors dataframe is extremely large, which means that using One Hot Encoder is not the best way to create a better model. In spite of this fact if we run the following code we can get a more precise model. ","d193b719":"The outcome is really disappointing, but real world models are much likely to be like that, it is the very known trade-off between learning and generalization.\n\nNext we can prove sistematically the same model with different parameters to find the most accurate one:","5b241b82":"In order to prove that the prices of Melbourne's houses rised untill 2008 mortgage crisis, we separate the data by the year sold and the saleprice, then we check the average price.   ","5774da60":"Here is a counterintuitive outcome, it seems that the economical context of that those years allowed many people to have resources to buy a house no matter the prices. Maybe people's intuition led them to believe that houses' prices were going to rise and they see a chance of doing bussiness. \n\nNow familiarized with the data, let's start constructing the model:\n\nFirst, we define X by convention this variable contains de variables used to predict the target variable, y, which contains the saleprices of the houses. For now, X will only contain numeric features and replace NaN values for 0.  ","620b2cbf":"As we can see, Melbourne's houses didn't escape from the global crisis\n\nFollowing the price and demand law we can assume that the years with fewer houses sold should be the ones with higher average prices. Let see","c0424df1":"Now we choose a model. In this case we will use a simple decission tree model.\n\n**Decission Tree Regressor:**  It is a nested labels based model which classifies data with similar characteristics.  Web page: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html\n\nWe instance the model, previously imported, then we compute de parameters of the model:","b5befdd2":"For a more comfortable data manipulation,  you might want to change all columns names to lowercase.","476d9d72":"This information is not enough to judge the model so we will implement some function to evaluate it.\n\n**Mean Absolute Error:** is a commonly use metric to evaluate a model, It is the mean obtained by resting the real values from the values generated by the model, all of them in absolute value. Web page: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_absolute_error.html","0e26bf9f":"Some people say that data science models are black boxes, because you cannot obtain information about the relationship between the variables which compose the model. Most of those people do not know Partial Dependence Plots. \n\n**Partial Dependence Plots:** Graphs that show the relationship between a variable and the target. Web page: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.partial_dependence.plot_partial_dependence.html\n\nNote: For now this method only works with Gradient Boosting Regressor model. Web page: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html","8210bee3":"Next, I like to import all the classes in the same note, so we can run the most pieces of code independently:","88aedc4b":"As we can see, the left graph show us a negative correlation between distance and price and the right graph show us a positive correlation between landsize and price. Both are intuitive but this will not be always the case, datasets can surprise us in many ways.\n\n### Returning to the Iowa dataset to prove some of these methods and some new ones:\n\nManaging NaN values: ","cdb7b66b":"# LEVEL 2\n\nNow we will apply more sophisticated techniques to the Melbourne data.","39c10004":"For the last method, we will introduce Pipelines.\n\n**Pipelines:** Functions that allow us to run a preprocessing method and a model in the same line, making our code more legible and easy to follow. Web page: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.make_pipeline.html","74f9cd08":"# LEVEL 1\n\nGetting to know with the data","49243cfe":"Before training a model with these variables let's see how large is the predictors dataframe:","de157d21":"**1)** Eliminate the columns with NaN values: ","3c9cee3a":"**2)** Use an imputer to replace NaN values: \n\n**Imputer:** It is a function which replaces NaN values with the mean value of the column, because it is the most common value in it, so by probability it is the best way to replace those values. You can also choose the mode or the median. Web page: http:\/\/scikit-learn.org\/dev\/modules\/generated\/sklearn.impute.SimpleImputer.html","869d8b7d":"# Leading Model For Predictions: XGboost\n\n**XGBoost:** is an implementation of the Gradient Boosted Decision Trees algorithm, which means that it go through cycles building ensemble models that it uses to keep improving the final model. This might not be an accurate definition but you can think of it like an ensemble model of ensemble models. Web page: https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html","9cc85c0b":"Finally, we can se how this model is the one with the best results, and how as the introduction continued our code become more legible and more efficient. But the most important part is that we get to this code without jumping the fundamentals, which means that we know exactly what is happening behind this elegant code.\n\n## I really hope this tutorial was helpful!  \n\n\n","db64df48":"**Encoding the data**"}}