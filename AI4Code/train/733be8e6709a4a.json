{"cell_type":{"f6c9e2e8":"code","a4b5bcc5":"code","dad03bbb":"code","42a8fa4e":"code","0c3e77c1":"code","eee91405":"code","0ad4ac75":"code","5235d060":"code","c3b2e66c":"code","06031249":"code","bf8349e7":"code","d9067f2a":"code","35f727d3":"code","38beb7d6":"code","1dce1367":"code","3336d71b":"code","811a366b":"code","289afd4f":"code","7a33dcd6":"code","4787fec9":"code","62bc59cd":"code","60df6abf":"code","9769a10c":"code","4f16d954":"code","68d48e06":"code","2041cafb":"code","c2fa4f3d":"code","85e2d69a":"code","703b8632":"code","fb75b71a":"code","09af7ede":"code","9bdedfe5":"code","3b82a26a":"code","e3168a07":"code","b24f300f":"code","2ad7edc4":"code","0a8640ab":"code","b18237b7":"code","9f74bdaa":"code","cbf632c7":"code","5ae05d0e":"code","005fd21e":"code","248795e3":"code","316dbed8":"code","93232465":"code","4fe7b2ea":"code","1ecda848":"code","f1788d21":"code","ae71ebe6":"code","c7544263":"code","8178158b":"code","c7a4c9d3":"code","50e1c187":"code","f8e32dbb":"code","86eaa74c":"code","e70d116b":"code","a526544d":"code","b8ce6587":"code","915ccdc5":"code","31f59b15":"code","06f8eb09":"code","51c7b593":"code","79420b25":"code","d521cdc9":"code","7fdd0298":"code","46a25886":"code","467b68eb":"code","ccd14ea1":"code","4b0ab35e":"code","181eb5c1":"code","f115e74b":"code","f82f5203":"code","92bea050":"code","d8c3685e":"code","a22dfbe0":"code","9a34e9e6":"code","be9c881e":"code","cc9eae14":"code","ba39b8f4":"code","789bf279":"code","8421eb39":"code","61ee0b9d":"code","be8bc346":"code","2bed8972":"code","6e704f1e":"code","9254fa08":"code","f816f154":"code","80e71bd4":"code","0cd53e7c":"code","909bfc18":"code","527c3a45":"code","a395b2a5":"code","ab0589e5":"code","bdeea354":"code","1d7704a3":"code","18e1bf54":"code","7f012f2f":"code","028e987c":"code","e904eb1f":"code","fc4ff32a":"code","b225187b":"code","06d5922a":"code","256384e1":"code","5fc39dd1":"code","ea2d0b84":"code","767dafdf":"code","5ffd2b8b":"code","206a4205":"code","7d3bd281":"code","a6efb427":"code","544906df":"code","7efabfcd":"code","c763b863":"code","17a8c624":"code","1bb9f4a6":"code","21a138dc":"code","20030fa8":"code","9ff95ca9":"code","07e3726b":"code","47b5b46c":"code","66db7b44":"code","3da13b10":"code","d225f1bb":"code","63bef0cc":"code","366ff64d":"code","513181fe":"code","14b1fc8c":"code","51957880":"code","fb6c3f6b":"code","a41e4794":"code","eb614382":"code","a71da1b9":"code","e0a315d0":"code","12bb91b8":"code","62f7b810":"code","0ba6ca3b":"code","a7759b3b":"code","aa11766f":"code","c5165ed6":"code","721519af":"code","6090a06b":"code","399f908c":"code","7c85c703":"code","7dd71b4e":"code","b053c317":"code","48e42a66":"code","31cedf4f":"code","86ba411e":"code","42ad81f1":"code","2511c8c1":"code","dbb532d7":"code","7f0d4bf8":"code","0befb01c":"code","6d789675":"code","5af3fb05":"code","f5161d72":"code","378bc7c8":"code","ec93d983":"code","81849624":"code","16f05aac":"markdown","f6351bce":"markdown","13d61519":"markdown","c1bb2f28":"markdown","396e6c4f":"markdown","90943d70":"markdown","8c08d41a":"markdown","7108cc26":"markdown","1064c998":"markdown","07ebc1a3":"markdown","b29ff889":"markdown","3c11e6a0":"markdown","79859935":"markdown","67c3896a":"markdown","76b45e2f":"markdown","36c04328":"markdown","b7b9254c":"markdown","24c67a0d":"markdown","27154476":"markdown","53fdb0db":"markdown","8b666057":"markdown","e4599d03":"markdown","b5dfa8b5":"markdown","7bf66e99":"markdown","9ecc1f71":"markdown","93e8d43b":"markdown","98332116":"markdown","b266752d":"markdown","fcc93b8d":"markdown","8a3a8917":"markdown","854ef2cc":"markdown","d3d141c4":"markdown","af4ce486":"markdown","c94a28e5":"markdown","80404e8f":"markdown","1cb78438":"markdown","cc8100e3":"markdown","dd31ff8d":"markdown"},"source":{"f6c9e2e8":"import os\nimport pandas as pd\nimport numpy as np","a4b5bcc5":"df=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","dad03bbb":"df.head()","42a8fa4e":"df.shape","0c3e77c1":"import matplotlib.pyplot as plt","eee91405":"plt.hist(df['SalePrice'])","0ad4ac75":"df['sale_price_log'] = np.log(df['SalePrice'])\nplt.hist(df['sale_price_log'])","5235d060":"import statsmodels.formula.api as smf\nimport statsmodels.stats.multicomp as multi","c3b2e66c":"def uni_analysis(df):\n    x = (df.select_dtypes(include=['O']))\n    p_value = []\n    for i in x:\n        para = 'SalePrice ~ '+str(i)\n        model = smf.ols(formula=para, data=df)\n        results = model.fit()\n        p_value.append(results.f_pvalue)\n    df1 = pd.DataFrame(list(zip(x,p_value)), columns =['Variable', 'p_value'])\n    df1['Drop_column'] = df1['p_value'].apply(lambda x: 'True' if x > 0.05 else 'False')\n    return df1","06031249":"uni_analysis(df)","bf8349e7":"drop_col = ['Street','PoolQC','Utilities','LandSlope','MiscFeature','Condition2']","d9067f2a":"df.drop(drop_col, axis=1, inplace=True)","35f727d3":"#3SsnPorch, there is some issue with the column name, the ols algorithm shows error with this name. hence we rename it.\ndf.rename(columns={'3SsnPorch': \"threessnporch\"}, inplace=True)\n","38beb7d6":"# YearRemodAdd teslls us if the house was remodelled. if the yearbuilt and yearremodadd are same that means there was no modification\ndf['YearRemodAdd'] = df['YearRemodAdd'].astype(str)\ndf.loc[(df.YearRemodAdd == df.YearBuilt), 'YearRemodAdd'] = 'No_remodel'","1dce1367":"def univar_cont2(df, threshold):\n    x = (df.select_dtypes(include=['int64','float64']))\n    p_value = []\n    col_name = []\n    for i in x:\n        if (df[i].nunique())<threshold:\n            df[i] = df[i].astype(str)\n            col_name.append(i)\n            para = 'SalePrice ~ '+str(i)\n            model = smf.ols(formula=para, data=df)\n            results = model.fit()\n            p_value.append(results.f_pvalue)\n        else:\n            print('columns are truely continous:',i,'and the unique entries are', df[i].nunique())\n    df1 = pd.DataFrame(list(zip(col_name,p_value)), columns =['Variable', 'p_value'])\n    df1['Drop_column'] = df1['p_value'].apply(lambda x: 'True' if x > 0.05 else 'False')\n    return df1\n","3336d71b":"univar_cont2(df, 50)  ","811a366b":"df.drop(['LowQualFinSF','BsmtHalfBath','threessnporch','MiscVal','MoSold','YrSold'], axis=1, inplace=True)","289afd4f":"x = (df.select_dtypes(include=['int64','float64']))\nfor i in x:\n    if (df[i].nunique())<50:\n        df[i] = df[i].astype(str)","7a33dcd6":"df.dtypes.value_counts()","4787fec9":"def univar_cont(df, target):\n    x = (df.select_dtypes(include=['int64','float64']))\n    print('There are ',len(x.columns),' columns with continous variable')\n    mean_val =[]\n    median_val = []\n    min_val = []\n    max_val = []\n    variance_val = []\n    std_val = []\n    q1_val = []\n    q3_val= []\n    corelation = []\n    \n    for i in x:\n        mean_val.append(df[i].mean())\n        median_val.append(df[i].median())\n        min_val.append(df[i].min())\n        max_val.append(df[i].max())\n        variance_val.append(df[i].var())\n        std_val.append(df[i].std())\n        \n        q1,q3 = df[i].quantile([0.25,0.75])\n        q1_val.append(q1)\n        q3_val.append(q3)\n        corelation.append(df[i].corr(df[target]))\n    df1 = pd.DataFrame(list(zip(x,mean_val,median_val,min_val,max_val,variance_val,std_val,q1_val,q3_val, corelation)), \n                      columns=['variable','mean','median','minimum','maximum','variance','std_deviation','quantile_1','quantile_3','corelation'])\n    return df1\n   ","62bc59cd":"univar_cont(df, 'SalePrice')","60df6abf":"df.drop(['BsmtFinSF2','EnclosedPorch','BsmtUnfSF'], axis=1, inplace=True)","9769a10c":"df.head()","4f16d954":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = df.select_dtypes(include=numerics)","68d48e06":"newdf.drop(['Id','sale_price_log'], axis=1, inplace=True)","2041cafb":"newdf.head()","c2fa4f3d":"cor_tab = newdf.corr()\ncor_tab.style.background_gradient(cmap='coolwarm')\n","85e2d69a":"df.drop('1stFlrSF', axis=1, inplace=True)","703b8632":"df.drop('GarageYrBlt', axis=1, inplace=True)","fb75b71a":"df.head()","09af7ede":"miss_val = df.isna().sum()\nmiss_val = pd.DataFrame(miss_val, columns=['miss'])\n\nmiss_val = miss_val.loc[(miss_val!=0).any(axis=1)]","9bdedfe5":"miss_val['miss_percent'] = (miss_val['miss']\/1460)*100","3b82a26a":"miss_val","e3168a07":"# since alley and fence have more than 80% of missing data we will drop these columns. note that alley has good p-value. so later we will try to include it to see if it will improve the model\ndf.drop(['Alley','Fence'], axis=1, inplace=True)","b24f300f":"# just noticed that fireplace is actually a numerical variable, it can be used as categorical. but to reduce the number of dummy\n#columns, we will make it as numberic\ndf['Fireplaces'] = df['Fireplaces'].astype('int32')","2ad7edc4":"df.shape","0a8640ab":"df['GarageType'].isna().sum()","b18237b7":"xx= df[['GarageType','GarageFinish','GarageQual','GarageCond']]","9f74bdaa":"xx.fillna(-99, inplace=True)","cbf632c7":"xx","5ae05d0e":"xy = xx.index[xx['GarageType']==-99].tolist()","005fd21e":"df['GarageCars'][xy]","248795e3":"df['GarageCond'][xy]","316dbed8":"df['GarageArea'][xy]","93232465":"df['GarageType'].fillna('No_garage', inplace=True)","4fe7b2ea":"df['GarageQual'].fillna('No_garage', inplace=True)","1ecda848":"df['GarageCond'].fillna('No_garage', inplace=True)","f1788d21":"df['GarageFinish'].fillna('No_garage', inplace=True)","ae71ebe6":"miss_val = df.isna().sum()\nmiss_val = pd.DataFrame(miss_val, columns=['miss'])\n\nmiss_val = miss_val.loc[(miss_val!=0).any(axis=1)]\nmiss_val","c7544263":"bs = df.index[df['BsmtQual'].isna()].tolist()","8178158b":"df['BsmtCond'][bs]","c7a4c9d3":"df['BsmtFinType1'][bs]","50e1c187":"df['BsmtExposure'][bs]","f8e32dbb":"# similar to garage, impute these columns with no basement","86eaa74c":"df['BsmtFullBath'][bs]","e70d116b":"df['BsmtQual'].fillna('No_basement', inplace=True)\ndf['BsmtCond'].fillna('No_basement', inplace=True)\ndf['BsmtExposure'].fillna('No_basement', inplace=True)\ndf['BsmtFinType1'].fillna('No_basement', inplace=True)\ndf['BsmtFinType2'].fillna('No_basement', inplace=True)","a526544d":"miss_val = df.isna().sum()\nmiss_val = pd.DataFrame(miss_val, columns=['miss'])\n\nmiss_val = miss_val.loc[(miss_val!=0).any(axis=1)]\nmiss_val","b8ce6587":"ms = df.index[df['MasVnrArea'].isna()].tolist()","915ccdc5":"df['MasVnrArea'][ms]","31f59b15":"df['MasVnrArea'].fillna(0, inplace=True)\ndf['MasVnrType'].fillna('No_Masonry', inplace=True)","06f8eb09":"fi = df.index[df['FireplaceQu'].isna()].tolist()","51c7b593":"df['Fireplaces'][fi]","79420b25":"# the missing values in fireplacequ is where the fireplace is 0","d521cdc9":"df['FireplaceQu'].fillna('No_Fireplace', inplace=True)","7fdd0298":"miss_val = df.isna().sum()\nmiss_val = pd.DataFrame(miss_val, columns=['miss'])\n\nmiss_val = miss_val.loc[(miss_val!=0).any(axis=1)]\nmiss_val","46a25886":"df=df.dropna(subset=['Electrical'])","467b68eb":"miss_val = df.isna().sum()\nmiss_val = pd.DataFrame(miss_val, columns=['miss'])\n\nmiss_val = miss_val.loc[(miss_val!=0).any(axis=1)]\nmiss_val","ccd14ea1":"df['LotFrontage'].fillna(df['LotFrontage'].median(), inplace=True)","4b0ab35e":"miss_val = df.isna().sum()\nmiss_val = pd.DataFrame(miss_val, columns=['miss'])\n\nmiss_val = miss_val.loc[(miss_val!=0).any(axis=1)]\nmiss_val","181eb5c1":"def outlier_detect (df, col, treatment=False):\n    from scipy.stats import iqr\n    iqr_c = iqr(df[col])\n    q1,q3 = df[col].quantile([0.25,0.75])\n    upper_wis = q3 + (1.5*iqr_c)\n    lower_wis = q1 - (1.5*iqr_c)\n    out_liers_upper = df.index[df[col] > upper_wis]\n    out_liers_lower = df.index[df[col] < lower_wis]\n    \n    col_num = df.columns.get_loc(col)\n    if treatment==True:\n        for x in out_liers_upper:\n            df.iloc[x,col_num]= upper_wis\n        for x in out_liers_lower:\n            df.iloc[x,col_num]= lower_wis\n    return out_liers_upper\n    return out_liers_lower\n    return df","f115e74b":"df_linear_reg = df\ndf_linear_reg.head()","f82f5203":"col_lis=[]\nfor i in df_linear_reg.columns:\n    if (df_linear_reg[i].dtype in numerics):\n        if (df_linear_reg[i].nunique())>20:\n            col_lis.append(i)\ncol_lis","92bea050":"#outlier_detect(df_linear_reg, 'LotFrontage',treatment=True)\n#outlier_detect(df_linear_reg, 'LotArea',treatment=True)\n#outlier_detect(df_linear_reg, 'BsmtFinSF1',treatment=True)\n#outlier_detect(df_linear_reg, 'TotalBsmtSF',treatment=True)\n#outlier_detect(df_linear_reg, '2ndFlrSF',treatment=True)\n#outlier_detect(df_linear_reg, 'GrLivArea',treatment=True)\n#outlier_detect(df_linear_reg, 'GarageArea',treatment=True)\n#outlier_detect(df_linear_reg, 'OpenPorchSF',treatment=True)\n#outlier_detect(df_linear_reg, 'ScreenPorch',treatment=True)\n","d8c3685e":"del df['Id']","a22dfbe0":"df.columns","9a34e9e6":"for i in df.columns:\n    if (df[i].nunique())<20:\n        df[i] = df[i].astype(str)","be9c881e":"df['MSSubClass']= df['MSSubClass'].astype(int)\ndf['PoolArea']= df['PoolArea'].astype(int)","cc9eae14":"df.dtypes.value_counts()","ba39b8f4":"df.shape","789bf279":"features = pd.get_dummies(df)\nfeatures.shape","8421eb39":"features.head()","61ee0b9d":"features.dtypes.value_counts()","be8bc346":"target = features.sale_price_log","2bed8972":"target_actuals = features['SalePrice']\ndel features['sale_price_log']\ndel features['SalePrice']","6e704f1e":"features_list = list(features.columns)","9254fa08":"features.shape","f816f154":"#features = np.array(features)","80e71bd4":"from sklearn.model_selection import train_test_split\ntrain_features, test_features, train_target, test_target = train_test_split(features, target, test_size = 0.20, random_state = 77)","0cd53e7c":"print('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_target.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_target.shape)","909bfc18":"# Import the modelg\nfrom sklearn.ensemble import RandomForestRegressor\n# Instantiate model with 500 decision trees\nrf = RandomForestRegressor(n_estimators = 820, random_state = 77, oob_score=True, max_features=0.80, n_jobs=-1)\n# Train the model on training data\nrf.fit(train_features, train_target)","527c3a45":"# predict on the test data splitted from the train\npredictions = rf.predict(test_features)","a395b2a5":"pred_actuals = np.exp(predictions)","ab0589e5":"# Calculate the absolute errors\nerrors = abs(predictions - test_target)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 5))","bdeea354":"def linear_report (y_actual, y_pred):\n    from sklearn.metrics import mean_squared_error\n    from sklearn.metrics import mean_absolute_error\n    from sklearn.metrics import r2_score\n    \n    mae = mean_absolute_error(y_actual, y_pred)\n    print('MAE value is: ',mae)\n    mse = mean_squared_error(y_actual, y_pred)\n    print('MSE value is: ', mse)\n    import math\n    rmse = math.sqrt(mse)\n    print('RMSE value is: ', rmse)\n    r2 = r2_score(y_actual, y_pred)\n    print('R2 score is: ',r2)\n    mape = 100*(mae\/y_actual)\n    accuracy = 100-np.mean(mape)\n    print('Accuracy score is: ',accuracy)","1d7704a3":"#Regression report for log values\nlinear_report(test_target,predictions)","18e1bf54":"#Regression report for actuals values\ntarget_actuals = np.exp(test_target)\nlinear_report(target_actuals,pred_actuals)","7f012f2f":"from sklearn.metrics import mean_absolute_error\nn_estimators = [800,820,830,850,860,870,880,900]\ntrain_results = []\ntest_results = []\nfor estimator in n_estimators:\n    rf = RandomForestRegressor(n_estimators=estimator, n_jobs=-1)\n    rf.fit(train_features, train_target)\n    train_pred = rf.predict(train_features)\n    mae = mean_absolute_error(train_target, train_pred)\n    mape = 100*(mae\/train_pred)\n    accuracy = 100-np.mean(mape)\n    train_results.append(accuracy)\n    \n    y_pred = rf.predict(test_features)\n    mae_p = mean_absolute_error(test_target, y_pred)\n    mape_p= 100*(mae\/y_pred)\n    accuracy_p = 100-np.mean(mape_p)\n    test_results.append(accuracy_p)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_estimators, train_results,'b', label='Train R2')\nline2, = plt.plot(n_estimators, test_results,'r', label='Test R2')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('R2 score')\nplt.xlabel('n_estimators')\nplt.show()","028e987c":"## number of estimators will be set to 820","e904eb1f":"from sklearn.metrics import mean_absolute_error\nmax_dep = np.arange(300, 500, 20)\ntrain_results = []\ntest_results = []\nfor max_d in max_dep:\n    rf = RandomForestRegressor(n_estimators=820, max_depth=max_d, n_jobs=-1)\n    rf.fit(train_features, train_target)\n    train_pred = rf.predict(train_features)\n    mae = mean_absolute_error(train_target, train_pred)\n    mape = 100*(mae\/train_pred)\n    accuracy = 100-np.mean(mape)\n    train_results.append(accuracy)\n    \n    y_pred = rf.predict(test_features)\n    mae_p = mean_absolute_error(test_target, y_pred)\n    mape_p= 100*(mae\/y_pred)\n    accuracy_p = 100-np.mean(mape_p)\n    test_results.append(accuracy_p)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_dep, train_results,'b', label='Train R2')\nline2, = plt.plot(max_dep, test_results,'r', label='Test R2')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('R2 score')\nplt.xlabel('max depth')\nplt.show()","fc4ff32a":"## the training and test does not show any overfitting hence we will keep it as none.","b225187b":"min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_s in min_samples_splits:\n    rf = RandomForestRegressor(n_estimators=820, max_depth=None, n_jobs=-1, min_samples_split=min_s)\n    rf.fit(train_features, train_target)\n    train_pred = rf.predict(train_features)\n    mae = mean_absolute_error(train_target, train_pred)\n    mape = 100*(mae\/train_pred)\n    accuracy = 100-np.mean(mape)\n    train_results.append(accuracy)\n    \n    y_pred = rf.predict(test_features)\n    mae_p = mean_absolute_error(test_target, y_pred)\n    mape_p= 100*(mae\/y_pred)\n    accuracy_p = 100-np.mean(mape_p)\n    test_results.append(accuracy_p)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_splits, train_results,'b', label='Train R2')\nline2, = plt.plot(min_samples_splits, test_results,'r', label='Test R2')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('R2 score')\nplt.xlabel('min sample splits')\nplt.show()","06d5922a":"## we will keep it as default, i.e int 2","256384e1":"min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_sl in min_samples_leafs:\n    rf = RandomForestRegressor(n_estimators=820, max_depth=None, n_jobs=-1, min_samples_split=2, min_samples_leaf=min_sl)\n    rf.fit(train_features, train_target)\n    train_pred = rf.predict(train_features)\n    mae = mean_absolute_error(train_target, train_pred)\n    mape = 100*(mae\/train_pred)\n    accuracy = 100-np.mean(mape)\n    train_results.append(accuracy)\n    \n    y_pred = rf.predict(test_features)\n    mae_p = mean_absolute_error(test_target, y_pred)\n    mape_p= 100*(mae\/y_pred)\n    accuracy_p = 100-np.mean(mape_p)\n    test_results.append(accuracy_p)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(min_samples_leafs, train_results,'b', label='Train R2')\nline2, = plt.plot(min_samples_leafs, test_results,'r', label='Test R2')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('R2 score')\nplt.xlabel('min sample leafs')\nplt.show()","5fc39dd1":"##there is no overfitting, so we will keep this as default i.e 1","ea2d0b84":"max_feat = np.linspace(0.1, 1, 10, endpoint=True)\ntrain_results = []\ntest_results = []\nfor min_fe in max_feat:\n    rf = RandomForestRegressor(n_estimators=820, max_depth=None, n_jobs=-1, min_samples_split=2, max_features=min_fe )\n    rf.fit(train_features, train_target)\n    train_pred = rf.predict(train_features)\n    mae = mean_absolute_error(train_target, train_pred)\n    mape = 100*(mae\/train_pred)\n    accuracy = 100-np.mean(mape)\n    train_results.append(accuracy)\n    y_pred = rf.predict(test_features)\n    mae_p = mean_absolute_error(test_target, y_pred)\n    mape_p= 100*(mae\/y_pred)\n    accuracy_p = 100-np.mean(mape_p)\n    test_results.append(accuracy_p)\n    \nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_feat, train_results,'b', label='Train R2')\nline2, = plt.plot(max_feat, test_results,'r', label='Test R2')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('R2 score')\nplt.xlabel('max features')\nplt.show()","767dafdf":"## max feature at 0.5 performs better","5ffd2b8b":"rf = RandomForestRegressor(n_estimators=820, max_depth=None, n_jobs=-1, min_samples_split=2, min_samples_leaf=1, random_state = 77, oob_score=True, max_features=0.45, criterion=\"mse\")\n#rf = RandomForestRegressor(n_estimators = 820, random_state = 77, oob_score=True, max_features=0.80, n_jobs=-1)\nrf.fit(train_features, train_target)\ntrain_pred = rf.predict(train_features)\n        \ny_pred = rf.predict(test_features)","206a4205":"linear_report(test_target,y_pred)","7d3bd281":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 5)) for feature, importance in zip(features_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]","a6efb427":"# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n\n# Make a line graph\nx_values = list(range(len(importances)))\nplt.plot(x_values, cumulative_importances, 'g-')\n\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances')\n\n#Draw line at 95% of importance retained\nplt.hlines(y = 0.98, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')","544906df":"# Find number of features for cumulative importance of 95%\n# Add 1 because Python is zero-indexed\nprint('Number of features for 98% importance:', np.where(cumulative_importances > 0.98)[0][0] + 1)","7efabfcd":"important_feature_names = [feature[0] for feature in feature_importances[0:177]]\nimportant_indices = [features_list.index(feature) for feature in important_feature_names]\nimportant_indices","c763b863":"important_train_features=train_features.iloc[:, important_indices]\nimportant_test_features=test_features.iloc[:, important_indices]","17a8c624":"important_train_features.shape","1bb9f4a6":"important_test_features.shape","21a138dc":"rf_imp = RandomForestRegressor(n_estimators=820, max_depth=None, n_jobs=-1, min_samples_split=2, min_samples_leaf=1, random_state = 77, oob_score=True, max_features=0.45, criterion=\"mse\")\nrf_imp.fit(important_train_features, train_target)\ntrain_pred_imp = rf_imp.predict(important_train_features)\n        \ny_pred_imp = rf_imp.predict(important_test_features)","20030fa8":"## Report\nlinear_report(test_target,y_pred_imp)","9ff95ca9":"print('Number of features for 99% importance:', np.where(cumulative_importances > 0.99)[0][0] + 1)","07e3726b":"important_feature_names = [feature[0] for feature in feature_importances[0:217]]\nimportant_indices = [features_list.index(feature) for feature in important_feature_names]","47b5b46c":"important_train_features=train_features.iloc[:, important_indices]\nimportant_test_features=test_features.iloc[:, important_indices]","66db7b44":"rf_imp = RandomForestRegressor(n_estimators=820, max_depth=None, n_jobs=-1, min_samples_split=2, min_samples_leaf=1, random_state = 77, oob_score=True, max_features=0.45, criterion=\"mse\")\nrf_imp.fit(important_train_features, train_target)\ntrain_pred_imp = rf_imp.predict(important_train_features)\n        \ny_pred_imp = rf_imp.predict(important_test_features)","3da13b10":"## Report\nlinear_report(test_target,y_pred_imp)","d225f1bb":"test=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","63bef0cc":"test.head()","366ff64d":"test.drop(drop_col, axis=1, inplace=True)\ntest.rename(columns={'3SsnPorch': \"threessnporch\"}, inplace=True)\ntest['YearRemodAdd'] = test['YearRemodAdd'].astype(str)\ntest.loc[(test.YearRemodAdd == test.YearBuilt), 'YearRemodAdd'] = 'No_remodel'\ntest.drop(['LowQualFinSF','BsmtHalfBath','threessnporch','MiscVal','MoSold','YrSold'], axis=1, inplace=True)\n\ntest.drop(['BsmtFinSF2','EnclosedPorch','BsmtUnfSF'], axis=1, inplace=True)\ntest.drop('1stFlrSF', axis=1, inplace=True)\ntest.drop('GarageYrBlt', axis=1, inplace=True)\n\n\n# missing values\n\ntest.drop(['Alley','Fence'], axis=1, inplace=True)\ntest['Fireplaces'] = test['Fireplaces'].astype('int32')\ntest['GarageType'].fillna('No_garage', inplace=True)\ntest['GarageQual'].fillna('No_garage', inplace=True)\ntest['GarageCond'].fillna('No_garage', inplace=True)\ntest['GarageFinish'].fillna('No_garage', inplace=True)\ntest['BsmtQual'].fillna('No_basement', inplace=True)\ntest['BsmtCond'].fillna('No_basement', inplace=True)\ntest['BsmtExposure'].fillna('No_basement', inplace=True)\ntest['BsmtFinType1'].fillna('No_basement', inplace=True)\ntest['BsmtFinType2'].fillna('No_basement', inplace=True)\ntest['MasVnrArea'].fillna(0, inplace=True)\ntest['MasVnrType'].fillna('No_Masonry', inplace=True)\ntest['FireplaceQu'].fillna('No_Fireplace', inplace=True)\ntest['LotFrontage'].fillna(test['LotFrontage'].median(), inplace=True)\n\ndel test['Id']\nfor i in df:\n    if (df[i].nunique())<20:\n        test[i] = test[i].astype(str)\ntest['MSSubClass']= test['MSSubClass'].astype(int)\ntest['PoolArea'] = test['PoolArea'].astype(int)\n# one hot encoding\n#final_test = pd.get_dummies(test)\n\n","513181fe":"test['GarageCars'].replace('nan',0, inplace=True)\ntest['GarageCars'] = test['GarageCars'].astype(float)\ntest['GarageCars'] = test['GarageCars'].astype(int)\ntest['GarageCars'] = test['GarageCars'].astype(str)","14b1fc8c":"test['BsmtFullBath'].replace('nan',0, inplace=True)\ntest['BsmtFullBath'] = test['BsmtFullBath'].astype(float)\ntest['BsmtFullBath'] = test['BsmtFullBath'].astype(int)\ntest['BsmtFullBath'] = test['BsmtFullBath'].astype(str)","51957880":"testmiss_val = test.isna().sum()\ntestmiss_val = pd.DataFrame(testmiss_val, columns=['miss'])\n\ntestmiss_val = testmiss_val.loc[(testmiss_val!=0).any(axis=1)]\ntestmiss_val","fb6c3f6b":"test['BsmtFinSF1'].fillna(test['BsmtFinSF1'].median(), inplace=True)\ntest['TotalBsmtSF'].fillna(test['TotalBsmtSF'].median(), inplace=True)\ntest['GarageArea'].fillna(test['GarageArea'].median(), inplace=True)\n","a41e4794":"testmiss_val = test.isna().sum()\ntestmiss_val = pd.DataFrame(testmiss_val, columns=['miss'])\n\ntestmiss_val = testmiss_val.loc[(testmiss_val!=0).any(axis=1)]\ntestmiss_val","eb614382":"test.shape","a71da1b9":"df.shape\n# the extra two columns are saleprice and log sale price","e0a315d0":"test.dtypes.value_counts()","12bb91b8":"# one hot encoding\nfinal_test = pd.get_dummies(test)","62f7b810":"col_1 = final_test.columns","0ba6ca3b":"col_2 = features.columns","a7759b3b":"len(col_2)","aa11766f":"for dd in col_1:\n    if dd in col_2:\n        pass\n    else:\n        print(dd, 'missing')","c5165ed6":"for dd in col_2:\n    if dd in col_1:\n        pass\n    else:\n        print(dd, 'missing')","721519af":"# removed the column MSZoning_nan","6090a06b":"final_test.drop('MSZoning_nan', axis=1, inplace=True)","399f908c":"final_test.drop(['Exterior1st_nan','Exterior2nd_nan','FullBath_4','KitchenQual_nan','TotRmsAbvGrd_13','TotRmsAbvGrd_15','Functional_nan','Fireplaces_4','GarageCars_5','SaleType_nan'], axis=1, inplace=True)","7c85c703":"df['HouseStyle'].nunique()","7dd71b4e":"test['HouseStyle'].nunique()","b053c317":"col_to_add = set( col_2 )-set(col_1)","48e42a66":"for c in col_to_add:\n    final_test[c]=0","31cedf4f":"col_1 = final_test.columns\ncol_2 = features.columns\nfor dd in col_1:\n    if dd in col_2:\n        pass\n    else:\n        print(dd, 'missing')","86ba411e":"for dd in col_2:\n    if dd in col_1:\n        pass\n    else:\n        print(dd, 'missing')","42ad81f1":"# now all the columns in train model and test dataset are matching.","2511c8c1":"set( train_features.columns ) - set( final_test.columns )","dbb532d7":"final_imp_test = final_test.loc[:, important_feature_names]","7f0d4bf8":"final_test_predictions = rf_imp.predict(final_imp_test)","0befb01c":"final_test_predictions","6d789675":"final_test_price = np.exp(final_test_predictions)","5af3fb05":"final_test_price","f5161d72":"ids=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","378bc7c8":"ids['Price'] = final_test_price","ec93d983":"ids.drop('SalePrice', axis=1, inplace=True)","81849624":"ids","16f05aac":"## Min samples leaf","f6351bce":"## Corelation between the continous varaibles","13d61519":"## The target variable is slightly right skewed. we will do a log trnasformation to make the target variable normally distributed","c1bb2f28":"## Train the Baseline Model\u00b6","396e6c4f":"## Feature importance","90943d70":"#### A little feature engineering before we go to the analysis","8c08d41a":"## Tuning number of trees","7108cc26":"### The below fucntion will give the p-values for each categorical variable against the target variable\n\n### If the value is less than 0.05 we will drop the column","1064c998":"## Max features","07ebc1a3":"# Predictions on test data","b29ff889":"## minimum splits","3c11e6a0":"## I noticed that the missing values in all the garage columns are same (81, except the Garagecar). lets see if these are same row indexes with na values. if so, that mean the na values can be imputed as 'no_garage'","79859935":"## Continous variables","67c3896a":"## Report","76b45e2f":"## One hot encoding for all the categorical variables","36c04328":"## Train and test split","b7b9254c":"## Predict on Train dataset","24c67a0d":"## Lets make predictions on test data set","27154476":"### The above graph shows the cumulative importance of the variables. the horizontal line is set at 98%, i.e the importance till this points is 98% and the rest of the variables contribute remaining 2%","53fdb0db":"# Univariate Analysis\n## 1. Categorical variables","8b666057":"### We are able to get almost same accuracy with just 177 features. lets increase the threshold value from 98 to 99 and see if we get different results","e4599d03":"## There are some variable which are numeric in data type but are actually categorical. We will deal with them separately ","b5dfa8b5":"## Train new model with important features","7bf66e99":"### We will convert the above numeric variables to object, so that univariate analysis for continous variables is easy.","9ecc1f71":"### We were able to increase the accuracy from 99.290% to 99.293%","93e8d43b":"### drop the row in electrical column where one value is missing.","98332116":"### Same for masvnrtype and masvnrarea","b266752d":"## Tuning Max depth","fcc93b8d":"### Now only one column with continous variable has missing values. we will impute it with median value","8a3a8917":"## Variable Identification\n\n### SalesPrice is the target variables and it is continous ","854ef2cc":"### Extract the features with high importance","d3d141c4":"## First we will have to make same imputations and treatment that were done on the train data","af4ce486":"### Similarly for BsmtQual and BsmtCond. ","c94a28e5":"## Missing values and treatment","80404e8f":"### Create new dataset with important features","1cb78438":"## Final model","cc8100e3":"### The column FireplaceQu is related to the Fireplaces. lets see if the missing values have any relationship.","dd31ff8d":"### Next step will be outlier detection\n\n#### I have later found that the model performs better without outlier treatment"}}