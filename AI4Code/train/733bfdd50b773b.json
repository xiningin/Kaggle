{"cell_type":{"f1b40a5d":"code","db1d69de":"code","de8edbba":"code","8e47c3b6":"code","d7bfda40":"code","d5c98792":"code","96ed46b6":"code","fd342e96":"code","73116220":"code","1c1fe771":"code","f8e8bedf":"code","a599eabf":"code","141d08d2":"code","d2f49ef7":"code","c3384b24":"code","b447f666":"code","ab09bf4f":"code","74b9c01a":"code","f4fced02":"code","7417480e":"code","a312a67d":"code","37bf888f":"code","4dce1ba5":"code","8ec6c412":"code","b3209fb9":"code","3ca78d1e":"code","36c634d2":"code","3a979a20":"code","7bce7713":"code","c975efaf":"code","43bfc451":"code","cc21bd4b":"code","0d58d1f6":"code","cca9a6cd":"code","ec99439d":"code","7eef0064":"code","8f1a634b":"code","e2244ba7":"code","b8dbfda5":"code","fbc8e74d":"code","a19d138b":"markdown","7c6d17df":"markdown","bce13748":"markdown","260c507f":"markdown","7910925c":"markdown","f304f42a":"markdown","96db1f5e":"markdown","ab0f519e":"markdown","2821cac5":"markdown","e0401eb6":"markdown","71274e45":"markdown","d786a668":"markdown","fed5d512":"markdown","5f1f194b":"markdown","7dc5d92a":"markdown","3306802d":"markdown","8771e44f":"markdown","c1ac52d2":"markdown","73cf389c":"markdown","74eff764":"markdown","f2af90ee":"markdown","9b4efd7a":"markdown","711f1083":"markdown","304f08a1":"markdown"},"source":{"f1b40a5d":"# Import the necessary packages used in this notebook\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objects as go\nimport plotly\ninit_notebook_mode(connected=True) #do not miss this line\n\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,recall_score,precision_recall_curve,average_precision_score\nfrom sklearn.utils import resample\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","db1d69de":"datafr = pd.read_csv(\"..\/input\/creditcard.csv\", error_bad_lines=False)","de8edbba":"display(datafr.shape)","8e47c3b6":"display(datafr.head(10))","d7bfda40":"display(datafr.tail(10))","d5c98792":"hours = (datafr['Time']\/3600).astype(int)\ndatafr['Hours'] = hours\n\ndays = (datafr['Time']\/86400).astype(int)\ndatafr['Days'] = days","96ed46b6":"bins = [0,100,1000,5000,10000,20000, 30000]\nlabels = [1,2,3,4,5,6]\ndatafr['binned'] = pd.cut(datafr['Amount'], bins=bins, labels=labels)\ndatafr.head(10)","fd342e96":"f, axes = plt.subplots(1, 2, sharey=True, figsize=(15, 8))\nsns.boxplot(x=\"binned\", y=\"Amount\", hue=\"Class\", data=datafr[datafr['Class']==0], palette='Blues', ax=axes[0])\naxes[0].set_title('BoxPlot for {}'.format(\"Class 0: Not Fraudulent\"))\nsns.boxplot(x=\"binned\", y=\"Amount\", hue=\"Class\", data=datafr[datafr['Class']==1], palette='Purples', ax=axes[1])\naxes[1].set_title('BoxPlot for {}'.format(\"Class 1: Fraudulent\"))","73116220":"plt.figure(figsize=(14,6))\nsns.set(style=\"darkgrid\")\nsns.countplot(x='binned',data = datafr, hue = 'Class',palette='BuPu')\nplt.title(\"Count Plot of Transactions per each amount bin\\n\", fontsize=16)\nsns.set_context(\"paper\", font_scale=1.4)\nplt.show()","1c1fe771":"plt.figure(figsize=(14,6))\nsns.set(style=\"darkgrid\")\nsns.countplot(x='Hours',data = datafr, hue = 'Class',palette='BuPu')\nplt.title(\"Count Plot of Transactions per each Hour\\n\", fontsize=16)\nsns.set_context(\"paper\", font_scale=1.4)\nplt.show()","f8e8bedf":"print(\"Fraudulent Transactions:\", len(datafr[datafr['Class']==1]))\nprint(\"Usual Transactions:\", len(datafr[datafr['Class']==0]))","a599eabf":"fraud =len(datafr[datafr['Class']==1])\nnotfraud = len(datafr[datafr['Class']==0])\n\n# Data to plot\nlabels = 'Fraud','Not Fraud'\nsizes = [fraud,notfraud]\n\n# Plot\nplt.figure(figsize=(7,6))\nplt.pie(sizes, explode=(0.1, 0.1), labels=labels, colors=sns.color_palette(\"BuPu\"),\nautopct='%1.1f%%', shadow=True, startangle=0)\nplt.title('Pie Chart Ratio of Transactions by their Class\\n', fontsize=16)\nsns.set_context(\"paper\", font_scale=1.2)","141d08d2":"y = datafr['Class']\nX = datafr.drop(['Time','Class', 'binned'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy is {:.2f}%\".format(accuracy * 100.0))","d2f49ef7":"import shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n","c3384b24":"model = XGBClassifier()\nmodel.fit(X_train[['V1','V2','V3']], y_train)\n\ny_pred = model.predict(X_test[['V1','V2','V3']])\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy is {:.2f}%\".format(accuracy * 100.0))","b447f666":"# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(y_test,y_pred)\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","ab09bf4f":"from sklearn.utils import resample\n# Separate input features and target\nY = datafr.Class\nX = datafr.drop(['Time','Class','binned'], axis=1)\n\n# setting up testing and training sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2727)\n\n# concatenate our training data back together\nX = pd.concat([X_train, Y_train], axis=1)","74b9c01a":"# separate minority and majority classes\nnot_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=2727) # reproducible results\n\n# combine majority and oversampled minority\noversampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\noversampled.Class.value_counts()","f4fced02":"# trying xgboost again with the balanced dataset\ny_train = oversampled.Class\nX_train = oversampled.drop('Class', axis=1)\n\nupsampled = XGBClassifier()\nupsampled.fit(X_train, y_train)\n\n# Predict on test\nupsampled_pred = upsampled.predict(X_test)\n\n# predict probabilities\nprobs = upsampled.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]","7417480e":"explainer = shap.TreeExplainer(upsampled)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","a312a67d":"# Checking accuracy\naccuracy = accuracy_score(Y_test, upsampled_pred)\nprint(\"Test Accuracy is {:.2f}%\".format(accuracy * 100.0))","37bf888f":"# f1 score\nf1_over = f1_score(Y_test, upsampled_pred)\nprint(\"F1 Score is {:.2f}%\".format(f1_over))","4dce1ba5":"from sklearn.metrics import auc\n# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(Y_test, probs)\n# calculate precision-recall AUC\nauc_over = auc(recall, precision)\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the precision-recall curve for the model\nplt.plot(recall, precision, marker='.')\nplt.title(\"Precison-Recall Curve for XGBoost with AUC score: {:.3f}\".format(auc_over))\n# show the plot\nplt.show()","8ec6c412":"# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,upsampled_pred)\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","b3209fb9":"# still using our separated classes fraud and not_fraud from above\n\n# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.Class.value_counts()","3ca78d1e":"# trying xgboost again with the balanced dataset\ny_train = downsampled.Class\nX_train = downsampled.drop('Class', axis=1)\n\nundersampled = XGBClassifier()\nundersampled.fit(X_train, y_train)\n\n# Predict on test\nundersampled_pred = undersampled.predict(X_test)\n# predict probabilities\nprobs = undersampled.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]","36c634d2":"explainer = shap.TreeExplainer(undersampled)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","3a979a20":"# Checking accuracy\naccuracy = accuracy_score(Y_test, undersampled_pred)\nprint(\"Test Accuracy is {:.2f}%\".format(accuracy * 100.0))","7bce7713":"# f1 score\nf1_under = f1_score(Y_test, undersampled_pred)\nprint(\"F1 Score is {:.2f}%\".format(f1_under))","c975efaf":"from sklearn.metrics import auc\n# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(Y_test, probs)\n# calculate precision-recall AUC\nauc_under = auc(recall, precision)\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the precision-recall curve for the model\nplt.plot(recall, precision, marker='.')\nplt.title(\"Precison-Recall Curve for XGBoost with AUC score: {:.3f}\".format(auc_under))\n# show the plot\nplt.show()","43bfc451":"# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,undersampled_pred)\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","cc21bd4b":"from imblearn.over_sampling import SMOTE\n\n# Separate input features and target\nY = datafr.Class\nX = datafr.drop(['Time','Class','binned'], axis=1)\n\n# setting up testing and training sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2727)\n\nsm = SMOTE(random_state=2727, ratio=1.0)\nX_train, Y_train = sm.fit_sample(X_train, Y_train)","0d58d1f6":"X_train = pd.DataFrame(data=X_train)\nX_train.columns = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Hours', 'Days']\nY_train = pd.Series(Y_train)","cca9a6cd":"smote = XGBClassifier()\nsmote.fit(X_train, Y_train)\n\n# Predict on test\nsmote_pred = smote.predict(X_test)\n# predict probabilities\nprobs = smote.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]","ec99439d":"explainer = shap.TreeExplainer(smote)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","7eef0064":"# Checking accuracy\naccuracy = accuracy_score(Y_test, smote_pred)\nprint(\"Test Accuracy is {:.2f}%\".format(accuracy * 100.0))","8f1a634b":"# f1 score\nf1_smote = f1_score(Y_test, smote_pred)\nprint(\"F1 Score is {:.2f}%\".format(f1_smote))","e2244ba7":"from sklearn.metrics import auc\n# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(Y_test, probs)\n# calculate precision-recall AUC\nauc_smote = auc(recall, precision)\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the precision-recall curve for the model\nplt.plot(recall, precision, marker='.')\nplt.title(\"Precison-Recall Curve for XGBoost with AUC score: {:.3f}\".format(auc_smote))\n# show the plot\nplt.show()","b8dbfda5":"# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,smote_pred)\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Blues', fmt = 'd')\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","fbc8e74d":"# F1 Score list for all models\nf1 = [f1_over, f1_under, f1_smote]\n# AUC Score list for all models\nauc = [auc_over, auc_under, auc_smote]\n# Name List of ML Models used\nmodels = ['Over-Sampling', 'Under-Sampling', 'SMOTE']\ny_pos = np.arange(len(models)) #Position = 0,1,2\n\n# Plot F1 Score\nplt.figure(figsize=(10, 6))  \nplt.bar(y_pos, f1, align='center', alpha=0.8, color=sns.color_palette(\"PuBu\"))\nplt.xticks(y_pos, models)\nplt.ylabel('F1 Score')\nplt.title('Performance based on F1 Score')\n\n# Plot AUC Score\nplt.figure(figsize=(10, 6))  \nplt.bar(y_pos, auc, align='center', alpha=0.8, color=sns.color_palette(\"PuBu\"))\nplt.xticks(y_pos, models)\nplt.ylabel('AUC Score')\nplt.title('Performance based on AUC Score')","a19d138b":"### Create a new binned column based on Amount","7c6d17df":"### Read the dataset and save to variable 'datafr'","bce13748":"**Note: Always split the dataset into test and train sets BEFORE trying any resampling techniques!**","260c507f":"**Restructure X_train & Y_train to execute XGBoost algorithm**","7910925c":"The data is provided for 2 days transaction. For some instances we have half-second transaction data. Since 172792 seconds approxs to (60x60x24) which equals to 2 days","f304f42a":"**Again trying to run a predictive model on first 3 features \"V1\", \"V2\", \"V3\" we should notice a change in model's performance**","96db1f5e":"# Understanding the Problem\n### The metric trap\nOne of the major issues that novice users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory.\n","ab0f519e":"### 1: Random over-sampling minority class\nOversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don\u2019t have a ton of data to work with.","2821cac5":"# SMOTE\nSynthetic Minority Over-sampling Technique has been designed to generate new samples that are coherent with the minor class distribution. The main idea is to consider the relationships that exist between samples and create new synthetic points along the segments connecting a group of neighbors. Below is the python code for implementing SMOTE.\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png)","e0401eb6":"## Variable Description\nFeatures V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. <br><br> Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. <br><br> The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","71274e45":"![](https:\/\/img.huffingtonpost.com\/asset\/5cdec4402400006f077f0af6.jpeg?ops=800_450)","d786a668":"**The dataset is highly imbalanced which could lead us to create an algorithm resulting in predicting only 1 type of class. Thus, we have to try different strategies as a solution to imbalance class problem.**\nHere are the few techniques that can be used for such a problem:\n1. Try Changing Performance Metric\n2. Try Resampling Dataset\n3. Try Generating Synthetic Samples\n4. Try Different Algorithms\n\n[Reference 1](https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/)\n<br>\n[Reference 2](https:\/\/towardsdatascience.com\/methods-for-dealing-with-imbalanced-data-5b761be45a18)","fed5d512":"After resampling we have an equal ratio of data points for each class! Let\u2019s try our above XGBoost Classifier again with the balanced training data.","5f1f194b":"## <a id='1'> Exploratory Data Analysis <\/a>","7dc5d92a":"The model accuracy hasn't differed by much suggesting that the achieved acccuracy is just an illusion which is caused due to imbalanced class dataset. Below we can actually see how the proportion of observations are actually predicted for each class using Confusion Matrix.","3306802d":"# Resampling Techniques\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)","8771e44f":"# Conclusion\nSo far we tried using various performance metrics like Confusion matrix, F1-Score, Precision-Recall curve for different techniques like over-sampling of minority class, under-sampling of majority class and SMOTE (Synthetic Minority Over-Sampling Technique). Based on our evaluation metrics we found that undersampling of majority class resulted in poor poerformance when compared to Over-Sampling techniques and SMOTE. It's still hard to pick a winner here. The algorithm we used in all the scenario was XGBoost.\n<br><br>\n**Further Improvements: To further improve the model, below options can be considered:**\n* Try using Deep Learning algorithms like CNN, or stacked or hybrid machine learning algorithms\n* Try using variants of SMOTE.\n* Tuning of hyper-parameters(learning rate, max-depth, etc.) of the above models.","c1ac52d2":"### Displaying the structure of dataset","73cf389c":"### 2: Random under-sampling majority class\nUndersampling can be defined as removing some observations of the majority class. Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback is that we are removing information that may be valuable. This could lead to underfitting and poor generalization to the test set.","74eff764":"**Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data.**","f2af90ee":"### Confusion matrix\n\nAn interesting way to evaluate the results is by means of a confusion matrix, which shows the correct and incorrect predictions for each class. In the first row, the first column indicates how many classes 0 were predicted correctly, and the second column, how many classes 0 were predicted as 1. In the second row, we note that all class 1 entries were erroneously predicted as class 0.\n\nTherefore, the higher the diagonal values of the confusion matrix the better, indicating many correct predictions.\n\n","9b4efd7a":"# Guide to Fraud Detection\n\n## Introduction\nIn this kernel we will explore & implement the scope of credit card fraud detection using predictive models to identify the level of accuracy of labelling a transaction as regular or fraud. (In the dataset we're using scaled features due to privacy reasons)\n\n## Scope\n1. <a href='#1'> Exploratory Data Analysis <\/a>\n2. <a href='#2'> Understanding Imbalance Data <\/a>\n3. <a href='#3'> Model Prediction for transaction classification <\/a>\n4. <a href='#4'> Result Evaluation <\/a>\n\n\n## Focus Points\n* Never implement testing directly on oversampled and undersampled data.\n* Resampling (Oversampling or Undersampling) should be done during cross-validation only.\n* Always implement evaluation metrics like F1 score, Confusion Matrix, AUC score over Accuracy.\n","711f1083":"# Change the Performance Metric\nAs we saw above, accuracy is not the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n* Confusion Matrix: a table showing correct predictions and types of incorrect predictions.\n* Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier\u2019s exactness. Low precision indicates a high number of false positives.\n* Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier\u2019s completeness. Low recall indicates a high number of false negatives.\n* F1: Score: the weighted average of precision and recall.","304f08a1":"### Feature Engineering"}}