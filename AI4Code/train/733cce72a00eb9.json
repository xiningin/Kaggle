{"cell_type":{"b98e4b7c":"code","51f90a16":"code","16f79db0":"code","cc82040d":"code","4f4c778c":"code","692648b8":"code","88630a8b":"code","1b96f748":"code","f0823146":"code","fe2af712":"code","e8424130":"code","4257626d":"code","f8f7a2ff":"code","41ebc2a2":"code","03e58762":"code","e6856143":"code","bd6e8d70":"code","01774e8b":"markdown","13ef9319":"markdown","18001610":"markdown","31fcf785":"markdown","65b427e4":"markdown","0db1d8b5":"markdown","925b2e35":"markdown","db25f11b":"markdown","e2d46f2c":"markdown","e5bb098c":"markdown","c6e2a1df":"markdown","d1dae9f5":"markdown","53563cd5":"markdown","1b90c6f9":"markdown","ebc24021":"markdown","43348d83":"markdown","de54e7cf":"markdown","0a22e174":"markdown","0f84f8d7":"markdown","b27b5b37":"markdown","96bf0aaa":"markdown","1689586a":"markdown","76af55ed":"markdown","7365b909":"markdown"},"source":{"b98e4b7c":"import re \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport scipy\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize \nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_predict, cross_val_score,train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout,SpatialDropout1D,Embedding\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","51f90a16":"#import disaster tweets dataframes\ndf_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\n#take a look at dataset sizes\nprint('train shape:', df_train.shape)\nprint('test shape:',df_test.shape)","16f79db0":"#take a look at the train dataset\npd.set_option('display.max_colwidth', 300) #set width of columns to display full tweet\ndf_train.head()","cc82040d":"#take a look at the test dataset\ndf_test.head()","4f4c778c":"target_count=df_train['target'].value_counts(dropna=False) #count target\nplt.figure(figsize=(5,5)) #set figure size\nplt.pie([target_count[0], target_count[1]],labels=['not disaster', 'disaster'], shadow=False)#pie chart","692648b8":"# percent of location appearing grouped by target = 1\/0\nlocation_count_1=pd.DataFrame(df_train[df_train['target']==1]['location'].value_counts(dropna=False)) #find only disaster tweets\nlocation_count_1 = location_count_1.reset_index() #reformate\nlocation_count_1.columns=['location','count'] #rename headers\nlocation_count_1['percent']=location_count_1['count']\/location_count_1['count'].sum() #percentage\n\nlocation_count_0=pd.DataFrame(df_train[df_train['target']==0]['location'].value_counts(dropna=False)) #only non-disaster\nlocation_count_0 = location_count_0.reset_index() #reformat\nlocation_count_0.columns=['location','count'] #headers\nlocation_count_0['percent']=location_count_0['count']\/location_count_0['count'].sum() #percentage\n\n#make separate bar charts for taget =1\/0\nfig,a =  plt.subplots(2,1,figsize=(15,10)) #make 2 subplots \nfig.tight_layout(pad=12) #padding between subplots\nprint('number of different locations (disaster):', location_count_1.shape[0]) \nsns.barplot(x='location',y='count', data=location_count_1[:20], palette='Spectral', ax=a[0]) # barplot for top 20 most common \na[0].set_title('target=1') \na[0].tick_params(labelrotation=45)\n\nprint('number of different locations (non disaster):', location_count_0.shape[0])\nsns.barplot(x='location',y='count', data=location_count_0[:20], palette='Spectral', ax=a[1]) # barplot for top 20 most common \na[1].set_title('target=0')\na[1].tick_params(labelrotation=45)\n\nprint(location_count_0.head(1))\nprint(location_count_1.head(1))","88630a8b":"# percent of keywords appearing grouped by target = 1\/0\nkey_count_1=pd.DataFrame(df_train[df_train['target']==1]['keyword'].value_counts(dropna=False)) # only disaster\nkey_count_1 = key_count_1.reset_index()\nkey_count_1.columns=['keyword','count']\nkey_count_1['percent']=key_count_1['count']\/key_count_1['count'].sum()\n\nkey_count_0=pd.DataFrame(df_train[df_train['target']==0]['keyword'].value_counts(dropna=False)) #only non disaster\nkey_count_0 = key_count_0.reset_index()\nkey_count_0.columns=['keyword','count']\nkey_count_0['percent']=key_count_0['count']\/key_count_0['count'].sum()\n\n\n#make separate bar charts for taget =1\/0\nfig,a =  plt.subplots(2,1,figsize=(15,10)) #make 2 subplots for target=1, target=0\nfig.tight_layout(pad=12)\nprint('number of different keywords (disaster):', key_count_1.shape[0])\nsns.barplot(x='keyword',y='count', data=key_count_1[:20], palette='Spectral', ax=a[0]) # barplot for top 20 most common \na[0].set_title('target=1')\na[0].tick_params(labelrotation=45)\n\nprint('number of different keywords (non disaster):', key_count_0.shape[0])\nsns.barplot(x='keyword',y='count', data=key_count_0[:20], palette='Spectral', ax=a[1]) # barplot for top 20 most common \na[1].set_title('target=0')\na[1].tick_params(labelrotation=45)","1b96f748":"# make a new variable for number of characters\ndf_train['characters']=df_train['text'].str.len()\n\n# split dataset by target\nchar_1=df_train[df_train['target']==0]['characters']\nchar_0=df_train[df_train['target']==1]['characters']\n\n# t test\nclengtht, clengthp=scipy.stats.ttest_ind(char_1, char_0)\nprint('T Test number of characters by target t={:.2f},p={:.2f}'.format(clengtht, clengthp))\n\n#histograms\nfig,a =  plt.subplots(1,3,figsize=(15,5)) #make 3 subplots for target=1, target=0, and complete sample\nsns.distplot(char_1,ax=a[0], color='purple')\na[0].set_title('target = 1')\nsns.distplot(char_0,ax=a[1], color='blue')\na[1].set_title('target = 0')\nsns.distplot(df_train['characters'],ax=a[2], color='green')\na[2].set_title('target = 0 and target = 1')","f0823146":"# make a new variable for number of words\ndf_train['words']=df_train['text'].apply(lambda x: len(str(x).split())) #split by space to turn tweet into words\n\n# split dataset by target\nw_1=df_train[df_train['target']==0]['words']\nw_0=df_train[df_train['target']==1]['words']\n\n# t test\nwlengtht, wlengthp=scipy.stats.ttest_ind(w_1, w_0)\nprint('T Test number of words by target t={:.2f},p={:.2f}'.format(wlengtht, wlengthp))\n\n#histograms\nfig,a =  plt.subplots(1,3,figsize=(15,5)) #make 3 subplots for target=1, target=0, and complete sample\nsns.distplot(w_1,ax=a[0], color='purple')\na[0].set_title('target = 1')\nsns.distplot(w_0,ax=a[1], color='blue')\na[1].set_title('target = 0')\nsns.distplot(df_train['words'],ax=a[2], color='green')\na[2].set_title('target = 0 and target = 1')","fe2af712":"# make a new variable for number of characters\ndf_train['wlen']=df_train['text'].apply(lambda x: sum([len(a) for a in str(x).split()])\/len(str(x).split()))\n#split by space to turn tweet into words, use list comprehension to get total char length, divide by word list length\n\n# split dataset by target\nwl_1=df_train[df_train['target']==0]['wlen']\nwl_0=df_train[df_train['target']==1]['wlen']\n\n# t test\nwllengtht, wllengthp=scipy.stats.ttest_ind(wl_1, wl_0)\nprint('T Test number of words by target t={:.2f},p={:.2f}'.format(wllengtht, wllengthp))\n\n#histograms\nfig,a =  plt.subplots(1,3,figsize=(15,5)) #make 3 subplots for target=1, target=0, and complete sample\nsns.distplot(wl_1,ax=a[0], color='purple')\na[0].set_title('target = 1')\nsns.distplot(wl_0,ax=a[1], color='blue')\na[1].set_title('target = 0')\nsns.distplot(df_train['wlen'],ax=a[2], color='green')\na[2].set_title('target = 0 and target = 1')","e8424130":"# putting all texts across rows together as a big string variable \nalltextdisaster=' '.join(set([text for text in df_train[df_train['target']==1]['text']])) # disaster\nalltextnondisaster=' '.join(set([text for text in df_train[df_train['target']==0]['text']])) # non disaster\n\n# build word clouds \nwc1 = WordCloud(background_color=\"white\", max_words=200, width=1000, height=800).generate(alltextdisaster)\nwc2 = WordCloud(background_color=\"white\", max_words=200, width=1000, height=800).generate(alltextnondisaster)\n\n# plotting word clouds\nfig,a =  plt.subplots(1,2,figsize=(20,10))\na[0].imshow(wc1, interpolation='bilinear')\na[0].axis(\"off\")\na[0].set_title('disaster tweet word cloud')\n\na[1].imshow(wc2, interpolation='bilinear')\na[1].axis(\"off\")\na[1].set_title('nondisaster tweet word cloud')\nplt.show()","4257626d":"def removePunctuation(text):\n    return \"\".join([c for c in text if c not in string.punctuation])\nprint('remove punctuation:', removePunctuation(\"It's me!!!! :\/\"))\n\ndef removeNumber(text):\n    return \"\".join([c for c in text if not c.isdigit()])\nprint('remove numbers:', removeNumber(\"123 abc\"))\n\ndef removeHTML(text):\n    return re.sub(r'<.*?>','', text) # match <tag> minimally\nprint('remove HTML tags:', removeHTML(\"<h1>heading<\/h1><p attribute=''>tag\"))\n\ndef removeURL(text):\n    return re.sub(r'https?:\/\/\\S+|www\\.\\S+', '', text) # match url patterns\nprint('remove url:', removeURL(\"url https:\/\/www.kaggle.com kaggle\"))\n\ndef removeEmoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE) # compiling all emojis as a reg ex expression\n    return emoji_pattern.sub(r'', text)\nprint('remove emoji:', removeEmoji('Sad\ud83d\ude14'))\n\ndef lowerCase(text): \n    return text.lower()\nprint('lower case:', lowerCase('crazy NoiSy Town!'))\n\ndef removeStopwords(text):\n    return ' '.join([word for word in text.split() if word not in stopwords.words('english')])\nprint('remove stop words:', removeStopwords('I am a cup of tea'))\n\n\nPstemmer=PorterStemmer()\ndef stemText(text):\n    return ' '.join([Pstemmer.stem(token) for token in text.split()])\nprint('stem Text:', stemText('Word clouds are visualizations of words in which the sizes of words reflect the relative importance of words'))\n\n\n# put all the above cleaning functions into one function\ndef cleanTextData(text):\n    text=lowerCase(text)\n    text=removePunctuation(text)\n    text=removeURL(text)\n    text=removeEmoji(text)\n    text=removeNumber(text)\n    text=removeHTML(text)\n    text=removeStopwords(text)\n    text=stemText(text)\n    return text\nprint('clean:', cleanTextData('Word clouds are visualizations of words in which the sizes of words reflect the relative importance of words <a>link https:\/\/www.kaggle.com<a\/> ttps:\/\/www.kaggle.com 321\ud83d\ude14'))\n\n#clean train and test\ndf_train['cleaned_text']=df_train['text'].apply(lambda x: cleanTextData(x))\ndf_test['cleaned_text']=df_test['text'].apply(lambda x: cleanTextData(x))\ndf_train.head(10)","f8f7a2ff":"train_vectors=TfidfVectorizer().fit_transform(df_train['cleaned_text'])\ntest_vectors=TfidfVectorizer().fit_transform(df_test['cleaned_text'])\ny=df_train['target']\nX=train_vectors","41ebc2a2":"#Multinomial NB\nmultinomialnb_classifier = MultinomialNB()\nprint('cv f1 scores:',cross_val_score(multinomialnb_classifier,X, y,scoring='f1', cv=10)) # 10 folds cross validation\n# Confusion Matrix Visualization\nmnb_pred=cross_val_predict(multinomialnb_classifier, X, y,cv=10)\nmultinomialnb_classifier_cm=confusion_matrix(mnb_pred,y)\nprint('correct 0: {}, correct 1: {}, incorrect: {}'.format(multinomialnb_classifier_cm[0][0],multinomialnb_classifier_cm[1][1],multinomialnb_classifier_cm[1][0]+multinomialnb_classifier_cm[0][1]))\nsns.heatmap(multinomialnb_classifier_cm, cmap='PuBu')\n","03e58762":"#GaussianNB\ngnb_classifier = GaussianNB()\nX_gnb=X.toarray() #converting X to dense, required by GaussianNB\nprint('cv f1 scores:',cross_val_score(gnb_classifier,X_gnb, y,scoring='f1', cv=10))\n# Confusion Matrix Visualization\ngnb_pred=cross_val_predict(gnb_classifier, X_gnb, y,cv=10)\ngnb_classifier_cm=confusion_matrix(gnb_pred,y)\nprint('correct 0: {}, correct 1: {}, incorrect: {}'.format(gnb_classifier_cm[0][0],gnb_classifier_cm[1][1],gnb_classifier_cm[1][0]+gnb_classifier_cm[0][1]))\nsns.heatmap(gnb_classifier_cm, cmap='PuBu')\n","e6856143":"logisticreg_classifier = LogisticRegression()\nprint('cv f1 scores:',cross_val_score(logisticreg_classifier,X, y,scoring='f1', cv=10))\n# Confusion Matrix Visualization\nlr_pred=cross_val_predict(logisticreg_classifier, X, y,cv=10)\nlogisticreg_classifier_cm=confusion_matrix(lr_pred,y)\nprint('correct 0: {}, correct 1: {}, incorrect: {}'.format(logisticreg_classifier_cm[0][0],logisticreg_classifier_cm[1][1],logisticreg_classifier_cm[1][0]+logisticreg_classifier_cm[0][1]))\nsns.heatmap(logisticreg_classifier_cm, cmap='PuBu')\n\n# multinomialnb_classifier.fit(X_train, y_train)\n# multinomialnb_classifier_pred = multinomialnb_classifier.predict(X_test)","bd6e8d70":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train['cleaned_text'].values)\nX = tokenizer.texts_to_sequences(df_train['cleaned_text'].values)\nX = pad_sequences(X)\n\n\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index) + 1, 128 ,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n\nmodel.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test,y_test))","01774e8b":"#### Acknowledgements\n\nThis notebook is inspired by and contains ideas from the following kaggle kernels. If you find this notebook helpful, please check out the following as well.\n\n* [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert) \n* [\ud83d\udcd9 CheatSheet: Text Data](https:\/\/www.kaggle.com\/prestonfan\/cheatsheet-text-data)\n* [Getting started with NLP - A general Intro](https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-a-general-intro) \n* [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove) \n* [Start From Here : Disaster Tweets EDA+Basic model](https:\/\/www.kaggle.com\/ratan123\/start-from-here-disaster-tweets-eda-basic-model)\n* [Disaster Tweets: EDA | NLP | Classifier Models](https:\/\/www.kaggle.com\/kushbhatnagar\/disaster-tweets-eda-nlp-classifier-models)\n* [Keras LSTM](https:\/\/www.kaggle.com\/adamlouly\/simple-keras-lstm-for-warming-up)\n* [A Detailed Explanation of Keras Embedding Layer](https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer)\n* [LSTM baseline](https:\/\/www.kaggle.com\/bibek777\/lstm-baseline)\n","13ef9319":"For this dataset, multinomial NB has better accuracies than gaussian NB.","18001610":"[back to top](#top)\n\n<a id=\"import\"><\/a>\n#  Import Libraries and Datasets\n\n<hr>","31fcf785":"[back to top](#top)\n\n<a id='keyword'><\/a>\n### Keyword\n\nSimilarly, we can look at keyword distribution.\n* The distrbution of keywords also differed by target.\n* There are not as much missing values as location but missing values still exist. ","65b427e4":"#### ***Using naive bayes***\n\nIn application, multinomial and guassian are two popular forms of naive bayes. The difference of the two comes from the assumption of feature distributions. As the names implied, multinomial naive bayes assumes a multinomial (discrete) probability mass function of data, whereas gaussian naive bayes assumes a gaussian (continuous) probability distribution of data. \n\nWe will use sklearns to run both a multinomial naive bayes model and a gaussian naive bayes model which is fairly simple to run. We will also use sklearn's cross validation, and f1 scoring metric. A confusion matrix also helps visualizing algorithm performance.","0db1d8b5":"#### ***Gaussian NB***","925b2e35":"#### ***Multinomial NB***","db25f11b":"[back to top](#top)\n\n\n<a id='eval'><\/a>\n## Evaluation and Notes\n\n<a id='f1'><\/a>\n### F1 score\nSklearn's f1 scoring criteria will be used to evaluate models. F1 score is the evaluation criterion specified by the *Real or Not? NLP with Disaster Tweets* challange. As pointed out in the challenge, f1 can be calculated as $F1=2\u2217\\frac{precision\u2217recall}{precision+recall}$. \n\n* Precision is defined as $\\frac{TP}{TP+FP}$.\n* Recall is defined as $\\frac{TP}{TP+FN}$.\n* TF(True Positive) means both prediction and actual label are 1.\n* TN(True Negative) means both prediction and actual label are 0.\n* FP(False Positive) means prediction is 1 but true label is 0.\n* FN(False Negative) means prediction is 0 but true label is 1.\n\nOften in research, TF, TN, FP, FN are related to type I and type II errors. FP is a false alert and is refered to as type I error and is related to statistical significance alpha, FN is referred to as type II error and is related to statistical power beta.\n\n(off topic) An interesting article about statistical significance and power above to read is Cohen (1990) - The earth is round (p<0.05).\n\n\n\n### Cross Validation\n\nCross Validation validates an algorithm's performance through sampling. K-fold cross validation will split the randomized sample into k groups. When validating, there will be k iterations, for each iteration, a group will be used for testing whereas the remaining  k-1 groups will be used for training. Sklearn's 10 fold cross validation is used in the following models to get f1 accuracies of the training dataset. \n","e2d46f2c":"[back to top](#top)\n\n<a id=\"EDA\"><\/a>\n#  Exploratory Data Analysis \n\n<hr>","e5bb098c":"[back to top](#top)\n\n<a id=\"preprocess\"><\/a>\n#  Data Preprocessing \n\n<hr>\n\nText processing is an important step in NLP. Here we will compile a cleanTextData() function that removes punctuations, numbers, html tags, urls, and emojis. This step also turns everything into lowercase. Removing stopwrods and stemming are also common techniques used in NLP data cleaning. \n\nStopwords are words often ignored in text processing and the ommission of which does not generally change meanings of texts. Here, we are using stopwords in the nltk library, which included 'i', 'me', 'as', 'until', 'so', 'than', 'too', 'very' etc.\n\nStemming is the process of converting words into their root forms. This step is helpful because there could be variants of the same word (eat and eating) in the text. We are using the nltk porterstemmer here to stem the words which follows algorithms by Porter (1980).\n\nNote that list comprehension and regular expressions are also used here in data preprocessing. There are other libraries which can automatically process punctuations, numbers, html tags etc as well.\n\n\n1. List comprehension: list comprehension is a simpler and more concise way create lists. The format of list comprehension is `[ statement to generate list ]`.\n\n    Example: `templist=[c for c in text if c not in string.punctuation]`\n\n    This is the same as: \n    ```\n    templist=[]\n        for c in text:\n            if c not in string.punctuation:\n                templist.append(c)\n    ```\n    \n2. Regular expression: regular expressions are text sequence pattern matching blocks. \n    * re.sub(pattern, replacement, string): replace *pattern* in *string* with *replacement*\n    * re.complile(pattern): compile the *pattern* into a regular expression object which could later be used to match\n    * r'pattern': convert *pattern* to raw string, which does not compile escape sequences i.e. '\\n'\n    * some regular expression operators used in the notebook:\n        * `?`  match 0 or 1 instance of the charater preceding. i.e. 123? will match 12 or 123\n        * `.`  match any character\n        * `*` match as many consecutive instances of preceding character as possible\n        * `.*?`: match as few characters as possible\n        * `+`: match 1 or more of the preceding character\n        * `\\S`: match any non white space character\n        * `1|2` : match 1 or 2\n        * `\\` : escape characters\n    * `r'<.\\*?>'` will match `<anytext>` minimally, so for `'<a><\/a>'`, the pattern matches `'<a>'` and `'<a\/>'` separately instead of matching the whole thing   \n    * `r'https?:\/\/\\S+|www\\.\\S+'` will match `http(s):\/\/www.AnyNonWhiteSpaceText` and `http(s):\/\/AnyNonWhiteSpaceText`\n    * you can test regular expressions online using websites such as https:\/\/regex101.com\/.\n\n","c6e2a1df":"[back to top](#top)\n\n<a id='dataset'><\/a>\n### First Look at Datasets\nNow we will import train and test datasets and take an initial look at the shapes of the datasets. \n\nThe training set has 7613 rows and 5 columns (id, keyword, location, text, and target).\n- id: a unique identifier for each tweet\n- text: the text of the tweet\n- location: the location the tweet was sent from (may be blank)\n- keyword: a particular keyword from the tweet (may be blank)\n- target: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\nThe test set has 3243 rows and 4 columns (id, keyword, location, and text).\n- id: a unique identifier for each tweet\n- text: the text of the tweet\n- location: the location the tweet was sent from (may be blank)\n- keyword: a particular keyword from the tweet (may be blank)\n\nThe test dataset does not contain the target variable. This need to be predicted.\n\n","d1dae9f5":"[back to top](#top)\n\n<a id='word'><\/a>\n#### ***Number of words***\n\nSimilarly, we can explore number of words in tweets.\n\n* The result of the t-test gives t=-3.49 and p<0.05. Number of characters does differ by target.   ","53563cd5":"<a id='target'><\/a>\n### Target\n\nHow many tweets are disaster tweets and how many are non disaster tweets? The following cell will compute these numbers and display proportions using a pie chart.\n\n|type| count | type | count|\n|------------|------|---------|-----|\n|Non disaster| 4342 | Disaster| 3271|","1b90c6f9":"[back to top](#top)\n\n<a id='text'><\/a>\n### Text\n\nFor natural language processing, text EDA tast is vital. Some commonly used text features to explore are word frequencies, word length, sentence length etc. Here, we will explore 3 features--number of characters, number of words, and word length. We will also complete a wordcloud to visualize the texts.\n\nMore information on EDA for NLP can be found on this blog https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools. \n\n<a id='char'><\/a>\n#### ***Number of characters***\n\nNow let's explore features of the tweet texts stratified by target. We will start with number of characters. \n\nIt is quite hard to tell whether the distributions differ, so I used a t-test to see if the distributions of tweet characters differ based on target. T-test assumes means of samples to be normal, for the tweet sample size, I believe it is okay to run a t-test based on CLT. \n* The result of the t-test gives t=16.13 and p<0.05. Number of characters does differ by target.   ","ebc24021":"[back to top](#top)\n\n<a id='cloud'><\/a>\n#### ***word clouds***\nWord clouds are visualizations of words in which the sizes of words reflect the relative importance of words. Here, we will build word clouds using the raw text data. We can also choose to build word clouds after cleaning the text data and getting rid of noises (punctuations, lines, etc)\n\nThis tutorial here walks through steps of building word clouds. https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python.\n\nThe generated word clouds, although containing some noise such as 'https' does show that words makeup of disaster tweets look differetly from words makeup of non disaster tweets such that the disaster tweet word cloud includes words such as 'suicide','bomber','building', whereas the non disaster tweet word cloud includes words such as 'good', 'great', and 'love'.\n\nFrom the word clouds, I think a good extra step to take is to do a simple sentiment analysis and explore the emotions of the disaster vs non disaster tweets. I have not completed this step in this notebook yet and might add it later. ","43348d83":"[back to top](#top)\n\n\n<a id='nb'><\/a>\n### Naive Bayes Model\n\n#### ***Theory behind naive bayes***\n\nThe first model we are trying out is the naive bayes model. The naive bayes model is based on the bayes theorem which links the probability of event C given event X to the probabily of event X given event C. The theorem states $P(C|X)=\\frac{P(X|C)P(C)}{P(X)}$ which is derived from conditional probability $P(X|C)=\\frac{P(X \\cap C)}{P(C)}$ => $P(C \\cap X)=P(X|C)*P(C)$ => $P(C|X)=\\frac{P(C \\cap X)}{P(X)}=\\frac{P(X|C)P(C)}{P(X)}$.\n* P(C) is the probability of C\n* P(X) is the probability of X\n* P(C|X) is the conditional probability of C given X\n* P(X|C) is the conditional probability of X given C\n\nIn building a classifier, we can view X as the feature and C as the target we are trying to classify. We want to find out probability of the variable to be classified given the occurence of the features. Because usually there are many features i.e. $X_1,X_2,X_3$ etc, the equation can be expanded to be $P(C|X_1,X_2,X_3...)=\\frac{P(X_1 \\cap X_2 \\cap X_3...|C)P(C)}{P(X_1 \\cap X_2 \\cap X_3...)}$.\n\nThe above equation will be very hard to calculate, but naive bayes assumers features $X_1,X_2,X_3$ etc to be independent, meaning that the occurance of the features do not affect each other. With this assumption of indepdendence, the equation becomes $P(C|X_1,X_2,X_3...)=\\frac{P(X_1|C)P(X_2|C)P(X_3|C)...P(C)}{P(X_1)P(X_2)P(X_3)...}$.\n\nWe often have two classes to classify the target into, i.e. in this example, disaster tweet vs non disaster tweet. Using the above naive bayes equation, $P(disaster|features)$ and $P(non-disaster|features)$ can be calculated because $P(X_1|C)$ and $P(X_1)$ etc can be derived with training data. The class label that should be assigned to the data of interest should the class with a larger probability.\n\nReferences:\n* [Naive Bayes Clearly Explained!!!](https:\/\/www.youtube.com\/watch?v=O2L2Uv9pdDA) \n* [naive bayes classifiers](https:\/\/www.geeksforgeeks.org\/naive-bayes-classifiers\/)\n* [Bayes theorem](https:\/\/www.youtube.com\/watch?v=HZGCoVF3YvM)","de54e7cf":"\n[back to top](#top)\n\n<a id='wlen'><\/a>\n#### ***word length in tweets***\n\nThe last feature we are exploring here is length of tweets. \n\n* t=-15.68,p=0.00,so the word length distribution differs by target.","0a22e174":"[back to top](#top)\n\n<a id=\"model\"><\/a>\n#  Modeling \n\nNow we can finally start to build models! (yay!)\n\nWe will compare the efficacies of different algorithms in this section. The algorithms compared are:\n* Naive bayes\n* Logistic regression\n* LSTM recurrent neural network","0f84f8d7":"[back to top](#top)\n\n<a id='LSTM'><\/a>\n### LSTM\n\n#### ***Theory behind LSTM***\n\nLSTM (long short term memory) is a RNN (recurrent neural network) structure. RNNs are different from more traditional NNs because RNNs take into account of previous states, whcih makes them suitable for natural language processing as text have different positions. I tried to make a simplified rnn graph below. Because rnn receive both input from previous units and input from x, it will have a \"memory\" over time. \n\n```\n->[rnn unit]->[rnn unit]->\n       ^           ^\n       |           |\n       x           x\n```\n\nLSTM allows the architecture to detect associations between words over \"extended time intervals\"(Hochreiter & Schmidhuber, 1997)\uff0c and it solves the \"vanishing gradient\" problem in RNNs using gates. \n\nIn LSTM's each unit, there are 3 gates: \n* forget gate: output 0\/1 to decide if the previous state should be kept\n* input gate: actives the input x to enter the unit, often using a sigmoid function\n* output gate: decides how much the cell will output\n\nreference:\n* [Long Short-Term Memory](https:\/\/www.mitpressjournals.org\/doi\/abs\/10.1162\/neco.1997.9.8.1735)\n* [Understanding LSTM Networks](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)\n\n\n#### ***Using LSTM***\n\nWe will use keras to build LSTM. Before building the model.\n1. Tokenize the text data and instead of using TF-IDF transformed data\n    * Tokenizer(), transforms data into tokens and assign counts to tokens\n2. Make an embed layer from the tokenized data\n    * Embedding(len(tokenizer.word_index) + 1, 256,input_length = X.shape[1])\n        * len(tokenizer.word_index) + 1 is number of words after tokenizing found [here](https:\/\/stackoverflow.com\/questions\/53525994\/how-to-find-num-words-or-vocabulary-size-of-keras-tokenizer-when-one-is-not-as)\n        * 256 is the embedding dimension\n        * X.shape[1] is the size of input for each tweet\n    * Adding Dropout to regularize the networks and prevent overfitting\n    * Adding LSTM layers\n    * Adding output dense layer","b27b5b37":"[back to top](#top)\n\n<a id='library'><\/a>\n### Import Libraries\n\nFirst, we will import all libraries that will be used. Here are some short introductions of all the used libraries.\n\n| Library     | Description and link to documentation |\n| :---------- | :---------- |\n| re          | [regular expression operations](https:\/\/docs.python.org\/3\/library\/re.html)                     |\n| numpy       | [linear algebra and data manipulation](https:\/\/numpy.org\/doc\/stable\/)                          |\n| pandas      | [data processing, csv manipulation ](https:\/\/pandas.pydata.org\/docs\/)                          |\n| matplotlib  | [basic data visualization](https:\/\/matplotlib.org\/users\/index.html)                            |\n| scipy       | [more various data visualization](https:\/\/seaborn.pydata.org\/tutorial.html)                    |\n| string      | [string and text operations](https:\/\/docs.python.org\/3\/library\/string.html)                    |\n| nltk        | [natural language processing](https:\/\/www.nltk.org\/)                                           |\n| wordcloud   | [visualizing word clouds](https:\/\/amueller.github.io\/word_cloud\/auto_examples\/index.html)      |\n| sklearn     | [machine learning](https:\/\/scikit-learn.org\/stable\/)                                           |\n| keras       | [neural networks and deep learning](https:\/\/keras.io\/guides\/)                                  |\n","96bf0aaa":"[back to top](#top)\n\n<a id='lr'><\/a>\n### Logistic Regression\n\n#### ***Theory behind logistic regression***\nLogistic regression uses the function $y=\\frac{1}{1+e^{-(\\beta_0+\\beta_1X_1+\\beta_2X_2...)}}$ in which $y$ is the class to be predicted,$X_n$ is the feature, and $\\beta_n$ is the parameter to be paired with $X_n$. If the parameters and features are vectorized, the logistic regression function can be written as $y=\\frac{1}{1+e^{-(\\beta^{T}X)}}$ in which $\\beta^{T}$ is the transpose of the parameter vector and X is the feature vector.\n\nLogistic regression can be seen as linear regression which predicts continuous results transformed to predict discrete classes by applying the sigmoid function which follows $sigmoid(x)=\\frac{1}{1+e^{-x}}$.\n\nreferences\n* [Logistic Regression Analysis](https:\/\/www.sciencedirect.com\/topics\/medicine-and-dentistry\/logistic-regression-analysis)\n* [Machine Learning Notes](http:\/\/cs229.stanford.edu\/notes\/cs229-notes1.pdf)\n\n\n#### ***Using logistic regression***\n\nSklearn's logistic regression module will be used here with 10 fold cross validation and f1 as the scoring metric.","1689586a":"<a id='top'><\/a>\n# Introduction\n\n<hr>\n\nTwitter is a popular social network service. Because of the accessibility and universality of Twitter, people are starting to tweet about disasters in emergencies. In this notebook, we will attempt to categorize tweets as either disaster tweets (target=1) or non-disasrer tweets (target=0) for the *Real or Not? NLP with Disaster Tweets* challenge using natural language processing in python.\n\n\n\n\n#### table of contents\n* [import libraries & datasets](#import)\n    * [import libraries](#library)\n    * [look at datasets](#dataset)\n    * [Evaluation Criteria](#)\n* [exploratory data analysis](#EDA)\n    * [target](#target)\n    * [location](#location)\n    * [keyword](#keyword)\n    * [text](#text)\n        * [number of characters](#char)\n        * [number of words](#word)\n        * [word length](#wlen)\n        * [word clouds](#cloud)\n* [data preprocessing](#preprocess)\n* [Evaluation criteria and notes](#eval)\n    * [f1 score](#f1)\n    * [cross validation](#cross)\n* [modeling](#model)\n    * [naive bayes](#nb)\n    * [logistic regression](#lr)\n    * [LSTM](#LSTM)","76af55ed":"\n[back to top](#top)\n\n<a id='location'><\/a>\n### Location\n\nNow we can explore the distribution of the location variable (location when tweet is posted) stratified by target.\n* Locations are not mutually exclusive from each other i.e. USA would include NY etc.\n* The distribution of most freqyebt locations differed by target (1\/0).\n* Although the visualization do not show missing values, there is a large proportion of missing values for location.","7365b909":"[back to top](#top)\n\n<a id='tf'><\/a>\n\n### TF-IDF\n\nWe need to vectorize the texts before using them in the models. sklearn's CountVectorizer and TfidfVectorizer (Tfidf stands for term frequency inverse document frequency) are both good choices to vectorize words based on term frequencies. The difference, put in simple words, is that TfidfVectorizor normalizes the count matrix after counting term frequencies. We will use TFidfVectorizer here. \n\nTerm frequency $TF$ is the number of times the term occured in a document. \n\nInverse document frequency can be calculated as $IDF = log(\\frac{Total Number Of Documents}{Frequency Of Term\/NumberOfDocumentsWithTerm})+ 1 $. \n\nFinally,TF-IDF can be calculated as $TF * IDF$.\n\nWe can run a simple example to see how countvectorizer and tfidfvectorizer works.\n\nThe following code uses the couuntctorizer to derive frequencies for 'cat runs' and 'dog runs'. \n\n```\nvectorizer=CountVectorizer()\nvectors = vectorizer.fit_transform(['cat runs', 'dog runs'])\nprint(vectorizer.get_feature_names(),vectors.toarray())\n```\nprinting the results shows\n\n|           | cat    | dog    | runs   |  \n|-----------|--------|--------|--------|\n|cat runs   | 1      | 0      | 1      | \n|dog runs   | 0      | 1      | 1      | \n\n<br>\n<br>\nSimilarly, the following code uses the tfidfvectorizer to derive frequencies for 'cat runs' and 'dog runs'. \n\n```\nvectorizer=TfidfVectorizer()\nvectors = vectorizer.fit_transform(['cat runs', 'dog runs'])\nprint(vectorizer.get_feature_names(),vectors.toarray())\n```\nprinting the results shows \n\n|           | cat    | dog    | runs   |  \n|-----------|--------|--------|--------|\n|cat runs   | 0.8148 | 0      | 0.5797 | \n|dog runs   | 0      | 0.8148 | 0.5797 | \n\n<br>\n<br>\nIt is quite obvious that CountVectorizer and TfidfVectorizers have similar mechanisms although countvectorizer returns integers whereas tfidfvectorizer returns floats. TfidfVectorizer has the benefit of avoiding putting too much weights on frequently appearing words by making the encoding in inverse proportion to the frequencies. \n"}}