{"cell_type":{"adf88d80":"code","280153c6":"code","b14a71b7":"code","12163320":"code","1573c792":"code","b46d6414":"code","0089b88d":"code","5b082c5c":"code","a2da82c6":"code","3de316b9":"code","232722f1":"code","7413504f":"code","c91f17b5":"code","67c5be2b":"code","82005b09":"code","e2cde26f":"code","9bea7b50":"code","212caa3f":"code","9e59489e":"code","914f5d90":"code","b1298447":"code","f5d1303c":"code","bd6f5ccd":"markdown","6bb91aba":"markdown","612d8406":"markdown","d3175e75":"markdown","36169023":"markdown","b5081ea2":"markdown","c50a476c":"markdown","f273d95e":"markdown","25d4f619":"markdown","fd7105b1":"markdown","be5fd4c6":"markdown","a416d2ad":"markdown","66ea10e1":"markdown"},"source":{"adf88d80":"!pip install selenium","280153c6":"%matplotlib inline\n\nimport os\nimport cv2\nimport glob\nimport pathlib\nimport requests\nimport selenium\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom io import BytesIO\nfrom PIL import Image\nfrom selenium import webdriver\nfrom sklearn.metrics import accuracy_score\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator","b14a71b7":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","12163320":"data_path = '..\/input\/blood-face-detection\/Blood_noblood'\nblood_path = '..\/input\/blood-face-detection\/Blood_noblood\/blood'\nnoblood_path = '..\/input\/blood-face-detection\/Blood_noblood\/noblood'\nfaces_path = '..\/input\/blood-face-detection\/Blood_noblood\/faces'\n\ndata_dir = pathlib.Path(data_path)\nblood_dir = pathlib.Path(blood_path)\nnoblood_dir = pathlib.Path(noblood_path)\nfaces_dir = pathlib.Path(faces_path)","1573c792":"blood_img_count = len(list(blood_dir.glob('*.jpg')))\nprint(\"Total images with faces having blood\", blood_img_count)\nnoblood_img_count = len(list(noblood_dir.glob('*.jpg')))\nprint(\"Total images with faces not having blood\", noblood_img_count)","b46d6414":"%mkdir \/kaggle\/working\/blood\n%mkdir \/kaggle\/working\/noblood","0089b88d":"! git clone https:\/\/github.com\/archisha-chandel\/Web-Scraping.git","5b082c5c":"%cat \/kaggle\/working\/Web-Scraping\/main.py","a2da82c6":"%%writefile \/kaggle\/working\/Web-Scraping\/main.py\nimport selenium\nimport os\nimport pathlib\nfrom selenium import webdriver\n\n# Put the path for your ChromeDriver here\nDRIVER_PATH = os.path.join('\/kaggle\/working\/Web-Scraping\/Driver','chromedriver.exe')\nwd = webdriver.Chrome(executable_path= DRIVER_PATH)\n\n\nfrom search_and_download import search_and_download\n\nsearch_term = \"bloody face\"\nsearch_and_download(\n    search_term = search_term,\n    driver_path = DRIVER_PATH,\n    target_path = pathlib.Path(r'\/kaggle\/working\/Web-Scraping\/blood\/'),\n    number_images = 300\n)","3de316b9":"os.chmod('\/kaggle\/working\/Web-Scraping\/Driver\/chromedriver.exe', 755)","232722f1":"%run \/kaggle\/working\/Web-Scraping\/main.py","7413504f":"faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\nfiles = glob.glob(\"..\/input\/blood-face-detection\/Blood_noblood\/*\/*.jpg\")\nfor file in files:\n    \n    folder = os.path.basename(os.path.dirname(file))\n    head, tail = os.path.split(file)\n\n    # Read the image\n    image = cv2.imread(file)\n    \n    if image is not None:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect faces in the image\n        faces = faceCascade.detectMultiScale(\n            gray,\n            scaleFactor=1.05,\n            minNeighbors=3,\n            minSize=(28, 28),\n            flags = cv2.CASCADE_SCALE_IMAGE\n        )\n\n        # if no faces are detected then continue\n        if (len(faces) == 0):\n#                 print('Failed to detect face')\n#                 print(\"xxxxxxxxxxxxxxxxxxxxx\")\n                continue\n\n#         print(\"Found {0} faces!\".format(len(faces)))\n\n        # Crop Padding\n        left = 10\n        right = 10\n        top = 10\n        bottom = 10\n\n        # Draw a rectangle around the faces\n        for (x, y, w, h) in faces:\n            image = cv2.rectangle(image, (x,y), (x+w,y+h), (127,0,255), 2)\n\n        image  = image[y:y+h, x:x+w]\n\n        out_path = os.path.join('\/kaggle\/working\/',folder,tail)\n        cv2.imwrite(\"{0}\".format(str(out_path)), image)\n#         print(\"----------------------\")","c91f17b5":"# the list of images and their class images\nimagePaths = glob.glob(\"\/kaggle\/working\/*\/*.jpg\")\ndata = []\nlabels = []\nIMAGE_SIZE = 200\nCLASSES = 2\nBATCH_SIZE = 32\nEPOCHS = 50\n\n# loop over each image in the image paths\nfor imagePath in imagePaths:\n    label= os.path.basename(os.path.dirname(imagePath))\n    head, tail = os.path.split(imagePath)\n    image = tf.io.read_file(imagePath)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.cast(image, tf.float32)\/256.0\n    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n    # adding image to arrays\n    data.append(image)\n    labels.append(label)\n        \n\n    \n# convert the data and labels to NumPy arrays\ndata = np.array(data, dtype=\"float32\")\nlabels = np.array(labels)\n\n# perform one-hot encoding on the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = tf.keras.utils.to_categorical(labels)\n\n# partition the data into training and testing splits using 80-20\n(train_X, test_X, train_y, test_y) = train_test_split(data, labels,test_size=0.20, stratify=labels, random_state=42)","67c5be2b":"# applying random transformations to the images\naugment = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=10,\n    zoom_range=0.15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.15,\n    horizontal_flip=True,\n    vertical_flip=False,\n    fill_mode=\"nearest\")\n\n# defining model-resnet50\nresnet_model = tf.keras.applications.ResNet50(\n    weights='imagenet',\n    include_top=False\n)\n\nx = resnet_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(512,activation='relu')(x)\npredictions = tf.keras.layers.Dense(\n    CLASSES,\n    activation='softmax'\n)(x)\n\nresnet50_model = tf.keras.models.Model(\n    inputs= resnet_model.input, \n    outputs=predictions\n)\n\nresnet50_model.compile(\n    loss='categorical_crossentropy', \n    optimizer=tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n    metrics=['accuracy']\n)\n\nresnet50_model.summary()","82005b09":"# using Cyclic Learning Rate\n! git clone https:\/\/github.com\/bckenstler\/CLR.git","e2cde26f":"from keras.callbacks import *\nfrom CLR.clr_callback import *\n\n# using the triangular learning rate policy\nclr_triangular = CyclicLR(mode='triangular')","9bea7b50":"# defining callbacks\nlearning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\ncallbacks = [ learning_rate_reduction, clr_triangular]\n\n# train the head of the network\nhistory_resnet50 = resnet50_model.fit(\n    augment.flow(train_X, train_y, batch_size=BATCH_SIZE),\n    steps_per_epoch = len(train_X) \/\/ BATCH_SIZE,\n    validation_data = (test_X, test_y),\n    validation_steps = len(test_X) \/\/ BATCH_SIZE,\n    epochs = EPOCHS,\n    callbacks = callbacks\n)","212caa3f":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_resnet50.history['accuracy'], label='training accuracy')\nplt.plot(history_resnet50.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_resnet50.history['loss'], label='training loss')\nplt.plot(history_resnet50.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","9e59489e":"lb.classes_","914f5d90":"test_images = [\n    \"https:\/\/image.shutterstock.com\/image-photo\/portrait-strong-man-beard-face-260nw-1089746795.jpg\",\n    \"https:\/\/image.shutterstock.com\/image-photo\/halloween-girl-applying-red-lipstick-600w-710850430.jpg\",\n    \"https:\/\/image.shutterstock.com\/image-photo\/portrait-female-vampire-over-black-600w-223796254.jpg\",\n    \"https:\/\/image.shutterstock.com\/image-photo\/beautiful-woman-face-close-studio-600w-694271455.jpg\",\n    \"https:\/\/image.shutterstock.com\/image-photo\/facial-beauty-treatment-happy-young-600w-1696369483.jpg\",\n    \"https:\/\/image.shutterstock.com\/image-photo\/skin-care-woman-beauty-face-600w-1543602947.jpg\"\n]\n\ntest_labels = ['blood', 'blood', 'blood', 'noblood', 'noblood', 'noblood']","b1298447":"test_labels = lb.fit_transform(test_labels)\ntest_labels = tf.keras.utils.to_categorical(test_labels)","f5d1303c":"for (i,l) in enumerate(test_labels):\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE])\n    img = tf.reshape(img, (1,IMAGE_SIZE, IMAGE_SIZE,3))\n    \n    prediction = resnet50_model.predict(img)\n    output = np.argmax(prediction)\n    \n    plt.title(\"Real: {} \\n Predict: {}\".format(test_labels[i], test_labels[output]))\n    acc = accuracy_score(test_labels[i], test_labels[output])\n    print(\"Accuracy: \", acc)\n    plt.imshow(img[0, ...])\n    plt.show()","bd6f5ccd":"Since there is less data to for a neural network to function efficiently, [Web Scraping](https:\/\/github.com\/archisha-chandel\/Web-Scraping) is used.\n1. Clone the git repository\n2. Make changes in `main.py`","6bb91aba":"Two labeled folders are created in the destination to add web-scraped images and preprocessed images ([next section](#data-preprocessing)).","612d8406":"<a id=\"loading-libraries\"><\/a>\n# Loading Libraries","d3175e75":"<a id=\"detecting-gpu\"><\/a>\n# Detecting GPU\nUsage of GPU is recommended!","36169023":"<a id=\"array-split\"><\/a>\n# Converting the Dataset to Array and performing Train-Test Split","b5081ea2":"<a id=\"data-preprocessing\"><\/a>\n# Data Preprocessing\nA `haarcascade_frontalface_default.xml` is used to filter images from the dataset based on the model's ability to detect faces (with and without blood). Only those images where the model can detect faces are filtered out and stored in `\/kaggle\/working\/`. Uncomment the print statements to check classifier performance and the number of faces being detected.","c50a476c":"<a id=\"aug-model\"><\/a>\n# Augmentation and Modeling","f273d95e":"<a id=\"callback-train\"><\/a>\n# Callbacks and Training","25d4f619":"# Blood Face Detection\n\n## Approach\nThis notebook follows a simple approach of research and augmentation along with training a fairly efficient network to test if images have faces with blood or without. The research papers studied are as follows:\n\n* [Deep Residual Learning for Image Recognition [1]](https:\/\/arxiv.org\/pdf\/1512.03385)\n* [A survey on Image Data Augmentation for Deep Leaning  [2]](https:\/\/journalofbigdata.springeropen.com\/articles\/10.1186\/s40537-019-0197-0)\n* [A Survey of the Recent Architectures of Deep Convolutional Neural Networks [3]](https:\/\/arxiv.org\/pdf\/1901.06032)\n* [Recent Advances in Convolutional Neural Networks [4]](https:\/\/arxiv.org\/pdf\/1512.07108)\n* [Cyclical Learning Rates for Training Neural Networks [5]](https:\/\/arxiv.org\/pdf\/1506.01186)\n* [Very Deep Convolutional Networks For Large-Scale Image Recognition [6]](https:\/\/arxiv.org\/pdf\/1409.1556)\n\nThese papers have contributed in understanding the correct use of different architectures for different use cases [1,3,4,6], the loss functions required, how to improve the time & performance efficiency of the network [5] along with how to perform different augmentation techniques [2] to overcome the hinderance due to insufficient dataset.\n\n### The Idea\nSince we have **limited dataset**, the least we expect from our model is to recognize faces correctly. With this thought in mind I have first tested a pretrained model to **filter the images from which it can clearly recognize faces**. This is done using a \"CascadeClassifier\" under CV2 library and the results are stored in a separate folder under `\/kaggle\/working\/` as `\/blood` and `\/noblood`. \n\nAnother way to deal with limited dataset could be to perform [Web-Scraping](https:\/\/github.com\/archisha-chandel\/Web-Scraping) and add more images of each kind to the existing dataset. The repository is cloned and the directions from the README.md are followed. However, there is an **unresolved issue due to its usage on Kaggle** and thus it could not been implemented in this notebook. But it **runs fine on local machines**, so one can run it there and add those images to the dataset (if this is being executed on a local machine).\n\nOnce we have made sure that the model would be able to detect faces we start with **augmentation**. In this section, different transformations that would be suitable to the given dataset are performed using \"ImageDataGenerator\". After this a **ResNet-50** is model is defined to fit on our dataset. **Callback** function using \"Cyclic Learning Rate\" are also defined along with \"ReduceLROnPlateau\".\n\nThe model is then tested on a few sample images requested from the different URLs and their accuracy is checked along with visual representations.\n\n### Contents\n\n- [Loading Libraries](#loading-libraries)\n- [Detecting GPU](#detecting-gpu)\n- [Configuring and Web Scraping](#configuring)\n- [Data Preprocessing](#data-preprocessing)\n- [Converting dataset to Array and Splitting](#array-split)\n- [Augmentation and Modeling](#aug-model)\n- [Callbacks and Training](#callback-train)\n- [Visualising Accuracy and Loss](#acc-loss)\n- [Testing](#prediction)\n\n## Future Scope\n1. The [web-scraping error](#error) could be resolved and the modeling could be done on a larger dataset.\n2. The face detection model used in data preprocessing could be used to produce bounding boxes for the faces detected along with the accuracy of detection.\n3. The ResNet-50 model used could be enhanced to identify if faces are present first and then predict if it has blood or not.\n4. A simple web application for the same could be built using Flask which could take input in the form of live image\/video.\n5. Other models apart from ResNet could be used and their performance could be measured and weighed.","fd7105b1":"<a id=\"prediction\"><\/a>\n# Prediction on Test Data\nTest images would be fetch from the URLs present in `test_images` and the model built above would be tested on them.","be5fd4c6":"<a id=\"error\"><\/a>\n## **Error!!**","a416d2ad":"<a id=\"configuring\"><\/a>\n# Configuring and Web-Scraping\nDefining data paths and implementing [web-scraping](https:\/\/github.com\/archisha-chandel\/Web-Scraping). Here, an error is encountered which could not be solved due to platform issue but it can be run on local machine and that dataset can be used.","66ea10e1":"<a id=\"acc-loss\"><\/a>\n# Visualising Accuracy and Loss"}}