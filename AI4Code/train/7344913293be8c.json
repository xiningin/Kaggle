{"cell_type":{"3e6b843f":"code","b27743a9":"code","990a2c85":"code","ec1f406f":"code","d4db565b":"code","d9bd44fd":"code","271f792c":"code","ee8d7445":"code","c3f6f506":"code","989ca1c1":"code","e9bc1924":"code","44b7c37a":"code","a214905e":"code","71fa7c61":"code","f03b0886":"code","6d3434e7":"code","95606169":"code","7f51af88":"code","38cfe0d2":"code","52cbeebc":"code","1f151efa":"code","429c67af":"code","c62b308e":"markdown","91bd01e3":"markdown","40cb65be":"markdown","6278ca3d":"markdown","a6949a6a":"markdown","a992648e":"markdown","de5f30bc":"markdown","ac142a8b":"markdown"},"source":{"3e6b843f":"import os\nimport shutil\nimport  joblib\nimport numpy as np\nimport pandas as pd\nimport librosa as lb\nimport librosa.display\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm","b27743a9":"DATA_ROOT = Path(\"..\/input\/rfcx-species-audio-detection\")\nTRAIN_AUDIO_ROOT = Path(\"..\/input\/rfcx-species-audio-detection\/train\")\nTEST_AUDIO_ROOT = Path(\"..\/input\/rfcx-species-audio-detection\/test\")","990a2c85":"df_train = pd.DataFrame({\n    \"recording_id\": [path.stem for path in Path(TRAIN_AUDIO_ROOT).glob(\"*.flac\")],\n})\n\ndf_test = pd.DataFrame({\n    \"recording_id\": [path.stem for path in Path(TEST_AUDIO_ROOT).glob(\"*.flac\")],\n})","ec1f406f":"class params:\n    \"\"\"\n    Parameters used for the audio data\n    \"\"\"\n    sr = 32000\n\n    # Melspectrogram\n    n_mels = 128\n    fmin = 20\n    fmax = sr \/\/ 2  # Shannon theorem","d4db565b":"def load_audio(record, sr=16000, root=\"\"):\n    y, _ = lb.load(\n        root.joinpath(record).with_suffix(\".flac\").as_posix(),\n        sr=sr, \n    )\n    return y","d9bd44fd":"def compute_melspec(y, params):\n    \"\"\"\n    Computes a mel-spectrogram and puts it at decibel scale\n    Arguments:\n        y {np array} -- signal\n        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n    Returns:\n        np array -- Mel-spectrogram\n    \"\"\"\n    melspec = lb.feature.melspectrogram(\n        y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n    )\n\n    melspec = lb.power_to_db(melspec).astype(np.float32)\n    return melspec","271f792c":"y = load_audio(df_train[\"recording_id\"][0], params.sr, TRAIN_AUDIO_ROOT)","ee8d7445":"melspec = compute_melspec(y, params)","c3f6f506":"fig, ax = plt.subplots(figsize=(15, 5))\nimg = librosa.display.specshow(\n    melspec[:, :512], \n#     melspec, \n    sr=params.sr,\n    x_axis='time', \n    y_axis='linear', \n    ax=ax)\nfig.colorbar(img, ax=ax, format=\"%+2.f dB\")\nplt.show()","989ca1c1":"np.save(\"melspec.npy\", melspec)","e9bc1924":"%%timeit \n\nspec = np.load(\"melspec.npy\")","44b7c37a":"%%timeit \n\ny = load_audio(df_train[\"recording_id\"][0], params.sr, TRAIN_AUDIO_ROOT)\nmelspec = compute_melspec(y, params)","a214905e":"def load_and_save_train(location, record):\n    y = load_audio(record, params.sr, TRAIN_AUDIO_ROOT)\n    melspec = compute_melspec(y, params)\n\n    np.save(location + record + \".npy\", melspec)","71fa7c61":"OUT_TRAIN_1 = 'train1\/'\nos.mkdir(OUT_TRAIN_1)\n\nOUT_TRAIN_2 = 'train2\/'\nos.mkdir(OUT_TRAIN_2)\n\nOUT_TRAIN_3 = 'train3\/'\nos.mkdir(OUT_TRAIN_3)\n\nOUT_TRAIN_4 = 'train4\/'\nos.mkdir(OUT_TRAIN_4)","f03b0886":"print(df_train.shape)\n\nchunk = (df_train.shape[0] \/\/ 4) + 1\nfor block in range(4):\n    start = block * chunk\n    stop = start + chunk\n    location = f'train{block+1}\/'\n    \n    print(location,start,stop)               \n    \n    _ = joblib.Parallel(n_jobs=8)(\n        joblib.delayed(load_and_save_train)(location,record) for record in tqdm(df_train['recording_id'][start:stop].values)\n    )    \n    \n    shutil.make_archive(location, 'zip', location)\n    shutil.rmtree(location)    ","6d3434e7":"# df_train['recording_id'][start:stop].values","95606169":"# _ = joblib.Parallel(n_jobs=8)(\n#     joblib.delayed(load_and_save_train)(OUT_TRAIN_1,record) for record in tqdm(df_train['recording_id'].values)\n# )\n\n# _ = joblib.Parallel(n_jobs=8)(\n#     joblib.delayed(load_and_save_train)(OUT_TRAIN_2,record) for record in tqdm(df_train['recording_id'].values)\n# )\n\n# _ = joblib.Parallel(n_jobs=8)(\n#     joblib.delayed(load_and_save_train)(OUT_TRAIN_3,record) for record in tqdm(df_train['recording_id'].values)\n# )\n\n# _ = joblib.Parallel(n_jobs=8)(\n#     joblib.delayed(load_and_save_train)(OUT_TRAIN_4,record) for record in tqdm(df_train['recording_id'].values)\n# )","7f51af88":"# shutil.make_archive(OUT_TRAIN, 'zip', OUT_TRAIN)\n# shutil.rmtree(OUT_TRAIN)","38cfe0d2":"def load_and_save_test(record):\n    y = load_audio(record, params.sr, TEST_AUDIO_ROOT)\n    melspec = compute_melspec(y, params)\n\n    np.save(OUT_TEST + record + \".npy\", melspec)","52cbeebc":"OUT_TEST = 'test\/'\nos.mkdir(OUT_TEST)","1f151efa":"_ = joblib.Parallel(n_jobs=8)(\n    joblib.delayed(load_and_save_test)(record) for record in tqdm(df_test['recording_id'].values)\n)","429c67af":"shutil.make_archive(OUT_TEST, 'zip', OUT_TEST)\nshutil.rmtree(OUT_TEST)","c62b308e":"x 3000 improvement ! ","91bd01e3":"# Example","40cb65be":"## Test","6278ca3d":"## Train","a6949a6a":"# Data","a992648e":"# Main","de5f30bc":"# Tools","ac142a8b":"# Time comparison"}}