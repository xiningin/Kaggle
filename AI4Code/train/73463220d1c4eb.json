{"cell_type":{"ce4debff":"code","a464960e":"code","0edf8430":"code","84bf411b":"code","d65cddae":"code","96ba1501":"code","4169c70e":"code","5efbda1c":"code","3443906e":"code","5e025709":"code","84cd1d70":"code","33130a24":"markdown","053465cf":"markdown","ab09cdb7":"markdown","3eac273f":"markdown","106f54f0":"markdown"},"source":{"ce4debff":"import tensorflow as tf\nimport os \nimport zipfile\nimport pandas as pd\nimport numpy as np","a464960e":"!pip install transformers==3.1.0","0edf8430":"from transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertForSequenceClassification","84bf411b":"import json\n\npath=\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\"\n\ndef parse_data(file):\n    for l in open(file,'r'):\n        yield json.loads(l)\n\ndata = list(parse_data(path))\n\ntraining_size = 25000\n\nsentences = []\nlabels = []\nurls = []\nfor item in data:\n    sentences.append(item['headline'])\n    labels.append(item['is_sarcastic'])\n\ntraining_sentences = sentences[0:training_size]\nvalidation_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\nvalidation_labels = labels[training_size:]","d65cddae":"len(training_sentences)","96ba1501":"len(validation_sentences)","4169c70e":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","5efbda1c":"train_encodings = tokenizer(training_sentences,\n                            truncation=True,\n                            padding=True)\nval_encodings = tokenizer(validation_sentences,\n                            truncation=True,\n                            padding=True)","3443906e":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    training_labels\n))\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings),\n    validation_labels\n))","5e025709":"# We classify two labels in this example. In case of multiclass classification, adjust num_labels value\nmodel = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n                                                              num_labels=2)","84cd1d70":"# Commented because it would take lot of time while committing\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n#model.fit(train_dataset.shuffle(42).batch(16),epochs=2,batch_size=16,validation_data=val_dataset.shuffle(42).batch(16))","33130a24":"## **Data Preparation**","053465cf":"## **Starting preprocessing on sentences**","ab09cdb7":"### Final result --> loss: 0.0357 - accuracy: 0.9871 - val_loss: 0.5317 - val_accuracy: 0.8900","3eac273f":"# **Sarcasm Detector using NLP**","106f54f0":"##**Applying distilBERT quantized simple, lighter BERT**"}}