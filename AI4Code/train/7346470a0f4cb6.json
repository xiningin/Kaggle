{"cell_type":{"0210bd6e":"code","3e275817":"code","a2daa6d5":"code","a91d99e9":"code","0f076e70":"code","d5dd0d0a":"code","1b0892e9":"code","bf6116dc":"code","a7a07a65":"code","610e9779":"code","916017aa":"code","31a53376":"code","60a72ddc":"code","72fac7d1":"code","88c20bff":"code","4b5fe8b5":"code","6df747cb":"code","408005f7":"code","12eddce6":"code","d66fb474":"code","d7649293":"code","f5fdeed0":"code","4c68e4d7":"code","0cf36baa":"code","92881f21":"code","307b19b4":"code","4f5ed034":"code","facca0f2":"code","43a35765":"code","af3770d0":"code","93ad92e4":"code","c6577295":"code","49062d43":"code","13be146e":"code","35714493":"code","76fa2d47":"code","0a00a0ad":"code","37fb4727":"code","e9972772":"code","acb7138a":"code","bec5f4fd":"code","08c3a6e6":"code","c913c510":"code","f39a01c6":"code","7940e556":"code","669c568e":"code","49107644":"code","7c37c2af":"code","06a48a7a":"code","a8e6a64c":"code","c479c963":"code","11591b27":"code","77db7d39":"code","069d1fc6":"code","bf376e28":"code","cfde0142":"code","bfb09411":"code","0614f045":"code","8313b5c0":"code","7c534b0a":"code","73252dfd":"code","fdfa1492":"code","d6d9f75f":"code","79adf431":"code","2585ebf1":"code","c41310c9":"code","46b272ee":"code","4c9bb9ee":"code","8a5a651f":"code","f6f67222":"code","3ed75df7":"code","a0af3da1":"code","4d2612da":"code","98d49deb":"code","930c43c9":"code","dcef5089":"code","b8579e9d":"code","d2e1d130":"code","3bc8b457":"code","d8f553dd":"code","7fa4a1a3":"code","fbae753a":"code","8feeaca2":"code","8a348c9a":"code","839f5f2e":"code","655e0d6d":"code","c428f80c":"code","36983901":"code","4d7607b1":"code","3914006e":"code","72485d14":"code","9ed5c323":"code","71252a2b":"code","00560731":"code","722934b6":"code","df057100":"code","627b8dd5":"code","aafcdee7":"code","dce40986":"code","9f29c21c":"code","3856c040":"code","108c7742":"code","6e43fbff":"code","dd678c5f":"code","1f786bfb":"code","8012b7b6":"code","ee6f8ba1":"code","8a2021fd":"code","ab8aa1f7":"code","ee18e52c":"code","57074216":"code","0b6e8dc5":"code","913b6983":"code","fa05fd7e":"code","397c5cb8":"code","2620241f":"code","9a2adbee":"code","a2e9eded":"code","e7e13703":"code","8603a71f":"code","ab950bda":"code","e2a11d09":"code","ff97844d":"code","a8e155ae":"code","cfab5d84":"code","66631377":"code","6e519f06":"code","16b4e24a":"code","aa8b8e7f":"code","13510a0a":"code","5fc0e501":"code","0626e8f5":"code","993aebc8":"code","e90ffeb9":"code","7255267c":"code","9a245b4f":"code","296e4d65":"markdown","5a9e8179":"markdown","55739fb4":"markdown","9b88ac0d":"markdown","c83b06a9":"markdown","81ae2e51":"markdown","aa6efa34":"markdown","e562bd3c":"markdown","d048d1cf":"markdown","6aa3e1c1":"markdown","f527326e":"markdown","c04e6ef9":"markdown","75d1f23b":"markdown","bb340751":"markdown","ec61ed6b":"markdown","1cfe0547":"markdown","c1604ad4":"markdown","c99f746c":"markdown","1a157231":"markdown","8557946f":"markdown","da8be3d9":"markdown","df990564":"markdown","6f4cd691":"markdown","b597d16e":"markdown","6f45823b":"markdown","d3ed2b0e":"markdown","dc149f30":"markdown","18fc4837":"markdown","d736fc8b":"markdown","ae54185a":"markdown","1ec6ed1b":"markdown","6a5a7ab6":"markdown","6c3834b5":"markdown","4d61c99a":"markdown","647626d3":"markdown","94f2e26c":"markdown","07beef40":"markdown","9a1d9c38":"markdown","4550bfa9":"markdown","bd921840":"markdown","b99b7c6c":"markdown","8e1d7373":"markdown","ff71268f":"markdown","eaf94cac":"markdown","289d9074":"markdown","50213e31":"markdown","71f3a317":"markdown","27584d40":"markdown","bf6fd4ff":"markdown","e3066751":"markdown","fd12c025":"markdown","3abd1fb3":"markdown","2a2a988a":"markdown","0b3035d4":"markdown","c14341e7":"markdown","250b4157":"markdown","d011f2fd":"markdown","1163f693":"markdown","bef888c2":"markdown","27fd12ba":"markdown","c68ced15":"markdown","e15f3489":"markdown","c9872a50":"markdown","cd318623":"markdown","a1a9889e":"markdown","1ba5acad":"markdown","ea49828a":"markdown","66daaa93":"markdown","e587e755":"markdown","83c4b1be":"markdown","f896b080":"markdown"},"source":{"0210bd6e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport os\nprint(os.listdir())\n\nimport warnings\nwarnings.filterwarnings('ignore')","3e275817":"data = pd.read_csv(\"..\/input\/heart.csv\")","a2daa6d5":"type(data)","a91d99e9":"data.shape","0f076e70":"data.head()","d5dd0d0a":"data.describe()","1b0892e9":"data.info()","bf6116dc":"data.sample(5)","a7a07a65":"data.isnull().sum()","610e9779":"data.isnull().sum().sum()","916017aa":"print(data.corr()[\"target\"].abs().sort_values(ascending=False))","31a53376":"y = data[\"target\"]","60a72ddc":"ax = sns.countplot(data[\"target\"])\ntarget_temp = data.target.value_counts()\nprint(target_temp)","72fac7d1":"print(\"Percentage of patience without heart problems: \"+str(round(target_temp[0]*100\/303,2)))\nprint(\"Percentage of patience with heart problems: \"+str(round(target_temp[1]*100\/303,2)))","88c20bff":"data[\"sex\"].unique()","4b5fe8b5":"sns.barplot(data[\"sex\"],data[\"target\"])","6df747cb":"def plotAge():\n    facet_grid = sns.FacetGrid(data, hue='target')\n    facet_grid.map(sns.kdeplot, \"age\", shade=True, ax=axes[0])\n    legend_labels = ['disease false', 'disease true']\n    for t, l in zip(axes[0].get_legend().texts, legend_labels):\n        t.set_text(l)\n        axes[0].set(xlabel='age', ylabel='density')\n\n    avg = data[[\"age\", \"target\"]].groupby(['age'], as_index=False).mean()\n    sns.barplot(x='age', y='target', data=avg, ax=axes[1])\n    axes[1].set(xlabel='age', ylabel='disease probability')\n\n    plt.clf()","408005f7":"fig_age, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 8))\n\nplotAge()","12eddce6":"countFemale = len(data[data.sex == 0])\ncountMale = len(data[data.sex == 1])\nprint(\"Percentage of Female Patients:{:.2f}%\".format((countFemale)\/(len(data.sex))*100))\nprint(\"Percentage of Male Patients:{:.2f}%\".format((countMale)\/(len(data.sex))*100))","d66fb474":"categorial = [('sex', ['female', 'male']), \n              ('cp', ['typical angina', 'atypical angina', 'non-anginal pain', 'asymptomatic']), \n              ('fbs', ['fbs > 120mg', 'fbs < 120mg']), \n              ('restecg', ['normal', 'ST-T wave', 'left ventricular']), \n              ('exang', ['yes', 'no']), \n              ('slope', ['upsloping', 'flat', 'downsloping']), \n              ('thal', ['normal', 'fixed defect', 'reversible defect'])]","d7649293":"def plotGrid(isCategorial):\n    if isCategorial:\n        [plotCategorial(x[0], x[1], i) for i, x in enumerate(categorial)] \n    else:\n        [plotContinuous(x[0], x[1], i) for i, x in enumerate(continuous)] ","f5fdeed0":"def plotCategorial(attribute, labels, ax_index):\n    sns.countplot(x=attribute, data=data, ax=axes[ax_index][0])\n    sns.countplot(x='target', hue=attribute, data=data, ax=axes[ax_index][1])\n    avg = data[[attribute, 'target']].groupby([attribute], as_index=False).mean()\n    sns.barplot(x=attribute, y='target', hue=attribute, data=avg, ax=axes[ax_index][2])\n    \n    for t, l in zip(axes[ax_index][1].get_legend().texts, labels):\n        t.set_text(l)\n    for t, l in zip(axes[ax_index][2].get_legend().texts, labels):\n        t.set_text(l)\n","4c68e4d7":"fig_categorial, axes = plt.subplots(nrows=len(categorial), ncols=3, figsize=(15, 30))\n\nplotGrid(isCategorial=True)","0cf36baa":"continuous = [('trestbps', 'blood pressure in mm Hg'), \n              ('chol', 'serum cholestoral in mg\/d'), \n              ('thalach', 'maximum heart rate achieved'), \n              ('oldpeak', 'ST depression by exercise relative to rest'), \n              ('ca', '# major vessels: (0-3) colored by flourosopy')]","92881f21":"def plotContinuous(attribute, xlabel, ax_index):\n    sns.distplot(data[[attribute]], ax=axes[ax_index][0])\n    axes[ax_index][0].set(xlabel=xlabel, ylabel='density')\n    sns.violinplot(x='target', y=attribute, data=data, ax=axes[ax_index][1])","307b19b4":"fig_continuous, axes = plt.subplots(nrows=len(continuous), ncols=2, figsize=(15, 22))\n\nplotGrid(isCategorial=False)","4f5ed034":"pd.crosstab(data.age,data.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","facca0f2":"pd.crosstab(data.sex,data.target).plot(kind=\"bar\",figsize=(20,10),color=['blue','#AA1111' ])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Don't have Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","43a35765":"data.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","af3770d0":"data.head()","93ad92e4":"pd.crosstab(data.fasting_blood_sugar,data.target).plot(kind=\"bar\",figsize=(20,10),color=['#4286f4','#f49242'])\nplt.title(\"Heart disease according to FBS\")\nplt.xlabel('FBS- (Fasting Blood Sugar > 120 mg\/dl) (1 = true; 0 = false)')\nplt.xticks(rotation=90)\nplt.legend([\"Don't Have Disease\", \"Have Disease\"])\nplt.ylabel('Disease or not')\nplt.show()","c6577295":"data[\"chest_pain_type\"].unique()","49062d43":"plt.figure(figsize=(26, 10))\nsns.barplot(data[\"chest_pain_type\"],y)","13be146e":"data[\"resting_blood_pressure\"].unique()","35714493":"plt.figure(figsize=(26, 10))\nsns.barplot(data[\"resting_blood_pressure\"],y)","76fa2d47":"data[\"rest_ecg\"].unique()","0a00a0ad":"plt.figure(figsize=(26, 15))\nsns.barplot(data[\"rest_ecg\"],y)","37fb4727":"data[\"exercise_induced_angina\"].unique()","e9972772":"plt.figure(figsize=(10, 10))\nsns.barplot(data[\"exercise_induced_angina\"],y)","acb7138a":"data[\"st_slope\"].unique()","bec5f4fd":"plt.figure(figsize=(25, 10))\nsns.barplot(data[\"st_slope\"],y)","08c3a6e6":"data[\"num_major_vessels\"].unique()","c913c510":"sns.countplot(data[\"num_major_vessels\"])","f39a01c6":"sns.barplot(data[\"num_major_vessels\"],y)","7940e556":"data[\"thalassemia\"].unique()","669c568e":"sns.distplot(data[\"thalassemia\"])","49107644":"sns.barplot(data[\"thalassemia\"],y)","7c37c2af":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='cholesterol',y='thalassemia',data=data,hue='target')\nplt.show()","06a48a7a":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='thalassemia',y='resting_blood_pressure',data=data,hue='target')\nplt.show()","a8e6a64c":"plt.figure(figsize=(20, 10))\nplt.scatter(x=data.age[data.target==1], y=data.thalassemia[(data.target==1)], c=\"green\")\nplt.scatter(x=data.age[data.target==0], y=data.thalassemia[(data.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","c479c963":"sns.pairplot(data=data)","11591b27":"data.hist()","77db7d39":"# store numeric variables in cnames\ncnames=['age','resting_blood_pressure','cholesterol','max_heart_rate_achieved','st_depression','num_major_vessels']","069d1fc6":"#Set the width and height of the plot\nf, ax = plt.subplots(figsize=(7, 5))\n\n#Correlation plot\ndf_corr = data.loc[:,cnames]\n#Generate correlation matrix\ncorr = df_corr.corr()\n\n#Plot using seaborn library\nsns.heatmap(corr, annot = True, cmap='coolwarm',linewidths=.1)\nplt.show()","bf376e28":"df_corr = data.loc[:,cnames]\ndf_corr","cfde0142":"from sklearn.model_selection import train_test_split\n\npredictors = data.drop(\"target\",axis=1)\ntarget = data[\"target\"]\n\nX_train,X_test,Y_train,Y_test = train_test_split(predictors,target,test_size=0.20,random_state=0)\nprint(\"Training features have {0} records and Testing features have {1} records.\".\\\n      format(X_train.shape[0], X_test.shape[0]))","bfb09411":"X_train.shape","0614f045":"X_test.shape","8313b5c0":"Y_train.shape","7c534b0a":"Y_test.shape","73252dfd":"from sklearn.metrics import accuracy_score","fdfa1492":"def train_model(X_train, y_train, X_test, y_test, classifier, **kwargs):\n    \n    \"\"\"\n    Fit the chosen model and print out the score.\n    \n    \"\"\"\n    \n    # instantiate model\n    model = classifier(**kwargs)\n    \n    # train model\n    model.fit(X_train,y_train)\n    \n    # check accuracy and print out the results\n    fit_accuracy = model.score(X_train, y_train)\n    test_accuracy = model.score(X_test, y_test)\n    \n    print(f\"Train accuracy: {fit_accuracy:0.2%}\")\n    print(f\"Test accuracy: {test_accuracy:0.2%}\")\n    \n    return model","d6d9f75f":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nlogreg.fit(X_train, Y_train)\n\ny_pred_lr = logreg.predict(X_test)\nprint(y_pred_lr)","79adf431":"score_lr = round(accuracy_score(y_pred_lr,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")","2585ebf1":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nprint(confusion_matrix(Y_test,y_pred_lr))\nprint(classification_report(Y_test,y_pred_lr))\nprint(\"Accuracy:\",accuracy_score(Y_test, y_pred_lr))","c41310c9":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nmodel = train_model(X_train, Y_train, X_test, Y_test, LogisticRegression)","46b272ee":"#Logistic Regression supports only solvers in ['liblinear', 'newton-cg'<-93.44, 'lbfgs'<-91.8, 'sag'<-72.13, 'saga'<-72.13]\nclf = LogisticRegression(random_state=0, solver='newton-cg').fit(X_test, Y_test)\n#The solver for weight optimization.\n#'lbfgs' is an optimizer in the family of quasi-Newton methods.\nclf.score(X_test, Y_test)","4c9bb9ee":"from sklearn.metrics import confusion_matrix","8a5a651f":"matrix= confusion_matrix(Y_test, y_pred_lr)","f6f67222":"sns.heatmap(matrix,annot = True, fmt = \"d\")","3ed75df7":"from sklearn.metrics import precision_score","a0af3da1":"precision = precision_score(Y_test, y_pred_lr)","4d2612da":"print(\"Precision: \",precision)","98d49deb":"from sklearn.metrics import recall_score","930c43c9":"recall = recall_score(Y_test, y_pred_lr)","dcef5089":"print(\"Recall is: \",recall)","b8579e9d":"print((2*precision*recall)\/(precision+recall))","d2e1d130":"from sklearn.ensemble import RandomForestClassifier\nrandfor = RandomForestClassifier(n_estimators=100, random_state=0)\n\nrandfor.fit(X_train, Y_train)\n\ny_pred_rf = randfor.predict(X_test)\nprint(y_pred_rf)","3bc8b457":"from sklearn.model_selection import learning_curve\n# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(), \n                                                        X_train, \n                                                        Y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=10,\n                                                        # Evaluation metric\n                                                        scoring='accuracy',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 50))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","d8f553dd":"score_rf = round(accuracy_score(y_pred_rf,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Random Forest is: \"+str(score_rf)+\" %\")","7fa4a1a3":"#Random forest with 100 trees\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\nrf.fit(X_train, Y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train, Y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test, Y_test)))","fbae753a":"rf1 = RandomForestClassifier(max_depth=3, n_estimators=100, random_state=0)\nrf1.fit(X_train, Y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(rf1.score(X_train, Y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(rf1.score(X_test, Y_test)))","8feeaca2":"from sklearn.metrics import confusion_matrix","8a348c9a":"matrix= confusion_matrix(Y_test, y_pred_rf)","839f5f2e":"sns.heatmap(matrix,annot = True, fmt = \"d\")","655e0d6d":"from sklearn.metrics import precision_score","c428f80c":"precision = precision_score(Y_test, y_pred_rf)","36983901":"print(\"Precision: \",precision)","4d7607b1":"from sklearn.metrics import recall_score","3914006e":"recall = recall_score(Y_test, y_pred_rf)","72485d14":"print(\"Recall is: \",recall)","9ed5c323":"print((2*precision*recall)\/(precision+recall))","71252a2b":"from sklearn.naive_bayes import GaussianNB\nnb = train_model(X_train, Y_train, X_test, Y_test, GaussianNB)\n\nnb.fit(X_train, Y_train)\n\ny_pred_nb = nb.predict(X_test)\nprint(y_pred_nb)","00560731":"score_nb = round(accuracy_score(y_pred_nb,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Naive Bayes is: \"+str(score_nb)+\" %\")","722934b6":"#Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nmodel = train_model(X_train, Y_train, X_test, Y_test, GaussianNB)","df057100":"from sklearn.metrics import confusion_matrix","627b8dd5":"matrix= confusion_matrix(Y_test, y_pred_nb)","aafcdee7":"sns.heatmap(matrix,annot = True, fmt = \"d\")","dce40986":"from sklearn.metrics import precision_score","9f29c21c":"precision = precision_score(Y_test, y_pred_nb)","3856c040":"print(\"Precision: \",precision)","108c7742":"from sklearn.metrics import recall_score","6e43fbff":"recall = recall_score(Y_test, y_pred_nb)","dd678c5f":"print(\"Recall is: \",recall)","1f786bfb":"print((2*precision*recall)\/(precision+recall))","8012b7b6":"from sklearn.neighbors import KNeighborsClassifier\nknn = train_model(X_train, Y_train, X_test, Y_test, KNeighborsClassifier, n_neighbors=8)\n\nknn.fit(X_train, Y_train)\n\ny_pred_knn = knn.predict(X_test)\nprint(y_pred_knn)","ee6f8ba1":"score_knn = round(accuracy_score(y_pred_knn,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using KNN is: \"+str(score_knn)+\" %\")","8a2021fd":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = train_model(X_train, Y_train, X_test, Y_test, KNeighborsClassifier)","ab8aa1f7":"# Seek optimal 'n_neighbours' parameter\nfor i in range(1,10):\n    print(\"n_neigbors = \"+str(i))\n    train_model(X_train, Y_train, X_test, Y_test, KNeighborsClassifier, n_neighbors=i)","ee18e52c":"from sklearn.metrics import confusion_matrix","57074216":"matrix= confusion_matrix(Y_test, y_pred_knn)","0b6e8dc5":"sns.heatmap(matrix,annot = True, fmt = \"d\")","913b6983":"from sklearn.metrics import precision_score","fa05fd7e":"precision = precision_score(Y_test, y_pred_knn)","397c5cb8":"print(\"Precision: \",precision)","2620241f":"from sklearn.metrics import recall_score","9a2adbee":"recall = recall_score(Y_test, y_pred_knn)","a2e9eded":"print(\"Recall is: \",recall)","e7e13703":"print((2*precision*recall)\/(precision+recall))","8603a71f":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth=3, random_state=0)\n\ndt.fit(X_train, Y_train)\n\ny_pred_dt = dt.predict(X_test)\nprint(y_pred_dt)","ab950bda":"score_dt = round(accuracy_score(y_pred_dt,Y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")","e2a11d09":"from sklearn.tree import DecisionTreeClassifier\ntree1 = DecisionTreeClassifier(random_state=0)\ntree1.fit(X_train, Y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree1.score(X_train, Y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree1.score(X_test, Y_test)))","ff97844d":"tree1 = DecisionTreeClassifier(max_depth=3, random_state=0)\ntree1.fit(X_train, Y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree1.score(X_train, Y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree1.score(X_test, Y_test)))","a8e155ae":"from sklearn.metrics import confusion_matrix","cfab5d84":"matrix= confusion_matrix(Y_test, y_pred_dt)","66631377":"sns.heatmap(matrix,annot = True, fmt = \"d\")","6e519f06":"from sklearn.metrics import precision_score","16b4e24a":"precision = precision_score(Y_test, y_pred_dt)","aa8b8e7f":"print(\"Precision: \",precision)","13510a0a":"from sklearn.metrics import recall_score","5fc0e501":"recall = recall_score(Y_test, y_pred_dt)","0626e8f5":"print(\"Recall is: \",recall)","993aebc8":"print((2*precision*recall)\/(precision+recall))","e90ffeb9":"# initialize an empty list\naccuracy = []\n\n# list of algorithms names\nclassifiers = ['KNN', 'Decision Trees', 'Logistic Regression', 'Naive Bayes', 'Random Forests']\n\n# list of algorithms with parameters\nmodels = [KNeighborsClassifier(n_neighbors=8), DecisionTreeClassifier(max_depth=3, random_state=0), LogisticRegression(), \n        GaussianNB(), RandomForestClassifier(n_estimators=100, random_state=0)]\n\n# loop through algorithms and append the score into the list\nfor i in models:\n    model = i\n    model.fit(X_train, Y_train)\n    score = model.score(X_test, Y_test)\n    accuracy.append(score)","7255267c":"# create a dataframe from accuracy results\nsummary = pd.DataFrame({'accuracy':accuracy}, index=classifiers)       \nsummary","9a245b4f":"scores = [score_lr,score_nb,score_knn,score_dt,score_rf]\nalgorithms = [\"Logistic Regression\",\"Naive Bayes\",\"K-Nearest Neighbors\",\"Decision Tree\",\"Random Forest\"] \nsns.set(rc={'figure.figsize':(15,8)})\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy score\")\n\nsns.barplot(algorithms,scores)","296e4d65":"# Analysing the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)","5a9e8179":"# F-Score","55739fb4":"### comparing with target","9b88ac0d":"### This shows that most columns are moderately correlated with target, but 'fbs' is very weakly correlated.","c83b06a9":"## Health rate vs age","81ae2e51":"# Recall","aa6efa34":"Slope '2' causes heart pain much more than Slope '0' and '1'","e562bd3c":"# F score","d048d1cf":"### plotting the thalassemia distribution (0,1,2,3)","6aa3e1c1":"performance metrics\n-Accuracy: is the ratio between the number of correct predictions and total number of predications.\n\n$acc = \\frac{TP + TN}{TP + TN + FP + FN}$\n\n-Precision: is the ratio between the number of correct positives and the number of true positives plus the number of false positives.\n\n$Precision (p) = \\frac{TP}{TP + FP}$\n\n-Recall: is the ratio between the number of correct positives and the number of true positives plus the number of false negatives.\n\n$recall = \\frac{TP}{TP + FN}$\n\n-F-score: is known as the harmonic mean of precision and recall.\n\n$acc = \\frac{1}{\\frac{1}{2}(\\frac{1}{p}+\\frac{1}{r})} = \\frac{2pr}{p+r}$\n\n-Problem characteristics in context of our case study:\n\nTP = True positive (has heart disease). TN = True negative (has no heart disease). FP = False positive (has no heart disease) FN = False negative (has heart disease)","f527326e":"## confusion matrix of Naive Bayes","c04e6ef9":"### num_major_vessels=4 has astonishingly large number of heart patients","75d1f23b":"# recall","bb340751":"### count num_major vessels","ec61ed6b":"# f score","1cfe0547":"# recall","c1604ad4":"\n\n---\n\n","c99f746c":"##Correlation analysis","1a157231":"\n\n---\n\n","8557946f":"# Heart disease according to Fasting Blood sugar ","da8be3d9":"###So, we have no missing values","df990564":"# Correlation plot","6f4cd691":"# Percentage of patient with or without heart problems in the given dataset","b597d16e":"# Final Score","6f45823b":"# Naive Bayes","d3ed2b0e":"# Analysing the Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)","dc149f30":"# precision score","18fc4837":"# f score","d736fc8b":"# precision Score","ae54185a":"It turns out that value of n_neighbours (8) is optimal.","1ec6ed1b":"## Confusion matrix","6a5a7ab6":"# Exploratory Data Analysis (EDA)","6c3834b5":"### comparing with target","4d61c99a":"# Modelling and predicting with Machine Learning\nThe main goal of the entire project is to predict heart disease occurrence with the highest accuracy. In order to achieve this, we will test several classification algorithms. This section includes all results obtained from the study and introduces the best performer according to accuracy metric. I have chosen several algorithms typical for solving supervised learning problems throughout classification methods.\n\nFirst of all, let's equip ourselves with a handy tool that benefits from the cohesion of SciKit Learn library and formulate a general function for training our models. The reason for displaying accuracy on both, train and test sets, is to allow us to evaluate whether the model overfits or underfits the data (so-called bias\/variance tradeoff).","647626d3":"# Analysing The person's resting blood pressure (mm Hg on admission to the hospital)","94f2e26c":"1. age: The person's age in years\n\n2. sex: The person's sex (1 = male, 0 = female)\n\n3. cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\n4. trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\n5. chol: The person's cholesterol measurement in mg\/dl\n\n6. fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n\n7. restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\n8. thalach: The person's maximum heart rate achieved\n\n9. exang: Exercise induced angina (1 = yes; 0 = no)\n\n10. oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\n11. slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\n12. ca: The number of major vessels (0-3)\n\n13. thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\n14. target: Heart disease (0 = no, 1 = yes)\n\nHeart disease risk factors to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking . \n\nAccording to another source , the major factors that can't be changed are: increasing age, male gender and heredity. \n\nNote that thalassemia, one of the variables in this dataset, is heredity. \n\nMajor factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. \n\nOther factors include stress, alcohol and poor diet\/nutrition.","07beef40":"# f score","9a1d9c38":"# Splitting the dataset to Train and Test","4550bfa9":"# recall","bd921840":"# precision score","b99b7c6c":"## importing Accuracy score","8e1d7373":"## people with restecg '1' and '0' are much more likely to have a heart disease than with restecg '2'","ff71268f":"# precision score","eaf94cac":"Let's see if KNN can perform even better by trying different 'n_neighbours' inputs.","289d9074":"###People with exercise_induced_angina=1 are much less likely to have heart problems","50213e31":"# Random Forest","71f3a317":"fmt = d is format = default","27584d40":"balance of precision and recall score","bf6fd4ff":"From the total dataset of 303 patients, 165 (54%) have a heart disease (target=1)","e3066751":"Best ACCURACY possible using Logistic regression","fd12c025":"# precision score","3abd1fb3":"# Analysing the chest pain (4 types of chest pain)\n\n#[Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic]","2a2a988a":"# Heart Disease Frequency for ages","0b3035d4":"# thalassemia vs resting blood pressure scatterplot","c14341e7":"# Learning curve for Training score & cross validation score","250b4157":"### Here 0 is female and 1 is male patients","d011f2fd":"Correlation analysis is a method of statistical evaluation used to study the strength of a relationship between two, numerically measured, continuous variables (e.g. height and weight)","1163f693":"# Logistic regression","bef888c2":"# thalassemia and cholesterol scatterplot","27fd12ba":"#Analysing Exercise induced angina (1 = yes; 0 = no)","c68ced15":"# KNN(K Nearest Neighbors)","e15f3489":"# Heart Disease frequency for sex (where 0 is female and 1 is male and \"red\" is have heart disease and \"blue\" is don't have heart disease)","c9872a50":"# Confusion Matrix","cd318623":"The accuracy on the training set is 100%, while the test set accuracy is much worse. This is an indicative that the tree is overfitting and not generalizing well to new data. Therefore, we need to apply pre-pruning to the tree.\n\nWe set max_depth=3, limiting the depth of the tree decreases overfitting. This leads to a lower accuracy on the training set, but an improvement on the test set.","a1a9889e":"# Analysing number of major vessels (0-3) colored by flourosopy","1ba5acad":"## confusion matrix of Random Forest","ea49828a":"# Decision Tree","66daaa93":"# Analysing A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\n","e587e755":"## Confusion Matrix","83c4b1be":"Now, let us prune the depth of trees and check the accuracy.","f896b080":"# recall"}}