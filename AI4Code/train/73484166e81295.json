{"cell_type":{"596afd47":"code","d589342c":"code","39fc9531":"markdown","b0b68083":"markdown","4dd491ba":"markdown"},"source":{"596afd47":"import random\nimport nltk\n# nltk.download()\n\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.probability import FreqDist\nfrom collections import defaultdict\nfrom heapq import nlargest\n\nimport math\nimport networkx\nimport numpy\n\nfrom nltk.tokenize.punkt import PunktSentenceTokenizer\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n\n# Sample data\n# I have taken the abstract of my final MSc Thesis\n# Consists of 10 lines\ntext = \"\"\"Federated edge cloud (FEC) is an edge cloud environment where multiple edge servers in a single administrative domain collaborate together to provide real-time services.\nAs the number of edge servers increases in FEC, the amount of energy consumed by servers and network switches also increases.\nThis creates another challenge for how to schedule delay-sensitive services over FEC, while minimizing the total energy consumption and reducing the QoS violation of a service at the same time.\nThis paper proposes an energy-efficient service scheduling mechanism in FEC called ESFEC, which consists of a placement algorithm and three types of reconfiguration algorithms.\nUnlike traditional approaches, the placement algorithm in ESFEC places delay-sensitive services on the edge servers in nearby edge domain instead of clouds. In addition, ESFEC schedules services with actual traffic requirements rather than using maximum traffic requirements to ensure QoS. This increases the number of services co-located in a single server and thereby reduces the total energy consumed by the services.\nAlthough this approach is likely to increase the number of service migrations in heavy traffic conditions, ESFEC reduces the migration overhead using a reinforcement learning (Q-learning) based reconfiguration algorithm, ESFEC-RL (reinforcement learning), that can dynamically adapt to a changing environment.\nESFEC also includes two different heuristic algorithms such as ESFEC-EF (energy first) and ESFEC-MF (migration first), which are more suitable for real scale scenarios.\nThe simulation results show that the ESFEC improves energy efficiency by up to 28% and lowers the service violation rate by up to 66% against a traditional approach used in the edge cloud environment.\"\"\"\n\n\ndef tokenize_content(content):\n\n    stop_words = set(stopwords.words('english') + list(punctuation))\n    # separating the input text by earch word.\n    words = word_tokenize(content.lower())\n    return (sent_tokenize(content), [word for word in words if word not in stop_words])\n\n\ndef score_tokens(sent_tokens, word_tokens):\n    # FreqDist is basically counting words in the \"text\"\n    word_freq = FreqDist(word_tokens)\n    # print(word_freq.keys())\n    # initializing a dictionary\n    rank = defaultdict(int)\n    for i, sentence in enumerate(sent_tokens):\n        for word in word_tokenize(sentence.lower()):\n            if word in word_freq:\n                # ranking each sentence in the input text\n                rank[i] += word_freq[word]\n    # print(rank)\n    return rank\n\n\ndef summarize(ranks, sentences):\n    # I set 4 as the parameter because I wanted my summary to be 5 sentences long.\n    # It can be chnaged\n    indices = nlargest(4, ranks, key=ranks.get)\n    final_summary = [sentences[j] for j in indices]\n    return ' '.join(final_summary)\n\n\n# print(text)  # contains 10 lines\nprint(\"---------------------------------------------------------------------------------------------------------------------------------------------\")\n# sentence tokens and word tokens, separated by each sentence and each word\nsent_tokens, word_tokens = tokenize_content(text)\nsent_ranks = score_tokens(sent_tokens, word_tokens)\nprint(summarize(sent_ranks, sent_tokens))","d589342c":"def getrank(document):\n\n    # separating each line whenever a \".\" is detected\n    sentences = PunktSentenceTokenizer().tokenize(document)\n    vectorizer = CountVectorizer()\n    bow_matrix = vectorizer.fit_transform(sentences)\n    # print(vectorizer.get_feature_names())\n    # print(bow_matrix.toarray())\n    # print(\"---------\")\n\n    normalized = TfidfTransformer().fit_transform(bow_matrix)\n    # print(normalized.toarray())\n\n    similarity_graph = normalized * normalized.T\n\n    nx_graph = networkx.from_scipy_sparse_matrix(similarity_graph)\n    values = networkx.pagerank(nx_graph)\n    sentence_array = sorted(\n        ((values[i], s) for i, s in enumerate(sentences)), reverse=True)\n\n    sentence_array = numpy.asarray(sentence_array)\n\n    freq_max = float(sentence_array[0][0])\n    freq_min = float(sentence_array[len(sentence_array) - 1][0])\n\n    temp_array = []\n    for i in range(0, len(sentence_array)):\n        if freq_max - freq_min == 0:\n            temp_array.append(0)\n        else:\n            temp_array.append(\n                (float(sentence_array[i][0]) - freq_min) \/ (freq_max - freq_min))\n\n    threshold = (sum(temp_array) \/ len(temp_array)) + 0.25\n\n    sentence_list = []\n\n    for i in range(0, len(temp_array)):\n        if temp_array[i] > threshold:\n            sentence_list.append(sentence_array[i][1])\n\n    seq_list = []\n    for sentence in sentences:\n        if sentence in sentence_list:\n            seq_list.append(sentence)\n\n    return seq_list\n\ntextSummarized = getrank(text)\nprint(textSummarized)","39fc9531":"# Summarization using PageRank Algorithm","b0b68083":"# Paragraph Summary - NLP\n### A very basic text summarization technique | Beginners \nA python code to summarize a paragraph into given number of sentences.\nIn this example my given input paragraph is 10 sentences long. The summary consists of 4 lines. \n\n**The algorithm scores each sentence and based on the score priority, the final summary is created.** \n\n## Libraries\n\n* nltk\n* collections\n* heapq\n---------------------------","4dd491ba":"### Library\n- networkx"}}