{"cell_type":{"0a52bbd2":"code","a55321e3":"code","a0c7773d":"code","d751b67e":"code","0e2ffcfc":"code","810a08e7":"code","9f3fa194":"code","e756a612":"code","057a50c4":"code","63a97931":"code","49d5935a":"code","516ea07c":"code","f26edbd3":"code","8a159ed6":"code","82d417d1":"code","103eed80":"code","84e9b27d":"code","6b4547be":"code","a238cbb2":"code","1fbad3c0":"code","c97f7cd5":"code","0e23ca54":"code","5aa6e4bb":"code","dcb0517a":"code","accc5fad":"code","18d721d3":"code","6a05d257":"code","4b457f32":"code","6289edb6":"code","ac686b0d":"code","6f31bc67":"code","770c18b4":"code","e3328b2b":"code","d6945529":"code","a60a44dd":"code","eaa7e502":"code","38ab4a56":"code","bc98eb0c":"code","a7b296b8":"code","0a07afa8":"code","d23986f5":"code","a0f8cf81":"code","d34a7c4b":"code","d69caa89":"markdown","6d9a5a30":"markdown","808456c7":"markdown","6b48ded9":"markdown","bd2a7055":"markdown","3a43e155":"markdown","08d65538":"markdown","f04f3d47":"markdown","286c2db2":"markdown","884e6726":"markdown","63b2c872":"markdown","92160e09":"markdown","8e89386f":"markdown","e099a47f":"markdown","999433f2":"markdown","4a10efa1":"markdown","1b06b9bf":"markdown","4d65aaba":"markdown","f940a775":"markdown","ca75724a":"markdown","4a647976":"markdown","fe521a0a":"markdown","58b76b78":"markdown","2cfd6670":"markdown","68bfec8c":"markdown","d2c8a315":"markdown","60f272fa":"markdown","231d78b0":"markdown","d77f4a2e":"markdown","761601b0":"markdown","7e45aa07":"markdown","49a4ddb2":"markdown","0869eef2":"markdown","40201685":"markdown","9368f64f":"markdown","e3d9af4b":"markdown","b962bb3a":"markdown"},"source":{"0a52bbd2":"## All minimum setup and libraries required\n\nimport numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n## TENSORFLOW        \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer   ## Generate dictionary of word encodings\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n## GENSIM and NLTK\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport nltk\nnltk.download('wordnet')\n\n# SKLEARN\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\n\nprint(\"Tensorflow\\t-\\t\",tf.__version__)\nprint(\"NLTK\\t\\t-\\t\",nltk.__version__)\nprint(\"Gensim\\t\\t-\\t\",nltk.__version__)","a55321e3":"path = \"..\/input\/medium-articles-with-content\/Medium_AggregatedData.csv\"\ndataframe_full = pd.read_csv(path)\ndataframe_imp = pd.read_csv(path)\nprint(\"Dataset have been read\")","a0c7773d":"dataframe_full.head()","d751b67e":"x = dataframe_full['name'][10]\ny = dataframe_full['publicationdescription'][15]\nprint(x)\nprint(y)\nprint(dataframe_full.shape)\nprint(dataframe_full['name'][10])\nprint(dataframe_full['name'][11])\nprint(dataframe_full['name'][12])\nprint(dataframe_full['title'][10])","0e2ffcfc":"print(dataframe_full.columns)","810a08e7":"required_col = ['language','subTitle','tagsCount','text','title','url','wordCount','publicationdescription'\n               ,'tag_name','name']\nmost_imp_col = ['subTitle','text','title']","9f3fa194":"# article_titles = dataframe_full['title']\n# art_grp_1 = article_titles[16:25]\n# print(art_grp_1)\nprint(dataframe_full.language.unique())","e756a612":"## Number of rows english rows\n\nenglish_titles = dataframe_full[dataframe_full['language'] == 'en']\n# english_titles.head()\nprint(english_titles.shape)","057a50c4":"## Number of rows dropped after removing null value rows\n\nprint(dataframe_imp.shape)\ndataframe_imp.dropna(how = 'all')\nprint(dataframe_imp.shape)","63a97931":"## After dropping non-english and columns that are not required really\n\ndataframe_imp.drop(dataframe_imp[dataframe_imp['language'] != 'en'].index, inplace = True)\n\ndataframe_imp = dataframe_imp.drop(['audioVersionDurationSec', 'codeBlock', 'codeBlockCount',\n       'collectionId', 'createdDate', 'createdDatetime', 'firstPublishedDate',\n       'firstPublishedDatetime', 'imageCount', 'isSubscriptionLocked',\n       'language', 'latestPublishedDate', 'latestPublishedDatetime',\n       'linksCount', 'postId', 'readingTime', 'recommends',\n       'responsesCreatedCount', 'socialRecommendsCount','tagsCount','totalClapCount', 'uniqueSlug',\n       'updatedDate', 'updatedDatetime', 'url', 'vote', 'wordCount',\n       'publicationdescription', 'publicationdomain',\n       'publicationfacebookPageName', 'publicationfollowerCount',\n       'publicationname', 'publicationpublicEmail', 'publicationslug',\n       'publicationtags', 'publicationtwitterUsername', 'tag_name', 'slug',\n       'name', 'postCount', 'author', 'bio', 'userId', 'userName',\n       'usersFollowedByCount', 'usersFollowedCount', 'scrappedDate'], axis=1)\n\ndataframe_imp['index'] = dataframe_imp.index\n\ndataframe_imp.shape","49d5935a":"## Run these cell to export the new trimmed down dataset\n\n# dataframe_imp.to_csv(\"medium_dataset.csv\",sep=\",\")\n# new_ds_size = os.stat(\"medium_dataset.csv\").st_size\n# new_ds_size = new_ds_size \/ 1000000\n# print(\"New dataset size in MB = \",new_ds_size)","516ea07c":"## Keeping 170000 rows greatly reduces the dataset size to around 100MB\n\nvery_reduced_dataset = dataframe_imp[:17000]\n# very_reduced_dataset.to_csv(\"very_reduced_dataset.csv\",sep=\",\")\n# print(\"New dataset size in MB = \",os.stat(\"very_reduced_dataset.csv\").st_size \/ 1000000)\n\n# from IPython.display import FileLink\n# FileLink(r'very_reduced_dataset.csv')","f26edbd3":"dataframe_imp.head()","8a159ed6":"print(dataframe_imp.title[15])\nprint(dataframe_imp.subTitle[15])\n# print(dataframe_imp.text[15])      ## Text is too huge to be displayed\nprint(dataframe_imp.index[15])","82d417d1":"## Stemmer initialization for english\nstemmer = SnowballStemmer(\"english\")","103eed80":"## Functions for lemmatization, removal of Stopwords\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","84e9b27d":"### Code to check the function\n\n## Run this for faster execution time wiht less acuracy\ndoc_sample = very_reduced_dataset[very_reduced_dataset['index'] == 1000].values[0][2]\n\n## Run this for slower execution but better accuracy\n# doc_sample = dataframe_imp[dataframe_imp['index'] == 1000].values[0][2]\n\nprint('original document: ')\nwords = []\nfor word in doc_sample.split(' '):\n    words.append(word)\n    \nprint(dataframe_imp[dataframe_imp['index'] == 1000].values[0][2])\nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))","6b4547be":"## Use this to reduce the training time at the cost of loss of rows and some accuracy.\ntitle_list = very_reduced_dataset['title'].astype(str)\n\n## Use this for higher model accuracy but very slow operating time. \n# title_list = dataframe_imp['title'].astype(str)   ## using astype(str) eliminates the floting type error\ntitle_list.describe()","a238cbb2":"## The titles are preprocessed and saved into processd_titles\n## The map function applies the preprocess method on each of the list entries\n\nprocessed_titles = title_list.map(preprocess)\nprocessed_titles[30:40]","1fbad3c0":"## bow --> Bag of Words\n\nbow = gensim.corpora.Dictionary(processed_titles)\n\n## Finding out words with a min_occurance = 10\n\nmin_occurance = 10\ncount = 0\nfor k, v in bow.iteritems():\n    print(k, v)\n    count += 1\n    if count > min_occurance:    # We can limit the selection based on the frequency\n        break","c97f7cd5":"bow.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\nprint(bow)","0e23ca54":"bow_corpus = [bow.doc2bow(doc) for doc in processed_titles]\nbow_corpus[:10]","5aa6e4bb":"## A example of the BOW for the 1000th title\n\nbow_example = bow_corpus[1000]\nfor i in range(len(bow_example)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_example[i][0], \n           bow[bow_example[i][0]], \n           bow_example[i][1]))","dcb0517a":"from gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\n# from pprint import pprint\n\nfor i in corpus_tfidf:\n    print(i)\n    break","accc5fad":"## LDA when the num_topics = 10\n\n# lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=bow, passes=5, workers=3)\nlda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                       id2word=bow,\n                                       num_topics=10, \n                                       random_state=100,\n                                       chunksize=100,\n                                       passes=10,\n                                       per_word_topics=True)\nprint(lda_model)\n# For optimal performance time set the workers as no of CPU cores-1","18d721d3":"for idx, topic in lda_model.print_topics(-1):\n    print('\\nTopic: {} \\nWords: {}'.format(idx, topic))\ndoc_lda = lda_model[bow_corpus]","6a05d257":"topics = lda_model.show_topics(formatted=False)\ntopic_words = dict(topics[1][1])\nprint(\"For topic 1, the words are: \",topic_words)","4b457f32":"%pylab inline\n\nimport pandas as pd\nimport pickle as pk\nfrom scipy import sparse as sp","6289edb6":"## Wordcloud of Top N words in each topic\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 5, figsize=(18,10), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","ac686b0d":"from collections import Counter\ntopics = lda_model.show_topics(formatted=False)\ndata_flat = [w for w_list in dataframe_imp for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 5, figsize=(18,10), sharey=True, dpi=160)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    ax_twin.set_ylim(0, 0.08); ax.set_ylim(0, 3500)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \nplt.show()","6f31bc67":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary=lda_model.id2word)\nvis","770c18b4":"from gensim.models import CoherenceModel\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=processed_titles, dictionary=lda_model.id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","e3328b2b":"# Compute Coherence Score using UMass\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=processed_titles, dictionary=lda_model.id2word, coherence=\"u_mass\")\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","d6945529":"def compute_coherence_values(dictionary, corpus, texts, limit,cs_type, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model=gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence=cs_type)\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","a60a44dd":"model_list, coherence_values = compute_coherence_values(dictionary=bow, corpus=bow_corpus, texts=processed_titles, start=2, limit=100, step=5,cs_type = 'c_v')\n# Show graph\nimport matplotlib.pyplot as plt\nlimit=100; start=2; step=5;\nx = range(start, limit, step)\nplt.plot(x, coherence_values, marker='o')\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","eaa7e502":"model_list, coherence_values = compute_coherence_values(dictionary=bow, corpus=bow_corpus, texts=processed_titles, start=2, limit=100, step=5,cs_type = 'u_mass')\n# Show graph\nimport matplotlib.pyplot as plt\nlimit=100; start=2; step=5;\nx = range(start, limit, step)\nplt.plot(x, coherence_values,marker = 'o')\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","38ab4a56":"## supporting function with num_topics = 60\n\ndef compute_coherence_values(corpus, dictionary, k, a, b):\n    \n    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                           id2word=bow,\n                                           num_topics=60, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=a,\n                                           eta=b,\n                                           per_word_topics=True)\n    \n    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_titles, dictionary=lda_model.id2word, coherence='c_v')\n    \n    return coherence_model_lda.get_coherence()","bc98eb0c":"import numpy as np\nimport tqdm\ngrid = {}\ngrid['Validation_Set'] = {}\n# Topics range\n# min_topics = 2\n# max_topics = 11\n# step_size = 1\n# topics_range = range(min_topics, max_topics, step_size)\n\n# Alpha parameter\n# alpha = list(np.arange(0.01, 1, 0.3))\nalpha = np.array([0.05,0.1,0.5,1,5,10])\n# alpha.append('symmetric')\n# alpha.append('asymmetric')\n\n# Beta parameter\n# beta = list(np.arange(0.01, 1, 0.3))\nbeta = np.array([0.05,0.1,0.5,1,5,10])\n# beta.append('symmetric')\n\n# Validation sets\nnum_of_docs = len(bow_corpus)\ncorpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n               gensim.utils.ClippedCorpus(bow_corpus, int(num_of_docs*0.75)), \n               bow_corpus]\ncorpus_title = ['75% Corpus', '100% Corpus']\nmodel_results = {'Validation_Set': [],\n                 'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': []\n                }\nk = 45\n# Can take a long time to run\n# if 1 == 1:\npbar = tqdm.tqdm(total=200, position=0, leave=True)\n\n\n# iterate through validation corpuses\nfor i in range(len(corpus_sets)):\n    # iterate through alpha values\n    for a in alpha:\n        # iterare through beta values\n        for b in beta:\n            # get the coherence score for the given parameters\n            cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=lda_model.id2word, \n                                          k=k, a=a, b=b)\n            # Save the model results\n            model_results['Validation_Set'].append(corpus_title[i])\n            model_results['Topics'].append(k)\n            model_results['Alpha'].append(a)\n            model_results['Beta'].append(b)\n            model_results['Coherence'].append(cv)\n            pbar.update(1)\npd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\npbar.close()","a7b296b8":"dataframe_results = pd.read_csv(\".\/lda_tuning_results.csv\")\ndataframe_results\ntopics = dataframe_results['Topics']\ncoherence_values = dataframe_results['Coherence']\ndataframe_results['Topics'].unique()","0a07afa8":"optimal = dataframe_results[dataframe_results['Validation_Set'] == '100% Corpus']\nmax_coherence_val = optimal['Coherence'].max()\noptimal = dataframe_results[dataframe_results['Coherence'] == max_coherence_val]\nprint(optimal)\n# print(val_set)","d23986f5":"final_lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n                                             id2word=bow,\n                                             num_topics=60,\n                                             random_state=100,\n                                             chunksize=100,\n                                             passes=10,\n                                             alpha = 0.5,\n                                             eta=0.1, \n                                             per_word_topics=True)","a0f8cf81":"final_coherence_model_lda = CoherenceModel(model=final_lda_model, texts=processed_titles, dictionary=lda_model.id2word, coherence='c_v')\nfinal_coherence_score = final_coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ',final_coherence_score )","d34a7c4b":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(final_lda_model, bow_corpus, dictionary=bow)\nvis","d69caa89":"### CASE_2: With u_mass coherence score and num_topics","6d9a5a30":"# Step_7: Final Model\nThe row with the max Coherence Score(c_v) of 0.631025 has `beta = 0.1` and `alpha = 0.5`\nNow training the new model with these values","808456c7":"### Perform lemmatization and stem preprocessing steps on the data set","6b48ded9":"# Step_6: Coherence score (base model)\n> ****Topic Coherence**** measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n\n> A set of statements or facts is said to be ****coherent****, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is \u201cthe game is a team sport\u201d, \u201cthe game is played with a ball\u201d, \u201cthe game demands great physical efforts\u201d","bd2a7055":"### Preview of the BOW for the preprocessed titles","3a43e155":"## Using c_v measure","08d65538":"### Grid search on the three hyper parameters wrt c_v Coherence score","f04f3d47":"# Step_2: Creation of the Bag of words\nBag of words is a frequency count of the words occuring in the `preprocessed_docs`","286c2db2":"### CASE_1: With c_v coherence score and num_topics","884e6726":"**For each topic, we will explore the words occuring in that topic and its relative weight.**","63b2c872":"# Step_3: TF-IDF \nTF-IDF stands for ***term frequency\u2013inverse document frequency***. The higher the TF-IDF score the rarer a word is in a given corpus and vice-versa. We will be using the TF-IDF model for the gensim models library.","92160e09":"# Step_5: Visualization \n1. Topic modeling with LDA\n2. Visualizing topic models with pyLDAvis\n3. Visualizing LDA results with t-SNE and bokeh","8e89386f":"### Creating the base model","e099a47f":"For each title we create a dictionary reporting how many words and how many times those words appear. This is saved to the `bow_corpus`.\n\n##### **NOTE:** This step gives a simmillar result for a very small corpus such as title of the articles, but it is important while working on the actual body of the articles.","999433f2":"### Generating the doc2bow dictionary","4a10efa1":"## Wordcloud of Top N words in each topic","1b06b9bf":"These are the base scores for the default model","4d65aaba":"## Word count vs Weights of topics","f940a775":"## Using UMass Measure","ca75724a":"**after dropping all the non english rows and after dropping all non essential columns `dataframe_imp` is the required dataframe****","4a647976":"So nothing is missing in any rows","fe521a0a":"From the above plot it is evident that coherence score increases\nwith respect to the `num_topics` till it reaches somewhere\nbetween 55 to 65 and declines thereafter. A `num_topics` value of 60\nwill have a good coherence score","58b76b78":"***Choose one among the two in the cell below***","2cfd6670":"# Step_4: Running LDA algo on the bag of words\nTesting LDA(Latent Dirichlet allocation) on the BOW. We will be training our LDA model using `gensim.models.LdaMulticore` and save it to `lda_model`","68bfec8c":"<h1 align=\"center\">Topic modeling on Medium articles<\/h1>","d2c8a315":"#### The row with the max Coherence Score(c_v) of 0.63341 has `beta = 0.1` and `alpha = 0.5`","60f272fa":"There is a little improvement of the coherence score of the final model over the base model","231d78b0":"### Filtering tokens based on\n* less than 15 occurances in titles\n* more than 0.5 of total titles\n* after the above two steps, keep only the first 100000 most frequent tokens.\n","d77f4a2e":"# Step_6: Hyperparameter tuning\nThe model hyperparameters to be determined are:\n* num_topics (k)\n* alpha (document density)\n* beta (Word-Topic Density)","761601b0":"<h3 style=\"color:blue;\">Here each topic can be visualized as recipie of the probability of the words that can appear in it.<\/h3>In geometry, a simplex is a generalization of the notion of a triangle or tetrahedron to arbitrary dimensions [source] (https:\/\/en.wikipedia.org\/wiki\/Simplex). LDA space is a simplex since we are dealing wiht probability distributions here. Dimensionality of the space depends on the number of topics we ask the model to make.","7e45aa07":"### Reduced dataset with 3col and ~20000 rows for reff.\nRun these cells only to produce the reduced and compact dataset","49a4ddb2":"### Topic distribution generated by the model","0869eef2":"### LDA visualization on the final model","40201685":"### Processed titles","9368f64f":"The required columns are:\n* language\n* subTitle\n* tagsCount\n* text\n* title\n* url\n* wordCount\n* publicationdescription\n* tag_name\n* name\n\nbut the most important columns are primarily:\n* subTitle\n* text\n* title","e3d9af4b":"**There are ~300000 entries**","b962bb3a":"# Step_1: Preprocessing and cleaning"}}