{"cell_type":{"29e857f1":"code","d156a14e":"code","640c6f47":"code","b30c6066":"code","3c81aa83":"code","80af6527":"code","a84ce74c":"code","c2b10fd9":"code","44f7b1ff":"code","5d0100b1":"code","207e33ef":"code","16d1cd25":"code","f487e560":"code","86592e62":"code","bcd6210c":"code","38d1e14a":"code","d332be8c":"code","639524da":"code","3933b446":"code","18e1fdaf":"code","e3b3a665":"code","9812454c":"code","f66ec727":"code","d24cc14a":"code","097a0466":"code","795aac46":"code","bfca5c89":"code","ae2896f3":"code","628c64be":"code","fcee71e6":"code","5513621e":"code","70f17c22":"code","340ce198":"markdown","f0d7c939":"markdown","b1438e3d":"markdown"},"source":{"29e857f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nfrom keras.datasets import imdb\nfrom keras.preprocessing.sequence import pad_sequences \nfrom keras.models import Sequential\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import SimpleRNN,Dense,Activation\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d156a14e":"(X_train,Y_train),(X_test,Y_test) = imdb.load_data(path=\"imdb.npz\",num_words=None,skip_top=0,maxlen=None,start_char=1,seed=13,oov_char=2,index_from=3)","640c6f47":"print(\"Type: \", type(X_train))\nprint(\"Type: \", type(Y_train))","b30c6066":"print(\"X train shape: \",X_train.shape)\nprint(\"Y train shape: \",Y_train.shape)","3c81aa83":"print(\"Y train values: \",np.unique(Y_train))\nprint(\"Y test values: \",np.unique(Y_test))","80af6527":"unique,counts = np.unique(Y_train,return_counts=True)\nprint(\"Y train distribution: \", dict(zip(unique,counts)))","a84ce74c":"unique,counts = np.unique(Y_test,return_counts=True)\nprint(\"Y test distribution: \", dict(zip(unique,counts)))","c2b10fd9":"plt.figure();\nsns.countplot(Y_train);\nplt.xlabel(\"Classes\");\nplt.ylabel(\"Frequency\");\nplt.title(\"Y Train\");","44f7b1ff":"plt.figure();\nsns.countplot(Y_test);\nplt.xlabel(\"Classes\");\nplt.ylabel(\"Frequency\");\nplt.title(\"Y Test\");","5d0100b1":"print(X_train[0])","207e33ef":"review_len_train = []\nreview_len_test = []\nfor i,j in zip(X_train,X_test):\n    review_len_train.append(len(i))\n    review_len_test.append(len(j))","16d1cd25":"print(\"min: \", min(review_len_train), \"max: \", max(review_len_train))","f487e560":"print(\"min: \", min(review_len_test), \"max: \", max(review_len_test))","86592e62":"sns.distplot(review_len_train,hist_kws={\"alpha\":0.3});\nsns.distplot(review_len_test,hist_kws={\"alpha\":0.3});","bcd6210c":"print(\"Train mean: \",np.mean(review_len_train))\nprint(\"Train median: \",np.median(review_len_train))\nprint(\"Train mode: \",stats.mode(review_len_train))","38d1e14a":"# number or words\nword_index = imdb.get_word_index()\nprint(type(word_index))","d332be8c":"print(\"length of word_index: \",len(word_index))","639524da":"for keys,values in word_index.items():\n    if values == 1:\n        print(keys)","3933b446":"def whatItSay(index=24):\n    reverse_index = dict([(value,key) for (key,value) in word_index.items()])\n    decode_review = \" \".join([reverse_index.get(i-3, \"!\") for i in X_train[index]])\n    print(decode_review)\n    print(Y_train[index])\n    return decode_review\n\ndecoded_review = whatItSay()","18e1fdaf":"decoded_review = whatItSay(5)","e3b3a665":"num_words = 15000\n(X_train,Y_train),(X_test,Y_test) = imdb.load_data(num_words=num_words)","9812454c":"maxlen=130\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)","f66ec727":"print(\"X train shape: \",X_train.shape)","d24cc14a":"print(X_train[5])","097a0466":"for i in X_train[0:10]:\n    print(len(i))","795aac46":"decoded_review = whatItSay(5)","bfca5c89":"rnn = Sequential()\n\nrnn.add(Embedding(num_words,32,input_length =len(X_train[0]))) # num_words=15000\nrnn.add(SimpleRNN(16,input_shape = (num_words,maxlen), return_sequences=False,activation=\"relu\"))\nrnn.add(Dense(1)) #flatten\nrnn.add(Activation(\"sigmoid\")) #using sigmoid for binary classification\n\nprint(rnn.summary())\nrnn.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n","ae2896f3":"history = rnn.fit(X_train,Y_train,validation_data = (X_test,Y_test),epochs = 5,batch_size=128,verbose = 1)","628c64be":"score = rnn.evaluate(X_test,Y_test)","fcee71e6":"print(\"accuracy:\", score[1]*100)","5513621e":"plt.figure()\nplt.plot(history.history[\"accuracy\"],label=\"Train\");\nplt.plot(history.history[\"val_accuracy\"],label=\"Test\");\nplt.title(\"Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show();","70f17c22":"plt.figure()\nplt.plot(history.history[\"loss\"],label=\"Train\");\nplt.plot(history.history[\"val_loss\"],label=\"Test\");\nplt.title(\"Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show();","340ce198":"## Construct RNN Model","f0d7c939":"## Exploratory Data Analysis(EDA)","b1438e3d":"## Preprocess"}}