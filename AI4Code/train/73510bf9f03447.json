{"cell_type":{"ef56f299":"code","02c74650":"code","8a749a45":"code","a9f67e1f":"code","8822e2ee":"code","ac73530b":"code","092d0eb4":"code","bd4563da":"code","e9590fe1":"code","809f09f8":"code","f485c0f1":"code","64f7d720":"code","6fd28a6c":"code","42f4cd31":"code","a4a95390":"code","1339d969":"code","90db0866":"code","4eb8f033":"code","b7a8edb2":"code","a4a892b7":"code","ecb586b3":"code","2785a880":"code","26236214":"code","f3414d1f":"code","f0c23105":"code","68e75175":"code","41dcd8fc":"code","d5d26e82":"code","6046d108":"code","2b3ed867":"code","567bb6ba":"code","80713c32":"code","611a9e92":"code","fed5dc84":"markdown","c8a421ef":"markdown","7d249ab1":"markdown","0a02c762":"markdown","09ac15ef":"markdown","01dc04f2":"markdown","4201621e":"markdown","490141ca":"markdown","e1ceaf8a":"markdown","91998cc0":"markdown","90261eee":"markdown","f25d9887":"markdown","d51780c4":"markdown","dc0a026c":"markdown","f2732cca":"markdown","87b07cf3":"markdown","3fd4ccd4":"markdown","98649f81":"markdown","74f75aed":"markdown","55ae3e6f":"markdown","02865bec":"markdown","f6619912":"markdown","d5566f31":"markdown","0c96aaf9":"markdown","796fe4cf":"markdown","09883329":"markdown","68791701":"markdown","6a6a39bc":"markdown","efbe432e":"markdown","9b19a74e":"markdown","c1239b37":"markdown","fd942a5c":"markdown","e5bd970a":"markdown","e621386a":"markdown","4b0a3e52":"markdown","b28c441d":"markdown","08e9a5e6":"markdown","5b26e65d":"markdown"},"source":{"ef56f299":"import numpy as np \nimport pandas as pd \nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\n\n## Models :\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier","02c74650":"pd.set_option('display.max_columns', None) # allows to display all columns of the df","8a749a45":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a9f67e1f":"import warnings # ignore warnings\nwarnings.filterwarnings('ignore')","8822e2ee":"df = pd.read_csv(\"\/kaggle\/input\/17k-apple-app-store-strategy-games\/appstore_games.csv\")","ac73530b":"df[:3]","092d0eb4":"df = df.filter([\"Age Rating\",\"Description\"])\ndf[:3]","bd4563da":"df.info()","e9590fe1":"print(\"Number of Age Rating classes : \" +str(df[\"Age Rating\"].nunique()))\nprint(\"Labels :\" + str(np.unique(list(df[\"Age Rating\"].values))))","809f09f8":"labels_freqs = df.groupby(\"Age Rating\")[\"Description\"]\\\n            .count()\\\n            .to_frame()\\\n            .reset_index()\\\n            .sort_values(\"Description\", ascending = False)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(11.7, 8.27)\n\nsns.set_theme(style = \"whitegrid\")\nax = sns.barplot(x = labels_freqs[\"Age Rating\"], y = labels_freqs[\"Description\"], color = \"orange\")\nax.set(xlabel = \"Class\", ylabel = \"Occurences\")\nplt.xticks(rotation = 0, size = 12, weight = 'normal')\nplt.title(\"Occurences for each class of Age Rating\\n\", fontsize=18, color= \"orange\")\nplt.show()","f485c0f1":"df[\"words_count\"] = df.Description.apply(lambda x : len(x.split()))\ndf[:3]","64f7d720":"average_lenght = df.groupby(\"Age Rating\")[\"words_count\"]\\\n            .mean()\\\n            .to_frame()\\\n            .reset_index()\\\n            .sort_values(\"words_count\", ascending = False)\n\n\nfig, ax = plt.subplots()\nfig.set_size_inches(11.7, 8.27)\n\nsns.set_theme(style = \"whitegrid\")\nax = sns.barplot(x = average_lenght[\"Age Rating\"], y = average_lenght[\"words_count\"], color = \"orange\")\nax.set(xlabel = \"Class\", ylabel = \"Average number of words\")\nplt.xticks(rotation = 0, size = 12, weight = 'normal')\nplt.title(\"Average number of words in a description for each class\\n\", fontsize=18, color= \"orange\")\nplt.show()","6fd28a6c":"df.Description[0]","42f4cd31":"# Remove some recurrent patterns with regular expressions :\n\nlist_expr_to_remove = [r\"\\\\n\",r\"OPTIONS\",r\"FEATURES\",\n                       r\"\\\\u\",r\"[2][0-9]{3}\",r\"\\\\\",\n                       r\"www.[a-z]*.[a-z]{2}\"]\n\nfor expr in list_expr_to_remove :\n    df.Description = df.Description.apply(lambda x : re.sub(expr,\"\",x))\n \n\n# Remove all symbols and ponctuation :\n\nlist_ponctuation_symbols = [\".\", \",\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"*\", \":\", '\"',\"-\"]\n\nfor elem in list_ponctuation_symbols :\n    df.Description = df.Description.apply(lambda x : x.replace(elem,\"\"))\n    \n# For each description, change all letters from upper case to lower case :\ndf.Description = df.Description.apply(lambda x : x.lower())\n    ","a4a95390":"df.Description[0]","1339d969":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df.Description)","90db0866":"X[0]","4eb8f033":"X.shape","b7a8edb2":"y = df[\"Age Rating\"].to_numpy()\ny = y.reshape(y.shape[0],1)\ny.shape","a4a892b7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)","ecb586b3":"naive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train, y_train)","2785a880":"y_pred = naive_bayes.predict(X_test)\ntarget_names = np.unique(list(df[\"Age Rating\"].values))\n\nprint(\"Classification Report : \")\nprint(classification_report(y_test, y_pred, target_names = target_names))\nprint(\"------------------------------------------\")\nprint(\"Confusion Matrix : \")\nprint(confusion_matrix(y_test, y_pred))","26236214":"parameters = {'alpha' : np.arange(0,5,0.1),\n              'fit_prior' : (True, False)}\n\nnaive_bayes = MultinomialNB()\nclf_naive_bayes = GridSearchCV(naive_bayes, parameters)\nclf_naive_bayes.fit(X_train, y_train)","f3414d1f":"clf_naive_bayes.best_score_","f0c23105":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)","68e75175":"decision_tree.score(X_test, y_test)","41dcd8fc":"y_pred = decision_tree.predict(X_test)\ntarget_names = np.unique(list(df[\"Age Rating\"].values))\n\nprint(\"Classification Report : \")\nprint(classification_report(y_test, y_pred, target_names = target_names))\n\nprint(\"------------------------------------------\")\nprint(\"Confusion Matrix : \")\nprint(confusion_matrix(y_test, y_pred))","d5d26e82":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","6046d108":"knn.score(X_test, y_test)","2b3ed867":"y_pred = knn.predict(X_test)\ntarget_names = np.unique(list(df[\"Age Rating\"].values))\n\nprint(\"Classification Report : \")\nprint(classification_report(y_test, y_pred, target_names = target_names))\n\nprint(\"------------------------------------------\")\nprint(\"Confusion Matrix : \")\nprint(confusion_matrix(y_test, y_pred))","567bb6ba":"scores_list = []\nn_neighbors_list = [2,5,10,20,30,40,50,60,70,80,90,100,120,140,180,200]\nfor n_neighbors_value in n_neighbors_list:\n    knn = KNeighborsClassifier(n_neighbors = n_neighbors_value)\n    knn.fit(X_train, y_train)\n    scores_list.append(knn.score(X_test, y_test))","80713c32":"fig, ax = plt.subplots()\nfig.set_size_inches(11.7, 8.27)\n\nsns.set_theme(style = \"whitegrid\")\nax = sns.lineplot(x = n_neighbors_list, y = scores_list, color = \"green\")\nax.set(xlabel = \"n_neighbors\", ylabel = \"KNN Model Score\")\nplt.xticks(rotation = 0, size = 12, weight = 'normal')\nplt.title(\"Evolution of the KNN Model Score depending of the number of neighbors hyperparameter\\n\", fontsize=18, color='#009432')\nplt.show()","611a9e92":"knn = KNeighborsClassifier(n_neighbors = 80)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)","fed5dc84":"We clearly see that there is a significant difference of representation between the different classes of the **Age Rating** : the class \"*4+*\" is highly represented, unlike the class \"*17+*\". More generally, there is a negative correlation between the number of observations and the age associated to the class. This phenomenon is quite intuitive as we can imagine that a higher age limit is less interesting for app creators by the fact that it is more restrictive. ","c8a421ef":"# I. Import phase : modules and data","7d249ab1":"The **TF-IDF** method is a really simple alternative in a context of classification when using texts as features. Formally, we want to turn text data into numerical data so Machine Learning models can clearly interpret the feature in input. To proceed, we need to identify in each text what are the most relevant words, i.e that bring us the most information to put a text in a certain category.\nTo do this, we need to identify for each category what are the most \"typical\" words, and so TF-IDF will help us for this. For a given category we will compute for each word :\n- his **TF** (*Term Frequency*) : \n\n    <center> tf(t, d) = $count(t, c)$ i.e., count of term t in category c.<center>\n\n    \n- his **IDF** (*Inverse Document Frequency*) :\n    \n    <center> idf (t, d) = $log ( D \/ { d \u2208 D : t \u2208 d })$ <center>\n        \nThen we will be able to compute the TF-IDF of each word in each category :\n       <center> tf_idf = tf x idf <center>","0a02c762":"## Thanks for the reading  :)","09ac15ef":"The main purpose of this notebook is to implement some Machine Learning models that will be able to predict the age rating for a random app by using its description in the App Store. So we need only two variables from the initial dataset to proceed :\n- the ***feature*** variable : **Description** of the game\n- the ***target*** variable : **Age Rating** of the game","01dc04f2":"A first good point is that there are no missing values, both for the age rating and for the descriptions. The dataset is composed of 17.007 observations. We can have a look at the possible values for the target variable **Age Rating** :","4201621e":"The result is quite satisfying. Now we have to turn text data into numerical data, that will be the aim of the next section.","490141ca":"# III. Pre-Processing Phase","e1ceaf8a":"As we only need our two variables of interest, we can drop the others :","91998cc0":"This model is perhaps the most adapted for our situation : indeed, the aim of this process is to determine the K-closest observations of each input (given K the number of observations to determine, and some metrics that define the distance evaluation to find the \"neighbors\"). Then the model will simply decide to classify the input in the class that most appears amongs its \"neighbors\" class.","90261eee":"### 3 - Decision Tree Classifier :","f25d9887":"# Age rating classifier for App Store Games Apps","d51780c4":"### 2 - TF-IDF Method : turn descriptions into vectors ","dc0a026c":"As we can see thanks to the classification report, the **accuracy** of this model is 69%. It is quite a good score, since we notice that we didn't specify the hyperparameters for this model. \nMoreover, if we observe the confusion matrix, we see that there remain many \"4+\" observations that were predicted as \"12+\" (397), \"17+\" (121) or \"9+\" (512). Perhaps it is possible to improve the quality of this model, by tuning the values of the hyperparameters. To proceed, we will use **GridSearchCV**, that allows us to select a list of hyperparameters, and then will test and select the best combinaison of parameters (in terms of accuracy) :","f2732cca":"### 2 - Naive Bayes Classifier :","87b07cf3":"For this task, we can use the function **TfidfVectorizer** from the sklearn library :  ","3fd4ccd4":"Results are pretty satisfying. We've tested three different machine learning models who were able to predict the age rating of a game app by simply interpreting its description on the app store. The accuracies obtained are quite encouraging :\n- 74.8 % for the Multinomial Naive Bayes Classifier\n- 74.5 % for the Decision Tree Classifier\n- 75.7 % for the K-Nearest Neighbors Classifier\n\nOf course these scores can be improved for two reasons :\n- there are others models to test in this context of classification\n- the hyperparameters of the models that has been used can be better tuned","98649f81":"The score here is nearly the same that for the previous model Naive Bayes (without tuning). However, the confusion matrix is very different : precision is better for the less represented classes than the Naive Bayes model. ","74f75aed":"# Conclusion","55ae3e6f":"Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation. Since we need to do Multi-label Classification, we will use the DecisionTreeClassifier model of sklearn. As for the Naive Bayes model, we will first proceed without specifying values for hyperparameters :","02865bec":"### 1. Cleaning of the **Description** feature","f6619912":"### 1 - Train & Test Split :","d5566f31":"Thanks to this method, we improved the accuracy of the model by 5%, which is quite great. Of course we could test more values to get a better score, but we will instead have a look to other models options.","0c96aaf9":"Indeed, the model score has increased a little. ","796fe4cf":"As we will be able to see by printing some descriptions, there is some cleaning to do : we want to remove the reccurent patterns (for example \"\\\\\\\\n\\\\\\\\n\") that bring us absolutely no more informations to predict the Age Rating. We can clean this up thanks to the use of **Regular Expressions** (RE package for python). After some exploring, here is a list of the elements to remove :\n- reccurent patterns probably due to scrapping : **\\\\\\\\n\\\\\\\\n** , **\\\\\\\\u**\n- categories of the description in the App Store : **OPTIONS**, **FEATURES**\n- symbols and punctuation : *, ., ; , ], ) , etc \n- web site links : **\"www.[a-z]*.[a-z]{2}\"**\n- years : \"[2][0-9]{3}\"\n\nIndeed, perhaps there remains other annoying patterns, but we can assume that it is negligeable due to the high amount of words and observations in the dataset. Let's have a look to one description before the cleaning process :","09883329":"We can now compute the average number of words for the descriptions of each class : ","68791701":"By a simple analysis of the graphic, we can se that the optimal number of neighbors is close to 80 (without touching the others parameters).","6a6a39bc":"### 4 - K-Nearest Neighbors Classifier","efbe432e":"As expected, the score of this kind of model is better than the two others (without tuning the parameters). Instead of tuning with **SearchGridCV**, we can just have a graphic representation of the impact of the number of neighbors on the score of the model :","9b19a74e":"Naive Bayes Classifier is a supervised learning algorithm based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable. So we here assume that all descriptions of the dataset are independent (it is quite credible for our situation). We will no go into details but you just have to know that this classifier will be able to choose a category for a description by maximizing the conditionnal probability of being in a class, given the informations of the description (the words observed in the description).","c1239b37":"Then let's see the previous description after the cleaning :","fd942a5c":"The average number of words for descriptions of each class is quite high. We can also notice that it is quite biased to compare the means of each class since we saw previously that the number of observations for each class is highly different.","e5bd970a":"# II. Basics analysis","e621386a":"Before beginning the pre-processing phase, we can just quickly have a look at the observations, with some descriptive statistics :","4b0a3e52":"We are now ready for the modelisation phase. First we need to split our data into two subsets : \n- the **train set** to fit the models \n- the **test set** to evaluate the models","b28c441d":"Then let's clean this up : ","08e9a5e6":"The **Age Rating** target is composed of 4 classes, so we are in a context of Multi-Label Classification. We can also analyse the representation of each label in this dataset :","5b26e65d":"# III. Modelisation Phase : Models"}}