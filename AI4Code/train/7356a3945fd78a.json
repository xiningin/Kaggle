{"cell_type":{"063405a2":"code","606b7cc6":"code","2d11dc27":"code","e9b10929":"code","4b24b86f":"code","984198e7":"code","9a6efc21":"code","812f7f14":"code","02fe5870":"code","d50104f7":"code","f34985d7":"code","d0acc784":"code","0d6672b7":"code","ce539a27":"code","c6ea181d":"code","a698b748":"code","2444c38a":"code","ea87c118":"code","bcf48924":"code","b1033bd4":"code","534ec43e":"code","67df614e":"code","ef6f0e5a":"code","ad1dd881":"code","dc469ad2":"code","cbe227a9":"code","6086d23d":"code","b51e267c":"code","1a2dee91":"code","869f5633":"code","a0031968":"code","c80c3aae":"code","6ca91425":"code","a181ab93":"code","9fdcf51b":"code","ccd75c3e":"code","2dbff5e6":"code","a92bb30a":"code","785f8b8a":"code","bc364bef":"code","ab43875e":"code","12a285f9":"code","c48fdf06":"code","bab79a6e":"code","04e71e48":"code","d1ca253f":"code","b8e91f52":"code","7ee0e5be":"code","645a92dc":"code","6813d413":"code","00a1b5fe":"code","d4d2f205":"code","cbe822cf":"code","6f9df3b6":"code","aecefdb8":"code","81947500":"code","c5459397":"code","b07e1903":"code","c7763d05":"code","a8e0183b":"code","58089870":"code","254efa53":"code","e9c44c02":"code","924e6358":"code","e365deb2":"code","f10616b9":"code","45211d15":"code","ebfa0cd5":"code","79ae2476":"code","ccc105e4":"code","cd9954b4":"code","5cb235ec":"code","6605de1f":"code","a2c3ea9a":"code","366b7f40":"code","02b251da":"code","27f48780":"markdown","b4dee928":"markdown","4ac499a2":"markdown","352e250d":"markdown","457b6879":"markdown","88db4fe1":"markdown","c1792626":"markdown","5f14d7bf":"markdown","dd939b98":"markdown","6bf17eba":"markdown","7257eb7d":"markdown","f0c19eaa":"markdown","e4f01c4d":"markdown","6f31b14e":"markdown","2c6aaf7f":"markdown","36e303f8":"markdown","55b7c78a":"markdown","d781b8de":"markdown","5bfe9023":"markdown","870072e4":"markdown","813443db":"markdown","c3ba2c00":"markdown","ef3b4522":"markdown","63c840e9":"markdown","3c2c8803":"markdown","d24c90bc":"markdown","cdec0860":"markdown"},"source":{"063405a2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os,gc,re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,roc_auc_score,precision_score,recall_score,f1_score\n\n\nimport lightgbm as lgb\nfrom numpy import mean\nfrom collections import Counter\nfrom imblearn.combine import SMOTETomek \nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import LabelEncoder\n\n#### The warnings from Sklearn shut off #######\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\ndef warn(*args, **kwargs):\n    pass\nwarnings.warn = warn","606b7cc6":"def load_data():\n    train = pd.read_csv(\"..\/input\/jobathon-data\/train.csv\")\n    test  = pd.read_csv(\"..\/input\/jobathon-data\/test.csv\")\n    submission = pd.read_csv(\"..\/input\/jobathon-data\/sample_submission.csv\")\n    return train,test,submission","2d11dc27":"train,_,_ = load_data()\ntrain.head(1)","e9b10929":"train.info()","4b24b86f":"train['City_Code'] = train['City_Code'].astype('category')\ntrain['Region_Code'] = train['Region_Code'].astype('category')\ntrain['Accomodation_Type']=train['Accomodation_Type'].astype('category') \ntrain['Reco_Insurance_Type'] = train['Reco_Insurance_Type'].astype('category')\ntrain['Reco_Policy_Cat'] = train['Reco_Policy_Cat'].astype('category')\ntrain['Is_Spouse']=train['Is_Spouse'].astype('category')","984198e7":"train['Holding_Policy_Duration'].value_counts()","9a6efc21":"train['Holding_Policy_Duration']=train['Holding_Policy_Duration'].replace('14+','15') # replace 14+ with 15\ntrain['Holding_Policy_Duration']=train['Holding_Policy_Duration'].fillna(train['Holding_Policy_Duration'].mode()[0]) # most frequent value\ntrain['Holding_Policy_Duration']=train['Holding_Policy_Duration'].astype('float64')\ntrain['Holding_Policy_Duration']=train['Holding_Policy_Duration'].astype('int64')\ntrain['Reco_Policy_Premium']=train['Reco_Policy_Premium'].astype('int64')\ntrain.info()","812f7f14":"#  NA counts\nprint('Health Indicator',train['Health Indicator'].isna().sum()\/train.shape[0])\nprint('Holding_Policy_Type',train['Holding_Policy_Type'].isna().sum()\/train.shape[0])","02fe5870":"## Replace NAN values with Unknown\n\ntrain['Health Indicator'] = np.where(train['Health Indicator'].isnull(),\"Unknown\",train['Health Indicator'])\ntrain['Health Indicator'].value_counts()\ntrain['Health Indicator'] = train['Health Indicator'].astype('category')","d50104f7":"train['Holding_Policy_Type'].value_counts()","f34985d7":"#fill NA values with most frequent\n\ntrain['Holding_Policy_Type']=train['Holding_Policy_Type'].fillna(train['Holding_Policy_Type'].mode()[0]) # most frequent value\ntrain['Holding_Policy_Type']=train['Holding_Policy_Type'].astype('int64')\ntrain['Holding_Policy_Type']=train['Holding_Policy_Type'].astype('category')\ntrain['Response']=train['Response'].astype('category')","d0acc784":"train[\"Average_age\"]=train[['Upper_Age','Lower_Age']].mean(axis=1)","0d6672b7":"train.head(1)","ce539a27":"from sklearn.feature_selection import mutual_info_classif","c6ea181d":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","a698b748":"train_data = train.copy()\ntrain_data.drop([\"ID\"],axis=1,inplace=True)\ny=train_data.pop('Response')\nX=train_data\nscores = make_mi_scores(X,y)\nscores","2444c38a":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    color = np.array([\"C0\"] * scores.shape[0])\n    # Color red for probes\n    idx = [i for i, col in enumerate(scores.index)\n           if col.startswith(\"PROBE\")]\n    color[idx] = \"C3\"\n    # Create plot\n    plt.figure(figsize=(30,10))\n    plt.barh(width, scores, color=color)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\\n\")\n    \nplot_mi_scores(scores)","ea87c118":"# removing least important features\n\ntrain.drop(['Is_Spouse','Accomodation_Type','Holding_Policy_Type','Reco_Insurance_Type'],axis=1,inplace=True)\ntrain.info()","bcf48924":"# processed data set\ntrain.to_csv(\"train.csv\",index=False)","b1033bd4":"del train\ngc.collect()","534ec43e":"train = pd.read_csv(\"train.csv\")\ntrain.drop([\"ID\"],axis=1,inplace=True)\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(train[c].values))\n        train[c]= le.transform(list(train[c].values))\n        \ntrain.head(1)","67df614e":"scores[scores>0.00005]","ef6f0e5a":"RANDOM_SEED = 99\ntarget = 'Response'\npreds = [x for x in list(train) if x != target]\nlen(preds)","ad1dd881":"from collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\n\n# define dataset\nX, y = train[preds],train[target]\nprint('Original dataset shape {}'.format(Counter(y)))\n\n\nsmt = SMOTETomek(random_state=42)\nX_res, y_res = smt.fit_resample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X_res, y_res,random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)","dc469ad2":"import xgboost as xgb\nxgmodel = xgb.XGBClassifier(learning_rate=0.01,eval_metric='auc')\n\nxgmodel.fit(X_train,y_train)\nfrom sklearn.metrics import balanced_accuracy_score, accuracy_score\nprint('ROC_AUC:',roc_auc_score(y_test,xgmodel.predict(X_test)))","cbe227a9":"oversample = RandomOverSampler(sampling_strategy='minority')\n# fit and apply the transform\ntraindf,target = oversample.fit_resample(train[preds],train[target])\n# summarize class distribution\ncounter = Counter(target)\nprint(counter)","6086d23d":"X_train,X_test,y_train,y_test= train_test_split(traindf, target,random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)","b51e267c":"xgmodel.fit(X_train,y_train)\nprint('ROC_AUC:',roc_auc_score(y_test,xgmodel.predict(X_test)))","1a2dee91":"del train,preds,xgmodel,X,y,X_train,X_test,y_train,y_test\ngc.collect()","869f5633":"train = pd.read_csv(\"train.csv\")\ntrain.drop([\"ID\",'Holding_Policy_Duration'],axis=1,inplace=True)\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(train[c].values))\n        train[c]= le.transform(list(train[c].values))\n        \ntrain.head(1)","a0031968":"from collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nRANDOM_SEED = 99\ntarget = 'Response'\npreds = [x for x in list(train) if x != target]        \n\n# define dataset\nX, y = train[preds],train[target]\nprint('Original dataset shape {}'.format(Counter(y)))\n\n\nsmt = SMOTETomek(random_state=42)\nX_res, y_res = smt.fit_resample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X_res, y_res,random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)","c80c3aae":"import xgboost as xgb\nxgmodel = xgb.XGBClassifier(learning_rate=0.01,eval_metric='auc')\nxgmodel.fit(X_train,y_train)\nfrom sklearn.metrics import balanced_accuracy_score, accuracy_score\nprint('ROC_AUC:',roc_auc_score(y_test,xgmodel.predict(X_test)))","6ca91425":"del train,preds,xgmodel,X,y,X_train,X_test,y_train,y_test\ngc.collect()","a181ab93":"train = pd.read_csv(\"train.csv\")\ntrain.drop([\"ID\",'Holding_Policy_Duration','Upper_Age','Lower_Age'],axis=1,inplace=True)\nfor c in train.columns:\n    if train[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(train[c].values))\n        train[c]= le.transform(list(train[c].values))\n        \n#train.to_csv('final_train.csv',index=False)\ntrain.head(1)","9fdcf51b":"RANDOM_SEED = 99\ntarget = 'Response'\npreds = [x for x in list(train) if x != target]\n\n# define dataset\nX, y = train[preds],train[target]\nprint('Original dataset shape {}'.format(Counter(y)))\n\n\nsmt = SMOTETomek(random_state=42)\nX_res, y_res = smt.fit_resample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X_res, y_res,random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)","ccd75c3e":"import xgboost as xgb\nxgmodel = xgb.XGBClassifier(learning_rate=0.01,eval_metric='auc')\nxgmodel.fit(X_train, y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,xgmodel.predict(X_test)),6))\nprint(\"Accuracy:\",round(accuracy_score(y_test,xgmodel.predict(X_test)),6))","2dbff5e6":"from catboost import CatBoostClassifier\nRFC = CatBoostClassifier()\nRFC.fit(X_train, y_train,metric_period=100)\nprint('ROC_AUC:',round(roc_auc_score(y_test,RFC.predict(X_test)),6))","a92bb30a":"from sklearn.ensemble import GradientBoostingClassifier\n#Gradient Boosting Classifier\n\ngb_model1 = GradientBoostingClassifier(n_estimators=500,max_depth=11)\ngb_model1.fit(X_train,y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,gb_model1.predict(X_test)),6))","785f8b8a":"# selected for the final model\n# train.drop([\"ID\",'Holding_Policy_Duration','Upper_Age','Lower_Age'],axis=1,inplace=True)","bc364bef":"import lightgbm as lgb\nmodel_clf = lgb.LGBMClassifier()\nmodel_clf.fit(X_train, y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,model_clf.predict(X_test)),6))","ab43875e":"del train,preds,xgmodel,X,y,X_train,X_test,y_train,y_test\ngc.collect()","12a285f9":"train = pd.read_csv(\"train.csv\")\ntrain.drop([\"ID\",'Holding_Policy_Duration','Health Indicator'],axis=1,inplace=True) # selected this data set\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(train[c].values))\n        train[c]= le.transform(list(train[c].values))\n        \n        \nRANDOM_SEED = 99\ntarget = 'Response'\npreds = [x for x in list(train) if x != target]\n\n# define dataset\nX, y = train[preds],train[target]\nprint('Original dataset shape {}'.format(Counter(y)))\n\n\nsmt = SMOTETomek(random_state=42)\nX_res, y_res = smt.fit_resample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X_res, y_res,random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)","c48fdf06":"train.head(1)","bab79a6e":"xgmodel = xgb.XGBClassifier(learning_rate=0.01,eval_metric='auc')\nxgmodel.fit(X_train, y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,xgmodel.predict(X_test)),6))\nprint(\"Accuracy:\",round(accuracy_score(y_test,xgmodel.predict(X_test)),6))","04e71e48":"from catboost import CatBoostClassifier\ncatboost = CatBoostClassifier()\ncatboost.fit(X_train, y_train,metric_period=100)\nprint('ROC_AUC:',round(roc_auc_score(y_test,catboost.predict(X_test)),6))","d1ca253f":"# train.drop([\"ID\",'Holding_Policy_Duration','Health Indicator'],axis=1,inplace=True) \n# Catboost model with this data set is selected for hyperparameter tuning with optuna","b8e91f52":"from sklearn.ensemble import GradientBoostingClassifier\n#Gradient Boosting Classifier\n\ngb_model2 = GradientBoostingClassifier(n_estimators=250,max_depth=10)\ngb_model2.fit(X_train,y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,gb_model2.predict(X_test)),6))","7ee0e5be":"import lightgbm as lgb\n\nmodel_clf = lgb.LGBMClassifier()\nmodel_clf.fit(X_train, y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,model_clf.predict(X_test)),6))","645a92dc":"# selected for the final model\n# train.drop([\"ID\",'Holding_Policy_Duration','Health Indicator'],axis=1,inplace=True) ","6813d413":"del train,preds,xgmodel,X,y,X_train,X_test,y_train,y_test\ngc.collect()","00a1b5fe":"!pip install optuna -q","d4d2f205":"RANDOM_SEED = 99\n\ntrain = pd.read_csv(\"train.csv\")\ntrain.drop([\"ID\",'Holding_Policy_Duration','Health Indicator'],axis=1,inplace=True)\n\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(train[c].values))\n        train[c]= le.transform(list(train[c].values))\n        \ntarget = 'Response'\npreds = [x for x in list(train) if x != target]\n\n# define dataset\nX, y = train[preds],train[target]\nprint('Original dataset shape {}'.format(Counter(y)))\n\n\nsmt = SMOTETomek(random_state=42)\nX_res, y_res = smt.fit_resample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\nX_train,X_test,y_train,y_test= train_test_split(X_res, y_res,random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)\nfrom catboost import CatBoostClassifier\ncatboost = CatBoostClassifier()\ncatboost.fit(X_train, y_train,metric_period=100)\nprint('ROC_AUC:',round(roc_auc_score(y_test,catboost.predict(X_test)),6))","cbe822cf":"import catboost as cb\nimport optuna\n\n\ndef objective(trial):\n    data, target = X_res, y_res \n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.3)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    gbm = cb.CatBoostClassifier(**param)\n\n    gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","6f9df3b6":"import catboost as cb\nParams = study.best_trial.params\ncbopt = cb.CatBoostClassifier(**Params)\ncbopt.fit(X_train, y_train,metric_period=100)\nprint('ROC_AUC:',round(roc_auc_score(y_test,cbopt.predict(X_test)),6))","aecefdb8":"import lightgbm as lgb\n\nmodel_clf = lgb.LGBMClassifier()\nmodel_clf.fit(X_train, y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,model_clf.predict(X_test)),6))","81947500":"RANDOM_SEED = 99\ntrain = pd.read_csv(\"train.csv\")\ntrain.drop([\"ID\",'Holding_Policy_Duration','Upper_Age','Lower_Age'],axis=1,inplace=True)\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(train[c].values))\n        train[c]= le.transform(list(train[c].values))\n        \ntarget = 'Response'\npreds = [x for x in list(train) if x != target]\n\n# define dataset\nX, y = train[preds],train[target]\nprint('Original dataset shape {}'.format(Counter(y)))\nsmt = SMOTETomek(random_state=42)\nX_res, y_res = smt.fit_resample(X, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\nX_train,X_test,y_train,y_test= train_test_split(X_res, y_res,random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)","c5459397":"preds","b07e1903":"gb_model1 = GradientBoostingClassifier(n_estimators=500,max_depth=11)\ngb_model1.fit(X_train,y_train)\nprint('ROC_AUC:',round(roc_auc_score(y_test,gb_model1.predict(X_test)),6))","c7763d05":"gc.collect()","a8e0183b":"from sklearn.ensemble import StackingClassifier\n\nestimators = [('cb',catboost),('lgb',model_clf)]\n\nclf = StackingClassifier(estimators=estimators, final_estimator= gb_model1)\nclf.fit(X_train, y_train).score(X_test, y_test)","58089870":"params = clf.get_params(deep=True)\nprint('ROC_AUC:',round(roc_auc_score(y_test,clf.predict(X_test)),6))","254efa53":"_,testf,submission = load_data()\ntestf[\"Average_age\"]=testf[['Upper_Age','Lower_Age']].mean(axis=1)\ntestf.drop([\"ID\",'Holding_Policy_Duration','Upper_Age','Lower_Age',\n            'Accomodation_Type','Reco_Insurance_Type','Holding_Policy_Type','Is_Spouse'],axis=1,inplace=True)\n\n\nfor c in testf.columns:\n    if testf[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(testf[c].values))\n        testf[c]= le.transform(list(testf[c].values))\n        \n\ntestf.to_csv('final_test.csv',index=False)\ntestf.head(1)","e9c44c02":"submission.head(2)","924e6358":"predclf = clf.predict(testf)\nsubmission['Response'] = predclf\nsubmission.to_csv(\"submission_stacked.csv\",index=False) ","e365deb2":"# catboost\npredictions = RFC.predict(testf)\nsubmission['Response'] = predictions\nsubmission.to_csv(\"submission_cat4.csv\",index=False)","f10616b9":"# Gradient Boost\npredictions2 = gb_model1.predict(testf)\nsubmission['Response'] = predictions2\nsubmission.to_csv(\"submission_gb4.csv\",index=False) ","45211d15":"#### requirements\n#ipython\n#jupyter\n#xgboost>=1.1.1\n#pandas\n#matplotlib\n#seaborn\n#scikit-learn>=0.23.1\n#networkx\n#category_encoders\n\n!pip install xlrd -q\n!pip install git+https:\/\/github.com\/AutoViML\/featurewiz.git -q\n!pip install category_encoders -q","ebfa0cd5":"import category_encoders\nimport xgboost \nimport networkx as nx\nfrom xgboost.sklearn import XGBClassifier\nfrom category_encoders import OneHotEncoder","79ae2476":"# selected data set\n\ntrain = pd.read_csv(\"train.csv\")\ntrain.drop([\"ID\",'Holding_Policy_Duration','Upper_Age','Lower_Age'],axis=1,inplace=True)\n\nfor c in train.columns:\n    if train[c].dtype=='object':\n        le=LabelEncoder()\n        le.fit(list(train[c].values))\n        train[c]= le.transform(list(train[c].values))\n        \n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(train, test_size=0.2,random_state=RANDOM_SEED)\nprint(train.shape, test.shape)","ccc105e4":"from featurewiz import featurewiz\n\nRANDOM_SEED = 99\ntarget = 'Response'\npreds = [x for x in list(train) if x != target]\n\noutput = featurewiz(train, target, corr_limit=0.70,verbose=1, test_data=test,\n                      feature_engg=[\"groupby\",'target','interactions'], \n                     category_encoders='OneHotEncoder'\n                     )","cd9954b4":"trainm,testm= output","5cb235ec":"RANDOM_SEED = 101\ntarget = 'Response'\npreds = [x for x in list(trainm) if x != target]\n\ntrainm, testm = train_test_split(trainm, test_size=0.2,random_state=RANDOM_SEED)\nprint(trainm.shape, testm.shape)","6605de1f":"# number of features\n\ncat_vars = trainm.select_dtypes(include='category').columns.tolist() + trainm.select_dtypes(include='object').columns.tolist()\ntrainm[cat_vars]=trainm[cat_vars].fillna(trainm.mean().iloc[0])\nfeats = [x for x in list(trainm) if x not in [target]]\nlen(feats)","a2c3ea9a":"feats","366b7f40":"RFC = CatBoostClassifier()\nRFC.fit(trainm[feats],trainm[target], metric_period=100)","02b251da":"print('ROC_AUC :',round(roc_auc_score(testm[target].values, RFC.predict(testm[feats])),6))\nprint(\"Accuracy:\",round(accuracy_score(testm[target].values, RFC.predict(testm[feats])),6))","27f48780":"### convert data types","b4dee928":"## Gradient Boosting","4ac499a2":"# Appendix A","352e250d":"# Model Training without Feature Engineering\n\nI am removing the least important features step by step","457b6879":"## Gradient Boost with 6 features","88db4fe1":"## RandomOverSampler","c1792626":"## Removing other least important features","5f14d7bf":"## Mutual Information","dd939b98":"### XGBoost","6bf17eba":"# XGBoost Classifier","7257eb7d":"## CatBoost and Light GBM with 7 features","f0c19eaa":"## Light GBM\n","e4f01c4d":"# Health Insurance Lead Prediction\n\n__FinMan__ is a financial services company that provides various financial services like loan, investment funds, insurance etc. to its customers. FinMan wishes to cross-sell health insurance to the existing customers who may or may not hold insurance policies with the company. \n\nThe company recommend health insurance to it's customers based on their profile once these customers land on the website. Customers might browse the recommended health insurance policy and consequently fill up a form to apply. When these customers fill-up the form, their Response towards the policy is considered positive and they are classified as a lead.\n\nOnce these leads are acquired, the sales advisors approach them to convert and thus the company can sell proposed health insurance to these leads in a more efficient manner.\n\nI am building a model to predict whether the person will be interested in their proposed Health plan\/policy given the information about:\n\nDemographics (city, age, region etc.)\nInformation regarding holding policies of the customer\nRecommended Policy Information\n\nThe evaluation metric for this data is __roc_auc_score__ across all entries in the test set.","6f31b14e":"# Final Model","2c6aaf7f":"## XGBoost","36e303f8":"### Removing Health Indicator","55b7c78a":"## LightGBM","d781b8de":"### CatBoost Classifier","5bfe9023":"### Train Data Description\n\nID - Unique Identifier for a row\n\nCity_Code - Code for the City of the customers\n\nRegion_Code - Code for the Region of the customers\n\nAccomodation_Type - Customer Owns or Rents the house\n\nReco_Insurance_Type\t- Joint or Individual type for the recommended insurance  \n\nUpper_Age - Maximum age of the customer \n\nLower _Age - Minimum age of the customer\n\nIs_Spouse - If the customers are married to each other(in case of joint insurance)\n\nHealth_Indicator - Encoded values for health of the customer\n\nHolding_Policy_Duration\tDuration - (in years) of holding policy (a policy that customer has already subscribed to with the company)\n\nHolding_Policy_Type - Type of holding policy\n\nReco_Policy_Cat\t- Encoded value for recommended health insurance \n\nReco_Policy_Premium\t- Annual Premium (INR) for the recommended health insurance\n\n>### Response (Target)\t\n\n0 : Customer did not show interest in the recommended policy\n\n1 : Customer showed interest in the recommended policy","870072e4":"# CatBoost with Feature Engineering","813443db":"### SMOTE balance","c3ba2c00":"## Gradient Boost Classifier","ef3b4522":"# Submissions","63c840e9":"### NA values","3c2c8803":"# Selected data set","d24c90bc":"### SMOTE produced better result than that of RandomOverSampler","cdec0860":"## CatBoost Classifier"}}