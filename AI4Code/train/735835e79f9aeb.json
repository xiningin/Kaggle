{"cell_type":{"b9ef78c4":"code","20e2c714":"code","b6b1465b":"code","4aa6d131":"code","5778505e":"code","2acda32b":"code","b0bed779":"code","2769b85c":"code","bfb67f0b":"code","a548e1f5":"code","2e22f4bd":"code","82f87037":"code","3f437b17":"code","e4959f42":"code","97dd76a7":"code","85d957c7":"code","8317da1f":"code","1d711f53":"code","390792d9":"code","e41999ef":"code","ec363eb6":"code","35372491":"code","a1b686e3":"code","b598bf8a":"code","4c229951":"code","3b41fe20":"code","31118350":"code","22838e65":"markdown","acc3ac48":"markdown","41da869e":"markdown","65e8a841":"markdown","b7e14ed4":"markdown","a48ed294":"markdown","d26ee604":"markdown","19ea413b":"markdown","e4f677bf":"markdown"},"source":{"b9ef78c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, recall_score, precision_score, accuracy_score, precision_recall_curve, roc_curve, roc_auc_score\n\nfrom collections import Counter\n\nfrom scipy.stats import norm, multivariate_normal\n\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nrandom.seed(0)","20e2c714":"#Loading Dataset\ncc_dataset = pd.read_csv(\"..\/input\/creditcard.csv\")\nprint(cc_dataset.shape)\nprint(cc_dataset.head())\nprint(cc_dataset.describe())","b6b1465b":"#Check for any feature has null values\ncc_dataset.isnull().any()","4aa6d131":"#Counts for class in the dataset\ncc_dataset['Class'].value_counts()","5778505e":"#Data Visualization for checking the distribution for Genuine cases & Fraud cases for each feature\nv_features = cc_dataset.columns\nplt.figure(figsize=(12,31*4))\ngs = gridspec.GridSpec(31,1)\n\nfor i, col in enumerate(v_features):\n    ax = plt.subplot(gs[i])\n    sns.distplot(cc_dataset[col][cc_dataset['Class']==0],color='g',label='Genuine Class')\n    sns.distplot(cc_dataset[col][cc_dataset['Class']==1],color='r',label='Fraud Class')\n    ax.legend()\nplt.show()","2acda32b":"cc_dataset.drop(labels = ['V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8','Time'], axis = 1, inplace=True)\ncc_dataset.columns","b0bed779":"cc_dataset.drop(labels = ['V1','V2','V5','V6','V7','V21','Amount'], axis = 1, inplace=True)\ncc_dataset.columns","2769b85c":"# Visualization to understand the relationship between features and also data pattern using pair plot from seaborn\ng = sns.pairplot(cc_dataset,hue=\"Class\",diag_kind='kde')","bfb67f0b":"#Method for selecting epsilon with best F1-score\ndef SelectThresholdByCV_Anomaly(probs,y):\n    best_epsilon = 0\n    best_f1 = 0\n    f = 0\n    precision =0\n    recall=0\n    best_recall = 0\n    best_precision = 0\n    \n    epsilons = sorted(np.unique(probs))\n    #print(epsilons)\n    \n    precisions=[]\n    recalls=[]\n    for epsilon in epsilons:\n        predictions = (probs < epsilon)\n        f = f1_score(y, predictions)\n        precision = precision_score(y, predictions)\n        recall = recall_score(y, predictions)\n        #print(\"Theshold {0},Precision {1},Recall {2}\".format(epsilon,precision,recall))\n          \n        if f > best_f1:\n            best_f1 = f\n            best_precision = precision\n            best_recall = recall\n            best_epsilon = epsilon\n        \n        precisions.append(precision)\n        recalls.append(recall)\n\n    #Precision-Recall Trade-off\n    plt.plot(epsilons,precisions,label='Precision')\n    plt.plot(epsilons,recalls,label='Recall')\n    plt.xlabel(\"Epsilon\")\n    plt.title('Precision Recall Trade Off')\n    plt.legend()\n    plt.show()\n\n    print ('Best F1 Score %f' %best_f1)\n    print ('Best Precision Score %f' %best_precision)\n    print ('Best Recall Score %f' %best_recall)\n    print ('Best Epsilon', best_epsilon)","a548e1f5":"#Method for calculating parameters Mu & Co-variance\ndef estimateGaussian(data):\n    mu = np.mean(data,axis=0)\n    sigma = np.cov(data.T)\n    return mu,sigma","2e22f4bd":"#Method for implementing multivariate gaussian distribution function\ndef MultivariateGaussianDistribution(data,mu,sigma):\n    p = multivariate_normal.pdf(data, mean=mu, cov=sigma)\n    p_transformed = np.power(p,1\/100) #transformed the probability scores by p^1\/100 since the values are very low (up to e-150)\n    return p_transformed","82f87037":"genuine_data = cc_dataset[cc_dataset['Class']==0]\nfraud_data = cc_dataset[cc_dataset['Class']==1]","3f437b17":"#Split Genuine records into train & test - 60:40 ratio\ngenuine_train,genuine_test = train_test_split(genuine_data,test_size=0.4,random_state=0)\nprint(genuine_train.shape)\nprint(genuine_test.shape)","e4959f42":"#Split 40% of Genuine Test records into Cross Validation & Test again (50:50 ratio)\ngenuine_cv,genuine_test = train_test_split(genuine_test,test_size=0.5,random_state=0)\nprint(genuine_cv.shape)\nprint(genuine_test.shape)","97dd76a7":"#Split Fraud records into Cross Validation & Test (50:50 ratio)\nfraud_cv,fraud_test = train_test_split(fraud_data,test_size=0.5,random_state=0)","85d957c7":"#Drop Y-label from Train data\ntrain_data = genuine_train.drop(labels='Class',axis=1)\nprint(train_data.shape)","8317da1f":"#Cross validation data\ncv_data = pd.concat([genuine_cv,fraud_cv])\ncv_data_y = cv_data['Class']\ncv_data.drop(labels='Class',axis=1,inplace=True)\nprint(cv_data.shape)","1d711f53":"#Test data\ntest_data = pd.concat([genuine_test,fraud_test])\ntest_data_y = test_data['Class']\ntest_data.drop(labels='Class',axis=1,inplace=True)\nprint(test_data.shape)","390792d9":"#Find out the parameters Mu and Covariance for passing to the probability density function\nmu,sigma = estimateGaussian(train_data)","e41999ef":"#Multivariate Gaussian distribution - This calculates the probability for each record.\np_train = MultivariateGaussianDistribution(train_data,mu,sigma)\nprint(p_train.mean())\nprint(p_train.std())\nprint(p_train.max())\nprint(p_train.min())","ec363eb6":"#Calculate the probabilities for cross validation and test records by passing the mean and co-variance matrix derived from train data\np_cv = MultivariateGaussianDistribution(cv_data,mu,sigma)\np_test = MultivariateGaussianDistribution(test_data,mu,sigma)","35372491":"#Let us use cross validation to find the best threshold where the F1 -score is high\nSelectThresholdByCV_Anomaly(p_cv,cv_data_y)","a1b686e3":"def Print_Accuracy_Scores(y,y_pred):\n    print(\"F1 Score: \", f1_score(y,y_pred))\n    print(\"Precision Score: \", precision_score(y,y_pred))\n    print(\"Recall Score: \", recall_score(y,y_pred))","b598bf8a":"#CV data - Predictions\npred_cv= (p_cv < 0.2425)\nPrint_Accuracy_Scores(cv_data_y, pred_cv)","4c229951":"#Confusion matrix on CV\ncnf_matrix = confusion_matrix(cv_data_y,pred_cv)\nrow_sum = cnf_matrix.sum(axis=1,keepdims=True)\ncnf_matrix_norm =cnf_matrix \/ row_sum \nsns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)\nplt.title(\"Normalized Confusion Matrix - Cross Validation\")","3b41fe20":"#Test data - Check the F1-score by using the best threshold from cross validation\npred_test = (p_test < 0.2425)\nPrint_Accuracy_Scores(test_data_y,pred_test)","31118350":"cnf_matrix = confusion_matrix(test_data_y, pred_test)\nrow_sum = cnf_matrix.sum(axis=1,keepdims=True)\ncnf_matrix_norm =cnf_matrix \/ row_sum \nsns.heatmap(cnf_matrix_norm,cmap='YlGnBu',annot=True)\nplt.title(\"Normalized Confusion Matrix - Test data\")","22838e65":"***Please notice that False negatives are around 24%. I tried to reduce false negatives & improve recall score by increasing the epsilon. I was successful in bringing the recall above 80%, however precsion is going down to 70% pretty quickly. Hence I decided to choose the epsilon with best f1-score, i.e: 0.2425***","acc3ac48":"### No Null Values found... Good to go with the dataset without any data edits","41da869e":"* We can see...\n    * we have only 492 (0.17%) fraud cases out of 284807 records. \n    * Remaining 284315 (99.8%) of the records belong to genuine cases.\n* So the dataset is clearly imbalanced!","65e8a841":"## **Credit Card Fraud Detection - Python code**","b7e14ed4":"#StandardScaler \u2013 Feature scaling is not required since all the features are already standardized via PCA\n#sc = StandardScaler()\n#train_data = sc.fit_transform(train_data)\n#cv_data = sc.transform(cv_data)\n#test_data = sc.transform(test_data)","a48ed294":"## Feature selection: \n* We can see Normal Distribution of anomalous transactions (class = 1) is matching with Normal Distribution of genuine transactions (class = 0) for V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8' features. It is better to delete these features as they may not be useful in finding anomalous records. \n* Time is also not useful variable since it contains the seconds elapsed between the transaction for that record and the first transaction in the dataset. So the data is in increasing order always.","d26ee604":"Below features doesn't have the same distribution for both genuine & fraud records. However distribution for fraud records are not unusual as well. So I'll delete these features, since the features with unusual behavior for Fraud records will be most useful in anomaly detection algorithm.","19ea413b":"**Conclusion:** Anomaly detection algorthm has provided decent results with **F1-score of 83**. We can improve recall & thus f1-score further by deriving new features based on the business knowledge. Since the features are transformed from PCA output, we couldn't understand their purpose and do feature engineering.","e4f677bf":"There is not much insight form the pairplot except that most of features have clear separation for fraud records versus genuine records. We can notice that distribution of fraud records is quite different compared to genuine records in the diagonal kde plots. All the features looks to be normally distributed. So we can train the Multivariate Guassian Distribution algorthm using the original features."}}