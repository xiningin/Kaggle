{"cell_type":{"acc3570e":"code","b5391b00":"code","7c5628d3":"code","353a99bd":"code","7b24c1cb":"code","24a3719e":"code","96db52a5":"code","17f9ba2c":"code","8b8d6e8b":"code","36ee5e6a":"code","2fec8102":"code","d37cc430":"code","ed542f83":"code","e945e6df":"code","9bd24bb2":"code","e845eec3":"code","97874e13":"code","d0cfd394":"code","00c23988":"code","00f7a4b8":"code","ee5cd179":"code","58d3b2da":"code","d96fd70e":"code","66976d38":"code","ef5bf758":"code","c9cee762":"code","4ee0bf4f":"code","94c8f264":"markdown","0aa077f1":"markdown","ab470718":"markdown","a92df6d0":"markdown","57d01a46":"markdown","01bb8bdb":"markdown","f6a8cb8f":"markdown","26c64869":"markdown","83a81579":"markdown","cd0881fb":"markdown","ef06e8fe":"markdown","350ebe2d":"markdown","966b6e2f":"markdown","c43e8267":"markdown","352bb550":"markdown","f1e2cec0":"markdown","4a3349b8":"markdown","a728d0ac":"markdown","2ee4ae69":"markdown","2472fd56":"markdown","065f6ec2":"markdown","57d9e0ba":"markdown","f686dc9f":"markdown","cccf414e":"markdown","67502520":"markdown","887198f6":"markdown","89b2d947":"markdown","17f4f55e":"markdown","8b1402ca":"markdown"},"source":{"acc3570e":"pip install pydotplus","b5391b00":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sb\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn.tree import export_graphviz\nfrom six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\n\nplt.style.use('seaborn-darkgrid')","7c5628d3":"%matplotlib inline","353a99bd":"wine_df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\nwine_df.head(10)","7b24c1cb":"wine_df.describe()","24a3719e":"print(wine_df.isna().sum())","96db52a5":"rcParams[\"figure.figsize\"] = [10, 8]\nplt.hist(wine_df['quality'], bins=6, edgecolor='black')\nplt.xlabel('quality', fontsize=20)\nplt.ylabel('count', fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","17f9ba2c":"X = wine_df.drop('quality', axis=1).values\nX = StandardScaler().fit_transform(X)\ny = np.ravel(wine_df[['quality']])","8b8d6e8b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=200)","36ee5e6a":"reg = DecisionTreeRegressor(random_state=200)\nreg = reg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\ny_pred = np.array([round(y) for y in y_pred])","2fec8102":"# Evaluating the Model\nprint('Accuracy:', sum(y_test == y_pred) \/ len(y_test == y_pred))","d37cc430":"dot_data = StringIO()\nexport_graphviz(reg, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,\n                feature_names = wine_df.drop('quality', axis=1).columns)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('wine_quality.png')\nImage(graph.create_png())","ed542f83":"bag_reg = BaggingRegressor(random_state=200)\nbag_reg = bag_reg.fit(X_train, y_train)\ny_pred_bag = bag_reg.predict(X_test)\ny_pred_bag = np.array([round(y) for y in y_pred_bag])","e945e6df":"# Evaluating the Model\nprint('Accuracy:', sum(y_test == y_pred_bag) \/ len(y_test == y_pred_bag))","9bd24bb2":"boost_reg = GradientBoostingRegressor(random_state=200)\nboost_reg = boost_reg.fit(X_train, y_train)\ny_pred_boost = boost_reg.predict(X_test)\ny_pred_boost = np.array([round(y) for y in y_pred_boost])","e845eec3":"# Evaluating the Model\nprint('Accuracy:', sum(y_test == y_pred_boost) \/ len(y_test == y_pred_boost))","97874e13":"rf_reg = RandomForestRegressor(random_state=200)\nrf_reg = rf_reg.fit(X_train, y_train)\ny_pred_rf = rf_reg.predict(X_test)\ny_pred_rf = np.array([round(y) for y in y_pred_rf])","d0cfd394":"# Evaluating the Model\nprint('Accuracy:', sum(y_test == y_pred_rf) \/ len(y_test == y_pred_rf))","00c23988":"keys = wine_df.columns\nvalues = rf_reg.feature_importances_\nvar_imp = dict(zip(keys, values))\nvar_imp = dict(sorted(var_imp.items(), key=lambda x: x[1]))\n\nrcParams[\"figure.figsize\"] = [10, 8]\nplt.title('Feature Importances', fontsize=20)\nplt.barh(list(var_imp.keys()), list(var_imp.values()))\nplt.xlabel('Relative Importance', fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","00f7a4b8":"oob_error_ntrees = []\n\nfor i in range(50,401):\n    rf_reg_ntrees = RandomForestRegressor(n_estimators=i, oob_score=True, random_state=200)\n    rf_reg_ntrees.fit(X_train, y_train)\n    oob_error_ntrees.append(1 - rf_reg_ntrees.oob_score_)","ee5cd179":"rcParams[\"figure.figsize\"] = [10, 8]\nplt.title('Tuning number of trees', fontsize=22)\nplt.plot([i for i in range(50,401)], oob_error_ntrees)\nplt.xlabel('No. of trees', fontsize=20)\nplt.ylabel('OOB Error', fontsize=20)\nplt.xticks([i for i in range(50,401,50)], fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","58d3b2da":"#Finding number of trees for minimum OOB Error\nntrees = oob_error_ntrees.index((min(oob_error_ntrees))) + 50\nprint(\"Number of trees for min OOB Error:\", ntrees)","d96fd70e":"oob_error_mtry = []\n\nfor j in range(1,12):\n    rf_reg_mtry = RandomForestRegressor(max_features=j, oob_score=True, random_state=200)\n    rf_reg_mtry.fit(X_train, y_train)\n    oob_error_mtry.append(1 - rf_reg_mtry.oob_score_)","66976d38":"rcParams[\"figure.figsize\"] = [10, 8]\nplt.title('Tuning number of variables', fontsize=22)\nplt.plot([j for j in range(1,12)], oob_error_mtry, marker='o')\nplt.xlabel('No. of variables', fontsize=20)\nplt.ylabel('OOB Error', fontsize=20)\nplt.xticks([j for j in range(1,12)], fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()","ef5bf758":"#Finding number of trees for minimum OOB Error\nmtry = oob_error_mtry.index((min(oob_error_mtry))) + 1\nprint(\"Number of variables for min OOB Error:\", mtry)","c9cee762":"rf_reg_final = RandomForestRegressor(n_estimators=ntrees, max_features=mtry, random_state=200)\nrf_reg_final = rf_reg_final.fit(X_train, y_train)\ny_pred_final = rf_reg_final.predict(X_test)\ny_pred_final = np.array([round(y) for y in y_pred_final])","4ee0bf4f":"# Evaluating the Model\nprint('Accuracy:', sum(y_test == y_pred_final) \/ len(y_test == y_pred_final))","94c8f264":"# Fitting the Decision Tree","0aa077f1":"# Objective\n\nThe objective of this project is to predict the quality of wine using the concepts learned in DSA5841 Learning from Data: Decision Trees. The Wine Quality dataset consists of red wine samples. The inputs include objective tests (e.g. pH values) and the output is based on sensory data (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (very excellent).\n\nThe dataset input variables (based on physicochemical tests) are:\n\n1. fixed acidity (tartaric acid - g \/ dm^3)\n2. volatile acidity (acetic acid - g \/ dm^3)\n3. citric acid (g \/ dm^3)\n4. residual sugar (g \/ dm^3)\n5. chlorides (sodium chloride - g \/ dm^3\n6. free sulfur dioxide (mg \/ dm^3)\n7. total sulfur dioxide (mg \/ dm^3)\n8. density (g \/ cm^3)\n9. pH\n10. sulphates (potassium sulphate - g \/ dm^3)\n11. alcohol (% by volume)\n\n\nThe output variable (based on sensory data) is:\n\n12. quality (score between 0 and 10)","ab470718":"# Exploratory Data Analysis","a92df6d0":"## Boosting","57d01a46":"### Check for missing values","01bb8bdb":"## Random Forest","f6a8cb8f":"From the Feature Importances plot shown above, it can be seen that no single feature has a relative importance that is too insignificant to discount. Hence, all features will be kept from this point forward.","26c64869":"We check for any missing values across all rows to see if there are any records that need to be removed or filled in. Since there are none, we proceed as usual.","83a81579":"### Dsitribution of wine quality","cd0881fb":"After tuning the parameters, we can see that there is an increase in accuracy from 71.25% to 72.50%, which can be considered quite good given the non-standard approach to this problem.","ef06e8fe":"# Final Model","350ebe2d":"|               | Accuracy          |\n|---------------|-------------------|\n| Decision Tree | 62.71%            |\n| Bagging       | 64.79%            |\n| Boosting      | 66.04%            |\n| Random Forest | 71.25%            |","966b6e2f":"# Tuning Parameters","c43e8267":"# Approach\n\nThe qualities of the wines are scored on a scale of 1 to 10, which means the data is comprised of discrete values.\n\nFraming it as a classification problem would require converting the wine quality into a binary variable. For example, wines with a quality score of 7 or more would be classified as good quality wine and wines with a quality score of less than 7 would be classified as bad quality wine. However, this approach is problematic as it does not differentiate a wine with a quality score of 3 and a wine with a quality score of 6, when in reality there is an actual difference to someone who tastes them.\n\nFraming it as a regression problem would mean the predictions made by the model are floating point numbers and not discrete values.\n\nHence, the approach taken in this project is to frame it as a regression problem but round up or round down the predictions made by the model in order to obtain discrete values. Then, the predictions are compared against the test set to obtain the accuracy of the predictions.","352bb550":"## Decision Tree Regressor","f1e2cec0":"## Number of trees","4a3349b8":"We plot a histogram of the wine qualities to see if there is a good distribution. From it, we can see that it is similar to a normal distribution and is not skewed to either side, and therefore we can proceed to use the data as is.","a728d0ac":"# Conclusion\n\nIn this particular case, the Random Forest ensemble method performed the best among all the models considered, and was further tuned to obtain a higher accuracy, and predict 72.50% of the test set correctly.\n\nTo obtain an even higher accuracy, other parameters of the model could be tuned such as the maximum depth of the tree or maximum number of leaf nodes.","2ee4ae69":"# Train\/Test Split","2472fd56":"From the testing done above, we obtain the optimum parameter values of $ntrees=376$ and $mtry=5$, which are inserted into the final model as follows.","065f6ec2":"The parameters that will be tuned for the Random Forest model are as follows:\n- Ntree: Number of trees to grow.\n- Mtry: Number of variables randomly sampled as candidates at each split.\n\nFor each round of tuning, The Out-of-Bag (OOB) Error is calculated to determine the best value for the parameter.","57d9e0ba":"## Bagging","f686dc9f":"# Feature Importances","cccf414e":"# Evaluation of Results","67502520":"## Number of variables randomly sampled","887198f6":"# Loading Wine Quality Dataset","89b2d947":"From the results above, at the baseline with no tuning of parameters, we can see that Random Forest gives the highest accuracy and therefore the Random Forest model will be used and its parameters will be tuned.","17f4f55e":"### Summary Statistics","8b1402ca":"The wine quality data is split into 70% for the training set and 30% for the test set."}}