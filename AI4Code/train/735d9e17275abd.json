{"cell_type":{"7b097049":"code","cc7860e2":"code","365e555b":"code","22a0eb85":"code","0efc6988":"code","1933f9fd":"code","f5c211d1":"code","2ad18e76":"code","65e5a2a8":"code","e80613a7":"code","2ecd6178":"code","6b7a19bf":"code","3284efc1":"code","1edefcae":"code","9c64450f":"code","652f3a4f":"code","83fc99a3":"code","c6632bd9":"code","458d8e24":"code","fa0fbc1a":"code","34ee1bf9":"code","46cce70c":"markdown","e07f318b":"markdown","cb85c7b3":"markdown","ba7a0da2":"markdown"},"source":{"7b097049":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cc7860e2":"!pip install -q transformers==2.11.0\n!wget https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-research\/scibert\/tensorflow_models\/scibert_scivocab_uncased.tar.gz\n!tar -xvf .\/scibert_scivocab_uncased.tar.gz","365e555b":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n!transformers-cli convert --model_type bert \\\n  --tf_checkpoint '.\/scibert_scivocab_uncased\/bert_model.ckpt' \\\n  --config '.\/scibert_scivocab_uncased\/bert_config.json' \\\n  --pytorch_dump_output '.\/scibert_scivocab_uncased\/pytorch_model.bin'","22a0eb85":"from tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 12)\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom transformers import BertTokenizer ,BertConfig\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import TFBertModel\nfrom tensorflow.keras.layers import Dense, Flatten\n\nimport time\nfrom transformers import create_optimizer\nimport transformers\n\nprint('transformers version :', transformers.__version__)","0efc6988":"train_data = pd.read_csv('..\/input\/researchtopictagaug\/train_aug_with_latex_tags.csv')\nprint(train_data.shape)\ntrain_data.head()","1933f9fd":"train_data = train_data.drop(train_data.index[16394])","f5c211d1":"test_data = pd.read_csv('..\/input\/researchtopictagaug\/test_aug_with_latex_tags.csv')\nprint(test_data.shape)\ntest_data.head()","2ad18e76":"train_data['combined_text'] = train_data['TITLE'] + \" <join> \" + train_data['ABSTRACT']\ntest_data['combined_text'] = test_data['TITLE'] + \" <join> \" + test_data['ABSTRACT']","65e5a2a8":"label_cols = ['Computer Science','Physics','Mathematics', 'Statistics','Quantitative Biology','Quantitative Finance']","e80613a7":"com_sc = train_data['Computer Science'].value_counts()[1]\n\nphy = train_data['Physics'].value_counts()[1]\n\nmat = train_data['Mathematics'].value_counts()[1]\n\nstats = train_data['Statistics'].value_counts()[1]\n\nbio = train_data['Quantitative Biology'].value_counts()[1]\n\nfin = train_data['Quantitative Finance'].value_counts()[1]\n\nfig = plt.figure()\n\nax = fig.add_axes([0,0,1,1])\n\ncounts = [com_sc,phy,mat,stats,bio,fin]\n\nax.bar(label_cols,counts)\n\nplt.show()","2ecd6178":"train_data['length'] = train_data['combined_text'].apply(lambda x : x.count(\" \") + 1)\n\nsns.distplot(train_data['length'])","6b7a19bf":"# Parameters\n\nBATCH_SIZE = 8\n\nTEST_BATCH_SIZE = 8\n\nNR_EPOCHS = 1\n\nMAX_LEN = 300 # try diffrent lengths\n\nthreshold = 0.4\n\nbert_model_name = '.\/scibert_scivocab_uncased'\n\nconfig = BertConfig.from_json_file('.\/scibert_scivocab_uncased\/bert_config.json')","3284efc1":"tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    \n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        \n        tokenized_sentence = tokenizer.encode(sentence,                  \n                                              add_special_tokens = True, \n                                              max_length = max_seq_len  \n                                             )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    \n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        \n        att_mask = [int(token_id > 0) for token_id in sentence]\n        \n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ninput_ids = tokenize_sentences(train_data['combined_text'], tokenizer, MAX_LEN)\n\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n\nattention_masks = create_attention_masks(input_ids)","1edefcae":"class BertClassifier(tf.keras.Model):    \n    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        \n        super().__init__()\n        \n        self.bert = bert\n        \n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        \n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n        \n        cls_output = outputs[1]\n        \n        cls_output = self.classifier(cls_output)\n                \n        return cls_output","9c64450f":"def create_dataset(data_tuple, batch_size, train=True):\n    \n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    \n    dataset = dataset.repeat(1)\n    \n    if train:\n        \n        dataset = dataset.shuffle(buffer_size=10000)\n    \n    dataset = dataset.batch(batch_size)\n    \n    if train:\n        \n        dataset = dataset.prefetch(1)\n        \n    if not train:\n        \n        dataset = dataset.cache()\n    \n    return dataset\n","652f3a4f":"def create_test_dataset():\n    \n    test_input_ids = tokenize_sentences(test_data['combined_text'], tokenizer, MAX_LEN)\n    \n    test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n    \n    test_attention_masks = create_attention_masks(test_input_ids)\n    \n    test_steps = len(test_data) \/\/ TEST_BATCH_SIZE\n    \n    test_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False)\n    \n    return test_dataset\n\nsubmission = pd.read_csv('..\/input\/researchtopictagaug\/sample.csv', index_col='ID')\n\ndef generate_class_probablities(model,test_dataset,test_steps):\n    \n    for i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n        \n        sample_ids = test_data.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['ID']\n        \n        predictions = model(token_ids, attention_mask=masks).numpy()\n        \n        submission.loc[sample_ids, label_cols] = predictions\n    \n    return submission.loc[:,label_cols].values\n\ntest_dataset = create_test_dataset()\n\ntest_steps = len(test_data) \/\/ TEST_BATCH_SIZE\n\nclass_probs = np.zeros(shape =(len(test_data),len(label_cols)))","83fc99a3":"#Gradient Calculation\n\ndef train_step(model, token_ids, masks, labels):\n    \n    labels = tf.dtypes.cast(labels, tf.float32)\n    \n    with tf.GradientTape() as tape:\n        \n        predictions = model(token_ids, attention_mask=masks)\n        \n        loss = loss_object(labels, predictions)\n    \n    \n    gradients = tape.gradient(loss, model.trainable_variables)\n    \n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), name = 'gradients')\n    \n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        \n        auc.update_state(labels[:,i], predictions[:,i])\n        \ndef validation_step(model, token_ids, masks, labels):\n    \n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    \n    v_loss = loss_object(labels, predictions)\n\n    validation_loss(v_loss)\n    \n    for i, auc in enumerate(validation_auc_metrics):\n        \n        auc.update_state(labels[:,i], predictions[:,i])                                    ","c6632bd9":"#seeds = [0 ,31 ,97,193,1001,83,42,456,21,237] # for ensembles\n\nseeds = [0]\n\nfor seed in range(len(seeds)):\n    \n    print('=' * 50, f\"CV {seed+1}\", '=' * 50)\n    \n    model = BertClassifier(TFBertModel.from_pretrained(bert_model_name, from_pt=True, config = config), len(label_cols))\n    \n    labels =  train_data[label_cols].values\n    \n    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=seed, test_size = 0.2)\n\n    train_masks, validation_masks = train_test_split(attention_masks, random_state=seed, test_size=0.2)\n\n    train_size = len(train_inputs)\n\n    validation_size = len(validation_inputs)\n\n\n    train_dataset = create_dataset((train_inputs, train_masks, train_labels), batch_size=BATCH_SIZE,train=True)\n\n    validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), batch_size=BATCH_SIZE,train=False)\n    \n    \n    steps_per_epoch = train_size \/\/ (BATCH_SIZE)\n\n    #  Loss Function\n    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\n\n    validation_loss = tf.keras.metrics.Mean(name='val_loss')\n\n    #  Optimizer (with 1-cycle-policy)\n    warmup_steps = steps_per_epoch \/\/ 3\n\n    total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\n\n    optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n    # Gradients\n    \n    gradients = 0\n    \n    #  Metrics\n    train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n    validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n\n    for epoch in range(NR_EPOCHS):\n\n        print('=' * 50, f\"EPOCH {epoch+1}\", '=' * 50)\n\n        start = time.time()\n\n\n        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(train_dataset)):\n\n            train_step(model, token_ids, masks, labels)\n\n            if batch_no % 100 == 0:\n\n                    print(f'\\nTrain Step: {batch_no}, Loss: {train_loss.result()}')\n\n                    for i, label_name in enumerate(label_cols):\n\n                        print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n\n                        train_auc_metrics[i].reset_states()\n\n        for batch_no, (token_ids, masks, labels) in enumerate(tqdm(validation_dataset)):\n\n            validation_step(model, token_ids, masks, labels)\n\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n\n        for i, label_name in enumerate(label_cols):\n\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n\n            validation_auc_metrics[i].reset_states()\n\n        print('\\n')\n        \n    probs = generate_class_probablities(model,test_dataset,test_steps)\n        \n    submission.loc[:, label_cols] = probs\n        \n    submission.to_csv('probs'+str(seed)+'.csv')\n        \n    class_probs += probs","458d8e24":"submission.loc[:, label_cols] = class_probs\n    \nsubmission.to_csv('probs_300.csv')","fa0fbc1a":"predictions = (class_probs\/len(seeds) > threshold).astype(int)\n\nsubmission.loc[:, label_cols] = predictions\n\nsubmission.head()","34ee1bf9":"submission.to_csv('submission.csv')","46cce70c":"# The model has been trained on 4 modifications of data:\n\n* Original\n* Original without Latex Tags\n* Augmented Data(Translate + De-Translate)\n* Augmented Data - without Latex Tags\n\n\nThe details can bee found out in this notebook https:\/\/www.kaggle.com\/gcspkmdr\/text-data-augmentation-latex-tag-translate\/edit","e07f318b":"# SciBERT Classifier","cb85c7b3":"## SciBERT is a BERT model trained on scientific text.\n\n* SciBERT is trained on papers from the corpus of semanticscholar.org. Corpus size is 1.14M papers, 3.1B tokens \n \n* It is trained on the full text of the papers in training,and not just abstracts\n\n* It has its own vocabulary (scivocab) that's built to best match the training corpus\n\nThe following is an implementation of SciBert via TensorFlow2","ba7a0da2":"# Tokenizer"}}