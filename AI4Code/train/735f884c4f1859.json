{"cell_type":{"ea33cfaf":"code","acc3ea7f":"code","2a5525b0":"code","9d4a03e1":"code","c9e46c17":"code","384f371d":"code","2d70704e":"code","ca8877ab":"code","73d5a1e0":"code","e52561db":"code","d659024e":"code","67d7d6c7":"code","0b88d0d4":"code","5947f27a":"code","d15ddc90":"code","0f469175":"code","9b057f54":"code","80814b3a":"code","fc71f3ef":"code","116433c9":"code","2185c65d":"code","f3762ba2":"code","a5af2121":"code","a608d4b3":"code","3c72d47b":"code","2ff6ff32":"code","69f97b89":"code","53f495ed":"code","f6b111ec":"code","9c196d9c":"markdown","39193ae6":"markdown","d4283afc":"markdown","d5516e06":"markdown","88cd6fa8":"markdown","a0c5df25":"markdown","882c1f35":"markdown","86497a2e":"markdown","0f34b628":"markdown","828bb66b":"markdown","da836975":"markdown"},"source":{"ea33cfaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acc3ea7f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np","2a5525b0":"data = pd.read_csv(\"..\/input\/datacsv\/data.csv\")\n\n","9d4a03e1":"data.head()","c9e46c17":"data.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.head()\n","384f371d":"M = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]","2d70704e":"#M.info() #B.info()","ca8877ab":"plt.scatter(M.radius_mean,M.area_mean,color =\"red\",label = \"bad\")\nplt.scatter(B.radius_mean,B.area_mean,color =\"green\",label = \"good\")\nplt.legend() #--> it is used to show labels.\nplt.show()","73d5a1e0":"plt.scatter(M.radius_mean,M.texture_mean,color =\"red\",label = \"bad\",alpha =0.5)          #alpha--->transparency\nplt.scatter(B.radius_mean,B.texture_mean,color =\"green\",label = \"good\",alpha =0.5)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend() #--> it is used to show labels.\nplt.show()","e52561db":"data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny= data.diagnosis.values\nx_data= data.drop([\"diagnosis\"],axis=1)","d659024e":"y","67d7d6c7":"# Normalization \n\nx= (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data)) \n\n#for example,one of features is 1000 and the other one is 0.1 . But both of these equality effect our model \n","0b88d0d4":"#train\/test split \n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split (x ,y,test_size=0.3,random_state=42)","5947f27a":"#KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier (n_neighbors=3)   #n_neighbors = k\nknn.fit(x_train,y_train)\nprediction =knn.predict(x_test)","d15ddc90":"prediction","0f469175":"print(\"{} nn score {}\".format(3,knn.score(x_test,y_test)))    #accuracy can be change  according to K","9b057f54":"#find k value \nscore_list=[]\nfor each in range(1,15):\n    knn2=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","80814b3a":"from sklearn.svm import SVC\nsvm=SVC(random_state=1)\nsvm.fit(x_train,y_train)","fc71f3ef":"print(\"print accuracy of svm algo:\" ,svm.score(x_test,y_test))","116433c9":"from sklearn.naive_bayes import GaussianNB\nnb= GaussianNB()\nnb.fit(x_train,y_train)","2185c65d":"print(\"print accuracy of naive bayes algorithm :\", nb.score(x_test,y_test))","f3762ba2":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=42)","a5af2121":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"decision tree score :\" ,dt.score(x_test,y_test))","a608d4b3":"from sklearn.ensemble import RandomForestClassifier\nrf= RandomForestClassifier(n_estimators=100,random_state=42)   #n_estimators = tree\nrf.fit(x_train,y_train)\nprint(\"random forest algorithm result :\",rf.score(x_test,y_test))","3c72d47b":"y_pred =rf.predict(x_test)\ny_true = y_test","2ff6ff32":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)","69f97b89":"cm","53f495ed":"import seaborn as sns\nimport matplotlib.pyplot as plt","f6b111ec":"f,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)","9c196d9c":"x=20\ny=30  K=3  ---->is malignant or bening?      \n\nwe say it is malignant because three nearest points are malignant","39193ae6":"we use euclidean distance to  find nearest points\n\n((x-x1)^2\/(y-y1)^2)^1\/2","d4283afc":"# Confusion matrix","d5516e06":"# Random Forest Classification","88cd6fa8":"Our data is cancer dataset. \n* diagnosis = m ----> malignant cancer \n* diagnosis = b ----> benign cancer","a0c5df25":"# Support Vector Machine (SVM) Classification","882c1f35":"* Choose K value\n* Find nearest data points as much \"K\"\n* Calculate how much class is there between nearest data points.\n* determine the points we tested in which class","86497a2e":"# Decision Tree Classification","0f34b628":"\"id\" and \"Unnamed: 32\" columns are not important for us to predict.","828bb66b":"* Import dataset","da836975":"# Naive Bayes Classification"}}