{"cell_type":{"aa01e057":"code","e37ff11e":"code","def1da0f":"code","3942c2f3":"code","ac3aa252":"code","968a32ed":"code","69e49337":"code","2ab91b93":"code","cfd108bf":"code","5c068874":"code","96b0513f":"code","7e09fb92":"code","4c92bdbb":"code","7f0a8525":"code","90ce7766":"code","c94da126":"code","d2a38494":"code","c475a15a":"code","1674e3f3":"code","397ac0eb":"code","14cd5391":"code","3b115b36":"code","5b505a54":"code","f7e0e6f0":"code","e023ced9":"code","236bf499":"code","2c2cb188":"code","801a69ad":"code","36156f6d":"code","3e999d90":"code","31c074dc":"code","18808b45":"code","8570cf1e":"code","8e7d9860":"code","6156c855":"code","1bd4e454":"code","cd34971c":"code","1baef002":"code","a8f8b79a":"code","f0635faf":"code","95acbf03":"code","5803af37":"code","064e39c6":"code","4432a172":"markdown","fd319f82":"markdown","5aaa55d5":"markdown","6e88c70b":"markdown","6bd32484":"markdown","0c3fc045":"markdown","24d7ba06":"markdown","2bb32272":"markdown","d77cfb6b":"markdown","3494aba0":"markdown","a81d6df9":"markdown","f8fc59e1":"markdown","fb1541e5":"markdown","d0fd880b":"markdown","bd0c20db":"markdown","8d00f6a1":"markdown","b1ce95b4":"markdown","e647d0a1":"markdown","11091652":"markdown","f5f49769":"markdown","6eda7424":"markdown","82cf2d84":"markdown","c37b4fb4":"markdown","16449407":"markdown","e2777e57":"markdown","da78bd52":"markdown","9eed2a74":"markdown"},"source":{"aa01e057":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport numpy as np","e37ff11e":"df = pd.read_csv('..\/input\/cars-dataset-audi-bmw-ford-hyundai-skoda-vw\/cars_dataset.csv')\nprint(df.shape)\ndf.head()","def1da0f":"df.info()","3942c2f3":"df.describe()","ac3aa252":"categoricals = list(df.select_dtypes('object').columns)\nnumericals = [col for col in df.columns if col not in categoricals]\nprint(categoricals)\nprint(numericals)","968a32ed":"# Turn 'object' into category for less memory usage.\ndf[categoricals] = df[categoricals].astype('category')\ndf[categoricals].dtypes","69e49337":"# Relationships between numerical features\nsns.pairplot(df, corner=True)","2ab91b93":"for col in df[categoricals]:\n    print(f'We have {len(df[col].unique())} unique values in --{col}-- column: {df[col].unique()}', '\\n')","cfd108bf":"# Let's induce some categorical variables on our relationships --- transmission\nsns.pairplot(df, hue='transmission', corner=True)","5c068874":"# Let's induce some categorical variables on our relationships --- fuelType\nsns.pairplot(df, hue='fuelType', corner=True)","96b0513f":"# Let's induce some categorical variables on our relationships --- Make\nsns.pairplot(df, hue='Make', corner=True)","7e09fb92":"# Count plots for 'transmission', 'fuelType', 'Make'.\nx=0\nfig=plt.figure(figsize=(20,10))\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Count of 'transmission', 'fuelType', and 'Make'\", x=0.4 ,y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[categoricals[1:]]:\n    ax = plt.subplot(241+x)\n    ax = sns.countplot(data=df, y=i, color='#a6ff4d')\n    plt.grid(axis='x')\n    x+=1","4c92bdbb":"colors = ['#101907', '#314c17', '#63992e', '#95e545', '#aeff5e', '#c0ff82', '#dbffb7']","7f0a8525":"# Count of 'transmission' by  'Make'\nfig=plt.figure(figsize=(15,8))\nplt.suptitle(\"Count of 'transmission' by 'Make'\", x=0.5 ,y=0.92, family='Sherif', size=18, weight='bold')\nsns.countplot(data=df, x='transmission', hue='Make', palette=colors)\nplt.grid(axis='y')","90ce7766":"# Count of 'fuelType' by 'Make'\nfig=plt.figure(figsize=(15,8))\nplt.suptitle(\"Count of 'fuelType' by 'Make'\", x=0.5 ,y=0.92, family='Sherif', size=18, weight='bold')\nsns.countplot(data=df, x='fuelType', hue='Make', palette=colors)\nplt.grid(axis='y')","c94da126":"# Variance of numerical features\ndf.var()","d2a38494":"# Correlation matrix\ncorr = df.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nwith sns.axes_style('white'):\n    fig, ax = plt.subplots(figsize=(18,10))\n    sns.heatmap(corr,  mask=mask, annot=True, cmap=colors, center=0, square=True)","c475a15a":"# Show spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = True\nplt.rcParams['axes.spines.right'] = True\nplt.rcParams['axes.spines.top'] = True\nplt.rcParams['axes.spines.bottom'] = True","1674e3f3":"# Distribution of numerical features\nx=0\nfig=plt.figure(figsize=(15,10),constrained_layout =True)\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Distribution of numerical variables\",y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[numericals]:\n    ax = plt.subplot(231+x)\n    ax = sns.histplot(data=df, x=i, bins=20, color='#a6ff4d')\n    x+=1","397ac0eb":"# Hide spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False","14cd5391":"mp_make = df.groupby('Make')['price'].mean().sort_values()\nmp_transmission = df.groupby('transmission')['price'].mean().sort_values()\nmp_fueltype = df.groupby('fuelType')['price'].mean().sort_values()\n\n\nfig, ax = plt.subplots(1,3, figsize=[15,7], constrained_layout=True)\nplt.suptitle(\"Mean price of a car by certain feature\",y=1.15, family='Sherif', size=18, weight='bold')\n\n# First plot\nvals_0 = [round(i) for i in mp_make]\nax[0].barh(mp_make.index, mp_make, color = '#95e545')\nax[0].set_title(\"Make\")\nax[0].set_xticks([])\nfor index, value in enumerate(vals_0):\n    ax[0].text(value, index, str(value))\n\n# Second plot\nvals_1 = [round(i) for i in mp_transmission]\nax[1].barh(mp_transmission.index, mp_transmission, 0.45, color = '#c0ff82')\nax[1].set_title(\"Transmission\")\nax[1].set_xticks([])\nfor index, value in enumerate(vals_1):\n    ax[1].text(value, index, str(value))\n\n# Third plot\nvals_2 = [round(i) for i in mp_fueltype]\nax[2].barh(mp_fueltype.index, mp_fueltype, 0.6, color = '#537f26')\nax[2].set_title(\"fuelType\")\nax[2].set_xticks([])\nfor index, value in enumerate(vals_2):\n    ax[2].text(value, index, str(value))\n\n# Layout spacing\nfig.set_constrained_layout_pads(w_pad=2 \/ 72, h_pad=2 \/ 72, hspace=0.2,\n                                wspace=0.2)","3b115b36":"# Show spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = True\nplt.rcParams['axes.spines.right'] = True\nplt.rcParams['axes.spines.top'] = True\nplt.rcParams['axes.spines.bottom'] = True","5b505a54":"\nfig, axes = plt.subplots(4, 1, figsize=(18, 12), constrained_layout =True)\n\n# First plot\nax = sns.lineplot(ax = axes[0], data=df, x=\"year\", y='mileage', ci=None)\nax.set_xticks(np.arange(1996, 2020, 1))\nax.set_title(\"Mean 'mileage' by 'year'\")\nax.set_xlim(1996,2020)\n\n# Second plot\nax1 = sns.lineplot(ax = axes[1], data=df, x='year', y='mileage', hue='transmission', ci=None)\nax1.set_xticks(np.arange(1996, 2020, 1))\nax1.set_title(\"Mean 'mileage' by 'year' for each 'transmission'\")\nax1.set_xlim(1996,2020)\n\n# Third plot\nax2 = sns.lineplot(ax = axes[2], data=df, x='year', y='mileage', hue='fuelType', ci=None)\nax2.set_xticks(np.arange(1996, 2020, 1))\nax2.set_title(\"Mean 'mileage' by 'year' for each 'fuelType'\")\nax2.set_xlim(1996,2020)\n\n# Third plot\nax3 = sns.lineplot(ax = axes[3], data=df, x='year', y='mileage', hue='Make', ci=None)\nax3.set_xticks(np.arange(1996, 2020, 1))\nax3.set_title(\"Mean 'mileage' by 'year' for each 'Make'\")\nax3.set_xlim(1996,2020)\nax3.legend(loc='upper right')\n\nplt.show()","f7e0e6f0":"# Bins number preset\nb = 30\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 7), constrained_layout =True)\n\n# First plot\nax = sns.histplot(ax = axes[0,0], data=df, x='price', hue='transmission', element='poly', bins = b)\nax.set_title(\"'price' distribution by 'transmission'\")\n\n# Second plot\nax1 = sns.histplot(ax = axes[0,1], data=df, x='mileage', hue='transmission', element='poly', bins = b, legend=False)\nax1.set_title(\"'mileage' distribution by 'transmission'\")\n\n# Third plot\nax2 = sns.histplot(ax = axes[0,2], data=df, x='year', hue='transmission', element='poly', bins = b, legend=False)\nax2.set_title(\"'year' distribution by 'transmission'\")\n\n\n# Fourth plot\nax3 = sns.histplot(ax = axes[1,0], data=df, x='mpg', hue='transmission', element='poly', bins = b, legend=False)\nax3.set_title(\"'mpg' distribution by 'transmission'\")\n\n# Fifth plot\nax4 = sns.histplot(ax = axes[1,1], data=df, x='engineSize', hue='transmission', element='poly', bins = b, legend=False)\nax4.set_title(\"'engine' distribution by 'transmission'\")\n\n# Sixth plot\nax5 = sns.histplot(ax = axes[1,2], data=df, x='tax', hue='transmission', element='poly', bins = b, legend=False)\nax5.set_title(\"'tax' distribution by 'transmission'\")\n\n\nplt.show()","e023ced9":"# Split data by 'make'\nbmw = df[df['Make'] == 'BMW']\nford = df[df['Make'] == 'Ford']\nhyundai = df[df['Make'] == 'Hyundai']\naudi = df[df['Make'] == 'audi']\nskoda = df[df['Make'] == 'skoda']\ntoyota = df[df['Make'] == 'toyota']\nvw = df[df['Make'] == 'vw']","236bf499":"# Bins number & color preset \nb = 40\nc='#a6ff4d'\n\n# Price distribution of each 'Make'\nfig, axes = plt.subplots(3, 3, figsize=(20, 7), constrained_layout =True)\naxes[-1, -1].axis('off') # hide axes\naxes[-1, -2].axis('off') # hide axes\nplt.suptitle(\"Price distribution for each 'Make'\",y=1.15, family='Sherif', size=18, weight='bold')\n\n# First plot\nax = sns.histplot(ax = axes[0,0], data=bmw, x='price', element='poly', bins = b, color = c)\nax.set_title(\"BMW\")\n\n# Second plot\nax = sns.histplot(ax = axes[0,1], data=ford, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Ford\")\n\n# Third plot\nax = sns.histplot(ax = axes[0,2], data=hyundai, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Hyundai\")\n\n# Fourth plot\nax = sns.histplot(ax = axes[1,0], data=audi, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Audi\")\n\n# Fifth plot\nax = sns.histplot(ax = axes[1,1], data=skoda, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Skoda\")\n\n# Sixth plot\nax = sns.histplot(ax = axes[1,2], data=toyota, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Toyota\")\n\n# Seventh plot\nax = sns.histplot(ax = axes[2,0], data=vw, x='price', element='poly', bins = b, color = c)\nax.set_title(\"VW\")\n\nplt.show()","2c2cb188":"transmission_other = df[df['transmission'] == 'Other']\nprint(transmission_other)\n\n# Replace 'other' with most frequent unique ('Manual') since  it only contains only 4 rows (I consider not enough information for the model)\ndf['transmission'] = df['transmission'].replace('Other', 'Manual')","801a69ad":"electric_fuel = df[df['fuelType'] == 'Electric']\nprint(electric_fuel)\n\nother_fuel = df[df['fuelType'] == 'Other']\nprint(other_fuel)","36156f6d":"# Assign 'Electric' fuelType  unique  to 'Other' since  it only contains only 5 rows (I consider not enough information for the model).\ndf['fuelType'] = df['fuelType'].replace('Electric', 'Other')","3e999d90":"# Turn categoricals into numeric\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor c in categoricals:\n    df[c] = le.fit_transform(df[c])","31c074dc":"# Convert our categorical columns to dummies\nfor col in categoricals:\n    dumm = pd.get_dummies(df[col], prefix = str(col)+'_', dtype=int)\n    df = pd.concat([df,dumm], axis=1)","18808b45":"# Drop the original categories since we one hot encoded them\ndf.drop(categoricals, axis=1, inplace=True)\ndf.shape","8570cf1e":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Separate features & target\ny = df['price']\nX = df.drop('price', axis = 1)\nprint(X.shape, y.shape)\nprint('\\n')\n\n# Standardize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# Split into training (80%) and testing set (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","8e7d9860":"from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet, Lars, Lasso, BayesianRidge, HuberRegressor\nfrom sklearn.metrics import mean_squared_error\n\nr_squared = []\nrmses = []\n \nlin_reg = [('LR', LinearRegression()), ('Ridge', Ridge()), ('SGDR', SGDRegressor()), \n            ('ElasticNet', ElasticNet()), ('Lars', Lars()), ('Lasso', Lasso()),\n            ('BayesianRidge', BayesianRidge()), ('HuberRegressor', HuberRegressor())] \n\n\nfor name, model in lin_reg:\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    rs = model.score(X_test, y_test)\n    r_squared.append(rs)\n    rmses.append(rmse)\n    print(f'The accuracy of {name} is {rmse:.3f}')\n    print(f'The R^2 of {name} is {rs:.3f}')\n    print('\\n')","6156c855":"# Create a dataframe that contains relevant performances of the linear regressors\nmodels = [name for name, model in lin_reg]\n\n# Exclude worst models & their performances\nremove_indices = [0,2,4]\nmodels = [i for j, i in enumerate(models) if j not in remove_indices]\nr_squared = [i for j, i in enumerate(r_squared) if j not in remove_indices]\nrmses =  [i for j, i in enumerate(rmses) if j not in remove_indices]\n\nscores_lin_reg = pd.DataFrame({'Model': models, 'Test_R^2': r_squared, 'Test_RMSE': rmses})","1bd4e454":"# Hide spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\n\n# Plot linear regressors performance\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), constrained_layout =True)\nplt.suptitle(\"Linear Regressors performance\", x=0.5 ,y=1.15, family='Sherif', size=18, weight='bold')\n\nax = sns.barplot(ax = axes[0], data=scores_lin_reg.sort_values('Test_RMSE'), x='Model', y='Test_RMSE', palette=colors)\nax.set_title('Root Mean Squared Error')\nax.grid(axis='y')\n\nax1 = sns.barplot(ax = axes[1], data=scores_lin_reg.sort_values('Test_R^2', ascending = False), x='Model', y='Test_R^2', palette=colors)\nax1.set_title('R^2')\nax1.grid(axis='y')","cd34971c":"from sklearn.ensemble import RandomForestRegressor\nSEED = 123\nrf = RandomForestRegressor(random_state=SEED)\nrf.fit(X_train, y_train)\npreds = rf.predict(X_test)\nrmse_rf = np.sqrt(mean_squared_error(y_test, preds))\nrs_rf = rf.score(X_test, y_test)\n\nprint(f'Random Forest RMSE is: {rmse_rf}')\nprint(f'Random Forest R^2 is: {rs_rf}')","1baef002":"from sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(random_state=SEED)\ngb.fit(X_train, y_train)\npreds = gb.predict(X_test)\nrmse_gb = np.sqrt(mean_squared_error(y_test, preds))\nrs_gb = gb.score(X_test, y_test)\n\nprint(f'Gradient Boosting RMSE is: {rmse_gb}')\nprint(f'Gradient Boosting R^2 is: {rs_gb}')","a8f8b79a":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(seed=SEED)\nxgb_reg.fit(X_train, y_train)\npreds = xgb_reg.predict(X_test)\nrmse_xgb = np.sqrt(mean_squared_error(y_test, preds))\nrs_xgb = xgb_reg.score(X_test, y_test)\n\nprint(f'XGBost RMSE is: {rmse_xgb}')\nprint(f'XGBoost R^2 is: {rs_gb}')","f0635faf":"# Create dataframe with ensemble methods performances\nmodels_en = ['RandomForest', 'GradientBoosting', 'XGBoost']\nrmses_en = [rmse_rf, rmse_gb, rmse_xgb]\nr_squared_en = [rs_rf, rs_gb, rs_xgb]\n\nscores_en = pd.DataFrame({'Model': models_en, 'Test_R^2': r_squared_en, 'Test_RMSE': rmses_en})","95acbf03":"# Plot ensemble methods performance\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), constrained_layout =True)\nplt.suptitle(\"Ensemble methods performance\", x=0.5 ,y=1.15, family='Sherif', size=18, weight='bold')\n\nax = sns.barplot(ax = axes[0], data=scores_en.sort_values('Test_RMSE'), x='Model', y='Test_RMSE', palette=colors)\nax.set_title('Root Mean Squared Error')\nax.grid(axis='y')\n\nax1 = sns.barplot(ax = axes[1], data=scores_en.sort_values('Test_R^2', ascending = False), x='Model', y='Test_R^2', palette=colors)\nax1.set_title('R^2r')\nax1.grid(axis='y')","5803af37":"from sklearn.model_selection import GridSearchCV\n\nparams_rf = {'n_estimators':[50, 100, 200],\n               'max_depth':[None, 1, 2],\n               'min_samples_leaf':[0.5,1,1.5,2]}\n\ngrid_rf = GridSearchCV(estimator=rf, \n                       param_grid=params_rf,\n                       cv=5, \n                       scoring = 'neg_mean_squared_error',\n                       )\n\ngrid_rf.fit(X_train, y_train)\n\nbest_hyperparams = grid_rf.best_params_\n\nprint(f'The best hyperparameters found for RF are: {best_hyperparams}')","064e39c6":"best_rf = grid_rf.best_estimator_\n\npreds = best_rf.predict(X_test)\nrmse_best_rf = np.sqrt(mean_squared_error(y_test, preds))\nrs_best_rf = best_rf.score(X_test, y_test)\n\nprint(f'Best RF RMSE is: {rmse_best_rf}')\nprint(f'Best RF R^2 is: {rs_best_rf}')","4432a172":"# Reading the data & gathering short information \ud83d\udcd6","fd319f82":"There are several values of 'mpg' higher than 400, and seems to appear as outliers if we check the scatterplots.\n\nHowever, these values seems to be very explainable for the BMW 'Make', since the values are for these cars. Considering that, the values will be kept.","5aaa55d5":"Answering some questions:\n\n - **What is the mean price of a car by its 'Make'?**\n\n - **What is the mean price of a car by its 'transmission'?**\n\n - **What is the mean price of a car by its 'fuelType'?**","6e88c70b":"# Hyperparameters tuning for Random Forest \u2728","6bd32484":"To investigate the distribution other numerical features for each 'Make', the above approach can be applied by changing the 'x' of each plot to a different numerical variable.","0c3fc045":"# Ensemble methods ","24d7ba06":"# Data preparation \ud83c\udf73","2bb32272":"## Evaluation","d77cfb6b":"# <p style=\"font-family: Helvetica, fantasy; line-height: 1.3; font-size: 26px; letter-spacing: 3px; text-align: center; color: #99e600\">Prediction of Cars prices using Linear Regressors & Ensemble methods<\/p>\n\n![](https:\/\/www.newneuromarketing.com\/media\/zoo\/images\/NNM-2015-019-Cost-consciousness-increase-product-sales-with-Price-Primacy_6a73d15598e2d828b0e141642ebb5de3.png)","3494aba0":"It can be observed that the ensemble methods perform much better than the linear regressors.\n\nThe best performance is obtained by the RandoMForest:\n - RMSE: 1821.37\n - R^2:  0.96\n\n\n Let's see if these results can be improved.","a81d6df9":"As can be observed there are 3 models with similar results.\n\nHowever, the one that slighty wins, is the BayesianRidge Regressor:\n - RMSE: 3104.238\n - R^2: 0.891\n\n Let's try other modeling approaches.","f8fc59e1":"# Beginning libraries \ud83d\udcda","fb1541e5":"# Modelling \ud83c\udfd7\ufe0f","d0fd880b":"## Extreme Gradient Boosting (XGBoost)","bd0c20db":"# Data Cleaning \ud83e\uddf9","8d00f6a1":"The performance of the RandomForest Regressor was slightly improved.\n\nThe RMSE was reduced to 1814.25","b1ce95b4":"## Explore numerical features \ud83d\udcc8","e647d0a1":"## Gradient Boosting","11091652":"## Random Forest","f5f49769":"**Explore 'mileage' on 'year'**","6eda7424":"The above approach can be applied to further investigate with other aggregate functions or other numerical columns such as: 'mileage', 'tax', 'mpg' or 'engineSize'.","82cf2d84":"## Explore categorical features \ud83d\udcca","c37b4fb4":"## Investigate both categorical & numerical features \ud83e\udd1d\n","16449407":"# Linear regressors","e2777e57":"# Feature engineering \u2699\ufe0f","da78bd52":" - Moderate positive correlation between price & engineSize\n - Moderate negative correlation between year & mileage","9eed2a74":"# Exploratory Data Analysis (EDA) \ud83e\udded"}}