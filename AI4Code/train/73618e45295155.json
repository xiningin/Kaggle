{"cell_type":{"5b86abea":"code","b2c2f32f":"code","4db33bef":"code","582b64ef":"code","5129c00e":"code","ca5cec86":"code","7cf83fee":"code","483ea8ab":"code","7d09d66e":"code","22689610":"markdown","c2a32212":"markdown","db039a07":"markdown","d51c70a1":"markdown","d5ad5150":"markdown","fe92c3c6":"markdown","a58b2ac5":"markdown","248fd7d9":"markdown","ab7ea845":"markdown","ac894a75":"markdown","b6da577f":"markdown","19e48195":"markdown","9bd8a819":"markdown"},"source":{"5b86abea":"# Define our test distributions: a mix of Cauchy-distributed variables\nimport numpy as np\nfrom scipy import stats\n\nnp.random.seed(0)\n\nx = np.concatenate([stats.cauchy(-5, 1.8).rvs(500),\n                    stats.cauchy(-4, 0.8).rvs(2000),\n                    stats.cauchy(-1, 0.3).rvs(500),\n                    stats.cauchy(2, 0.8).rvs(1000),\n                    stats.cauchy(4, 1.5).rvs(500)])\n\n# Truncate values to a reasonable range:\nx = x[(x > -15) & (x < 15)]","b2c2f32f":"import pylab as pl\npl.hist(x, normed=True)","4db33bef":"pl.hist(x, bins=100, normed=True)","582b64ef":"def bayesian_blocks(t):\n    \"\"\"Bayesian Blocks Implementation\n\n    By Jake Vanderplas.  License: BSD\n    Based on algorithm outlined in http:\/\/adsabs.harvard.edu\/abs\/2012arXiv1207.5578S\n\n    Parameters\n    ----------\n    t : ndarray, length N\n        data to be histogrammed\n\n    Returns\n    -------\n    bins : ndarray\n        array containing the (N+1) bin edges\n\n    Notes\n    -----\n    This is an incomplete implementation: it may fail for some\n    datasets.  Alternate fitness functions and prior forms can\n    be found in the paper listed above.\n    \"\"\"\n    # copy and sort the array\n    t = np.sort(t)\n    N = t.size\n\n    # create length-(N + 1) array of cell edges\n    edges = np.concatenate([t[:1],\n                            0.5 * (t[1:] + t[:-1]),\n                            t[-1:]])\n    block_length = t[-1] - edges\n\n    # arrays needed for the iteration\n    nn_vec = np.ones(N)\n    best = np.zeros(N, dtype=float)\n    last = np.zeros(N, dtype=int)\n\n    #-----------------------------------------------------------------\n    # Start with first data cell; add one cell at each iteration\n    #-----------------------------------------------------------------\n    for K in range(N):\n        # Compute the width and count of the final bin for all possible\n        # locations of the K^th changepoint\n        width = block_length[:K + 1] - block_length[K + 1]\n        count_vec = np.cumsum(nn_vec[:K + 1][::-1])[::-1]\n\n        # evaluate fitness function for these possibilities\n        fit_vec = count_vec * (np.log(count_vec) - np.log(width))\n        fit_vec -= 4  # 4 comes from the prior on the number of changepoints\n        fit_vec[1:] += best[:K]\n\n        # find the max of the fitness: this is the K^th changepoint\n        i_max = np.argmax(fit_vec)\n        last[K] = i_max\n        best[K] = fit_vec[i_max]\n\n    #-----------------------------------------------------------------\n    # Recover changepoints by iteratively peeling off the last block\n    #-----------------------------------------------------------------\n    change_points =  np.zeros(N, dtype=int)\n    i_cp = N\n    ind = N\n    while True:\n        i_cp -= 1\n        change_points[i_cp] = ind\n        if ind == 0:\n            break\n        ind = last[ind - 1]\n    change_points = change_points[i_cp:]\n\n    return edges[change_points]","5129c00e":"from matplotlib import pyplot\n\n# plot a standard histogram in the background, with alpha transparency\nH1 = pyplot.hist(x, bins=200, histtype='stepfilled',\n          alpha=0.2, normed=True)\n# plot an adaptive-width histogram on top\nH2 = pyplot.hist(x, bins=bayesian_blocks(x), color='black',\n          histtype='step', normed=True)","ca5cec86":"from astropy import stats\nimport multiprocessing as mp\nfrom functools import reduce\n\ndef variable_to_bin(var, df_train):\n    \n    # Lets calculate bin values for a particular column in the dataframe passed to this function\n    bin_values = stats.bayesian_blocks(df_train[var],\n                                      fitness='events',\n                                      p0=0.01)\n    \n    # Lets create labels for bin values so as to use these labels in dataframe \n    labels = []\n    for i, x in enumerate(bin_values):\n        labels.append(i)\n    \n    # delete the last bin label \n    del labels[-1]\n\n    # create a new dataframe to \n    df = pd.DataFrame(index=df_train.index)\n\n    df[\"ID_code\"] = df_train[\"ID_code\"]\n    df['new' + var] = pd.cut(df_train[var], \n                               bins = bin_values, \n                               labels = labels)\n    \n    df.set_index('ID_code')\n    \n    # Lets delete the bin values and labels to some some space.\n    del bin_values, labels\n    \n    return df","7cf83fee":"def get_new_feature_train():\n    features = [c for c in df_train.columns if c not in [\"ID_code\", \"target\"]]\n\n    # Use below line to test whether the binning works or not. \n    #features = ('var_2', 'var_3')\n    \n    new_df = pd.DataFrame()\n    \n    # Lets create a multi processing pool but N - 4 CPU's = 4 less CPU's then what your machine has.\n    # My machine has 16 CPU's, but I wanted to use 12 of them for calculating bins. \n    pool = mp.Pool(mp.cpu_count() - 4)\n    \n    # Lets map each CPU to each variable coming out of features list. This line helps in parallel computation\n    # of bayesian block bins. \n    results = pool.map(variable_to_bin, features)\n    \n    pool.close()\n    pool.join()\n    \n    # Lets reduce the series coming out of variable_to_bin function and create a new dataframe.\n    results_df = reduce(lambda x, y: pd.merge(x, y, on = 'ID_code'), results)\n\n    return results_df","483ea8ab":"df_train.set_index('ID_code')\ntrain_df = pd.DataFrame(index=df_train.index)\ntrain_df = get_new_feature_train()","7d09d66e":"from sklearn.preprocessing import LabelEncoder \n\nfeatures = [c for c in train_df.columns if c not in [\"ID_code\", \"target\"]]\n\nlbl_enc = LabelEncoder()\nlbl_enc.fit(train_df[features])\n\ndf_Train_cat = lbl_enc.transform(train_df[features])","22689610":"# Important Links:\n\nTo refer if you want to know more about Bayesian Blocks and its implementation in AstroML:\n\n1. [AstroML library and post](http:\/\/www.astroml.org\/user_guide\/density_estimation.html#bayesian-blocks-histograms-the-right-way)\n2. [Jake's post](https:\/\/jakevdp.github.io\/blog\/2012\/09\/12\/dynamic-programming-in-python\/)","c2a32212":"The adaptive-width bins lead to a very clean representation of the important features in the data. More importantly, these bins are quantifiably optimal, and their properties can be used to make quantitative statistical statements about the nature of the data. This type of procedure has proven very useful in analysis of time-series data in Astronomy.","db039a07":"# Introduction\n\nThe aim of this kernel is to help people learn a new method of binning, called Bayesian Blocks. The other reason for releasing this kernel near the end of the competition is that I have not been able to devote that much time to look at other promising methods\/discussions that have been shared in this competition so far. So, I hope people can use this new method in their models and learn something new in the process.\n\n## What is binning?\n\nFeature binning is a method of turning continuous variables into categorical variables. This is accomplished by grouping the values into a pre-defined number of bins. The continuous value then gets replaced by a string describing the bin that contains that value.\n\n## What is Bayesian Block Binning?\n\nBayesian blocks is essentially a method creating histograms with bin sizes that adapt to the data. I am going to quote the whole post from this [link](http:\/\/jakevdp.github.io\/blog\/2012\/09\/12\/dynamic-programming-in-python\/). Please feel free to go through either posts to understand more about Bayesian Blocks. \n\nI have given a working example of using bayesian blocks at the end of this kernel. You might want to run this kernel on your own machine because of the use of Multiprocessing framework.\n\nTo illustrate that, let's look at some sampled data. \n","d51c70a1":"Not too infomative. The default bins in matplotlib are too wide for this dataset. We might be able to do better by increasing the number of bins:","d5ad5150":"Please use label encoder to encode the new features generated using the above method as the columns coming out of get_new_feature_train function are of type: category.\n\nSmall snippet on how to use label encoder:","fe92c3c6":"Simple, right?\n\nWell, no. The problem is, as the number of points N grows large, the number of possible configurations grows as $2^N$. For N=300 points, there are already more possible configurations than the number of subatomic particles in the observable universe! Clearly an exhaustive search will fail in cases of interest. This is where dynamic programming comes to the rescue.\n\n### Dynamic Programming\n\nIn our Bayesian Blocks example, we can easily find the optimal binning for a single point. By making use of some mathematical proofs concerning the fitness functions, we can devise a simple step from the optimal binning for $k$ points to the optimal binning for $k + 1$ points (the details can be found in the appendices of the Scargle paper). In this way, Scargle and collaborators showed that the $2^N$ possible states can be explored in $N^2$ time.\n\n### Algorithm:\n\nThe resulting algorithm is deceptively simple, but it can be proven to converge to the single best configuration among the $2^N$ possibilities. Below is the basic code written in python. Note that there are a few details that are missing from this version (e.g. priors on the number of bins, other forms of fitness functions, etc.) but this gets the basic job done:\n","a58b2ac5":"The details of the step from $K$ to $K + 1$ may be a bit confusing from this implementation: it boils down to the fact that Scargle et al. were able to show that given an optimal configuration of $K$ points, the $(K + 1)$^th configuration is limited to one of $K$ possibilities.\n\nThe function as written above takes a sequence of points, and returns the edges of the optimal bins. We'll visualize the result on top of the histogram we saw earlier:","248fd7d9":"# Conclusion of this post:\n\nI believe this method can be a good way to bin features and use them in models that have been shared in this competition. I would love to see if this method is able to proceed boost to your models\/score on the leaderboard.\n\nPlease upvote if you like this kernel and please feel free to provide recommendations.\n\nHappy Competing!","ab7ea845":"Now, what does this distribution look like? Let's plot a histogram:","ac894a75":"This is better. But having to choose the bin width each time we plot a distribution is not only tiresome, it may lead to missing some important information in our data. In a perfect world, we'd like for the bin width to be learned in an automated fashion, based on the properties of the data itself. There have been many rules-of-thumb proposed for this task (look up Scott's Rule, Knuth's Rule, the Freedman-Diaconis Rule, and others in your favorite statistics text). But all these rules of thumb share a disadvantage: they make the assumption that all the bins are the same size. This is not necessarily optimal. But can we do better?\n\nScargle and collaborators showed that the answer is yes. This is their insight: **For a set of histogram bins or blocks, each of an arbitrary size, one can use a Bayesian likelihood framework to compute a fitness function which only depends on two numbers: the width of each block, and the number of points in each block. ** The edges between these blocks (the change-points) can be varied, and the overall block configuration with the maximum fitness is quantitatively the best binning.","b6da577f":"### Conclusion of the post from Jake:\n\nWe've just scratched the surface of Bayesian Blocks and Dynamic Programming. Some of the more interesting details of this algorithm require much more depth: the appendicies of the Scargle paper provide these details. Dynamic Programming ideas have been shown to be useful in many optimization problems. One other example I've worked with extensively is Dijkstra's Algorithm for computing the shortest paths on a connected graph. This is available in the scipy.sparse.csgraph submodule, which is included in the most recent release of scipy.","19e48195":"(ToDo: This needs to be completed, so please check after 1 day)\n\n## Different types of binning described\/used in this competition (based on public kernels):\n\nGoing through few popular kernels, I could find these binning approaches used:\n","9bd8a819":"# Use of Bayesian Block in Santander Customer Transaction Prediction Challenge:\n\nBelow is a multiprocessing based approach to calculate bins for the dataset in this competition. As this is a computing intensive task, if you want to run this in a shorter amount of time, please use your machines instead of computing providing by Kaggler kernel as kaggle kernel only have 4 CPU's available for processing. \n\nI have not tried to improve the performance of this method (more like I am still learning performance tunning of Python functions, so please feel free to recommend improvements)."}}