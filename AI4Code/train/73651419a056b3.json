{"cell_type":{"38d5dbb8":"code","e84e9a0c":"code","8ba34e5e":"code","14fb742a":"code","838a0bb2":"code","10cfde39":"code","7bbce647":"code","219501bf":"code","0e6214fc":"code","7de0c136":"code","eb6790ad":"code","32e8e7d7":"code","d154c250":"code","23523407":"code","d7b70368":"code","0bc889a3":"markdown","079b49de":"markdown","9f0512fb":"markdown","1d6cd866":"markdown","93f03e48":"markdown","7c5e4791":"markdown","808676be":"markdown","46fc37b7":"markdown","5ef32513":"markdown","7373aaa1":"markdown","71ccfe90":"markdown","9752df14":"markdown"},"source":{"38d5dbb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy\nimport math\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e84e9a0c":"train_data = pd.read_csv(\"..\/input\/emotions-dataset-for-nlp\/train.txt\", sep = \";\", names = ['sentence', 'emotion'])\ntrain_data.head()","8ba34e5e":"nlp = spacy.load('en_core_web_lg')\n\n\ndef cosine_similarity(a, b):\n    '''similarity cheking from kaggle NLP cource'''\n    return np.dot(a, b)\/np.sqrt(a.dot(a)*b.dot(b))\n\ndef euclidean_similarity(a, b):\n    '''similarity cheking using euclidean distance'''\n    \n    s = 0\n    for i,h in enumerate(a):\n        s = s + (h - b[i])**2\n        \n    return math.sqrt(s)\n\n# we can check this for any English word of prase\nw1 = nlp(\"hello\").vector\nw2 = nlp(\"hi\").vector\n\nprint(\"hi, hello test\")\nprint(\"cos sim : \", cosine_similarity(w1,w2))\nprint(\"eucl sim : \", euclidean_similarity(w1, w2))\n\nw1 = nlp(\"Internet\").vector\nw2 = nlp(\"Internet\").vector\n\nprint(\"internet test\")\nprint(\"cos sim : \", cosine_similarity(w1,w2))\nprint(\"eucl sim : \", euclidean_similarity(w1, w2))\n\nw1 = nlp(\"Internet\").vector\nw2 = nlp(\"senior\").vector\n\nprint(\"internet , senior test\")\nprint(\"cos sim : \", cosine_similarity(w1,w2))\nprint(\"eucl sim : \", euclidean_similarity(w1, w2))\n\nw1 = nlp(\"work\").vector\nw2 = nlp(\"working\").vector\n\nprint(\"'work' forms test\")\nprint(\"cos sim : \", cosine_similarity(w1,w2))\nprint(\"eucl sim : \", euclidean_similarity(w1, w2))","14fb742a":"#with nlp.disable_pipes():\n#    vectors = pd.DataFrame([nlp(sent).vector for sent in train_data['sentence']])\n    \n# I assume that the calculations will be long, so I will save the results in csv\n#vectors.to_csv('vectors.csv', index=False)","838a0bb2":"# loading data and prepare it for model\nX = pd.read_csv('..\/input\/vectorised-emotions-dataset\/vectors.csv')\ny = train_data['emotion']\n\n\nprint(pd.unique(y))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)","10cfde39":"data_params = {'anger' : {}, 'sadness' : {}, 'love' : {}, 'surprise': {}, 'fear' : {}, 'joy' : {}}\n\n# intial values\nfor keys, items in data_params.items():\n    items['vect_sum'] = [0]*300\n    items['count'] = 0\n\nfor i in y.index:\n    #computing sum of all vectors \n    data_params[y[i]]['vect_sum'] = [x+y for x, y in zip(data_params[y[i]]['vect_sum'], X.iloc[i].to_list())]\n    #count of vectors\n    data_params[y[i]]['count'] += 1\n\ncov_mat_sum = False\n\nfor keys, items in data_params.items():\n    # computing avarage points for every emotion\n    items['centroid'] = [i\/items['count'] for i in items['vect_sum']]\n    # computin of covariation matrixes\n    items['cov_matrix'] = np.cov(X.loc[y == keys].to_numpy(), bias = True, rowvar = False)\n    if type(cov_mat_sum) == False: cov_mat_sum = items['cov_matrix']*items['count']\n    else: cov_mat_sum = cov_mat_sum + items['cov_matrix']*items['count']\n\n\n# united covavatriation matrix\nu_cov_mat = cov_mat_sum \/ (X.shape[0] - len(data_params))\ninv_u_cov_mat = np.linalg.inv(u_cov_mat)\n    ","7bbce647":"for keys, items in data_params.items():\n    items['b'] = np.dot(inv_u_cov_mat,np.array(items['centroid']))\n    items['b_0'] = np.dot(items['b'], np.array(items['centroid']))*(-0.5)","219501bf":"for i in pd.unique(y):\n    print(i)","0e6214fc":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlin = LinearDiscriminantAnalysis()\nlin.fit(X_train, y_train)\n\nprint(f'Linear model accuracy: {lin.score(X_test, y_test)*100:.3f}%')","7de0c136":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqad = QuadraticDiscriminantAnalysis()\nqad.fit(X_train, y_train)\n\nprint(f'Quadratic model accuracy: {qad.score(X_test, y_test)*100:.3f}%')","eb6790ad":"from sklearn.svm import LinearSVC\n\nsvc = LinearSVC(random_state=1, dual=False, max_iter=10000)\nsvc.fit(X_train, y_train)\n\nprint(f'hello model accuracy: {svc.score(X_test, y_test)*100:.3f}%')","32e8e7d7":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(max_depth=20, random_state=0, n_estimators=400)\nclf.fit(X_train, y_train)\n\nprint(f'random forest model accuracy: {clf.score(X_test, y_test)*100:.3f}%')","d154c250":"lin.fit(X, y)\nprint(f'Linear model accuracy: {lin.score(X, y)*100:.3f}%')\n\nw = pd.DataFrame(nlp(\"I did not like the goods I received, the quality is poor\").vector).transpose()\nprint(lin.predict(w))\n\nw = pd.DataFrame(nlp(\"Best product in this price range, recommended to everyone!\").vector).transpose()\nprint(lin.predict(w))\n\nw = pd.DataFrame(nlp(\"Terribly angry at the staff. I hate this shop!\").vector).transpose()\nprint(lin.predict(w))\n\nw = pd.DataFrame(nlp(\"wow this product is really amazing\").vector).transpose()\nprint(lin.predict(w))\n\nw = pd.DataFrame(nlp(\"This monster haunts me!\").vector).transpose()\nprint(lin.predict(w))\n","23523407":"\ncorrect_counter = 0\n\nfor i in range(X.shape[0]):\n    fun_val = []\n    for keys, items in data_params.items():\n        fun_val.append(np.dot(items['b'], X.iloc[i].to_numpy()) + items['b_0'])\n\n    max_ind = fun_val.index(max(fun_val))\n    #print(list(data_params.keys())[max_ind] + \"  \" + y.iloc[i])\n    if list(data_params.keys())[max_ind] == y.iloc[i]: correct_counter += 1\n\n        \nprint(\"print correct result: \" + str(correct_counter))\nprint(\"percent of correct: \" + str(correct_counter \/ X.shape[0]))\n    ","d7b70368":"# printing of distriminant fucntions\nfor keys, items in data_params.items():\n    result_str = \"\"\n    for i, coef in enumerate(items['b']):\n        if coef > 0: sign = \"+\"\n        else: sign = \"-\"\n        result_str += sign + \"x\" + str(i + 1) + \"*\" + str(abs(coef))[:5]\n        \n    result_str += str(items['b_0'])\n    print(keys)\n    print(result_str)\n        ","0bc889a3":"# Now let's try to vectorise our sentences.","079b49de":"getting covariation matrixes","9f0512fb":"# Model building","1d6cd866":"Creating a spacy and checking how it works","93f03e48":"lets try to compute differend data params\n","7c5e4791":"So lets try quadratic model.","808676be":"now lets create vector for every sentence from data set","46fc37b7":"Let's create a simple linear model and evaluate its accuracy ","5ef32513":"print(y)As well as i understand linear model is the best. So lets try some sentences with it.","7373aaa1":"getting coefficients of discriminant funcitons","71ccfe90":"# My own model checking","9752df14":"# **Lets load data.**"}}