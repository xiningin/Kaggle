{"cell_type":{"4f346c61":"code","5cd40dbe":"code","f79be937":"code","c0bebcd4":"code","cf97dae9":"code","3e4e14c3":"code","49b91f7a":"code","51658023":"code","bbc415b8":"code","9ecc67ae":"markdown","b70098a1":"markdown","5576e7ed":"markdown","c3c5be16":"markdown"},"source":{"4f346c61":"!pip install transformers","5cd40dbe":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)","f79be937":"from torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\n\nbatch_size = 16\nmax_len = 128\nmodel_name = 'bert-base-multilingual-cased'\n\nclass ContradictionDataset(Dataset):\n    def __init__(self, data):\n        self.examples = self.encode(data)\n    \n    def __getitem__(self, index):\n        example = self.examples[index]\n        return self.move_to_device(example)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def encode(self, data):  \n        inputs = []\n        for index, row in train_data.iterrows():\n            encoding = tokenizer(\n                text=row['premise'], \n                text_pair=row['hypothesis'],\n                truncation=True,\n                padding='max_length',\n                max_length=max_len,\n                return_tensors='pt'\n            )\n            if 'label' in data:\n                encoding['labels'] = torch.tensor([row['label']])\n            inputs.append(encoding)\n        return inputs\n\n    def move_to_device(self, inputs):\n        return {key: torch.squeeze(inputs[key]).to(device) for key in inputs}\n\ntokenizer = BertTokenizer.from_pretrained(model_name)\n\n%cd '\/content\/drive\/My Drive\/ml_hw\/kaggle\/contradictory-my-dear-watson'\ntrain_data = pd.read_csv('.\/train.csv')\ntrain_data, valid_data = train_test_split(train_data, train_size=0.9, test_size=0.1)\ntest_data = pd.read_csv('.\/test.csv')\n\ntrain_set = ContradictionDataset(train_data)\nvalid_set = ContradictionDataset(valid_data)\ntest_set = ContradictionDataset(test_data)\n\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)","c0bebcd4":"learning_rate = 1e-5\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nmodel.to(device)","cf97dae9":"LOG_INTERVAL = round(len(train_loader) \/ 10)\n\ndef train(epoch):\n    model.train()\n    total_loss = 0\n\n    for batch_index, batch in enumerate(train_loader):\n        model.zero_grad()\n        output = model(**batch)\n        loss = output[0]\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if batch_index % LOG_INTERVAL == 0 and batch_index > 0:\n            current_loss = total_loss \/ LOG_INTERVAL\n            print('| epoch {:3d} | ' \n                  '{:5d}\/{:5d} batches | '\n                  'loss {:5.2f}'.format(\n                    epoch, \n                    batch_index, len(train_loader), \n                    current_loss))\n            total_loss = 0\n\ndef test(data_loader):\n    model.eval()\n    total_score = 0\n\n    with torch.no_grad():\n        for batch_index, batch in enumerate(data_loader):\n            output = model(**batch)\n            preds = np.argmax(output[1].cpu(), axis=1)\n            total_score += preds.eq(batch['labels'].cpu()).sum()\n    return (total_score.item() \/ (len(data_loader) *batch_size)) * 100","3e4e14c3":"EPOCHS = 5\n\naccuracy = test(valid_loader)\nprint('| Pretraining Accuracy: {:.2f}%\\n'.format(accuracy))\n\nfor epoch in range(1, EPOCHS + 1):\n    train(epoch)\n    accuracy = test(valid_loader)\n    print('| epoch   {} |  Accuracy: {:.2f}%\\n'.format(epoch, accuracy))","49b91f7a":"model.eval()\npreds = []\nwith torch.no_grad():\n    for batch_index, batch in enumerate(test_loader):\n        output = model(**batch)\n        preds += np.argmax(output[0].cpu(), axis=1).tolist()\n\n","51658023":"submission = pd.DataFrame(test_data['id'])\nsubmission['prediction'] = pd.Series(preds)\nsubmission.sample(10)","bbc415b8":"submission.to_csv(\"submission.csv\", index = False)","9ecc67ae":"# Data preprocessing","b70098a1":"# Model","5576e7ed":"# Training","c3c5be16":"# Generating test set predictions"}}