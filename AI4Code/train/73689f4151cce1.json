{"cell_type":{"5326825a":"code","643a6961":"code","db1de76e":"code","224727b2":"code","95010302":"code","b23bebfa":"code","a52e7fdc":"code","fedd35fc":"code","5ff32d86":"code","50cb1d3b":"code","8c5560ce":"code","35f7bd35":"code","39709e05":"code","7dd07fb3":"code","969b1f12":"code","0ea7c2c4":"code","af8225f2":"code","dc994170":"code","b341e1fc":"markdown","acec4428":"markdown","39c3fa38":"markdown","fbd49aef":"markdown","aa967814":"markdown","97c95f3d":"markdown","596a475c":"markdown","53b6af90":"markdown","e3ad1834":"markdown","5d42ab28":"markdown","da6a2b1c":"markdown","1f49c52e":"markdown"},"source":{"5326825a":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nimport sys\nimport matplotlib.pyplot as plt","643a6961":"# reading input data\ninput_data = pd.read_csv(\"..\/input\/Life Expectancy Data.csv\")\ninput_data.head()","db1de76e":"# we will ignore countries and years, as they are a big giveaway for multiple years of the same country\nx_input = input_data.drop(['Life expectancy ', 'Country', 'Year'], axis=1)  \ny_input = input_data['Life expectancy ']","224727b2":"x_input = x_input.drop(['Status'], axis=1)","95010302":"my_imputer = SimpleImputer(strategy=\"most_frequent\") #strategy accounts for both strings and integer values\nx_input = pd.DataFrame( my_imputer.fit_transform(x_input) )\ny_input = pd.DataFrame( my_imputer.fit_transform(pd.DataFrame(y_input)) )","b23bebfa":"train_x, temp_x, train_y, temp_y = train_test_split(x_input,y_input, test_size=0.2)\nval_x, test_x, val_y, test_y = train_test_split(temp_x, temp_y, test_size=0.5)","a52e7fdc":"def add_bias(data):\n    temp = data.copy()\n    temp.insert(0,'bias',1)\n    return temp","fedd35fc":"b_train_x = add_bias(train_x)\nb_val_x = add_bias(val_x)\nb_test_x = add_bias(test_x)","5ff32d86":"plain_lin_reg_weights = np.linalg.inv(np.transpose(b_train_x) @ b_train_x) @ np.transpose(b_train_x) @ train_y\n\nplain_lin_reg_predictions = b_test_x @ plain_lin_reg_weights\nplain_lin_reg_error = mean_squared_error(plain_lin_reg_predictions, test_y)","50cb1d3b":"def normalize (data):\n    return (data - data.mean() ) \/ data.std()","8c5560ce":"norm_train_x = normalize(train_x)\nnorm_val_x = normalize(val_x)\nnorm_test_x = normalize(test_x)","35f7bd35":"bnorm_train_x = add_bias(norm_train_x)\nbnorm_val_x = add_bias(norm_val_x)\nbnorm_test_x = add_bias(norm_test_x)","39709e05":"# finds the optimal lambda value between the optional parameters for min(mn), max(mx), and jump size\ndef cross_val(train_x, train_y, val_x, val_y, mn = 0.001, mx = 2, jump = 0.0005):\n    best_lambd = mn\n    lowest_error = sys.maxsize\n    \n    for lambd in np.arange(mn, mx, jump):\n        cur_weights = np.linalg.inv((np.transpose(train_x) @ train_x) + (lambd*np.eye(len(train_x.axes[1]))) ) @ np.transpose(train_x) @ train_y\n        cur_predictions = val_x @ cur_weights\n        cur_error = mean_squared_error(cur_predictions, val_y)\n        if (lowest_error > cur_error):\n            lowest_error = cur_error\n            best_lambd = lambd\n            \n    return best_lambd","7dd07fb3":"ridge_lambda = cross_val(bnorm_train_x, train_y, bnorm_val_x, val_y)\nridge_lin_reg_weights = np.linalg.inv((np.transpose(bnorm_train_x) @ bnorm_train_x) + (ridge_lambda * np.eye(len(bnorm_train_x.axes[1]))) ) @ np.transpose(bnorm_train_x) @ train_y\n\nridge_lin_reg_predictions = bnorm_test_x @ ridge_lin_reg_weights\nridge_lin_reg_error = mean_squared_error(ridge_lin_reg_predictions, test_y)","969b1f12":"def lasso_cross_val2( train_x, train_y, val_x, val_y, mn = 0.001, mx = 2, jump = 0.0005):\n    best_lambd = mn\n    lowest_error = sys.maxsize\n    for lambd in np.arange(mn, mx, jump):\n        temp_model = linear_model.Lasso(alpha=lambd, max_iter=10000)\n        temp_model.fit(train_x, train_y)\n        cur_error = mean_squared_error(temp_model.predict(val_x), val_y)\n        if (lowest_error > cur_error):\n            lowest_error = cur_error\n            best_lambd = lambd\n            \n    return best_lambd","0ea7c2c4":"lasso_lambd = lasso_cross_val2(norm_train_x, train_y, norm_val_x, val_y)\nlasso_model = linear_model.Lasso(alpha=lasso_lambd, max_iter=10000)\nlasso_model.fit(norm_train_x, train_y)\n\nlasso_lin_reg_predictions = lasso_model.predict(norm_test_x)\nlasso_lin_reg_error = mean_squared_error(lasso_lin_reg_predictions, test_y.values)","af8225f2":"alphas, _, coefs = linear_model.lars_path(train_x.values, train_y.values.ravel(), method='lasso')\n\nxx = np.sum(np.abs(coefs.T), axis=1)\nxx \/= xx[-1]\n\nplt.figure(figsize=[20,7])\nplt.plot(xx, coefs.T)\n\nplt.xlabel('|coef| \/ max|coef|')\nplt.ylabel('Coefficients')\nplt.title('LASSO Path')\nplt.axis('tight')\n\nplt.ylim(0,3)\nplt.show()","dc994170":"print('The residual sum of square error is:\\nPlain Least Squares Regression: ', plain_lin_reg_error, \\\n                                          '\\nRidge Regression: ', ridge_lin_reg_error, \\\n                                           '\\nLasso Regression: ', lasso_lin_reg_error)","b341e1fc":"### Model Evaluations","acec4428":"Now let us split our data into a training, validation, and test set.\nThe training set will be used for both the linear regression and the ridge regularization version, yet the validation set will only be used in order to tune the lambda hyperparameter of the ridge and lasso regularization. \n\nAfter we have both models, we will evaluate each with the test set and compare.","39c3fa38":"### Linear Regression Excersize\nFor Freq ML class at the Cooper Union.\n\nBy: Guy Bar Yosef\n\nIn this kernel we will run a linear regression on a dataset provided by the World Health Organization (WHO) to predict the life expectancy of a country given various informaiton about diseases and the like.\n\nMore specifically, we will do plain Least Squares Regression, Ridge Regression, and Lasso Regression(the first two of which will be implemented by hand) and compare our results between the three","fbd49aef":"Before we run any regression, let us clean up our data:\n1. We will drop the 'Status' feature, as it is a categorical data with only 2 possibilities and when we incorporated it with one-hot encoding we ended up with very large weights, resulting in a large variance in our results.\n2. We will use an imputer to deal with missing values such as NaN.","aa967814":"### Plain Linear Regression through Least Squares\n\nLet us now add a bias column to our input matrix, which will be used in our calculations. Note that this isn't part of our inputs, it is just used to simplify our calculations. Later when we do the lasso regression, we will get rid of it as we are using a package and not implementing it ourselves.","97c95f3d":"Let us now write out a function that will automate the cross-validation needed to find a good lambda, or penalty parameter.","596a475c":"Having run this a few times (each time the training, validation, and test data being split differently) we have seen different regressors achieving the lowest mean squared error. With that said, all three preform very similarily overall. We found, on average, the mean squared error of all three hover around 16.5, meaning that on average we are 4ish years off in our life expectancy predictions.\n\nIntrestingly, ridge and lasso regularization have not managed to produce consistently superior results to the plain least squares regression. This could be due to a flaw with the data; having multiple years for each country results in much of our input entries looking very similar. This could of course also be a fault of the ridge and lasso implementation, and as such a good follow up will be to use the scikit-learn implementations of plain least squares and ridge, and compare our results.","53b6af90":"### Least Squares with Lasso Regression\n\nNote that we are using a pacakge here. However we will find a new penalty coefficient, with an altered cross-validation function (due to the hand-implementation for the ridge regression and the package implementation of the lasso regression)","e3ad1834":"#### Lasso Plot\n\nLets see which features the lasso regression decided to keep with a lasso plot (very much taken from http:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py):\n","5d42ab28":"### Linear Regression with Ridge Regularization","da6a2b1c":"### Data Processing","1f49c52e":"Before we begin, we will normalize our data (train, validation, and test) and add the bias to it."}}