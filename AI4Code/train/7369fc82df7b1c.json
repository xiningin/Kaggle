{"cell_type":{"faa686c2":"code","865dd434":"code","956d95b3":"code","7fc24bc3":"code","06f9e26a":"code","5a9e3a32":"code","67f5861b":"code","586777fe":"code","904ddb4b":"code","57db9ef5":"code","08ababce":"code","faae16ae":"code","8badba69":"code","516afa42":"code","982613bf":"code","0634aff9":"code","b38b87fc":"code","72a9d5dd":"code","de4e1bad":"code","b7cd00be":"code","91e17667":"code","9efe7501":"markdown","8f57eb78":"markdown"},"source":{"faa686c2":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))","865dd434":"# load data\ntrain_df = pd.read_csv('..\/input\/fashion-mnist_train.csv')\ntest_df = pd.read_csv('..\/input\/fashion-mnist_test.csv')\nprint(train_df.shape, test_df.shape)","956d95b3":"# create dict for the class labels\nindex_to_class = {0:'T-shirt\/top',\n                  1:'Trouser',\n                  2:'Pullover',\n                  3:'Dress',\n                  4:'Coat',\n                  5:'Sandal',\n                  6:'Shirt',\n                  7:'Sneaker',\n                  8:'Bag',\n                  9:'Ankle boot'}","7fc24bc3":"# create arrays from dataframes\ntrain_X = train_df.drop(['label'], axis=1).values\ntrain_Y = train_df['label'].values\ntest_X = test_df.drop(['label'], axis=1).values\ntest_Y = test_df['label'].values\nprint(train_X.shape, train_Y.shape, test_X.shape, test_Y.shape)","06f9e26a":"import matplotlib.pyplot as plt\n\n# look at some of the pics from train_X\nplt.figure(figsize=(15,10))\nfor i in range(40):  \n    plt.subplot(5, 8, i+1)\n    plt.imshow(train_X[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"label=%s\" % index_to_class[train_Y[i]],y=1)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","5a9e3a32":"# prepare the data for CNN\n\n# reshape flattened data into 3D tensor\nn_x = 28\ntrain_X_pic = train_X.reshape((-1, n_x, n_x, 1))  \ntest_X_pic = test_X.reshape((-1, n_x, n_x, 1))    # similarly for test set\nprint(train_X_pic.shape, test_X_pic.shape)\n\n# standardize the values in the datasets by dividing by 255\ntrain_X_pic = train_X_pic \/ 255.\ntest_X_pic = test_X_pic \/ 255.","67f5861b":"# one-hot encode the labels in train_Y\nfrom keras.utils.np_utils import to_categorical\ntrain_labels = to_categorical(train_Y)\nprint(train_labels.shape)\nprint(train_Y[181], index_to_class[train_Y[181]], train_labels[181])\nplt.figure(figsize=(1,1))\nplt.imshow(train_X[181].reshape((28,28)),cmap=plt.cm.binary)\nplt.show()","586777fe":"# one-hot encode the labels in test_Y\ntest_labels = to_categorical(test_Y)\nprint(test_labels.shape)\nprint(test_Y[181], index_to_class[test_Y[181]], test_labels[181])\nplt.figure(figsize=(1,1))\nplt.imshow(test_X[181].reshape((28,28)),cmap=plt.cm.binary)\nplt.show()","904ddb4b":"# use Keras data generator to augment the training set\n\nfrom keras_preprocessing.image import ImageDataGenerator\ndata_augment = ImageDataGenerator(rotation_range=10,\n                                 width_shift_range=0.1,\n                                 height_shift_range=0.1,\n                                 zoom_range=0.1)","57db9ef5":"# build the CNN from keras\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28, 1)))\nmodel.add(layers.Conv2D(32, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2), strides=2))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Conv2D(64, kernel_size=5, padding='same', activation='relu'))\nmodel.add(layers.Conv2D(64, kernel_size=5, padding='valid', activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2), strides=2))\nmodel.add(layers.Dropout(rate=0.4))\nmodel.add(layers.Conv2D(128, kernel_size=3, padding='same', activation='relu'))\nmodel.add(layers.Conv2D(128, kernel_size=3, padding='valid', activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))\n\nmodel.summary()","08ababce":"# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', \n              metrics=['accuracy'])","faae16ae":"# set up a dev set (5000 samples) to check the performance of the CNN\nX_dev = train_X_pic[:5000]\nrem_X_train = train_X_pic[5000:]\nprint(X_dev.shape, rem_X_train.shape)\n\nY_dev = train_labels[:5000]\nrem_Y_train = train_labels[5000:]\nprint(Y_dev.shape, rem_Y_train.shape)","8badba69":"# Train and validate the model\nepochs = 100\nbatch_size = 128\nhistory = model.fit_generator(data_augment.flow(rem_X_train, rem_Y_train, batch_size=batch_size), \n                              epochs=epochs, steps_per_epoch=rem_X_train.shape[0]\/\/batch_size, \n                              validation_data=(X_dev, Y_dev))","516afa42":"# plot and visualise the training and validation losses\nloss = history.history['loss']\ndev_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\nfrom matplotlib import pyplot as plt\nplt.plot(epochs, loss, 'bo', label='training loss')\nplt.plot(epochs, dev_loss, 'b', label='validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","982613bf":"# do error analysis on the predictions for X_dev\npred_dev = model.predict(X_dev)\npred_dev_labels = []\nfor i in range(5000):\n    pred_dev_label = np.argmax(pred_dev[i])\n    pred_dev_labels.append(pred_dev_label)","0634aff9":"# look at those that were classified wrongly in X_dev\nresult = pd.DataFrame(train_Y[:5000], columns=['Y_dev'])\nresult['Y_pred'] = pred_dev_labels\nresult['correct'] = result['Y_dev'] - result['Y_pred']\nerrors = result[result['correct'] != 0]\nerror_list = errors.index\nprint('Number of errors is ', len(errors))\nprint('The indices are ', error_list)","b38b87fc":"# plot the image of the wrong in predictions for X_dev\nplt.figure(figsize=(15,90))\nfor i in range(len(error_list)):\n    plt.subplot(43, 8, i+1)\n    plt.imshow(X_dev[error_list[i]].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"true={}\\npredict={}\".format(index_to_class[train_Y[error_list[i]]], \n                                           index_to_class[pred_dev_labels[error_list[i]]]), y=1)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","72a9d5dd":"# predict on test set\npredictions = model.predict(test_X_pic)\nprint(predictions.shape)","de4e1bad":"model.evaluate(x=test_X_pic, y=test_labels)","b7cd00be":"# set the predicted labels to be the one with the highest probability\npredicted_labels = []\nfor i in range(10000):\n    predicted_label = np.argmax(predictions[i])\n    predicted_labels.append(predicted_label)","91e17667":"# look at some of the predictions for test_X\nplt.figure(figsize=(15,12))\nfor i in range(40):  \n    plt.subplot(5, 8, i+1)\n    plt.imshow(test_X[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.title(\"true={}\\npredict={}\".format(index_to_class[test_Y[i]], \n                                           index_to_class[predicted_labels[i]]), y=1)\n    plt.axis('off')\nplt.subplots_adjust(wspace=0.3, hspace=-0.1)\nplt.show()","9efe7501":"Looking at those that were predicted wrongly, I see that there are quite many difficult and perhaps ambiguous ones that I think a person may also classify incorrectly. If the validation set is also representative of those in the test set (10,000), I would think that an accuracy above 97% could be considered very good. Just commenting without much basis.","8f57eb78":"### I used the codes for my \"Digit Recognizer\" kernel, adapted it and applied to the Fashion MNIST. I got 93% accuracy on the test dataset."}}