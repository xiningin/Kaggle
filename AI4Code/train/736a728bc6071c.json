{"cell_type":{"fb0fd4cf":"code","45af2701":"code","b06a5359":"code","e75e953c":"code","41911bc0":"code","4d1d5e87":"code","20caca1d":"code","b5848d99":"code","2bbb81b0":"code","65fdeb63":"code","dced6696":"code","ba619e19":"code","877cc022":"code","e3d72224":"code","5b4ce437":"code","1efeb6cf":"code","a30b9db9":"code","b262c691":"code","4d8a188e":"code","801982a5":"code","d9cd2da8":"code","3c64e0e7":"code","6a11d74a":"code","9a266bfd":"markdown","acda63e5":"markdown","c306c5ba":"markdown","0640615c":"markdown","6ef30121":"markdown"},"source":{"fb0fd4cf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","45af2701":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","b06a5359":"data = pd.read_csv(\"..\/input\/europarl-parallel-corpus-19962011\/english_german.csv\")\ndata = data.dropna()\ndata.head()","e75e953c":"# Append <START> and <END> to each english sentence\nSTART = 'ssss '\nEND = ' eeee'\n\ndata['English'] = data['English'].apply(lambda x: START+x+END)\ndata.head()","41911bc0":"eng_text = data['English'].tolist()\ndeu_text = data['German'].tolist()\n\nprint(eng_text[5])\nprint(deu_text[5])","4d1d5e87":"num_words = 10000\nclass TokenizerWrap(Tokenizer):\n    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n    \n    def __init__(self, texts, padding,\n                 reverse=False, num_words=None):\n        \"\"\"\n        :param texts: List of strings. This is the data-set.\n        :param padding: Either 'post' or 'pre' padding.\n        :param reverse: Boolean whether to reverse token-lists.\n        :param num_words: Max number of words to use.\n        \"\"\"\n\n        Tokenizer.__init__(self, num_words=num_words)\n\n        # Create the vocabulary from the texts.\n        self.fit_on_texts(texts)\n\n        # Create inverse lookup from integer-tokens to words.\n        self.index_to_word = dict(zip(self.word_index.values(),\n                                      self.word_index.keys()))\n\n        # Convert all texts to lists of integer-tokens.\n        # Note that the sequences may have different lengths.\n        self.tokens = self.texts_to_sequences(texts)\n\n        if reverse:\n            # Reverse the token-sequences.\n            self.tokens = [list(reversed(x)) for x in self.tokens]\n        \n            # Sequences that are too long should now be truncated\n            # at the beginning, which corresponds to the end of\n            # the original sequences.\n            truncating = 'pre'\n        else:\n            # Sequences that are too long should be truncated\n            # at the end.\n            truncating = 'post'\n\n        # The number of integer-tokens in each sequence.\n        self.num_tokens = [len(x) for x in self.tokens]\n\n        # Max number of tokens to use in all sequences.\n        # We will pad \/ truncate all sequences to this length.\n        # This is a compromise so we save a lot of memory and\n        # only have to truncate maybe 5% of all the sequences.\n        self.max_tokens = np.mean(self.num_tokens) \\\n                          + 2 * np.std(self.num_tokens)\n        self.max_tokens = int(self.max_tokens)\n\n        # Pad \/ truncate all token-sequences to the given length.\n        # This creates a 2-dim numpy matrix that is easier to use.\n        self.tokens_padded = pad_sequences(self.tokens,\n                                           maxlen=self.max_tokens,\n                                           padding=padding,\n                                           truncating=truncating)\n\n    def token_to_word(self, token):\n        \"\"\"Lookup a single word from an integer-token.\"\"\"\n\n        word = \" \" if token == 0 else self.index_to_word[token]\n        return word \n\n    def tokens_to_string(self, tokens):\n        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n\n        # Create a list of the individual words.\n        words = [self.index_to_word[token]\n                 for token in tokens\n                 if token != 0]\n        \n        # Concatenate the words to a single string\n        # with space between all the words.\n        text = \" \".join(words)\n\n        return text\n    \n    def text_to_tokens(self, text, reverse=False, padding=False):\n        \"\"\"\n        Convert a single text-string to tokens with optional\n        reversal and padding.\n        \"\"\"\n\n        # Convert to tokens. Note that we assume there is only\n        # a single text-string so we wrap it in a list.\n        tokens = self.texts_to_sequences([text])\n        tokens = np.array(tokens)\n\n        if reverse:\n            # Reverse the tokens.\n            tokens = np.flip(tokens, axis=1)\n\n            # Sequences that are too long should now be truncated\n            # at the beginning, which corresponds to the end of\n            # the original sequences.\n            truncating = 'pre'\n        else:\n            # Sequences that are too long should be truncated\n            # at the end.\n            truncating = 'post'\n\n        if padding:\n            # Pad and truncate sequences to the given length.\n            tokens = pad_sequences(tokens,\n                                   maxlen=self.max_tokens,\n                                   padding='pre',\n                                   truncating=truncating)\n\n        return tokens","20caca1d":"%%time\ntokenizer_src = TokenizerWrap(texts=deu_text,\n                              padding='pre',\n                              reverse=True,\n                              num_words=num_words\n                             )","b5848d99":"%%time\ntokenizer_des = TokenizerWrap(texts=eng_text,\n                              padding='post',\n                              reverse=False,\n                              num_words=num_words\n                             )","2bbb81b0":"token_src = tokenizer_src.tokens_padded\ntoken_des = tokenizer_des.tokens_padded\n\nprint(token_src.shape)\nprint(token_des.shape)","65fdeb63":"token_start = tokenizer_des.word_index[START.strip()]\ntoken_end = tokenizer_des.word_index[END.strip()]\n\nprint(token_start)\nprint(token_end)","dced6696":"encoder_inp_data = token_src\ndecoder_inp_data = token_des[:, :-1]\ndecoder_out_data = token_des[:, 1:]","ba619e19":"# Glue all the encoder components together\ndef connect_encoder():\n    net = encoder_input\n    \n    net = encoder_emb(net)\n    net = encoder_gru1(net)\n    net = encoder_gru2(net)\n    out = encoder_gru3(net)\n    \n    return out","877cc022":"def connect_decoder(initial_state):    \n    # Start the decoder-network with its input-layer.\n    net = decoder_input\n\n    # Connect the embedding-layer.\n    net = decoder_emb(net)\n    \n    # Connect all the GRU-layers.\n    net = decoder_gru1(net, initial_state=initial_state)\n    net = decoder_gru2(net, initial_state=initial_state)\n    net = decoder_gru3(net, initial_state=initial_state)\n\n    # Connect the final dense layer that converts to\n    # one-hot encoded arrays.\n    decoder_output = decoder_dense(net)\n    \n    return decoder_output","e3d72224":"# Connect all the models\nwith strategy.scope():\n    \n    embedding_size = 128\n    state_size = 512\n\n    encoder_input = Input(shape=(None,), name='encoder_input')\n    encoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='encoder_embedding')\n\n    encoder_gru1 = GRU(state_size, name='enc_gru1', return_sequences=True)\n    encoder_gru2 = GRU(state_size, name='enc_gru2', return_sequences=True)\n    encoder_gru3 = GRU(state_size, name='enc_gru3', return_sequences=False)\n    \n    encoder_op = connect_encoder()\n    \n    # Initial state placeholder takes a \"thought vector\" produced by the GRUs\n    # That's why it needs the inputs with \"state_size\" (which was used in GRU size)\n    decoder_initial_state = Input(shape=(state_size,), name='decoder_init_state')\n\n    # Decoder also needs an input, which is the basic input setence of the destination language\n    decoder_input = Input(shape=(None,), name='decoder_input')\n\n    # Have the decoder embedding\n    decoder_emb = Embedding(input_dim=num_words, output_dim=embedding_size, name='decoder_embedding')\n\n    # GRU arch similar to Encoder one with small changes\n    decoder_gru1 = GRU(state_size, name='dec_gru1', return_sequences=True)\n    decoder_gru2 = GRU(state_size, name='dec_gru2', return_sequences=True)\n    decoder_gru3 = GRU(state_size, name='dec_gru3', return_sequences=True)\n\n    # Final dense layer for prediction\n    decoder_dense = Dense(num_words, activation='softmax', name='decoder_output')\n    decoder_op = connect_decoder(encoder_op)\n    model_train = Model(inputs=[encoder_input, decoder_input],\n                        outputs=[decoder_op])\n    model_train.compile(optimizer=RMSprop(lr=1e-3),\n                        loss='sparse_categorical_crossentropy')","5b4ce437":"tf.keras.utils.plot_model(model_train)","1efeb6cf":"path_checkpoint = '21_checkpoint.keras'\ncallback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n                                      monitor='val_loss',\n                                      verbose=1,\n                                      save_weights_only=True,\n                                      save_best_only=True)","a30b9db9":"callback_early_stopping = EarlyStopping(monitor='val_loss',\n                                        patience=3, verbose=1)","b262c691":"callbacks = [callback_early_stopping,\n             callback_checkpoint]","4d8a188e":"try:\n    model_train.load_weights(path_checkpoint)\nexcept Exception as error:\n    print(\"Error trying to load checkpoint.\")\n    print(error)","801982a5":"x_data = {\n    \"encoder_input\": encoder_inp_data,\n    \"decoder_input\": decoder_inp_data\n}\n\ny_data = {\n    \"decoder_output\": decoder_out_data\n}","d9cd2da8":"validation_split = 10000 \/ len(encoder_inp_data)\nprint(f\"Validation Split: {validation_split:.4f}%\")","3c64e0e7":"# Train the model\nwith strategy.scope():\n    model_train.fit(\n        x=x_data,\n        y=y_data,\n        batch_size=384,\n        epochs=10,\n        validation_split=validation_split,\n        callbacks=callbacks\n    )","6a11d74a":"model_train.save(\"eng_to_deu.hdf5\")","9a266bfd":"### Encoder Model","acda63e5":"<p style=\"color:grey\">This work was largely inspired by Hvass Labs work on the same (Thank you!)<\/p>","c306c5ba":"### Decoder Model","0640615c":"## Model","6ef30121":"<p style=\"color:red\">If you like my work, please upvote!<\/p>"}}