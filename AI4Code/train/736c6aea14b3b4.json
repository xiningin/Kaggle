{"cell_type":{"70fb6ee6":"code","697d4d4f":"code","a13d368a":"code","c2ff431c":"code","136a8d55":"code","59693cdc":"code","222954de":"code","efb601b5":"code","a6bc3194":"code","74261785":"code","f05bd853":"code","54f2562c":"code","997995a0":"code","c8b33699":"code","660b66cc":"code","df24edc0":"code","234eeb91":"code","e300873d":"code","c8e66c9d":"code","2379910c":"code","a902cee1":"code","b790094c":"code","29681c7d":"code","30f99746":"code","c9e89718":"code","d445c7b6":"code","0ede5446":"code","73bfd4ec":"code","3e4c4e90":"code","204b7ef7":"code","981577a0":"code","29f086c2":"code","442be4fc":"code","332fcba2":"code","448296d3":"code","9da5a95d":"code","0d701d80":"code","5140cb70":"code","0a33440c":"code","a56ed57f":"code","b5b134e6":"code","64282973":"markdown","0b1c630f":"markdown","b38ec0ab":"markdown","b676843e":"markdown","321b5f44":"markdown","0f044740":"markdown","ffff10ec":"markdown","9f9e41c4":"markdown","0f8d9abb":"markdown","fa01efdd":"markdown","952a9ca6":"markdown","e15f984a":"markdown","e2af06a8":"markdown","2ffaa167":"markdown","88eedaa4":"markdown","b326ddd9":"markdown"},"source":{"70fb6ee6":"import re # regex\nimport pandas as pd # tables\nimport matplotlib.pyplot as plt # plots\nimport seaborn as sns # plots\nimport numpy as np # operations with arrays and matrices ","697d4d4f":"# reading the dataset \ntrain = pd.read_csv('\/kaggle\/input\/emotions-dataset-for-nlp\/train.txt', header=None, sep=';', names=['Lines','Emotions'], encoding='utf-8')\ntest = pd.read_csv('\/kaggle\/input\/emotions-dataset-for-nlp\/test.txt', header=None, sep =';', names=['Lines','Emotions'], encoding='utf-8')\nvalidation = pd.read_csv('\/kaggle\/input\/emotions-dataset-for-nlp\/val.txt', header=None, sep=';', names=['Lines','Emotions'], encoding='utf-8')","a13d368a":"# adding a column with encoded emotions\nemotions_to_labels = {'anger': 0, 'love': 1, 'fear': 2, 'joy': 3, 'sadness': 4,'surprise': 5}\nlabels_to_emotions = {j:i for i,j in emotions_to_labels.items()}\n\ntrain['Labels'] = train['Emotions'].replace(emotions_to_labels)\ntest['Labels'] = test['Emotions'].replace(emotions_to_labels)\nvalidation['Labels'] = validation['Emotions'].replace(emotions_to_labels)","c2ff431c":"train.head()","136a8d55":"def visualize_labels_distribution(df, title='the'):\n  '''\n  Accepts a dataframe with 'Emotions' column and dataset title (e.g. 'train')\n  Creates bar chart with num of elements of each category\n  Returns nothing\n\n  '''\n  # create a pandas series with labels and their counts\n  num_labels = df['Emotions'].value_counts()\n\n  # num of unique categories\n  x_barchart = range(df['Emotions'].nunique())\n  # list of labels\n  x_barchart_labels = [str(emotions_to_labels[emotion]) +\\\n                       ' - ' + emotion for emotion in list(num_labels.index)]\n\n  # list of counts\n  y_barchart = list(num_labels.values)\n\n  # creating bar chart\n  plt.figure(figsize = (5, 4))\n  plt.bar(x_barchart, y_barchart, color='#707bfb')\n\n  # adding num of elements for each category on plot as text\n  for index, data in enumerate(y_barchart):\n    plt.text(x = index, \n            y = data+max(y_barchart)\/100, \n            s = '{}'.format(data), \n            fontdict = dict(fontsize=10), \n            ha = 'center',)\n  \n  plt.xticks(x_barchart, x_barchart_labels, rotation=40)\n  plt.title('Num of elements of each category for {} dataset'.format(title))\n  plt.tight_layout()\n\n  print('There are {} records in the dataset.\\n'.format(len(df.index)))\n\n  plt.show()","59693cdc":"visualize_labels_distribution(train, 'train')\nvisualize_labels_distribution(test, 'test')\nvisualize_labels_distribution(validation, 'val')","222954de":"import nltk \nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# downloading a set of stop-words\nSTOPWORDS = set(stopwords.words('english'))\n\n# tokenizer\nfrom nltk.tokenize import word_tokenize","efb601b5":"def text_preprocess(text, stop_words=False):\n  '''\n  Accepts text (a single string) and\n  a parameters of preprocessing\n  Returns preprocessed text\n\n  '''\n  # clean text from non-words\n  text = re.sub(r'\\W+', ' ', text).lower()\n\n  # tokenize the text\n  tokens = word_tokenize(text)\n\n  if stop_words:\n    # delete stop_words\n    tokens = [token for token in tokens if token not in STOPWORDS]\n\n  return tokens","a6bc3194":"print('Before: ')\nprint(train.head())\n\nx_train = [text_preprocess(t, stop_words=True) for t in train['Lines']]\ny_train = train['Labels'].values\n\nprint('\\nAfter:')\nfor line_and_label in list(zip(x_train[:5], y_train[:5])):\n  print(line_and_label)","74261785":"x_test = [text_preprocess(t, stop_words=True) for t in test['Lines']]\ny_test = test['Labels'].values\n\nx_validation = [text_preprocess(t, stop_words=True) for t in validation['Lines']]\ny_validation = validation['Labels'].values","f05bd853":"# load pre-trained model\nimport gensim.downloader as api\nmodel_wiki = api.load('fasttext-wiki-news-subwords-300')","54f2562c":"from gensim.models import Word2Vec\n\n# train word2vec model on the corpus\nmodel_w2v = Word2Vec(x_train + x_test + x_validation,   # data for model to train on\n                 vector_size = 300,                            # embedding vector size\n                 min_count = 2).wv  ","997995a0":"from tensorflow.keras.preprocessing.text import Tokenizer","c8b33699":"DICT_SIZE = 15000\n\n# creating a dictionary with most used words\n# where num of words = DICT_SIZE\ntokenizer = Tokenizer(num_words=DICT_SIZE)\ntotal = x_train + x_train + x_validation\ntokenizer.fit_on_texts(total)","660b66cc":"# words and their indexes\nlist(tokenizer.word_index.items())[:5]","df24edc0":"# number of unique words in the corpus\nlen(tokenizer.word_index)","234eeb91":"# find max length of sentences across all parts of the dataset\nx_train_max_len = max([len(i) for i in x_train])\nx_test_max_len = max([len(i) for i in x_test])\nx_validation_max_len = max([len(i) for i in x_validation])\n\nMAX_LEN = max(x_train_max_len, x_test_max_len, x_validation_max_len)","e300873d":"# max length across all sentences\nMAX_LEN","c8e66c9d":"from tensorflow.keras.preprocessing.sequence import pad_sequences","2379910c":"# replace words with their indexes, \n# change size of vectors to MAX_LEN and pad indexes \nX_train = tokenizer.texts_to_sequences(x_train)\nX_train_pad = pad_sequences(X_train, maxlen=MAX_LEN)\n\nX_test = tokenizer.texts_to_sequences(x_test)\nX_test_pad = pad_sequences(X_test, maxlen=MAX_LEN)\n\nX_val = tokenizer.texts_to_sequences(x_validation)\nX_val_pad = pad_sequences(X_val, maxlen=MAX_LEN)","a902cee1":"# words are replaced by their indexes\nX_train[0]","b790094c":"# sentence after padding\nX_train_pad[0]","29681c7d":"def create_weight_matrix(model, second_model=False):\n  '''\n  Accepts word embedding model\n  and the second model, if provided\n  Returns weight matrix of size m*n, where\n  m - size of the dictionary\n  n - size of the word embedding vector\n\n  '''\n  vector_size = model.get_vector('like').shape[0]\n  w_matrix = np.zeros((DICT_SIZE, vector_size))\n  skipped_words = []\n\n  for word, index in tokenizer.word_index.items():\n    if index < DICT_SIZE:\n      if word in model.key_to_index: \n        w_matrix[index] = model.get_vector(word)\n      else:\n        if second_model:\n          if word in second_model.key_to_index:\n            w_matrix[index] = second_model.get_vector(word)\n          else:\n            skipped_words.append(word)\n        else:\n          skipped_words.append(word)\n \n  print(f'{len(skipped_words)} words were skipped. Some of them:')\n  print(skipped_words[:50])\n  return w_matrix","30f99746":"weight_matrix = create_weight_matrix(model_wiki, model_w2v)","c9e89718":"weight_matrix.shape","d445c7b6":"# import models, layers, optimizers from tensorflow\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam","0ede5446":"# import and initialize early stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\nstop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)","73bfd4ec":"# initialize sequential model\nmodel = Sequential()\nmodel.add(Embedding(input_dim = DICT_SIZE, # the whole vocabulary size\n                    output_dim = weight_matrix.shape[1], # vector space dimension\n                    input_length = X_train_pad.shape[1], # max_len of text sequence\n                    weights=[weight_matrix], # assign the embedding weight with embedding marix\n                    trainable=False)) # set the weight to be not trainable (static)","3e4c4e90":"model.add(Bidirectional(LSTM(128, return_sequences=True))) \nmodel.add(Dropout(0.2))\nmodel.add(Bidirectional(LSTM(256, return_sequences=True)))\nmodel.add(Dropout(0.2))\nmodel.add(Bidirectional(LSTM(128, return_sequences=False)))\nmodel.add(Dense(6, activation = 'softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy')","204b7ef7":"history = model.fit(X_train_pad, y_train, \n                    validation_data = (X_val_pad, y_validation),\n                    batch_size = 8,\n                    epochs = 20, \n                    callbacks = stop)","981577a0":"def plot_history(history):\n    '''\n    Plots training and validation accuracy and loss\n    Accepts a single param - history, where\n    history - keras.callbacks.History object\n    Returns nothing\n    \n    '''\n    loss = history.history['loss']\n    accuracy = history.history['accuracy']\n    val_loss = history.history['val_loss']\n    val_accuracy = history.history['val_accuracy']\n    x = range(1, len(loss) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, accuracy, label='Training acc', color='#707bfb')\n    plt.plot(x, val_accuracy, label='Validation acc', color='#fbcbff')\n    plt.title('Training and validation accuracy')\n    plt.grid(True)\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, label='Training loss', color='#707bfb')\n    plt.plot(x, val_loss, label='Validation loss', color='#fbcbff')\n    plt.title('Training and validation loss')\n    plt.grid(True)\n    plt.legend()","29f086c2":"plot_history(history)","442be4fc":"model.evaluate(X_test_pad, y_test) ","332fcba2":"y_pred = np.argmax(model.predict(X_test_pad), axis=1)","448296d3":"from sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred))","9da5a95d":"# setting a custom colormap\nfrom matplotlib.colors import LinearSegmentedColormap\ncolors = ['#ffffff', '#fbcbff', '#707bfb']\ncmap = LinearSegmentedColormap.from_list('mycmap', colors)","0d701d80":"def plot_confusion_matrix(matrix, fmt=''):\n  '''\n  Accepts a confusion matrix and a format param\n  Plots the matrix as a heatmap\n  Returns nothing\n\n  '''\n  plt.figure(figsize=(6, 5))\n  sns.heatmap(matrix, annot=True, \n              cmap=cmap, \n              fmt=fmt, \n              xticklabels=emotions_to_labels.keys(), \n              yticklabels=emotions_to_labels.keys())\n  plt.ylabel('True labels')\n  plt.xlabel('Predicted labels')\n  plt.show()","5140cb70":"matrix = metrics.confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(matrix)","0a33440c":"# create new confusion matrix\n# where values are normed by row\nmatrix_new = np.zeros(matrix.shape)\n\nfor row in range(len(matrix)):\n  sum = np.sum(matrix[row])\n  for element in range(len(matrix[row])):\n    matrix_new[row][element] = matrix[row][element] \/ sum\n\nplot_confusion_matrix(matrix_new, fmt='.2')","a56ed57f":"def predict(texts):\n  '''\n  Accepts array if texts (strings)\n  Prints sentence and the corresponding label (emotion)\n  Returns nothing\n  \n  '''\n  texts_prepr = [text_preprocess(t) for t in texts]\n  sequences = tokenizer.texts_to_sequences(texts_prepr)\n  pad = pad_sequences(sequences, maxlen=MAX_LEN)\n\n  predictions = model.predict(pad)\n  labels = np.argmax(predictions, axis=1)\n  \n  for i, lbl in enumerate(labels):\n    print(f'\\'{texts[i]}\\' --> {labels_to_emotions[lbl]}')\n","b5b134e6":"test_texts = ['I am so happy', 'The man felt lonely', 'The guests felt satisfied']\n\npredict(test_texts)","64282973":"Accuracy of the model on test data:","0b1c630f":"## Text preprocessing","b38ec0ab":"## Test model on custom data","b676843e":"Some words may be specific for this dataset, so I use a combination of pre-trained fastText model and a Word2vec model, trained on the corpus of data.\n\nOccurence of words in the corpus must be >=2, leftovers are probably typos or terms that don`t have a semantic meaning.","321b5f44":"## Word embeddings","0f044740":"Training, validation accuracy and loss plots:","ffff10ec":"## DL model","9f9e41c4":"Import basic libraries.","0f8d9abb":"Precision, recall, F1-score on test data:","fa01efdd":"## Evaluate model","952a9ca6":"Text in the dataset seems to be quite 'clean', so it doesn`t require many preprocessing steps, only stop-word deletion and tokenization.","e15f984a":"## Load & explore data","e2af06a8":"## Converting sentences into vectors","2ffaa167":"Confusion matrix:","88eedaa4":"## Embedding matrix","b326ddd9":"Bar chart of the distribution of records among categories:"}}