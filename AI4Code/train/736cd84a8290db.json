{"cell_type":{"7502e7aa":"code","6da473b7":"code","1399410e":"code","13e71e33":"code","c173ab15":"code","542466e2":"code","1bf2c3f8":"code","b97564a6":"code","540192a1":"code","1b1fa99a":"markdown","9f8e79a2":"markdown","ab606f6d":"markdown","5c61fe2d":"markdown","ecf57745":"markdown","9f8ae45c":"markdown","e199a535":"markdown","74300804":"markdown","7641f8c5":"markdown","f85b292c":"markdown","878af004":"markdown"},"source":{"7502e7aa":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")","6da473b7":"X_train_original = train[[\"0\", \"1\", \"2\", \"3\", \"4\"]]\ny_train_original = train[\"target\"]","1399410e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_original, y_train_original, test_size=0.2)\nprint(X_train.shape, X_test.shape)","13e71e33":"from sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(n_neighbors=2)\nclf.fit(X_train, y_train)","c173ab15":"from sklearn.metrics import log_loss\n\nprobas = clf.predict_proba(X_train)\nll = log_loss(y_train, probas)\n\nprint(\"Log loss: {0}\".format(ll))","542466e2":"probas = clf.predict_proba(X_test)\nll = log_loss(y_test, probas)\n\nprint(\"Log loss: {0}\".format(ll))","1bf2c3f8":"for n in range(5):\n    print(\"=\"*40)\n    print(\"Neighbours: {0}\".format(n+1))\n    \n    clf = KNeighborsClassifier(n_neighbors=n+1)\n    clf.fit(X_train, y_train)\n\n    probas = clf.predict_proba(X_train)\n    ll = log_loss(y_train, probas)\n    print(\"Train log loss: {0}\".format(ll))\n    \n    probas = clf.predict_proba(X_test)\n    ll = log_loss(y_test, probas)\n    print(\"Test log loss: {0}\".format(ll))","b97564a6":"clf = KNeighborsClassifier(n_neighbors=5)\nclf.fit(X_train_original, y_train_original)\ntest_probas = clf.predict_proba(test[[\"0\", \"1\", \"2\", \"3\", \"4\"]])","540192a1":"sample_submission[\"target\"] = test_probas[:,1]\nsample_submission.to_csv(\"knn_baseline.csv\",index=False)","1b1fa99a":"Let's see how it performs on our train set using the competition evaluation metric (log loss).","9f8e79a2":"By adjusting the hyperparameters in our model, we can get the train and test set to *agree* more.\n\nOnce we're happy with the result, we can use the final model to train on our entire (original) training set and make our predictions on the test set.","ab606f6d":"## Train and test split\nSince machine learning models are built to make predictions of future events, it's critical to evaluate their performance on **unseen** data. Unseen data just refers to data that the model hasn't been trained on (it hasn't seen it before). This is commonly done by splitting your original training dataset into two smaller datasets: a train and test set. Then we train the model on the train set, and test the model on the test set. This is easily done with the module \"train_test_split\" from SKLearn.\n\nLet's start by separating our independent (input) and dependent (target) variables.","5c61fe2d":"Let's now make a simple k-nearest neighbour classifier and train it on the training set.","ecf57745":"Then we take our predicted probabilities for class 1 and use as our submission.","9f8ae45c":"That's a much better log loss than the logistic regression baseline - but let's see how it performs on unseen data; our test set.","e199a535":"Well, that's not what we saw on the training set. This is what is called \"overfitting\"; our model has learned how to predict our training set *too* well, so it's not able to generalize.\n\nTo address this problem, we can change the hyperparameters in our model, until we're satisfied with the performance on the test set.","74300804":"## Further notes\nWe just skimmed the surface of validation in machine learning. Validation is used for many different things and there are much more complicated validation systems. A big part of building a machine learning model is figuring out exactly how you can properly validate your model; making sure it performs as well as you think. This varies from problem to problem, but a commonly used (thorough) validation method is called **k-fold cross-validation**.\n\nIf you want to know a bit more about validation, a good place to start is just wikipedia: https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics).","7641f8c5":"## Overfitting","f85b292c":"Now we use the module from SKLearn to further split our entire training dataset into two smaller ones. Note that we can change the size of our test set.","878af004":"## Introduction\nWe'll briefly go over, how to set up a validation system for approaching a machine learning problem. If you want an introduction to Kaggle's online kernels, you can check the public \"EDA\"-kernel."}}