{"cell_type":{"7bddadf3":"code","20188931":"code","504a7ea5":"code","d99e67d5":"code","5ae8abff":"code","d489593e":"code","f9f47789":"code","085830ba":"code","069e1103":"code","ffc2c01b":"code","ffed63c9":"code","47f4ef3d":"code","7e2ad1f0":"code","20db5860":"code","12b512a6":"code","97932bc7":"code","2694fd89":"code","0e7024af":"code","fc440a4b":"code","c569b68d":"code","c8350485":"code","579c3bf1":"code","bcc0ed54":"code","3af47780":"code","7c265b01":"code","14dea00d":"code","c15097f0":"code","eea8e383":"code","8145f541":"code","5d050885":"code","3a817402":"code","7f905e10":"code","0a9fce5c":"markdown","98e3ece7":"markdown","82d0b729":"markdown","8a306c8d":"markdown","caff0f16":"markdown","2a6c85da":"markdown","1f74a601":"markdown","b284e65f":"markdown","6f8e12df":"markdown","d3924704":"markdown","b456ce22":"markdown","dd19da6f":"markdown","1c7d7375":"markdown","43d649c6":"markdown","22c1c7d5":"markdown","da083e41":"markdown","4b876c7e":"markdown","f822d3f3":"markdown","9dc9010f":"markdown","a28bd3d5":"markdown"},"source":{"7bddadf3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","20188931":"df_insurance = pd.read_csv('..\/input\/insurance\/insurance.csv')\ndf_insurance.head()","504a7ea5":"df_insurance.shape","d99e67d5":"df_insurance.describe()","5ae8abff":"df_insurance.info()","d489593e":"# check for null values in dataset\ndf_insurance.isnull().sum()","f9f47789":"# check for dupliacte row in dataset\ndf_insurance.duplicated().sum()","085830ba":"# treating duplictaes value of dataset\ndf_insurance.drop_duplicates(inplace = True)\ndf_insurance.duplicated().sum()","069e1103":"# We found via data that having numeric column has not differ values\n# To get the better understanding we can impute these columns\ncol = list(df_insurance.columns)\nfor i in col:\n    if df_insurance[i].value_counts().shape[0] < 10:\n        df_insurance[i] = df_insurance[i].astype(str)\ndf_insurance.info()","ffc2c01b":"# columns in dataset\n# columns in dataset\n# Univariate categorical analysis\n# check for the cols having categorical type\nnum_col = list(df_insurance._get_numeric_data().columns)\ncat_col = list(set(col)- set(num_col))\ncat_col","ffed63c9":"# create a func to plot graphs for univariate categorical analysis\ndef plot_cat(df,catColumns):\n    fig,axes = plt.subplots(2,2, figsize = (24,12), sharey = True)\n    plt.suptitle('Univariate Categorical Analysis',color ='brown',fontsize = 20,fontweight='bold')\n    index = 0\n    for i in range(2):\n        for j in range(2):\n            ax= sns.boxplot(data = df , x = catColumns[index],y = 'charges',ax = axes[i][j])\n            ax.title.set_text(f'Graph for {catColumns[index]}')\n            index = index+1\nplot_cat(df_insurance, cat_col)","47f4ef3d":"# create a func to plot graphs for univariate numeric analysis\nsns.pairplot(df_insurance)","7e2ad1f0":"#For categorical columns\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor colName in ['sex','smoker']:\n    le.fit(df_insurance[colName].drop_duplicates())\n    df_insurance[colName] = le.transform(df_insurance[colName])\n# for region field\nregions = pd.get_dummies(df_insurance['region'])\ndf_insurance = pd.concat([df_insurance,regions],axis = 1 )\ndf_insurance.head()","20db5860":"# lets drop extra col\ndf_insurance.drop(columns = ['region','northeast'], axis = 1, inplace = True)\ndf_insurance .head()","12b512a6":"sns.heatmap(df_insurance.corr(),annot = True , cmap = 'Greens')\ncorr_df = df_insurance.corr()\ncorr_df = corr_df.where(np.triu(np.ones(corr_df.shape), k =1).astype(np.bool)).unstack().reset_index()\ncorr_df = corr_df.sort_values(by = 0 ,ascending = False)\ncorr_df = corr_df[corr_df['level_0'] == 'charges']\ncorr_df.dropna(inplace = True)\ncorr_df.head()","97932bc7":"from sklearn.model_selection import train_test_split\ndf_train ,df_test = train_test_split(df_insurance,test_size = .70,random_state = 100)\nprint(df_train.shape)\nprint(df_test.shape)","2694fd89":"# Check for variables scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_train[['age','bmi']]=scaler.fit_transform(df_train[['age','bmi']])\ndf_train.head()","0e7024af":"y_train = df_train.pop('charges')\nX_train = df_train\nX_train.head()","fc440a4b":"# add a constant\nimport statsmodels.api as sm\nX_train_sm = sm.add_constant(X_train)\nX_train_sm.head()","c569b68d":"# create a model\nlr = sm.OLS(y_train,X_train_sm.astype(float))\nlr_model = lr.fit()\nprint(lr_model.params)\nprint(lr_model.summary())\np_values = pd.DataFrame()\np_values['Features'] = X_train_sm.columns\np_values['Pvalue'] = [round(lr_model.pvalues[i],2) for i in X_train_sm.columns]","c8350485":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif_func(X_train_sm):\n    vif = pd.DataFrame()\n    vif['Features'] = X_train_sm.columns\n    vif['VIF'] = [variance_inflation_factor(X_train_sm.astype(float).values,i) for i in range(X_train_sm.astype(float).shape[1])]\n    vif['VIF'] = round(vif['VIF'],2)\n    vif = vif.sort_values(by = 'VIF',ascending = False)\n    return vif\n#call vif function\nvif = vif_func(X_train_sm)","579c3bf1":"# concat p_value and VIF dataframe\np_vif_df = pd.DataFrame()\np_vif_df = vif.merge(p_values , how = 'inner')\n# drop for constance variable\np_vif_df.drop(index = 0, axis = 0)","bcc0ed54":"# function to improvise model\ncol_name = []\ndef model_improvise(p_vif_df,y_train,X_train_sm):\n    # check for highest p value as data is already soted accross Pvalue\n    for i in range(p_vif_df.shape[0]):\n        if p_vif_df.loc[i,'Pvalue'] > 0.05:\n            col_name.append(p_vif_df.loc[i,'Features'])\n    X_train_sm.drop(columns = col_name,axis = 1 , inplace = True)\nmodel_improvise(p_vif_df,y_train,X_train_sm)","3af47780":"#call vif function\nvif_func(X_train_sm)","7c265b01":"# build a linear model with left columns\nlr = sm.OLS(y_train,X_train_sm.astype(float))\nlr_model = lr.fit()\nlr_model.summary()","14dea00d":"y_train_pred = lr_model.predict(X_train_sm.astype(float))\n# calculating residual\nres_train = y_train - y_train_pred\n# plot distribution of residual\nsns.distplot(res_train)\n# lets check efficency of model of train sample\nfrom sklearn.metrics import r2_score\nr2_score(y_true = y_train,y_pred = y_train_pred )","c15097f0":"# Check for variables scaling\ndf_test[['age','bmi']]=scaler.transform(df_test[['age','bmi']])\ndf_test.head()","eea8e383":"# Lets check for test data does the model holds good\ny_test = df_test.pop('charges')\nX_test = df_test\n# adding a constant\nX_test_sm = sm.add_constant(X_test)\n# drop columns as in train data\nX_test_sm.drop(columns = col_name , axis =1 ,inplace = True)","8145f541":"# building train model\ny_test_pred = lr_model.predict(X_test_sm.astype(float))\n# calculating residual\nres_test = y_test - y_test_pred\n# plot distribution of residual\nsns.distplot(res_test)\n#efficency of test model\nr2_score(y_true = y_test,y_pred = y_test_pred )","5d050885":"##Comparing the actual output values with the predicted values\ndf = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})\ndf.tail(5)","3a817402":"#plot for ytest and yPred\nplt.scatter(y_test,y_test_pred)","7f905e10":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(X_train,y_train)\nrfe = RFE(lr,9)\nrfe = rfe.fit(X_train,y_train)\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","0a9fce5c":"### scalling of the data\n1. Standardisation : normalize data to have mean and standard deviation 0 and 1 respectively\n2. MinMax : normalize value between 0 to 1\n#### preffered MInMax as it will make the dataset independent of outliers","98e3ece7":"clearly we can see the highest correlation is smoking followed by age and bmi","82d0b729":"#### Another approach for Recurssive Feature ","8a306c8d":"## for both train and test data the R2_score is .74 and .75 respectively which is good sign that indicates whatever we have predict for train data holds good with test data","caff0f16":"## looking for the missing value in dataset","2a6c85da":"## Univariate analysis of the dataset","1f74a601":"#### SPLIT Data into TEST and TRAIN data","b284e65f":"### Observation\n1. We can see bmi has some sort of linear relationship with charges","6f8e12df":"#### now we need to follow reverse method approch to find best fit model\n1. If V factor and p value is above 0.5 drop that column\n2. If Vfactor and P value same than irst drop column having high P value than see whether its affecting V factor","d3924704":"#### check again VF","b456ce22":"### Checking for outliers","dd19da6f":"#### Check for correlation","1c7d7375":"### Observations:\n1. We can clearly see prices are very high for smokers","43d649c6":"#### Data preperation for regression\n","22c1c7d5":"### let's start model building via train data","da083e41":"### find the inverse variance Factor","4b876c7e":"#### Observations:\n1. Models looks good with low VIF and P value less than .5","f822d3f3":"## Reading the data","9dc9010f":"### Observation:\n 1. no missing values in data \n 2. In the given dataset there is 1 duplicate row which we need to treat before procceding ahead","a28bd3d5":"#### Observation:\nModel looks significant with P value less than .5 and R square .74 "}}