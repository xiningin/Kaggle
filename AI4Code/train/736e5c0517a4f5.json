{"cell_type":{"a7bf3891":"code","c7bf6f40":"code","41837fbd":"code","ff2d004f":"code","14ebce69":"code","fb3c83db":"code","87649e9f":"code","c5199b06":"code","2a821492":"code","b25cee84":"code","eb9d9194":"code","e0fe4afb":"code","5e2b17a9":"code","a7d70553":"code","18eaa5ff":"code","ef032df8":"code","e77a3c76":"code","4140294e":"code","91f53b38":"code","5f083769":"code","b6c2f722":"code","5f5db914":"code","60885f3f":"code","a39a71a9":"code","c8807705":"code","65950e93":"code","9e402e52":"code","2e6a022e":"code","a0471948":"code","124be428":"code","34a5d1cd":"code","3e1adbb2":"code","11e52654":"markdown","3a5bd5e8":"markdown","8c9efb87":"markdown","b7e2c263":"markdown","9c7b3993":"markdown","ca8b4262":"markdown","f0a03893":"markdown","56e02627":"markdown","4bf66b15":"markdown","53385b82":"markdown","b0182e41":"markdown","680b68ad":"markdown","6dbd2995":"markdown","1ceee93a":"markdown","f991df6c":"markdown","0a4f534a":"markdown"},"source":{"a7bf3891":"import gc\nimport glob\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport pprint\n\nimport numpy as np\nimport pandas as pd\n\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\nfrom PIL import Image\n\n%matplotlib inline\n\npd.options.display.max_rows = 128\npd.options.display.max_columns = 128","c7bf6f40":"plt.rcParams['figure.figsize'] = (12, 9)","41837fbd":"os.listdir('..\/input\/test\/')","ff2d004f":"train = pd.read_csv('..\/input\/train\/train.csv')\ntest = pd.read_csv('..\/input\/test\/test.csv')\nsample_submission = pd.read_csv('..\/input\/test\/sample_submission.csv')","14ebce69":"labels_breed = pd.read_csv('..\/input\/breed_labels.csv')\nlabels_state = pd.read_csv('..\/input\/color_labels.csv')\nlabels_color = pd.read_csv('..\/input\/state_labels.csv')","fb3c83db":"train_image_files = sorted(glob.glob('..\/input\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/train_sentiment\/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\n\ntest_image_files = sorted(glob.glob('..\/input\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/test_sentiment\/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","87649e9f":"plt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\n\n# Images:\ntrain_df_ids = train[['PetID']]\nprint(train_df_ids.shape)\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\nprint(len(train_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images \/ train_df_ids.shape[0]))\n\n# Metadata:\ntrain_df_ids = train[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas \/ train_df_ids.shape[0]))\n\n# Sentiment:\ntrain_df_ids = train[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('\/')[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments \/ train_df_ids.shape[0]))","c5199b06":"# Images:\ntest_df_ids = test[['PetID']]\nprint(test_df_ids.shape)\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\nprint(len(test_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images \/ test_df_ids.shape[0]))\n\n\n# Metadata:\ntest_df_ids = test[['PetID']]\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas \/ test_df_ids.shape[0]))\n\n\n\n# Sentiment:\ntest_df_ids = test[['PetID']]\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('\/')[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments \/ test_df_ids.shape[0]))\n\n\n# are distributions the same?\nprint('images and metadata distributions the same? {}'.format(\n    np.all(test_metadata_pets == test_imgs_pets)))","2a821492":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        # Does not have to be extracted because main DF already contains description\n        self.extract_sentiment_text = False\n        \n        \n    def open_metadata_file(self, filename):\n        \"\"\"\n        Load metadata file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            metadata_file = json.load(f)\n        return metadata_file\n            \n    def open_sentiment_file(self, filename):\n        \"\"\"\n        Load sentiment file.\n        \"\"\"\n        with open(filename, 'r') as f:\n            sentiment_file = json.load(f)\n        return sentiment_file\n            \n    def open_image_file(self, filename):\n        \"\"\"\n        Load image file.\n        \"\"\"\n        image = np.asarray(Image.open(filename))\n        return image\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n\n        if self.extract_sentiment_text:\n            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns').sum()\n        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n        \n        file_sentiment.update(file_sentences_sentiment)\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        if self.extract_sentiment_text:\n            df_sentiment['text'] = file_sentences_text\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\n# Helper function for parallel data processing:\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = '..\/input\/{}_sentiment\/{}.json'.format(mode, pet_id)\n    try:\n        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except FileNotFoundError:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob('..\/input\/{}_metadata\/{}*.json'.format(mode, pet_id)))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_metadata_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","b25cee84":"# Unique IDs from train and test:\ndebug = False\ntrain_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\n# Train set:\n# Parallel processing of data:\ndfs_train = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\n# Extract processed data and format them as DFs:\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\n# Test set:\n# Parallel processing of data:\ndfs_test = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\n# Extract processed data and format them as DFs:\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","eb9d9194":"# Extend aggregates and improve column naming\naggregates = ['mean', 'sum']\n\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntrain_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntest_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","e0fe4afb":"# Train merges:\ntrain_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\n\nprint(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","5e2b17a9":"train_breed_main = train_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntrain_breed_main = train_breed_main.iloc[:, 2:]\ntrain_breed_main = train_breed_main.add_prefix('main_breed_')\n\ntrain_breed_second = train_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntrain_breed_second = train_breed_second.iloc[:, 2:]\ntrain_breed_second = train_breed_second.add_prefix('second_breed_')\n\n\ntrain_proc = pd.concat(\n    [train_proc, train_breed_main, train_breed_second], axis=1)\n\n\ntest_breed_main = test_proc[['Breed1']].merge(\n    labels_breed, how='left',\n    left_on='Breed1', right_on='BreedID',\n    suffixes=('', '_main_breed'))\n\ntest_breed_main = test_breed_main.iloc[:, 2:]\ntest_breed_main = test_breed_main.add_prefix('main_breed_')\n\ntest_breed_second = test_proc[['Breed2']].merge(\n    labels_breed, how='left',\n    left_on='Breed2', right_on='BreedID',\n    suffixes=('', '_second_breed'))\n\ntest_breed_second = test_breed_second.iloc[:, 2:]\ntest_breed_second = test_breed_second.add_prefix('second_breed_')\n\n\ntest_proc = pd.concat(\n    [test_proc, test_breed_main, test_breed_second], axis=1)\n\nprint(train_proc.shape, test_proc.shape)","a7d70553":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\nprint('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))","18eaa5ff":"column_types = X.dtypes\n\nint_cols = column_types[column_types == 'int']\nfloat_cols = column_types[column_types == 'float']\ncat_cols = column_types[column_types == 'object']\n\nprint('\\tinteger columns:\\n{}'.format(int_cols))\nprint('\\n\\tfloat columns:\\n{}'.format(float_cols))\nprint('\\n\\tto encode categorical columns:\\n{}'.format(cat_cols))","ef032df8":"# Copy original X DF for easier experimentation,\n# all feature engineering will be performed on this one:\nX_temp = X.copy()\n\n\n# Select subsets of columns:\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']\ncategorical_columns = ['main_breed_BreedName', 'second_breed_BreedName']\n\n# Names are all unique, so they can be dropped by default\n# Same goes for PetID, it shouldn't be used as a feature\nto_drop_columns = ['PetID', 'Name', 'RescuerID']\n# RescuerID will also be dropped, as a feature based on this column will be extracted independently","e77a3c76":"# Count RescuerID occurrences:\nrescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\n# Merge as another feature onto main DF:\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')","4140294e":"# Factorize categorical columns:\nfor i in categorical_columns:\n    X_temp.loc[:, i] = pd.factorize(X_temp.loc[:, i])[0]","91f53b38":"# Subset text features:\nX_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')","5f083769":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n\nn_components = 5\ntext_features = []\n\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print('generating features from: {}'.format(i))\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    nmf_ = NMF(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n    \n    nmf_col = nmf_.fit_transform(tfidf_col)\n    nmf_col = pd.DataFrame(nmf_col)\n    nmf_col = nmf_col.add_prefix('NMF_{}_'.format(i))\n    \n    text_features.append(svd_col)\n    text_features.append(nmf_col)\n\n    \n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\n\n# Concatenate with main DF:\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\n# Remove raw text columns:\nfor i in X_text.columns:\n    X_temp = X_temp.drop(i, axis=1)","b6c2f722":"# Remove unnecessary columns:\nX_temp = X_temp.drop(to_drop_columns, axis=1)\n\n# Check final df shape:\nprint('X shape: {}'.format(X_temp.shape))","5f5db914":"# Split into train and test again:\nX_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\n# Remove missing target column from test:\nX_test = X_test.drop(['AdoptionSpeed'], axis=1)\n\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]\n\n\n# Check if columns between the two DFs are the same:\ntrain_cols = X_train.columns.tolist()\ntrain_cols.remove('AdoptionSpeed')\n\ntest_cols = X_test.columns.tolist()\n\nassert np.all(train_cols == test_cols)","60885f3f":"np.sum(pd.isnull(X_train))","a39a71a9":"np.sum(pd.isnull(X_test))","c8807705":"import scipy as sp\n\nfrom collections import Counter\nfrom functools import partial\nfrom math import sqrt\n\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\n\n# FROM: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features\n\n# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']\n    \ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))","65950e93":"import lightgbm as lgb\n\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 70,\n          'max_depth': 9,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.02,\n          'min_child_samples': 150,\n          'min_child_weight': 0.02,\n          'lambda_l2': 0.0475,\n          'verbosity': -1,\n          'data_random_seed': 17}\n\n# Additional parameters:\nearly_stop = 500\nverbose_eval = 100\nnum_rounds = 10000\nn_splits = 5","9e402e52":"from sklearn.model_selection import StratifiedKFold\n\n\nkfold = StratifiedKFold(n_splits=n_splits, random_state=1337)\n\n\noof_train = np.zeros((X_train.shape[0]))\noof_test = np.zeros((X_test.shape[0], n_splits))\n\n\ni = 0\nfor train_index, valid_index in kfold.split(X_train, X_train['AdoptionSpeed'].values):\n    \n    X_tr = X_train.iloc[train_index, :]\n    X_val = X_train.iloc[valid_index, :]\n    \n    y_tr = X_tr['AdoptionSpeed'].values\n    X_tr = X_tr.drop(['AdoptionSpeed'], axis=1)\n    \n    y_val = X_val['AdoptionSpeed'].values\n    X_val = X_val.drop(['AdoptionSpeed'], axis=1)\n    \n    print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n    \n    d_train = lgb.Dataset(X_tr, label=y_tr)\n    d_valid = lgb.Dataset(X_val, label=y_val)\n    watchlist = [d_train, d_valid]\n    \n    print('training LGB:')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    \n    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n    test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    \n    oof_train[valid_index] = val_pred\n    oof_test[:, i] = test_pred\n    \n    i += 1","2e6a022e":"plt.hist(oof_train)","a0471948":"# Compute QWK based on OOF train predictions:\noptR = OptimizedRounder()\noptR.fit(oof_train, X_train['AdoptionSpeed'].values)\ncoefficients = optR.coefficients()\npred_test_y_k = optR.predict(oof_train, coefficients)\nprint(\"\\nValid Counts = \", Counter(X_train['AdoptionSpeed'].values))\nprint(\"Predicted Counts = \", Counter(pred_test_y_k))\nprint(\"Coefficients = \", coefficients)\nqwk = quadratic_weighted_kappa(X_train['AdoptionSpeed'].values, pred_test_y_k)\nprint(\"QWK = \", qwk)","124be428":"# Manually adjusted coefficients:\n\ncoefficients_ = coefficients.copy()\n\ncoefficients_[0] = 1.645\ncoefficients_[1] = 2.115\ncoefficients_[3] = 2.84\n\ntrain_predictions = optR.predict(oof_train, coefficients_).astype(int)\nprint('train pred distribution: {}'.format(Counter(train_predictions)))\n\ntest_predictions = optR.predict(oof_test.mean(axis=1), coefficients_)\nprint('test pred distribution: {}'.format(Counter(test_predictions)))","34a5d1cd":"# Distribution inspection of original target and predicted train and test:\n\nprint(\"True Distribution:\")\nprint(pd.value_counts(X_train['AdoptionSpeed'], normalize=True).sort_index())\nprint(\"\\nTrain Predicted Distribution:\")\nprint(pd.value_counts(train_predictions, normalize=True).sort_index())\nprint(\"\\nTest Predicted Distribution:\")\nprint(pd.value_counts(test_predictions, normalize=True).sort_index())","3e1adbb2":"# Generate submission:\n\nsubmission = pd.DataFrame({'PetID': test['PetID'].values, 'AdoptionSpeed': test_predictions.astype(np.int32)})\nsubmission.head()\nsubmission.to_csv('submission.csv', index=False)","11e52654":"### add breed mapping:","3a5bd5e8":"### load mapping dictionaries:","8c9efb87":"### feature engineering:","b7e2c263":"import joblib\n\njoblib.dump(X, 'X_temp.joblib')","9c7b3993":"### merge processed DFs with base train\/test DF:","ca8b4262":"### train analysis:","f0a03893":"### group extracted features by PetID:","56e02627":"### data parsing & feature extraction:\n\nAfter taking a look at the data, we know its structure and can use it to extract additional features and concatenate them with basic train\/test DFs.","4bf66b15":"### load core DFs (train and test):","53385b82":"### concatenate train & test:\n\nInspect NaN structure of the processed data:\n`AdoptionSpeed` is the target column.","b0182e41":"### adapted from: https:\/\/www.kaggle.com\/myltykritik\/simple-lgbm-image-features","680b68ad":"### model training:","6dbd2995":"### train\/test split:","1ceee93a":"### extract different column types:\n\n- integer columns are usually categorical features, which do not need encoding\n- float columns are numerical features\n- object columns are categorical features, which should be encoded","f991df6c":"### additional data:\n\nWe have also additional information about pets available in form of:\n\n- images\n- metadata\n- sentiment\n\nIntegration of those will enable us to possibly improve the score.\nInformation derived from example from images should be very important, as picture of a pet influences the way we look at an animal in a significant way.","0a4f534a":"### train and test NaN structure:"}}