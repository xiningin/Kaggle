{"cell_type":{"dfbdabe1":"code","5e8673c0":"code","04bb7d3f":"code","5e3a22d1":"code","52dcaf28":"code","6f0f8c1c":"code","8665f86e":"code","4a31e890":"code","17d2ea16":"code","304547ff":"code","535ac287":"code","98363bf9":"code","4f38dcdd":"code","8fc2d5f3":"code","8b409324":"code","67cc9911":"code","9b5e4fbd":"code","8a2dce9d":"code","f9380d1c":"code","9bd1a202":"code","312c122d":"code","0e22f763":"code","ada4b03f":"code","460c800c":"code","db8d7e74":"code","7389a72e":"code","9b27a170":"code","ddeb75d1":"code","46b173fc":"code","438b27ac":"code","15ad327b":"markdown","dc1d463d":"markdown"},"source":{"dfbdabe1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5e8673c0":"train = pd.read_csv('\/kaggle\/input\/dmia-dl-nlp-2019\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/dmia-dl-nlp-2019\/test.csv')","04bb7d3f":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom nltk.tokenize import word_tokenize, wordpunct_tokenize\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","5e3a22d1":"def process_text(text):\n    \n    # \u043f\u0440\u043e\u0441\u0442\u043e \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430, \u0442\u043e \u0435\u0441\u0442\u044c \u0440\u0430\u0437\u0431\u0438\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u043d\u0430 \u0442\u043e\u043a\u0435\u043d\u044b (\u0441\u043b\u043e\u0432\u0430)\n    words = wordpunct_tokenize(text.lower())\n    \n    return words","52dcaf28":"process_text('\u043a\u0440\u0430\u0441\u0438\u0432\u0430\u044f \u043c\u0430\u043c\u0430 \u043c\u044b\u043b\u0430 \u043a\u0440\u0430\u0441\u0438\u0432\u0443\u044e \u0440\u0430\u043c\u0443')","6f0f8c1c":"# \u0432\u0441\u0435 \u043d\u0430\u0448\u0438 \u0442\u0435\u043a\u0441\u0442\u044b\ntexts = list(train.question.map(process_text)) + list(test.question.map(process_text))","8665f86e":"# \u0441\u043e\u0431\u0435\u0440\u0435\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0432\n# \u043f\u0440\u043e\u0441\u0442\u043e \u043f\u043e\u0434\u0441\u0447\u0435\u0442 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u0442\u043e \u0438\u043b\u0438 \u0438\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u0432\u0441\u0442\u0440\u0435\u0442\u0438\u043b\u043e\u0441\u044c \u0432 \u043d\u0430\u0448\u0438\u0445 \u0442\u0435\u043a\u0441\u0442\u0430\u0445\n\nword2freq = {}\n\nfor text in texts:\n    \n    for word in text:\n        \n        word2freq[word] = word2freq.get(word, 0) + 1","4a31e890":"word2index = {'PAD': 0}\nvectors = []\n    \nword2vec_file = open('\/kaggle\/input\/fasttest-common-crawl-russian\/cc.ru.300.vec')\n    \nn_words, embedding_dim = word2vec_file.readline().split()\nn_words, embedding_dim = int(n_words), int(embedding_dim)\n\n# Zero vector for PAD\nvectors.append(np.zeros((1, embedding_dim)))\n\nprogress_bar = tqdm(desc='Read word2vec', total=n_words)\n\nwhile True:\n\n    line = word2vec_file.readline().strip()\n\n    if not line:\n        break\n        \n    current_parts = line.split()\n\n    current_word = ' '.join(current_parts[:-embedding_dim])\n\n    if current_word in word2freq:\n\n        word2index[current_word] = len(word2index)\n\n        current_vectors = current_parts[-embedding_dim:]\n        current_vectors = np.array(list(map(float, current_vectors)))\n        current_vectors = np.expand_dims(current_vectors, 0)\n\n        vectors.append(current_vectors)\n\n    progress_bar.update()\n\nprogress_bar.close()\n\nword2vec_file.close()\n\nvectors = np.concatenate(vectors)","17d2ea16":"unk_words = [word for word in word2freq if word not in word2index]\nunk_counts = [word2freq[word] for word in unk_words]\nn_unk = sum(unk_counts) * 100 \/ sum(list(word2freq.values()))\n\nsub_sample_unk_words = {word: word2freq[word] for word in unk_words}\nsorted_unk_words = list(sorted(sub_sample_unk_words, key=lambda x: sub_sample_unk_words[x], reverse=True))\n\nprint('\u041c\u044b \u043d\u0435 \u0437\u043d\u0430\u0435\u043c {:.2f} % \u0441\u043b\u043e\u0432 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435'.format(n_unk))\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 {} \u0438\u0437 {}, \u0442\u043e \u0435\u0441\u0442\u044c {:.2f} % \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0441\u043b\u043e\u0432 \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435'.format(\n    len(unk_words), len(word2freq), len(unk_words) * 100 \/ len(word2freq)))\nprint('\u0412 \u0441\u0440\u0435\u0434\u043d\u0435\u043c \u043a\u0430\u0436\u0434\u043e\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f {:.2f} \u0440\u0430\u0437'.format(np.mean(unk_counts)))\nprint()\nprint('\u0422\u043e\u043f 5 \u043d\u0435\u0432\u043e\u0448\u0435\u0434\u0448\u0438\u0445 \u0441\u043b\u043e\u0432:')\n\nfor i in range(5):\n    print(sorted_unk_words[i], '\u0441 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u0432\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0438\u0439 -', word2freq[sorted_unk_words[i]])","304547ff":"class WordData(Dataset):\n    \n    def __init__(self, x_data, y_data, word2index, sequence_length=32, pad_token='PAD', verbose=True):\n        \n        super().__init__()\n        \n        self.x_data = []\n        self.y_data = y_data\n        \n        self.word2index = word2index\n        self.sequence_length = sequence_length\n        \n        self.pad_token = pad_token\n        self.pad_index = self.word2index[self.pad_token]\n        \n        self.load(x_data, verbose=verbose)\n        \n    @staticmethod\n    def process_text(text):\n    \n        words = wordpunct_tokenize(text.lower())\n\n        return words\n        \n    def load(self, data, verbose=True):\n        \n        data_iterator = tqdm(data, desc='Loading data', disable=not verbose)\n        \n        for text in data_iterator:\n            words = self.process_text(text)\n            indexed_words = self.indexing(words)\n            self.x_data.append(indexed_words)\n    \n    def indexing(self, tokenized_text):\n\n        # \u0432\u044b\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u0435\u043c \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430 \u0432 \u0438\u043d\u0434\u0435\u043a\u0441 \u043f\u043e\u0437\u0438\u0446\u0438\u0439 \u0432 \u043c\u0430\u0442\u0440\u0438\u0446\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\n\n        return [self.word2index[token] for token in tokenized_text if token in self.word2index]\n    \n    def padding(self, sequence):\n        \n        # \u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0438\u0442\u044c \u0434\u043b\u0438\u043d\u0443 self.sequence_length\n        # \u0435\u0441\u043b\u0438 \u0434\u043b\u0438\u043d\u0430 \u043c\u0435\u043d\u044c\u0448\u0435 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e - \u0437\u0430\u043f\u0430\u0434\u0438\u0442\u044c\n\n        return sequence[:self.sequence_length] + [self.pad_index] * (self.sequence_length - len(sequence))\n    \n    def __len__(self):\n        \n        return len(self.x_data)\n    \n    def __getitem__(self, idx):\n        \n        x = self.x_data[idx]\n        x = self.padding(x)\n        x = torch.Tensor(x).long()\n        \n        y = self.y_data[idx]\n        \n        return x, y","535ac287":"x_train, x_validation, y_train, y_validation = train_test_split(train.question, train.main_category, test_size=0.15)\n\ntrain_dataset = WordData(list(x_train), list(y_train), word2index)\ntrain_loader = DataLoader(train_dataset, batch_size=64)\n\nvalidation_dataset = WordData(list(x_validation), list(y_validation), word2index)\nvalidation_loader = DataLoader(validation_dataset, batch_size=64)\n\ntest_dataset = WordData(list(test.question), np.zeros((test.shape[0])), word2index)\ntest_loader = DataLoader(test_dataset, batch_size=64)","98363bf9":"for x, y in train_loader:\n    break","4f38dcdd":"# x - \u044d\u0442\u043e \u0431\u0430\u0442\u0447 \u0440\u0430\u0437\u043c\u0435\u0440\u043e\u043c 64\nx","8fc2d5f3":"# \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u043c\u044b \u043e\u0442\u0440\u0435\u0437\u0430\u043b\u0438 \u0434\u043b\u0438\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f \u0434\u043e 32 \u0442\u043e\u043a\u0435\u043d\u043e\u0432, \u0430 \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0435 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u043b\u0438 \u0438\u043d\u0434\u0435\u043a\u0441\u043e\u043c PAD \u0434\u043e \u043d\u0443\u0436\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u044b","8b409324":"x.shape","67cc9911":"# \u043d\u0430\u0448\u0438 \u0442\u0430\u0440\u0433\u0435\u0442\u044b\ny","9b5e4fbd":"n_classes = train.main_category.unique().shape[0]","8a2dce9d":"class DeepAverageNetwork(torch.nn.Module):\n    \n    def __init__(self, embedding_matrix, n_classes):\n        \n        super().__init__()\n        \n        # \u0437\u0434\u0435\u0441\u044c \u043c\u044b \u043a\u0430\u043a \u0440\u0430\u0437 \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0432\u0435\u043a\u0442\u043e\u0440\u0430 \u0441\u043b\u043e\u0432 \u0432 \u043d\u0430\u0448\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\n        # \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e \u043c\u0435\u0442\u043e\u0434 from_pretrained \u0437\u0430\u043c\u043e\u0440\u0430\u0436\u0438\u0432\u0430\u0435\u0442 \u044d\u0442\u0443 \u043c\u0430\u0442\u0440\u0438\u0446\u0443\n        self.embedding_layer = torch.nn.Embedding.from_pretrained(torch.Tensor(embedding_matrix))\n        \n        self.layers = torch.nn.Sequential(torch.nn.Linear(300, 256),\n                                          torch.nn.ReLU(), \n                                          torch.nn.Linear(256, 128),\n                                          torch.nn.ReLU(),\n                                          torch.nn.Linear(128, n_classes))\n    def forward(self, x):\n        \n        # \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u0438\u043d\u0434\u0435\u043a\u0441\u044b \u0441\u043b\u043e\u0432 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\n        x = self.embedding_layer(x)\n        \n        # \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u0435\u043c \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0441\u043b\u043e\u0432\n        # \u043f\u0435\u0440\u0435\u0445\u043e\u0434\u0438\u043c \u043a \u043e\u0434\u043d\u0443 \u0432\u0435\u043a\u0442\u043e\u0440\u0443 \u043d\u0430 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\n        # \u043e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u0437\u0430 \u0441\u0447\u0435\u0442 \u043d\u0443\u043b\u0435\u0432\u043e\u0433\u043e \u0442\u043e\u043a\u0435\u043d\u0430 PAD \u043c\u044b \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u0435\u043c \u043d\u0435\u0447\u0435\u0441\u0442\u043d\u043e, \u0441\u0447\u0438\u0442\u0430\u044f, \u0447\u0442\u043e \u0443 \u0432\u0441\u0435\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0434\u043b\u0438\u043d\u0430 32 \u0442\u043e\u043a\u0435\u043d\u0430\n        x = x.mean(dim=-2)\n        \n        # \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0445 \u0441\u043b\u043e\u0435\u0432 \u0441 \u0440\u0435\u043b\u0443\n        x = self.layers(x)\n        \n        return x","f9380d1c":"# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\nmodel = DeepAverageNetwork(embedding_matrix=vectors, n_classes=n_classes)","9bd1a202":"# \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043b\u0438 \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c\n# \u043d\u0435\u0442 \u043b\u0438 \u0431\u0430\u0433\u043e\u0432\nwith torch.no_grad():\n    pred = model(x)\n    \npred.shape","312c122d":"embeddings = model.embedding_layer(x)","0e22f763":"# \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0441\u043b\u043e\u0432\n# 64 - \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\n# 32 - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u043b\u043e\u0432 \u0432 \u043f\u0440\u0438\u043c\u0435\u0440\u0435\n# 300 - \u0440\u0430\u0437\u043c\u0435\u0440 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0435 \u0441\u043b\u043e\u0432\u043e\nembeddings.shape","ada4b03f":"# \u0437\u0430\u0434\u0430\u0435\u043c \u0434\u0435\u0432\u0430\u0439\u0441, \u0433\u0434\u0435 \u0431\u0443\u0434\u0435\u0442 \u0443\u0447\u0438\u0442\u044c\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u044c\n# \u0435\u0441\u043b\u0438 \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u0430 \u0433\u043f\u0443, \u0442\u043e \u0437\u0430\u0434\u0430\u0434\u0438\u043c \u0433\u043f\u0443\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","460c800c":"# \u043d\u0430\u043f\u043e\u043c\u043d\u044e, \u0447\u0442\u043e \u043c\u044b \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0432 \u043c\u043e\u0434\u0435\u043b\u0435 \u0441\u043e\u0444\u0442\u043c\u0430\u043a\u0441, \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043e\u043d \u0443\u0436\u0435 \u0435\u0441\u0442\u044c \u0437\u0434\u0435\u0441\u044c\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model.parameters())\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","db8d7e74":"epochs = 10\nlosses = []\nbest_test_loss = 10.\n\ntest_f1 = []\n\nfor n_epoch in range(epochs):\n    \n    train_losses = []\n    test_losses = []\n    test_targets = []\n    test_pred_class = []\n    \n    progress_bar = tqdm(total=len(train_loader.dataset), desc='Epoch {}'.format(n_epoch + 1))\n    \n    model.train()\n    \n    for x, y in train_loader:\n\n        x = x.to(device)\n        y = y.to(device)\n        \n        optimizer.zero_grad()\n        \n        pred = model(x)\n        loss = criterion(pred, y)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        train_losses.append(loss.item())\n        losses.append(loss.item())\n        \n        progress_bar.set_postfix(train_loss = np.mean(losses[-500:]))\n\n        progress_bar.update(x.shape[0])\n        \n    progress_bar.close()\n    \n    model.eval()\n    \n    for x, y in validation_loader:\n        \n        x = x.to(device)\n        y = y.to(device)\n\n        with torch.no_grad():\n\n            pred = model(x)\n\n            pred = pred.cpu()\n            y = y.cpu()\n\n            test_targets.append(y.numpy())\n            test_pred_class.append(np.argmax(pred, axis=1))\n\n            loss = criterion(pred, y)\n\n            test_losses.append(loss.item())\n        \n    mean_test_loss = np.mean(test_losses)\n\n    test_targets = np.concatenate(test_targets).squeeze()\n    test_pred_class = np.concatenate(test_pred_class).squeeze()\n\n    f1 = f1_score(test_targets, test_pred_class, average='micro')\n\n    test_f1.append(f1)\n    \n    print()\n    print('Losses: train - {:.3f}, test - {:.3f}'.format(np.mean(train_losses), mean_test_loss))\n\n    print('F1 test - {:.3f}'.format(f1))\n        \n    # \u043d\u0430\u0438\u0432\u043d\u044b\u0439 early stopping\n    if mean_test_loss < best_test_loss:\n        best_test_loss = mean_test_loss\n    else:\n        print('Early stopping')\n        break","7389a72e":"model.eval()\n\npredictions = []\n\nfor x, _ in test_loader:\n\n    x = x.to(device)\n\n    with torch.no_grad():\n\n        pred = model(x)\n\n        pred = pred.cpu()\n        \n        predictions.append(np.argmax(pred, axis=1))\n        \npredictions = np.concatenate(predictions).squeeze()","9b27a170":"test['main_category'] = predictions","ddeb75d1":"test = test[['index', 'main_category']]","46b173fc":"test.head()","438b27ac":"test.to_csv('submission.csv', index=False)","15ad327b":"# \u041e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435\n\u042f \u043f\u0440\u0438\u043c\u043e\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043b \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435: Fasttext Common Crawl (Russian)\n\u042d\u0442\u043e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0438 \u0434\u043b\u044f \u0440\u0443\u0441\u0441\u043a\u0438\u0445 \u0441\u043b\u043e\u0432","dc1d463d":"# \u041f\u0440\u043e\u0447\u0438\u0442\u0430\u0435\u043c \u0444\u0430\u0439\u043b \u0441 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438\n\u0412 \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0435 \u044d\u0442\u043e\u0433\u043e \u0444\u0430\u0439\u043b\u0430 \u0441\u0442\u043e\u0438\u0442 \u0441\u043b\u043e\u0432\u043e, \u0430 \u0437\u0430\u0442\u0435\u043c \u0447\u0435\u0440\u0435\u0437 \u043f\u0440\u043e\u0431\u0435\u043b \u0443\u043a\u0430\u0437\u0430\u043d\u043e 300 \u0447\u0438\u0441\u0435\u043b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0443 \u044d\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430.\n\u041c\u044b \u043d\u0435 \u0431\u0443\u0434\u0435\u043c \u0447\u0438\u0442\u0430\u0442\u044c \u0432\u0441\u0435 2 000 000 \u0441\u043b\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0432 \u043f\u0430\u043c\u044f\u0442\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u0443 \u0438\u0437 2 000 000 * 300 \u0447\u0438\u0441\u0435\u043b. \u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0432 \u043d\u0430\u0448 \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0441\u043b\u043e\u0432\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u044e\u0442\u0441\u044f \u0432 \u043d\u0430\u0448\u0438\u0445 \u0442\u0435\u043a\u0441\u0442\u0430\u0445.\n\n\u041c\u044b \u0431\u0443\u0434\u0435\u043c \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0442\u044c \u0432 ```word2index``` \u0441\u043b\u043e\u0432\u0430 \u0438 \u0438\u0445 \u0438\u043d\u0434\u0435\u043a\u0441 \u0432 \u043c\u0430\u0442\u0440\u0438\u0446\u0435 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u043e\u0432\n\n\u0422\u0430\u043a\u0436\u0435 \u043f\u0435\u0440\u0432\u044b\u043c \u0441\u043b\u043e\u0432\u043e\u043c \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0441\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u043e\u043a\u0435\u043d PAD, \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0431\u0443\u0434\u0435\u0442 \u0441\u043e\u0441\u0442\u043e\u044f\u0442\u044c \u0438\u0437 \u043d\u0443\u043b\u0435\u0439. \u041e\u043d \u043d\u0443\u0436\u0435\u043d \u0434\u043b\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043d\u0430\u0448\u0438\u0445 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0434\u043e \u043d\u0443\u0436\u043d\u043e\u0439 \u0434\u043b\u0438\u043d\u044b, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0431\u0430\u0442\u0447 \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u043e\u0432."}}