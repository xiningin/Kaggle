{"cell_type":{"46105818":"code","c7f43a48":"code","24909e91":"code","2126620a":"code","9016f307":"code","7753102a":"code","b854b3a6":"code","c3d0c162":"code","edfb29c1":"code","e2a3a426":"code","1a687df4":"code","21942652":"code","6866bfa7":"code","5fd8ce18":"code","cd4e7106":"code","80d0afe4":"code","464d0b07":"code","23fc5be2":"code","ab130df7":"code","7f04d07c":"code","6d1b5655":"code","088a6f18":"code","0b7c13f5":"code","a65ee156":"code","40aba12c":"code","b2101669":"code","fc10150a":"code","d32fd4e2":"code","b551d5f6":"code","d137000e":"code","7087144c":"code","fc825f5e":"code","327d3607":"code","d8659ed2":"code","86c59cd7":"code","7c0ae13e":"code","0c2eb448":"code","0552d4cc":"code","dafe69ec":"code","3ba581ee":"code","b4d26a96":"code","63b32d5d":"code","fbd090fc":"code","f20eec36":"code","555c581f":"code","36239289":"code","fda25e5c":"code","32926e7b":"code","d895d7f7":"code","94a85c75":"code","6ecc94e7":"code","a5e0d890":"code","ac229f24":"markdown","ee05f177":"markdown","9b8c3416":"markdown","ab3d8a46":"markdown","7b9b828c":"markdown","ba7e8217":"markdown","0177682c":"markdown","b877c2ec":"markdown","705ecc9a":"markdown","86abaea3":"markdown","8e3a4d47":"markdown","15a3e759":"markdown","7c3b5d3a":"markdown","d1e57ce4":"markdown"},"source":{"46105818":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7f43a48":"# reading the data and also checking the computation time\ntest = pd.read_csv('\/kaggle\/input\/employee-promotion-datasets\/test (3).csv')\ntrain = pd.read_csv('\/kaggle\/input\/employee-promotion-datasets\/train.csv')\n\n\n# lets also check the shape of the dataset\nprint(test.shape)","24909e91":"test.head()","2126620a":"train.head()","9016f307":"# lets import all the required libraries\n\n# for mathematical operations\nimport numpy as np\n# for dataframe operations\nimport pandas as pd\n\n# for data visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# setting up the size of the figures\nplt.rcParams['figure.figsize'] = (16, 5)\n# setting up the style of the plot\nplt.style.use('fivethirtyeight')\n\n# for interactivity\nimport ipywidgets as widgets\nfrom ipywidgets import interact\nfrom ipywidgets import interact_manual\n\n\n\n# for machine learning\nimport sklearn\nimport imblearn","7753102a":"train.info()","b854b3a6":"# lets check the Target Class Balance\n\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('fivethirtyeight')\n\nplt.subplot(1, 2, 1)\nsns.countplot(train['is_promoted'],)\n\nplt.xlabel('Promoted or Not?', fontsize = 10)\n\nplt.subplot(1, 2, 2)\ntrain['is_promoted'].value_counts().plot(kind = 'pie', explode = [0, 0.1], autopct = '%.2f%%', startangle = 90,\n                                       labels = ['1','0'], shadow = True, pctdistance = 0.5)\nplt.axis('off')\n\nplt.suptitle('Target Class Balance', fontsize = 15)\nplt.show()","c3d0c162":"train.iloc[:,1:].describe().style.background_gradient(cmap = 'copper')","edfb29c1":"train.describe(include = 'object')","e2a3a426":"# Lets make an interactive function to check the statistics of these numerical columns at a time\n\n@interact\ndef check(column = list(train.select_dtypes('number').columns[1:8])):\n    print(\"Maximum Value :\", train[column].max())\n    print(\"Minimum Value :\", train[column].min())\n    print(\"Mean : {0:.2f}\".format(train[column].mean()))\n    print(\"Median :\", train[column].median())\n    print(\"Standard Deviation :  {0:.2f}\".format(train[column].std()))","1a687df4":"# missing values in training data set\n\n# lets calculate the total missing values in the dataset\ntrain_total = train.isnull().sum()\n\n# lets calculate the percentage of missing values in the dataset\ntrain_percent = ((train.isnull().sum()\/train.shape[0])*100).round(2)\n\n# lets calculate the total missing values in the dataset\ntest_total = test.isnull().sum()\n\n# lets calculate the percentage of missing values in the dataset\ntest_percent = ((test.isnull().sum()\/test.shape[0])*100).round(2)\n\n# lets make a dataset consisting of total no. of missing values and percentage of missing values in the dataset\ntrain_missing_data = pd.concat([train_total, train_percent, test_total, test_percent],\n                                axis=1, \n                                keys=['Train_Total', 'Train_Percent %','Test_Total', 'Test_Percent %'],\n                                sort = True)\n\n# lets check the head\ntrain_missing_data.style.bar(color = ['gold'])","21942652":"# checking datatype of columns in the data\ntrain.dtypes[train.isnull().any()]","6866bfa7":"# lets impute the missing values in the Training Data\n\ntrain['education'] = train['education'].fillna(train['education'].mode()[0])\ntrain['previous_year_rating'] = train['previous_year_rating'].fillna(train['previous_year_rating'].mode()[0])\n\n# lets check whether the Null values are still present or not?\nprint(\"Number of Missing Values Left in the Training Data :\", train.isnull().sum().sum())","5fd8ce18":"# lets impute the missing values in the Testing Data\n\ntest['education'] = test['education'].fillna(test['education'].mode()[0])\ntest['previous_year_rating'] = test['previous_year_rating'].fillna(test['previous_year_rating'].mode()[0])\n\n# lets check whether the Null values are still present or not?\nprint(\"Number of Missing Values Left in the Training Data :\", test.isnull().sum().sum())","cd4e7106":"train.select_dtypes('number').head()","80d0afe4":"# lets check the boxplots for the columns where we suspect for outliers\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('fivethirtyeight')\n\n# Box plot for average training score\nplt.subplot(1, 2, 1)\nsns.boxplot(train['avg_training_score'], color = 'red')\nplt.xlabel('Average Training Score', fontsize = 12)\nplt.ylabel('Range', fontsize = 12)\n\n# Box plot for length of service\nplt.subplot(1, 2, 2)\nsns.boxplot(train['length_of_service'], color = 'red')\nplt.xlabel('Length of Service', fontsize = 12)\nplt.ylabel('Range', fontsize = 12)\n\nplt.suptitle('Box Plot', fontsize = 20)\nplt.show()","464d0b07":"# Lets check the distribution for the columns for which we suspect for the outliers\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('fivethirtyeight')\n\n# Distribution plot for Average training score\nplt.subplot(1, 2, 1)\nsns.distplot(train['avg_training_score'], color = 'darkblue')\nplt.xlabel('Average Training Score', fontsize = 12)\nplt.ylabel('Range', fontsize = 12)\n\n# Distribution plot for Length of Service\nplt.subplot(1, 2, 2)\nsns.distplot(train['length_of_service'], color = 'darkblue')\nplt.xlabel('Length of Service', fontsize = 12)\nplt.ylabel('Range', fontsize = 12)\n\nplt.suptitle('Distribution Plot', fontsize = 20)\nplt.show()","23fc5be2":"# lets plot pie chart for the columns where we have very few categories\nplt.rcParams['figure.figsize'] = (16,5)\nplt.style.use('fivethirtyeight')\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 1)\nlabels = ['0','1']\nsizes = train['KPIs_met >80%'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nexplode = [0, 0]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('KPIs Met > 80%', fontsize = 20)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 2)\nlabels = ['1', '2', '3', '4', '5']\nsizes = train['previous_year_rating'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nexplode = [0, 0, 0, 0, 0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Previous year Ratings', fontsize = 20)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 3)\nlabels = ['0', '1']\nsizes = train['awards_won?'].value_counts()\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nexplode = [0,0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Awards Won?', fontsize = 20)\n\n\nplt.legend()\nplt.show()","ab130df7":"# lets check the distribution of trainings undertaken by the employees\n\nplt.rcParams['figure.figsize'] = (17, 4)\nsns.countplot(train['no_of_trainings'], palette = 'spring')\nplt.xlabel(' ', fontsize = 14)\nplt.title('Distribution of Trainings undertaken by the Employees')\nplt.show()","7f04d07c":"# lets check the Age of the Employees\n\nplt.rcParams['figure.figsize'] = (8, 4)\nsns.distplot(train['age'], color = 'black')\nplt.title('Distribution of Age among the Employees', fontsize = 15)\nplt.xlabel('Age of the Employees')\nplt.grid()\nplt.show()","6d1b5655":"train.select_dtypes('object').head()","088a6f18":"# lets check different Departments\n\nplt.rcParams['figure.figsize'] = (8, 5)\nsns.countplot(y = train['department'], palette = 'cividis', orient = 'v')\nplt.xlabel('')\nplt.ylabel('Department Name')\nplt.title('Distribution of Employees in Different Departments', fontsize = 15)\n\nplt.show()","0b7c13f5":"# lets check distribution of different Regions\n\nplt.rcParams['figure.figsize'] = (8,15)\nsns.countplot(y = train['region'], palette = 'copper', orient = 'v')\nplt.xlabel('')\nplt.ylabel('Region')\nplt.title('Different Regions', fontsize = 15)\nplt.xticks(rotation = 90)\n\nplt.show()","a65ee156":"# lets plot pie chart for the columns where we have very few categories\nplt.rcParams['figure.figsize'] = (16,5)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 1)\nlabels = train['education'].value_counts().index\nsizes = train['education'].value_counts()\ncolors = plt.cm.copper(np.linspace(0, 1, 5))\nexplode = [0, 0, 0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Education', fontsize = 20)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 2)\nlabels = train['gender'].value_counts().index\nsizes = train['gender'].value_counts()\ncolors = plt.cm.copper(np.linspace(0, 1, 5))\nexplode = [0, 0]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Gender', fontsize = 20)\n\n# plotting a pie chart to represent share of Previous year Rating of the Employees\nplt.subplot(1, 3, 3)\nlabels = train['recruitment_channel'].value_counts().index\nsizes = train['recruitment_channel'].value_counts()\ncolors = plt.cm.copper(np.linspace(0, 1, 5))\nexplode = [0,0,0.1]\n\nplt.pie(sizes, labels = labels, colors = colors, explode = explode, shadow = True, startangle = 90)\nplt.title('Recruitment Channel', fontsize = 20)\n\nplt.show()","40aba12c":"# interactive function for plotting univariate charts for categorical data\n\nplt.rcParams['figure.figsize'] = (15, 4)\n@interact_manual\ndef check(column = list(train.select_dtypes('object').columns),\n          palette = ['cividis','copper','spring','Reds','Blues']):\n    sns.countplot(train[column], palette = palette)\n   \n    plt.show()","b2101669":"# Lets compare the Gender Gap in the promotion\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (10, 3)\nx = pd.crosstab(train['gender'], train['is_promoted'])\ncolors = plt.cm.Wistia(np.linspace(0, 1, 5))\nx.div(x.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = False, color = colors)\nplt.title('Effect of Gender on Promotion', fontsize = 15)\nplt.xlabel(' ')\nplt.show()","fc10150a":"# lets compare the effect of different Departments and Promotion\n\nplt.rcParams['figure.figsize'] = (12,4)\nx = pd.crosstab(train['department'], train['is_promoted'])\ncolors = plt.cm.copper(np.linspace(0, 1, 3))\nx.div(x.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = False, color = colors)\nplt.title('Effect of Department on Promotion', fontsize = 15)\nplt.xlabel(' ')\nplt.show()","d32fd4e2":"# lets compare the effect of Education and Promotion\n\nplt.rcParams['figure.figsize'] = (12,4)\nx = pd.crosstab(train['education'], train['is_promoted'])\ncolors = plt.cm.bone(np.linspace(0, 1, 3))\nx.div(x.sum(1).astype(float), axis = 0).plot(kind = 'bar', stacked = False, color = colors)\nplt.title('Effect of Education on Promotion', fontsize = 15)\nplt.xlabel(' ')\nplt.show()","b551d5f6":"# checking the effect of number of trainings on promotion\n\nsns.violinplot(train['no_of_trainings'], train['is_promoted'])\nplt.title('Effect of Trainings on Promtions', fontsize = 15)\nplt.xlabel('Number of Trainings', fontsize = 10)\nplt.ylabel('Employee Promoted or not?', fontsize = 10)\nplt.show()","d137000e":"# Effect of Age on the Promotion\n\nsns.boxenplot(train['is_promoted'], train['age'], palette = 'PuRd')\nplt.title('Effect of Age on Promotion', fontsize = 15)\nplt.xlabel('Is the Employee Promoted?', fontsize = 10)\nplt.ylabel('Age of the Employee', fontsize = 10)\nplt.show()","7087144c":"# lets check relation between number of trainings and average training score\n\nsns.boxplot(train['no_of_trainings'], train['avg_training_score'])\nplt.title('Relation between No. of Trainings and Average Training Score', fontsize = 15)\nplt.xlabel('Number of Trainings', fontsize = 10)\nplt.ylabel('Average Training Score', fontsize = 10)\nplt.show()","fc825f5e":"# lets check the relation between the length of service and the average training score\n\nsns.stripplot(train['avg_training_score'], train['length_of_service'], palette = 'Set3')\nplt.title('Average Training Score vs Length of Service', fontsize = 15)\nplt.xlabel('Average Training Score', fontsize = 10)\nplt.ylabel('Length of Service', fontsize = 10)\nplt.show()","327d3607":"## lets check the relation between KPIs Met and Promotion\n\nx = pd.crosstab(train['KPIs_met >80%'], train['is_promoted'])\nx.style.background_gradient(cmap = 'bone')","d8659ed2":"x = pd.crosstab(train['awards_won?'], train['is_promoted'])\nx.style.background_gradient(cmap = 'Blues')","86c59cd7":"# lets make an Interactive Function for Bivariate Analysis\n\nplt.rcParams['figure.figsize'] = (15, 4)\n@interact_manual\ndef bivariate_plot(column1 = list(train.select_dtypes('object').columns),\n                   column2 = list(train.select_dtypes('number').columns[1:])):\n    sns.boxplot(train[column1], train[column2])","7c0ae13e":"# lets create some extra features from existing features to improve our Model\n\n# creating a Metric of Sum\ntrain['sum_metric'] = train['awards_won?']+train['KPIs_met >80%'] + train['previous_year_rating']\ntest['sum_metric'] = test['awards_won?']+test['KPIs_met >80%'] + test['previous_year_rating']\n\n# creating a total score column\ntrain['total_score'] = train['avg_training_score'] * train['no_of_trainings']\ntest['total_score'] = test['avg_training_score'] * test['no_of_trainings']","0c2eb448":"# lets remove some of the columns which are not very useful for predicting the promotion.\n\n# we already know that the recruitment channel is very least related to promotion of an employee, so lets remove this column\n# even the region seems to contribute very less, when it comes to promotion, so lets remove it too.\n# also the employee id is not useful so lets remove it.\n\ntrain = train.drop(['recruitment_channel', 'region', 'employee_id'], axis = 1)\ntest = test.drop(['recruitment_channel', 'region', 'employee_id'], axis = 1)\n\n# lets check the columns in train and test data set after feature engineering\ntrain.columns","0552d4cc":"# lets group the employees based on their Education\n\ntrain[['education','is_promoted']].groupby(['education']).agg(['count','sum'])","dafe69ec":"## lets use the interactive function to make it more reusable\n\n@interact\ndef group_operations(column = list(train.select_dtypes('object').columns)):\n    return train[[column, 'is_promoted']].groupby([column]).agg('count').style.background_gradient(cmap = 'Wistia')","3ba581ee":"# lets get the names of all the employees who have taken trainings more than 7 Times\n\n@interact\ndef check(column = 'no_of_trainings', x = 5):\n    y = train[train['no_of_trainings'] > x]\n    return y['is_promoted'].value_counts()","b4d26a96":"# lets remove the above two columns as they have a huge negative effect on our training data\n\n# lets check shape of the train data before deleting two rows\nprint(\"Before Deleting the above two rows :\", train.shape)\n\ntrain = train.drop(train[(train['KPIs_met >80%'] == 0) & (train['previous_year_rating'] == 1.0) & \n      (train['awards_won?'] == 0) & (train['avg_training_score'] < 40)].index)\n\n# lets check the shape of the train data after deleting the two rows\nprint(\"After Deletion of the above two rows :\", train.shape)","63b32d5d":"# lets check how many of the employees have greater than 30 years of service and still do not get promotion\n\n@interact\ndef check_promotion(x = 20):\n    x = train[(train['length_of_service'] > x)]\n    return x['is_promoted'].value_counts()\n   ","fbd090fc":"# lets start encoding these categorical columns to convert them into numerical columns\n\n# lets encode the education in their degree of importance \ntrain['education'] = train['education'].replace((\"Master's & above\", \"Bachelor's\", \"Below Secondary\"),\n                                                (3, 2, 1))\ntest['education'] = test['education'].replace((\"Master's & above\", \"Bachelor's\", \"Below Secondary\"),\n                                                (3, 2, 1))\n\n# lets use Label Encoding for Gender and Department to convert them into Numerical\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain['department'] = le.fit_transform(train['department'])\ntest['department'] = le.fit_transform(test['department'])\ntrain['gender'] = le.fit_transform(train['gender'])\ntest['gender'] = le.fit_transform(test['gender'])\n\n# lets check whether we still have any categorical columns left after encoding\nprint(train.select_dtypes('object').columns)\nprint(test.select_dtypes('object').columns)","f20eec36":"# lets check the data after encoding\ntrain.head(3)","555c581f":"# lets split the target data from the train data\n\ny = train['is_promoted']\nx = train.drop(['is_promoted'], axis = 1)\nx_test = test\n\n# lets print the shapes of these newly formed data sets\nprint(\"Shape of the x :\", x.shape)\nprint(\"Shape of the y :\", y.shape)\nprint(\"Shape of the x Test :\", x_test.shape)","36239289":"# It is very important to resample the data, as the Target class is Highly imbalanced.\n# Here We are going to use Over Sampling Technique to resample the data.\n# lets import the SMOTE algorithm to do the same.\n\nfrom imblearn.over_sampling import SMOTE\n\nx_resample, y_resample  = SMOTE().fit_sample(x, y.values.ravel())\n\n# lets print the shape of x and y after resampling it\nprint(x_resample.shape)\nprint(y_resample.shape)","fda25e5c":"# lets also check the value counts of our target variable4\n\nprint(\"Before Resampling :\")\nprint(y.value_counts())\n\nprint(\"After Resampling :\")\ny_resample = pd.DataFrame(y_resample)\nprint(y_resample[0].value_counts())","32926e7b":"# lets create a validation set from the training data so that we can check whether the model that we have created is good enough\n# lets import the train_test_split library from sklearn to do that\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_resample, y_resample, test_size = 0.2, random_state = 0)\n\n# lets print the shapes again \nprint(\"Shape of the x Train :\", x_train.shape)\nprint(\"Shape of the y Train :\", y_train.shape)\nprint(\"Shape of the x Valid :\", x_valid.shape)\nprint(\"Shape of the y Valid :\", y_valid.shape)\nprint(\"Shape of the x Test :\", x_test.shape)","d895d7f7":"# lets split the target data from the train data\n\ny = train['is_promoted']\nx = train.drop(['is_promoted'], axis = 1)\nx_test = test\n\n# It is very import to scale all the features of the dataset into the same scale\n# Here, we are going to use the standardization method, which is very commonly used.\n\n# lets import the standard scaler library from sklearn to do that\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_valid = sc.transform(x_valid)\nx_test = sc.transform(x_test)# lets print the shapes of these newly formed data sets\nprint(\"Shape of the x :\", x.shape)\nprint(\"Shape of the y :\", y.shape)\nprint(\"Shape of the x Test :\", x_test.shape)","94a85c75":"\n\n\n# Lets use Logistic Regression to classify the data\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nmodel = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_valid)\n\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Testing Accuracy :\", model.score(x_valid, y_valid))\n\ncm = confusion_matrix(y_valid, y_pred)\nplt.rcParams['figure.figsize'] = (3, 3)\nsns.heatmap(cm, annot = True, cmap = 'Wistia', fmt = '.8g')\nplt.show()","6ecc94e7":"# lets take a look at the Classification Report\n\ncr = classification_report(y_valid, y_pred)\nprint(cr)","a5e0d890":"prediction = model.predict(np.array([[2, #department code\n                                      3, #masters degree\n                                      1, #male\n                                      1, #1 training\n                                      30, #30 years old\n                                      5, #previous year rating\n                                      10, #length of service\n                                      1, #KPIs met >80%\n                                      1, #awards won\n                                      95, #avg training score\n                                      7, #sum of metric \n                                      700 #total score\n                                     ]]))\n\nprint(\"Whether the Employee should get a Promotion : 1-> Promotion, and 0-> No Promotion :\", prediction)","ac229f24":"## Dealing with Categorical Columns\n","ee05f177":"## <center>Data Description<\/center>\n\n<table>\n    <tr>\n        <td><b>Variable<\/b><\/td>\n        <td><b>Definition<\/b><\/td>\n    <\/tr>\n    <tr>\n        <td>employee_id<\/td>\n        <td>Unique ID for employee<td>\n    <\/tr>\n    <tr>\n        <td>department<\/td>\n        <td>Department of employee<\/td>\n    <\/tr>\n    <tr>\n        <td>region<\/td>\n        <td>Region of employment (unordered)<\/td>\n    <\/tr>\n    <tr>\n        <td>education<\/td>\n        <td>Education Level<\/td>\n    <\/tr>\n    <tr>\n        <td>gender<\/td>\n        <td>Gender of Employee<\/td>\n    <\/tr>\n    <tr>\n        <td>recruitment_channel<\/td>\n        <td>Channel of recruitment for employee<\/td>\n    <\/tr>\n    <tr>\n        <td>no_of_trainings<\/td>\n        <td>no of other trainings completed in previous year on soft skills, technical skills etc.<\/td>\n    <\/tr>\n    <tr>\n        <td>age<\/td>\n        <td>Age of Employee<\/td>\n    <\/tr>\n    <tr>\n        <td>previous_year_rating<\/td>\n        <td>Employee Rating for the previous year<\/td>\n    <\/tr>\n    <tr>\n        <td>length_of_service<\/td>\n        <td>Length of service in years<\/td>\n    <\/tr>\n    <tr>\n        <td>KPIs_met >80%<\/td>\n        <td>if Percent of KPIs(Key performance Indicators) >80% then 1 else 0<\/td>\n    <\/tr>\n    <tr>\n        <td>awards_won?<\/td>\n        <td>if awards won during previous year then 1 else 0<\/td>\n    <\/tr>\n    <tr>\n        <td>avg_training_score<\/td>\n        <td>Average score in current training evaluations<\/td>\n    <\/tr>\n    <tr>\n        <td>is_promoted\t(Target)<\/td>\n        <td>Recommended for promotion<\/td>\n    <\/tr>\n<\/table>","9b8c3416":"## Bivariate Analysis","ab3d8a46":"## Resampling","7b9b828c":"## Grouping Operations and Queries","ba7e8217":"## Feature Engineering","0177682c":"## Treating the Missing Values","b877c2ec":"## Univariate Analysis","705ecc9a":"## Importing required libraries","86abaea3":"### Decision Tree Classifier","8e3a4d47":"### Objective: To predict whether an Employee should get a Promotion or Not?\n\n![image](https:\/\/corehr.files.wordpress.com\/2013\/02\/wrong-promotion1.jpg?w=290)","15a3e759":"## RESULT","7c3b5d3a":"## Descriptive Statistics","d1e57ce4":"As, we can clearly see that Decision Tree Classifier works much better as there are so many attributes which are are equally contributing to the cause, and generally linear Models such as Logistic Regression, wont perform very good, also there is a clear result using RFECV for feature selection, we can see that all the features are Important for Building the Model."}}