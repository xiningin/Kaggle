{"cell_type":{"b7fc5622":"code","a0eee2a3":"code","d304709e":"code","35c2ceb7":"code","9cb9ed39":"code","805dfcba":"code","ca1b58e9":"code","8930f51c":"code","d0cccdbf":"code","54c24d83":"markdown","9f83bfa9":"markdown","12e2f396":"markdown","9afdce78":"markdown","9747729b":"markdown","9c678fcf":"markdown","cdef1ba0":"markdown","c009dc59":"markdown","d31099ee":"markdown","ba9df6d0":"markdown"},"source":{"b7fc5622":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a0eee2a3":"df = pd.read_csv('..\/input\/predictingese\/AttendanceMarksSA.csv')\ndf.head()","d304709e":"X= df['MSE']\nY=df['ESE']\nsns.scatterplot(X,Y)","35c2ceb7":"beta0=0\nbeta1=0\nalpha=0.01\ncount =10000\nn=float(len(X))","9cb9ed39":"for i in range(count): \n    Ybar = beta1*X + beta0    \n    beta1 = beta1 - (alpha\/n)*sum(X*(Ybar-Y))\n    beta0 = beta0 - (alpha\/n)*sum(Ybar-Y)\n    \nprint(beta0,beta1)","805dfcba":"Ybar = beta1*X + beta0\n\nplt.scatter(X, Y) \nplt.plot([min(X), max(X)], [min(Ybar), max(Ybar)], color='red')  # regression line\nplt.show()","ca1b58e9":"import math\ndef RSE(y_true, y_predicted):\n   \n    y_true = np.array(y_true)\n    y_predicted = np.array(y_predicted)\n    RSS = np.sum(np.square(y_true - y_predicted))\n\n    rse = math.sqrt(RSS \/ (len(y_true) - 2))\n    return rse\n\n\nrse= RSE(df['ESE'],Ybar)\nprint(rse)","8930f51c":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n ","d0cccdbf":"x = np.array(df['MSE']).reshape(-1,1)\ny = np.array(df['ESE']).reshape(-1,1)\n \n\nlr = LinearRegression()\nlr.fit(x,y)\n\n\nprint(lr.coef_)\nprint(lr.intercept_)\n\nyp = lr.predict(x)\nrse = RSE(y,yp)\n\nprint(rse)\n\n \n","54c24d83":"x = np.array(df['MSE']).reshape(-1,1)\ny = np.array(df['ESE']).reshape(-1,1)\n \n\nlr = LinearRegression()\nlr.fit(x,y)\n\n\nprint(lr.coef_)\nprint(lr.intercept_)\n\nyp = lr.predict(x)\nrse = RSE(y,yp)\n\nprint(rse)\n\n \n","9f83bfa9":"The code segment implements the iterative process of gradient descent algorithm. This code segment calculates the partial derivative of the error function. It minimizes the error function to calculate the values of beta0 and beta1.","12e2f396":"The following code segment plots the identified best fit line or regression line visually.","9afdce78":"We are reusing the same error caluclation RSE to calculate the Residual Standard Error.","9747729b":"In the initialization phase, all the required Python libraries are imported, and the data set has been loaded as the data frame in the memory for further processing. \n\nAlso, the data format is analyzed, and the relationship between the dependent and independent variable is visualized to develop an initial understanding.","9c678fcf":"The following code segments first extract input and output feature vector from the data frame and convert them into the array representation.","cdef1ba0":"The second way to indirectly implement the gradient descent algorithm is to use the LinearRegression module form Scikit-Learn. The SciKit-Learn use Object-Oriented approach to implementing various machine learning algorithms. \n\nHere, I am using LinearRegression() class and the fit() method from the LinearRegression() class assuming it implements the gradient descent algorithm.","c009dc59":"The parameters (i.e., beta0,beta1) and hyper-parameters (e.g., alpha, count) are initialised to zero to initiate the implementation of the gradient descent algorithm.","d31099ee":"**Interpretation of the Result:**\n\nThe Linear Regression model implementd through the gradient descent algorithm  and from the LinearRegression() class from the Scikit-Learn module approximately claculates the same values for slope (beta1) and y-intercept(beta0) and also with the acceptable level of RSE i.e. **4.39.** i.e. the model predicts  end sem exam marks with +\/- 4 error from mid-sem exam.","ba9df6d0":"\n# Simple Linear Regression with Gradient Descent and Scikit-Learn\n\n\nThe purpose of this notebook is to explain the implementation of a simple linear regression model with two different approaches. The first is with the gradient descent algorithm. Second is using linear regression model from the Scikit-Learn module from Python library.\nThis notebook is also the continuation of our previous lab activity of implementing simple linear regression using ordinary least square method.\nI have explained the general intuition and mathematical theory behind the gradient descent algorithm in my Youtube [video](https:\/\/youtu.be\/T-ExXMXQxF0)."}}