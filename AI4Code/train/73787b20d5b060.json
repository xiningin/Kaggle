{"cell_type":{"0375b517":"code","2d284146":"code","01dcc43a":"code","543ee75d":"code","439a68de":"code","1326cee6":"code","68897e14":"code","71a8a981":"code","ffe9f3c3":"code","a176cf31":"code","89d31254":"code","6cef6a9e":"code","6e2356dd":"code","57f3afad":"code","0729101c":"code","534efc2c":"code","75f8e507":"code","d091ea85":"code","0ae87ce0":"code","dfec0488":"code","cb285307":"code","c3b97b13":"code","e28c2a9b":"code","60024ce5":"code","4dece2fa":"code","ae3e897d":"code","493c0176":"code","8ca23af2":"code","40f524ee":"code","809e4208":"code","6f05d037":"code","ba0fc9ae":"code","9c6f9ae3":"code","a438d515":"code","feb42fda":"code","3c7aec5e":"code","784336f5":"code","c44ecb65":"code","b841945b":"code","f49daa4a":"code","7cd37f89":"code","34256cb0":"code","308aa1c9":"code","1b7792ae":"code","61c2c358":"code","13c37182":"code","623dae0d":"code","a0c97bc7":"code","b1e2727d":"code","2db310d0":"code","08c8340a":"code","04cc41ae":"code","9d63c0db":"code","d867edd6":"code","a34570dc":"code","39800592":"code","cbe02460":"code","2798cf71":"code","fa248916":"code","09a6a175":"code","1d8e0ea3":"code","6d17dab2":"code","e9b56be5":"code","b0322b8e":"code","922a0655":"code","38fb1323":"code","746c5df1":"code","8114ed94":"code","719695ac":"code","ae4eb9f2":"code","a5a6adba":"code","751167ca":"code","9b473567":"code","0954616a":"code","3703fb8d":"code","91f29515":"code","a267a36a":"code","910bd147":"code","13b62e03":"code","20fb0243":"code","18372e2e":"code","3f89e175":"code","185a8e25":"code","e50c6b4e":"code","8ab82f60":"code","7d26aea1":"code","6d02ab56":"code","0c8597d9":"code","d0a6517c":"code","046eea91":"code","9674d793":"code","20758172":"code","ced7f955":"code","7d8a0545":"code","386384d3":"code","3d47982e":"code","d3d6928a":"code","8d57f3ee":"code","aa479ffa":"code","4d31efa7":"code","9d5e813b":"code","a94b53b4":"code","e5166276":"code","a5c288fa":"code","26532880":"code","18ecf301":"code","29a0b7d1":"code","55dadb65":"code","3020c01b":"code","38605de3":"code","60e9651f":"code","9bbd9fcc":"code","95db2bae":"code","e6258ff3":"code","bc0d6b2b":"code","093b23ca":"code","e4ccbade":"code","1dfbf2ee":"code","981044f3":"code","4dc09245":"code","502e367e":"code","7aab2e88":"code","c2dacc89":"code","c3dc7840":"code","560f90e0":"code","1069699c":"code","2c388045":"code","7c89e264":"code","9b69d407":"code","a4791860":"code","71004453":"code","f0aba3cf":"code","d8600098":"code","48c27eea":"code","f5f560dd":"code","d1d95367":"code","f85c0c5b":"code","5a1485af":"code","dd21b5d8":"code","f384ea9e":"code","5a2a081c":"code","1f5b8067":"code","889bdfa9":"code","974b9e0e":"code","de2d3bf1":"code","6688380a":"code","099810dc":"code","82ccf158":"code","3fff8abd":"code","4e3cc9b0":"code","cdd10591":"code","7119d24c":"code","b057b298":"code","8de397c9":"code","6f9ceff3":"code","160fa5c0":"code","f84e17af":"code","3c64e8fa":"code","66d2e81f":"code","9e494876":"code","2c016e8f":"code","df70c6b2":"code","7fed9238":"code","5568cee3":"code","d55c13ff":"code","b9cb511f":"code","5e7246cc":"code","36fba782":"code","2cd2bc10":"code","dff2473b":"code","09f0d39f":"code","90e51a4c":"code","6335d9bc":"code","0e2e7d0a":"code","153b7a17":"code","d6b5af48":"code","58f0572d":"code","0d9a45c3":"code","2eec8a3f":"code","59e3fea8":"code","956be452":"code","0955adea":"code","d58f4eae":"code","79e220c6":"code","f710f13b":"code","28ee11d7":"code","f400b44f":"code","de4ea689":"code","62ae924c":"code","7c29d918":"code","f4e61859":"code","290af745":"code","51a57c8f":"code","f75b9d57":"code","6cb3ba4f":"code","b71a065c":"code","a83dbe55":"code","a7dc2ef8":"code","9c61bc6c":"code","277f9179":"code","27118936":"code","24b0f070":"code","3bcd216b":"code","f8be1589":"code","2b760483":"code","16da541d":"code","b54cb7ed":"code","d47a794a":"code","6ed03662":"code","d0b86245":"code","2b9f39f1":"code","f0733a3a":"markdown","01bdfc2a":"markdown","94230b5f":"markdown","429ba818":"markdown","371269a4":"markdown","688e953b":"markdown","5f29a170":"markdown","1ca68a6b":"markdown","80f6c12d":"markdown","1ffbee19":"markdown","53a1c181":"markdown","63cb7224":"markdown","91379f71":"markdown","b71de51a":"markdown","3b2581c1":"markdown","a3557625":"markdown","f1d7d3c3":"markdown","e21952c8":"markdown","41064c9c":"markdown","bfa2ce01":"markdown","0f89cc75":"markdown","cda82e16":"markdown","6a6b5ffc":"markdown","43516fb8":"markdown","06993ff8":"markdown","e788bdb2":"markdown","7137299c":"markdown","51d5cd53":"markdown","1be7cb78":"markdown","edd784a2":"markdown","b9b57f9e":"markdown","ff7731a2":"markdown","12425c58":"markdown","11394b55":"markdown","ace35e55":"markdown","2ba3075c":"markdown","ec6e10fc":"markdown","8d560730":"markdown","3171c6ad":"markdown","1e94a342":"markdown","1e5b75e7":"markdown","51e9c979":"markdown","9270d2a9":"markdown","a536bb09":"markdown","2eabe64c":"markdown","741eae65":"markdown","38698cdc":"markdown","38275a26":"markdown","8c93a612":"markdown","03b4b6a0":"markdown","0a8f33e4":"markdown","606ffc15":"markdown","3dea22a5":"markdown","58bbc486":"markdown","0bb07fcc":"markdown","a3f4ea53":"markdown","3d451fff":"markdown","88e1fdea":"markdown","8d014639":"markdown","5f30eba7":"markdown","6ee67ca2":"markdown","b0e62912":"markdown","1e33126d":"markdown","23aa8388":"markdown","72e3468c":"markdown","cd78a0b0":"markdown","9a39889b":"markdown","45e4bbcc":"markdown","3b9978aa":"markdown","9c04cf54":"markdown","699eac43":"markdown","d26882a2":"markdown","738a518e":"markdown","0bd59788":"markdown","03c382c0":"markdown","df7a98c0":"markdown","ec29cb55":"markdown","4e34d55b":"markdown","0ed753cf":"markdown","657a4d7a":"markdown","8d0f44a6":"markdown","a3cec908":"markdown","2996920f":"markdown","05c96fb4":"markdown","3ce1c8c5":"markdown","e9b0bada":"markdown","d398fd1e":"markdown","5e6cac36":"markdown","00c4eb17":"markdown","911cac17":"markdown","07dd9d97":"markdown","09e186fb":"markdown","3f8027c1":"markdown","03372386":"markdown","a928351b":"markdown","8edcd6ca":"markdown","ad86803f":"markdown","9b0d56d9":"markdown","ea9f863a":"markdown","e8565f28":"markdown","06387ebc":"markdown","98c9f25c":"markdown"},"source":{"0375b517":"from matplotlib import pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.autograd import Variable\nimport torch","2d284146":"from sklearn.metrics import f1_score\ndef F1score(actuals, preds):\n    \"\"\"\n    To get F1 score (macro) for our predictions.\n    -----------------------------------------------------------\n    Parameters:\n        preds: Array of Predicted values\n        actuals: Array of Actual labels\n    Output:\n        Return F1 score (macro)\n    \"\"\"\n    return f1_score(actuals, preds, average = 'macro')","01dcc43a":"PATH = \"..\/input\/\"\n\ntrain = pd.read_csv(f'{PATH}train.csv')\ntest = pd.read_csv(f'{PATH}test.csv')\n\ntrain.info()","543ee75d":"obj_cols = train.columns[train.dtypes == \"object\"]; obj_cols","439a68de":"train[obj_cols].describe()","1326cee6":"# For saving space and compute time (mostly comparison for these)\nfrom sklearn.preprocessing import LabelEncoder\n# We will have to use two different label encoders. One for 'Id' and other for 'idhogar'.\nlb1 = LabelEncoder()\nlb1.fit(list(train['Id'].values))\nlb2 = LabelEncoder()\nlb2.fit(list(train['idhogar'].values))\n# Now we will replace each unique id's with a unique number.\ntrain['Id'] = lb1.transform(list(train['Id'].values))\ntrain['idhogar'] = lb2.transform(list(train['idhogar'].values))\n\nlb3 = LabelEncoder()\nlb3.fit(list(test['Id'].values))\nlb4 = LabelEncoder()\nlb4.fit(list(test['idhogar'].values))\n# Now we will replace each unique id's with a unique number.\ntest['Id'] = lb3.transform(list(test['Id'].values))\ntest['idhogar'] = lb4.transform(list(test['idhogar'].values))","68897e14":"train['dependency'].unique()  # rate dependency  (yes:1, no:0)","71a8a981":"train['dependency'].replace('yes', '1', inplace=True)\ntrain['dependency'].replace('no', '0', inplace=True)\ntrain['dependency'].astype(np.float64);","ffe9f3c3":"test['dependency'].replace('yes', '1', inplace=True)\ntest['dependency'].replace('no', '0', inplace=True)\ntest['dependency'].astype(np.float64);","a176cf31":"train['edjefe'].unique()  # years of education of male head of household  (given, yes:1, no:0)","89d31254":"train['edjefe'].replace('yes', '1', inplace=True)\ntrain['edjefe'].replace('no', '0', inplace=True)\ntrain['edjefe'].astype(np.float64);","6cef6a9e":"test['edjefe'].replace('yes', '1', inplace=True)\ntest['edjefe'].replace('no', '0', inplace=True)\ntest['edjefe'].astype(np.float64);","6e2356dd":"train['edjefa'].unique()  # years of education of female head of household  (given, yes:1, no:0)","57f3afad":"train['edjefa'].replace('yes', '1', inplace=True)\ntrain['edjefa'].replace('no', '0', inplace=True)\ntrain['edjefa'].astype(np.float64);","0729101c":"test['edjefa'].replace('yes', '1', inplace=True)\ntest['edjefa'].replace('no', '0', inplace=True)\ntest['edjefa'].astype(np.float64);","534efc2c":"null_counts = train.isnull().sum()\nnull_counts[null_counts>0]","75f8e507":"test_null_counts = test.isnull().sum()\ntest_null_counts[test_null_counts>0]","d091ea85":"cols = ['Id', 'parentesco1', 'v2a1', 'v18q', 'hacapo', 'rooms', 'r4t3', 'hhsize', 'escolari', 'epared2',\n        'epared3', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', 'Target']","0ae87ce0":"v2a1_null = train.query('v2a1 == \"NaN\"')[cols]; v2a1_null.shape","dfec0488":"# Let us get the family heads of each household in this\nv2a1_null_heads = v2a1_null.query('parentesco1 == 1'); v2a1_null_heads.shape","cb285307":"v2a1_null_heads.query('hacapo != 1').shape, v2a1_null_heads.query('hacapo == 1').shape","c3b97b13":"v2a1_null.query('Target == 1').shape, v2a1_null.query('Target == 2').shape","e28c2a9b":"v2a1_null_heads.query('epared2 != 1 & epared3 != 1').shape # Families who don't have regular or good walls","60024ce5":"v2a1_null_heads.query('tipovivi1 != 1').shape # Families who don't own thier own home.","4dece2fa":"v2a1_null_heads.query('tipovivi2 == 1').shape, v2a1_null_heads.query('tipovivi3 == 1').shape, v2a1_null_heads.query('tipovivi4 == 1').shape, v2a1_null_heads.query('tipovivi5 == 1').shape","ae3e897d":"v2a1_null_heads.query('tipovivi1 != 1 & tipovivi2 != 1 & tipovivi3 != 1 & tipovivi4 != 1 & tipovivi5 != 1') \n# Checking for any wrong data","493c0176":"v2a1_null_heads.query('(tipovivi4 == 1 | tipovivi5 == 1) & Target == 1').shape, v2a1_null_heads.query('(tipovivi4 == 1 | tipovivi5 == 1) & Target == 2').shape, v2a1_null_heads.query('(tipovivi4 == 1 | tipovivi5 == 1) & Target == 3').shape","8ca23af2":"train.loc[train['v2a1'].isnull() & train['tipovivi1'] == 1, 'v2a1'] = 0","40f524ee":"test.loc[test['v2a1'].isnull() & test['tipovivi1'] == 1, 'v2a1'] = 0","809e4208":"train.query('v2a1 == \"NaN\"').shape, test.query('v2a1 == \"NaN\"').shape","6f05d037":"train.query('v2a1 != \"NaN\"')['v2a1'].describe()","ba0fc9ae":"a, b = train.query('Target == 1 & v2a1 != \"NaN\"')['v2a1'].mean(), train.query('Target == 2 & v2a1 != \"NaN\"')['v2a1'].mean(); a, b","9c6f9ae3":"c, d = train.query('Target == 3 & v2a1 != \"NaN\"')['v2a1'].mean(), train.query('Target == 4 & v2a1 != \"NaN\"')['v2a1'].mean(); c, d","a438d515":"train.loc[train['v2a1'].isnull() & (train['Target']== 1), 'v2a1'] = a\ntrain.loc[train['v2a1'].isnull() & (train['Target']== 2), 'v2a1'] = b\ntrain.loc[train['v2a1'].isnull() & (train['Target']== 3), 'v2a1'] = c\ntrain.loc[train['v2a1'].isnull() & (train['Target']== 4), 'v2a1'] = d\ntrain.loc[train['v2a1'].isnull()]","feb42fda":"test.loc[test['v2a1'].isnull(), 'v2a1'] = (a+b+c+d)\/4  # We cannot check Target value here\ntest.loc[test['v2a1'].isnull()]","3c7aec5e":"v18q1_null = train.query('v18q1 == \"NaN\"'); v18q1_null.shape","784336f5":"h_ids = v18q1_null['idhogar'].unique(); h_ids.shape","c44ecb65":"# For every household we will calulate how many of them owns a tablet and put 'v18q1' equal to that sum\nfor idn in h_ids:\n    train.loc[(train['idhogar'] == idn), 'v18q1'] = train.query(f'idhogar == {idn}')['v18q'].sum()","b841945b":"test_v18q1_null = test.query('v18q1 == \"NaN\"')\nh_ids = test_v18q1_null['idhogar'].unique()","f49daa4a":"for idn in h_ids:\n    test.loc[(test['idhogar'] == idn), 'v18q1'] = test.query(f'idhogar == {idn}')['v18q'].sum()","7cd37f89":"cols = ['Id', 'idhogar', 'escolari', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4',\n        'estadocivil5', 'estadocivil6', 'estadocivil7', 'instlevel1', 'age', 'Target']\ncols2 = cols[:-1]","34256cb0":"rez_esc_null = train.query('rez_esc == \"NaN\"')[cols]\ntest_rez_esc_null = test.query('rez_esc == \"NaN\"')[cols2]; rez_esc_null.shape, test_rez_esc_null.shape","308aa1c9":"rez_esc_null.query('instlevel1 == 1').shape, test_rez_esc_null.query('instlevel1 == 1').shape","1b7792ae":"train.loc[(train['rez_esc'].isnull()) & (train['instlevel1'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['instlevel1'] == 1), 'rez_esc'] = 0","61c2c358":"rez_esc_null = train.query('rez_esc == \"NaN\"')[cols]\ntest_rez_esc_null = test.query('rez_esc == \"NaN\"')[cols2]; rez_esc_null.shape, test_rez_esc_null.shape","13c37182":"# estadocivil1: =1 if less than 10 years old\nrez_esc_null.query('estadocivil1 == 1').shape, test_rez_esc_null.query('estadocivil1 == 1').shape","623dae0d":"rez_esc_null.query('estadocivil2 == 1').shape, rez_esc_null.query('estadocivil3 == 1').shape, rez_esc_null.query('estadocivil4 == 1').shape","a0c97bc7":"test_rez_esc_null.query('estadocivil2 == 1').shape, test_rez_esc_null.query('estadocivil3 == 1').shape, test_rez_esc_null.query('estadocivil4 == 1').shape","b1e2727d":"rez_esc_null.query('estadocivil5 == 1').shape, rez_esc_null.query('estadocivil6 == 1').shape, rez_esc_null.query('estadocivil7 == 1').shape","2db310d0":"test_rez_esc_null.query('estadocivil5 == 1').shape, test_rez_esc_null.query('estadocivil6 == 1').shape, test_rez_esc_null.query('estadocivil7 == 1').shape","08c8340a":"a = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil2'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nb = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil3'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nc = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil4'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nd = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil5'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\ne = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil6'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\nf = train.loc[(~train['rez_esc'].isnull()) & (train['estadocivil7'] == 1) & (train['escolari'] > 0)]['rez_esc'].mean()\na, b, c, d, e, f\n\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil2'] == 1), 'rez_esc'] = 3\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil3'] == 1), 'rez_esc'] = 0\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil4'] == 1), 'rez_esc'] = 0\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil5'] == 1), 'rez_esc'] = 2\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil6'] == 1), 'rez_esc'] = 0\ntrain.loc[(train['rez_esc'].isnull()) & (train['estadocivil7'] == 1), 'rez_esc'] = 1\n\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil2'] == 1), 'rez_esc'] = 3\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil3'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil4'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil5'] == 1), 'rez_esc'] = 2\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil6'] == 1), 'rez_esc'] = 0\ntest.loc[(test['rez_esc'].isnull()) & (test['estadocivil7'] == 1), 'rez_esc'] = 1","04cc41ae":"train['rez_esc'].isnull().sum(), test['rez_esc'].isnull().sum()","9d63c0db":"cols = ['Id', 'idhogar', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5',\n       'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9']","d867edd6":"# We will just put this value equal to avg year of education of adults with the help of selected cols\nmeaneduc_null = train.query('meaneduc == \"NaN\"')[cols]\ntest_meaneduc_null = test.query('meaneduc == \"NaN\"')[cols]\nh_ids = meaneduc_null['idhogar'].unique(); h_ids","a34570dc":"print(train.loc[(train['idhogar'] ==  326), 'meaneduc'].values)\nprint(train.loc[(train['idhogar'] == 1959), 'meaneduc'].values) \nprint(train.loc[(train['idhogar'] == 2908), 'meaneduc'].values)","39800592":"def meaneduc_correction(null_view, df, hids):\n    \"\"\"\n    Function to correct null_values in \"meaneduc\" feature. Will put them equal to mean, after calculating it\n    using \"instlevel\"'s.\n    ---------------------------------------------------------------------------------------------------------\n    Parameters:\n        null_view: View of origianl dataframe with null values of \"meaneduc\"\n        df: Original DataFrame\n        hids: Unique Household ids of households with null \"meaneduc\"\n    \"\"\"\n    for idn in hids:\n        # Number of people with no education and so on\n        a = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel1'] == 1)].shape[0] # No ed\n        b = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel2'] == 1)].shape[0] # Inc. prim\n        c = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel3'] == 1)].shape[0] # Com. prim\n        d = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel4'] == 1)].shape[0] # Inc Acad Sec L.\n        e = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel5'] == 1)].shape[0] # Com Acad Sec L.\n        f = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel6'] == 1)].shape[0] # Inc Tech Sec L.\n        g = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel7'] == 1)].shape[0] # Com Tech Sec L.\n        h = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel8'] == 1)].shape[0] # UndGrad n HigEd\n        i = null_view.loc[(null_view['idhogar']==idn) & (null_view['instlevel9'] == 1)].shape[0] # Postgrad\n\n        mean_educ = (a*0 + b*4 + c*8 + d*2 + e*4 + f + g*2 + h*4 + i) \/ (a+b+c+d+e+f+g+h+i)\n\n        df.loc[(df['meaneduc'].isnull()) & (df['idhogar'] == idn), 'meaneduc'] =  mean_educ\n        df.loc[(df['SQBmeaned'].isnull()) & (df['idhogar'] == idn), 'SQBmeaned'] =  mean_educ**2","cbe02460":"meaneduc_correction(meaneduc_null, train, h_ids)","2798cf71":"null_counts = train.isnull().sum()\nnull_counts[null_counts>0]","fa248916":"h_ids = test_meaneduc_null['idhogar'].unique(); h_ids","09a6a175":"meaneduc_correction(test_meaneduc_null, test, h_ids)","1d8e0ea3":"test_null_counts = test.isnull().sum()\ntest_null_counts[test_null_counts>0]","6d17dab2":"train.shape, test.shape","e9b56be5":"train['Id'].unique().size, test['Id'].unique().size  # So, Ids are unique","b0322b8e":"# Now for the second part\ncols = ['v2a1', 'hacdor', 'rooms', 'hacapo', 'v14a', 'refrig', 'v18q1',\n       'r4h3', 'r4m3', 'r4t3', 'tamhog', 'tamviv', 'hhsize', 'paredblolad',\n       'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc',\n       'paredfibras', 'paredother', 'pisomoscer', 'pisocemento', \n       'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera', 'techozinc',\n       'techoentrepiso', 'techocane', 'techootro', 'cielorazo',\n       'abastaguadentro', 'abastaguafuera', 'abastaguano', 'public', \n       'planpri', 'noelec', 'coopele', 'sanitario1', 'sanitario2', \n       'sanitario3', 'sanitario5', 'sanitario6', \n       'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',\n       'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5',\n       'elimbasu6', 'epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', \n       'etecho3', 'eviv1', 'eviv2', 'eviv3', 'hogar_nin', 'hogar_adul',\n       'hogar_mayor', 'hogar_total', 'dependency', 'meaneduc', 'bedrooms', \n       'overcrowding', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4',\n       'tipovivi5', 'computer', 'television', 'qmobilephone', 'lugar1',\n       'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'Target']","922a0655":"import warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\n\ndef check_for_wrong_data(data, columns, labelE, gpby='idhogar'):\n    \"\"\"\n    Checks for data mismatches in rows of every group, which we get by gpby (groupby)\n    feature, on columns in \"columns\". If mismatch is there, put it equal to value in\n    columns of head of the household and print a message for this.\n    ----------------------------------------------------------------------------------\n    Input:\n        data : Train or Test set or their sliced views\n        columns : columns to check for corrupted data\n        labelE : Label encoder of \"data\"'s  \"idhogar\" column used in inverse_transform \n        gpby : feature to group by to check for diff \"cols\" in that group\n    Output:\n        Return four arrays:\n        1) Array with ids of households with no head\n        2) Array with ids of households with wrong data\n        3) Array of arrays with column name with wrong data for each household in (2)nd array\n        4) Array of arrays with ids of members with wrong data for each household in (2)nd array\n    \"\"\"\n    id_head_zero = [] # Will contain house ids with no head\n    idhogarId_f = []\n    cols_f = []\n    mem_f = []\n    grouped = data.groupby(gpby, sort=True)\n    for gid in range(len(grouped)):\n        members = grouped.get_group(gid)\n        h_Head = members.loc[(members['parentesco1'] == 1)]\n        if h_Head.shape[0] == 0:\n            id_head_zero.append(members['idhogar'].values[0])\n            continue\n        idhogarId_w = []\n        cols_t = []\n        mem_t = []\n        if members.shape[0] > 1:\n            for col in columns:\n                for m in members.iterrows():\n                    if h_Head[col].values[0] != m[1][col]:\n                        if h_Head['idhogar'].values[0] not in idhogarId_w : idhogarId_w.append(h_Head['idhogar'].values[0])\n                        if col not in cols_t : cols_t.append(col)\n                        if m[1]['Id'] not in mem_t : mem_t.append(m[1]['Id'])\n                        # Correct this column\n                        data.loc[(train['Id'] == m[1]['Id']), col] = h_Head[col].values[0]\n        idhogarId_f.append(idhogarId_w); cols_f.append(cols_t); mem_f.append(mem_t)\n        if len(idhogarId_w) > 0:\n            for i in range(len(idhogarId_w)):\n                print(\"Household with Id: \"\n                +str(labelE.inverse_transform([idhogarId_w[i]])[0])\n                +\" has \" + str(len(mem_t)) + \" member(s) with diff. value(s) of \" + str(len(cols_t)) + \" column(s).\"\n                + \" \" + str(cols_t) )\n    return id_head_zero, idhogarId_f, cols_f, mem_f","38fb1323":"id_head_zero, *_ = check_for_wrong_data(train, cols, lb2)","746c5df1":"train.loc[(train['idhogar'] == id_head_zero[11])]   # 4, 6, 7, 8, 11 have more than 1 persons in home but no head","8114ed94":"cols = cols[:-1] # Remove \"Target\"","719695ac":"id_head_zero, *_ = check_for_wrong_data(test, cols, lb4)","ae4eb9f2":"import seaborn as sns\ncolumns = train.select_dtypes('number').drop(['Id', 'idhogar', 'Target'], axis=1).columns\n\nfig, axes = plt.subplots(nrows=4, ncols=1, figsize=(20, 15))\nfig.subplots_adjust(top=1.3)\n#train.loc[:,columns[1:22]].boxplot(ax=axes[0])\na = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[1:22]]), ax=axes[0])\nb = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[22:70]]), ax=axes[1])\nb.set_xticklabels(rotation=30, labels = columns[22:70]);\nc = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[70:98]]), ax=axes[2])\nc.set_xticklabels(rotation=30, labels = columns[70:120]);\nd = sns.boxplot(x = \"variable\", y = \"value\", data = pd.melt(train.loc[:,columns[99:120]]), ax=axes[3])","a5a6adba":"possible_outliers = [columns[0]] + [columns[98]]; columns[0], columns[98]","751167ca":"sns.boxplot(data = train[possible_outliers[0]])","9b473567":"train.loc[(train['v2a1'] > 300000), ['idhogar', 'v2a1', 'Target']].query(\"Target != 4\")  # Actually, all above 300,000 are from \"Target\" of 4","0954616a":"train.loc[(train['v2a1'] > 2000000), ['idhogar', 'v2a1', 'Target']]   # So leave it","3703fb8d":"sns.boxplot(data = train[possible_outliers[1]])","91f29515":"train.loc[(train['meaneduc'] > 25), ['idhogar', 'Target', 'meaneduc']].query(\"Target != 4\")","a267a36a":"columns = ['v2a1', 'rooms', 'tamhog', 'overcrowding', 'v18q1', 'r4t3', 'meaneduc', 'qmobilephone', 'Target']","910bd147":"sns.pairplot(train[columns])","13b62e03":"train.drop(['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding',\n            'SQBdependency', 'SQBmeaned', 'agesq'], axis = 1, inplace=True)","20fb0243":"test.drop(['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding',\n            'SQBdependency', 'SQBmeaned', 'agesq'], axis = 1, inplace=True)","18372e2e":"# Plotting a heat map\nimport seaborn as sns\nplt.subplots(figsize=(20,15))\nsns.heatmap(train.corr().abs(), cmap=\"BuPu\")","3f89e175":"DropCols = ['energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2', 'epared1', 'epared2', 'epared3',\n        'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', #'instlevel1', 'instlevel2', 'instlevel3', \n        #'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n        'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5']\n\ntrain['CookingType'] = np.argmax(np.array(train[[ 'energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2' ]]), axis=1)\ntrain['WallType'] = np.argmax(np.array(train[[ 'epared1', 'epared2', 'epared3' ]]), axis=1)\ntrain['RoofType'] = np.argmax(np.array(train[[ 'etecho1', 'etecho2', 'etecho3' ]]), axis=1)\ntrain['FloorType'] = np.argmax(np.array(train[[ 'eviv1', 'eviv2', 'eviv3' ]]), axis=1)\n# EdLevel is being removed during deletion of highly correlated features\n# train['EdLevel'] = np.argmax(np.array(train[ [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9' ]]), axis=1)\ntrain['HouseType'] = np.argmax(np.array(train[[ 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5' ]]), axis=1)\n\ntest['CookingType'] = np.argmax(np.array(test[[ 'energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2' ]]), axis=1)\ntest['WallType'] = np.argmax(np.array(test[[ 'epared1', 'epared2', 'epared3' ]]), axis=1)\ntest['RoofType'] = np.argmax(np.array(test[[ 'etecho1', 'etecho2', 'etecho3' ]]), axis=1)\ntest['FloorType'] = np.argmax(np.array(test[[ 'eviv1', 'eviv2', 'eviv3' ]]), axis=1)\n# EdLevel is being removed during deletion of highly correlated features\n# test['EdLevel'] = np.argmax(np.array(test[[ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9' ]]), axis=1)\ntest['HouseType'] = np.argmax(np.array(test[[ 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5' ]]), axis=1)\n\ntrain.drop(DropCols, axis=1, inplace=True)\ntest.drop(DropCols, axis=1, inplace=True)\ntest.shape, train.shape","185a8e25":"# Per member features\ntrain['phones-per-mem'] = train['qmobilephone'] \/ train['tamviv']\ntrain['tablets-per-mem'] = train['v18q1'] \/ train['tamviv']\ntrain['rooms-per-mem'] = train['rooms'] \/ train['tamviv']\ntrain['rent-per-adult'] = train['v2a1'] \/ train['hogar_adul']\n\ntest['phones-per-mem'] = test['qmobilephone'] \/ test['tamviv']\ntest['tablets-per-mem'] = test['v18q1'] \/ test['tamviv']\ntest['rooms-per-mem'] = test['rooms'] \/ test['tamviv']\ntest['rent-per-adult'] = test['v2a1'] \/ test['hogar_adul']","e50c6b4e":"def chk_n_remove_corr(df):\n    \"\"\"\n    Checks for highly correlated features and removes them.\n    ---------------------------------------------------------------------\n    Parameters:\n        df: Dataframe to check for correlation\n    Output:\n        Return list of removed features\/columns.\n    \"\"\"\n    corr_matrix = train.corr()\n    \n    # Taking only the upper triangular part of correlation matrix: (We want to remove only one of corr features)\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find index of feature columns with correlation greater than 0.975\n    to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.975)]\n    \n    train.drop(to_drop, axis=1, inplace=True)\n    return to_drop","8ab82f60":"to_drop = chk_n_remove_corr(train)\nto_drop","7d26aea1":"test.drop(to_drop, axis=1, inplace=True)\ntrain.shape, test.shape","6d02ab56":"hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6',\n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1','tamviv','hogar_nin',# 'hhsize', 'tamhog',\n              'CookingType', 'WallType', 'RoofType', 'HouseType' , 'FloorType', \n              'hogar_adul','hogar_mayor',  'bedrooms', 'qmobilephone'] # ,'hogar_total']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding',\n          'phones-per-mem', 'tablets-per-mem', 'rooms-per-mem', 'rent-per-adult']\n\nind_bool = ['v18q', 'dis', 'male', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5',\n            'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',\n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'mobilephone']\n\nind_ordered = ['age', 'escolari', 'rez_esc']#, 'EdLevel']","0c8597d9":"train[hh_bool + ind_bool] = train[hh_bool + ind_bool].astype(bool)\ntest[hh_bool + ind_bool] = test[hh_bool + ind_bool].astype(bool)","d0a6517c":"train[hh_cont] = train[hh_cont].astype('float64')\ntest[hh_cont] = test[hh_cont].astype('float64');","046eea91":"train[hh_ordered + ind_ordered] = train[hh_ordered + ind_ordered].astype(int)\ntest[hh_ordered + ind_ordered] = test[hh_ordered + ind_ordered].astype(int);","9674d793":"train['Target'] = train['Target'].astype(int);","20758172":"train.isnull().sum()[train.isnull().sum() > 0], test.isnull().sum()[test.isnull().sum() > 0]","ced7f955":"# Only one value in train and 10 in test\ntrain.loc[train['rent-per-adult'].isnull(), 'rent-per-adult'] = train.loc[train['rent-per-adult'].isnull(), 'v2a1'].values[0]\ntest.loc[test['rent-per-adult'].isnull(), 'rent-per-adult'] = test.loc[test['rent-per-adult'].isnull(), 'v2a1'].values[0]","7d8a0545":"train.isnull().sum()[train.isnull().sum() > 0], test.isnull().sum()[test.isnull().sum() > 0]","386384d3":"for c in train.columns:\n    if train[c].dtype != 'float64': continue\n    s = np.where(train[c].values >= np.finfo(np.float32).max)\n    if len(s[0])>0:\n        print(c)\n        print(s)","3d47982e":"train[train['rent-per-adult'] > np.finfo(np.float32).max][['rent-per-adult']]","d3d6928a":"train[train['rent-per-adult'] > np.finfo(np.float32).max][['Id', 'idhogar', 'v2a1', 'hogar_adul', 'age']]","8d57f3ee":"train[train['idhogar']==1959][['idhogar', 'v2a1', 'age']]","aa479ffa":"train[train['idhogar']==2908][['idhogar', 'v2a1', 'age']]","4d31efa7":"h_ids = train[train['rent-per-adult'] > np.finfo(np.float32).max]['idhogar'].unique()","9d5e813b":"for h_id in h_ids:\n    rent_per_adul = train.loc[(train['idhogar']==h_id), 'v2a1'].values[0] \/ train.loc[(train['idhogar']==h_id)].shape[0]\n    # Assuming the rent is being divided among them equally\n    train.loc[train['idhogar']==h_id, 'rent-per-adult'] = rent_per_adul\n    train.loc[train['idhogar']==h_id, 'rent-per-adult_sum'] = train.loc[(train['idhogar']==h_id), 'v2a1'].values[0]","a94b53b4":"for c in test.columns:\n    if test[c].dtype != 'float64': continue\n    s = np.where(test[c].values >= np.finfo(np.float32).max)\n    if len(s[0])>0:\n        print(c)\n        print(s)","e5166276":"test[test['rent-per-adult'] > np.finfo(np.float32).max][['rent-per-adult']]","a5c288fa":"test[test['rent-per-adult'] > np.finfo(np.float32).max][['Id', 'idhogar', 'v2a1', 'hogar_adul', 'age']]","26532880":"h_ids = test[test['rent-per-adult'] > np.finfo(np.float32).max]['idhogar'].unique()\n\nfor h_id in h_ids:\n    rent_per_adul = test.loc[(test['idhogar']==h_id), 'v2a1'].values[0] \/ test.loc[(test['idhogar']==h_id)].shape[0]\n    # Assuming the rent is being divided among them equally\n    test.loc[test['idhogar']==h_id, 'rent-per-adult'] = rent_per_adul\n    test.loc[test['idhogar']==h_id, 'rent-per-adult_sum'] = test.loc[(test['idhogar']==h_id), 'v2a1'].values[0]","18ecf301":"def make_new_features_grouping(df, dtypes, gpby, customAggFunc=None):\n    \"\"\"\n    Make new features aggregating on groups found by \"gbpy\".\n    -----------------------------------------------------------------\n    Parameters:\n        df: Dataset for which new features are to be made\n        dtypes: Data Types of features which will be used to create new features (string, type or array)\n                eg: bool, 'number', 'float' etc\n        gbpy: Feature on which grouping will be done\n        customAggFunc: A custom Aggregation function or a list of such functions\n    Output: \n        Returns Original DataFrame with new features\n    \"\"\"\n    # Grouping\n    if 'Target' in df.columns: numeric_type = df.select_dtypes(dtypes).drop(['Target', 'Id'], axis=1).copy()\n    else: numeric_type = df.select_dtypes(dtypes).drop(['Id'], axis=1).copy()\n    \n    funcs = ['count', 'mean', 'max', 'min', 'sum', 'std', 'var', 'quantile']\n    \n    if customAggFunc is None: new = numeric_type.groupby(gpby).agg(funcs)\n    elif isinstance(customAggFunc, list): new = numeric_type.groupby(gpby).agg(funcs + customAggFunc)\n    else: new = numeric_type.groupby(gpby).agg(funcs + [customAggFunc])\n    \n    # Rename all columns and remove levels\n    columns = []\n    for old_col in new.columns.levels[0]:\n        if old_col != 'idhogar':\n            for new_col in new.columns.levels[1]:\n                columns.append(old_col + '_' + new_col)\n    new.columns = columns\n    \n    return df.merge(new.reset_index(), on=\"idhogar\", how='left')","29a0b7d1":"train.shape, test.shape","55dadb65":"%time train = make_new_features_grouping(train, ['number', bool], \"idhogar\")\n%time test = make_new_features_grouping(test, [\"number\", bool], \"idhogar\")","3020c01b":"train.shape, test.shape","38605de3":"train.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)","60e9651f":"to_drop = chk_n_remove_corr(train)\nlen(to_drop), 'Target' in to_drop","9bbd9fcc":"test.drop(to_drop, axis=1, inplace=True)\ntrain.shape, test.shape","95db2bae":"from sklearn.preprocessing import RobustScaler\nscaler1 = RobustScaler()\nscaler2 = RobustScaler()\n\nscaled1 = scaler1.fit_transform(train.drop(['Target', 'Id', 'idhogar'], axis=1))\nscaled2 = scaler2.fit_transform(test.drop(['Id', 'idhogar'], axis=1))\n\ncols1 = train.drop(['Target', 'Id', 'idhogar'], axis=1).columns\ncols2 = test.drop(['Id', 'idhogar'], axis=1).columns\n\ntrPCA = pd.DataFrame(scaled1, index=np.arange(train.shape[0]), columns=cols1)\ntsPCA = pd.DataFrame(scaled2, index=np.arange(test.shape[0]), columns=cols2)","e6258ff3":"from sklearn.decomposition import PCA\npca = PCA(n_components=5, svd_solver='full')","bc0d6b2b":"transformed1 = pca.fit_transform(trPCA)\ntransformed2 = pca.transform(tsPCA)\nfor i in range(5):\n    train[f'PCA{i+1}'] = transformed1[:,i]\n    test[f'PCA{i+1}'] = transformed2[:,i]\ntrain.shape, test.shape","093b23ca":"pca.explained_variance_ratio_","e4ccbade":"train['Target'].value_counts().plot.barh()","1dfbf2ee":"train['Target'].value_counts(), 774+1221+1558+1500","981044f3":"rows1 = (train['Target'] == 1)\nrows2 = (train['Target'] == 2)\nrows3 = (train['Target'] == 3)\nrows123 = (rows1 | rows2 | rows3)\nrows4 = (train['Target'] == 4)\nrows123.sum(), rows4.sum()","4dc09245":"# We will take only first count1+count2 rows. Where count1 will go to train and count2 will go to validation set.\ndef train_val_split(rows, tvlen=None, vper=None):\n    \"\"\"\n    Takes in \"row\" array which is location matrix for specific category(say) and \n    divides it into \"train row\" and \"validation row\" of locations. If you only want\n    limited rows from \"rows\" then specify tvlen, which is a tuple of number of rows\n    you want in train and validaion set.\n    -----------------------------------------------------------------------------------\n    Parameters:\n        rows = An array of specific selected rows. (Where ith row is true if selected)\n        tvlen = An array or a tuple of number of elements in train and val. set\n        vper = perecent of elements you want in Validation set (Use it if you want all rows \n                to be divided into test and val sets from the \"rows\" Array or pd.Series)\n    Output:\n        Returns two Arrays or pd.Series of selected rows for train and validation set\n        where ith element is True if that row is selected.\n    \"\"\"\n    if tvlen is not None and vper is None:\n        count1 = tvlen[0]\n        count2 = tvlen[1]\n    elif tvlen is None and vper is not None:\n        c = rows.sum()\n        count1 = int((1-vper)*c)\n        count2 = int(vper*c)\n    else:\n        raise Exception('One of \"tvlen\" or \"vper\" should be given.')\n    \n    rowst, rowsv = rows.copy(), rows.copy()\n    \n    for i in range(len(rows)):\n        \n        # If we have taken count1 rows in training set, put all values equal to False. (after, count1 == 0)\n        if not count1:\n            rowst[i] = False\n            # If we have got count2 rows in validation set, set all others equal to False.\n            if not count2:\n                rowsv[i] = False\n            # Don't do anything to fisrt count2 rows after first count1 rows of training set,\n            # where Target = selected Target and dec. count2\n            count2 -= rowsv[i] # As True = 1 and False = 0\n            continue\n        # Equal to False because they will be in Training set\n        rowsv[i] = False\n        # Don't do anything to fisrt count2 rows, where Target = selected Target, and dec. count1\n        count1 -= rowst[i]\n    \n    return rowst, rowsv","502e367e":"rows123t, rows123v = train_val_split(rows123, vper=0.1)\nrows4t, rows4v = train_val_split(rows4, tvlen=(1300, 200))\nrows123t.sum(), rows123v.sum(), rows4t.sum(), rows4v.sum()","7aab2e88":"train.drop(['Id', 'idhogar'], axis=1, inplace=True)","c2dacc89":"xtrain, xvalid = train.loc[rows123t|rows4t].drop('Target', axis=1).copy(), train.loc[rows123v|rows4v].drop('Target', axis=1).copy()\nytrain, yvalid = train['Target'].loc[rows123t|rows4t].copy(), train['Target'].loc[rows123v|rows4v].copy()","c3dc7840":"xtrain.shape, ytrain.shape, xvalid.shape, yvalid.shape","560f90e0":"xtrain.head()","1069699c":"ytrain.value_counts()","2c388045":"yvalid.value_counts()","7c89e264":"ytrain.value_counts().plot.barh()","9b69d407":"train['Target'].value_counts()","a4791860":"target1 = train.loc[train['Target']==1].copy()\ntarget2 = train.loc[train['Target']==2].copy()\ntarget3 = train.loc[train['Target']==3].copy()","71004453":"target1 = pd.concat([target1]*8, ignore_index=True).copy(); target1.shape","f0aba3cf":"target2 = pd.concat([target2]*4, ignore_index=True).copy(); target2.shape","d8600098":"target3 = pd.concat([target3]*5, ignore_index=True).copy(); target3.shape","48c27eea":"train2 = train.copy()\ntrain2 = pd.concat([train2, target1, target2, target3], ignore_index=True); train2.shape","f5f560dd":"train2 = train2.sample(frac=1).reset_index(drop=True)","d1d95367":"xtrain2, xvalid2 = train2.iloc[:15000].drop(['Target'], axis=1).copy(), train2.iloc[15000:].drop(['Target'], axis=1).copy()\nytrain2, yvalid2 = train2.iloc[:15000]['Target'].copy(), train2.iloc[15000:]['Target'].copy()","f85c0c5b":"xtrain2.shape, ytrain2.shape, xvalid2.shape, yvalid2.shape","5a1485af":"train2['Target'].value_counts().plot.barh()","dd21b5d8":"from sklearn.ensemble import RandomForestClassifier\nimport math","f384ea9e":"def print_score(m, trn, val):\n    \"\"\"\n    Print F1 score for training set and validation set, where m is a\n    RandomForestClassifier.\n    ----------------------------------------------------------------------\n    Parameters:\n        m: RandomForestClassifier model\n        trn: tuple or array of Input and Output training data points\n        val: tuple or array of Input and Output validation data points\n    \"\"\"\n    print(\"Train F1score: \", str(F1score(trn[1], m.predict(trn[0]))),\n    \",  Valid. F1score: \", str(F1score(val[1], m.predict(val[0]))))\n    print(\"Train Acc.: \", str(m.score(trn[0], trn[1])),\n    \", Valid. Acc.: \", str(m.score(val[0], val[1])))","5a2a081c":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.7, n_jobs=-1)\nm.fit(xtrain, ytrain)\nprint_score(m, (xtrain, ytrain), (xvalid, yvalid))","1f5b8067":"# Here I increased min_sample_leaf hyperparameter\nm2 = RandomForestClassifier(n_estimators=100, min_samples_leaf=150, max_features=0.5, n_jobs=-1)\nm2.fit(xtrain2, ytrain2)\nprint_score(m2, (xtrain2, ytrain2), (xvalid2, yvalid2))","889bdfa9":"from sklearn.model_selection import train_test_split\na, b, c, d = train_test_split(train.drop('Target', axis=1), train['Target'], test_size=0.20,\n                                                    stratify=train['Target'])\nxtrain3, xvalid3, ytrain3, yvalid3 = a.copy(), b.copy(), c.copy(), d.copy()","974b9e0e":"m3 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1, class_weight='balanced')\nm3.fit(xtrain3, ytrain3)\nprint_score(m3, (xtrain3, ytrain3), (xvalid3, yvalid3))","de2d3bf1":"def plot_bar_stacked(y, preds):\n    \"\"\"\n    For plotting predictions, right and wrong. For wrong predictions it will\n    plot stacked bars in diff. colors denoting the class to which it was \n    misplaced.\n    -------------------------------------------------------------------------\n    Parameters:\n        y : actual ouput values\n        preds : predicted output values\n    Output:\n        Plot a stacked graph of count of right and wrong\n    \"\"\"\n    # Output Categories \n    categories = np.array([1, 2, 3, 4])\n    # This will keep count of right predictions and count of wrong prediction in each category\n    counts = [[0, 0, 0, 0], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]\n    # Calculating wrong and right predictions for all categories\n    for cat in categories:\n        index = (y == cat)\n        right = (preds[index] == y[index]).sum()\n        # For wrong preds:\n        p = preds[index]\n        w1 = (p[(p != y[index])] == 1).sum()\n        w2 = (p[(p != y[index])] == 2).sum()\n        w3 = (p[(p != y[index])] == 3).sum()\n        w4 = (p[(p != y[index])] == 4).sum()\n\n        counts[1][cat-1] = [w1, w2, w3, w4]\n        counts[0][cat-1] = right\n        \n    # Plotting\n    ind = np.arange(4)\n    width = 0.15\n\n    fig, ax = plt.subplots(figsize=(15,10), sharey=True)\n    \n    # Quite a simple way to plot stacked bar plot\n    df = pd.DataFrame(counts[1], index=np.arange(1, 5), columns=['W Pred=1', 'W Pred=2', 'W Pred=3', 'W Pred=4'])\n    df.plot.bar(ax=ax, width=width, stacked=True, colormap='RdYlBu')\n\n    ax.bar(ind+width, counts[0], width=-width, color='green', label='Right')\n\n    ax.set(xticks=ind + width, xticklabels=categories, xlim=[2*width - 1, 4])\n    ax.legend()","6688380a":"preds = m.predict(xvalid)\nplot_bar_stacked(yvalid, preds)","099810dc":"# It is not necessary that it will generalize well for test set too. (Though this is \n# giving me better results on public leaderboard.)\npreds = m2.predict(xvalid2)\nplot_bar_stacked(yvalid2, preds)","82ccf158":"preds = m3.predict(xvalid3)\nplot_bar_stacked(yvalid3, preds)","3fff8abd":"fi = pd.DataFrame({'cols':xtrain.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False); fi[0:10]","4e3cc9b0":"fi.plot('cols', 'imp', figsize=(10, 6), legend=False)","cdd10591":"to_keep = fi.loc[(fi['imp']>0.005), 'cols']; to_keep.shape","7119d24c":"xtrain_new, xvalid_new = xtrain[to_keep].copy(), xvalid[to_keep].copy()","b057b298":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=3, max_features=0.5, n_jobs=-1)\nm.fit(xtrain_new, ytrain)\nprint_score(m, (xtrain_new, ytrain), (xvalid_new, yvalid))","8de397c9":"fi = pd.DataFrame({'cols':xtrain2.columns, 'imp':m2.feature_importances_}).sort_values('imp', ascending=False); fi[0:10]","6f9ceff3":"fi.plot('cols', 'imp', figsize=(10, 6), legend=False)","160fa5c0":"to_keep = fi.loc[(fi['imp']>0.005), 'cols']; to_keep.shape","f84e17af":"xtrain2_new, xvalid2_new = xtrain2[to_keep].copy(), xvalid2[to_keep].copy()\nm2 = RandomForestClassifier(n_estimators=100, min_samples_leaf=150, max_features=0.5, n_jobs=-1)\nm2.fit(xtrain2_new, ytrain2)\nprint_score(m2, (xtrain2_new, ytrain2), (xvalid2_new, yvalid2))","3c64e8fa":"fi = pd.DataFrame({'cols':xtrain3.columns, 'imp':m3.feature_importances_}).sort_values('imp', ascending=False); fi[0:10]","66d2e81f":"to_keep = fi.loc[(fi['imp']>0.005), 'cols']; to_keep.shape","9e494876":"xtrain3_new, xvalid3_new = xtrain3[to_keep].copy(), xvalid3[to_keep].copy()\nm3 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1, class_weight=\"balanced\")\nm3.fit(xtrain3_new, ytrain3)\nprint_score(m3, (xtrain3_new, ytrain3), (xvalid3_new, yvalid3))","2c016e8f":"import scipy\nfrom scipy.cluster import hierarchy as hc","df70c6b2":"corr = np.round(scipy.stats.spearmanr(xtrain_new).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,15))\ndendrogram = hc.dendrogram(z, labels=xtrain_new.columns, orientation='left', leaf_font_size=16)\nplt.show()","7fed9238":"m = RandomForestClassifier(n_estimators=50, min_samples_leaf=25, max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(xtrain_new, ytrain)\nprint(m.oob_score_)\ncheck_with = m.oob_score_","5568cee3":"#scores = []\n#for col in ['v2a1_sum', 'v2a1']:\n#    m = RandomForestClassifier(n_estimators=50, min_samples_leaf=25, max_features=0.6, n_jobs=-1, oob_score=True)\n#    m.fit(xtrain_new.drop(col, axis=1), ytrain)\n#    scores.append(m.oob_score_)\n#    print(m.oob_score_)","d55c13ff":"#to_drop = []\n#for i, col in enumerate(['v2a1_sum', 'v2a1']):\n#    if scores[i] > check_with: to_drop.append(col)","b9cb511f":"#xtrain_new, xvalid_new = xtrain_new.drop(to_drop, axis=1), xvalid_new.drop(to_drop, axis=1) ","5e7246cc":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1)\nm.fit(xtrain_new, ytrain)\nprint_score(m, (xtrain_new, ytrain), (xvalid_new, yvalid))","36fba782":"preds = m.predict(xvalid_new)\nplot_bar_stacked(yvalid, preds)","2cd2bc10":"pass","dff2473b":"# from xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV","09f0d39f":"# It needs labels from 0 to n-1, where n is number of classes\n#ytrain3 = ytrain3-1\n#yvalid3 = yvalid3-1","90e51a4c":"# For decreasing learning rate of model with time\ndef learningRateAnnl(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True)","6335d9bc":"fit_params={\"early_stopping_rounds\":300, \n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(xvalid3, yvalid3.copy()-1)],\n            'eval_names': ['valid'],\n            'callbacks': [lgb.reset_parameter(learning_rate=learningRateAnnl)],\n            'verbose': False,\n            'categorical_feature': 'auto'}","0e2e7d0a":"from scipy.stats import randint\nfrom scipy.stats import uniform\nparam_test ={'num_leaves': randint(12, 20), \n             'min_child_samples': randint(40, 120), \n             #'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': uniform(loc=0.75, scale=0.20), \n             'colsample_bytree': uniform(loc=0.8, scale=0.15),\n             #'reg_alpha': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n             #'reg_lambda': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n             #'boosting': ['dart', 'goss', 'gbdt']\n            }","153b7a17":"maxHPs = 400\nclassifier = lgb.LGBMClassifier(learning_rate=0.05, n_jobs=-1, n_estimators=500, objective='multiclass')\n\n#rs = RandomizedSearchCV(estimator= classifier, param_distributions=param_test, n_iter=maxHPs,\n#                        scoring='f1_macro', cv=5, refit=True, verbose=True)","d6b5af48":"#_ = rs.fit(xtrain3, (ytrain3).copy()-1, **fit_params)","58f0572d":"#opt_parameters = rs.best_params_; opt_parameters\n# op_parameters found by above method (Random Search)\nopt_parameters = {'colsample_bytree': 0.8755593602517565,\n 'min_child_samples': 51,\n 'num_leaves': 19,\n 'subsample': 0.9437154452377117}","0d9a45c3":"classifier = lgb.LGBMClassifier(**classifier.get_params())\nclassifier.set_params(**opt_parameters)\n\nfit_params['verbose'] = 200\n_ = classifier.fit(xtrain3, (ytrain3).copy() -1, **fit_params)","2eec8a3f":"kfold = 5\nkf = StratifiedKFold(n_splits=kfold, shuffle=True)\n\nfor trn_idx, tst_idx in kf.split(train.drop(['Target'], axis=1), train['Target']):\n    xtr, xval = train.drop(['Target'], axis=1).iloc[trn_idx], train.drop(['Target'], axis=1).iloc[tst_idx]\n    ytr, yval = train['Target'].iloc[trn_idx].copy() -1, train['Target'].iloc[tst_idx].copy() -1\n    \n    classifier.fit(xtr, ytr, eval_set=[(xval, yval)], \n            early_stopping_rounds=300, verbose=200)","59e3fea8":"preds = classifier.predict(xvalid)","956be452":"((preds+1) == yvalid).sum()\/len(preds)","0955adea":"plot_bar_stacked(yvalid, preds+1)","d58f4eae":"classifier2 = lgb.LGBMClassifier(**classifier.get_params())\n\nkfold = 5\nkf = StratifiedKFold(n_splits=kfold, shuffle=True)\n\nfor trn_idx, tst_idx in kf.split(train2.drop(['Target'], axis=1), train2['Target']):\n    xtr, xval = train2.drop(['Target'], axis=1).iloc[trn_idx].copy(), train2.drop(['Target'], axis=1).iloc[tst_idx].copy()\n    ytr, yval = train2['Target'].iloc[trn_idx].copy()-1, train2['Target'].iloc[tst_idx].copy() -1\n    \n    classifier2.fit(xtr, ytr, eval_set=[(xval, yval)], \n            early_stopping_rounds=300, verbose=200)","79e220c6":"# Won't necessarily generalize well.\npreds = classifier2.predict(xvalid2)\nplot_bar_stacked(yvalid2, preds+1)","f710f13b":"pass","28ee11d7":"from torch.utils.data import TensorDataset, DataLoader\nfrom torch.autograd.variable import Variable","f400b44f":"Ttrain = TensorDataset(torch.DoubleTensor(np.array(xtrain.values, dtype=\"float32\")), #.cuda\n                       torch.LongTensor(np.array(ytrain.values, dtype=\"float32\")-1)) #.cuda","de4ea689":"trainLoader = DataLoader(Ttrain, batch_size = 20, shuffle=True)","62ae924c":"class Net(nn.Module):\n    def __init__(self, n_cols):\n        super(Net, self).__init__()\n        \n        self.first = nn.Sequential(\n            nn.BatchNorm1d(n_cols),\n            nn.Linear(n_cols, 10),\n            nn.ReLU(),\n            nn.Dropout(p=0.5),\n            #nn.Linear(80, 80),\n            #nn.BatchNorm1d(80),\n            #nn.ReLU(),\n            #nn.Linear(80, 80),\n            #nn.BatchNorm1d(80),\n            #nn.ReLU(),\n            #nn.Dropout(p=0.25),\n            #nn.Linear(80, 20),\n            #nn.BatchNorm1d(20),\n            #nn.ReLU(),\n            #nn.Linear(50, 20),\n            #nn.ReLU(),\n            nn.Linear(10, 4),\n            nn.Softmax(dim=1)\n        )\n    \n    def forward(self, x):\n        return self.first(x)\nnet = Net(len(xtrain.columns)).double() #.cuda()","7c29d918":"loss = nn.CrossEntropyLoss()\nmetrics = [F1score]\n#opt = optim.SGD(net.parameters(), 1e-3, momentum=0.999, weight_decay=1e-3, nesterov=True)\nopt = optim.Adam(net.parameters(), weight_decay=1e-3)\n#opt = optim.RMSprop(net.parameters(), momentum=0.9, weight_decay=1e-3)","f4e61859":"def fit(model, lr, xtr, ytr, xvl, yvl, train_dl, n_epochs, loss, opt, metrics, annln=False, mult_dec=True):\n    \"\"\"\n    Function to fit the model to training set and print F1 scores for both training set\n    and validation set.\n    -------------------------------------------------------------------------------------\n    Parameters:\n        model: Model (Neural Network) to which Training set will fit\n        lr: Learning rate (initil learning rate if annln=True)\n        xtr: Input train array (for getting F1score on whole array)\n        ytr: Output train array (for getting F1score on whole array)\n        xvl: Input validation array (for val. F1score)\n        yvl: Output validation array (for val. F1score)\n        train_dl: Train DataLoader which loads training data in batches (should give Tensors as output)\n        n_epochs: number of epochs\n        loss: Loss function to calculate and backpropagate loss (eg: CrossEntropy)\n        opt: Optimizer, to update weights (eg: RMSprop)\n        metrics: Function to calculate score of model (eg: accuracy, F1 score)\n        annln: (default=False) If to use LRAnnealing or not\n        mult_dec: (default=True) If to dec. max Learning rate on every cosine cycle\n    \"\"\"\n    if(annln): annl = lrAnnealing(lr, 40, 449, mult_dec)  # itr_per_epoch = len(xtrain) \/\/ batch_size\n    for epoch in range(n_epochs):\n        tl = iter(train_dl)\n        length = len(train_dl)\n        \n        for t in range(length):\n            xt, yt = next(tl)\n\n            #y_pred = model(Variable(xt).cuda())\n            #l = loss(y_pred, Variable(yt).cuda())\n            y_pred = model(Variable(xt))\n            l = loss(y_pred, Variable(yt))\n            if(annln): annl(opt)\n            opt.zero_grad()\n            l.backward()\n            opt.step()\n        \n        val_score = get_f1score(model, \n                                torch.DoubleTensor(np.array(xvl, dtype = \"float32\")), #.cuda\n                                torch.LongTensor(np.array(yvl, dtype = \"float32\")-1)) #.cuda\n        trn_score = get_f1score(model, \n                                torch.DoubleTensor(np.array(xtr, dtype = \"float32\")), #.cuda\n                                torch.LongTensor(np.array(ytr, dtype = \"float32\")-1)) #.cuda\n        \n        if (epoch+1)%5 == 0:\n            print(\"Epoch \" + str(epoch) + \"::\"\n                + \"  trnF1score: \" + str(trn_score)\n                +\", valF1score: \" + str(val_score))\n            \ndef get_f1score(model, x, y):\n    \"\"\"\n    To get F1score of predictions from Neural Network.\n    -----------------------------------------------------------------\n    Parameters:\n        model: Neural Network Model\n        x: Input Values to be sent to model() function to get predictions\n        y: Output Values to be checked with predictions\n    Output:\n        Return F1 score of predictions \n    \"\"\"\n    pred = model(Variable(x).contiguous())\n    ypreds = np.argmax(pred.contiguous().data.numpy(), axis=1) #.cpu()\n    yactuals = y.contiguous().numpy() #.cpu()\n    return F1score(yactuals, ypreds)\n\ndef set_lr(opt, lr):\n    \"\"\"\n    Function to set lr for optimizer in every layer.\n    ------------------------------------------------------------------\n    Parameters:\n        opt: optimizer used in neural network\n        lr: New Learning rate to be set in each layer\n    \"\"\"\n    for pg in opt.param_groups: pg['lr'] = lr\n\nclass lrAnnealing():\n    def __init__(self, ini_lr, epochs, itr_per_epoch, mult_dec):\n        \"\"\"\n        Class to Anneal learning rate with warm restarts with time. It decreases \n        learning rate as multiple cosine waves with dec. amplitudes.1e-10 is taken \n        as zero. (The lower point for cosine)\n        ---------------------------------------------------------------------------\n        Parameters:\n            ini_lr: Initial learning rate\n            epochs: Number of epochs\n            itr_per_epoch: iterations per epoch\n            mult_dec: T\/F, If to use Annealing with warm restarts or hard\n        \"\"\"\n        self.epochs = epochs\n        self.ipe = itr_per_epoch\n        self.m_dec = mult_dec\n        self.ppw = (self.ipe * self.epochs) \/\/ 4    # Points per wave of cosine (For 4 waves per fit method)\n        self.count = 0\n        self.lr = ini_lr\n        self.values = np.cos(np.linspace(np.arccos(self.lr), np.arccos(1e-10), self.ppw))\n        self.mult = 1\n    def __call__(self, opt):\n        \"\"\"\n            opt: optimizer of which lr is to set\n        \"\"\"\n        self.count += 1\n        set_lr(opt, self.values[self.count-1]*self.mult)\n        if self.count == len(self.values):\n            self.count = 0\n            if(self.m_dec): self.mult \/= 2","290af745":"from sklearn.exceptions import UndefinedMetricWarning\nwarnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)","51a57c8f":"%time fit(net, 1e-2, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","f75b9d57":"%time fit(net, 1e-3, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","6cb3ba4f":"# Getting max F1 score of .85 in training and .85 for test\n# But not giving good results on public leaderboard (0.303)\n%time fit(net, 1e-4, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","b71a065c":"%time fit(net, 1e-5, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","a83dbe55":"%time fit(net, 1e-6, xtrain, ytrain, xvalid, yvalid, trainLoader, 40, loss, opt, metrics, annln=False)","a7dc2ef8":"# Plotting the result:\nypreds = net(Variable(torch.DoubleTensor(np.array(xvalid, dtype=\"float32\"))).contiguous()).data.numpy().argmax(1)+1 #.cuda, .cpu()\nplot_bar_stacked(yvalid, ypreds)","9c61bc6c":"#Ttrain = TensorDataset(torch.DoubleTensor(np.array(xtrain2.values, dtype=\"float32\")), #.cuda\n#                       torch.LongTensor(np.array(ytrain2.values, dtype=\"float32\")-1)) #.cuda\n#trainLoader = DataLoader(Ttrain, batch_size = 20, shuffle=True)\n#net2 = Net(len(xtrain2.columns)).double() #.cuda()\n#loss = nn.CrossEntropyLoss()\n#metrics = [F1score]\n#opt = optim.Adam(net.parameters(), weight_decay=1e-3)\n#opt = optim.SGD(net.parameters(), 1e-3, momentum=0.999, weight_decay=1e-3, nesterov=True)\n#opt = optim.RMSprop(net.parameters(), momentum=0.9, weight_decay=1e-3)","277f9179":"#ytrain2.unique(), yvalid2.unique()","27118936":"#%time fit(net2, 1e-2, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","24b0f070":"#%time fit(net2, 1e-3, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","3bcd216b":"#%time fit(net, 1e-4, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","f8be1589":"#%time fit(net, 1e-5, xtrain2, ytrain2, xvalid2, yvalid2, trainLoader, 40, loss, opt, metrics, annln=False)","2b760483":"# Plotting the result:\n#ypreds = net2(Variable(torch.DoubleTensor(np.array(xvalid2, dtype=\"float32\"))).contiguous()).data.numpy().argmax(1)+1 #.cuda, .cuda()\n#plot_bar_stacked(yvalid2, ypreds)","16da541d":"m = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1)\nm.fit(xtrain, ytrain)\nprint_score(m, (xtrain, ytrain), (xvalid, yvalid))\n\nto_pred = test[xtrain.columns]\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values), m.predict(to_pred)], axis=-1)\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission1.csv\", index=False)","b54cb7ed":"m2 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1)\nm2.fit(xtrain2, ytrain2)\nto_pred = test[xtrain2.columns]\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values), m2.predict(to_pred)], axis=-1)\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission2.csv\", index=False)","d47a794a":"# Uncomment it to get output for 3rd RandomForest Classifier (Ready for submission)\nm3 = RandomForestClassifier(n_estimators=100, min_samples_leaf=25, max_features=0.5, n_jobs=-1, class_weight='balanced')\nm3.fit(xtrain3, ytrain3)\nto_pred = test[xtrain3.columns]\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values), m3.predict(to_pred)], axis=-1)\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission3.csv\", index=False)","6ed03662":"# For output of net (DNN-1 with all features)\npred = net(Variable(torch.DoubleTensor(np.array(test.drop(['Id', 'idhogar'], axis=1).values, dtype=\"float32\"))).contiguous()) # .cuda\nypreds = pred.contiguous().data.numpy().argmax(1) + 1 # .cpu()\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values),ypreds], axis=-1); npArray[0]\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission4.csv\", index=False)","d0b86245":"ypreds = classifier.predict(test[xtrain.columns]) + 1\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values),ypreds], axis=-1)\n\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission5.csv\", index=False)","2b9f39f1":"ypreds = classifier2.predict(test[xtrain2.columns]) + 1\nnpArray = np.stack([lb3.inverse_transform(test['Id'].values),ypreds], axis=-1)\nres = pd.DataFrame(npArray, index=np.arange(len(npArray)), columns=['Id', 'Target'])\nres.to_csv(\"submission6.csv\", index=False)","f0733a3a":"Doing the same with test set:","01bdfc2a":"#### 2) Upsampling:\n---","94230b5f":"---\nNow according to the third RandomForestClassifier `m3`:","429ba818":"# Data Preprocessing <a id=\"dataPreprocessing\"><\/a>\n---","371269a4":"Wow, only a few features can predict quite accurately. (every point shows its contribution to the model prediction.)","688e953b":"# Make Submission File: <a id=\"makingSubmission\"><\/a>","5f29a170":"Now all of them are fixed.","1ca68a6b":"Same as train.","80f6c12d":"### RandomForest with top features:","1ffbee19":"### During prediction, I got an error that some values in validation set are null or infinity. So lets check: <a id=\"anErrorIGot\"><\/a>","53a1c181":"# Comparison of Different models: <a id=\"compModels\"><\/a>\n\n---\n\n*We had very small dataset here. We had total 9557 rows and 4 categories to predict from. Out of the total 9557 rows about 6000 belonged to one category only. Thats a huge mismatch in quantity. And that was the main challenge. But still F1score of about 0.40 was achievable.* \n\n\\*\\* = Kaggle Takes only 5 submissions per day.\n\n-- = Not Implemented Yet\n\n| Models \\ Data Type | Downsampled Data | Upsampled Data | FeatEng Data | Original Data | If class_weight = 'balanced' for original data |\n|-|:-:|:-:|:-:|:-:|:-:|\n| Random Forest | 0.99, 0.60, 0.346,  | 0.94, 0.92, **0.420** | 0.99, 0.59, \\*\\*  |  0.80, 0.74, **0.414**  |  Yes  |\n| LightGBM 5-Fold| --  |  --, 0.99, **0421**  |  --  |  --, 0.96, 0.387 (0.406)  |\n| Neural Network (4 Hidden Layers) |  0.88, 0.40, 0.342 |  0.90, 0.50, 0.295   |  --   |   --   |\n| Neural Network (2 hidden Layers) | 0.90, 0.87, 0.303 | -- | -- | -- |\n\n.\n\nFormat : TrainF1Score, ValF1Score, PublicF1Score\n\n---","63cb7224":"Out of *`6860`* people having null `v2a1`, *`1862`* are from `Target` of *`1`* or *`2`*. Not much. \n\nSo it is a possibility that many of them own their house or data is missing for some other reason.","91379f71":"## Adding more features using PCA:  <a id=\"pcaFeat\"><\/a>\n\n\nNow we will add more features by **`PCA`** (Principal Component Analysis) method. It uses `SVD` (Singular Value Decomposition) method to reduce dimentionality from `N` to `n`, where `n < N`. This actually gives us direction of vectors in which data has the most variance with top component having the most variance. All `n` components are orthogonal to each other because the next highest variance direction is always orthogonal to the previous ones.\n*And because they are orthogonal they are not linearly correlated*.\n\n![Source](http:\/\/www.nlpca.org\/fig_pca_principal_component_analysis.png)","b71de51a":"Same for `test` dataset:","3b2581c1":"### Taking care of non-numerical features:  <a id=\"numFeatures\"><\/a>","a3557625":"*`3`* families whose `meaneduc` is not available.","f1d7d3c3":"### Random Forest with top features from Upsampled Dataset:","e21952c8":"Here, we will check for features (columns) that are non numerical. We need to take care of them because we cannot send `objects` into a Neural Network.\nSo, we will convert non-numerical data to numerical data and then it will work for any model we use.","41064c9c":"If we have only few data points for some category\/categories, our model might not learn about that category that much or anything at all.\n\nHere to bridge the gap we I have taken two approaches:\n1.  Cutting down the category with many data points to make it comparable to others.  [Down Sampling]\n1.  Copying category with small datapoints again and again to make them comparable to others. [Up Sampling] (For more info on such methods see [SMOTE](https:\/\/www.cs.cmu.edu\/afs\/cs\/project\/jair\/pub\/volume16\/chawla02a-html\/chawla2002.html))\n1. Using `sample_weights` or `class_weight` hyperparameter. (We will see these during [RandomForest](#randomForest) and GradientBoosting)","bfa2ce01":"But this one particular family pays about **`2,000,000`** (i.e. `$3456.20` at current rates) and I checked at a [site](https:\/\/www.propertiesincostarica.com) and some bunglows have similar rates...","0f89cc75":"### Taking care of NULL values: <a id=\"nullValues\"><\/a>","cda82e16":"** 2) v18q1** : number of tablets household owns\n\n    For this lets check these columns:\n    a) v18q : owns a tablet  # And no value is null here, we will use this.\n","6a6b5ffc":"### Gradient boosting with copied rows in training set:\n\nWe will use the same hyper-parameters that we discovered above:","43516fb8":"Doing the same for test set:","06993ff8":"# Deep Neural Network: <a id=\"dnn\"><\/a>\n\nFor introduction on how to make custom Neural Network with PyTorch look at my work: [Training your own CNN using PyTorch](https:\/\/www.kaggle.com\/puneetgrover\/training-your-own-cnn-using-pytorch)","e788bdb2":"We will take only 1500 rows from `Target` of 4 to make a balance between all categories.","7137299c":"* **Random Forest** with Downsampled data:","51d5cd53":"### Now fitting to Upsampled dataset:","1be7cb78":"* **XGBoost** with Upsampled data:","edd784a2":"More Domain Knowledge Features: (from [here](https:\/\/www.kaggle.com\/willkoehrsen\/featuretools-for-good))","b9b57f9e":"So there are *`26`* families who have overcrowding in their home. (where `v2a1` is null)\n\nIt means out of these null values, most of them are not living in poverty or extreme poverty. i.e. having `Target` value of *`1`* (most probably).","ff7731a2":"So, the most important feature is `escolari_mean`, which is average of years of schooling of members per household. Makes sense.","12425c58":"Good, out of these *`1183`* don't have any level of education (for training data). We will put `rez_esc` for them equal to *`0`*.","11394b55":"We also can combine some ordinal groups, by making one feature from a group of features which give information about the same thing and have a ordinal relationship between them. Combining features will save us some space and compute time.\n\nOrdinal feature Groups:\n*  Material outside wall:  `[ 'paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother' ]`\n*  Material Floor : `[ 'pisomoscer', 'pisocemento', 'pisoother' , 'pisonatur', 'pisonotiene', 'pisomadera' ]`\n*  Material Roof : `[ 'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo' ]`\n*  Water Provisoin : `[ 'abastaguadentro', 'abastaguafuera', 'abastaguano' ]`\n*  Electricity Provision : `[ 'public', 'planpri', 'noelec', 'coopele' ]`\n*  Sanitary Provision : `[ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6' ]`\n*  Cooking Provision : `[ 'energcocinar1', 'energcocinar4', 'energcocinar3', 'energcocinar2' ]`\n*  Disposal Type : `[ 'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6' ]`\n*  Walls Type : `[ 'epared1', 'epared2', 'epared3' ]`\n*  Roof Type : `[ 'etecho1', 'etecho2', 'etecho3' ]`\n*  Floor Type : `[ 'eviv1', 'eviv2', 'eviv3' ]`\n*  Education Level : `[ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9' ]`\n*  House Type : `[ 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5' ]`\n\nWe won't combine Material used features because we don't know the relative price of things there. \n\n**For water provision** : `no` < `outside` < `inside` **OR** `no` < `inside` < `outside`, we don't really know. So we will leave this group too.\n\n**For elec Povision** : `no` < `JASEC\/ESPH` < `Cooperative` < `private plant` **OR** `no` < `Coop` < `ESPH` < `Private Plant`; Leave can leave this too\n\n**For Sanit Provision** : `no` < `blackhole` < `septic` < `sever`, `other` (?) **OR**  `no` < `blackhole` < `sever` < `septic`, `other`(?)\n\n**For Cook'n Prov** : `no` < `wood` < `gas` < `elec`\n\n**For Disposal type** : `river` >< `burning` >< `throwUnoccu` < `botan` < `Truck`, `other`(?); Leave it.\n\n**For Wall Type** : `bad` < `reg` < `good`\n\n**For Roof Type** : `bad` < `reg` < `good`\n\n**For Floor Type** : `bad` < `reg` < `good`\n\n**Ed Level** :  This group can be used\n\n**House Type** : `Precarious` < `other` < `rented` < `installment` < `fullyOwned`\n","ace35e55":"# Imports  <a id=\"imports\"><\/a>\n---","2ba3075c":"#### 1) Downsampling:\n---","ec6e10fc":"#### Changing the type of columns:\n\nWe will set type of all columns according to their possible values. ","8d560730":"They are also from `Target` value of **`4`**, so leave them too...\n\nNow let's check correlation between some of the columns I selected:\n1. `v2a1` :  Monthly rent payment\n1. `rooms`: number of all rooms in the house\n1. `tamhog`: size of the household\n1. `overcrowding`: # persons per room\n1. `v18q1`: number of tablets household owns\n1. `r4t3`: Total persons in the household\n1. `meaneduc`: average years of education for adults (18+)\n1. `qmobilephone`: # of mobile phones\n1. `Target`: the target is an ordinal variable indicating groups of income levels","3171c6ad":"And now we will make others equal to their means, taking data only from their category.","1e94a342":"## Handling small number of data points for some categories: <a id=\"handlingSmallData\"><\/a>","1e5b75e7":"First,  we will take care of `Id` and `idhogar`:\n\nWe will use sklearn's `LabelEncoder` to encode these values. We can delete them as they are unique person or household identifier and they are different for every single unit of them, so they can't give any useful information for our output `Target`. For example: every household with`Target` value of `1` will have different `idhogar` and every individual with `Target` value of `1` will have different `Id`.\nBut we will keep them for now, as we will use them in further Data Preprocessing.","51e9c979":" 4) **meaneduc** : average years of education for adults (18+) and \n \n 5) **SQBmeaned**: square of the mean years of education of adults (>=18) in the household\n\n       For this we will check: \n       a) instlevel1 : =1 no level of education\n       b) instlevel2 : =1 incomplete primary\n       c) instlevel3 : =1 complete primary\n       d) instlevel4 : =1 incomplete academic secondary level\n       e) instlevel5 : =1 complete academic secondary level\n       f) instlevel6 : =1 incomplete technical secondary level\n       g) instlevel7 : =1 complete technical secondary level\n       h) instlevel8 : =1 undergraduate and higher education\n       i) instlevel9 : =1 postgraduate higher education","9270d2a9":" **1) v2a1 **: Monthly rent payment\n\n        For this lets check these columns:\n        a) v2a1 : Monthly rent payment\n        b) v18q : owns a tablet\n        c) hacapo : Overcrowding by rooms\n        d) rooms : number of all rooms in the house\n        e) r4t3 : Total persons in the household\n        f) hhsize : household size\n        g) escolari : years of schooling\n        h) epared3 : =1 if walls are good\n        i) epared2 : =1 if walls are regular\n        j) tipovivi1 : =1 own and fully paid house\n        k) tipovivi2 : =1 own,  paying in installments\n        l) tipovivi3 : =1 rented\n        m) tipovivi4 : =1 precarious\n        n) tipovivi5 : =1 other(assigned,  borrowed)\n        p) Target : poverty level\n\n    And check if they own their house or not.","a536bb09":"*`564`* `separated`, *`279`* `widower` and *`2005`* `single`.","2eabe64c":"# Data Exploration and Visualization: <a id=\"explore\"><\/a>\n---","741eae65":"We will leave it as it is, as we don't know what to change it to.","38698cdc":"### Combining some of the features: <a id=\"combiningFeatures\"><\/a>","38275a26":"* **Neural Network** with Downsampled data:","8c93a612":"So, all of them are actually here. Otherwise we could have put `meaneduc` equal to someone in their family who had that value present.","03b4b6a0":"Now same for test:","0a8f33e4":"---\nNow according to the second RandomForestClassifier `m2`:","606ffc15":"Thats why, because for these `hogar_adul` is zero.","3dea22a5":"Though it giving me acceptable score here, but on Public Leaderboard this giving me poor results.\nSo, neural network is not giving good results for the optimizers and hyperparameters I tried.\n\nI am still trying to tweak it a bit. I have made it less deeper now.","58bbc486":"* **Random Forest** with original Dataset:","0bb07fcc":" # Random Forest Ensemble: <a id=\"randomForest\"><\/a>\n ---","a3f4ea53":"# Gradient Boosting: <a id=\"gradBoosting\"><\/a>","3d451fff":"## Making new features: <a id=\"makingNewFeatures\"><\/a>\n\nMaking new features from the existing features can help our model to learn new trends in data which were not given in dataset before. We take groups of data points from the dataset, and calculate a feature which is true for that group, and we do this for all groups.\n\nHere, we group by `idhogar` (household id) and calculate `count`, `mean`, `max`, `min`, and `sum` of all numeric type features and make new features for all groups, in hopes that now these features will explain more about `Target` value.","88e1fdea":"Not much, again.","8d014639":"So, we don't have any child for whom this value is missing. Maybe we were right about thinking that they don't have this value for adults.","5f30eba7":"# Checking for corrupted data <a id=\"checkingCorruptedData\"><\/a>\n---","6ee67ca2":"**Checking the correlation of all features again: **\n\nMost probably we have created many correlated features in previous step. Infact some features even have 100% correlation. That is they are exactly same.","b0e62912":"Description of columns with missing values:\n\n1. **v2a1 :** Monthly rent payment\n1. **v18q1 :** number of tablets household owns\n1. **ez_esc : ** Years behind in school\n1. **meaneduc :** average years of education for adults (18+)\n1. **SQBmeaned :** square of the mean years of education of adults (>=18) in the household\n---","1e33126d":"Our Upsampled dataset, is giving worse results than our first neural network. (Opposite of what we saw in case of RandomForeset and GradientBoosting)","23aa8388":"### K-Fold Fitting:","72e3468c":"So, they are the only ones in their household.","cd78a0b0":"Someone or some families have `v2a1` i.e. `Monthly rent Payment` of more than **`2,000,000`**!!","9a39889b":"Null values in dataset can arise from many factors:\n1.  Non availability of data as not applicable for that particular row\n1.  Non availability of data as the Org. was not able to get it for some reason\n1. Due to some error or misplacement\n\nHere, we will consider that every feature had some data for every individual and put missing values equal to mean if no other option is available.","45e4bbcc":"### Removing redundant features: <a id=\"removingRedundantFeatures\"><\/a>","3b9978aa":"But firstly lets put `v2a1` values for people who own their homes equal to zero.","9c04cf54":"**3) rez_esc** : Years behind in school\n\n        For this we will check columns:\n        a) escolari: years of schooling\n        b) estadocivil1: =1 if less than 10 years old\n        c) estadocivil2: =1 if free or coupled union        # We are checking these ones, because they may be old\n        d) estadocivil3: =1 if married                      # and it might be the case that IADB does not have \n        e) estadocivil4: =1 if divorced                     # this data about them.\n        f) estadocivil5: =1 if separated\n        g) estadocivil6: =1 if widower\n        h) estadocivil7: =1 if single\n        i) instlevel1: =1 no level of education\n        j) age: Age in years","699eac43":"Description of these variables:\n1.  **Id : ** a unique identifier for each row.\n1.  **idhogar : ** this is a unique identifier for each household.\n1.  **dependency : ** Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)\/(number of member of household between 19 and 64)\n1.  **edjefe : ** years of education of male head of household\n1. **edjefa : ** years of education of female head of household","d26882a2":"Out of people who don't own their home have either `precarious`, or `other (assigned or borrowed)` homes.","738a518e":"### Gradient Boosting with top features:","0bd59788":"Now let's see the others:\n\n1) **Dependency** :","03c382c0":"* **Random Forest** with Upsampled data:","df7a98c0":"Woah! how did this happen?","ec29cb55":"For this we will use Dendrogram plot to see closely related features.","4e34d55b":"So, we have *`1111`* `free or coupled union`, *`2486`* `married` and *`300`* `divorced`.","0ed753cf":"# Introduction\n\nThis is a step by step kernel for: `Data Cleaning`, `Feature Engineering`, and `Model Making and Prediction`. I will explain as I go what I am doing. \n\n## Index\n\n+ [Imports](#imports)\n+ [Some Useful Functions](#someUsefulFunctions)\n+ [Data Preprocessing](#dataPreprocessing)\n    - [Taking care of Non-Numerical Features](#numFeatures)\n    - [Taking care of Null Values](#nullValues)\n+ [Checking corrupted data](#checkingCorruptedData)\n+ [Data Exploration and Visualization](#explore) **\\***\n+ [Feature Engineering](#featureEng)\n    - [Combine Some Features](#combiningFeatures)\n    - [Remove Highly Correlated Features](#removeHighlyCorrelatedFeatures)\n    - [An Error I got during training (Infinite values)](#anErrorIGot)\n    - [Making new Features](#makingNewFeatures)\n        * [More features using PCA](#pcaFeat)\n+ [Handling - few data points for some categories](#handlingSmallData)\n+ [Random Forest](#randomForest)\n    - [Check Feature's Importance](#checkFeatureImportance)\n    - [Removing Redundant Features](#removingRedundantFeatures)\n+ [Gradient Boosting](#gradBoosting)\n+ [Deep Neural Network](#dnn)\n+ [Comparing Models](#compModels) **\\*\\***\n+ [Making Submission File](#makingSubmission)","657a4d7a":"We are getting this much validation score here, because we have many repetitions of rows and it has learned many of them. (i.e. it is overfitted, but we can control that by `max_depth`, `max_leaf_nodes` etc. hyperparameters.)\n\nNow, we will also use the hyperparameter `class_weights` = `'balanced'` which we discussed in [Handling small datapoints](#handlingSmallData) section.","8d0f44a6":"We will put `rez_esc` equal to average value in their category:","a3cec908":"There are 3 groups which are quite closer to each other than others. Lets remove some features from them from model one by one and lets see what happens to our `F1score`.","2996920f":"* **XGBoost** with Downsampled data:","05c96fb4":"Assuming the rent is being divided among them equally.","3ce1c8c5":"# Some Useful functions <a id=\"someUsefulFunctions\"><\/a>\n---","e9b0bada":"The only things we can check here are:\n     \n    1. Check if all Id's are unique. (Should have checked first. But all are unique)\n    2. Check if same household has same Target value, meaneduc value, zone value ( urban or rural), region value, house properties (wall type, ceiling type etc), number of persons in houshold, number of adults, number of childern, number of tablets household owns.\n\nWe cannot check the others, because there is no way to check their validity. If for some reson they are wrong, they are wrong. But such cases happen rarely.  So, we don't need to worry about that.","d398fd1e":"#### Now for test set:","5e6cac36":"\nHere we have somewhat different features' importances, but on the top is still `escolari_mean`.","00c4eb17":"So out of *`2156`* families, *`300`* don't own their home.","911cac17":"### Checking the feature inportances: <a id=\"checkFeatureImportance\"><\/a>","07dd9d97":"We have any rows with same `Target` value in the end. If we don't want same `Target` value in our validation set. And to do that I found a way [here](https:\/\/stackoverflow.com\/questions\/29576430\/shuffle-dataframe-rows).","09e186fb":"3) **Edjefa** :","3f8027c1":"# Feature Engineering <a id=\"featureEng\"><\/a>\n---","03372386":"Function to plot stacked bar plot:","a928351b":"And PCA works [better](http:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py) if we standardize our data first. So:","8edcd6ca":"Firstly we can remove all the Square values columns : \n\n    SQBescolari     : escolari squared\n    SQBage          : age squared\n    SQBhogar_total  : hogar_total squared\n    SQBedjefe       : edjefe squared\n    SQBhogar_nin    : hogar_nin squared\n    SQBovercrowding : overcrowding squared\n    SQBdependency   : dependency squared\n    SQBmeaned       : square of the mean years of education of adults (>=18) in the household\n    agesq           : Age squared\n\nbecause they will be highly correlated with their unit degree counterparts, and we will be using a Neural Network and in Neural Network you don't need higher order terms of your features to check if that degree of feature explains better or not.  We do that in Linear Regression to capture non linear relationship of some features with the output. Here, Neural Network will learn these relations by itself by adjusting the weights.\n\nIn RandomForest and GradientBoosting also, we wont use these highly correlated featues.","ad86803f":"Best hyperparameter search method and LR reduction callback are taken from [here](https:\/\/www.kaggle.com\/mlisovyi\/lighgbm-hyperoptimisation-with-f1-macro).","9b0d56d9":"2) **Edjefe** :","ea9f863a":"F1 score (macro) is what we need as a `metrics` to check how good or bad our model is. F1 score is a better quantifier of viability of model than `accuracy`.  ","e8565f28":"### Remove Highly Correlated features:  <a id=\"removeHighlyCorrelatedFeatures\"><\/a>\n\nAll Highly Correlated features are not necessary to kept in the dataset. We can take only one of them, which will be sufficient for getting what they were all telling together. Keeping all of them will be redundant, as they have same trend in dataset, and even one of them can capture that trend.","06387ebc":"Some insights from this pair plot:\n1. `meaneduc` with `v2a1` has a kind of Gaussian Distribution with people with about 20yrs of education paying greater house rent than others,\n1. `v2a1` decreases as `overcrowing` increases. It may be possible that most of the overcrowded are from `Target` **`4`**,\n1. `v2a1` also decreases as `r4t3` increases. It may be because of similar reason above, more # of people increase may be indication of a lower `Target` value,\n1. `r4t3` and `tamhog` showing strong positive linear behaviour. Size of house hold doesn't guarantee quality.","98c9f25c":"Out of families with `precarious` or `other` homes, *`114`* have `Target` value <=*`2`*.\n\nAs we see, out of these *`300`* families who don't own their homes, we have a mix of families, with all `Target` values.\n\nSo, we will put `v2a1` value equal to mean of `v2a1` values in set of that `Target` value."}}