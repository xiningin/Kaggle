{"cell_type":{"d5f95b60":"code","83c58507":"code","7d26423a":"code","4a8cfd9b":"code","088e3bbb":"code","29a71567":"code","b9c829a3":"code","af5cfb7b":"code","7c9fad05":"code","f4cce792":"code","a7b7fbe7":"code","bb6d9df7":"code","7cf686c9":"code","44730a3f":"code","051e0471":"code","e9e71086":"code","464687a5":"code","3e7e3bcc":"code","d71e8064":"code","9b65dc7a":"code","a2326c2d":"code","0ca55ee2":"code","44897236":"code","b79751c3":"code","e2e1bd8d":"code","a11a47a4":"code","d2fd25d5":"code","25c028b6":"code","b68a00c0":"code","e03b51cd":"code","89a092db":"code","0a647b9b":"code","435cd0d6":"code","f11ea121":"code","bf6acc9e":"code","2bcdb7f0":"code","e8c7b2e2":"code","bcba62d9":"code","e7fa0fcf":"code","b121099d":"code","25964cdf":"code","b7217e65":"code","66adbabe":"code","4c1db33b":"code","037ae4d7":"code","01f9c00d":"code","daa1eac7":"code","aa5f5db4":"code","bc31f6f4":"code","b33ebb81":"code","8e992291":"code","0c2abc76":"code","73e712b6":"code","5301136e":"code","3af87917":"code","7f6782ac":"code","20d12b9e":"code","e866a86d":"code","139b9d57":"code","e08ea416":"code","320803db":"code","f6646cff":"code","2328ac19":"code","1b25240b":"code","0e59dfe9":"code","a87cac10":"code","277955e8":"code","5b7f177f":"code","00b1ac76":"code","fd6d88c9":"code","c752cc93":"code","6e326ccc":"code","f8735bfe":"code","cdd4e0be":"code","058b05ec":"code","474bc23d":"code","c9c31661":"code","4282e6e0":"code","956f3cfb":"code","1e0874e0":"code","2083a92d":"code","260ac4e1":"code","4424307b":"code","9dc68237":"code","3bbef4a9":"code","77d17f9f":"code","3f911f96":"code","a046a96a":"code","ce7b3b48":"code","a1368d3d":"code","f07f593f":"code","3ca0c48b":"code","f34e28f5":"code","59459f78":"code","5edbab43":"code","d4eb01b9":"code","170601b0":"code","2faea190":"code","6e55489e":"code","6345ed18":"code","88bcbafd":"code","0cf9f9d7":"code","34c754cd":"code","70664322":"code","d240240d":"code","e8d9f60e":"code","33412db9":"code","68241bea":"code","562645d0":"code","e3068488":"code","92c5462c":"code","e613da6d":"code","2de1e82b":"code","a5fe4532":"code","5d2f368d":"code","e2297ee3":"code","68a69f78":"code","27a08730":"code","76314d3b":"code","f6166b4e":"code","d1e924b4":"code","8d95c2ee":"code","826af2cf":"code","94bdea5c":"code","1e6671d0":"code","0f1aaf86":"code","b13e21ac":"code","fce7174d":"code","9f395466":"code","80a8fe1a":"code","b5da263b":"code","cf160045":"code","c82485dc":"code","989f76d8":"code","1012d091":"code","ae0d9cda":"code","931f5b4a":"code","549d8940":"code","0a282760":"code","6828902d":"code","82bb4654":"code","caf9431b":"code","2e61e1a7":"code","f5261d36":"code","8466059b":"code","ac2021fb":"code","5a61f4f7":"code","8cdf7cb0":"code","98ede8bb":"code","e09a3036":"code","7353733b":"code","df3485ab":"code","f3d29538":"code","c5fc5453":"code","b4402806":"code","e6601fea":"code","d072724e":"code","4262bbf7":"code","2f9b29a8":"code","52ed56e6":"code","3b083ba8":"code","4892dc44":"code","381fc70f":"code","6185896d":"code","4d301032":"code","73939e6d":"code","6917d74a":"code","107dd398":"code","49743bc1":"code","711c747f":"code","afb87006":"code","475b14e0":"code","398cf5a9":"code","52952380":"code","8e7109b1":"code","5c01785a":"code","1907bd4c":"code","8b929014":"code","1078de6e":"code","745e8390":"code","a0f7a85a":"code","d38e250a":"code","f041a592":"code","a7d15069":"code","c224be7b":"code","a6a49cc7":"code","8171c8d2":"code","158870b3":"code","1f43f965":"code","091ff3e6":"code","75537e23":"code","d1492d76":"code","31048ea7":"code","8bdf93f7":"code","cf02f621":"code","b0a90f03":"code","d38ea9bf":"code","7b56caaa":"code","e16a3c66":"code","8b1ecf08":"code","07540712":"code","88786425":"code","bbf73d72":"code","6df319a0":"code","afaf4bd9":"code","66ec7243":"code","ab928b7c":"code","c86b8492":"code","d9da826c":"code","2b4cbdd8":"code","55b0e85d":"code","fd106c5d":"code","9886579a":"code","b1b9ddb9":"code","2c47e1ed":"code","e24d32bd":"code","e3771dd8":"code","c0ac8e93":"code","5121cea6":"code","68b43b50":"code","eb02518c":"code","f20094ac":"code","87e4b88d":"code","9ea2e88f":"code","dce0b294":"code","0aaf6d48":"code","356e19f4":"code","af968596":"code","c080bc7e":"code","d3324643":"code","9fb7c934":"code","8eb50f3e":"code","487daa0f":"code","fc1e0ab6":"code","37733242":"code","4d6c5f91":"code","3808f46b":"code","5a34d1f2":"code","692eb16b":"code","ab75b7f7":"code","3f7fb377":"code","16dbca10":"code","c8f27ee3":"code","fe3f82bd":"code","6d5e3cc3":"code","725d029b":"code","a7455a4d":"code","a863d691":"code","fab07341":"code","f5a4d1b1":"code","cd3895bf":"code","702356a4":"code","2b53a9eb":"code","19574269":"code","f32dabfc":"code","d49ceb45":"code","fee9ee36":"code","b5b2b6f6":"code","a273989f":"code","b2019442":"code","58fb04cd":"code","4d97229f":"code","3a6df41d":"code","9545d7fd":"code","2a7f867c":"markdown","31f14311":"markdown","dcd4c6bf":"markdown","d7ae4568":"markdown","76c78ed7":"markdown","180e405e":"markdown","8b8f3c18":"markdown","238defc5":"markdown","c2cdaa7a":"markdown","7ccb8131":"markdown","1c9ffd71":"markdown","cf65c59c":"markdown","b2a30e08":"markdown","3771dec0":"markdown","b0a129c0":"markdown","63375dd5":"markdown","04a43709":"markdown","a541fba4":"markdown","00606998":"markdown","7f9b9aa3":"markdown","4af947d8":"markdown","330e8d43":"markdown","2424a617":"markdown","3c50c03d":"markdown","69398edc":"markdown","43c6cc05":"markdown","d9b130ca":"markdown","d32c3547":"markdown","a473c3a2":"markdown","2ddef1cb":"markdown","deadfcaa":"markdown","017d41ab":"markdown","761eb04d":"markdown","3cd35f5f":"markdown","6a45ef61":"markdown","9fa0bbbd":"markdown","b805ade8":"markdown","fa2e5ea2":"markdown","aede5457":"markdown","f8f052ec":"markdown","90d0af5d":"markdown","e3047a21":"markdown","ab4498ec":"markdown","c1ccd26b":"markdown","f6fb615d":"markdown","7a1b6c27":"markdown","37996862":"markdown","fbeb170f":"markdown","4a2cd6f6":"markdown","621ca248":"markdown","4a6970d5":"markdown","b66a9709":"markdown","0ad561fb":"markdown","2906b9f4":"markdown","b0643b7a":"markdown","65eb7c6f":"markdown","df5b5a48":"markdown","ed64dce8":"markdown","f57f7d23":"markdown","9ad2e6a9":"markdown","dad796b6":"markdown","3efc97aa":"markdown","50473883":"markdown","e0c017ba":"markdown","f0f526c5":"markdown","b3ad8272":"markdown","543763ee":"markdown","f814de47":"markdown","3c1d62ac":"markdown","be5f9b8b":"markdown","26ef488e":"markdown","a0511d24":"markdown","fefc0ece":"markdown","a6fff87e":"markdown","2f02e682":"markdown","7247cc62":"markdown","bdf5bf23":"markdown","9842a2df":"markdown","57d232c6":"markdown","504d5d34":"markdown","ae08962c":"markdown","4c5c2f7f":"markdown","b59179c7":"markdown","e3e3fefc":"markdown","10f1a8c9":"markdown","500b664a":"markdown","689e95bb":"markdown","a8d2df43":"markdown","9ab3ed79":"markdown","6c3dbd04":"markdown","cafa96b4":"markdown","c26b5a91":"markdown","ce54aba1":"markdown","91a4bdd7":"markdown","931bb252":"markdown","c7af58e6":"markdown","13b869f8":"markdown","58398681":"markdown","ad6e9ff6":"markdown","5af53283":"markdown","57a5a49b":"markdown","9950e089":"markdown","5f4ee963":"markdown","dc90dd1d":"markdown","717a8f55":"markdown","ea5d6c87":"markdown","93ea5205":"markdown","07c3f470":"markdown","6063b043":"markdown","5f5cc96d":"markdown","105b6d48":"markdown","19e0de56":"markdown","051e9bf6":"markdown","f809d042":"markdown","064b2dd2":"markdown","3af6e214":"markdown","8a38da41":"markdown","b21196c7":"markdown","19dd450c":"markdown","27a3c0c5":"markdown","6f591906":"markdown","6c8bc050":"markdown","818ca57c":"markdown","d5eb9615":"markdown","312730aa":"markdown","e7e9c079":"markdown","251ffbdd":"markdown","746969b1":"markdown","0fa53e35":"markdown","1ae03b39":"markdown","5f61e8f8":"markdown","beeda824":"markdown","5f2e6a8f":"markdown","165a56f4":"markdown","4bbcb2a6":"markdown","c706bbc7":"markdown","40cf327b":"markdown","785c1cbc":"markdown","1ff2996b":"markdown","f7ac0649":"markdown","e9def7e6":"markdown","f454c69e":"markdown","95077b08":"markdown","b10ffef9":"markdown","a1abaf8d":"markdown","73b30e08":"markdown","e4b6434b":"markdown","cac07fc5":"markdown","087986ef":"markdown","c2b050f8":"markdown","e183861e":"markdown","6a065fea":"markdown","a223edae":"markdown","4d8d918f":"markdown","fe113638":"markdown","9c30e827":"markdown","9eb70c31":"markdown","3d4ca116":"markdown","a63f2ee1":"markdown","37df2fc5":"markdown","f0ebd82c":"markdown","f193877d":"markdown","dd016ecd":"markdown","cae94181":"markdown","c0aba876":"markdown","1c33c017":"markdown","f9cbbb70":"markdown","0b172f9e":"markdown","f70e6332":"markdown","6781fa32":"markdown","d6cd26fa":"markdown","70219bb3":"markdown","02819e5a":"markdown","ec9f0af1":"markdown","a09f6549":"markdown","f9864385":"markdown","3ac792dd":"markdown","fe385abe":"markdown","9da5beb3":"markdown","2ba1cb12":"markdown","7a36d8f9":"markdown","a41b8b9c":"markdown","8776298e":"markdown","b4a6defd":"markdown","29aec415":"markdown","9be66839":"markdown","14f5824a":"markdown","c8c457a5":"markdown","1afac5f2":"markdown","0ec9ba02":"markdown","fee1d2d3":"markdown","2b8e3ab9":"markdown","16e3fed0":"markdown","a55c9642":"markdown","f6c6c5fe":"markdown","a35e78b0":"markdown","e84b35c2":"markdown","edd4cc0f":"markdown","a24742c7":"markdown","7d3738d8":"markdown","e20d38ed":"markdown","08156094":"markdown","7437d225":"markdown","d62b06c2":"markdown","38324e15":"markdown","9c7c19e4":"markdown","7c1369b2":"markdown","9b2a8db1":"markdown","25c6aa4d":"markdown","b33a9b57":"markdown","a16955fc":"markdown","bc0a8f1f":"markdown","da25d43d":"markdown","ffbf0a3b":"markdown","7e578fe9":"markdown","65626778":"markdown","7980f498":"markdown","f4e2022c":"markdown","49410080":"markdown","d3f3d83c":"markdown","8fbd01b0":"markdown","92d52dcd":"markdown","bac4d85a":"markdown","729afe7c":"markdown","e09fb2aa":"markdown","c358af34":"markdown","961dbb66":"markdown","ee346d65":"markdown","a34a99c9":"markdown","252bc2d2":"markdown","98954c5c":"markdown","fde58250":"markdown","9113ce94":"markdown","95a8a38b":"markdown","acda484b":"markdown","b1bead03":"markdown","4cbaec3f":"markdown","3fdaf369":"markdown","62ffeee6":"markdown","71a4f6f9":"markdown","57154af5":"markdown","2fc6e09e":"markdown","83769e10":"markdown","7110ad15":"markdown","73a85d3d":"markdown","fba41565":"markdown","0c8e55b6":"markdown","3e10d3a7":"markdown","e790d879":"markdown","9718ce39":"markdown","ef9728e7":"markdown","01c8e39c":"markdown","3e94297d":"markdown","62e6dc65":"markdown","c506fdc8":"markdown","e5b34ed0":"markdown","c134a5c0":"markdown","07817225":"markdown","03e25f1b":"markdown","951b1818":"markdown","a52c77ef":"markdown","56771db0":"markdown","764da000":"markdown","a10aa0b3":"markdown","2c15bfa8":"markdown","effff7e1":"markdown","5885b0d3":"markdown","313e3268":"markdown","14bfd405":"markdown","96f72aea":"markdown","70936e3a":"markdown","5bf32b9e":"markdown","9cf4c859":"markdown","30cf4b08":"markdown","e8394d01":"markdown","0d280ffd":"markdown","ec999c8a":"markdown","94046721":"markdown","d571c283":"markdown","974355a2":"markdown","d8251146":"markdown","b3e1558c":"markdown","c0d4a1fd":"markdown","83b6b12a":"markdown","c2de65e1":"markdown","914b51d5":"markdown","86676ad8":"markdown","c82e8419":"markdown","4c1ac4e3":"markdown","fc7a3da8":"markdown","532d400e":"markdown","4c5b90d8":"markdown","ac08e3f4":"markdown","b845d727":"markdown","cefd438b":"markdown","c9bf19e2":"markdown","85380ac0":"markdown","e1687fca":"markdown","9a11cb97":"markdown","87eb10c7":"markdown","d61806ae":"markdown","8d8b1e6a":"markdown","ac77fa62":"markdown","b4ed03ed":"markdown","22b7b345":"markdown","1862ce53":"markdown","9890da5e":"markdown","734b5e9a":"markdown","cf120afb":"markdown","6502f32a":"markdown","b8540e77":"markdown","3bcb3f55":"markdown","c1cafba9":"markdown","b920ffaa":"markdown","f92027d6":"markdown","78028ed5":"markdown","650708b2":"markdown","33028e07":"markdown","7766be89":"markdown","5014a811":"markdown","92ee3ce8":"markdown","a596d4ee":"markdown","a6e25f7e":"markdown","306bbb9a":"markdown","df8b11db":"markdown","11e1e9be":"markdown","089563e1":"markdown","cbec7d71":"markdown","e74a71c8":"markdown","dc4a3284":"markdown","c24eadaf":"markdown","4ace1183":"markdown","c8a1f23f":"markdown","175d6ffd":"markdown","ff14d4bf":"markdown","a6c305df":"markdown","4d100f55":"markdown","f429c08d":"markdown","c1a89844":"markdown","172af2ad":"markdown","00f798d9":"markdown","9c3df7df":"markdown","f1af30e0":"markdown","c94b4cbe":"markdown","f7d565d6":"markdown","a9ee2d19":"markdown","0756998d":"markdown","58aaab2f":"markdown","0a1fec42":"markdown","07238486":"markdown","c8f99b1e":"markdown","d8a4f908":"markdown"},"source":{"d5f95b60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os as os\nimport sys\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# From Scikit Learn\nfrom sklearn import preprocessing, decomposition, tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix, classification_report\nfrom astropy.table import Table, Column\n# Set DEBUG = True to produce debug results\nDEBUG = False","83c58507":"print(\"The Python version is %s.%s.%s.\" % sys.version_info[:3])","7d26423a":"%pwd","4a8cfd9b":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nchurn = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\", header=0, sep=\",\")","088e3bbb":"churn.describe(include='all')","29a71567":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.dropna.html\nchurn = churn.dropna(axis = 0, how = 'all')\nif DEBUG:\n    #Dimensions of dataset\n    print(\"Shape of Data\", churn.shape)\n    #Colum names\n    print(\"Colums Names\", churn.columns)\n    #See bottol few rows of dataset\n    print(churn.tail())","b9c829a3":"# designate target variable name\ntargetName = 'Churn'\ntargetSeries = churn[targetName]\n#remove target from current location and insert in collum 0\ndel churn[targetName]\nchurn.insert(0, targetName, targetSeries)\n#reprint dataframe and see target is in position 0\nchurn.head()","af5cfb7b":"churn.info()","7c9fad05":"#Basic bar chart since the target is binominal\ngroupby = churn.groupby(targetName)\ntargetEDA=groupby[targetName].aggregate(len)\nplt.figure()\ntargetEDA.plot(kind='bar', grid=False)\nplt.axhline(0, color='k')\nplt.title('Bar Chart of Churn')","f4cce792":"groupby.mean()","a7b7fbe7":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.17.0\/generated\/pandas.crosstab.html\nchurn_ac_cross = pd.crosstab(churn['PaperlessBilling'], churn['Churn'])\nif DEBUG:\n    print(churn_ac_cross)","bb6d9df7":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.html\nchurn_ac_cross.plot(kind='bar', stacked=True)\nplt.title('Churn by Paperless Billing')","7cf686c9":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.17.0\/generated\/pandas.crosstab.html\nchurn_ac_cross = pd.crosstab(churn['Contract'], churn['Churn'])\nif DEBUG:\n    print(churn_ac_cross)","44730a3f":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.html\nchurn_ac_cross.plot(kind='bar', stacked=True)\nplt.title('Churn by Contract')","051e0471":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.17.0\/generated\/pandas.crosstab.html\nchurn_st_cross = pd.crosstab(churn['OnlineSecurity'], churn['Churn'])\nif DEBUG:\n    print(churn_st_cross)","e9e71086":"# Source: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.html\n#plt.rcParams[\"figure.figsize\"] = (20,3)\nchurn_st_cross.plot(kind='bar', stacked=True)\nplt.title('Churn by Online Security')","464687a5":"print(churn.info())","3e7e3bcc":"from sklearn import preprocessing\nle_dep = preprocessing.LabelEncoder()\n#to convert into numbers\nchurn['Churn'] = le_dep.fit_transform(churn['Churn'])\nchurn['gender'] = le_dep.fit_transform(churn['gender'])\nchurn['Partner'] = le_dep.fit_transform(churn['Partner'])\nchurn['Dependents'] = le_dep.fit_transform(churn['Dependents'])\nchurn['PhoneService'] = le_dep.fit_transform(churn['PhoneService'])\nchurn['MultipleLines'] = le_dep.fit_transform(churn['MultipleLines'])\nchurn['InternetService'] = le_dep.fit_transform(churn['InternetService'])\nchurn['OnlineSecurity'] = le_dep.fit_transform(churn['OnlineSecurity'])\nchurn['OnlineBackup'] = le_dep.fit_transform(churn['OnlineBackup'])\nchurn['DeviceProtection'] = le_dep.fit_transform(churn['DeviceProtection'])\nchurn['TechSupport'] = le_dep.fit_transform(churn['TechSupport'])\nchurn['StreamingTV'] = le_dep.fit_transform(churn['StreamingTV'])\nchurn['StreamingMovies'] = le_dep.fit_transform(churn['StreamingMovies'])\nchurn['Contract'] = le_dep.fit_transform(churn['Contract'])\nchurn['PaperlessBilling'] = le_dep.fit_transform(churn['PaperlessBilling'])\nchurn['PaymentMethod'] = le_dep.fit_transform(churn['PaymentMethod'])","d71e8064":"#Note: axis=1 denotes that we are referring to a column, not a row\nchurn=churn.drop(['customerID'],axis=1)\nif DEBUG:\n    print(churn.head())","9b65dc7a":"churn.info()","a2326c2d":"churn['TotalCharges'] = churn['TotalCharges'].convert_objects(convert_numeric=True)","0ca55ee2":"print(churn['TotalCharges'].describe())","44897236":"churn['TotalCharges'].fillna(churn['TotalCharges'].median(), inplace=True)\nprint(churn['TotalCharges'].describe())","b79751c3":"if DEBUG:\n    print(churn.shape)\n    print(churn.info())\n    print(churn.head())","e2e1bd8d":"groupby = churn.groupby(targetName)\nprint(groupby.mean())","a11a47a4":"# Source: https:\/\/etav.github.io\/python\/scikit_pca.html\nchurn_numeric = churn.select_dtypes(include=['number'])\nfeatures = list(churn_numeric)\nX = churn_numeric.loc[:, features].values\nX = scale(X)","d2fd25d5":"pca = PCA(n_components=0.99, whiten=True)","25c028b6":"pca.fit(X)\nvariance = pca.explained_variance_ratio_ #calculate variance ratios\nX_pca = pca.fit_transform(X)\nvar=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\nvar #cumulative sum of variance explained with [n] features","b68a00c0":"# Show results\nprint('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_pca.shape[1])","e03b51cd":"plt.ylabel('% Variance Explained')\nplt.xlabel('# of Features')\nplt.title('PCA Analysis')\nplt.ylim(30,100.5)\nplt.style.context('seaborn-whitegrid')\n\nplt.plot(var)","89a092db":"# split dataset into testing and training\nfeatures_train, features_test, target_train, target_test = train_test_split(churn.iloc[:,1:].values, churn.iloc[:,0].values, test_size=0.33, random_state=0)","0a647b9b":"from sklearn.preprocessing import MinMaxScaler\nscaler = preprocessing.MinMaxScaler().fit(features_train)\nfeatures_train = scaler.transform(features_train)\nfeatures_test = scaler.transform(features_test)","435cd0d6":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(25,50))\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1, algorithm='auto', weights='uniform')\n    scores = cross_val_score(knn, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    knn.fit(features_train, target_train)\n    train_pred = knn.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = knn.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","f11ea121":"if DEBUG:\n    scores = pd.DataFrame(k_scores)\n    print(scores)","bf6acc9e":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('KNN Neighbors')\nplt.show()","2bcdb7f0":"#KNN train model. Call up my model and name it clf\nclf_knn = KNeighborsClassifier(n_neighbors=27, n_jobs=-1, algorithm='auto', weights='uniform')\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_knn)\n#Fit clf to the training data\nclf_knn = clf_knn.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_knn = clf_knn.predict(features_test)","e8c7b2e2":"acc_knn = accuracy_score(target_test, target_predicted_knn)\nprec_knn = precision_score(target_test, target_predicted_knn)\nrecall_knn = recall_score(target_test, target_predicted_knn)\nf1_knn = f1_score(target_test, target_predicted_knn)\ncm_knn = confusion_matrix(target_test, target_predicted_knn)\nprint(\"KNN Accuracy Score\", acc_knn)\nprint(classification_report(target_test, target_predicted_knn))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_knn))","bcba62d9":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_knn, annot=True, fmt='d')\nplt.title('KNN Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_knn))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","e7fa0fcf":"#verify KNN with Cross Validation\nscores_knn = cross_val_score(clf_knn, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_knn)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_knn.mean(), scores_knn.std() * 2))","b121099d":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_knn.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_knn = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_knn)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_knn)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('KNN Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","25964cdf":"#KNN train model. Call up my model and name it clf\nclf_knn1 = KNeighborsClassifier(n_neighbors=27, n_jobs=-1, algorithm='auto', weights='distance')\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_knn1)\n#Fit clf to the training data\nclf_knn1 = clf_knn1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_knn1 = clf_knn1.predict(features_test)","b7217e65":"acc_knn1 = accuracy_score(target_test, target_predicted_knn1)\nprec_knn1 = precision_score(target_test, target_predicted_knn1)\nrecall_knn1 = recall_score(target_test, target_predicted_knn1)\nf1_knn1 = f1_score(target_test, target_predicted_knn1)\ncm_knn1 = confusion_matrix(target_test, target_predicted_knn1)\nprint(\"KNN Accuracy Score\", acc_knn1)\nprint(classification_report(target_test, target_predicted_knn1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_knn1))","66adbabe":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_knn1, annot=True, fmt='d')\nplt.title('KNN Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_knn1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","4c1db33b":"#verify KNN with Cross Validation\nscores_knn1 = cross_val_score(clf_knn1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_knn1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_knn1.mean(), scores_knn1.std() * 2))","037ae4d7":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_knn1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_knn1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_knn1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_knn1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('KNN Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","01f9c00d":"#KNN train model. Call up my model and name it clf\nclf_knn2 = KNeighborsClassifier(p=1, n_neighbors=27, n_jobs=-1, algorithm='kd_tree', weights='uniform')\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_knn2)\n#Fit clf to the training data\nclf_knn2 = clf_knn2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_knn2 = clf_knn2.predict(features_test)","daa1eac7":"acc_knn2 = accuracy_score(target_test, target_predicted_knn2)\nprec_knn2 = precision_score(target_test, target_predicted_knn2)\nrecall_knn2 = recall_score(target_test, target_predicted_knn2)\nf1_knn2 = f1_score(target_test, target_predicted_knn2)\ncm_knn2 = confusion_matrix(target_test, target_predicted_knn2)\nprint(\"KNN Accuracy Score\", acc_knn2)\nprint(classification_report(target_test, target_predicted_knn2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_knn2))","aa5f5db4":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_knn2, annot=True, fmt='d')\nplt.title('KNN Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_knn2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","bc31f6f4":"#verify KNN with Cross Validation\nscores_knn2 = cross_val_score(clf_knn2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_knn2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_knn2.mean(), scores_knn2.std() * 2))","b33ebb81":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_knn2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_knn2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_knn2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_knn2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('KNN Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","8e992291":"# Source: http:\/\/docs.astropy.org\/en\/stable\/table\/construct_table.html\nt = Table()\nt[''] = ['KNN Model 1','KNN Model 2','KNN Model 3']\nt['Cross Validation Score'] = [round(scores_knn.mean(),4),round(scores_knn1.mean(),4),round(scores_knn2.mean(),4)]\nt['Accuracy Score'] = [round(acc_knn,4),round(acc_knn1,4),round(acc_knn2,4)]\nt['Precision'] = [round(prec_knn,4),round(prec_knn1,4),round(prec_knn2,4)]\nt['Recall'] = [round(recall_knn,4),round(recall_knn1,4),round(recall_knn2,4)]\nt['F1 Score'] = [round(f1_knn,4),round(f1_knn1,4),round(f1_knn2,4)]\nt['ROC AUC'] = [round(roc_auc_knn,4),round(roc_auc_knn1,4),round(roc_auc_knn2,4)]\nt","0c2abc76":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    dt = tree.DecisionTreeClassifier(max_depth=k)\n    scores = cross_val_score(dt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    dt.fit(features_train, target_train)\n    train_pred = dt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = dt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","73e712b6":"if DEBUG:\n    scores = pd.DataFrame(k_scores)\n    print(scores)","5301136e":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Tree depth')\nplt.show()","3af87917":"#Decision Tree train model. Call up my model and name it clf \nclf_dt = tree.DecisionTreeClassifier(max_depth=5)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_dt)\n#Fit clf to the training data\nclf_dt = clf_dt.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_dt = clf_dt.predict(features_test)","7f6782ac":"acc_dt = accuracy_score(target_test, target_predicted_dt)\nprec_dt = precision_score(target_test, target_predicted_dt)\nrecall_dt = recall_score(target_test, target_predicted_dt)\nf1_dt = f1_score(target_test, target_predicted_dt)\ncm_dt = confusion_matrix(target_test, target_predicted_dt)\nprint(\"DT Accuracy Score\", acc_dt)\nprint(classification_report(target_test, target_predicted_dt))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt))","20d12b9e":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt, annot=True, fmt='d')\nplt.title('Decision Tree Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","e866a86d":"#verify DT with Cross Validation\nscores_dt = cross_val_score(clf_dt, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_dt)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_dt.mean(), scores_dt.std() * 2))","139b9d57":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_dt.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_dt = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_dt)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_dt)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","e08ea416":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\n#k_range = list(range(2,40))\nk_range = np.linspace(0.1, 0.3, 20, endpoint=True)\nk_scores = []\nfor k in k_range:\n    dt = tree.DecisionTreeClassifier(max_depth=5, min_samples_leaf=k)\n    scores = cross_val_score(dt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    dt.fit(features_train, target_train)\n    train_pred = dt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = dt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","320803db":"if DEBUG:\n    scores = pd.DataFrame(k_scores)\n    print(scores)","f6646cff":"#from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Leaf')\nplt.show()","2328ac19":"#Decision Tree train model. Call up my model and name it clf \nclf_dt1 = tree.DecisionTreeClassifier(max_depth=5, min_samples_leaf=0.225)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_dt1)\n#Fit clf to the training data\nclf_dt1 = clf_dt1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_dt1 = clf_dt1.predict(features_test)","1b25240b":"acc_dt1 = accuracy_score(target_test, target_predicted_dt1)\nprec_dt1 = precision_score(target_test, target_predicted_dt1)\nrecall_dt1 = recall_score(target_test, target_predicted_dt1)\nf1_dt1 = f1_score(target_test, target_predicted_dt1)\ncm_dt1 = confusion_matrix(target_test, target_predicted_dt1)\nprint(\"DT Accuracy Score\", acc_dt1)\nprint(classification_report(target_test, target_predicted_dt1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt1))","0e59dfe9":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt1, annot=True, fmt='d')\nplt.title('Decision Tree Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","a87cac10":"#verify DT with Cross Validation\nscores_dt1 = cross_val_score(clf_dt1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_dt1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_dt1.mean(), scores_dt1.std() * 2))","277955e8":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_dt1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_dt1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_dt1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_dt1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","5b7f177f":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\n#k_range = list(range(2,40))\nk_range = np.linspace(0.1, 1.0, 20, endpoint=True)\nk_scores = []\nfor k in k_range:\n    dt = tree.DecisionTreeClassifier(max_depth=5, min_samples_split=k)\n    scores = cross_val_score(dt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    dt.fit(features_train, target_train)\n    train_pred = dt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = dt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","00b1ac76":"#from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Split')\nplt.show()","fd6d88c9":"#Decision Tree train model. Call up my model and name it clf \nclf_dt2 = tree.DecisionTreeClassifier(max_depth=5, min_samples_split=0.5)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_dt2)\n#Fit clf to the training data\nclf_dt2 = clf_dt2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_dt2 = clf_dt2.predict(features_test)","c752cc93":"acc_dt2 = accuracy_score(target_test, target_predicted_dt2)\nprec_dt2 = precision_score(target_test, target_predicted_dt2)\nrecall_dt2 = recall_score(target_test, target_predicted_dt2)\nf1_dt2 = f1_score(target_test, target_predicted_dt2)\ncm_dt2 = confusion_matrix(target_test, target_predicted_dt2)\nprint(\"DT Accuracy Score\", acc_dt2)\nprint(classification_report(target_test, target_predicted_dt2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt2))","6e326ccc":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt2, annot=True, fmt='d')\nplt.title('Decision Tree Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","f8735bfe":"#verify DT with Cross Validation\nscores_dt2 = cross_val_score(clf_dt2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_dt2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_dt2.mean(), scores_dt2.std() * 2))","cdd4e0be":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_dt2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_dt2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_dt2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_dt2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Decision Tree Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","058b05ec":"t = Table()\nt[''] = ['Decision Tree Model 1','Decision Tree Model 2','Decision Tree Model 3']\nt['Cross Validation Score'] = [round(scores_dt.mean(),4),round(scores_dt1.mean(),4),round(scores_dt2.mean(),4)]\nt['Accuracy Score'] = [round(acc_dt,4),round(acc_dt1,4),round(acc_dt2,4)]\nt['Precision'] = [round(prec_dt,4),round(prec_dt1,4),round(prec_dt2,4)]\nt['Recall'] = [round(recall_dt,4),round(recall_dt1,4),round(recall_dt2,4)]\nt['F1 Score'] = [round(f1_dt,4),round(f1_dt1,4),round(f1_dt2,4)]\nt['ROC AUC'] = [round(roc_auc_dt,4),round(roc_auc_dt1,4),round(roc_auc_dt2,4)]\nt","474bc23d":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    rf = RandomForestClassifier(max_depth=k, n_jobs=-1)\n    scores = cross_val_score(rf, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    rf.fit(features_train, target_train)\n    train_pred = rf.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = rf.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","c9c31661":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Depth')\nplt.show()","4282e6e0":"# Random Forest train model. Call up my model and name it clf\nclf_rf = RandomForestClassifier(max_depth=7, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_rf)\n#Fit clf to the training data\nclf_rf = clf_rf.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_rf = clf_rf.predict(features_test)","956f3cfb":"acc_rf = accuracy_score(target_test, target_predicted_rf)\nprec_rf = precision_score(target_test, target_predicted_rf)\nrecall_rf = recall_score(target_test, target_predicted_rf)\nf1_rf = f1_score(target_test, target_predicted_rf)\ncm_rf = confusion_matrix(target_test, target_predicted_rf)\nprint(\"RF Accuracy Score\", acc_rf)\nprint(classification_report(target_test, target_predicted_rf))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_rf))","1e0874e0":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_rf, annot=True, fmt='d')\nplt.title('Random Forest Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_rf))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","2083a92d":"#verify RF with Cross Validation\nscores_rf = cross_val_score(clf_rf, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_rf)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_rf.mean(), scores_rf.std() * 2))","260ac4e1":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_rf.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_rf = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_rf)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_rf)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","4424307b":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    rf = RandomForestClassifier(n_estimators=k, n_jobs=-1, max_depth=7)\n    scores = cross_val_score(rf, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    rf.fit(features_train, target_train)\n    train_pred = rf.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = rf.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","9dc68237":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","3bbef4a9":"# Random Forest train model. Call up my model and name it clf\nclf_rf1 = RandomForestClassifier(n_estimators=17, n_jobs=-1, max_depth=7)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_rf1)\n#Fit clf to the training data\nclf_rf1 = clf_rf1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_rf1 = clf_rf1.predict(features_test)","77d17f9f":"acc_rf1 = accuracy_score(target_test, target_predicted_rf1)\nprec_rf1 = precision_score(target_test, target_predicted_rf1)\nrecall_rf1 = recall_score(target_test, target_predicted_rf1)\nf1_rf1 = f1_score(target_test, target_predicted_rf1)\ncm_rf1 = confusion_matrix(target_test, target_predicted_rf1)\nprint(\"RF Accuracy Score\", acc_rf1)\nprint(classification_report(target_test, target_predicted_rf1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_rf1))","3f911f96":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_rf1, annot=True, fmt='d')\nplt.title('Random Forest Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_rf1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","a046a96a":"#verify RF with Cross Validation\nscores_rf1 = cross_val_score(clf_rf1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_rf1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_rf1.mean(), scores_rf1.std() * 2))","ce7b3b48":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_rf1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_rf1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_rf1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_rf1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","a1368d3d":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    rf = RandomForestClassifier(n_estimators=17, n_jobs=-1, max_depth=7, min_samples_leaf=k)\n    scores = cross_val_score(rf, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    rf.fit(features_train, target_train)\n    train_pred = rf.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = rf.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","f07f593f":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Leaf')\nplt.show()","3ca0c48b":"# Random Forest train model. Call up my model and name it clf\nclf_rf2 = RandomForestClassifier(n_estimators=17, n_jobs=-1, max_depth=7, min_samples_leaf=14)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_rf2)\n#Fit clf to the training data\nclf_rf2 = clf_rf2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_rf2 = clf_rf2.predict(features_test)","f34e28f5":"acc_rf2 = accuracy_score(target_test, target_predicted_rf2)\nprec_rf2 = precision_score(target_test, target_predicted_rf2)\nrecall_rf2 = recall_score(target_test, target_predicted_rf2)\nf1_rf2 = f1_score(target_test, target_predicted_rf2)\ncm_rf2 = confusion_matrix(target_test, target_predicted_rf2)\nprint(\"RF Accuracy Score\", acc_rf2)\nprint(classification_report(target_test, target_predicted_rf2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_rf2))","59459f78":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_rf2, annot=True, fmt='d')\nplt.title('Random Forest Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_rf2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","5edbab43":"#verify RF with Cross Validation\nscores_rf2 = cross_val_score(clf_rf2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_rf2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_rf2.mean(), scores_rf2.std() * 2))","d4eb01b9":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_rf2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_rf2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_rf2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_rf2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Random Forest Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","170601b0":"t = Table()\nt[''] = ['RF Model 1','RF Model 2','RF Model 3']\nt['Cross Validation Score'] = [round(scores_rf.mean(),4),round(scores_rf1.mean(),4),round(scores_rf2.mean(),4)]\nt['Accuracy Score'] = [round(acc_rf,4),round(acc_rf1,4),round(acc_rf2,4)]\nt['Precision'] = [round(prec_rf,4),round(prec_rf1,4),round(prec_rf2,4)]\nt['Recall'] = [round(recall_rf,4),round(recall_rf1,4),round(recall_rf2,4)]\nt['F1 Score'] = [round(f1_rf,4),round(f1_rf1,4),round(f1_rf2,4)]\nt['ROC AUC'] = [round(roc_auc_rf,4),round(roc_auc_rf1,4),round(roc_auc_rf2,4)]\nt","2faea190":"from sklearn.ensemble import BaggingClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_bag = BaggingClassifier(base_estimator=clf_knn2, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_bag)\n#Fit clf to the training data\nclf_bag = clf_bag.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_bag = clf_bag.predict(features_test)","6e55489e":"acc_bag = accuracy_score(target_test, target_predicted_bag)\nprec_bag = precision_score(target_test, target_predicted_bag)\nrecall_bag = recall_score(target_test, target_predicted_bag)\nf1_bag = f1_score(target_test, target_predicted_bag)\ncm_bag = confusion_matrix(target_test, target_predicted_bag)\nprint(\"Bag Accuracy Score\", acc_bag)\nprint(classification_report(target_test, target_predicted_bag))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_bag))","6345ed18":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_bag, annot=True, fmt='d')\nplt.title('Bagging Classifier Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_bag))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","88bcbafd":"#verify RF with Cross Validation\nscores_bag = cross_val_score(clf_bag, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_bag)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_bag.mean(), scores_bag.std() * 2))","0cf9f9d7":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_bag.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_bag = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_bag)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_bag)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","34c754cd":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    bag = BaggingClassifier(base_estimator=clf_dt, n_estimators=k, n_jobs=-1)\n    scores = cross_val_score(bag, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    bag.fit(features_train, target_train)\n    train_pred = bag.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = bag.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","70664322":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","d240240d":"# Random Forest train model. Call up my model and name it clf\nclf_bag1 = BaggingClassifier(base_estimator=clf_dt, n_estimators=4, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_bag1)\n#Fit clf to the training data\nclf_bag1 = clf_bag1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_bag1 = clf_bag1.predict(features_test)","e8d9f60e":"acc_bag1 = accuracy_score(target_test, target_predicted_bag1)\nprec_bag1 = precision_score(target_test, target_predicted_bag1)\nrecall_bag1 = recall_score(target_test, target_predicted_bag1)\nf1_bag1 = f1_score(target_test, target_predicted_bag1)\ncm_bag1 = confusion_matrix(target_test, target_predicted_bag1)\nprint(\"Bag Accuracy Score\", acc_bag1)\nprint(classification_report(target_test, target_predicted_bag1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_bag1))","33412db9":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_bag1, annot=True, fmt='d')\nplt.title('Bagging Classifier Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_bag1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","68241bea":"#verify RF with Cross Validation\nscores_bag1 = cross_val_score(clf_bag1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_bag1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_bag1.mean(), scores_bag1.std() * 2))","562645d0":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_bag1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_bag1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_bag1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_bag1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","e3068488":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    bag = BaggingClassifier(base_estimator=clf_rf, n_estimators=k, n_jobs=-1)\n    scores = cross_val_score(bag, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    bag.fit(features_train, target_train)\n    train_pred = bag.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = bag.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","92c5462c":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","e613da6d":"# Random Forest train model. Call up my model and name it clf\nclf_bag2 = BaggingClassifier(base_estimator=clf_rf, n_estimators=10, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_bag2)\n#Fit clf to the training data\nclf_bag2 = clf_bag2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_bag2 = clf_bag2.predict(features_test)","2de1e82b":"acc_bag2 = accuracy_score(target_test, target_predicted_bag2)\nprec_bag2 = precision_score(target_test, target_predicted_bag2)\nrecall_bag2 = recall_score(target_test, target_predicted_bag2)\nf1_bag2 = f1_score(target_test, target_predicted_bag2)\ncm_bag2 = confusion_matrix(target_test, target_predicted_bag2)\nprint(\"Bag Accuracy Score\", acc_bag2)\nprint(classification_report(target_test, target_predicted_bag2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_bag2))","a5fe4532":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_bag2, annot=True, fmt='d')\nplt.title('Bagging Classifier Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_bag2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","5d2f368d":"#verify RF with Cross Validation\nscores_bag2 = cross_val_score(clf_bag2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_bag2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_bag2.mean(), scores_bag2.std() * 2))","e2297ee3":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_bag2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_bag2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_bag2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_bag2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Bagging Classifier Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","68a69f78":"t = Table()\nt[''] = ['Bagging Classifier Model 1','Bagging Classifier Model 2','Bagging Classifier Model 3']\nt['Cross Validation Score'] = [round(scores_bag.mean(),4),round(scores_bag1.mean(),4),round(scores_bag2.mean(),4)]\nt['Accuracy Score'] = [round(acc_bag,4),round(acc_bag1,4),round(acc_bag2,4)]\nt['Precision'] = [round(prec_bag,4),round(prec_bag1,4),round(prec_bag2,4)]\nt['Recall'] = [round(recall_bag,4),round(recall_bag1,4),round(recall_bag2,4)]\nt['F1 Score'] = [round(f1_bag,4),round(f1_bag1,4),round(f1_bag2,4)]\nt['ROC AUC'] = [round(roc_auc_bag,4),round(roc_auc_bag1,4),round(roc_auc_bag2,4)]\nt","27a08730":"from sklearn.ensemble import ExtraTreesClassifier\ntrain_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,15))\nk_scores = []\nfor k in k_range:\n    xdt = ExtraTreesClassifier(max_depth=k, n_jobs=-1)\n    scores = cross_val_score(xdt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    xdt.fit(features_train, target_train)\n    train_pred = xdt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = xdt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","76314d3b":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Depth')\nplt.show()","f6166b4e":"# Random Forest train model. Call up my model and name it clf\nclf_xdt = ExtraTreesClassifier(max_depth=8, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_xdt)\n#Fit clf to the training data\nclf_xdt = clf_xdt.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_xdt = clf_xdt.predict(features_test)","d1e924b4":"acc_xdt = accuracy_score(target_test, target_predicted_xdt)\nprec_xdt = precision_score(target_test, target_predicted_xdt)\nrecall_xdt = recall_score(target_test, target_predicted_xdt)\nf1_xdt = f1_score(target_test, target_predicted_xdt)\ncm_xdt = confusion_matrix(target_test, target_predicted_xdt)\nprint(\"XDT Accuracy Score\", acc_xdt)\nprint(classification_report(target_test, target_predicted_xdt))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_xdt))","8d95c2ee":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_xdt, annot=True, fmt='d')\nplt.title('Extra Trees Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_xdt))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","826af2cf":"#verify RF with Cross Validation\nscores_xdt = cross_val_score(clf_xdt, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_xdt)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_xdt.mean(), scores_xdt.std() * 2))","94bdea5c":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_xdt.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_xdt = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_xdt)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_xdt)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extra Trees Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","1e6671d0":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,20))\nk_scores = []\nfor k in k_range:\n    xdt = ExtraTreesClassifier(min_samples_leaf=k, max_depth=8, n_jobs=-1)\n    scores = cross_val_score(xdt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    xdt.fit(features_train, target_train)\n    train_pred = xdt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = xdt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","0f1aaf86":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Min Samples Leaf')\nplt.show()","b13e21ac":"# Random Forest train model. Call up my model and name it clf\nclf_xdt1 = ExtraTreesClassifier(min_samples_leaf=7, max_depth=8, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_xdt1)\n#Fit clf to the training data\nclf_xdt1 = clf_xdt1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_xdt1 = clf_xdt1.predict(features_test)","fce7174d":"acc_xdt1 = accuracy_score(target_test, target_predicted_xdt1)\nprec_xdt1 = precision_score(target_test, target_predicted_xdt1)\nrecall_xdt1 = recall_score(target_test, target_predicted_xdt1)\nf1_xdt1 = f1_score(target_test, target_predicted_xdt1)\ncm_xdt1 = confusion_matrix(target_test, target_predicted_xdt1)\nprint(\"XDT Accuracy Score\", acc_xdt1)\nprint(classification_report(target_test, target_predicted_xdt1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_xdt1))","9f395466":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_xdt1, annot=True, fmt='d')\nplt.title('Extra Trees Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_xdt1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","80a8fe1a":"#verify RF with Cross Validation\nscores_xdt1 = cross_val_score(clf_xdt1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_xdt1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_xdt1.mean(), scores_xdt1.std() * 2))","b5da263b":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_xdt1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_xdt1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_xdt1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_xdt1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extra Trees Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","cf160045":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(50,70))\nk_scores = []\nfor k in k_range:\n    xdt = ExtraTreesClassifier(max_leaf_nodes=k, min_samples_leaf=7, max_depth=8, n_jobs=-1)\n    scores = cross_val_score(xdt, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    xdt.fit(features_train, target_train)\n    train_pred = xdt.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = xdt.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","c82485dc":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Leaf Nodes')\nplt.show()","989f76d8":"# Random Forest train model. Call up my model and name it clf\nclf_xdt2 = ExtraTreesClassifier(max_leaf_nodes=66, min_samples_leaf=7, max_depth=8, n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_xdt2)\n#Fit clf to the training data\nclf_xdt2 = clf_xdt2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_xdt2 = clf_xdt2.predict(features_test)","1012d091":"acc_xdt2 = accuracy_score(target_test, target_predicted_xdt2)\nprec_xdt2 = precision_score(target_test, target_predicted_xdt2)\nrecall_xdt2 = recall_score(target_test, target_predicted_xdt2)\nf1_xdt2 = f1_score(target_test, target_predicted_xdt2)\ncm_xdt2 = confusion_matrix(target_test, target_predicted_xdt2)\nprint(\"XDT Accuracy Score\", acc_xdt2)\nprint(classification_report(target_test, target_predicted_xdt2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_xdt2))","ae0d9cda":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_xdt2, annot=True, fmt='d')\nplt.title('Extra Trees Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_xdt2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","931f5b4a":"#verify RF with Cross Validation\nscores_xdt2 = cross_val_score(clf_xdt2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_xdt2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_xdt2.mean(), scores_xdt2.std() * 2))","549d8940":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_xdt2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_xdt2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_xdt2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_xdt2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Extra Trees Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","0a282760":"t = Table()\nt[''] = ['XDT Model 1','XDT Model 2','XDT Model 3']\nt['Cross Validation Score'] = [round(scores_xdt.mean(),4),round(scores_xdt1.mean(),4),round(scores_xdt2.mean(),4)]\nt['Accuracy Score'] = [round(acc_xdt,4),round(acc_xdt1,4),round(acc_xdt2,4)]\nt['Precision'] = [round(prec_xdt,4),round(prec_xdt1,4),round(prec_xdt2,4)]\nt['Recall'] = [round(recall_xdt,4),round(recall_xdt1,4),round(recall_xdt2,4)]\nt['F1 Score'] = [round(f1_xdt,4),round(f1_xdt1,4),round(f1_xdt2,4)]\nt['ROC AUC'] = [round(roc_auc_xdt,4),round(roc_auc_xdt1,4),round(roc_auc_xdt2,4)]\nt","6828902d":"from sklearn.ensemble import GradientBoostingClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_gbc = GradientBoostingClassifier()\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_gbc)\n#Fit clf to the training data\nclf_gbc = clf_gbc.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_gbc = clf_gbc.predict(features_test)","82bb4654":"acc_gbc = accuracy_score(target_test, target_predicted_gbc)\nprec_gbc = precision_score(target_test, target_predicted_gbc)\nrecall_gbc = recall_score(target_test, target_predicted_gbc)\nf1_gbc = f1_score(target_test, target_predicted_gbc)\ncm_gbc = confusion_matrix(target_test, target_predicted_gbc)\nprint(\"GBC Accuracy Score\", acc_gbc)\nprint(classification_report(target_test, target_predicted_gbc))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_gbc))","caf9431b":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_gbc, annot=True, fmt='d')\nplt.title('Gradient Boost Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_gbc))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","2e61e1a7":"#verify RF with Cross Validation\nscores_gbc = cross_val_score(clf_gbc, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_gbc)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_gbc.mean(), scores_gbc.std() * 2))","f5261d36":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_gbc.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_gbc = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_gbc)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_gbc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boost Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","8466059b":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,10))\nk_scores = []\nfor k in k_range:\n    gbc = GradientBoostingClassifier(max_depth=k)\n    scores = cross_val_score(gbc, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    gbc.fit(features_train, target_train)\n    train_pred = gbc.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = gbc.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","ac2021fb":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Max Depth')\nplt.show()","5a61f4f7":"# Random Forest train model. Call up my model and name it clf\nclf_gbc1 = GradientBoostingClassifier(max_depth=2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_gbc1)\n#Fit clf to the training data\nclf_gbc1 = clf_gbc1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_gbc1 = clf_gbc1.predict(features_test)","8cdf7cb0":"acc_gbc1 = accuracy_score(target_test, target_predicted_gbc1)\nprec_gbc1 = precision_score(target_test, target_predicted_gbc1)\nrecall_gbc1 = recall_score(target_test, target_predicted_gbc1)\nf1_gbc1 = f1_score(target_test, target_predicted_gbc1)\ncm_gbc1 = confusion_matrix(target_test, target_predicted_gbc1)\nprint(\"GBC Accuracy Score\", acc_gbc1)\nprint(classification_report(target_test, target_predicted_gbc1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_gbc1))","98ede8bb":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_gbc1, annot=True, fmt='d')\nplt.title('Gradient Boost Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_gbc1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","e09a3036":"#verify RF with Cross Validation\nscores_gbc1 = cross_val_score(clf_gbc1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_gbc1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_gbc1.mean(), scores_gbc1.std() * 2))","7353733b":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_gbc1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_gbc1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_gbc1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_gbc1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boost Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","df3485ab":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(100,200))\nk_scores = []\nfor k in k_range:\n    gbc = GradientBoostingClassifier(n_estimators=k, max_depth=2)\n    scores = cross_val_score(gbc, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    gbc.fit(features_train, target_train)\n    train_pred = gbc.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = gbc.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","f3d29538":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('N Estimators')\nplt.show()","c5fc5453":"# Random Forest train model. Call up my model and name it clf\nclf_gbc2 = GradientBoostingClassifier(n_estimators=140, max_depth=2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_gbc2)\n#Fit clf to the training data\nclf_gbc2 = clf_gbc2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_gbc2 = clf_gbc2.predict(features_test)","b4402806":"acc_gbc2 = accuracy_score(target_test, target_predicted_gbc2)\nprec_gbc2 = precision_score(target_test, target_predicted_gbc2)\nrecall_gbc2 = recall_score(target_test, target_predicted_gbc2)\nf1_gbc2 = f1_score(target_test, target_predicted_gbc2)\ncm_gbc2 = confusion_matrix(target_test, target_predicted_gbc2)\nprint(\"GBC Accuracy Score\", acc_gbc2)\nprint(classification_report(target_test, target_predicted_gbc2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_gbc2))","e6601fea":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_gbc2, annot=True, fmt='d')\nplt.title('Gradient Boost Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_gbc2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","d072724e":"#verify RF with Cross Validation\nscores_gbc2 = cross_val_score(clf_gbc2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_gbc2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_gbc2.mean(), scores_gbc2.std() * 2))","4262bbf7":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_gbc2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_gbc2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_gbc2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_gbc2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Gradient Boost Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","2f9b29a8":"t = Table()\nt[''] = ['Gradient Boast Model 1','Gradient Boast Model 2','Gradient Boast Model 3']\nt['Cross Validation Score'] = [round(scores_gbc.mean(),4),round(scores_gbc1.mean(),4),round(scores_gbc2.mean(),4)]\nt['Accuracy Score'] = [round(acc_gbc,4),round(acc_gbc1,4),round(acc_gbc2,4)]\nt['Precision'] = [round(prec_gbc,4),round(prec_gbc1,4),round(prec_gbc2,4)]\nt['Recall'] = [round(recall_gbc,4),round(recall_gbc1,4),round(recall_gbc2,4)]\nt['F1 Score'] = [round(f1_gbc,4),round(f1_gbc1,4),round(f1_gbc2,4)]\nt['ROC AUC'] = [round(roc_auc_gbc,4),round(roc_auc_gbc1,4),round(roc_auc_gbc2,4)]\nt","52ed56e6":"from sklearn.linear_model import SGDClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_sgd_huber = SGDClassifier(loss='modified_huber', penalty='l2', n_jobs=-1, max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_sgd_huber)\n#Fit clf to the training data\nclf_sgd_huber = clf_sgd_huber.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_sgd_huber = clf_sgd_huber.predict(features_test)","3b083ba8":"acc_sgd_huber = accuracy_score(target_test, target_predicted_sgd_huber)\nprec_sgd_huber = precision_score(target_test, target_predicted_sgd_huber)\nrecall_sgd_huber = recall_score(target_test, target_predicted_sgd_huber)\nf1_sgd_huber = f1_score(target_test, target_predicted_sgd_huber)\ncm_sgd_huber = confusion_matrix(target_test, target_predicted_sgd_huber)\nprint(\"SGD Accuracy Score\", acc_sgd_huber)\nprint(classification_report(target_test, target_predicted_sgd_huber))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_sgd_huber))","4892dc44":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_sgd_huber, annot=True, fmt='d')\nplt.title('Stochastic Gradient Descent \"Modified Huber\" Model Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_sgd_huber))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","381fc70f":"#verify RF with Cross Validation\nscores_sgd_huber = cross_val_score(clf_sgd_huber, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_sgd_huber)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_sgd_huber.mean(), scores_sgd_huber.std() * 2))","6185896d":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_sgd_huber.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_sgd_huber = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_sgd_huber)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_sgd_huber)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Stochastic Gradient Descent \"Modified Huber\" Model ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","4d301032":"# Random Forest train model. Call up my model and name it clf\nclf_sgd_log = SGDClassifier(loss='log', n_jobs=-1, max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_sgd_log)\n#Fit clf to the training data\nclf_sgd_log = clf_sgd_log.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_sgd_log = clf_sgd_log.predict(features_test)","73939e6d":"acc_sgd_log = accuracy_score(target_test, target_predicted_sgd_log)\nprec_sgd_log = precision_score(target_test, target_predicted_sgd_log)\nrecall_sgd_log = recall_score(target_test, target_predicted_sgd_log)\nf1_sgd_log = f1_score(target_test, target_predicted_sgd_log)\ncm_sgd_log = confusion_matrix(target_test, target_predicted_sgd_log)\nprint(\"SGD Accuracy Score\", acc_sgd_log)\nprint(classification_report(target_test, target_predicted_sgd_log))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_sgd_log))","6917d74a":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_sgd_log, annot=True, fmt='d')\nplt.title('Stochastic Gradient Descent \"Log\" Model Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_sgd_log))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","107dd398":"#verify RF with Cross Validation\nscores_sgd_log = cross_val_score(clf_sgd_log, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_sgd_log)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_sgd_log.mean(), scores_sgd_log.std() * 2))","49743bc1":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_sgd_log.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_sgd_log = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_sgd_log)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_sgd_log)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Stochastic Gradient Descent \"Log\" Model ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","711c747f":"# Random Forest train model. Call up my model and name it clf\nclf_sgd_logl1 = SGDClassifier(loss='log', penalty='l1', n_jobs=-1, max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_sgd_logl1)\n#Fit clf to the training data\nclf_sgd_logl1 = clf_sgd_logl1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_sgd_logl1 = clf_sgd_logl1.predict(features_test)","afb87006":"acc_sgd_logl1 = accuracy_score(target_test, target_predicted_sgd_logl1)\nprec_sgd_logl1 = precision_score(target_test, target_predicted_sgd_logl1)\nrecall_sgd_logl1 = recall_score(target_test, target_predicted_sgd_logl1)\nf1_sgd_logl1 = f1_score(target_test, target_predicted_sgd_logl1)\ncm_sgd_logl1 = confusion_matrix(target_test, target_predicted_sgd_logl1)\nprint(\"SGD Accuracy Score\", acc_sgd_logl1)\nprint(classification_report(target_test, target_predicted_sgd_logl1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_sgd_logl1))","475b14e0":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_sgd_logl1, annot=True, fmt='d')\nplt.title('Stochastic Gradient Descent \"Log L1\" Model Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_sgd_logl1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","398cf5a9":"#verify RF with Cross Validation\nscores_sgd_logl1 = cross_val_score(clf_sgd_logl1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_sgd_logl1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_sgd_logl1.mean(), scores_sgd_logl1.std() * 2))","52952380":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_sgd_logl1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_sgd_logl1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_sgd_logl1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_sgd_logl1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Stochastic Gradient Descent \"Log L1\" Model ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","8e7109b1":"t = Table()\nt[''] = ['SGD Modified Huber Model','SGD Log Model','SGD Log L1 Model']\nt['Cross Validation Score'] = [round(scores_sgd_huber.mean(),4),round(scores_sgd_log.mean(),4),round(scores_sgd_logl1.mean(),4)]\nt['Accuracy Score'] = [round(acc_sgd_huber,4),round(acc_sgd_log,4),round(acc_sgd_logl1,4)]\nt['Precision'] = [round(prec_sgd_huber,4),round(prec_sgd_log,4),round(prec_sgd_logl1,4)]\nt['Recall'] = [round(recall_sgd_huber,4),round(recall_sgd_log,4),round(recall_sgd_logl1,4)]\nt['F1 Score'] = [round(f1_sgd_huber,4),round(f1_sgd_log,4),round(f1_sgd_logl1,4)]\nt['ROC AUC'] = [round(roc_auc_sgd_huber,4),round(roc_auc_sgd_log,4),round(roc_auc_sgd_logl1,4)]\nt","5c01785a":"from sklearn.svm import LinearSVC\n# Random Forest train model. Call up my model and name it clf\nclf_lsvm = LinearSVC()\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_lsvm)\n#Fit clf to the training data\nclf_lsvm = clf_lsvm.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_lsvm = clf_lsvm.predict(features_test)","1907bd4c":"acc_lsvm = accuracy_score(target_test, target_predicted_lsvm)\nprec_lsvm = precision_score(target_test, target_predicted_lsvm)\nrecall_lsvm = recall_score(target_test, target_predicted_lsvm)\nf1_lsvm = f1_score(target_test, target_predicted_lsvm)\ncm_lsvm = confusion_matrix(target_test, target_predicted_lsvm)\nprint(\"LSVM Accuracy Score\", acc_lsvm)\nprint(classification_report(target_test, target_predicted_lsvm))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_lsvm))","8b929014":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_lsvm, annot=True, fmt='d')\nplt.title('Linear Support Vector Classification Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_lsvm))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","1078de6e":"#verify RF with Cross Validation\nscores_lsvm = cross_val_score(clf_lsvm, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_lsvm)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_lsvm.mean(), scores_lsvm.std() * 2))","745e8390":"# Random Forest train model. Call up my model and name it clf\nclf_lsvml1 = LinearSVC(penalty='l1',dual=False)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_lsvml1)\n#Fit clf to the training data\nclf_lsvml1 = clf_lsvml1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_lsvml1 = clf_lsvml1.predict(features_test)","a0f7a85a":"acc_lsvml1 = accuracy_score(target_test, target_predicted_lsvml1)\nprec_lsvml1 = precision_score(target_test, target_predicted_lsvml1)\nrecall_lsvml1 = recall_score(target_test, target_predicted_lsvml1)\nf1_lsvml1 = f1_score(target_test, target_predicted_lsvml1)\ncm_lsvml1 = confusion_matrix(target_test, target_predicted_lsvml1)\nprint(\"LSVM Accuracy Score\", acc_lsvml1)\nprint(classification_report(target_test, target_predicted_lsvml1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_lsvml1))","d38e250a":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_lsvml1, annot=True, fmt='d')\nplt.title('Linear Support Vector Classification Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_lsvml1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","f041a592":"#verify RF with Cross Validation\nscores_lsvml1 = cross_val_score(clf_lsvml1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_lsvml1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_lsvml1.mean(), scores_lsvml1.std() * 2))","a7d15069":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\nk_range = list(range(1,10))\nk_scores = []\nfor k in k_range:\n    lsvm = LinearSVC(penalty='l2',C=k,dual=False,fit_intercept=False)\n    scores = cross_val_score(lsvm, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    lsvm.fit(features_train, target_train)\n    train_pred = lsvm.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = lsvm.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","c224be7b":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('C Value')\nplt.show()","a6a49cc7":"# Random Forest train model. Call up my model and name it clf\nclf_lsvml2 = LinearSVC(penalty='l2',C=3,dual=False,fit_intercept=False)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_lsvml2)\n#Fit clf to the training data\nclf_lsvml2 = clf_lsvml2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_lsvml2 = clf_lsvml2.predict(features_test)","8171c8d2":"acc_lsvml2 = accuracy_score(target_test, target_predicted_lsvml2)\nprec_lsvml2 = precision_score(target_test, target_predicted_lsvml2)\nrecall_lsvml2 = recall_score(target_test, target_predicted_lsvml2)\nf1_lsvml2 = f1_score(target_test, target_predicted_lsvml2)\ncm_lsvml2 = confusion_matrix(target_test, target_predicted_lsvml2)\nprint(\"LSVM Accuracy Score\", acc_lsvml2)\nprint(classification_report(target_test, target_predicted_lsvml2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_lsvml2))","158870b3":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_lsvml2, annot=True, fmt='d')\nplt.title('Linear Support Vector Classification Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_lsvml2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","1f43f965":"#verify RF with Cross Validation\nscores_lsvml2 = cross_val_score(clf_lsvml2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_lsvml2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_lsvml2.mean(), scores_lsvml2.std() * 2))","091ff3e6":"t = Table()\nt[''] = ['Linear SVC Model 1','Linear SVC Model 2','Linear SVC Model 3']\nt['Cross Validation Score'] = [round(scores_lsvm.mean(),4),round(scores_lsvml1.mean(),4),round(scores_lsvml2.mean(),4)]\nt['Accuracy Score'] = [round(acc_lsvm,4),round(acc_lsvml1,4),round(acc_lsvml2,4)]\nt['Precision'] = [round(prec_lsvm,4),round(prec_lsvml1,4),round(prec_lsvml2,4)]\nt['Recall'] = [round(recall_lsvm,4),round(recall_lsvml1,4),round(recall_lsvml2,4)]\nt['F1 Score'] = [round(f1_lsvm,4),round(f1_lsvml1,4),round(f1_lsvml2,4)]\nt","75537e23":"from sklearn.svm import SVC\n# Random Forest train model. Call up my model and name it clf\nclf_svc = SVC(probability=True, gamma='auto', max_iter=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_svc)\n#Fit clf to the training data\nclf_svc = clf_svc.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_svc = clf_svc.predict(features_test)","d1492d76":"acc_svc = accuracy_score(target_test, target_predicted_svc)\nprec_svc = precision_score(target_test, target_predicted_svc)\nrecall_svc = recall_score(target_test, target_predicted_svc)\nf1_svc = f1_score(target_test, target_predicted_svc)\ncm_svc = confusion_matrix(target_test, target_predicted_svc)\nprint(\"SVC Accuracy Score\", acc_svc)\nprint(classification_report(target_test, target_predicted_svc))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_svc))","31048ea7":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_svc, annot=True, fmt='d')\nplt.title('SVC Mode 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_svc))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","8bdf93f7":"#verify RF with Cross Validation\nscores_svc = cross_val_score(clf_svc, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_svc)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_svc.mean(), scores_svc.std() * 2))","cf02f621":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_svc.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_svc = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_svc)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_svc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('SVM Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","b0a90f03":"# Random Forest train model. Call up my model and name it clf\nclf_svc1 = SVC(probability=True, gamma='scale', max_iter=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_svc1)\n#Fit clf to the training data\nclf_svc1 = clf_svc1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_svc1 = clf_svc1.predict(features_test)","d38ea9bf":"acc_svc1 = accuracy_score(target_test, target_predicted_svc1)\nprec_svc1 = precision_score(target_test, target_predicted_svc1)\nrecall_svc1 = recall_score(target_test, target_predicted_svc1)\nf1_svc1 = f1_score(target_test, target_predicted_svc1)\ncm_svc1 = confusion_matrix(target_test, target_predicted_svc1)\nprint(\"SVC Accuracy Score\", acc_svc1)\nprint(classification_report(target_test, target_predicted_svc1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_svc1))","7b56caaa":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_svc1, annot=True, fmt='d')\nplt.title('SVC Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_svc1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","e16a3c66":"#verify RF with Cross Validation\nscores_svc1 = cross_val_score(clf_svc1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_svc1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_svc1.mean(), scores_svc1.std() * 2))","8b1ecf08":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_svc1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_svc1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_svc1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_svc1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Support Vector RBF Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","07540712":"train_results = []\ntest_results = []\n# search for an optimal value of max_depth for decision tree\n#k_range = list(range(1,100))\nk_range = np.linspace(0.1, 2.0, 20, endpoint=True)\nk_scores = []\nfor k in k_range:\n    svc = SVC(C=k, probability=True, gamma='scale', max_iter=-1)\n    scores = cross_val_score(svc, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    k_scores.append(scores.mean())\n    # Code for plotting results\n    svc.fit(features_train, target_train)\n    train_pred = svc.predict(features_train)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_train, train_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous train results\n    train_results.append(roc_auc)\n    y_pred = svc.predict(features_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(target_test, y_pred)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    # Add auc score to previous test results\n    test_results.append(roc_auc)\nif DEBUG:\n    print(k_scores)","88786425":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(k_range, train_results, 'b', label=\"Train AUC\")\nline2, = plt.plot(k_range, test_results, 'r', label=\"Test AUC\")\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('C Value')\nplt.show()","bbf73d72":"# Random Forest train model. Call up my model and name it clf\nclf_svc2 = SVC(C=1.25, probability=True, gamma='scale', max_iter=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_svc2)\n#Fit clf to the training data\nclf_svc2 = clf_svc2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_svc2 = clf_svc2.predict(features_test)","6df319a0":"acc_svc2 = accuracy_score(target_test, target_predicted_svc2)\nprec_svc2 = precision_score(target_test, target_predicted_svc2)\nrecall_svc2 = recall_score(target_test, target_predicted_svc2)\nf1_svc2 = f1_score(target_test, target_predicted_svc2)\ncm_svc2 = confusion_matrix(target_test, target_predicted_svc2)\nprint(\"SVC Accuracy Score\", acc_svc2)\nprint(classification_report(target_test, target_predicted_svc2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_svc2))","afaf4bd9":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_svc2, annot=True, fmt='d')\nplt.title('Support Vector RBF Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_svc2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","66ec7243":"#verify RF with Cross Validation\nscores_svc2 = cross_val_score(clf_svc2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_svc2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_svc2.mean(), scores_svc2.std() * 2))","ab928b7c":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_svc2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_svc2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_svc2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_svc2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Support Vector RBF Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","c86b8492":"t = Table()\nt[''] = ['Support Vector RBF Model 1','Support Vector RBF Model 2','Support Vector RBF Model 3']\nt['Cross Validation Score'] = [round(scores_svc.mean(),4),round(scores_svc1.mean(),4),round(scores_svc2.mean(),4)]\nt['Accuracy Score'] = [round(acc_svc,4),round(acc_svc1,4),round(acc_svc2,4)]\nt['Precision'] = [round(prec_svc,4),round(prec_svc1,4),round(prec_svc2,4)]\nt['Recall'] = [round(recall_svc,4),round(recall_svc1,4),round(recall_svc2,4)]\nt['F1 Score'] = [round(f1_svc,4),round(f1_svc1,4),round(f1_svc2,4)]\nt","d9da826c":"from sklearn.neural_network import MLPClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_NN = MLPClassifier(activation='tanh', solver='adam', hidden_layer_sizes=(20,8), max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_NN)\n#Fit clf to the training data\nclf_NN = clf_NN.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_NN = clf_NN.predict(features_test)","2b4cbdd8":"acc_NN = accuracy_score(target_test, target_predicted_NN)\nprec_NN = precision_score(target_test, target_predicted_NN)\nrecall_NN = recall_score(target_test, target_predicted_NN)\nf1_NN = f1_score(target_test, target_predicted_NN)\ncm_NN = confusion_matrix(target_test, target_predicted_NN)\nprint(\"MLP Accuracy Score\", acc_NN)\nprint(classification_report(target_test, target_predicted_NN))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_NN))","55b0e85d":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_NN, annot=True, fmt='d')\nplt.title('MLP Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_NN))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","fd106c5d":"#verify RF with Cross Validation\nscores_NN = cross_val_score(clf_NN, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_NN)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_NN.mean(), scores_NN.std() * 2))","9886579a":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_NN.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_NN = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_NN)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_NN)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('MLP Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","b1b9ddb9":"# Random Forest train model. Call up my model and name it clf\nclf_NN1 =MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n       beta_1=0.9, beta_2=0.999, early_stopping=False,\n       epsilon=1e-08, hidden_layer_sizes=(15,), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n       warm_start=False)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_NN1)\n#Fit clf to the training data\nclf_NN1 = clf_NN1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_NN1 = clf_NN1.predict(features_test)","2c47e1ed":"acc_NN1 = accuracy_score(target_test, target_predicted_NN1)\nprec_NN1 = precision_score(target_test, target_predicted_NN1)\nrecall_NN1 = recall_score(target_test, target_predicted_NN1)\nf1_NN1 = f1_score(target_test, target_predicted_NN1)\ncm_NN1 = confusion_matrix(target_test, target_predicted_NN1)\nprint(\"MLP Accuracy Score\", acc_NN1)\nprint(classification_report(target_test, target_predicted_NN1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_NN1))","e24d32bd":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_NN1, annot=True, fmt='d')\nplt.title('MLP Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_NN1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","e3771dd8":"#verify RF with Cross Validation\nscores_NN1 = cross_val_score(clf_NN1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_NN1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_NN1.mean(), scores_NN1.std() * 2))","c0ac8e93":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_NN1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_NN1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_NN1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_NN1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('MLP Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","5121cea6":"# Random Forest train model. Call up my model and name it clf\nclf_NN2 = MLPClassifier(solver='sgd', hidden_layer_sizes=(30,30,30),max_iter=1000)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_NN2)\n#Fit clf to the training data\nclf_NN2 = clf_NN2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_NN2 = clf_NN2.predict(features_test)","68b43b50":"acc_NN2 = accuracy_score(target_test, target_predicted_NN2)\nprec_NN2 = precision_score(target_test, target_predicted_NN2)\nrecall_NN2 = recall_score(target_test, target_predicted_NN2)\nf1_NN2 = f1_score(target_test, target_predicted_NN2)\ncm_NN2 = confusion_matrix(target_test, target_predicted_NN2)\nprint(\"MLP Accuracy Score\", acc_NN2)\nprint(classification_report(target_test, target_predicted_NN2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_NN2))","eb02518c":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_NN2, annot=True, fmt='d')\nplt.title('MLP Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_NN2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","f20094ac":"#verify RF with Cross Validation\nscores_NN2 = cross_val_score(clf_NN2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_NN2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_NN2.mean(), scores_NN2.std() * 2))","87e4b88d":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_NN2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_NN2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_NN2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_NN2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('MLP Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","9ea2e88f":"t = Table()\nt[''] = ['MLP Classifier Model 1','MLP Classifier Model 2','MLP Classifier Model 3']\nt['Cross Validation Score'] = [round(scores_NN.mean(),4),round(scores_NN1.mean(),4),round(scores_NN2.mean(),4)]\nt['Accuracy Score'] = [round(acc_NN,4),round(acc_NN1,4),round(acc_NN2,4)]\nt['Precision'] = [round(prec_NN,4),round(prec_NN1,4),round(prec_NN2,4)]\nt['Recall'] = [round(recall_NN,4),round(recall_NN1,4),round(recall_NN2,4)]\nt['F1 Score'] = [round(f1_NN,4),round(f1_NN1,4),round(f1_NN2,4)]\nt['ROC AUC'] = [round(roc_auc_NN,4),round(roc_auc_NN1,4),round(roc_auc_NN2,4)]\nt","dce0b294":"from sklearn.ensemble import AdaBoostClassifier\n# Random Forest train model. Call up my model and name it clf\nclf_ada = AdaBoostClassifier(base_estimator=clf_dt2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_ada)\n#Fit clf to the training data\nclf_ada = clf_ada.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_ada = clf_ada.predict(features_test)","0aaf6d48":"acc_ada = accuracy_score(target_test, target_predicted_ada)\nprec_ada = precision_score(target_test, target_predicted_ada)\nrecall_ada = recall_score(target_test, target_predicted_ada)\nf1_ada = f1_score(target_test, target_predicted_ada)\ncm_ada = confusion_matrix(target_test, target_predicted_ada)\nprint(\"Ada Accuracy Score\", acc_ada)\nprint(classification_report(target_test, target_predicted_ada))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_ada))","356e19f4":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_ada, annot=True, fmt='d')\nplt.title('AdaBoost Classifier Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_ada))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","af968596":"#verify RF with Cross Validation\nscores_ada = cross_val_score(clf_ada, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_ada)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_ada.mean(), scores_ada.std() * 2))","c080bc7e":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_ada.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_ada = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_ada)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_ada)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AdaBoost Classifier Model 1 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","d3324643":"# Random Forest train model. Call up my model and name it clf\nclf_ada1 = AdaBoostClassifier(base_estimator=clf_gbc2)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_ada1)\n#Fit clf to the training data\nclf_ada1 = clf_ada1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_ada1 = clf_ada1.predict(features_test)","9fb7c934":"acc_ada1 = accuracy_score(target_test, target_predicted_ada1)\nprec_ada1 = precision_score(target_test, target_predicted_ada1)\nrecall_ada1 = recall_score(target_test, target_predicted_ada1)\nf1_ada1 = f1_score(target_test, target_predicted_ada1)\ncm_ada1 = confusion_matrix(target_test, target_predicted_ada1)\nprint(\"Ada Accuracy Score\", acc_ada1)\nprint(classification_report(target_test, target_predicted_ada1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_ada1))","8eb50f3e":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_ada1, annot=True, fmt='d')\nplt.title('AdaBoost Classifier Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_ada1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","487daa0f":"#verify RF with Cross Validation\nscores_ada1 = cross_val_score(clf_ada1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_ada1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_ada1.mean(), scores_ada1.std() * 2))","fc1e0ab6":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_ada1.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_ada1 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_ada1)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_ada1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AdaBoost Classifier Model 2 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","37733242":"# Random Forest train model. Call up my model and name it clf\nclf_ada2 = AdaBoostClassifier(base_estimator=clf_svc)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_ada2)\n#Fit clf to the training data\nclf_ada2 = clf_ada2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_ada2 = clf_ada2.predict(features_test)","4d6c5f91":"acc_ada2 = accuracy_score(target_test, target_predicted_ada2)\nprec_ada2 = precision_score(target_test, target_predicted_ada2)\nrecall_ada2 = recall_score(target_test, target_predicted_ada2)\nf1_ada2 = f1_score(target_test, target_predicted_ada2)\ncm_ada2 = confusion_matrix(target_test, target_predicted_ada2)\nprint(\"Ada Accuracy Score\", acc_ada2)\nprint(classification_report(target_test, target_predicted_ada2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_ada2))","3808f46b":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_ada2, annot=True, fmt='d')\nplt.title('AdaBoost Classifier Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_ada2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","5a34d1f2":"#verify RF with Cross Validation\nscores_ada2 = cross_val_score(clf_ada2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_ada2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_ada2.mean(), scores_ada2.std() * 2))","692eb16b":"# Determine the false positive and true positive rates\nfpr, tpr, _ = roc_curve(target_test, clf_ada2.predict_proba(features_test)[:,1]) \n    \n# Calculate the AUC\nroc_auc_ada2 = auc(fpr, tpr)\nprint('ROC AUC: %0.3f' % roc_auc_ada2)\n \n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc_ada2)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AdaBoost Classifier Model 3 ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","ab75b7f7":"t = Table()\nt[''] = ['AdaBoost Classifier Model 1','AdaBoost Classifier Model 2','AdaBoost Classifier Model 3']\nt['Cross Validation Score'] = [round(scores_ada.mean(),4),round(scores_ada1.mean(),4),round(scores_ada2.mean(),4)]\nt['Accuracy Score'] = [round(acc_ada,4),round(acc_ada1,4),round(acc_ada2,4)]\nt['Precision'] = [round(prec_ada,4),round(prec_ada1,4),round(prec_ada2,4)]\nt['Recall'] = [round(recall_ada,4),round(recall_ada1,4),round(recall_ada2,4)]\nt['F1 Score'] = [round(f1_ada,4),round(f1_ada1,4),round(f1_ada2,4)]\nt","3f7fb377":"from sklearn.ensemble import VotingClassifier\n# Random Forest train model. Call up my model and name it clf\nclf1 = clf_knn2\nclf2 = clf_dt2\nclf3 = clf_rf\nclf_eclf = VotingClassifier(estimators=[('knn', clf1), ('dt', clf2), ('rf', clf3)], voting='hard', n_jobs=-1)\n\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf)\n#Fit clf to the training data\nclf_eclf = clf_eclf.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf = clf_eclf.predict(features_test)","16dbca10":"acc_eclf = accuracy_score(target_test, target_predicted_eclf)\nprec_eclf = precision_score(target_test, target_predicted_eclf)\nrecall_eclf = recall_score(target_test, target_predicted_eclf)\nf1_eclf = f1_score(target_test, target_predicted_eclf)\ncm_eclf = confusion_matrix(target_test, target_predicted_eclf)\nprint(\"Stacking Accuracy Score\", acc_eclf)\nprint(classification_report(target_test, target_predicted_eclf))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf))","c8f27ee3":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf, annot=True, fmt='d')\nplt.title('Stacking Model 1 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","fe3f82bd":"#verify RF with Cross Validation\nscores_eclf = cross_val_score(clf_eclf, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_eclf.mean(), scores_eclf.std() * 2))","6d5e3cc3":"for MV, label in zip([clf1, clf2, clf3, clf_eclf], ['KNN', 'Decision Tree', 'Random Forest', 'Ensemble Model 1']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+\/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","725d029b":"# Random Forest train model. Call up my model and name it clf\nclf1 = clf_bag\nclf2 = clf_xdt2\nclf3 = clf_gbc2\nclf_eclf1 = VotingClassifier(estimators=[('bag', clf1), ('xdt', clf2), ('sgd', clf3)], voting='hard', n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf1)\n#Fit clf to the training data\nclf_eclf1 = clf_eclf1.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf1 = clf_eclf1.predict(features_test)","a7455a4d":"acc_eclf1 = accuracy_score(target_test, target_predicted_eclf1)\nprec_eclf1 = precision_score(target_test, target_predicted_eclf1)\nrecall_eclf1 = recall_score(target_test, target_predicted_eclf1)\nf1_eclf1 = f1_score(target_test, target_predicted_eclf1)\ncm_eclf1 = confusion_matrix(target_test, target_predicted_eclf1)\nprint(\"Stacking Accuracy Score\", acc_eclf1)\nprint(classification_report(target_test, target_predicted_eclf1))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf1))","a863d691":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf1, annot=True, fmt='d')\nplt.title('Stacking Model 2 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf1))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","fab07341":"#verify RF with Cross Validation\nscores_eclf1 = cross_val_score(clf_eclf1, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf1)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_eclf1.mean(), scores_eclf1.std() * 2))","f5a4d1b1":"for MV, label in zip([clf1, clf2, clf3, clf_eclf1], ['Bagging', 'Extra Trees', 'Gradient Boost Classification', 'Ensemble Model 2']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+\/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","cd3895bf":"# Random Forest train model. Call up my model and name it clf\nclf1 = clf_sgd_log\nclf2 = clf_lsvm\nclf3 = clf_svc\nclf4 = clf_NN2\nclf_eclf2 = VotingClassifier(estimators=[('sgd', clf1), ('lsvm', clf2), ('svc', clf3), ('nn', clf4)], voting='hard', n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf2)\n#Fit clf to the training data\nclf_eclf2 = clf_eclf2.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf2 = clf_eclf2.predict(features_test)","702356a4":"acc_eclf2 = accuracy_score(target_test, target_predicted_eclf2)\nprec_eclf2  = precision_score(target_test, target_predicted_eclf2)\nrecall_eclf2 = recall_score(target_test, target_predicted_eclf2)\nf1_eclf2 = f1_score(target_test, target_predicted_eclf2)\ncm_eclf2 = confusion_matrix(target_test, target_predicted_eclf2)\nprint(\"Stacking Accuracy Score\", acc_eclf2)\nprint(classification_report(target_test, target_predicted_eclf2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf2))","2b53a9eb":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf2, annot=True, fmt='d')\nplt.title('Stacking Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","19574269":"#verify RF with Cross Validation\nscores_eclf2 = cross_val_score(clf_eclf2, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf2)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_eclf2.mean(), scores_eclf2.std() * 2))","f32dabfc":"for MV, label in zip([clf1, clf2, clf3, clf4, clf_eclf2], ['Stochastic Gradient Descent', 'Linear Support Vector Classification', 'Support Vector Model RBF', 'Neural Network', 'Ensemble Model 3']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+\/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","d49ceb45":"# Random Forest train model. Call up my model and name it clf\nclf1 = clf_dt2\nclf2 = clf_rf\nclf3 = clf_xdt2\nclf4 = clf_gbc2\nclf5 = clf_lsvm\nclf6 = clf_NN2\nclf_eclf3 = VotingClassifier(estimators=[('dt', clf1), ('rf', clf2), ('xdt', clf3), ('gbc', clf4), ('lsvm', clf5), ('nn', clf6)], voting='hard', n_jobs=-1)\n#clf_eclf3 = VotingClassifier(estimators=[('knn', clf1), ('dt', clf2), ('rf', clf3), ('bag', clf4), ('xdt', clf5), ('gbc', clf6), ('sgd', clf7), ('lsvm', clf8), ('svc', clf9), ('nn', clf10)], voting='hard', n_jobs=-1)\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(clf_eclf3)\n#Fit clf to the training data\nclf_eclf3 = clf_eclf3.fit(features_train, target_train)\n#Predict clf DT model again test data\ntarget_predicted_eclf3 = clf_eclf3.predict(features_test)","fee9ee36":"acc_eclf3 = accuracy_score(target_test, target_predicted_eclf3)\nprec_eclf3 = precision_score(target_test, target_predicted_eclf3)\nrecall_eclf3 = recall_score(target_test, target_predicted_eclf3)\nf1_eclf3 = f1_score(target_test, target_predicted_eclf3)\ncm_eclf3 = confusion_matrix(target_test, target_predicted_eclf3)\nprint(\"Stacking Accuracy Score\", acc_eclf3)\nprint(classification_report(target_test, target_predicted_eclf3))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_eclf3))","b5b2b6f6":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_eclf3, annot=True, fmt='d')\nplt.title('Stacking Model 4 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_eclf3))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","a273989f":"#verify RF with Cross Validation\nscores_eclf3 = cross_val_score(clf_eclf3, features_train, target_train, cv=10, n_jobs=-1)\nprint(\"Cross Validation Score for each K\",scores_eclf3)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores_eclf3.mean(), scores_eclf3.std() * 2))","b2019442":"for MV, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf_eclf3], ['Decision Tree','Random Forest','Extra Trees','Gradient Boost Classification','Linear Support Vector Classificaiton','Neural Network','Ensemble Model 4']):\n    scores2 = cross_val_score(MV, features_train, target_train, cv=10, scoring='recall', n_jobs=-1)\n    print(\"Recall: %0.2f (+\/- %0.2f) [%s]\" % (scores2.mean(), scores2.std(), label))","58fb04cd":"t = Table()\nt[''] = ['Stacking Model 1','Stacking Model 2','Stacking Model 3','Stacking Model 4']\nt['Cross Validation Score'] = [round(scores_eclf.mean(),4),round(scores_eclf1.mean(),4),round(scores_eclf2.mean(),4),round(scores_eclf3.mean(),4)]\nt['Accuracy Score'] = [round(acc_eclf,4),round(acc_eclf1,4),round(acc_eclf2,4),round(acc_eclf3,4)]\nt['Precision'] = [round(prec_eclf,4),round(prec_eclf1,4),round(prec_eclf2,4),round(prec_eclf3,4)]\nt['Recall'] = [round(recall_eclf,4),round(recall_eclf1,4),round(recall_eclf2,4),round(recall_eclf3,4)]\nt['F1 Score'] = [round(f1_eclf,4),round(f1_eclf1,4),round(f1_eclf2,4),round(f1_eclf3,4)]\nt","4d97229f":"acc_dt2 = accuracy_score(target_test, target_predicted_dt2)\nprec_dt2 = precision_score(target_test, target_predicted_dt2)\nrecall_dt2 = recall_score(target_test, target_predicted_dt2)\nf1_dt2 = f1_score(target_test, target_predicted_dt2)\ncm_dt2 = confusion_matrix(target_test, target_predicted_dt2)\nprint(\"DT Accuracy Score\", acc_dt2)\nprint(classification_report(target_test, target_predicted_dt2))\nif DEBUG:\n    print(confusion_matrix(target_test, target_predicted_dt2))","3a6df41d":"# Source: https:\/\/seaborn.pydata.org\/examples\/heatmap_annotation.html\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_dt2, annot=True, fmt='d')\nplt.title('Decision Tree Model 3 Confusion Matrix \\nAccuracy:{0:.3f}'.format(acc_dt2))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","9545d7fd":"!pip install pydotplus\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\nimport collections\n\ndata_feature_names = features[1:]\ndot_data = tree.export_graphviz(clf_dt2, feature_names=data_feature_names, class_names=True, rounded = True, proportion = False, precision = 2, filled = True)\ngraph = pydotplus.graphviz.graph_from_dot_data(dot_data)\ncolors = ('turquoise', 'orange')\nedges = collections.defaultdict(list)\n\nfor edge in graph.get_edge_list():\n    edges[edge.get_source()].append(int(edge.get_destination()))\n\nfor edge in edges:\n    edges[edge].sort()    \n    for i in range(2):\n        dest = graph.get_node(str(edges[edge][i]))[0]\n        dest.set_fillcolor(colors[i])\n\nImage(graph.create_png())","2a7f867c":"From the chart above, I chose **max_leaf_nodes=66**.","31f14311":"In the above array we see that the first feature explains roughly 21% of the variance within our data set while the first two explain 34% and so on.","dcd4c6bf":"The mean score and the 95% confidence interval of the score estimate for the Multi-layer Perceptron Classifier Model 1 is 79% with a variance of 3%.","d7ae4568":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics.","76c78ed7":"For Linear Support Vector Classification Model 2, I set **penalty='l1'** and **dual=False**. The 'l1' leads to coef_ vectors that are sparse. The **dual** parameter selects the algorithm to either solve the dual or primal optimization problem. Prefer **dual=False** when n_samples > n_features.","180e405e":"### KNN Model 2","8b8f3c18":"### Stacking Model Evaluation","238defc5":"#### Decision Tree Model 3 Area Under Curve (AUC)","c2cdaa7a":"The Bagging Classifier Model 2 produced an AUC score of 0.830. From an overall performance perspective, this is consistent with the previous models.","7ccb8131":"#### Extra Trees Model 2 Accuracy Scores and Confusion Matrix","1c9ffd71":"#### Support Vector RBF Model 3 Area Under Curve (AUC)","cf65c59c":"Cross-validation, sometimes called out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. Cross-validation can be used to compare the performances of different predictive modeling procedures.","b2a30e08":"#### Stochastic Gradient Descent \"Modified Huber\" Model Cross Validation (CV = 10)","3771dec0":"Based on the analysis above, I chose **min_samples_split=0.225** as this produced the highest AUC score for the Train and Test datasets. Since I chose to use a float instead of int, **min_samples_leaf** is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","b0a129c0":"The AUC score of 0.822 is the best of all three KNN models. This shows that this model displays some class separation capacity.","63375dd5":"### Random Forest Model 1","04a43709":"The Random Forest Model 3 produced an accuracy score of 80.2% with a recall of 0.51. So far, no significant change from the previous Random Forest Models.","a541fba4":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 3 is 80% with a variance of 3%.","00606998":"For AdaBoost Classifier Model 2, I choose the Gradient Boost Model 3 as the base_estimator.","7f9b9aa3":"The mean score and the 95% confidence interval of the score estimate for the KNN Model 2 is 75% with a variance of 3%. This model had a slightly lower accuracy score with the same variance.","4af947d8":"### Missing Value Replacement (NaN)","330e8d43":"#### KNN Model 2 Accuracy Scores and Confusion Matrix","2424a617":"### Random Forest Model 3","3c50c03d":"When testing my model against the Test Data, I noticed an overall Accuracy Score of 76.6% with a Recall of 0.48. This model performed similarly in terms of Accuracy but slighty worse in terms of Recall.","69398edc":"Using the [pandas.DataFrame.describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html) method, I generated descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values. This helps me analyzes both numeric and object series, as well as DataFrame column sets of mixed data types.","43c6cc05":"Random Forest Model 2 produced an overall accuracy of 79.7% with a recall score of 0.49.","d9b130ca":"## Categorical Feature Conversion to Dummy Values","d32c3547":"#### AdaBoost Classifier Model 2 Area Under Curve (AUC)","a473c3a2":"#### Gradient Boost Model 2 Area Under Curve (AUC)","2ddef1cb":"Using the code below, I set the **churn** feature as the target feature and moved this to the beginning of my DataFrame.","deadfcaa":"The mean score and the 95% confidence interval of the score estimate for the Bagging Classifier Model 3 is 80% with a variance of 4%.","017d41ab":"#### Gradient Boost Model 3 Cross Validation (CV = 10)","761eb04d":"## Stacking Model","3cd35f5f":"For Decision Tree Model 3, I focused on changing the **min_samples_split** parameter. This parameter represents the minimum number of samples required to split an internal node. This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node. Here we will vary the parameter from 10% to 100% of the samples.","6a45ef61":"If Contract is Month-to-Month (transformed to 0), we move left to the Online Security leaf. If Contract is One Year or Two Year (1 or 2), we move to the right leaf. Of the 2119 samples, 1989 observations did not churn while 130 did (class = y[0] indicates Churn = No).","9fa0bbbd":"## Cross-Tabulation of Paperless Billing, Contract, and Payment Method with Churn","b805ade8":"This model produced an AUC Score of 0.822 and is consistent with the models we've seen thus far.","fa2e5ea2":"AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. The AUC score of 0.808 shows that this model displays some class separation capacity. An excellent model has AUC closest to 1 which means it has good measure of separability. A poor model has AUC closer to 0 which means it has worse measure of separability.","aede5457":"#### Random Forest Model 2 Accuracy Scores and Confusion Matrix","f8f052ec":"From my analysis, all three models produces very similar performance metrics. While no distinct separation could be made, I chose Random Forest Model 1 as my best model based on the highest Recall score.","90d0af5d":"#### Decision Tree Model 2 Cross Validation (CV = 10)","e3047a21":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model is 95% with a variance of 1%.","ab4498ec":"For Gradient Boost Model 3, I chose to keep **max_depth=2** and optimize **n_estimators** which controls the number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.","c1ccd26b":"#### Linear Support Vector Classification Model 1 Cross Validation (CV = 10)","f6fb615d":"#### Linear Support Vector Classification Model 2 Accuracy Scores and Confusion Matrix","7a1b6c27":"Using the [Linear Support Vector Classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) library, I created three Linear Support Vector Classification Models. I started with a baseline model below with default values.","37996862":"### Stochastic Gradient Descent \"Log L1\" Model","fbeb170f":"From the chart above, as **max_depth** increases, we see the blue line (train data) approach 1.0 while the red line (test data) start to level off at 7. As the lines become further apart, this is where overfitting starts to happen. I chose **max_depth=7** as my tuning parameter to best address this.","4a2cd6f6":"#### KNN Model 1 Area Under Curve (AUC)","621ca248":"## Support Vector Model RBF","4a6970d5":"#### Decision Tree Baseline Model Cross Validation (CV = 10)","b66a9709":"Based on the analysis above, the Bagging Classifier Model 1 (KNN Model 3) produced the best recall score.","0ad561fb":"### AdaBoost Classifier Model 2","2906b9f4":"The Decision Tree Model 2 produced a slightly lower accuracy than Model 1 but had a higher recall score (0.61) for Churn = Yes.","b0643b7a":"### Multiple-layer Perceptron Model Evaluation","65eb7c6f":"From the table above, we can see some statistics on those customers who stayed vs. those who left. On average, those left (churm = yes) had higher **MonthlyCharges** and lower **tenure**.","df5b5a48":"From above, we can see 7032 observations for TotalCharges. Using the [pandas.DataFrame.fillna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.fillna.html) method, I filled the NA\/NaN values using the specified method (median).","ed64dce8":"### Gradient Boost Model 2","f57f7d23":"#### Multi-layer Perceptron Classifier Model 2 Cross Validation (CV = 10)","9ad2e6a9":"### Random Forest Model Evaluation","dad796b6":"Using [pandas.DataFrame.info](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.info.html), I printed a concise summary of the **churn** DataFrame. This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.","3efc97aa":"#### Stochastic Gradient Descent \"Log L1\" Model Accuracy Scores and Confusion Matrix","50473883":"With regards to Recall, the consequence could impact business decisions to minimize customer churn. Given that False Negatives indicates that our model predicted Churn = No but actual Churn = Yes, this performance metric proved to be of great importance. The Decision Tree Model 3 exhibited the best Recall score for all the models. ","e0c017ba":"Initial analysis indicates that customers who use paperless billing, have short-term contracts, and pay by electronic check are more likely to churn.","f0f526c5":"Using the [pandas.DataFrame.dropna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.dropna.html) method with the **how = 'all'** parameter, I removed all observations where all features were **NaN**.","b3ad8272":"The mean score and the 95% confidence interval of the score estimate for the Linear Support Vector Classification Model 1 is 80% with a variance of 4%.","543763ee":"#### Stacking Model 2 Cross Validation (CV = 10)","f814de47":"### Decision Tree Model 3","3c1d62ac":"The Bagging Classifier Model 1 (KNN Model 3) produced an accuracy of 78.5% with a Recall score of 0.56. This is a great start, especially with the Recall score.","be5f9b8b":"#### Support Vector RBF Model 1 Area Under Curve (AUC)","26ef488e":"#### Support Vector RBF Model 2 Accuracy Scores and Confusion Matrix","a0511d24":"Using the [sklearn.svm.SVC](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html) library, I created a C-Support Vector Classification Model. The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. For the first model, I set **gamma='auto'** which uses 1 \/ n_features.","fefc0ece":"Overall, the Decision Tree Models performed better than the KNN models, specicifically with regards to their Recall scores. I noticed a degredation in overall accuracy as I tuned the model but found an increase in the Recall scores. I chose Decision Tree Model 3 as the best DT model based on the highest Recall score thus far.","a6fff87e":"#### Decision Tree Model 2 Accuracy Scores and Confusion Matrix","2f02e682":"I standardized **features_train** and **features_test** using the [sklearn.preprocessing.MinMaxScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html) class to achieve this. This class ransforms features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.","7247cc62":"From the chart above, we can see that customers who have paperless billing are more likely to churn.","bdf5bf23":"# Model Evaluation and Summary","9842a2df":"For the first model, I chose to stack KNN Model 3, Decision Tree Model 3, and Random Forest Model 1 as these were deemed the best for recall.","57d232c6":"#### AdaBoost Classifier Model 2 Accuracy Scores and Confusion Matrix","504d5d34":"The AUC score of 0.792 shows that this model displays some class separation capacity however not as good as KNN Model 1 (0.808).","ae08962c":"## Import Dataset","4c5c2f7f":"In the **churn** dataset, we have two types of customers: those who stayed and those who left the telecom service. We can divide the data into two groups and compare their characteristics. Here, you can find the average of both the groups using the groupby() and mean() functions.","b59179c7":"### Extra Trees Model 3","e3e3fefc":"#### Stacking Model 4 Recall Scores","10f1a8c9":"### Linear Support Vector Classification Model Evaluation","500b664a":"#### Stochastic Gradient Descent \"Log\" Model Cross Validation (CV = 10)","689e95bb":"#### AdaBoost Classifier Model 1 Cross Validation (CV = 10)","a8d2df43":"From the analysis above, the Stochastic Gradient Descent \"Log\" Model was the most performent in terms of recall score.","9ab3ed79":"### Standardize the Dataset","6c3dbd04":"#### Random Forest Model 1 Area Under Curve (AUC)","cafa96b4":"The mean score and the 95% confidence interval of the score estimate for the KNN Model 1 is 76% with a variance of 3%. A high variance indicates that a model is prone to overfitting.","c26b5a91":"## Target Feature Designation","ce54aba1":"Using the [pandas.crosstab](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.17.0\/generated\/pandas.crosstab.html) function, I computed a simple cross-tabulation of two (or more) factors. By default, this computes a frequency table of the factors unless an array of values and an aggregation function are passed.","91a4bdd7":"### Support Vector RBF Model Evaluation","931bb252":"#### Bagging Classifier Model 3 Accuracy Scores and Confusion Matrix","c7af58e6":"#### Support Vector RBF Model 1 Cross Validation (CV = 10)","13b869f8":"#### Extra Trees Model 2 Cross Validation (CV = 10)","58398681":"### Stacking Model 1","ad6e9ff6":"#### Decision Tree Model 3 Cross Validation (CV = 10)","5af53283":"#### Support Vector RBF Model 2 Cross Validation (CV = 10)","57a5a49b":"### Bagging Classifier Model 2","9950e089":"#### Random Forest Model 1 Accuracy Scores and Confusion Matrix","5f4ee963":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics.","dc90dd1d":"#### Multi-layer Perceptron Classifier Model 3 Cross Validation (CV = 10)","717a8f55":"The mean score and the 95% confidence interval of the score estimate for the AdaBoost Classifier Model 3 is 78% with a variance of 3%.","ea5d6c87":"#### Multi-layer Perceptron Classifier Model 3 Area Under Curve (AUC)","93ea5205":"From the analysis above, the AdaBoost Classifier Model 3 was the most performant.","07c3f470":"The mean score and the 95% confidence interval of the score estimate for the Stochastic Gradient Descent \"Log L1\" Model is 80% with a variance of 3%.","6063b043":"## Bagging Classifier Model","5f5cc96d":"#### Extra Trees Model 2 Area Under Curve (AUC)","105b6d48":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 1 is 78% with a variance of 4%.","19e0de56":"From the Decision Tree above, **Contract** and **Online Security** are key factors in determining Churn. We saw this during our initial EDA. **OnlineSecurity** indicates whether the customer has online security or not (Yes, No, No internet service). **Contract** indicates the contract term of the customer (Month-to-month, One year, Two year).","051e9bf6":"The mean score and the 95% confidence interval of the score estimate for the Linear Support Vector Classification Model 2 is 80% with a variance of 4%.","f809d042":"#### Stacking Model 1 Cross Validation (CV = 10)","064b2dd2":"## Linear Support Vector Classification Model","3af6e214":"#### AdaBoost Classifier Model 3 Area Under Curve (AUC)","8a38da41":"## Normalization","b21196c7":"#### Stacking Model 2 Accuracy Scores and Confusion Matrix","19dd450c":"The mean score and the 95% confidence interval of the score estimate for the Decision Tree Model 2 is 75% with a variance of 4%. These values are consistent with what we've seen with all the models thus far.","27a3c0c5":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics.","6f591906":"Using the [Stochastic Gradient Descent](https:\/\/scikit-learn.org\/stable\/modules\/sgd.html) library, I created three Stochastic Gradient Descent Models. For the first model, I set **loss='modified_huber'**. **'modified_huber'** is a smooth loss that brings tolerance to outliers as well as probability estimates.","6c8bc050":"#### Bagging Classifier Model 1 Area Under Curve (AUC)","818ca57c":"This model produced an accuracy of 79.1% and a recall score of 0.46. This did not perform as well as the original Decision Tree Model 1.","d5eb9615":"Using the [sklearn.tree.DecisionTreeClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) library, I created three Decision Tree models. With the first model, I chose the best **max_depth** parameter that would maximize the Recall score.","312730aa":"# Exploratory Data Analysis","e7e9c079":"#### Extra Trees Baseline Model Cross Validation (CV = 10)","251ffbdd":"Based on the results above, I set **min_samples_split=0.5**. Since I used a float, **min_samples_split** is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.","746969b1":"### AdaBoost Classifier Model 1","0fa53e35":"## Check Present Working Directory","1ae03b39":"#### Decision Tree Model 2 Area Under Curve (AUC)","5f61e8f8":"#### Extra Trees Model 3 Area Under Curve (AUC)","beeda824":"#### Stochastic Gradient Descent \"Modified Huber\" Model Area Under Curve (AUC)","5f2e6a8f":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model 3 is 80% with a variance of 4%.","165a56f4":"#### Random Forest Model 2 Cross Validation (CV = 10)","4bbcb2a6":"#### Gradient Boost Model 2 Cross Validation (CV = 10)","c706bbc7":"The mean score and the 95% confidence interval of the score estimate for the Support Vector RBF Model 3 is 79% with a variance of 3%.","40cf327b":"Using the [sklearn.ensemble.GradientBoostingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html) library, I created three models. For the first model, I chose to use default values to develop a baseline.","785c1cbc":"For Stacking Model 3, I chose Stochastic Gradient Descenst \"Log\" Model, Linear Support Vector Classification Model 1, Support Vector RBF Model 1, and Multi-layer Perceptron Classifier 3 as these were deemed the best for recall.","1ff2996b":"For AdaBoost Classifier Model 3, I chose Support Vector RBF Model 1 as base_estimator.","f7ac0649":"## Check Python Version","e9def7e6":"### Linear Support Vector Classification Model 1","f454c69e":"From the analysis above, I choose **n_estimators=17** as this produced the highest AUC score against the Test Data.","95077b08":"## Gradient Boost Model","b10ffef9":"#### Decision Tree Baseline Model Area Under Curve (AUC)","a1abaf8d":"#### Multi-layer Perceptron Classifier Model 3 Accuracy Scores and Confusion Matrix","73b30e08":"## Stochastic Gradient Descent Model","e4b6434b":"The mean score and the 95% confidence interval of the score estimate for the AdaBoost Classifier Model 2 is 77% with a variance of 3%.","cac07fc5":"### Stacking Model 4","087986ef":"The mean score and the 95% confidence interval of the score estimate for the KNN Model 3 is 77% with a variance of 4%. This model has produced the highest cross-validated accuracy but the variance has also increased by 1%.","c2b050f8":"### Bagging Classifier Model 3","e183861e":"Using the [sklearn.ensemble.VotingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html) library, I created three Voting Classifier Models to stack previous models. The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses. For the models below, the VotingClassifier (with voting='hard') would classify the sample as \u201cclass 1\u201d based on the majority class label.","6a065fea":"#### Bagging Classifier Model 1 Accuracy Scores and Confusion Matrix","a223edae":"The mean score and the 95% confidence interval of the score estimate for the Bagging Classifier Model 1 is 77% with a variance of 4%.","4d8d918f":"The mean score and the 95% confidence interval of the score estimate for the Multi-layer Perceptron Classifier Model 3 is 80% with a variance of 3%.","fe113638":"#### Gradient Boost Model 3 Accuracy Scores and Confusion Matrix","9c30e827":"For the model below, I changed **gamma='scale'**. The model now uses 1 \/ (n_features * X.var()) as value of gamma. ","9eb70c31":"### Gradient Boost Model Evaluation","3d4ca116":"#### Gradient Boost Model 1 Cross Validation (CV = 10)","a63f2ee1":"Using the [sklearn.ensemble.AdaBoostClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html) library, I created an AdaBoost Classification Model. An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. For the first boosting model, I chose the Decision Tree Model 1.","37df2fc5":"#### Stochastic Gradient Descent \"Log L1\" Model Cross Validation (CV = 10)","f0ebd82c":"#### Stacking Model 4 Cross Validation (CV = 10)","f193877d":"### Churn by Paperless Billing using [pandas.DataFrame.plot](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.html)","dd016ecd":"#### Multi-layer Perceptron Classifier Model 2 Accuracy Scores and Confusion Matrix","cae94181":"### Linear Support Vector Classification Model 3","c0aba876":"#### Bagging Classifier Model 2 Accuracy Scores and Confusion Matrix","1c33c017":"### Multi-layer Perceptron Classifier Model 2","f9cbbb70":"Below I created train and test (67\/33) datasets to begin model evaluation using the [sklearn.model_selection.train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) method. I set the random number generator seed by keeping **random_state = 0** to replicate consistent results during my analysis.","0b172f9e":"The mean score and the 95% confidence interval of the score estimate for the SVC Model 1 is 80% with a variance of 3%.","f70e6332":"#### KNN Model 3 Cross Validation","6781fa32":"The mean score and the 95% confidence interval of the score estimate for the Gradient Boost Model 1 is 80% with a variance of 3%.","d6cd26fa":"### AdaBoost Classifier Model Evaluation","70219bb3":"## Extra Trees Model","02819e5a":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model 2 is 80% with a variance of 3%. This is consistent with the previous models.","ec9f0af1":"#### AdaBoost Classifier Model 3 Cross Validation (CV = 10)","a09f6549":"The mean score and the 95% confidence interval of the score estimate for the Stochastic Gradient Descent \"Log\" Model is 80% with a variance of 3%.","f9864385":"#### Stochastic Gradient Descent \"Log\" Model Area Under Curve (AUC)","3ac792dd":"From my analysis above, I noticed that **TotalCharges** is listed as an object data type. I used the code below to convert this to a numeric type.","fe385abe":"From the chart above, I chose **min_samples_leaf=7**.","9da5beb3":"### Create a Covariance Matrix","2ba1cb12":"After converting our dummy variables, we can see that gender and PhoneService do not have much impact on churn.","7a36d8f9":"Using the [sklearn.neighbors.KNeighborsClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html) library, I created multiple K-Nearest Neighbors models. The code below interated over a k-range of 25 to 50. My goal is to maximize Recall for Churn = Yes.","a41b8b9c":"#### Bagging Classifier Model 3 Cross Validation (CV = 10)","8776298e":"Decision Tree Model 3 produced the worst accuracy score compared to the first two DT models but did produce the highest recall score of 0.71.","b4a6defd":"#### KNN Model 2 Cross Validation","29aec415":"For the Bagging Classifier Model 3, I chose **Random Forest Model 1** as my base estimator and chose to tune **n_estimators**.","9be66839":"### Stochastic Gradient Descent \"Modified Huber\" Model","14f5824a":"#### Support Vector RBF Model 3 Accuracy Scores and Confusion Matrix","c8c457a5":"## Neural Network Classification Model","1afac5f2":"### Multi-layer Perceptron Classifier Model 1","0ec9ba02":"The AUC score of 0.842 is comparable with the previous models.","fee1d2d3":"From the analysis above,, the Extra Trees Model 3 proved to be the best performing model in terms of recall score.","2b8e3ab9":"The mean score and the 95% confidence interval of the score estimate for the Bagging Classifier Model 2 is 79% with a variance of 4%.","16e3fed0":"#### Stochastic Gradient Descent \"Log\" Model Accuracy Scores and Confusion Matrix","a55c9642":"From the analysis above, I choose **min_samples_leaf=14**. This produced the highest AUC score for the Test Data.","f6c6c5fe":"#### Random Forest Model 2 Area Under Curve (AUC)","a35e78b0":"#### Stochastic Gradient Descent \"Modified Huber\" Model Accuracy Scores and Confusion Matrix","e84b35c2":"### Decision Tree Model 1","edd4cc0f":"#### Support Vector RBF Model 1 Accuracy Scores and Confusion Matrix","a24742c7":"#### Gradient Boost Model 2 Accuracy Scores and Confusion Matrix","7d3738d8":"The mean score and the 95% confidence interval of the score estimate for the Extra Trees Model 2 is 80% with a variance of 3%.","e20d38ed":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics.","08156094":"The AUC score of 0.836 is a comparable with Model 1.","7437d225":"### Stochastic Gradient Descent \"Log\" Model","d62b06c2":"The mean score and the 95% confidence interval of the score estimate for the Random Forest Model 1 is 80% with a variance of 4%. So far, this is a good accuracy score but the variance leads me to believe we can continue to tune this model.","38324e15":"#### Multi-layer Perceptron Classifier Model 2 Area Under Curve (AUC)","9c7c19e4":"From my analysis, the Random Forest and Decision Tree Models performed significantly better than the default KNN=3 Model. Below, I review the metrcs returned and go into some detail behind the metrics.","7c1369b2":"Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance.","9b2a8db1":"#### AdaBoost Classifier Model 1 Accuracy Scores and Confusion Matrix","25c6aa4d":"The mean score and the 95% confidence interval of the score estimate for the Linear Support Vector Classification Model 3 is 80% with a variance of 4%.","b33a9b57":"Using the [sklearn.ensemble.BaggingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html) library, I created three Bagging Classifier Model. A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. For my first model, I choose KNN Model 3 as my base estimator.","a16955fc":"From the analysis above, I choose **n_estimators=10**.","bc0a8f1f":"#### Support Vector RBF Model 3 Accuracy Scores and Confusion Matrix","da25d43d":"From the results above, I chose K=27 for my first model. This value produced the highest AUC score for the Test data set (denoted by the red line).","ffbf0a3b":"For Gradient Boost Model 2, I chose to tune the maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree.","7e578fe9":"#### Support Vector RBF Model 2 Area Under Curve (AUC)","65626778":"For this SGD model, I kept **loss='log'** and changed **penalty='l1'**. 'l1' might bring sparsity to the model (feature selection) not achievable with 'l2'.","7980f498":"From the chart above, we can see that customers who have a short-term (month-to-month) contract are more likely to churn than those who have one or two year contracts.","f4e2022c":"From the chart above, I chose **max_depth=8** as this value seemed to maximize the AUC score of the Test Data. As the tree grows deeper, this model becomes prone to overfitting.","49410080":"#### Extra Trees Model 3 Cross Validation (CV = 10)","d3f3d83c":"For the Online Security leaf, if Online Security is No (transformed to 0), we move the left leaf, else right (class = y[1] indicates Churn = Yes).","8fbd01b0":"For Stacking Model 4, I chose only the models above that produce a recall score greater than or equal to 0.50 with a low variance (STD).","92d52dcd":"### Stacking Model 2","bac4d85a":"#### Stacking Model 1 Recall Scores","729afe7c":"#### Random Forest Model 3 Accuracy Scores and Confusion Matrix","e09fb2aa":"The code below turns the **churn** target feature into numeric so some scikit learn alogrythms can process it. **No** becomes 0 and **Yes** becomes 1 as the [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) function converts in alphabetical order.","c358af34":"When testing my model against the Test Data, I noticed an overall Accuracy Score of 78.1% with a Recall of 0.53.","961dbb66":"The mean score and the 95% confidence interval of the score estimate for the AdaBoost Classifier Model 1 is 78% with a variance of 4%.","ee346d65":"For KNN Model 3, I chose to keep KNN=27, weights = 'uniform' but chose the algorithm 'kd_tree'. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no D-dimensional distances need to be computed. Though the KD tree approach is very fast for low-dimensional (D < 20) neighbors searches, it becomes inefficient as D grows very large: this is one manifestation of the so-called \u201ccurse of dimensionality\u201d. Since my feature set is < 20, I wanted to test if this algorithm improved my model.","a34a99c9":"From above, we can see that this model did produce a high number of False Positives (Predicted Churn = Yes, Actual Churn = No). From a busines perspective, there is not as much cost associated to this versus the False Negatives (Predicted Churn = No, Actual Churn = Yes). Below is a recap of the importance of Recall.","252bc2d2":"#### KNN Model 1 Cross Validation","98954c5c":"From the information above, we can see that we have a combination of **object** and numerical (**int64** and **float64**) data types in our dataframe. We will need to convert the **object** data types to dummy variables or drop them from the data frame depending on their predictive value.","fde58250":"#### Stacking Model 2 Recall Scores","9113ce94":"### Stochastic Gradient Descent Model Evaluation","95a8a38b":"#### Multi-layer Perceptron Classifier Model 1 Accuracy Scores and Confusion Matrix","acda484b":"For Extra Trees Model 2, I chose to keep **max_depth=8** and optimize **min_samples_leaf**.","b1bead03":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 4 is 80% with a variance of 4%.","4cbaec3f":"Decision Tree Model 3 produced an overall performance similar to Model 2 but much less than Model 1.","3fdaf369":"With the model below, I chose to keep KNN=27 but change the weights parameter from the default 'uniform' to 'distance'. \u2018uniform\u2019 indicates uniform weights. All points in each neighborhood are weighted equally. \u2018distance\u2019 indicates weight points by the inverse of their distance. In this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.","62ffeee6":"#### AdaBoost Classifier Model 2 Cross Validation (CV = 10)","71a4f6f9":"#### Decision Tree Baseline Model Accuracy Scores and Confusion Matrix","57154af5":"### KNN Model 1","2fc6e09e":"### Extra Trees Model 2","83769e10":"#### Extra Trees Model 3 Accuracy Scores and Confusion Matrix","7110ad15":"Using the [pandas.read_csv](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html) method, I read the **WA_Fn-UseC_-Telco-Customer-Churn.csv** dataset and stored this in the **churn** DataFrame.","73a85d3d":"#### Stacking Model 1 Accuracy Scores and Confusion Matrix","fba41565":"From the analysis above, I choose **n_estimators=4**.","0c8e55b6":"For Extra Trees Model 3, I kept **max_depth=8**, **min_samples_leaf=7**, and chose to optimize **max_leaf_nodes**. This parameter grows trees with **max_leaf_nodes** in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.","3e10d3a7":"Using the [sklearn.ensemble.RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) library, I created three Random Forest Models. For my first model, I focused on setting the best **max_depth** parameter for my dataset. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. This leads to overfitting.","e790d879":"#### Support Vector RBF Model 3 Cross Validation (CV = 10)","9718ce39":"\"explained variance\" returns the amount of variance explained by each of the selected components. This is equal to n_components largest eigenvalues of the covariance matrix of the dataset.","ef9728e7":"From the analysis above, Multi-layer Perceptron Classifier Model 3 produced the highest accuracy and recall scores.","01c8e39c":"### Decision Tree Model Evaluation","3e94297d":"#### Stochastic Gradient Descent \"Log L1\" Model Area Under Curve (AUC)","62e6dc65":"#### Random Forest Model 3 Cross Validation (CV = 10)","c506fdc8":"## Train Test Split (67\/33)","e5b34ed0":"#### Stacking Model 3 Recall Scores","c134a5c0":"### Multi-layer Perceptron Classifier Model 3","07817225":"## AdaBoost Classifier Model","03e25f1b":"For Stacking Model 2, I chose Bagging Classifier Model 1, Extra Trees Model 3, and Gradient Boost Model 3 as these were deemed the best for recall.","951b1818":"### Bagging Classifier Model Evaluation","a52c77ef":"### Linear Support Vector Classification Model 2","56771db0":"#### Linear Support Vector Classification Model 3 Cross Validation (CV = 10)","764da000":"The mean score and the 95% confidence interval of the score estimate for the Support Vector RBF Model 2 is 79% with a variance of 3%.","a10aa0b3":"### Bagging Classifier Model 1","2c15bfa8":"# Telco Customer Churn Classification\n- Assignment 2\n- [Chris Tan](https:\/\/www.linkedin.com\/in\/christan\/)\n- 4\/28\/2019","effff7e1":"#### Linear Support Vector Classification Model 3 Accuracy Scores and Confusion Matrix","5885b0d3":"#### Support Vector RBF Model 1 Area Under Curve (AUC)","313e3268":"#### Decision Tree Model 3 Accuracy Scores and Confusion Matrix","14bfd405":"For MLP Model 3, I changed **solver='sgd'** and introduced three layers of 30 for hidden_layer_sizes.","96f72aea":"![Recall](https:\/\/cdn-images-1.medium.com\/max\/800\/1*dXkDleGhA-jjZmZ1BlYKXg.png)","70936e3a":"For my analysis, I dropped the **customerID** feature. This is a descriptive feature for each customer and doesn't provide any predictive value.","5bf32b9e":"### KNN Model 3","9cf4c859":"### Support Vector RBF Model 1","30cf4b08":"#### Bagging Classifier Model 3 Area Under Curve (AUC)","e8394d01":"For Random Forest Model 3, I chose to tune **min_samples_leaf**. This parameter determines the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least **min_samples_leaf** training samples in each of the left and right branches.","0d280ffd":"## Recall Definition","ec999c8a":"## Random Forest Model","94046721":"### Stacking Model 3","d571c283":"The mean score and the 95% confidence interval of the score estimate for the Decision Tree Model 3 is 74% with a variance of 5%.","974355a2":"### AdaBoost Classifier Model 3","d8251146":"#### Linear Support Vector Classification Model 2 Cross Validation (CV = 10)","b3e1558c":"#### Bagging Classifier Model 2 Area Under Curve (AUC)","c0d4a1fd":"I created Decision Tree Model 2 and focused on adding the **min_samples_leaf** parameter. **min_samples_split** represents the minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least **min_samples_split** training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.","83b6b12a":"#### KNN Model 2 Area Under Curve (AUC)","c2de65e1":"### KNN Model Evaluation","914b51d5":"#### KNN Model 3 Accuracy Scores and Confusion Matrix","86676ad8":"### Gradient Boost Model 1","c82e8419":"### Churn by Online Security using [pandas.DataFrame.plot](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.html).","4c1ac4e3":"### Extra Trees Model 1","fc7a3da8":"### Calculate Eigenvalues","532d400e":"Based on the analysis above, the Support Vector RBF Model 1 produced the best accuracy and recall scores.","4c5b90d8":"The mean score and the 95% confidence interval of the score estimate for the Multi-layer Perceptron Classifier Model 2 is 79% with a variance of 4%.","ac08e3f4":"#### Extra Trees Baseline Model Area Under Curve (AUC)","b845d727":"For Linear Support Vector Classification Model 3, I set **penalty='l2'** and used the code below to optimize the penalty parameter **C** of the error term.","cefd438b":"The mean score and the 95% confidence interval of the score estimate for the Stochastic Gradient Descent \"Modified Huber\" Model is 80% with a variance of 4%.","c9bf19e2":"To tune SVC Model 2, I kept the existing parameters but chose to optimize the penalty parameter **C** of the error term.","85380ac0":"### Random Forest Model 2","e1687fca":"For Bagging Classifier Model 2, I chose the **Decision Tree Model 1** as my base estimator model and added **n_estimators** as a tuning parameter. This parameter determines the number of base estimators in the ensemble.","9a11cb97":"The Random Forest Model 1 produced an accuracy of 79.3% with a recall score of 0.49. This is comparable to previous models but not any better or worse.","87eb10c7":"#### Random Forest Model 1 Cross Validation (CV = 10)","d61806ae":"#### Linear Support Vector Classification Model 1 Accuracy Scores and Confusion Matrix","8d8b1e6a":"The mean score and the 95% confidence interval of the score estimate for the Extra Trees Model 1 is 80% with a variance of 3%.","ac77fa62":"### Gradient Boost Model 3","b4ed03ed":"## Principal Component Analysis (PCA)","22b7b345":"## Import Libraries","1862ce53":"Using the [sklearn.ensemble.ExtraTreesClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html) library, I created an extra-trees classifier model. This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. For the first model, I chose to optimize the maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than **min_samples_split** samples.","9890da5e":"#### Bagging Classifier Model 2 Cross Validation (CV = 10)","734b5e9a":"PCA dimensionality reduction means the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (proportional to the spread of points about the line formed in this figure) is roughly a measure of how much \"information\" is discarded in this reduction of dimensionality. Given the number of features, I chose to include all features in developing my models.","cf120afb":"For Model 2, I changed acitivation to \u2018relu\u2019, the rectified linear unit function, returns f(x) = max(0, x). hidden_layer_sizes has been set to (15,)","6502f32a":"#### Bagging Classifier Baseline Model Cross Validation (CV = 10)","b8540e77":"### Support Vector RBF Model 3","3bcb3f55":"#### KNN Model 3 Area Under Curve (AUC)","c1cafba9":"#### Gradient Boost Model 1 Area Under Curve (AUC)","b920ffaa":"### Churn by Contract using [pandas.DataFrame.plot](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.html)","f92027d6":"## K-Nearest Neighbors Model","78028ed5":"From the chart above, customers who use an electronic check as their payment method are more likely to churn that those who do not.","650708b2":"In principal component analysis, this relationship is quantified by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn's PCA estimator, we can compute this.","33028e07":"### Decision Tree Model 2","7766be89":"#### KNN Model 1 Accuracy Scores and Confusion Matrix","5014a811":"When testing my model against the Test Data, I noticed an overall Accuracy Score of 78.2% with a Recall of 0.60 (Churn = Yes). At this point, this model has already exceeded the recall scores of all three KNN models.","92ee3ce8":"### Extra Trees Model Evaluation","a596d4ee":"After reviewing all models, I chose the Decision Tree Model 3 as the best model for this business problem. This model produced the highest recall score compared to all models.","a6e25f7e":"The AUC score of 0.819 is an excellent starting point as this value indicates that this model is fairly good and predicting class separation.","306bbb9a":"### Support Vector RBF Model 2","df8b11db":"From the analysis above, I choose **n_estimators=140**.","11e1e9be":"#### Support Vector RBF Model 3 Cross Validation (CV = 10)","089563e1":"## Decision Tree Visualization","cbec7d71":"#### Stacking Model 4 Accuracy Scores and Confusion Matrix","e74a71c8":"#### AdaBoost Classifier Model 3 Accuracy Scores and Confusion Matrix","dc4a3284":"In this notebook, I performed binary classification on [Telco Customer Churn](https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn#WA_Fn-UseC_-Telco-Customer-Churn.csv) from Kaggle. The data set includes information about:\n- Customers who left within the last month \u2013 the column is called Churn\n- Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n- Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n- Demographic info about customers \u2013 gender, age range, and if they have partners and dependents","c24eadaf":"For the SGD model below, I set **loss='log'**. The \u2018log\u2019 loss gives logistic regression, a probabilistic classifier. ","4ace1183":"# Model Instantiation, Fitting, and Analysis","c8a1f23f":"#### Gradient Boost Model 1 Accuracy Scores and Confusion Matrix","175d6ffd":"## Decision Tree Model","ff14d4bf":"The AUC score of 0.758 is a reduction in overall performance when compared to Model 1.","a6c305df":"The Gradient Boost Model 3 produced the best recall score but only slightly. I chose this to be my best GBC model.","4d100f55":"#### Gradient Boost Model 3 Area Under Curve (AUC)","f429c08d":"From the chart above, I chose **C=3**.","c1a89844":"The mean score and the 95% confidence interval of the score estimate for the Decision Tree Model 1 is 79% with a variance of 5%. The cross-validation method produces a more accurate estimate of out-of-sample accuracy and is a more efficient use of data (every observation is used for both training and testing).","172af2ad":"We see that our model overfits for large depth values. The tree perfectly predicts all of the train data, however, it fails to generalize the findings for new data. Based on these findings, I chose **max_depth=5** to minimize overfitting.","00f798d9":"The AUC score of 0.833 is a respectable starting point for the Random Forest Models and is consistent with DT and KNN.","9c3df7df":"#### Multi-layer Perceptron Classifier Model 1 Cross Validation (CV = 10)","f1af30e0":"Based on the three KNN models, KNN Model 3 with the KD tree algorithm produced the best scores for all classification metrics. All scores for were close but the recall score was substantially higher than the first two KNN models. I would designate KNN Model 3 as the best KNN model from my analysis.","c94b4cbe":"The mean score and the 95% confidence interval of the score estimate for the Extra Trees Model 3 is 80% with a variance of 4%.","f7d565d6":"#### Multi-layer Perceptron Classifier Model 1 Area Under Curve (AUC)","a9ee2d19":"Using the [sklearn.neural_network.MLPClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) library, I created three Multi-layer Perceptron Classification Model. The model below uses **hidden_layer_sizes=(20,8)**. This represents the ith element represents the number of neurons in the ith hidden layer. The activation function for the hidden layer has been set to \u2018tanh\u2019, the hyperbolic tan function, returns f(x) = tanh(x). The default **solver='adam'** is maintained.","0756998d":"For Random Forest Model 2, I retained **max_depth=7** and chose to determine the best **n_estimators** parameter. This number determines the number of trees in the forest (default=10).","58aaab2f":"The mean score and the 95% confidence interval of the score estimate for the Gradient Boost Model 3 is 80% with a variance of 4%.","0a1fec42":"From the analysis above, the Linear Support Vector Classification Model 3 produced the best recall score.","07238486":"The mean score and the 95% confidence interval of the score estimate for the Stacking Model 2 is 80% with a variance of 3%.","c8f99b1e":"#### Random Forest Model 3 Area Under Curve (AUC)","d8a4f908":"#### Extra Trees Model 1 Accuracy Scores and Confusion Matrix"}}