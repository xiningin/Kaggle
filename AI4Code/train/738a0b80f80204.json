{"cell_type":{"6b64f4b9":"code","bc2380e2":"code","44b09d20":"code","09b649ff":"code","d79c2a47":"code","b6d3becd":"code","f815ca79":"code","c33c6224":"code","03b33046":"code","ad1442a4":"code","cffdf6ea":"code","863fe259":"code","aa0feac9":"code","82f2d850":"code","f1909af6":"code","2171c929":"code","c99c2318":"code","1cf78137":"code","4025fd6a":"code","568534f2":"code","c18fde3d":"code","69534151":"code","e9093ade":"code","5890bc72":"code","42d8454d":"code","c4597acf":"code","0e88bb2a":"code","2ba3b24f":"code","1d6d8d2c":"code","365baff9":"code","b4a18359":"code","290fcd9e":"code","e09c56fa":"code","5e75e655":"code","40c3a356":"code","f30b88f1":"code","8400ff01":"code","21e94f8f":"code","c0993b5a":"code","5ea0a508":"code","e54eb577":"code","63539541":"markdown","e9a64ed9":"markdown","6abf99e3":"markdown","3752ffe7":"markdown","f9041ba9":"markdown","0ba8c9a6":"markdown","143e22a5":"markdown","2c3aa47d":"markdown","677f92d9":"markdown","ec9e35f1":"markdown","2562325a":"markdown","0bcf2dbd":"markdown","9c514e03":"markdown","5f195fc3":"markdown","ef8654d3":"markdown","eb410df5":"markdown","ee755c6e":"markdown","bf3acc74":"markdown","158dfad8":"markdown","f51679b9":"markdown","9749f767":"markdown","9ab2b071":"markdown","0268d348":"markdown","8bd18454":"markdown","663a4425":"markdown"},"source":{"6b64f4b9":"pip install mljar-supervised","bc2380e2":"import numpy as np\nimport pandas as pd\nimport missingno as msno\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz","44b09d20":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\nfinal = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\ntrain.head()","09b649ff":"print(train.shape)\nprint(test.shape)","d79c2a47":"train.info()","b6d3becd":"train.describe()","f815ca79":"train.isnull().sum()","c33c6224":"test.isnull().sum()","03b33046":"missing_percentages = (train[train.columns].isnull().sum() \/ train.shape[0]) * 100\nmissing_percentages","ad1442a4":"missing_percentages_test = (test[test.columns].isnull().sum() \/ test.shape[0]) * 100\nmissing_percentages_test","cffdf6ea":"for i in train.columns:\n    print(\"The number of unique values in {} is {}\".format(i, len(train[i].unique())))","863fe259":"categorical_features = [\"Pclass\",\"Sex\",\"Embarked\",\"Parch\",\"SibSp\"]\ncontinuous_features = [\"Age\",\"Fare\"]","aa0feac9":"fig = plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nax = sns.countplot(x=\"Survived\",data=train)\nplt.subplot(1,2,2)\nsns.distplot(train.loc[: ,'Survived'], hist_kws={\"color\":\"r\"}, kde_kws={\"color\":\"b\", \"lw\":2})\nplt.show()","82f2d850":"fig = plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nax = sns.countplot(x=\"Pclass\",data=train)\nplt.subplot(1,2,2)\nsns.distplot(train.loc[: ,\"Pclass\"], hist_kws={\"color\":\"r\"}, kde_kws={\"color\":\"b\", \"lw\":2})\nplt.show()","f1909af6":"fig = plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nax = sns.countplot(x=\"Parch\",data=train)\nplt.subplot(1,2,2)\nsns.distplot(train.loc[: ,\"Parch\"], hist_kws={\"color\":\"r\"}, kde_kws={\"color\":\"b\", \"lw\":2})\nplt.show()","2171c929":"fig = plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nax = sns.countplot(x=\"SibSp\",data=train)\nplt.subplot(1,2,2)\nsns.distplot(train.loc[: ,\"SibSp\"], hist_kws={\"color\":\"r\"}, kde_kws={\"color\":\"b\", \"lw\":2})\nplt.show()","c99c2318":"fig = plt.figure(figsize=(6,6))\nsns.set_palette([\"#8072fa\",\"orange\"])\nax = sns.countplot(x=\"Sex\",data=train)\nplt.show()","1cf78137":"fig = plt.figure(figsize=(6,6))\nsns.set_palette([\"#8072fa\",\"orange\",\"Red\"])\nax = sns.countplot(x=\"Embarked\",data=train)\nplt.show()","4025fd6a":"train['Age'].fillna(train['Age'].mean(),inplace=True)\ntest['Age'].fillna(train['Age'].mean(),inplace=True)\n\ntrain['Fare'].fillna(train['Fare'].mean(),inplace=True)\ntest['Fare'].fillna(train['Fare'].mean(),inplace=True)\n\ntrain['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)\ntest['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)","568534f2":"train.drop(['Name','Ticket','Cabin','PassengerId'], axis=1, inplace=True)\ntest.drop(['Name','Ticket','Cabin','PassengerId'], axis=1, inplace=True)","c18fde3d":"train['Fare'] = train['Fare'].map(lambda i: np.log(i) if i > 0 else 0)\ntest['Fare'] = test['Fare'].map(lambda i: np.log(i) if i > 0 else 0)","69534151":"train[\"relatives\"] = train[\"Parch\"] + train[\"SibSp\"] + 1\ntest[\"relatives\"] = test[\"Parch\"] + test[\"SibSp\"] + 1","e9093ade":"object_cols = ['Sex','Embarked']\nfor col in object_cols:\n    label_encoder = LabelEncoder()\n    label_encoder.fit(train[col])\n    train[col] = label_encoder.transform(train[col])\n    test[col] = label_encoder.transform(test[col])","5890bc72":"features = ['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','relatives']\ntarget = train['Survived'].values","42d8454d":"lr = LogisticRegression()\nlr.fit(train[features], target)\nprint(\"Logistic Regression ROC AUC score:\", roc_auc_score(target, lr.predict_proba(train[features])[:,1]))\nprint('Logistic Regression Accuracy score:', accuracy_score(target, lr.predict(train[features])))","c4597acf":"final['Survived'] = lr.predict(test[features])\nfinal.to_csv('LR.csv',index=False)","0e88bb2a":"xgb = XGBClassifier()\nxgb.fit(train[features], target)\nprint(\"XGB ROC AUC score:\", roc_auc_score(target, xgb.predict_proba(train[features])[:,1]))\nprint('XGB Accuracy score:', accuracy_score(target, xgb.predict(train[features])))","2ba3b24f":"final['Survived'] = xgb.predict(test[features])\nfinal.to_csv('XGB.csv',index=False)","1d6d8d2c":"from supervised.automl import AutoML\nautoml = AutoML(eval_metric=\"accuracy\")\nautoml.fit(train[features], target)\nautoml.report()","365baff9":"final['Survived'] = automl.predict(test[features])\nfinal.to_csv('AutoML.csv',index=False)","b4a18359":"dt = DecisionTreeClassifier(\n    max_depth=4,\n    min_samples_leaf=2)\ndt.fit(train[features], target)","290fcd9e":"dot_data = export_graphviz(\n    dt,\n    out_file=None,\n    feature_names=train[features].columns,\n    class_names=['0', '1'],\n    filled=True,\n    rounded=False,\n    special_characters=True,\n    precision=3\n)\ngraph = graphviz.Source(dot_data)\ngraph ","e09c56fa":"y_pred = dt.predict(test[features]).astype(int)\nfinal['Survived'] = y_pred\nfinal.to_csv(\"DT.csv\", index=False)","5e75e655":"lgbm = LGBMClassifier(boosting_type = 'dart',num_leaves = 32,max_depth = 10,colsample_bytree = 0.8,extra_trees = True,n_jobs = -1,random_state = 42)\nlgbm.fit(train[features], target)","40c3a356":"y_pred = lgbm.predict(test[features]).astype(int)\nfinal['Survived'] = y_pred\nfinal.to_csv(\"LGBM.csv\", index=False)","f30b88f1":"ext = ExtraTreesClassifier(n_estimators = 1000,max_depth = 17,min_samples_split = 25,min_samples_leaf = 18,n_jobs = -1,random_state = 42)\next.fit(train[features], target)","8400ff01":"y_pred = ext.predict(test[features]).astype(int)\nfinal['Survived'] = y_pred\nfinal.to_csv(\"EXT.csv\", index=False)","21e94f8f":"gb=GradientBoostingClassifier(max_depth= 2, n_estimators = 400)\ngb.fit(train[features], target)","c0993b5a":"y_pred = gb.predict(test[features]).astype(int)\nfinal['Survived'] = y_pred\nfinal.to_csv(\"GB.csv\", index=False)","5ea0a508":"clf = VotingClassifier(estimators=[('DT',dt),('EXT',ext),('LGBM' , lgbm) , ('GB',gb)], voting='soft')\nclf.fit(train[features], target)","e54eb577":"y_pred = clf.predict(test[features]).astype(int)\nfinal['Survived'] = y_pred\nfinal.to_csv(\"VC.csv\", index=False)","63539541":"# LGBM","e9a64ed9":"# Modelling","6abf99e3":"# Voting CLassifier","3752ffe7":"Label Encoding the categorical features","f9041ba9":"# Understanding Data","0ba8c9a6":"### The categorical features are\n* Pclass\n* Sex\n* Embarked\n* Parch\n* SibSp\n\n### The continuous features are\n* Age\n* Fare\n\n### The ones which will be dealt manually are\n* Ticket\n* Cabin\n* Name","143e22a5":"## Logistic Regression","2c3aa47d":"# Data Preprocessing","677f92d9":"# AutoML","ec9e35f1":"## Sex","2562325a":"Handling missing data","0bcf2dbd":"## Pclass","9c514e03":"# Extra Classifier","5f195fc3":"## Embarked","ef8654d3":"# EDA","eb410df5":"## Parch","ee755c6e":"## SibSp","bf3acc74":"Applying a log function to reduce the influence of outliers in Fare column. Since there are a large number of outliers, removing them will lead to loss of a large numberof points.","158dfad8":"# Decision Tree","f51679b9":"# Gradient Boosting","9749f767":"Adding Parch and SibSp into one feature","9ab2b071":"## Checking the count & distribution of Survived\n","0268d348":"Ticket number,Cabin & Name number doesn't seem to influence Survival.","8bd18454":"### The columns with missing values are-\n\n* Age\n* Ticket\n* Fare\n* Cabin\n* Embarked","663a4425":"## XGBoost"}}