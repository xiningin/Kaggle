{"cell_type":{"40c262a0":"code","fecfb46a":"code","012e0f24":"code","f6b6b306":"code","ee6dbd41":"code","33389e8e":"code","c0ff20d7":"code","a5fa1ef5":"code","8a0ea79e":"code","3cddc736":"code","4ce4a68b":"code","08328ad4":"code","5f2ed207":"code","30cebdfa":"code","bb243049":"code","b321aae5":"code","ac780af4":"code","74f6bc74":"code","0f0fcdc0":"code","376f6a0d":"markdown","13a1746b":"markdown","cdd418da":"markdown","c347f304":"markdown","fb517d6f":"markdown","7c94f12f":"markdown","b0a7e231":"markdown","de4e0d40":"markdown","bb194791":"markdown","8dbb8077":"markdown","d99acc5b":"markdown","c5ca31a2":"markdown","fde359a6":"markdown","0a08bfe6":"markdown","020f6619":"markdown","d7fa75ea":"markdown","406515ea":"markdown","9ef8cd43":"markdown","727f0a6d":"markdown","67bdf0b4":"markdown"},"source":{"40c262a0":"!pip install pyDOE","fecfb46a":"import os\nimport random\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nimport tensorflow_hub as hub\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nfrom tabulate import tabulate\nfrom pyDOE import lhs\nimport matplotlib.pyplot as plt","012e0f24":"class Dataset:\n    def __init__(self, data_root: str, *, test_size: float, img_size: int, seed: int = 0) -> None:\n        self.label2index = {}\n        self.index2label = {}\n        \n        # Discover the class label names.\n        class_labels = os.listdir(data_root)\n        self.nclasses = len(class_labels)\n        X, y = [], []\n        \n        for label_index, label in enumerate(class_labels):\n            # Load the images for this class label.\n            self.label2index[label_index] = label\n            self.index2label[label] = label_index\n            \n            img_names = os.listdir(os.path.join(data_root, label))\n            for img_name in img_names:\n                img_path = os.path.join(data_root, label, img_name)\n                img = load_img(img_path, target_size=(img_size, img_size, 3))\n                X.append(img_to_array(img))\n                y.append(label_index)\n        \n        X = np.array(X)\n        y = np.array(y)\n        one_hot_y = to_categorical(y, num_classes=self.nclasses)\n        \n        # Make a stratified split.\n        self.X, self.X_test, self.labels, self.labels_test, self.y, self.y_test = train_test_split(\n            X, y, one_hot_y, test_size=test_size, random_state=seed, stratify=y\n        )\n\n        \ndata = Dataset(\"\/kaggle\/input\/rice-leaf-diseases\/rice_leaf_diseases\", test_size=0.3, img_size=256)\nprint(data.X.shape, data.y.shape)","f6b6b306":"embed = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/bit\/m-r101x1\/1\", trainable=False)\nX_embedding = embed(data.X)\nX_test_embedding = embed(data.X_test)\nprint(X_embedding.shape, X_test_embedding.shape)","ee6dbd41":"def make_model(\n    nclasses: int, *, dropout_rate: float, nhiddenunits: int, l2_regularization: float\n) -> tf.keras.Model:\n    model = tf.keras.Sequential()\n    # One fully connected hidden layer\n    model.add(L.Dense(nhiddenunits, activation=\"relu\", kernel_regularizer=l2(l2_regularization)))\n    model.add(L.Dropout(dropout_rate))\n    # Output layer\n    model.add(L.Dense(nclasses, activation=\"softmax\", kernel_regularizer=l2(l2_regularization)))\n    return model","33389e8e":"def evaluate_model(\n    nclasses, X, y, X_dev, y_dev, *,\n    epochs: int, batch_size: int, learning_rate: float,\n    model_maker = make_model, **model_params\n) -> tuple:\n    \n    # Math to compute the learning rate schedule. We will divide our\n    # learning rate by a factor of 10 every 30% of the optimizer's\n    # total steps.\n    steps_per_epoch = math.ceil(len(X) \/ batch_size)\n    third_of_total_steps = math.floor(epochs * steps_per_epoch \/ 3)\n    \n    # Make and compile the model.\n    model = model_maker(nclasses, **model_params)\n    model.compile(\n        optimizer=Adam(\n            learning_rate=ExponentialDecay(\n                learning_rate,\n                decay_steps=third_of_total_steps,\n                decay_rate=0.1,\n                staircase=True\n            )\n        ),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    \n    # Train the model on the training set and evaluate it on the dev set.\n    model.fit(X, y, batch_size=batch_size, epochs=epochs, verbose=0)\n    _, train_acc = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n    _, dev_acc = model.evaluate(X_dev, y_dev, batch_size=batch_size, verbose=0)\n    return model, train_acc, dev_acc","c0ff20d7":"def cv_evaluate_model(\n    X, y, labels, *, nfolds: int, nrepeats: int, epochs: int, batch_size: int,\n    learning_rate: float, model_maker, verbose: bool = True, seed: int = 0,\n    **model_params\n) -> dict:\n    \"\"\"\n    Performs `nfolds` cross-validated training and evaluation of a\n    model hyperparameter configuration. Returns a dictionary of\n    statistics about the outcome of the cross-validated experiment.\n    \"\"\"\n    _, nclasses = y.shape\n    train_accs, dev_accs = [], []\n    \n    # Train and evaluate the model for each fold.\n    for train_index, dev_index in tqdm(\n        RepeatedStratifiedKFold(\n            n_splits=nfolds, n_repeats=nrepeats, random_state=seed\n        ).split(X, labels),\n        total=nfolds*nrepeats, disable=not verbose\n    ):\n        \n        # Select the data for this fold.\n        X_train_fold = tf.gather(X, train_index) \n        y_train_fold = tf.gather(y, train_index)\n        X_dev_fold = tf.gather(X, dev_index)\n        y_dev_fold = tf.gather(y, dev_index)\n        \n        # Train and evaluate the model.\n        _, train_acc, dev_acc = evaluate_model(\n            nclasses,\n            X_train_fold,\n            y_train_fold,\n            X_dev_fold,\n            y_dev_fold,\n            epochs=epochs,\n            batch_size=batch_size,\n            learning_rate=learning_rate,\n            model_maker=model_maker,\n            **model_params\n        )\n        train_accs.append(train_acc)\n        dev_accs.append(dev_acc)\n    \n    # Aggregate.\n    results = {\n        \"train_mean\": np.mean(train_accs),\n        \"train_std\": np.std(train_accs),\n        \"dev_mean\": np.mean(dev_accs),\n        \"dev_std\": np.std(dev_accs)\n    }\n    \n    # Report.\n    if verbose:\n        print(\n            tabulate(\n                [\n                    [\"Train\", results[\"train_mean\"], results[\"train_std\"]],\n                    [\"Dev\", results[\"dev_mean\"], results[\"dev_std\"]]\n                ],\n                headers=[\"Set\", \"Mean\", \"Std. Dev.\"]\n            )\n        )\n    \n    return results","a5fa1ef5":"# We'll refer to these values throughout the notebook.\ndefault_cv_evaluate_params = {\n    \"X\": X_embedding,\n    \"y\": data.y,\n    \"labels\": data.labels,\n    \"nfolds\": 10,\n    \"nrepeats\": 3,\n    \"model_maker\": make_model,\n    \"epochs\": 200,\n    \"batch_size\": 32,\n    \"verbose\": False,\n    \"learning_rate\": 3e-3,\n    \"dropout_rate\": 0.3,\n    \"nhiddenunits\": 64,\n    \"l2_regularization\": 1e-6\n}\n\n_ = cv_evaluate_model(\n    **{\n        **default_cv_evaluate_params,\n        \"verbose\": True\n    }\n)","8a0ea79e":"nsamples = 20\n# `lhs` will yield a `nsamples`x2 matrix, with all values following\n# the uniform distribution in [0,1]\nnp.random.seed()\nx = lhs(n=2, samples=nsamples)\n# Scale dropout samples to be in [0,0.6]\ndropout_rates = (x[:, 0]*.6)\n# Scale l2 samples to be in [-8, -1]\nl2_rates = (x[:, 1]*-7 - 1)\n# Now scale l2 samples to follow the\n# log scale in the range [10**-8,10**-1]\nl2_rates = 10**l2_rates","3cddc736":"dev_means = np.zeros((nsamples, 1))\ndev_stds = np.zeros((nsamples, 1))\nfor i in tqdm(range(nsamples)):\n    results = cv_evaluate_model(\n        **{\n            **default_cv_evaluate_params,\n            \"dropout_rate\": dropout_rates[i],\n            \"l2_regularization\": l2_rates[i]\n        }\n    )\n    dev_means[i] = results[\"dev_mean\"]\n    dev_stds[i] = results[\"dev_std\"]","4ce4a68b":"best_i = np.argmax(dev_means)\nbest_l2 = l2_rates[best_i]\nprint(f\"Best l2_rate:\\t{best_l2}\")\nbest_dropout = dropout_rates[best_i]\nprint(f\"Best dropout:\\t{best_dropout}\")\n\nplt.scatter(dropout_rates, l2_rates, c=dev_means, cmap=plt.cm.coolwarm)\nplt.xlabel(\"Dropout Rate\")\nplt.ylabel(\"L2 Rate\")\nplt.yscale(\"log\")\nplt.colorbar()\nplt.show()","08328ad4":"default_cv_evaluate_params[\"dropout_rate\"] = best_dropout\ndefault_cv_evaluate_params[\"l2_regularization\"] = best_l2","5f2ed207":"def search_hyperparam(\n    param_name: str, lower: int, upper: int, nsamples: int,\n    scale: str = \"float\", **cv_evaluate_params\n) -> float:\n    \"\"\"\n    Cross validates a model along an evenly spaced range of `nsamples`\n    values for a single hyperparameter (identified by `param_name`. The\n    values are evenly spaced in the range `[lower,upper]`, and scaled\n    according to `scale`.\n    \"\"\"\n    # Sample the hyperparameter values to try.\n    param_values = np.linspace(lower, upper, nsamples)\n    if scale == \"log\":\n        param_values = 10**param_values\n    elif scale == \"int\":\n        param_values = param_values.astype(int)\n    \n    # Evaluate the model at each of the values.\n    dev_means = np.zeros((nsamples, 1))\n    dev_stds = np.zeros((nsamples, 1))\n    for i in tqdm(range(nsamples)):\n        results = cv_evaluate_model(\n            **{\n                **cv_evaluate_params,\n                param_name: param_values[i]\n            }\n        )\n        dev_means[i] = results[\"dev_mean\"]\n        dev_stds[i] = results[\"dev_std\"]\n    \n    # Find the best value.\n    best_param_value = param_values[np.argmax(dev_means)]\n    print(f\"Best {param_name} value: {best_param_value}\")\n    \n    # Plot scores achieved for all values.\n    plt.plot(param_values, dev_means)\n    plt.xlabel(param_name)\n    plt.ylabel(\"Mean Dev Set Accuracy\")\n    if scale == \"log\":\n        plt.xscale(\"log\")\n    plt.show()\n    \n    return best_param_value","30cebdfa":"best_epochs = search_hyperparam(\"epochs\", 50, 500, 10, scale=\"int\", **default_cv_evaluate_params)","bb243049":"best_epochs2 = search_hyperparam(\"epochs\", 400, 800, 9, scale=\"int\", **default_cv_evaluate_params)","b321aae5":"default_cv_evaluate_params[\"epochs\"] = best_epochs2","ac780af4":"def evaluate_final(params: dict) -> float:\n    \n    steps_per_epoch = math.ceil(len(X_embedding) \/ params[\"batch_size\"])\n    third_of_total_steps = math.floor(params[\"epochs\"] * steps_per_epoch \/ 3)\n\n    # Make and compile the model.\n    model = make_model(\n        data.nclasses,\n        dropout_rate=params[\"dropout_rate\"],\n        nhiddenunits=params[\"nhiddenunits\"],\n        l2_regularization=params[\"l2_regularization\"]\n    )\n    model.compile(\n        optimizer=Adam(\n            learning_rate=ExponentialDecay(\n                params[\"learning_rate\"],\n                decay_steps=third_of_total_steps,\n                decay_rate=0.1,\n                staircase=True\n            )\n        ),\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    # Train the model on the full training set (no development set)\n    model.fit(\n        X_embedding,\n        tf.convert_to_tensor(data.y),\n        batch_size=params[\"batch_size\"],\n        epochs=params[\"epochs\"],\n        verbose=0\n    )\n\n    # Evaluate the fitted model on the test set.\n    _, test_acc = model.evaluate(\n        X_test_embedding,\n        data.y_test,\n        verbose=0\n    )\n    return test_acc\n\ntest_acc = evaluate_final(default_cv_evaluate_params)\nprint(f\"Final test set accuracy:\\t{test_acc:.6f}\")","74f6bc74":"test_accs = []\nfor _ in tqdm(range(30)):\n    test_accs.append(evaluate_final(default_cv_evaluate_params))","0f0fcdc0":"plt.hist(test_accs)\nplt.xlabel(\"Test Set Accuracy\")\nplt.ylabel(\"Model Count\")\nplt.show()","376f6a0d":"# References\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n\nKolesnikov, Alexander, et al. \u201cBig Transfer (BiT): General Visual Representation Learning.\u201d ArXiv:1912.11370 [Cs], May 2020. arXiv.org, http:\/\/arxiv.org\/abs\/1912.11370\n","13a1746b":"## Overview\n\nThe goal of this notebook is to learn to classify rice leaf diseases, given a small dataset of labeled images of rice leaf diseases taken in a laboratory setting. A few techniques this notebook explores:\n\n- Few shot learning\n- Using pre-computed embeddings for rapid model training\n- Latin Hypercube Sampling (LHS)\n- Hyperparameter search\n\nFirst, let's set up our environment.","cdd418da":"In the above plot, dev set accuracy is represented by the color of the points.\n\nWe can see a rough picture painted here. The top right area of the search square seems to give the best performance for this dataset. It seems that both individually and together, a low L2 regularization coefficient $\\lambda$ and a low dropout rate negatively affect generalization performance. Ideally we would zoom in on that well performing search square and search again to see if we can find even better values for our parameters, but for the sake of brevity in this notebook, we'll take the best found values for both $\\lambda$ and the dropout rate.\n\n","c347f304":"\n## Final Test Set Performance and A Variance Problem\nNow that we've tuned the regularization parameters of our model to improve its generalizability performance, let's train a final model using our best found configuration and evaluate it on our independent test sample. To average out the model's variance.","fb517d6f":"With a dataset this small, we'll need to conduct repeated k-fold cross validation in order to get a real sense of the generalization performance of any given model, and to compare it effectively against models trained under other hyperparameter configurations. The reason for this is that because of the small size of the dataset, the performance of any given model will vary depending on the random seed it was trained under.\n\nLet's define a method for evaluating a model configuration using repeated k-fold cross validation.","7c94f12f":"## Using Pretrained Image Embeddings\n\nFor a datset this small, it is not necessary to fine tune a pretrained model. It is better to freeze the weights of the pretrained model and just train a small model on top of it. We'll use a pretrained BiT model by Kolesnikov et al. (2020) to first create embeddings for each image, then train a very small model to learn to map those embeddings to the correct labels. This accomplishes the same thing as training a new head on top of a pretrained model, but without the need to execute forward propagation through the potentially large pretrained model on each training step. By computing the embeddings first, we only need to conduct forward propagation through the pretrained model once.\n\nWe will use Kolesnikov et al's `BiT-M R101x1` model to compute our embeddings. It is a 101 layer ResNet (He et al. 2016) pretrained on the full ImageNet dataset.","b0a7e231":"## Conclusion\n\nIn this notebook we have explored few shot learning strategies, using pre-computed embeddings for rapid model training, Latin Hypercube Sampling, and basic hyperparameter search, including joint and isolated hyperparameter search strategies.\n\nThere is value in using pre-computed embeddings during training, so long as there is no intention to fine-tune the weights of the pretrained model itself. By using pre-trained embeddings, model training becomes much more rapid. Indeed, when we searched jointly over the dropout rate and L2 regularization coefficient, in 28 minutes we were able to complete 20 different executions of 10-fold cross validation, each repeated 3 times, which comes out to $20 \\times 10 \\times 3 = 600$ individual models.","de4e0d40":"As you can see, this model is not as robust as we'd like it to be. Depending on the model's random weight initialization, the sequence of node dropouts that occur during training, and any other stochasticity that Keras uses, sometimes our final trained model does very well on the test set, and sometimes it does very poorly (although much less frequently). This model variance is undesirable. Addressing it is outside the scope of this notebook but there are a few options (one resource that describes them is [this excellent post](https:\/\/machinelearningmastery.com\/how-to-reduce-model-variance\/) by Jason Brownlee). The best option for this few-shot learning problem would probably be to ensemble multiple final models.","bb194791":"There is again noise in these results, but a very rough trend exists going upward, peaking at 650, then going downward. We will use our best found value among both searches of 650 epochs. ","8dbb8077":"This is a good test set accuracy. However, look at what happens when we train this model multiple times (always on the same training data), evaluating it on the test set each time.","d99acc5b":"## Evaluating a Model's Performance\n\nWe have our embeddings. Now let's define a way to make a model which can learn the mappings from the image embedding space to the class labels.","c5ca31a2":"This is where we perform the actual cross validation of the model. We'll use 10 folds, repeated 3 times. Because the model is small, it will execute relatively quickly.","fde359a6":"Now we can visualize the results over the hyperparameter space, to hopefully learn about the interplay between l2 regularization and the dropout rate for this model.","0a08bfe6":"As an aside, note that training for a smaller number of epochs has the effect of regularizing a network because the weights do not have as much time to become \"overly-opinionated\" during training. Another way to view it is that without other forms of regularization, the decision boundary the network learns becomes less and less smooth as the network trains longer and longer, and therefore less and less able to generalize to an independent test sample. L2 regularization (a.k.a. weight decay) encourages a smooth decision boundary, and dropout is functionally an adaptive form of L2 regularization.\n\nFor all experiments we use the learning rate Kolesnikov et al. used of `3e-3`, and the learning rate reduction schedule of reducing the learning rate by a factor of 10 at each of 30%, 60%, and 90% of the total training steps.\n\nLet's now search over the number of epochs. We will search in steps of 50 in the range $[50, 500]$.","020f6619":"Next we cross validate a model on each hyperparameter sample, storing the dev set performance statistics for each run.","d7fa75ea":"Now, let's define a way to make, compile, train, and evaluate the generalization performance of a model.","406515ea":"## Latin Hypercube Sampling and Tuning the Regularization\n\nWith an average training set accuracy of 100% or near 100%, we see this model has no bias. To reduce the variance, we could obtain more training data via data collection or data augmentation, try a different network architecture, or fine tune our regularization process. In this notebook, will start with the low hanging fruit of trying to fine tune our regularization process.\n\nSince the model evaluates fairly quickly, let's search over a 2D sampling of L2 regularization and dropout rate hyperparameter values to find a good regularization scheme for this architecture and dataset. We'll do a random search over a sensible square of the 2D hyperparameter space, with the samples being produced by latin hypercube sampling (LHS). LHS improves the quality of the coverage over the sample space by optimizing the spread of the samples, with the constraint that they still follow the sampling distribution.\n\nWe sample the L2 regularization values from a log scale ([$10^{-8},10^{-1}]$), and the dropout rate values from a uniform scale ($[0.0,0.6]$). We'll keep the other hyperparameter values the same as before.","9ef8cd43":"There is a little noise, but we can see a trend upward. The trend has not flattened out by 500 epochs so let's try another search over a second portion of the space, to see when that trend plateaus. We'lll search again in steps of 50, but this time in the range $[400,800]$.","727f0a6d":"Let's move on with tuning the number of epochs we train for, which Kolesnikov et al. found to be an important hyperparameter to tune when training their pretrained BiT models on transfer tasks. First, let's define a method we can use to search across an individual hyperparameter.","67bdf0b4":"## The Dataset\n\nSince the rice leaf diseases dataset is very small (this is essentially a few-shot learning problem), let's use a pretrained image classification model. First, we'll load the images and their labels into two stratified train and test set numpy arrays. In order to accurately estimate the final generalizability of our model, we need to set aside the test set, and not look at it for the entire model analysis. That allows it to remain a truly independent sample. "}}