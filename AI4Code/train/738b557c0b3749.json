{"cell_type":{"4dda8fc9":"code","f1eac337":"code","ba8d7877":"code","f02dc122":"code","7062d607":"code","64c7208a":"code","6056d83c":"code","71e138b8":"code","62888b33":"code","16cde901":"code","38029977":"code","71de5c4d":"code","6aa40074":"code","3d0463ff":"code","71fd817e":"code","4ec63867":"markdown","7eb1be37":"markdown","6d1a420b":"markdown","a8f87886":"markdown","970baca0":"markdown","cf9d5c5b":"markdown","1a56b9d3":"markdown","784d1093":"markdown","2e7ff615":"markdown","619a3839":"markdown","d31be98e":"markdown","ed882090":"markdown","19936cfe":"markdown","b77aa596":"markdown"},"source":{"4dda8fc9":"import numpy as np                # linear algebra\nimport pandas as pd               # data frames\nimport seaborn as sns             # visualizations\nimport matplotlib.pyplot as plt   # visualizations\nimport scipy.stats                # statistics\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nimport os\nprint(os.listdir(\"..\/input\"))","f1eac337":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\n\n# Print the head of df\nprint(df.head())\n\n# Print the info of df\nprint(df.info())\n\n# Print the shape of df\nprint(df.shape)","ba8d7877":"df.describe()","f02dc122":"# Compute the correlation matrix\ncorr=df.iloc[:,1:9].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","7062d607":"sns.jointplot(x=\"GRE Score\", y=\"CGPA\", data=df)","64c7208a":"#Correlation for different deciles of the most important variable to be admitted\ndef corr_parts(data,x,y,z,z_cutoff):\n    df_temp = data.loc[data[z] > z_cutoff]\n    return df_temp[x].corr(df_temp[y])\n\ndl_contrast = np.around(np.percentile(df['CGPA'], np.arange(0, 100, 10)),1)\n\ncorr_sop = []\nfor x in dl_contrast:\n    corr_sop.append(corr_parts(df,'SOP','Chance of Admit ','CGPA', x ))\ncorr_lor = []\nfor x in dl_contrast:\n    corr_lor.append(corr_parts(df,'LOR ','Chance of Admit ','CGPA', x ))\n    \nresult = pd.DataFrame ({'decile': dl_contrast, 'sop': corr_sop, 'lor': corr_lor  })\nresult = result.melt('decile', var_name='vars',  value_name='corr')\n\n# Set up the seaborn figure\nsns.factorplot(x=\"decile\", y=\"corr\", hue='vars', data=result)","6056d83c":"#Scaling the continuos variables\ndf_scale = df.copy()\nscaler = preprocessing.StandardScaler()\ncolumns =df.columns[1:7]\ndf_scale[columns] = scaler.fit_transform(df_scale[columns])\ndf_scale.head()","71e138b8":"#Elbow graph\nks = range(1, 6)\ninertias = []\n\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n    \n    # Fit model to samples\n    model.fit(df_scale.iloc[:,1:])\n    \n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","62888b33":"# Create a KMeans instance with 3 clusters: model\nmodel = KMeans(n_clusters=3)\n\n# Fit model to points\nmodel.fit(df_scale.iloc[:,2:9])\n\n# Determine the cluster labels of new_points: labels\ndf_scale['cluster'] = model.predict(df_scale.iloc[:,2:9])\n\ndf_scale.head()","16cde901":"# Create PCA instance: model\nmodel_pca = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model_pca.fit_transform(df_scale.iloc[:,2:9])\n\n# Assign 0th column of pca_features: xs\nxs = pca_features[:,0]\n\n# Assign 1st column of pca_features: ys\nys = pca_features[:,1]\n\n# Scatter plot xs vs ys\nsns.scatterplot(x=xs, y=ys, hue=\"cluster\", data=df_scale)","38029977":"sns.boxplot(x=\"cluster\", y=\"Chance of Admit \", data=df_scale, palette=\"Set2\" )","71de5c4d":"centroids = model.cluster_centers_\ndf_scale.iloc[:,1:10].groupby(['cluster']).mean()","6aa40074":"sns.heatmap(df_scale.iloc[:,1:10].groupby(['cluster']).mean(), cmap=\"YlGnBu\")","3d0463ff":"pd.DataFrame(df_scale['cluster'].value_counts(dropna=False))","71fd817e":"g = sns.PairGrid(df_scale.iloc[:,1:10], hue=\"cluster\", palette=\"Set2\")\ng.map(plt.scatter);","4ec63867":"## Data Transformations","7eb1be37":"The cluster works nice! The tree of them represent a significative population and different to the other populations.","6d1a420b":"Each register has a cluster now, lets visualized using [PCA](https:\/\/www.kaggle.com\/camiloemartinez\/is-the-human-freedom-index-a-good-index). \n","a8f87886":"In order to understand, explain and give a meaningful name we explore the centroids of each cluster.","970baca0":"The scatter plots show the segmentation separate very well the population across all the variables. Which is the desire outcome of the exercise.","cf9d5c5b":"## Clustering","1a56b9d3":"The correlation of the statement of purpose and recommendation letters decay in the upper GPA deciles. It means that GPA is so strong that if you have a good one the rest of the variables do not matter that much, on the opposite if your GPA is not the best the these variables really influence the result of the admission.","784d1093":"## Basic Exploratory Data Analysis\n\nMore about [preparation and exploratory analysis](https:\/\/www.kaggle.com\/camiloemartinez\/lucky-charms-lovers).","2e7ff615":"Does all the features are important given different GPAs?","619a3839":"It look like GRE and GPA are the most significant variables to be admitted into a graduate program, and maybe not coincidentally GRE and GPA area heavily correlated.","d31be98e":"- Cluster 0: **Top** *students, higher score in all the variables than rest of the population.*\n- Cluster 1: **Average** *students, almost average in each variable but some of them have a good score in one variable in particular that make them more eligible in the admission.*\n- Cluster 2: **Aspirational** *student, below the average of the population. In limited cases eligible given extraordinary score in a particular variable.*\n","ed882090":"## Objective\nUsing unsupervised machine learning techniques the idea is to identify different profiles into the people who applies to the graduate program.","19936cfe":"The first step is to find the number of clusters that minimize the variance but still are a practical number to analyze.","b77aa596":"The dataset has 400 aspirants with 9 variables consider for its admission."}}