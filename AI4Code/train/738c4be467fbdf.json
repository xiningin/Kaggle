{"cell_type":{"2c100560":"code","3610bb4d":"code","e8d687d3":"code","7a06fd0d":"code","7b05e5d7":"code","44447db3":"code","df13d879":"code","6bdad651":"code","d5d410db":"code","199c2466":"code","5d8d0050":"code","cc28ffc7":"code","e80a8757":"code","db621e81":"code","d6ebab23":"code","d1fec50a":"code","e3e63ff5":"code","62e4d7d8":"code","ee19e384":"code","3b58376c":"code","f7f14e96":"code","f9796ac4":"code","16e580f8":"code","77e38caa":"code","cd1cb9d5":"code","112edfac":"code","abe8f148":"code","b04e08a4":"code","5ae3371a":"code","b0c6c211":"code","30486f1e":"code","0c09f53d":"code","53084b9e":"code","602cb123":"code","31ae5ccb":"code","6d617efb":"code","05bcf221":"code","586e244c":"code","42d17c15":"code","ce8a7cce":"code","f3d45a72":"code","996b8c30":"code","4474f120":"code","e68d46ed":"code","059025f9":"code","0824e4b8":"code","0543011d":"code","3e4c5ead":"markdown","0736bf16":"markdown","c4444737":"markdown","a8453c21":"markdown","b4226b53":"markdown","5ac1802e":"markdown","00ed71c2":"markdown","29d6a9ca":"markdown","bbcfdc63":"markdown","c7361eb4":"markdown","7ca1d2d7":"markdown","7ccd3e3b":"markdown","6ffb0118":"markdown","355ac009":"markdown","c8e0d1b8":"markdown","0620a782":"markdown","9b7fb1da":"markdown","74893738":"markdown","9e952a3e":"markdown","9ee629c4":"markdown","14eec4fa":"markdown","27f1123a":"markdown","88dbc174":"markdown","24f87ed9":"markdown","7981285c":"markdown","d329db50":"markdown","b65bf773":"markdown","d1b5a286":"markdown","49bba847":"markdown","716b1881":"markdown","4dcf51d5":"markdown","a8778d1d":"markdown","5f9d4fc8":"markdown","34fe24f5":"markdown","002042c7":"markdown","88594cf3":"markdown","c6d97e65":"markdown","42258e07":"markdown","dfeef03f":"markdown","d2993dc3":"markdown","39a4a00a":"markdown","d399d032":"markdown","3bce3305":"markdown","01c0aaa6":"markdown","4fd439a6":"markdown","d6b58e01":"markdown","13a4b8c8":"markdown","18ff5bd8":"markdown","ae45233c":"markdown","b5edc4b6":"markdown"},"source":{"2c100560":"import pandas as pd\nimport numpy as np\n\n# read csv into DataFrame\ndf = pd.read_csv('..\/input\/train.csv')\n\n# preview the first 5 entries\ndf.head()","3610bb4d":"# summary of columns\ndf.info()","e8d687d3":"# statistical summary\ndf.describe()","7a06fd0d":"import matplotlib.pyplot as plt\n\n# generate a historgram for all numerical features\ndf.hist(bins = 50, figsize = [30, 20])\nplt.show()","7b05e5d7":"titanic = df.copy()","44447db3":"titanic = df.drop(['Survived'], axis = 1)\ntitanic_labels = df['Survived'].copy()","df13d879":"# list of columns to drop\ndrop_columns = ['PassengerId', 'Ticket', 'Name', 'Cabin']\n\ntitanic.drop(drop_columns, axis = 1, inplace = True)","6bdad651":"titanic.info()","d5d410db":"# split both categorical and numerical columns\ntitanic_num = titanic.drop(['Sex', 'Embarked'], axis = 1)\ntitanic_cat = titanic[['Sex', 'Embarked']]","199c2466":"titanic_cat.Sex.unique()","5d8d0050":"titanic_cat.Embarked.unique()","cc28ffc7":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n# fill in the missing values, then scale the numerical columns\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy = 'median')),\n        ('std_scaler', StandardScaler()),\n])\n\n# fill in the missing values, then encode the categorical columns\ncat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy = 'most_frequent')),\n        ('one_hot', OneHotEncoder(categories = [['female', 'male'], ['S', 'C', 'Q']]))\n])","e80a8757":"from sklearn.compose import ColumnTransformer\n\n# list of both numerical and categorical features\nnum_attribs = list(titanic_num)\ncat_attribs = list(titanic_cat)\n\nfull_pipeline = ColumnTransformer([\n        # apply numerical pipeline transformation to our numerical features\n        ('num', num_pipeline, num_attribs),\n        # apply categorical pipeline transformation to our categorical features\n        ('cat', cat_pipeline, cat_attribs)\n])\n\n# apply transformations to our training features\ntitanic_prepared = full_pipeline.fit_transform(titanic)","db621e81":"from sklearn.linear_model import SGDClassifier\n\n# model constructor\nsgd_clf = SGDClassifier()\n\n# train model given passenger details and labels\nsgd_clf.fit(titanic_prepared, titanic_labels)","d6ebab23":"# preview features\nsome_passenger = titanic_prepared[1]\nsome_passenger","d1fec50a":"# preview label\ntitanic_labels[1]","e3e63ff5":"# use model to predict based on features\nsgd_clf.predict([some_passenger])","62e4d7d8":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(sgd_clf, titanic_prepared, titanic_labels, cv = 3, scoring = 'accuracy')","ee19e384":"from sklearn.model_selection import cross_val_predict\n\ntitanic_labels_pred = cross_val_predict(sgd_clf, titanic_prepared, titanic_labels, cv = 3)","3b58376c":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(titanic_labels, titanic_labels_pred)","f7f14e96":"# pretend we reached perfection\ntitanic_labels_perfect_pred = titanic_labels\nconfusion_matrix(titanic_labels, titanic_labels_perfect_pred)","f9796ac4":"from sklearn.metrics import precision_score, recall_score\n\nprecision_score(titanic_labels, titanic_labels_pred)","16e580f8":"recall_score(titanic_labels, titanic_labels_pred)","77e38caa":"from sklearn.metrics import f1_score\nf1_score(titanic_labels, titanic_labels_pred)","cd1cb9d5":"titanic_labels_scores = sgd_clf.decision_function([some_passenger])\ntitanic_labels_scores","112edfac":"titanic_labels_scores = cross_val_predict(sgd_clf, titanic_prepared, titanic_labels, cv = 3, method = 'decision_function')","abe8f148":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(titanic_labels, titanic_labels_scores)","b04e08a4":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label = 'Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label = 'Recall')\n    \n    plt.legend()\n    plt.xlabel('Threshold')\n    plt.grid(True)\n\nplt.figure(figsize = [10,5])\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","5ae3371a":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n    plt.xlabel(\"Recall\", fontsize=16)\n    plt.ylabel(\"Precision\", fontsize=16)\n    plt.axis([0, 1, 0, 1])\n    plt.grid(True)\n\nplt.figure(figsize=(8, 6))\nplot_precision_vs_recall(precisions, recalls)\nplt.show()","b0c6c211":"threshold_90_precision = thresholds[np.argmax(precisions >= .90)]\nthreshold_90_precision","30486f1e":"titanic_labels_pred_90 = (titanic_labels_scores >= threshold_90_precision)\n\nprecision_score(titanic_labels, titanic_labels_pred_90)","0c09f53d":"recall_score(titanic_labels, titanic_labels_pred_90)","53084b9e":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(titanic_labels, titanic_labels_scores)\n\ndef plot_roc_curve(fpr, tpr, label = 'None'):\n  plt.plot(fpr, tpr, linewidth = 2, label = label)\n  plt.plot([0, 1], [0, 1], 'k--')\n  plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n  plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n  plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n  plt.grid(True)                                            # Not shown\n\nplt.figure(figsize=(8, 6))                         # Not shown\nplot_roc_curve(fpr, tpr)\n  \nplt.show()","602cb123":"from sklearn.metrics import roc_auc_score\n\n# auc score based on our training set and our scores\nroc_auc_score(titanic_labels, titanic_labels_scores)","31ae5ccb":"from sklearn.ensemble import RandomForestClassifier\n\n# Random Forest Classifier constructor\nforest_clf = RandomForestClassifier(n_estimators = 100)\n\n# returns array of probability the random forest believes the passenger survived or not\ntitanic_probas_forest = cross_val_predict(forest_clf, titanic_prepared, titanic_labels, cv = 3, method = 'predict_proba')","6d617efb":"# scores of only the positive class\ntitanic_scores_forest = titanic_probas_forest[:, 1]\n\n# compute fpr, tpr, at various thresholds\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(titanic_labels, titanic_scores_forest)","05bcf221":"# plot both classifiers\nplt.figure(figsize = [8, 6])\nplt.plot(fpr, tpr, 'b:', label = 'SGD')\nplot_roc_curve(fpr_forest, tpr_forest, label = 'Random Forest')\nplt.legend(loc = 'lower right')\nplt.show()","586e244c":"# roc score\nroc_auc_score(titanic_labels, titanic_scores_forest)","42d17c15":"# predict labels based on RFC\ntitanic_labels_pred_forest = cross_val_predict(forest_clf, titanic_prepared, titanic_labels, cv = 3)","ce8a7cce":"precision_score(titanic_labels, titanic_labels_pred_forest)","f3d45a72":"recall_score(titanic_labels, titanic_labels_pred_forest)","996b8c30":"from sklearn.model_selection import RandomizedSearchCV\n\n# number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 2000, num = 10)]\n# number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\n# iterate to find the best hyperparameters\nforest_clf = RandomForestClassifier()\nrnd_search = RandomizedSearchCV(forest_clf, param_distributions = random_grid, n_iter = 100, cv = 10, verbose = 2, scoring = 'accuracy')\n\n# fit the data\nrnd_search.fit(titanic_prepared, titanic_labels)","4474f120":"# view best estimator\nrnd_search.best_estimator_","e68d46ed":"feature_importances = rnd_search.best_estimator_.feature_importances_\nfeature_importances","059025f9":"# from our transformation pipeline, extract the categorical transformation\ncat_encoder = full_pipeline.named_transformers_['cat']\n\n# extract the one hot encoder step\nname, encoder = cat_encoder.steps[1]\n\n# empty list for one hot categories\ncat_one_hot_attribs = []\n\n# iterate through categories to append the one hot categories\nfor i in range(len(encoder.categories_)):\n    cat_one_hot_attribs += list(encoder.categories_[i])\n\n# combine numerical features with one hot categories\nattributes = num_attribs + cat_one_hot_attribs\n\n# zip and view the feature importances next to the attribute name\nsorted(zip(feature_importances, attributes), reverse = True)","0824e4b8":"# get our best model\nfinal_model = rnd_search.best_estimator_\n\n# load in the test set\nX_test = pd.read_csv('..\/input\/test.csv')\n\n# drop columns\nX_test_prepared = X_test.drop(drop_columns, axis = 1)\n\n# call our transformation pipeline\nX_test_prepared = full_pipeline.transform(X_test_prepared)\n\n# make our final predictions\nfinal_predictions = final_model.predict(X_test_prepared)","0543011d":"from IPython.display import HTML\nimport base64\n\n# define download link function\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index = False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload = payload, title = title, filename = filename)\n    return HTML(html)\n\n# create submission DataFrame with the corresponding Id\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = X_test['PassengerId']\nsubmission['Survived'] = final_predictions\n\n# create download link\ncreate_download_link(submission)","3e4c5ead":"# Select and Train a Model\n\nLet's start with a Stochastic Gradient Descent (SGD) classifier, using Scikit-Learn's `SGDClassifier` class.","0736bf16":"Let's plot the precision and recall as a function of the threshold value.","c4444737":"The `RandomForestClassifier`'s ROC curve looks much better than the `SGDClassifer`'s. Remember, closer to the top left corner is better!\n\nLet's also check the ROC AUC score:","a8453c21":"Now that we have all of the decision function scores, we will use the `precision_recall_curve()` function to compute precision and recall for all thresholds:","b4226b53":"It looks like the model correctly predicted that the passenger survived!\n\nOf course this was only one passenger... we can take advantage of some other functions to get a better understanding of our model's performance.","5ac1802e":"Looks like we scored over 74% on each fold!\n\nLet's look at the confusion matrix to evaluate the performance in more detail.\n\nFirst, we will get predictions with the `cross_val_predict()` function across 3 folds just like the previous `cross_val_score()` function.","00ed71c2":"# Performance Measures\n## Measuring Accuracy Using Cross-Validation\n\nWe can use the `cross_val_score()` function to evaluate the performance of our `SGDClassifier` model using K-fold cross-validation with 3 folds.  The function will split the training set into K-folds (3 in this case), make predictions and evaluate them on each fold using the model trained on the remaining folds.","29d6a9ca":"## The ROC Curve\n\nThe *receiving operating characteristic* (ROC) is similar to the precision\/recall curve, but plots the *true positive rate* (recall) against the *false positive rate*.\n\nTo plot the ROC curve, we will use the `roc_curve()` function to compute the TPR and FPR for various threshold values:","bbcfdc63":"## Transformation Pipeline\n\nNow that our data is split into both numerical and categorical sets, we can apply separate transformation pipelines.\n\nFor the numerical columns, we will fill in the missing values with the median using the `SimpleImputer` and scale the features to be closer in magnitude using the `StandardScaler`.\n\nFor the categorical columns, we will fill in the missing values with the mode using the `SimpleImputer` and encode the strings into numbers using the `OneHotEncoder`.\n\nFor the `OneHotEncoder`, we will explicitly specify the categorical names since they may appear in different orders when we test several folds or if they appear in a different order in our testing set.","c7361eb4":"We were able to achieve nearly 85% coverage!  This is over 10 points better than our previous `SGDClassifer`.\n\nLet's also calculate our recall and precision scores.","7ca1d2d7":"Now let's combine both pipelines using the `ColumnTransformer` function and apply the transformations to our training features.","7ccd3e3b":"# Fine tune the model\n\nWe will use `RandomizedSearchCV` to help us look for the best hyperparameters for our `RandomForestClassifier`.","6ffb0118":"Now that we have 3 predictions, we can call the `confusion_matrix()` function.","355ac009":"Done!  That was easy right?  Let's look at the first passenger's standardized features:","c8e0d1b8":"# Get the data\n\nFirst we will import our training data from our workspace into a pandas DataFrame.","0620a782":"# Introduction\n\nIn this notebook, I apply my learnings from the 2nd edition of the book [*Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow 2: Concepts, Tools, and Techniques to Build Intelligent Systems* by Aur\u00e9lien G\u00e9ron](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=as_li_ss_tl?crid=1RIE6GMGYGEB7&keywords=hands+on+machine+learning+with+scikit-learn+and+tensorflow&qid=1564493355&s=gateway&sprefix=hands+on+machine+l,aps,424&sr=8-4&linkCode=sl1&tag=lejimmy00-20&linkId=752dc952853b9f472dddde246b504cfb&language=en_US).\n\nIn [Chapter 2](https:\/\/www.kaggle.com\/lejimmy\/chapter-2-applying-hands-on-ml-with-scikit-learn), we explored a regression task predicting house values.  It just so happens that Kaggle has a housing dataset that was extremely similiar to the problem presented in the textbook.\n\nFor Chapter 3, we turn our attention to classification systems.  Though the chapter uses the popular MNIST data set, we'll be exploring the classic Titanic dataset.\n\nWe will use the following checklist to guide us through our project.\n\n## Machine Learning Project Checklist\n1. Frame the problem and look at the big picture\n2. Get the data\n3. Explore the data to gain insights\n4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms\n5. Explore many different models and short-list the best ones\n6. Fine-tune your mdoels and combine them into a great solution","9b7fb1da":"And whether or not they survived...","74893738":"# Look at the big picture\n\n## 1. What's the problem?\n\nLet's pretend we work for an international cruise line like the Carnival Corporation.  Let's also assume that they want to draw insights from all previous shipwrecks to ensure passengers in their current fleet have the best chances of survival.\n\nWe will start with the RMS Titanic.  The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n## 2. How will we frame the problem?\n\nOur goal is to predict if a passenger survived the sinking of the Titanic or not.  For each in the test set, we must predict a 0 or 1 value for the variable.\n\nSince there are only 2 possible outcomes, we will frame the problem as a binary classifier.\n\nOur score is based on how many passengers we are able to correctly predict.","9e952a3e":"Done!  Now we have a 90% precision classifier!  Of course... our recall score is quite low.  Let's look at another common tool to compare classifiers.","9ee629c4":"To see what this looks like next to the categories, we'll have to add the column names back in.  This is a bit tricky since the one hot encoder turned our categorical features into columns, but here's how we'll do it:","14eec4fa":"Since machine learning algorithms don't do well with string data, let's get a list of the possible categorical values so we can encode them as numbers later.","27f1123a":"From a quick preview of the first few rows, we can see their PassengerID number, if they survived (the label of interest), their class, age, sex, etc.\n\nWe can also see that there are some missing fields in the `Cabin` column.  \n\nLet's continue to preview the dataset to familiarize ourselves.","88dbc174":"# Discover and visualize the data to gain insights\n\n*Placeholder for future exploration and iterations*","24f87ed9":"Here, we scored above a 0.7.  Not bad, but also not that great.  Let's move on to evaluating a different model.","7981285c":"To see how the model is determining if a passenger survives or not, we will have to look at the `SGDClassifier`'s decision function.  The model predicts a score and if it is greater than a threshold it assigns the instance to the positive class and vice versa.","d329db50":"Let's drop the unique and rare columns such as `PassengerID`, `Ticket` and `Name`.  When we run the `value_counts()` method for these columns, there are very few occurences such that the machine learning algorithms probably won't be able to pick up any patterns.\n\nLet's also drop the `Cabin` column as there are too many missing rows for us to fill in the missing data with meaningful values.","b65bf773":"# Evaluate another model\n\nNow that we a a trained model and some baseline metrics, let's train a `RandomForestClassifier` and compare its ROC and ROC AUC score to the `SGDClassifier`.\n\nWe'll first need to get the scores for each instance in the training set, but since the `RandomForestClassifier` doesn't have a `decision_function()` method, we will use the `predict_proba()` method instead.","d1b5a286":"Now that our data is prepared, let's train a model!","49bba847":"Next, we will need to split our categorical and numerical columns to be prepared separately.","716b1881":"It looks like they did... let's give the model the same features and see if it can properly predict if they survived or not.","4dcf51d5":"From this list, we can see that the gender, fare price, and age of the passenger accounted for over 75% of determining whether the passenger survived or not!\n\nAt the tail end, we can see that the port in which the passenger embarked on did not matter as much.\n\nFor our next iteration, we should consider dropping the `Embarked` column to see if it may reduce noise and improve our results.","a8778d1d":"From our graph, we can see there is a sharp drop off in precision around 22% recall and an additional sharp drop off around 80% recall.  Since the competetition is graded on accuracy, we will aim for a 90% precision rate.","5f9d4fc8":"Now that we have our predictions, let's download them and prepare for submission!","34fe24f5":"Again, we can see the trade-off between higher recall (TPR) and more false positives (FPR). The dotted line represents the ROC curve of a purely random classifier, a good classifer moves towards the top-left corner.\n\nAnother way to compare classifers is to measure the area under the curve (AUC). A perfect classifer will have a ROC AUC equal to one. A purely random classifier will have a ROC AUC equal to 0.5.","002042c7":"From the chart, we can see that as we increase our threshold, the precision increases (the model can accurately predict whether passengers will survive), however our recall will decrease (the model inaccurately predicts more passengers did not survive).\n\nAnother way to predict a good threshold is by plotting the precision vs recall.","88594cf3":"The first row represents an *actual class* while each column represents a *predicted class*.\n\nSo for the first row, we have 460 *true negatives* where the model predicted the passenger did not survive and they didn't.\n\nNext to that we have 89 *false positives* where the model predicted the passenger survived but they didn't.\n\nOn the next row, we have 121 *false negaties* where the model predicted they did not survive but they did.\n\nNext to that we have 221 *true positives* where the model predicted they survived and they did.\n\nTo test what a perfect classifier would look like, let's consider the following confusion matrix:","c6d97e65":"Now that we've tweaked our model to our heart's content, let's evaluate the final model on the test set. We will get the predictors and the labels from the test set, run our transformation pipeline, and evaluate the final model on the test set.","42258e07":"Now, let's see what columns we have left to work with:","dfeef03f":"Some quick observations:\n- Most people rode in the 3rd class\n- This looks correlated to most people paying for the cheapest fares\n- Age is not quite normally distributed with a skew towards babies\n- Most passengers also travelled without parents, spouses, or children\n\nNow that we have a high level understanding of what our data looks like, let's dive a bit deeper into each of the features.","d2993dc3":"From our best estimator, let's look at which features were the most important.","39a4a00a":"# Prepare the data for machine learning algorithms\n\nWe will separate the label we want to predict (survived or not) from the features since our testing set only contains the features.","d399d032":"It looks like most of the missing data is in the `Cabin` column.  The `Age` and `Embarked` columns are also missing a number of rows.  We can either drop these missing features or fill in the missing rows with some sort of average later on.\n\nFor our datatypes, we have a mix of ints, floats, and strings.\n\nLet's continue our exploration by looking at the statistical summary.","3bce3305":"Now that the `RandomizedSearchCV` is done, let's look at which hyperparameters were the best for our training data.","01c0aaa6":"The chapter on classification moves into multiclass classification of the MNIST dataset, but is not relevant to this dataset.\n\nWe will move forward from what we learned in Chapter 2, to evaluate our model on the test set.  We will start with tuning our hyperparameters.","4fd439a6":"So what does this mean?  When the model claims a passenger survived, it is only correct 72% of the time.  The model also only detects 65% of the survivors.\n\nReview the confusion matrix chart based on the MNIST dataset:\n\n![](https:\/\/learning.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/assets\/mls2_0302.png)","d6b58e01":"A more convenient metric is called the F1 score which combines precision and recall.  The F1 score is more sensitive to low values so both precision and recall both must be high to achieve a high F1 score.","13a4b8c8":"# Evaluate the system on our test set","18ff5bd8":"## Precision and Recall\n\n*Precision* is the accuracy of the positive predictions.  *Recall* is the ratio of positive instances that are correctly detected.\n\nTo compute these classifier metrics, we can use Scikit-Learn's `precision_score` and `recall_score`.","ae45233c":"For this passenger, because the score is about 0, the model assigned the passenger to the positive (survived) class.\n\nTo determine what threshold we should use, let's use the `cross_val_predict()` function to get the scores, but instead of the prediction itself, we want the decision scores.","b5edc4b6":"At a quick glance we see that the average age was around 30, less than 40% of the passengers survived, and the average fare was about $30.\n\nWe can read more about the column descriptions [here](https:\/\/www.kaggle.com\/c\/titanic\/data).\n\nFor now, let's make some quick plots to try and visualize our data."}}