{"cell_type":{"55f59386":"code","fa984857":"code","ca3a7404":"code","9d2797fd":"code","45699dc4":"code","aac7b0b0":"code","b724b0c7":"code","d6e47005":"code","9d2cdab2":"code","14733b36":"code","b6375d77":"code","9200180b":"code","cf3942ee":"code","dd01331d":"code","299b4153":"code","1a13b5fb":"code","62b579a4":"code","f7ae5b56":"code","6153aeb9":"code","c0c300c1":"code","686619b9":"code","1a09860c":"code","c6ad03d3":"code","5aec599a":"code","0ac43035":"code","f2da4263":"code","14cf46ac":"code","968222c0":"markdown","af03285e":"markdown","b4aecbf2":"markdown","806ab16e":"markdown","9c455ef0":"markdown","0f6052fd":"markdown","1fb0a3fb":"markdown","65c1a530":"markdown"},"source":{"55f59386":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fa984857":"# I import the libraries as and when they are required in the code. ","ca3a7404":"# Load the data\nraw_data = pd.read_csv('..\/input\/indian_liver_patient.csv')\nip_data = raw_data.copy()","9d2797fd":"#ip_data","45699dc4":"# See the types of data and missing values in the dataset.\n# Categorical data (like gender) need to be converted using dummy variables. \nip_data.info()","aac7b0b0":"#Since there are only 4 records with null values in Albumin_and_Globulin_Ratio column, we can drop those records.\nip_data = ip_data.dropna(how='any', axis = 0)\n","b724b0c7":"# Convert Gender to 0s (for Male) and 1s (for Female)\nip_data['Gender'] = ip_data['Gender'].map({'Male':0, 'Female':1})\nip_data['Dataset'] = ip_data['Dataset'].map({2:0, 1:1}) # To solve ValueError: endog must be in the unit interval.\n\n# Check if mapping happened properly\nip_data","d6e47005":"# Correlations between variables help us identify the features that can be excluded. \n# We can exclude one in two features which has a strong correlatoin (|correlation| > 0.5) with another feature\n# e.g: In the below given case, Total_Bilirubin and Direct_Bilirubin are strongly correlated. So we can\n# discard one of those features. The exclusion of features happens 2 cells down.  \nip_corr = ip_data.drop(['Gender', 'Dataset'], axis = 1)\nip_corr.corr()","9d2cdab2":"samples_count = ip_data.shape[0]\n# You will see drastic change in models' accuracy when you change the train and test sample proportion (80:20, 70:30 etc)\nip_train_count = int(0.8*samples_count) \nip_test_count = samples_count - ip_train_count\n\nip_train_data = ip_data[:ip_train_count]\nip_test_data = ip_data[ip_train_count:]","14733b36":"print(ip_train_count)\nprint(ip_test_count)","b6375d77":"import statsmodels.api as sm","9200180b":"## You can start with all features and then optimize the model by removing the features from the model.\n\nX1_train = ip_train_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',\n              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]\n\n#X1_train = ip_train_data[['Age', 'Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin']]\n\nX_train= sm.add_constant(X1_train)\n#X_train= X1_train.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant\ny_train = ip_train_data['Dataset']\n\n\nX1_test = ip_test_data[['Age','Gender','Total_Bilirubin','Direct_Bilirubin','Alkaline_Phosphotase','Alamine_Aminotransferase',\n              'Aspartate_Aminotransferase','Total_Protiens','Albumin', 'Albumin_and_Globulin_Ratio' ]]\n\n#X1_test = ip_test_data[['Age','Direct_Bilirubin','Alamine_Aminotransferase','Total_Protiens','Albumin' ]]\n\nX_test = sm.add_constant(X1_test)\n#X_test = X1_test.copy() # Independent variables without constant. I have doubts in the use of vaiables without adding a constant\ny_test = ip_test_data['Dataset']","cf3942ee":"reg_log = sm.Logit(y_train,X_train)\nresult_log = reg_log.fit()\n\nresult_log.summary2()\n\n# See the values against each feature in 'P>|z|' column. The value less than 0.05 means the feature is insignificant in the model.\n# Surprisingly, gender is insignificant in this model. \n# We saw a strong correlation between Total_Bilirubin and Direct_Bilirubin, and we could drop one of those features.\n# But in this model both Total_Bilirubin and Direct_Bilirubin are insignificant(P > 0.05). So we must discard both features.\n","dd01331d":"# Confusion matrix using the train dataset\nresult_log.predict()\nresult_log.pred_table()","299b4153":"# Print X_test and X_train and see if the order of features are same in both datasets\nX_test.head()","1a13b5fb":"X_train.head()","62b579a4":"logit_predicted = result_log.predict(X_test)","f7ae5b56":"# Uncomment below line, if you wanted to see the predicted values\n# logit_predicted","6153aeb9":"# Confusion Matrix of predicted values. if you wanted to use sklearn's 'confusion_matrix', \n# you should convert float values in 'logit_predicted' to 0s and 1s.\n\n# Here I use a custom confusion matrix code \nbins = np.array([0,0.5,1])\ncm_log = np.histogram2d(y_test, logit_predicted, bins = bins)[0]\nlogit_accuracy = (cm_log[0,0] + cm_log[1,1])\/cm_log.sum()\n","c0c300c1":"print('Confusion Matrix (Logit): \\n', cm_log)\nprint('---------------------------------------------')\nprint('Accuracy (Logit): \\n', round(logit_accuracy*100,2), '%')","686619b9":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\n\nlog_predicted = logreg.predict(X_test)","1a09860c":"print('Confusion Matrix (Logistic Reg): \\n', confusion_matrix(y_test,log_predicted))\nprint('---------------------------------------------')\nprint('Accuracy (Logistict Reg): \\n', round(accuracy_score(y_test, log_predicted)*100,2), '%')","c6ad03d3":"from sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\nresult_gauss = gaussian.fit(X_train,y_train)\n\ngauss_predicted = gaussian.predict(X_test)","5aec599a":"# Uncomment below line, if you wanted to see the predicted values\n#gauss_predicted","0ac43035":"print('Confusion Matrix (Gaussian): \\n', confusion_matrix(y_test,gauss_predicted))\nprint('---------------------------------------------')\nprint('Accuracy (Gaussian): \\n', round(accuracy_score(y_test, gauss_predicted)*100,2), '%')","f2da4263":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, y_train)\n\nrf_predicted = random_forest.predict(X_test)","14cf46ac":"print('Confusion Matrix (Random Forest): \\n', confusion_matrix(y_test,rf_predicted))\nprint('---------------------------------------------')\nprint('Accuracy (Random Forest): \\n', round(accuracy_score(y_test, rf_predicted)*100,2), '%')\n","968222c0":"## Random Forest","af03285e":"## Logitstic Regression (sklearn)","b4aecbf2":"#  Indian Liver Patients Analysis (Logistic, Gaussian, Random Forest)","806ab16e":"Tasks to perform\n1. Data Analysis\n2. Data cleanup\n3. Feature selection\n4. Train and test different models (Logistic, Gaussian, Random Forest)","9c455ef0":"## Logit function from statsmodels","0f6052fd":"### Split the dataset into train and test","1fb0a3fb":"## Gaussian Naive Bayes","65c1a530":"### Declare the Dependent (y_...) and Independent (X_...) variables"}}