{"cell_type":{"0a51b618":"code","5234ceba":"code","8aa390a0":"code","fb12506b":"code","068a9271":"code","1b0b76fb":"code","a88b7aa4":"markdown","40e2a504":"markdown","9761f885":"markdown","776296fd":"markdown","86705397":"markdown","6457030a":"markdown","67d0a11a":"markdown","0916dcca":"markdown","9b55c424":"markdown","c959a042":"markdown"},"source":{"0a51b618":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d import proj3d\nimport matplotlib.animation as animation\nfrom matplotlib.patches import FancyArrowPatch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nurl = '..\/input\/breast-cancer-wisconsin-data\/data.csv'\ndf = pd.read_csv( url )\n\nrndperm = np.random.permutation(df.shape[0])\nD = df.iloc[rndperm, :]\n\n## Print the number of rows in the data set\ndf_rows, df_cols = df.shape\nprint('Table size : {} x {}'.format(df_rows, df_cols) )\n\nix_mn = [*range(0, 10, 1)]\n\ndef arr_shift(arr, shift):\n    return [i+shift for i in arr]\n\nclass_feat = ['Radius', 'Texture', 'Perimeter', 'Area', 'Smoothness', 'Compactness', 'Concavity', 'Concave points', 'Symmetry', 'Fractal dim.']\n\nle = LabelEncoder()                 # label encoding\nX, y = D.iloc[:, 2:], D[['diagnosis']]\ny = y.rename(columns={'diagnosis': 'Diagnosis'})\ny = le.fit_transform( y['Diagnosis'].values )\nif isinstance(y, pd.DataFrame):\n    y = y.values.ravel()\n    \nX_mn = X.iloc[:, ix_mn]","5234ceba":"# **************************************************************** #\n# ------------------------ 2D PCA scatter ------------------------ #\n# **************************************************************** #\n\nfrom sklearn.preprocessing import StandardScaler\n\n# In general, it's a good idea to scale the data prior to PCA.\nscaler = StandardScaler()\nscaler.fit(X_mn)\nX_mn = scaler.transform(X_mn)\npca = PCA()\nx_new = pca.fit_transform(X_mn)\n\ndef PCA_scatter(X, coeff, y_M, labels):\n    \n    n = coeff.shape[0]\n    xs, ys = X[:,0], X[:,1]             # zs = X[:,2]\n    scalex, scaley = 1.0\/(xs.max() - xs.min()) , 1.0\/(ys.max() - ys.min())\n    # scalez = 1.0\/(zs.max() - zs.min()) <-- 3D of Z\n    \n    # ---------- Scatter color by class ----------- #\n    plt.scatter(xs[y_M] * scalex, ys[y_M] * scaley, c = 'orange', alpha=0.5) \n    plt.scatter(xs[1-y_M==True] * scalex, ys[1-y_M==True] * scaley, c = 'blue', alpha=0.5)\n    # plt.scatter(xs * scalex, ys * scaley, c = 'blue')\n\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'r',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', fontsize=12, ha = 'center', va = 'center')\n    \n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid(linestyle='-', linewidth=0.5)\n\n#Call the function. Use only the 2 PCs\ny_M, y_B = y==1, y==0           # Logical statement for Benign indication\n\npca_i = 2\n\nPCA_scatter(x_new[:, 0:pca_i], np.transpose(pca.components_[0:pca_i, :]), y_M, class_feat)\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.show()","8aa390a0":"D.iloc[:, 2:].head(10)        # Show ten first samples (after random shuffling)","fb12506b":"# ******************************************************** #\n# ------------- PCA : Utilization functions -------------- #\n# ******************************************************** #\n\n# --------------- Dimensionality Reduction --------------- #\ndef PCA_reduction(X, PC_num):\n    scaler = StandardScaler()\n    scaler.fit(X)\n    X = scaler.transform(X)\n    pca = PCA()                                            # Perform PCA transformation\n    X_pca = pca.fit_transform(X)[:, 0:PC_num]              # Low dim : Projected  instances\n    max_Var = np.transpose(pca.components_[0:PC_num, :])   # Direction of maximum variance\n    return X_pca, max_Var\n\n\nclass Arrow3D(FancyArrowPatch):\n    def __init__(self, xs, ys, zs, *args, **kwargs):\n        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n        self._verts3d = xs, ys, zs\n\n    def draw(self, renderer):\n        xs3d, ys3d, zs3d = self._verts3d\n        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n        FancyArrowPatch.draw(self, renderer)\n\n# ------------------- PCA Visualization ------------------ #\ndef Scatter_3D(X_pca, max_Var, y, labels):\n    \n    y_M, y_B = y==1, y==0           # Logical statement for Benign indication\n    xs, ys, zs = X_pca[:,0], X_pca[:,1], X_pca[:,2]\n    s_x, s_y, s_z = 1.0\/(xs.max() - xs.min()), 1.0\/(ys.max() - ys.min()), 1.0\/(zs.max() - zs.min())\n    \n    # -------------- Scatter color by class -------------- #\n    \n    ax.scatter(xs[y_M]*s_x, ys[y_M]*s_y, zs[y_M]*s_z, s=42, c='orange', alpha=0.35) \n    ax.scatter(xs[y_B]*s_x, ys[y_B]*s_y, zs[y_B]*s_z, s=42, c='blue',   alpha=0.35)\n    n = max_Var.shape[0]\n\n    for i in range(n):\n        mean_x, mean_y, mean_z = max_Var[i,0], max_Var[i,1], max_Var[i,2]\n        a = Arrow3D([mean_x, 0.0], [mean_y, 0.0], [mean_z, 0.0], mutation_scale=15, lw=3, arrowstyle=\"<|-\", color=\"r\")\n        ax.add_artist(a)\n\n        if labels is None:\n            ax.text(max_Var[i,0]* 1.15, max_Var[i,1] * 1.15, max_Var[i,2] * 1.15, \"Var\"+str(i+1), color = 'g', fontsize=14, ha = 'center', va = 'center')\n        else:\n            ax.text(max_Var[i,0]* 1.15, max_Var[i,1] * 1.15, max_Var[i,2] * 1.15, labels[i],      color = 'g', fontsize=14, ha = 'center', va = 'center')\n\n\n# --------------- Normalize data structure --------------- #\ndef self_Normalize( X ):\n    X_n = (X-X.mean())\/(X.max(axis=0)-X.min(axis=0))\n    return X_n\n","068a9271":"# ******************************************************** #\n# ----------------- Data Visualization ------------------- #\n# ******************************************************** #\n\n\n# ---------- Static : Setting of the figure -------------- #\ndef initialize_figure(pca_exp_mn):\n    fig = plt.figure(figsize=(10, 6))\n    ax = Axes3D(fig)\n    ax.xaxis.pane.fill, ax.yaxis.pane.fill, ax.zaxis.pane.fill = False, False, False\n    ax.legend(['Malignant', 'Benign'], fontsize=15, loc='best')\n    ax.set_xlabel('PC-1 : %.2f [%%]'%pca_1, fontsize=13)\n    ax.set_ylabel('PC-2 : %.2f [%%]'%pca_2, fontsize=13)\n    ax.set_zlabel('PC-3 : %.2f [%%]'%pca_3, fontsize=13)\n    return ax, fig\n\n\n\n# --------- Static : points at constant location --------- #\ndef init():\n    Scatter_3D(X_pca, max_Var, y, class_feat)\n    return fig,\n\n\n# ----------- Dynamic : define desired motion ------------ #\ndef animate(i):\n    ''' \n    input i : number of frames \n    Total_frame : defines length of footage\n    [elev, azim] : parameters of 3D point of view\n    '''\n    thres = 200\n    if i > thres:\n        j = i-thres\n    else:\n        j = 0\n\n    # Explain on motion preferences\n    Elev = 45 - i\/6 + 2*j\/6\n    Azim = -120+i\/2\n    ax.view_init(elev=Elev, azim=Azim)\n    \n    frame_freq = 20\n    if (i%frame_freq - Total_frame%10) == 0:\n        print('Remaining frames : ', Total_frame-i)\n    return fig,\n\n\n# -------------- Implement PCA on the data --------------- #\nPC_num = 3                     # Dimensionallity reduction to 3D\npca_mn = PCA()\nX_mn = self_Normalize(X_mn)\npca_mn.fit( X_mn )\npca_exp_mn = pca_mn.explained_variance_ratio_\npca_1, pca_2, pca_3 = pca_exp_mn[0]*100, pca_exp_mn[1]*100, pca_exp_mn[2]*100\nX_pca, max_Var = PCA_reduction(X_mn, PC_num)\n\n\n# ------------- Processing of data before PCA ------------ #\ny_M, y_B = y==1, y==0      # Logical statement for Benign indication\nxs, ys, zs = X_pca[:,0], X_pca[:,1], X_pca[:,2]\ns_x, s_y, s_z = 1.0\/(xs.max() - xs.min()), 1.0\/(ys.max() - ys.min()), 1.0\/(zs.max() - zs.min())\nax, fig = initialize_figure(pca_exp_mn)\n\n\n# ------------- Parameter for video footage -------------- #\nTotal_frame = 101\nanim = animation.FuncAnimation(fig, animate, init_func=init, frames=Total_frame, interval=20, blit=True)\nanim.save('PCA_vid.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\nprint('Finished ! Download video from : \"..\/input\/output\/\"')","1b0b76fb":"# @title\n\nplt.rcParams['figure.figsize'] = (9, 5)\nclass_full =  class_feat + ['Diagnosis']\n\n# Initialize the PCA method\npca_mn = PCA()\n\n# Mean Dataset\npca_mn.fit(X_mn)\npca_exp_mn = pca_mn.explained_variance_ratio_\nt_solo = [*range(1, X_mn.shape[1]+1)]\n\n\ndef PCA_plot(t, pca_exp):\n    # ---------------------------------------------- #\n    # Instantiate the prinicipal (LHS) plot\n    pca_cum = np.cumsum(pca_exp)\n    fig, ax1 = plt.subplots()\n    color = 'tab:blue'\n\n    ax1.grid(color='b', ls = '-.', lw = 0.25)\n    ax1.set_xlabel('n-th component', fontsize=16)\n    ax1.set_ylabel('Explained Variance Ratio (EVR)', color=color, fontsize=17)\n    ax1.plot(t, pca_exp, 'bo', color=color, markersize=7)\n    ax1.plot(t, pca_exp, '--', color=color, linewidth=2.5)\n    ax1.tick_params(axis=\"x\", labelsize=12)\n    ax1.tick_params(axis=\"y\", labelsize=12)\n\n    # ---------------------------------------------- #\n    # Instantiate a second axes that shares the same x-axis\n    ax2 = ax1.twinx()  \n    color = 'tab:green'\n\n    ax2.set_ylabel('Cumulative EVR', color=color, fontsize=17)  # we already handled the x-label with ax1\n    ax2.plot(t, pca_cum, 'go', color=color, markersize=7)\n    ax2.plot(t, pca_cum, '--', color=color, linewidth=2.5)\n    ax2.tick_params(axis=\"y\", labelsize=12)\n    t_score, t_loc = pca_cum[2], pca_cum[2]*1.025\n    ax2.annotate('%.2f '%(t_score*100)+'[%]', fontsize=18, xy =(3, t_loc), xytext =(3, t_loc*1.06), arrowprops = dict(facecolor ='green', shrink = 0.05),) \n\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    plt.xticks(t)\n    plt.show()\n\nPCA_plot(t_solo, pca_exp_mn)","a88b7aa4":"### $\\text{Points in Motion :}$\n\nAnother study case for animation, is when the observer point of view is frozen and the points are in motion.\n\n* Consider the amazing work of on *Pierre Segonne* on [Medium](https:\/\/medium.com\/@pnpsegonne\/animating-a-3d-scatterplot-with-matplotlib-ca4b676d4b55) :\n\n<img src=\"https:\/\/miro.medium.com\/max\/600\/1*jF7UplHE94z59ihR3ZD5uw.gif\" width=\"550px\" style=\"vertical-align:middle;margin:0px 150px\">\n","40e2a504":"$$\n- \\, fin \\, -\n$$","9761f885":"### $\\text{Table of Contents}$\n\n* [Introduction](#Introduction)\n* [Principal Components Analysis](#PCA)\n* [3D Visualization](#3D-data)\n* [Explanation](#Closure)\n\n### ${\\text{Abstract}} \\ :$\n\nThe notebook proposes a simple method for turning any boring presentation of data into a lively animation,\nusing **proj3d** and **Axes3D** from the [*mplot3d*](https:\/\/matplotlib.org\/mpl_toolkits\/mplot3d\/api.html) library.\nSamples are taken from the Wisconsin Breast Cancer Dataset ([WBCD]( https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic) )).\n\nConsider the following 2d [biplot](https:\/\/en.wikipedia.org\/wiki\/Biplot#:~:text=Biplots%20are%20a%20type%20of,matrix%20to%20be%20displayed%20graphically.) which shows the PCA projection of the samples onto a 2D PC plane ($PCA: \\mathbb{R}^{10} \\rightarrow \\mathbb{R}^{2}$) :","776296fd":"$$\n\\circ \\quad \\text{Comments (\ud83d\udcac) , feedback (\ud83e\udd14) and upvotes (\ud83d\udc4d) are much welcome !} \\quad \\circ \\\\[1cm]\n$$","86705397":"All good, but such high dimensionality (of extracted feartues) still imposes difficulties for classic ML algorithms. To that end, a great friend of us is the principal component analysis (PCA).\n\n<a id=\"PCA\"><\/a>\n### $\\text{Principal Component Analysis (PCA)}$\n\n**PCA** can be done in many techniques (covariance, correlation, SVD etc.), for further theoretical development [[Wiki](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis)].","6457030a":"### $\\text{Download video :}$\n\n<img src=\"https:\/\/i.imgur.com\/Pw5fDa2.png\" width=\"650px\">\n\n<a id=\"Closure\"><\/a>\n### ${\\text{Explanation}} \\ :$\n\nThe major contribution of the PCA, is the transform capability to reveal the data's internal structure, in a manner that explains most the variance (EVR) :","67d0a11a":"The red **lines** denote the 10 original features (variables) projected after rescaling. Each feature lies in a direction that maximizes the original data's variance. Thereby, it can be seen that the *Concave points* feature maximizes the **1st PC** while the *Fractal dimension* contributes poorly to the **2nd PC**.\n\n* Consider the same procedure, but this time : $PCA: \\mathbb{R}^{10} \\rightarrow \\mathbb{R}^{3}$\n\n<img src=\"https:\/\/github.com\/Daniboy370\/Machine-Learning\/blob\/master\/Misc\/Animation\/PCA_vid.gif?raw=true\" width=\"700px\">","0916dcca":"Such that instead of using the full feature space $\\mathbb{R}^{10}$, we can capture more than $90\\%$ of it, using only 3 principal components.","9b55c424":"<a id=\"3D-data\"><\/a>\n### $\\text{3D Visualization}$\n\nIn this notebook I utilize the [*FuncAnimation*](https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.animation.FuncAnimation.html) library for a simple rotated motion footage :","c959a042":"<a id=\"section-one\"><\/a>\n### $\\text{Introduction}$\n\nThe original dataset was composed of $n=569 \\,$  digitized images of a fine needle aspirate ([FNA](https:\/\/en.wikipedia.org\/wiki\/Fine-needle_aspiration)), which later were engnireed by researchers, and concentrated as a tabular feature space of $X \\in \\mathbb{R}^{n \\times 30}$ . The instances are differed by labelled **diagnosis** (=$y$ \/ target variable), challanging the user to train a classifier that will be able to discriminate between unseen samples.\n"}}