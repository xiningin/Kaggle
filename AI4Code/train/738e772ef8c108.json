{"cell_type":{"14a99c48":"code","a79e51f8":"code","a6541bf7":"code","b7130bee":"code","f3a736c9":"code","09d75aad":"code","60d279bf":"code","11be47c8":"code","08579e04":"code","e0a4f5f0":"code","e950f079":"code","98f8180a":"code","489ac88e":"code","c192c0a7":"code","98705df4":"code","0e4e53e2":"code","ec0a17f6":"code","c1af8ffe":"code","b323a1d6":"code","1efd17df":"code","7e497b1c":"code","7cfe6f05":"code","c1edf7f3":"code","107ddc58":"code","1a891cc7":"code","27bc66a7":"code","05774ef9":"code","cd1cfbc3":"code","db48a850":"code","f6e94f23":"code","8800a9a1":"code","e8dbd144":"code","300472d2":"markdown","a03cdfc2":"markdown","d38c8f30":"markdown","d604ced1":"markdown","50139bc9":"markdown","d21571f6":"markdown","fd68d2d5":"markdown","49d68761":"markdown","6a870928":"markdown","c591a3cb":"markdown","b27bbbd4":"markdown","c5ed1db0":"markdown","dc5e3565":"markdown","4354fd7e":"markdown","aa26ce89":"markdown","a8970321":"markdown","957a32bf":"markdown","df6b6739":"markdown","64e34017":"markdown","d3093ccb":"markdown","e7ac15ca":"markdown","e8441c95":"markdown","d226039a":"markdown","c25d5cdd":"markdown","8dd1f94a":"markdown","b8388f14":"markdown","4ecc50c5":"markdown"},"source":{"14a99c48":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix\n\nplt.style.use(\"fivethirtyeight\")\n\ncolors = [\"Teal\",\"Indigo\",\"HotPink\",\"DarkGoldenRod\",\"Coral\"]","a79e51f8":"print(os.listdir('..\/input'))","a6541bf7":"df = pd.read_csv('..\/input\/data_arrhythmia.csv', delimiter=';')","b7130bee":"df.head()","f3a736c9":"df.drop([\"J\",\"R'_wave\",\"S'_wave\", \"AB\", \"AC\", \"AD\",\"AE\", \"AF\", \"AG\", \"AL\", \"AN\", \"AO\", \"AP\", \"AR\", \"AS\", \"AT\", \"AZ\", \"AB'\", \"BC\", \"BD\", \"BE\", \"BG\", \"BH\", \"BP\", \"BR\", \"BS\", \"BT\", \"BU\", \n          \"CA\", \"CD\", \"CE\", \"Cf\", \"CG\", \"CH\", \"CI\", \"CM\",\"CN\",\"CP\",\"CR\",\"CS\",\"CT\",\"CU\",\"CV\",\"DE\",\"DF\",\"DG\",\"DH\",\"DI\",\"DJ\",\"DR\",\"DS\",\"DT\",\"DU\",\"DV\",\"DY\",\"EG\",\n          \"EH\", \"EL\", \"ER\", \"ET\", \"EU\", \"EV\", \"EY\", \"EZ\", \"FA\", \"FE\", \"FF\", \"FH\", \"FI\", \"FJ\", \"FK\", \"FL\", \"FM\", \"FR\", \"FS\", \"FU\", \"FV\", \"FY\", \"FZ\", \"GA\",\n          \"GB\", \"GG\", \"GH\", \"HD\", \"HE\", \"HO\", \"IA\", \"IB\", \"IK\", \"IL\", \"IY\", \"JI\", \"JS\", \"JT\", \"KF\", \"KO\", \"KP\", \"LB\", \"LC\", \"T\", \"P\", \"QRST\", \"heart_rate\"], axis=1, inplace=True)","09d75aad":"df.head()","60d279bf":"df['height'].value_counts().sort_index()","11be47c8":"df.loc[df[\"height\"] == 608, \"height\"] = 61\ndf.loc[df[\"height\"] == 780, \"height\"] = 78\ndf['height'].value_counts().sort_index()","08579e04":"norm_risk_list = []\nfor diagnose in df.diagnosis:\n    if diagnose == 1:\n        norm_risk_list.append(True)\n    else:\n        norm_risk_list.append(False)\ndf[\"label\"] = np.array(norm_risk_list)\ndf.drop(columns = [\"diagnosis\"],inplace = True)\ndf.label.value_counts()","e0a4f5f0":"X = df.drop(columns = [df.columns[-1]])\ny = df[df.columns[-1]]\nprint(X.shape)\nprint(y.shape)","e950f079":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","98f8180a":"scaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nX_train = X_train_scaled\nX_test = X_test_scaled","489ac88e":"model_names = [\"Logistic Regression\",\n               \"K-Nearest Neighbors\",\n               \"Decision Tree Classifier\",\n               \"Random Forest Classifier\",\n               \"Gaussian Naive Bayes\"]\n\nmodels = []\npredictions = []\npred_probabilities = []","c192c0a7":"log_model = LogisticRegression(random_state=0,solver = \"saga\")\nmodels.append(log_model)\nlog_model.fit(X_train, y_train)\nlog_predprob = log_model.predict_proba(X_test)\npred_probabilities.append(log_predprob)\nlog_pred = log_model.predict(X_test)\npredictions.append(log_pred)","98705df4":"knn_model = KNeighborsClassifier(n_neighbors=80)\nmodels.append(knn_model)\nknn_model.fit(X_train, y_train)\nknn_predprob = knn_model.predict_proba(X_test)\npred_probabilities.append(knn_predprob)\nknn_pred = knn_model.predict(X_test)\npredictions.append(knn_pred)","0e4e53e2":"#best_score = -np.inf\n#best_n = np.inf\n#for n in range(1,df.columns.shape[0]):\n#    temp_model = KNeighborsClassifier(n_neighbors=n)\n#    print(n,end=\" \")\n#    temp_model.fit(X_train, y_train)\n#    temp_predprob = temp_model.predict_proba(X_test)\n#    temp_score = roc_auc_score(y_test,temp_predprob[:, 1])\n#    if temp_score > best_score:\n#            best_score = temp_score\n#            best_n = n\n#print(\"Best performing number of n_neighbors is\",best_n,\"scoring\",round(best_score * 100 , 2))","ec0a17f6":"tree_model = DecisionTreeClassifier(random_state=0,max_depth = 8,max_features=\"auto\")\nmodels.append(tree_model)\ntree_model.fit(X_train, y_train)\ntree_predprob = tree_model.predict_proba(X_test)\npred_probabilities.append(tree_predprob)\ntree_pred = tree_model.predict(X_test)\npredictions.append(tree_pred)","c1af8ffe":"#best_score = -np.inf\n#best_n = np.inf\n#for n in range(1,df.columns.shape[0]):\n#    temp_tree_model = DecisionTreeClassifier(random_state=0,max_depth = n,max_features=\"auto\")\n#    print(n,end=\" \")\n#    temp_tree_model.fit(X_train, y_train)\n#    temp_tree_predprob = temp_tree_model.predict_proba(X_test)\n#    temp_score = roc_auc_score(y_test,temp_tree_predprob[:, 1])\n#    if temp_score > best_score:\n#            best_score = temp_score\n#            best_n = n\n#print(\"Best performing number of max_depth is\",best_n,\"scoring\",round(best_score * 100 , 2))","b323a1d6":"rndfor_model = RandomForestClassifier(max_depth=9, random_state=0,n_estimators = 100)\nmodels.append(rndfor_model)\nrndfor_model.fit(X_train, y_train)\nrndfor_predprob = rndfor_model.predict_proba(X_test)\npred_probabilities.append(rndfor_predprob)\nrndfor_pred = rndfor_model.predict(X_test)\npredictions.append(rndfor_pred)","1efd17df":"#best_score = -np.inf\n#best_n = np.inf\n#for n in range(1,df.columns.shape[0]):\n#    temp_tree_model = RandomForestClassifier(max_depth=n, random_state=0,n_estimators = 100)\n#    print(n,end=\" \")\n#    temp_tree_model.fit(X_train, y_train)\n#    temp_tree_predprob = temp_tree_model.predict_proba(X_test)\n#    temp_score = roc_auc_score(y_test,temp_tree_predprob[:, 1])\n#    if temp_score > best_score:\n#            best_score = temp_score\n#            best_n = n\n#print(\"Best performing number of max_depth is\",best_n,\"scoring\",round(best_score * 100 , 2))","7e497b1c":"nb_model = GaussianNB(var_smoothing = 0.00001)\nmodels.append(nb_model)\nnb_model.fit(X_train, y_train)\nnb_predprob = nb_model.predict_proba(X_test)\npred_probabilities.append(nb_predprob)\nnb_pred = nb_model.predict(X_test)\npredictions.append(nb_pred)\nnb_predprob.shape\n","7cfe6f05":"for name,pred in zip(model_names,predictions):\n    print(name,\"Accuracy:\",round(accuracy_score(y_test,pred) * 100 , 2),\"%\")","c1edf7f3":"for name,pred in zip(model_names,pred_probabilities):\n    print(name,\"AUROC:\",round(roc_auc_score(y_test,pred[:, 1]) * 100 , 2),\"%\")","107ddc58":"fprs = []\ntprs = []\nfor i, pred in enumerate(pred_probabilities):\n    fpr, tpr, thresholds = roc_curve(y_test, pred[:, 1],drop_intermediate = False)\n    fprs.append(fpr)\n    tprs.append(tpr)\n    plt.figure(figsize=(5,5))\n    plt.plot(fpr,tpr, color=colors[i], lw=2)\n    plt.plot([0,1],[0,1],linestyle='--', color='black', lw=.8)\n    plt.title(model_names[i] + \" ROC - AUC \" + str(round(roc_auc_score(y_true=y_test,y_score=pred[:,1]) * 100,2)) + \" % \" + \" 0-PCA\",fontsize=10)\n    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=8)\n    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=8)\n    plt.xticks([x \/ 10 for x in range(0,11)], fontsize=10)\n    plt.yticks([x \/ 10 for x in range(0,11)], fontsize=10)\n    plt.show()","1a891cc7":"from sklearn.decomposition import PCA\nbest_scores = []\nbest_ns = []\n\nfor i, model in enumerate(models):\n    best_score = -np.inf\n    best_n = np.inf\n    for n in range(1,df.columns.shape[0]):\n        pca = PCA(n_components=n,random_state = 0)\n        temp_X_train = pca.fit_transform(X_train)\n        temp_X_test = pca.transform(X_test)\n        model.fit(temp_X_train,y_train)\n        temp_pred = model.predict_proba(temp_X_test)\n        temp_score = roc_auc_score(y_test,temp_pred[:, 1])\n        if temp_score > best_score:\n            best_score = temp_score\n            best_n = n\n    print(\"Best performing number of components for\",model_names[i],\"is\",best_n,\"scoring\",round(best_score * 100 , 2))\n    best_scores.append(best_score)\n    best_ns.append(best_n)","27bc66a7":"from sklearn.decomposition import PCA\n\noverall_best_score = -np.inf\noverall_best_n = np.inf\nfor n in range(1,df.columns.shape[0]):\n    #print(str(round(n \/ df.columns.shape[0] * 100,2)) + \"%\",end = \" \")\n    pca = PCA(n_components=n,random_state = 0)\n    temp_X_train = pca.fit_transform(X_train)\n    temp_X_test = pca.transform(X_test)\n    temp_score = 0\n    for i, model in enumerate(models):\n        model.fit(temp_X_train,y_train)\n        temp_pred = model.predict_proba(temp_X_test)\n        model_score = roc_auc_score(y_test,temp_pred[:, 1])\n        temp_score += model_score\n    temp_score \/= len(model_names)\n    if temp_score > overall_best_score:\n        overall_best_score = temp_score\n        overall_best_n = n\nprint(\"Best performing number of components for all models is\",best_n,\"scoring\",round(best_score * 100 , 2))","05774ef9":"pca_pred_probs = []\nfor i, model in enumerate(models):\n    pca = PCA(n_components=best_ns[i],random_state = 0)\n    temp_X_train = pca.fit_transform(X_train)\n    temp_X_test = pca.transform(X_test)\n    model.fit(temp_X_train,y_train)\n    temp_pred = model.predict_proba(temp_X_test)\n    pca_pred_probs.append(temp_pred)","cd1cfbc3":"fprs = []\ntprs = []\nfor i, pred in enumerate(pca_pred_probs):\n    fpr, tpr, thresholds = roc_curve(y_test, pred[:, 1],drop_intermediate = False)\n    fprs.append(fpr)\n    tprs.append(tpr)\n    plt.figure(figsize=(5,5))\n    plt.plot(fpr,tpr, color=colors[i], lw=2)\n    plt.plot([0,1],[0,1],linestyle='--', color='black', lw=.8)\n    plt.title(model_names[i] + \" ROC - AUC \" + str(round(roc_auc_score(y_true=y_test,y_score=pred[:,1]) * 100,2)) + \"%\" + str(best_ns[i]) + \"-PCA\",fontsize=10)\n    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=8)\n    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=8)\n    plt.xticks([x \/ 10 for x in range(0,11)], fontsize=10)\n    plt.yticks([x \/ 10 for x in range(0,11)], fontsize=10)\n    plt.show()","db48a850":"overall_pca_pred_probs = []\nfor i, model in enumerate(models):\n    pca = PCA(n_components=overall_best_n,random_state = 0)\n    temp_X_train = pca.fit_transform(X_train)\n    temp_X_test = pca.transform(X_test)\n    model.fit(temp_X_train,y_train)\n    temp_pred = model.predict_proba(temp_X_test)\n    overall_pca_pred_probs.append(temp_pred)","f6e94f23":"fprs = []\ntprs = []\nfor i, pred in enumerate(overall_pca_pred_probs):\n    fpr, tpr, thresholds = roc_curve(y_test, pred[:, 1],drop_intermediate = False)\n    fprs.append(fpr)\n    tprs.append(tpr)\n    plt.figure(figsize=(5,5))\n    plt.plot(fpr,tpr, color=colors[i], lw=2)\n    plt.plot([0,1],[0,1],linestyle='--', color='black', lw=.8)\n    plt.title(model_names[i] + \" ROC - AUC \" + str(round(roc_auc_score(y_true=y_test,y_score=pred[:,1]) * 100,2)) + \"%\" + str(overall_best_n) + \"-PCA\",fontsize=10)\n    plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=8)\n    plt.ylabel('True Positive Rate (Sensitivity)', fontsize=8)\n    plt.xticks([x \/ 10 for x in range(0,11)], fontsize=10)\n    plt.yticks([x \/ 10 for x in range(0,11)], fontsize=10)\n    plt.show()","8800a9a1":"best_model = RandomForestClassifier(max_depth=9, random_state=0,n_estimators = 100)\n\npca = PCA(n_components=132,random_state = 0)\nbest_X_train = pca.fit_transform(X_train)\nbest_X_test = pca.transform(X_test)\nbest_model.fit(best_X_train, y_train)\nbest_predprob = best_model.predict_proba(best_X_test)\nbest_pred = best_model.predict(best_X_test)","e8dbd144":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(best_model, best_X_test, y_test)\nplt.grid(which = \"major\")\nplt.show()","300472d2":"A train test split have been implemented with 0.2 test_size value.","a03cdfc2":"KNeighborsClassifier model have been trained","d38c8f30":"Data is a .CSV file which contains all the possible instructions that can be obtained from ECG readings. However some of the columns have unreliable data (nulls and just vectors of singular values to be exact). So the owner of the dataset seems to drop these columns not applying any interpolation implementation. So, this notebook doesn't follow any interpolation measure either.","d604ced1":"\n\nDecisionTreeClassifier model have been trained","50139bc9":"For each model, a ROC curve have been created using Scikit-Learn's own roc_curve function and stylized for demstrative purposes. Each plot has AUROC score in the title as well as an indicator of how many components in PCA have been chosen (in this case none chosen).","d21571f6":"Data is prepared as it has been provided by B\u00fclent Esen's notebook https:\/\/www.kaggle.com\/bulentesen\/cardiac-arrythmia-naive-bayes-v2 who is the publisher of the dataset. No further data preparation besides the ones in cell 8 were not pursued since the data preparataion should be at it's best under the owner's own usage.","fd68d2d5":"There are 16 classes as follows\n<ol>\n<li>Normal                     <\/li>\n<li>Ischemic changes (Coronary Artery Disease<\/li>\n<li>Old Anterior Myocardial Infarction     <\/li>\n<li>Old Inferior Myocardial Infarction     <\/li>\n<li>Sinus tachycardy                 <\/li>\n<li>Sinus bradycardy                 <\/li>\n<li>Ventricular Premature Contraction (PVC)<\/li>\n<li>Supraventricular Premature Contraction <\/li>\n<li>Left bundle branch block                 <\/li>\n<li>Right bundle branch block             <\/li>\n<li>1. degree AtrioVentricular block            <\/li>\n<li>2. degree AV block             <\/li>\n<li>3. degree AV block             <\/li>\n<li>Left ventricule hypertrophy          <\/li>\n<li>Atrial Fibrillation or Flutter         <\/li>\n<li>Others                     <\/li>\n<\/ol>\n\nHowever only first one is a normal kind of arrythmia where the other classes can be superclassed as risky arrythmia cases. In this notebook, all cases are classified as either normal or risky. Risky being the positive (True) case and normal being the negative (False) case, diagnosis column is reworked as follows.","49d68761":"Further investigating the data have revealed there are some odd entries for the entries that are made presumably for babies which is handled manually. https:\/\/www.kaggle.com\/mtavares51\/binary-classification-on-arrhythmia-dataset","6a870928":"For every column, a MinMaxScaler have been implemented","c591a3cb":"All model names are inputted for demonstration use afterwards.","b27bbbd4":"For each model, a ROC curve have been created using Scikit-Learn's own roc_curve function and stylized for demstrative purposes. Each plot has AUROC score in the title as well as an indicator of how many components in PCA have been chosen.","c5ed1db0":"GaussianNB model have been trained","dc5e3565":"LogisticRegression have been trained.","4354fd7e":"For each model, a ROC curve have been created using Scikit-Learn's own roc_curve function and stylized for demstrative purposes. Each plot has AUROC score in the title as well as an indicator of how many components in PCA have been chosen.","aa26ce89":"The cell below checks for a range of max_depth values and picking the best n_neighbors with the highest AUROC result. Commented out since it's implemented above manually however it's left here for demonstration purposes","a8970321":"Cell below computes predictions with probabilities using varying number of PCA components, computed above and stores them in an array.","957a32bf":"The cell below checks for a range of max_depth values and picking the best n_neighbors with the highest AUROC result. Commented out since it's implemented above manually however it's left here for demonstration purposes","df6b6739":"For each model AUROC scores are as below.","64e34017":"For each model, the cell above finds the best performing number of components in PCA. In this way, each model has their own best performing dataset and using features as much as they need to further optimize AUROC score.","d3093ccb":"Just for demonstration accuracy scores have been demonstrated however ignored in fine-tuning to create a mor clinically applicable model favoring a varibale threshold rather than a 50 % fixed threshold.","e7ac15ca":"The cell below computes a common number of components which all models perform the best in average. Although this approach limits the performance of each model on their own, it demonstrates how much of the features is actually put into good use especially since the number of futures are less than half of the original number of features after this implementation.","e8441c95":"Cell below computes predictions with probabilities using the best performing overall number of PCA components, computed above and stores them in an array.","d226039a":"## Best Model = Random Forest Classifier\n## Best PCA N-Component = 132\n## Best AUROC Score = 84.06 %\nBest model is being used once again to compute the confusion matrix.","c25d5cdd":"X is formed by importing every data at first. All the dimensionality reduction will be applied afterwards. Y is the label column.","8dd1f94a":"RandomForestClassifier model have been trained","b8388f14":"The cell below checks for a range of n_neighbors values and picking the best n_neighbors with the highest AUROC result. Commented out since it's implemented above manually however it's left here for demonstration purposes","4ecc50c5":"For a 50% threshold approach confusion matrix is calculated as below however since ROC curves are provided, one can implement their own threshold favoring either high sensitivity or specificity."}}