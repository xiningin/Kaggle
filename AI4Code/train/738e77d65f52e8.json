{"cell_type":{"7ea04d57":"code","838a0202":"code","dd18f583":"code","23742f32":"code","732b3e43":"code","7c381e4b":"code","0801216c":"code","8dd48592":"code","32abb6f1":"code","0f867fe4":"code","642298e2":"code","e58f1d60":"code","0c6734ae":"code","499f0dc9":"code","488fa97f":"code","1a9a7394":"code","ed5adc9f":"code","6424124f":"code","3f6291c8":"code","c6159beb":"code","9589ca91":"code","931cce8a":"code","caeb7808":"code","14fc9e32":"code","3bbd35fe":"code","5c9af5b2":"code","e66c6914":"code","b95d169e":"code","8cab2f1b":"code","98a1c099":"code","e7184efd":"code","f20ae920":"code","fa811408":"code","7f687796":"code","85351573":"code","fa0ac167":"code","543d5baf":"code","e4d92ba0":"code","67773cb2":"code","5f4bcd42":"code","8e1b9f41":"code","c8566d29":"code","9c5da8d9":"code","fc60a395":"code","4225a6e5":"code","2038e748":"code","a6aa80cf":"code","6876d1e6":"code","d17f4cf2":"code","4370769e":"code","4acaab0b":"code","d35dcaad":"code","e5c4e972":"code","7ab980d4":"code","61b6faaa":"code","b7b50c63":"code","f3fab1da":"code","6fd39270":"code","268fc636":"markdown","5465a041":"markdown","416bcc86":"markdown","3af92154":"markdown","b1628398":"markdown","a98bf248":"markdown","4e847ab7":"markdown","3bda9381":"markdown","6c3fc1d9":"markdown","64a1ef29":"markdown","b4fb56a9":"markdown","17c59a45":"markdown","a3f3d079":"markdown","70af4090":"markdown","77aa69f7":"markdown","235f1a0e":"markdown","9124ae63":"markdown","cbd310c7":"markdown","de9f0324":"markdown","be4c3770":"markdown"},"source":{"7ea04d57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","838a0202":"# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom IPython.display import display\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","dd18f583":"# define datatype for the columns in the dataset\n# loading data with predefined datatype helps improve RAM utilization\ndtypes = {\n    'Src IP': 'category',\n    'Src Port': 'uint16',\n    'Dst IP': 'category',\n    'Dst Port': 'uint16',\n    'Protocol': 'category',\n    'Flow Duration': 'uint32',\n    'Tot Fwd Pkts': 'uint32',\n    'Tot Bwd Pkts': 'uint32',\n    'TotLen Fwd Pkts': 'float32',\n    'TotLen Bwd Pkts': 'float32',\n    'Fwd Pkt Len Max': 'float32',\n    'Fwd Pkt Len Min': 'float32',\n    'Fwd Pkt Len Mean': 'float32',\n    'Fwd Pkt Len Std': 'float32',\n    'Bwd Pkt Len Max': 'float32',\n    'Bwd Pkt Len Min': 'float32',\n    'Bwd Pkt Len Mean': 'float32',\n    'Bwd Pkt Len Std': 'float32',\n    'Flow Byts\/s': 'float32',\n    'Flow Pkts\/s': 'float32',\n    'Flow IAT Mean': 'float32',\n    'Flow IAT Std': 'float32',\n    'Flow IAT Max': 'float32',\n    'Flow IAT Min': 'float32',\n    'Fwd IAT Tot': 'float32',\n    'Fwd IAT Mean': 'float32',\n    'Fwd IAT Std': 'float32',\n    'Fwd IAT Max': 'float32',\n    'Fwd IAT Min': 'float32',\n    'Bwd IAT Tot': 'float32',\n    'Bwd IAT Mean': 'float32',\n    'Bwd IAT Std': 'float32',\n    'Bwd IAT Max': 'float32',\n    'Bwd IAT Min': 'float32',\n    'Fwd PSH Flags': 'category',\n    'Bwd PSH Flags': 'category',\n    'Fwd URG Flags': 'category',\n    'Bwd URG Flags': 'category',\n    'Fwd Header Len': 'uint32',\n    'Bwd Header Len': 'uint32',\n    'Fwd Pkts\/s': 'float32',\n    'Bwd Pkts\/s': 'float32',\n    'Pkt Len Min': 'float32',\n    'Pkt Len Max': 'float32',\n    'Pkt Len Mean': 'float32',\n    'Pkt Len Std': 'float32',\n    'Pkt Len Var': 'float32',\n    'FIN Flag Cnt': 'category',\n    'SYN Flag Cnt': 'category',\n    'RST Flag Cnt': 'category',\n    'PSH Flag Cnt': 'category',\n    'ACK Flag Cnt': 'category',\n    'URG Flag Cnt': 'category',\n    'CWE Flag Count': 'category',\n    'ECE Flag Cnt': 'category',\n    'Down\/Up Ratio': 'float32',\n    'Pkt Size Avg': 'float32',\n    'Fwd Seg Size Avg': 'float32',\n    'Bwd Seg Size Avg': 'float32',\n    'Fwd Byts\/b Avg': 'uint32',\n    'Fwd Pkts\/b Avg': 'uint32',\n    'Fwd Blk Rate Avg': 'uint32',\n    'Bwd Byts\/b Avg': 'uint32',\n    'Bwd Pkts\/b Avg': 'uint32',\n    'Bwd Blk Rate Avg': 'uint32',\n    'Subflow Fwd Pkts': 'uint32',\n    'Subflow Fwd Byts': 'uint32',\n    'Subflow Bwd Pkts': 'uint32',\n    'Subflow Bwd Byts': 'uint32',\n    'Init Fwd Win Byts': 'uint32',\n    'Init Bwd Win Byts': 'uint32',\n    'Fwd Act Data Pkts': 'uint32',\n    'Fwd Seg Size Min': 'uint32',\n    'Active Mean': 'float32',\n    'Active Std': 'float32',\n    'Active Max': 'float32',\n    'Active Min': 'float32',\n    'Idle Mean': 'float32',\n    'Idle Std': 'float32',\n    'Idle Max': 'float32',\n    'Idle Min': 'float32',\n    'Label': 'category'\n}","23742f32":"# load the data\ndf = pd.read_csv(\n    '\/kaggle\/input\/ddos-datasets\/ddos_balanced\/final_dataset.csv',\n    dtype=dtypes,\n    parse_dates=['Timestamp'],\n    usecols=[*dtypes.keys(), 'Timestamp'],\n    engine='c',\n    low_memory=True\n)\ndel dtypes\ngc.collect()","732b3e43":"df.shape","7c381e4b":"df.describe(include='all')","0801216c":"def print_mem_usage(df):\n    mb = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(mb))","8dd48592":"print_mem_usage(df)","32abb6f1":"colsToDrop = np.array([])\ndropnaCols = np.array([])","0f867fe4":"missing = df.isna().sum()\nmissing = pd.DataFrame({'count': missing, '% of total': missing\/len(df)*100}, index=df.columns)\nmissing.T","642298e2":"colsToDrop = np.union1d(colsToDrop, missing[missing['% of total'] >= 50].index.values)\ndropnaCols = missing[(missing['% of total'] > 0) & (missing['% of total'] <= 5)].index.values","e58f1d60":"colsToDrop = np.union1d(colsToDrop, ['Fwd Byts\/b Avg', 'Fwd Pkts\/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts\/b Avg', 'Bwd Pkts\/b Avg', 'Bwd Blk Rate Avg'])\ngc.collect()","0c6734ae":"# counting unique values and checking for skewness in the data\nrowbuilder = lambda col: {'col': col, 'unique_values': df[col].nunique(), 'most_frequent_value': df[col].value_counts().index[0],'frequency': df[col].value_counts(normalize=True).values[0]}\nfrequency = [rowbuilder(col) for col in df.select_dtypes(include=['category']).columns]\nstats = pd.DataFrame(frequency).sort_values(by='frequency', ascending=False)\nstats","499f0dc9":"skewed = stats[stats['frequency'] >= 0.95]\ncolsToDrop = np.union1d(colsToDrop, skewed['col'].values)\ncolsToDrop\ndel skewed\ndel rowbuilder\ndel frequency\ngc.collect()","488fa97f":"df['Flow Byts\/s'].replace(np.inf, np.nan, inplace=True)\ndf['Flow Pkts\/s'].replace(np.inf, np.nan, inplace=True)\ndropnaCols = np.union1d(dropnaCols, ['Flow Byts\/s', 'Flow Pkts\/s'])","1a9a7394":"colsToDrop","ed5adc9f":"dropnaCols","6424124f":"# perform actual drop\ndf.drop(columns=colsToDrop, inplace=True)\ndf.dropna(subset=dropnaCols, inplace=True)\ngc.collect()","3f6291c8":"negValCols = ['Flow Pkts\/s', 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Max', 'Bwd IAT Min']\nfor col in negValCols:\n    df = df[df[col] >= 0]","c6159beb":"print_mem_usage(df)","9589ca91":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df[\"Label\"]):\n    traindf = df.iloc[train_index]\ntraindf.to_csv('train.csv', index=False)\ngc.collect();","931cce8a":"# traindf = pd.read_csv(\n#     'train.csv',\n#     dtype=dtypes,\n#     parse_dates=['Timestamp'],\n#     engine='c',\n#     low_memory=True\n# )","caeb7808":"# plotting the target variable\nlabelCount = traindf['Label'].value_counts(normalize=True)*100\nax = sns.barplot(x=labelCount.index, y=labelCount.values)\nax1 = ax.twinx()\nax.set_ylabel('Frequency [%]')\nax1.set_ylabel(\"Count (in millions)\")\nax1.set_ylim(0, len(traindf)\/10**6)\nax.set_ylim(0, 100)\nplt.title('Target Variable')","14fc9e32":"cnt = pd.crosstab(traindf['Protocol'], traindf['Label'])\ncnt = cnt.stack().reset_index().rename(columns={0: 'Count'})\nsns.barplot(x=cnt['Protocol'], y=cnt['Count'], hue=cnt['Label'])","3bbd35fe":"def getNetworkClass(col):\n    networkClasses = col.str.split('.',n=1, expand=True)[0]\n    networkClasses = networkClasses.astype('uint8')\n    networkClasses = pd.cut(\n        networkClasses,\n        bins=[0, 127, 191, 223, 239, np.inf],\n        labels=['A', 'B', 'C', 'D', 'E'],\n        include_lowest=True\n    )\n    return networkClasses","5c9af5b2":"srcNetworkClass = getNetworkClass(traindf['Src IP'])\ndstNetworkClass = getNetworkClass(traindf['Dst IP'])","e66c6914":"cnt = pd.crosstab(srcNetworkClass, traindf['Label'], rownames=['Class'])\ncnt = cnt.stack().reset_index().rename(columns={0: 'Count'})\nsns.barplot(x=cnt['Class'], y=cnt['Count'], hue=cnt['Label'])","b95d169e":"cnt = pd.crosstab(dstNetworkClass, traindf['Label'], rownames=['Class'])\ncnt = cnt.stack().reset_index().rename(columns={0: 'Count'})\nsns.barplot(x=cnt['Class'], y=cnt['Count'], hue=cnt['Label'])","8cab2f1b":"del srcNetworkClass\ndel dstNetworkClass\ngc.collect()","98a1c099":"num_cols = traindf.select_dtypes(exclude=['category', 'datetime64[ns]']).columns\nfwd_cols = [col for col in num_cols if 'Fwd' in col]\nbwd_cols = [col for col in num_cols if 'Bwd' in col]","e7184efd":"corr = traindf[fwd_cols].corr()","f20ae920":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.subplots(figsize=(10,10))\nsns.heatmap(corr, mask=mask)","fa811408":"def getCorrelatedFeatures(corr):\n    correlatedFeatures = set()\n    for i in range(len(corr.columns)):\n        for j in range(i):\n            if abs(corr.iloc[i, j]) > 0.8:\n                correlatedFeatures.add(corr.columns[i])\n    return correlatedFeatures","7f687796":"correlatedFeatures = set()\ncorrelatedFeatures = correlatedFeatures | getCorrelatedFeatures(corr)","85351573":"corr = traindf[bwd_cols].corr()","fa0ac167":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.subplots(figsize=(10,10))\nsns.heatmap(corr, mask=mask)","543d5baf":"correlatedFeatures = correlatedFeatures | getCorrelatedFeatures(corr)\ncorrelatedFeatures","e4d92ba0":"traindf.drop(columns=correlatedFeatures, inplace=True)","67773cb2":"gc.collect()","5f4bcd42":"num_cols = set(traindf.select_dtypes(exclude=['category', 'datetime64[ns]']).columns)\ncols = [col for col in num_cols if 'Fwd' in col or 'Bwd' in col]","8e1b9f41":"corr = traindf[cols].corr()","c8566d29":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.subplots(figsize=(10,10))\nsns.heatmap(corr, mask=mask)","9c5da8d9":"correlatedFeatures = correlatedFeatures | getCorrelatedFeatures(corr)\ntraindf.drop(columns=getCorrelatedFeatures(corr), inplace=True)","fc60a395":"traindf.shape","4225a6e5":"traindf.describe()","2038e748":"num_cols = traindf.select_dtypes(exclude=['category', 'datetime64[ns]']).columns\nskew = traindf[num_cols].skew().sort_values(ascending=False)","a6aa80cf":"skew","6876d1e6":"del traindf\ngc.collect();","d17f4cf2":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df[\"Label\"]):\n    traindf = df.iloc[train_index]\ngc.collect();","4370769e":"def logTransformation(X):\n    for col in X.columns:\n        X.loc[X[col] == 0] = 1\n    return np.log10(X)","4acaab0b":"def dropCorrelatedFeatures(X):\n    return X.drop(columns=correlatedFeatures)","d35dcaad":"num_pipeline = Pipeline([\n    ('dropCorrelatedFeatures', FunctionTransformer(dropCorrelatedFeatures)),\n    ('logTransformation', FunctionTransformer(logTransformation))\n])","e5c4e972":"num_cols = list(traindf.columns[(traindf.dtypes != 'category') &  (traindf.dtypes != 'datetime64[ns]')])\nX = num_pipeline.transform(traindf[num_cols])\nX.head()","7ab980d4":"def addNetworkClasses(X):\n    X['SrcIPClass'] = getNetworkClass(X['Src IP'])\n    X['DstIPClass'] = getNetworkClass(X['Dst IP'])\n    return X.drop(columns=['Src IP', 'Dst IP'])","61b6faaa":"cat_pipeline = Pipeline([\n    ('AddNewCols', FunctionTransformer(addNetworkClasses)),\n    ('OrdinalEncoding', OrdinalEncoder())\n])","b7b50c63":"num_cols = list(traindf.columns[(traindf.dtypes != 'category') &  (traindf.dtypes != 'datetime64[ns]')])\ncat_cols = list(traindf.columns[traindf.dtypes == 'category'])","f3fab1da":"full_pipeline = ColumnTransformer([\n    ('numColTransformer', num_pipeline, num_cols),\n    ('catColTransformer', cat_pipeline, cat_cols)\n])","6fd39270":"X = full_pipeline.fit_transform(traindf)","268fc636":"Now lets check correlation between forward & backward direction predictors","5465a041":"We observe that only Flow Byts\/s has about 0.2% missing values. We therefore drop the corresponding rows.","416bcc86":"From the data statistics computed earlier, we can see that some columns have only one value. Such columns will not provide any significant information for our classification task. We will therefore drop these columns.","3af92154":"We also observe that some columns have infinity values. ML algorithms cannot work on infinity values. There are two ways to handle this. First, impute the infinity values to contain very large numbers less than infinity. Second, drop the rows that contain infinity values. In our case, only ~ 2% of the data contains infinity values. Thus we will adopt the second strategy.","b1628398":"### Handling missing values\nWe find the % of missing values for each column. If a column has more than 50% missing values, then we drop the entire column. If the column has less than 5% missing values, then we drop those rows where the column value is missing.","a98bf248":"We also observe from the data statistics that some columns have negative values. Based on our understanding of the variables, negative values indicate incorrect\/faulty data. We will therefore filter out all the negative values from our dataset.","4e847ab7":"We can see that some categorical variables have very high dominance of a single category. For classification task, such categorical variables will be of little use. We will therefore drop those columns where the dominance of the most frequent category is more than 95%","3bda9381":"We can see that ddos attacks Source IPs belong to primarily Class A & Class B networks","6c3fc1d9":"## Data Visualization","64a1ef29":"A skew value of greater than 1 determines very high skew in the data. We will need to perform log transformation to lower the skew","b4fb56a9":"Our target variable is very balanced. Dataset contains almost equal instances of ddos and benign network activity.","17c59a45":"### Handling incorrect\/corrupt data","a3f3d079":"Let us now see some statistics for the categorical variables","70af4090":"### Full Pipeline","77aa69f7":"So our intuition was correct. There is high correlation in data. Lets drop these columns","235f1a0e":"## Train-test split","9124ae63":"## Data Loading","cbd310c7":"## Data Pipeline\n### Pipeline for numerical columns","de9f0324":"## Data preparation","be4c3770":"### Pipeline for categorical data"}}