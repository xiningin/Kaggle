{"cell_type":{"52a94ad5":"code","c8e9f9e8":"code","cf0d1b90":"code","cc474101":"code","c33ec553":"code","c94d594b":"code","c76a74e0":"code","029b0d40":"code","6789b623":"code","cfae0b73":"code","0e4eeb40":"code","d085e416":"code","1a48df9f":"code","d12c2c2e":"code","6f19910d":"code","3aa3c761":"code","93947c01":"code","efc63b2b":"code","780a9efb":"code","220ffda4":"code","81432d17":"code","882cdbc6":"code","dd16bb57":"code","0d698648":"code","e7d524e5":"code","6cd1030b":"code","b52c1c09":"code","7bae9564":"code","38ae66af":"code","a77416e3":"code","3073e0b2":"code","21aefe83":"code","3afecf9c":"code","d5c3df20":"code","aad8fa28":"code","9deb8327":"code","e95312f4":"code","649a325a":"code","27dd753e":"code","1eb3bd96":"code","c2d06e75":"code","e0181835":"code","9b5eb058":"code","03e0acb5":"code","d3bcbb94":"code","a93a910a":"code","d4472065":"code","4fb27fc9":"code","4b57b09a":"code","c98476fa":"code","f5615316":"code","395ba433":"code","7a75e276":"code","608838af":"code","74daebfe":"code","a4b5687e":"code","0a965ebe":"code","6edd7d28":"code","9218815e":"code","905d94d7":"code","22ffd839":"code","9a0e30c7":"code","d714ab51":"code","d3af5f90":"code","a7cc96d6":"code","da6785c1":"code","ce1e0bad":"code","ea7838ff":"code","1c0e5d65":"code","21a2f355":"code","fd82e53c":"code","16d9691b":"code","3ae2b5df":"code","30cf1cd2":"code","78a92be2":"code","68212b43":"code","07507669":"code","8d23cfdb":"code","57a38af9":"code","187e7623":"code","2fc147f2":"code","f5090c16":"code","f3d350d0":"code","17ae414b":"code","fdc7f387":"code","af7801ef":"code","dc501573":"code","9c53fbcd":"code","54cb80d1":"code","117d9809":"code","45e7f3ef":"code","d8f1b686":"code","40cc0aeb":"code","aabaf789":"code","44de1914":"code","7182dd85":"code","9b9d33e6":"code","7e31e0b6":"code","d2833b23":"code","befa96ea":"code","868da8d1":"code","1c11e8f3":"code","f27e1f99":"code","9593d0c9":"code","cfca792f":"code","1bb7fad4":"code","7e5030ad":"code","321fbaf7":"code","a0a5d251":"code","9c766fc9":"code","136839ad":"code","a16fc9e8":"code","c5de031f":"code","d5a87d04":"code","235a491c":"code","ce716ee5":"code","013aa013":"code","713ab699":"code","7a7a9498":"code","b79bd553":"code","b17e9816":"code","b19145da":"code","4b0c7bf1":"code","988bf4be":"code","eb670b9d":"code","fd69c09d":"code","f9d7e9b3":"code","d9ea817c":"code","afc2d487":"code","d827a4c8":"code","e9ea7c6d":"code","ea8f549e":"code","9148abdd":"code","59a0456c":"code","d9b94a9d":"code","9654055f":"code","8409e754":"code","b7cfc8bc":"code","67a4a03a":"code","55745c97":"code","333b1696":"code","47ba1b52":"code","a2ce09ce":"code","5ffd0da9":"code","dbf09dd6":"code","e537c7c3":"code","ecedfc2f":"code","ef13d6f4":"code","3bbb04de":"code","a4cad693":"code","ade3ced3":"code","03c8ca54":"code","58e59b6b":"code","8247a804":"code","293406b5":"code","dae3e8dd":"code","fd57bbf3":"code","b48bca5c":"code","1b49bf79":"code","3c095abe":"code","3e89404e":"code","c5cd1b7c":"code","73633aa5":"code","e902c1cc":"code","d57fca16":"code","c49cae50":"code","778721b1":"code","19d8f4e6":"code","1355b72f":"code","0d611d13":"markdown","26eb0ff2":"markdown","aa507cbb":"markdown","03efd16c":"markdown","4bb697e2":"markdown","36121401":"markdown","320c1755":"markdown","c3fb963a":"markdown","c7c322ba":"markdown","adaf0621":"markdown","4ba6a986":"markdown","f3e28651":"markdown","92073e2c":"markdown","8018d222":"markdown","2f733ad1":"markdown","9737033f":"markdown","dd44b90e":"markdown","e95c4a6f":"markdown","64394c54":"markdown","2c91ff25":"markdown","f57f76db":"markdown","167a3d99":"markdown","d65b1d3d":"markdown","9be25e75":"markdown","666de1c3":"markdown","9035bdb0":"markdown","93f4a04f":"markdown","24945a7a":"markdown","d1028a4f":"markdown","b94d6430":"markdown","a28295fa":"markdown","772f06cb":"markdown","ba321319":"markdown","ab2e0b78":"markdown","ecfa9626":"markdown","04362e75":"markdown","546d012f":"markdown","132fccf5":"markdown","bd44960d":"markdown","9b1d46c2":"markdown","af7056fc":"markdown"},"source":{"52a94ad5":"#installing the pycaret module and its sub-modules (only run in GPU mode)\n!pip install pycaret[full]","c8e9f9e8":"## The magic four\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n# Train Test Split \nfrom sklearn.model_selection import train_test_split\n\n#Scaler \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n## Metrics \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import metrics\nfrom sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score)\nfrom sklearn.metrics import log_loss\n\n## StatsModels & SkLearn\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n\n#garbage collection (clear up some RAM)\nimport gc\n\n#imputer\nfrom sklearn.impute import SimpleImputer\n\n#Random Forest\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\n#XGBoost\nfrom xgboost import XGBClassifier\n\n#pycaret (only run in GPU mode)\nfrom pycaret.classification import *\n\n#cuML (only run in GPU mode)\nimport cudf\nfrom cuml.ensemble import RandomForestClassifier as cuRF\nfrom cuml.model_selection import GridSearchCV as cuGridSearchCV\nfrom cuml.model_selection import RandomizedSearchCV as cuRandomizedSearchCV\n\n%matplotlib inline","cf0d1b90":"def aprF1auc(y_real, y_pred):\n    '''Function that takes in two columns of a DataFrame and returns metrics scores\n    \n    Input:\n    2 Series or columns of DataFrame\n    \n    Output:\n    5 floats corresponding to each of the metrics calculated\n    '''\n    accuracy = accuracy_score(y_real, y_pred)\n    precision = precision_score(y_real, y_pred)\n    recall = recall_score(y_real, y_pred)\n    f1 = f1_score(y_real, y_pred)\n    auc = roc_auc_score(y_real, y_pred)\n    \n    print(f\"Accuracy:{accuracy}\")\n    print(f\"Precision:{precision}\")\n    print(f\"Recall:{recall}\")\n    print(f\"F1:{f1}\")\n    print(f\"AUC:{auc}\")\n    #return accuracy, precision, recall, f1, auc","cc474101":"## confusion matrix\n\ndef produce_confusion(positive_label, negative_label, cut_off, df, y_pred_name, y_real_name):\n    \n    #Set pred to 0 or 1 depending on whether it's higher than the cut_off point.\n    \n    if cut_off != 'binary':      \n        df['pred_binary'] = np.where(df[y_pred_name] > cut_off , 1, 0)\n    else: \n        df['pred_binary'] = df[y_pred_name]\n    \n    #Build the CM\n    cm = confusion_matrix(df[y_real_name], df['pred_binary'])  \n    \n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax=ax, fmt='g'); \n\n    # labels, title, ticks\n    ax.set_xlabel('Predicted labels');ax.set_ylabel('Real labels'); \n    ax.set_title('Confusion Matrix'); \n    ax.xaxis.set_ticklabels([negative_label, positive_label])\n    ax.yaxis.set_ticklabels([negative_label, positive_label]);\n    \n    return aprF1auc(df[y_real_name], df['pred_binary'])\n    \n    #print('Accuracy = ', accuracy_score(df[y_real_name], df['pred_binary']))\n    #print('Precision = ', precision_score(df[y_real_name], df['pred_binary']))\n    #print('Recall = ', recall_score(df[y_real_name], df['pred_binary']))\n    #print('F1 score = ', f1_score(df[y_real_name], df['pred_binary']))\n    #print('ROC_AUC score = ', roc_auc_score(df[y_real_name], df['pred_binary']))","c33ec553":"#this is an aesthetic choice and just removes the many warnings that some functions and comands produce\n#it helps significantly declutter the workbook\nimport warnings\nwarnings.filterwarnings('ignore')","c94d594b":"#importing data and setting index column\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', index_col='id')","c76a74e0":"train.head()","029b0d40":"test.head()","6789b623":"train.shape","cfae0b73":"train.dtypes","0e4eeb40":"train.info()","d085e416":"train.describe()","1a48df9f":"test.shape","d12c2c2e":"train.isnull().sum()","6f19910d":"test.isnull().sum()","3aa3c761":"'''\n#correlation heatmap\nplt.figure(figsize = (30,30))\ncorrplot = sns.heatmap(train.corr(), square = True)\n\ncorrplot.figure.savefig('corrplot.png')\n'''","93947c01":"#defining the train and test datasets\nX_train = train.copy()\ny_train = X_train.pop('claim')\nX_test = test.copy()\n\n#saving the index of the test dataset for later use\nidx = X_test.index","efc63b2b":"#saving a copy of column headings\ntrain_cols = X_train.columns\ntest_cols = X_test.columns","780a9efb":"#fills null value in each column with column mean\nSI = SimpleImputer(strategy = 'mean')\nX_train_fill = SI.fit_transform(X_train)\nX_train_fill = pd.DataFrame(X_train_fill, columns = train_cols)","220ffda4":"#fill null values in test and set index from orginal dataset\nX_test_fill = SI.fit_transform(X_test)\nX_test_fill = pd.DataFrame(X_test_fill, columns = test_cols)\nX_test_fill.set_index(idx, inplace = True)","81432d17":"gc.collect()","882cdbc6":"#adding additional features to both train and test\nX_train_fill['n_missing'] = X_train.isnull().sum(axis=1).astype(int)\nX_train_fill['std'] = X_train_fill[train_cols].std(axis=1)\nX_train_fill['avg'] = X_train_fill[train_cols].mean(axis=1)\nX_train_fill['max'] = X_train_fill[train_cols].max(axis=1)\nX_train_fill['min'] = X_train_fill[train_cols].min(axis=1)\n\nX_test_fill['n_missing'] = X_test.isnull().sum(axis=1).astype(int) \nX_test_fill['std'] = X_test_fill[test_cols].std(axis=1)\nX_test_fill['avg'] = X_test_fill[test_cols].mean(axis=1)\nX_test_fill['max'] = X_test_fill[test_cols].max(axis=1)\nX_test_fill['min'] = X_test_fill[test_cols].min(axis=1)","dd16bb57":"#updated list of column headings\ntrain_cols = X_train_fill.columns\ntest_cols = X_test_fill.columns","0d698648":"#scaling train\nscaler = RobustScaler()\n\nscaled_X_train = scaler.fit_transform(X_train_fill)\nscaled_X_train = pd.DataFrame(scaled_X_train, columns = train_cols)","e7d524e5":"#scaling test\nscaled_X_test = scaler.transform(X_test_fill)\nscaled_X_test = pd.DataFrame(scaled_X_test, columns = test_cols)\nscaled_X_test.set_index(idx, inplace = True)","6cd1030b":"#to reduce datset size in RAM\nscaled_X_train = scaled_X_train.astype(np.float32)\nscaled_X_test = scaled_X_test.astype(np.float32)","b52c1c09":"#adding a column to say if the row contains nulls\n\nscaled_X_train['any_missing'] = X_train_fill['n_missing'] > 0\nscaled_X_test['any_missing'] = X_test_fill['n_missing'] > 0\n\nscaled_X_train['any_missing'] = scaled_X_train['any_missing'].astype(np.int8)\nscaled_X_test['any_missing'] = scaled_X_test['any_missing'].astype(np.int8)\n\ngc.collect()","7bae9564":"#eliminate unnecessary objects to reduce RAM usage\ntry:\n    del test, train, scaler, SI, X_train, X_test, X_train_fill, X_test_fill\nexcept:\n    print('already dropped!')\nfinally:\n    gc.collect()","38ae66af":"Logit_X_train = scaled_X_train.copy()\ngc.collect()","a77416e3":"#defining empty model\ndef run_model(dfx, dfy):\n    y = dfy\n    X = dfx\n    return sm.Logit(y, X).fit()","3073e0b2":"#feature engineering \n\ndef feature_eng(df):\n    df = df.copy()\n    \n    df = sm.add_constant(df)\n    \n    #manually selecting columns to use in the logistic regression\n    feature_cols = df.columns\n    to_remove = ['f12','f17','f18','f19','f22','f26','f29','f33','f37','f40','f41','f42','f43','f49',\n                 'f51','f55','f56','f58','f59','f63','f64','f66','f67','f72','f74','f75','f76',\n                 'f80','f84','f85','f88','f94','f101','f103','f115','avg','std']\n    feature_cols = sorted(list(set(feature_cols)-set(to_remove)))\n    \n    df = df[feature_cols]\n    \n    return df","21aefe83":"Logit_X_train = feature_eng(Logit_X_train)\nLogit_X_test = feature_eng(scaled_X_test)","3afecf9c":"#running model\ntest_0 = run_model(Logit_X_train,y_train)\ntest_0.summary()","d5c3df20":"#saving regression output as an image\n'''\nimport matplotlib.pyplot as plt\nplt.rc('figure', figsize=(10, 20))\n#plt.text(0.01, 0.05, str(model.summary()), {'fontsize': 12}) old approach\nplt.text(0.01, 0.05, str(test_0.summary()), {'fontsize': 10}, fontproperties = 'monospace') # approach improved by OP -> monospace!\nplt.axis('off')\nplt.tight_layout()\nplt.savefig('output.png')\n'''","aad8fa28":"#probability values\nLogit_X_train['train_pred'] = test_0.predict(Logit_X_train)\n\n#bianry predictions\nLogit_X_train['train_pred_bin'] = np.where(Logit_X_train.train_pred > 0.5, 1, 0)\n\n#metrics\naprF1auc(y_train, Logit_X_train.train_pred_bin)","9deb8327":"#calculating Log Loss Score\nprint(\"Log Loss Score: \" + str(log_loss(y_train, Logit_X_train.train_pred_bin)))","e95312f4":"gc.collect()","649a325a":"#preparing DataFrame for the metrics function\nlr_metric = Logit_X_train.copy()\nlr_metric = lr_metric.join(y_train)","27dd753e":"#running the metrics function\nproduce_confusion('Claim','No claim','binary',lr_metric,'train_pred_bin','claim')","1eb3bd96":"#probability values\ntest_pred = test_0.predict(Logit_X_test)\n\n#binary predictions\ntest_pred_bin = np.where(test_pred > 0.5, 1, 0)\ny_test = pd.Series(test_pred_bin)","c2d06e75":"gc.collect()","e0181835":"LRCV_X_train = scaled_X_train.copy()\ngc.collect()","9b5eb058":"LRCV_X_train.dtypes\nLRCV_X_train.columns","03e0acb5":"#crating empty model\nsimple_regressor = LogisticRegressionCV(cv = 10, n_jobs=-1)","d3bcbb94":"LRCV_cols = [i for i in LRCV_X_train.columns if 'f' in i or 'missing' in i]\nLRCV_X_train = LRCV_X_train[LRCV_cols]","a93a910a":"#fitting model to train data\nsimple_regressor.fit(LRCV_X_train,y_train)\ngc.collect()","d4472065":"#probability values\nLRCV_X_train['predicted_proba'] = simple_regressor.predict_proba(LRCV_X_train)[:,1]\n\n#binary prediction\nLRCV_predicted_values = simple_regressor.predict(LRCV_X_train[LRCV_cols])","4fb27fc9":"#calculating Log Loss Score\nprint(\"Log Loss Score: \" + str(log_loss(y_train, LRCV_predicted_values)))","4b57b09a":"gc.collect()","c98476fa":"aprF1auc(y_train, LRCV_predicted_values)","f5615316":"# preparing for the beautiful metrics function\nlrCV_metric = LRCV_X_train.copy()\nlrCV_metric = lrCV_metric.join(y_train)","395ba433":"#running the beautiful metrics function\nproduce_confusion('Claim','No claim','binary',lrCV_metric,'predicted_proba','claim')","7a75e276":"#probability values\npredicted_proba_test = simple_regressor.predict_proba(scaled_X_test)[:,1]\n\n#binary prediction\npredicted_values_test = simple_regressor.predict(scaled_X_test)","608838af":"#create a copy of the dataset to avoid overwriting issues\ntry:\n    RF_X_train = scaled_X_train.copy()\nexcept:\n    print('already done!')\nfinally:\n    gc.collect()","74daebfe":"#creating empty classifier\nrf = RandomForestClassifier(n_estimators=10, max_depth = 5, n_jobs = -1)","a4b5687e":"#fitting the RF model on train\nrf.fit(RF_X_train, y_train)","0a965ebe":"# RF classfier score\nrf.score(RF_X_train, y_train)","6edd7d28":"#classfication scores\nrf_score = cross_val_score(rf, RF_X_train, y_train, cv=5, n_jobs = -1).mean()\nprint(f'Random scored {rf_score}')","9218815e":"## Predict on Train\nRF_X_train['rf_pred'] = rf.predict(RF_X_train)\n\n## Check Accuracy, Precision, Recall & F1\naprF1auc(RF_X_train['rf_pred'], y_train)","905d94d7":"#calculating Log Loss Score\nprint(\"Log Loss Score: \" + str(log_loss(y_train, RF_X_train['rf_pred'])))","22ffd839":"# preparing for the beautiful metrics function\nrf_metric = RF_X_train.copy()\nrf_metric = rf_metric.join(y_train)","9a0e30c7":"#running the beautiful metrics function\nproduce_confusion('Claim','No claim','binary',rf_metric,'rf_pred','claim')","d714ab51":"gc.collect()","d3af5f90":"#create a copy to avoid overwriting issues\ntry:\n    ET_X_train = scaled_X_train.copy()\n    #del scaled_X_train\nexcept:\n    print('already done!')\nfinally:\n    gc.collect()","a7cc96d6":"et = ExtraTreesClassifier(n_estimators=10, max_depth = 5, n_jobs = -1)","da6785c1":"#fitting the ET model on train\net.fit(ET_X_train, y_train)","ce1e0bad":"# ET classfier score\net.score(ET_X_train, y_train)","ea7838ff":"et_score = cross_val_score(et, ET_X_train, y_train, cv=5, n_jobs = -1).mean()\nprint(f'Extra Random scored {et_score}')","1c0e5d65":"## Predict on Train\nET_X_train['et_pred'] = et.predict(ET_X_train)\n\n## Check Accuracy, Precision, Recall & F1\naprF1auc(ET_X_train['et_pred'], y_train)","21a2f355":"#calculating Log Loss Score\nprint(\"Log Loss Score: \" + str(log_loss(y_train, ET_X_train['et_pred'])))","fd82e53c":"# preparing for the beautiful metrics function\net_metric = ET_X_train.copy()\net_metric = et_metric.join(y_train)","16d9691b":"#running the beautiful metrics function\nproduce_confusion('Claim','No claim','binary',et_metric,'et_pred','claim')","3ae2b5df":"gc.collect()","30cf1cd2":"scaled_X_test","78a92be2":"## Predict on Test\n\nrf_test_pred = rf.predict(scaled_X_test)\nrf_test_pred","68212b43":"## Predict on Test\n\net_test_pred = et.predict(scaled_X_test)\npd.Series(et_test_pred)","07507669":"#create a copy to avoid overwriting issues\nGS_X_train = scaled_X_train.copy()\n#del scaled_X_train\ngc.collect()","8d23cfdb":"## running GridSearch and RandomizedSearch using ExtraTrees (cause it's much quicker)\net_params = {\n    'n_estimators': [5, 10, 20, 50],\n    'max_depth': [2, 3, 4, 5, 6],\n    'min_samples_split' : [10, 100, 1000, 10000, 100000],\n    'min_samples_leaf' : [1, 10, 100, 1000, 10000, 100000]\n}\net = ExtraTreesClassifier()\n#gs = GridSearchCV(et, param_grid=et_params, cv=5, verbose = 1, n_jobs = -1, scoring = \u2018roc_auc\u2019)\nrs = RandomizedSearchCV(et, param_distributions=et_params, \n                        n_iter = 100, cv=5, refit = 'roc_auc',\n                        verbose = 1, n_jobs = -1,\n                        scoring = ['roc_auc','accuracy'])","57a38af9":"#fitting and identification of best classifier\n\n#GridSearch\n#gs.fit(GS_X_train, y_train)\n#print(gs.best_score_)\n#gs.best_params_\n\n#RandomizedSearch\nrs.fit(GS_X_train, y_train)\nprint(rs.best_score_)\nrs.best_params_","187e7623":"gc.collect()","2fc147f2":"#classfier score\nrs.score(GS_X_train, y_train)","f5090c16":"## Predict on Train\nGS_X_train['gs_pred'] = rs.predict(GS_X_train)\n\n## Check Accuracy, Precision, Recall & F1\naprF1auc(GS_X_train['gs_pred'], y_train)","f3d350d0":"print(\"Root Mean Squared Error: \" + str(mean_squared_error(GS_X_train['gs_pred'], y_train, squared = False)))","17ae414b":"# preparing for the beautiful metrics function\ngs_metric = GS_X_train.copy()\ngs_metric = gs_metric.join(y_train)","fdc7f387":"#running the beautiful metrics function\nproduce_confusion('Claim', 'No claim', 'binary', gs_metric, 'gs_pred', 'claim')","af7801ef":"#create a copy to avoid overwriting issues\ncuRF_X_train = scaled_X_train.copy()\n#del scaled_X_train\ngc.collect()","dc501573":"## creating empty GridSearch using cuML Random Forest Classifier (GPU accelerated)\ncu_RF_params = {\n    'n_estimators': [5, 10, 20],\n    'max_depth': [2, 3, 4, 5, 6],\n    'min_samples_split' : [10, 100, 1000, 10000],\n    'min_samples_leaf' : [1, 10, 100, 1000, 10000]\n}\ncu_RF = cuRF()\ngs = cuGridSearchCV(cu_RF, param_grid=cu_RF_params,\n                    scoring = ['roc_auc','accuracy'],\n                    refit = 'roc_auc',cv=5, \n                    verbose = 1, \n                    n_jobs = -1)  ","9c53fbcd":"#fitting and identification of best classifier\ngs.fit(cuRF_X_train, y_train)\n\nprint(gs.best_score_)\n\ngs.best_params_","54cb80d1":"#give it a shot even though it needs GPU acceleration","117d9809":"from cuml.svm import SVC\nSVM_X_train = scaled_X_train.copy()\nSVM_X_train.drop(columns = 'any_missing', inplace = True)\ngc.collect()","45e7f3ef":"train = SVC(gamma='auto'\n              , kernel='rbf'\n              , C=10\n              , probability = True)\n\ntrain.fit(SVM_X_train, y_train)","d8f1b686":"# choosing list of parameter values\nGS_tuned_parameters = [{'kernel': ['rbf'], 'gamma': ['auto'],'C': [0.1, 1, 10, 100]}]","40cc0aeb":"#creating GridSearch\nGS_train = cuGridSearchCV(SVC(), GS_tuned_parameters, scoring='roc_auc', verbose = 1)","aabaf789":"gc.collect()","44de1914":"#fitting all combinations of parameters\nGS_train.fit(scaled_X_train, y_train)","7182dd85":"#best model parameters\nGS_train.best_params_","9b9d33e6":"#train scores\nprint(classification_report(y_train, GS_train.predict(scaled_X_train)))","7e31e0b6":"XGB_X_train = scaled_X_train.copy()\ngc.collect()","d2833b23":"##this needs GPU acceleration\n\nxgb = XGBClassifier(tree_method='gpu_hist', n_jobs = -1)\n#xgb = XGBClassifier(n_jobs = -1)","befa96ea":"gc.collect()\nxgb.fit(XGB_X_train, y_train)","868da8d1":"#classfier score\ngc.collect()\nxgb.score(XGB_X_train, y_train)","1c11e8f3":"## Predict on Train\nXGB_X_train['xgb_pred'] = xgb.predict(XGB_X_train)","f27e1f99":"print(\"Root Mean Squared Error: \" + str(mean_squared_error(XGB_X_train['xgb_pred'], y_train, squared = False)))","9593d0c9":"# preparing for the beautiful metrics function\nxgb_metric = XGB_X_train.copy()\nxgb_metric = xgb_metric.join(y_train)","cfca792f":"#running the beautiful metrics function\nproduce_confusion('Claim', 'No claim', 'binary', xgb_metric, 'xgb_pred', 'claim')","1bb7fad4":"test_predict = xgb.predict(scaled_X_test)","7e5030ad":"# pycaret wants a single dataframe which includes the target column\ntry:\n    clf_data = scaled_X_train.copy()\n    clf_data = clf_data.join(y_train)\n    del scaled_X_train\nexcept:\n    pass\nfinally:\n    gc.collect()","321fbaf7":"#setting up the pipeline\nclf = setup(data = clf_data, \n            target = 'claim',\n            data_split_stratify = True,\n            use_gpu = True, \n            n_jobs = -1, \n            silent = True\n           )","a0a5d251":"gc.collect()\nlr = create_model('lr')","9c766fc9":"predict_model(lr);","136839ad":"final_lr = finalize_model(lr)\npredict_model(final_lr);","a16fc9e8":"#this lists all models that can be run\nmodels()","c5de031f":"#comparing all models\n#excluded some models that did not benefit from GPU acceleration and took too long to run (ada, gbc)\n#excluded SVM since it does not support AUC score\n#excluded some models that were run independently before (dt, et)\ncompare_models(exclude = ['dt','ada','gbc','et','svm'], sort = 'AUC')","d5a87d04":"#creating a model\ncatboost = create_model('catboost')\ngc.collect()","235a491c":"gc.collect()","ce716ee5":"#hyperparameter tuning\ngc.collect()\ntuned_catboost = tune_model(catboost, choose_better = True, optimize = 'AUC')","013aa013":"#model plotting\nplot_model(tuned_catboost, plot = 'auc')","713ab699":"plot_model(tuned_catboost, plot = 'pr')","7a7a9498":"plot_model(tuned_catboost, plot='feature')","b79bd553":"plot_model(tuned_catboost, plot = 'confusion_matrix')","b17e9816":"gc.collect()","b19145da":"#predict on validation set\npredict_model(tuned_catboost);","4b0c7bf1":"#finalise model\n#fits the model onto the complete dataset including the test\/hold-out sample\nfinal_catboost = finalize_model(tuned_catboost)","988bf4be":"#predict using entire dataset\npredict_model(final_catboost);","eb670b9d":"#prediction on unseen data\nunseen_predictions_cat = predict_model(final_catboost, data=scaled_X_test, raw_score = True)\nunseen_predictions_cat.head()","fd69c09d":"save_model(final_catboost,'Final CatBoost Model 21Sep2021')","f9d7e9b3":"#creating a model\nlightgbm = create_model('lightgbm')\ngc.collect()","d9ea817c":"#hyperparameter tuning\ngc.collect()\ntuned_lightgbm = tune_model(lightgbm, search_library = 'scikit-optimize', early_stopping = 'asha')","afc2d487":"#model plotting\nplot_model(tuned_lightgbm, plot = 'auc')","d827a4c8":"plot_model(tuned_lightgbm, plot = 'pr')","e9ea7c6d":"plot_model(tuned_lightgbm, plot='feature')","ea8f549e":"plot_model(tuned_lightgbm, plot = 'confusion_matrix')","9148abdd":"gc.collect()","59a0456c":"#predict on validation set\npredict_model(tuned_lightgbm);","d9b94a9d":"#finalise model\n#fits the model onto the complete dataset including the test\/hold-out sample\nfinal_lightgbm = finalize_model(tuned_lightgbm)","9654055f":"#predict using entire dataset\npredict_model(final_lightgbm);","8409e754":"#performance on unseen data\nunseen_predictions_lgbm = predict_model(final_lightgbm, data=scaled_X_test)\nunseen_predictions_lgbm.head()","b7cfc8bc":"save_model(final_lightgbm,'Final LightGBM Model 17Sep2021')","67a4a03a":"gc.collect()","55745c97":"svm = create_model('svm')\n#predict_model(svm);\n\nfinal_svm = finalize_model(svm)\npredict_model(final_svm);","333b1696":"meow = create_model('catboost')","47ba1b52":"tuned_meow = tune_model(meow, choose_better = True, optimize = 'AUC')","a2ce09ce":"predict_model(meow);","5ffd0da9":"cats = ensemble_model(meow, choose_better = True, optimize = 'AUC')","dbf09dd6":"predict_model(cats);","e537c7c3":"final_model = finalize_model(cats)\npredict_model(final_model);","ecedfc2f":"#performance on unseen data\nunseen_predictions_ensemble = predict_model(final_model, data = scaled_X_test, raw_score = True)\nunseen_predictions_ensemble.head()","ef13d6f4":"top3 = compare_models(n_select = 3, exclude = ['dt','ada','gbc','et','svm'], sort = 'AUC')","3bbb04de":"tuned_top3 = [tune_model(i, choose_better = True, optimize = 'AUC') for i in top3]","a4cad693":"#voting classifier\ntuned_blend = blend_models(tuned_top3)","ade3ced3":"basic_blend = blend_models(top3)","03c8ca54":"best_auc_model = automl(optimize = 'AUC')","58e59b6b":"predict_model(basic_blend);","8247a804":"final_model = finalize_model(basic_blend)\npredict_model(final_model);","293406b5":"#performance on unseen data\nunseen_predictions_best = predict_model(final_model, data = scaled_X_test, raw_score = True)\nunseen_predictions_best.head()","dae3e8dd":"top3","fd57bbf3":"#model plotting\nplot_model(final_model, plot = 'auc')","b48bca5c":"plot_model(final_model, plot = 'pr')","1b49bf79":"plot_model(final_model, plot='feature')","3c095abe":"plot_model(final_model, plot = 'confusion_matrix')","3e89404e":"save_model(final_model, '3top23Sept2021')","c5cd1b7c":"predict_model(basic_blend);","73633aa5":"basic_blend_final = finalize_model(basic_blend)\npredict_model(basic_blend_final);","e902c1cc":"unseen_predictions_best = predict_model(basic_blend_final, data = scaled_X_test, raw_score = True)\nunseen_predictions_best.head()","d57fca16":"plot_model(basic_blend_final, plot = 'auc')","c49cae50":"plot_model(basic_blend_final, plot = 'confusion_matrix')","778721b1":"plot_model(basic_blend_final, plot = 'feature')","19d8f4e6":"plot_model(basic_blend_final, plot = 'pr')","1355b72f":"assert(len(idx)==len(unseen_predictions_best))\n\nsub = pd.DataFrame(list(zip(idx, unseen_predictions_best.Score_1)),columns = ['id', 'claim'])\n\nsub.to_csv('submission.csv', index = False)\n\nprint(sub)","0d611d13":"### Hyperparameter tuning","26eb0ff2":"### Prediction on internal validation set","aa507cbb":"# SVM (needs GPU)","03efd16c":"## Model comparison","4bb697e2":"### Predict on unseen data (X_test)","36121401":"### Model plots","320c1755":"# Data Cleaning","c3fb963a":"### Predict on unseen data (X_test)","c7c322ba":"# PyCaret Classification","adaf0621":"# Logistic regression","4ba6a986":"## Test","f3e28651":"### Finalise model","92073e2c":"## Importing packages and functions","8018d222":"### Model plots","2f733ad1":"# Submission","9737033f":"### Prediction on internal validation set","dd44b90e":"### Model creation","e95c4a6f":"# Exploratory Data Analysis","64394c54":"## cuML Random Forest (GPU acceleration)","2c91ff25":"## Extra Trees Classifier","f57f76db":"# Predict","167a3d99":"# Random Forest","d65b1d3d":"### Save model","9be25e75":"# Importing data","666de1c3":"## GridSearch","9035bdb0":"# Scaler","93f4a04f":"## Setup","24945a7a":"## Filling in null values by column mean","d1028a4f":"### Save model","b94d6430":"## Random Forest Classifier","a28295fa":"## CatBoost","772f06cb":"### Model creation","ba321319":"# GridSearch","ab2e0b78":"### Finalise model","ecfa9626":"## Ensemble model","04362e75":"# Logistic Regression CV","546d012f":"### Hyperparameter tuning","132fccf5":"## AutoML","bd44960d":"## LightGBM","9b1d46c2":"# XGBoost (needs GPU)","af7056fc":"## SVM"}}