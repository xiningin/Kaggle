{"cell_type":{"c99e4fb5":"code","effc2369":"code","833d370d":"code","705b51fe":"code","3c838cc7":"code","31cfa59e":"code","80660367":"code","347e3d69":"code","dcd8ced3":"code","15f588cf":"code","c9a1149f":"code","73f6a6f2":"code","c1c008b6":"code","9d9c620e":"code","c2946954":"code","bb73cc6a":"code","54c5b2e7":"code","e2955da5":"code","2a0ce5f0":"code","fa515fb5":"code","21a2114c":"code","c3cf35c6":"code","4e6889d4":"code","0b47d7c2":"code","d38f3646":"code","b312ab3b":"code","fc71e507":"code","ce3d0669":"code","a63a94a2":"code","8b1e32c5":"code","cbf323de":"code","61e536c7":"code","bbf719d6":"code","c286770c":"code","0c216df0":"code","115e3154":"code","9a4c0fe8":"code","fe78a293":"code","190fcc24":"code","2e665c47":"code","72f06569":"code","5dd531d3":"code","1240b821":"code","6d23f3e8":"code","02ffa1b6":"code","30bca350":"code","70ef58ea":"code","2d462f2d":"code","95d1ced8":"code","d2fe7428":"code","2d6e9aad":"code","065ce26a":"code","d5720d7a":"code","35101ab0":"code","9a28bbfa":"code","9a17a85d":"code","bb23624b":"code","c90b9de2":"markdown","fe8b2e02":"markdown","ec3e08cf":"markdown","c85ad22b":"markdown","98c3e70b":"markdown","49a365d9":"markdown","3ff9ebeb":"markdown","38762a35":"markdown","7acd153e":"markdown","fafa3a10":"markdown","8fcd96ab":"markdown","7ae61a2d":"markdown","063e6337":"markdown","c7896764":"markdown","a3cb7092":"markdown","84549ee1":"markdown","ec0b9bd0":"markdown","11750760":"markdown","e539ebfa":"markdown","d2d4f7a2":"markdown","fb2d982a":"markdown","b7d8aff7":"markdown","5e3be599":"markdown","8f279cd5":"markdown","98c4391c":"markdown","d6e0da13":"markdown","05c9b00b":"markdown","272bda49":"markdown","cba0ed54":"markdown","2c1bc41c":"markdown","c8c22a61":"markdown","1cf69771":"markdown","5b8e2dd5":"markdown","02e3f8d7":"markdown","80c25d92":"markdown","81d9ec36":"markdown","e5d5e5d9":"markdown","ef6ffa79":"markdown","f0f14cef":"markdown","c1addc23":"markdown","aff5e4e9":"markdown","5e2d1c39":"markdown","ccdaaac4":"markdown","2d7a5bc9":"markdown","e5e6c79b":"markdown","3e261c1f":"markdown","6f8012fa":"markdown","3b526d5f":"markdown","c8ca7a5d":"markdown"},"source":{"c99e4fb5":"import pandas as pd\nimport numpy as np\n\ndf_train = pd.read_csv(r'..\/input\/titanic\/train.csv')\ndf_train.head()\n","effc2369":"df_test = pd.read_csv(r'..\/input\/titanic\/test.csv')\ndf_test.head()","833d370d":"df_train.shape","705b51fe":"from collections import Counter\nimport numpy as np\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","3c838cc7":"Outliers_to_drop = detect_outliers(df_train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","31cfa59e":"df_train.loc[Outliers_to_drop] # Show the outliers rows","80660367":"df_train = df_train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","347e3d69":"df_train.shape","dcd8ced3":"len_train=len(df_train)\nlen_train","15f588cf":"df_train=pd.concat([df_train,df_test],sort=False)","c9a1149f":"df_train.shape","73f6a6f2":"df_train.isnull().sum()","c1c008b6":"df_train[[\"Age\"]].describe()","9d9c620e":"#df_train.dropna(subset=[\"Age\"],inplace=True)\ndf_train['Age']=df_train['Age'].fillna(df_train['Age'].median())","c2946954":"df_train.isnull().sum()","bb73cc6a":"df_train[\"Fare\"] = df_train[\"Fare\"].fillna(df_train[\"Fare\"].mean())","54c5b2e7":"df_train.isnull().sum()","e2955da5":"df_train['Cabin'] = df_train['Cabin'].astype(str).str[0]","2a0ce5f0":"df_train['Cabin'].value_counts()","fa515fb5":"df_train.isnull().sum()","21a2114c":"df_train[\"Embarked\"].mode()","c3cf35c6":"df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')","4e6889d4":"df_train.isnull().sum()","0b47d7c2":"df_train.info()","d38f3646":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.heatmap(df_train[[\"Survived\",\"Age\",\"SibSp\",\"Parch\",\"Fare\"]].corr(),annot=True,cmap=\"BrBG\")","b312ab3b":"sns.catplot(x=\"Survived\", y=\"Age\", kind=\"box\", data=df_train)","fc71e507":"sns.catplot(y=\"Survived\", x=\"SibSp\",kind=\"box\", data=df_train)","ce3d0669":"sns.catplot(y=\"Survived\", x=\"Parch\", kind=\"box\", data=df_train);","a63a94a2":"sns.catplot(x=\"Survived\", y=\"Fare\", kind=\"bar\", data=df_train);","8b1e32c5":"df_train.info()","cbf323de":"sns.catplot(x=\"Sex\", y=\"Survived\", kind=\"bar\", hue=\"Pclass\",data=df_train)","61e536c7":"sns.catplot(x=\"Cabin\", y=\"Survived\", kind=\"bar\",data=df_train)","bbf719d6":"sns.catplot(x=\"Embarked\", y=\"Survived\", kind=\"bar\",data=df_train)","c286770c":"df_train.head()","0c216df0":"df_train = pd.concat([df_train, pd.get_dummies(df_train['Sex'])], axis=1)\ndf_train=df_train.drop(['Sex'],axis=1)\ndf_train.head()","115e3154":"df_train[\"F_size\"]=df_train[\"SibSp\"]+df_train[\"Parch\"]+1\ndf_train=df_train.drop(['SibSp','Parch'],axis=1)\ndf_train.head()","9a4c0fe8":"sns.catplot(x=\"F_size\", y=\"Survived\", kind=\"bar\",data=df_train)","fe78a293":"df_train['Single'] = df_train['F_size'].map(lambda s: 1 if s == 1 else 0)\ndf_train['Small_F'] = df_train['F_size'].map(lambda s: 1 if s == 2 else 0)\ndf_train['Medium_F'] = df_train['F_size'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf_train['Large_F'] = df_train['F_size'].map(lambda s: 1 if s >= 5 else 0)\ndf_train.head()","190fcc24":"df_train = pd.concat([df_train, pd.get_dummies(df_train['Pclass'], prefix=\"Pclass\")], axis=1)\ndf_train = pd.concat([df_train, pd.get_dummies(df_train['Cabin'], prefix=\"Cabin\")], axis=1)\ndf_train = pd.concat([df_train, pd.get_dummies(df_train['Embarked'], prefix=\"Embarked\")], axis=1)\ndf_train=df_train.drop(['Pclass','Cabin','Embarked','F_size'],axis=1)\ndf_train.head()","2e665c47":"features = df_train.drop(['PassengerId','Name','Ticket'],axis=1)\nfeatures.head()","72f06569":"X_train = features[features['Survived'].notnull()]\nX_train.shape","5dd531d3":"X_test = features[features['Survived'].isnull()]\nX_test.shape","1240b821":"X_train.isnull().sum()","6d23f3e8":"Y_train = X_train['Survived']\nX_train = X_train.drop(['Survived'],axis=1)","02ffa1b6":"X_test=X_test.drop(['Survived'],axis=1)","30bca350":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","70ef58ea":"kfold = StratifiedKFold(n_splits=10)","2d462f2d":"random_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"KNeighboors\",\"LogisticRegression\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","95d1ced8":"cv_res","d2fe7428":"LR = LogisticRegression()\nLR.fit(X_train,Y_train)","2d6e9aad":"Predict=LR.predict(X_test)","065ce26a":"Y_test = pd.read_csv(r'..\/input\/titanic\/gender_submission.csv')\nY_test.head()","d5720d7a":"#Y_test=pd.merge(df_test, Y_test, on='PassengerId')\n#Y_test=Y_test[['Age','Survived']]\n#Y_test=Y_test.dropna()\nPassengerId=Y_test['PassengerId']\nY_test=Y_test['Survived']\nY_test.head()","35101ab0":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_test, Predict))","9a28bbfa":"from sklearn.metrics import accuracy_score\n\naccuracy_score(Y_test, Predict)","9a17a85d":"Submission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': Predict })","bb23624b":"Submission.to_csv(\"submission.csv\", index=False)","c90b9de2":"#### Features Selection for model training:\n\nI have decided Age, Fare, Sex, F_size, Pclass, Cabin and Embarked as my features.","fe8b2e02":"### Age vs Survived","ec3e08cf":"#### Variable Notes:\n##### pclass: \nA proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n##### age:\nAge is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n##### sibsp:\nThe dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n##### parch:\nThe dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","c85ad22b":"Coverting the Sex Feature into male and female Features","98c3e70b":"From above graph it is clear that Logistic Regression has higher accuracy.","49a365d9":"### Prediction:\nI am using Logistic Regression for prediction.","3ff9ebeb":"The above shows that survival rate for individuals with more Parent\/Children is less.","38762a35":"### Finding missing values:","7acd153e":"##### Cabin:\nCabin has 774 null values. It is clear that we can not drop all these rows. so we have to find a value to replace it with.\nI am taking the first character from each row of the Cabin column. this gives me the cabin group the passenger belongs to.","fafa3a10":"### Survived vs Pclass\/Sex","8fcd96ab":"#### Data Dictionary:\n<table>\n<tbody>\n<tr><th><b>Variable<\/b><\/th><th><b>Definition<\/b><\/th><th><b>Key<\/b><\/th><\/tr>\n<tr>\n<td>survival<\/td>\n<td>Survival<\/td>\n<td>0 = No, 1 = Yes<\/td>\n<\/tr>\n<tr>\n<td>pclass<\/td>\n<td>Ticket class<\/td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td>\n<\/tr>\n<tr>\n<td>sex<\/td>\n<td>Sex<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>Age<\/td>\n<td>Age in years<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>sibsp<\/td>\n<td># of siblings \/ spouses aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>parch<\/td>\n<td># of parents \/ children aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>ticket<\/td>\n<td>Ticket number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>fare<\/td>\n<td>Passenger fare<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>cabin<\/td>\n<td>Cabin number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>embarked<\/td>\n<td>Port of Embarkation<\/td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n<\/tr>\n<\/tbody>\n<\/table>","7ae61a2d":"I compared below classifiers and evaluate the mean accuracy of each of them by stratified kfold cross validation procedure.\n\nSVC\n\nDecision Tree\n\nKNN\n\nLogistic regression","063e6337":"Above code shows that there are:\n\nSurvived has 418 null values which are from test data set. As test dataset does not have column Survived.\n\nAge has 256 null values.\n\nFare has 1 null value.\n\nCabin has 1007 null values.\n\nEmbarked has 2 null values.","c7896764":"Above box plot shows that chance of survival is more for younger people.","a3cb7092":"### Categorical Features:","84549ee1":"### Parch vs Survived","ec0b9bd0":"Now the train dataset is free of missing values.","11750760":"## Detecting Outliers from the dataset.\n\nBelow code will identify indexes of the row which has more than 'n' outliers","e539ebfa":"From above it is clear that those who got into ship at 'C = Cherbourg' have higher survival rate.","d2d4f7a2":"### Filling missing values:\n\nIn this we decide wether to drop the rows with missing values or to replace the missing values with different values.","fb2d982a":"We can group these families as Single, Small, Medium, Large categories.","b7d8aff7":"Now i will get_dummies for Pclass, Cabin and Embarked Features.","5e3be599":"From the above graph it is clear that female passengers have higher survival rates than male passengers regardless of which class the are in.\n\nAnd passengers in higher class have more survival rates that those in lower classes.","8f279cd5":"From above correlation marix it is clear that Fare has higer correlation with survival rate( Higher the fare higer the class). ","98c4391c":"From above it is clear that passengers with a cabin has more survival rate than those without a cabin('n').","d6e0da13":"### Modeling:","05c9b00b":"##### Fare:\nFare has 1 missing value, we will replace it with mean Fare value. ","272bda49":"Since the ditribution is high for Age feature, we cannot replace the missing Age values with Age.median().\n\nSo i have decided to drop the Age rows with NaN values.","cba0ed54":"##### Survived:\nAs the ones with survived value as NaN are from Test dataset. they will be gone once we split train and test datasets.","2c1bc41c":"We will have to drop few rows cause in the before steps we dropped the null values in 'Age' feature from combined dataset which includes df_test dataset.","c8c22a61":"   ### Numerical Features:\n   \n   Creating a heapmap to indentify the correlation of each feature on survived column.","1cf69771":"If we observe the above dataset, we can get the family size of an individual by adding SibSp + Parch + 1.","5b8e2dd5":"##### Seperating Y_train from X_train:","02e3f8d7":"### Fare vs Survived","80c25d92":"#### Survived vs Embarked","81d9ec36":"### Combining the tow datasets:\nFor handlng missing values and for Feature Engineering ","e5d5e5d9":"#### Importing gender_submission.csv to validate our prediction.","ef6ffa79":"##### The model has a f1-score of 94% when used against the test data.","f0f14cef":"Passengers with higher ticket price comes under higher class and their chance of survival is more.","c1addc23":"### Feature Engineering:","aff5e4e9":"Splitting the train and test datasets.","5e2d1c39":"###### Age:","ccdaaac4":"##### Embarked:\nEmbarked has 2 null values. we replace them ith most frequesnt embarked value.","2d7a5bc9":"The above shows that survival rate for individuals with more Siblings\/Spouse is less.","e5e6c79b":"#### Survived vs Cabin","3e261c1f":"## Introduction\nOverview\nThe data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.","6f8012fa":"### SibSp vs Survived","3b526d5f":"Below line of code will check outliers in continous features.","c8ca7a5d":"## Feature Analysis:"}}