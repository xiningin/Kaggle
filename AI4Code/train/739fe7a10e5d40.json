{"cell_type":{"a2fb1661":"code","8876e9eb":"code","2797a33a":"code","24db65ab":"code","5207015f":"code","29a4af7b":"code","32e59556":"code","95c3eea9":"code","72e04874":"code","53751866":"code","0906e1ee":"code","bd4bf6df":"code","1ee77878":"code","f290eb70":"code","45da4670":"code","55cc5a6a":"code","b53f3206":"code","49347d37":"code","dea34b3a":"code","31de946a":"code","f322df2d":"code","92b655b6":"code","89821c25":"code","4f939768":"code","842acb64":"code","d137df79":"code","ee8d3478":"code","ac28d05b":"code","cb8d5b03":"code","ef7ba35c":"code","a895511b":"code","9af1214a":"code","6f2a7b78":"code","af87300a":"code","4e554492":"code","0e125288":"code","e33c3f8b":"code","b993a6a0":"code","40fe0cf2":"code","29fde343":"code","a394074c":"code","7a5fae21":"code","58208992":"code","619d1d70":"code","c2b3219c":"code","abca6544":"code","2ea3d6ca":"code","3c7d048a":"code","d4188e20":"code","22f79708":"code","1d757162":"code","57004f55":"code","b9650467":"code","4c435367":"code","8dccb7f8":"code","622e923f":"code","472d5b21":"code","2664f6ad":"code","625eafa5":"code","3cb004b7":"code","64731835":"code","65c3d490":"code","7aa49c80":"code","43861aff":"markdown","a2a18da6":"markdown","d751cd4e":"markdown","cc62d120":"markdown","34124c9e":"markdown","8b13bd65":"markdown","811a6f3b":"markdown","18d6e586":"markdown","ef039e59":"markdown","cb05b7ba":"markdown","88611dfc":"markdown","67a3ebe9":"markdown","4a8f1e47":"markdown","02e5cb66":"markdown","0a96c09f":"markdown","c7c9a181":"markdown","8000da3f":"markdown","e6889556":"markdown","66ac1365":"markdown","8726e013":"markdown","1694ec21":"markdown","c9735db5":"markdown","6943caf6":"markdown","40180b9b":"markdown","58c6e1b3":"markdown","47a848d3":"markdown"},"source":{"a2fb1661":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8876e9eb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2797a33a":"# Google Drive\n# df = pd.read_csv('\/content\/drive\/MyDrive\/Colab Notebooks\/ML Self-Projects\/HR Analytics Job Prediction\/HR_comma_sep.csv')\n# Local\n# df = pd.read_csv('HR_comma_sep.csv')\n\n# Kaggle\ndf = pd.read_csv('\/kaggle\/input\/hr-analytics-and-job-prediction\/HR_comma_sep.csv')\ndf.head()","24db65ab":"df.info()","5207015f":"df.isnull().sum()","29a4af7b":"df","32e59556":"plt.figure(figsize=(20,10))\nplt.title('Count of total person left')\nsns.countplot(data=df, x='left');","95c3eea9":"plt.figure(figsize=(20,10))\nplt.title('Count of salary vs left')\nsns.countplot(data=df, x='salary', hue='left');","72e04874":"plt.figure(figsize=(20,10))\nplt.title('Count of Department vs left')\nsns.countplot(data=df, x='Department', hue='left');","53751866":"plt.figure(figsize=(20,20))\nsns.heatmap(data=df.corr(),annot=True);","0906e1ee":"df.corr()['left'].sort_values()[:-1]","bd4bf6df":"plt.figure(figsize=(20,20))\nsns.pairplot(data=df,hue='left')","1ee77878":"plt.figure(figsize=(20,10))\nplt.title('Count of satisfaction level vs time spend company vs left')\nsns.scatterplot(data=df, x='satisfaction_level',y='time_spend_company', hue='left',s=100);","f290eb70":"plt.figure(figsize=(20,10))\nplt.title('Count of Work accident vs left')\nsns.countplot(data=df, x='Work_accident', hue='left');","45da4670":"plt.figure(figsize=(20,10))\nplt.title('Count of number project vs time spend company vs left')\nsns.countplot(data=df, x='number_project', hue='left');","55cc5a6a":"plt.figure(figsize=(20,10))\nplt.title('Count of promotion in last 5 years vs time spend company vs left')\nsns.countplot(data=df, x='promotion_last_5years', hue='left');","b53f3206":"df = pd.get_dummies(df, drop_first=True)","49347d37":"X = df.drop('left', axis=1)\ny = df['left']","dea34b3a":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","31de946a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","f322df2d":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","92b655b6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier","89821c25":"def fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(X_train,y_train)\n        model_scores[name] = model.score(X_test,y_test)\n\n    model_scores = pd.DataFrame(model_scores, index=['Score']).transpose()\n    model_scores = model_scores.sort_values('Score')\n        \n    return model_scores","4f939768":"models = {'LogisticRegression': LogisticRegression(max_iter=10000),\n          'KNeighborsClassifier': KNeighborsClassifier(),\n          'SVC': SVC(),\n          'DecisionTreeClassifier': DecisionTreeClassifier(),\n          'RandomForestClassifier': RandomForestClassifier(),\n          'AdaBoostClassifier': AdaBoostClassifier(),\n          'GradientBoostingClassifier': GradientBoostingClassifier(),\n          'XGBClassifier': XGBClassifier(),\n          'XGBRFClassifier': XGBRFClassifier()}","842acb64":"baseline_model_scores = fit_and_score(models, X_train, X_test, y_train, y_test)","d137df79":"baseline_model_scores.sort_values('Score')","ee8d3478":"plt.figure(figsize=(20,10))\nsns.barplot(data=baseline_model_scores.sort_values('Score').T)\nplt.title('Baseline Model Precision Score')\nplt.xticks(rotation=90);","ac28d05b":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import f1_score","cb8d5b03":"def randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_rs_scores = {}\n    model_rs_best_param = {}\n    \n    for name, model in models.items():\n        rs_model = RandomizedSearchCV(model,\n                                     param_distributions=params[name],\n                                        scoring='f1',\n                                      cv=5,\n                                     n_iter=20,n_jobs=-1,\n                                     verbose=2)        \n        rs_model.fit(X_train,y_train)\n        y_pred = rs_model.predict(X_test)\n        model_rs_scores[name] = f1_score(y_test,y_pred)\n        model_rs_best_param[name] = rs_model.best_params_\n        \n    return model_rs_scores, model_rs_best_param","ef7ba35c":"models = {'LogisticRegression' : LogisticRegression(),\n          'GradientBoostingClassifier': GradientBoostingClassifier(),\n          'DecisionTreeClassifier': DecisionTreeClassifier(),\n          'RandomForestClassifier': RandomForestClassifier(),\n          }\n\nparams = {'LogisticRegression': {'C': [0.001,0.01,0.1,1.0,10,100],\n                                 'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n                                 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n          'GradientBoostingClassifier' : {'loss': ['deviance', 'exponential'],\n                                          'learning_rate': [0.001,0.01,0.1,1.0],\n                                          'n_estimators': [20,50,100,200,400],\n                                          'criterion': ['friedman_mse', 'mse'],\n                                          'max_depth' : [2,3,6,10,20],\n                                          'ccp_alpha' : [0.0,0.001,0.01,0.1,1]\n                                          },\n          'DecisionTreeClassifier' : {'criterion': ['gini', 'entropy'],\n                                      'max_depth': [None, 3,5,10,20,50],\n                                      'max_leaf_nodes': [None, 3,5,10,20,50],\n                                      'ccp_alpha' : [0.0,0.001,0.01,0.1,1]\n                                      },\n          'RandomForestClassifier': {'n_estimators': [20,50,100,200,400],\n                                     'criterion': ['gini', 'entropy'],\n                                     'max_depth': [None, 2,10,50,100],\n                                     'bootstrap': [True, False],\n                                     'oob_score': [True, False],\n                                     'ccp_alpha': [0.1,0.01,0.001],\n                                     },\n          }","a895511b":"model_rs_scores_1, model_rs_best_param_1 =randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","9af1214a":"model_rs_scores_1","6f2a7b78":"model_rs_best_param_1","af87300a":"params = {'LogisticRegression': {'C': [0.1,0.2,0.4,0.8,1],\n                                 'penalty': ['none'],\n                                 'solver': ['saga']},\n          'GradientBoostingClassifier' : {'loss': ['exponential'],\n                                          'learning_rate': [0.01,0.02,0.05],\n                                          'n_estimators': [150,200,250,300],\n                                          'criterion': ['friedman_mse'],\n                                          'max_depth' : [15,20,30,50],\n                                          'ccp_alpha' : [0.0]\n                                          },\n          'DecisionTreeClassifier' : {'criterion': ['gini', 'entropy'],\n                                      'max_depth': [40,50,60,70],\n                                      'max_leaf_nodes': [30,50,60,100],\n                                      'ccp_alpha' : [0.0]\n                                      },\n          'RandomForestClassifier': {'n_estimators': [10,15,20,25,30],\n                                     'criterion': ['entropy'],\n                                     'max_depth': [5,10,20,25],\n                                     'bootstrap': [True],\n                                     'oob_score': [True],\n                                     'ccp_alpha': [0.0001,0.001,0.005],\n                                     },\n          }","4e554492":"model_rs_scores_2, model_rs_best_param_2 = randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","0e125288":"model_rs_scores_2","e33c3f8b":"model_rs_best_param_2","b993a6a0":"params = {'LogisticRegression': {'C': [0.01,0.0001,0.1],\n                                 'penalty': ['none'],\n                                 'solver': ['saga']},\n          'GradientBoostingClassifier' : {'loss': ['exponential'],\n                                          'learning_rate': [0.02,0.03,0.04],\n                                          'n_estimators': [275,300,400],\n                                          'criterion': ['friedman_mse'],\n                                          'max_depth' : [11,12,13,14,15],\n                                          'ccp_alpha' : [0.0]\n                                          },\n          'DecisionTreeClassifier' : {'criterion': ['gini'],\n                                      'max_depth': [65,70,75,80],\n                                      'max_leaf_nodes': [25,30,35,40],\n                                      'ccp_alpha' : [0.0]\n                                      },\n          'RandomForestClassifier': {'n_estimators': [18,20,22,34],\n                                     'criterion': ['entropy'],\n                                     'max_depth': [18,20,22],\n                                     'bootstrap': [True],\n                                     'oob_score': [True],\n                                     'ccp_alpha': [0.0001,0.001,0.005],\n                                     },\n          }","40fe0cf2":"model_rs_scores_3, model_rs_best_param_3 = randomsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","29fde343":"model_rs_scores_3","a394074c":"model_rs_best_param_3","7a5fae21":"from sklearn.model_selection import GridSearchCV","58208992":"def gridsearch_cv_scores(models, params, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_gs_scores = {}\n    model_gs_best_param = {}\n    \n    for name, model in models.items():\n        gs_model = GridSearchCV(model,\n                                param_grid=params[name],\n                                scoring='f1',\n                                n_jobs=-1,\n                                cv=5,\n                                verbose=2)\n        \n        gs_model.fit(X_train,y_train)\n        y_pred = gs_model.predict(X_test)\n        model_gs_scores[name] = f1_score(y_test,y_pred)\n        model_gs_best_param[name] = gs_model.best_params_\n\n    model_gs_scores = pd.DataFrame(model_gs_scores, index=['F1'])\n    model_gs_scores = model_gs_scores.transpose().sort_values('F1')\n        \n    return model_gs_scores, model_gs_best_param","619d1d70":"models = {'GradientBoostingClassifier': GradientBoostingClassifier(),\n          }\n\nparams = {'GradientBoostingClassifier' : {'loss': ['exponential'],\n                                          'learning_rate': [0.04,0.05,0.06,0.07],\n                                          'n_estimators': [350,400,450,500],\n                                          'criterion': ['friedman_mse'],\n                                          'max_depth' : [12],\n                                          'ccp_alpha' : [0.0]\n                                          },\n          }","c2b3219c":"model_gs_scores_1, model_gs_best_param_1 = gridsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","abca6544":"model_gs_scores_1","2ea3d6ca":"model_gs_best_param_1","3c7d048a":"params = {'GradientBoostingClassifier' : {'loss': ['exponential'],\n                                          'learning_rate': [0.07,0.08,0.09],\n                                          'n_estimators': [500, 550, 600],\n                                          'criterion': ['friedman_mse'],\n                                          'max_depth' : [12],\n                                          'ccp_alpha' : [0.0]\n                                          },\n          }","d4188e20":"model_gs_scores_2, model_gs_best_param_2 = gridsearch_cv_scores(models, params, X_train, X_test, y_train, y_test)","22f79708":"model_gs_scores_2","1d757162":"model_gs_best_param_2","57004f55":"model = GradientBoostingClassifier(ccp_alpha=0.0, \n                                   criterion='friedman_mse', \n                                   learning_rate=0.08, \n                                   loss= 'exponential',\n                                   max_depth=12,\n                                   n_estimators=500)\nmodel.fit(X_train, y_train)\ny_preds = model.predict(X_test)","b9650467":"from sklearn.metrics import classification_report, plot_confusion_matrix, plot_roc_curve ","4c435367":"print(classification_report(y_test,y_preds))","8dccb7f8":"plot_confusion_matrix(model, X_test, y_test)","622e923f":"plot_roc_curve(model, X_test, y_test)","472d5b21":"from sklearn.model_selection import cross_val_score","2664f6ad":"def get_cv_score(model, X, y, cv=5):\n    \n    cv_accuracy = cross_val_score(model,X,y,cv=5,\n                         scoring='accuracy')\n    print(f'Cross Validaion accuracy Scores: {cv_accuracy}')\n    print(f'Cross Validation accuracy Mean Score: {cv_accuracy.mean()}')\n    \n    cv_precision = cross_val_score(model,X,y,cv=5,\n                         scoring='precision')\n    print(f'Cross Validaion precision Scores: {cv_precision}')\n    print(f'Cross Validation precision Mean Score: {cv_precision.mean()}')\n    \n    cv_recall = cross_val_score(model,X,y,cv=5,\n                         scoring='recall')\n    print(f'Cross Validaion recall Scores: {cv_recall}')\n    print(f'Cross Validation recall Mean Score: {cv_recall.mean()}')\n    \n    cv_f1 = cross_val_score(model,X,y,cv=5,\n                         scoring='f1')\n    print(f'Cross Validaion f1 Scores: {cv_f1}')\n    print(f'Cross Validation f1 Mean Score: {cv_f1.mean()}')   \n    \n    cv_merics = pd.DataFrame({'Accuracy': cv_accuracy.mean(),\n                         'Precision': cv_precision.mean(),\n                         'Recall': cv_recall.mean(),\n                         'f1': cv_recall.mean()},index=[0])\n    \n    return cv_merics","625eafa5":"cv_merics = get_cv_score(model, X, y, cv=5)","3cb004b7":"cv_merics","64731835":"plt.figure(figsize=(20,10))\nplt.title('CV Scores')\nsns.barplot(data=cv_merics);","65c3d490":"feat_importances = pd.DataFrame(model.feature_importances_, index=X.columns)","7aa49c80":"plt.figure(figsize=(20,10))\nplt.xticks(rotation=90)\nplt.title('Feature Importances')\nsns.barplot(data= feat_importances.sort_values(0).T);","43861aff":"## ROC Curve","a2a18da6":"# HR Analytics Job Prediction\n\nThis notebook is a workflow for various Python-based machine learning model for predicing if a person leave the company or will continue to work.\n\nGoing to take the following approach:\n\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Model Evaluation","d751cd4e":"## Confusion Matrix","cc62d120":"### RS model 2","34124c9e":"With the GradientBoostingClassifier model, we have managed to get scores of:\n* Accuracy: 0.990466\n* Precision: 0.987063\n* Recall: 0.97451 \n* f1: 0.97451","8b13bd65":"## Model Imports","811a6f3b":"## Standard Imports","18d6e586":"# 3. Evalutation\n\nCreating a Classification Model mainly Logisitic Regression model (we will also try other model) and to score it by classification metrics to check it's performance","ef039e59":"Since GradientBoostingClassifier is performing the best we will use that to perfrom a Grid Search to tune it's hyperparams.","cb05b7ba":"# 6. Model Evalution ","88611dfc":"# 2. Data\n\nhttps:\/\/www.kaggle.com\/mfaisalqureshi\/hr-analytics-and-job-prediction\n\n## Context\n\nHr Data Analytics\nThis dataset contains information about employees who worked in a company.\n\n## Content\n\nThis dataset contains columns: Satisfactory Level, Number of Project, Average Monthly Hours, Time Spend Company, Promotion Last 5\nYears, Department, Salary\n\n## Acknowledgements\n\nYou can download, copy and share this dataset for analysis and Predictions employees Behaviour.\n\n## Inspiration\n\nAnswer the following questions would be worthy\n1. Do Exploratory Data analysis to figure out which variables have a direct and clear impact on employee retention (i.e. whether they leave the company or continue to work)\n2. Plot bar charts showing the impact of employee salaries on retention\n3. Plot bar charts showing a correlation between department and employee retention\n4. Now build a logistic regression model using variables that were narrowed down in step 1\n5. Measure the accuracy of the model","67a3ebe9":"We will use the top 3 model to turn the hyperparameters and as in the task, we will also include the logisic Regression to compare the scores\n\n* LogisticRegression \t0.786000\n* GradientBoostingClassifier \t0.972444\n* DecisionTreeClassifier \t0.973556\n* RandomForestClassifier \t0.986889","4a8f1e47":"# 4. Features\n\n## Inputs \/ Features:\n\n* Satisfactory Levels\n* Number Project\n* Average Monthly Hour\n* Time Spent\n* Promotion last 5 years\n* Salary\n* Satisfactory Levels\n* Number Project\n* Average Monthly Hour\n* Time Spend\n\n# Output \/ label:\n* left\n","02e5cb66":"### GS Model 2","0a96c09f":"## Feature Importances","c7c9a181":"### RS model 1","8000da3f":"## Baseline Modelling","e6889556":"## Grid Search CV","66ac1365":"## HyperTurning via Random Seach CV\n\nAs the labels are in-balance in the dataset we will rate the scoring using the F1 score.","8726e013":"# 5. Modelling","1694ec21":"### GS Model 1","c9735db5":"## Calculate evalution metrices using cross-validation","6943caf6":"# 1. Problem Definition\n\nGiven the set of parameters, can we predict if a person leave the company or will continue to work?","40180b9b":"### RS Model 3","58c6e1b3":"## Classification Report ","47a848d3":"## Data Exploration (Exploratory Data Analysis (EDA) )"}}