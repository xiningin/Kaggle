{"cell_type":{"17673009":"code","9fe1e96f":"code","f180b834":"code","dbdb8a81":"code","6111e5f6":"code","191a54b2":"code","7f76c1b5":"code","98dccdda":"code","e4efd381":"code","91303ada":"code","f1a32b04":"code","f9a32190":"code","4f75447e":"code","fdf72379":"code","ac2df6fb":"code","6bb2b6a8":"code","8c6b6b45":"code","36a9c8b4":"code","ca5dfe5f":"code","60bcd753":"code","2743864e":"code","c03daef9":"code","6fcc034b":"code","d10efada":"code","448882bf":"code","5baeec0f":"code","37f38dc5":"code","b02cde4c":"code","d0838b89":"code","d086ee07":"code","e0dd4dc0":"code","faea2204":"code","c41677dc":"code","58c4a63c":"code","3dfef9f0":"code","69310178":"code","efceb5c0":"code","1a817797":"code","3775f088":"code","96ee2ba6":"code","df4201bd":"code","c75af958":"code","59d9a985":"code","bffd8789":"code","a7c82f76":"code","5374c222":"code","af15fc59":"code","6fadd587":"code","18094896":"code","6ae3ef67":"code","a25b2ade":"code","ac3253c4":"code","d488e97d":"code","7173d972":"code","ef3b8302":"code","a4198003":"code","8cc5393b":"code","586e6c33":"code","f6f1b9c6":"code","bbe3c1c5":"code","6761bfa0":"code","8114d320":"code","37420b44":"code","d34ea9bd":"code","cc7664fe":"code","2a4a17f2":"code","7b9be5bb":"code","02d20494":"code","8054bcdf":"code","2f25688c":"code","39fef840":"code","5d120c7e":"code","e98c9746":"code","7d8275d2":"code","e58e8ba6":"code","539e643b":"code","901777b7":"code","db01e9a6":"code","e712a454":"code","1b3bc284":"code","dc6e2038":"code","d5d83319":"markdown","f56514d3":"markdown","d6cbefa0":"markdown","8f7265d1":"markdown","dae6dbaf":"markdown","a033215a":"markdown"},"source":{"17673009":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9fe1e96f":"import tensorflow as tf","f180b834":"import pathlib","dbdb8a81":"os.getcwd()","6111e5f6":"train_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/train\/'\nval_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/val\/'\ntest_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/test\/'","191a54b2":"train_data_dir = pathlib.Path(train_path)\ntest_data_dir = pathlib.Path(test_path)\nval_data_dir = pathlib.Path(val_path)\ntrain_data_dir","7f76c1b5":"train_image_count = len(list(train_data_dir.glob('*\/*.jpeg')))\ntrain_image_count","98dccdda":"test_image_count = len(list(test_data_dir.glob('*\/*.jpeg')))\ntest_image_count","e4efd381":"val_image_count = len(list(val_data_dir.glob('*\/*.jpeg')))\nval_image_count","91303ada":"CLASS_NAMES = np.array([item.name for item in train_data_dir.glob('*')])\nCLASS_NAMES","f1a32b04":"BATCH_SIZE = 32\nIMAGE_SIZE = 224\nSTEPS_PER_EPOCH = np.ceil(train_image_count\/BATCH_SIZE)","f9a32190":"image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0\/255,validation_split=0.2)","4f75447e":"train_data_gen = image_generator.flow_from_directory(directory=str(train_data_dir),\n                                                     batch_size=BATCH_SIZE,\n                                                     shuffle=True,\n                                                     target_size=(IMAGE_SIZE, IMAGE_SIZE),\n                                                     class_mode='binary',\n                                                     subset='training')\n\n\nval_data_gen = image_generator.flow_from_directory(directory=str(train_data_dir),\n                                                     batch_size=BATCH_SIZE,\n                                                     shuffle=True,\n                                                     target_size=(IMAGE_SIZE, IMAGE_SIZE),\n                                                     class_mode='binary',\n                                                  subset='validation')\n    ","fdf72379":"test_data_gen = image_generator.flow_from_directory(directory=str(test_data_dir),\n                                                     batch_size=BATCH_SIZE,\n                                                     shuffle=True,\n                                                     target_size=(IMAGE_SIZE, IMAGE_SIZE),\n                                                     class_mode='binary',\n                                                     )","ac2df6fb":"train_data_gen.class_indices","6bb2b6a8":"images,labels = train_data_gen.next()\nprint(images.shape)\nprint(labels)","8c6b6b45":"IMAGE_SHAPE = (224,224,3)","36a9c8b4":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","ca5dfe5f":"# # Create the base model from the pre-trained model MobileNet V2\n# base_model = tf.keras.applications.MobileNetV2(input_shape=IMAGE_SHAPE,include_top=False,weights='imagenet')\n# base_model.trainable = False","60bcd753":"# base_model.summary()","2743864e":"IMAGE_SHAPE = (224,224,3)","c03daef9":"# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    base_model = tf.keras.applications.MobileNetV2(input_shape=IMAGE_SHAPE,include_top=False,weights='imagenet')\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(1)\n    ])\n    initial_epochs = 10\n    base_learning_rate = 0.001\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate),metrics=['accuracy'])","6fcc034b":"model.summary()","d10efada":"history = model.fit(train_data_gen,\n                    epochs=initial_epochs,\n                    validation_data=val_data_gen)","448882bf":"train_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/train\/'\nval_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/val\/'\ntest_path = '..\/input\/chest-xray-pneumonia\/chest_xray\/test\/'","5baeec0f":"train_data_dir = pathlib.Path(train_path)\ntest_data_dir = pathlib.Path(test_path)\nval_data_dir = pathlib.Path(val_path)\ntrain_data_dir","37f38dc5":"train_list_ds = tf.data.Dataset.list_files(str(train_data_dir\/'*\/*'))\nval_list_ds = tf.data.Dataset.list_files(str(val_data_dir\/'*\/*'))\ntest_list_ds = tf.data.Dataset.list_files(str(test_data_dir\/'*\/*'))\n","b02cde4c":"CLASS_NAMES = np.array([item.name for item in train_data_dir.glob('*') if item.name != \"LICENSE.txt\"])\nCLASS_NAMES","d0838b89":"for i in train_list_ds:\n    print(i)\n    img = tf.io.read_file(i)\n    img = tf.image.decode_jpeg(img, channels=3)\n    print(img.shape)\n    break\nparts = tf.strings.split(i, os.path.sep)\nprint(parts[-2])\nprint(parts[-1])\nprint(parts[-2]==CLASS_NAMES)","d086ee07":"IMAGE_SIZE=224\nBATCH_SIZE=32","e0dd4dc0":"def get_label(file_path):\n  # convert the path to a list of path components\n  parts = tf.strings.split(file_path, os.path.sep)\n  # The second to last is the class-directory\n  return parts[-2] == CLASS_NAMES","faea2204":"def decode_img(img):\n  # convert the compressed string to a 3D uint8 tensor\n  img = tf.image.decode_jpeg(img, channels=3)\n  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n  img = tf.image.convert_image_dtype(img, tf.float32)\n  # resize the image to the desired size.\n  return tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE])","c41677dc":"def process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label","58c4a63c":"AUTOTUNE = tf.data.experimental.AUTOTUNE","3dfef9f0":"# Set `num_parallel_calls` so multiple images are loaded\/processed in parallel.\ntrain_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\nval_labeled_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\ntest_labeled_ds = test_list_ds.map(process_path,num_parallel_calls=AUTOTUNE)","69310178":"for image, label in train_labeled_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","efceb5c0":"train_ds = train_labeled_ds.shuffle(1000).batch(BATCH_SIZE)\nval_ds = val_labeled_ds.shuffle(1000).batch(16)\ntest_ds = test_labeled_ds.shuffle(1000).batch(BATCH_SIZE)","1a817797":"image_batch, label_batch = next(iter(train_ds))","3775f088":"label_batch","96ee2ba6":"label_batch[0]","df4201bd":"image_batch, label_batch = next(iter(val_ds))","c75af958":"image_batch.shape\n","59d9a985":"arr = label_batch.numpy()\narr","bffd8789":"import matplotlib.pyplot as plt","a7c82f76":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10,10))\n    for n in range(16):\n        ax = plt.subplot(4,4,n+1)\n        plt.imshow(image_batch[n])\n        plt.title(CLASS_NAMES[label_batch[n]==1][0].title())\n        plt.axis('off')\n        \n\nshow_batch(image_batch.numpy(),label_batch.numpy())","5374c222":"arr[0]","af15fc59":"print(arr[0]==0)\nprint(arr[0]==1)","6fadd587":"print(CLASS_NAMES[arr[0]==1])\nprint(CLASS_NAMES[arr[0]==1][0])\nprint(CLASS_NAMES[arr[0]==1][0].title())","18094896":"image_batch[0].numpy().max()","6ae3ef67":"IMAGE_SHAPE = (IMAGE_SIZE,IMAGE_SIZE,3)\nIMAGE_SHAPE","a25b2ade":"# Create the base model from the pre-trained model MobileNet V2\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMAGE_SHAPE,include_top=False,weights='imagenet')","ac3253c4":"base_model.trainable = False","d488e97d":"base_model.summary()","7173d972":"model = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    #tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512,activation='relu'),\n    tf.keras.layers.Dense(2,activation='softmax')\n])","ef3b8302":"model.summary()","a4198003":"base_learning_rate = 0.001\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n              metrics=['accuracy'])","8cc5393b":"model.summary()","586e6c33":"initial_epochs = 10\n#validation_steps=20\n\nhistory = model.fit(train_ds,\n                    epochs=initial_epochs,\n                    validation_data=val_ds)","f6f1b9c6":"base_model.trainable = True\n# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(base_model.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 100\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","bbe3c1c5":"model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              metrics=['accuracy'])","6761bfa0":"model.summary()","8114d320":"# b = 0 \n# for i,l in train_ds:\n#     b+=1\n# print(b,STEPS_PER_EPOCH)","37420b44":"initial_epochs = 10\n#validation_steps=20\n\nhistory = model.fit(train_ds,\n                    epochs=initial_epochs,\n                    validation_data=val_ds)","d34ea9bd":"model.evaluate(test_ds)","cc7664fe":"image,label = next(iter(test_ds))","2a4a17f2":"model.evaluate(image,label)","7b9be5bb":"predict = model.predict(image)","02d20494":"predict","8054bcdf":"label","2f25688c":"model1 = tf.keras.Sequential([\n    base_model,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(2,activation='softmax')\n])\n\nmodel1.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              metrics=['accuracy'])\n\nmodel1.summary()","39fef840":"initial_epochs = 5\n#validation_steps=20\n\nhistory = model1.fit(train_ds,\n                    epochs=initial_epochs,\n                    validation_data=val_ds)","5d120c7e":"vgg_base_model = tf.keras.applications.VGG16(input_shape=IMAGE_SHAPE,include_top=False,weights='imagenet')","e98c9746":"vgg_base_model.trainable = False","7d8275d2":"vgg_base_model.summary()","e58e8ba6":"vgg_model = tf.keras.models.Sequential([\n    vgg_base_model,\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(2,activation='softmax')\n])","539e643b":"vgg_model.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","901777b7":"initial_epochs = 5\n\nvgg_model.fit(train_ds,\n                     epochs=initial_epochs,\n                     validation_data=val_ds)","db01e9a6":"len(vgg_base_model.layers)","e712a454":"vgg_base_model.trainable = True\n\n\n# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(vgg_base_model.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 12\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in vgg_base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","1b3bc284":"vgg_model.compile(optimizer='adam',\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nvgg_model.summary()","dc6e2038":"fine_tune_epochs = 5\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = vgg_model.fit(train_ds,\n                         epochs=total_epochs,\n                         initial_epoch =  5,\n                         validation_data=val_ds)","d5d83319":"# TRY DIFF. MODELS","f56514d3":"## VGG16 Model","d6cbefa0":"# USING TPU","8f7265d1":"## Decode Images","dae6dbaf":"### Fine Tunning","a033215a":"# IGNORE ALL  FROM HERE"}}