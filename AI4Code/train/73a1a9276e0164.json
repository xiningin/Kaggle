{"cell_type":{"5c12745d":"code","33abdf3b":"code","4c26547d":"code","2a8b0184":"code","1e4f808b":"code","bf7751db":"code","2fee389b":"code","b6998477":"code","203a5de0":"code","bb46560d":"code","dbd59a85":"code","f49f3fd8":"code","14b7b13c":"code","4d9f1876":"code","69f07a18":"code","95f895be":"code","9e82fd88":"code","afae5128":"code","b8b5f4f4":"code","38c4a40e":"code","ff000320":"code","953b12c8":"code","3f290763":"code","315d4bff":"code","c4090510":"code","56e0a284":"code","cee32d0c":"code","8527a83e":"code","0c461efc":"code","496b5631":"code","55cb48e5":"code","b1faa7e9":"code","b05ea1a9":"code","6ccf43fe":"code","5d492956":"code","e013097e":"code","61236d37":"code","7da882db":"code","7aedfca4":"code","ac77cbae":"code","34b124d1":"code","83c842e9":"code","86772324":"code","baadfa9b":"code","53ca53e0":"code","ddc09595":"code","72d8ab1c":"code","e92f8f9b":"code","2ff0a353":"code","a95a0633":"code","c54900cd":"code","182177d8":"code","b2a8f636":"code","24925e13":"markdown","3f325759":"markdown","521929e2":"markdown","b4e77738":"markdown","da0c5b3f":"markdown","e8630bef":"markdown","75b90fb5":"markdown","f97b9bbd":"markdown","f3617506":"markdown","eb83826c":"markdown"},"source":{"5c12745d":"!pip install torchtext==0.2.3\n!pip install fastai==0.7.0","33abdf3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt, rcParams, animation\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom fastai.imports import *\nfrom fastai.metrics import *\nfrom fastai.model import *\nfrom fastai.dataset import *\nfrom fastai.torch_imports import *\nfrom fastai.io import *\nimport torch.nn as nn\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4c26547d":"#data label\nlabels = ['T-shirt\/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']","2a8b0184":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100): \n        display(df)\ndef split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\ndef show(img, title=None):\n    plt.imshow(img, cmap=\"gray\")\n    if title is not None: plt.title(labels[int(title)])\n\ndef plots(ims, figsize=(12,6), rows=2, titles=None):\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(labels[int(titles[i])], fontsize=16)\n        plt.imshow(ims[i], cmap='gray')","1e4f808b":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","bf7751db":"PATH = '..\/input\/'\n!ls {PATH}\n","2fee389b":"df_raw = pd.read_csv(f'{PATH}fashion-mnist_train.csv', low_memory=False)","b6998477":"display_all(df_raw.head().T)","203a5de0":"# Randomly sampling the dataset\ndf_raw = df_raw.sample(frac=1)","bb46560d":"# Setting the validation size to 10,000\nn_valid = 10000\nn_train = len(df_raw) - n_valid","dbd59a85":"y, x = df_raw['label'].values, df_raw.loc[:, df_raw.columns != 'label'].values","f49f3fd8":"x_train, x_valid = split_vals(x, n_train)\ny_train, y_valid = split_vals(y, n_train)","14b7b13c":"x_train.shape, x_valid.shape, y_train.shape, y_valid.shape","4d9f1876":"mean = x_train.mean()\nstd = x_train.std()\n\nx_train=(x_train-mean)\/std\nmean, std, x_train.mean(), x_train.std()","69f07a18":"# To maintain consistency we subtract and divide the validation set data with the mean and standard deviation of training data\nx_valid = (x_valid-mean)\/std\nx_valid.mean(), x_valid.std()","95f895be":"x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape","9e82fd88":"np.unique(y_train), np.unique(y_valid)","afae5128":"show(x_imgs[14], y_valid[14])","b8b5f4f4":"x_imgs[0,10:15,10:15]","38c4a40e":"show(x_imgs[0,10:15,10:15])","ff000320":"plots(x_imgs[:10], titles=y_valid[:10])","953b12c8":"# loss function\ndef binary_loss(y, p):\n    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))","3f290763":"from fastai.dataset import *\nmd = ImageClassifierData.from_arrays(PATH, (x_train,y_train), (x_valid, y_valid))","315d4bff":"import torch.nn as nn\nnet = nn.Sequential(\n    nn.Linear(28*28, 10),\n    nn.LogSoftmax()\n).cuda()","c4090510":"loss=nn.NLLLoss()\nmetrics=[accuracy]\n# setting learning rate as 0.1\nopt=optim.SGD(net.parameters(), 1e-1)","56e0a284":"fit(net, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)","cee32d0c":"set_lrs(opt, 1e-2)","8527a83e":"fit(net, md, n_epochs=3, crit=loss, opt=opt, metrics=metrics)","0c461efc":"preds = predict(net, md.val_dl)","496b5631":"preds.shape","55cb48e5":"preds.argmax(axis=1)[:5]","b1faa7e9":"preds = preds.argmax(1)","b05ea1a9":"np.mean(preds == y_valid)","6ccf43fe":"plots(x_imgs[:8], titles=preds[:8])","5d492956":"def get_weights(*dims): return nn.Parameter(torch.randn(dims)\/dims[0])\ndef softmax(x): return torch.exp(x)\/(torch.exp(x).sum(dim=1)[:,None])\n\nclass LogReg(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1_w = get_weights(28*28, 10)  # Layer 1 weights\n        self.l1_b = get_weights(10)         # Layer 1 bias\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = (x @ self.l1_w) + self.l1_b  # Linear Layer\n        x = torch.log(softmax(x)) # Non-linear (LogSoftmax) Layer\n        return x","e013097e":"net2 = LogReg().cuda()\nopt=optim.SGD(net2.parameters(), 1e-2)","61236d37":"fit(net2, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)","7da882db":"preds = predict(net2, md.val_dl).argmax(1)\nplots(x_imgs[35:45], titles=preds[35:45])","7aedfca4":"np.mean(preds == y_valid)","ac77cbae":"opt=optim.SGD(net2.parameters(), 1e-2, weight_decay=1e-3)","34b124d1":"fit(net2, md, n_epochs=5, crit=loss, opt=opt, metrics=metrics)","83c842e9":"preds = predict(net2, md.val_dl).argmax(1)\nplots(x_imgs[35:45], titles=preds[35:45])","86772324":"np.mean(preds == y_valid)","baadfa9b":"net3 = LogReg().cuda()\nloss=nn.NLLLoss()\nlearning_rate = 1e-2\noptimizer=optim.SGD(net3.parameters(), lr=learning_rate)","53ca53e0":"def score(x, y):\n    y_pred = to_np(net3(V(x)))\n    return np.sum(y_pred.argmax(axis=1) == to_np(y))\/len(y_pred)","ddc09595":"# Fit function unboxed\ndef train(no_of_epochs=10):\n    for epoch in range(no_of_epochs):\n        losses=[]\n        dl = iter(md.trn_dl)\n        for t in range(len(md.trn_dl)):\n            # Forward pass: compute predicted y and loss by passing x to the model.\n            xt, yt = next(dl)\n            y_pred = net3(V(xt))\n            l = loss(y_pred, V(yt))\n            losses.append(l)\n\n            # Before the backward pass, use the optimizer object to zero all of the\n            # gradients for the variables it will update (which are the learnable weights of the model)\n            optimizer.zero_grad()\n\n            # Backward pass: compute gradient of the loss with respect to model parameters\n            l.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n\n        val_dl = iter(md.val_dl)\n        val_scores = [score(*next(val_dl)) for i in range(len(md.val_dl))]\n        print(np.mean(val_scores))","72d8ab1c":"train(1)","e92f8f9b":"train(3)","2ff0a353":"net4 = LogReg().cuda()\nloss_fn = nn.NLLLoss()\nlr = 1e-2\nw,b = net4.l1_w, net4.l1_b","a95a0633":"def score(x, y):\n    y_pred = to_np(net4(V(x)))\n    return np.sum(y_pred.argmax(axis=1) == to_np(y))\/len(y_pred)","c54900cd":"def train(no_epochs):\n    for epoch in range(no_epochs):\n        losses = []\n        dl = iter(md.trn_dl)\n        for t in range(len(md.trn_dl)):\n            xt, yt = next(dl)\n            y_pred = net4(V(xt))\n\n            # compute the loss\n            l = loss(y_pred, Variable(yt).cuda())\n            losses.append(loss)\n            \n            # computing gradient of the loss with respect to parameter\n            l.backward()\n\n            # updating the parameters with gradient\n            w.data -= w.grad.data * lr\n            b.data -= b.grad.data * lr\n            \n            #Initialize the gradient to zeros\n            w.grad.data.zero_()\n            b.grad.data.zero_()\n            \n        val_dl = iter(md.val_dl)\n        val_scores = [score(*next(val_dl)) for i in range(len(md.val_dl))]\n        print(np.mean(val_scores))","182177d8":"train(1)","b2a8f636":"train(3)","24925e13":"**Unboxing the fit function**","3f325759":"**Normalizing the data**","521929e2":"**Creating training and validation set**","b4e77738":"**Defining Logistic Regression**","da0c5b3f":"**Logisitic Regression: Neural Network with no hidden layer using Pytorch**","e8630bef":"* Working on Logistic Regression with FashionMNIST data.\n* We will first use the custom packages. \n* Then we will unbox each of the function by creating our own functions ","75b90fb5":"**Unboxing the SGD (Stochastic Gradient Descent) optimiser**","f97b9bbd":"**With Weight Pytorch Decay**","f3617506":"**Plotting the data**","eb83826c":"Lets see the predictions"}}