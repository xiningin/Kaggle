{"cell_type":{"67e7b533":"code","9c8ea9c2":"code","c50e742b":"code","a2bb08b8":"code","53490800":"code","28246251":"code","97c8d00d":"code","facd2a7d":"code","046e1228":"code","a406600f":"code","aff18056":"code","72d142e2":"code","4ed33c3a":"code","4dcff841":"code","f1b752e4":"code","876eee10":"code","02be340f":"code","51b8510e":"code","dec3556a":"code","bb7a5fcc":"code","1f80f242":"code","0506efb0":"code","810f7498":"code","584d6ff7":"code","760e67f9":"code","516055a3":"code","72ad084d":"code","f976c655":"code","e85db8e4":"code","13e405d3":"code","a67e4e88":"code","77713b07":"code","62ec8020":"code","1b69129d":"code","3625d2be":"code","4f17b4b0":"code","ca09a859":"code","d05b9ec2":"code","6180117f":"code","bd2b045e":"code","60069ffa":"code","e8a93020":"code","dcb62539":"code","c815899c":"code","4bd7bc82":"code","fa3e8477":"code","10695088":"code","541ed96d":"code","fbeff30b":"code","a501da80":"code","cf6bd1e1":"code","5f0bd15b":"code","7e9aaf25":"code","adcb50bf":"code","0577bc72":"code","44314749":"code","5a4e50f0":"code","ebd2a584":"code","72d116b3":"code","b53dea4a":"code","24e6ffa4":"code","1886f284":"code","307aadd1":"code","bce0a4ec":"code","d6318032":"code","3fcd2dde":"code","8194b065":"code","011f53e3":"code","06e4d915":"code","456ec276":"code","b32b8a16":"code","68311c27":"code","7762d235":"code","194045aa":"code","a9c1cbd7":"code","50798431":"markdown","546f655c":"markdown","15c45581":"markdown","528742ce":"markdown","ca9b6a50":"markdown","b063438d":"markdown","7c5d9155":"markdown","cddfdca9":"markdown","f4632aa5":"markdown","acc9c5e6":"markdown","548b33ac":"markdown","c5e009bd":"markdown","2baaea13":"markdown","a845faf5":"markdown","cf042b2a":"markdown","36347834":"markdown","952001c5":"markdown","a3c7ea08":"markdown","5e758beb":"markdown","498742fc":"markdown","e6e0ea1b":"markdown","c445c6c3":"markdown","3794781f":"markdown","e42a8b61":"markdown","9a115805":"markdown"},"source":{"67e7b533":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tensorflow.python.saved_model import tag_constants\nfrom sklearn.metrics import confusion_matrix","9c8ea9c2":"train = pd.read_csv('..\/input\/train.csv')","c50e742b":"train.head()","a2bb08b8":"train.info()","53490800":"train.columns\nm = train.shape[0]","28246251":"catCols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nj = 0\nfor i in catCols:\n    plt.figure(j)\n    sns.barplot(x = i, y = 'Survived', data = train)\n    plt.show()\n    j+=1","97c8d00d":"test = pd.read_csv('..\/input\/test.csv')","facd2a7d":"dfs = [train,test]\ndata = pd.concat(dfs,ignore_index=True)","046e1228":"data.head()","a406600f":"def getTitles(names):\n    titleRegex = re.compile(r',.\\w+\\.')    \n    title = []\n    for str in names:\n        titlePat = re.search(titleRegex,str)\n        if titlePat is None:\n            title.append(str)\n        else:\n            x = titlePat.group()\n            x = x[2:len(x)-1]\n            title.append(x)\n    return title\n\n\ntitle = getTitles(data['Name'])","aff18056":"set(title)","72d142e2":"def getCleanTitles(title):\n    for i in range(len(title)):\n        if title[i] in ['Don', 'Sir', 'Jonkheer']:\n            title[i] = 'Noble'\n        elif title[i] in ['Rothes, the Countess. of (Lucy Noel Martha Dyer-Edwards)', 'Lady', 'Dona']:\n            title[i] = 'Noble'\n        elif title[i] in ['Mlle', 'Ms']:\n            title[i] = 'Miss'\n        elif title[i] == 'Mme':\n            title[i] = 'Mrs'\n        elif title[i] in ['Capt', 'Col', 'Dr', 'Major', 'Rev']:\n            title[i] = 'Other'\n    return title\n\ndata['Title'] = getCleanTitles(title)","4ed33c3a":"data.head()","4dcff841":"data.info()","f1b752e4":"data.groupby('Title').Age.mean()","876eee10":"data['Age'].fillna(data.groupby('Title')['Age'].transform(\"mean\"), inplace=True)","02be340f":"data.loc[pd.isnull(data['Embarked'])]","51b8510e":"data.loc[61,'Embarked'] = 'S'\ndata.loc[829,'Embarked'] = 'S'","dec3556a":"data['Fare'].fillna(data['Fare'].mean(), inplace = True)","bb7a5fcc":"num_family = (data['Parch'] + data['SibSp']).astype(int)","1f80f242":"data['num_family'] = num_family","0506efb0":"sns.barplot(x = 'num_family', y = 'Survived', data = data[:m])\nplt.show()","810f7498":"data.columns","584d6ff7":"catCols.extend(['Title', 'num_family'])","760e67f9":"catCols","516055a3":"def convertCatValToNum(catVal):\n    le = LabelEncoder()\n    le.fit(catVal)\n    catVal = le.transform(catVal)\n    return catVal\n\n\nfor i in catCols:\n    data[i] = convertCatValToNum(data[i])","72ad084d":"data.columns","f976c655":"Xcols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Title', 'num_family']","e85db8e4":"data[Xcols].info()","13e405d3":"scaler = MinMaxScaler()\nscaler.fit(data[Xcols])\nX = scaler.transform(data[Xcols])","a67e4e88":"# Check if the features have been correctly scaled\n\nX_stats = pd.DataFrame()\nX_stats['Min'] = np.min(X, axis = 0)\nX_stats['Max'] = np.max(X, axis = 0)\nX_stats['Mean'] = np.mean(X, axis = 0)\nX_stats","77713b07":"y = np.expand_dims(data[:m].Survived.values,1)","62ec8020":"y.shape","1b69129d":"# Save preprocessed data to file\n\nX_file = 'X.npy'\n#np.save(X_file, X)\n\ny_file = 'y.npy'\n#np.save(y_file, y)","3625d2be":"# Load preprocessed data from file\n\n\n#X = np.load(X_file)\n\n#y = np.load(y_file)","4f17b4b0":"# Set random seed\n\nseed = 2\nnp.random.seed(seed)\n\n# Get random training index\n\ntrain_index = np.random.choice(m, round(m*0.9), replace=False)\ndev_index = np.array(list(set(range(m)) - set(train_index)))\n\ntest_index = range(m, data.shape[0])\n# Make training and dev\n\n\nX_train = X[train_index]\nX_dev = X[dev_index]\nX_test = X[test_index]\n\ny_train = y[train_index]\ny_dev = y[dev_index]\n","ca09a859":"y_dev.shape","d05b9ec2":"# Initialize placeholders for data\nn = X.shape[1]\ntf.reset_default_graph()\nx = tf.placeholder(dtype=tf.float32, shape=[None, n], name = 'inputs_ph')\ny = tf.placeholder(dtype=tf.float32, shape=[None, 1], name = 'labels_ph')","6180117f":"# number of neurons in each layer\n\ninput_num_units = n\nhidden_num_units_1 = 224\nhidden_num_units_2 = 120\nhidden_num_units_3 = 56\noutput_num_units = 1","bd2b045e":"# Build Neural Network Weights\ninitializer = tf.contrib.layers.xavier_initializer()\nweights = {\n    'hidden1': tf.Variable(initializer([input_num_units, hidden_num_units_1])),\n    'hidden2': tf.Variable(initializer([hidden_num_units_1, hidden_num_units_2])),\n    'hidden3': tf.Variable(initializer([hidden_num_units_2, hidden_num_units_3])),\n    'output': tf.Variable(initializer([hidden_num_units_3, output_num_units])),\n}\n\nbiases = {\n    'hidden1': tf.Variable(initializer([hidden_num_units_1])),\n    'hidden2': tf.Variable(initializer([hidden_num_units_2])),\n    'hidden3': tf.Variable(initializer([hidden_num_units_3])),\n    'output': tf.Variable(initializer([output_num_units])),\n}","60069ffa":"# Build model \n\nkeep_prob_1 = tf.placeholder(dtype=tf.float32, name = 'keep_prob_1')\nkeep_prob_2 = tf.placeholder(dtype=tf.float32, name = 'keep_prob_2')\nkeep_prob_3 = tf.placeholder(dtype=tf.float32, name = 'keep_prob_3')\n\nhidden_1_layer = tf.add(tf.matmul(x, weights['hidden1']), biases['hidden1'])\nhidden_1_layer = tf.nn.dropout(tf.nn.relu(hidden_1_layer),keep_prob = keep_prob_1)\nhidden_2_layer = tf.add(tf.matmul(hidden_1_layer, weights['hidden2']), biases['hidden2'])\nhidden_2_layer = tf.nn.dropout(tf.nn.relu(hidden_2_layer),keep_prob = keep_prob_2)\nhidden_3_layer = tf.add(tf.matmul(hidden_2_layer, weights['hidden3']), biases['hidden3'])\nhidden_3_layer = tf.nn.dropout(tf.nn.relu(hidden_3_layer),keep_prob = keep_prob_3)\n\noutput_layer = tf.matmul(hidden_3_layer, weights['output']) + biases['output']\n","e8a93020":"# Set hyperparameters\n\nlearning_rate = 7e-5\nepochs = 1000","dcb62539":"# Set loss function and goal i.e. minimize loss\n\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_layer, labels=y))\nopt = tf.train.AdamOptimizer(learning_rate)\ngoal = opt.minimize(loss)","c815899c":"prediction = tf.round(tf.nn.sigmoid(output_layer), name = 'prediction')\ncorrect = tf.cast(tf.equal(prediction, y), dtype=tf.float32)\nrecall = tf.metrics.recall(labels = y, predictions = prediction)\naccuracy = tf.reduce_mean(correct)","4bd7bc82":"# Initialize lists to store loss and accuracy while training the model\n\nloss_trace = []\ntrain_acc = []\ndev_acc = []","fa3e8477":"# Start tensorflow session\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)","10695088":"# Train the model\n\nfor epoch in range(epochs):\n    \n    sess.run(goal, feed_dict={x: X_train, y: y_train, keep_prob_1: 0.9, keep_prob_2: 0.8, keep_prob_3: 0.7})\n\n    # calculate results for epoch\n    \n    temp_loss = sess.run(loss, feed_dict={x: X_train, y: y_train, keep_prob_1: 1, keep_prob_2: 1, keep_prob_3: 1})\n    temp_train_acc = sess.run(accuracy, feed_dict={x: X_train, y: y_train, keep_prob_1: 1, keep_prob_2: 1, keep_prob_3: 1})\n    temp_dev_acc = sess.run(accuracy, feed_dict={x: X_dev, y: y_dev, keep_prob_1: 1, keep_prob_2: 1, keep_prob_3: 1})\n\n    # save results in a list\n\n    loss_trace.append(temp_loss)\n    train_acc.append(temp_train_acc)\n    dev_acc.append(temp_dev_acc)\n\n    # output\n\n    if (epoch + 1) % 200 == 0:\n        print('epoch: {:4d} loss: {:5f} train_acc: {:5f} dev_acc: {:5f}'.format(epoch + 1, temp_loss, temp_train_acc, temp_dev_acc))\n\n","541ed96d":"plt.plot(loss_trace)","fbeff30b":"y_train_preds_nn = sess.run(prediction, feed_dict ={x: X_train, keep_prob_1: 1, keep_prob_2: 1, keep_prob_3: 1})\ny_dev_preds_nn = sess.run(prediction, feed_dict ={x: X_dev, keep_prob_1: 1, keep_prob_2: 1, keep_prob_3: 1})\ny_test_preds_nn = sess.run(prediction, feed_dict ={x: X_test, keep_prob_1: 1, keep_prob_2: 1, keep_prob_3: 1})","a501da80":"sess.close()","cf6bd1e1":"def get_recall(labels, preds):\n    tp = int(np.dot(labels.T,preds))\n    fn = int(np.dot(labels.T,1-preds))\n    recall = tp\/(tp+fn)\n    return recall","5f0bd15b":"recall_nn = get_recall(y_train, y_train_preds_nn)\nrecall_nn","7e9aaf25":"test['Survived_nn'] = y_test_preds_nn.astype(int)","adcb50bf":"test[['PassengerId', 'Survived_nn']].to_csv('submission_nn.csv', index = False, header = ['PassengerId', 'Survived'])","0577bc72":"svclassifier = SVC()\nsvclassifier.fit(X_train, y_train) ","44314749":"y_train_preds_svm = np.expand_dims(svclassifier.predict(X_train),1)\ntrain_acc_svm = np.mean(y_train == y_train_preds_svm)\nprint('Train Accuracy for SVM: {:5f}'.format(train_acc_svm))","5a4e50f0":"y_test_preds_svm = svclassifier.predict(X_test)","ebd2a584":"recall_svm = get_recall(y_train, y_train_preds_svm)\nrecall_svm","72d116b3":"test['Survived_svm'] = y_test_preds_svm.astype(int)","b53dea4a":"test[['PassengerId', 'Survived_svm']].to_csv('submission_svm.csv', index = False, header = ['PassengerId', 'Survived'])","24e6ffa4":"rfclassifier = RandomForestClassifier(n_estimators = 100, max_features = 3)\nrfclassifier.fit(X_train, y_train) ","1886f284":"y_train_preds_rf = np.expand_dims(rfclassifier.predict(X_train),1)\ntrain_acc_rf = np.mean(y_train == y_train_preds_rf)\nprint('Train Accuracy for Random Forest: {:5f}'.format(train_acc_rf))","307aadd1":"y_dev_preds_rf = np.expand_dims(rfclassifier.predict(X_dev),1)\ndev_acc_rf = np.mean(y_dev == y_dev_preds_rf)\nprint('Dev Accuracy for Random Forest: {:5f}'.format(dev_acc_rf))","bce0a4ec":"recall_rf = get_recall(y_train, y_train_preds_rf)\nrecall_rf","d6318032":"y_test_preds_rf = rfclassifier.predict(X_test)","3fcd2dde":"test['Survived_rf'] = y_test_preds_rf.astype(int)","8194b065":"test[['PassengerId', 'Survived_rf']].to_csv('submission_rf.csv', index = False, header = ['PassengerId', 'Survived'])","011f53e3":"test.columns","06e4d915":"test['Survived_nn_wtd'] = test['Survived_nn']*recall_nn\ntest['Survived_svm_wtd'] = test['Survived_svm']*recall_svm\ntest['Survived_rf_wtd'] = test['Survived_nn']*recall_rf","456ec276":"y_test_preds_avg = np.round(np.mean(test[['Survived_nn', 'Survived_svm','Survived_rf']],axis = 1))","b32b8a16":"y_test_preds_wtd_avg = np.round(np.mean(test[['Survived_nn_wtd', 'Survived_svm_wtd','Survived_rf_wtd']],axis = 1))","68311c27":"test['Survived_avg'] = y_test_preds_avg.astype(int)","7762d235":"test['Survived_wtd_avg'] = y_test_preds_wtd_avg.astype(int)","194045aa":"test[['PassengerId', 'Survived_avg']].to_csv('submission_avg.csv', index = False, header = ['PassengerId', 'Survived'])","a9c1cbd7":"test[['PassengerId', 'Survived_wtd_avg']].to_csv('submission_wtd_avg.csv', index = False, header = ['PassengerId', 'Survived'])","50798431":"## Step 6: Convert data to categorical variables","546f655c":"## Step 3: Extract titles from names","15c45581":"## Step 1: See correlation between survived and other columns","528742ce":"## Step 4: Fill in missing data\n\nThe more data we have available, the better the predictions can be. Hence, instead of discarding rows with data missing in some columns, we will try to fill it with some reasonable value so that the rest of the data in the row does not go to waste.","ca9b6a50":"Since PassengerId and name is unique to an individual, there is no relation that can be drawn with 'Survived'. \n\nHowever, Pclass, Sex, SibSp, Parch, and Embarked are categorical variables with distinct values that can impact 'Survived'.","b063438d":"There are 2 missing values in Embarked which can be found online","7c5d9155":"## Step 7: Normalize data to be between 0 and 1\n\nSince our output is binary i.e. either 0 or 1, training a machine learning algorithm is faster when inputs are also in the same range.","cddfdca9":"Since the confusion matrix only has diagonal elements, the predictions match.","f4632aa5":"### A. Age\n\nAge is closely associated with Title, hence we will use the mean of ages for titles to replace missing values for rows with those titles.","acc9c5e6":"Out of the above columns we will select only those we think affect the outcome i.e. 'Survived' column. ","548b33ac":"### B. Embarked\n","c5e009bd":"## Step 10: Set up neural network to predict y","2baaea13":"The names begin with certain titles - Master\/Miss for children and Mr. \/ Mrs. for adults. These can be extracted to create a new feature using RegEx","a845faf5":"## Step 2: Combine train and test dataset\n\nWe will be transforming the training set to make it suitable for use in a machine learning model. Eg. convert string variables to categorical variables. The words 'male' and 'female' cannot be directly used for input. Instead they have to be replaced by 0 and 1 to feed to the model.\n\nThe same transformations also need to be applied on the test set because the trained model expects the input in this format. The best way to achieve this is to combine the two dataset and apply all transformations together. ","cf042b2a":"As per this source:\n\nhttps:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html\n\nhttps:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html","36347834":"## Step 5: Add variable for number of family members on board\n\nThe chances of survival go up for families. To find if someone is alone, we check if the sum of SibSp and Parch is 0.","952001c5":"### C. Fare\n\nFare still has one missing value. Fill it with the mean","a3c7ea08":"## Step 11: Create submission file and submit","5e758beb":"As it can be seen, there are several missing data points. Specifically, in age and cabin. \n\nSince, we need to predict the 'Survived' column based on other columns, we will need as much data as possible. Steps to do this:\n\n1. See correlation between survived and other columns\n2. Combine train and test dataset\n3. Extract titles from names\n4. Fill in missing data\n    1. Age\n    2. Embarked\n    3. Fare\n5. Add is_alone variable\n6. Convert data to categorical variables\n7. Normalize data to be between 0 and 1\n8. Separate labels (y) from data\n9. Create training and dev sets\n10. Set up then neural networks to predict y\n11. Create submission file and submit\n12. Try Other Models - SVM and Random Forest\n13. Average All Predictions","498742fc":"# Step 13: Average All Predictions","e6e0ea1b":"Since Cabin has several missing values, we will not include it in our list of features. However, we can try filling in missing values in Age, Embarked and Fare.","c445c6c3":"## Step 12: Try Other Models - SVM and Random Forest","3794781f":"## Step 8: Separate labels (y) from data","e42a8b61":"We can use all these variables as they all seem to have an impact on the survival of the passenger.","9a115805":"## Step 9: Create training and dev sets\n\nValidation (dev) set is useful to know how the model performs on data it has not seen. We will randomly select 10% of the data as a validation set. The test set is already provided separately by Kaggle."}}