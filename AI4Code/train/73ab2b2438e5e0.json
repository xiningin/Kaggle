{"cell_type":{"f2189a50":"code","c725bd68":"code","6fdacacd":"code","3042e046":"code","286f2743":"code","aac0bdac":"code","eaf05296":"code","fb414de7":"code","b8ad0932":"code","937ab9e9":"code","1d4c66f9":"code","7d611bcc":"code","b8d6a6d5":"code","8441e524":"code","14dbfb53":"markdown","51c4ea20":"markdown","485d0d63":"markdown","6884f644":"markdown","600bf587":"markdown","fcb40d47":"markdown","e689c3af":"markdown"},"source":{"f2189a50":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom joblib import dump, load\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import load_model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.activations import relu\nfrom tensorflow.keras.metrics import RootMeanSquaredError as RMSE\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.resnet_v2 import ResNet101V2, ResNet50V2\nfrom tensorflow.keras.layers import Dropout, Conv2D, Dense, MaxPool2D, Flatten","c725bd68":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nprint(train.shape, test.shape)\n\ntrain_images_path = '..\/input\/petfinder-pawpularity-score\/train'\ntest_images_path = '..\/input\/petfinder-pawpularity-score\/test'\n\ntrain_images_list = os.listdir(train_images_path)\ntest_images_list = os.listdir(test_images_path)\ntrain_label = train['Pawpularity']\n\nprint(len(train_images_list),len(test_images_list), train_label.shape)","6fdacacd":"# # Training Only\n# train_images = []\n# for i in tqdm(range(train.shape[0])):\n#     path = os.path.join(train_images_path, train_images_list[i])\n#     image = cv2.imread(path)\n#     image = image \/ 255\n#     image = cv2.resize(image, (128, 128))\n#     train_images.append(image)\n# train_images = np.array(train_images)  \n# print(train_images.shape)","3042e046":"# Training + Inferencing\ntest_images = []\nfor i in tqdm(range(test.shape[0])):\n    path = os.path.join(test_images_path, test_images_list[i])\n    image = cv2.imread(path)\n    image = image \/ 255\n    image = cv2.resize(image, (128, 128))\n    test_images.append(image)\ntest_images = np.array(test_images)  \nprint(test_images.shape)","286f2743":"plt.hist(train_label, bins = 50);","aac0bdac":"# # To be used for training\n# model = VGG16(weights = 'imagenet', include_top = False, input_shape = (128, 128, 3))\n\n# # Freezing the weights of all the layers\n# for layer in model.layers:\n#     layer.trainable = False\n    \n# # Saving the model: Training\n# model.save('vgg16_model.h5')","eaf05296":"# Loading the model: Inferencing\nmodel = load_model('..\/input\/petfindermy-pawpularity-contest\/vgg16_model.h5')","fb414de7":"# # Training only\n# # The top layer of VGG16 gives us (4*4*512) feature vector for each of the images\n# train_fea = model.predict(train_images)\n\n# # Reshaping the feature vectors\n# train_fea = train_fea.reshape(train_fea.shape[0], -1)\n# train_fea = pd.DataFrame(train_fea)\n# train_fea.to_csv('train_fea_vgg16.csv', index = False)","b8ad0932":"train_fea = pd.read_csv('..\/input\/petfindermy-pawpularity-contest\/train_fea_vgg16.csv')\nprint(train_fea.shape)","937ab9e9":"# Training + Inferencing\n# The top layer of VGG16 gives us (4*4*512) feature vector for each of the images\ntest_fea = model.predict(test_images)\n\n# Reshaping the feature vectors\ntest_fea = test_fea.reshape(test_fea.shape[0], -1)\nprint(test_fea.shape)","1d4c66f9":"model = Sequential(layers = [\n    Dense(units = 32, activation = 'relu', input_shape = (8192,)),\n    Dropout(0.45),\n    Dense(units = 64, activation = 'relu'),\n    Dropout(0.45),\n    Dense(units = 64, activation = 'relu'),\n    Dropout(0.45),\n    Dense(units = 4, activation = 'relu'),\n    Dense(units = 1, activation = 'relu'),\n])\n\nmodel.summary()","7d611bcc":"# Defining the callbacks and optimizers\nadam = Adam(learning_rate = 0.001)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.9, patience = 5, \n    min_lr = 0.0001, min_delta = 0.001)\n\nmodel.compile(loss = \"mse\", optimizer = adam, metrics = RMSE())\npredictor = model.fit(train_fea, train_label, validation_split = 0.2, \n    epochs = 100, batch_size = 32, callbacks = [reduce_lr])","b8d6a6d5":"# Training (RMSE) = 17.96 | Validation (RMSE) = 20.5583\n# Saving the model: Training\n# model.save('mlp8_model_v2.h5')\n\n# Loading the model: Inferencing\nmodel = load_model('..\/input\/petfindermy-pawpularity-contest\/mlp8_model_v2.h5')","8441e524":"y_test_preds = model.predict(test_fea)\nsubmission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['Pawpularity'] = y_test_preds\nsubmission.to_csv('submission.csv',index = False)","14dbfb53":"# PetFinder.my\n- Hola amigos, this notebook covers my code for the **PetFinder.my - Pawpularity Contest**, which can be found [here](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score).\n- In this notebook, I have just used the images and dropped all the meta features. I first used a **VGG-16** to extract a 8192-dimensional representation of each of the images.\n- Then I used a custom multi-layer perceptron (MLP), to extract the final value of the `Pawpularity` variable, for each of the points in the dataset.\n- Other experiments that I did, include:\n    - Using ResNets for feature extraction\n    - Determining the 10th and 90th percentiles of the `Pawpularity` variable, and using those percentiles to clip the predicted values of the variable itself.","51c4ea20":"# Training the Model","485d0d63":"# Installing and Importing Packages","6884f644":"# Feature Extraction using VGG16","600bf587":"# Importing the CSV(s) & Image(s)","fcb40d47":"# Visualization(s)","e689c3af":"# Making the Submission"}}