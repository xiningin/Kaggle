{"cell_type":{"47afae21":"code","a1fddcf2":"code","58372d9d":"code","9f914269":"code","bc752ed9":"code","435ed17f":"code","803bd904":"code","5dda9f8e":"code","13534a80":"code","d922cabc":"code","e54c7846":"code","ee32bec5":"code","4d521f8e":"code","66f2d560":"code","c0d2d339":"code","7c8c6a8a":"code","c85af07d":"code","c3b3e6db":"code","ae4afe2d":"code","33f589e4":"code","ece1e5a0":"code","e7e6a02b":"code","55e083e8":"code","19db542c":"code","d4c69f1b":"code","fcd00740":"code","75398c74":"code","05463844":"code","c0c3b3f0":"markdown","00fbe99c":"markdown","85245584":"markdown","ee036901":"markdown","d86a4c0e":"markdown","8e637837":"markdown","5f203273":"markdown","cb814dd8":"markdown","5e06ba7f":"markdown","600828bb":"markdown","f770f84b":"markdown","b5735abc":"markdown","b11e24be":"markdown","8157c119":"markdown"},"source":{"47afae21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a1fddcf2":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score,roc_curve,accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, cross_val_score","58372d9d":"TrainDF = pd.read_csv(\"..\/input\/train.csv\")\nTestDF = pd.read_csv(\"..\/input\/test.csv\")\nRawData_DF = pd.concat([TrainDF,TestDF])\nRawData_DF.describe()","9f914269":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)","bc752ed9":"display_all(TrainDF)","435ed17f":"((TrainDF['target'].value_counts())\/len(TrainDF)).plot.bar()","803bd904":"len(TrainDF[TrainDF['target']==0.0]),len(TrainDF[TrainDF['target']==1.0])","5dda9f8e":"TrainDF['target'].value_counts()","13534a80":"TrainPositive = TrainDF[TrainDF['target'] == 1.0]\nTrainPositive.shape","d922cabc":"TrainSF = TrainDF[TrainDF['target'] == 0.0].sample(frac=0.25, random_state=1)\nTrainSF.shape","e54c7846":"TrainDF = pd.concat([TrainSF,TrainPositive])","ee32bec5":"TrainDF['target'].value_counts()","4d521f8e":"X_train, X_val, y_train, y_val = train_test_split(\n    TrainDF.drop(['ID_code','target'], axis=1),\n    TrainDF['target'],\n    test_size=0.2, random_state=42)","66f2d560":"X_train.shape,y_train.shape,X_val.shape,y_val.shape","c0d2d339":"import xgboost as xgb\nimport matplotlib.pyplot as pyplot","7c8c6a8a":"d_train = xgb.DMatrix(X_train, label=y_train)\nd_valid = xgb.DMatrix(X_val, label=y_val)","c85af07d":"param = {'max_depth': 4, 'eta': .4, 'silent': 1, 'objective': 'binary:logistic'}\nparam['nthread'] = 4\nparam['eval_metric'] = 'auc'\nparam['min_child_weight'] = 5\nparam['subsample'] = .5","c3b3e6db":"evallist = [(d_train, 'train'),(d_valid, 'eval')]","ae4afe2d":"num_round = 50\nbst = xgb.train(param, d_train, num_round, evallist, \n                early_stopping_rounds=10)","33f589e4":" check = bst.predict(xgb.DMatrix(X_val), ntree_limit=bst.best_iteration+1)","ece1e5a0":"auc = roc_auc_score(y_val, check)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_val, check)\n# plot no skill\npyplot.title('Receiver Operating Characteristic')\npyplot.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\npyplot.plot(fpr, tpr,marker='.')\npyplot.ylabel('True Positive Rate')\npyplot.xlabel('False Positive Rate')\n# show the plot\npyplot.show() ","e7e6a02b":"dictvar = bst.get_score(importance_type='gain')\nimpft = pd.DataFrame(columns=['feature','importance'])","55e083e8":"for column in TrainDF:\n    impft = impft.append({'feature' : column , 'importance' : dictvar.get(column)} , ignore_index=True)","19db542c":"impft.sort_values('importance',ascending=False)","d4c69f1b":"xgb.plot_tree(bst, num_trees=1)\nfig = pyplot.gcf()\nfig.set_size_inches(150, 100)\nfig.savefig('tree.png')","fcd00740":"fig, ax = pyplot.subplots(figsize=(200, 500))\nxgb.plot_importance(bst, ax=ax)","75398c74":"TestDF['target'] = bst.predict(xgb.DMatrix(TestDF.drop(['ID_code'],axis=1)), ntree_limit=bst.best_iteration+1)","05463844":"Solution = TestDF[['ID_code','target']]\nSolution.to_csv(\"Santander_CustomerTransaction.csv\", index=False)","c0c3b3f0":"Creating train and validation data. we are making 20% data a validation****","00fbe99c":"We can plot a tree with below code****","85245584":"We are plotting below AUC curve for the same****","ee036901":"XGBoost import****","d86a4c0e":"We can see above that our xgboost model is predicting around 89% ,however on validation data it is 83% accurate.****","8e637837":"**Let's create a Dataframe for both Train and Test CSV files.","5f203273":"We are finally predicting on our Test data now****","cb814dd8":"**Hyperparameter tuning **\nSince data is skewed we are doing sampling kepping 25% data of label 0. In order make robust model.","5e06ba7f":"**Counts of target variables 0 and 1  in order to see what is the distribution .","600828bb":"**We have checked In dataset, null & NaN values are not present !","f770f84b":"As shown above we can see featurewise importance ****","b5735abc":"Training XGboost molel below****","b11e24be":"As we can data is skewed 90% data having target var as 0 and other 10% have lebel 1.","8157c119":"**Importing Some important libraries"}}