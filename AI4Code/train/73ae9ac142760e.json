{"cell_type":{"910c2b52":"code","3d82ee6c":"code","094400ff":"code","656c298e":"code","1aa52156":"code","4d61b625":"code","caa43235":"code","08d8a34f":"code","1f87dad9":"code","5f77c075":"code","af71ef7c":"code","236fe4f4":"code","8e27e129":"code","db19b41b":"code","ad7d6534":"code","d1963d7f":"code","d401d3f1":"code","e920aadc":"code","fe314466":"code","b8f0ae56":"code","4e23aded":"code","a7980f0f":"code","e7a9e152":"code","ffe8e695":"code","093cdef2":"code","c24e1844":"code","4679f5bc":"code","c4a60c7b":"code","c4baa942":"code","21002447":"code","92248ab3":"code","5edfca57":"code","a8193853":"code","cd954865":"code","90947e5c":"code","b47befe6":"code","933274fe":"code","89da9200":"markdown","4dfc1ee3":"markdown","80577fdd":"markdown","cc42caf5":"markdown","b45d3892":"markdown","a6a09a10":"markdown","087a6f38":"markdown","57ba364b":"markdown","7e0cd362":"markdown","77e19d66":"markdown","437e65f6":"markdown","4da91fe7":"markdown","8a76be5a":"markdown","268d623b":"markdown","0be0bfb4":"markdown","df0ba6d6":"markdown","8287d350":"markdown","05b6182e":"markdown","b0657efd":"markdown","29fe3e9c":"markdown","3f5df686":"markdown","1d51c593":"markdown","595c3e23":"markdown","bb3972eb":"markdown","dceb477e":"markdown","477e37d6":"markdown","9971ad70":"markdown","03aa7309":"markdown","17f57fec":"markdown","84b08ae1":"markdown","09c3fb7b":"markdown","37753c12":"markdown","da91998d":"markdown","6b739d2f":"markdown","4ed2f1f2":"markdown","62b0fc8e":"markdown","ab2b6185":"markdown"},"source":{"910c2b52":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import zscore\nfrom pprint import pprint\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, mean_squared_error,mean_absolute_error,r2_score\nfrom sklearn.utils import shuffle\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings( 'ignore')","3d82ee6c":"df = pd.read_csv(\"\/kaggle\/input\/regression-with-neural-networking\/concrete_data.csv\")","094400ff":"# Description of datatypes, columnheads, rowcounts\n#each columnhead is an attribute\ndf.info()","656c298e":"print(\"Shape of raw data is: \",df.shape)","1aa52156":"print(\"Attributes present in the data are: \")\nfor i in df.columns:\n  print(i)","4d61b625":"#lets make sure that there is no null value present\ndf.isnull().values.any()","caa43235":"print(\"Central values (mean and median), standard deviation and quartiles of the dataset are: \")\ndf.describe().T","08d8a34f":"corr = df.corr()\nsns.set(rc = {'figure.figsize':(15,10)})\nsns.heatmap(corr, annot=True, cmap='flare')","1f87dad9":"fig, axes = plt.subplots(ncols=3, nrows=3,figsize=(10,10))\nfig.tight_layout()\nfor i, ax in zip(df.columns, axes.flat):\n    sns.distplot(df[i], ax=ax)\nplt.show()","5f77c075":"for i in list(df.columns):\n  print(\"\\n\\n----------------\",i,\"----------------\\n\\n\")\n  print('Null values in',i,\":\",df[i].isnull().any())\n  print('Range of values',i,\":\", df[i].max()-df[i].min())\n  print('Min age of',i,\":\",df[i].min())\n  print('Maxage of',i,\":\",df[i].max())\n  print('Mean value of',i,\":\", df[i].mean())\n  print('Median value of',i,\":\",df[i].median())\n  print('Std deviation of',i,\":\", df[i].std())\n  Q1=df[i].quantile(q=0.25)\n  Q3=df[i].quantile(q=0.75)\n  print('Q1 of',i,\":\", Q1)\n  print('Q3 of',i,\":\", Q3)\n  print('Interquartile range (IQR) is ', stats.iqr(df[i]))\n  L_outliers=Q1-1.5*(Q3-Q1)\n  U_outliers=Q3+1.5*(Q3-Q1)\n  print('Lower outliers in ',i,\":\" , L_outliers)\n  print('Upper outliers in ',i,\":\" , U_outliers)\n  print('No. of outliers in Upper ',\":\", df[df[i]>586.4375][i].count())\n  print('No. of outliers in Lower ',\":\", df[df[i]<-44.0625][i].count())\n  print('% of Outliers in Upper',\":\",round(df[df[i]>586.4375][i].count()*100\/len(df)), '%')\n  print('% of Outliers in Lower',\":\",round(df[df[i]<-44.0625][i].count()*100\/len(df)), '%')\n","af71ef7c":"for idx, i in enumerate(list(df.columns)):\n  if idx<8:\n    fig, (ax1,ax2)=plt.subplots(1,2,figsize=(12,6))\n    sns.boxplot(x=i,data=df,orient='h',ax=ax1)\n    ax1.set_ylabel(i, fontsize=12)\n    ax1.set_title('Distribution', fontsize=12)\n    ax1.tick_params(labelsize=12)\n    ax2.hist(df[i])\n    ax2.set_xlabel(i, fontsize=12)\n    ax2.set_ylabel('Strength', fontsize=12)\n    ax2.set_title('Strength Comparison', fontsize=12)\n    ax2.tick_params(labelsize=12)\n    plt.subplots_adjust(wspace=1)\n    plt.tight_layout() ","236fe4f4":"# Distribution plot for each attributes\nfig, ax2 = plt.subplots(3, 3, figsize=(18, 18))\nsns.histplot(df['Cement'],ax=ax2[0][0],kde=True)\nsns.histplot(df['Blast Furnace Slag'],ax=ax2[0][1],kde=True)\nsns.histplot(df['Fly Ash'],ax=ax2[0][2],kde=True)\nsns.histplot(df['Water'],ax=ax2[1][0],kde=True)\nsns.histplot(df['Superplasticizer'],ax=ax2[1][1],kde=True)\nsns.histplot(df['Coarse Aggregate'],ax=ax2[1][2],kde=True)\nsns.histplot(df['Fine Aggregate'],ax=ax2[2][0],kde=True)\nsns.histplot(df['Age'],ax=ax2[2][1],kde=True)\nsns.histplot(df['Strength'],ax=ax2[2][2],kde=True)","8e27e129":"# Visualize the analysis using pair plots, with Histogram in diagonal.\n# This will help understang the disribution more accurately\nsns.pairplot(df)","db19b41b":"sns.scatterplot( x=\"Cement\",y=\"Strength\", hue=\"Water\",size=\"Age\", data=df, ax=ax, sizes=(15,15))","ad7d6534":"df.isnull().sum()","d1963d7f":"df1=df.copy()\ndf1.boxplot(figsize=(20,10))","d401d3f1":"#Replacing the outliers by median\nfor col_name in df1.columns[:-1]:\n    q1 = df1[col_name].quantile(0.25)\n    q3 = df1[col_name].quantile(0.75)\n    iqr = q3 - q1\n    \n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    df1.loc[(df1[col_name] < low) | (df1[col_name] > high), col_name] = df1[col_name].median()","e920aadc":"df1.boxplot(figsize=(20,10))","fe314466":"df_z = df1.apply(zscore)\ndf_z=pd.DataFrame(df_z,columns=df.columns)\ndf_z.head()","b8f0ae56":"#independent and dependent variables\nX = df_z.iloc[:,0:-1]\ny = df_z.iloc[:,-1]\nprint(X,y)","4e23aded":"# Split X and y into training and train_size of 0.25 and let's keep random_state=10 for making observation\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 10)","a7980f0f":"# Linear Regression \nlreg = LinearRegression() \nlasso = Lasso()\nridge = Ridge() \nlreg.fit(X_train, y_train) \nlasso.fit(X_train, y_train) \nridge.fit(X_train, y_train) \ny_pred_lreg = lreg.predict(X_test) \ny_pred_lasso = lasso.predict(X_test) \ny_pred_ridge = ridge.predict(X_test)  ","e7a9e152":"multi_coef = lreg.coef_\nprint('By multi-Linear regression on all variables')\nfor i in range(8):\n    pprint(f'Value of beta{i+1} = {round(multi_coef[i],2)}')\n  ","ffe8e695":"print(\"Model\\t\\t\\t RMSE \\t\\t R2\") \nprint(\"\"\"LinearRegression \\t {:.2f} \\t\\t{:.2f}\"\"\".format(  np.sqrt(mean_squared_error(y_test, y_pred_lreg)), r2_score(y_test, y_pred_lreg))) \nprint(\"\"\"LassoRegression \\t {:.2f} \\t\\t{:.2f}\"\"\".format( np.sqrt(mean_squared_error(y_test, y_pred_lasso)), r2_score(y_test, y_pred_lasso)))\nprint(\"\"\"RidgeRegression \\t {:.2f} \\t\\t{:.2f}\"\"\".format( np.sqrt(mean_squared_error(y_test, y_pred_ridge)), r2_score(y_test, y_pred_ridge)))","093cdef2":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,6)) \nax1.scatter(y_pred_lreg, y_test) \nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k-.', lw=1) \nax1.set_ylabel(\"True\") \nax1.set_xlabel(\"Predicted\") \nax1.set_title(\"Linear Regression\") \n\nax2.scatter(y_pred_ridge, y_test) \nax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k-.', lw=1) \nax2.set_ylabel(\"True\") \nax2.set_xlabel(\"Predicted\") \nax2.set_title(\"Ridge Regression\") \nfig.suptitle(\"True vs Predicted\") \nfig.tight_layout()","c24e1844":"Poly_model = Pipeline([('poly', PolynomialFeatures(degree=2)),\n                  ('linear', LinearRegression(fit_intercept=False))])\n                  \nPoly_model.fit(X_train, y_train)\n\nprint(\"regression coefficients\")\nprint(Poly_model.named_steps['linear'].coef_)    \nprint(Poly_model.named_steps['linear'].intercept_) ","4679f5bc":"y_pred_poly = Poly_model.predict(X_test)\nprint(\"Model\\t\\t\\t RMSE \\t\\t R2\") \nprint(\"\"\"PolynomialRegression \\t {:.2f} \\t\\t{:.2f}\"\"\".format(  np.sqrt(mean_squared_error(y_test, y_pred_poly)), r2_score(y_test, y_pred_poly))) ","c4a60c7b":"fig, ax1 = plt.subplots(1, figsize=(12,6)) \nax1.scatter(y_pred_poly, y_test) \nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k-.', lw=1) \nax1.set_ylabel(\"True\") \nax1.set_xlabel(\"Predicted\") \nax1.set_title(\"Polynomial Regression\") ","c4baa942":"X","21002447":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Feature importances'],index=X_train.columns))","92248ab3":"y_pred_DT = dt_model.predict(X_test)\nprint('Train Accuracy:',dt_model.score(X_train,y_train))\nprint('Test Accuracy:',dt_model.score(X_test,y_test))\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred_DT))\nprint(\"\\nModel\\t\\t\\t\\t RMSE \\t\\t R2\") \nprint(\"\"\"Decision Tree Regressor \\t {:.2f} \\t\\t{:.2f}\"\"\".format( np.sqrt(mean_squared_error(y_test, y_pred_DT)), r2_score(y_test, y_pred_DT))) ","5edfca57":"fig, ax1 = plt.subplots(1, figsize=(12,6)) \nax1.scatter(y_pred_DT, y_test) \nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k-.', lw=1) \nax1.set_ylabel(\"True\") \nax1.set_xlabel(\"Predicted\") \nax1.set_title(\"Decision Tree\") ","a8193853":"df2=df_z.copy()\ndf2.head()","cd954865":"df2=df_z.copy()\nX = df2.drop( ['Strength','Fly Ash','Coarse Aggregate','Fine Aggregate'] , axis=1)\ny = df2['Strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","90947e5c":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)","b47befe6":"#printing the feature importance\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns))","933274fe":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)","89da9200":"## b. Multivariate Analysis","4dfc1ee3":"### Now let's look at distribution of each data first","80577fdd":"# c. Pick one strategy to address the presence outliers and missing values and perform necessary imputation ","cc42caf5":"This dataset consists of 1030 instances with 9 variables in total. There are 8 independent variables and 1 response variable, \"strength\".\n* independent variables are:\n1. cement\n2. slag\n3. ash\n4. water\n5. superplastic\n6. coarseagg\n7. fineagg\n8. age\n* Dependent variable\/reponse\/output is: strength\n\nFrom exploration of each component we see that cement, slag, ash, water, superplastic, coarseagg, fineagg are quantities of raw material present in the mix. (kg\/m\u00b3). Age is in number of days required. The response variable is Concrete Compressive Strength (MPa).","b45d3892":"Now, we can see that R2 score is faily high all the points are lying further close to the line and this indicates the improvement in the model. The train as train accuracy is 99.5% but test accuracy is at 80.8% only. This implies overfitting in the model!","a6a09a10":"We can see there is a significant improvement and outliers have and dealt with till an extent and are well within the range. We can use this data now for feature extraction and creating model.","087a6f38":"* Boxplot implies that the slag, ash, water superplastic, and age contains outliers.","57ba364b":"#### *To address the complexity of the model whether it should it be simple linear model in terms of parameters or would it be a quadratic or higher degree. For now we can conclude that Linear model will not be a good fit. We will test the now a polynomial model with higher degree then a Decision Tress regression model to find the best model and will further tune the model*","7e0cd362":"#### Let's analyze each attributes from statistical point of view. let's look at some of them like, range of values observed, central values (mean and median), standard deviation and quartiles.","77e19d66":"### Earlier we saw a high positive correlation between Strength and cement. We also noticed a negative correlation between ash and cement. Lets look at the box plot and strength comparison for each attribute","437e65f6":"We can notice some variables are positively co-related while others are negatively co-related.\n1. We can observe a positive correlation between Strength and Cement. It is certainly true since increase in the amount of cement will improve the strength of Concrete as the grnularity is very dense it will improve binding strength as well as overall strength.\n2. Moreover, Age and superplastic are also affecting Compressive strength positively.\n3. There is also a negative correlation between water and superplasticity and fineagg. We also see a negative correlation between cement and ash\n\n* **We will explore these correlation more up ahead.**\n\nHowever, we can't see a very strong positive or negative correlation that lies above 0.8 that might highlights any redundant features.","4da91fe7":"#### Now when we look at Box-plots and distribution for each attribute we notice that:\n1. Cement has an almost normal distribution.\n2. Slag has two gausssians. We can see it is rightly skewed. This implies the presence of outliers.\n3. Ash has two gaussians and rightly skewed. It implies the presence of outliers.\n4. Water has 3 gaussians and looking at the distribution plots we notice that it is slighly left skewed. It implies the presence of outliers.\n5. superplastic has 2 gaussians and looking at the distribution plots we notice that it is rightly skewed. It implies the presence of outlies.\n6. coarseagg has 3 guassians and has an almost normal distribution.\n7. fineagg has two guassians and has an almost normal distribution.\n8. age has multiple guassians and and looking at the distribution plots we notice that it is rightly skewed. It implies the presence of outlies.\n9. strength has an almost normal distribution.\n\nEarlier with description we saw that we have missing values in our data now, we can see that there are outliers problem in the dataset as well.\n\n#### Let's look at relationship between all indpendent attributes\n#### We will use our heat map that we plotted at the starting of our analysis to find variables that might have strong correlation with each other.\n*Correlation Coefficient r lies between, {-1,+1} where -1 and +1 both represents very strong correlation. -1 however, conclues the Inverse relation while +1 implies positive correlation. This implies that they will gave same information thus are redundant from predictive analysis point of view. Nonetheless repeating predictor will result in a biased result. In order to avoid that we would only keep one of them. We can later explore further which dimensions should we keep and which one to drop among them. As each will have a different effect on model* \n\n1. Cement : No significant relationship with other attributes.\n2. Slag : No significant relationship with other attributes.\n3. Ash : No significant relationship with other attributes.\n4. Water : Negative linear relationship with superplastic,coarseagg and fineagg. No relationship with any other atributes.\n5. Superplastic : Negative linear relationship with water. No relationship with any other atributes.\n6. Coarseagg : No significant relationship with other attributes.\n7. Fineagg : No significant relationship with other attributes.\n8. Age: Has multiple relationships\n9. Strength: Has multiple relationships","8a76be5a":"## Data Description: \n\nThe actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from laboratory. Data is in raw form (not scaled). The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations). \n","268d623b":"#### Let's boxplot our data again and copare it with our previous boxplot","0be0bfb4":"## Splitting the data into train and test set","df0ba6d6":"### We saw earlier that there are no missing (null) values.","8287d350":"### These points are noted from the above plots\n1. Compressive strength increases as the amount of cement increases.\n2. Compressive strength increases with age upto some extent.\n3. Cement with less age requires more cement for higher strength\n4. The older the cement is the more water it requires\n5. Concrete strength increases when less water is used in preparing it","05b6182e":"### Lets ","b0657efd":"#### Let's have a look at how these variables are correlated with each other and if there is any relationship that might give us a better model later","29fe3e9c":"## Context\nConcrete is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate. \nAttribute Information: \nin kg in a m3 mixture \n1. Fly ash Cement : measured in kg in a m3 mixture \n2. Blast : measured: measured in kg in a m3 mixture \n3. Water : measured in kg in a m3 mixture \n4. Superplasticizer : measured in kg in a m3 mixture \n5. Coarse Aggregate : measured in kg in a m3 mixture \n6. Fine Aggregate : measured in kg in a m3 mixture \n7. Age : day (1~365) \n8. Concrete compressive strength measured in MPa \n","3f5df686":"#### We can conclude 2 things from earlier plots and these Feature Importance\n1. cement, age and water are attributes with relatively important Features\n2. ash, coarseagg, fineagg, superplastic and slag are the less significant Features.","1d51c593":"#### Lets predict on the test and train data to evaluate losses","595c3e23":"## Outliers can be handles in several ways below are some of the methods of treating the outliers:\n\n1. Trimming the outlier\n2. Quantile based flooring and capping\n3. Mean\/Median imputation","bb3972eb":"### i. Data types and description of the independent attributes which should include (name, meaning, range of values observed, central values (mean and median), standard deviation and quartiles, analysis of the body of distributions \/ tails, missing values, outliers ","dceb477e":"## Splitting the data into predictor and Response","477e37d6":"## a.) Univariate analysis","9971ad70":"# 1. Deliverable -> 1 (Exploratory data quality report reflecting the following) ","03aa7309":"#### **This does not give us any hint if the missing values have been replaced with zero, it is just representing that there is no \"null\" value.**\n\nWe will now look at minimum value along with other details in a brief glance.","17f57fec":"## We Can use several approaches for identifying and handling the outliers\n1. Boxplots\n2. Z-score\n3. Inter Quantile Range(IQR)(that we used in start)\n\nLet's use Boxplots to identify outliers","84b08ae1":"# 2. Deliverable -2 (Feature Engineering techniques) ","09c3fb7b":"# **Objective**: \n#Modeling of strength of high performance concrete using Machine Learning \n","37753c12":"### Let's train our data using a Decision Tree Regression model","da91998d":"df.head()","6b739d2f":"We can see LassoRegression has RMSE of: 0.98 and R2 Score of\t-0.00.\nWe know that, Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a \nR2 score of 0.0.\nAlso, Ridge regularization does not change prediction by much so we can go with either model as they are nearly same.\nWe notice that values are scattered along the line. So, it is not a model that I,ll recommend for the purpose.","4ed2f1f2":"## Lets test in on Linear Regression model first with regularization\n\nWe will also try different combinations to try several parameters to see what might fit best. We have alread perform the normalization to have each data on similar scale","62b0fc8e":"\nDistribution indicates that the following data are left skewed:\n* age\n* cement\n* ash\n* slag","ab2b6185":"### Lets first standardize our scales before proceeding as each feature is on a different scale thus will take huge time to converge if used directly.\n\nHere, we can see that age is in years and thus whole model needs to be scaled"}}