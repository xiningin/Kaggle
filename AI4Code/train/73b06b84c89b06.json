{"cell_type":{"56f7f290":"code","ab150b00":"code","d1556420":"code","e8dcfe6b":"code","d34306a3":"code","af83d7b2":"code","80c64f01":"code","3632b56c":"code","5d1b52cd":"code","c86b6ee1":"code","cd48b3ac":"code","23f6ec5e":"code","f6035de0":"code","47fedd73":"code","59929181":"code","77e23516":"code","78b43351":"code","a2a91dbe":"code","a59ad685":"code","e7c69485":"code","55691381":"code","c22a035b":"code","fa799e73":"code","3ed6a509":"code","2fc39e4a":"code","4d128c51":"code","d5b42599":"code","2b7e66a5":"code","5995be2a":"code","04cc1b24":"code","790dc063":"code","f68ecfd7":"code","a33b0bc2":"code","d51d7148":"code","1ec78511":"code","ac06cedf":"code","3861d91b":"code","ac82a57c":"code","edfd19db":"code","ee64b944":"code","05d1a55b":"code","072c0fad":"code","4a1be207":"code","46dd94b0":"code","60c8e989":"code","a4857e46":"code","00d44a6a":"code","e3d3f1a8":"code","ee6fc434":"code","0185dd16":"code","85270ee3":"code","1583ee82":"code","9fae4bff":"code","f4ec3b48":"code","59ab0768":"code","a5042dbd":"code","6b8272fc":"code","e609b2e8":"code","83be3eef":"code","450455f8":"code","407621db":"code","87b6d60b":"code","540075fd":"code","1e322d1f":"code","89b9ef4a":"code","574464d3":"code","482de07f":"code","28fbe4a8":"code","d07f2468":"code","e978fec7":"code","60faac10":"code","4e5cb74b":"code","593b4bcc":"code","964a2866":"code","0d654bdd":"code","e90981b3":"code","a1758896":"code","e0872a08":"code","688ef254":"code","c91cdf83":"code","d3c014d6":"code","7b629705":"code","470e5704":"code","1b3b7abd":"code","7a4e7cf6":"code","69b8dc31":"code","62db6db7":"code","0206530a":"code","52b54a6d":"code","5de4a6c4":"code","3ddc180b":"code","782dc3fe":"code","813ce4a7":"code","96d86a68":"code","fc0880c9":"code","09d9ad38":"code","141258d6":"code","b1b60776":"code","de369380":"code","0a54f71f":"code","a6734eb2":"code","b77880a8":"code","d9bc0f6f":"code","92d5fab2":"code","8a9be85a":"code","d3679e3d":"code","44d000fe":"code","5a5058f5":"code","ef4a7077":"code","fe5c2160":"markdown","ba334f88":"markdown","a48cf8ba":"markdown","315a4cf6":"markdown","a2b0b53a":"markdown","22e6ead9":"markdown","1eef0727":"markdown","0c22624f":"markdown","5c1683e5":"markdown","8417a110":"markdown","06b13967":"markdown","bf839edb":"markdown","becc6360":"markdown","d4a28d9b":"markdown","55c1eb9d":"markdown","39bc7249":"markdown","9eb0a602":"markdown","41b1489e":"markdown","9604ca5f":"markdown","6a3a1a00":"markdown","41540038":"markdown","c26386bb":"markdown","77788b4b":"markdown","07575179":"markdown","c697d6cb":"markdown","17ec6668":"markdown","7d20fe42":"markdown","9d6c5831":"markdown","b74016d6":"markdown","3ba1c48b":"markdown","99036f44":"markdown","1638541e":"markdown","cc02cfdb":"markdown","d47d7445":"markdown","72b9570a":"markdown","db5dd522":"markdown","fd9c8427":"markdown","071ca3f5":"markdown","3d31de76":"markdown","30682b9f":"markdown","e5db5667":"markdown","96ed5919":"markdown","8747f6cf":"markdown","067a339d":"markdown","81ee07b2":"markdown","bd281ce3":"markdown","968ade09":"markdown","26baf638":"markdown","c2672e67":"markdown","5bee16ab":"markdown","9a74e836":"markdown","08645776":"markdown","00959268":"markdown","04d1b6f6":"markdown","6cd0d391":"markdown","7fdb1867":"markdown","1fb9ff53":"markdown","1fdd781b":"markdown","d7051326":"markdown","b639c4bd":"markdown","f54c0ba2":"markdown","b79b9e6a":"markdown","d857c058":"markdown"},"source":{"56f7f290":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","ab150b00":"df = pd.read_csv('..\/input\/electric-motor-temperature\/pmsm_temperature_data.csv')","d1556420":"df_test = df[(df['profile_id'] == 65) | (df['profile_id'] == 72)]\ndf = df[(df['profile_id'] != 65) & (df['profile_id'] != 72)]","e8dcfe6b":"df","d34306a3":"df.describe()","af83d7b2":"df.isnull().sum()","80c64f01":"plt.figure(figsize=(15,6))\ndf['profile_id'].value_counts().sort_values().plot(kind = 'bar')","3632b56c":"for i in df.columns:\n    sns.distplot(df[i],color='g')\n    sns.boxplot(df[i],color = 'y')\n    plt.vlines(df[i].mean(),ymin = -1,ymax = 1,color = 'r')\n    plt.show()","5d1b52cd":"import scipy.stats as stats\nfor i in df.columns:\n    print(i,' :\\nSkew : ',df[i].skew(),' : \\nKurtosis : ',df[i].kurt())\n    print()","c86b6ee1":"plt.figure(figsize=(14,7))\nsns.heatmap(df.corr(),annot=True)","cd48b3ac":"plt.figure(figsize=(20,5))\ndf[df['profile_id'] == 20]['stator_yoke'].plot(label = 'stator yoke')\ndf[df['profile_id'] == 20]['stator_tooth'].plot(label = 'stator tooth')\ndf[df['profile_id'] == 20]['stator_winding'].plot(label = 'stator winding')\nplt.legend()","23f6ec5e":"df.drop('profile_id',axis = 1,inplace=True)\ndf_test.drop('profile_id',axis = 1,inplace=True)","f6035de0":"sns.distplot(df['ambient'])","47fedd73":"from scipy.stats import shapiro\nshapiro(df['ambient'])","59929181":"shapiro(df['pm'])","77e23516":"from scipy.stats import bartlett\nbartlett(df['ambient'],df['pm'])","78b43351":"sns.distplot(df['coolant'])","a2a91dbe":"from scipy.stats import shapiro\nshapiro(df['coolant'])","a59ad685":"shapiro(df['pm'])","e7c69485":"from scipy.stats import bartlett\nbartlett(df['coolant'],df['pm'])","55691381":"sns.distplot(df['u_d'])","c22a035b":"from scipy.stats import shapiro\nshapiro(df['u_d'])","fa799e73":"shapiro(df['pm'])","3ed6a509":"from scipy.stats import bartlett\nbartlett(df['u_d'],df['pm'])","2fc39e4a":"sns.distplot(df['u_q'])","4d128c51":"from scipy.stats import shapiro\nshapiro(df['u_q'])","d5b42599":"shapiro(df['pm'])","2b7e66a5":"from scipy.stats import bartlett\nbartlett(df['u_q'],df['pm'])","5995be2a":"sns.distplot(df['motor_speed'])","04cc1b24":"from scipy.stats import shapiro\nshapiro(df['motor_speed'])","790dc063":"shapiro(df['pm'])","f68ecfd7":"from scipy.stats import bartlett\nbartlett(df['motor_speed'],df['pm'])","a33b0bc2":"sns.distplot(df['i_d'])","d51d7148":"from scipy.stats import shapiro\nshapiro(df['i_d'])","1ec78511":"shapiro(df['pm'])","ac06cedf":"from scipy.stats import bartlett\nbartlett(df['i_d'],df['pm'])","3861d91b":"sns.distplot(df['i_q'])","ac82a57c":"from scipy.stats import shapiro\nshapiro(df['i_q'])","edfd19db":"shapiro(df['pm'])","ee64b944":"from scipy.stats import bartlett\nbartlett(df['i_q'],df['pm'])","05d1a55b":"df = df.sample(frac=1,random_state=3)","072c0fad":"df.head()","4a1be207":"sns.scatterplot(df['ambient'],df['pm'])","46dd94b0":"sns.scatterplot(df['coolant'],df['pm'])","60c8e989":"sns.scatterplot(df['motor_speed'],df['pm'])","a4857e46":"sns.scatterplot(df['u_q'],df['pm'])","00d44a6a":"sns.scatterplot(df['u_d'],df['pm'])","e3d3f1a8":"sns.scatterplot(df['i_q'],df['pm'])","ee6fc434":"sns.scatterplot(df['i_d'],df['pm'])","0185dd16":"from sklearn.preprocessing import MinMaxScaler\nX = df.drop(['pm','stator_yoke','stator_tooth','stator_winding','torque'],axis = 1)\nX_df_test = df_test.drop(['pm','stator_yoke','stator_tooth','stator_winding','torque'],axis = 1)\nmm = MinMaxScaler()\nX = mm.fit_transform(X)\nX_df_test = mm.fit_transform(X_df_test)\ny = df['pm']\ny_df_test = df_test['pm']\nX = pd.DataFrame(X,columns = ['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d','i_q'])\nX_df_test = pd.DataFrame(X_df_test,columns = ['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d','i_q'])\ny.reset_index(drop = True,inplace = True)\ny_df_test.reset_index(drop = True,inplace = True)","85270ee3":"print(X.shape)\nprint(y.shape)","1583ee82":"for i in X.columns:\n    print(X[i].skew())\n    sns.distplot(X[i],color='g')\n    sns.boxplot(X[i],color = 'y')\n    plt.vlines(X[i].mean(),ymin = -1,ymax = 1,color = 'r')\n    plt.show()","9fae4bff":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)","f4ec3b48":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","59ab0768":"from statsmodels.stats.diagnostic import linear_rainbow\nlinear_rainbow(lin_reg)","a5042dbd":"from statsmodels.stats.api import het_goldfeldquandt\nhet_goldfeldquandt(lin_reg.resid,lin_reg.model.exog)","6b8272fc":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(X_train_const.values,i) for i in range(X_train_const.shape[1])]\npd.DataFrame(vif,index=X_train_const.columns)","e609b2e8":"lin_reg.resid.plot(kind = 'density')","83be3eef":"import scipy.stats as stats\nimport pylab\nst_residual = lin_reg.get_influence().resid_studentized_internal\nstats.probplot(st_residual, dist=\"norm\", plot = pylab)\nplt.show()","450455f8":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))\/len(y_train))\ntrain_rmse","407621db":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","87b6d60b":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))\/len(y_test))\ntest_rmse","540075fd":"lin_reg.rsquared_adj","1e322d1f":"X_trans = X\nX_trans['coolant'] = np.power(X_trans['coolant'],1\/3)\nX_trans['ambient'] = np.power(X_trans['ambient'],3)\nX_trans['i_d'] = np.power(X_trans['i_d'],3)","89b9ef4a":"for i in X_trans.columns:\n    print(X_trans[i].skew())\n    sns.distplot(X_trans[i],color='g')\n    sns.boxplot(X_trans[i],color = 'y')\n    plt.vlines(X_trans[i].mean(),ymin = -1,ymax = 1,color = 'r')\n    plt.show()","574464d3":"z = np.abs(stats.zscore(X_trans))\nprint(z)","482de07f":"X_trans = X_trans.drop(np.where(z > 3)[0][0:])\nX_trans.reset_index(drop=True,inplace = True)\ny = y.drop(np.where(z > 3)[0][0:])\ny.reset_index(drop = True,inplace = True)","28fbe4a8":"print(X_trans.shape)\nprint(y.shape)","d07f2468":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.3, random_state=3)","e978fec7":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","60faac10":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))\/len(y_train))\ntrain_rmse","4e5cb74b":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","593b4bcc":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))\/len(y_test))\ntest_rmse","964a2866":"X = X_trans","0d654bdd":"from sklearn.decomposition import PCA\npca  = PCA()\npca.fit(X)","e90981b3":"pca.explained_variance_ratio_","a1758896":"np.cumsum(pca.explained_variance_ratio_)","e0872a08":"pca5 = PCA(n_components=5)\nX_pca = pca5.fit_transform(X)\nX_pca","688ef254":"X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=3)","c91cdf83":"X_pca_train_const = sm.add_constant(X_pca_train)\nlin_reg = sm.OLS(y_train,X_pca_train_const).fit()\nlin_reg.summary()","d3c014d6":"y_train_pred = lin_reg.predict(X_pca_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))\/len(y_train))\ntrain_rmse","7b629705":"X_pca_test_const = sm.add_constant(X_pca_test)\ny_test_pred = lin_reg.predict(X_pca_test_const)\ny_test_pred","470e5704":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))\/len(y_test))\ntest_rmse","1b3b7abd":"X_wo_dqi = X.drop(['i_d','i_q'],axis = 1)","7a4e7cf6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_wo_dqi, y, test_size=0.3, random_state=3)","69b8dc31":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","62db6db7":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))\/len(y_train))\ntrain_rmse","0206530a":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","52b54a6d":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))\/len(y_test))\ntest_rmse","5de4a6c4":"X_wo_ms = X.drop(['motor_speed'],axis = 1)","3ddc180b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_wo_ms, y, test_size=0.3, random_state=3)","782dc3fe":"import statsmodels.api as sm\nX_train_const = sm.add_constant(X_train)\nlin_reg = sm.OLS(y_train,X_train_const).fit()\nlin_reg.summary()","813ce4a7":"y_train_pred = lin_reg.predict(X_train_const)\ntrain_rmse = np.sqrt(np.sum(((y_train-y_train_pred)**2))\/len(y_train))\ntrain_rmse","96d86a68":"X_test_const = sm.add_constant(X_test)\ny_test_pred = lin_reg.predict(X_test_const)\ny_test_pred","fc0880c9":"test_rmse = np.sqrt(np.sum(((y_test-y_test_pred)**2))\/len(y_test))\ntest_rmse","09d9ad38":"y = pd.DataFrame(y)","141258d6":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\n# lr = LinearRegression()\n# ridge = Ridge(alpha = 20000)\nlasso = Lasso(alpha = 0.012)","b1b60776":"# from sklearn.model_selection import KFold\n# from sklearn import metrics\n# kf = KFold(n_splits=5,shuffle=True,random_state=0)\n# for model,name in zip([lr,ridge,lasso],['LR','Ridge','Lasso']):\n#     mse_li = []\n#     for train_idx,test_idx in kf.split(X,y):\n#         X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n#         y_train,y_test = y.iloc[train_idx,:],y.iloc[test_idx,:]\n#         model.fit(X_train,y_train)\n#         y_pred = model.predict(X_test)\n#         mse = metrics.mean_squared_error(y_test,y_pred)\n#         mse_li.append(mse)\n#     print('RMSE scores : %0.03f (+\/- %0.08f) [%s]'%(np.mean(mse_li), np.var(mse_li,ddof = 1), name))\n#     print()","de369380":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import randint\n# dt = DecisionTreeRegressor(random_state=0)\n# rf = RandomForestRegressor(random_state=0,n_jobs = -1)\n# param_dt = {\n#         'criterion' : ['mse','mae'],\n#         'max_depth' : randint(1,11)\n# }\n# param_rf = {\n#         'n_estimators' : randint(1,70),\n#         'max_depth' : randint(1,11)\n# }\n# rscv_dt = RandomizedSearchCV(dt,param_dt,scoring='neg_mean_squared_error',cv = 5,n_jobs=1,n_iter = 2,verbose = 1000,random_state = 0)\n# rscv_rf = RandomizedSearchCV(rf,param_rf,scoring='neg_mean_squared_error',cv = 5,n_jobs=-1,n_iter = 2,verbose = 1000,random_state = 0)\n# rscv_dt.fit(X,y)\n# rscv_rf.fit(X,y)\n# print(rscv_dt.best_params_)\n# print(rscv_rf.best_params_)","0a54f71f":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.neighbors import KNeighborsRegressor\n# dt = DecisionTreeRegressor(criterion='mse',max_depth=6,random_state=0)\n# rf = RandomForestRegressor(n_estimators=41,max_depth=6,random_state=0,n_jobs = -1)","a6734eb2":"# from sklearn.model_selection import KFold\n# from sklearn import metrics\n# kf = KFold(n_splits=5,shuffle=True,random_state=0)\n# for model,name in zip([dt,rf],['DT','RF']):\n#     mse_li = []\n#     for train_idx,test_idx in kf.split(X,y):\n#         X_train,X_test = X.iloc[train_idx,:],X.iloc[test_idx,:]\n#         y_train,y_test = y.iloc[train_idx,:],y.iloc[test_idx,:]\n#         model.fit(X_train,y_train)\n#         y_pred = model.predict(X_test)\n#         mse = metrics.mean_squared_error(y_test,y_pred)\n#         mse_li.append(mse)\n#     print('RMSE scores : %0.03f (+\/- %0.08f) [%s]'%(np.mean(mse_li), np.var(mse_li,ddof = 1), name))\n#     print()","b77880a8":"from sklearn.ensemble import BaggingRegressor\n# from sklearn.model_selection import KFold, cross_val_score\n# models = []\n# models.append((\"LinearRegression\",lr))\n# models.append((\"Lasso\",lasso))\n# models.append((\"Ridge\",ridge))\n# models.append((\"DT\",dt))\n# for name,model in models:\n#     mse_var = []\n#     for val in np.arange(1,21):\n#         bg_model = BaggingRegressor(base_estimator=model,n_estimators=val,n_jobs=-1,verbose = 1000, random_state = 0)\n#         kfold = KFold(n_splits=5,shuffle=True,random_state=0)\n#         results = cross_val_score(bg_model,X,y,cv=kfold,n_jobs=-1,scoring='neg_mean_squared_error',verbose = 1000)\n#         mse_var.append(np.var(results,ddof = 1))\n#     print(name,np.argmin(mse_var)+1)","d9bc0f6f":"# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.model_selection import KFold, cross_val_score\n# models = []\n# models.append((\"LinearRegression\",lr))\n# models.append((\"Lasso\",lasso))\n# models.append((\"Ridge\",ridge))\n# models.append((\"DT\",dt))\n# models.append((\"RF\",rf))\n# for name,model in models:\n#     mse_mean = []\n#     for val in np.arange(1,21):\n#         bg_model = AdaBoostRegressor(base_estimator=model,n_estimators=val, random_state = 0)\n#         kfold = KFold(n_splits=5,shuffle=True,random_state=0)\n#         results = cross_val_score(bg_model,X,y,cv=kfold,n_jobs=-1,scoring='neg_mean_squared_error',verbose = 1000)\n#         mse_mean.append(np.mean(results))\n#     print(name,np.argmax(mse_mean)+1)","92d5fab2":"# #Bagging Models\n# LR_bag = BaggingRegressor(base_estimator = lr,n_estimators = 12,random_state = 0,n_jobs = -1)\nlasso_bag = BaggingRegressor(base_estimator = lasso,n_estimators = 2,random_state = 0,n_jobs = -1)\n# DT_bag = BaggingRegressor(base_estimator = dt,n_estimators = 3,random_state = 0,n_jobs = -1,verbose = 1000)\n# ridge_bag = BaggingRegressor(base_estimator = ridge,n_estimators = 2,random_state = 0,n_jobs = -1) \n# # #Boosting models\n# lasso_boost = AdaBoostRegressor(base_estimator = lasso,n_estimators = 10,random_state = 0)\n# ridge_boost = AdaBoostRegressor(base_estimator = ridge,n_estimators = 3,random_state = 0)\n# DT_boost = AdaBoostRegressor(base_estimator = dt,n_estimators = 15,random_state = 0)\n# RF_boost = AdaBoostRegressor(base_estimator = rf,n_estimators = 8,random_state = 0)","8a9be85a":"# from sklearn.ensemble import GradientBoostingRegressor\n# GBC = GradientBoostingRegressor(n_estimators = 100,random_state = 0)","d3679e3d":"# models = []\n# models.append(('LR Bagged',LR_bag))\n# models.append(('Lasso Bagged',lasso_bag))\n# models.append(('Lasso Boosted',lasso_boost))\n# models.append(('Ridge Bagged',ridge_bag))\n# models.append(('Ridge Boosted',ridge_boost))\n# models.append(('DTree Bagged',DT_bag))\n# models.append(('DTree Boosted',DT_boost))\n# models.append(('Gradient Boost',GBC))\n# models.append(('RF Boosted',RF_boost))","44d000fe":"# results = []\n# names = []\n# for name, model in models:\n#     kfold = KFold(n_splits = 5,random_state = 0,shuffle = True)\n#     cv_results = cross_val_score(model,X,y,cv = kfold,scoring='neg_mean_squared_error',n_jobs = -1)\n#     results.append(cv_results)\n#     names.append(name)\n#     print(name,' : ',np.mean(cv_results),' -- ',np.var(cv_results,ddof = 1))","5a5058f5":"from sklearn.metrics import r2_score,mean_squared_error\nlasso_bag.fit(X,y)\ntest_pred = lasso_bag.predict(X_df_test)","ef4a7077":"test_pred","fe5c2160":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for ambient temperature is not equal to the variance of rotor temperature.","ba334f88":"H0 : variance_u_d = variance_pm\n\nH1 : variance_u_d != variance_pm","a48cf8ba":"### Checking skewness and kurtosis numerically","315a4cf6":"# Permanent Magnet Synchronous Motor\n\n![alt text](https:\/\/alliedmarketresearch.files.wordpress.com\/2017\/02\/permanent-magnet-synchronous-motor-pmsm.png?w=705)\n\nThe permanent-magnet synchronous machine (PMSM) drive is one of best choices for a full range of motion control applications. For example, the PMSM is widely used in robotics, machine tools, actuators, and it is being considered in high-power applications such as industrial drives and vehicular propulsion. It is also used for residential\/commercial applications. The PMSM is known for having low torque ripple, superior dynamic performance, high efficiency and high power density.","a2b0b53a":"### Boosting Models\nFinding best number of estimators","22e6ead9":"RMSE scores : 0.380 (+\/- 0.00000606) [DT]\n\nRMSE scores : 0.374 (+\/- 0.00000853) [RF]","1eef0727":"### Taking care of multicollinearity using PCA","0c22624f":"pvalue is higher than 0.05. So we fail to reject the null hypothesis and can say that we do not have enough evidence to reject the null hypothesis. So we do not have enough evidence to prove that variance of q component of current is not equal to the variance of motor temperature.","5c1683e5":"As we can see from the plot, all three stator components follow a similar measurment variance.","8417a110":"As it is not highly skewed data and looking at the values of the dataset it seems there already has been some normalization done.","06b13967":"As the dataset author mentioned, the records in the same profile id have been sorted by time, we can assume that these recordings have been arranged in series of time.\n\nDue to this we can infer that there has not been much time given for the motor to cool down in between recording the sensor data as we can see that initially the stator yoke temperature is low as compared to temperature of stator winding but as we progress in time, the stator yoke temperature goes above the temperature of stator winding.","bf839edb":"From the heatmap above, we can see that torque and q component of current are almost perfectly correlated. Also there seems to be a very high correlation between temperature measurements of stator yoke, stator tooth and stator windings.","becc6360":"As we want to predict the temperatures of stator components and rotor(pm), we will drop these values from our dataset for regression. Also, torque is a quantity, which is not reliably measurable in field applications, so this feature shall be omitted in this modelling.","d4a28d9b":"### Ambient Temperature","55c1eb9d":"# Non Parametric Models","39bc7249":"As profile_id is an id for each measurement session, we can remove it from any furthur analysis and model building.","9eb0a602":"As we can see the from the QQ plot as well as kde plot that the residuals are quiet well normally distributed around the centre but deviate from normal distribution towards the extremes which might be the factor influencing JB test to fail the normality test.","41b1489e":"There is no imporvement in our rmse by transforming the data. So we will not go ahead with the transformation.","9604ca5f":"There is no imporvement in our rmse by using elimination motor speed feature. So we will not go ahead with the elimination.","6a3a1a00":"There are no missing values in the dataset.","41540038":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for voltage q-component is not equal to the variance of rotor temperature.","c26386bb":"## Basic multivariate regression (Base Model)","77788b4b":"### Motor speed","07575179":"DT : criterion=mse, max_depth=6","c697d6cb":"### Voltage q-component","17ec6668":"For a random measurement, we can try to compare the temperatures of the 3 stator components.","7d20fe42":"H0 : variance_ambient = variance_pm\n\nH1 : variance_ambient != variance_pm","9d6c5831":"### Current d-component","b74016d6":"##### Please upvote if you like the work!!!","3ba1c48b":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for coolant temperature is not equal to the variance of rotor temperature.","99036f44":"### Bagging Models\nFinding best number of estimators","1638541e":"Model : Bias Error -- Variance Error\n\nRMSE scores : 0.536 (+\/- 0.00000489) [LR]\n\nRMSE scores : 0.631 (+\/- 0.00000388) [Ridge]\n\nRMSE scores : 0.564 (+\/- 0.00000385) [Lasso]\n\nRMSE scores : 0.380 (+\/- 0.00000606) [DT]\n\nRMSE scores : 0.374 (+\/- 0.00000853) [RF]\n\nLR Bagged  :  -0.5363121272813037  --  4.881295834654977e-06\n\nLasso Bagged  :  -0.5645736874272845  --  3.2947376969099687e-06\n\nLasso Boosted  :  -0.571080275441007  --  5.1543103985968674e-06\n\nRidge Bagged  :  -0.6309426367758058  --  3.2555007823844387e-06\n\nRidge Boosted  :  -0.6172788016072153  --  3.302273899220944e-06\n\nDTree Bagged  :  -0.37582901559820736  --  5.251632737569686e-06\n\nDTree Boosted  :  -0.3232454863288907  --  2.7692299178526762e-05\n\nGradient Boost  :  -0.3165118622627031  --  8.654229505655393e-06\n\nRF Boosted  :  -0.315700682698399  --  4.0892902046352836e-05\n\nAs we can see from the result, ridge bagged seems to gives the best result as far as the handling of variance error is concerned, but on the other hand, Gradient boost and RF boosted gives the best result as far as the handling of bias error is concerned. Overall if we see, Lasso bagged gives quite a reasonable and acceptable result as far as handling both bias and variance error is concerned. So, we will select Lasso Bagged as our final model.","cc02cfdb":"There is no imporvement in our rmse by using PCA. So we will not go ahead with the PCA transformation.","d47d7445":"LinearRegression 1\n\nLasso 10\n\nRidge 3\n\nDT 15\n\nRF 8","72b9570a":"##### Giving a range estimate rather than giving a point estimate is always a more believable strategy. This can be achieved by using k-fold cross validation.","db5dd522":"### Coolant Temperature","fd9c8427":"H0 : variance_i_q = variance_pm\n\nH1 : variance_i_q != variance_pm","071ca3f5":"### Dropping the d and q components of current(i) looking at the statistical analysis","3d31de76":"As we can see, session ids 66, 6 and 20 have the most number of measurements recorded.","30682b9f":"##### Please upvote if you like the work!!!","e5db5667":"RMSE scores : 0.536 (+\/- 0.00000489) [LR]\n\nRMSE scores : 0.631 (+\/- 0.00000388) [Ridge]\n\nRMSE scores : 0.564 (+\/- 0.00000385) [Lasso]","96ed5919":"RF : max_depth=6, n_estimators=41","8747f6cf":"### Dropping the motor speed looking at the vif values","067a339d":"The data description did not provide us with any information on the units of measure. So its difficult to interpret the values measured.","81ee07b2":"pvalue is higher than 0.05. So we fail to reject the null hypothesis and can say that we do not have enough evidence to reject the null hypothesis. So we do not have enough evidence to prove that variance of d component of current is not equal to the variance of motor temperature.","bd281ce3":"### Shuffling the data","968ade09":"H0 : variance_coolant = variance_pm\n\nH1 : variance_coolant != variance_pm","26baf638":"# EDA","c2672e67":"There is no imporvement in our rmse by using elimination d and q components of current. So we will not go ahead with the elimination.","5bee16ab":"LinearRegression 12\n\nLasso 2\n\nRidge 2\n\nDT 3","9a74e836":"As we can see, 96 percent of the variance in data is explained by the first 5 principal components. So we'll choose these 5 components and see if there is any improvement in the Linear model.","08645776":"H0 : variance_i_d = variance_pm\n\nH1 : variance_i_d != variance_pm","00959268":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance of motor speed is not equal to the variance of rotor temperature.","04d1b6f6":"##### Cells with requirement of high computational power (kfold cv) have been commented but the results have been displayed in the following cells.","6cd0d391":"As we can see from the the above plots, the mean and median for most of the plots are very close to each other. So the data seems to have low skewness for almost all variables.","7fdb1867":"##### Observations :\n1. Looking at the pvalues of the each feature, all the them seems to be significant is predicting the stator winding temperature as pvalues are very low.\n2. The Durbin watson test score also is very close to 2, so we can say there seems to be very low autocorrelation in the dataset.\n3. The pvalue for Jarque-Bera test is less that 0.05, so we reject the null hypothesis that the residuals are normally distributed. We will also check for distribution of residuals as well as QQ-plot to check visually.\n4. The pvalue for rainbow test is greater than 0.05, so we fail to reject the null hypothesis and can say that the data follows linearity.\n5. The pvalue for Goldfeld Quantile distribution test is greater than 0.05, so we fail to reject the null hypothesis and can say that the data is homoskedastic in nature.\n6. But we can also see that there are high vif value for motor_speed. So we can say that there seems to be some multicollinearity in our model.","1fb9ff53":"# Statistical Analysis of Variables\nWe'll see which particular variables contribute to the rotor temperature individually by checking their statistical significance.","1fdd781b":"### Transforming skewed data and capping outliers","d7051326":"H0 : variance_u_q = variance_pm\n\nH1 : variance_u_q != variance_pm","b639c4bd":"H0 : variance_motor_speed = variance_pm\n\nH1 : variance_motor_speed != variance_pm","f54c0ba2":"### Current q-component","b79b9e6a":"### Voltage d-component","d857c058":"pvalue is less than 0.05. So we reject the null hypothesis and can say that variance for voltage d-component is not equal to the variance of rotor temperature."}}