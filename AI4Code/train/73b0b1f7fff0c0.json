{"cell_type":{"29cb5db4":"code","e4f94d4c":"code","1d13f002":"code","357e82ed":"code","a710fdc5":"code","02ea7635":"code","d9fbbebb":"code","4356cf13":"code","7fe5d06f":"code","ae9f0b67":"code","49b7ae83":"code","12603330":"code","3de4fd87":"code","f6f7449f":"code","2c09ead0":"code","3b65c460":"code","c2b942ac":"code","11fdea1e":"code","04feda51":"code","d7d110a5":"code","1c18319c":"code","27f8d65f":"code","997b73ab":"code","b677fefa":"code","6aae0734":"code","ec9e50a4":"code","5acfce19":"code","e379314e":"code","309462ca":"code","d7955adf":"code","4fddaab7":"code","ac9cc2db":"code","fda183e2":"code","ee209b85":"code","1abb13a5":"code","998ff889":"code","9b879798":"code","2dd186f7":"code","cb41c425":"code","9b7743c8":"code","781fb659":"code","a073c3ec":"code","756f9712":"code","29a083b6":"code","c9e4bfa5":"code","0bfe5e2c":"code","dae90d2d":"code","9a398934":"code","ba5c6b6c":"code","671306f8":"code","30c879d2":"code","0e438374":"code","38bf8e52":"code","62e8e005":"code","f2bbb556":"code","2d3bfb75":"code","b2c0aefd":"code","ec1a32c3":"code","5805c717":"code","c61fd37c":"code","f78c73ae":"code","ceb2d31a":"code","a173c1df":"code","6937249e":"code","ff91c194":"code","330875a3":"code","e9cd87ef":"code","02f98304":"code","331f8550":"code","b397307b":"code","0da3d905":"code","8b5206b4":"code","6d195f7e":"code","86a39749":"code","7874e440":"code","39eaf17b":"code","188727bf":"code","b7f5e3b2":"code","bb257b63":"markdown","2679324f":"markdown","2cf0ac7a":"markdown","7f2c9cf8":"markdown","7ec93b08":"markdown","27a39990":"markdown","4713ea31":"markdown","84f8d495":"markdown","85a09659":"markdown","d2a45bd6":"markdown","66048aef":"markdown","098bf709":"markdown","5334c27a":"markdown","8f916db3":"markdown","f100a6e7":"markdown","ae20ac6d":"markdown","ce40baf5":"markdown","ab152947":"markdown","7898bf8d":"markdown","bf3354e1":"markdown","a2adb9f7":"markdown","c7a6ae6f":"markdown","64cae8f4":"markdown","12ee0506":"markdown","f878dd3a":"markdown","9193fbf1":"markdown","5e2fefab":"markdown","176bf430":"markdown","8cde5312":"markdown","1e3fe759":"markdown","cf35afca":"markdown","8900941a":"markdown","37e662ea":"markdown","f0a47578":"markdown","947ee9ac":"markdown","a34620b2":"markdown","88db968b":"markdown","8e045e02":"markdown","50f86df1":"markdown","c7e7606b":"markdown"},"source":{"29cb5db4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4f94d4c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing as skpe\nimport sklearn.metrics as sklm\nimport sklearn.model_selection as ms\nimport sklearn.tree as tree\nimport sklearn.ensemble as ensemble\nimport sklearn.linear_model as lm\nimport scipy.stats as stats\nimport numpy.random as nr\nimport sklearn.neighbors as neighbors\nimport xgboost as xgb\nimport lightgbm as lgb","1d13f002":"path1 = \"..\/input\/titanic\/train.csv\"\ntrain = pd.read_csv(path1)\ntrain.head()","357e82ed":"path2 = \"..\/input\/titanic\/test.csv\"\ntest = pd.read_csv(path2)\ntest.head()","a710fdc5":"print(train.info())\nprint(\"\\n\")\nprint(\"--------------------------------------\")\nprint(test.info())","02ea7635":"train.describe()","d9fbbebb":"test.describe()","4356cf13":"# Let's look at the target variable first\ntrain['Survived'].describe()","7fe5d06f":"sns.distplot(train['Age'])","ae9f0b67":"sns.boxplot(y=train['Age'], data=train, width=0.2, palette='autumn')","49b7ae83":"sns.distplot(train['Fare'])","12603330":"# Let's look at the sex ratio.\n(train['Sex'].value_counts()\/len(train['Sex'])*100).plot.bar()","3de4fd87":"# Mean age of both sex groups\ntrain.groupby('Sex')['Age'].mean().plot.bar()","f6f7449f":"# Performing ch-square test to know how different both sex groups are\nstats.chi2_contingency(pd.crosstab(train['Sex'], train['Survived']))","2c09ead0":"# Now, let's look at how gender affected the survival rate of passengers\npd.crosstab(train['Sex'], train['Survived'])","3b65c460":"# Relationship between survived and fare\nsns.set_style(style='whitegrid')\nsns.scatterplot(x='Fare', y='Survived', data=train)","c2b942ac":"train = train.drop(train[((train['Fare'] > 500) & (train['Survived'] > 0.8))].index)","11fdea1e":"# Relationship between fare and age\nsns.scatterplot(x='Age', y='Fare', data=train, legend='brief')","04feda51":"train = train.drop(train[((train['Fare'] > 500) & (train['Age'] > 30))].index)","d7d110a5":"sns.boxplot(data=train, y='Age', x='Pclass')","1c18319c":"sns.boxplot(data=train, y='Age', x='Survived')","27f8d65f":"sns.boxplot(data=train, y='Age', x='Parch')","997b73ab":"sns.boxplot(data=train, y='Age', x='SibSp')","b677fefa":"# Checking distribution of fare\nsns.distplot(train[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))","6aae0734":"# Checking correlation bw different variables\nsns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=.7, cbar=True, annot=True)","ec9e50a4":"corr = train.corr()\n\n# Sort in descending order\ncorr_top = corr['Survived'].sort_values(ascending=False)[:10]\ntop_features = corr_top.index[1:]\nprint(corr_top)","5acfce19":"train[['Pclass', 'Survived']].groupby(['Pclass']).mean().sort_values(by='Survived', ascending=False)","e379314e":"train[['Pclass', 'Survived']].groupby(['Pclass']).mean().sort_values(by='Survived', ascending=False).plot(kind='bar')","309462ca":"train[['Sex', 'Survived']].groupby(['Sex']).mean().sort_values(by='Survived', ascending=False)","d7955adf":"train[['Sex', 'Survived']].groupby(['Sex']).mean().sort_values(by='Survived', ascending=False).plot(kind='bar')","4fddaab7":"train[['Embarked', 'Survived']].groupby(['Embarked']).mean().sort_values(by='Survived', ascending=False)","ac9cc2db":"train[['Embarked', 'Survived']].groupby(['Embarked']).mean().sort_values(by='Survived', ascending=False).plot(kind='bar')","fda183e2":"train[['SibSp', 'Survived']].groupby(['SibSp']).mean().sort_values(by='Survived', ascending=False)","ee209b85":"train[['SibSp', 'Survived']].groupby(['SibSp']).mean().sort_values(by='Survived', ascending=False).plot(kind='bar')","1abb13a5":"train[['Parch', 'Survived']].groupby(['Parch']).mean().sort_values(by='Survived', ascending=False)","998ff889":"train[['Parch', 'Survived']].groupby(['Parch']).mean().sort_values(by='Survived', ascending=False).plot(kind='bar')","9b879798":"\"\"\"\"Q1 = []\nQ3 = []\nLower_Bound = []\nUpper_Bound = []\nOutliers = []\n\nfor i in top_features:\n    \n    # 25th and 75th percentiles\n    q1, q3 = np.percentile(train[i], 25), np.percentile(train[i], 75)\n    \n    # Interquartile range\n    iqr = q3 - q1\n    \n    # Outlier cutoff\n    cut_off = 1.5*iqr\n    \n    # Lower and upper bounds\n    lower_bound = q1 - cut_off\n    upper_bound = q3 + cut_off\n    \n    # Save outlier indexes\n    outlier = [x for x in train.index if train.loc[x,i] < lower_bound or train.loc[x,i] > upper_bound]\n    \n    # Append values for dataframe\n    Q1.append(q1)\n    Q3.append(q3)\n    Lower_Bound.append(lower_bound)\n    Upper_Bound.append(upper_bound)\n    Outliers.append(len(outlier))\n    \n    try:\n        train.drop(outlier, inplace=True, axis=0)\n        \n    except:\n        continue\n        \ndf_out = pd.DataFrame({'column':top_features,'Q1':Q1,'Q3':Q3,'Lower_Bound':Lower_Bound,'Upper_Bound':Upper_Bound,'No. of Outliers':Outliers})\ndf_out.sort_values(by='No. of Outliers', ascending=False)\"\"\"","2dd186f7":"# Now, look at the size of this dataset\ntrain.shape","cb41c425":"# Saving train rows\nntrain = train.shape[0]\n\n# Save the target variable\ntarget = train['Survived']\n\n# Drop Id and SalePrice from train dataframe\ntrain.drop(['PassengerId', 'Ticket', 'Survived'], inplace=True, axis=1)\n\n# Store test Id\ntest_Id = test['PassengerId']\n\n# Drop test Id\ntest.drop(['PassengerId', 'Ticket'], inplace=True, axis=1)\n\n# Concatenate train and test dataframes\ntrain = pd.concat([train, test])","9b7743c8":"train.isnull().sum().sort_values(ascending=False)","781fb659":"# Filling Cabin with most frequent occurences\ntrain['Cabin'].fillna(train['Cabin'].mode()[0], inplace=True)\n\n# Getting the first letter of the cabin as the cabin name\ndef take_section(code):\n    return code[0]\ntrain['Cabin'] = train['Cabin'].apply(take_section)\n\n# Converting all cabin categories into numericals\ntrain['Cabin'].replace(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'],[2,1,4,7,6,5,3,0], inplace=True)\n\n# Filling Embarked with most frequent occurences\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)\n\n# Converting all embarked categories into numericals\ntrain['Embarked'].replace(['Q', 'C', 'S'],[1,2,0], inplace=True)\n\n# Filling age\ntrain['Age'].fillna(train['Age'].astype('float').median(axis=0), inplace=True)\n\n# Filling Fare\ntrain['Fare'].fillna(train['Fare'].astype('float').dropna().median(axis=0), inplace=True)","a073c3ec":"# Getting useful ticket no.\n#ticket = []\n#for i in list(train[\"Ticket\"]):\n    #if i.isdigit():\n        #ticket.append(\"x\")  # Displaying ticket as a 'x' wherever the ticket as a whole is an integer\n    #else:\n        #ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0])  # Else getting the prefix as ticket no.\n        \n#train[\"Ticket\"] = ticket\n#train[\"Ticket\"].head()","756f9712":"# Using expression pattern to extract the Title of the passenger\ntrain['Title'] = train['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Changing to common category\ntrain['Title'] = train['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Countess', 'Sir', 'Jonkheer', 'Lady', 'Capt', 'Don', 'Dona'], 'Others')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\n\n# Converting all Title categories into numericals\ntitle_mapping = {\"Mr\": 1, \"Miss\": 4, \"Mrs\": 5, \"Master\": 3, \"Others\": 2}\ntrain['Title'] = train['Title'].map(title_mapping)\ntrain['Title'] = train['Title'].fillna(0)\n\n# After getting title from name, drop the Name variable\ntrain.drop(['Name'],axis=1,inplace=True)\n\ntrain.head()","29a083b6":"# Forming ageband\ntrain['AgeBand'] = pd.cut(train['Age'], 5)\n\n# Overwriting values in age with the help of ageband\ntrain.loc[train['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[train['Age'] > 64, 'Age'] = 4\n\ntrain.head()","c9e4bfa5":"# Now remove this feature\ntrain = train.drop(['AgeBand'],axis=1)\ntrain.head()","0bfe5e2c":"# Similarly forming Fareband\ntrain['FareBand'] = pd.cut(train['Fare'], 4)\n\n# Overwriting values in age with the help of ageband\ntrain.loc[train['Fare'] <= 7.91, 'Fare'] = 0\ntrain.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\ntrain.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31), 'Fare'] = 2\ntrain.loc[train['Fare'] > 31, 'Fare'] = 3\n\ntrain.head()","dae90d2d":"# Now remove this feature\ntrain = train.drop(['FareBand'],axis=1)\ntrain.head()","9a398934":"# Getting family size from sibling\/spouse and parent\/children variable and adding 1 is for the person himself \ntrain['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n\n# Converting all FamilySize categories into numericals\ntrain['FamilySize'].replace([1,2,3,4,5,6,7,8,11],[3,5,6,7,2,1,4,0,0], inplace=True)\n\ntrain.head()","ba5c6b6c":"# Converting categorical feature into numericals\ntrain['Sex'] = train['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","671306f8":"train.head()","30c879d2":"# Getting new features from FamilySize\n#train['Single'] = train['FamilySize'].map(lambda x: 1 if x == 1 else 0)\n#train['SmallFam'] = train['FamilySize'].map(lambda x: 1 if 2 <= x <= 3 else 0)\n#train['MedFam'] = train['FamilySize'].map(lambda x: 1 if 4 <= x <= 5 else 0)\n#train['LargeFam'] = train['FamilySize'].map(lambda x: 1 if x >= 6 else 0)\n\n# Dropping this feature\n#train = train.drop(['FamilySize'], axis=1)\n#train.head()","0e438374":"rand_state = 25\n# Train dataset\ndf = train.iloc[:ntrain,:]\n\n# Test dataset\ntest = train.iloc[ntrain:,:]\n\n# Seperating independent and dependent variables\nX = df\ny = target\n\n# train,test split to get training,validation and testing\nX_train,X_test,y_train,y_test = ms.train_test_split(X, y, random_state=rand_state, test_size=0.2)","38bf8e52":"#Validation function\nn_folds = 5\n\ndef scores_cv(model):\n    kf = ms.StratifiedKFold(n_folds, shuffle=True, random_state=rand_state).get_n_splits(train.values)\n    scores = ms.cross_val_score(model, X_train, y_train, scoring=\"accuracy\", cv = kf)\n    return(scores)","62e8e005":"DTC = tree.DecisionTreeClassifier(random_state=rand_state)\nDTC.fit(X_train, y_train)","f2bbb556":"ABC = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(random_state=rand_state),random_state=rand_state,learning_rate=0.1)\nABC.fit(X_train, y_train)","2d3bfb75":"XGBC = xgb.XGBClassifier(learning_rate=0.05,random_state =rand_state)\nXGBC.fit(X_train, y_train)","b2c0aefd":"LGBMC = lgb.LGBMClassifier(learning_rate=0.05)\nLGBMC.fit(X_train, y_train)","ec1a32c3":"RFC = ensemble.RandomForestClassifier(random_state=rand_state)\nRFC.fit(X_train, y_train)","5805c717":"KNNC = neighbors.KNeighborsClassifier(n_neighbors=7)\nKNNC.fit(X_train, y_train)","c61fd37c":"LR = lm.LogisticRegression(random_state = rand_state)\nLR.fit(X_train, y_train)","f78c73ae":"scores = scores_cv(DTC)\nprint(\"\\nDecision Tree score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","ceb2d31a":"scores = scores_cv(ABC)\nprint(\"\\nAda Boost score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","a173c1df":"scores = scores_cv(XGBC)\nprint(\"\\nXG Boost score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","6937249e":"scores = scores_cv(LGBMC)\nprint(\"\\nLightGBM score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","ff91c194":"scores = scores_cv(RFC)\nprint(\"\\nRandom Forest score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","330875a3":"scores = scores_cv(KNNC)\nprint(\"\\nKNN score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","e9cd87ef":"scores = scores_cv(LR)\nprint(\"\\nLogistic Regression score: {:.4f} ({:.4f})\\n\".format(scores.mean(), scores.std()))","02f98304":"param_dist = {'num_leaves':stats.randint(1,20), 'max_depth':stats.randint(1,15), 'learning_rate':[0.05, 0.1, 0.3]\n              , 'n_estimators':[100, 300, 500], 'min_child_weight':nr.random(5), 'min_child_samples':stats.randint(1,20)\n              , 'subsample':nr.random(1), 'colsample_bytree':nr.random(1)}\nLightGBM = lgb.LGBMClassifier(random_state=rand_state)\nLightGBM_cv = ms.RandomizedSearchCV(LightGBM,param_distributions=param_dist,cv=5)\nLightGBM_cv.fit(X_train, y_train)\nprint(\"Tuned LightGBM Parameters: {}\".format(LightGBM_cv.best_params_)) \nprint(\"Best score is {}\".format(LightGBM_cv.best_score_))","331f8550":"param_dist = {'colsample_bytree':nr.random(1), \"learning_rate\":[0.05, 0.01, 0.1, 0.3]\n              , \"max_depth\":stats.randint(1,20), \"min_child_weight\":nr.random(5)\n              , \"n_estimators\":[100, 300, 500]\n              , \"subsample\":nr.random(1)}\nXGBC = xgb.XGBClassifier(random_state=rand_state)\nXGBC_cv = ms.RandomizedSearchCV(XGBC,param_distributions=param_dist,cv=5)\nXGBC_cv.fit(X_train, y_train)\nprint(\"Tuned XGBoost Parameters: {}\".format(XGBC_cv.best_params_)) \nprint(\"Best score is {}\".format(XGBC_cv.best_score_)) ","b397307b":"param_dist = {'n_estimators':[100,200,300,400,500,600], 'criterion':['gini','entropy']\n              , 'max_depth':stats.randint(1,15), 'max_features':stats.randint(1,9), 'min_samples_leaf':stats.randint(1,9)}\nRFC = ensemble.RandomForestClassifier(random_state=rand_state)\nRFC_cv = ms.RandomizedSearchCV(RFC,param_distributions=param_dist,cv=5)\nRFC_cv.fit(X_train, y_train)\nprint(\"Tuned Random Forest Tree Parameters: {}\".format(RFC_cv.best_params_)) \nprint(\"Best score is {}\".format(RFC_cv.best_score_)) ","0da3d905":"param_dist = {'C':[.1,1,10,100,1000]}\nLR = lm.LogisticRegression(random_state=rand_state)\nLR_cv = ms.RandomizedSearchCV(LR,param_distributions=param_dist,cv=5)\nLR_cv.fit(X_train, y_train)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(LR_cv.best_params_)) \nprint(\"Best score is {}\".format(LR_cv.best_score_))","8b5206b4":"RFC_best = ensemble.RandomForestClassifier(criterion='entropy', max_depth=4, max_features=4, min_samples_leaf=4, n_estimators=500)\nXGB_best = xgb.XGBClassifier(colsample_bytree=0.11928413027995177, learning_rate=0.1, max_depth=7, min_child_weight=0.6181595928891843\n                             , n_estimators=500, subsample=0.8332606985908653)\nLGBM_best = lgb.LGBMClassifier(colsample_bytree=0.1050164438734883, learning_rate=0.05, max_depth=11, min_child_samples=13\n                               , min_child_weight=0.7362051196763799, n_estimators=300, num_leaves=9, subsample=0.5648846078715707)\nLR_best = lm.LogisticRegression(C=0.1)","6d195f7e":"#votingC = ensemble.VotingClassifier(estimators=[('RFC', RFC_best), ('LR', LR_best),\n#('XGB', XGB_best), ('LGBM', LGBM_best)], voting='hard', n_jobs=4)\n\n#votingC.fit(X_train, y_train)\n\n# Filling the predictions into test_Survived\n#test_survived = pd.Series(votingC.predict(test), name=\"Survived\")","86a39749":"LGBM_best = lgb.LGBMClassifier(colsample_bytree=0.1050164438734883, learning_rate=0.05, max_depth=11, min_child_samples=13\n                               , min_child_weight=0.7362051196763799, n_estimators=300, num_leaves=9, subsample=0.5648846078715707)\nLGBM_best.fit(X_train, y_train)\n\ntest_survived = pd.Series(LGBM_best.predict(test), name=\"Survived\")","7874e440":"RFC_best = ensemble.RandomForestClassifier(criterion='entropy', max_depth=4, max_features=4, min_samples_leaf=4, n_estimators=500)\nRFC_best.fit(X_train, y_train)\n\ntest_survived = pd.Series(RFC_best.predict(test), name=\"Survived\")","39eaf17b":"#XGB_best = xgb.XGBClassifier(colsample_bytree=0.11928413027995177, learning_rate=0.1, max_depth=7, min_child_weight=0.6181595928891843\n                             #, n_estimators=500, subsample=0.8332606985908653)\n#XGB_best.fit(X_train, y_train)\n#test_survived = pd.Series(XGB_best.predict(test), name=\"Survived\")","188727bf":"#LR_best = lm.LogisticRegression(C=0.1)\n#LR_best.fit(X_train, y_train)\n#test_survived = pd.Series(LR_best.predict(test), name=\"Survived\")","b7f5e3b2":"subm_dict = {'PassengerId':test_Id, 'Survived':test_survived}\nsubmit = pd.DataFrame(subm_dict)\nsubmit.to_csv('titanic_submission_a.csv', index=False)","bb257b63":"# Variable Transformation","2679324f":"# Outliers","2cf0ac7a":"* Random Forest Classifier","7f2c9cf8":"**One of the main theoretical backings to motivate the use of random search in place of grid search is the fact that for most cases, hyperparameters are not equally important.**\n\n*A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. - Bergstra, 2012*\n\n**The grid search strategy blatantly misses the optimal model and spends redundant time exploring the unimportant parameter whereas the random search has much improved exploratory power and can focus on finding the optimal value for the important hyperparameter.**","7ec93b08":"# Modelling","27a39990":"**Let's look at the the relationship of other features with age.**","4713ea31":"**Hyperparameter Tuning can be done with two methods:-**\n\n1. GridSearchCV\n2. RandomizedSearchCV","84f8d495":"# Missing Data","85a09659":"# Final Prediction","d2a45bd6":"# Feature Transformation","66048aef":"**Define a cross-validation function**","098bf709":"*With almost 65% men onboard, clearly women were in minority. But, women were given major preference at the time of sinking of ship.*","5334c27a":"* KNN Classifier","8f916db3":"**Submission**","f100a6e7":"*I have made a theory about what features could have been responsible in saving a person's life in that situation:*\n\n* Women and children would have been given a high preference than men(Sex and Age).\n* Preference would have been given in accordance of their status and money(Pclass and Fare)\n\n*Also, there could be many more, we will find out later in this notebook, when we will do Feature Engineering.*","ae20ac6d":"*We can clearly see that women have a better survival rate(74.2%) than men(18.9%).*","ce40baf5":"*Most of the passengers are between the age of 20-40.*","ab152947":"*Looking at this plot, we have a pretty rough idea about the inter-quartile range, median and the outliers.*","7898bf8d":"*There are two points above the 500 fare mark which are not following the usual trend.*","bf3354e1":"* Age","a2adb9f7":"* Ada Boost Classifier","c7a6ae6f":"*If you found this notebook helpful and hopefully learned smething new, please vote. THANK YOU!!!*","64cae8f4":"*Most people were travelling with almost no Fare.There could be two reasons to explain this:-*\n\n* Either they were a part of crew working in the ship\n* Or, they could be a trespasser(like Jack!! in the movie).","12ee0506":"*It seems that Pclass, SibSp and Parch are showing wide variations age variable.*","f878dd3a":"* XG Boost Classifier","9193fbf1":"*It seems that the variable Survived is not a variable which gets along with any of them.*","5e2fefab":"* Fare","176bf430":" **Combinig Models**\n\n I have chosen a voting classifier to combine the predictions coming from the 4 classifiers.I preferred to pass the argument \"soft\" to the voting parameter to take into account the probability of each vote.","8cde5312":"# Base models\n\n1. Decision Tree\n2. Ada Boost\n3. XG Boost\n4. Light GBM\n5. Random Forest\n6. KNN\n7. Logistic Regression","1e3fe759":"# Univariate Analysis","cf35afca":"# Hyperparameter Tuning","8900941a":"* Logistic Regression","37e662ea":"* Decision Tree Classifier","f0a47578":"**Let's see how survival rate is affected by features(objects):** \n1. Pclass\n2. Sex\n3. SibSp\n4. Parch\n5. Embarked","947ee9ac":"*First value is chi-square value and the second one is the p-value(<0.05 in order to be statistically different). *","a34620b2":"* LightGBM Classifier ","88db968b":"# Bivariate Analysis","8e045e02":"# Feature Engineering","50f86df1":"*Now I will perform Randomized Search on the top 4 models(LightGBM, XGBoost, Logistic Regression and Random Forest) which got more than 80% accuracy.*","c7e7606b":"*The median of both(survived or not survived) with age is almost same(28 approx.)*"}}