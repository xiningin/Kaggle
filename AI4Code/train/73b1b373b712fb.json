{"cell_type":{"af89facc":"code","1d889363":"code","716d3201":"code","bdf50acc":"code","5e78d49d":"code","b8f64673":"code","3a110a1a":"code","d914ed37":"code","46029e8a":"code","ddca035a":"code","bf8edec5":"code","8e34d123":"code","66e97580":"code","a136ad97":"code","9e2d248d":"code","80dc1499":"code","a6bea470":"code","4652d4c7":"code","ab4f6bd0":"code","1208ca76":"code","71531a9e":"code","beee5f97":"code","aa21d02b":"code","66db289c":"code","f9b43cec":"code","9065607d":"code","8c88943e":"code","1e7e8250":"code","8f7da189":"code","4283c84a":"code","bfe8c885":"code","ac837cd1":"code","1ed82e24":"code","ffb3e15e":"markdown","7de9bbd3":"markdown","8f300c2f":"markdown","781a3731":"markdown","22b6a83a":"markdown","5f86d15f":"markdown","45fb58ec":"markdown","e9ff4f41":"markdown","9e098739":"markdown","980b4a5b":"markdown","fa8b38f9":"markdown","ba6d9408":"markdown","cfde3003":"markdown","02722407":"markdown","b2e2c3ab":"markdown","5c78f467":"markdown","5f73997d":"markdown"},"source":{"af89facc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d889363":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nimport pandas as pd\npd.set_option('display.max_columns', 500)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)","716d3201":"print('Reading datasets')\n\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\n\nfeatures_meta = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/features.csv')\n\nprint('Finished reading')","bdf50acc":"print('Shape of train dataset {} and of features {}'.format(train.shape, features_meta.shape))","5e78d49d":"train = train.query('date > 85')\ntrain_weights = train[train['weight'] != 0]\ndel train\n\ntrain_weights.reset_index(drop=True, inplace=True)","b8f64673":"train_weights.head(n=10)","3a110a1a":"train_weights['resp_all'] = train_weights['resp'] + train_weights['resp_1'] + \\\ntrain_weights['resp_2'] + train_weights['resp_3'] + train_weights['resp_4']\n","d914ed37":"resp_different = ( (((train_weights['resp_all'].values) > 0) & ((train_weights['resp'].values) < 0)) \\\n                  | (((train_weights['resp_all'].values) < 0) & ((train_weights['resp'].values) > 0)) )\nresp_different_count = ((resp_different > 0).astype(int)).sum()","46029e8a":"print('There are {0} differences out of {1} trades if we take in account resp_1,2,3,4 '.format(resp_different_count, train_weights.shape[0]))","ddca035a":"train_weights['action'] = (train_weights['resp_all'].values > 0).astype(int)\ny_full = train_weights.loc[:, 'action']","bf8edec5":"# Number of missing values in each column of training data\nmissing_val_count_by_column = (train_weights.isnull().sum())\nmissing_values_count = missing_val_count_by_column[missing_val_count_by_column > 0]\nprint('number of columns with missing values ', len(missing_values_count))","8e34d123":"plt.hist(missing_values_count.values, bins=50)\nplt.show()","66e97580":"feature_cols = [col_name for col_name in train_weights.columns if 'feature' in col_name]","a136ad97":"print('Number of features ', len(feature_cols))","9e2d248d":"features_selection = {feature:0 for feature in feature_cols}\nprint('Length of feature selection dictionary ', len(features_selection))","80dc1499":"X_not_imputed = train_weights[feature_cols]","a6bea470":"from sklearn.impute import SimpleImputer\nsimple_imputer = SimpleImputer()\n\nX_imputed = pd.DataFrame(simple_imputer.fit_transform(X_not_imputed))\nX_imputed.columns = X_not_imputed.columns","4652d4c7":"del train_weights","ab4f6bd0":"X_imputed.head(n=10)","1208ca76":"def make_classifier(verbosity=0):\n    return xgb.XGBClassifier(\\\n                             n_estimators=1000,\\\n                             max_depth=7,\\\n                             learning_rate=0.05,\\\n                             missing=None,\\\n                             random_state=42,\\\n                             tree_method='gpu_hist',\\\n                             subsample=0.8,\\\n                             colsample_bytree=1,\\\n                             eval_metric='auc',\\\n                             objective='binary:logistic',\\\n                             verbosity=verbosity)","71531a9e":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\ndef select_features_xgb(X, y):\n    \"\"\"Return selected features using xgb classifier \"\"\"\n    xgb_model_for_selection = make_classifier()\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                          random_state=42)\n    print('Fit xgb classifier for feature selection with shape of X ', X.shape)\n    xgb_model_for_selection.fit(X_train, y_train,\\\n                                eval_set=[(X_valid, y_valid)],\\\n                                early_stopping_rounds=3)\n\n    model_for_selection = SelectFromModel(xgb_model_for_selection, prefit=True)\n    print('Transform selection model')\n    X_selected = model_for_selection.transform(X)\n    print('Create df with selected features having non-zero variance')\n    selected_features = pd.DataFrame(model_for_selection.inverse_transform(X_selected), index = X.index, columns=X.columns)\n    del X_selected\n    print('Selecting columns wih best features')\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    del selected_features\n\n    selected_columns_list = list(selected_columns)\n    print('Number of selected columns ', len(selected_columns_list))\n    for feature_col in selected_columns_list:\n        features_selection[feature_col] += 1\n    ","beee5f97":"folds = 4\nprint('Starting feature selection with {0} folds and rows of dataframe {1}'.format(folds, X_imputed.shape[0]))\nfor k in range(folds):\n    nb_elem_fold = int(X_imputed.shape[0] \/ folds)\n    left_margin = k * nb_elem_fold\n    right_margin = (k + 1) * nb_elem_fold if k < folds - 1 else X_imputed.shape[0]\n    print('Start for fold ', k)\n    selected_features_per_fold = select_features_xgb(X_imputed[left_margin:right_margin], y_full[left_margin:right_margin])\n    print('End fold ', k)\n","aa21d02b":"del X_imputed","66db289c":"print('Dictionary of feature selection')","f9b43cec":"plt.hist(features_selection.values(), bins=5)\nplt.show()","9065607d":"features_at_least_1 = [feat_col for (feat_col, nb_times) in features_selection.items() if nb_times >= 1]","8c88943e":"print(len(features_at_least_1))","1e7e8250":"all_selected_features = features_at_least_1.copy()\nprint('Number of selected features is ', len(all_selected_features))","8f7da189":"simple_imputer = SimpleImputer()\n\nX_ni_feat_sel = X_not_imputed.loc[:, all_selected_features]\ndel X_not_imputed\n\nX_full = pd.DataFrame(simple_imputer.fit_transform(X_ni_feat_sel))\nX_full.columns = X_ni_feat_sel.columns\ndel X_ni_feat_sel\n\nprint('Shape of X_full is ', X_full.shape)","4283c84a":"X_train, X_valid, y_train, y_valid = train_test_split(X_full, y_full, train_size=0.8, test_size=0.2,\n                                                      random_state=42)\ndel X_full","bfe8c885":"print('Creating classifier...', end='')\nmodel = make_classifier(1)\nprint('Finished.')","ac837cd1":"# Fit the model\nprint('Training classifier...', end='')\n%time model.fit(X_train, y_train,\\\n                eval_set=[(X_valid, y_valid)],\\\n                early_stopping_rounds=3)\nprint('Finished.')\nprint('Done')","1ed82e24":"import janestreet\nenv = janestreet.make_env() \n\nprint('Creating submissions file...', end='')\nrcount = 0\nfor (test_df, prediction_df) in env.iter_test():\n    test_df_selected_features = test_df.loc[:, all_selected_features]\n    X_test = pd.DataFrame(simple_imputer.transform(test_df_selected_features))\n    X_test.columns = test_df_selected_features.columns\n    \n    y_preds = model.predict(X_test)\n    prediction_df['action'] = y_preds\n    env.predict(prediction_df)\n    rcount += len(test_df.index)\nprint(f'Finished processing {rcount} rows.')","ffb3e15e":"Let's choose the column features based on partitions in K folds and a dictionary counting occurrences of selected features per each fold.","7de9bbd3":"Make a dictionary with number of selections per feature, which will be used when splitting the train df into folds","8f300c2f":"No more need for X_imputed","781a3731":"Some cleanup is mandatory, but keep X not imputed for later imputer fitting, when features are selected","22b6a83a":"A lot of NaNs in the features, let's address that via feature selection with xgb classifier and sci-kit learn's selection model.","5f86d15f":"Define the method responsible for selecting a subset of features.\\\nWe'll use sci-kit learn's train test split with a train ratio of 80% and valid ratio of 20%","45fb58ec":"And predict with the provided environment in the submission file","e9ff4f41":"Binarize the targets according to resp_all","9e098739":"Creating XGBoost classifier to be used for predictions","980b4a5b":"See how different the signs of the resp and resp_all columns are","fa8b38f9":"Recreate the imputer and fit it to the df with features that will be selected","ba6d9408":"This notebook is based on [this](https:\/\/www.kaggle.com\/wilddave\/xgb-starter) starter kit. Thanks for sharing it!","cfde3003":"Train the model","02722407":"Eliminate 0-weights and create a binary action column, to be 1 if resp is positive and 0 otherwise.","b2e2c3ab":"We shall select only features that were selected at least once during the folding selection algorithm above","5c78f467":"Define a method to return a XGBoost classifier with same configuration for feature selection and later generating the model used for prediction","5f73997d":"Handle missing values with simple imputer, to be able to select best features"}}