{"cell_type":{"560102a7":"code","1ea7e7fe":"code","343a6e97":"code","8944a89a":"code","cd9ee3a8":"code","5b56c3b1":"code","0053332c":"code","3d984d27":"code","bb2e8883":"code","517f6327":"code","ae0d4221":"code","5c2e1950":"code","82f78370":"code","c80157a6":"code","7e38f47c":"code","bc96106f":"code","c3b844ae":"code","d6fe2a58":"markdown","a15a8eb6":"markdown","e6c25064":"markdown","9c8d2528":"markdown","60bbf8a1":"markdown","59ef45ce":"markdown","acf78897":"markdown","499acc83":"markdown","824bbf43":"markdown","9700ce50":"markdown","267f1bdf":"markdown","2e4d86c6":"markdown","9d7635f5":"markdown","fbc2f3eb":"markdown"},"source":{"560102a7":"# Importing the Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","1ea7e7fe":"# Importing the Dataset\ntry:\n    data = pd.read_csv(\"..\/input\/Wholesale customers data.csv\")\n    data.drop(labels=(['Channel','Region']),axis=1,inplace=True)\n    print('Wholesale customers has {} samples with {} features each'.format(*data.shape))\nexcept:\n    print('Sorry! Dataset could not be loaded.')","343a6e97":"data.head()","8944a89a":"# Display a brief description of the overall dataset\ndata.describe()","cd9ee3a8":"# Display complete information of the data frame\ndata.info()","5b56c3b1":"# Select three indices of your choice you wish to sample from the dataset\nindices = [22,154,398]\n\n# Create a DataFrame of the chosen samples\nsamples = pd.DataFrame(data.loc[indices], columns=data.keys()).reset_index(drop=True)\nprint(\"Chosen samples of wholesale customers dataset:\")\ndisplay(samples)\n","0053332c":"# look at percentile ranks\n#pcts = 100. * data.rank(axis=0, pct=True).iloc[indices].round(decimals=3)\npcts = 100. * data.rank(axis=0, pct=True).iloc[indices].round(decimals=3)\n# visualize percentiles with heatmap\n\nsns.heatmap(pcts, annot=True, vmin=1, vmax=99, fmt='.1f', cmap='YlGnBu')\nplt.title('Percentile ranks of\\nsamples\\' category spending')\nplt.xticks(rotation=45, ha='center');\n","3d984d27":"# Import libraries for Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Remove column Milk\nnew_data = data.drop('Milk',axis=1)","bb2e8883":"# Split the data into training and testing sets(0.25) using the given feature as the target\n# Set a random state.\nX_train, X_test, y_train, y_test = train_test_split(new_data, data['Milk'], test_size=0.25, random_state=1)\n\n# Create a decision tree regressor and fit it to the training set\nregressor =  DecisionTreeRegressor(random_state=1)\nregressor.fit(X_train, y_train)\n\n# Report the score of the prediction using the testing set\nscore = regressor.score(X_test, y_test)\nprint(score)\n","517f6327":"pd.plotting.scatter_matrix(data, alpha=0.3,figsize=(15,8),diagonal='kde' )\nplt.tight_layout() # To avoid overlapping of plots","ae0d4221":"# Scale the data using the natural logarithm\nlog_data = np.log(data.copy())\n\n# Scale the sample data using the natural logarithm\nlog_samples = np.log(samples)\n\n# Produce a scatter matrix for each pair of newly-transformed features\npd.plotting.scatter_matrix(log_data, alpha=0.5, figsize=(14,8),diagonal='kde')\nplt.tight_layout()","5c2e1950":"# Let's compare the original sample data to the log-transformed sample data\nprint(\"Original chosen samples of wholesale customers dataset:\")\ndisplay(samples)\n\n# Display the log-transformed sample data\nprint(\"Log-transformed samples of wholesale customers dataset:\")\ndisplay(log_samples)","82f78370":"# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n\n    # Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data, 25)\n\n    # Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data, 75)\n\n    # Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n# Display the outliers\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    display(log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))])\n    \n    # Select the indices for data points you wish to remove\noutliers  = [66, 75, 338, 142, 154, 289]\n\n# Remove the outliers, if any were specified\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)","c80157a6":"from sklearn.decomposition import PCA\n\n# Apply PCA by fitting the good data with the same number of dimensions as features\npca = PCA(n_components=6)\npca.fit(good_data)\n\n# Transform log_samples using the PCA fit above\npca_samples = pca.transform(log_samples)","7e38f47c":"print(pca.components_)","bc96106f":"print(pca.explained_variance_)","c3b844ae":"pca_samples","d6fe2a58":"#### As you can see, we attempted to predict Milk using the other features in the dataset and the score ended up being 0.515. At this initial stage we might say that this feature is somewhat difficult to predict because the score is around the halfway point of possible scores. Remember that R^2 goes from 0 to 1. This might indicate that it could be an important feature to consider.","a15a8eb6":"#### There were a handful of specific rows containing outliers in multiple features based on our definition of an outlier. I chose to remove these rows because having a row show up as multiple outliers can add to our confidence that it is truly an outlier.","e6c25064":"#### I downloaded this wholesale customer dataset from UCI Machine Learning Repository. The data set refers to clients of a wholesale distributor. It includes the annual spending in monetary units on diverse product categories.\n\n#### My goal today is to use various clustering techniques to segment customers. Clustering is an unsupervised learning algorithm that tries to cluster data based on their similarity. Thus, there is no outcome to be predicted, and the algorithm just tries to find patterns in the data. ","9c8d2528":"#### One interesting thought to consider is if one (or more) of the six product categories is actually relevant for understanding customer purchasing. That is to say, is it possible to determine whether customers purchasing some amount of one category of products will necessarily purchase some proportional amount of another category of products? We can make this determination quite easily by training a supervised regression learner on a subset of the data with one feature removed, and then score how well that model can predict the removed feature.","60bbf8a1":"## Observation\nLets run the code below to see how the log-transformed sample data has changed after having a PCA transformation applied to it in six dimensions. Observe the numerical value for the first four dimensions of the sample points.","59ef45ce":"## Feature Transformation\nIn this section we will use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.","acf78897":"## Data Preprocessing\nNow we will start to preprocess the data to create a better representation of customers by performing a scaling on the data and detecting (and optionally removing) outliers. Preprocessing data is often times a critical step in assuring that results we obtain from your analysis are significant and meaningful.","499acc83":"## Observation\n#### After applying a natural logarithm scaling to the data, the distribution of each feature appears much more normal. For any pairs of features you may have identified earlier as being correlated, observe here whether that correlation is still present (and whether it is now stronger or weaker than before).","824bbf43":"Milk showed some signs of correlation for about half of the features it was compared to which aligns with our earlier prediction. The pair of features with the highest correlation are Detergents_Paper and Grocery which intuitively makes sense as many people shop for both when they go \"grocery shopping.\" One other visible point to note is how many of the points are around 0 for features compared to Delicatessen. The data for all of these features are right-skewed with many points hovering at the origin or near it and long tails.","9700ce50":"## Implementation: Feature Scaling\n#### Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.","267f1bdf":"#### Samples: - 0: This customer ranks above the 90th percentile for annual spending amounts in Fresh, Frozen, and the Delicatessen categories. These features along with above average spending for detergents_paper could lead us to believe this customer is a market. Markets generally put an emphasis on having a large variety of fresh foods available and often contain a delicatessen or deli.\n 1: On the opposite side of the spectrum, this customer ranks in the bottom 10th percentile across all product categories. It's highest ranking category is 'Fresh' which might suggest it is a small cafe or similar.\n \n2: Our last customer spends a lot in the Fresh and Frozen categories but moreso in the latter. I would suspect this is a wholesale retailer because of the focus on Fresh and Frozen foods.","2e4d86c6":"### Visualize Feature Distributions","9d7635f5":"## Implementation: PCA\nNow that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can go ahead and apply PCA to the good_data to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the explained variance ratio of each dimension \u2014 how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.","fbc2f3eb":"## Implementation: Outlier Detection\n#### Detecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use [Tukey's Method](http:\/\/datapigtechnologies.com\/blog\/index.php\/highlighting-outliers-in-your-data-with-the-tukey-method\/) for identfying outliers: An outlier step is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal."}}