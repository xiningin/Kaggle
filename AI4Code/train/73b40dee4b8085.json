{"cell_type":{"c7dc86e5":"code","6a939c0e":"code","ec28b636":"code","e35246ac":"code","3c52bc43":"code","58a65e73":"code","36649874":"code","f6c2b218":"code","bbc51fd7":"code","9036f28a":"code","cef7d6eb":"code","b6b07fa7":"code","9782033b":"code","464b7eb8":"code","29fbd4a6":"code","99d76282":"code","316838c6":"code","c96940cb":"code","3929722d":"code","2912bde0":"code","46877c44":"code","05305d7a":"code","20e12d56":"markdown","acdfba44":"markdown","60bebf88":"markdown","87aec9c9":"markdown","eac83a28":"markdown","190358f2":"markdown","8fa519e0":"markdown","c79fcbda":"markdown","a092dae1":"markdown","24fa0869":"markdown","ac39f051":"markdown","713ee2dd":"markdown","bc958d79":"markdown","2f6b26db":"markdown","48c9fbbf":"markdown"},"source":{"c7dc86e5":"from IPython.display import clear_output","6a939c0e":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport xgboost as xg\n\nclear_output()","ec28b636":"abrasan_df = pd.read_csv(\"..\/input\/tapds\/TabrizPollution\/Abrasan.csv\",delimiter=';')\nbashumal_df = pd.read_csv(\"..\/input\/tapds\/TabrizPollution\/Bashumal.csv\",delimiter=\";\")\nrastakucha_df = pd.read_csv(\"..\/input\/tapds\/TabrizPollution\/RastaKucha.csv\",delimiter=';')","e35246ac":"df = pd.concat([abrasan_df,bashumal_df,rastakucha_df])","3c52bc43":"df.head(100)","58a65e73":"filter_criteria = (df['air_temperature'] != -9999.0) & (df['dewpoint'] != -9999.0) & (df['wind_direction_corr'] != -9999.0) & (df['wind_speed'] != -9999.0) & (df['relative_pressure'] != -9999.0) & (df['PM10'] != -9999.0) & (df['PM2.5'] != -9999.0)\nprevious_length = len(df)\ndf = df.loc[filter_criteria]\nprint(\"Number of rows with missing data: \",previous_length-len(df))\ndel previous_length","36649874":"df.info()","f6c2b218":"df.isnull().sum()","bbc51fd7":"df.describe()","9036f28a":"print(\"Number of duplicates: \" , len(df[df.duplicated()]))","cef7d6eb":"del df['Time']","b6b07fa7":"X = np.array(df[df.columns[:-1]])\ny = np.array(df['PM2.5'])","9782033b":"scaler = MinMaxScaler(copy=True, feature_range=(-1, 1))\n\nX = scaler.fit_transform(X)","464b7eb8":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)","29fbd4a6":"model1 = LinearRegression()\n\nmodel1.fit(X_train,y_train)\n\nprint(\"Simple Linear Regression Score: \", round(model1.score(X_test,y_test)*100,3), \"%\")\n\nmodel1 = Ridge(alpha = 0.08)\n\nmodel1.fit(X_train,y_train)\n\nprint(\"Ridge Regression Score: \", round(model1.score(X_test,y_test)*100,3), \"%\")\n\nmodel1 = Lasso(alpha = 0.001)\n\nmodel1.fit(X_train,y_train)\n\nprint(\"Lasso Regression Score: \", round(model1.score(X_test,y_test)*100,3), \"%\")","99d76282":"poly2 = PolynomialFeatures(degree=2)\npoly3 = PolynomialFeatures(degree=3)\npoly4 = PolynomialFeatures(degree=4)\npoly5 = PolynomialFeatures(degree=5)\npoly6 = PolynomialFeatures(degree=6)\npoly7 = PolynomialFeatures(degree=7)\n\nX_train_2 = poly2.fit_transform(X_train)\nX_train_3 = poly3.fit_transform(X_train)\nX_train_4 = poly4.fit_transform(X_train)\nX_train_5 = poly5.fit_transform(X_train)\nX_train_6 = poly6.fit_transform(X_train)\nX_train_7 = poly7.fit_transform(X_train)\n\n\nmodel1 = LinearRegression()\n\nmodel1.fit(X_train_2,y_train)\nprint(\"Polynomial Regression accuracy(degree=2): \", round(model1.score(poly2.fit_transform(X_test),y_test)*100,3), \"%\")\nmodel1.fit(X_train_3,y_train)\nprint(\"Polynomial Regression accuracy(degree=3): \", round(model1.score(poly3.fit_transform(X_test),y_test)*100,3), \"%\")\nmodel1.fit(X_train_4,y_train)\nprint(\"Polynomial Regression accuracy(degree=4): \", round(model1.score(poly4.fit_transform(X_test),y_test)*100,3), \"%\")\nmodel1.fit(X_train_5,y_train)\nprint(\"Polynomial Regression accuracy(degree=5): \", round(model1.score(poly5.fit_transform(X_test),y_test)*100,3), \"%\")\nmodel1.fit(X_train_6,y_train)\nprint(\"Polynomial Regression accuracy(degree=6): \", round(model1.score(poly6.fit_transform(X_test),y_test)*100,3), \"%\")\nmodel1.fit(X_train_7,y_train)\nprint(\"Polynomial Regression accuracy(degree=7): \", round(model1.score(poly7.fit_transform(X_test),y_test)*100,3), \"%\")\n","316838c6":"model2 = DecisionTreeRegressor()\n\nmodel2.fit(X_train, y_train)\nprint(\" Decision Tree Regressor Score: \", round(model2.score(X_test, y_test)*100,3),\"%\")\n\nmodel2 = RandomForestRegressor(max_depth=10)\n\nmodel2.fit(X_train, y_train)\n\nprint(\"Random Forest Regressor Score: \", round(model2.score(X_test, y_test)*100,3),\"%\")\n\nmodel2 = xg.XGBRegressor(objective ='reg:squarederror')\nmodel2.fit(X_train, y_train)\n\nprint(\"XGBoost Regressor Score: \", round(model2.score(X_test, y_test)*100,3),\"%\")","c96940cb":"def scheduler(epoch, lr):\n  if epoch < 20:\n    return lr\n  elif epoch < 80:\n    return lr * np.exp(-0.03)\n  else:\n    return lr","3929722d":"model3 = Sequential()\nmodel3.add(InputLayer(input_shape=(6,)))\n\nfor _ in range(3):\n    model3.add(Dense(128,activation=\"relu\",kernel_initializer=\"normal\"))\n\nmodel3.add(Dense(1,activation=\"linear\",kernel_initializer=\"normal\"))\n\noptim = Adam(\n    learning_rate=0.0005,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name=\"Adam\",\n)\ncallback = LearningRateScheduler(scheduler)\n\nmodel3.compile(loss=\"mse\",optimizer=optim,metrics=\"mae\")\nclear_output()","2912bde0":"model3.fit(X_train,y_train,batch_size=32,epochs=250,validation_data=(X_test,y_test),callbacks = [callback])","46877c44":"predictions = model3.predict(X_test)\n\nprint(\"Neural Networks Score: \",metrics.r2_score(y_test,predictions)*100)","05305d7a":"random_index = np.random.randint(0,X_test.shape[0])\n\n\nprint(\"Prediction made by XGBoost Regressor:\")\nprint(\"\\t\\tPredicted price:  \", np.round(model2.predict(X_test[random_index:random_index+1])[0],3))\nprint(\"\\t\\tActual price:  \", np.round(y_test[random_index],3))\n\nprint(\"Prediction made by Neural Networks:\")\nprint(\"\\t\\tPredicted price:  \", np.round(model3.predict(X_test[random_index:random_index+1])[0][0],3))\nprint(\"\\t\\tActual price:  \", np.round(y_test[random_index],3))","20e12d56":"<p style=\"font-size:110%;\">So, we identified number of missing data and got rid of that. Now let's process it.<p>","acdfba44":"<h2>Random Demonstration<\/h2>","60bebf88":"<p style=\"font-size:130%;\"><b>Neural Networks<\/b><\/p>","87aec9c9":"<h2> Preparing and Testing regression models <\/h2>","eac83a28":"<p style=\"font-size:130%;\"><b>Simple Linear Regression<\/b><\/p>","190358f2":"<h2> Reading and processing data <\/h2>","8fa519e0":"<p style=\"font-size:120%;\">We can notice that when we increase degree of polynomial, accuracy increases, but at some point it starts to decrease dramatically.<\/p>","c79fcbda":"<p style=\"font-size:110%;\">These 3 dataframes represents air pollution data for different regions in Tabriz. For our purposes we will concatinate them.<p>","a092dae1":"<p style=\"font-size:130%;\"> <b> Polynomial Regression <\/b><\/p>","24fa0869":"<p style=\"font-size:120%;\"><b>Reading data<\/b><\/p>","ac39f051":"<p style=\"font-size:110%;\">Here we see lots of -9999 and most probably they represent missing data. Let's find how much of them we have.<\/p>","713ee2dd":"<p style=\"font-size:110%;\">Data is ready, now we can train and test our models.<\/p>","bc958d79":"<p style=\"font-size:130%;\"><b>Tree based algorithms<\/b><\/p>","2f6b26db":"<p style=\"font-size:130%;\"><b>Processing data<\/b><\/p>","48c9fbbf":"<p style=\"font-size:120%;\"><b>Imports<\/b><p>"}}