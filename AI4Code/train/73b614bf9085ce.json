{"cell_type":{"412294eb":"code","8ec1f811":"code","c27ca252":"code","2f7488ec":"code","3c74e4f0":"code","6909a357":"code","51754c61":"code","4f34b84a":"code","fdff95b4":"code","18ead615":"code","92493a38":"code","c46d60f9":"code","83a28834":"code","e604bd08":"code","5fd2d9a6":"code","ee53e63f":"code","83ea5e6c":"code","a8ca46df":"code","ccafc537":"code","aecfa126":"code","d26b10a5":"code","d824e73f":"code","e03dc97b":"markdown","c7760736":"markdown","716bbea3":"markdown","2dac288b":"markdown","fffc1159":"markdown","05dd9202":"markdown","ed67d0f2":"markdown","a6e1f2cc":"markdown","8dfc4ce4":"markdown","cbcd25c8":"markdown","5d2ac201":"markdown","a86e02bf":"markdown","dca33d85":"markdown","a4cede5f":"markdown","ac2c7c6d":"markdown","d1e28360":"markdown","b7d66002":"markdown","ae241acd":"markdown"},"source":{"412294eb":"# Imports related to the CNN\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, MaxPool2D\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.applications.resnet50 import ResNet50\n\n# Utility Imports\nimport os\nimport time\nimport cv2\n\n# Imports related to visualisation\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, mean_absolute_error","8ec1f811":"BATCH_SIZE = 32\nEPOCHS = 50\nIMG_SIZE = 175\nBASE_DIR = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/'\nLABELS = ['NORMAL', 'PNEUMONIA']\n\ntrain_dir = os.path.join(BASE_DIR, 'train')\nval_dir = os.path.join(BASE_DIR, 'val')\ntest_dir = os.path.join(BASE_DIR, 'test')","c27ca252":"def get_preview_data(data_dir):\n    data = []\n    for label in LABELS:\n        path = os.path.join(data_dir, label)\n        class_num = LABELS.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                img_arr = cv2.resize(img_arr, (IMG_SIZE, IMG_SIZE))\n                data.append([img_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","2f7488ec":"val = get_preview_data(val_dir)\ntrain = get_preview_data(train_dir)\ntest = get_preview_data(test_dir)","3c74e4f0":"x_train = []\ny_train = []\n\nx_val = []\ny_val = []\n\nx_test = []\ny_test = []\n\nfor feature, label in train:\n    x_train.append(feature)\n    y_train.append(label)\n\nfor feature, label in test:\n    x_test.append(feature)\n    y_test.append(label)\n    \nfor feature, label in val:\n    x_val.append(feature)\n    y_val.append(label)\n    \n# Normalize the data\nx_train = np.array(x_train) \/ 255\nx_val = np.array(x_val) \/ 255\nx_test = np.array(x_test) \/ 255\n\n# Resize for the model\nx_train = x_train.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_train = np.array(y_train)\n\nx_val = x_val.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_val = np.array(y_val)\n\nx_test = x_test.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny_test = np.array(y_test)","6909a357":"fig, ax = plt.subplots(1, 4, figsize=(12, 12))\nax = ax.flatten()\nax[0].imshow(val[0][0], cmap='gray')\nax[0].set_title(LABELS[val[0][1]])\nax[1].imshow(val[14][0], cmap='gray')\nax[1].set_title(LABELS[val[14][1]])\nax[2].imshow(val[6][0], cmap='gray')\nax[2].set_title(LABELS[val[6][1]])\nax[3].imshow(val[11][0], cmap='gray')\nax[3].set_title(LABELS[val[11][1]])\nplt.tight_layout()\nplt.show()","51754c61":"img_gen_train = ImageDataGenerator(\n    horizontal_flip = True,\n    zoom_range = 0.2,\n    rotation_range = 30,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1,\n    fill_mode = 'nearest'\n)\n\nimg_gen_def = ImageDataGenerator()","4f34b84a":"img_gen_train.fit(x_train)","fdff95b4":"def plot_augmented_img(img_arr):\n    fig, axs = plt.subplots(1, 5, figsize=(20, 20))\n    axs = axs.flatten()\n    for img, ax in zip(img_arr, axs):\n        ax.imshow(np.squeeze(img), cmap='gray')\n    plt.tight_layout()\n    plt.show()","18ead615":"train_aug_imgs = [x_train[i] for i in range(5)]\nplot_augmented_img(train_aug_imgs)","92493a38":"callback = []\ncallback.append(ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=1, factor=0.3, min_lr=0.000001))\ncallback.append(ModelCheckpoint('model-checkpoint.h5', monitor='val_loss', save_best_only=True))\ncallback.append(EarlyStopping(patience=25, monitor='val_loss'))","c46d60f9":"model = Sequential([\n    Conv2D(32, (3, 3), strides=1, padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n    BatchNormalization(),\n    MaxPooling2D((2, 2), strides=2, padding='same'),\n    \n    Conv2D(32, (3, 3), strides=1, padding='same', activation='relu'),\n    Dropout(0.3),\n    BatchNormalization(),\n    MaxPooling2D((2, 2), strides=2, padding='same'),\n    \n    Conv2D(64, (3, 3), strides=1, padding='same', activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D((2, 2), strides=2, padding='same'),\n    \n    Conv2D(128, (3, 3), strides=1, padding='same', activation='relu'),\n    Dropout(0.3),\n    BatchNormalization(),\n    MaxPooling2D((2, 2), strides=2, padding='same'),\n    \n    Conv2D(256, (2, 2), strides=1, padding='same', activation='relu'),\n    Dropout(0.3),\n    BatchNormalization(),\n    MaxPooling2D((2, 2), strides=2, padding='same'),\n    \n    Flatten(),\n    Dense(units=256, activation='relu'),\n    Dropout(0.3),\n    Dense(units=128, activation='relu'),\n    Dropout(0.2),\n    Dense(units=1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()","83a28834":"history = model.fit(\n    img_gen_train.flow(x_train, y_train, batch_size=32),\n    epochs=EPOCHS,\n    steps_per_epoch=163,\n    validation_data=img_gen_def.flow(x_val, y_val),\n    callbacks=[callback]\n)\n\nt = time.time()\nexport_path_keras = \".\/pneumonia-model2-{}.h5\".format(int(t))\nmodel.save(export_path_keras)","e604bd08":"model = keras.models.load_model('.\/model-checkpoint.h5')","5fd2d9a6":"epoch_range = range(41)\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","ee53e63f":"fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\nax[0].plot(epoch_range, acc, label='Training Accuracy')\nax[0].plot(epoch_range, val_acc, label='Validation Accuracy')\nax[0].set_xlabel('Training Epoch')\nax[0].set_ylabel('Accuracy (%)')\nax[0].set_title('Model Accuracy Variation over Training Period')\nax[0].legend()\n\nax[1].plot(epoch_range, loss, label='Training Loss')\nax[1].plot(epoch_range, val_loss, label='Validation Loss')\nax[1].set_xlabel('Training Epoch')\nax[1].set_ylabel('Loss')\nax[1].set_title('Model Loss Variation over Training Period')\nax[1].legend()\n\nplt.grid(True)\nplt.show()","83ea5e6c":"print('Model Accuracy:', model.evaluate(img_gen_def.flow(x_test, y_test, batch_size=32))[1])\nprint('Model Loss:', model.evaluate(img_gen_def.flow(x_test, y_test, batch_size=32))[0])","a8ca46df":"predictions = model.predict_classes(x_test)\npredictions = predictions.reshape(1,-1)[0]\n\n# Take a random sample of the predictions to plot.\npredictions_length = len(predictions)\nrands = np.random.rand(30)*predictions_length\nrands = np.round(rands, 0).astype(int)\n\npredictions = predictions[rands]\ny_preds = y_test[rands]","ccafc537":"plt.figure(figsize=(20, 20))\n\nfor i, im_num in enumerate(rands):\n    plt.subplot(6,5,i+1)\n    plt.subplots_adjust(hspace = 0.3)\n    plt.imshow(np.squeeze(x_test[im_num]), cmap='gray')\n    color = \"blue\" if predictions[i] == y_preds[i] else \"red\"\n    title_text = str(LABELS[predictions[i]])\n    plt.title(title_text, color=color)\n    plt.axis('off')\n    _ = plt.suptitle(\"Model predictions (blue: correct, red: incorrect)\")\n    \nplt.show()","aecfa126":"confm = confusion_matrix(y_preds, predictions)\nconfm = pd.DataFrame(confm , index = ['0','1'] , columns = ['0','1'])\n\nplt.figure(figsize = (8,8))\nsns.heatmap(confm ,cmap=\"Blues\", linecolor='black' , linewidth=2 , annot=True, fmt='', xticklabels=LABELS, yticklabels=LABELS)","d26b10a5":"print('Mean Absolute Error:', np.round(mean_absolute_error(y_preds, predictions), 3))","d824e73f":"successive_outputs = [layer.output for layer in model.layers[6:8]]\n\nvisualisation_model = keras.models.Model(\n    inputs=model.input,\n    outputs=successive_outputs\n)\n\nrand_num = int(np.floor(np.random.rand()*x_train.shape[0]))\nx = np.squeeze(x_train[rand_num])\nx = x.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n\nsuccessive_feature_maps = visualisation_model.predict(x)\nlayer_names = [layer.name for layer in model.layers]\n\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n    if len(feature_map.shape) == 4:\n        n_features = feature_map.shape[-1]\n        size = feature_map.shape[1]\n        display_grid = np.zeros((size, size*n_features))\n        for i in range(n_features):\n            x = feature_map[0, :, :, i]\n            x -= x.mean()\n            x *= 64\n            x += 128\n            x = np.clip(x, 0, 255).astype('uint8')\n            display_grid[:, i*size:(i+1)*size] = x\n        scale = 20.\/n_features\n        plt.figure(figsize=(2* scale * n_features, 2*scale))\n        plt.title(layer_name),\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')","e03dc97b":"# Imports and Constants\nBelow we import the necessary modules and libraries used in the below code. We also define some constants that will be used later. We follow convention that all constants are named in full capitals.\nThe batch size choice is rather arbitrary and will not drastically affect the model's performance. The image size has a far greater effect for model accuracy. We opt for a modest size of 150 x 150 since this allows details to be preserved whilst reducing compute time significantly. *Using images much larger than about 250 square pixels causes significant increases in compute time.*","c7760736":"Let's test to see the rough effect of our augmentation on the images. We can see if the transformations are appropriate this way.","716bbea3":"## Evaluating Our Model\nLet us examine some metrics to see how good our model is and possibly how it could be further improved. We start simply with the training and validation accuracy over the training period along with the corressponding losses.","2dac288b":"We arbitrarily choose to use the validation images as the ones we use for visualisation as I just happen to know there are not many images in this directory so everything will run a bit quicker.\n","fffc1159":"**Note that this is my first notebook so there are bound to be some mistakes and or bad practices etc. Please point these out constructively if you feel so inclined.**","05dd9202":"# Dataset Loading and Preview\nThe dataset will be loaded in firstly using cv2 and a trivial function (see below). This is just so that we can visualise a few of the images and get a 'feel' for these data before going any further. We reccomend using ImageDataGenerator and the flow_from_directory method to load images in for use in a CNN. \n\nThe data has a file structure as follows;<br>\n`\nchest_xray\n|__ test\n    |____ NORMAL: [IM-0001-0001.jpeg, IM-0003-0001.jpeg, ...]\n    |____ PNEUMONIA: [person100_bacteria_475.jpeg, ...]\n|__ train\n    |____ NORMAL: [IM-0005-0003.jpeg, IM-0005-0004.jpeg, ...]\n    |____ PNEUMONIA: [person100_bacteria_475.jpeg, ...]\n|__ val\n    |____ NORMAL: [IM-0001-0001.jpeg, IM-0003-0001.jpeg, ...]\n    |____ PNEUMONIA: [person100_bacteria_475.jpeg, ...]\n`","ed67d0f2":"## Loading Images","a6e1f2cc":"# Loading and Augmenting Using ImageDataGenerator\nSince there is a significant skew to pneumonia positive images in these data we will augment the training data to provide more images for our model to train on. This also helps to prevent overfitting of the training data. Preventing overfitting was the greatest challenge during this notebook as will be seen below eventually.","8dfc4ce4":"# Introduction\nPneumonia is a \"sweeling (inflammation) of the tissue in one or both lungs. It's usually caused by a bacterial infection. It can also be caused by a virus, such as coronavirus (COVID-19)\" [[NHS]](https:\/\/www.nhs.uk\/conditions\/pneumonia\/). In an unhealthy lung the alveoli fill with fluid. This can be seen with a standard x-ray scan of the chest\/lung region. \n\nIn these data there were 17,568 x-ray images from cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. By using a CNN (convoluted neural network) we were able to develop a model which could correctly classify whether a patient does or does not have pneumonia with an accuracy of ACCURACY.","cbcd25c8":"# Creating and Training the Model\nWe use a sequential CNN model it's summary is given below. Much iteration and testing was taken to get to this point. *Basically, do not be disheartened if your own attempts do not succeed at first because nobodies first attempts do*.","5d2ac201":"### Image Predictions\nSee the below visualisations for some of the models predictions versus the actual x-ray and true value.","a86e02bf":"### Confusion Matrix","dca33d85":"Clearly we could have stopped training at about epoch 20, but it is best to be thorough. We used the ModelCheckpoint callback so it will have saved the optimal model (determined by the minimal validation loss) during the training period. We loaded this model in as our model a few lines above just after training was finished. How does this optimal model perform on the test data?","a4cede5f":"An accuracy of 92%. That seems quite good, I fear that the model may have overfit slightly to the training data so maybe next time I could increase\/add more dropout layers.","ac2c7c6d":"### Other Metrics\nThese are only measured over the small selection of 30 samples I took from all the predictions - mostly for convenience. ","d1e28360":"## Previewing Images\nFrom the preview below we can already calearly see that the chest x-ray with pneumonia has a lot more white\/grey regions. Could this indicate where there is more fluid present in the alveoli, possibly, though I am not a medical expert and so cannot comment. What is useful to know is that there seems to be some distinctive difference between the images.","b7d66002":"Everything seems to be working and our images are clearly distorted. Though they do not seem to have been distorted too much. I suspect this is important since most x-rays will be taken by medical professionals so there will be a high-level of consistency between the images (e.g. little roation, or strangely large amounts of zoom etc). Therefore if we augment our images too much we may find the model performs poorly in any real-world applications.","ae241acd":"# What is the model focusing upon?\nWe now extract the output of some of the intermediate layers of our model so that we can see what the model is focusing on in the images. This is both interesting and insightful since I have no medical background and so it will be interesting to learn what classifier pneumonia."}}