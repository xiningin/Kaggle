{"cell_type":{"efbf6bd9":"code","6cd05eea":"code","19467ad2":"code","241e7dba":"code","f30f57fe":"code","e88b8b31":"code","577a29ab":"code","98de61ff":"code","778224aa":"code","af88a10c":"code","c811e688":"code","24d39fa4":"code","c4806f3d":"code","7f580633":"code","bd4660eb":"code","2dff98ea":"code","a6e3a286":"code","e5e3eec9":"code","663b9d11":"markdown","24b79777":"markdown","2a04f559":"markdown","63c1cd5f":"markdown","fd64075a":"markdown","21a2d6b3":"markdown","30e30bb9":"markdown"},"source":{"efbf6bd9":"!git clone https:\/\/github.com\/jose-carmona\/keras-transformer.git\n!cd keras-transformer && pip install .","6cd05eea":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_path = \"\/kaggle\/input\/data-without-drift\/train_clean.csv\"\ntest_path = \"\/kaggle\/input\/data-without-drift\/test_clean.csv\"\n\ntrain_data = pd.read_csv(train_path)\ntest_data = pd.read_csv(test_path)\n\n# batch7 not clean still. We remove it in train\nbs = 500000\ntrain_data = train_data[0:bs*7].append(train_data[bs*8:])","19467ad2":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","241e7dba":"max_open_channels = 10\nshift = 30\nn_features = 1\ntrain_size = 0.2\neval_size = 0.01\nepochs = 8","f30f57fe":"features = []\nresult = []\n\nfor i in reversed(range(shift)):\n    j=i+1\n    features.append(f'signal_{j}')\n    result.append(f'open_channels_{j}')\n\nfeatures.append(f'signal')\nresult.append('open_channels')","e88b8b31":"def create_shift_columns(df, column, shift=5):\n    for i in range(1,shift+1):\n        df[f'{column}_{i}'] = df[f'{column}'].shift(i)\n\n    return df.fillna(0)\n\ndef extract_columns(df, columns, shape):\n    return np.reshape(df[columns].values, shape)","577a29ab":"from keras import backend as K\n\nK.clear_session()","98de61ff":"from keras.layers import Input, Dense, Add, BatchNormalization, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD\nfrom keras.losses import CategoricalCrossentropy\nfrom keras_transformer.transformer import TransformerACT, TransformerBlock\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\n\nimport matplotlib.pyplot as plt\n\n# train_size = 0.005 \/ eval_size = 0.05\n# H = 1 \/ depth = 16 \/ BS = 32 \/ dense = 64 --> Score = 0.8430171235798919\n# H = 4 \/ depth = 16 \/ BS = 32 \/ dense = 64 --> Score = 0.784785769989308\n# H = 1 \/ depth = 32 \/ BS = 32 \/ dense = 64 --> Score = 0.8533075081800677\n# H = 1 \/ depth = 48 \/ BS = 32 \/ dense = 64 --> Score = 0.8464452560468306\n# H = 1 \/ depth = 32 \/ BS = 32 \/ dense = 32 --> Score = 0.7489237855413848\n# H = 1 \/ depth = 32 \/ BS = 32 \/ dense = 96 --> Score = 0.7958475746068415\n# BatchNormalization \/ lr = 0.0015 --> Score = 0.7999503052690716\n# BatchNormalization \/ lr = 0.01 --> Score = 0.7472128169138309\n# optimizers = SGD --> X\n\n# shift = 20 (BN) --> Score = 0.8027335987295007\n# shift = 30 (BN) --> Score = 0.8224195152711268\n# shift = 40 (BN) --> Score = 0.7339714910308296\n# shift = 30 --> Score = 0.8747070792176976 *BEST*\n\n\nclass TransformerModel():\n    \n    def __init__(self, n_features, max_open_channels, epochs):\n        self.type = 'Transformer'\n        self.n_features = n_features\n        self.shape = (max_open_channels+1, n_features)\n        self.max_open_channels = max_open_channels\n        \n        self.lr = 0.0015\n        self.epochs = epochs\n        self.batch_size = 32\n        self.train_verbose = 1\n        \n        self.num_heads = 1\n        self.dense_units = 64\n        self.transformer_depth = 32\n        self.transformer_dropout: float = 0.1\n            \n        self.checkpoint = ModelCheckpoint(self.type + \".hdf5\", \n                                          monitor='val_accuracy',\n                                          verbose=1,\n                                          save_best_only=True,\n                                          mode='auto',\n                                          period=1)\n\n\n\n    def create_model(self):\n        inp = Input(shape = self.shape)\n        \n        next_step_input = inp\n        next_step_input = Dense(self.dense_units)(next_step_input)\n        \n        act_layer = TransformerACT(name='adaptive_computation_time')\n        transformer_block = TransformerBlock(name = 'transformer',\n                                             num_heads = self.num_heads,\n                                             residual_dropout = self.transformer_dropout,\n                                             attention_dropout = self.transformer_dropout,\n                                             # Allow bi-directional attention\n                                             use_masking = False)\n\n        act_output = next_step_input\n        for i in range(self.transformer_depth):\n            next_step_input = transformer_block(next_step_input)\n            next_step_input, act_output = act_layer(next_step_input)\n\n        act_layer.finalize()\n        next_step_input = act_output\n        \n        #-- next_step_input = BatchNormalization()(next_step_input)\n        #-- next_step_input = Dropout(0.2)(next_step_input)\n        \n        out = Dense(self.max_open_channels+1, activation = 'softmax', name = 'out')(next_step_input)\n        \n        self.model = Model(inputs = inp, outputs = out)\n    \n    def compile_model(self):\n        opt = Adam(lr = self.lr)\n        # opt = SGD(lr = self.lr)\n        self.model.compile(loss = CategoricalCrossentropy(), optimizer = opt, metrics = ['accuracy'])\n    \n    def create_and_compile(self):\n        print('Create Model...')\n        self.create_model()\n        print('Compile Model...')\n        self.compile_model()\n\n    def print(self):\n        print(self.model.summary())\n        \n    def fit(self, X, y):\n        y = to_categorical(y, num_classes = self.max_open_channels+1)\n        X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n        self.history = self.model.fit(X_train,\n                                      y_train,\n                                      validation_data = (X_valid,y_valid),\n                                      epochs = self.epochs,\n                                      batch_size = self.batch_size,\n                                      callbacks = [self.checkpoint],\n                                      verbose = self.train_verbose)\n\n    def predict(self, X):\n        preds = np.argmax(self.model.predict(X), axis=-1)\n        return preds[:,shift]\n     \n    def load_weights(self):\n        self.model.load_weights(self.type + \".hdf5\")\n    \n    def plot(self):\n        plt.figure(figsize=(20,5))\n\n        # summarize history for accuracy\n        plt.subplot(1, 2, 1)\n        plt.plot(self.history.history['accuracy'])\n        plt.plot(self.history.history['val_accuracy'])\n        plt.title('model accuracy')\n        plt.ylabel('accuracy')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n\n        # summarize history for loss\n        plt.subplot(1, 2, 2)\n        plt.plot(self.history.history['loss'])\n        plt.plot(self.history.history['val_loss'])\n        plt.title('model loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n\n        plt.show()","778224aa":"m = TransformerModel(n_features, shift, epochs)\nm.create_and_compile()","af88a10c":"!date\n\ntrain_data = create_shift_columns(train_data, 'signal', shift)\ntrain_data = create_shift_columns(train_data, 'open_channels', shift)\ntrain_data = reduce_mem_usage(train_data)\n\nX = extract_columns(train_data, features, (-1,shift+1,n_features))\ncl = train_data['open_channels'].values\ny = extract_columns(train_data, result, (-1,shift+1,1))","c811e688":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify = cl, test_size = 1-train_size)","24d39fa4":"m.fit(X_train, y_train)","c4806f3d":"m.plot()","7f580633":"!date\n\nm.load_weights()","bd4660eb":"from sklearn.metrics import f1_score\n\nX1, _, y1, _ = train_test_split(X_valid, y_valid, test_size = 1-eval_size)\nprint('Score =', f1_score(y1[:,shift], m.predict(X1) ,average='macro'))","2dff98ea":"import gc\n\ndel train_data\ndel X, y, X_train, X_valid, y_train, y_valid\ndel X1, y1\ngc.collect()","a6e3a286":"!date\n\ntest_data = create_shift_columns(test_data, 'signal', shift)\ntest_data = reduce_mem_usage(test_data)\n\nt = extract_columns(test_data, features, (-1,shift+1,n_features))\nr = m.predict(t)","e5e3eec9":"!date\n\nsubmission = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsubmission['open_channels'] = r\nsubmission.open_channels.hist()\nplt.show()\nsubmission.to_csv('submission.csv', float_format='%0.4f', index = False)\n\n!date","663b9d11":"### Features","24b79777":"### Params","2a04f559":"### Eval","63c1cd5f":"### Submission","fd64075a":"### Fit Model","21a2d6b3":"### Model","30e30bb9":"### Predict"}}