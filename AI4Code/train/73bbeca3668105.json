{"cell_type":{"82d4cb05":"code","fecefd54":"code","f1bc0766":"code","9c1a66c8":"code","e18e8e70":"code","591f83aa":"code","d4d55b3a":"code","5668f0fe":"code","cf9285d0":"code","6c198080":"code","d516cc6c":"code","bf65571d":"code","a85bcd0f":"code","ca8c9a60":"code","202e17ab":"code","27b69838":"code","2e22885d":"code","4073e3ea":"code","aab8e547":"code","dcc27e9f":"code","58dec5fb":"code","75f80c1f":"code","b7d2d625":"code","9072cf83":"code","b0f3dd0d":"code","bdfdcc6c":"code","9345a491":"code","2e86b30c":"code","1482efc0":"code","9b848b76":"code","9bce5d1a":"code","b783d3ce":"code","0c0a9300":"code","2c985d2a":"code","3cd15326":"code","97764114":"code","815f9d9f":"code","6b76a07e":"code","02082399":"code","98828240":"code","ae4711f7":"code","2c6becc4":"code","f6341b51":"code","016820d1":"code","8789b244":"code","6ec26317":"code","8586e980":"code","9ade932e":"code","5df31ff4":"code","cde5fcf7":"code","afc57866":"code","550b479d":"code","4044fb35":"code","651af978":"markdown","c6018c63":"markdown","bd35c85e":"markdown","cc184d77":"markdown","4dec431a":"markdown","bdb492f9":"markdown","3f98dc7f":"markdown","9266cb03":"markdown","7e725d97":"markdown","d51f3eaf":"markdown","e63732d4":"markdown","5f8f7760":"markdown","7c02853a":"markdown","30bd314c":"markdown","3f26297f":"markdown"},"source":{"82d4cb05":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","fecefd54":"# Run to sort out dependency issues with Tensorflow\n!pip install tensorflow\n!pip install imblearn\n# Test dependencies\nfrom imblearn.over_sampling import SMOTENC","f1bc0766":"from pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom pandas_summary import DataFrameSummary\n\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')","9c1a66c8":"from pathlib import Path\nimport pandas as pd\nimport numpy as np","e18e8e70":"data_path = Path('\/kaggle\/input\/odfpresenceabsenceinvasivespecies\/kaggle-dnn-invasive-species\/'); data_path","591f83aa":"# data\nabsence_data = pd.read_csv(Path(data_path, \"Absence.csv\"),\n                           sep=';', index_col=0)\npresence_data = pd.read_csv(Path(data_path,\n                                \"Presence.csv\"), sep=';', index_col=0)\n\nabsence_data = absence_data[presence_data.columns]\nassert(np.sum(absence_data.columns == presence_data.columns) == len(presence_data.columns))","d4d55b3a":"# label the data\nabsence_data['obs'] = 0; presence_data['obs'] = 1","5668f0fe":"data = absence_data.append(presence_data).reset_index()\ndata = data[~data.eq(-9999.0).any(1)]","cf9285d0":"data.head(10)","6c198080":"df_summary = DataFrameSummary(data)","d516cc6c":"# Check for missing data and column types\ndf_summary.columns_stats","bf65571d":"# Plot the distribution of feature by observation value. Try another feature to see how it works!\nfig = plt.figure(figsize=(15,10))\nsns.boxplot(x='obs', y=data.columns[2], data=data)","a85bcd0f":"from sklearn.model_selection import train_test_split","ca8c9a60":"X = data[[i for i in data.columns if i != 'obs' and i != 'pointid']]; y = data['obs']","202e17ab":"y.value_counts()","27b69838":"from imblearn.under_sampling import ClusterCentroids\nfrom imblearn.over_sampling import SMOTENC, SMOTE\nfrom imblearn.ensemble import BalancedRandomForestClassifier","2e22885d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","4073e3ea":"rus = SMOTENC(categorical_features=[2], random_state=42)\nX_train, y_train = rus.fit_resample(X_train, y_train)","aab8e547":"y_train.value_counts()","dcc27e9f":"train_ids = X_train.index\ntest_ids = X_test.reset_index().index\nprint(len(train_ids), len(test_ids))","58dec5fb":"## Tree Classifiers","75f80c1f":"from sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, auc, roc_curve, f1_score, confusion_matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","b7d2d625":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","9072cf83":"models = [tree.DecisionTreeClassifier(max_depth=5), RandomForestClassifier(100),\n         BalancedRandomForestClassifier(100), GradientBoostingClassifier(n_estimators=100)]","b0f3dd0d":"def fit_eval(model, X_train, y_train, X_test, y_test):\n    m = model\n    m.fit(X_train, y_train)\n    pred = m.predict(X_test)\n    fpr, tpr, thresholds = roc_curve(y_test, pred, pos_label=1)\n    cm = confusion_matrix(y_test, pred)\n    plot_confusion_matrix(cm, title=f'Model: {type(m).__name__}', target_names=['Absent','Present'], normalize=False)\n    return {type(m).__name__: {'model': m, 'AUROC': auc(fpr, tpr), 'F1': f1_score(y_test, pred),  \n            'ACCURACY': accuracy_score(y_test, pred)}}","bdfdcc6c":"%time results = [fit_eval(_, X_train, y_train, X_test, y_test) for _ in models]; results","9345a491":"#xgboost\n\nimport shap\n\n# load JS visualization code to notebook\nshap.initjs()\n\n# train model\nmodel = results[1]['RandomForestClassifier']['model']\n\n# explain the model's predictions using SHAP\nexplainer = shap.TreeExplainer(model, model_output='margin')\nshap_values = explainer.shap_values(X_test)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value[1], shap_values[1][1,:], X_test.iloc[1,:])","2e86b30c":"shap.summary_plot(shap_values, X_train)","1482efc0":"# fastai library has a set of utility classes for processing tabular data, such as the csv file we are working with\nfrom fastai.tabular import *\nimport torch","9b848b76":"# specify the independent variable (target)\ndep_var = 'obs'\n# specify any categorical and continuous variables\ncat_names = ['Absence_Substrate']\ncont_names = list(set(data.columns) - set(cat_names) - set([dep_var]) - set(['pointid']))\n# preprocessing steps removes any missing values, ensures categoricals are not read as continuous\n# and normalises the data\nprocs = [FillMissing, Categorify, Normalize]","9bce5d1a":"# Set up training and test datasets using pre-defined data structure TabularList from fastai","b783d3ce":"test = TabularList.from_df(X_test.copy(), path=data_path, cat_names=cat_names, cont_names=cont_names)\n\ndf = (TabularList.from_df(pd.concat([X_train,y_train],1).reset_index().set_index('index').sample(frac=1),\n                          path=data_path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_idx(range(int(0.8*len(X_train)), int(len(X_train))))\n                           .label_from_df(cols=dep_var)\n                           .add_test(test)\n                           .databunch(num_workers=0))","0c0a9300":"df","2c985d2a":"# Initialise the metrics that we want to use for evaluation again\nf1 = FBeta() # same as f1-score\nauroc = AUROC()","3cd15326":"# Create a deep neural network with 2 layers, in this case the default of 200 hidden units in the first layer and\n# 100 in the second layer. ","97764114":"learn = tabular_learner(df, layers=[200,100], metrics=[accuracy, f1, auroc], callback_fns=ShowGraph)","815f9d9f":"# Since this is a classification problem, we use the Cross-Entropy loss function from PyTorch to evaluate the\n# model predictions.\nlearn.loss_func = torch.nn.CrossEntropyLoss()","6b76a07e":"# View the model architecture\nlearn.model","02082399":"#Fastai provides a utility to find an appropriate learning rate to accelerate training","98828240":"learn.model_dir = '\/kaggle\/working\/models'\nlearn.lr_find()","ae4711f7":"g = learn.recorder.plot(suggestion=True, return_fig=True)","2c6becc4":"# Fit the model based on selected learning rate\nlearn.fit_one_cycle(5, max_lr=slice(1e-03))\n\n# Analyse our model\nlearn.recorder.plot_losses()","f6341b51":"# Predict our target value\npredictions, *_ = learn.get_preds(DatasetType.Test)\nlabels = np.argmax(predictions, 1)","016820d1":"accuracy_score(y_test, labels)","8789b244":"interp = ClassificationInterpretation.from_learner(learn, ds_type=DatasetType.Test)","6ec26317":"y_test.value_counts()","8586e980":"from sklearn.metrics import confusion_matrix","9ade932e":"cm = confusion_matrix(y_test, labels)","5df31ff4":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","cde5fcf7":"plot_confusion_matrix(cm, title='Confusion matrix: FF Neural Network', \n                      target_names=['Absent','Present'], normalize=False)","afc57866":"from sklearn.metrics import f1_score, roc_auc_score","550b479d":"f1_score(y_test, labels)","4044fb35":"roc_auc_score(y_test, labels)","651af978":"<div class=\"alert alert-info\" role=\"alert\">\n    \n#### But wait, we have an extreme case of data imbalance. Even a naive classifier would obtain 28608\/28667=99.8% accuracy by just choosing every point to be an absence location. So we need to deal with this imbalance first.\n    \n* imblearn (imbalanced learn) is a Python package with the explicit purpose of dealing with class imbalanced data and provides a host of useful sampling and modelling techniques.\n    \n<\/div>","c6018c63":"<div class=\"alert alert-info\" role=\"alert\">\n    \n* One option is to oversample the minority class, i.e. to balance out the class distribution. For this purpose, we use SMOTE encoding. This would increase the data and thus the time required for the model to train.\n* Alternatively, we could undersample the majority class, which would reduce the amount of data available for the model to train.\n    \n* Important NOTE: Any sampling done to the data should only be done on the training set. The test set should always remain unprocessed and independent for the evaluation stage. \n    \n* As an exercise, you may want to try both under and over sampling and see which method performs the best. \n    \n<\/div>","bd35c85e":"<div class=\"alert alert-success\" role=\"alert\">\n    \n#### Great news! In just 3 epochs we were able to achieve an AUROC close to 1 on the validation set which is a marked improvement on our tree models. But how do we fare on the unseen test set (without any of the pre-processing steps)?\n    \n\n<\/div>","cc184d77":"<div class=\"alert alert-info\" role=\"alert\">\n    \n### Quick exploratory data analysis\n    \n<\/div>","4dec431a":"<div class=\"alert alert-success\" role=\"alert\">\n    \n### At first glance, it seems as though the model does not perform very well when we evaluate using F1-score. However, in this case we are more concerned with identifying the presence cases and so misclassifications of absence locations is not a major error compared to a misclassification of a presence location. Thus, our final model has managed to reduce the misclassifications of absence cases whilst not reducing our ability to detect presence cases, which is a good result overall.\n    \n### Interestingly, we see that the neural network produces a very similar result to our Random Forest Classifier, and in this case it may be more useful to opt for the Random Forest as it offers more insight into the decisions made by the model. \n\n<\/div>","bdb492f9":"<div class=\"alert alert-info\" role=\"alert\">\n    \n* A second option is to use a classifier that accounts for the imbalance in the data, such as the Balanced Random Forest\n- This classifier undersamples the majority class in each bootstrapped sample used to build its decision trees\n    \n<\/div>","3f98dc7f":"<div class=\"alert alert-info\" role=\"alert\">\n    \n### Set up presence and absence data extracted from rasters\n    \n<\/div>","9266cb03":"<div class=\"alert alert-success\" role=\"alert\">\n    \n### How do we evaluate the model?\n    \nAs mentioned before, accuracy values mean little when the data is imbalanced. So we need to include other metrics that indicate the discriminatory power between the two classes. In this case, we have chosen the AUROC which indicates the trade-off between false positives and true positives. We also include the F1-score as the weighted average of precision and recall. More about these evaluation metrics can be found at: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\n    \n<\/div>","7e725d97":"<div class=\"alert alert-info\" role=\"alert\">\n    \n\nAveraging over all predictions\u201a\u00a0we see that the depth feature drives the largest impact on the difference between the prediction and the mean outcome, suggesting that it has a large influence on the final classification. \n    \n<\/div>","d51f3eaf":"<div class=\"alert alert-info\" role=\"alert\">\n    \n### Split the data into training and test sets\n    \n<\/div>","e63732d4":"<div class=\"alert alert-success\" role=\"alert\">\n    \n### How do we better understand the output from our model? Answer: SHAP values \n    \nSHAP values tell us how the tree model makes decisions by looking at how each feature contributes to a difference between the average (naive) classification and the prediction made by the model. \n    \n<\/div>\n","5f8f7760":"<div class=\"jumbotron jumbotron-fluid\">\n  <div class=\"container\">\n    <h1 class=\"display-4\">Invasive Species Prediction - D. Villosus<\/h1>\n    <p class=\"lead\">Part 2<\/p>\n  <\/div>\n<\/div>","7c02853a":"<div class=\"alert alert-info\" role=\"alert\">\n    \n\n#### Now, we can hopefully improve on the evaluation metrics we set out in the last section by capturing non-linear feature combinations using neural networks.\n    \n#### Here, we will use PyTorch (pytorch.org) and FastAI (docs.fast.ai) libraries to do the modelling. \n    \n<\/div>","30bd314c":"<div class=\"alert alert-info\" role=\"alert\">\n    \n### First remove any \"No data\" points\n    \n<\/div>","3f26297f":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png)"}}