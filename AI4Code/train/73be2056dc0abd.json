{"cell_type":{"724efe22":"code","b57b67d1":"code","deeb30f5":"code","1baec44b":"code","5bfb822c":"code","c3b43352":"code","a7ebf36e":"code","a70e4aa6":"code","61d3ec2d":"code","1f995012":"code","3d6c6da6":"code","070db809":"code","ec66dc1d":"code","1fce8464":"code","6e8bff2e":"code","b236706d":"code","dd03f55b":"code","492f7c1d":"code","c715a143":"code","6a67a71a":"code","4bb5413a":"code","d484aa4d":"code","5ceabaaf":"code","485e7680":"code","6f109f4a":"code","1857897c":"code","c17caaa9":"code","80e033e4":"code","02e9e51c":"code","dce8b89d":"code","3c512a73":"code","40f6db3a":"code","9c5f2087":"code","bc05ffe6":"code","d2ac9f9e":"code","ad26d941":"code","dbb70f14":"code","af5b85ee":"code","c247be43":"code","29b84a43":"code","8c0c105e":"code","72fa6f65":"code","900a48eb":"code","68ad88e2":"code","18daab87":"code","a5252100":"code","f2fb09e9":"code","1655a339":"code","1cf047de":"code","63a1e36a":"code","45399b72":"code","4b7a8654":"code","b87119ec":"code","f6eb4f90":"code","41d01156":"code","26153559":"code","06fa2239":"code","93005488":"code","018de028":"code","aab15737":"code","224f724a":"code","0368d41c":"code","764ff53d":"code","c6102ee7":"code","bec764df":"code","aac452a4":"code","baede5e5":"code","dd38a129":"code","69e388ae":"code","17610e82":"code","41b1b0e7":"code","2cb7b161":"code","0e41db39":"code","d43cc2e1":"code","296f4290":"code","d09a8415":"code","84512484":"code","409e172e":"code","3bd23136":"code","e0093891":"code","eb86005a":"code","3996f06a":"code","3d3462b7":"code","3fdb5a57":"code","3a3daddc":"code","5a4f2e38":"code","9a474b62":"code","196f3f68":"code","65a521f6":"code","e682c0f5":"code","8699594e":"code","c79caf7d":"code","15912ced":"code","76810424":"code","ba4e8a87":"code","7ca3b583":"code","677e0cb4":"code","7f016bd8":"code","e7a9e397":"code","91856e04":"code","fb470932":"code","1acb0f72":"code","cc58fb2e":"code","41261b15":"code","ba92f9a5":"code","a7d5da6f":"code","5bad37e9":"code","dbb44ba0":"markdown","b1879cdf":"markdown","a3c8f2b6":"markdown","aa1d3fbe":"markdown","06409c01":"markdown","658fa6c7":"markdown","4a5185cc":"markdown","83f11006":"markdown","7949c8d4":"markdown","a71f9a64":"markdown","f0580758":"markdown","a659d985":"markdown","a33e5f76":"markdown","9dd2b15e":"markdown","bb23a5d4":"markdown","760249b3":"markdown","0a22f5fb":"markdown","7a8be66b":"markdown","92cb09d1":"markdown","e897ce46":"markdown","4752cab2":"markdown","2427b811":"markdown","4eeb5390":"markdown","1702e784":"markdown","07f2d0e0":"markdown","0a521512":"markdown","8c25833a":"markdown","00f41508":"markdown","f97a985e":"markdown","c2107d04":"markdown","0d89e5ee":"markdown"},"source":{"724efe22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b57b67d1":"import numpy as np\nimport pandas as pd","deeb30f5":"import seaborn as sns\nimport matplotlib.pyplot as plt","1baec44b":"%matplotlib inline\n#set the plots to use the default seaborn style\nsns.set()","5bfb822c":"# Set the option to view all columns\npd.set_option('display.max_columns', None)","c3b43352":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","a7ebf36e":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","a70e4aa6":"test_df.head()","61d3ec2d":"df = train_df.append(test_df)","1f995012":"df.head()","3d6c6da6":"df.tail()","070db809":"print(train_df.info())","ec66dc1d":"## Get a list of columns except for the SalePrice column\ncolumns = list(df.columns.values)\ncolumns.remove('SalePrice')","1fce8464":"#get the length of combined dataframe\ndf_N = df.shape[0]","6e8bff2e":"#get a list of all columns with atleast 10% missing values\nmissing_val_cols ={}\nfor col in columns:\n    col_empties_N = df[col][df[col].isnull()].shape[0]\n    empties_perc = (col_empties_N\/df_N) * 100\n    if empties_perc > 10:\n        missing_val_cols[col] = empties_perc","b236706d":"pd.Series(missing_val_cols).plot.bar()","dd03f55b":"df['LotFrontage'][df['LotFrontage'].isnull()] = 0","492f7c1d":"df['Alley'][df['Alley'].isnull()] = 'None'","c715a143":"df['FireplaceQu'][df['FireplaceQu'].isnull()] = 'None'","6a67a71a":"df.PoolQC[df.PoolQC.isnull()] = 'None'","4bb5413a":"df['Fence'][df['Fence'].isnull()] = 'None'","d484aa4d":"df['MiscFeature'][df['MiscFeature'].isnull()] = 'None'","5ceabaaf":"#let us see how the dataframe now looks like\ndf.head()","485e7680":"df[df.dtypes[df.dtypes == 'int64'].index].fillna(0,inplace=True)","6f109f4a":"#use forward fill to fill the missing values\ndf.fillna(method='ffill',inplace=True)","1857897c":"df.tail()","c17caaa9":"from collections import defaultdict","80e033e4":"object_cols = [] #container to store all column names where the data type is \"object\"\ncol_dummies = [] #container to store all dataframes created from pandas get_dummies method\n\nfor col in df.columns:\n    if str(df[col].dtypes) == 'object':\n        object_cols.append(col)\n        col_dummy = pd.get_dummies(df[['Id',col]],drop_first=True,prefix=col)\n        col_dummies.append(col_dummy)  ","02e9e51c":"#drop off all columns where the data\ndf_clean = df.drop(object_cols,axis=1)\n#now append each dataframe from above to the left of our original dataframe\nfor index,col in enumerate(object_cols):\n    df_clean= pd.merge(df_clean,col_dummies[index],on='Id') \ndf_clean.set_index('Id',inplace=True)","dce8b89d":"train_df.plot.scatter(x='OverallQual',y='SalePrice')","3c512a73":"plt.figure(figsize=(8,7))\nsns.boxplot(y='SalePrice',x='OverallQual',data=train_df)","40f6db3a":"train_df.plot.scatter(x='GrLivArea',y='SalePrice')","9c5f2087":"df_clean['OverallQual^2'] = df_clean['OverallQual'] ** 2\ndf_clean.drop('OverallQual',inplace=True,axis=1)","bc05ffe6":"df_clean.head()","d2ac9f9e":"#df_clean = df_clean.drop(df_clean[(df_clean['GrLivArea']>4000)].index)# Summarize features\ndf_clean['TotalSF'] = df_clean['1stFlrSF'] + df_clean['2ndFlrSF'] + df_clean['TotalBsmtSF']\ndf_clean = df_clean.drop(['1stFlrSF','2ndFlrSF','TotalBsmtSF'], axis=1)\n\ndf_clean['TotalArea'] = df_clean['LotFrontage'] + df_clean['LotArea']\ndf_clean = df_clean.drop(['LotFrontage','LotArea'], axis=1)\n\ndf_clean['BSF'] = df_clean['BsmtFinSF1'] + df_clean['BsmtFinSF2']\ndf_clean = df_clean.drop(['BsmtFinSF1','BsmtFinSF2'], axis=1)\n\ndf_clean['BsmtBath'] = df_clean['BsmtFullBath'] + (0.5 * df_clean['BsmtHalfBath'])\ndf_clean = df_clean.drop(['BsmtFullBath','BsmtHalfBath'], axis=1)\n\ndf_clean['Bath'] = df_clean['FullBath'] + (0.5 * df_clean['HalfBath'])\ndf_clean = df_clean.drop(['FullBath','HalfBath'], axis=1)","ad26d941":"#get the correlation values against each column\ntrain_length = train_df.shape[0]\ncorrs = df_clean.iloc[:train_length].corr()","dbb70f14":"#convert them into a dataframe\ncorrs_df = pd.DataFrame(corrs.SalePrice)","af5b85ee":"#pick only those whose correlation with the sale price is over 0.5\ncontenders = corrs_df[corrs_df.SalePrice > 0.5]\ncontenders.drop('SalePrice',inplace=True)","c247be43":"#reset the index of the dataframe so that we may be able to access the columns names\ncontenders.reset_index(inplace=True)","29b84a43":"#rename the new column for easy acces\ncontenders.columns = ['Predictor','Correlation']","8c0c105e":"#show the correlations in descending order\ncontenders.sort_values('Correlation',ascending=False)","72fa6f65":"corrs.loc[contenders.Predictor[:-1]][contenders.Predictor[:-1]]","900a48eb":"to_plot = corrs.loc[contenders.Predictor][contenders.Predictor]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(to_plot,annot=True,cmap=\"RdYlGn\")","68ad88e2":"to_drop = contenders.iloc[[3,8,5]]\nto_drop","18daab87":"contenders = contenders.drop([3,8,5])\ncontenders.sort_values('Correlation',ascending=False)","a5252100":"corrs.loc['SalePrice'][contenders.Predictor].plot.bar()\nplt.title('Top 6 Predictors Correlation to SalePrice')\nplt.ylabel('Correlation')","f2fb09e9":"#get the length of the original train_df dataframe\ntrain_length = train_df.shape[0]","1655a339":"#make X by taking only the rows from train_df besides the SalePrice column\nX = df_clean.iloc[:train_length].drop('SalePrice',axis=1)","1cf047de":"X.head()","63a1e36a":"#make y by taking all the rows from train_df but only the SalePrice column\ny = df_clean.iloc[:train_length]['SalePrice']","45399b72":"#use the remaining data for testing the final model\nX_validation = df_clean.iloc[train_length:].drop('SalePrice',axis=1)","4b7a8654":"X_validation.head()","b87119ec":"idx_to_drop = X[(X['GrLivArea']>4000)].index\nX.drop(X.loc[idx_to_drop].index,inplace=True)\ny.drop(y.loc[idx_to_drop].index,inplace=True)","f6eb4f90":"from sklearn.model_selection import train_test_split","41d01156":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=45)\n","26153559":"from sklearn.preprocessing import StandardScaler","06fa2239":"scaler = StandardScaler()","93005488":"from sklearn.metrics import mean_squared_error,r2_score ,mean_squared_log_error","018de028":"def get_model_scores(y_pred,y_test):\n    \"\"\"\n        Calculates and returns the mean squared error,r squared score and mean squared log error of the given input\n        \n        input: y_pred array-like. Prediction values of a dataset\n               y_test array-like. Actual values of a dataset\n               \n       output: tuple in the form of (mean squared error,r squared,mean squared log error) scores\n    \"\"\"\n    mse = mean_squared_error(y_test,y_pred)\n    r2 = r2_score(y_test,y_pred)\n    msle = mean_squared_log_error(y_test,y_pred)\n    return mse,r2,msle","aab15737":"def print_model_score(y_pred,y_test):\n    \"\"\"\n        Prints the mse, r-squared and msle score of the inputs\n    \n        input: y_pred array-like items of the predicted values\n               y_test array-like items of the actual values   \n    \"\"\"\n    test_mse,test_r2,test_log = get_model_scores(y_pred,y_test)\n    print('Test MSE: {}'.format(test_mse))\n    print('Test R-Squared Score: {}'.format(test_r2))\n    print('Test Mean-Squared Log Error: {}'.format(test_log))","224f724a":"def plot_actual_preds(actuals,preds):\n    \"\"\"\n        Plots the actual house prices overlayed on the predicted house prices.\n        \n        Input: actuals array-like values of target variable\n               preds array-like predicted values of the target variable\n    \"\"\"\n    plt.plot(actuals,linestyle=None,linewidth=0,marker='o',label='Actual Values',alpha=0.5)\n    plt.plot(preds,color='red',linestyle=None,linewidth=0,marker='o',\n         label='Predictions',alpha=0.2)\n    plt.ylabel('Sale Price')\n    plt.legend(loc=(1,1))\n    plt.show()","0368d41c":"def get_scalers(to_scale):\n    return scaler.fit_transform(to_scale)","764ff53d":"model_performance ={}","c6102ee7":"from sklearn.linear_model import LinearRegression","bec764df":"linear_model = LinearRegression()","aac452a4":"linear_model.fit(scaler.fit_transform(X_train[contenders.Predictor]),np.log(y_train))","baede5e5":"y_pred = linear_model.predict(scaler.transform(X_test[contenders.Predictor]))","dd38a129":"model_performance['Linear'] = get_model_scores(np.exp(y_pred),y_test)\nprint_model_score(y_test.values,np.exp(y_pred))","69e388ae":"plot_actual_preds(y_test.values,np.exp(y_pred))","17610e82":"from sklearn.linear_model import LassoCV","41b1b0e7":"lasso_model = LassoCV(cv=3)","2cb7b161":"lasso_model.fit(scaler.fit_transform(X_train),np.log(y_train))","0e41db39":"y_pred = lasso_model.predict(scaler.transform(X_test))","d43cc2e1":"model_performance['Lasso'] = get_model_scores(np.exp(y_pred),y_test)\nprint_model_score(np.exp(y_pred),y_test)","296f4290":"from sklearn.linear_model import RidgeCV","d09a8415":"ridge_model = RidgeCV(cv=3)","84512484":"ridge_model.fit(get_scalers(X_train),np.log(y_train))","409e172e":"y_pred = ridge_model.predict(get_scalers(X_test))","3bd23136":"model_performance['Ridge'] = get_model_scores(np.exp(y_pred),y_test)\nprint_model_score(np.exp(y_pred),y_test)","e0093891":"plot_actual_preds(y_test.values,np.exp(y_pred))","eb86005a":"from sklearn.ensemble import RandomForestRegressor","3996f06a":"rf_model = RandomForestRegressor(n_estimators=200,min_samples_leaf=3\n                              ,max_features=0.5,warm_start=True,\n                              bootstrap=False,random_state=123,\n                                  )","3d3462b7":"rf_model.fit(scaler.fit_transform(X_train),np.log(y_train))","3fdb5a57":"y_pred = rf_model.predict(scaler.fit_transform(X_test))","3a3daddc":"model_performance['RandomTree'] =  get_model_scores(np.exp(y_pred),y_test)\nprint_model_score(np.exp(y_pred),y_test)","5a4f2e38":"rf_fi =pd.DataFrame(rf_model.feature_importances_,index=X_train.columns,columns=['Feature Importance'])\nrf_fi = rf_fi[rf_fi['Feature Importance'] > 0].sort_values('Feature Importance',ascending=False)*100","9a474b62":"#check how much each feature from our selected features, actually contributes\nrf_fi.loc[contenders.Predictor].plot.bar()\nplt.title('Top 6 Predictors')\nplt.ylabel('Contribution (%)')","196f3f68":"rf_fi.loc[contenders.Predictor].sum().plot.bar()\nplt.title('Top 6 Predictors Total Contribution')\nplt.ylabel('Contribution (%)')","65a521f6":"rf_fi.head(13).plot.bar()\nplt.title('Top 16 Predictors')\nplt.ylabel('Total Contribution (%)')","e682c0f5":"plot_actual_preds(y_test.values,np.exp(y_pred))","8699594e":"from sklearn.ensemble import GradientBoostingRegressor","c79caf7d":"xgb_model = GradientBoostingRegressor(alpha=0.95, criterion='friedman_mse',\n             learning_rate=0.01, loss='huber',max_features='sqrt'\n             ,min_samples_leaf=10,min_samples_split=10,n_estimators=3000,\n             random_state=None, subsample=0.4)","15912ced":"xgb_model.fit(scaler.fit_transform(X_train),np.log(y_train))","76810424":"y_pred = xgb_model.predict(scaler.fit_transform(X_test))","ba4e8a87":"model_performance['XGB'] = get_model_scores(np.exp(y_pred),y_test)\nprint_model_score(np.exp(y_pred),y_test)","7ca3b583":"plot_actual_preds(y_test.values,np.exp(y_pred))","677e0cb4":"def blended_prediction(X):\n    return ((0.45 * xgb_model.predict(X)) + \\\n            (0.25 * lasso_model.predict(X)) + \\\n            (0.25 * ridge_model.predict(X)) +\\\n            (0.05 * rf_model.predict(X))\n            )","7f016bd8":"y_pred = blended_prediction(scaler.fit_transform(X_test))","e7a9e397":"model_performance['Stacked'] =get_model_scores(np.exp(y_pred),y_test)\nprint_model_score(np.exp(y_pred),y_test)","91856e04":"plot_actual_preds(y_test.values,np.exp(y_pred))","fb470932":"mp_array_dic =  {model:np.array(perf) for model,perf in model_performance.items()}\nperf_df = pd.DataFrame(mp_array_dic).transpose()\nperf_df.columns = ['MSE','R-Squared','MSLE']","1acb0f72":"perf_df[['MSE']].plot.bar(legend=None)\nplt.title('Mean Sqaured Error per Model')\nplt.ylabel(\"MSE (100 Million)\")","cc58fb2e":"perf_df[['R-Squared']].plot.bar(legend=None)\nplt.title('R-Sqaured Score per Model')\nplt.ylabel(\"Score\")","41261b15":"perf_df[['MSLE']].plot.bar(legend=None)\nplt.title('Mean Squared Log Error per Model')\nplt.ylabel(\"Mean Squared Log Error\")","ba92f9a5":"y_pred = blended_prediction(scaler.fit_transform(X_validation))\nvalidation_df = X_validation.copy()\nvalidation_df['SalePrice'] = np.exp(y_pred)\nvalidation_df['SalePrice'].tail()","a7d5da6f":"validation_df[['SalePrice']].tail()","5bad37e9":"validation_df[['SalePrice']].to_csv('submission.csv')","dbb44ba0":"# Select the best performing model","b1879cdf":"## Split the Data into train, test and validation sets","a3c8f2b6":"*`Before we merge the two datasets together, lets first make sure that they are of the same shape. We will now add SalePrice to test_df and call head() to see the changes`*","aa1d3fbe":"It looks like some columns are closely related. Lets go ahead and merge them together.","06409c01":"## Investigate all columns with missing values of over 10% of the data","658fa6c7":"## Create methods\/functions to reuse\n\n\nSince we will be trying out different models to see which performs best. To avoid repeatition of code, we must create functions which we will call to perform these repeatative tasks.","4a5185cc":"### Engineer Features based on the above\n\nIt looks like OverallQual takes on a quadratic shape. So lets go ahead and transform it. In actual fact, after taking the sqaure of OverallQual it's correlation to the sale price increases by two units i.e from 79% to 81%. We can also see that there are outliers on GrLivArea. We will need to deal with these later on.","83f11006":"## Random Forest Tree Model","7949c8d4":"### Removing outliers from our train set\n\nNow that we have performed all necessary taks, it safe to remove all outliers identified by our GrLivArea column","a71f9a64":"test_df['SalePrice'] = 0","f0580758":"### Investigate the relationships between predictors and the response\n\nAfter running OLS on the training dataset, we found out that only two predictors have a correlation of over 0.7. We will go ahead plot the relationship between the two predictors and response, respectively","a659d985":"#### Import all relevant libraries","a33e5f76":"## Stacked Regressor Model","9dd2b15e":"## Train the models","bb23a5d4":"## Create Dummy Variables\n\n\nSince most machine learning models\/algorithms require that the input data be numerical. We are going to go through each column that contains categorical data and create dummy columns for each.","760249b3":"### Linear Model","0a22f5fb":"## Lasso Model","7a8be66b":"#### Drop the least correlated variables to the sale price from above.","92cb09d1":"## Import the data","e897ce46":"## Merge the two datasets for easy data manipulation","4752cab2":"## Save results into the submission file","2427b811":"> As expected, GarageCars and GarageArea are correlated\n\n> So is, 1stFlrSF and TotalBsmtSF\n\n> But 1stFlrSF < TotalBsmtSF\n\n> and GarageCars > GarageArea","4eeb5390":"## Ridge Model","1702e784":"## Create A Scaler Object\n\nSince the data is measured in different metrices, we will need to normalise the data. To avoid any bias towards features with higher values. This will also help in making sure that the variance in our dataset is minimal. Thanks to scikits learn family of libraries, we have the right tool to get the job done.","07f2d0e0":"# House Price Prediction\n\nGiven a set of features for several houses including their prices. Predict the price of unseen houses.","0a521512":"### Look out for any correlation between contenders variables","8c25833a":"From the above options, it appears that *`Lasso`, `Ridge`* and *`XGB`* are the prime candidates. But combining the models together and taking a weighted contribution for each we get a better model. So lets go ahead and predict with the new model.","00f41508":"By plotting a boxplot for OverallQual against SalePrice we can easily predict the price of a house based on its overall quality rating. The prices do overlap but the average price for each rating is easily distinguished from the rest.","f97a985e":"## Extreme Gradient Boosting Model","c2107d04":"### Let us view the information on our train dataset","0d89e5ee":"## Fill the values accordingly\n\n\nFrom the above investigate the nature of the missing values. What does the presence of missing values mean for this dataset?\nCareful inspection of the above coupled with the instructions for the dataset we have come to realise that NA values have been encorded as NaN hence we will go ahead and convert them into the string literal \"None\". Plus looking at other numerical variables where it is believed that the feature does not exist hence unmeasurable, zero (0) hence been used in that case. We will go ahead and do it for the \"LofFrontage\" column"}}