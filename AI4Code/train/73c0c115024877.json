{"cell_type":{"3d6d8217":"code","1652782d":"code","d474f260":"code","444ed3a1":"code","075c585f":"code","e0cfac48":"code","ac00de76":"code","998027f4":"code","5ce4834b":"code","4866c865":"code","b196092c":"code","7d56f89b":"code","35793c3d":"code","043fe1f2":"code","bca49988":"code","967f5667":"code","664de4a6":"code","8a4d4c62":"code","91bd97f5":"code","983b8127":"code","93a2eacb":"markdown","0034e997":"markdown","b5fd3fbd":"markdown","b5cd9bbe":"markdown","43a41737":"markdown","c68d8461":"markdown","d5ed8d75":"markdown","0a00b97c":"markdown","d72342aa":"markdown","9f2ef030":"markdown","c9215d6b":"markdown","17fe318c":"markdown","35413adc":"markdown","2ba3bbfc":"markdown","ea129a06":"markdown","924cc5ba":"markdown"},"source":{"3d6d8217":"import numpy as np\nimport pandas as pd\nimport json\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom albumentations import HorizontalFlip, VerticalFlip, ShiftScaleRotate, Transpose, HueSaturationValue, RandomResizedCrop, RandomBrightnessContrast, Cutout, Compose, Normalize, CoarseDropout\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\nimport sys\nimport random\n\n# add efficientnet path\nefnet_path = '..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master'\nsys.path.append(efnet_path)\nfrom efficientnet_pytorch import EfficientNet\n\n# add fmix path\nfmix_path = '..\/input\/image-fmix\/FMix-master' #'..\/input\/efficientnet-pytorch-07\/efficientnet_pytorch-0.7.0'\nsys.path.append(fmix_path)\nfrom fmix import sample_mask\n","1652782d":"DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nOUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nTRAIN_DIR = '..\/input\/cassava-leaf-disease-classification\/train_images\/'\nTEST_DIR = '..\/input\/cassava-leaf-disease-classification\/test_images\/'","d474f260":"labels = json.load(open(\"..\/input\/cassava-leaf-disease-classification\/label_num_to_disease_map.json\"))\ntrain = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/train.csv')\nsample = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/sample_submission.csv')\n\nX, Y = train['image_id'].values, train['label'].values\nX_test = [name for name in (os.listdir(TEST_DIR))]","444ed3a1":"class Conf:\n    seed=777\n    \n    BATCH = 16\n    EPOCHS = 20\n\n    LR = 0.0001\n    IM_SIZE = 600\n\n    n_fold=5\n    target_col='label'\n    \n    modelname='efficientnet-b0'","075c585f":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(Conf.seed)\n\ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    return im_rgb","e0cfac48":"folds = train.copy()\nFold = StratifiedKFold(n_splits=Conf.n_fold, shuffle=True, random_state=Conf.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[Conf.target_col])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nprint(folds.groupby(['fold', Conf.target_col]).size())","ac00de76":"def train_transform():\n    return Compose([\n            RandomResizedCrop(Conf.IM_SIZE, Conf.IM_SIZE),\n            Transpose(p=0.5),\n            HorizontalFlip(0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1,0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            CoarseDropout(p=0.5),\n            ToTensorV2(p=1.0)\n])\n\ndef test_transform():\n    return Compose([\n            RandomResizedCrop(Conf.IM_SIZE, Conf.IM_SIZE, (1.0,1.0), ratio=(1.0,1.0)),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n            ToTensorV2(p=1.0)\n])\n\ndef TTA1():\n    return Compose([\n            RandomResizedCrop(Conf.IM_SIZE, Conf.IM_SIZE),\n            HorizontalFlip(1.0),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n            ToTensorV2(p=1.0)\n])\n\ndef TTA2():\n    return Compose([\n            RandomResizedCrop(Conf.IM_SIZE, Conf.IM_SIZE),\n            VerticalFlip(1.0),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n            ToTensorV2(p=1.0)\n])\n\ndef TTA3():\n    return Compose([\n            RandomResizedCrop(Conf.IM_SIZE, Conf.IM_SIZE),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=1.0),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1,0.1), p=1.0),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1.0),\n            ToTensorV2(p=1.0)\n])\n\nTTAs = [test_transform, TTA1, TTA2, TTA3]","998027f4":"class TrainData(Dataset):\n    def __init__(self, Dir, FNames, Labels, Transform):\n        self.dir = Dir\n        self.fnames = FNames\n        self.transform = Transform\n        self.lbs = Labels\n        \n    def __len__(self):\n        return len(self.fnames)\n\n    def __getitem__(self, index):\n        img = get_img(os.path.join(self.dir, self.fnames[index]))  \n        img = self.transform()(image=img)['image']\n        return img, self.lbs[index] \n        \nclass TestData(Dataset):\n    def __init__(self, Dir, FNames, TTAs=TTAs):\n        self.dir = Dir\n        self.fnames = FNames\n#         self.transform = Transform\n        self.ttas = TTAs\n        \n    def __len__(self):\n        return len(self.fnames)\n\n    def __getitem__(self, index):\n        img = get_img(os.path.join(self.dir, self.fnames[index]))\n        imglist = [tta()(image=img)['image'] for tta in self.ttas]\n        imglist = torch.stack(imglist)\n        return imglist, self.fnames[index]","5ce4834b":"trn_idx = folds[folds['fold'] != 0].index\nval_idx = folds[folds['fold'] == 0].index\n\nX_train, Y_train = X[trn_idx], Y[trn_idx]\nX_val, Y_val = X[val_idx], Y[val_idx]\n\ntrainset = TrainData(TRAIN_DIR, X_train, Y_train, train_transform)\ntrainloader = DataLoader(trainset,\n                         batch_size=Conf.BATCH,\n                         shuffle=True,\n                         num_workers=4)\n\nvalidset = TrainData(TRAIN_DIR, X_val, Y_val, test_transform)\nvalidloader = DataLoader(validset,\n                         batch_size=Conf.BATCH,\n                         shuffle=False,\n                         num_workers=4)\n\ntestset = TestData(TEST_DIR, X_test, TTAs)\ntestloader = DataLoader(testset,\n                        batch_size=1,\n                        shuffle=False,\n                        num_workers=4)","4866c865":"def rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n    bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n    bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n    bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix(data, target, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_target = target[indices]\n\n    lam = np.clip(np.random.beta(alpha, alpha),0.5,0.6)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n    new_data = data.clone()\n    new_data[:, :, bby1:bby2, bbx1:bbx2] = data[indices, :, bby1:bby2, bbx1:bbx2]\n    # adjust lambda to exactly match pixel ratio\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) \/ (data.size()[-1] * data.size()[-2]))\n    targets = (target, shuffled_target, lam)\n\n    return new_data, targets\n\ndef fmix(data, targets, alpha, decay_power, shape, max_soft=0.0, reformulate=False):\n    lam, mask = sample_mask(alpha, decay_power, shape, max_soft, reformulate)\n    mask =torch.tensor(mask, device=DEVICE).float()\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n    x1 = mask.to(DEVICE)*data\n    x2 = (1-mask).to(DEVICE)*shuffled_data\n    targets=(targets, shuffled_targets, lam)\n    \n    return (x1+x2), targets","b196092c":"class UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, images):\n\n        images = images.mul_(self.std).add_(self.mean)\n            # The normalize code -> t.sub_(m).div_(s)\n        return images\n\nmean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\ninv_normalize = UnNormalize(mean, std)","7d56f89b":"row = 4\ncol = 4\nfig, axes = plt.subplots(row, col, figsize=(16, 16))\naxes = axes.ravel()\n\nimages,labels = next(iter(trainloader))\nimages,_ = cutmix(images, labels, 1.)\nimages = inv_normalize(images)\nimages = images.detach().numpy().transpose(0,2,3,1)\n\nfor j in range(Conf.BATCH):\n    axes[j].imshow(images[j])","35793c3d":"# Model\nclass enetv2(nn.Module):\n    def __init__(self, out_dim=1, ModelName=\"efficientnet-b0\"):\n        super(enetv2, self).__init__()\n        self.basemodel = EfficientNet.from_name(Conf.modelname)\n        \n        self.myfc = nn.Linear(self.basemodel._fc.in_features, out_dim)\n        self.basemodel._fc = nn.Identity()        \n            \n    def extract(self, x):\n        return self.basemodel(x)\n\n    def forward(self, x):\n        x = self.basemodel(x)\n        x = self.myfc(x)\n        return x\n    \n# Loss\nclass LabelSmoothingLoss(nn.Module):\n\n    def __init__(self, classes, smoothing=0.0, dim=-1): \n        super(LabelSmoothingLoss, self).__init__() \n        self.confidence = 1.0 - smoothing \n        self.smoothing = smoothing \n        self.cls = classes \n        self.dim = dim \n    def forward(self, pred, target): \n        pred = pred.log_softmax(dim=self.dim) \n        with torch.no_grad(): \n            true_dist = torch.zeros_like(pred) \n            true_dist.fill_(self.smoothing \/ (self.cls - 1)) \n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","043fe1f2":"model = enetv2(5, Conf.modelname)\ncheckpoint = torch.load(\"..\/input\/efficientnet-pytorch\/efficientnet-b0-08094119.pth\", map_location=DEVICE)\nmodel.load_state_dict(checkpoint, strict=False)\nmodel = model.to(DEVICE)\n\ncriterion = LabelSmoothingLoss(5, 0.2)\noptimizer = torch.optim.Adam(model.parameters(), lr=Conf.LR)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', verbose=True)","bca49988":"def train_one_epoch(model, train_loader):\n    model.train()\n    running_loss = 0\n    correct = 0\n    total = 0\n    \n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n        \n        #cutmix and fmix\n#         mix_decision = np.random.rand()\n#         if mix_decision < 0.25:\n#             images, labels = cutmix(images, labels, 1.)\n#         elif mix_decision >= 0.25 and mix_decision < 0.5:\n#             images, labels = fmix(images, labels, alpha=1, decay_power=5., shape=(512,512))\n\n        mix_decision = np.random.rand()\n        if mix_decision < 0.5:\n            images, labels = cutmix(images, labels, 1.)\n        \n        optimizer.zero_grad() \n        outputs = model(images)\n        \n        if mix_decision < 0.5:\n            loss = criterion(outputs, labels[0]) * labels[2] + criterion(outputs, labels[1]) * (1. - labels[2])\n            labels = labels[0]\n        else:\n            loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predict = torch.max(outputs.data, 1)\n        correct += (predict == labels).sum().item()\n        total += labels.size(0)\n        \n    train_loss = running_loss \/ len(train_loader)\n    train_acc = correct \/ total\n    \n    return train_loss, train_acc\n\ndef valid_one_epoch(model, valid_loader):\n    model.eval()\n    running_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        \n        for batch_idx, (images, labels) in enumerate(valid_loader):\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            outputs = model(images)\n            \n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            \n            _, predict = torch.max(outputs.data, 1)\n            correct += (predict == labels).sum().item()\n            total += labels.size(0)\n            \n    val_loss = running_loss \/ len(valid_loader)\n    val_acc = correct \/ total\n    \n    return val_loss, val_acc","967f5667":"# best_score = 0.\n\n# if len(testloader) >= 2:\n#     for epoch_idx in range(Conf.EPOCHS):\n\n#         train_loss, train_acc = train_one_epoch(model, trainloader)\n#         valid_loss, valid_acc = valid_one_epoch(model, validloader)\n\n#         # model save\n#         if valid_acc > best_score:\n#             best_score = valid_acc\n\n#             torch.save(model.state_dict(), OUTPUT_DIR+f'{Conf.modelname}_best.pth')\n\n#             print('model saved')\n\n#         # rl scheduler\n#         scheduler.step(valid_loss)\n\n#         print('Epoch: {} |train_loss: {:.3f} valid loss: {:.3f} train_acc: {:.3f} valid_acc: {:.3f}'.format(epoch_idx, train_loss, valid_loss, train_acc, valid_acc))","664de4a6":"best_score = 0.\n\nfor epoch_idx in range(Conf.EPOCHS):\n\n    train_loss, train_acc = train_one_epoch(model, trainloader)\n    valid_loss, valid_acc = valid_one_epoch(model, validloader)\n\n    # model save\n    if valid_acc > best_score:\n        best_score = valid_acc\n\n        torch.save(model.state_dict(), OUTPUT_DIR+f'{Conf.modelname}_best.pth')\n\n        print('model saved')\n\n    # rl scheduler\n    scheduler.step(valid_loss)\n\n    print('Epoch: {} |train_loss: {:.3f} valid loss: {:.3f} train_acc: {:.3f} valid_acc: {:.3f}'.format(epoch_idx, train_loss, valid_loss, train_acc, valid_acc))","8a4d4c62":"if len(testloader) >= 2:\n    model.load_state_dict(torch.load(OUTPUT_DIR+f'{Conf.modelname}_best.pth'), strict=False)\ns_ls = []\n\nwith torch.no_grad():\n    model.eval()\n    for images, fname in testloader: \n        images = images.to(DEVICE)\n        batch_size, n_crops, c, h, w = images.size()\n        images = images.view(-1, c, h, w)\n        output = model(images)\n        output = output.mean(0)\n        ps = torch.exp(output)\n        _, top_class = ps.topk(1, dim=0)\n        \n        s_ls.append([fname[0], top_class.item()])","91bd97f5":"sub = pd.DataFrame.from_records(s_ls, columns=['image_id', 'label'])\nsub.to_csv(\"submission.csv\", index=False)","983b8127":"sub.head()","93a2eacb":"### Cutmix and Fmix","0034e997":"## Data loading","b5fd3fbd":"## Directory Setting","b5cd9bbe":"## Library import","43a41737":"## Config","c68d8461":"## CV split","d5ed8d75":"## Helper Functions","0a00b97c":"## Dataset\/Dataloader","d72342aa":"### Image Visualization","9f2ef030":"## Augmentation","c9215d6b":"## Inference","17fe318c":"## Make submission","35413adc":"#### For saving quota","2ba3bbfc":"# Training","ea129a06":"# Efficientnet baseline by Pytorch\n\n* model : efficientnet-b0(pretrained)\n* loss : CategoricalCrossEntropy(+label smoothing)\n* optimizer : Adam\n* preprocessing\n * RandomResizedCrop\n * RandomHorizontalFlip\n * RandomVerticalFlip\n * ColorJitter\n * ToTensor\n * Normalize\n*  TTA : True\n\n\n# log\n\nver2 : add \"RondomResizeCrop\" and \"ColorJitter\"  \nver3 : reduce the effect of \"ColorJitter\", add labelsmoothing  \nver4 : add TTA  \nver5 : add TTA3, increace epoch(13->25)  \nver6 : add cutmix, add fmix  \nver7 : delete fmix, change smoothing(0.2 -> 0.1)  \nver8 : delete rotate, change bbx size, change epoch(25->20)  \nver9 : imsize 512->600, change smoothing(0.1 -> 0.2), change ColorJitter rate  \nver10: using alubmentation(Fundamentally changed the augmentation)","924cc5ba":"## Model"}}