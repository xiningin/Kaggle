{"cell_type":{"fb4e1b05":"code","73420a5b":"code","a4b7f319":"code","59ec2213":"code","ab7235df":"code","bde859d3":"code","238b99cf":"code","62d57720":"code","f371c6be":"code","e5142361":"code","d36d548e":"code","ff3f1def":"code","81d01f8c":"code","fb43b36b":"code","07730760":"code","937e5bae":"code","cda501fc":"code","be2697ca":"code","d9d4fa53":"code","6fab7a84":"code","0e6cf322":"code","5864cdfe":"code","58435776":"code","aa879784":"code","54ffc4f4":"code","3ea2b94e":"code","1ab875e4":"code","aeea7779":"code","0d4b715c":"code","2dcc4fb8":"code","4d9eeffa":"code","9f5877f3":"code","234b8113":"code","47c97f79":"code","aa4d6670":"code","53747f4d":"code","8dd1982b":"code","ea2a561b":"code","963a3f66":"code","21b40063":"code","82e3bee1":"code","ac57da92":"code","22e3a2fe":"code","9eb6dccb":"code","0032c144":"code","e10e0060":"code","818f43f6":"code","4745b96d":"code","2ca6f4cd":"code","9e35ea28":"markdown","07ca675b":"markdown","7c5d9b76":"markdown","1453c962":"markdown","f0a4db9c":"markdown","338f2e78":"markdown","44f82e52":"markdown","4708ba74":"markdown","aa7bd5a0":"markdown","d9ca57b0":"markdown","6f8bb1c0":"markdown","ed71c919":"markdown","e26850d4":"markdown","b037bf7e":"markdown","f4fe25ec":"markdown","d97c0e97":"markdown","046b432b":"markdown","fe4a1927":"markdown","23fd51b0":"markdown","b7c0fa17":"markdown","e1b77ddd":"markdown","96c8f669":"markdown","6584c6a4":"markdown","f12605d7":"markdown","929c2c67":"markdown","80e8de8e":"markdown","dfc2b4ca":"markdown","fd261ef9":"markdown","b4126b88":"markdown","40a8fa3d":"markdown","348495ff":"markdown","e86e6dd6":"markdown","f891bfe1":"markdown","165cb52e":"markdown","15e5a198":"markdown","a5317cbb":"markdown","1c36eb33":"markdown","0af71e18":"markdown","b89f097c":"markdown","f133f796":"markdown","ce518824":"markdown","03ea50b2":"markdown","78e0a1a8":"markdown","f44a5f77":"markdown","674fb985":"markdown","586f4aba":"markdown","84746654":"markdown","051745f1":"markdown","c528e36a":"markdown","140bf430":"markdown","1c7c4fcb":"markdown","51263eca":"markdown","184a25dd":"markdown","e02ac75a":"markdown","9942e1f8":"markdown","169ab80e":"markdown","0936d563":"markdown","199ffec1":"markdown","7fc12e8a":"markdown","d3c633e1":"markdown","5b0784da":"markdown"},"source":{"fb4e1b05":"import pandas as pd # importing Pandas library for performing dataframe related operations\nimport numpy as np  # importing numpy for performing numeric array related operations\nimport matplotlib.pyplot as plt # importing matplotlib.pyplot for basic plotting operations\nimport seaborn as sns  # importing seaborn for advanced data visualization\n# Below is the magic function to display and save graphs\/figures in the output cells\n%matplotlib inline   \nfrom sklearn.model_selection import train_test_split # For train-test split\n# For standardizing\/normalizing the data (let's import many and see which suits the best)\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler \nfrom sklearn.preprocessing import PolynomialFeatures # To create polynomial features\nimport warnings # Import warnings module\nwarnings.filterwarnings('ignore') # Ignore the warnings\nfrom sklearn.feature_selection import f_regression,SelectKBest,mutual_info_regression,RFE # For feature selection\n# Let's import the various supervised ML models\nfrom sklearn.linear_model import LinearRegression # Linear Regression model\nfrom sklearn.neighbors import KNeighborsRegressor # K-NN Regressor Model\nfrom sklearn.svm import SVR # Support vector regressor\nfrom sklearn.tree import DecisionTreeRegressor # Decision tree regressor\n# Let's import the ensemble regressor models\nfrom sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor\nfrom sklearn.pipeline import Pipeline # Import sklearn pipeline\nfrom sklearn.model_selection import cross_val_score, KFold # Cross validation\nfrom sklearn.metrics import explained_variance_score # Metric used to evaluate the regression models\nfrom scipy.stats import zscore # zscore normalization from scipy.stats\nfrom sklearn.utils import resample # Used to find the bootstrapping confidence interval","73420a5b":"df_orig = pd.read_csv('..\/input\/concrete-compressive-strength-data-set\/compresive_strength_concrete.csv')\n# Creating a copy of the original dataframe\ndf = df_orig.copy()","a4b7f319":"df.head()","59ec2213":"# Renaming the column names\ndf.columns = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'fineagg', 'age', 'strength']","ab7235df":"# Lets see the dataframe once again..\ndf.head()","bde859d3":"# Let's see shape of the dataset\ndf.shape","238b99cf":"# Let's see datatypes of the attributes\ndf.info()","62d57720":"# Let's see the names of the independent columns\nfor col in df.columns:\n    if col!='strength': # Print all column names except target column\n        print(col)","f371c6be":"# Let's see the five point summary of the columns\ndf.describe()","e5142361":"# Let us check the missing values..\ndf.isnull().sum()","d36d548e":"# Let's us only plot the distributions of independent attributes\ndf.drop('strength',axis=1).hist(figsize=(12,16),layout=(4,2));","ff3f1def":"# Let's check the skewness values quantitatively\ndf.skew().sort_values(ascending=False)","81d01f8c":"# Let us check presence of outliers\nplt.figure(figsize=(18,14))\nbox = sns.boxplot(data=df)\nbox.set_xticklabels(labels=box.get_xticklabels(),rotation=90);","fb43b36b":"# Let us see how many of them are correlated..\nplt.figure(figsize=(12,10))\nsns.heatmap(df.corr(),annot=True, cmap=\"YlGnBu\")","07730760":"# Let us see the significant correlation either negative or positive among independent attributes..\nc = df.drop('strength',axis=1).corr().abs() # Since there may be positive as well as -ve correlation\ns = c.unstack() # \nso = s.sort_values(ascending=False) # Sorting according to the correlation\nso=so[(so<1) & (so>0.3)].drop_duplicates().to_frame() # Due to symmetry.. dropping duplicate entries.\nso.columns = ['correlation']\nso","937e5bae":"sns.pairplot(df,diag_kind='kde');","cda501fc":"# let us remove the outliers\nfor column in df.columns.tolist():\n    Q1 = df[column].quantile(.25) # 1st quartile\n    Q3 = df[column].quantile(.75) # 3rd quartile\n    IQR = Q3-Q1 # get inter quartile range\n    # Replace elements of columns that fall below Q1-1.5*IQR and above Q3+1.5*IQR\n    df[column].replace(df.loc[(df[column] > Q3+1.5*IQR)|(df[column] < Q1-1.5*IQR), column], df[column].median(),inplace=True)","be2697ca":"# Let us check presence of outliers\nplt.figure(figsize=(18,14))\nbox = sns.boxplot(data=df)\nbox.set_xticklabels(labels=box.get_xticklabels(),rotation=90);","d9d4fa53":"# Let's add this new composite feature before target attribute.\ndf.insert(8,'water\/cement',df['water']\/df['cement'])\n# Let's check whether the feature is added properly or not?\ndf.head()","6fab7a84":"df.corr()","0e6cf322":"poly3 = PolynomialFeatures(degree = 3, interaction_only=True)\npoly3_ft = poly3.fit_transform(df.drop('strength',axis=1))\ndf_poly3= pd.DataFrame(poly3_ft,columns=['feat_'+str(x) for x in range(poly3_ft.shape[1])])\ndf_poly3.head()","5864cdfe":"# Let us create the dataframe with all features\ndf_feat = df.drop('strength',axis=1).join(df_poly3)\ndf_feat['strength'] = df['strength']\nprint(df_feat.shape)\ndf_feat.head()","58435776":"df_feat.head()","aa879784":"from sklearn.linear_model import Lasso\nX = df_feat.drop('strength',axis=1)\ny = df_feat['strength']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nlasso = Lasso() # Since it uses L1 reglarization features with zero coefficients will be insignificant.\nlasso.fit(X_train,y_train)\nprint (\"Lasso model:\", (lasso.coef_))\n# Let's us get the features selected by Lasso\nlasso_feat = X_train.columns[lasso.coef_!=0].tolist() # Dropping the features with 0 coefficient value\nprint(lasso_feat) # Features selected using LASSO regularization\nprint(\"Out of total {} independent features, number of features selected by LASSO regularization are {} \".format(X_train.shape[1],len(lasso_feat)))\ndf_feat = df_feat[lasso_feat] # Select independent features \ndf_feat.head()","54ffc4f4":"from sklearn.cluster import KMeans\ndf_z = df_orig.apply(zscore) # Get the normalized dataframe\ncluster_range = range( 1, 15 )\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 10 )\n  clusters.fit(df_z)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:14]","3ea2b94e":"# Elbow plot\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","1ab875e4":"kmeans = KMeans(n_clusters= 7)\nkmeans.fit(df_z)","aeea7779":"labels = kmeans.labels_\ncounts = np.bincount(labels[labels>=0])\nprint(counts)","0d4b715c":"## creating a new dataframe only for labels and converting it into categorical variable\ncluster_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\ncluster_labels['labels'] = cluster_labels['labels'].astype('category')\ndf_labeled = df_orig.join(cluster_labels)\ndf_labeled.boxplot(by = 'labels',  layout=(3,3), figsize=(30, 20));","2dcc4fb8":"df_orig.columns = ['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg', 'fineagg', 'age', 'strength']\n# Let's create train and test sets\nX = df_orig.drop('strength',axis=1)\ny = df_orig['strength']","4d9eeffa":"# Let's split into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","9f5877f3":"# Create empty dataframe to store the results\ndf_result_raw_train = pd.DataFrame({'Regressor':[],'ExplVarianceScore':[],'StdDev':[]})\n# We will use the pipeline approach\npipelines = []\npipelines.append(('Linear Regression',Pipeline([('scaler',RobustScaler()),('LR',LinearRegression())])))\npipelines.append(('KNN Regressor',Pipeline([('scaler',RobustScaler()),('KNNR',KNeighborsRegressor())])))\npipelines.append(('SupportVectorRegressor',Pipeline([('scaler',RobustScaler()),('SVR',SVR())])))\npipelines.append(('DecisionTreeRegressor',Pipeline([('scaler',RobustScaler()),('DTR',DecisionTreeRegressor())])))\npipelines.append(('AdaboostRegressor',Pipeline([('scaler',RobustScaler()),('ABR',AdaBoostRegressor())])))\npipelines.append(('RandomForestRegressor',Pipeline([('scaler',RobustScaler()),('RBR',RandomForestRegressor())])))\npipelines.append(('BaggingRegressor',Pipeline([('scaler',RobustScaler()),('BGR',BaggingRegressor())])))\npipelines.append(('GradientBoostRegressor',Pipeline([('scaler',RobustScaler()),('GBR',GradientBoostingRegressor())])))","234b8113":"# Let's find and store the cross-validation score for each pipeline for training data with raw features.\nfor ind, val in enumerate(pipelines):\n    # unpack the val\n    name, pipeline = val\n    kfold = KFold(n_splits=10,random_state=2020) \n    cv_results = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='explained_variance')\n    df_result_raw_train.loc[ind] = [name,cv_results.mean()*100,cv_results.std()*100]","47c97f79":"# Let's check the training results with raw features \ndf_result_raw_train","aa4d6670":"# Let's find and store the cross-validation score for each pipeline for test data with raw features.\ndf_result_raw_test = pd.DataFrame({'Regressor':[],'ExplVarianceScore':[]})\nfor ind, val in enumerate(pipelines):\n    # unpack the val\n    name, pipeline = val\n    pipeline.fit(X_train,y_train)\n    y_pred = pipeline.predict(X_test)\n    df_result_raw_test.loc[ind] = [name,explained_variance_score(y_test,y_pred)*100]","53747f4d":"# Let's check the test results with raw features\ndf_result_raw_test","8dd1982b":"df_feat.head()","ea2a561b":"# Let's create train and test sets from modified dataframe with raw as well as new features.\nX = df_feat\ny = df['strength']\n# Let's split into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Let's create dataframe to store the results.\ndf_result_mod_train = pd.DataFrame({'Regressor':[],'ExplVarianceScore':[],'StdDev':[]})\nfor ind, val in enumerate(pipelines):\n    # unpack the val\n    name, pipeline = val\n    kfold = KFold(n_splits=10,random_state=2020) \n    cv_results = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='explained_variance')\n    df_result_mod_train.loc[ind] = [name,cv_results.mean()*100,cv_results.std()*100]\n# Let's check the training results with raw features  as well as new features\ndf_result_mod_train","963a3f66":"# Let's find and store the cross-validation score for each pipeline for training data with raw as well as new features.\ndf_result_mod_test = pd.DataFrame({'Regressor':[],'ExplVarianceScore':[]})\nfor ind, val in enumerate(pipelines):\n    # unpack the val\n    name, pipeline = val\n    pipeline.fit(X_train,y_train)\n    y_pred = pipeline.predict(X_test)\n    df_result_mod_test.loc[ind] = [name,explained_variance_score(y_test,y_pred)*100]\n# Let's check the test results with raw features  as well as new features\ndf_result_mod_test","21b40063":"# Separate target and independent features\nX = df_orig.drop('strength',axis=1)\ny = df_orig['strength']\n# Let's split into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Creating pipelines for 3 best models.\npipe_rf = Pipeline([('scaler',RobustScaler()),('RBR',RandomForestRegressor())])\npipe_br = Pipeline([('scaler',RobustScaler()),('BGR',BaggingRegressor())])\npipe_gbr = Pipeline([('scaler',RobustScaler()),('GBR',GradientBoostingRegressor())])\n# Initalize the empty dataframes to capture the feature importances given by these models..\ndf_featImp_rf = df_featImp_br = df_featImp_gbr = pd.DataFrame({'Features':[], 'Importance':[]})","82e3bee1":"# feature importance given by random forest regressor\npipe_rf.fit(X_train,y_train)\nfeatImp_rf = pipe_rf.steps[1][1].feature_importances_\ndf_featImp_rf['Features'] = X_train.columns\ndf_featImp_rf['Importance'] = featImp_rf\n# Feature importance given by Random Forest Regressor\ndf_featImp_rf.sort_values(by='Importance', ascending=False)","ac57da92":"# feature importance given by Gradient Boost Regressor\npipe_gbr.fit(X_train,y_train)\nfeatImp_gbr = pipe_gbr.steps[1][1].feature_importances_\ndf_featImp_gbr['Features'] = X_train.columns\ndf_featImp_gbr['Importance'] = featImp_gbr\n# Feature importance given by Random Forest Regressor\ndf_featImp_gbr.sort_values(by='Importance', ascending=False)","22e3a2fe":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# Separate target and independent features\nX = df_orig.drop('strength',axis=1)\ny = df_orig['strength']\n# Let's split into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\npipe_gbr = Pipeline([('scaler',RobustScaler()),('GBR',GradientBoostingRegressor())])","9eb6dccb":"# Let's see what are the hyper parameters for gradient boosting regressor model\npipe_gbr.steps[1][1]","0032c144":"param_grid=[{'GBR__n_estimators':[100,500,1000], 'GBR__learning_rate': [0.1,0.05,0.02,0.01], 'GBR__max_depth':[4,6], \n            'GBR__min_samples_leaf':[3,5,9,17], 'GBR__max_features':[1.0,0.3,0.1] }]","e10e0060":"search = GridSearchCV(pipe_gbr, param_grid, cv = kfold, scoring = 'explained_variance', n_jobs=-1)\nsearch.fit(X_train, y_train)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","818f43f6":"y_pred_train = search.predict(X_train)\ny_pred_test = search.predict(X_test)\nfrom sklearn.metrics import explained_variance_score,r2_score\nprint('Testing Explained Variance Score is  {}'.format(explained_variance_score(y_test,y_pred_test)))\nprint('Testing R2 Score is  {}'.format(r2_score(y_test,y_pred_test)))","4745b96d":"random_grid={'GBR__n_estimators':[100,500,1000], 'GBR__learning_rate': [0.1,0.05,0.02,0.01], 'GBR__max_depth':[4,6], \n            'GBR__min_samples_leaf':[3,5,9,17], 'GBR__max_features':[1.0,0.3,0.1] }\nsearch = RandomizedSearchCV(estimator=pipe_gbr, param_distributions=random_grid, n_iter = 5, cv = kfold, scoring = 'explained_variance', n_jobs=-1)\nsearch.fit(X_train, y_train)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","2ca6f4cd":"y_pred_train = search.predict(X_train)\ny_pred_test = search.predict(X_test)\nfrom sklearn.metrics import explained_variance_score,r2_score\nprint('Testing Explained Variance Score is  {}'.format(explained_variance_score(y_test,y_pred_test)))\nprint('Testing R2 Score is  {}'.format(r2_score(y_test,y_pred_test)))","9e35ea28":"#### Accuracy of models using raw features","07ca675b":"## Dataset Information:\n* **Name: concrete.csv**\n* **Domain: Cement Manufacturing**\n\n## Aim: Predicting concrete compressive strength given specifications of the age and other ingredients of the mixture.\n","7c5d9b76":"### 1.a. Univariate analysis","1453c962":"#### In this section, we first examine the effect of step 2.a (creating composite feature, droping a feature) on the model accuracy and will decide which dataframe to use in the final analysis\n\nWe will use df_orig (our original dataframe with raw features) and df_feat (modified) dataframes for this analysis","f0a4db9c":"**By observing the first five rows, one can see that all attributes are numeric in nature. Columns names are much longer and should be renamed.**","338f2e78":"### 2.c. Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and present your findings in terms of the independent attributes and their suitability to predict strength","44f82e52":"## 3. Deliverable -3 (create the model )","4708ba74":"## About the dataset, column attributes (independent variables) and target variable (dependent variable)\n\nThe data is related to cement manufacturing industry where actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from laboratory. Data is in raw form (not scaled).The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations). Note that- **The concrete compressive strength is a highly nonlinear function of age and ingredients**. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.","aa7bd5a0":"**Feature importances given by random forest regressor and gradient boost regressor differ. One can see that,**\n* *age* and *cement* attributes are the most important features to predict the concrete strength\n* *ash* attribute the weakest predictor of concrete strength","d9ca57b0":"#### Using GridSearchCV based model tuning the training and test scores (explained variance) are about 94%.","6f8bb1c0":"One can see that, the newly added feature is moderately correlated (negatively) with target attribute ","ed71c919":"**There are total 9 attributes in the dataset and in the context of the given problem, the target (or dependent) attribute is \"strength\" whereas the remaining are independent attributes.**","e26850d4":"### 1.b. Multivariate analysis","b037bf7e":"There is a non-linear relationship between water-cement ratio and concrete strenth. Smaller values of water-cement ratio (W\/C) are associated with higher values of concrete strength where as higher values of ratio provide small concrete strength.","f4fe25ec":"### 4.a Algorithms that you think will be suitable for this project","d97c0e97":"**Observations:**\n* Target attribute strength is moderately correlated (positive) to cement and to some level superplastic and age. It is  slightly correlated with slag.\n* Target attribute is slightly negatively correlated with ash, fineagg and coarseagg. It is negatively correlated with water to some extent.","046b432b":"### Let us take a look at the raw dataset","fe4a1927":"**The dataset do not have any missing values.**","23fd51b0":"**Dropping a feature: We won't drop any feature for the time being, however after feature engineering stage, we may choose to drop some features to avoid overfitting** ","b7c0fa17":"## 2. Deliverable -2 (Feature Engineering techniques)\n* 2.a. Identify opportunities (if any) to create a composite feature, drop a feature\n* 2.b. Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help\n* 2.c. Explore for gaussians. If data is likely to be a mix of gaussians, explore individual clusters and present your findings in terms of the independent attributes and their suitability to predict strength","e1b77ddd":"### 3.a. Obtain feature importance for the individual features and present your findings","96c8f669":"## Learning Outcomes:\n* Exploratory Data Analysis\n* Building ML models for regression\n* Hyper parameter tuning","6584c6a4":"**Observations:** Assuming correlation values > 0.3 to be significant among the independent variables, one can observe that\n* superplastic and water are strongly correlated\n* fineagg and water are moderately correlated\n* cement and ash, ash and superplastic, slag and ash are also correlated to some extent.","f12605d7":"**The newly created dataframe with features contains 139 independent and 1 target attribute. Since, all the features will not be useful, we apply LASSO regularization step and select the most relevant features. This will also reduce the problem of overfitting models.**","929c2c67":"In this problem, we do have an opportunity to add a composite feature which is highly useful to predict the concrete strength in MPa (Mega Pascal) namely **water-cement ratio**. (Ref: https:\/\/en.wikipedia.org\/wiki\/Water%E2%80%93cement_ratio )","80e8de8e":"**Since Bagging Regressor can be used with many base estimators, no method to find feature importances is implemented**","dfc2b4ca":"From results presented above, we can conclude that ensemble based regression models namely gradient boost regressor, bagging regressor and random forest regressor are most effective for this problem and we will **tune** the gradient boosting model (best performing) to squeeze the extra performance out of it.","fd261ef9":"## 1. Deliverable -1 (Exploratory data quality report )","b4126b88":"Except age attribute, the data type of remaining attributes is float since they indicate measurement values of physical variables. Datatype of age attribute is integer as per the expectation. Dataset seems to have no missing values.","40a8fa3d":"**Goal: To perform bi-variate analysis between the predictor variables and between the predictor variables and target column which should include**\n* finding the relationship and degree of relation between independent variables and between independent variables and target variable\n* Visualization using boxplots, pair plots, histograms or density curves\n* Selecting the most appropriate attributes","348495ff":"**Goal: Explaining data types and description of the independent attributes which should include**\n* name\n* meaning \n* range of values observed \n* central values (mean and median) \n* standard deviation and quartiles \n* analysis of the body of distributions \/ tails \n* missing values, outliers","e86e6dd6":"**There are 1030 observations and 9 column attributes in the dataset**","f891bfe1":"**No distinct clusters are visible at any number of clusters. Looks like the attributes are weak predictors except for cement. The potential of getting better results by breaking data into clusters is unlikely to give the desired result.**","165cb52e":"**Observations:**\n* Target attribute strength do not show any kind of relationship with other independent attributes except cement and superplastic. Attributes cement and superplastic seem to have linear relationship with target attribute.\n* Attribute water seems to be negatively related with strength \n* Attribute age do not show any relation with other independent attributes. Slightly non-linear relation can be observed between age and strength\n* fineagg and coarseagg shows slight negative linear relationship with water\n* superplastic attribute has negative correlation with the water. \n* Except for strength and cement attributes, density curves for most of the attribute show multiple peaks","15e5a198":"### 2.b. Decide on complexity of the model, should it be simple linear model in terms of parameters or would a quadratic or higher degree help","a5317cbb":"**Strategy to remove outliers:** We choose to replace attribute outlier values by their respective medians.","1c36eb33":"Observations:\n* One can see that, box-plots of independent attributes namely slag, water, superplastic, fineagg and age show outliers\n* Target attribute i.e., strength also has outliers\n* water attribute shows presence of outliers on both sides of the whiskers\n* Box plot of age attribute shows high amount of outliers","0af71e18":"**By refering to the boxplot shown in the univariate analysis section, one can oberve that**\n* box-plots of attributes namely slag, ash, superplastic, fineagg, age and strength show presence of outliers above third quartile only.\n* water attribute has presence of outliers both below the first quartile as well as above the third quartile","b89f097c":"Observations:\n* Distribition of most of the columns show positive skewness (long right tail)\n* The skewness values show agreement with our qualitative analysis of the distribution plot shown above\n* Water and coarseag have approximately symmetric distributions\n* Attributes coarseagg and fineagg show negative skewness","f133f796":"**Since there are no missing values in the dataset, no missing value imputation is necessary.** (please refer univariate analysis section)","ce518824":"Observations: \n* cement: the range of this attribute is 102 (min) to 540 (max). The mean (281.16) and median (272.90) values differ slightly. The standard deviation value is quite high (104.50). For 75% of the given observations, this value is less than 350 (3rd quartile).\n* slag: the range of this attribute is 0 (min) to 359.40 (max). The mean (73.89) and median (22) values differ significantly indicating skewness of the distribution. The standard deviation value is 86.27. For 25% of the given observations, this value is 0..! (1st quartile).\n* ash: the range of this attribute is 0 (min) to 200 (max). Difference between the mean (54.18) and median (0) values is very high. The standard deviation value (64) is also high. For 50% of the given observations, this value is 0..! (2nd quartile). This attribute seems to have highly skewed distribution.\n* water: the range of this attribute is 121.8 (min) to 247 (max). The mean (181.56) and median (185) values differ slightly. The standard deviation value is 21.35. For 75% of the given observations, this value is 192 (3rd quartile). This attribute seems to have normal distribution.\n* superplastic: the range of this attribute is 0 (min) to 32.20 (max). Difference between the mean (620) and median (6.40) values is negligible. The standard deviation value is almost 6. For 25% of the given observations, this value is 0..! (1st quartile). \n* coarseagg: the range of this attribute is 801 (min) to 1145 (max). The mean (aprox. 973) and median (968) values differ slightly. The standard deviation value is 77.75. For 75% of the given observations, this value is 1029 (3rd quartile). \n* fineagg: the range of this attribute is 594 (min) to 992 (max). The mean (aprox. 773) and median (779) values differ slightly. The standard deviation value is 80.17. For 25% of the given observations, this value is 731 (1st quartile).\n* age: the range of this attribute is 1 (min) to 365 (max) days. The mean (approx. 46) and median (28) values differ significantly. The standard deviation value is approx 63 which is quite high. For 75% of the given observations, this value is 56 days (3rd quartile). This attribute seems to have skewed distribution.","03ea50b2":"Observations:\n* Disributions of attributes namely age, ash, slag and superplastic show high skewness\n* Disribution of cement attribute shows moderate skewness\n* Distributions of coarseagg, fineagg and water look normally distributed","78e0a1a8":"**One can see that, using the feature engineered dataframe, we do not get any significant improvement in the performance of ML models than that of obtained with raw features. Although, performance of linear regression model is increased significantly (from 62% to 72%), we use dataframe with raw features in our final stage i.e., model tuning since ensemble models which are better choice for this problem, do not show any significant improvement for selected features (118) over performance with raw features (8).**","f44a5f77":"#### Accuracy of models using modified dataframe","674fb985":"**Observations after outlier removal:**\n* Outliers are completely removed from slag, water, superplastic attributes\n* New outliers below first quartile are created in fineagg column. However, we are not going to treat them again.\n* Age attribute still shows some outliers\n* Outliers in the strength column are not removed completely.","586f4aba":"### 4. b. Techniques employed to squeeze that extra performance out of the model without making it overfit or underfit","84746654":"### 2) Reading the dataset as dataframe","051745f1":"**Attribute Information**:\n* Cement : measured in kg in a m3 mixture\n* Blast : measured in kg in a m3 mixture\n* Fly ash : measured in kg in a m3 mixture\n* Water : measured in kg in a m3 mixture\n* Superplasticizer : measured in kg in a m3 mixture\n* Coarse Aggregate : measured in kg in a m3 mixture\n* Fine Aggregate : measured in kg in a m3 mixture\n* Age : day (1~365)\n* Strength: Concrete compressive strength measured in MPa (**Target Attribute**)","c528e36a":"As provided in the dataset information the strength of the concrete is a highly nonlinear function of age and ingredients. Thus using a simple linear model won't help in this problem to get better modeling accuracy. Use of quadratic and higher degree makes sense in this case to incorporate the nonlinear relationship between target and independent attributes","140bf430":"### 1.c. Pick one strategy to address the presence outliers and missing values and perform necessary imputation","1c7c4fcb":"### 1) Importing Necessary Libraries","51263eca":"In this section we test and compare predictive performance of various ML Models using the amount of explained variance (in percentage) as an evaluation metric. ML Models used for comparison are:\n* Linear Regression\n* K-NN Regressor\n* SVR\n* Decision Tree Regressor\n* Adaboost Regressor\n* Random Forest Regressor\n* Bagging Regressor\n* Gradient Boost Regressor","184a25dd":"## Deliverable -4 (Tuning the model)\n* a. Algorithms that you think will be suitable for this project\n* b. Techniques employed to squeeze that extra performance out of the model without making it overfit or underfit\n* c. Model performance range at 95% confidence level","e02ac75a":"Meaning of each attribute name:\n* cement: it indicates amount of cement quantity measured in kg (kilogram) in a m3 mixture \n* slag: it indicates amount of blast furnace slag quantity measured in kg (kilogram) in a m3 mixture\n* ash: it indicates amount of fly ash measured in kg in a m3 mixture\n* water : it indicates amount of water quantity measured in kg in a m3 mixture\n* superplastic : it indicates amount of superplasticizer quantity measured in kg in a m3 mixture\n* coarseagg : it indicates amount of coarse aggregate measured in kg in a m3 mixture\n* fineagg : it indicates amount of fine aggregate quantity measured in kg in a m3 mixture\n* age : it indicates age of the concrete mixure measured days (1~365)\n\nAll the independent column names are appropriate as per their description.","9942e1f8":"In order to incorporate the non-linear feature interaction among independent features we choose to add polynomial features with degree 3.","169ab80e":"#### Using RandomizedSearchCV","0936d563":"We will make use of GridSearchCV and RandomSearchCV functions to find the best model parameters by tuning the hyper parameters","199ffec1":"### 2.a Identify opportunities (if any) to create a composite feature, drop a feature","7fc12e8a":"**Since, the target variable in this problem i.e., strength is a highly non-linear function of independent variables, any algorithm which captures this non-linear relashionship is suitable for this project. Basic models applicable  for this problem can be linear regression with polynomial features of degree d with d>1, models with nonlinear kernels such as support vector regressor with nonlinear kernels such as RBF. Decision tree regression model is also suitable since it can also capture nonlinearity however more generalized model such as random forest regression would be more effective. Finally, ensemble regression models which consist of multiple models which either work parallely or sequentially to give final output such as adaboost, bagging and gradient boost regressor should be most effective**","d3c633e1":"**From the above result dataframes, it is clear that ensemble models are better choice for this problem. Without any feature engneering, we get best score using Gradient Boosting Regressor both in terms of explained variance in percentage and standard deviation. As per the expectations, linear regression performs poorly.**","5b0784da":"### Feature importance given by the ensemble models\nFor this problem, the ensemble models work well. In this section we analyse the feature importance given by these models."}}