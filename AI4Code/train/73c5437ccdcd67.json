{"cell_type":{"6dbe4e33":"code","75156b25":"code","0bbf8620":"code","0ff042de":"code","67ff3b85":"code","1d874ff6":"code","5db6f832":"code","a314bdcb":"code","cb362ec3":"code","e93300d0":"code","ceab463b":"code","de9972d8":"code","39ddffd3":"code","01c1446c":"code","d4ba4018":"code","7105739e":"code","3954cf46":"code","c80ff4f0":"code","92e6ca41":"code","a2860a3b":"code","b70ddf64":"code","4c58a1ab":"code","975e0500":"code","1060d473":"code","113584db":"code","b29fc5b5":"code","794da83e":"code","ae5db133":"code","5400586e":"code","b672f531":"code","96baf886":"code","8c8b9cba":"code","47f3761b":"code","319f4b77":"code","15b2275c":"code","55141f1d":"code","0c67b31c":"code","a0d3ebfd":"code","9c68f99c":"code","777d9f0f":"markdown","03fc5cdc":"markdown","f63578b8":"markdown","d1ca3b8c":"markdown"},"source":{"6dbe4e33":"!pip install neurokit2\n!pip install wfdb\n!pip install ecg_plot\n!pip install tensorflow_addons","75156b25":"import ecg_plot\nimport wfdb\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport neurokit2 as nk\nfrom scipy import signal\nfrom scipy.io import loadmat\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import plot_model\nfrom keras.preprocessing.sequence import pad_sequences\nfrom scipy.io import loadmat\nimport cv2\nfrom matplotlib.collections import LineCollection\nfrom sklearn import metrics","0bbf8620":"def load_challenge_data(filename):\n    x = loadmat(filename)\n    data = np.asarray(x['val'], dtype=np.float64)\n    new_file = filename.replace('.mat','.hea')\n    input_header_file = os.path.join(new_file)\n    with open(input_header_file,'r') as f:\n        header_data=f.readlines()\n    return data, header_data","0ff042de":"normal_sinus = []\ndirectory = \"..\/input\/normalsinusdataset\/only_sinus\"\n\nfor ecgfilename in tqdm(sorted(os.listdir(directory))):\n    filepath = directory + os.sep + ecgfilename\n    if filepath.endswith(\".mat\"):\n        data, header_data = load_challenge_data(filepath)\n        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n        normal_sinus.append(data)\nnormal_sinus = np.asarray(normal_sinus)","67ff3b85":"ischemia = []\ndirectory = \"..\/input\/ischemia-dataset\/ischemia_dataset\"\nj = 0\nfor ecgfilename in tqdm(sorted(os.listdir(directory))):\n    filepath = directory + os.sep + ecgfilename\n    if filepath.endswith(\".mat\"):\n        data, header_data = load_challenge_data(filepath)\n        data = pad_sequences(data, maxlen=5000, truncating='post',padding=\"post\")\n        ischemia.append(data)\nischemia = np.asarray(ischemia)","1d874ff6":"def resample_beats(beats):\n    rsmp_beats=[]\n    for i in beats:\n        i = np.asarray(i)\n\n        #i = i[~np.isnan(i)]\n        f = signal.resample(i, 250)\n        rsmp_beats.append(f)\n    rsmp_beats = np.asarray(rsmp_beats)\n    return rsmp_beats","5db6f832":"def median_beat(beat_dict):\n    beats = []\n    for i in beat_dict.values():\n        #print(i['Signal'])\n        beats.append(i['Signal'])\n    beats = np.asarray(beats)\n    rsmp_beats = resample_beats(beats)\n    med_beat = np.median(rsmp_beats,axis=0)\n    return med_beat","a314bdcb":"def process_ecgs(raw_ecg):    \n    processed_ecgs=[]\n    for i in tqdm(range(len(raw_ecg))):\n        leadII = raw_ecg[i][1]\n        leadII_clean = nk.ecg_clean(leadII, sampling_rate=500, method=\"neurokit\")\n        r_peaks = nk.ecg_findpeaks(leadII_clean, sampling_rate=500, method=\"neurokit\", show=False)\n        twelve_leads = []\n        for j in raw_ecg[i]:\n            try:\n                beats = nk.ecg_segment(j, rpeaks=r_peaks['ECG_R_Peaks'], sampling_rate=500, show=False)\n                med_beat = median_beat(beats)\n                twelve_leads.append(med_beat)\n            except:\n                beats = np.ones(250)*np.nan\n                twelve_leads.append(beats)\n        #twelve_leads = np.asarray(twelve_leads)\n        processed_ecgs.append(twelve_leads)\n    processed_ecgs = np.asarray(processed_ecgs)\n    return processed_ecgs","cb362ec3":"norm_ecgs = process_ecgs(normal_sinus)","e93300d0":"ischemia_ecgs = process_ecgs(ischemia)","ceab463b":"def remove_nans(ecg_arr):\n    new_arr = []\n    for i in tqdm(ecg_arr):\n        twelve_lead = []\n        for j in i:\n            if j[0] != j[0]:\n                j = np.ones(250)\n            twelve_lead.append(j)\n        new_arr.append(twelve_lead)\n    new_arr = np.asarray(new_arr)\n    return new_arr","de9972d8":"new_norm = remove_nans(norm_ecgs)","39ddffd3":"new_ischemia = remove_nans(ischemia_ecgs)","01c1446c":"def delete_garbage(ecg_arr):\n    #new_ecg_arr = ecg_arr.copy()\n    delete_list = []\n    for i in tqdm(range(len(ecg_arr))):\n        if np.all(ecg_arr[i].T[0]==1):\n            delete_list.append(i)\n    ecg_arr = np.delete(ecg_arr,delete_list,axis=0)\n    \n    return ecg_arr","d4ba4018":"clean_ischemia = delete_garbage(new_ischemia)","7105739e":"clean_norm = delete_garbage(new_norm)","3954cf46":"clean_ischemia = clean_ischemia.reshape(clean_ischemia.shape[0],250,12)","c80ff4f0":"clean_norm = clean_norm.reshape(clean_norm.shape[0],250,12)","92e6ca41":"leads = [\"Lead-I\",\"Lead-II\", \"Lead-III\",\"aVR\",\"aVL\",\"aVF\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]\nplt.figure(figsize=(26, 16))\nfor l in range(12):\n    plt.subplot(3, 4, l + 1)\n    plt.title(\"Normal - Lead: {} \".format(leads[l]), fontsize=20)\n    plt.plot(clean_norm[0].reshape(12,250)[l],'k')","a2860a3b":"print(f\"Patients with Normal Sinus Rythm = {clean_norm.shape[0]}\")\nprint(f\"Patients with Ischemia = {clean_ischemia.shape[0]}\")","b70ddf64":"def FCN():\n    inputlayer = keras.layers.Input(shape=(250,12)) \n\n    conv1 = keras.layers.Conv1D(filters=128, kernel_size=15,input_shape=(250,12), padding='same')(inputlayer)\n    conv1 = keras.layers.BatchNormalization()(conv1)\n    conv1 = keras.layers.Activation(activation='relu')(conv1)\n    #Legger til spatial dropout for \u00e5 f\u00e5 med mer enn bare V4 som prediksjonsgrunnlag\n    conv1 = keras.layers.SpatialDropout1D(0.1)(conv1)\n\n    conv2 = keras.layers.Conv1D(filters=256, kernel_size=10, padding='same')(conv1)\n    conv2 = keras.layers.BatchNormalization()(conv2)\n    conv2 = keras.layers.Activation('relu')(conv2)\n    #Legger til spatial dropout for \u00e5 f\u00e5 med mer enn bare V4 som prediksjonsgrunnlag\n    conv2 = keras.layers.SpatialDropout1D(0.1)(conv2)\n\n    conv3 = keras.layers.Conv1D(512, kernel_size=5,padding='same')(conv2)\n    conv3 = keras.layers.BatchNormalization()(conv3)\n    conv3 = keras.layers.Activation('relu')(conv3)\n    #Legger til spatial dropout for \u00e5 f\u00e5 med mer enn bare V4 som prediksjonsgrunnlag\n    conv3 = keras.layers.Dropout(0.2)(conv3)\n\n    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n    #gap_layer = keras.layers.Flatten()(conv3)\n\n\n    output_layer = tf.keras.layers.Dense(units=2,activation='softmax')(gap_layer)\n\n    model = keras.Model(inputs=inputlayer, outputs=output_layer)\n    \n    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n    return model","4c58a1ab":"def encoder_model():\n    input_layer = tf.keras.layers.Input(shape=(250, 12))\n\n\n     # conv block -1\n    conv1 = tf.keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(input_layer)\n    conv1 = tfa.layers.InstanceNormalization()(conv1)\n    conv1 = tf.keras.layers.PReLU(shared_axes=[1])(conv1)\n    conv1 = tf.keras.layers.SpatialDropout1D(rate=0.1)(conv1)\n    conv1 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1)\n    # conv block -2\n    conv2 = tf.keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n    conv2 = tfa.layers.InstanceNormalization()(conv2)\n    conv2 = tf.keras.layers.PReLU(shared_axes=[1])(conv2)\n    conv2 = tf.keras.layers.SpatialDropout1D(rate=0.1)(conv2)\n    conv2 = tf.keras.layers.MaxPooling1D(pool_size=2)(conv2)\n    # conv block -3\n    conv3 = tf.keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n    conv3 = tfa.layers.InstanceNormalization()(conv3)\n    conv3 = tf.keras.layers.PReLU(shared_axes=[1])(conv3)\n    conv3 = tf.keras.layers.SpatialDropout1D(rate=0.1)(conv3)\n    # split for attention\n    attention_data = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n    attention_softmax = tf.keras.layers.Lambda(lambda x: x[:,:,:])(conv3)\n    # attention mechanism\n    attention_softmax = tf.keras.layers.Softmax()(attention_softmax)\n    multiply_layer = tf.keras.layers.Multiply()([attention_softmax,attention_data])\n    # last layer\n    dense_layer = tf.keras.layers.Dense(units=512,activation='sigmoid')(multiply_layer)\n    dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n    dense_layer = tf.keras.layers.Dropout(rate=0.2)(dense_layer)\n    # output layer\n    flatten_layer = tf.keras.layers.Flatten()(dense_layer)\n    output_layer = tf.keras.layers.Dense(units=2,activation='softmax')(flatten_layer)\n\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n\n    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n\n    return model","975e0500":"def simple_CNN():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(250,12)))\n    model.add(tf.keras.layers.SpatialDropout1D(0.1))\n    model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n    model.add(tf.keras.layers.SpatialDropout1D(0.1))\n    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(100, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])\n    return model","1060d473":"def FCN_new():\n    inputlayer = keras.layers.Input(shape=(250,12)) \n\n    conv1 = keras.layers.Conv1D(filters=128, kernel_size=15,input_shape=(250,12), padding='same')(inputlayer)\n    conv1 = keras.layers.BatchNormalization()(conv1)\n    conv1 = keras.layers.Activation(activation='relu')(conv1)\n    #Legger til spatial dropout for \u00e5 f\u00e5 med mer enn bare V4 som prediksjonsgrunnlag\n    conv1 = keras.layers.SpatialDropout1D(0.1)(conv1)\n\n    conv2 = keras.layers.Conv1D(filters=256, kernel_size=10, padding='same')(conv1)\n    conv2 = keras.layers.BatchNormalization()(conv2)\n    conv2 = keras.layers.Activation('relu')(conv2)\n    #Legger til spatial dropout for \u00e5 f\u00e5 med mer enn bare V4 som prediksjonsgrunnlag\n    conv2 = keras.layers.SpatialDropout1D(0.1)(conv2)\n\n    conv3 = keras.layers.Conv1D(512, kernel_size=5,padding='same')(conv2)\n    conv3 = keras.layers.BatchNormalization()(conv3)\n    conv3 = keras.layers.Activation('relu')(conv3)\n    #Legger til spatial dropout for \u00e5 f\u00e5 med mer enn bare V4 som prediksjonsgrunnlag\n    conv3 = keras.layers.Dropout(0.2)(conv3)\n\n    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n    #gap_layer = keras.layers.Flatten()(conv3)\n\n\n    output_layer = tf.keras.layers.Dense(units=2,activation='softmax')(gap_layer)\n\n    model = keras.Model(inputs=inputlayer, outputs=output_layer)\n    \n    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[\"accuracy\"])\n    return model","113584db":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_accuracy', factor=0.1, patience=1, verbose=1, mode='max',\n    min_delta=0.0001, cooldown=5, min_lr=0\n)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', verbose=3, patience=15, restore_best_weights=True)","b29fc5b5":"print(f\"Training dataset = {clean_ischemia.shape[0] + clean_norm.shape[0] - 400}\")\nprint(\"Validation set = 400\")","794da83e":"model = FCN()","ae5db133":"tf.keras.utils.plot_model(model)","5400586e":"val_acc = []\ntrain_acc = []\ntrue_pos = np.ones([10,100])\nfalse_pos = np.ones([10,100])\nbatchsize=30\nfor i in range(10):\n    print(\"{}-fold\".format(i))\n    norm_index = np.random.choice(np.arange(len(clean_norm)),200)\n    ischemia_index = np.random.choice(np.arange(len(clean_ischemia)),200)\n    \n    norm_val = clean_norm[norm_index]\n    norm_train = np.delete(clean_norm,norm_index, axis=0)\n    \n    ischemia_val = clean_ischemia[ischemia_index]\n    ischemia_train = np.delete(clean_ischemia,ischemia_index, axis=0)\n    \n    y_norm_train = np.zeros(norm_train.shape[0])\n    y_norm_val = np.zeros(norm_val.shape[0])\n    \n    y_ischemia_train = np.ones(ischemia_train.shape[0])\n    y_ischemia_val = np.ones(ischemia_val.shape[0])\n    \n    train_data = np.vstack([norm_train,ischemia_train])\n    y_train = np.hstack([y_norm_train,y_ischemia_train])\n    \n    val_data = np.vstack([norm_val,ischemia_val])\n    y_val = np.hstack([y_norm_val,y_ischemia_val])\n    \n    sample_weight = np.ones(shape=(len(y_train),))\n    sample_weight[y_train == 1] = len(norm_train)\/len(ischemia_train)\n    \n    model = FCN()\n\n    history = model.fit(x=train_data,y=tf.keras.utils.to_categorical(y_train), epochs=50, batch_size=batchsize, \n              validation_data=(val_data,tf.keras.utils.to_categorical(y_val)),steps_per_epoch=(len(train_data)\/batchsize), \n              shuffle=True, sample_weight=sample_weight)\n    \n    val_acc.append(history.history['val_accuracy'])\n    train_acc.append(history.history['accuracy'])\n    fpr, tpr, _ = metrics.roc_curve(y_val, model.predict(val_data)[:,1])\n    true_pos[i,:len(tpr)]=tpr\n    false_pos[i,:len(fpr)]=fpr\n    \nfalse_pos = np.asarray(false_pos)    \ntrue_pos = np.asarray(true_pos)\ntrain_acc = np.asarray(train_acc)\nval_acc = np.asarray(val_acc)","b672f531":"import plotly.graph_objs as go\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.express as px","96baf886":"\nfig = go.Figure([\n    go.Scatter(\n        name='Accuracy Train',\n        x=np.arange(0, len(train_acc.mean(axis=0)+1)),\n        y=train_acc.mean(axis=0),\n        mode='lines',\n        line=dict(color='rgb(31, 119, 180)'),\n    ),\n    go.Scatter(\n        name='Upper Bound',\n        x=np.arange(0, len(train_acc.mean(axis=0)+1)),\n        y=train_acc.mean(axis=0)+train_acc.std(axis=0),\n        mode='lines',\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        showlegend=False\n    ),\n    go.Scatter(\n        name='Lower Bound',\n        x=np.arange(0, len(train_acc.mean(axis=0)+1)),\n        y=train_acc.mean(axis=0)-train_acc.std(axis=0),\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)',\n        fill='tonexty',\n        showlegend=False\n    )\n])\nfig.update_layout(\n    yaxis_title='Accuracy',\n    xaxis_title='Epocs',\n    title='Accuracy training data',\n    hovermode=\"x\"\n)\nfig.show()","8c8b9cba":"fig = go.Figure([\n    go.Scatter(\n        name='MSE_validation',\n        x=np.arange(0, len(val_acc.mean(axis=0)+1)),\n        y=val_acc.mean(axis=0),\n        mode='lines',\n        line=dict(color='rgb(31, 119, 180)'),\n    ),\n    go.Scatter(\n        name='Upper Bound',\n        x=np.arange(0, len(val_acc.mean(axis=0)+1)),\n        y=val_acc.mean(axis=0)+val_acc.std(axis=0),\n        mode='lines',\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        showlegend=False\n    ),\n    go.Scatter(\n        name='Lower Bound',\n        x=np.arange(0, len(val_acc.mean(axis=0)+1)),\n        y=val_acc.mean(axis=0)-val_acc.std(axis=0),\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)',\n        fill='tonexty',\n        showlegend=False\n    )\n])\nfig.update_layout(\n    yaxis_title='True positive rat',\n    xaxis_title='False positive rate',\n    title='Accuracy validation data',\n    hovermode=\"x\"\n)\nfig.show()","47f3761b":"\nfig = go.Figure([\n    go.Scatter(\n        name='AUC val',\n        x=false_pos.mean(axis=0),\n        y=true_pos.mean(axis=0),\n        mode='lines',\n        line=dict(color='rgb(31, 119, 180)'),\n    ),\n    go.Scatter(\n        name='Upper Bound',\n        x=false_pos.mean(axis=0),\n        y=true_pos.mean(axis=0)+true_pos.std(axis=0),\n        mode='lines',\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        showlegend=False\n    ),\n    go.Scatter(\n        name='Lower Bound',\n        x=false_pos.mean(axis=0),\n        y=true_pos.mean(axis=0)-true_pos.std(axis=0),\n        marker=dict(color=\"#444\"),\n        line=dict(width=0),\n        mode='lines',\n        fillcolor='rgba(68, 68, 68, 0.3)',\n        fill='tonexty',\n        showlegend=False\n    )\n])\nfig.update_layout(\n    yaxis_title='Accuracy',\n    xaxis_title='Epocs',\n    title='ROC AUC Validation data',\n    hovermode=\"x\",\n    height=800,\n    width = 1000\n    \n)\nfig.show()","319f4b77":"loss, acc = model.evaluate(x = val_data, y = tf.keras.utils.to_categorical(y_val), batch_size=1 )\nprint(\"Accuracy : {} , Loss: {}\".format(acc,loss))","15b2275c":"model.save('wpw_model.h5')","55141f1d":"!pip install lime","0c67b31c":"def average_and_rebin(array, bin_size):\n    new_arr =[]\n    temp_len = int(len(array)\/bin_size)\n    for i in range(temp_len):\n        bin = []\n        for j in range(bin_size):\n            bin_val = array[j + (i*bin_size)]\n            bin.append(bin_val)\n        new_arr.append(np.repeat(np.mean(bin),bin_size))\n    return np.asarray(new_arr).ravel()","a0d3ebfd":"import lime\nfrom lime import lime_tabular\nexplainer = lime_tabular.RecurrentTabularExplainer(val_data[:-10],training_labels=tf.keras.utils.to_categorical(y_val[:-10]), feature_names=[\"Lead-I\",\"Lead-II\", \"Lead-III\",\"aVR\",\"aVL\",\"aVF\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"],\n                                                   discretize_continuous=False, feature_selection='auto', class_names=['Normal','Ischemia'])","9c68f99c":"for i,j in enumerate(val_data[-10:]):\n    print(\"predicted labels ([Normal, Ischemia])\")\n    print(model.predict(np.expand_dims(j,axis=0))[0])\n    print(\"-----------------------\")\n    print(\"Ischemia = 1, Normal = 0\")\n    print(\"correct label is: {}\".format(y_val[-1*(i+1)]))\n    print(\"-----------------------\")\n    exp = explainer.explain_instance(np.expand_dims(j,axis=0),model.predict, num_features=250*12)\n    explanations = exp.as_list()\n    heatmap = np.zeros([12,250])\n\n    for k in explanations:\n        if k[0].split(\"_\")[0]=='Lead-I':\n            heatmap[0][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='Lead-II':\n            heatmap[1][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='Lead-III':\n            heatmap[2][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='aVR':\n            heatmap[3][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='aVL':\n            heatmap[4][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='aVF':\n            heatmap[5][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='V1':\n            heatmap[6][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='V2':\n            heatmap[7][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='V3':\n            heatmap[8][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='V4':\n            heatmap[9][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='V5':\n            heatmap[10][int(k[0].split(\"-\")[-1])]=k[1]\n        elif k[0].split(\"_\")[0]=='V6':\n            heatmap[11][int(k[0].split(\"-\")[-1])]=k[1]\n            \n    test_heatmap = heatmap.copy()\n    test_heatmap[np.where(heatmap > 0)] = 0.0\n    test_heatmap = abs(test_heatmap)\n    leads = [\"Lead-I\",\"Lead-II\", \"Lead-III\",\"aVR\",\"aVL\",\"aVF\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\"]\n    plt.figure(figsize=(26, 16))\n    for l in range(12):\n        plt.subplot(3, 4, l + 1)\n        plt.title(\"Explain Ischemia - Lead: {} \".format(leads[l]), fontsize=20)\n\n        plt.imshow(np.expand_dims(average_and_rebin(test_heatmap[l],7),axis=0),cmap='Reds', aspect=\"auto\", interpolation='nearest',extent=[0,250,round(j.reshape(12,250)[l].min()*1.05),round(j.reshape(12,250)[l].max()*1.05)],\n                   vmin=test_heatmap.min(), vmax=test_heatmap.max(), alpha=1.0)\n        plt.plot(j.reshape(12,250)[l],'k')\n\n    #plt.colorbar()\n    plt.savefig(\"wpw_{}.png\".format(i),dpi=300)\n    plt.suptitle(\"ECG_{}\".format(i))\n    plt.show()   ","777d9f0f":"# Resampling method: Bootstrap (x 10)","03fc5cdc":"# 10 seconds ECGs --> median beats","f63578b8":"# <center>Myocardial ischemia prediction using 1D Convolutional Neural Networks<\/center>\n## <center>Median Heartbeat Classification and Explanation using LIME<\/center>","d1ca3b8c":"# LIME Model agnostic explanation"}}