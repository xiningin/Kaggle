{"cell_type":{"f914023b":"code","47b7a512":"code","e279d3e5":"code","02357af6":"code","a9c40378":"code","d7de6cac":"code","ae650a10":"code","59a8cc51":"code","c49381fd":"code","b0749e40":"code","04f29ae3":"code","53b6d651":"code","d6b1100c":"code","cf5ef210":"code","16798703":"code","c7bf4695":"code","a61c6c96":"code","34bb678a":"code","05fafcd5":"code","94d66772":"code","1447b365":"code","6b27389f":"code","0934a0d5":"code","22b2adee":"code","30d8ddbd":"code","503115c7":"code","ebe23dd8":"code","f4611cf8":"code","a254d51f":"code","c28f8781":"code","fe4f2709":"code","91e163a5":"code","dc1eb4fc":"code","1e0382e5":"code","e71b415a":"code","ab0b8240":"code","6e3775ee":"code","dabd9be9":"markdown","b53d6590":"markdown","f38108ed":"markdown","4ecf0a23":"markdown","6aaba472":"markdown","498659bc":"markdown","5c46461b":"markdown","8d70be61":"markdown","91c82d61":"markdown","d9aa1414":"markdown","ac3f1e8b":"markdown","d39023b4":"markdown","77d12606":"markdown","8130d230":"markdown","1e83a08e":"markdown","e84e94dd":"markdown","ce08310b":"markdown","9420daaf":"markdown","391b8f9f":"markdown","de3cd51d":"markdown","639d222f":"markdown","3fb0ce2d":"markdown","36a51db8":"markdown","71bdd457":"markdown","f131c18f":"markdown","2d16306f":"markdown","ea9a8363":"markdown","3fbc395e":"markdown","7660416b":"markdown","b5aa9f26":"markdown","69297d83":"markdown","d0513cc0":"markdown","22f31fd4":"markdown","07bba50a":"markdown","9e293688":"markdown","4d32c724":"markdown","fa3ac97c":"markdown"},"source":{"f914023b":"import numpy as np\nimport pandas as pd \nimport os\nimport math\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer","47b7a512":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\").set_index('PassengerId')\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\").set_index('PassengerId')\n\ny = train['Survived']\ntrain = train.drop('Survived',axis=1)\n\ndisplay(train.head())\ndisplay(test.head())\n","e279d3e5":"X = pd.concat([train,test])\nX.head()","02357af6":"X.dtypes","a9c40378":"X.describe()","d7de6cac":"#Pclass is actually categorical\nX['Pclass'] = X['Pclass'].astype(object)\n\nnum_col = X.select_dtypes(include=['float64','int64']).columns\ncat_col = X.select_dtypes(include=['object']).columns","ae650a10":"sns.pairplot(X[num_col],corner=True)","59a8cc51":"plt.figure(figsize=(8,6))\ncorrelation = X[num_col].corr()\nsns.heatmap(correlation, mask = correlation <0.4, cmap='Blues')","c49381fd":"X[num_col].isnull().sum()","b0749e40":"sns.distplot(X.Age).set_title(\"Age Before Imputing\")","04f29ae3":"imputer = SimpleImputer(strategy='median')\nimputed = imputer.fit_transform(X[['Age']])\n\nsns.distplot(imputed).set_title(\"Age After Median Imputing\")","53b6d651":"def replace_with_random(a):\n    \"\"\"\n    a: Value or NaN to be replaced\n    \n    Cannot set a random state as it would generate the same value each time this function\n    is called. This is unlikely to be the derired behaviour\n    \"\"\"    \n    \n    from random import randint\n        \n    if pd.isnull(a):\n        return randint(20,55)\n    else:\n        return a","d6b1100c":"randimpute = X['Age'].apply(lambda a: replace_with_random(a))\n\nsns.distplot(randimpute).set_title(\"Age After Random Imputing\")","cf5ef210":"# For now I will use my random approach for Age\nX['Age'] = randimpute","16798703":"imputer = SimpleImputer(strategy='median')\nX['Fare'] = imputer.fit_transform(X[['Fare']])","c7bf4695":"X[num_col].isnull().sum()","a61c6c96":"X.describe()","34bb678a":"cat_col = X.select_dtypes(include=['object']).columns\nX[cat_col]","05fafcd5":"X[cat_col].isnull().sum()","94d66772":"# Do something about cabin feature, at least extract deck where possible\nplt.figure(figsize=(8,6))\nX['Deck'] = X['Cabin'].str[0]\nsns.countplot(x='Deck',data=X,palette=\"husl\")","1447b365":"temp_data = pd.merge(X['Deck'],y,on='PassengerId')\ntemp_data = temp_data.groupby('Deck').sum()\nsns.barplot(x=temp_data.index,y=temp_data['Survived'],palette='husl')","6b27389f":"X = X.drop(['Cabin','Deck'],axis=1)","0934a0d5":"cat_col = X.select_dtypes(include=['object']).columns\nX[cat_col].isnull().sum()","22b2adee":"X['Fare'] = X['Fare'][X['Fare']<400]","30d8ddbd":"X['FamilySize'] = X['SibSp'] + X['Parch']\nX = X.drop(['SibSp','Parch'],axis=1)\n\nnum_col = X.select_dtypes(include=['float64','int64']).columns","503115c7":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer',SimpleImputer(strategy='constant')),\n    ('scaler',StandardScaler())\n    ])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_col),\n        ('cat', categorical_transformer, cat_col)\n    ])","ebe23dd8":"# IMPORTANT: Now data is pre-processed, put it back into train and test sets and then split X and y.\ntest = X.loc[test.index]\nX = X.loc[train.index]\ny = y.loc[train.index]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.75,random_state=81)","f4611cf8":"# Import models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.ensemble import AdaBoostClassifier","a254d51f":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nparameters = {'model__n_estimators':[100,200,500],\n              'model__min_samples_split':[2],\n              'model__min_samples_leaf':[1]}\n\nscorer = make_scorer(accuracy_score,greater_is_better=True)\n\ngrid = GridSearchCV(pipeline,parameters,scoring=scorer)\n\ngrid.fit(X_train,y_train)\n\ny_pred = grid.predict(X_test)\n\naccuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",accuracy)\n\n#final_params = grid.best_params_\n\n\"\"\"","c28f8781":"#Train RF model model, I did a Grid Search CV on this, and it yielded the following setup of parameters:\nRandomForest = RandomForestClassifier(n_estimators=500,\n                                      min_samples_split=2,\n                                      min_samples_leaf=1,\n                                      random_state=81)\n\nRF_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', RandomForest)])\n\nRF_pipeline.fit(X_train, y_train)\n\ny_pred = RF_pipeline.predict(X_test)\n\nRF_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",RF_accuracy)","fe4f2709":"Perceptron = Perceptron()\n\nPerc_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('model',Perceptron)])\n\nPerc_pipeline.fit(X_train,y_train)\n\ny_pred = Perc_pipeline.predict(X_test)\n\nPerceptron_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",Perceptron_accuracy)","91e163a5":"LogRegCV = LogisticRegressionCV(cv=5)\n\nLR_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('model',LogRegCV)])\n\nLR_pipeline.fit(X_train,y_train)\n\ny_pred = LR_pipeline.predict(X_test)\n\nLogReg_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",LogReg_accuracy)","dc1eb4fc":"ADA = AdaBoostClassifier()\n\nADA_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('model',ADA)])\n\nADA_pipeline.fit(X_train,y_train)\n\ny_pred = ADA_pipeline.predict(X_test)\n\nADA_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",ADA_accuracy)\n","1e0382e5":"%%time\n\nfrom sklearn.ensemble import StackingClassifier\n\nestimators = [('RF',RF_pipeline),\n              ('Perceptron',Perc_pipeline),\n              ('ADA',ADA_pipeline),\n              ('LogReg',LR_pipeline)]\n\nstack = StackingClassifier(estimators=estimators)\nstack.fit(X_train,y_train)\ny_pred = stack.predict(X_test)\nstack_accuracy = accuracy_score(y_test,y_pred)\nprint(\"Accruacy:\",stack_accuracy)","e71b415a":"results = pd.DataFrame({'Model':['Random Forest','Perceptron','Logistic Regression','ADA Boost','Stacked Model'],\n                        'Accuracy':[RF_accuracy, Perceptron_accuracy,LogReg_accuracy,ADA_accuracy,stack_accuracy]}).set_index('Model')","ab0b8240":"results.sort_values('Accuracy',ascending=True)","6e3775ee":"#Choosing RF pipeline, seems best...\ntest_pred = RF_pipeline.predict(test)\n\nsubmission = pd.DataFrame(test_pred,index=test.index,columns=['Survived'])\n\nsubmission.to_csv(\".\/submission.csv\")","dabd9be9":"That's a lot of missing data for cabin, lets explore deck from it:","b53d6590":"From this, I would make the following points:\n* Most of the data is positively skewed\n* There are a couple of outliers in Fare\n* There are no glaringly obvious strong correlations here","f38108ed":"## View the data","4ecf0a23":"## Remove outliers\nBack to the numerical data. Let's clean up those outliers from before. Pipline imputation will take care of any missing values","6aaba472":"So the Random Forest model performs best. It is possible that running a GridSearchCV on ADA Boost and Perceptron may lead to better results and ultimately the Stack may improve by extension.","498659bc":"Here we prepare the pipeline. See sklearn for further imformation","5c46461b":"### Stacking for all models\nThis combines all the models, to see if the combined models can predict better. Logistic Regression is used to choose the overall result from amongst the underlying models.","8d70be61":"## Feature Engineering","91c82d61":"That should be it for missing data in numerical columns","d9aa1414":"### Perceptron","ac3f1e8b":"So on the limited data we have, clearly being in the upper decks improves survival. It would be nice to include this, but 1,014 values is just too many to impute. For now I will drop Deck\/Cabin\n\nAn idea would be to try to infer Deck from ticket, as there seems to be some information in there that might help.","d39023b4":"![UPVOTE](https:\/\/i.imgur.com\/RVyQY7r.png)","77d12606":"And this is the direibution of ages we will use.","8130d230":"### Logistic Regression","1e83a08e":"## Import basic packages","e84e94dd":"## View the Seaborn Pairplot for Numerical Data","ce08310b":"### Ada Boost","9420daaf":"This isn't great, as our model may end up thinking that being 28 is super important in determining chance of survival, which it probably wasn't. As such, I have created a random imputer in the range 20,55 to retain the distribution. This is a somewhat arbitrary choice and a more scientific method would be preferred but for our purposes here, this should be fine:","391b8f9f":"There are still two missing values for \"Embarked\", but we will let pipline handle the missing embarked value, see below for embedded iumputation using Sklearn's pipeline functions. There are only 2, so \"most common\" method should be OK.","de3cd51d":"Now let's see the median imputed results...","639d222f":"In order to deal with the above missing values, we explore a few approaches. The most troublesome at the moment is the missing values in age. Simply imputing the median has a huge effect on the distribution, see charts below:","3fb0ce2d":"### ... beautiful!","36a51db8":"In order to simplify our features, let's create a family variable that combines Parch and SibSp. It would be preferalble to have 1 dimension with only 60% zeros over 2 dimensions with at least 70% zeros. This is OK to do because, the data Parent\/Child and Sibling\/Spouse is of the same kind: they are counts of people.","71bdd457":"## Imputation for Numerical Columns\n","f131c18f":"Convert Pclass to categorical, as it is made up of class either 1,2 or 3.","2d16306f":"What does survival look like by Deck?","ea9a8363":"## Load the data","3fbc395e":"### Fare","7660416b":"There is only one value to impute in Fare, using a median seems sensible","b5aa9f26":"## Generate Submission","69297d83":"### Random Forest","d0513cc0":"### Age","22f31fd4":"As discussed above, there are no correlations between our features here, so there is no **multicolinearity**. What does this mean? In essense a model with features that share some form of relationship won't improve our model. It's probable that it won't negatively effect are model but it's far more likely to result in an overfitted and biased model. Leaving any variables in that have multicolinearity will make your fit appear to be good but when you come to submit the model, you will find poor results. ","07bba50a":"Split the data back into train and test datasets and split for modelling purposes.","9e293688":"## Imputation for categorical columns","4d32c724":"## Pipline","fa3ac97c":"\n\nThe approach for building a prediction model is as follows:\n\n1. Load the data\n2. Explore the data\n3. Modify the data\n    3.1 Impute\n    3.2 Remove outliers\n    3.3 Scale\n    3.4 Drop where too many missing values\n4. Create new features\n5. Build an Sklearn Pipeline\n6. Train and Test several models\n7. Generate submission file\n    "}}