{"cell_type":{"237a32d2":"code","35ff73cf":"code","a08f706a":"code","de7efb21":"code","8dad21c9":"code","e6f0d7fc":"code","4b5c0e32":"code","811f5e7a":"code","5b35321d":"code","d00d6e12":"code","225bdb85":"code","0f12f1e3":"code","ec12af7b":"markdown","b0a29501":"markdown","45829190":"markdown","11ad23d8":"markdown","ca189117":"markdown"},"source":{"237a32d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35ff73cf":"import numpy as np\nimport torch\nimport torch.optim as optim\nimport pandas as pd\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import MinMaxScaler  # \ub370\uc774\ud130 \uc815\uaddc\ud654\nimport torchvision.datasets as data\nimport torchvision.transforms as transforms\nimport random\nfrom torch.utils.data import  TensorDataset, DataLoader\nimport matplotlib.pyplot as plt","a08f706a":"device = torch.device('cuda') # \ub514\ubc14\uc774\uc2a4 GPU \uc124\uc815\ntorch.manual_seed(777)\nrandom.seed(777)\ntorch.cuda.manual_seed_all(777)\n\nlearning_rate = 0.1\ntraining_epochs = 10000\nbatch_size = 200\ndrop_prob = 0.3","de7efb21":"xy_train = pd.read_csv('train_seoul_grandpark.csv', header = None, skiprows=1, usecols=range(1, 8))\nx_data = xy_train.loc[: , 1:6]\ny_data = xy_train.loc[: , [7]]\nx_data = np.array(x_data)\ny_data = np.array(y_data)\n\nscaler = MinMaxScaler()\nx_data = scaler.fit_transform(x_data)\n\nx_train = torch.FloatTensor(x_data).to(device)\ny_train = torch.FloatTensor(y_data).to(device) ","8dad21c9":"train_dataset = TensorDataset(x_train, y_train)\ndata_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size, \n                                           shuffle = True, \n                                           drop_last = True)","e6f0d7fc":"linear1 = torch.nn.Linear(6, 4,bias=True)\nlinear2 = torch.nn.Linear(4, 4,bias=True)\nlinear3 = torch.nn.Linear(4, 4,bias=True)\nlinear4 = torch.nn.Linear(4, 4,bias=True)\nlinear5 = torch.nn.Linear(4, 1,bias=True)\nrelu = torch.nn.ReLU()\n\ntorch.nn.init.xavier_normal_(linear1.weight)\ntorch.nn.init.xavier_normal_(linear2.weight)\ntorch.nn.init.xavier_normal_(linear3.weight)\ntorch.nn.init.xavier_normal_(linear4.weight)\ntorch.nn.init.xavier_normal_(linear5.weight)\n\nmodel = torch.nn.Sequential(linear1,relu,\n                            linear2,relu,\n                            linear3,relu,\n                            linear4,relu,\n                            linear5).to(device)","4b5c0e32":"loss = torch.nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\nlosses = []\nmodel_history = []\nerr_history = []\n\ntotal_batch = len(data_loader)\n\nfor epoch in range(training_epochs + 1):\n  avg_cost = 0\n\n  for X, Y in data_loader:\n    X = X.to(device)\n    Y = Y.to(device)\n\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    cost = loss(hypothesis, Y)\n    cost.backward()\n    optimizer.step()\n\n    avg_cost += cost \/ total_batch\n    \n  model_history.append(model)\n  err_history.append(avg_cost)\n  \n  if epoch % 10 == 0:  \n    print('Epoch:', '%d' % (epoch + 1), 'Cost =', '{:.9f}'.format(avg_cost))\n  losses.append(cost.item())\nprint('Learning finished')","811f5e7a":"plt.plot(losses)\nplt.plot(err_history)\nplt.show()","5b35321d":"best_model = model_history[np.argmin(err_history)]","d00d6e12":"xy_test = pd.read_csv('test_seoul_grandpark.csv', header = None, skiprows=1, usecols = range(1, 7))\nx_data = xy_test.loc[:, 1:6]\nx_data = np.array(x_data)\nx_data = scaler.transform(x_data)\nx_test = torch.FloatTensor(x_data).to(device)\n\nwith torch.no_grad():\n    model.eval()  # \uc8fc\uc758\uc0ac\ud56d (dropout=False)\n    \n    predict = best_model(x_test)","225bdb85":"submit = pd.read_csv('submit_sample.csv')\nsubmit['Expected'] = submit['Expected'].astype(float)\nfor i in range(len(predict)):\n  submit['Expected'][i] = predict[i]\nsubmit.to_csv('submit.csv', mode = 'w', index = False, header = True)","0f12f1e3":"!kaggle competitions submit -c 2020-ai-termproject-18011817 -f submit.csv -m \"18011876 \uc774\ubcd1\ucc2c\"","ec12af7b":"# \ucc28\ubcc4\uc8104 - optimizer Adam \uc0ac\uc6a9","b0a29501":"# \ucc28\ubcc4\uc8101 - x_train \ubc0f x_test \uc815\uaddc\ud654","45829190":"# \ucc28\ubcc4\uc8103 - 5\uacc4\uce35 \ub525\ub7ec\ub2dd \ubaa8\ub378 \uc0ac\uc6a9","11ad23d8":"# \ucc28\ubcc4\uc8102 - \ubc30\uce58 \uc774\uc6a9 \ud559\uc2b5","ca189117":"18011876 \uc774\ubcd1\ucc2c\n# \uae30\uc874\uacfc\uc758 \ucc28\uc774\uc810\n1. \ub370\uc774\ud130 \ud30c\uc2f1 \ud6c4 x_train \uacfc x_test \uc815\uaddc\ud654\n2. \ubc30\uce58 \uc774\uc6a9 \ud559\uc2b5\n3. \ub525\ub7ec\ub2dd \ubaa8\ub378 \uc0ac\uc6a9\n4. optimizer Adam \uc0ac\uc6a9\n"}}