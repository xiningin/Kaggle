{"cell_type":{"ab9b354d":"code","33d2fa67":"code","6cff6be4":"code","35ff5f6f":"code","aaf37ca8":"code","ec52bd5a":"code","06147e72":"code","f047dc68":"code","5dd67792":"code","e043dcd6":"code","335728a5":"code","5aefb43a":"code","6a1e57a9":"code","8d37f935":"code","f358f99a":"code","198d945f":"code","d8bc5e06":"code","d38466a3":"code","a84ed15c":"code","ea1eebd8":"code","e8aad6bc":"code","377d2986":"code","08ec04d6":"code","7994be78":"code","c8ddef13":"code","f11aab38":"code","5860afe5":"code","c1581d37":"code","6cad2ba9":"code","a4d0b1d6":"code","bc0ef471":"code","7f045f1a":"markdown","29255c6b":"markdown","906ff863":"markdown","1c8d8c81":"markdown","e8fe4c1e":"markdown","6117c961":"markdown","a1369d34":"markdown","38f3748e":"markdown","715315de":"markdown","463985d4":"markdown","01ba19af":"markdown","448c4f58":"markdown","cd254cad":"markdown","8c75903d":"markdown","d8921f29":"markdown"},"source":{"ab9b354d":"import os\nimport re\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom datetime import date, datetime\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats\nfrom nltk.corpus import stopwords\nfrom collections import Counter","33d2fa67":"df = pd.read_csv('\/kaggle\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv')","6cff6be4":"df.head()","35ff5f6f":"df = df.drop(columns=['id', 'url', 'created'])\ndf.head()","aaf37ca8":"df.shape","ec52bd5a":"from datetime import date, datetime\nyear_col = []\nmonth_col = []\nhour_col = []\nminute_col = []\nfor i, content in df['timestamp'].items():\n    t1 = datetime.strptime(content, '%Y-%m-%d %H:%M:%S')\n    year_col.append(t1.year)\n    month_col.append(t1.month)\n    hour_col.append(t1.hour)\n    minute_col.append(t1.minute)\ndf['year'] = year_col\ndf['month'] = month_col\ndf['hour'] = hour_col\ndf['minute'] = minute_col","06147e72":"df.head()","f047dc68":"df['title'] = df['title'].str.lower()\ndf['body'] = df['body'].str.lower()","5dd67792":"df.drop_duplicates(subset=['title'], keep='first', inplace=True)\ndf.shape","e043dcd6":"count = df['title'].str.split().str.len()\ncount.index = count.index.astype(str) + ' words:'\ncount.sort_index(inplace=True)\n\nprint(\"Total number of words: \", count.sum(), \"words\")","335728a5":"print(\"Average number of words per post: \", round(count.mean(),2), \"words\")\nprint(\"Max number of words per post: \", count.max(), \"words\")\nprint(\"Min number of words per post: \", count.min(), \"words\")","5aefb43a":"def word_count(df):\n    \"\"\"\n    This function takes the dataframe and adds a new colun with the number of words.\n    :param df: The dataframe to be transformed.\n    :return: The transformed dataframe.\n    \"\"\"\n    words_count = []\n    for i, content in df['title'].items():\n        new_values =[]\n        new_values = content.split()\n        words_count.append(len(new_values))\n    df['title_word_count'] = words_count\n    return df\n\ndf = word_count(df)\n\ndf.head()","6a1e57a9":"df['title_length'] = df['title'].str.len()\n\nprint(\"Total length of a dataset: \", df.title_length.sum(), \"characters\")\nprint(\"Average length of a tweet: \", round(df.title_length.mean(),0), \"characters\")\nprint(df.head())","8d37f935":"plt.subplots(figsize=(10,8))\nsns.heatmap(df.drop(columns=['year']).corr(), annot=True, linewidths=1.5, fmt=\".2f\");","f358f99a":"def create_text_blob(df, text_column):\n    blob_text=[]\n    for i, content in df[text_column].items():\n        for i in content.split():\n            blob_text.append(i.lower())\n    return blob_text\n\nblob_text = create_text_blob(df, 'title')\nprint(blob_text[0:100])","198d945f":"nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))  \nfiltered_sentence = [w for w in blob_text if not w in stop_words]  \nfiltered_sentence = []  \n  \nfor w in blob_text:  \n    if w not in stop_words:  \n        filtered_sentence.append(w)  \n\nprint(filtered_sentence[0:100])  ","d8bc5e06":"counts = Counter(filtered_sentence)","d38466a3":"import plotly.express as px\n\ntop_20_words = {}\n\nfor (key, value) in counts.items():\n   # Check if value is greater than 200 and add to new dictionary\n    if value > 700 :\n        top_20_words[key] = value\n    continue\n\nsorted_top_20_words = dict(sorted(top_20_words.items(), key=lambda item: item[1], reverse=False))\n\nword = sorted_top_20_words.keys()\ncount = sorted_top_20_words.values()\n\n\nfig = px.bar(y=word, x=count, text = count)\nfig.update_traces(texttemplate='%{text:}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","a84ed15c":"top_20_words_clean = {}\n\nfor (key, value) in counts.items():\n    # Check if key length is greater than 3 and value greater than 150 and add to new dictionary\n    if len(key)>2 and value > 700 :\n        top_20_words_clean[key] = value\n    continue\n\nsorted_top_20_words_clean = dict(sorted(top_20_words_clean.items(), key=lambda item: item[1], reverse=False))\n\nword = sorted_top_20_words_clean.keys()\ncount = sorted_top_20_words_clean.values()\n\nfig = px.bar(y=word, x=count, text = count)\nfig.update_traces(texttemplate='%{text:}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","ea1eebd8":"count = df['body'].str.split().str.len()\ncount.index = count.index.astype(str) + ' words:'\ncount.sort_index(inplace=True)\n\nprint(\"Total number of words: \", count.sum(), \"words\")","e8aad6bc":"print(\"Average number of words per post: \", round(count.mean(),2), \"words\")\nprint(\"Max number of words per post: \", count.max(), \"words\")\nprint(\"Min number of words per post: \", count.min(), \"words\")","377d2986":"df['body_length'] = df['body'].str.len()\n\nprint(\"Total length of a dataset: \", df.body_length.sum(), \"characters\")\nprint(\"Average length of a tweet: \", round(df.body_length.mean(),0), \"characters\")\nprint(df.head())","08ec04d6":"def create_text_blob(df, text_column):\n    blob_text=[]\n    for i, content in df[text_column].items():\n        for i in str(content).split():\n            blob_text.append(i.lower())\n    return blob_text\n\nblob_text = create_text_blob(df, 'body')\nprint(blob_text[0:100])","7994be78":"nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))  \nstop_words.add('nan')\nfiltered_sentence = [w for w in blob_text if not w in stop_words]  \nfiltered_sentence = []  \n  \nfor w in blob_text:  \n    if w not in stop_words:  \n        filtered_sentence.append(w)  \n\nprint(filtered_sentence[0:100])  ","c8ddef13":"counts_body = Counter(filtered_sentence)","f11aab38":"import plotly.express as px\n\ntop_20_words = {}\n\nfor (key, value) in counts_body.items():\n   # Check if value is greater than 3000 and add to new dictionary\n    if value != \"nan\" and value > 3000:\n        top_20_words[key] = value\n    continue\n\nsorted_top_20_words = dict(sorted(top_20_words.items(), key=lambda item: item[1], reverse=False))\n\nword = sorted_top_20_words.keys()\ncount = sorted_top_20_words.values()\n\n\nfig = px.bar(y=word, x=count, text = count)\nfig.update_traces(texttemplate='%{text:}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","5860afe5":"nyse_tickers = pd.read_csv(\"..\/input\/tickers\/nyse-listed_csv.csv\")\nother_tickers = pd.read_csv(\"..\/input\/tickers\/other-listed_csv.csv\")","c1581d37":"nyse_tickers.head()","6cad2ba9":"nyse_tickers_list = list(nyse_tickers['ACT Symbol'].str.lower())\nother_tickers_list = list(other_tickers['ACT Symbol'].str.lower())\nnyse_tickers_list[0:10]","a4d0b1d6":"import plotly.express as px\n\ntop_words = {}\n\nfor (key, value) in counts_body.items():\n   # Check if value is greater than 100 and add to new dictionary\n    if key in nyse_tickers_list and value > 300: \n        top_words[key] = value\n    continue\n    \nsorted_top_words = dict(sorted(top_words.items(), key=lambda item: item[1], reverse=False))\n\nword = sorted_top_words.keys()\ncount = sorted_top_words.values()\n\nfig = px.bar(y=word, x=count, text = count, title='Nyse Tickers')\nfig.update_traces(texttemplate='%{text:}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()\n\n","bc0ef471":"import plotly.express as px\n\ntop_words = {}\n\nfor (key, value) in counts_body.items():\n   # Check if value is greater than 100 and add to new dictionary\n    if key in other_tickers_list and value > 500: \n        top_words[key] = value\n    continue\n    \nsorted_top_words = dict(sorted(top_words.items(), key=lambda item: item[1], reverse=False))\n\nword = sorted_top_words.keys()\ncount = sorted_top_words.values()\n\nfig = px.bar(y=word, x=count, text = count, title='Other Tickers')\nfig.update_traces(texttemplate='%{text:}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\nfig.show()","7f045f1a":"# Let's print the most popular words, used over 700 times","29255c6b":"# Normalize the text to be lowercase","906ff863":"# Descriptive statistics - body","1c8d8c81":"# Most popular words used in title","e8fe4c1e":"# Let's remove the stop words","6117c961":"**Short summary:**\nThe average title length is 11 words.\nThe average title length is 120 words.\nThe most popular words are, without a surprise: gme, buy, robinhood, hold, amc.\nThe most popular tickers are: gme, know, one, hold, see, time, big, amc","a1369d34":"# Check the head of the dataframe","38f3748e":"# Add a few data realted columns for further analysis","715315de":"# Import the libraries","463985d4":"**Count the number of characters and length of a title**","01ba19af":"# **Let's see what are the most popular tickers mentioned in the body text**","448c4f58":"# Read the dataframe","cd254cad":"# Drop useless columns","8c75903d":"# Descriptive statistics - title","d8921f29":"# **This is a initial descriptive analysis of the Reddit wallstreetbets posts. It contains a basic statistics of words, character count, and occurence. At the bottom, you will find the analysis of the most common mentioned NYSE or other stock tickers. Enjoy!**"}}