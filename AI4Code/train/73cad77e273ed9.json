{"cell_type":{"e82a87c7":"code","24a9b4cb":"code","dcce6f60":"code","4d320c47":"code","213ddfcb":"code","5a98122f":"code","7375eebb":"code","27cf27b3":"code","fb31a69b":"code","7ccbed55":"code","06ab9920":"code","68a5715b":"code","a620f205":"code","2b1d859c":"code","cf776275":"code","ce29e055":"code","9ad317e8":"code","5819a268":"code","abe62231":"markdown","7030934f":"markdown","0aa70b5e":"markdown","6fc04be9":"markdown","3a63b85d":"markdown","f5974fda":"markdown","07350f02":"markdown","2f44de2f":"markdown","b6f86f67":"markdown","e23aba94":"markdown","bc8f4ca9":"markdown"},"source":{"e82a87c7":"!pip install efficientnet_pytorch torchtoolbox","24a9b4cb":"import pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport cv2\nimport os\nimport numpy as np\nfrom PIL import Image, ImageChops\nimport matplotlib.pylab as plt\nimport PIL\nimport torch\nimport albumentations\nfrom PIL import Image\nimport random\nfrom tqdm import tqdm","dcce6f60":"def img_binary(img0):\n    img0[:, :, 0] = cv2.adaptiveThreshold(img0[:, :, 0], 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n    img0[:, :, 1] = cv2.adaptiveThreshold(img0[:, :, 1], 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n    img0[:, :, 2] = cv2.adaptiveThreshold(img0[:, :, 2], 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)\n    return img0","4d320c47":"def labelling(x):\n    try:\n        return int(x[0]), int(x[1]), int(x[2]), int(x[3]), str(x[4]), str(x[5]), str(x[6])\n    except:\n        return int(x[0]), int(x[1]), int(x[2]), int(x[3]), str(x[4]), str(x[5]), str('\u0427')","213ddfcb":"def get_contour_precedence(contour, cols):\n    tolerance_factor = 10\n    origin = cv2.boundingRect(contour)\n    return ((origin[1] \/\/ tolerance_factor) * tolerance_factor) * cols + origin[0]","5a98122f":"def square(img):\n\n    # image after making height equal to width\n    squared_image = img\n\n    # Get image height and width\n    h = img.shape[0]\n    w = img.shape[1]\n\n    # In case height superior than width\n    if h > w:\n        diff = h-w\n        if diff % 2 == 0:\n            x1 = np.zeros(shape=(h, diff\/\/2))\n            x2 = x1\n        else:\n            x1 = np.zeros(shape=(h, diff\/\/2))\n            x2 = np.zeros(shape=(h, (diff\/\/2)+1))\n\n        squared_image = np.concatenate((x1, img, x2), axis=1)\n\n    # In case height inferior than width\n    if h < w:\n        diff = w-h\n        if diff % 2 == 0:\n            x1 = np.zeros(shape=(diff\/\/2, w))\n            x2 = x1\n        else:\n            x1 = np.zeros(shape=(diff\/\/2, w))\n            x2 = np.zeros(shape=((diff\/\/2)+1, w))\n\n        squared_image = np.concatenate((x1, img, x2), axis=0)\n\n    return squared_image","7375eebb":"def sort(vector):\n    sort = True\n    while (sort == True):\n\n        sort = False\n        for i in range(len(vector) - 1):\n            x_1 = vector[i][0]\n            y_1 = vector[i][1]\n\n            for j in range(i + 1, len(vector)):\n\n                x_2 = vector[j][0]\n                y_2 = vector[j][1]\n\n                if (x_1 >= x_2 and y_2 >= y_1):\n                    tmp = vector[i]\n                    vector[i] = vector[j]\n                    vector[j] = tmp\n                    sort = True\n\n                elif (x_1 < x_2 and y_2 > y_1):\n                    tmp = vector[i]\n                    vector[i] = vector[j]\n                    vector[j] = tmp\n                    sort = True\n    return vector","27cf27b3":"\ndef plate_segmentation(img_file_path):\n    img = cv2.imread(img_file_path)\n    imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n    height = img.shape[0]\n    width = img.shape[1]\n    area = height * width\n\n    # scale1 = 0.001\n    scale1 = 0.04\n    scale2 = 0.9\/7\n    area_condition1 = area * scale1\n    area_condition2 = area * scale2\n\n    # Otsu's thresholding\n    ret2, th2 = cv2.threshold(imgray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n    # Otsu's thresholding after Gaussian filtering\n    blur = cv2.GaussianBlur(imgray, (5, 5), 0)\n    ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n\n    contours, hierarchy = cv2.findContours(th3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # sort contours\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n\n    cropped = dict()\n    cnts = []\n    for cnt in contours:\n        (x, y, w, h) = cv2.boundingRect(cnt)\n        distance_center = (2*x + w) \/ 2\n\n        if distance_center in cropped:\n            pass\n        else:\n            if (w * h > area_condition1 and w * h < area_condition2 and w<90 and h>30 and h<100 and w>30):\n                cnts.append(cnt)\n                cv2.drawContours(img, [cnt], 0, (0, 255, 0), 1)\n                cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 1)\n                c = th2[y:y + h, x:x + w]\n                c = np.array(c)\n                c = cv2.bitwise_not(c)\n                c = square(c)\n                c = cv2.resize(c, (28, 28), interpolation=cv2.INTER_AREA)\n                cropped[distance_center] = c\n                #cropped.append(cnt)\n\n    sorted_cropped = []\n    for x_center in sorted(cropped):\n        sorted_cropped.append(cropped[x_center])\n\n    return img, sorted_cropped, cnts\n","fb31a69b":"######### Running #############\n## \u0411\u0438\u0434 \u0441\u0443\u0440\u0433\u0430\u043b\u0442\u044b\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u04e9\u04e9\u0441 \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440\u044b\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u04af\u04af\u0434\u0438\u0439\u0433 \u0434\u0435\u0442\u0435\u043a\u0442 \u0445\u0438\u0439\u0441\u044d\u043d.\n## \u0414\u0435\u0442\u0435\u043a\u0442 \u0445\u0438\u0439\u0441\u044d\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u043d \u043d\u044d\u043c\u044d\u043b\u0442 \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440 \u04af\u04af\u0441\u0433\u044d\u0445\u044d\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d.\n## \u0425\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440\u044b\u0433 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d.\n\nseed = 54\nnp.random.seed(54)\nROOT = '..\/input\/mlub-mongolian-car-plate-prediction\/'\n\ntrain = pd.read_csv(ROOT + 'training.csv')\n\ntrain['label_1'],train['label_2'], train['label_3'], \\\ntrain['label_4'], train['label_5'], train['label_6'], \\\ntrain['label_7'] = zip(*train['plate_number'].map(labelling))\n\ntrain_label = {'file_name':[], 'x':[], 'y':[], 'w':[], 'h':[]}\n\nfor img_id in tqdm(train['file_name'].iloc[:10]):\n    image = cv2.imread(ROOT + 'training\/training\/' + img_id)\n    img, digits, cnts = plate_segmentation(ROOT + 'training\/training\/' + img_id)\n\n    if len(cnts)==7:\n        size = np.zeros((7, 4))\n        for n, cnt in enumerate(cnts):\n            (x, y, w, h) = cv2.boundingRect(cnt)\n            size[n, :] = [x, y, w, h]\n\n        if np.max(size[:, 1])<=75:\n            for n, cnt in enumerate(cnts):\n                (x, y, w, h) = cv2.boundingRect(cnt)\n\n                train_label['file_name'].append(img_id)\n                train_label['x'].append(x)\n                train_label['y'].append(y)\n                train_label['w'].append(w)\n                train_label['h'].append(h)\n\n                img = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n    \n        plt.imshow(img)\n        plt.show()\n\ntrain_label = pd.DataFrame.from_dict(train_label)\nprint(train_label.head())","7ccbed55":"mn = ['\u0410','\u0411', '\u0427', '\u0414', '\u042d', '\u0415','\u0413',\n       '\u0425','\u0418','\u041a','\u041b','\u041c', '\u041d', '\u041e', '\u04e8',\n       '\u041f','\u0420','\u0421','\u0422', '\u0426',\n       '\u0423','\u04ae','\u0412','\u042f', '\u0417']\n\nmn2eng = ['A','B', 'Ch', 'D', 'E', 'Ee','G',\n           'H','I','K','L','M', 'N', 'O', 'Oo',\n           'P','R','S','T', 'Ts',\n           'U','Uu','V','Ya', 'Z']\n\ntrain_eng = train.copy()\nfor letter in range(len(mn)):\n    train_eng[['label_5', 'label_6', 'label_7']] = np.where(train_eng[['label_5', 'label_6', 'label_7']].values==mn[letter],\n                                                        mn2eng[letter], train_eng[['label_5', 'label_6', 'label_7']].values)\n    \ntrain_label = train_label.sort_values(by = ['file_name', 'x']).reset_index(drop=True)\n\nfor img_id in tqdm(train_label['file_name'].unique()):\n    sub_train = train_label.loc[train_label['file_name']==img_id]\n    img = cv2.imread(ROOT + 'training\/training\/' + img_id)\n\n    if sub_train['y'].max()>40:\n        for i, (x, y, w, h) in enumerate(zip(sub_train['x'], sub_train['y'],\n                                             sub_train['w'], sub_train['h'])):\n            if i == 0:\n                plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_1'].values[0])\n                plt.show()\n    \n            elif i == 1:\n                plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_5'].values[0])\n                plt.show()\n\n            elif i == 2:\n                plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_2'].values[0])\n                plt.show()\n\n            elif i==3:\n                plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_6'].values[0])\n                plt.show()\n\n            elif i == 4:\n                plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_3'].values[0])\n                plt.show()\n                \n            elif i == 5:\n                plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_7'].values[0])\n                plt.show()\n\n            elif i == 6:\n                plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_4'].values[0])\n                plt.show()\n    else:\n        for i, (x, y, w, h) in enumerate(zip(sub_train['x'], sub_train['y'],\n                                             sub_train['w'], sub_train['h'])):\n\n            plt.imshow(img[y:y + h, x:x + w, :]), plt.title(train.loc[train['file_name'] == img_id, 'label_%s' %(i+1)].values[0])\n            plt.show()","06ab9920":"def random_bright(img):\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    img = np.array(img, dtype=np.float64)\n    random_bright = .5 + np.random.uniform()\n    img[:, :, 2] = img[:, :, 2] * random_bright\n    img[:, :, 2][img[:, :, 2] > 255] = 255\n    img = np.array(img, dtype=np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    return img\n\ndef border(img, border_size=10):\n\n    img = cv2.copyMakeBorder(\n        img,\n        top=border_size,\n        bottom=border_size,\n        left=border_size,\n        right=border_size,\n        borderType=cv2.BORDER_CONSTANT,\n        value=[15, 15, 15]\n    )\n\n    return img\n\ndef get_symbol(symbol, SYMBOL_ROOT, w, h, type=1):\n    type_files = os.listdir(SYMBOL_ROOT + str(symbol) + '\/type%s' %type)\n    img_id = np.random.randint(len(type_files), size=1)\n    img = cv2.imread(SYMBOL_ROOT + str(symbol) + '\/type%s' %type + \"\/\" + type_files[img_id[0]])\n    if type==1:\n        img = cv2.resize(img, (w, h), interpolation=cv2.INTER_AREA)\n    else:\n        img = cv2.resize(img, (h, w), interpolation=cv2.INTER_AREA)\n\n    return img\n\n\ndef image_augmentation(img, ang_range=4, shear_range=2, trans_range=2):\n    # Rotation\n    ang_rot = np.random.uniform(ang_range) - ang_range \/ 2\n    rows, cols, ch = img.shape\n    Rot_M = cv2.getRotationMatrix2D((cols \/ 2, rows \/ 2), ang_rot, 0.9)\n\n    # Translation\n    tr_x = trans_range * np.random.uniform() - trans_range \/ 2\n    tr_y = trans_range * np.random.uniform() - trans_range \/ 2\n    Trans_M = np.float32([[1, 0, tr_x], [0, 1, tr_y]])\n\n    # Shear\n    pts1 = np.float32([[5, 5], [20, 5], [5, 20]])\n\n    pt1 = 5 + shear_range * np.random.uniform() - shear_range \/ 2\n    pt2 = 20 + shear_range * np.random.uniform() - shear_range \/ 2\n    pts2 = np.float32([[pt1, 5], [pt2, pt1], [5, pt2]])\n    shear_M = cv2.getAffineTransform(pts1, pts2)\n\n    img = cv2.warpAffine(img, Rot_M, (cols, rows))\n    img = cv2.warpAffine(img, Trans_M, (cols, rows))\n    img = cv2.warpAffine(img, shear_M, (cols, rows))\n\n    # Brightness\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    img = np.array(img, dtype=np.float64)\n    random_bright = .4 + np.random.uniform()\n    img[:, :, 2] = img[:, :, 2] * random_bright\n    img[:, :, 2][img[:, :, 2] > 255] = 255\n    img = np.array(img, dtype=np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n\n    # Blur\n    blur_value = random.randint(0,5) * 2 + 1\n    img = cv2.blur(img,(blur_value, blur_value))\n\n    return img\n\n\ndef plate_generation(SYMBOL_ROOT, PLATE_ROOT, count=5000, augment=False, example=False):\n\n    aug_train = {'file_name': [], 'label_1': [],\n                 'label_2': [], 'label_3': [], 'label_4': [],\n                 'label_5': [], 'label_6': [], 'label_7': []}\n\n    for c in tqdm(range(count)):\n        w = np.random.randint(30, 50, 1)[0]\n        h = np.random.randint(55, 80, 1)[0]\n\n        margin_left_type1 = int((430-7*w)\/2)\n        margin_top_type1 = int((100-h)\/1.5)\n\n        margin_left_type2 = int((470-4*h)\/2)\n        margin_top_type2 = int((100-w*2)\/2)\n\n        digits = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n        type1 = (np.random.randint(230, 255, (110, 470, 3)).astype(np.float32))\n        type2 = (np.random.randint(230, 255, (110, 470, 3)).astype(np.float32))\n\n        bordersize = 10\n        type1 = border(type1, border_size=bordersize)\n        type2 = border(type2, border_size=bordersize)\n\n        ids = np.random.randint(len(digits), size=4)\n\n        for i, num in enumerate(ids):\n            aug_train['label_%s' %(i+1)].append(num)\n\n            img0 = img_binary(get_symbol(num, SYMBOL_ROOT, w, h, type=1))\n            img1 = img_binary(get_symbol(num, SYMBOL_ROOT, w, h, type=2))\n\n            type1[margin_top_type1:h+margin_top_type1, margin_left_type1+i*w:margin_left_type1+(i+1)*w, :] = img0\n            type2[margin_top_type2:margin_top_type2+w, margin_left_type2 + i * h:margin_left_type2+ (i + 1) * h, :] = img1\n\n        label_5 = ['A','B','G', 'Z', 'Oo','H','Ts', 'T']\n        label_5_mn = ['\u0410', '\u0411', '\u0413', '\u0417', '\u04e8', '\u0425', '\u0426', '\u0422']\n\n        ids = np.random.choice(len(label_5), 1)\n        aug_train['label_5'].append(str(label_5_mn[ids[0]]))\n\n        img0 = img_binary(get_symbol(label_5[ids[0]], SYMBOL_ROOT, w, h, type=1))\n        img1 = img_binary(get_symbol(label_5[ids[0]], SYMBOL_ROOT, w, h, type=2))\n\n        type1[margin_top_type1:h+margin_top_type1, margin_left_type1+30 + 5 * w:30+margin_left_type1 + 6 * w, :] = img0\n        type2[margin_top_type2+w+10:margin_top_type2+2*w+10, margin_left_type2 + int(h*0.5) + 0 * h:margin_left_type2 + int(h*0.5) + (0 + 1) * h, :] = img1\n\n        label_6 = ['G', 'M','O', 'S', 'U', 'Ts', 'V', 'Ch', 'Uu', 'Oo', 'R']\n        label_6_mn = ['\u0413', '\u041c','\u041e', '\u0421', '\u0423', '\u0426', '\u0412', '\u0427', '\u04ae', '\u04e8', '\u0420']\n\n        ids = np.random.choice(len(label_6), 1)\n\n        aug_train['label_6'].append(str(label_6_mn[ids[0]]))\n\n        img0 = img_binary(get_symbol(label_6[ids[0]], SYMBOL_ROOT, w, h, type=1))\n        img1 = img_binary(get_symbol(label_6[ids[0]], SYMBOL_ROOT, w, h, type=2))\n\n        type1[margin_top_type1:h+margin_top_type1, margin_left_type1 +30 + 6 * w:30+ margin_left_type1 + 7 * w, :] = img0\n        type2[margin_top_type2 + w + 10:margin_top_type2 + 2 * w + 10,margin_left_type2 + int(h * 0.5) + 1 * h:margin_left_type2 + int(h * 0.5) + (1 + 1) * h, :] = img1\n\n        label_7 = ['A','B','G','D','Ee','Z',\n                   'I','K','L','M','N',\n                   'Oo','P','S','T','U',\n                   'Ts','Ch','E','Ya', 'H']\n\n        letters = ['\u0410','\u0411', '\u0413', '\u0414', '\u0415', '\u0417',\n                   '\u0418', '\u041a', '\u041b', '\u041c', '\u041d',\n                   '\u04e8', '\u041f', '\u0421', '\u0422', '\u0423',\n                   '\u0426', '\u0427', '\u042d', '\u042f', '\u0425']\n\n        ids = np.random.choice(len(label_7), 1)\n        aug_train['label_7'].append(str(letters[ids[0]]))\n\n        img0 = img_binary(get_symbol(label_7[ids[0]], SYMBOL_ROOT, w, h, type=1))\n        img1 = img_binary(get_symbol(label_7[ids[0]], SYMBOL_ROOT, w, h, type=2))\n\n        type1[margin_top_type1:h+margin_top_type1, margin_left_type1 +30 + 7 * w:30+ margin_left_type1 + 8 * w, :] = img0\n        type2[margin_top_type2 + w + 10:margin_top_type2 + 2 * w + 10, margin_left_type2 + int(h * 0.5) + 2 * h:margin_left_type2 + int(h * 0.5) + (2 + 1) * h, :] = img1\n\n        add = np.random.randint(3, size=1)\n        if add[0]>=1:\n            img0 = img_binary(get_symbol('soyombo', SYMBOL_ROOT, w, h, type=1))\n            img1 = img_binary(get_symbol('soyombo', SYMBOL_ROOT, w, h, type=2))\n            img0 = cv2.resize(img0, (int(0.6*margin_left_type1), h), interpolation=cv2.INTER_AREA)\n            img1 = cv2.resize(img1, (int(0.6*(margin_left_type2 + int(h * 0.5))), w), interpolation=cv2.INTER_AREA)\n\n            type1[margin_top_type1:h + margin_top_type1, int(margin_left_type1*0.2):int(margin_left_type1*0.2)+int(margin_left_type1*0.6),:] = img0\n            type2[margin_top_type2 + w + 10:margin_top_type2 + 2 * w + 10, int(0.2*(margin_left_type2 + int(h * 0.5))):\n                                                                           int(0.2*(margin_left_type2 + int(h * 0.5)))+int(0.6*(margin_left_type2 + int(h * 0.5))), :] = img1\n\n\n        type2 = cv2.resize(type2.astype(np.float32), (470, 110), interpolation=cv2.INTER_AREA)\n        type1 = cv2.resize(type1.astype(np.float32), (470, 110), interpolation=cv2.INTER_AREA)\n\n        # type1 = random_bright(type1)\n        # type2 = random_bright(type2)\n        if augment==True:\n            type1 = image_augmentation(type1)\n            type2 = image_augmentation(type2)\n\n        if example==True:\n            plt.subplot(121), plt.imshow(type1), plt.title('Input')\n            plt.subplot(122), plt.imshow(type2), plt.title('Output')\n            plt.show()\n        \n        else:\n            sel = np.random.randint(3, size=1)\n            if sel[0]==0:\n                cv2.imwrite(PLATE_ROOT + '%s.png' %(c+3698), type2)\n            else:\n                cv2.imwrite(PLATE_ROOT + '%s.png' %(c+3698), type1)\n\n        aug_train['file_name'].append('%s.png' %(c+3698))\n\n    aug_train = pd.DataFrame.from_dict(aug_train)\n    aug_train['plate_number'] = aug_train.agg('{0[label_1]}{0[label_2]}{0[label_3]}'\n                                              '{0[label_4]}{0[label_5]}{0[label_6]}'\n                                              '{0[label_7]}'.format, axis=1)\n\n    aug_train = aug_train[['file_name', 'plate_number', 'label_1',\n                'label_2', 'label_3', 'label_4',\n                'label_5', 'label_6', 'label_7']]\n\n    return aug_train\n","68a5715b":"## \u0425\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440\u044b\u0433 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d.\nseed = 54\nnp.random.seed(54)\n\nSYMBOL_ROOT = '..\/input\/digits\/digits\/'\nPLATE_ROOT = 'generated_plate\/'\ncount = 10\n\n## \u0411\u0438\u0434 \u0445\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d. \u042d\u0445\u043d\u0438\u0439 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u044f\u043c\u0430\u0440 \u043d\u044d\u0433\u044d\u043d image augmentation \u0431\u0430\u0439\u0445\u0433\u04af\u0439.\naug_train = plate_generation(SYMBOL_ROOT=SYMBOL_ROOT, PLATE_ROOT=PLATE_ROOT, count=count, augment=False, example=True)\n\n\nPLATE_ROOT = 'generated_plate_aug\/'\nif not os.path.exists(PLATE_ROOT):\n    os.makedirs(PLATE_ROOT)\n\n## \u0411\u0438\u0434 \u0445\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d. \u0425\u043e\u0451\u0440\u0434\u0445\u044c \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u044f\u043c\u0430\u0440 \u043d\u044d\u0433\u044d\u043d image augmentation \u0445\u0438\u0439\u0441\u044d\u043d.\naug_train = plate_generation(SYMBOL_ROOT=SYMBOL_ROOT, PLATE_ROOT=PLATE_ROOT, count=count, augment=True, example=True)","a620f205":"## \u0425\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440\u044b\u0433 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d.\nseed = 54\nnp.random.seed(54)\n\nSYMBOL_ROOT = '..\/input\/digits\/digits\/'\nPLATE_ROOT = 'generated_plate\/'\nif not os.path.exists(PLATE_ROOT):\n    os.makedirs(PLATE_ROOT)\ncount = 2000\n\n## \u0411\u0438\u0434 \u0445\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d. \u042d\u0445\u043d\u0438\u0439 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u044f\u043c\u0430\u0440 \u043d\u044d\u0433\u044d\u043d image augmentation \u0431\u0430\u0439\u0445\u0433\u04af\u0439.\naug_train = plate_generation(SYMBOL_ROOT=SYMBOL_ROOT, PLATE_ROOT=PLATE_ROOT, count=count, augment=False, example=False)\naug_train.to_csv('aug_train.csv', index=False)\n\n\nPLATE_ROOT = 'generated_plate_aug\/'\nif not os.path.exists(PLATE_ROOT):\n    os.makedirs(PLATE_ROOT)\n\n## \u0411\u0438\u0434 \u0445\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d. \u0425\u043e\u0451\u0440\u0434\u0445\u044c \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u044f\u043c\u0430\u0440 \u043d\u044d\u0433\u044d\u043d image augmentation \u0445\u0438\u0439\u0441\u044d\u043d.\naug_train = plate_generation(SYMBOL_ROOT=SYMBOL_ROOT, PLATE_ROOT=PLATE_ROOT, count=count, augment=True, example=False)\naug_train.to_csv('aug_train_aug.csv', index=False)","2b1d859c":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom efficientnet_pytorch import EfficientNet\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass block(nn.Module):\n    def __init__(self, input_dim=470, out_dim=10):\n        super(block, self).__init__()\n\n        self.output = nn.Linear(input_dim, out_dim)\n\n    def forward(self, x):\n        y = self.output(x)#[:, -1, :])\n        return y\n\n\nclass block_v0(nn.Module):\n    def __init__(self, input_dim=470, out_dim=10):\n        super(block_v0, self).__init__()\n        self.hidden = nn.Linear(input_dim, 256)\n        self.output = nn.Linear(256, out_dim)\n\n    def forward(self, x):\n        x = self.hidden(x)\n        y = self.output(x)#[:, -1, :])\n\n        return x,  y\n\nclass block_v1(nn.Module):\n    def __init__(self, input_dim=470, out_dim=10):\n        super(block_v1, self).__init__()\n        self.hidden = nn.Linear(input_dim, 256)\n        self.output = nn.Linear(512, out_dim)\n\n    def forward(self, x0, x1):\n        x0 = self.hidden(x0)\n        x = torch.cat([x0, x1], dim=1)\n        y = self.output(x)\n\n        return x0,  y\n\n\nclass Effnet_MLUB(nn.Module):\n    def __init__(self, enet_type, num_classes, pretrained=True):\n        super(Effnet_MLUB, self).__init__()\n        self.enet = EfficientNet.from_pretrained('efficientnet-b2')\n\n        in_ch = 1000\n        self.fc1 = block(in_ch, num_classes['label_1'])\n        self.fc2 = block(in_ch, num_classes['label_2'])\n        self.fc3 = block(in_ch, num_classes['label_3'])\n        self.fc4 = block(in_ch, num_classes['label_4'])\n\n        self.fc5 = block(in_ch, num_classes['label_5'])\n        self.fc6 = block(in_ch, num_classes['label_6'])\n        self.fc7 = block(in_ch, num_classes['label_7'])\n        \n    def extract(self, x):\n        x = self.enet(x)\n        return x\n\n    def forward(self, x):\n\n        out = self.extract(x).squeeze(-1).squeeze(-1)\n\n        out1 = self.fc1(out)\n        out2 = self.fc2(out)\n        out3 = self.fc3(out)\n        out4 = self.fc4(out)\n        out5 = self.fc5(out)\n        out6 = self.fc6(out)\n        out7 = self.fc7(out)\n\n        return out1, out2, out3, \\\n               out4, out5, out6, out7","cf776275":"class CatPlateDataset(Dataset):\n    def __init__(self, df, imfolder, train, label='label_1',transforms=None, image_root = 'generated\/generated_plate\/'):\n\n        self.df = df\n        self.imfolder = imfolder\n        self.transforms = transforms\n        self.train = train\n        self.label = label\n        self.image_root = image_root\n\n    def __getitem__(self, index):\n\n        im_path = os.path.join(self.imfolder, self.df.iloc[index]['file_name'])\n        if os.path.isfile(im_path):\n            x = cv2.imread(im_path)\n            x0 = img_binary(x.copy())\n        else:\n            x = cv2.imread(self.image_root+ self.df.iloc[index]['file_name'])\n            x0 = x.copy()\n\n        if self.transforms!=None:\n            x=self.transforms(x)\n\n        x = x\/255\n        x = x.transpose(2, 0, 1)\n\n        x0 = x0 \/ 255\n        x0 = x0.transpose(2, 0, 1)\n\n        if self.train:\n            y1 = np.asarray(self.df.iloc[index]['label_1'])\n            y2 = np.asarray(self.df.iloc[index]['label_2'])\n            y3 = np.asarray(self.df.iloc[index]['label_3'])\n            y4 = np.asarray(self.df.iloc[index]['label_4'])\n            y5 = np.asarray(self.df.iloc[index]['label_5'])\n            y6 = np.asarray(self.df.iloc[index]['label_6'])\n            y7 = np.asarray(self.df.iloc[index]['label_7'])\n            return (x, x0), (y1, y2, y3, y4, y5, y6, y7)\n        else:\n            return (x, x0)\n\n    def __len__(self):\n        return len(self.df)","ce29e055":"from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import KFold, GroupKFold\nimport pickle","9ad317e8":"########### Parameters ##################\nseed = 54\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n#competition data root\nDATA_ROOT = '..\/input\/mlub-mongolian-car-plate-prediction\/'\n\n## \u0417\u0430\u0433\u0432\u0430\u0440 \u0445\u0430\u0434\u0433\u0430\u043b\u0441\u0430\u043d \u0444\u043e\u043b\u0434\u0435\u0440\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445.\n# Augmentation \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0445\u0430\u0434\u0433\u0430\u043b\u0441\u0430\u043d \u0444\u043e\u043b\u0434\u0435\u0440 \"models\/efn_b2_aug\/\",\n# \u044d\u043d\u0433\u0438\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0445\u0430\u0434\u0433\u0430\u043b\u0441\u0430\u043d \u0444\u043e\u043b\u0434\u0435\u0440 \"models\/efn_b2\/\"\nSAVE_ROOT = 'models\/efn_b2\/'\nif not os.path.exists(SAVE_ROOT):\n    os.makedirs(SAVE_ROOT)\n\n# Augmentation \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0437\u0443\u0440\u0433\u0438\u0439\u0433 \u0445\u0430\u0434\u0433\u0430\u043b\u0441\u0430\u043d \u0444\u043e\u043b\u0434\u0435\u0440 \"generated\/generated_plate_aug\/\",\n# \u044d\u043d\u0433\u0438\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0437\u0443\u0440\u0433\u0438\u0439\u0433 \u0444\u043e\u043b\u0434\u0435\u0440 \"generated\/generated_plate\/\"\nIMAGE_ROOT = 'generated_plate\/'\n\n# 'V0' is single model, 'V1' is merged model\nMODEL_TYPE = 'V0'\n\n# 'Binary' is gray image, 'Not_binary' is RGB image\nINPUT_TYPE = 'Binary'\n\n## \u041d\u044d\u043c\u044d\u043b\u0442\u044d\u044d\u0440 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0441\u043e\u043d\u0433\u043e\u0445.\n# Augmentation \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u0445\u0430\u0434\u0433\u0430\u043b\u0441\u0430\u043d \u0444\u043e\u043b\u0434\u0435\u0440 'aug_train_aug.csv',\n# \u044d\u043d\u0433\u0438\u0439\u043d \u043d\u044d\u043c\u044d\u0433\u0434\u044d\u043b \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b 'aug_train.csv'\naug = pd.read_csv('.\/aug_train.csv')\n\nmodel_name = 'model'\nefn_type1 = 'efficientnet_b2'\nefn_type2 = 'efficientnet_b2'\nepochs = 10\nkfold = 3\n\n\n########### training phase #############\n\ntrain_comp = pd.read_csv(DATA_ROOT+'training.csv')\n\ntrain_comp = train_comp.reset_index(drop=True)\n\ntrain_comp['label_1'],train_comp['label_2'], train_comp['label_3'], \\\ntrain_comp['label_4'], train_comp['label_5'], train_comp['label_6'], \\\ntrain_comp['label_7'] = zip(*train_comp['plate_number'].map(labelling))\n\ntrain = pd.concat([train_comp, aug], axis=0)\ntrain = train.reset_index(drop=True)\n\nlabel = ['label_1', 'label_2', 'label_3', 'label_4',\n         'label_5', 'label_6', 'label_7']\n\n\nout_dim = {}\nif os.path.isfile(SAVE_ROOT+'encoders.pickle'):\n    with open(SAVE_ROOT+'encoders.pickle', 'rb') as handle:\n        encoders = pickle.load(handle)\n\n    for lab in label:\n        train[lab] = encoders[lab].transform(train[lab])\n        out_dim[lab] = len(train[lab].unique())\n\nelse:\n    encoders = {}\n    for lab in label:\n        encoders[lab] = LabelEncoder().fit(train[lab])\n        out_dim[lab] = len(train[lab].unique())\n        train[lab] = encoders[lab].transform(train[lab])\n\n    with open(SAVE_ROOT+'encoders.pickle', 'wb') as handle:\n        pickle.dump(encoders, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nkf = KFold(n_splits=kfold, shuffle=True, random_state=42)\nfor i, (train_index, test_index) in enumerate(kf.split(train, train['label_1'].values)):\n\n    val = train.loc[test_index]\n\n    val.to_csv(SAVE_ROOT+'val_%s.csv' %(i), index=False)\n    train_data = train.loc[train_index]\n\n    train_set = CatPlateDataset(df=train_data.reset_index(),\n                            imfolder=DATA_ROOT + 'training\/training',\n                            train=True,\n                            label=label,\n                            transforms=None,\n                            image_root=IMAGE_ROOT)\n\n    train_loader = DataLoader(dataset=train_set, batch_size=16, shuffle=True)\n\n    val_set = CatPlateDataset(df=val.reset_index(),\n                                imfolder=DATA_ROOT + 'training\/training',\n                                train=True,\n                                label=label,\n                                transforms=None,\n                              image_root=IMAGE_ROOT)\n\n    val_loader = DataLoader(dataset=val_set, batch_size=32, shuffle=False)\n\n    best_val = 0\n    es_patience = 2\n\n    if MODEL_TYPE == 'V0':\n        model = Effnet_MLUB(enet_type=efn_type1, num_classes=out_dim)\n    else:\n        model = Effnet_MLUB_v1(enet_type1=efn_type1, enet_type2=efn_type2, num_classes=out_dim)\n\n    model = model.to(device)\n\n    optim = torch.optim.Adam(model.parameters(), lr=0.001)\n    scheduler = ReduceLROnPlateau(optimizer=optim, mode='min', patience=1, verbose=True, factor=0.1)\n\n    criterion = nn.CrossEntropyLoss()\n    #criterion = FocalLoss()\n    \n    model_path = SAVE_ROOT + '\/%s_%s.pth' %(model_name, i)\n    \n    if os.path.isfile(model_path):\n            # load the last checkpoint with the best model\n            model = torch.load(model_path)\n\n    else:\n    \n        for epoch in range(epochs):\n\n            train_preds = []\n            y_true = []\n            epoch_loss = 0\n            correct = 0\n            total = 0\n            for x, y in tqdm(train_loader):\n                if MODEL_TYPE=='V0':\n                    if INPUT_TYPE=='Binary':\n                        outs = model(x[1].to(device).float())\n                    else:\n                        outs = model(x[0].to(device).float())\n                else:\n                    outs = model(x[0].to(device).float(), x[1].to(device).float())\n\n                loss = 0\n                for m, out in enumerate(outs):\n                    loss = loss + criterion(out, y[m].to(device).long())\n\n                optim.zero_grad()\n                loss.backward()\n                optim.step()\n\n                epoch_loss += loss.item()\n\n            print(\n                'Epoch {:03}: | Loss: {:.3f}'.format(\n                    epoch + 1,\n                    epoch_loss))\n\n            total = 0\n            correct = 0\n            model.eval()\n            with torch.no_grad():\n                for x, y in tqdm(val_loader):\n\n                    if MODEL_TYPE == 'V0':\n                        outs = model(x[1].to(device).float())\n                    else:\n                        outs = model(x[0].to(device).float(), x[1].to(device).float())\n\n                    for j, pred in enumerate(outs):\n\n                        _, predicted = torch.max(pred.data, 1)\n\n                        total += y[j].size(0)\n                        correct += (predicted == y[j].to(device).long()).sum().item()\n\n            acc = 100 * correct \/ total\n            print('Test Accuracy of the model: {} %'.format(100 * correct \/ total))\n\n            #torch.save(model, model_path)  # Saving current best model\n\n            scheduler.step(acc)\n            if not best_val:\n                patience = es_patience\n                best_val = acc  # So any validation roc_auc we have is the best one for now\n                torch.save(model, model_path)  # Saving the model\n                continue\n\n            if acc > best_val:\n                best_val = acc\n                patience = es_patience  # Resetting patience since we have new best validation accuracy\n                torch.save(model, model_path)  # Saving current best model\n                best_epoch = epoch\n            else:\n                patience -= 1\n                if patience == 0:\n                    print('Early stopping. Best Val accuracy: {:.3f}'.format(best_val))\n                    break","5819a268":"########### Parameters ##################\nseed = 54\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\nDATA_ROOT = '..\/input\/mlub-mongolian-car-plate-prediction\/'\nkfold = 3\nbatch_size = 32\n\nSAVE_ROOT = 'models\/efn_b2\/'\nMODEL_TYPE = 'V0' #'V0' is single model\nINPUT_TYPE = 'Binary'\nmodel_name = 'model'\n\n## \u0411\u0438\u0434 \u0445\u043e\u0451\u0440 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u0437\u0430\u0433\u0432\u0430\u0440 \u04af\u04af\u0441\u0433\u044d\u0436, \u0441\u04af\u04af\u043b\u0438\u0439\u043d \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u043b \u0445\u0438\u0439\u0445\u0434\u044d\u044d \u044d\u0434\u0433\u044d\u044d\u0440 \u0437\u0430\u0433\u0432\u0430\u0440\u0443\u0443\u0434\u044b\u0433 \u0445\u043e\u0441\u043b\u0443\u0443\u043b\u0436 \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u0441\u0430\u043d.\nENSEMBLE = False\nEnsemble_Model_ROOT = 'models\/efn_b2_aug\/'\nEnsemble_Model_Name = 'model'\nEnsemble_Model_Type = 'V0'\nEnsemble_INPUT_TYPE = 'Binary'\n\nENSEMBLE2 = False\nEnsemble_Model_ROOT2 = 'models\/efn_b2_no_binary\/'\nEnsemble_Model_Name2 = 'model'\nEnsemble_Model_Type2 = 'V0'\nEnsemble_INPUT_TYPE2 = 'Binary'\n\n############# Running phase ##############\n\nlabel = ['label_1', 'label_2', 'label_3', 'label_4',\n         'label_5', 'label_6', 'label_7']\n\ntest = pd.read_csv(DATA_ROOT+'submission.csv')\n\ntest['label_1'],test['label_2'], test['label_3'], \\\ntest['label_4'], test['label_5'], test['label_6'], \\\ntest['label_7'] = zip(*test['plate_number'].map(labelling))\n\n\nwith open(SAVE_ROOT + 'encoders.pickle', 'rb') as handle:\n    encoders = pickle.load(handle)\n\ntest_set = CatPlateDataset(df=test.reset_index(),\n                            imfolder=DATA_ROOT + 'test\/test',\n                            train=False,\n                            label=label,\n                            transforms=None)\n\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\npred_y = {'label_1': [], 'label_2': [], 'label_3': [], 'label_4': [],\n          'label_5': [], 'label_6': [], 'label_7': []}\n\nfor x in tqdm(test_loader):\n    out_prob = {}\n    for k in range(kfold):\n        model = torch.load(SAVE_ROOT+'%s_%s.pth' % (model_name, k))\n        model.eval()  # switch model to the evaluation mode\n        with torch.no_grad():\n\n            if MODEL_TYPE=='V0':\n                if INPUT_TYPE == 'Binary':\n                    outs = model(x[1].to(device).float())\n                else:\n                    outs = model(x[0].to(device).float())\n            else:\n                outs = model(x[0].to(device).float(), x[1].to(device).float())\n\n            for out, lab in zip(outs, label):\n                if k==0:\n                    out_prob[lab] = F.softmax(out.data, dim=1)\n                else:\n                    out_prob[lab] = out_prob[lab] + F.softmax(out.data, dim=1)\n\n        if ENSEMBLE==True:\n            model = torch.load(Ensemble_Model_ROOT + '%s_%s.pth' % (Ensemble_Model_Name, k))\n            model.eval()  # switch model to the evaluation mode\n            with torch.no_grad():\n                if Ensemble_Model_Type=='V0':\n                    if Ensemble_INPUT_TYPE == 'Binary':\n                        outs = model(x[1].to(device).float())\n                    else:\n                        outs = model(x[0].to(device).float())\n                else:\n                    outs = model(x[0].to(device).float(), x[1].to(device).float())\n\n                for out, lab in zip(outs, label):\n                    out_prob[lab] = out_prob[lab] + F.softmax(out.data, dim=1)\n\n        if ENSEMBLE2 == True:\n            model = torch.load(Ensemble_Model_ROOT2 + '%s_%s.pth' % (Ensemble_Model_Name2, k))\n            model.eval()  # switch model to the evaluation mode\n            with torch.no_grad():\n                if Ensemble_Model_Type2 == 'V0':\n                    if Ensemble_INPUT_TYPE2 == 'Binary':\n                        outs = model(x[1].to(device).float())\n                    else:\n                        outs = model(x[0].to(device).float())\n                else:\n                    outs = model(x[0].to(device).float(), x[1].to(device).float())\n\n                for out, lab in zip(outs, label):\n                    out_prob[lab] = out_prob[lab] + F.softmax(out.data, dim=1)\n\n    for lab in label:\n        _, predicted = torch.max(out_prob[lab].data, 1)\n        pred = predicted.detach().cpu().numpy()\n        pred_y[lab]+=list(encoders[lab].inverse_transform(pred).flatten())\n\npreds = pd.DataFrame.from_dict(pred_y)\n\npreds['plate_number'] = preds.agg('{0[label_1]}{0[label_2]}{0[label_3]}{0[label_4]}{0[label_5]}{0[label_6]}{0[label_7]}'.format, axis=1)\npreds['file_name'] = test['file_name'].values\npreds[['file_name', 'plate_number']].to_csv('submission.csv', index=False)","abe62231":"\u042d\u043d\u044d\u0445\u04af\u04af \u0442\u044d\u043c\u0446\u044d\u044d\u043d\u0434 \u0431\u0438\u0434\u043d\u0438\u0439 \u0445\u0438\u0439\u0441\u044d\u043d \u0433\u043e\u043b \u0430\u0436\u0438\u043b \u043d\u044d\u043c\u044d\u043b\u0442 \u0445\u0438\u0439\u043c\u044d\u043b \u0430\u0432\u0442\u043e\u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440 \u04af\u04af\u0441\u0433\u044d\u0445 \u0431\u0430\u0439\u043b\u0430\u0430. \u0417\u0430\u0433\u0432\u0430\u0440\u044b\u043d \u0445\u0443\u0432\u044c\u0434 \u0431\u0438\u0434 efficient net B2 \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d. ","7030934f":"\u0411\u0438\u0434 \u044d\u043d\u044d \u043a\u0435\u0440\u043d\u0435\u043b\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0434 \u0437\u04e9\u0432\u0445\u04e9\u043d augmentation \u0445\u0438\u0439\u0433\u044d\u044d\u0433\u04af\u0439\u0433\u044d\u044d\u0440 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0441\u0443\u0440\u0433\u0430\u0432. \u0422\u044d\u043c\u0446\u044d\u044d\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u0431\u0438\u0434 augmentation \u0445\u0438\u0439\u0441\u044d\u043d \u0437\u0443\u0440\u0433\u0438\u0439\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u043d 3 \u0444\u043e\u043b\u0434\u043e\u043e\u0440 \u0434\u0430\u0445\u0438\u043d \u0437\u0430\u0433\u0432\u0430\u0440 \u0441\u0443\u0440\u0433\u0430\u0436, \u043d\u0438\u0439\u0442 6 \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0442\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0442\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u0445\u0430\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d. ","0aa70b5e":"\u0417\u0443\u0440\u0433\u0438\u0439\u0433 grayscale-\u0440\u04af\u04af \u0434\u043e\u043e\u0440\u0445 \u0431\u0430\u0439\u0434\u043b\u0430\u0430\u0440 \u0445\u04e9\u0440\u0432\u04af\u04af\u043b\u044d\u0445 \u043d\u044c \u04af\u0440 \u0434\u04af\u043d\u0433 \u0431\u0430\u0433\u0430 \u0437\u044d\u0440\u044d\u0433 \u0441\u0430\u0439\u0436\u0440\u0443\u0443\u043b\u0436 \u0431\u0430\u0439\u0441\u0430\u043d. ","6fc04be9":"\u0411\u0438\u0434 \u044d\u043d\u044d\u0445\u04af\u04af \u043a\u0435\u0440\u043d\u0435\u043b\u0434 \u0437\u043e\u0440\u0438\u0443\u043b\u0436 2000 \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u04af\u04af\u0441\u0433\u044d\u0432. \u0422\u044d\u043c\u0446\u044d\u044d\u043d\u0438\u0439 \u0445\u0443\u0432\u044c\u0434 \u0431\u0438\u0434 5000 \u0445\u0438\u0439\u043c\u044d\u043b \u04e9\u0433\u04e9\u0433\u0434\u04e9\u043b \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d. ","3a63b85d":"# Print submission","f5974fda":"**\u0417\u0443\u0440\u0430\u0433\u043d\u0430\u0430\u0441 \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u04af\u04af\u0434\u0438\u0439\u0433 \u0438\u043b\u0440\u04af\u04af\u043b\u044d\u0445: **","07350f02":"# \u0425\u043e\u0451\u0440\u0434\u0443\u0433\u0430\u0430\u0440 \u0431\u0430\u0439\u0440\u044b\u043d \u0448\u0438\u0439\u0434\u044d\u043b","2f44de2f":"\u0411\u0438\u0434 \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u04af\u04af\u0434\u0438\u0439\u0433 \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d\u0438\u0439 \u0434\u0430\u0440\u0430\u0430\u0433\u0430\u0430\u0440 \u0445\u0438\u0439\u043c\u044d\u043b \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440 \u0431\u043e\u043b\u043e\u0432\u0441\u0440\u0443\u0443\u043b\u0430\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439. \u0413\u044d\u0445\u0434\u044d\u044d \u04af\u04af\u0441\u0441\u044d\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0433\u0430\u0440\u0430\u0430\u0440 \u0437\u0430\u0439\u043b\u0448\u0433\u04af\u0439 \u0448\u0430\u043b\u0433\u0430\u0445 \u0448\u0430\u0430\u0440\u0434\u043b\u0430\u0433\u0430\u0442\u0430\u0439. \u0423\u0447\u0438\u0440 \u043d\u044c \u0437\u0430\u0440\u0438\u043c \u0444\u043e\u043b\u0434\u0435\u0440\u0442 \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440\u044b\u043d \u0437\u0443\u0440\u0430\u0433\u043d\u0430\u0430\u0441 \u0448\u0430\u043b\u0442\u0433\u0430\u0430\u043b\u0430\u043d \u0431\u0443\u0440\u0443\u0443 \u0442\u044d\u043c\u0434\u044d\u0433\u0442 \u043e\u0440\u043e\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439. \u0411\u0438\u0434 \u044d\u043d\u044d \u043a\u0435\u0440\u043d\u0435\u043b\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0434 \u04e9\u04e9\u0440\u0441\u0434\u0438\u0439\u043d \u04af\u04af\u0441\u0433\u044d\u0441\u044d\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u0438\u0439\u043d \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u0433 \u0448\u0443\u0443\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u043b\u0430\u0430. \u0425\u044d\u0440\u044d\u0432 \u04e9\u04e9\u0440\u0441\u0434\u04e9\u04e9 \u043d\u044d\u043c\u044d\u043b\u0442 \u0442\u044d\u043c\u0434\u044d\u0433\u0442 \u04af\u04af\u0441\u0433\u044d\u0445 \u0431\u043e\u043b \u0434\u0430\u0440\u0430\u0430\u0445 \u0437\u04e9\u0440\u04af\u04af\u0442\u044d\u0439 \u0431\u0430\u0439\u0434\u0430\u043b \u0433\u0430\u0440\u0447 \u0431\u043e\u043b\u043d\u043e. \n 1. \"1\" \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0434 \u0430\u043b\u044c \u043d\u044d\u0433 \u0442\u04e9\u0440\u043b\u0438\u0439\u043d \u0430\u0432\u0442\u043e\u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440\u0442 \u0442\u044d\u043c\u0434\u044d\u0433\u0442 \u04af\u04af\u0441\u044d\u0445\u0433\u04af\u0439 \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b \u043d\u04e9\u0433\u04e9\u04e9 \u0442\u04e9\u0440\u043b\u04e9\u04e9\u0441 \u043d\u044c \u043e\u0440\u043b\u0443\u0443\u043b\u0441\u0430\u043d \u0431\u043e\u043b\u043d\u043e. \n 2. soyombo \u0444\u043e\u043b\u0434\u0435\u0440\u0442 \u04af\u04af\u0441\u0441\u044d\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u0438\u0439\u0433 \u043c\u04e9\u043d \u0433\u0430\u0440\u0430\u0430\u0440 \u0442\u04af\u04af\u0436 \u0442\u0443\u0445\u0430\u0439\u043d \u0444\u043e\u043b\u0434\u0435\u0440\u0442 \u0446\u0443\u0433\u043b\u0443\u0443\u043b\u0441\u0430\u043d.  ","b6f86f67":"\u0414\u0435\u0442\u0435\u043a\u0442 \u0445\u0438\u0439\u0441\u044d\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u0438\u0439\u0433 \u0442\u0443\u0445\u0430\u0439\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442 \u0431\u04af\u0440\u044d\u044d\u0440 \u0430\u043d\u0433\u0438\u043b\u0436 \u0445\u0430\u0434\u0433\u0430\u043b\u0430\u0445\n\u0422\u044d\u043c\u0434\u044d\u0433\u0442 \u0431\u04af\u0440\u0438\u0439\u0433 \u0442\u0443\u0445\u0430\u0439\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442 \u0431\u04af\u0445\u0438\u0439 \u0444\u043e\u043b\u0434\u0435\u0440 \u0445\u0430\u0434\u0433\u0430\u043b\u0441\u0430\u043d\u0430\u0430\u0440 \u0431\u0438\u0434 \u043b\u0430\u0431\u0435\u043b \u0431\u04af\u0445\u0438\u0439 \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440 \u04af\u04af\u0441\u0433\u044d\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439.\n\u0425\u044d\u0434\u0438\u0439\u0433\u044d\u044d\u0440 \u043a\u043e\u0434 \u0430\u0448\u0438\u0433\u043b\u0430\u043d \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u04af\u04af\u0434\u0438\u0439\u0433 \u0430\u043d\u0433\u0438\u043b\u0441\u0430\u043d \u0431\u043e\u043b\u043e\u0432\u0447 \u0437\u0430\u0440\u0438\u043c \u0442\u044d\u043c\u0434\u044d\u0433\u0442 \u0434\u0443\u043d\u0434 \u0431\u0443\u0440\u0443\u0443 \u0442\u044d\u043c\u0434\u044d\u0433\u0442 \u043e\u0440\u043e\u0445 \u0431\u043e\u043b\u043e\u043c\u0436\u0442\u043e\u0439.\n\u0411\u0443\u0440\u0443\u0443 \u0442\u044d\u043c\u0434\u044d\u0433\u0442\u0438\u0439\u0433 \u043d\u044d\u0433 \u0431\u04af\u0440 \u0448\u0430\u043b\u0433\u0430\u0436, \u0442\u0443\u0445\u0430\u0439 \u0444\u043e\u043b\u0434\u0435\u0440\u043e\u043e\u0441 \u0443\u0441\u0442\u0433\u0430\u0445. \u042d\u043d\u044d \u04af\u0439\u043b \u044f\u0432\u0446\u044b\u0433 \u0433\u0430\u0440\u0430\u0430\u0440 \u0445\u0438\u0439\u0445 \u0431\u04e9\u0433\u04e9\u04e9\u0434 \u0442\u0438\u0439\u043c \u0447 \u0438\u0445 \u0446\u0430\u0433 \u0437\u0430\u0440\u0446\u0443\u0443\u043b\u0430\u0433\u0434\u0430\u0445\u0433\u04af\u0439.","e23aba94":"\u0411\u0438\u0434 pretrained model-\u0438\u0439\u043d \u0445\u0443\u0432\u044c\u0434 geffnet (https:\/\/github.com\/rwightman\/gen-efficientnet-pytorch\/tree\/master\/geffnet) \u0441\u0430\u043d\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0441\u0430\u043d. \n\u042d\u043d\u044d \u043a\u0435\u0440\u043d\u0435\u043b\u0434 pytorch-\u0438\u0439\u043d efficientnet_pytorch \u0437\u0430\u0433\u0432\u0430\u0440\u044b\u0433 \u0430\u0448\u0438\u0433\u043b\u0430\u0432. ","bc8f4ca9":"# \u0425\u0438\u0439\u043c\u044d\u043b \u043c\u0430\u0448\u0438\u043d\u044b \u0434\u0443\u0433\u0430\u0430\u0440 \u04af\u04af\u0441\u0433\u044d\u0445"}}