{"cell_type":{"3d974553":"code","53be474e":"code","64503810":"code","be342e53":"code","549b98fc":"code","2b4a935e":"code","0dfe8779":"code","4321446d":"code","c1c2ad9a":"code","bcb87541":"code","542e8780":"code","f2d406e4":"code","bd7681ff":"code","f13d8e74":"code","e4bbf268":"code","d2480f22":"code","e3a3ceb3":"code","91061838":"code","aa180f4f":"code","1ad47cf7":"code","43cb0e12":"code","c8dd88ed":"code","0e64bbab":"code","9af467d7":"code","fbb66641":"code","a04df6fc":"code","9e692105":"code","a8a1983e":"code","336dddb4":"code","fd81f76a":"markdown","f351b7fb":"markdown","c4131180":"markdown","8fd3d2d3":"markdown","daafe71b":"markdown","6d9bed78":"markdown","35a15ad0":"markdown","c663bfe0":"markdown","aa6ad8b0":"markdown","c240ecf4":"markdown","367d5d88":"markdown","dbdf728c":"markdown","acd552d3":"markdown","4cb35ff1":"markdown","b4609866":"markdown","6eb8160c":"markdown","fb5f4abf":"markdown"},"source":{"3d974553":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53be474e":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier","64503810":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df =  pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission =pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\ntrain_df.head()","be342e53":"train_df.shape, test_df.shape","549b98fc":"labels = train_df['target']\ntrain_df = train_df.drop(['keyword','location','target'],axis = 1)\ntest_df = test_df.drop(['keyword','location'],axis = 1)\n\n","2b4a935e":"train_df.shape,test_df.shape","0dfe8779":"labels.head()","4321446d":"labels.value_counts().plot(kind = 'bar')","c1c2ad9a":"train_df[labels == 0].head()","bcb87541":"train_df[labels == 1].head()","542e8780":"test_df.head()","f2d406e4":"train_length = train_df['text'].str.len()\ntest_length = test_df['text'].str.len()\nplt.figure(figsize = (8,3))\nplt.hist(train_length,bins = 30, label = \"train_texts\")\nplt.hist(test_length,bins = 30, label = \"test_texts\")\nplt.xlim(0,180)\nplt.legend()\nplt.show()","bd7681ff":"combined_data = pd.concat([train_df,test_df],ignore_index = True)","f13d8e74":"combined_data.shape","e4bbf268":"#stemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nclean = []\nfor i in range(0,len(combined_data)):\n    clean_text = re.sub('[^a-zA-Z]',' ',combined_data['text'][i])\n    clean_text = clean_text.lower()\n    clean_text = clean_text.split()\n    \n    clean_text = [lemmatizer.lemmatize(word) for word in clean_text if word not in stopwords.words('english')]\n    #clean_text = [stemmer.stem(word) for word in clean_text if word not in stopwords.words('english')]\n    clean_text = ' '.join(clean_text)\n    clean.append(clean_text)\n\ncombined_data['clean_text']= clean","d2480f22":"combined_data.head()","e3a3ceb3":"tfidf = TfidfVectorizer(max_features = 2500)\ntfidfvectors = tfidf.fit_transform(combined_data['clean_text']).toarray()","91061838":"tfidfvectors.shape","aa180f4f":"train = tfidfvectors[:7613]\ntest = tfidfvectors[7613:]","1ad47cf7":"X_train,X_test,y_train,y_test = train_test_split(train,labels,test_size = 0.2,\n                                                 random_state = 48)","43cb0e12":"model = MultinomialNB().fit(X_train,y_train)\ny_predict = model.predict(X_test)\ny_train_predict = model.predict(X_train)","c8dd88ed":"confusion_matrix(y_test,y_predict)","0e64bbab":"f1_score(y_test,y_predict)","9af467d7":"f1_score(y_train,y_train_predict)","fbb66641":"predict = model.predict(test)","a04df6fc":"submission.head()","9e692105":"submission['target'] = predict","a8a1983e":"submission.head()","336dddb4":"submission.to_csv('submission.csv',index=False)","fd81f76a":"bg.shape","f351b7fb":"Since it is a NLP problem we will get rid of Location and keyword Column","c4131180":"# Reading the files","8fd3d2d3":"Creating Confusion matrix and analysing our model performance","daafe71b":"Training the meodel using train dataset","6d9bed78":"cv = CountVectorizer(max_features = 3000)\nbg = cv.fit_transform(combined_data['clean_text']).toarray()","35a15ad0":"Lets combine the train and test dataset and apply cleaning process on texts together.","c663bfe0":"Distribution of lengths of tweets in test and train dataset","aa6ad8b0":"Lets Split the data Into Orginal train and test shape.","c240ecf4":"Train set is not highly imbalanced so we can go on with dataset, Otherwise we would have over\nsampled the data","367d5d88":"Training the model using NaiveBayes Classifier","dbdf728c":"# Creating Bag of Words from the Combined dataset","acd552d3":"We can see that there are 24464 different features made using the bag of words.\nThese 24464 count nothing but the unique words from the combined dataset.","4cb35ff1":"# Info about the Data.\nid -> a unique identifier for each tweet.\n\ntext -> the text of the tweet.\n\nlocation -> the location the tweet was sent from (may be blank).\n\nkeyword -> a particular keyword from the tweet (may be blank).\n\ntarget -> in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0).","b4609866":"cleaning text file and appending it to dataframe","6eb8160c":"# Importing the libraries Needed.","fb5f4abf":"# Creating TFidfVectors\n"}}