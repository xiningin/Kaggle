{"cell_type":{"1797d3eb":"code","8e01a300":"code","a6beb491":"code","33b0455e":"code","87ae0196":"code","d14d84a1":"code","9c056e0b":"code","326971ad":"code","a6149e20":"code","f2c13495":"code","4448c03d":"code","04525b5a":"code","e329532b":"code","16f1cc90":"code","4cd003b5":"code","1f615a31":"code","67d891dc":"code","8d826ce9":"code","d73c084c":"code","b845e60a":"code","b370c64d":"code","63c03eba":"code","ab72390b":"code","c3c0ac46":"code","24668482":"code","baed3bd6":"code","a6c06360":"code","e745e3d2":"code","973cc213":"code","b726e663":"code","a08cdb0d":"code","56c1c515":"code","934369b0":"code","ba2a060e":"code","6eceecb2":"code","31672c64":"code","97c60b35":"code","960444c9":"code","904f9fb4":"code","08512851":"code","063d20c2":"code","2d3b121a":"code","2285c76c":"code","03be1b9d":"code","92eb472f":"code","1a9bacbe":"code","bcb8ca1f":"code","3a83f0e0":"code","b6c2dd29":"markdown","cc7146fc":"markdown","42cd5934":"markdown","6c33897a":"markdown","ee5001bc":"markdown","7c5a058b":"markdown","e77ffe28":"markdown","cfd17df1":"markdown","51719c5a":"markdown","14aa06eb":"markdown","d86cd023":"markdown","ed7c651c":"markdown","301c6ea0":"markdown","a45ec512":"markdown","268e058e":"markdown","511a97b6":"markdown","581cd258":"markdown","4f2df708":"markdown","ce11dd82":"markdown","38b63b51":"markdown","809e8aae":"markdown","cce605cb":"markdown","3d83f2c6":"markdown","acd00ce5":"markdown"},"source":{"1797d3eb":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nroot_path = '\/kaggle\/input\/brazilian-ecommerce\/'\n\ncustomers_df = pd.read_csv(root_path + 'olist_customers_dataset.csv')\nitems_df = pd.read_csv(root_path + 'olist_order_items_dataset.csv')\npayments_df = pd.read_csv(root_path + 'olist_order_payments_dataset.csv')\norders_df = pd.read_csv(root_path + 'olist_orders_dataset.csv')\nproducts_df = pd.read_csv(root_path + 'olist_products_dataset.csv')\nsellers_df = pd.read_csv(root_path + 'olist_sellers_dataset.csv')\ncategories_df = pd.read_csv(root_path + 'product_category_name_translation.csv')","8e01a300":"the_df = {'customers': customers_df,\n              'items': items_df, \n              'payments': payments_df, \n              'orders': orders_df, \n              'products': products_df, \n              'sellers': sellers_df, \n              'categories': categories_df}\nprint(\"_______________________________________________________________________________________________\")\nprint(\"Description of the {} dataframes\".format(len(the_df)))\nprint(\"_______________________________________________________________________________________________\")\nfor i, j in the_df.items():\n    print('{} dataframe:      {} rows and {} columns'.format(str(i),j.shape[0],j.shape[1]))\n    print(list(j.columns))\n    print(\"\")\nprint(\"________________________________________________________________________________________________\")","a6beb491":"customers_df=customers_df.drop(columns=[ 'customer_zip_code_prefix'])\nitems_df=items_df.drop(columns=[ 'shipping_limit_date'])\npayments_df=payments_df.drop(columns=['payment_sequential'])\norders_df=orders_df.drop(columns=['order_approved_at'\n                                  , 'order_delivered_carrier_date'\n                                  , 'order_delivered_customer_date'\n                                  , 'order_estimated_delivery_date'])\n","33b0455e":"categories_df.head(3)","87ae0196":"products_df.head(3)","d14d84a1":"df = pd.merge(products_df, categories_df, on='product_category_name', how='left')\ndf.head(3)","9c056e0b":"df=df.drop(columns=[\"product_category_name\"])\ndel categories_df\n# Rename the column\ndf.rename(columns={'product_category_name_english': 'product_category_name'}, inplace=True)","326971ad":"(fig, ax) = plt.subplots(figsize=(18, 7))\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"The categories\")\nsns.countplot(data=df,x=\"product_category_name\",edgecolor='black',color=\"#6fd67b\")\nplt.xticks(range(0,df[\"product_category_name\"].nunique()+1)\n           ,df[\"product_category_name\"].unique(),rotation=90)\nplt.grid()","a6149e20":"#I merge all the orther dataframe together\ndf2 = pd.merge(orders_df, customers_df, on='customer_id')\ndf2 = df2.merge(items_df, on='order_id')\ndf2 = df2.merge(payments_df, on='order_id')\ndf2 = df2.merge(sellers_df, on='seller_id')\ndf2.head(3)","f2c13495":"print(\"We have {} differents customers,{} diffrents orders, {} differents products and {} differents sellers\".format(df2[\"customer_unique_id\"].nunique(),\n                                                                                                                     df2[\"order_id\"].nunique(),\n                                                                                                                    df2[\"product_id\"].nunique(),\n                                                                                                                    df2[\"seller_id\"].nunique()))","4448c03d":"the_data= df.merge(df2, on='product_id')\nthe_data.info()","04525b5a":"\nthe_data['order_purchase_timestamp'] = pd.to_datetime(the_data['order_purchase_timestamp'])\n\nthe_data.head(2)","e329532b":"print(\"Orders have been placed between \",min(the_data['order_purchase_timestamp']),\" and \",max(the_data['order_purchase_timestamp']))","16f1cc90":"\ntime=the_data.loc[:,['order_id']]\ntime['order_purchase_year'] = pd.to_datetime(the_data['order_purchase_timestamp']).dt.year\ntime['order_purchase_month'] = pd.to_datetime(the_data['order_purchase_timestamp']).dt.month\n","4cd003b5":"time = time.groupby(['order_purchase_month', 'order_purchase_year']).count().reset_index()","1f615a31":"time[\"period\"] =  time[\"order_purchase_year\"].astype(str)  + \"\/\" +time[\"order_purchase_month\"].astype(str)\ntime.columns=[\"order_purchase_month\",\"order_purchase_year\",\"Number of order\",\"period\"]\ntime=time.sort_values(by=['order_purchase_year', 'order_purchase_month'])\ntime.head()","67d891dc":"(fig, ax) = plt.subplots(figsize=(13, 7))\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Number of orders as a function of time\")\nax=plt.bar(range(0,time[\"period\"].nunique()), time[\"Number of order\"].values\n           ,edgecolor='black',color=\"#6fd67b\")\nplt.xticks(range(0,time[\"period\"].nunique()),time[\"period\"].unique(),rotation=90)\nplt.xlabel(\"Period\")\nplt.ylabel(\"Number of orders\")\nplt.grid()","8d826ce9":"import datetime as dt\n\ndata=the_data.loc[ the_data['order_purchase_timestamp']>dt.datetime(2018,2,1,0,0,0,0,None)]\ndata=data.loc[ data['order_purchase_timestamp']<dt.datetime(2018,8,1,0,0,0,0,None)]\n","d73c084c":"\ndata=data[['product_id','product_category_name', 'price', 'freight_value','payment_type','customer_unique_id',\"order_id\", 'customer_city', 'customer_state']]\n\n","b845e60a":"\n\ndef extract_values(cust_id):\n    d=data.loc[data[\"customer_unique_id\"]==cust_id]\n    cat=list(d[\"product_category_name\"].unique())\n    n_order=d[\"order_id\"].nunique()\n    tot_price=d[\"price\"].sum()\n    tot_frei=d[\"freight_value\"].sum()\n    pay_type=list(d[\"payment_type\"].unique())\n    if len(pay_type)>0:\n        pay_type=pay_type[0]\n    else:\n        pay_type=np.nan\n    state=d[\"customer_state\"].unique()\n    if len(state)==1:\n        state=state[0]\n    else:\n        state=np.nan\n    row=[cust_id,cat,n_order,tot_price,tot_frei,pay_type,state]\n    \n    return row\n    \n    ","b370c64d":"def get_dataset(customer_list):\n    df= pd.DataFrame(columns=[\"customer_id\",\"category\",\"n_order\",\"total_price\",\"total_freight_value\",\"payement_type\",\"costumer_state\"])\n    print(\"We are going to create a {} rows dataset.\".format(len(customer_list)))\n    #Extract data\n    for i, cust in enumerate(customer_list):\n        x= extract_values(cust)\n        df.loc[i+1]=x\n    \n    print(\"Data extracted\")\n    #encode payment_type and costumer_state\n    df2=pd.get_dummies(data=df, columns=[\"payement_type\",\"costumer_state\"])\n    \n    #Encode category\n    cat=df2[\"category\"].sum()\n    cat=(pd.Series(cat).unique())     \n    for c in cat:\n        def create_var(liste):\n            if c in liste:\n                return 1\n            else:\n                return 0 \n        df2[\"cat\"+str(c)]=df2[\"category\"].apply(create_var)\n    \n    print(\"Category encoded\")\n         \n   \n    \n    #Drop useless columns\n    df2=df2.drop(columns=[ 'category'])\n    print(\"the final dataset has {} rows and {} columns .\".format(df2.shape[0],df2.shape[1]))\n    return df2","63c03eba":"n_rows=6000\ncustomer_id=list(data[\"customer_unique_id\"].unique())\nsample=list(np.random.choice(customer_id, n_rows))\ndonne=get_dataset(sample)\ndonne.head(2)","ab72390b":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\ndef display_scree_plot(pca):\n#Function display the cumulative variance ratio\n    scree = pca.explained_variance_ratio_*100\n    fig=plt.figure(figsize=[16,5])\n    fig.patch.set_facecolor('#E0E0E0')\n    fig.patch.set_alpha(0.7)\n    plt.bar(np.arange(len(scree))+1, scree,color=\"#6fd67b\",edgecolor='black')\n    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='+')\n    plt.xlabel(\"Rank of the principal component\")\n    plt.ylabel(\"Percent of inertia\")\n    plt.title(\"cumulative variance ratio\")\n    plt.grid()\n    plt.show(block=False)","c3c0ac46":"X=donne.drop(columns=[\"customer_id\"])\nX=X.dropna() # PCA don't take nans\n\n\n#On Standardise les donn\u00e9es\nscaler=StandardScaler() \nXs=scaler.fit_transform(X)\n\n# On applique la PCA\npca = PCA(n_components=min(X.shape[0],X.shape[1]))\nd=pca.fit_transform(Xs)","24668482":"display_scree_plot(pca)","baed3bd6":"scree = pca.explained_variance_ratio_\nnbr_pca=0\nN=110\nfor i in range(N):\n    a = scree.cumsum()[i]\n    if a >= 0.80:\n        print(\"{} principal components explain at least 80% of the total variance\".format(i))\n        print(\"Exact value of variance explained: {}%\".format(round(a*100,2)))\n        nbr_pca=i\n        break\npca = PCA(n_components=nbr_pca)\nd=pca.fit_transform(Xs)","a6c06360":"data_pca=pd.DataFrame(d)\ncol=[\"pca \"+ str(n+1) for n in data_pca.columns]\ndata_pca.columns=col\ndata_pca.head()\n","e745e3d2":"fig=plt.figure(figsize=[8,6])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Projection of the {} customers on the 1st PCA plan\".format(data_pca.shape[0]))\nplt.scatter(data=data_pca,x=\"pca 1\",y=\"pca 2\")\nplt.grid()","973cc213":"from mpl_toolkits.mplot3d import Axes3D\nimport sys\n\nfig=plt.figure(figsize=[8,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nax = Axes3D(fig)\nplt.title(\"PCA - Projection of the {} customers on the first three axes \".format(data_pca.shape[0]),fontsize=18)\n\nax.set_xlabel ('PCA 1 ',fontsize=13)\nax.set_ylabel ('PCA 2 ',fontsize=13)\nax.set_zlabel ('PCA 3 ',fontsize=13)\nax.scatter(data_pca['pca 1'], data_pca['pca 2'],data_pca['pca 3'], s=50)\n","b726e663":"data_pca[\"customer_id\"]=donne[\"customer_id\"].values\ndata_pca.head(1)\n","a08cdb0d":"\n\nX=data_pca.drop(columns=[\"customer_id\"]).copy()\nscaler=StandardScaler() \nXs=scaler.fit_transform(X)\n\n","56c1c515":"from sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\nn_clusters=range(2,15)\nscores=[]\nfor n in n_clusters:\n    \n    kmeans = KMeans(n_clusters=n, random_state=0).fit(Xs)\n    labs=kmeans.labels_\n    score=silhouette_score(Xs,labs)\n    scores.append(score)","934369b0":"fig=plt.figure(figsize=[15,7])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Kmeans: Silhouette score values in function of the number of clusters\")\n\nplt.plot(n_clusters,scores)\nplt.ylabel(\"Silhouette score values\")\nplt.xlabel(\"Number of clusters\")\nplt.grid()\nplt.legend()","ba2a060e":"kmeans = KMeans(n_clusters=4, random_state=0).fit(Xs)\nlabs=kmeans.labels_\n\ntest=data_pca.loc[:,[\"pca 1\",\"pca 2\"]]\ntest[\"cluster kmeans\"]=labs\nfig=plt.figure(figsize=[8,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Projection of individuals on the foreground of PCA. colored with Kmeans cluster\")\nfor clu in test[\"cluster kmeans\"].unique():\n    t=test.loc[test[\"cluster kmeans\"]==clu]\n    sns.scatterplot(x=t['pca 1'],y= t['pca 2'],label=clu)\nplt.legend()\nplt.grid()\nplt.show()","6eceecb2":"cluster_size={}\nfor i in labs:\n    if i not in cluster_size:\n        cluster_size[i]=1\n    else:\n        cluster_size[i]+=1\n\nfig=plt.figure(figsize=[8,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Clusters size with kmeans.\") \nplt.bar(range(0,len(cluster_size)),cluster_size.values(),color=\"#a1c2cc\",edgecolor='black')\nfor i,num in enumerate(cluster_size.keys()):\n    height=cluster_size[num]+20\n    x=i-0.25\n    pourcent=cluster_size[num]\/labs.shape[0]*100\n    plt.text(x,height,\"{} % \".format(round(pourcent,2)))\nplt.xticks(range(0,len(cluster_size)),cluster_size.keys())","31672c64":"donne[\"N\u00b0 cluster\"]=labs\n","97c60b35":"df=data.loc[data[\"customer_unique_id\"].isin(sample)]\ndic_customer={}\nfor i in cluster_size.keys():\n    d=donne.loc[donne[\"N\u00b0 cluster\"]==i]\n    dic_customer[i]=list(d[\"customer_id\"].unique())\n\ndef give_cluster(num):\n    for i in dic_customer.keys():\n        if num in dic_customer[i]:\n            return i\n    \n                         \n","960444c9":"df[\"N\u00b0 cluster\"]=df[\"customer_unique_id\"].apply(give_cluster)","904f9fb4":"df.head()","08512851":"def extract_pourcent(df,list_top3,var):\n    dict_freq={}\n    N=df.shape[0]\n    for i in list_top3:\n        d=df.loc[df[var]==i]\n        n=d.shape[0]\/N*100\n        dict_freq[i]=round(n,1)\n    return dict_freq\n\ndef most_pop(variable,cluster):\n    d=df.loc[df[\"N\u00b0 cluster\"]==cluster]\n    cat_list=list(d[variable].values)\n    cat_counter = {}\n    for word in cat_list:\n        if word in cat_counter:\n            cat_counter[word] += 1\n        else:\n            cat_counter[word] = 1\n\n    popular_cat = sorted(cat_counter, key = cat_counter.get, reverse = True)\n    top_4 = popular_cat[:4]\n    top_4=extract_pourcent(d,top_4,variable)\n    return top_4","063d20c2":"print(df[\"N\u00b0 cluster\"].unique())","2d3b121a":"def cluster_analysis(n_cluster):\n    the_cluster=n_cluster\n    popular_cat=most_pop(\"product_category_name\",the_cluster)\n    popular_state=most_pop(\"customer_state\",the_cluster)\n    popular_payment_type=most_pop(\"payment_type\",the_cluster)\n\n    fig=plt.figure(1,figsize=[19,5])\n    fig.patch.set_facecolor('#E0E0E0')\n    fig.patch.set_alpha(0.7)\n\n    plt.suptitle(\"Clusters N\u00b0 {}: {} customers.\".format(the_cluster,cluster_size[the_cluster]),size=15)\n    plt.subplot(1,3,1)\n    plt.title(\"Top of product category\")\n    plt.bar(range(0,len(popular_cat)),popular_cat.values(),color=\"#a1c2cc\",edgecolor='black')\n    plt.xticks(range(0,len(popular_cat)),popular_cat.keys(),rotation=90)\n    plt.ylabel(\"Percentage\")\n\n    plt.subplot(1,3,2)\n    plt.title(\"Top of State of residence\")\n    plt.bar(range(0,len(popular_state)),popular_state.values(),color=\"#a1c2cc\",edgecolor='black')\n    plt.xticks(range(0,len(popular_state)),popular_state.keys())\n\n    plt.subplot(1,3,3)\n    plt.title(\"Top of State of residence\")\n    plt.bar(range(0,len(popular_payment_type)),popular_payment_type.values(),color=\"#a1c2cc\",edgecolor='black')\n    plt.xticks(range(0,len(popular_payment_type)),popular_payment_type.keys(),rotation=90)\n    plt.show()","2285c76c":"cluster_analysis(0)","03be1b9d":"cluster_analysis(1)","92eb472f":"cluster_analysis(2)","1a9bacbe":"cluster_analysis(3)","bcb8ca1f":"fig=plt.figure(figsize=[8,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Distribution of the price in the clusters.\")\nsns.boxplot(data=df,x=\"N\u00b0 cluster\",y=\"price\",width=0.5,showfliers=False,color=\"#a1c2cc\")","3a83f0e0":"fig=plt.figure(figsize=[8,8])\nfig.patch.set_facecolor('#E0E0E0')\nfig.patch.set_alpha(0.7)\nplt.title(\"Distribution of the freight value in the clusters.\")\nsns.boxplot(data=df,x=\"N\u00b0 cluster\",y=\"freight_value\",width=0.5,showfliers=False,color=\"#a1c2cc\")","b6c2dd29":"# 2. **Dataset creation: Feature engineering**","cc7146fc":"# 4. Cluster Analysis","42cd5934":"We choose the number of clusters to generate by relying on the silhouette coefficient","6c33897a":"First, let's look at the size of each cluster","ee5001bc":"# 3. kmeans clustering  ","7c5a058b":"\nThe following function allows you to **display the 4 most frequent classes**, (for 3 indicators: product categories, state of residence and type of payment) for a **cluster number** passed as an argument.","e77ffe28":"\nI extract the list of all the clients of each cluster\n And reports the cluster for each client in the raw data dataframe","cfd17df1":"We have 2 massives clusters, then 2 smaller in size ","51719c5a":"I start by removing a few columns that I will not use in my analysis","14aa06eb":"I'm going to use the category name in english. I merge together the product an category dataframe and then drop the Portuguese names","d86cd023":"\nWe extract a sample of customers and create the dataset.","ed7c651c":"**In view of the number of columns we will apply a dimensional reduction with PCA**","301c6ea0":"# **Conclusion:**\n> Individuals from different clusters are distinguished mainly by the type of product purchased. \n","a45ec512":"\nThe following functions are used to extract a **dictionary of the most frequent classes**, with the associated percentage, for a **cluster number** and a **variable** passed as an argument","268e058e":"Now i merge df and df2 on 'product_id'","511a97b6":"I will display the order count as a function of time.\n\nFor that I will extract the month and the year for each row and bring them together in another dataframe","581cd258":"This function encodes categorical variables to ultimately obtain a dataframe of numeric variables","4f2df708":"let's take a look on the number of customer, order, products et seller we have","ce11dd82":"The following function is used to extract information about a customer number passed as an argument.\n\nit returns this informations in the form of a list.","38b63b51":"\nWe will focus on the orders placed between 2-2018 and 8-2018,because these are the most recent orders, they will better represent the purchasing behavior of current customers","809e8aae":"Choose the number of principal component to take","cce605cb":"Let's take a look at the different categories of objects and their frequency of purchase","3d83f2c6":"# 1.  Exploring the data base","acd00ce5":"# **Student project: Segment the clientele of an e-commerce site**\n\nIn this notebook, I will segment the clientele of the Brazilian e-commerce site Olist.\n\nThe available database consists of 7 tables,whose content description will be presented in the next block.\n\n\n**This notebook consists of 4 parts:**\n\n**1- Exploring the data base:**\n\n> In this section I would select some relevant variables, and put them together in a single dataframe.\n> this will also be the occasion for some visualizations\n> \n\n**2- Dataset creation: Feature engineering**\n\n> In this section, I create a dataset from a sample of recent customers (having placed an order in the last 6 months)\n> In this dataset each row will represent a customer.\n> \n\n**3- Kmeans clustering**\n\n**4- Cluster Analysis**\n\n> In this section, I would try to make sense of the established clusters"}}