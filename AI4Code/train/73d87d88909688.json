{"cell_type":{"00692d7b":"code","8dce4cf5":"code","9fc41aa5":"code","8369d771":"code","5a63e9cc":"code","cd981e12":"code","770700f5":"code","397963ce":"code","62d25773":"code","fe89d5fb":"code","e0780ece":"code","007d89e2":"code","7f5de317":"code","74f91b99":"code","c28caa17":"code","d0df4b87":"code","126be520":"code","25fc9906":"code","fad63051":"code","0c25ad55":"code","2e01cca5":"code","d9d34091":"code","c8ef589a":"code","0888f89d":"code","9eb2c832":"code","11353628":"code","22fb11e5":"code","5bf5a4c2":"code","4cfaeb04":"code","b96fa902":"code","efb84ca0":"code","9599eae3":"code","fd76d94c":"code","0a477d8e":"code","c17ec369":"code","fb510f1e":"markdown","70c4a50a":"markdown","6175b711":"markdown","ecb51d20":"markdown","395ef40c":"markdown","d699a0ba":"markdown","dc021ba8":"markdown","8a2e4338":"markdown","b705e5a4":"markdown","b217da97":"markdown","74560232":"markdown","0d8f480b":"markdown","5a8af989":"markdown","03e5b438":"markdown","1daab171":"markdown","fcb044a4":"markdown","5c24d0d5":"markdown","6add0c85":"markdown","5aea1d9a":"markdown","301e3a72":"markdown","b2a23c64":"markdown","4a1f135d":"markdown","7a5c6c67":"markdown","9a9978ff":"markdown","1a452212":"markdown","72500ed0":"markdown","5a8d9fc3":"markdown","a4fb69f5":"markdown","0441f85f":"markdown","ca3b1994":"markdown","ed0fe035":"markdown","d17224c0":"markdown","548f2863":"markdown","8977d694":"markdown","f5eeec50":"markdown","7aad9da4":"markdown","de468a88":"markdown","fb26892a":"markdown","f5567d6f":"markdown","5f652767":"markdown","1a0d682c":"markdown","b5ac5693":"markdown","6412280f":"markdown","2b8126d8":"markdown"},"source":{"00692d7b":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8dce4cf5":"dataset = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","9fc41aa5":"dataset.head()","8369d771":"dataset.isnull().sum()","5a63e9cc":"plt.figure(figsize=(12,8))\nsns.heatmap(dataset.drop(columns=['DEATH_EVENT']).corr(),vmin=-1, cmap='coolwarm', annot=True)\nplt.show()","cd981e12":"dataset.drop(columns=['DEATH_EVENT']).corrwith(dataset['DEATH_EVENT'])","770700f5":"X = dataset.iloc[:, 0:12].values\ny = dataset.iloc[:, -1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","397963ce":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","62d25773":"from sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(random_state = 0)\nclassifier_lr.fit(X_train, y_train)","fe89d5fb":"y_pred_lr = classifier_lr.predict(X_test)","e0780ece":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_lr = confusion_matrix(y_test, y_pred_lr)\nprint(cm_lr)\naccuracy_score(y_test, y_pred_lr)","007d89e2":"from sklearn.naive_bayes import GaussianNB\nclassifier_NB = GaussianNB()\nclassifier_NB.fit(X_train, y_train)","7f5de317":"y_pred_NB = classifier_NB.predict(X_test)","74f91b99":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_NB = confusion_matrix(y_test, y_pred_NB)\nprint(cm_NB)\naccuracy_score(y_test, y_pred_NB)","c28caa17":"from sklearn.svm import SVC\nclassifier_SVM = SVC(kernel = 'linear', random_state = 0)\nclassifier_SVM.fit(X_train, y_train)","d0df4b87":"y_pred_SVM = classifier_SVM.predict(X_test)","126be520":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_SVM = confusion_matrix(y_test, y_pred_SVM)\nprint(cm_SVM)\naccuracy_score(y_test, y_pred_SVM)","25fc9906":"from sklearn.svm import SVC\nclassifier_KSVM = SVC(kernel = 'rbf', random_state = 0)\nclassifier_KSVM.fit(X_train, y_train)","fad63051":"y_pred_KSVM = classifier_KSVM.predict(X_test)","0c25ad55":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_KSVM = confusion_matrix(y_test, y_pred_KSVM)\nprint(cm_KSVM)\naccuracy_score(y_test, y_pred_KSVM)","2e01cca5":"from sklearn.ensemble import RandomForestClassifier\nclassifier_RF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_RF.fit(X_train, y_train)","d9d34091":"y_pred_RF = classifier_RF.predict(X_test)","c8ef589a":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_RF = confusion_matrix(y_test, y_pred_RF)\nprint(cm_RF)\naccuracy_score(y_test, y_pred_RF)","0888f89d":"from sklearn.tree import DecisionTreeClassifier\nclassifier_DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier_DT.fit(X_train, y_train)","9eb2c832":"y_pred_DT = classifier_DT.predict(X_test)","11353628":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_DT = confusion_matrix(y_test, y_pred_DT)\nprint(cm_DT)\naccuracy_score(y_test, y_pred_DT)","22fb11e5":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier_KNN.fit(X_train, y_train)","5bf5a4c2":"y_pred_KNN = classifier_KNN.predict(X_test)","4cfaeb04":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_KNN = confusion_matrix(y_test, y_pred_KNN)\nprint(cm_KNN)\naccuracy_score(y_test, y_pred_KNN)","b96fa902":"from xgboost import XGBClassifier\nclassifier_XGBoost = XGBClassifier()\nclassifier_XGBoost.fit(X_train, y_train)","efb84ca0":"y_pred_XGBoost = classifier_XGBoost.predict(X_test)","9599eae3":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm_XGBoost = confusion_matrix(y_test, y_pred_XGBoost)\nprint(cm_XGBoost)\naccuracy_score(y_test, y_pred_XGBoost)","fd76d94c":"comparison = []\ncomparison_accuracy = []\n","0a477d8e":"comparison.append('Logistic Regression')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_lr))\ncomparison.append('Naive Bayes')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_NB))\ncomparison.append('Linear SVM')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_SVM))\ncomparison.append('Kernel SVM')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_KSVM))\ncomparison.append('Random Forest')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_RF))\ncomparison.append('Decision Tree')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_DT))\ncomparison.append('K-Nearest Neighbors (K-NN)')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_KNN))\ncomparison.append('XG Boost')\ncomparison_accuracy.append(accuracy_score(y_test, y_pred_XGBoost))\n\naccuracy_comparison = pd.DataFrame({'Classification Model': comparison, 'Accuracy': comparison_accuracy})\nprint(accuracy_comparison)","c17ec369":"from sklearn.model_selection import cross_val_score\nacc_lr = cross_val_score(estimator = classifier_lr, X = X_train, y = y_train, cv = 10)\nacc_NB = cross_val_score(estimator = classifier_NB, X = X_train, y = y_train, cv = 10)\nacc_SVM = cross_val_score(estimator = classifier_SVM, X = X_train, y = y_train, cv = 10)\nacc_KSVM = cross_val_score(estimator = classifier_KSVM, X = X_train, y = y_train, cv = 10)\nacc_RF = cross_val_score(estimator = classifier_RF, X = X_train, y = y_train, cv = 10)\nacc_DT = cross_val_score(estimator = classifier_DT, X = X_train, y = y_train, cv = 10)\nacc_KNN = cross_val_score(estimator = classifier_KNN, X = X_train, y = y_train, cv = 10)\nacc_XGBoost = cross_val_score(estimator = classifier_XGBoost, X = X_train, y = y_train, cv = 10)\n\nkfold_acc_mean = [np.mean(acc_lr), np.mean(acc_NB), np.mean(acc_SVM), np.mean(acc_KSVM), np.mean(acc_RF), \n                  np.mean(acc_DT), np.mean(acc_KNN), np.mean(acc_XGBoost)]\nkfold_acc_std = [np.std(acc_lr), np.std(acc_NB), np.std(acc_SVM), np.std(acc_KSVM), np.std(acc_RF), \n                 np.std(acc_DT), np.std(acc_KNN), np.std(acc_XGBoost)]\n\nK_Fold_cross_val = pd.DataFrame({'Classification Model': comparison, 'KFold accuracy mean': kfold_acc_mean, \n                                 'KFold accuracy std': kfold_acc_std})\nprint(K_Fold_cross_val)","fb510f1e":"***Training XGBoost on the Training set***","70c4a50a":"***Training the Kernel SVM model on the Training set***","6175b711":"***Predicting the Test set results***","ecb51d20":"***Confusion matrix and Accuracy score***","395ef40c":"***Training the Naive Bayes model on the Training set***","d699a0ba":"***Confusion matrix and Accuracy score***","dc021ba8":"***Predicting the Test set results***","8a2e4338":"***Predicting the Test set results***","b705e5a4":"***Predicting the Test set results***","b217da97":"# K-Nearest Neighbors (K-NN)","74560232":"# Importing libraries","0d8f480b":"***Confusion matrix and Accuracy score***","5a8af989":"# Importing the dataset","03e5b438":"# Logistic Regression","1daab171":"***Correlation between each explanatory variable and the response variable***","fcb044a4":"# Exploratory Analysis","5c24d0d5":"***Training the K-NN model on the Training set***","6add0c85":"# Decision Tree Classification","5aea1d9a":"# K-Fold Cross Validation","301e3a72":"I have run and compared 8 machine learning classification models on the given dataset to predict if the patient died or not during the follow-up period.\n\n1. Logistic Regression\n2. Naive Bayes\n3. Linear SVM\n4. Kernel SVM\n5. Random Forest\n6. Decision Tree\n7. K-Nearest Neighbors (K-NN)\n8. XGBoost\n","b2a23c64":"# Kernel Support Vector Machines Model","4a1f135d":"# Feature Scaling\n","7a5c6c67":"***Predicting the Test set results***","9a9978ff":"***Training the Logistic Regression model on the Training set***","1a452212":"***Confusion matrix and Accuracy score***","72500ed0":"# Check for null values","5a8d9fc3":"***Confusion matrix and Accuracy score***","a4fb69f5":"***Predicting the Test set results***","0441f85f":"# Data Preprocessing","ca3b1994":"# Model Accuracies Comparison","ed0fe035":"***Training the Decision Tree Classification model on the Training set***","d17224c0":"***Confusion matrix and Accuracy score***","548f2863":"***Training the Random Forest Classification model on the Training set***","8977d694":"***Predicting the Test set results***","f5eeec50":"***Confusion matrix and Accuracy score***","7aad9da4":"# Support Vector Machines Model","de468a88":"***Predicting the Test set results***","fb26892a":"# Random Forest Classification","f5567d6f":"# Explore the data","5f652767":"# XGBoost","1a0d682c":"***Training the SVM model on the Training set***","b5ac5693":"***Correlation between variables***","6412280f":"***Confusion matrix and Accuracy score***","2b8126d8":"# Naive Bayes"}}