{"cell_type":{"31e22d04":"code","a9ee63fe":"code","31583b1e":"code","e7fde394":"code","f795afe0":"code","d4734ed5":"code","ce20ece5":"code","b9c0de37":"code","74d80c17":"code","650a70da":"code","6d4ea449":"code","f573b546":"code","73d7451a":"code","5010e75f":"code","2a61946a":"code","a03543b0":"code","32346e62":"code","b870f0bf":"code","bd00b2da":"code","4c7eb794":"code","20ff3895":"code","1728aeee":"code","caf61413":"code","f842c820":"code","db0c69b9":"code","54b1d68f":"code","c66ade1e":"code","16a84543":"code","fa5f1095":"code","0ce80d33":"code","cb9d7132":"code","8fa7b0db":"code","8f3abee2":"code","68be5881":"code","ac4831e0":"code","03de5298":"code","fec407fa":"code","611ba8de":"code","cb6ee153":"code","ca502c68":"code","ff52c248":"code","c7f06a94":"code","1ea529b2":"code","5e004a08":"code","c1b96996":"code","aa15840a":"code","4d307706":"code","386b4572":"code","b9eeea03":"code","9afbb511":"markdown","b6e2f61a":"markdown","0a38bbd4":"markdown","8eca97e5":"markdown","5ce42a0e":"markdown","f226054f":"markdown","bcf5e272":"markdown"},"source":{"31e22d04":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy as sp\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nfrom sklearn.cluster import KMeans","a9ee63fe":"df=pd.read_csv('..\/input\/usarrests\/USArrests.csv')\ndf.head()","31583b1e":"df.index","e7fde394":"df.size","f795afe0":"df.shape","d4734ed5":"# stil during 'Unnamed: 0' let's get out of the list\ndf.columns","ce20ece5":"# Let's see if I have any missing observations\ndf.isnull().sum()","b9c0de37":"df.rename( columns={'Unnamed: 0':'States'}, inplace=True )\ndf.rename( columns={'Rape':'Rape_rate'}, inplace=True )\ndf.rename( columns={'Murder':'Murder_rate'}, inplace=True )\n","74d80c17":"# data.rename( columns={'Unnamed: 0':'new column name'}, inplace=True )","650a70da":"df.columns","6d4ea449":"df.set_index('States',inplace=True)","f573b546":"df.describe()","73d7451a":"df.index","5010e75f":"df.loc['North Carolina','Murder_rate']","2a61946a":"df.columns = list(map(str, df.columns))","a03543b0":"df.columns","32346e62":"df.index","b870f0bf":"df.dtypes","bd00b2da":"df[['Assault', 'UrbanPop']] = df[['Assault', 'UrbanPop']].astype('float')","4c7eb794":"df.dtypes","20ff3895":"df = df\/np.max(df)","1728aeee":"df.describe().T","caf61413":"# here it is appropriate to visualize the data to try to understand it.\n# For example, 3 Assault (Assault) histogram looks like 3 peaks or accumulation.\ndf.hist(figsize = (10,10));","f842c820":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 4)\nkmeans","db0c69b9":"df.set_index(['States'],inplace =True)\nk_fit = kmeans.fit(df)","54b1d68f":"k_fit.n_clusters","c66ade1e":"k_fit.cluster_centers_","16a84543":"# If I want to visualize, now let's reduce the set numbers to 2\nkmeans = KMeans(n_clusters = 2)\nk_fit = kmeans.fit(df)        ","fa5f1095":"sets = k_fit.labels_","0ce80d33":"# Let's visualize the data we reduced to 2 sets.\n\nplt.scatter(df.iloc[:,0], df.iloc[:,1], c = sets, s = 50, cmap = \"viridis\")\n\ncenters = k_fit.cluster_centers_                                 # We want to create 2 centers and show them on the visual.\n\nplt.scatter(centers[:,0], centers[:,1], c = \"black\", s = 200, alpha = 0.5);","cb9d7132":"# Let us import 3D visualization. Otherwise it is necessary to download\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n# Let's create our sets again, this time it will be 3 dimensional variable\n\nkmeans = KMeans(n_clusters = 3)\nk_fit = kmeans.fit(df)\nsets = k_fit.labels_\ncenters = kmeans.cluster_centers_","8fa7b0db":"plt.rcParams['figure.figsize'] = (16, 9)\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2]);","8f3abee2":"# Let's look at the visualization of these sets and centers on the plot.\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2], c=sets)\nax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], \n           marker='*', \n           c='#050505', \n           s=1000);","68be5881":"# To provide cluster numbers and information about which states (observations) these numbers belong to\n# If we want, we can take the model with 2 variables or 3 variables above, let's take the 3 ones\n\nkmeans = KMeans(n_clusters = 3)\nk_fit = kmeans.fit(df)\nsets = k_fit.labels_","ac4831e0":"# to see which set and index you have for the top 10 states\n\npd.DataFrame({\"Provinces\" : df.index, \"Sets\": sets})[0:15]","03de5298":"# to look at the set number that each belongs to\n\ndf[\"set_no\"] = sets\n\ndf.head()","fec407fa":"df[\"set_no\"] = df[\"set_no\"] + 1\n\ndf.head()","611ba8de":"# It is called one from 2 to 50. The number of sets should decrease, because we should approach zero, because we reduce the sands.\n# When you have 10 thousand customers, you are not interested in 100 people. It is necessary to put the customers with high degrees or features into a segment(sets).\n#!pip install yellowbrick\nfrom yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisualizer = KElbowVisualizer(kmeans, k=(2,50))\nvisualizer.fit(df) \nvisualizer.poof()  \n\n# We understand the presentation from gorsel each point segment (set), ie, the set of elements with similar properties in it\n# For example, when the customer enters our site, a presentation can be made about what the monthly income it brings to us.","cb6ee153":"# Let's take our model above again\n# To provide cluster numbers and information about which states (observations) these numbers belong to\n# If we want we can take the model with 2 variables or 3 variables above, let's take the 4 normal ones\n\nkmeans = KMeans(n_clusters = 4)\nk_fit = kmeans.fit(df)\nsets = k_fit.labels_","ca502c68":"# to see which set and index you have for the top 10 states\n\npd.DataFrame({\"Provinces\" : df.index, \"Sets\": sets})[0:10]","ff52c248":"df = pd.read_csv(\"\/Users\/raufsafarov\/Downloads\/US_violent_crime.csv\").copy()\ndf.index = df.iloc[:,0]\ndf = df.iloc[:,1:5]\n#del df.index.name\ndf.index.name = \"Index\"\ndf.head()","c7f06a94":"from scipy.cluster.hierarchy import linkage\n\nhc_complete = linkage(df, \"complete\")\nhc_average = linkage(df, \"average\")\nhc_single = linkage(df, \"single\")","1ea529b2":"# We can watch its features and see what it does\n\ndir(hc_complete)","5e004a08":"# We need to create Dendogram\n\nfrom scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15, 10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexs')\nplt.ylabel('Distance')\ndendrogram(\n    hc_complete,\n    leaf_font_size=10\n);","c1b96996":"# another form of representation and the number of elements below it\n\nfrom scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15, 10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexs')\nplt.ylabel('Distance')\ndendrogram(\n    hc_complete,\n    truncate_mode = \"lastp\",\n    p = 4,\n    show_contracted = True\n);","aa15840a":"from scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15, 10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexs')\nplt.ylabel('Distance')\nden = dendrogram(\n    hc_complete,\n    leaf_font_size=10\n);","4d307706":"# When we look at the dendogram, it will be logical to divide it into 4 large clusters. Then we say n_cluster = 4.\n\nfrom sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters = 4, \n                                  affinity = \"euclidean\", \n                                  linkage = \"ward\")\n\ncluster.fit_predict(df)","386b4572":"# if we want to see which state is in which bank\n\npd.DataFrame({\"Provinces\" : df.index, \"Sets\": cluster.fit_predict(df)})[0:10]","b9eeea03":"df[\"set_no\"] = cluster.fit_predict(df)\ndf.head()","9afbb511":"# Optimum Set Number","b6e2f61a":"# Violent Crime in USA\n","0a38bbd4":"## Exploratory Data Analysis","8eca97e5":"## K-Means Model and Visualization","5ce42a0e":"## Normalization","f226054f":"## Hierarchical Clustering","bcf5e272":"## Visualization"}}