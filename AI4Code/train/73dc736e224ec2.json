{"cell_type":{"f6042cbe":"code","31e53da1":"code","3a73ee25":"code","33064f70":"code","28d8328c":"code","c508c6ae":"code","e6a2d51b":"code","90a2451e":"code","739653c8":"code","2d818b87":"code","1dc91853":"code","ea1b69d5":"code","ce895e4f":"code","36fd21d3":"markdown","6729484e":"markdown","4cdcfef8":"markdown","f395315b":"markdown","98993165":"markdown","3b3cdf2d":"markdown","b7adf310":"markdown","21204ed2":"markdown","df3881ed":"markdown","84083f53":"markdown","4438823c":"markdown","ec8dcaab":"markdown"},"source":{"f6042cbe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","31e53da1":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\n\n# listed on the data page\ncategorical_features = [\"ProductCD\", \"card1\", \"card2\", \"card3\", \"card4\", \"card5\", \"card6\",\n                        \"addr1\", \"addr2\", \"P_emaildomain\", \"R_emaildomain\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\",\n                        \"M6\", \"M7\", \"M8\", \"M9\", \"DeviceType\", \"DeviceInfo\", \"id_12\", \"id_13\", \"id_14\",\n                        \"id_15\", \"id_16\", \"id_17\", \"id_18\", \"id_19\", \"id_20\", \"id_21\", \"id_22\", \"id_23\",\n                        \"id_24\", \"id_25\", \"id_26\", \"id_27\", \"id_28\", \"id_29\", \"id_30\", \"id_31\", \"id_32\", \n                        \"id_33\", \"id_34\", \"id_35\", \"id_36\", \"id_37\", \"id_38\"]","3a73ee25":"#\u00a0looking at the head of the training data you can see a significant number of NaN values.\n\ntrain.head()","33064f70":"isna = train.isna().sum(axis=1)\nisna_test = test.isna().sum(axis=1)","28d8328c":"plt.hist(isna, normed=True, bins=30, alpha=0.4, label='train')\nplt.hist(isna_test, normed=True, bins=30, alpha=0.4, label='test')\nplt.xlabel('Number of features which are NaNs')\nplt.legend()","c508c6ae":"plt.plot(train['TransactionDT'], isna, 'r.', label='train', markersize=2)\nplt.plot(test['TransactionDT'], isna_test, 'b.', label='test', markersize=2)\nplt.legend()\nplt.xlabel('Transaction DT')\nplt.ylabel('Number of NaNs')\n\nplt.axvline(1e7, color='gray', ls='--')\nplt.axvline(2.2e7, color='gray', ls='--')\nplt.axvline(2.5e7, color='gray', ls='--')\n","e6a2d51b":"_ = plt.hist(isna[(train['TransactionDT']>1e7)], bins=50, normed=True, alpha=0.4, label='Train')\n_ = plt.hist(isna_test[(test['TransactionDT']<2.2e7)], bins=50, normed=True, alpha=0.4, label='Test - early time')\n_ = plt.hist(isna_test[(test['TransactionDT']>2.5e7)], bins=50, normed=True, alpha=0.4, label='Test\u00a0- late time')\n\nplt.legend()\nplt.xlabel('Number of NaNs for training instance')","90a2451e":"training_missing = train.isna().sum(axis=0) \/ train.shape[0] \ntest_missing = test.isna().sum(axis=0) \/ test.shape[0] \n","739653c8":"change = (training_missing \/ test_missing).sort_values(ascending=False)\nchange = change[change<1e6] # remove the divide by zero errors","2d818b87":"change","1dc91853":"fig, axs = plt.subplots(ncols=2)\n\ntrain_vals = train[\"D15\"].fillna(-999)\ntest_vals = test[test[\"TransactionDT\"]>2.5e7][\"D15\"].fillna(-999) # values following the shift\n\n\naxs[0].hist(train_vals, alpha=0.5, normed=True, bins=25)\n    \naxs[1].hist(test_vals, alpha=0.5, normed=True, bins=25)\n\n\nfig.set_size_inches(7,3)\nplt.tight_layout()","ea1b69d5":"isna_df = pd.DataFrame({'missing_count':isna,'isFraud':train['isFraud']})","ce895e4f":"plt.plot(isna_df.groupby('missing_count').mean(), 'k.')\nplt.ylabel('Fraction of fradulent transactions')\nplt.xlabel('Number of missing variables')\nplt.axhline(0)","36fd21d3":"Clearly, there are certain numbers of missing data which correlates with a an increased chance of the transaction being fradulent. \n\nWhere there is a 100% chance of the transaction being fradulent this corresponds to there being only one training instance with this number of missing variables.\n\n\nWe will have to investigate which features are missing and what this can tell us about the transaction.","6729484e":"# Missing data\n\nThis kernel aims to do some exploratory analysis of the missing data in the training and test sets. \n\n## Key findings:\n\n- The distribution of missing values is different between the training and test set (Figure 1).\n- There seems to be a distinct point in time when this distribution changes (Figures 2 and 3).\n- We have identified the main features where this change is occuring. \n\n\nThe different distributions could affect models ability to generalize well to the test set, and so the missing data will have to be appropiately handled.","4cdcfef8":"## Overview of the missing values per training instance \n\nThe graph below (Figure 1), displays a histogram of the number of NaN (missing) values for a given training instance. Overall it shows there are, on average, a greater number of missing values in the training set than compared to the test set. ","f395315b":"## Looking at the distribution of values for a feature which changes significantly.\n\nWe will look at D15. ","98993165":"### Temporal split in missing data\n\nIn Figure 2 (below) you can clearly see there is shift in the number of missing values per training instance partially through the test set data (I've confirmed this with additional moving average plots - not shown). \n\nThis could reflect a change in recording practices or a temporal dependence for more values to be missing (e.g., some seasonal dependence to the transactions?)","3b3cdf2d":"## Which features have reduced in the number of missing values\n\nWe can see there are some features with a lot less missing values in the test sets (namely D12 and a number of the VI features).\n\nWe will have to look into what these features are and how we can deal with the missing data. \n\n","b7adf310":"### Figure 2\n\nThis is a scatter plot of the number of missing values for each instance as a function of 'time', for both the training and test sets.  \n\nThe dashed grey lines help to guide the eyes to the regions used in the histograms of Figure 3","21204ed2":"## Does more missing data increase the chance of a Fradulent transaction","df3881ed":"### Figure 3\n\nNext we look at histograms of the number of missing data entries for each instance at three key time periods: during the training set, before the 'step' in the test set and after the 'step' in the test set. \n\nIt is clear that the train and test at early times appear equivalent, but after approximately Transaction DT = 2.3e7 the test set changes: there are signifcantly less missing entries for a given instance of the test set.","84083f53":"This notebook will originally focus on the number of NaNs (i.e., missing values) for each instance (row) in training and tests set.","4438823c":"# Work in progress","ec8dcaab":"### Figure 1\n\nThis shows the distribution of the number of missing values for the training and test sets. "}}