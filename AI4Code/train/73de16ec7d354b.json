{"cell_type":{"cb596b47":"code","e862672f":"code","d51aa8c4":"code","5e14b82f":"code","3cf37657":"code","c871f6b1":"code","9179953c":"code","bf30e861":"code","a4d6b97e":"code","e2ac6950":"code","19d76b1f":"code","530a585a":"code","b492fa38":"markdown","02d11d31":"markdown","4b7f44c0":"markdown","a49f5e7a":"markdown","17428a5f":"markdown","4fae7a35":"markdown","55938822":"markdown","58370151":"markdown","56029689":"markdown","50fe2c08":"markdown","ec716925":"markdown","ae637187":"markdown","5620695a":"markdown","912802b3":"markdown","2a3784a3":"markdown"},"source":{"cb596b47":"#Importing necessary libraries. I will explain the different layers later.\nimport matplotlib.pyplot as plt\nimport numpy as np; np.random.seed(1337) #Setting random seed for reproducible results\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dense, Activation, Flatten, Reshape, Input\nfrom tensorflow.keras import Model\nimport tensorflow.keras.backend as K\nprint(\"Tensorflow Version \" + tf.__version__)","e862672f":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","d51aa8c4":"#Loading data - we don't want the training labels\nfrom keras.datasets import mnist\n(x_train, _), (x_test, _) = mnist.load_data()","5e14b82f":"#Normalizing the data between 0 and 1\nimage_size = x_train.shape[1]\nx_train = np.reshape(x_train, [-1, image_size, image_size, 1])\nx_test = np.reshape(x_test, [-1, image_size, image_size, 1])\nx_train = x_train.astype('float32') \/ 255\nx_test = x_test.astype('float32') \/ 255\n\n# Generate corrupted MNIST images by adding noise with normal distribution centered at 0.5 and std=0.5\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\nx_train_noisy = x_train + noise\nnoise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\nx_test_noisy = x_test + noise\n\n#Even after noise, we don't want values below 0 or more than 1, hence we will \nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n","3cf37657":"#Defining how small the middle compression layer should be\nlatent_dim = 16","c871f6b1":"#Building the Encoder - Same structure as standard CNN\n\nenc_input = Input(shape=(28, 28, 1))\n\nenc = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='Same', activation='relu')(enc_input)\nenc = Conv2D(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='Same', activation='relu')(enc)\nenc = Conv2D(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='Same', activation='relu')(enc)\n\nenc_shape = K.int_shape(enc)\n\nenc = Flatten()(enc)\n\n#Note that here we want activation to be linear. This is because the denoiser will take in a linear input, and bring it back to image data.\n#Using sigmoid or relu will lose more information making it harder for the model to learn.\nenc = Dense(latent_dim)(enc)\n\nencoder = Model(inputs=enc_input, outputs=enc)\nencoder.summary()","9179953c":"#Building the Decoder - We will use Conv2D Transpose instead. Otherwise, Model will be identical.\n\ndec_input = Input(shape=(latent_dim,))\n\ndec = Dense(enc_shape[1] * enc_shape[2] * enc_shape[3])(dec_input)\ndec = Reshape((enc_shape[1], enc_shape[2], enc_shape[3]))(dec)\n\n#Note the descending number of filters\ndec = Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), padding='Same', activation='relu')(dec)\ndec = Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='Same', activation='relu')(dec)\ndec = Conv2DTranspose(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='Same', activation='relu')(dec)\n\n#Final conv2dtranspose layer to reconstruct image. We are using sigmoid activation since image is normalized between 0 and 1.\n#If image was between -1 and 1, we would use tanh activation\ndec = Conv2DTranspose(filters=1, kernel_size=(3, 3), padding='Same', activation='sigmoid')(dec)\n\ndecoder = Model(inputs=dec_input, outputs=dec)\ndecoder.summary()","bf30e861":"#Building Full Denoising AutoEncoder\n#Output of encoder is input to decoder.\ndenoiser = Model(inputs=enc_input, outputs=decoder(encoder(enc_input)))\n\n#Using Adam Optimizer with default values (learning_rate = 0.001)\nfrom tensorflow.keras.optimizers import Adam\nopt = Adam()\n\n#Loss should be mean squared error, since we want the generated output to be as close to original image.\n#We also want to penalize large deviations more than small deviations.\ndenoiser.compile(loss='mse', optimizer=opt)\n\ndenoiser.summary()","a4d6b97e":"#Training Final AutoEncoder\n\ndenoiser.fit(x_train_noisy, x_train, validation_data=(x_test_noisy, x_test), epochs=30, batch_size=500)","e2ac6950":"#Retreving Model's predictions for the test data.\nrecovered = denoiser.predict(x_test_noisy)","19d76b1f":"c = 1\nfig=plt.figure(figsize=(8, 8))\n\nfor i in range(10):  #This only plots the first 10 images. You can change it to a larget number for more results\n    for j in range(3):\n        if j == 0:\n            img = x_test[i, :, :].reshape(28, 28)\n        elif j == 1:\n            img = x_test_noisy[i, :, :].reshape(28, 28)\n        elif j == 2:\n            img = recovered[i, :, :].reshape(28, 28)\n        else:\n            pass\n\n        fig.add_subplot(10, 3, c)\n        plt.imshow(img, cmap='gray')\n        c += 1\nprint(\"  Real              Noisy              Denoised\")\nplt.show()","530a585a":"#Saving Weights\ndenoiser.save(\"denoiser.h5\")\n\n#If you don't want to train the model again, you can just use the weights of this model (in the outputs section). Uncomment below line to load in these weights for fresh model.\n#denoiser.load_weights(filepath='.\/denoiser.h5')","b492fa38":"First let's see if the GPU is enabled and if tensorflow is able to locate it.","02d11d31":"Wow, these results are genuinely good. The images are extremely noisy, yet the autoencoder is able to clean them with no problem.\n\nHowever, to be fair, this is bit of a stretch. The autoencdoer actually modifies the number sometimes (look at the 5 - second last picture - the outline has changed). If you do more digging, you will see that sometimes, a noise 4 will look like a 9 after being clean (the autoencoder adds a small bar connecting the two ends of the 4). Occasionally, the autoencoder also generates faded images, so the number is barely visible.\n\nBut given the amount of noise, this model is actually quite good even on unseen data!","4b7f44c0":"# Encoder","a49f5e7a":"Now let's plot our model's output. However, to make it clear, let's first plot the original image, then the noisy image we generated, and then the filtered image outputed by the AutoEncoder.","17428a5f":"# Final AutoEncoder","4fae7a35":"# Further Applications\n\nI am also experimenting with the idea of using denoising autoencoders to combat adversarial examples, since adversarial examples are also essentially noisy images (where the noise is selected in a carefull way). This is the link to the [github code for that project](https:\/\/github.com\/Yushgoel\/AdversarialAttack\/blob\/master\/adversarial_attack.ipynb) (You should scroll to the bottom for the denoising autoencoder use case).","55938822":"Now that the model architecture is complete, we just need to train it. We will let it run for 30 epochs, although 20 - 25 should do good enough. \nWe don't want the model to overfit to noise in every update and make slow progress, so we will use a batch size of 500.","58370151":"Now let's move on to build the encoder and decoder. \n\nThe encoder will have 3 convolutional layers, with increasing number of filters but decreasing image size (like a standard CNN). We will then flatten it and feed it into a Dense layer, which will finally compress the information into a smaller number of neurons.","56029689":"# Introduction\n\nAuto Encoders are by far one of the most powerful deep learning models that can be used for a variety of purposes. Over time, many kinds of Auto Encoders have come out\n\n1. Standard AutoEncoders - Can be used to compress and decompress information (including images)\n2. Variational AutoEncoders - Can be used to generate images like GANs.\n3. Denoising AutoEncoders - Can be used to remove noise from images, making them clearer. (They have a much wider use case such as increasing resolution of images as well).\n\nI was suprised to see a low mention of denoising autoencoders in the MNIST challenge on kaggle, so I decided to make a notebook on this topic using keras. \n\n# Why Denoising AutoEncoders\n\nThe obvious use case of denoising autoencoders is to, well, clean noisy images. \n\nHowever, the same model (with different training and testing data) can also be used to increase the resolution of blurry images. Then even photos taken by an amateur on a low resolution camera can be turned into stunning hig res images!\n\nI have also mentioned another unique use case that I am working on at the end of the notebook\n\nI am using a simple MNIST dataset as an example, so that it makes a simple tutorial, although it can (and should) be used on other datasets and real life problems as well. I hope you will learn how to implement your own Denoising Autoencoder through this notebook.\n\n\n# How Do AutoEncoders Work\n![autoencoder.png](attachment:autoencoder.png)\n\n(Image from towardsdatascience)\n\nAs seen from the image, high dimensional input data is fed to the model, which encodes it to a small number of neurons (in this case 3). This part of the network is called the encoder.\nThe outputs of these 3 neurons is then fed to layers which increase the dimension of the data to the original size. This way, we are able to compress lots of information in just a few numbers, rather than needing data for every pixel (Think of this as a super compression method using deep learning!). This part is the decoder, which reconstructs the high dimensional data.\n\nLoss Function: The loss function essentially wants the encoder and decoder to cooperate such that the data reconstructed at the end, is as close as possible to the original input image. This way, we don't need any labels, because the training data is the label.\n\nAnd that's it!\n\n\nThese autoencoders can be used for denoising images (we will using convolutional layers instead since they work better on image data).\n\n# Implementation in Keras","50fe2c08":"As mentioned before, the training data is the target data as well, so when we load in the mnist, we will ignore the labels.","ec716925":"Now we will combine the two models to form the final Denoising AutoEncoder. In Keras this is simple, since we can easily pass the output of 1 model to the input of a second, and create another model out of that.","ae637187":"Thanks a lot for going through my entire notebook.\n\nI also used a lot of help from the implementation on the official keras documentation, although this model is deeper and has a few parameters changed.\n\nPlease upvote if you found it helpful.","5620695a":"The denoiser, as expected, will be an exactly flipped version of the encoder. One difference is that we will be using Conv2DTranspose instead of Conv2D. For simplicity, a Conv2DTranspose can be thought of as an inverse to the Conv2D layer, so it can scale back the image and add image elements (instead of understanding image elements).\n\nNote that the order of the filter sizes will also change. The encoder used 16 --> 32 --> 64. The decoder will use 64 --> 32 --> 16.\n\nAlso, the decoder will have an extra convolutional layer with sigmoid activation purely to reconstruct the image. Thus, there will only be one filter, and stride will also be 1.","912802b3":"Now, we want the model to filter noise, so the input images will be noisy. We will have to create these from the training data, and we will do that by adding random values to the data.","2a3784a3":"# Decoder"}}