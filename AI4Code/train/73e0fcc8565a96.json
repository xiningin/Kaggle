{"cell_type":{"269efbfe":"code","83b86919":"code","e4a1cce0":"code","f2b1e746":"code","e973ff94":"code","4ac22814":"code","62cc83fa":"code","fb7fc6a7":"code","ead91a99":"code","f50f700f":"code","b5977e8f":"code","396dd06c":"code","0bb387f5":"code","51c32fc7":"code","c0c4e4c0":"code","8a62ec43":"code","dd2cbe96":"code","58d4b2cd":"code","377631b5":"code","394d5d87":"code","c861beb6":"code","5896f5af":"code","f5260a1e":"code","81f72118":"code","1f2a04d3":"code","8f411e50":"code","9f763187":"code","73e13ad7":"code","5e065005":"code","6a7eafae":"code","57c44d59":"markdown","caa7a429":"markdown","f86a7d15":"markdown","58ae8ba3":"markdown","246b8a94":"markdown","5413362d":"markdown","fe6bdc34":"markdown","7863bf10":"markdown","ab729482":"markdown","8ac00861":"markdown","e3349563":"markdown","197a502a":"markdown","d9dff8b9":"markdown","b554f80c":"markdown","b1b413ae":"markdown","8a0bf3a1":"markdown"},"source":{"269efbfe":"#Install spacy & spacylangdetect to take only english articles \n!pip install spacy\n!pip install spacy-langdetect\n!mkdir output","83b86919":"import re\nfrom spacy_langdetect import LanguageDetector\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport json\nfrom nltk.corpus import stopwords\nfrom copy import deepcopy\nfrom langdetect import detect\nimport numpy as np\nimport os\nimport spacy\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport string  \nimport gensim\nfrom pprint import pprint\nfrom gensim.models.doc2vec import Doc2Vec\nfrom sklearn.neighbors import NearestNeighbors\nimport re","e4a1cce0":"nltk.download('stopwords')\nnltk.download('wordnet')\nnlp = spacy.load('en')\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)","f2b1e746":"\nclass CleanAndTokenizeText(BaseEstimator, TransformerMixin):\n    \n    def tokenizer(self, input_text):\n       tokens = re.split('\\W+', input_text)\n       return tokens\n\n    def remove_urls(self, input_text):\n        return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', input_text)\n    \n    def remove_punctuation(self, input_text):\n        # Make translation table\n        punct = string.punctuation\n        trantab = str.maketrans('', '', punct)\n        return input_text.translate(trantab)\n    \n    def remove_digits(self, input_text):\n        return re.sub('\\d+', '', input_text)\n    \n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords_and_non_latin_words(self, words):\n        stopwords_list = stopwords.words('english')\n        stopwords_list.append('al')\n        stopwords_list.append('et')\n        stopwords_list.append('also')\n        whitelist=[]\n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return clean_words\n    \n    def stemming(self, words):\n        porter = PorterStemmer()\n        stemmed_words = [porter.stem(word) for word in words]\n        return \" \".join(stemmed_words)\n\n    def lemma(self,words):\n        lemmatizer = WordNetLemmatizer()\n        stemmed_words = [lemmatizer.lemmatize(word) for word in words]\n        return stemmed_words \n        \n    def english_only(self, words):\n      english_words = []\n      for word in words:\n        if detect(word) == 'en':\n          english_words.append(word)\n      return english_words\n\n\ndef transform(text):\n  ct = CleanAndTokenizeText() \n  text_st = str(text)\n  clean_x = ct.remove_urls(text_st)\n  clean_x = ct.remove_punctuation(clean_x)\n  clean_x = ct.remove_digits(clean_x)\n  clean_x = ct.to_lower(clean_x)\n  clean_x = ct.tokenizer(clean_x)\n  clean_x = ct.remove_stopwords_and_non_latin_words(clean_x)\n  clean_x = ct.lemma(clean_x)\n  return clean_x","e973ff94":"#Reading Dataset\ndf_clean_biorxiv = pd.read_csv('..\/input\/covid19-challenge-dataset\/biorxiv_clean.csv')\ndf_clean_pmc = pd.read_csv('..\/input\/covid19-challenge-dataset\/clean_pmc.csv')\ndf_clean_ncu = pd.read_csv('..\/input\/covid19-challenge-dataset\/clean_noncomm_use.csv')\ndf_clean_cu = pd.read_csv('..\/input\/covid19-challenge-dataset\/clean_comm_use.csv')\n\n#Concatenate all datasets in one dataframe\nfinal_frames = [df_clean_biorxiv, df_clean_pmc, df_clean_ncu, df_clean_cu]\ndf_final = pd.concat(final_frames)\n\n#remove null data from title, texte, abstract columns\ndf_final['title'] = df_final['title'].fillna('')\ndf_final['text'] = df_final['text'].fillna('')\ndf_final['abstract'] = df_final['abstract'].fillna('')\n\n\n#detect language of articles using title\ndf_final['lang'] = df_final['title'].apply(lambda title : nlp(title)._.language['language'])","4ac22814":"df_final.head(3)","62cc83fa":"#Clean title, text and abstract \ndf_final['title_tokenized'] = df_final['title'].apply(lambda x : transform(x))\ndf_final['text_tokenized'] = df_final['text'].apply(lambda x : transform(x))\ndf_final['abstract_tokenized'] = df_final['abstract'].apply(lambda x : transform(x))","fb7fc6a7":"#Combine title, text, and abstract\ndf_final['complete_text_tokenized'] = df_final['title_tokenized'] + df_final['text_tokenized'] + df_final['abstract_tokenized']\n#Take only entries with more than 200 keywords\ndf_final = df_final[df_final['complete_text_tokenized'].map(len) > 200]\n#Take only english entries\ndf_final = df_final[df_final['lang'] == 'en']","ead91a99":"#Describing our final dataframe.\ndf_final['complete_text_tokenized'].describe\n","f50f700f":"#Prepare corpus \ndef read_corpus(df, column):\n    for i, line in enumerate(df[column]):\n        yield gensim.models.doc2vec.TaggedDocument(line, [i])\n\n\n#Take 100 % of our dataset\ntrain_df  = df_final.sample(frac=1, random_state=42)\n\n#train corpus\ntrain_corpus = (list(read_corpus(train_df, 'complete_text_tokenized'))) ","b5977e8f":"# Doc2VEC : using distributed memory model\nmodel = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=300, min_count=10, epochs=20, seed=42, workers=10)\nmodel.build_vocab(train_corpus)\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)","396dd06c":"#Task 1 \ntask_1 = \"improve clinical processes disease models including animal models varies across age personal protective equipment monitor phenotypic change inform decontamination efforts infected person implementation community settings role movement control strategies community settings effectiveness virus immune response prevent secondary transmission immunity effectiveness control range transmission tools stainless steel reduce risk provide information potential adaptation physical science natural history nasal discharge long individuals health status health care fecal matter environmental survival different materials charge distribution affected areas viral shedding asymptomatic shedding phobic surfaces infection prevention incubation periods environmental stability disease virus transmission shedding surfaces stability infection incubation usefulness urine substrates studies sputum sources seasonality recovery products prevalence plastic persistence multitude learned know hydrophilic humans even environment diagnostics covid coronavirus copper contagious blood adhesion 19\"\ntask_2 = \"populations public health mitigation measures fatality among symptomatic hospitalized patients risk patient groups susceptibility potential risks factors smoking existing pulmonary disease co pregnant women socio epidemiological studies data environmental factors severity basic reproductive number 19 risk factors viral infections make existing respiratory behavioral factors including risk serial interval morbidities neonates incubation period transmission dynamics economic impact disease co infections transmission including economic whether virus virulent understand transmissible pre modes high effective differences covid could control\"\ntask_3 = \"temporal diverse sample sets sustainable risk reduction strategies behavioral risk factors nagoya protocol could test host range animal interface real understand geographic distribution whether farmers could animal host whether farmers livestock could determine whether whole genomes track variations southeast asia receptor binding rapid dissemination one strain mixed wildlife management measures livestock farms lateral agreements humans socioeconomic genomic differences genetic sequencing experimental infections epidemic appears virus genetics time tracking field surveillance continued spill virus origin geographic virus time surveillance spill origin therapeutics serve sars role reservoir played pathogen multi mechanism leveraged information inform infected human evolution evidence diagnostics development cov coronaviruses coordinating circulation access 2 \"\ntask_4 = \"investigate less common viral inhibitors methods evaluating potential complication could include identifying approaches alongside suitable animal models standardize challenge studies efforts develop prophylaxis clinical studies evaluate vaccine immune response develop animal models include antiviral agents healthcare workers approaches best animal models published concerning research may exert effects aid decision makers clinical effectiveness studies expanding production capacity universal coronavirus vaccine newly proven therapeutics viral replication evaluate risk alternative models therapeutics effectiveness production ramps vaccine recipients human vaccine evaluation efforts efforts targeted vaccination assays timely distribution predictive value ensure equitable distribute scarce dependent enhancement bench trials treat covid process development enhanced disease discover therapeutics 19 patients clinical vaccine efforts therapeutics disease discover development covid 19 vaccines use tried therapeutic prioritize populations need naproxen minocyclinethat exploration drugs developed determining conjunction clarithromycin capabilities antibody ade \"\ntask_5 = \"vary among different populations models health care delivery system capacity would include identifying policy policy changes necessary public health advice support real time give us time excellence could potentially critical government services health insurance status financial costs may social distancing approaches compare npis currently critical household supplies needed care health diagnoses critical shortfalls predict costs immigration status housing status employment status various sizes take account school closures rapid design rapid assessment qualified participants programmatic alternatives potential interventions pharmaceutical interventions people fail mobilize resources mitigate risks mass gatherings limited resources likely efficacy geographic location geographic areas gain consensus food distribution establish funding economic impact dhs centers coordinated way travel bans consistent guidance supplies social npis guidance bans ways want underserved treatment states spread scale respond research regardless race pay pandemic non methods leveraged lessen infrastructure individuals increase income implemented identified high factors experiments execution examine even equity enhance enable effectiveness disability control conduct comply compliance communities collaboration cases benefits barriers authorities authoritative age access ability \"\ntask_6 = \"epidemic preparedness innovations could provide critical funding help quickly migrate assays onto predict clinical outcomes )? make immediate policy recommendations g ., heavily trafficked companion species ), inclusive potential testers using pcr predict severe disease progression targeted surveillance experiments calling states might leverage universities ad hoc local interventions understanding best clinical practice public health surveillance perspective experiments could aid public health officials one health surveillance sufficient viral load streamlined regulatory environment published concerning systematic occupational risk factors improve response times host response markers collecting longitudinal samples determine asymptomatic disease existing surveillance platforms widespread current exposure existing diagnostic platforms rapid influenza test new diagnostic tests assay development issues detect early disease new platforms therapeutic interventions public ). potential sources local expertise best practices side tests ongoing exposure early detection care test diagnostic testing operational issues latency issues rapid design rapid bed unknown pathogens transmission hosts target regions supplies associated specific entity sampling methods rapidly sharing private sector private laboratories particular variant occurring pathogens national guidance mitigation measures market forces large scale holistic approaches holistic approach genetic drift future spillover future pathogens future diseases farmed wildlife extent possible explore capabilities evolutionary hosts enhance capabilities domestic food distinguishing naturally detection schemes defined area critical coupling genomics avoid locking allow specificity advanced analytics accelerator models testing purposes start testing mass testing rapid sequencing including swabs including legal including demographics technology roadmap technology crispr increase capacity enhance capacity specific reagents future coalition environmental sampling surveillance disease understanding states development detect testing including technology capacity sequencing reagents environmental coalition would virus use tradeoffs track terms tap support speed separation screening scaling role report recruitment recognizing protocols policies point people pathogen organism opportunities needed mutations mitigate mechanism like intentional instruments information important impact humans guidelines genome execution evolution ethical employ efforts efficacy effects e diagnostics devices developing denominators demographic data cytokines covid coordination communications biological bioinformatics barriers accuracy accessibility able 19 \"\ntask_7 = \"viral etiologies extracorporeal membrane oxygenation organ failure \u2013 particularly acute respiratory distress syndrome core clinical outcome set support skilled nursing facilities published concerning alternative methods long term care facilities time health care delivery published concerning surge capacity 19 patients outcomes data published concerning processes surge medical staff personal protective equipment might potentially work mechanical ventilation adjusted infection prevention control public health interventions published concerning efforts adjusted mortality data simple things people high flow oxygen based support resources across state boundaries supply chain management overwhelmed communities age g ., eua best telemedicine practices inform clinical care clinical trials efforts data across clinical outcomes outcomes data nursing homes medical care infected patients enhance capacity best practices clinical characterization trials efforts take care sick people inform allocation hospital flow care level adapt care supportive interventions evaluate interventions workforce protection workforce allocation specific actions scarce resources save thousands risk factors regulatory standards possible cardiomyopathy oral medications natural history n95 masks maximize usability innovative solutions elastomeric respirators done manually determine adjunctive critical challenges crisis standards cardiac arrest address shortages manage disease disease management extrapulmonary manifestations published care 19 outcomes efforts resources management g age disease manifestations without within way virus use transmission technologies steroids remove real range production payment organization mobilization limited knowledge know including improve home guidance frequency facilitating faciitators expand encouraging efficiency ecmo e develop define covid course could community clia barriers ards approaches application ai advise ability \"\ntask_8 = \"implementing public health measures affects systematically collect information related develop qualitative assessment frameworks published concerning ethical considerations integrated within multidisciplinary research translate existing ethical principles published concerning social sciences public health measures health seeking behaviors novel ethical issues support sustained education expanded global networks embed ethics across social sciences psychological health social media salient issues ethics efforts underlying drivers thematic areas surgical masks secondary impacts school closures rapid identification providing care operational platforms minimize duplication local barriers immediate needs fuel misinformation capacity building 19 patients oversight efforts 2019 efforts outbreak response measures research existing efforts outbreak use uptake team stigma standards srh rumor responding prevention physical particularly must modification includes identify g fear establish engage enablers e covid coordinate control connect burden articulate arise area anxiety adherence addressed access \"\ntask_9 = \"baseline public health response infrastructure preparedness modes sharing response information among planners local public health surveillance systems public health emergency response health care workers ). risk populations \u2019 families governmental public health public health capability misunderstanding around containment agendas incorporate attention indicates potential risk including academic ). understanding coverage policies clarify community measures data systems coordinate local risk populations information sharing nation \u2019 disadvantaged populations access information risk communication underrepresented minorities sectoral collaboration research priorities reach marginalized population groups mitigating threats mitigate gaps include targeting incarcerated people equity considerations data standards coordinating data assuring access action plan public target high standardized nomenclature mitigating barriers capacity relevant surveillance sharing ). information care understanding measures nomenclature high communication capacity barriers value understand treatment testing supported support state related recruit published providers profit problems private prevention others opportunities non needs need mitigation methods know investments inter integration inequity guidelines gathering funding follow federal expertise ensure elderly easy disease diagnosis covid communicating commercial citizens circumstances 19 \"\n\nlist_of_tasks = [task_1, task_2, task_3, task_4, task_5, task_6, task_7, task_8, task_9]\n\ndef get_doc_vector(doc):\n    tokens = transform(doc) \n    vector = model.infer_vector(tokens)\n    return vector\n\narray_of_tasks = [get_doc_vector(task) for task in list_of_tasks]\n","0bb387f5":"train_df['complete_text_vector'] = [vec for vec in model.docvecs.vectors_docs]\ntrain_array = train_df['complete_text_vector'].values.tolist()\n\n#Apply KNN to extract 50 neighbors\nball_tree = NearestNeighbors(algorithm='ball_tree', leaf_size=20).fit(train_array)\ndistances, indices = ball_tree.kneighbors(array_of_tasks, n_neighbors=50)\n","51c32fc7":"df_output = pd.DataFrame(columns=['Task','Result_Paper_ID','complete_text_tokenized'])\n\nfor i, info in enumerate(list_of_tasks):\n    df =  train_df.iloc[indices[i]]\n    dist = distances[i]\n    papers_ids = df['paper_id']\n    titles = df['title']\n    complete_texts_tokenized = df['complete_text_tokenized']\n    for l in range(len(dist)):\n        df_output = df_output.append({'Task': i, 'Result_Paper_ID' : papers_ids.iloc[l], 'complete_text_tokenized' : complete_texts_tokenized.iloc[l]}, ignore_index=True)","c0c4e4c0":"df_output.to_csv('.\/output\/final_output.csv', sep=',', encoding='utf-8')\n","8a62ec43":"import pandas as pd\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom ast import literal_eval\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nnltk.download('wordnet')","dd2cbe96":"df_output_results = pd.read_csv('..\/input\/covid19-challenge-dataset\/final_output.csv')\ndf_output_results['complete_text_tokenized'].describe()","58d4b2cd":"%matplotlib inline \nstopwords = set(STOPWORDS)\nnew_stopwords = ['copyright', 'dq', 'license', 'display', 'author', 'preprint', 'patient', 'authorfunder','ef','using', 'new', 'set', 'yet', 'fully', 'expected', 'medrxiv', 'available', 'granted','futhermore']\nnew_stopwords_list = stopwords.union(new_stopwords)\nlem = WordNetLemmatizer()","377631b5":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=new_stopwords_list,\n        max_words=200,\n        max_font_size=40, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n  \n    plt.imshow(wordcloud)\n    plt.show()","394d5d87":"df_task1 = df_output_results['complete_text_tokenized'][0:10] # What is known about transmission, incubation, and environmental stability?\ndf_task2 = df_output_results['complete_text_tokenized'][50:60] # What do we know about COVID-19 risk factors?\ndf_task3 = df_output_results['complete_text_tokenized'][100:110] # What do we know about virus genetics, origin, and evolution?\ndf_task4 = df_output_results['complete_text_tokenized'][150:160] # What do we know about vaccines and therapeutics?\ndf_task5 = df_output_results['complete_text_tokenized'][200:210] # What has been published about medical care?\ndf_task6 = df_output_results['complete_text_tokenized'][250:260] # What do we know about non-pharmaceutical interventions?\ndf_task7 = df_output_results['complete_text_tokenized'][300:310] # What do we know about diagnostics and surveillance?\ndf_task8 = df_output_results['complete_text_tokenized'][350:360] # What has been published about ethical and social science considerations?\ndf_task9 = df_output_results['complete_text_tokenized'][400:410] # What has been published about information sharing and inter-sectoral collaboration?","c861beb6":"liste1 = []\nfor el in df_task1 : \n  liste_of_keywords = set(literal_eval(el))\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste1.append(lem.lemmatize(el2))\n\nwordsT1 = ' '.join(liste1)\n\nliste2 = []\nfor el in df_task2 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste2.append(lem.lemmatize(el2))\n\nwordsT2 = ' '.join(liste2)\n\nliste3 = []\nfor el in df_task3 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste3.append(lem.lemmatize(el2))\n\nwordsT3 = ' '.join(liste3)\n\nliste4 = []\nfor el in df_task4 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste4.append(lem.lemmatize(el2))\n\nwordsT4 = ' '.join(liste4)\n\nliste5 = []\nfor el in df_task5 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste5.append(lem.lemmatize(el2))\n\nwordsT5 = ' '.join(liste5)\n\nliste6 = []\nfor el in df_task6 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste6.append(lem.lemmatize(el2))\n\nwordsT6 = ' '.join(liste6)\n\nliste7 = []\nfor el in df_task7 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste7.append(lem.lemmatize(el2))\n\nwordsT7 = ' '.join(liste7)\n\nliste8 = []\nfor el in df_task8 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste8.append(lem.lemmatize(el2))\n\nwordsT8 = ' '.join(liste8)\n\nliste9 = []\nfor el in df_task9 : \n  liste_of_keywords = literal_eval(el)\n  for el2 in liste_of_keywords:\n    if(nlp(el2)._.language['language'] == 'en'):\n      liste9.append(lem.lemmatize(el2))\n\nwordsT9 = ' '.join(liste9)","5896f5af":"show_wordcloud(wordsT1, title = 'Task : What is known about transmission, incubation, and environmental stability? - wordcloud (10 samples)')","f5260a1e":"show_wordcloud(wordsT2, title = 'Task : What do we know about COVID-19 risk factors? - wordcloud (10 samples)')","81f72118":"show_wordcloud(wordsT3, title = 'Task : What do we know about virus genetics, origin, and evolution? - wordcloud (10 samples)')","1f2a04d3":"show_wordcloud(wordsT4, title = 'Task : What do we know about vaccines and therapeutics? - wordcloud (10 samples)')","8f411e50":"show_wordcloud(wordsT5, title = 'Task : What has been published about medical care? - wordcloud (10 samples)')","9f763187":"show_wordcloud(wordsT6, title = 'Task : What do we know about non-pharmaceutical interventions? - wordcloud (10 samples)')","73e13ad7":"show_wordcloud(wordsT7, title = 'Task : What do we know about diagnostics and surveillance? - wordcloud (10 samples)')","5e065005":"show_wordcloud(wordsT8, title = 'Task : What has been published about ethical and social science considerations? - wordcloud (10 samples)')","6a7eafae":"show_wordcloud(wordsT9, title = 'Task : What has been published about information sharing and inter-sectoral collaboration? - wordcloud (10 samples)')","57c44d59":"# Training the model using Doc2Vec\n\nWe started by creating the train corpus on 100% of our dataframe.","caa7a429":"Stopwords are words that are filtered after processing the data. We also manually added irrelevant words. ","f86a7d15":"**Reloading our presaved output without having to go through every step above **\n\nThe file that we are going to use is a dataframe in which each line corresponds to an article, with in column the tasks (Task), the first 50 articles responding best to the task (Result_Paper_ID) as well as the text column resulting from the aggregation of the text of the article, the title and the abstract of the article (complete_text_tokenized).\n","58ae8ba3":"### We generate the output files containing 50 most relevent articles per task.","246b8a94":"Creating a text for each task containing all the tokens found in each row.\nLemmatization is a Text Normalization technique in the field of Natural Language Processing that is used to prepare text, words, and documents for further processing.","5413362d":"We are going to define a function which will display the word clouds in order to visualize the content of the text.","fe6bdc34":"# Preprocessing\n\n\nOur dataset is based on 3 columns : Title, Abstract, Full body text.\nIn this notebook, we focused only on english scientific papers. For this, we used the package [NLTK](https:\/\/www.nltk.org\/api\/nltk.html) to apply a filter on the title.\n\n\nOur data preprocessing consists in :\n\n* Remvoing URLs\n* removing digits\n* removing stopwords\n* set in lower case the data\n* Tokenizing the text\n* Lemmatizing the keywords.\n\n\nWe applied a filter to take only articles with more than 200 tokens, these tokens are the result of the concatenantion of tokens generated by the 3 columns cited above.\n\nAs a result of this step,we will train our Doc2Vec Model on 27 877 scientific papers.","7863bf10":"# Results\n\nBefore using the model, we first started by extracting relevant keywords phrases for each task using the package [Rake NLTK](https:\/\/pypi.org\/project\/rake-nltk\/) \n\nExample of the first task :\n\n**Input text** : *transmission, incubation, and environmental stability of COVID-19\nWhat do we know about natural history, transmission, and diagnostics for the virus\nWhat have we learned about infection prevention and control\nRange of incubation periods for the disease in humans and how this varies across age and health status and how long individuals are contagious, even after recovery.\nPrevalence of asymptomatic shedding and transmission.\nSeasonality of transmission.\nPhysical science of the coronavirus, charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding.\nPersistence and stability on a multitude of substrates and sources, nasal discharge, sputum, urine, fecal matter, blood.\nPersistence of virus on surfaces of different materials, copper, stainless steel, plastic.\nNatural history of the virus and shedding of it from an infected person\nImplementation of diagnostics and products to improve clinical processes\nDisease models, including animal models for infection, disease and transmission\nTools and studies to monitor phenotypic change and potential adaptation of the virus\nImmune response and immunity\nEffectiveness of movement control strategies to prevent secondary transmission in health care and community settings\nEffectiveness of personal protective equipment and its usefulness to reduce risk of transmission in health care and community settings\nRole of the environment in transmission *\n\n\n\n\n**Output text after applying Rake NLTK : **\n*improve clinical processes disease models including animal models varies across age personal protective equipment monitor phenotypic change inform decontamination efforts infected person implementation community settings role movement control strategies community settings effectiveness virus immune response prevent secondary transmission immunity effectiveness control range transmission tools stainless steel reduce risk provide information potential adaptation physical science natural history nasal discharge long individuals health status health care fecal matter environmental survival different materials charge distribution affected areas viral shedding asymptomatic shedding phobic surfaces infection prevention incubation periods environmental stability disease virus transmission shedding surfaces stability infection incubation usefulness urine substrates studies sputum sources seasonality recovery products prevalence plastic persistence multitude learned know hydrophilic humans even environment diagnostics covid coronavirus copper contagious blood adhesion 19\n*","ab729482":"We will just take a subset of data (10 samples) from the dataframe.","8ac00861":"****Turn our tasks in vectors ****","e3349563":"**** The following cell is used to train de model. ****","197a502a":"**Using WordCloud, we visualize our results in order to get the most frequent words for each task based on the 10 first  relevant scientific papers. **","d9dff8b9":"**We use KNN-ball-tree algorithm to extract the 50-nearest neighbors for each task**","b554f80c":"**Worldclouds for text field** : ","b1b413ae":"# Introduction\n\n\nIn response to the COVID-19 pandemic, a group of Data Scientists and Engineers at Atos France (Big Data Entity) have colloborated to respond to [COVID-19 Open Research Dataset Challenge (CORD-19)](https:\/\/https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge) using a free resource of over 51,000 scholarly articles, including over 40,000 with full text, about COVID-19 and the coronavirus family of viruses for use by the global research community.\n\nOur goal is to identify relevent articles based on questions given in tasks by implementing a Doc2Vec model trained on the title, abstract and full body text of COVID-19 research topics.\n\n***We included a pretrained Doc2Vec Model to skip the training steps as well as the output file containing the relevent topics  without having to run the preprocessing cells***\n\nBefore getting started, the first step will be to transform all the datasets which are Json files to CSV files.\nThen, we move to preprossessing of our data ","8a0bf3a1":"## DATA VISUALIZATION"}}