{"cell_type":{"b1406e26":"code","e4187cdb":"code","3e1f0721":"code","db23381b":"code","12d52cbd":"code","e117b2ce":"code","2e021666":"code","526a9b86":"code","60891d1a":"code","94901bad":"code","fe744da8":"code","26436d90":"code","483ecf7e":"code","fbfd702c":"code","f0a98d30":"code","6eaa1eae":"code","9dc5756c":"code","f99f4e68":"code","45951e73":"code","204cf5d9":"code","25f98a9a":"code","5952a385":"code","3275bb50":"code","3c71672a":"code","9a05f863":"code","b16e4188":"code","cf5c5b6e":"markdown","8fb87024":"markdown","62790a97":"markdown","491bef6e":"markdown","6e9c2894":"markdown","2f311218":"markdown","7d80ab34":"markdown","f6dd2cce":"markdown","af427a70":"markdown","bf0f12f3":"markdown"},"source":{"b1406e26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4187cdb":"df=pd.read_csv(\"\/kaggle\/input\/social-network-ads\/Social_Network_Ads.csv\")","3e1f0721":"df.head()","db23381b":"df.columns","12d52cbd":"df=df.drop(columns=['User ID','Gender']).copy()","e117b2ce":"df.head()","2e021666":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(df.drop(\"Purchased\",axis=1),df[\"Purchased\"],test_size=0.3,random_state=0)","526a9b86":"x_train.shape ,x_test.shape","60891d1a":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\n#fit the data into scaler it will learn parameters\nscaler.fit(x_train)\n#transform train and test split into requirement for standardization\nx_train_scaled=scaler.transform(x_train)\nx_test_scaled=scaler.transform(x_test)","94901bad":"scaler.mean_","fe744da8":"x_train.columns","26436d90":"x_train_scaled= pd.DataFrame(x_train_scaled, columns=x_train.columns)\nx_train_scaled.head()","483ecf7e":"display(x_train_scaled.describe().round())\ndisplay(x_train.describe().round())","fbfd702c":"import seaborn as sns","f0a98d30":"sns.scatterplot","6eaa1eae":"x_train.columns","9dc5756c":"import matplotlib.pyplot as plt","f99f4e68":"fig,(ax1,ax2)=plt.subplots(nrows=1,ncols=2,figsize=(12,5))","45951e73":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,5))\nax1.set_title(\"Before Standardization\")\nsns.scatterplot(data=x_train,x=x_train['Age'],y=x_train['EstimatedSalary'],ax=ax1)\nax2.set_title(\"After Standardization\")\nsns.scatterplot(data=x_train_scaled,x=x_train_scaled['Age'],y=x_train_scaled['EstimatedSalary'],ax=ax2,color=\"red\")\n","204cf5d9":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,5))\nax1.set_title(\"Before Standardization\")\nsns.kdeplot(x_train['Age'],ax=ax1)\nsns.kdeplot(x_train['EstimatedSalary'],ax=ax1)\nax2.set_title(\"After Standardization\")\nsns.kdeplot(x_train_scaled['Age'],ax=ax2,legend=True)\nsns.kdeplot(x_train_scaled['EstimatedSalary'],ax=ax2,legend=True)\nplt.show()","25f98a9a":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nlr_scaled=LogisticRegression()","5952a385":"lr.fit(x_train,y_train)\nlr_scaled.fit(x_train_scaled,y_train)","3275bb50":"y_predict=lr.predict(x_test)\ny_predict_scaled=lr_scaled.predict(x_test_scaled)","3c71672a":"from sklearn.metrics import accuracy_score","9a05f863":"print(\"Actual\",accuracy_score(y_test,y_predict))\nprint(\"Scaled\",accuracy_score(y_test,y_predict_scaled))","b16e4188":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt_scaled = DecisionTreeClassifier()\n\ndt.fit(x_train,y_train)\ndt_scaled.fit(x_train_scaled,y_train)\n\ny_pred = dt.predict(x_test)\ny_pred_scaled = dt_scaled.predict(x_test_scaled)\nprint(\"Actual\",accuracy_score(y_test,y_pred))\nprint(\"Scaled\",accuracy_score(y_test,y_pred_scaled))","cf5c5b6e":"# > apply standardization in \n# 1)k nearest neighbour-due to euclidian distance\n# 2)k-means-due to euclidian distance\n# 3)artificial neural network-for gradient descent\n# 4)principle component analysis(PCA)-to get maximum variance\n# 5)Gradient Descent- theta cal will become faster\n# \n# > No need to apply on\n# 1)Decision tree\n# 2)Random forest\n# 3)XG Boost,Gradient Boost","8fb87024":"# Train test split","62790a97":"# Feature Scaling-Standardization\n* Technique to scale individual features and standardize them\n* also called as z score normalization\n* after standardization mean is nearly equal to zero and standard deviation to 1","491bef6e":"in standardization scale changes but the dist of cols are same before and after","6e9c2894":"# Comparison of Distributions","2f311218":"# No diff in accuracy after standardization","7d80ab34":"# Effect of Scaling","f6dd2cce":"# Big diff in accuracy after standardization","af427a70":"# Why scaling is important?","bf0f12f3":"# Standard scalar"}}