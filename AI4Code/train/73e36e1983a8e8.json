{"cell_type":{"30db8dd1":"code","ab8ea39a":"code","3b1970a0":"code","81f1aed8":"code","cdf770de":"code","37521ac6":"code","7ba0037c":"code","df197a89":"code","7d84821b":"code","cacc405e":"code","5acddd5b":"code","04695417":"code","58131382":"code","e24c348f":"code","610d270f":"code","4fccd7bf":"markdown","9e443618":"markdown","2d9bf5b8":"markdown","c25b9c1c":"markdown","b5d9d351":"markdown","0c3a48b9":"markdown"},"source":{"30db8dd1":"# Install required packages\n! pip install natsort","ab8ea39a":"import pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os \nimport cv2\nimport numpy as np\nimport glob\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\n#from torch.utils.data import Dataset, DataLoader\nimport SimpleITK as sitk\n\n# to sort file names by its order\nfrom natsort import natsorted\n%matplotlib inline\n\n# Global variables\nmodalities = ['FLAIR', 'T1w', 'T1wCE', 'T2w']","3b1970a0":"# Load y : labels & preds\ntrain_labels = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\npreds_labels = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv')\n#train_labels = pd.read_csv('..\/png_data\/train_labels.csv')\n#preds_labels = pd.read_csv('..\/png_data\/sample_submission.csv')\n\nprint(f'Number of patients = {len(train_labels)}'.format())\ntrain_labels.head()","81f1aed8":"# Reference: https:\/\/www.kaggle.com\/ayuraj\/brain-tumor-eda-and-interactive-viz-with-w-b\ndef ReadMRI(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    # min-max normalization\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    return data","cdf770de":"def load_imgs(idx, train=True): \n    images_idx = {}\n    for modal in modalities:\n        images_modal = []\n        if train:\n            file_path_list = glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train\/'+idx+'\/'+modal+'\/*')\n        else:\n            file_path_list = glob.glob('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/test\/'+idx+'\/'+modal+'\/*')\n            \n        # Should be sorted again. \n        file_path_list = natsorted(file_path_list)\n        \n        for file_path in file_path_list:\n            image = ReadMRI(file_path)\n            # In case, direc is empty!\n            if len(image) == 0:\n                print('yes')\n                image = np.zeros((1,256,256)) # pt no. 109(no FLAIR) & 123(no T1w) & 709(no FLAIR) >> excluded! \n            images_modal.append(image)\n        images_idx[modal] = np.array(images_modal)\n        \n    return images_idx","37521ac6":"%%time\n# Just to check how long would \"load_imgs\" be. \n# %%time \uc704\uc5d0 \uc8fc\uc11d \ub2ec\uba74 time \uccb4\ud06c \ubd88\uac00....!?\nfor i in range(1):\n    idx = str(train_labels.BraTS21ID[i]).zfill(5)\n    print(idx)\n    imgs = load_imgs(idx)\n    img_ = imgs[modalities[0]]#.mean(axis=0)\n    print(img_.shape)","7ba0037c":"# Again, just to check sample\nfig, ax = plt.subplots(2,2, figsize=(10,10))\nfor i in range(2):\n    for j in range(2):\n        m = ax[i,j].imshow(imgs[modalities[2*i+j]].mean(axis=0))\n        ax[i,j].set_title(modalities[2*i+j])\nplt.show()","df197a89":"# 1. Create a data loader with N4biasFieldCorrectionImageFilter\nclass PreprocessedImage2Dver():\n    def __init__(self, list_BraTS21ID, list_labels=None,\n                dim=(512, 512), n_modals=len(modalities), n_classes=2,\n                 num_slices_from_center:int=3,\n                 is_train=True, transform = None): # For single pt. \n        self.dim = dim\n        ### ????? 1. How to do batch norm for this task..?\n        #self.batch_size = batch_size\n        self.list_labels = list_labels\n        self.is_train = (list_labels is not None)\n        self.list_BraTS21ID = list_BraTS21ID\n        self.n_modals = n_modals # number of modalities\n        self.num_slices_from_center = num_slices_from_center\n        \n    \n    def __getitem__(self, index):\n        BraTS21ID_temp = self.list_BraTS21ID[index] #self.list_BraTS21ID[index*self.batch_size:(index+1)*self.batch_size] # index\ub85c \uc801\uc5b4\uc900 batch \ub9cc \uc9c4\ud589!\n        \n        X = self.__data_generation(BraTS21ID_temp)\n        \n        if self.is_train:\n            y = self.list_labels[index] #self.list_labels[index*self.batch_size:(index+1)*self.batch_size]\n            return np.array(X), np.array(y)\n        else:\n            return np.array(X)\n    \n    def __data_generation(self, BraTS21ID_temp):\n        new_imgs = np.zeros((self.num_slices_from_center*2,  *self.dim, self.n_modals))\n        #print(new_imgs.shape)\n        #new_imgs = None\n        \n        idx = str(BraTS21ID_temp).zfill(5)\n        imgs = load_imgs(idx, train=self.is_train) # imgs = {'FLAIR': ~, 'T1w': ~, 'T1wCE': ~} lib\ub2e4. \n        \n        index_modal = 0\n        for modal in modalities:\n            corrct_norm_imgs_modal = []\n\n            img_modal = imgs[modal] #.shape :  ex. (288, 256, 256) \n            \n            if img_modal.shape[0] < self.num_slices_from_center *2 :\n                print('The number of slice is smaller than total number of slices!!')\n                break\n            \n            central_slices = int(img_modal.shape[0]\/2)\n            \n            list_slices = list(range(central_slices-self.num_slices_from_center, central_slices+self.num_slices_from_center))\n            for slice_i in list_slices:\n                img_slice_i = cv2.resize(img_modal[slice_i,:,:], dsize= self.dim, interpolation = cv2.INTER_LINEAR) #???\n                img_slice_i_ndarray = np.array(img_slice_i, dtype = 'float32')\n\n                # 1. Removing radiofrequency inhomogeneity using N4BiasFieldCorrection\n                # ref : https:\/\/www.kaggle.com\/josepc\/rsna-effnet\n                inputImage = sitk.GetImageFromArray(img_slice_i_ndarray)\n                maskImage = sitk.GetImageFromArray((img_slice_i_ndarray>0.1)*1) #sitk.OtsuThreshold(inputImage, 0,1,200) \n                inputImage = sitk.Cast(inputImage, sitk.sitkFloat32) #?? \uc65c 32\ub85c?\n                maskImage = sitk.Cast(maskImage, sitk.sitkUInt8) #?? \uc65c 8\ub85c?\n                corrector = sitk.N4BiasFieldCorrectionImageFilter()\n                numberFittingLevels = 4 # ??\n                maxIter = 100 # ??\n                if maxIter is not None:\n                    corrector.SetMaximumNumberOfIterations([maxIter]*numberFittingLevels) # ??\n                corrected_image = corrector.Execute(inputImage, maskImage)\n                \n                corrected_image = sitk.GetArrayFromImage(corrected_image)\n                \n                # 2. Normalization (\uadfc\ub370 \uc55e\uc5d0 load_imgs\uc5d0\uc11c normalization term\uc774 \uc788\uae34 \ud588\uc74c.)\n                # ????? 2. \ud559\uc2b5 \uc18d\ub3c4\ub9cc \ud574\uacb0\ud558\uba74 \ub418\ub294\uac70\ub2c8, \uc5b4\ub5bb\uac8c\ub4e0 normalization\ub9cc \ud558\uba74 \ub418\ub294\uac78\uc9c0?\n                max_2D = np.amax(corrected_image) \n                min_2D = np.amin(corrected_image)\n                mean_2D = np.mean(corrected_image)\n                \n                normalized_corrected_image = (corrected_image-min_2D)\/(max_2D-min_2D)\n                \n                \n                corrct_norm_imgs_modal.append(normalized_corrected_image)\n                \n            corrct_norm_imgs_modal = np.array(corrct_norm_imgs_modal)\n            \n            new_imgs[:, :, :, index_modal] = corrct_norm_imgs_modal\n            \n            index_modal += 1\n        \n        return new_imgs # shape = (num_slices_from_center*2, *dim, n_modals) !!! not 3D. 4D.","7d84821b":"# Do train_test_split. \n# - Train : Test = 8:2 \/ Stratified random sampling \/ random_state = 42\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_labels.BraTS21ID, train_labels.MGMT_value,\n                                                 test_size = .2, random_state = 42, stratify = train_labels.MGMT_value)","cacc405e":"# ??????? 3. \uc6d0\ub798\uc758 W*H \uc0ac\uc774\uc988\ub97c \ubcf4\uc874\ud574\uc57c?\ndim = (256, 256) \ntrain_set = PreprocessedImage2Dver(X_train, y_train, dim=dim)\nval_set = PreprocessedImage2Dver(X_val, y_val, dim=dim)\ntest_set = PreprocessedImage2Dver(preds_labels.BraTS21ID,  dim = dim)","5acddd5b":"sample_preprocessed_imgs = train_set[1][0] # pt no.00002\n\ncenter_slice_idx = int(sample_preprocessed_imgs.shape[0]\/2)\n#print(center_slice_idx) # In this cas, returns 3. \nimg_at_center = sample_preprocessed_imgs[center_slice_idx] # slice at center","04695417":"# Check sampled space of sample_img (FLAIR ver). \nprint(img_at_center[100:150, 100:130, 0])","58131382":"# Again, just to check sample preprocessed image.\nfig, ax = plt.subplots(2,2, figsize=(10,10))\nfor i in range(2):\n    for j in range(2):\n        m = ax[i,j].imshow(img_at_center[:, :, 2*i+j], vmin=0, vmax=1)\n        ax[i,j].set_title(modalities[2*i+j])\nplt.show()","e24c348f":"sample_pt_ID = str(train_labels.BraTS21ID[1]).zfill(5)\n\nbefore_prepro_img = load_imgs(idx=sample_pt_ID)","610d270f":"print('Before preprocessing - pt no.00002')\n# Again, just to check sample\nfig, ax = plt.subplots(2,2, figsize=(10,10))\nfor i in range(2):\n    for j in range(2):\n        center_slice_idx = int(before_prepro_img[modalities[2*i+j]].shape[0]\/2)\n        #print(center_slice_idx)\n        m = ax[i,j].imshow(before_prepro_img[modalities[2*i+j]][center_slice_idx])\n        ax[i,j].set_title(modalities[2*i+j])\nplt.show()","4fccd7bf":"## 3. Check sample preprocessed image\n### - num_slices_from_center:int=3\n### - for \"pt no.00002\" \n### - for 2D slice image at center","9e443618":"## 1. Show sample","2d9bf5b8":"### Before preprocessing","c25b9c1c":"# Things to be solved..\n- pt no. 109(no FLAIR) & 123(no T1w) & 709(no FLAIR) should be excluded!\n- choice :  \n1. Normalization(for 2D slice? or 3D bulk image?) : 2 options\n2. Again Normlization(for Patch? or for overall 2D(or 3D) image?) : * 2 options\n3. Normalization method (mean-norm, zero-mean, unit-varia~) : * n options\n#### \uadf8\ub7f0\ub370! But all related to normalization.. (\ud559\uc2b5 \uc18d\ub3c4\ub97c \ube60\ub974\uac8c \ud558\ub290\ub0d0 \ucc28\uc774\ub2c8\uae4c...\ubb50\uac00 \ub418\uc5c8\ub358 normalization\uc744 \ud574\uc8fc\uae30\ub9cc \ud558\uba74 \ub418\ub294\uac70 \uc544\ub2cc\uc9c0?)","b5d9d351":"# 2. Preprocessing each image (per pt, modality)","0c3a48b9":"## 0. Load required libraries & csv files"}}