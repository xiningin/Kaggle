{"cell_type":{"84f72e7c":"code","a1206c3b":"code","8393da61":"code","8a75cbce":"code","ff75302f":"code","f9f767b6":"code","9d3d0693":"code","b80df079":"code","c71d1beb":"code","3f2cd694":"code","1031a80a":"code","c9fb98d3":"code","b4252970":"code","9fb1a862":"code","b6e88cb9":"code","8a8174a8":"code","095fe9ac":"code","2a569143":"code","b736fe0e":"code","7e02acd4":"code","eed43a71":"code","2e61b5ba":"code","027afc8d":"code","8a2c7b67":"code","41c66300":"code","2be5aa21":"code","7278cdbf":"code","601e5663":"code","fd0fdc60":"code","06fd555a":"code","8be4b031":"code","483b07a4":"code","1c0b4969":"code","b07dd699":"code","6f8c4a90":"code","34d06c7c":"code","2f151659":"code","05430866":"code","a1b97718":"code","962c8e95":"code","9b964f19":"code","b59eb136":"code","288154b6":"code","505cedc7":"code","164085aa":"code","272d5e97":"code","144d5d29":"code","1038e5c0":"code","435fede2":"code","ca1481b0":"code","a1d9fee9":"code","f2724bfa":"code","3ad87fe9":"code","c69967f9":"code","84b34354":"code","3b3e10ab":"code","55fcdf1c":"code","81e67adb":"code","a64cd645":"code","58a97108":"code","cf604fb2":"code","848fcff0":"code","a9081e8d":"code","8eb0f241":"code","34f75141":"code","9ca203f5":"code","29807587":"code","364ca74e":"code","b7e88897":"code","a1cc5abf":"code","06dcca6c":"code","acbbf934":"code","885b7ede":"code","b2c8426b":"code","3377c609":"code","39f8beae":"code","7266d60b":"code","b5c6d061":"code","39616e18":"code","f4264008":"code","9dd61de0":"code","55163734":"code","dbbe325b":"code","00c398ce":"markdown","d0721088":"markdown","fa0a1bf2":"markdown","23e4270e":"markdown","2ba2ba36":"markdown","32f293ea":"markdown","28a35102":"markdown","a8c3175f":"markdown"},"source":{"84f72e7c":"\n## data manupulation\nimport numpy as np\nimport pandas as pd\n\n## label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\n## for plotting\nimport matplotlib.pyplot as plt\n\n\n\n###### plotting confiduration dont focus on that its just a  configuration\nfig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 50\nfig_size[1] = 20\nplt.rcParams[\"figure.figsize\"] = fig_size\nplt.rcParams.update({'font.size': 50})","a1206c3b":"\n## read the data\ndf = pd.read_csv('..\/input\/data.csv')","8393da61":"df.head()\n","8a75cbce":"\n## init the encoder\nencoder = LabelEncoder()","ff75302f":"#3 encode the target\ntarget=encoder.fit_transform(df['class'])\n","f9f767b6":"## assign the encoded target toa  new column\ndf['num_class'] = np.array(target)\n","9d3d0693":"## we need 1 vs 1 multioutput classification\n## package for multioutput classifier\nfrom sklearn.multioutput import MultiOutputClassifier\n","b80df079":"## we deine the whole encoding a function to do it easily with other column\ndef encode(df):\n    encoder = LabelEncoder()\n    target=encoder.fit_transform(df)\n    return np.array(target)","c71d1beb":"## encode the protocol\nnum_proto = encode(df['protocol_type'])","3f2cd694":"## save with a another name \ndf['num_proto']  = num_proto\nservice_num = encode(df['service'])\ndf['service_num'] = service_num\nflag_num = encode(df['flag'])\ndf['flag_num'] = flag_num","1031a80a":"df.head()","c9fb98d3":"df.corr()['num_proto'].plot()\n","b4252970":"df_working = df[['duration','dst_bytes','wrong_fragment','num_failed_logins','logged_in','num_compromised','su_attempted','num_root','num_file_creations','num_shells','num_access_files','is_guest_login','srv_count','same_srv_rate','srv_diff_host_rate','dst_host_same_srv_rate','num_proto','flag_num','num_class']]","9fb1a862":"## working dataset .the data set we work with\ndf_working.head()","b6e88cb9":"df_working.corr().plot()","8a8174a8":"df_working.corr()['num_class']","095fe9ac":"\n##dropping my two target\nX = df_working.drop(['num_class'], axis=1)\nX = X.drop(['num_proto'], axis=1)\n\n\n## y1 is our main target\ny1 = df_working[['num_class','num_proto']]\n\n# this is a single target for plotting putpose\ny2 = df_working[['num_class']]\n\n\n## two working label in necessayy because we want to find the first best param | independently\n## why we take another one i mean y2 cause we cant do grid on basis of both er have to choose 1 for grid\n","2a569143":"X.head()","b736fe0e":"y1.head() ## two target at once ","7e02acd4":"y2.head()  ## single first target","eed43a71":"##","2e61b5ba":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV,KFold,StratifiedKFold","027afc8d":"x_train1,x_test1,y_train1,y_test1 = train_test_split(X,y1,test_size=.2)  ## for both\nx_train,x_test,y_train,y_test = train_test_split(X,y2,test_size=.2)    ## for finding grid\n","8a2c7b67":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier","41c66300":"k_range = list(range(1,31))\nweight_options = [\"uniform\", \"distance\"]\n\nparam_grid = dict(n_neighbors = k_range, weights = weight_options)\nprint (param_grid)\nknn = KNeighborsClassifier()\n\ngrid = GridSearchCV(knn, param_grid, cv = 10,n_jobs=-1,verbose=2 ,scoring = 'accuracy')\ngrid.fit(x_train,y_train)","2be5aa21":"## for plotting the grid here not the result .and its not part of the main thesis just plotting the grid behaviour\nimport seaborn as sns\nimport pandas as pd\n\ndef plot_cv_results(cv_results, param_x, param_z, metric='mean_test_score'):\n    cv_results = pd.DataFrame(cv_results)\n    col_x = 'param_' + param_x\n    col_z = 'param_' + param_z\n    fig, ax = plt.subplots(1, 1, figsize=(11, 8))\n    sns.pointplot(x=col_x, y=metric, hue=col_z, data=cv_results, ci=99, n_boot=64, ax=ax)\n    ax.set_title(\"CV Grid Search Results\")\n    ax.set_xlabel(param_x)\n    ax.set_ylabel('Accuracy')\n    ax.legend(title=param_z)\n    return fig","7278cdbf":"plt.rcParams.update({'font.size': 15})  ## plotting configuration\n\n## plotting grid behaviour\nplot_cv_results(grid.cv_results_, 'n_neighbors', 'weights')","601e5663":"## jupyter can render without show command thats why two pic","fd0fdc60":"## plotting with respect to whole","06fd555a":"from sklearn.metrics import confusion_matrix\nknn_final = grid.best_estimator_  ## find the best estimator\ntmp_pred_knn = knn_final.fit(x_train,y_train)  ## individual training for individual confusion matrix precision,recall,F1","8be4b031":"import seaborn as sns\npr1 = tmp_pred_knn.predict(x_test)\n\nconfusion_matrix1 =confusion_matrix(y_test,pr1)\nprint (confusion_matrix1)\n\nfrom sklearn.metrics import precision_score,recall_score,f1_score\nprint (\"Precision score \"+str(precision_score(y_test,pr1)))\nprint (\"Recall score \"+str(recall_score(y_test,pr1)))\nprint (\"F1 score \"+str(f1_score(y_test,pr1)))","483b07a4":"sns.heatmap(confusion_matrix1)","1c0b4969":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n# Create a based model\nrf = RandomForestClassifier()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2) ## njobs force the computer to use all their resources","b07dd699":"grid_search.fit(x_train, y_train)","6f8c4a90":"rf_final=grid_search.best_estimator_   # find the best estimator\ntmp_pred_rf = rf_final.fit(x_train,y_train)       ## individual training \npr1 = tmp_pred_rf.predict(x_test)   #3individual prediction\nconfusion_matrix1 =confusion_matrix(y_test,pr1)\nprint (confusion_matrix1)\nsns.heatmap(confusion_matrix1)\nprint (\"Precision score \"+str(precision_score(y_test,pr1)))\nprint (\"Recall score \"+str(recall_score(y_test,pr1)))\nprint (\"F1 score \"+str(f1_score(y_test,pr1)))","34d06c7c":"plot_cv_results(grid_search.cv_results_, 'n_estimators', 'max_depth')\n#param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}","2f151659":"plot_cv_results(grid_search.cv_results_, 'n_estimators', 'min_samples_split')\n#grid2 = GridSearchCV(SVC(),param_grid,n_jobs=-1,refit = True, verbose=2)","05430866":"plot_cv_results(grid_search.cv_results_,'min_samples_split','max_depth')\n#grid2.fit(x_train,y_train)","a1b97718":"#svc_final = grid2.best_estimator_","962c8e95":"svc_final = SVC()\nsvc_final.fit(x_train,y_train)\npr1 = svc_final.predict(x_test)\nconfusion_matrix1 =confusion_matrix(y_test,pr1)\nprint (confusion_matrix1)\nsns.heatmap(confusion_matrix1)\n\nplt.rcParams[\"figure.figsize\"] = fig_size\nplt.rcParams.update({'font.size': 50})\nprint (\"Precision score \"+str(precision_score(y_test,pr1)))\nprint (\"Recall score \"+str(recall_score(y_test,pr1)))\nprint (\"F1 score \"+str(f1_score(y_test,pr1)))","9b964f19":"param_grid = {'C':[1,10,100,1000]}\ngrid3 = GridSearchCV(LinearSVC(),param_grid,refit = True, verbose=2,n_jobs=-1)\n","b59eb136":"grid3.fit(x_train,y_train)","288154b6":"## for plotting we can take all the parameter into accountso we take the most important for plotting separately\nACP=[]\n#plot_cv_results(grid3.cv_results_,'C',)\nC = range(1,100)\nfor item in C:\n    model = LinearSVC(C=item)\n    model.fit(x_train,y_train)\n    ACP.append(model.score(x_test,y_test))\n    ","505cedc7":"plt.rcParams[\"figure.figsize\"] = fig_size\nplt.rcParams.update({'font.size': 50})\nplt.plot(C,ACP)","164085aa":"print (ACP)","272d5e97":"lsvc_final = grid3.best_estimator_\n\nlsvc_final.fit(x_train,y_train)\npr1 = lsvc_final.predict(x_test)\nconfusion_matrix1 =confusion_matrix(y_test,pr1)\nprint (confusion_matrix1)\nsns.heatmap(confusion_matrix1)\nprint (\"Precision score \"+str(precision_score(y_test,pr1)))\nprint (\"Recall score \"+str(recall_score(y_test,pr1)))\nprint (\"F1 score \"+str(f1_score(y_test,pr1)))","144d5d29":"parameters = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n              'max_depth': [4, 6, 8],\n              'min_samples_leaf': [20, 50,100,150],\n              #'max_features': [1.0, 0.3, 0.1] \n              }\n\ngrid4 = GridSearchCV(GradientBoostingClassifier(), parameters,verbose=2, cv=10, n_jobs=-1)\n\ngrid4.fit(x_train, y_train)\nprint (grid4.best_estimator_)\n","1038e5c0":"gb_final = grid4.best_estimator_\n\ngb_final.fit(x_train,y_train)\npr1 = gb_final.predict(x_test)\nconfusion_matrix1 =confusion_matrix(y_test,pr1)\nprint (confusion_matrix1)\nprint (\"Precision score \"+str(precision_score(y_test,pr1)))\nprint (\"Recall score \"+str(recall_score(y_test,pr1)))\nprint (\"F1 score \"+str(f1_score(y_test,pr1)))\nsns.heatmap(confusion_matrix1)","435fede2":"learning_rate1= [0.1, 0.05, 0.02, 0.01]\nmax_depth1= [4, 6, 8]\nmin_samples_leaf1= [20, 50,100,150]\n\nACP1=[]\n\n## plotting based on the most imp charactisitcs learning_rate\nfor item in learning_rate1:\n    model1 = GradientBoostingClassifier(learning_rate=item)\n    model1.fit(x_train,y_train)\n    ACP1.append(model1.score(x_test,y_test))\n    print(item)","ca1481b0":"plt.plot(learning_rate1,ACP1)","a1d9fee9":"ACP1","f2724bfa":"learning_rate1= [0.1, 0.05, 0.02, 0.01]\nmax_depth1= [4, 6, 8]\nmin_samples_leaf1= [20, 50,100,150]\n\nACP2=[]\nfor item in max_depth1:\n    model2 = GradientBoostingClassifier(max_depth=item)\n    model2.fit(x_train,y_train)\n    ACP2.append(model2.score(x_test,y_test))\n    print(item)\nplt.plot(max_depth1,ACP2)","3ad87fe9":"learning_rate1= [0.1, 0.05, 0.02, 0.01]\nmax_depth1= [4, 6, 8]\nmin_samples_leaf1= [20, 50,100,150]\n\nACP3=[]\nfor item in min_samples_leaf1:\n    model3 = GradientBoostingClassifier(min_samples_leaf=item)\n    model3.fit(x_train,y_train)\n    ACP3.append(model3.score(x_test,y_test))\n    print(item)\nplt.plot(min_samples_leaf1,ACP3)","c69967f9":"print (gb_final)","84b34354":"# Define the parameter values that should be searched\nsample_split_range = list(range(2, 50))\n\n# Create a parameter grid: map the parameter names to the values that should be searched\n# Simply a python dictionary\n# Key: parameter name\n# Value: list of values that should be searched for that parameter\n# Single key-value pair for param_grid\nparam_grid = dict(min_samples_split=sample_split_range)\ndtc = DecisionTreeClassifier()\n# instantiate the grid\ngrid5 = GridSearchCV(dtc, param_grid, cv=10,n_jobs=-1,verbose=2, scoring='accuracy')\n\n# fit the grid with data\ngrid5.fit(x_train, y_train)","3b3e10ab":"dt_final = grid5.best_estimator_\ndt_final.fit(x_train,y_train)\npr1 = dt_final.predict(x_test)\nconfusion_matrix1 =confusion_matrix(y_test,pr1)\nprint (confusion_matrix1)\nsns.heatmap(confusion_matrix1)\nprint (\"Precision score \"+str(precision_score(y_test,pr1)))\nprint (\"Recall score \"+str(recall_score(y_test,pr1)))\nprint (\"F1 score \"+str(f1_score(y_test,pr1)))","55fcdf1c":"min_sample_split1=list(range(2, 50))\n\nACP4=[]\nfor item in min_sample_split1:\n    model4 = DecisionTreeClassifier(min_samples_split=item)\n    model4.fit(x_train,y_train)\n    ACP4.append(model4.score(x_test,y_test))\n    print(item)\nplt.plot(min_sample_split1,ACP4)","81e67adb":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nML=[]\nM=['DecisionTreeClassifier','KNeighborsRegressor','SVC','LinearSVC','RandomForestRegressor','GradientBoostingClassifier']\nZ=[gb_final,knn_final,svc_final,lsvc_final,rf_final,gb_final]\n##new\n#M=['DecisionTreeClassifier','KNeighborsRegressor','SVC']\n#Z=[DecisionTreeClassifier(),KNeighborsClassifier(),SVC()]","a64cd645":"print (x_train.shape)\nprint (y_train.shape)\nprint (x_test.shape)\nprint (y_test.shape)\n\n##new\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier# Using pipeline for applying logistic regression and one vs rest classifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfor model in Z:\n    \n    ##new\n    models=MultiOutputClassifier(model)\n    ##new\n    models.fit(x_train1,y_train1)      ## training the model this could take a little time\n    accuracy=models.score(x_test1,y_test1)    ## comparing result with the test data set\n    ML.append(accuracy) \n","58a97108":"ML","cf604fb2":"d={'Accuracy':ML,'Algorithm':M}\ndf1=pd.DataFrame(d)","848fcff0":"df1","a9081e8d":"from sklearn.ensemble import VotingClassifier\nvoting_clf = MultiOutputClassifier(VotingClassifier(estimators=[('knn',knn_final),('rf',rf_final),('dt',dt_final),('svc',svc_final),('gb',gb_final)],voting='hard'))","8eb0f241":"voting_clf.fit(x_train1,y_train1)","34f75141":"predicted=voting_clf.predict(x_test1)","9ca203f5":"print (predicted)\n#tmp_pred = gb_final.fit(x_train,y_train)\n#pr1 = tmp_pred.predict(x_test)\n","29807587":"cf1_d=[]\ncf2_d=[]\nfor item in predicted:\n    cf1_d.append(item[0])\nfor item in predicted:\n    cf2_d.append(item[1])\n    \n","364ca74e":"y_test1","b7e88897":"voting_hybrid_classifier=voting_clf.score(x_test1,y_test1)","a1cc5abf":"\nconfusion_matrix1 =confusion_matrix(np.array(y_test1['num_class']),np.array(cf1_d))\n#confusion_matrix2 =confusion_matrix(y_test,pr1)\n\nprint (confusion_matrix1)\nprint (\"Precision score \"+str(precision_score(np.array(y_test1['num_class']),np.array(cf1_d))))\nprint (\"Recall score \"+str(recall_score(np.array(y_test1['num_class']),np.array(cf1_d))))\nprint (\"F1 score \"+str(f1_score(np.array(y_test1['num_class']),np.array(cf1_d))))\n#print (confusion_matrix2)\n\nsns.heatmap(confusion_matrix1)","06dcca6c":"confusion_matrix2 =confusion_matrix(np.array(y_test1['num_proto']),np.array(cf2_d))\n#confusion_matrix2 =confusion_matrix(y_test,pr1)\n\nprint (confusion_matrix2)\nprint (\"Precision score \"+str(precision_score(np.array(y_test1['num_proto']),np.array(cf2_d),average='micro')))\nprint (\"Recall score \"+str(recall_score(np.array(y_test1['num_proto']),np.array(cf2_d),average='micro')))\nprint (\"F1 score \"+str(f1_score(np.array(y_test1['num_proto']),np.array(cf2_d),average='micro')))\n#print (confusion_matrix2)\n\nsns.heatmap(confusion_matrix2)","acbbf934":"## mse\nfrom sklearn.metrics import mean_squared_error","885b7ede":"mse = mean_squared_error(y_test1,predicted)","b2c8426b":"mse","3377c609":"from sklearn.model_selection import cross_val_score","39f8beae":"print(cross_val_score(voting_clf, x_test1, y_test1, cv=3))","7266d60b":"print(cross_val_score(voting_clf, x_test1, y_test1, cv=10))","b5c6d061":"voting_hybrid_classifier","39616e18":"#df1=df1.drop('hybrid_voting_Classifier',axis=1)","f4264008":"ML.append(voting_hybrid_classifier)","9dd61de0":"M=['DecisionTreeClassifier','KNeighborsRegressor','SVC','LinearSVC','RandomForestRegressor','GradientBoostingClassifier','hybrid_voting_classifier']\nd={'Accuracy':ML,'Algorithm':M}\ndf1=pd.DataFrame(d)","55163734":"df1","dbbe325b":"fig_size = plt.rcParams[\"figure.figsize\"]\nfig_size[0] = 50\nfig_size[1] = 20\nplt.rcParams[\"figure.figsize\"] = fig_size\n\nplt.bar(df1['Algorithm'],df1['Accuracy'])","00c398ce":"## 3 for split we get more than 95 % accuracy so we dont have overfitting problem","d0721088":"# Best upon 1 we predict the next one we cant do grid both all at once cause .each require different parameter set  and we are doing one vs one prediction","fa0a1bf2":"# going for 10 split","23e4270e":"## combined classifier shows the best accuracy without any overfitting problem","2ba2ba36":"# going for cross val comparison for more accurate evaluation","32f293ea":"##  for 10 split we get more than 94 % accuracy so we definitly  dont have overfitting problem","28a35102":"# Primary validation shows 97.6 % accuracy","a8c3175f":"## LinearSVC failrd to predict cause its genarelly used for linearly genarated data ommiting this\n## making combined classifier but ommiting the LSVC\n## combined will use hard voting for selection"}}