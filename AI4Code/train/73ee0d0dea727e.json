{"cell_type":{"fcce1b34":"code","7c823a93":"code","1f0226b3":"code","b48942c0":"code","b11889df":"code","20028f71":"code","0ac8658e":"code","2050bb9f":"code","8670caf3":"code","2112ed34":"code","ac91e9bd":"code","666d922d":"code","b658eae1":"code","77592358":"code","9070d10d":"code","520823ee":"code","f9e1985a":"code","9fdac22c":"code","ded3d52a":"code","db5554eb":"code","27d8f7ae":"code","9161e9d4":"code","cfa17a63":"markdown","140b86ae":"markdown","d1233a09":"markdown","26ca4b6e":"markdown","474a2269":"markdown"},"source":{"fcce1b34":"import pandas as pd\nimport re\nimport os\nimport torch\nimport numpy as np","7c823a93":"!pip install transformers","1f0226b3":"from transformers import BertModel, BertTokenizer","b48942c0":"path_to_dataset = '\/kaggle\/input\/nlp-getting-started\/'","b11889df":"test_df = pd.read_csv(os.path.join(path_to_dataset, 'test.csv'))","20028f71":"class Model(torch.nn.Module):\n    \n    def __init__(self, ):\n        \n        super(Model, self).__init__()\n        self.base_model = BertModel.from_pretrained('bert-base-uncased') #pretrained bert model\n        self.fc1 = torch.nn.Linear(768, 1) #use logistic regression\n        \n    def forward(self, ids, masks):\n        \n        x = self.base_model(ids, attention_mask=masks)[1]\n        x = self.fc1(x)\n        return x\n        ","0ac8658e":"path_to_model = '\/kaggle\/input\/nlpgetstartedbertbasemoel\/'","2050bb9f":"model = torch.load(os.path.join(path_to_model, 'model.pth'))","8670caf3":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'","2112ed34":"model = model.to(device)","ac91e9bd":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","666d922d":"def bert_encode(text, max_len=512):\n    \n    text = tokenizer.tokenize(text)\n    text = text[:max_len-2]\n    input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n    tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n    tokens += [0] * (max_len - len(input_sequence))\n    pad_masks = [1] * len(input_sequence) + [0] * (max_len - len(input_sequence))\n\n    return tokens, pad_masks","b658eae1":"class TestDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, test_tokens, test_pad_masks):\n        \n        super(TestDataset, self).__init__()\n        self.test_tokens = test_tokens\n        self.test_pad_masks = test_pad_masks\n        \n    def __getitem__(self, index):\n        \n        tokens = self.test_tokens[index]\n        masks = self.test_pad_masks[index]\n        \n        return (tokens, masks)\n    \n    def __len__(self,):\n        \n        return len(self.test_tokens)","77592358":"test_tokens = []\ntest_pad_masks = []\nfor text in test_df.text:\n    tokens, masks = bert_encode(text)\n    test_tokens.append(tokens)\n    test_pad_masks.append(masks)\n    \ntest_tokens = np.array(test_tokens)\ntest_pad_masks = np.array(test_pad_masks)","9070d10d":"test_dataset = TestDataset(\n    test_tokens=test_tokens,\n    test_pad_masks=test_pad_masks\n)","520823ee":"test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=3, shuffle=False)","f9e1985a":"model.eval()\ny_preds = []\nfor (tokens, masks) in test_dataloader:\n\n    y_pred = model(\n                torch.tensor(tokens, dtype=torch.long).to(device),\n                torch.tensor(masks, dtype=torch.long).to(device),\n            )\n    y_preds += y_pred.detach().cpu().numpy().squeeze().tolist()","9fdac22c":"submission_df = pd.read_csv(os.path.join(path_to_dataset, 'sample_submission.csv'))","ded3d52a":"submission_df['target'] = (np.array(y_preds) >= 0).astype('int')","db5554eb":"submission_df.target.value_counts()","27d8f7ae":"submission_df.head()","9161e9d4":"submission_df.to_csv('submission.csv', index=False)","cfa17a63":"Target is 1 if the output is greater than 0.","140b86ae":"Install HuggingFace implementation of bert (https:\/\/huggingface.co\/).","d1233a09":"Writing to submission.csv file","26ca4b6e":"Load the pretrained model","474a2269":"Defining our simple model (logistic regression over the bert base model)."}}