{"cell_type":{"4e1f2af7":"code","9e8684d4":"code","7bb6bb3d":"code","4bcb8cec":"code","f6dce461":"code","50bfd952":"code","d76bed1e":"code","2b06144d":"code","b428ab21":"code","a4ccecc8":"code","e375a7b3":"code","5709a494":"code","2c38c75b":"code","cd3c31c8":"code","f59245fd":"code","5bfcf5dd":"code","3278a51d":"code","7a4fb422":"code","90aa7ff6":"code","cb389007":"code","70a70c6f":"code","6ddc13f2":"code","55ad16d8":"code","3ecbc393":"code","aee9acd2":"code","f3ae4be0":"code","61f9324e":"code","04c2e799":"code","3e729d3e":"code","b1c3cedd":"code","da1468fd":"code","c4f124f1":"code","82cdd946":"markdown","0215f45e":"markdown","c7a1a179":"markdown","89120d2a":"markdown","015f1bd5":"markdown","e725a7ad":"markdown","8a17d563":"markdown","2656cb33":"markdown","18dddc08":"markdown","f61020ef":"markdown","568723b8":"markdown","06cc53e2":"markdown","365a4326":"markdown","315beb65":"markdown","273f3a64":"markdown","1a95dccb":"markdown","25959936":"markdown","1c7f8852":"markdown","d45fd92c":"markdown","34128912":"markdown","ee7d6cb3":"markdown","61356425":"markdown","e74d92fc":"markdown","024d3b9b":"markdown","c6fb7102":"markdown","b9c9e6a6":"markdown","bdd8234c":"markdown","6d647e9a":"markdown","a0162eff":"markdown","547aef4e":"markdown","ea4fe89d":"markdown"},"source":{"4e1f2af7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9e8684d4":"dataset = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndataset.head()","7bb6bb3d":"dataset.info()","4bcb8cec":"dataset.describe()","f6dce461":"X = dataset.iloc[:,0:13].values\ny = dataset.loc[:,['target']].values","50bfd952":"print(X.shape)\nprint(y.shape)","d76bed1e":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","2b06144d":"sns.set(font_scale=1)\nsns.set_style(\"darkgrid\")\nfig, ax = plt.subplots(figsize=(12 , 6))\nsns.heatmap(dataset.corr(), annot = True , ax = ax, cmap=\"Blues\")","b428ab21":"from sklearn.model_selection import cross_val_score # For K-Fold \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn import metrics # required for precision_score, recall_score, f1_score in K-fold (macros)\nfrom sklearn.model_selection import StratifiedKFold # For stratified K-Fold\nfrom sklearn.metrics import roc_curve,auc\nfrom numpy import interp","a4ccecc8":"def k_fold(classifer):\n    accuracy  = cross_val_score(classifer, X,y.ravel(), cv =10)\n    precision = cross_val_score(classifer, X, y.ravel(), cv=10, scoring='precision_macro')\n    recall = cross_val_score(classifer, X, y.ravel(), cv=10, scoring='recall_macro')\n    f1 = cross_val_score(classifer, X, y.ravel(), cv=10, scoring='f1_macro')\n    accuracy = accuracy.mean()\n    precision = precision.mean()\n    recall = recall.mean()\n    f1 = f1.mean()\n    yplot = [accuracy, precision, recall, f1]\n    xplot = ['accuracy', 'precision' ,'recall', 'f1 score']\n    sns.barplot(x = xplot, y= yplot)\n    print(accuracy)\n    print(precision)\n    print(recall)\n    print(f1)","e375a7b3":"class k:\n    def __init__(self,classifier,skf):\n        self.classifier = classifier\n        self.skf = skf\n        \n    def strat_k(self):\n        \n        accuracy   = []\n        precision  = []\n        recall     = []\n        f1_value = []\n\n        for train_index, test_index in skf.split(X, y.ravel()):\n          #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n          X_train, X_test = X[train_index], X[test_index]\n          y_train, y_test = y[train_index].ravel(), y[test_index].ravel()\n\n          self.classifier.fit(X_train,y_train)\n          y_pred = self.classifier.predict(X_test)\n\n          ac = accuracy_score(y_test,y_pred)\n          pre = precision_score(y_test,y_pred)\n          rec = recall_score(y_test,y_pred)\n          f1  = f1_score(y_test, y_pred)\n\n\n\n          accuracy.append(ac)\n          precision.append(pre)\n          recall.append(rec)\n          f1_value.append(f1)\n\n        yplot = [sum(accuracy)\/len(accuracy), sum(precision)\/len(precision), sum(recall)\/len(recall), sum(f1_value)\/len(f1_value)]\n        xplot = ['accuracy', 'precision' ,'recall', 'f1 score']\n        sns.barplot(x = xplot, y= yplot)\n        print(sum(accuracy)\/len(accuracy))\n        print(sum(precision)\/len(precision))\n        print(sum(recall)\/len(recall))\n        print(sum(f1_value)\/len(f1_value))\n        \n    def roc(self):\n\n\n        tprs = []\n        aucs = []\n        mean_fpr = np.linspace(0,1,100)\n\n\n        plt.figure(figsize=(3,2), dpi=300)\n        sns.set(font_scale=.4)\n        i = 1\n        for train,test in skf.split(X,y):\n            prediction =self.classifier.fit(X[train],y[train].ravel()).predict_proba(X[test])\n            fpr, tpr, t = roc_curve(y[test].ravel(), prediction[:, 1])\n            tprs.append(interp(mean_fpr, fpr, tpr))\n            roc_auc = auc(fpr, tpr)\n            aucs.append(roc_auc)\n            plt.plot(fpr, tpr, lw=.5, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i, roc_auc))\n            i= i+1\n\n        plt.plot([0,1],[0,1],linestyle = '--',lw = .7,color = 'black')\n        sns.set_style(\"darkgrid\")\n        mean_tpr = np.mean(tprs, axis=0)\n        mean_auc = auc(mean_fpr, mean_tpr)\n        plt.plot(mean_fpr, mean_tpr, color='blue',\n                 label='Mean ROC (AUC = %0.4f )' % (mean_auc),lw=.7, alpha=1)\n\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('ROC Curve')\n        plt.legend(loc=\"lower right\")\n        plt.grid(True)\n        plt.show()","5709a494":"from sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(random_state  = 0)\nk_fold(classifier_lr)","2c38c75b":"skf = StratifiedKFold(n_splits=10,shuffle=False)\nc = k(classifier_lr,skf)\nc.strat_k()","cd3c31c8":"c.roc()","f59245fd":"from sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB()\nk_fold(classifier_nb)","5bfcf5dd":"skf = StratifiedKFold(n_splits=10,shuffle=False)\nc = k(classifier_nb,skf)\nc.strat_k()","3278a51d":"c.roc()","7a4fb422":"from sklearn.ensemble import GradientBoostingClassifier\nclassifier_xg = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth = 4, random_state = 0)\nk_fold(classifier_xg)","90aa7ff6":"skf = StratifiedKFold(n_splits=10,shuffle=False)\nc = k(classifier_xg,skf)\nc.strat_k()","cb389007":"c.roc()","70a70c6f":"from sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nk_fold(classifier_knn)","6ddc13f2":"skf = StratifiedKFold(n_splits=10,shuffle=False)\nc = k(classifier_knn,skf)\nc.strat_k()","55ad16d8":"c.roc()","3ecbc393":"from sklearn.svm import SVC\nclassifier_svm = SVC(kernel = 'rbf',probability=True, random_state = 0)\nk_fold(classifier_svm)","aee9acd2":"skf = StratifiedKFold(n_splits=10,shuffle=False)\nc = k(classifier_svm,skf)\nc.strat_k()","f3ae4be0":"c.roc()","61f9324e":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nk_fold(classifier_dt)","04c2e799":"skf = StratifiedKFold(n_splits=10,shuffle=False)\nc = k(classifier_dt,skf)\nc.strat_k()","3e729d3e":"c.roc()","b1c3cedd":"from sklearn.ensemble import AdaBoostClassifier\nclassifier_abc = AdaBoostClassifier(n_estimators=100, random_state=0)\nk_fold(classifier_abc)","da1468fd":"skf = StratifiedKFold(n_splits=10,shuffle=False)\nc = k(classifier_abc,skf)\nc.strat_k()","c4f124f1":"c.roc()","82cdd946":"# ROC CURVE FOR SVM","0215f45e":"# Roc curve for decision Trees","c7a1a179":"# Heatmap for correlation","89120d2a":"# Traning different models and using cross validation as evaluation method\n# Importing required libraries for K-fold and stratified K-fold","015f1bd5":"# Stratified K- fold for SVM","e725a7ad":"# XGBOOST\nk-fold results","8a17d563":"# Dividing into input and output variables for our models","2656cb33":" # User Defined Function to Find Accuracy, Precision, recall ,f1_score using K-fold \n We will use the function for later models","18dddc08":"# ROC CURVE FOR XGBOOST","f61020ef":"# KNN","568723b8":"# ROC for Logistic Regression","06cc53e2":"# Decision Tree","365a4326":"# SVM","315beb65":"# Logistic Regression\nWe will see that we dont have to separately fit the data and predict the cross validation methods inbuilt library will do it for us.","273f3a64":"# Stratified K-Fold results for Naive Bayes","1a95dccb":"# Stratified K-Fold Results for XGBOOST","25959936":"# ROC Curve for Naive Bayes","1c7f8852":"# Naive Bayes\nk-fold","d45fd92c":"K -Fold Results","34128912":"# Import Necessary Libraries","ee7d6cb3":"# Feature Scaling \nA point to note we are not fiting and transforming the X_test, X_train because we wont split the data the cross validation methods will do it for us.","61356425":"# Stratified K-Fold Results","e74d92fc":"# User Defined Class for Stratified K-Fold and it's AOC\n We will use the function for later models","024d3b9b":"# Load csv file","c6fb7102":"# Roc Curve for AdaBoost","b9c9e6a6":"# Stratified K-fold results for Decision Trees","bdd8234c":"# Looking at the basic properties of the dataset","6d647e9a":"# Stratified K -Fold results for AdaBoost","a0162eff":"# ADA Boost","547aef4e":"# Stratified K-fold results for KNN","ea4fe89d":"# ROC CURVE FOR KNN"}}