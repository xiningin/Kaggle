{"cell_type":{"cdd1e82f":"code","6a2db7a0":"code","14b1436d":"code","2649374f":"code","8fa0e3f9":"code","8359be57":"code","e0fd4e6a":"code","73ec2584":"code","0e9bc823":"code","69e47981":"code","6609b16b":"code","443af939":"code","0a8b43d3":"code","cc1aa1c1":"code","6fd1521f":"code","b3215435":"code","ecb55086":"code","ff2c0638":"code","62e08997":"code","0255f0b1":"code","b5d03dea":"code","c6d829ae":"code","389876da":"code","83dd4702":"code","b315a14b":"code","9f858848":"code","6d008941":"code","84f6ac2e":"code","0aba823b":"code","360efcfd":"code","5d9c1aba":"code","c8b175da":"code","9ebae1fc":"code","be8b488d":"code","0d1f5b4f":"code","408999a3":"code","8820ca38":"code","d9f64c04":"code","2f635fe2":"code","71bbd27e":"markdown","f7266c3a":"markdown","28c50808":"markdown","082aac36":"markdown","27aa08cd":"markdown","37abb50a":"markdown","524f09c6":"markdown","7c0f748b":"markdown","930d76d4":"markdown","d0bdb9b4":"markdown","4c0ef232":"markdown","84220655":"markdown","c6ef1506":"markdown","74620520":"markdown","59edbfc7":"markdown","d3250e48":"markdown","05e9fc45":"markdown","553ca67f":"markdown","c85d13e9":"markdown","501a6b0e":"markdown","cfda8de1":"markdown","a1e0533d":"markdown","dd2c9fd1":"markdown","6d82afbf":"markdown","e6446763":"markdown","59971d22":"markdown"},"source":{"cdd1e82f":"import numpy as np\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport sys\nfrom textblob import TextBlob \n\nimport re\n\nfrom IPython.display import display\nimport plotly.express as px\nimport spacy\n","6a2db7a0":"# Input data files are available in the read-only \"..\/input\/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","14b1436d":"path = '\/kaggle\/input\/contradictory-my-dear-watson\/'\n\ntrain = pd.read_csv(path + 'train.csv')\ntest = pd.read_csv(path + 'test.csv')\nsample_submission = pd.read_csv(path + 'sample_submission.csv')","2649374f":"print(\"  - Train: \\ntrain shape:\", train.shape)\nprint(\"Head:\")\ntrain.head()","8fa0e3f9":"print(\"  - Test: \\ntest shape:\", test.shape)\nprint(\"Head:\")\ntest.head()","8359be57":"train.info()","e0fd4e6a":"test.info()","73ec2584":"fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,5))\ntrain['label'].value_counts().plot.bar(ax=axes)\nplt.title('label - target')\nplt.show()","0e9bc823":"def plot_table(data, name_data):\n    \n    pd_language_prob = round(pd.DataFrame(data.language.value_counts()).transpose()\/data['language'].count(),2)\n    \n    lang = pd_language_prob.columns.tolist()\n    prob = pd_language_prob.values.tolist()[0]\n\n    table = [['prob'+'('+i+')',j] for (i,j) in zip(lang,prob)]\n\n    fig = plt.figure()\n    \n    # definitions for the axes\n    left, width = 0.10, 1.5\n    bottom, height = 0.1, .8\n    bottom_h = left_h = left + width + 0.02\n\n    rect_cones = [left, bottom, width, height]\n    rect_box = [left_h, bottom, 0.17, height]\n\n    # plot\n    ax1 = plt.axes(rect_cones)\n    data.groupby('language')['language'].agg(['count']).plot.bar(ax=ax1)\n   \n  \n    plt.title(\"Frequency of languages \" + name_data + \" set\")\n    \n    ax2 = plt.axes(rect_box)\n    my_table = ax2.table(cellText = table, loc ='right')\n    my_table.set_fontsize(40)\n    my_table.scale(4,4)\n    ax2.axis('off')\n    plt.show()","69e47981":"\nplot_table(train,'train')","6609b16b":"plot_table(test,'test')","443af939":"pd_language_label = pd.DataFrame(train[['language','label']].groupby(['language','label'])['label'].count())\nfig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,9))\npd_language_label.unstack().plot.barh(ax=axes)\nplt.title('label vs language')\nplt.show()","0a8b43d3":"df_English = train[train['language']=='English']","cc1aa1c1":"# Create an empty model\nnlp = spacy.load('en')","6fd1521f":"class removing():\n    '''Clean text'''\n    def __init__(self):\n        self.text = texto\n    def lower(texto):\n        return(str(texto).lower())\n    def remove_url(texto):\n        return(re.sub(r'http:\/\/\\S+|https:\/\/\\S+','', texto))\n    def remove_emoji(texto):\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', texto)\n    def remove_punctuation(texto):\n        return(re.sub(r'[^\\w\\s]','',texto))","b3215435":"# cleaning text from premise and hypothesis and we create two new features\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x: removing.lower(x.premise), axis=1)\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x:removing.remove_url(x.premise_modify), axis=1)\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x: removing.remove_punctuation(x.premise_modify),axis=1)\ndf_English.loc[:,'premise_modify'] = df_English.apply(lambda x: removing.remove_emoji(x.premise_modify), axis=1)\n\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x:removing.lower(x.hypothesis), axis=1)\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x:removing.remove_url(x.hypothesis_modify), axis=1)\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x: removing.remove_punctuation(x.hypothesis_modify), axis=1)\ndf_English.loc[:,'hypothesis_modify'] = df_English.apply(lambda x: removing.remove_emoji(x.hypothesis_modify), axis=1)\n","ecb55086":"def count_words(df, feature):\n    num_words = [] \n    for i in df[feature]:\n        aux = nlp(i)\n        num_words.append(len(aux))\n    return(num_words)\n","ff2c0638":"df_English.loc[:,'num_words_premise'] = count_words(df_English,'premise_modify')\ndf_English.loc[:,'num_words_hypothesis'] = count_words(df_English,'hypothesis_modify')\n","62e08997":"fig, axes = plt.subplots(nrows=3,ncols=2, figsize=(12,8))\nplt.subplots_adjust(hspace = 0.5)\ndf_English[df_English.label == 0].num_words_premise.plot.hist(bins=40,  ax = axes[0][0], label = 'Fake', color='blue')\naxes[0][0].set_title('# Words Premise Histogram (Entailment)')\ndf_English[df_English.label == 0].num_words_hypothesis.plot.hist(bins=40,  ax = axes[0][1], label = 'Fake', color='orange')\naxes[0][1].set_title('# Words Hypothesis Histogram (Entailment)')\n\ndf_English[df_English.label == 1].num_words_premise.plot.hist(bins=40,  ax = axes[1][0], label = 'Fake', color='blue')\naxes[1][0].set_title('# Words Premise Histogram (Neutral)')\ndf_English[df_English.label == 1].num_words_hypothesis.plot.hist(bins=40,  ax = axes[1][1], label = 'Fake', color='orange')\naxes[1][1].set_title('# Words Hypothesis Histogram (Neutral)')\n\ndf_English[df_English.label == 2].num_words_premise.plot.hist(bins=40,  ax = axes[2][0], label = 'Fake', color='blue')\naxes[2][0].set_title('# Words Premise Histogram (Contradiction)')\ndf_English[df_English.label == 1].num_words_hypothesis.plot.hist(bins=40,  ax = axes[2][1], label = 'Fake', color='orange')\naxes[2][1].set_title('# Words Hypothesis Histogram (Contradiction)')\nplt.show()\n\n# dataframe\n\nmean_premise = []\nstd_premise = []\nmean_hypothesis = []\nstd_hypothesis = []\nfor i in range(3):\n    for j in ['num_words_premise', 'num_words_hypothesis']:\n        mean_aux = round(df_English[df_English.label == i][j].mean(),2)\n        std_aux = round(df_English[df_English.label == i][j].std(),2)\n        if j == 'num_words_premise':\n            mean_premise.append(mean_aux)\n            std_premise.append(std_aux)\n        else:\n            mean_hypothesis.append(mean_aux)\n            std_hypothesis.append(std_aux)\n\nindex_list = ['Entailment', 'Neutral', 'Contradiction']\n\npd_aux = pd.DataFrame({'mean_premise': mean_premise, 'std_premise':std_premise, \n                           'mean_hypothesis': mean_hypothesis, 'std_hypothesis': std_hypothesis}, index = index_list)\n\n\npd_aux","0255f0b1":"stop = stopwords.words('english')\nclass stop_words():\n    def __init__(self):\n        self.text = texto\n    def get_stopwords(texto):\n        return(' '.join([x for x in texto.split(' ') if x in stop]))\n    def remove_stopwords(texto):\n        return(' '.join([x for x in texto.split(' ') if not x in stop]))","b5d03dea":"df_English.loc[:, 'stopwords_premise'] = df_English.premise_modify.apply(lambda x: stop_words.get_stopwords(x))\ndf_English.loc[:, 'stopwords_hypothesis'] = df_English.hypothesis_modify.apply(lambda x: stop_words.get_stopwords(x))\ndf_English.loc[:, 'premise_notstop'] = df_English.premise_modify.apply(lambda x: stop_words.remove_stopwords(x))\ndf_English.loc[:, 'hypothesis_notstop'] = df_English.hypothesis_modify.apply(lambda x: stop_words.remove_stopwords(x))","c6d829ae":"\ndef count_list(list_pass):\n        '''you pass a str'''\n        count = {}\n        for word in list_pass:\n            if word in count :\n                count[word] += 1\n            else:\n                count[word] = 1\n        return(count)","389876da":"def draw_treemap(serie, title_name):\n    '''It is a function that draw a treemap with the most 20 popular words from one serie\n    Note: the variable you have to pass is df[[feature]] and title of the plot \n    '''\n    list_aux = [w.split(' ') for w in serie]\n    list_aux = [item for sublist in list_aux for item in sublist]\n    dict_aux = count_list(list_aux)\n    df_aux = pd.DataFrame({'words': list(dict_aux.keys()), 'number':list(dict_aux.values())})\n    df_aux = df_aux.sort_values(by=['number'], ascending=False)[0:20]\n\n    fig = px.treemap(df_aux, path=['words'], values='number', width=900, height=400, title=title_name)\n    fig.show()","83dd4702":"#print('STOPWORDS_PREMISE:')\ndraw_treemap(df_English[df_English['label']==0].stopwords_premise, 'Treemap - StopWords_Premise(Entailment)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==1].stopwords_premise, 'Treemap - StopWords_Premise(Neutral)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==2].stopwords_premise, 'Treemap - StopWords_Premise(Contradiction)(the 20 most popular)')\n#print('-------------- STOPWORDS_hypothesis --------------')\ndraw_treemap(df_English[df_English['label']==0].stopwords_hypothesis, 'Treemap - StopWords_Hypothesis(Entailment)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==1].stopwords_hypothesis, 'Treemap - StopWords_Hypothesis(Neutral)(the 20 most popular)')\ndraw_treemap(df_English[df_English['label']==2].stopwords_hypothesis, 'Treemap - StopWords_Hypothesis(Contradiction)(the 20 most popular)')","b315a14b":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer, TFAutoModel, BertTokenizer, TFBertModel\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model \nfrom tensorflow.keras.callbacks import ModelCheckpoint\n","9f858848":"# TPU \ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU', tpu.master())\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\nprint(\"REPLICAS:\", strategy.num_replicas_in_sync)","6d008941":"#variables \nmodel_name = 'bert-base-multilingual-cased'\n\nmax_len = 40\n#batch size depend on replica\nbatch_size = 16 * strategy.num_replicas_in_sync\n","84f6ac2e":"tokenizer = BertTokenizer.from_pretrained(model_name)","0aba823b":"X_train, X_valid, y_train, y_valid = train_test_split(train, \n                                                      train.label.values, \n                                                      test_size = 0.2, \n                                                     random_state = 245)","360efcfd":"train_text = X_train[['hypothesis', 'premise']].values.tolist()\nval_text = X_valid[['hypothesis', 'premise']].values.tolist()\ntest_text = test[['hypothesis', 'premise']].values.tolist()","5d9c1aba":"def preparing_data(data):\n    '''this function you pass data that is a list with premise and hypothesis and \n        they have to be encoded in order to use in BERT model\n        RETURN: a dictionary with keys: 'input_ids', 'token_type_ids', 'attention_mask'\n    '''\n    # encoded the words (assign a number in every word) and also add '[SEP]'token between premise and hypothesis\n    # and add '[CLS]'token the begining of inputs\n    # and using padding in order to unify the length of the vectors (adding 0's)\n    text_encoded = tokenizer.batch_encode_plus(data, pad_to_max_length = True)\n    #create a dictionary with tokens\n    dict_input = {}\n    #convert list to tensor\n    dict_input['input_ids'] = tf.convert_to_tensor(text_encoded.input_ids)\n    dict_input['token_type_ids'] = tf.convert_to_tensor(text_encoded.token_type_ids)\n    dict_input['attention_mask'] = tf.convert_to_tensor(text_encoded.attention_mask)\n    return(dict_input)\n\n","c8b175da":"train_input = preparing_data(train_text)\nval_input = preparing_data(val_text)\ntest_input  = preparing_data(test_text)","9ebae1fc":"with strategy.scope():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n    token_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='token_type_ids')\n    \n    embedding = bert_encoder([input_ids, attention_mask, token_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n   \n\nmodel.summary()","be8b488d":"model_history = model.fit(train_input, y_train,\n                          epochs = 10, \n                          batch_size = 64,\n                          validation_data = (val_input, y_valid),\n                          verbose = 1)","0d1f5b4f":"model_history_df = pd.DataFrame(model_history.history, index = range(1, len(model_history.history['loss'])+1))","408999a3":"fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,5))\nmodel_history_df.plot(style=['b-','r-','b--','r--'], ax = axes)\nplt.title('Model Accuracy and Loss')\naxes.set_xlabel('epochs')\naxes.set_ylabel('Accuracy - Loss')\nplt.show()","8820ca38":"model_history = model.fit(train_input, y_train,\n                          epochs = 2, \n                          batch_size = 64,\n                          validation_data = (val_input, y_valid),\n                          verbose = 1)","d9f64c04":"# Predict on test\ntest_preds = model.predict(test_input, verbose = 1)\nsample_submission['prediction'] = test_preds.argmax(axis = 1)","2f635fe2":"sample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","71bbd27e":"- languages and label\n\nNow we focus on how the target is distributed in the different languages. We plot a barplot, with three variables  every language that symbolise the three labels: entailment, neutral and contradiction.","f7266c3a":"## 2. Dataframe Analysis <a id='section2'><\/a>\n \n We are going to analyse the data that composes the different dataframes.","28c50808":"We observe that all treemaps contain the same words in a very similar proportions.","082aac36":"### 2.1. Shape and head","27aa08cd":"- Test\n\nTest set is composed 5 columns (5 features) and  5191 rows.","37abb50a":"#### 2.3.1. English\n\nIn this section, we focuss on the entries in English language only. After that, we study the main features: Hypothesis and Premise, because they will be used as \"inputs\" for our model. We try to visualise some variables in order to find some patterns.","524f09c6":"### 2.3 Features and Target\n\nIn this section, we want to visualise the variables, in order to have an idea what type of data is.\n","7c0f748b":"- stopwords\n\nWe would like to see how the stopwords are distributed, for this we select the most 20 popular stopwords and they are plotted in treemaps, distinguishing the different labels (entailment, neutral and contradiction) for premise and hypothesis. ","930d76d4":"### 2.2 Type of features and Nan values","d0bdb9b4":"# NLP: Contradictory, Mr. Watson\n\nWe are going to analyse Contradictor, Mr. Watson competition from Kaggle","4c0ef232":"The distribution of the three labels accross the different languages is quite uniform, in every language has the same behaviour.","84220655":"The dataframes contain 15 languages. The highest frequency language is English. and the others 14 have the same  proportion of representation. Moreover, observing the distribution of both datasets, they are equal.","c6ef1506":"Looking at the plot, we see that the target has quite similar representation of each three groups of target.","74620520":"- train\n\nTrain set is composed by 6 columns (5 features and 1 target) and 12120 rows. ","59edbfc7":"Observing the previous values, train and test sets do not contain any NAN value, and the features' type\nis string and the target is integer (categorical variable).","d3250e48":"- Preparing the data","05e9fc45":"- number of words' histograms\n\nWe modify the text for premise and hypothesis because we only want to count the words, so we eliminate the punctuation and another symbols. Futhermore, we draw 6 histograms, distinguishin between the three labels (row) and between premise and hypothesis (column).","553ca67f":"Comparing the previous histograms, premise histograms have similar distribution between them. The same is true for the hypothesis histograms. Therefore, we can not observe any diffence between three labels, since they have similar behaviour in terms of number of words.","c85d13e9":"## 1. Import libraries and download data <a id='section1'><\/a>","501a6b0e":"Meaning of the label:\n\n0-> entailment\n\n1-> neutral\n\n2-> contradiction","cfda8de1":"- language\n\nThe data is multilingual, therefore we would like to see what type of languages and what proportion are represented. In order to see it, we plot one frequency barplot for train set and another for test. In addition, it is good to observe whether both sets have similar distribution.","a1e0533d":"For the model, we use some hyperparameters, as example:\n    - max_len = 40\n    - batch_size = 64\n    - epochs = 10\n    \nWe plot the Accuracy and the Loss with respect to the accuracy. We observe that the Accuracy increases and the Loss decreases when we increase the number of epochs, however val_acuracy flattens after epoch = 2 whereas val_loss keeps increasing. This indicates that the model is overfitting after this value, therefore we select epoch = 2 for the final submission. ","dd2c9fd1":"## References\n\nhttps:\/\/www.kaggle.com\/kksienc\/comprehensive-nlp-tutorial-3-bert","6d82afbf":"## Index\n\n- [1. Import libraries and download data](#section1)\n- [2. Dataframe Analysis](#section2)\n- [3. Model](#section3)\n","e6446763":"### 3. Model","59971d22":"- Target (label)\n\nWe draw a barplot that show us the frequency of each value that contain the target."}}