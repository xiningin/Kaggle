{"cell_type":{"40bd79ec":"code","e1070018":"code","bc9805d8":"code","4f070d12":"code","8158d43f":"code","33402f89":"code","aa26fe07":"code","764d8f58":"code","e708804d":"code","73e1c3c7":"code","4d5d6385":"code","71cb69b9":"code","49a35ab4":"code","89e78350":"code","9a888c37":"code","4ca2c20d":"code","a5b26993":"code","3c520de4":"code","f038b202":"code","7aa3c20f":"code","9f049298":"code","5452d84c":"code","cb53dc93":"code","65a0bddd":"code","2372879d":"markdown","313750c3":"markdown","f6cadcd1":"markdown","ce9057d8":"markdown","0df1ca91":"markdown","c56dce30":"markdown","69b606e8":"markdown","2ac35c70":"markdown","a8aaf5b3":"markdown","b8d7cb87":"markdown","e1139d25":"markdown","9818a69c":"markdown","70a5f908":"markdown","225a25ba":"markdown","aedb762d":"markdown","1d3a665a":"markdown"},"source":{"40bd79ec":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, ZeroPadding2D, BatchNormalization\nfrom tensorflow.keras import regularizers","e1070018":"seed = 2021\ndataset_path =  \"..\/input\/intel-image-classification\/seg_train\/seg_train\/\"\ntest_dataset_path = \"..\/input\/intel-image-classification\/seg_test\/seg_test\/\"\nnum_classes = 6\n\nvalidation_split = 0.15\nwidth = 150\nheight = 150\nnum_batches = 16\nlearning_rate = 0.0001\nepochs = 150","bc9805d8":"train_datagen = keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1.\/255,\n    rotation_range = 25,\n    zoom_range = 0.15,\n    width_shift_range=0.15,\n    height_shift_range=0.15,\n    shear_range=0.15,\n    horizontal_flip=True,\n    validation_split = validation_split,\n)","4f070d12":"val_datagen = keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1.\/255,\n    validation_split = validation_split,\n)","8158d43f":"test_datagen = keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1.\/255,\n)","33402f89":"train_dataset = train_datagen.flow_from_directory(\n    dataset_path, \n    batch_size = num_batches, \n    target_size = (width, height), \n    subset = \"training\", \n    seed = seed\n)\n\nvalidation_dataset = val_datagen.flow_from_directory(\n    dataset_path, \n    batch_size = num_batches, \n    target_size = (width, height), \n    subset = \"validation\", \n    seed = seed\n)","aa26fe07":"test_dataset = test_datagen.flow_from_directory(\n    test_dataset_path, \n    batch_size = num_batches, \n    target_size = (width, height),\n    seed = seed\n)","764d8f58":"base_model = keras.applications.xception.Xception(weights='imagenet', include_top=False) #We dont include top, because we need our custom output\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = Dense(num_classes, activation=\"softmax\")(avg) #Softmax because of 6 classes\nmodel = keras.Model(inputs=base_model.input, outputs=output)\n\n#First freeze xception model parameters\nfor layer in base_model.layers:\n  layer.trainable = False","e708804d":"optimizer = keras.optimizers.Adam(learning_rate)\nloss = keras.losses.CategoricalCrossentropy()\nmodel.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\",\"Recall\", \"Precision\"])","73e1c3c7":"checkpoints = keras.callbacks.ModelCheckpoint(\"mode.h5\", monitor='val_accuracy', mode=\"max\")\nearlystop = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode=\"max\", patience=5)","4d5d6385":"history = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[checkpoints, earlystop])","71cb69b9":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","49a35ab4":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","89e78350":"model.evaluate(validation_dataset)","9a888c37":"model.evaluate(train_dataset)","4ca2c20d":"model.evaluate(test_dataset)","a5b26993":"#Unfreezing layers\nfor layers in base_model.layers:\n    layers.trainable = True","3c520de4":"#Smaller learning rate\noptimizer = keras.optimizers.Adam(0.00001)\nloss = keras.losses.CategoricalCrossentropy()\nmodel.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\",\"Recall\", \"Precision\"])","f038b202":"history_finetune = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs,callbacks=[checkpoints, earlystop])","7aa3c20f":"model.evaluate(validation_dataset)","9f049298":"model.evaluate(train_dataset)","5452d84c":"model.evaluate(test_dataset)","cb53dc93":"plt.plot(history_finetune.history['loss'])\nplt.plot(history_finetune.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","65a0bddd":"plt.plot(history_finetune.history['accuracy'])\nplt.plot(history_finetune.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","2372879d":"# Second model evaluation","313750c3":"Lets create data generators to artificialy extend our database. We will use diffrent generator for validaton\/prediction data","f6cadcd1":"# Second model training","ce9057d8":"Create early stopping to stop overfitting and model checkpoint to maximize validation dataset performance","0df1ca91":"Some settings","c56dce30":"Preety good fair model with ~90% accuracy, which generalize well.","69b606e8":"# Data loading","2ac35c70":"I am gonna use pretrained xception model and try to finetune it. Lets freeze core layers at first.","a8aaf5b3":"Load data as flow","b8d7cb87":"# Imports","e1139d25":"Compile model","9818a69c":"# Model preparation","70a5f908":"# First model evaluation","225a25ba":"# First model training","aedb762d":"First train top layer","1d3a665a":"Let's try to finetune it on freezed xception layers now.\nWe need to use smaller learning rate, because otherwise we can lose, a lot of already trained informations."}}