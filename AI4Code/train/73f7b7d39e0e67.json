{"cell_type":{"f441c00c":"code","badfb396":"code","3929f42c":"code","8b7deb37":"code","7b09cb0d":"code","c4885367":"code","2854e286":"code","d81e1ec4":"code","0fa3b3b7":"code","f4f98a16":"code","6bb8ad71":"code","d464bde8":"code","9d793f1c":"code","72cd788a":"code","51e80766":"code","571a1374":"code","267474b1":"code","808fb239":"code","87f1170c":"code","45bd4db8":"code","9d027944":"code","25616bb8":"code","b5f0a7a9":"code","bfe630cd":"code","49966e75":"code","4141a4d1":"code","80b0a268":"code","7e5711ce":"markdown","f28733cb":"markdown","2a40e986":"markdown","ff786e60":"markdown","f2e3243d":"markdown","19a3fe28":"markdown","7f34f189":"markdown","a788bb69":"markdown","fd6d4fba":"markdown","d7f70e22":"markdown","bd31fd7f":"markdown","eb2a7cee":"markdown","002f5089":"markdown","7027eea0":"markdown","5b87d86c":"markdown","381af42b":"markdown","a983b07d":"markdown","796d2228":"markdown","6afa3096":"markdown","64da2dff":"markdown","d94704bb":"markdown","4a2b453d":"markdown","ecdf1204":"markdown","75bee3cd":"markdown","79ddef79":"markdown","8e55aa44":"markdown","463f4611":"markdown","8c43e8f9":"markdown","dae35b41":"markdown","e1332df0":"markdown","2eba3ab0":"markdown","12754259":"markdown","efd0a784":"markdown","b728eb8c":"markdown","57be03bf":"markdown","e3f0de2d":"markdown","d4f26341":"markdown","fc922ce3":"markdown","820bfd09":"markdown","c555f318":"markdown","10e726f6":"markdown","c1579317":"markdown","c0a68b7f":"markdown","deb4dbeb":"markdown","baf8cfe2":"markdown","9dff4138":"markdown","5ef30984":"markdown","eb9a98b9":"markdown","5b113e54":"markdown","3e4ca7af":"markdown","ceb1b023":"markdown","6378f5ca":"markdown","afd2be5d":"markdown","b0567a91":"markdown","f309b839":"markdown","6149a12c":"markdown"},"source":{"f441c00c":"!pip install sentence_transformers\n!pip install faiss-gpu","badfb396":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sentence_transformers import SentenceTransformer, util, CrossEncoder, InputExample, losses, models, datasets\nimport torch\nimport os\nimport csv\nimport pickle\nimport time\nimport faiss\nimport glob\nfrom pprint import pprint\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom tqdm import tqdm\nfrom torch import nn\nimport random\nimport gc\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3929f42c":"embedder = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')","8b7deb37":"for ix,csv in enumerate(glob.glob('..\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/*.csv')):\n    if ix==0:\n        data=pd.read_csv(csv)\n    else:\n        temp=pd.read_csv(csv)\n        data=pd.concat([data,temp],axis=0).reset_index(drop=True)","7b09cb0d":"data=data.dropna(subset=['title','abstract']).reset_index(drop=True)\ndata['abstract']=data['abstract'].apply(lambda x: x.replace('\\n',' ')[9:].strip())\ndata['text']=data['text'].apply(lambda x: x.replace('\\n',' ')[12:].strip())","c4885367":"data.head()","2854e286":"data.shape","d81e1ec4":"class faiss_search:\n    \n    def __init__(self,max_corpus_size = 100000,embedding_size= 768,top_k_hits= 5):\n        \n        self.embedding_cache_path = 'abstract-embeddings-{}-size-{}.pkl'.format('msmarco-distilbert-base-dot-prod-v3', max_corpus_size)\n\n        #Defining our FAISS index\n        #Number of clusters used for faiss. Select a value 4*sqrt(N) to 16*sqrt(N) - https:\/\/github.com\/facebookresearch\/faiss\/wiki\/Guidelines-to-choose-an-index\n        n_clusters = round(np.sqrt(data.shape[0])*4)\n\n        #We use Inner Product (dot-product) as Index. We will normalize our vectors to unit length, then is Inner Product equal to cosine similarity\n        quantizer = faiss.IndexFlatIP(embedding_size)\n        self.index = faiss.IndexIVFFlat(quantizer, embedding_size, n_clusters, faiss.METRIC_INNER_PRODUCT)\n\n        #Number of clusters to explorer at search time. We will search for nearest neighbors in 3 clusters.\n        self.index.nprobe = 3\n        \n    \n    def embed_corpus(self,data,embedder):\n    \n        #Check if embedding cache path exists\n        if not os.path.exists(self.embedding_cache_path):\n\n            corpus_sentences = data['abstract'].values.tolist()\n            print(\"Encode the corpus. This might take a while\")\n            corpus_embeddings = embedder.encode(corpus_sentences, show_progress_bar=True, convert_to_numpy=True)\n\n            print(\"Store file on disc\")\n            with open(self.embedding_cache_path, \"wb\") as fOut:\n                pickle.dump({'sentences': corpus_sentences, 'embeddings': corpus_embeddings}, fOut)\n        else:\n            print(\"Load pre-computed embeddings from disc\")\n            with open(self.embedding_cache_path, \"rb\") as fIn:\n                cache_data = pickle.load(fIn)\n                corpus_sentences = cache_data['sentences']\n                corpus_embeddings = cache_data['embeddings']\n                \n        return corpus_sentences,corpus_embeddings\n                \n    def index_data(self,corpus_sentences,corpus_embeddings):\n        ### Create the FAISS index\n        print(\"Start creating FAISS index\")\n        # First, we need to normalize vectors to unit length\n        corpus_embeddings = corpus_embeddings \/ np.linalg.norm(corpus_embeddings, axis=1)[:, None]\n\n        # Then we train the index to find a suitable clustering\n        self.index.train(corpus_embeddings)\n\n        # Finally we add all embeddings to the index\n        self.index.add(corpus_embeddings)\n\n        print(\"Corpus loaded with {} sentences \/ embeddings\".format(len(corpus_sentences)))\n","0fa3b3b7":"faiss_obj=faiss_search()\ncorpus_sentences,corpus_embeddings=faiss_obj.embed_corpus(data,embedder)\nfaiss_obj.index_data(corpus_sentences,corpus_embeddings)","f4f98a16":"######### Search in the index ###########\ntop_k_hits=5\nquery='death rates of covid case'\n\nstart_time = time.time()\ntitle_embedding = embedder.encode(query)\n\n#FAISS works with inner product (dot product). When we normalize vectors to unit length, inner product is equal to cosine similarity\ntitle_embedding = title_embedding \/ np.linalg.norm(title_embedding)\ntitle_embedding = np.expand_dims(title_embedding, axis=0)\n\n# Search in FAISS. It returns a matrix with distances and corpus ids.\ndistances, corpus_ids = faiss_obj.index.search(title_embedding, top_k_hits)\n\n# We extract corpus ids and scores for the first query\nhits = [{'corpus_id': id, 'score': score} for id, score in zip(corpus_ids[0], distances[0])]\nhits = sorted(hits, key=lambda x: x['score'], reverse=True)\nend_time = time.time()\n\nprint(\"Input title:\", query)\nprint(\"\\n\")\nprint(\"Results (after {:.3f} seconds):\".format(end_time-start_time))\nprint(\"\\n\")\nfor hit in hits[0:top_k_hits]:\n    print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n\n# Approximate Nearest Neighbor (ANN) is not exact, it might miss entries with high cosine similarity\n# Here, we compute the recall of ANN compared to the exact results\ncorrect_hits = util.semantic_search(title_embedding, corpus_embeddings, top_k=top_k_hits)[0]\ncorrect_hits_ids = set([hit['corpus_id'] for hit in correct_hits])\n\nann_corpus_ids = set([hit['corpus_id'] for hit in hits])\nif len(ann_corpus_ids) != len(correct_hits_ids):\n    print(\"Approximate Nearest Neighbor returned a different number of results than expected\")\n\nrecall = len(ann_corpus_ids.intersection(correct_hits_ids)) \/ len(correct_hits_ids)\nprint(\"\\nApproximate Nearest Neighbor Recall@{}: {:.2f}\".format(top_k_hits, recall * 100))\n\nif recall < 1:\n    print(\"Missing results:\")\n    for hit in correct_hits[0:top_k_hits]:\n        if hit['corpus_id'] not in ann_corpus_ids:\n            print(\"\\t{:.3f}\\t{}\".format(hit['score'], corpus_sentences[hit['corpus_id']]))\n            \ngc.collect()\ndel faiss_obj,corpus_sentences,corpus_embeddings","6bb8ad71":"class faiss_index:\n    def __init__(self,data,model):\n        self.data=data\n        self.model=model\n    \n    def index(self):\n        encoded_data = self.model.encode(self.data['abstract'].values.tolist())\n        encoded_data = np.asarray(encoded_data.astype('float32'))\n        self.index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n        self.index.add_with_ids(encoded_data, np.array(range(0, len(self.data))))\n        faiss.write_index(self.index, 'test.index')\n        \n        \n    def fetch(self,idx):\n        info = self.data.iloc[idx]\n        meta_dict = {}\n        meta_dict['abstract'] = info['abstract']\n        return meta_dict\n\n    def search(self,query, top_k):\n        t=time.time()\n        query_vector = self.model.encode([query])\n        top_k = self.index.search(query_vector, top_k)\n        print('>>>> Results in Total Time: {}'.format(time.time()-t))\n        top_k_ids = top_k[1].tolist()[0]\n        top_k_ids = list(np.unique(top_k_ids))\n        results =  [self.fetch(idx) for idx in top_k_ids]\n        return results","d464bde8":"faiss_ix=faiss_index(data,embedder)\nfaiss_ix.index()\nresults=faiss_ix.search(query, 5)\n\nprint(\"\\n\")\nfor result in results:\n    print('\\t',result)\n    \ngc.collect()\ndel faiss_ix","9d793f1c":"#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\nbi_encoder = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\ntop_k = 10     #Number of passages we want to retrieve with the bi-encoder\n\n#The bi-encoder will retrieve k documents. We use a cross-encoder, to re-rank the results list to improve the quality\ncross_encoder = CrossEncoder('cross-encoder\/ms-marco-TinyBERT-L-2-v2')\n\npassages=data['abstract'].values.tolist()\n\n#If you like, you can also limit the number of passages you want to use\nprint(\"Passages:\", len(passages))\n\n\ndef search(query,index):\n    print(\"Input question:\", query)\n\n    ##### Sematic Search #####\n    # Encode the query using the bi-encoder and find potentially relevant passages\n    t=time.time()\n    query_vector = embedder.encode([query])\n    top_k = index.search(query_vector, 3)\n    top_k_ids = top_k[1].tolist()[0]\n    top_k_ids = list(np.unique(top_k_ids))\n    print('>>>> Results in Total Time: {}'.format(time.time()-t))\n\n    ##### Re-Ranking #####\n    # Now, score all retrieved passages with the cross_encoder\n    t=time.time()\n    cross_inp = [[query, passages[hit]] for hit in top_k_ids]\n    bienc_op=[passages[hit] for hit in top_k_ids]\n    cross_scores = cross_encoder.predict(cross_inp)\n    print('>>>> Results in Total Time: {}'.format(time.time()-t))\n\n    # Output of top-5 hits from bi-encoder\n    print(\"\\n-------------------------\\n\")\n    print(\"Top-3 Bi-Encoder Retrieval hits\")\n    results =  [passages[idx] for idx in top_k_ids]\n    for result in results:\n        print(\"\\t{}\".format(result.replace(\"\\n\", \" \")))\n        \n    # Output of top-5 hits from re-ranker\n    print(\"\\n-------------------------\\n\")\n    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n    for hit in np.argsort(np.array(cross_scores))[::-1]:\n        print(\"\\t{}\".format(bienc_op[hit].replace(\"\\n\", \" \")))\n","72cd788a":"query='what are the symptoms of influenza virus?'\nindex=faiss.read_index('test.index')\nsearch(query,index)","51e80766":"gc.collect()\ndel index,cross_encoder,bi_encoder,passages","571a1374":"tokenizer = T5Tokenizer.from_pretrained('BeIR\/query-gen-msmarco-t5-base-v1') #change to large and generate synthetic data fully\nmodel = T5ForConditionalGeneration.from_pretrained('BeIR\/query-gen-msmarco-t5-base-v1')\nmodel.eval()","267474b1":"#Select the device\ndevice = 'cuda'\nmodel.to(device)","808fb239":"# Parameters for generation\nbatch_size = 16 #Batch size\nnum_queries = 5 #Number of queries to generate for every paragraph\nmax_length_paragraph = 512 #Max length for paragraph\nmax_length_query = 64   #Max length for output query\n\ndef _removeNonAscii(s): return \"\".join(i for i in s if ord(i) < 128)\n\nparagraphs=data['abstract'].values.tolist()[:1000] ## we generate a sample synthetic data generation on 1000 abstracts\n\nwith open('generated_queries_all.tsv', 'w') as fOut:\n    for start_idx in tqdm(range(0, len(paragraphs), batch_size)):\n        sub_paragraphs = paragraphs[start_idx:start_idx+batch_size]\n        inputs = tokenizer.prepare_seq2seq_batch(sub_paragraphs, max_length=max_length_paragraph, truncation=True, return_tensors='pt').to(device)\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length_query,\n            do_sample=True,\n            top_p=0.95,\n            num_return_sequences=num_queries)\n\n        for idx, out in enumerate(outputs):\n            query = tokenizer.decode(out, skip_special_tokens=True)\n            query = _removeNonAscii(query)\n            para = sub_paragraphs[int(idx\/num_queries)]\n            para = _removeNonAscii(para)\n            fOut.write(\"{}\\t{}\\n\".format(query.replace(\"\\t\", \" \").strip(), para.replace(\"\\t\", \" \").strip()))","87f1170c":"gc.collect()\ndel model,tokenizer,paragraphs","45bd4db8":"query,para","9d027944":"train_examples = [] \nwith open('generated_queries_all.tsv') as fIn:\n    for line in fIn:\n        try:\n            query, paragraph = line.strip().split('\\t', maxsplit=1)\n            train_examples.append(InputExample(texts=[query, paragraph]))\n        except:\n            pass\n        \nrandom.shuffle(train_examples)\n\n# For the MultipleNegativesRankingLoss, it is important\n# that the batch does not contain duplicate entries, i.e.\n# no two equal queries and no two equal paragraphs.\n# To ensure this, we use a special data loader\ntrain_dataloader = datasets.NoDuplicatesDataLoader(train_examples, batch_size=8)\n\n# Now we create a SentenceTransformer model from scratch\nword_emb = models.Transformer('sentence-transformers\/msmarco-distilbert-base-dot-prod-v3')\npooling = models.Pooling(word_emb.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_emb, pooling])\n\n\n# MultipleNegativesRankingLoss requires input pairs (query, relevant_passage)\n# and trains the model so that is is suitable for semantic search\ntrain_loss = losses.MultipleNegativesRankingLoss(model)\n\n\n#Tune the model\nnum_epochs = 3\nwarmup_steps = int(len(train_dataloader) * num_epochs * 0.1)\nmodel.fit(train_objectives=[(train_dataloader, train_loss)], epochs=num_epochs, warmup_steps=warmup_steps, show_progress_bar=True)\n\nos.makedirs('search', exist_ok=True)\nmodel.save('search\/search-model')\n","25616bb8":"gc.collect()\ndel word_emb,pooling","b5f0a7a9":"faiss_ix=faiss_index(data,model)\nfaiss_ix.index()\n\ngc.collect()\ndel faiss_ix,model","bfe630cd":"#We use the Bi-Encoder to encode all passages, so that we can use it with sematic search\nmodel_name = 'search\/search-model'\nbi_encoder = SentenceTransformer(model_name)\ntop_k = 100     #Number of passages we want to retrieve with the bi-encoder\n\n#The bi-encoder will retrieve 100 documents. We use a cross-encoder, to re-rank the results list to improve the quality\ncross_encoder = CrossEncoder('cross-encoder\/ms-marco-TinyBERT-L-2-v2')\n\n# As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n\npassages=data['abstract'].values.tolist()\n\n#If you like, you can also limit the number of passages you want to use\nprint(\"Passages:\", len(passages))\n\n\ndef search(query,index):\n    print(\"Input question:\", query)\n\n    ##### Sematic Search #####\n    # Encode the query using the bi-encoder and find potentially relevant passages\n    t=time.time()\n    query_vector = bi_encoder.encode([query])\n    top_k = index.search(query_vector, 3)\n    top_k_ids = top_k[1].tolist()[0]\n    top_k_ids = list(np.unique(top_k_ids))\n    print('>>>> Results in Total Time: {}'.format(time.time()-t))\n\n    ##### Re-Ranking #####\n    # Now, score all retrieved passages with the cross_encoder\n    t=time.time()\n    cross_inp = [[query, passages[hit]] for hit in top_k_ids]\n    bienc_op=[passages[hit] for hit in top_k_ids]\n    cross_scores = cross_encoder.predict(cross_inp)\n    print('>>>> Results in Total Time: {}'.format(time.time()-t))\n\n    # Output of top-5 hits from bi-encoder\n    print(\"\\n-------------------------\\n\")\n    print(\"Top-3 Bi-Encoder Retrieval hits\")\n    for result in bienc_op:\n        print(\"\\t{}\".format(result.replace(\"\\n\", \" \")))\n        \n#     for idx in range(len(cross_scores)):\n#         hits[idx]['cross-score'] = cross_scores[idx]\n    \n    # Output of top-5 hits from re-ranker\n    print(\"\\n-------------------------\\n\")\n    print(\"Top-3 Cross-Encoder Re-ranker hits\")\n    for hit in np.argsort(np.array(cross_scores))[::-1]:\n        print(\"\\t{}\".format(bienc_op[hit].replace(\"\\n\", \" \")))\n","49966e75":"query='death rates of covid case'\nindex=faiss.read_index('test.index')\nsearch(query,index)","4141a4d1":"print(len(passages), \"papers loaded\")\n\n#We then load the allenai-specter model with SentenceTransformers\nmodel = SentenceTransformer('allenai-specter')\n\n#To encode the papers, we must combine the title and the abstracts to a single string\npaper_texts = [paper[1]['title'] + '[SEP]' + paper[1]['abstract'] for paper in data.iterrows()]\n\n#Compute embeddings for all papers\ncorpus_embeddings = model.encode(paper_texts, convert_to_tensor=True)\n\n\n#We define a function, given title & abstract, searches our corpus for relevant (similar) papers\ndef search_papers(title, abstract):\n  query_embedding = model.encode(title+'[SEP]'+abstract, convert_to_tensor=True)\n\n  search_hits = util.semantic_search(query_embedding, corpus_embeddings)\n  search_hits = search_hits[0]  #Get the hits for the first query\n\n  print(\"\\n\\nPaper:\", title)\n  print(\"Most similar papers:\")\n  for hit in search_hits:\n    related_paper = data.loc[hit['corpus_id']]\n    print(\"{:.2f}\\t{}\\t{} {}\".format(hit['score'], related_paper['title'], related_paper['authors'], related_paper['affiliations']))","80b0a268":"search_papers(title='Specializing Word Embeddings (for Parsing) by Information Bottleneck',\n              abstract='Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.')\n","7e5711ce":"# <font color='brown' size=4>5. Acknowledgements<\/font>\n\n* https:\/\/medium.com\/mlearning-ai\/semantic-search-with-s-bert-is-all-you-need-951bc710e160\n* https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\n* https:\/\/www.sbert.net\/examples\/applications\/semantic-search\/README.html#background","f28733cb":"#### <b>Cheers, happy kaggling!!!<\/b>","2a40e986":"Suitable models for <b>symmetric semantic search<\/b>:\n\n* paraphrase-distilroberta-base-v1 \/ paraphrase-xlm-r-multilingual-v1\n\n* quora-distilbert-base \/ quora-distilbert-multilingual\n\n* distiluse-base-multilingual-cased-v2\n\nSuitable models for <b>asymmetric semantic search<\/b>:\n\n* msmarco-distilbert-base-v2","ff786e60":"<b>Bi-encoder with faiss search using flat indexing<\/b>","f2e3243d":"# <font color='brown' size=4>2. Sbert overview<\/font>","19a3fe28":"## <font color='brown' size=4>2.1 Architecture (STS benchmark)<\/font>","7f34f189":"The below cell compares the actual result which is computed using similarity index with faiss indexed results. Let's query for scientific datasets which are publicly available among this corpus","a788bb69":"<img src='https:\/\/www.sbert.net\/_images\/SBERT_Siamese_Network1.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Sbert<\/font><\/div>","fd6d4fba":"Here, we use inverted index and the dataset is clustered into buckets and at search time, only a fraction of the buckets are visited (nprobe buckets). The clustering is performed on a representative sample of the dataset vectors, typically a sample of the dataset. We indicate the optimal size for this sample.","d7f70e22":"# <font color='brown' size=4>3. FAISS<\/font>","bd31fd7f":"<b>Synthetic Query Generation:<\/b>\n\n   <p>We use synthetic query generation to achieve our goal. We start with the passage from our document collection and create for these possible queries users might ask \/ might search for.<\/p>\n\n<b>BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval<\/b> Models presented a method to learn (or adapt) model for asymmetric semantic search without requiring training data.\n\n\nLet's finetune for few epochs on our dataset based on the above idea","eb2a7cee":"Overall, our flow for semantic search will look like this:","002f5089":"<p>We could have easily fine-tuned a sentence-transformer model on our dataset given if we had query & relevant passages information. But you would not have this data if building something from ground zero. (Here we are not dealing with pre-training approaches of transformer models. It is expensive and requires huge deal of data. Not to forget the domain here )<\/p>\n\n<p>But, Can we device some unsupervised approach to fine-tune our model on our dataset.<\/p>","7027eea0":"<div class=\"alert simple-alert\">\n\ud83d\udccc <b>Note<\/b>: Here we use dot product pretrained models, since our data has features like title,abstract and text. we can use either keyword\/title to get similar research papers as our document, which falls under short query-long document retrieval approach\n<\/div>","5b87d86c":"## <font color='brown' size=4>3.1 Different indexes<\/font>","381af42b":"<p>Faiss is a C++ based library built by Facebook AI with a complete wrapper in python, to index vectorized data and to perform efficient searches on them. It solves limitations of traditional query search engines that are optimised for hash-based searches, and provides more scalable similarity search functions. Most importantly they have support for both <b>CPU and GPU version<\/b><\/p>","a983b07d":"For complex search tasks, the search can significantly be improved by using <b>Retrieve & Re-Rank<\/b>.","796d2228":"# <font color='brown' size=4>4.3 Finetuning Bi-encoder unsupervised way<\/font>","6afa3096":"<b>Retrieve & Re-Rank Pipeline:<\/b>","64da2dff":"FAISS has a handful of features including:\n\n* GPU and multithreaded support for index operations\n* Dimensionality reduction: vectors with large dimensions can be reduced to smaller dimensions using PCA\n* Quantisation: FAISS emphasises on product quantisation for compressing and storing vectors of large dimensions\n* Batch processing i.e searching for multiple queries at a time","d94704bb":"<p> Semantic search is a data searching technique in a which a search query aims to not only find keywords, but to determine the intent and contextual meaning of the words a person is using for search.<\/p> \n    \n<p>Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines, that only finds documents based on lexical matches, semantic search can also find synonyms.<\/p>","4a2b453d":"# <font color='brown' size=4>1. Semantic search<\/font>","ecdf1204":"<img src='https:\/\/www.sbert.net\/_images\/SBERT_Architecture.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Sbert<\/font><\/div>","75bee3cd":"## <font color='brown' size=4>1.2 Models available in sbert<\/font>","79ddef79":"<img src='https:\/\/raw.githubusercontent.com\/UKPLab\/sentence-transformers\/master\/docs\/img\/InformationRetrieval.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Medium<\/font><\/div>","8e55aa44":"<b>Re-indexing<\/b>","463f4611":"<p>The only index that can guarantee exact results is the IndexFlatL2 or IndexFlatIP. It provides the baseline for results for the other indexes. It does not compress the vectors, but does not add overhead on top of them. The flat index does not require training and does not have parameters.<\/p>","8c43e8f9":"Spector encodes paper titles and abstracts into a vector space, then use util.semantic_search() to find the most similar papers. Same pipeline we followed above but without faiss indexing","dae35b41":"<div class=\"alert simple-alert\">\n\ud83d\udccc <b>Note<\/b>: We could see that our recall seems to be @60 for the given query, when we use normal cosine similarity based approach vs indexing. We missed out few important articles on death rates\n<\/div>","e1332df0":"## <font color='brown' size=4>1.1 Types of semantic search<\/font>","2eba3ab0":"<img src='https:\/\/www.ontotext.com\/wp-content\/uploads\/2019\/07\/GraphDB-Semantic-Similarity-Plugin-for-Identifying-Related-Terms-Documents-1.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","12754259":"<p>The most simple way is to have sentence pairs annotated with a score indicating their similarity, e.g. on a scale 0 to 1. We can then train the network with a Siamese Network Architecture<\/p>","efd0a784":"<p>Given a search query, we first use a retrieval system that retrieves a large list of e.g. 100 possible hits which are potentially relevant for the query. For the retrieval, we can use either lexical search, e.g. with ElasticSearch, or we can use dense retrieval with a bi-encoder.\n\nHowever, the retrieval system might retrieve documents that are not that relevant for the search query. Hence, in a second stage, we use a re-ranker based on a cross-encoder that scores the relevancy of all candidates for the given search query.\n\nThe output will be a ranked list of hits we can present to the user.<\/p>","b728eb8c":"There are multiple models available in sbert which was finetuned on different datasets and benchmarked against for comparison.  Some of them are listed below \n\n<img src='https:\/\/miro.medium.com\/max\/700\/1*-lVFYR44EnS1LpRtN6ZVvg.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","57be03bf":"<img src=\"https:\/\/miro.medium.com\/max\/3840\/1*zIko_UJAnI5oOI7f9Dwacw.png\">\n<center><font color='brown' size=5>\ud83d\udcd6 Billion scale - similarity search<\/font><\/center>","e3f0de2d":"# <font color='brown' size=4>4.3.1 Bi-encoder + Cross encoder with FAISS search (finetuned)<\/font>","d4f26341":"<p>Voila, we seem to have a slight improvement when re-ranked<\/p>","fc922ce3":"# <font color='brown' size=4>4.2 Bi-encoder + Cross encoder with FAISS search<\/font>","820bfd09":"# <font color='brown' size=4>4.1 Bi-encoder with FAISS search (inverted index search)<\/font>","c555f318":"Faiss offers different indexes based on the following factors\n* search time\n* search quality\n* memory used per index vector\n* training time\n* need for external data for unsupervised training","10e726f6":"The paragraphs input to this code are nothing but chunks of abstracts and each chunk will have maximum of 5 synthetically generated queries. If you think for a moment what are we trying to do here is have the possible information present in a paragraphs represented as questions and then use this knowledge tuple to fine-tune a s-bert model which will capture the semantic and syntactic information mapping between these tuples.","c1579317":"# <font color='brown' size=4>4.4 Allen-ai spector model<\/font>","c0a68b7f":"<div class=\"alert simple-alert\">\n\ud83d\udccc <b>Note<\/b>: For each sentence pair, we pass sentence A and sentence B through our network which yields the embeddings u und v. The similarity of these embeddings is computed using cosine similarity and the result is compared to the gold similarity score. This allows our network to be fine-tuned and to recognize the similarity of sentences.\n<\/div>","deb4dbeb":"**Types:**\n* Symmetric search\n* Asymmetric search","baf8cfe2":"<p>A critical distinction between <b>symmetric vs. asymmetric semantic search:<\/b> is as follows<\/p>\n\n<p>For symmetric semantic search your query and the entries in your corpus are of about the same length and have the same amount of content. An example would be searching for similar questions: Your query could for example be \u201cHow to learn Python online?\u201d and you want to find an entry like \u201cHow to learn Python on the web?\u201d. For symmetric tasks, you could potentially flip the query and the entries in your corpus.<\/p>\n\n<p>For asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query. An example would be a query like \u201cWhat is Python\u201d and you wand to find the paragraph \u201cPython is an interpreted, high-level and general-purpose programming language. Python\u2019s design philosophy \u2026\u201d. For asymmetric tasks, flipping the query and the entries in your corpus usually does not make sense.<\/p>","9dff4138":"# <font color='brown' size=4>4. Implementation<\/font>","5ef30984":"<div class=\"alert simple-alert\">\n\ud83d\udccc <b>Note<\/b>: We feed the input sentence or text into a transformer network like BERT. BERT produces contextualized word embeddings for all input tokens in our text. As we want a fixed-sized output representation (vector u), we need a pooling layer. Different pooling options are available, the most basic one is mean-pooling: We simply average all contextualized word embeddings BERT is giving us. This gives us a fixed 768 dimensional output vector independet how long our input text was.\n<\/div>","eb9a98b9":"We will use this [dataset](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge) for our exploration. This is a resource of over 500,000 scholarly articles, including over 200,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease.","5b113e54":"<img src='https:\/\/raw.githubusercontent.com\/UKPLab\/sentence-transformers\/master\/docs\/img\/SemanticSearch.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Sbert<\/font><\/div>","3e4ca7af":"## <font color='brown' size=4>2.2 Choosing the right model<\/font>","ceb1b023":"# <font color='brown' size=4>Objective:<\/font> \n        \n<p> Here, we explore on how semantic search is used for information retrieval to search across millions of records in less than a second. The same idea of indexing based retrieval can be extended to other structured and unstructured data like image and tabular data. Let's get started<\/p>","6378f5ca":"# Table of Contents\n\n- 1. Semantic search\n   - 1.1 Types of semantic search\n   - 1.2 Models available in sbert\n\n- 2. Sbert overview\n   - 2.1 Architecture\n   - 2.2 Choosing the right model\n      \n- 3. FAISS\n   - 3.1 Different indexes\n\n- 4. Implementation\n   - 4.1 Bi-encoder with FAISS search\n   - 4.2 Bi-encoder + Cross encoder with FAISS search\n   - 4.3 Finetuning Bi-encoder unsupervised way\n       - 4.3.1 Bi-encoder + Cross encoder with FAISS search (finetuned)\n   - 4.4 Allen-ai spector model\n       \n- 5. Acknowledgements","afd2be5d":"Let's see the sample query and passage which got generated above","b0567a91":"<p>It is critical that you choose the right model for your type of task. It is mostly distinguished by the type of data it has been trained on. Also models tuned for cosine-similarity will prefer the retrieval of short documents, while models tuned for dot-product will prefer the retrieval of longer documents. Depending on your task, the models of the one or the other type are preferable.<\/p>","f309b839":"<p>Sentence-BERT (SBERT), a modification of the BERT network uses siamese and triplet networks to derive semantically meaningful sentence embeddings\n    \nFor sentence \/ text embeddings, we want to map a variable length input text to a fixed sized dense vector. The most basic network architecture we can use is the following:\n<\/p>","6149a12c":"<img src='https:\/\/miro.medium.com\/max\/4560\/1*k7rgUFTqWdHyBY72PlCIow.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Medium<\/font><\/div>"}}