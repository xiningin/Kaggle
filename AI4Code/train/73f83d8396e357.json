{"cell_type":{"3ecbfc32":"code","93590719":"code","19612d9f":"code","cb5fce55":"code","c8d5e71a":"code","92f69e21":"code","fd452b41":"code","72850bef":"code","aa61802d":"code","a3900100":"code","8e73a76f":"code","4b4db5d7":"code","9761502d":"code","4f229dd6":"code","b3ee4df9":"code","bc962e1e":"code","9320a9dd":"code","d6e994aa":"code","9989179c":"code","62b2d65b":"code","0df5c24b":"code","beee18e3":"code","dfda88d6":"code","8008eda4":"code","cd872262":"code","ff382ea5":"code","675e900d":"code","d5c69b46":"code","f71be467":"code","56e94c31":"code","e950e72c":"code","53d20da1":"code","f293700e":"markdown","4fc693bf":"markdown","88362642":"markdown","e57087db":"markdown","e17f6444":"markdown","a3c69ae9":"markdown","cd4753a2":"markdown","456a0f5e":"markdown","867960a3":"markdown","368c5f15":"markdown","7c1da40c":"markdown","d5c80299":"markdown","4e8c7ca0":"markdown","98324328":"markdown","77b3fd16":"markdown","558345fe":"markdown","5323af87":"markdown","fa7ad742":"markdown","20db7acc":"markdown"},"source":{"3ecbfc32":"# !pip install -r requirements.txt","93590719":"# !python preprocess.py --data_root \".\/mvlrs_v1\/main\" --preprocessed_root \".\/lrs2_preprocessed\" ","19612d9f":"%cd ..\/input\/wave2lip\/wav2lip_homework","cb5fce55":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass Conv2d(nn.Module):\n    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        ########TODO######################\n        # \u6309\u4e0b\u9762\u7684\u7f51\u7edc\u7ed3\u6784\u8981\u6c42\uff0c\u8865\u5168\u4ee3\u7801\n        # self.conv_block: Sequential\u7ed3\u6784\uff0cConv2d+BatchNorm\n        # self.act: relu\u6fc0\u6d3b\u51fd\u6570\n        self.conv_block = nn.Sequential(\n                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n                            nn.BatchNorm2d(cout)\n                            )\n        self.act = nn.ReLU()\n        self.residual = residual\n\n    def forward(self, x):\n        out = self.conv_block(x)\n        if self.residual:\n            out += x\n        return self.act(out)","c8d5e71a":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass SyncNet_color(nn.Module):\n    def __init__(self):\n        super(SyncNet_color, self).__init__()\n        \n        ################TODO###################\n        #\u6839\u636e\u4e0a\u9762\u63d0\u4f9b\u7684\u7f51\u7edc\u7ed3\u6784\u56fe\uff0c\u8865\u5168\u4e0b\u9762\u5377\u79ef\u7f51\u7edc\u7684\u53c2\u6570\n\n        self.face_encoder = nn.Sequential(\n            Conv2d(15, 32, kernel_size=(7, 7), stride=1, padding=3),\n\n            Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=1),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=0),\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n\n        self.audio_encoder = nn.Sequential(\n            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n\n    def forward(self, audio_sequences, face_sequences): # audio_sequences := (B, dim, T)\n        \n        #########################TODO#######################\n        # \u6b63\u5411\u4f20\u64ad\n        face_embedding = self.face_encoder(face_sequences)\n        audio_embedding = self.audio_encoder(audio_sequences)\n\n        audio_embedding = audio_embedding.view(audio_embedding.size(0), -1)\n        face_embedding = face_embedding.view(face_embedding.size(0), -1)\n\n        audio_embedding = F.normalize(audio_embedding, p=2, dim=1)\n        face_embedding = F.normalize(face_embedding, p=2, dim=1)\n\n\n        return audio_embedding, face_embedding","92f69e21":"from os.path import dirname, join, basename, isfile\nfrom tqdm import tqdm\n\nfrom models import SyncNet_color as SyncNet\nimport audio\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils import data as data_utils\nimport numpy as np\n\nfrom glob import glob\n\nimport os, random, cv2, argparse\nfrom hparams import hparams, get_image_list","fd452b41":"global_step = 0 #\u8d77\u59cb\u7684step\nglobal_epoch = 0 #\u8d77\u59cb\u7684epoch\nuse_cuda = torch.cuda.is_available()#\u8bad\u7ec3\u7684\u8bbe\u5907 cpu or gpu\nprint('use_cuda: {}'.format(use_cuda))\n\nsyncnet_T = 5 ## \u6bcf\u6b21\u9009\u53d6200ms\u7684\u89c6\u9891\u7247\u6bb5\u8fdb\u884c\u8bad\u7ec3\uff0c\u89c6\u9891\u7684fps\u4e3a25\uff0c\u6240\u4ee5200ms\u5bf9\u5e94\u7684\u5e27\u6570\u4e3a\uff1a25*0.2=5\u5e27\nsyncnet_mel_step_size = 16 # 200ms\u5bf9\u5e94\u7684\u58f0\u97f3\u7684mel-spectrogram\u7279\u5f81\u7684\u957f\u5ea6\u4e3a16.\ndata_root=\"\/kaggle\/input\/wav2lippreprocessed\/lrs2_preprocessed\" #\u6570\u636e\u96c6\u7684\u4f4d\u7f6e","72850bef":"class Dataset(object):\n    def __init__(self, split):\n        self.all_videos = get_image_list(data_root, split)\n\n    def get_frame_id(self, frame):\n        return int(basename(frame).split('.')[0])\n\n    def get_window(self, start_frame):\n        start_id = self.get_frame_id(start_frame)\n        vidname = dirname(start_frame)\n\n        window_fnames = []\n        for frame_id in range(start_id, start_id + syncnet_T):\n            frame = join(vidname, '{}.jpg'.format(frame_id))\n            if not isfile(frame):\n                return None\n            window_fnames.append(frame)\n        return window_fnames\n\n    def crop_audio_window(self, spec, start_frame):\n        # num_frames = (T x hop_size * fps) \/ sample_rate\n        start_frame_num = self.get_frame_id(start_frame)\n        start_idx = int(80. * (start_frame_num \/ float(hparams.fps)))\n\n        end_idx = start_idx + syncnet_mel_step_size\n\n        return spec[start_idx : end_idx, :]\n\n\n    def __len__(self):\n        return len(self.all_videos)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        return: x,mel,y\n        x: \u4e94\u5f20\u5634\u5507\u56fe\u7247\n        mel\uff1a\u5bf9\u5e94\u7684\u8bed\u97f3\u7684mel spectrogram\n        t\uff1a\u540c\u6b65or\u4e0d\u540c\u6b65\n        \n        \"\"\"\n        while 1:\n            idx = random.randint(0, len(self.all_videos) - 1)\n            vidname = self.all_videos[idx]\n\n            img_names = list(glob(join(vidname, '*.jpg')))\n            if len(img_names) <= 3 * syncnet_T:\n                continue\n            img_name = random.choice(img_names)\n            wrong_img_name = random.choice(img_names)\n            while wrong_img_name == img_name:\n                wrong_img_name = random.choice(img_names)\n            \n            \n            #\u968f\u673a\u51b3\u5b9a\u662f\u4ea7\u751f\u8d1f\u6837\u672c\u8fd8\u662f\u6b63\u6837\u672c\n            if random.choice([True, False]):\n                y = torch.ones(1).float()\n                chosen = img_name\n            else:\n                y = torch.zeros(1).float()\n                chosen = wrong_img_name\n\n            window_fnames = self.get_window(chosen)\n            if window_fnames is None:\n                continue\n\n            window = []\n            all_read = True\n            for fname in window_fnames:\n                img = cv2.imread(fname)\n                if img is None:\n                    all_read = False\n                    break\n                try:\n                    img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n                except Exception as e:\n                    all_read = False\n                    break\n\n                window.append(img)\n\n            if not all_read: continue\n\n            try:\n                wavpath = join(vidname, \"audio.wav\")\n                wav = audio.load_wav(wavpath, hparams.sample_rate)\n\n                orig_mel = audio.melspectrogram(wav).T\n            except Exception as e:\n                continue\n\n            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n\n            if (mel.shape[0] != syncnet_mel_step_size):\n                continue\n\n            # H x W x 3 * T\n            x = np.concatenate(window, axis=2) \/ 255.\n            x = x.transpose(2, 0, 1)\n            x = x[:, x.shape[1]\/\/2:]\n\n            x = torch.FloatTensor(x)\n            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n\n            return x, mel, y","aa61802d":"ds=Dataset(\"train\")\nx,mel,t=ds[0]\nprint(x.shape)\nprint(mel.shape)\nprint(t.shape)","a3900100":"import matplotlib.pyplot as plt\nplt.imshow(mel[0].numpy())","8e73a76f":"plt.imshow(x[:3,:,:].transpose(0,2).numpy())","4b4db5d7":"#\u635f\u5931\u51fd\u6570\u7684\u5b9a\u4e49\nlogloss = nn.BCELoss() # \u4ea4\u53c9\u71b5\u635f\u5931\ndef cosine_loss(a, v, y):#\u4f59\u5f26\u76f8\u4f3c\u5ea6\u635f\u5931\n    \"\"\"\n    a: audio_encoder\u7684\u8f93\u51fa\n    v: video face_encoder\u7684\u8f93\u51fa\n    y: \u662f\u5426\u540c\u6b65\u7684\u771f\u5b9e\u503c\n    \"\"\"\n    d = nn.functional.cosine_similarity(a, v)\n    loss = logloss(d.unsqueeze(1), y)\n\n    return loss","9761502d":"def train(device, model, train_data_loader, test_data_loader, optimizer,\n          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n\n    global global_step, global_epoch\n    resumed_step = global_step\n    \n    while global_epoch < nepochs:\n        running_loss = 0.\n        prog_bar = tqdm(enumerate(train_data_loader))\n        for step, (x, mel, y) in prog_bar:\n            model.train()\n            optimizer.zero_grad()\n\n            #####TODO###########\n            ####################\n            #\u8865\u5168\u6a21\u578b\u7684\u8bad\u7ec3\n            x = x.to(device)\n\n            mel = mel.to(device)\n\n            a, v = model(mel, x)\n            y = y.to(device)\n\n            loss = cosine_loss(a, v, y)\n            loss.backward()\n            optimizer.step()\n                \n            \n\n            global_step += 1\n            cur_session_steps = global_step - resumed_step\n            running_loss += loss.item()\n\n            if global_step == 1 or global_step % checkpoint_interval == 0:\n                save_checkpoint(\n                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n\n            if global_step % hparams.syncnet_eval_interval == 0:\n                with torch.no_grad():\n                    eval_model(test_data_loader, global_step, device, model, checkpoint_dir)\n\n            prog_bar.set_description('Epoch: {} Loss: {}'.format(global_epoch, running_loss \/ (step + 1)))\n\n        global_epoch += 1\n\ndef eval_model(test_data_loader, global_step, device, model, checkpoint_dir):\n    #\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\n    eval_steps = 1400\n    print('Evaluating for {} steps'.format(eval_steps))\n    losses = []\n    while 1:\n        for step, (x, mel, y) in enumerate(test_data_loader):\n\n            model.eval()\n\n            # Transform data to CUDA device\n            x = x.to(device)\n\n            mel = mel.to(device)\n\n            a, v = model(mel, x)\n            y = y.to(device)\n\n            loss = cosine_loss(a, v, y)\n            losses.append(loss.item())\n\n            if step > eval_steps: break\n\n        averaged_loss = sum(losses) \/ len(losses)\n        print(averaged_loss)\n\n        return\n\nlatest_checkpoint_path = ''\ndef save_checkpoint(model, optimizer, step, checkpoint_dir, epoch):\n    #\u4fdd\u5b58\u8bad\u7ec3\u7684\u7ed3\u679c checkpoint\n    global latest_checkpoint_path\n    \n    checkpoint_path = join(\n        checkpoint_dir, \"checkpoint_step{:09d}.pth\".format(global_step))\n    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n    torch.save({\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer_state,\n        \"global_step\": step,\n        \"global_epoch\": epoch,\n    }, checkpoint_path)\n    latest_checkpoint_path = checkpoint_path\n    print(\"Saved checkpoint:\", checkpoint_path)\n\ndef _load(checkpoint_path):\n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path,\n                                map_location=lambda storage, loc: storage)\n    return checkpoint\n\ndef load_checkpoint(path, model, optimizer, reset_optimizer=False):\n    #\u8bfb\u53d6\u6307\u5b9acheckpoint\u7684\u4fdd\u5b58\u4fe1\u606f\n    global global_step\n    global global_epoch\n\n    print(\"Load checkpoint from: {}\".format(path))\n    checkpoint = _load(path)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    if not reset_optimizer:\n        optimizer_state = checkpoint[\"optimizer\"]\n        if optimizer_state is not None:\n            print(\"Load optimizer state from {}\".format(path))\n            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    global_step = checkpoint[\"global_step\"]\n    global_epoch = checkpoint[\"global_epoch\"]\n\n    return model","4f229dd6":"checkpoint_dir = \"\/kaggle\/working\/expert_checkpoints\/\" #\u6307\u5b9a\u5b58\u50a8 checkpoint\u7684\u4f4d\u7f6e\ncheckpoint_path = '\/kaggle\/input\/wav2lip24epoch\/expert_checkpoints\/checkpoint_step000060000.pth'\n# \u6307\u5b9a\u52a0\u8f7dcheckpoint\u7684\u8def\u5f84\uff0c\u7b2c\u4e00\u6b21\u8bad\u7ec3\u65f6\u4e0d\u9700\u8981\uff0c\u540e\u7eed\u5982\u679c\u60f3\u4ece\u67d0\u4e2acheckpoint\u6062\u590d\u8bad\u7ec3\uff0c\u53ef\u6307\u5b9a\u3002\n\nif not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n\n# Dataset and Dataloader setup\ntrain_dataset = Dataset('train')\ntest_dataset = Dataset('val')\n\n############TODO#########\n#####Train Dataloader and Test Dataloader \n#### \u5177\u4f53\u7684bacthsize\u7b49\u53c2\u6570\uff0c\u53c2\u8003 hparams.py\u6587\u4ef6\ntrain_data_loader = data_utils.DataLoader(\n    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n    num_workers=hparams.num_workers)\n\ntest_data_loader = data_utils.DataLoader(\n    test_dataset, batch_size=hparams.batch_size,\n    num_workers=8)\n\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n# Model\n#####\u5b9a\u4e49 SynNet\u6a21\u578b\uff0c\u5e76\u52a0\u8f7d\u5230\u6307\u5b9a\u7684device\u4e0a\nmodel = SyncNet().to(device)\nprint('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n\n####\u5b9a\u4e49\u4f18\u5316\u5668\uff0c\u4f7f\u7528adam\uff0clr\u53c2\u8003hparams.py\u6587\u4ef6\noptimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n                       lr=1e-5)\n\nif checkpoint_path is not None:\n    load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n\ntrain(device, model, train_data_loader, test_data_loader, optimizer,\n      checkpoint_dir=checkpoint_dir,\n      checkpoint_interval=hparams.syncnet_checkpoint_interval,\n      nepochs=27)","b3ee4df9":"class nonorm_Conv2d(nn.Module): #\u4e0d\u9700\u8981\u8fdb\u884c norm\u7684\u5377\u79ef\n    def __init__(self, cin, cout, kernel_size, stride, padding, residual=False, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.conv_block = nn.Sequential(\n                            nn.Conv2d(cin, cout, kernel_size, stride, padding),\n                            )\n        self.act = nn.LeakyReLU(0.01, inplace=True)\n\n    def forward(self, x):\n        out = self.conv_block(x)\n        return self.act(out)\n\nclass Conv2dTranspose(nn.Module):# \u9006\u5377\u79ef\uff0c\u4e0a\u91c7\u6837\n    def __init__(self, cin, cout, kernel_size, stride, padding, output_padding=0, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        ############TODO###########\n        ## \u5b8c\u6210self.conv_block: \u4e00\u4e2a\u9006\u5377\u79ef\u548cbatchnorm\u7ec4\u6210\u7684 Sequential\u7ed3\u6784\n        self.conv_block = nn.Sequential(\n                            nn.ConvTranspose2d(cin, cout, kernel_size, stride, padding, output_padding),\n                            nn.BatchNorm2d(cout)\n                            )\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        out = self.conv_block(x)\n        return self.act(out)","bc962e1e":"\n#####################TODO############################\n#\u6839\u636e\u4e0b\u9762\u6253\u5370\u7684\u7f51\u7edc\u6a21\u578b\u56fe\uff0c\u8865\u5168\u7f51\u7edc\u7684\u53c2\u6570\n\nclass Wav2Lip(nn.Module):\n    def __init__(self):\n        super(Wav2Lip, self).__init__()\n\n        self.face_encoder_blocks = nn.ModuleList([\n            nn.Sequential(Conv2d(6, 16, kernel_size=7, stride=1, padding=3)), # 96,96\n\n            nn.Sequential(Conv2d(16, 32, kernel_size=3, stride=2, padding=1), # 48,48\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(32, 64, kernel_size=3, stride=2, padding=1),    # 24,24\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(64, 128, kernel_size=3, stride=2, padding=1),   # 12,12\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(128, 256, kernel_size=3, stride=2, padding=1),       # 6,6\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True)),\n\n            nn.Sequential(Conv2d(256, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n            \n            nn.Sequential(Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n\n        self.audio_encoder = nn.Sequential(\n            Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(32, 64, kernel_size=3, stride=(3, 1), padding=1),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(64, 128, kernel_size=3, stride=3, padding=1),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(128, 256, kernel_size=3, stride=(3, 2), padding=1),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n\n            Conv2d(256, 512, kernel_size=3, stride=1, padding=0),\n            Conv2d(512, 512, kernel_size=1, stride=1, padding=0),)\n\n        self.face_decoder_blocks = nn.ModuleList([\n            nn.Sequential(Conv2d(512, 512, kernel_size=1, stride=1, padding=0),),\n\n            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=1, padding=0), # 3,3\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),),\n\n            nn.Sequential(Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(512, 512, kernel_size=3, stride=1, padding=1, residual=True),), # 6, 6\n\n            nn.Sequential(Conv2dTranspose(768, 384, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(384, 384, kernel_size=3, stride=1, padding=1, residual=True),), # 12, 12\n\n            nn.Sequential(Conv2dTranspose(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(256, 256, kernel_size=3, stride=1, padding=1, residual=True),), # 24, 24\n\n            nn.Sequential(Conv2dTranspose(320, 128, kernel_size=3, stride=2, padding=1, output_padding=1), \n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),), # 48, 48\n\n            nn.Sequential(Conv2dTranspose(160, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),\n            Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),),]) # 96,96\n\n        self.output_block = nn.Sequential(Conv2d(80, 32, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(32, 3, kernel_size=1, stride=1, padding=0),\n            nn.Sigmoid()) \n\n    def forward(self, audio_sequences, face_sequences):\n        # audio_sequences = (B, T, 1, 80, 16)\n        B = audio_sequences.size(0)\n\n        input_dim_size = len(face_sequences.size())\n        if input_dim_size > 4:\n            audio_sequences = torch.cat([audio_sequences[:, i] for i in range(audio_sequences.size(1))], dim=0)\n            face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n\n        audio_embedding = self.audio_encoder(audio_sequences) # B, 512, 1, 1\n\n        feats = []\n        x = face_sequences\n        for f in self.face_encoder_blocks:\n            x = f(x)\n            feats.append(x)\n\n        x = audio_embedding\n        for f in self.face_decoder_blocks:\n            x = f(x)\n            try:\n                x = torch.cat((x, feats[-1]), dim=1)\n            except Exception as e:\n                print(x.size())\n                print(feats[-1].size())\n                raise e\n            \n            feats.pop()\n\n        x = self.output_block(x)\n\n        if input_dim_size > 4:\n            x = torch.split(x, B, dim=0) # [(B, C, H, W)]\n            outputs = torch.stack(x, dim=2) # (B, C, T, H, W)\n\n        else:\n            outputs = x\n            \n        return outputs","9320a9dd":"\n###########TODO##################\n####\u8865\u5168\u5224\u522b\u5668\u6a21\u578b\nclass Wav2Lip_disc_qual(nn.Module):\n    def __init__(self):\n        super(Wav2Lip_disc_qual, self).__init__()\n\n        self.face_encoder_blocks = nn.ModuleList([\n            nn.Sequential(nonorm_Conv2d(3, 32, kernel_size=7, stride=1, padding=3)), # 48,96\n\n            nn.Sequential(nonorm_Conv2d(32, 64, kernel_size=5, stride=(1, 2), padding=2), # 48,48\n            nonorm_Conv2d(64, 64, kernel_size=5, stride=1, padding=2)),\n\n            nn.Sequential(nonorm_Conv2d(64, 128, kernel_size=5, stride=2, padding=2),    # 24,24\n            nonorm_Conv2d(128, 128, kernel_size=5, stride=1, padding=2)),\n\n            nn.Sequential(nonorm_Conv2d(128, 256, kernel_size=5, stride=2, padding=2),   # 12,12\n            nonorm_Conv2d(256, 256, kernel_size=5, stride=1, padding=2)),\n\n            nn.Sequential(nonorm_Conv2d(256, 512, kernel_size=3, stride=2, padding=1),       # 6,6\n            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1)),\n\n            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=2, padding=1),     # 3,3\n            nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=1),),\n            \n            nn.Sequential(nonorm_Conv2d(512, 512, kernel_size=3, stride=1, padding=0),     # 1, 1\n            nonorm_Conv2d(512, 512, kernel_size=1, stride=1, padding=0)),])\n\n        self.binary_pred = nn.Sequential(nn.Conv2d(512, 1, kernel_size=1, stride=1, padding=0), nn.Sigmoid())\n        self.label_noise = .0\n\n    def get_lower_half(self, face_sequences):\n        return face_sequences[:, :, face_sequences.size(2)\/\/2:]\n\n    def to_2d(self, face_sequences):\n        B = face_sequences.size(0)\n        face_sequences = torch.cat([face_sequences[:, :, i] for i in range(face_sequences.size(2))], dim=0)\n        return face_sequences\n\n    def perceptual_forward(self, false_face_sequences):\n        false_face_sequences = self.to_2d(false_face_sequences)\n        false_face_sequences = self.get_lower_half(false_face_sequences)\n\n        false_feats = false_face_sequences\n        for f in self.face_encoder_blocks:\n            false_feats = f(false_feats)\n\n        false_pred_loss = F.binary_cross_entropy(self.binary_pred(false_feats).view(len(false_feats), -1), \n                                        torch.ones((len(false_feats), 1)).cuda())\n\n        return false_pred_loss\n\n    def forward(self, face_sequences):\n        face_sequences = self.to_2d(face_sequences)\n        face_sequences = self.get_lower_half(face_sequences)\n\n        x = face_sequences\n        for f in self.face_encoder_blocks:\n            x = f(x)\n\n        return self.binary_pred(x).view(len(x), -1)","d6e994aa":"global_step = 0\nglobal_epoch = 0\nuse_cuda = torch.cuda.is_available()\nprint('use_cuda: {}'.format(use_cuda))\n\nsyncnet_T = 5\nsyncnet_mel_step_size = 16\n\nclass Dataset(object):\n    def __init__(self, split):\n        self.all_videos = get_image_list(data_root, split)\n\n    def get_frame_id(self, frame):\n        return int(basename(frame).split('.')[0])\n\n    def get_window(self, start_frame):\n        start_id = self.get_frame_id(start_frame)\n        vidname = dirname(start_frame)\n\n        window_fnames = []\n        for frame_id in range(start_id, start_id + syncnet_T):\n            frame = join(vidname, '{}.jpg'.format(frame_id))\n            if not isfile(frame):\n                return None\n            window_fnames.append(frame)\n        return window_fnames\n\n    def read_window(self, window_fnames):\n        if window_fnames is None: return None\n        window = []\n        for fname in window_fnames:\n            img = cv2.imread(fname)\n            if img is None:\n                return None\n            try:\n                img = cv2.resize(img, (hparams.img_size, hparams.img_size))\n            except Exception as e:\n                return None\n\n            window.append(img)\n\n        return window\n\n    def crop_audio_window(self, spec, start_frame):\n        if type(start_frame) == int:\n            start_frame_num = start_frame\n        else:\n            start_frame_num = self.get_frame_id(start_frame) # 0-indexing ---> 1-indexing\n        start_idx = int(80. * (start_frame_num \/ float(hparams.fps)))\n        \n        end_idx = start_idx + syncnet_mel_step_size\n\n        return spec[start_idx : end_idx, :]\n\n    def get_segmented_mels(self, spec, start_frame):\n        mels = []\n        assert syncnet_T == 5\n        start_frame_num = self.get_frame_id(start_frame) + 1 # 0-indexing ---> 1-indexing\n        if start_frame_num - 2 < 0: return None\n        for i in range(start_frame_num, start_frame_num + syncnet_T):\n            m = self.crop_audio_window(spec, i - 2)\n            if m.shape[0] != syncnet_mel_step_size:\n                return None\n            mels.append(m.T)\n\n        mels = np.asarray(mels)\n\n        return mels\n\n    def prepare_window(self, window):\n        # 3 x T x H x W\n        x = np.asarray(window) \/ 255.\n        x = np.transpose(x, (3, 0, 1, 2))\n\n        return x\n\n    def __len__(self):\n        return len(self.all_videos)\n\n    def __getitem__(self, idx):\n        while 1:\n            idx = random.randint(0, len(self.all_videos) - 1) #\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u89c6\u9891id\n            vidname = self.all_videos[idx]\n            img_names = list(glob(join(vidname, '*.jpg')))\n            if len(img_names) <= 3 * syncnet_T:\n                continue\n            \n            img_name = random.choice(img_names)\n            wrong_img_name = random.choice(img_names)#\u968f\u673a\u9009\u62e9\u5e27\n            while wrong_img_name == img_name:\n                wrong_img_name = random.choice(img_names)\n\n            window_fnames = self.get_window(img_name)\n            wrong_window_fnames = self.get_window(wrong_img_name)\n            if window_fnames is None or wrong_window_fnames is None:\n                continue\n\n            window = self.read_window(window_fnames)\n            if window is None:\n                continue\n\n            wrong_window = self.read_window(wrong_window_fnames)\n            if wrong_window is None:\n                continue\n\n            try:\n                #\u8bfb\u53d6\u97f3\u9891\n                wavpath = join(vidname, \"audio.wav\")\n                wav = audio.load_wav(wavpath, hparams.sample_rate)\n                #\u63d0\u53d6\u5b8c\u6574mel-spectrogram\n                orig_mel = audio.melspectrogram(wav).T\n            except Exception as e:\n                continue\n            # \u5206\u5272 mel-spectrogram\n            mel = self.crop_audio_window(orig_mel.copy(), img_name)\n            \n            if (mel.shape[0] != syncnet_mel_step_size):\n                continue\n\n            indiv_mels = self.get_segmented_mels(orig_mel.copy(), img_name)\n            if indiv_mels is None: continue\n\n            window = self.prepare_window(window)\n            y = window.copy()\n            window[:, :, window.shape[2]\/\/2:] = 0.\n\n            wrong_window = self.prepare_window(wrong_window)\n            x = np.concatenate([window, wrong_window], axis=0)\n\n            x = torch.FloatTensor(x)\n            mel = torch.FloatTensor(mel.T).unsqueeze(0)\n            indiv_mels = torch.FloatTensor(indiv_mels).unsqueeze(1)\n            y = torch.FloatTensor(y)\n            return x, indiv_mels, mel, y","9989179c":"ds=Dataset(\"train\")\nx, indiv_mels, mel, y=ds[0]\nprint(x.shape)\nprint(indiv_mels.shape)\nprint(mel.shape)\nprint(y.shape)","62b2d65b":"#bce \u4ea4\u53c9\u5892loss\nlogloss = nn.BCELoss()\ndef cosine_loss(a, v, y):\n    d = nn.functional.cosine_similarity(a, v)\n    loss = logloss(d.unsqueeze(1), y)\n\n    return loss\n\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nsyncnet = SyncNet().to(device) # \u5b9a\u4e49syncnet \u6a21\u578b\nfor p in syncnet.parameters():\n    p.requires_grad = False\n\n    \n#####L1 loss    \nrecon_loss = nn.L1Loss()\ndef get_sync_loss(mel, g):\n    g = g[:, :, :, g.size(3)\/\/2:]\n    g = torch.cat([g[:, :, i] for i in range(syncnet_T)], dim=1)\n    # B, 3 * T, H\/\/2, W\n    a, v = syncnet(mel, g)\n    y = torch.ones(g.size(0), 1).float().to(device)\n    return cosine_loss(a, v, y)\n\ndef train(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n          checkpoint_dir=None, checkpoint_interval=None, nepochs=None):\n    global global_step, global_epoch\n    resumed_step = global_step\n\n    while global_epoch < nepochs:\n        print('Starting Epoch: {}'.format(global_epoch))\n        running_sync_loss, running_l1_loss, disc_loss, running_perceptual_loss = 0., 0., 0., 0.\n        running_disc_real_loss, running_disc_fake_loss = 0., 0.\n        prog_bar = tqdm(enumerate(train_data_loader))\n        for step, (x, indiv_mels, mel, gt) in prog_bar:\n            disc.train()\n            model.train()\n\n            x = x.to(device)\n            mel = mel.to(device)\n            indiv_mels = indiv_mels.to(device)\n            gt = gt.to(device)\n\n            ### Train generator now. Remove ALL grads. \n            #\u8bad\u7ec3\u751f\u6210\u5668\n            optimizer.zero_grad()\n            disc_optimizer.zero_grad()\n\n            g = model(indiv_mels, x)#\u5f97\u5230\u751f\u6210\u7684\u7ed3\u679c\n\n            if hparams.syncnet_wt > 0.:\n                sync_loss = get_sync_loss(mel, g)# \u4ece\u9884\u8bad\u7ec3\u7684expert \u6a21\u578b\u4e2d\u83b7\u5f97\u5507\u97f3\u540c\u6b65\u7684\u635f\u5931\n            else:\n                sync_loss = 0.\n\n            if hparams.disc_wt > 0.:\n                perceptual_loss = disc.perceptual_forward(g)#\u5224\u522b\u5668\u7684\u611f\u77e5\u635f\u5931\n            else:\n                perceptual_loss = 0.\n\n            l1loss = recon_loss(g, gt)#l1 loss\uff0c\u91cd\u5efa\u635f\u5931\n            \n            #\u6700\u7ec8\u7684\u635f\u5931\u51fd\u6570\n            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n\n            loss.backward()\n            optimizer.step()\n\n            ### Remove all gradients before Training disc\n            # \u8bad\u7ec3\u5224\u522b\u5668\n            disc_optimizer.zero_grad()\n\n            pred = disc(gt)\n            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n            disc_real_loss.backward()\n\n            pred = disc(g.detach())\n            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n            disc_fake_loss.backward()\n\n            disc_optimizer.step()\n\n            running_disc_real_loss += disc_real_loss.item()\n            running_disc_fake_loss += disc_fake_loss.item()\n\n            # Logs\n            global_step += 1\n            cur_session_steps = global_step - resumed_step\n\n            running_l1_loss += l1loss.item()\n            if hparams.syncnet_wt > 0.:\n                running_sync_loss += sync_loss.item()\n            else:\n                running_sync_loss += 0.\n\n            if hparams.disc_wt > 0.:\n                running_perceptual_loss += perceptual_loss.item()\n            else:\n                running_perceptual_loss += 0.\n\n            if global_step == 1 or global_step % checkpoint_interval == 0:\n                save_checkpoint(\n                    model, optimizer, global_step, checkpoint_dir, global_epoch)\n                save_checkpoint(disc, disc_optimizer, global_step, checkpoint_dir, global_epoch, prefix='disc_')\n\n\n            if global_step % hparams.eval_interval == 0:\n                with torch.no_grad():\n                    average_sync_loss = eval_model(test_data_loader, global_step, device, model, disc)\n\n                    if average_sync_loss < .75:\n                        hparams.set_hparam('syncnet_wt', 0.03)\n\n            prog_bar.set_description('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(running_l1_loss \/ (step + 1),\n                                                                                        running_sync_loss \/ (step + 1),\n                                                                                        running_perceptual_loss \/ (step + 1),\n                                                                                        running_disc_fake_loss \/ (step + 1),\n                                                                                        running_disc_real_loss \/ (step + 1)))\n\n        global_epoch += 1\n\ndef eval_model(test_data_loader, global_step, device, model, disc):\n    eval_steps = 300\n    print('Evaluating for {} steps'.format(eval_steps))\n    running_sync_loss, running_l1_loss, running_disc_real_loss, running_disc_fake_loss, running_perceptual_loss = [], [], [], [], []\n    while 1:\n        for step, (x, indiv_mels, mel, gt) in enumerate((test_data_loader)):\n            model.eval()\n            disc.eval()\n\n            x = x.to(device)\n            mel = mel.to(device)\n            indiv_mels = indiv_mels.to(device)\n            gt = gt.to(device)\n\n            pred = disc(gt)\n            disc_real_loss = F.binary_cross_entropy(pred, torch.ones((len(pred), 1)).to(device))\n\n            g = model(indiv_mels, x)\n            pred = disc(g)\n            disc_fake_loss = F.binary_cross_entropy(pred, torch.zeros((len(pred), 1)).to(device))\n\n            running_disc_real_loss.append(disc_real_loss.item())\n            running_disc_fake_loss.append(disc_fake_loss.item())\n\n            sync_loss = get_sync_loss(mel, g)\n            \n            if hparams.disc_wt > 0.:\n                perceptual_loss = disc.perceptual_forward(g)\n            else:\n                perceptual_loss = 0.\n\n            l1loss = recon_loss(g, gt)\n\n            loss = hparams.syncnet_wt * sync_loss + hparams.disc_wt * perceptual_loss + \\\n                                    (1. - hparams.syncnet_wt - hparams.disc_wt) * l1loss\n\n            running_l1_loss.append(l1loss.item())\n            running_sync_loss.append(sync_loss.item())\n            \n            if hparams.disc_wt > 0.:\n                running_perceptual_loss.append(perceptual_loss.item())\n            else:\n                running_perceptual_loss.append(0.)\n\n            if step > eval_steps: break\n\n        print('L1: {}, Sync: {}, Percep: {} | Fake: {}, Real: {}'.format(sum(running_l1_loss) \/ len(running_l1_loss),\n                                                            sum(running_sync_loss) \/ len(running_sync_loss),\n                                                            sum(running_perceptual_loss) \/ len(running_perceptual_loss),\n                                                            sum(running_disc_fake_loss) \/ len(running_disc_fake_loss),\n                                                             sum(running_disc_real_loss) \/ len(running_disc_real_loss)))\n        return sum(running_sync_loss) \/ len(running_sync_loss)\n\nlatest_wav2lip_checkpoint = ''\ndef save_checkpoint(model, optimizer, step, checkpoint_dir, epoch, prefix=''):\n    global latest_wav2lip_checkpoint\n    checkpoint_path = join(\n        checkpoint_dir, \"{}checkpoint_step{:09d}.pth\".format(prefix, global_step))\n    if 'disc' not in checkpoint_path:\n        latest_wav2lip_checkpoint = checkpoint_path\n    optimizer_state = optimizer.state_dict() if hparams.save_optimizer_state else None\n    torch.save({\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer_state,\n        \"global_step\": step,\n        \"global_epoch\": epoch,\n    }, checkpoint_path)\n    print(\"Saved checkpoint:\", checkpoint_path)\n\ndef _load(checkpoint_path):\n    if use_cuda:\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path,\n                                map_location=lambda storage, loc: storage)\n    return checkpoint\n\n\ndef load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n    global global_step\n    global global_epoch\n\n    print(\"Load checkpoint from: {}\".format(path))\n    checkpoint = _load(path)\n    s = checkpoint[\"state_dict\"]\n    new_s = {}\n    for k, v in s.items():\n        new_s[k.replace('module.', '')] = v\n    model.load_state_dict(new_s)\n    if not reset_optimizer:\n        optimizer_state = checkpoint[\"optimizer\"]\n        if optimizer_state is not None:\n            print(\"Load optimizer state from {}\".format(path))\n            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    if overwrite_global_states:\n        global_step = checkpoint[\"global_step\"]\n        global_epoch = checkpoint[\"global_epoch\"]\n\n    return model","0df5c24b":"checkpoint_dir = \"\/kaggle\/working\/wav2lip_checkpoints\"  #checkpoint \u5b58\u50a8\u7684\u4f4d\u7f6e\n\n# Dataset and Dataloader setup\ntrain_dataset = Dataset('train')\ntest_dataset = Dataset('val')\n\ntrain_data_loader = data_utils.DataLoader(\n    train_dataset, batch_size=hparams.batch_size, shuffle=True,\n    num_workers=hparams.num_workers)\n\ntest_data_loader = data_utils.DataLoader(\n    test_dataset, batch_size=hparams.batch_size,\n    num_workers=4)\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n # Model\nmodel = Wav2Lip().to(device)####### \u751f\u6210\u5668\u6a21\u578b\ndisc = Wav2Lip_disc_qual().to(device)####### \u5224\u522b\u5668\u6a21\u578b\n\nprint('total trainable params {}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\nprint('total DISC trainable params {}'.format(sum(p.numel() for p in disc.parameters() if p.requires_grad)))\n\noptimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n                       lr=hparams.initial_learning_rate,\n                       betas=(0.5, 0.999))#####adam\u4f18\u5316\u5668\uff0cbetas=[0.5,0.999]\ndisc_optimizer = optim.Adam([p for p in disc.parameters() if p.requires_grad],\n                            lr=hparams.disc_initial_learning_rate,\n                            betas=(0.5, 0.999))#####adam\u4f18\u5316\u5668\uff0cbetas=[0.5,0.999]\n\n#\u7ee7\u7eed\u8bad\u7ec3\u7684\u751f\u6210\u5668\u7684checkpoint\u4f4d\u7f6e\n# checkpoint_path=\"\"\n# load_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=False)\n#\u7ee7\u7eed\u8bad\u7ec3\u7684\u5224\u522b\u5668\u7684checkpoint\u4f4d\u7f6e\n# disc_checkpoint_path=\"\"\n# load_checkpoint(disc_checkpoint_path, disc, disc_optimizer, \n#                             reset_optimizer=False, overwrite_global_states=False)\n\n# syncnet\u7684checkpoint\u4f4d\u7f6e\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u6b64\u6a21\u578b\u8ba1\u7b97\u751f\u6210\u7684\u5e27\u548c\u8bed\u97f3\u7684\u5507\u97f3\u540c\u6b65\u635f\u5931\nsyncnet_checkpoint_path = latest_checkpoint_path\n# syncnet_checkpoint_path=\"\/kaggle\/working\/expert_checkpoints\/checkpoint_step000000001.pth\"\nload_checkpoint(syncnet_checkpoint_path, syncnet, None, reset_optimizer=True,\n                            overwrite_global_states=False)\n\nif not os.path.exists(checkpoint_dir):\n    os.mkdir(checkpoint_dir)\n\n# Train!\ntrain(device, model, disc, train_data_loader, test_data_loader, optimizer, disc_optimizer,\n          checkpoint_dir=checkpoint_dir,\n          checkpoint_interval=hparams.checkpoint_interval,\n          nepochs=5)","beee18e3":"# !python wav2lip_train.py --data_root lrs2_preprocessed\/ --checkpoint_dir <folder_to_save_checkpoints> --syncnet_checkpoint_path <path_to_expert_disc_checkpoint>","dfda88d6":"from os import listdir, path\nimport numpy as np\nimport scipy, cv2, os, sys, argparse, audio\nimport json, subprocess, random, string\nfrom tqdm import tqdm\nfrom glob import glob\nimport torch, face_detection\nfrom models import Wav2Lip\nimport platform\nimport audio","8008eda4":"checkpoint_path=\"\/kaggle\/working\/wav2lip_checkpoints\/checkpoint_step000000001.pth\"#\u751f\u6210\u5668\u7684checkpoint\u4f4d\u7f6e\ncheckpoint_path = latest_wav2lip_checkpoint\nface=\"input_video.mp4\" #\u53c2\u7167\u89c6\u9891\u7684\u6587\u4ef6\u4f4d\u7f6e, *.mp4\nspeech=\"input_audio.wav\"#\u8f93\u5165\u8bed\u97f3\u7684\u4f4d\u7f6e\uff0c*.wav\nresize_factor=1 #\u5bf9\u8f93\u5165\u7684\u89c6\u9891\u8fdb\u884c\u4e0b\u91c7\u6837\u7684\u500d\u7387\ncrop=[0,-1,0,-1] #\u662f\u5426\u5bf9\u89c6\u9891\u5e27\u8fdb\u884c\u88c1\u526a,\u5904\u7406\u89c6\u9891\u4e2d\u6709\u591a\u5f20\u4eba\u8138\u65f6\u6709\u7528\nfps=25#\u89c6\u9891\u7684\u5e27\u7387\nstatic=False #\u662f\u5426\u53ea\u4f7f\u7528\u56fa\u5b9a\u7684\u4e00\u5e27\u4f5c\u4e3a\u89c6\u9891\u7684\u751f\u6210\u53c2\u7167","cd872262":"if not os.path.isfile(face):\n    raise ValueError('--face argument must be a valid path to video\/image file')\n\n\nelse:# \u82e5\u8f93\u5165\u7684\u662f\u89c6\u9891\u683c\u5f0f\n    video_stream = cv2.VideoCapture(face)# \u8bfb\u53d6\u89c6\u9891\n    fps = video_stream.get(cv2.CAP_PROP_FPS)# \u8bfb\u53d6 fps\n\n    print('Reading video frames...')\n\n    full_frames = []\n    #\u63d0\u53d6\u6240\u6709\u7684\u5e27\n    while 1:\n        still_reading, frame = video_stream.read()\n        if not still_reading:\n            video_stream.release()\n            break\n        if resize_factor > 1: # \u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u964d\u4f4e\u5206\u8fa8\u7387\n            frame = cv2.resize(frame, (frame.shape[1]\/\/resize_factor, frame.shape[0]\/\/resize_factor))\n\n        \n\n        y1, y2, x1, x2 =crop  # \u88c1\u526a\n        if x2 == -1: x2 = frame.shape[1]\n        if y2 == -1: y2 = frame.shape[0]\n\n        frame = frame[y1:y2, x1:x2]\n\n        full_frames.append(frame)\n\nprint (\"Number of frames available for inference: \"+str(len(full_frames)))","ff382ea5":"#\u68c0\u67e5\u8f93\u5165\u7684\u97f3\u9891\u662f\u5426\u4e3a .wav\u683c\u5f0f\u7684\uff0c\u82e5\u4e0d\u662f\u5219\u8fdb\u884c\u8f6c\u6362\nif not speech.endswith('.wav'):\n    print('Extracting raw audio...')\n    command = 'ffmpeg -y -i {} -strict -2 {}'.format(speech, 'temp\/temp.wav')\n\n    subprocess.call(command, shell=True)\n    speech = 'temp\/temp.wav'\n\nwav = audio.load_wav(speech, 16000)#\u4fdd\u8bc1\u91c7\u6837\u7387\u4e3a16000\nmel = audio.melspectrogram(wav)\nprint(mel.shape)\n","675e900d":"wav2lip_batch_size=128 #\u63a8\u7406\u65f6\u8f93\u5165\u5230\u7f51\u7edc\u7684batchsize\nmel_step_size=16\n\n#\u63d0\u53d6\u8bed\u97f3\u7684mel\u8c31\nmel_chunks = []\nmel_idx_multiplier = 80.\/fps \ni = 0\nwhile 1:\n    start_idx = int(i * mel_idx_multiplier)\n    if start_idx + mel_step_size > len(mel[0]):\n        mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n        break\n    mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n    i += 1\n\nprint(\"Length of mel chunks: {}\".format(len(mel_chunks)))\n\nfull_frames = full_frames[:len(mel_chunks)]\n\nbatch_size = wav2lip_batch_size","d5c69b46":"img_size = 96 #\u9ed8\u8ba4\u7684\u8f93\u5165\u56fe\u7247\u5927\u5c0f\npads=[0,20,0,0] # \u586b\u5145\u7684\u957f\u5ea6\uff0c\u4fdd\u8bc1\u4e0b\u5df4\u4e5f\u5728\u62a0\u56fe\u7684\u8303\u56f4\u4e4b\u5185\nnosmooth=False\nface_det_batch_size=16\n\ndef get_smoothened_boxes(boxes, T):\n    for i in range(len(boxes)):\n        if i + T > len(boxes):\n            window = boxes[len(boxes) - T:]\n        else:\n            window = boxes[i : i + T]\n        boxes[i] = np.mean(window, axis=0)\n    return boxes\n\n#\u4eba\u8138\u68c0\u6d4b\u51fd\u6570\ndef face_detect(images):\n    detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n                                            flip_input=False, device=device)\n\n    batch_size = face_det_batch_size\n\n    while 1:\n        predictions = []\n        try:\n            for i in tqdm(range(0, len(images), batch_size)):\n                predictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n        except RuntimeError:\n            if batch_size == 1: \n                raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n            batch_size \/\/= 2\n            print('Recovering from OOM error; New batch size: {}'.format(batch_size))\n            continue\n        break\n\n    results = []\n    pady1, pady2, padx1, padx2 = pads\n    for rect, image in zip(predictions, images):\n        if rect is None:\n            cv2.imwrite('temp\/faulty_frame.jpg', image) # check this frame where the face was not detected.\n            raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n\n        y1 = max(0, rect[1] - pady1)\n        y2 = min(image.shape[0], rect[3] + pady2)\n        x1 = max(0, rect[0] - padx1)\n        x2 = min(image.shape[1], rect[2] + padx2)\n\n        results.append([x1, y1, x2, y2])\n\n    boxes = np.array(results)\n    if not nosmooth: boxes = get_smoothened_boxes(boxes, T=5)\n    results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n\n    del detector\n    return results \n\nbox=[-1,-1,-1,-1]\n\ndef datagen(frames, mels):\n    img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n\n    if box[0] == -1:# \u5982\u679c\u672a\u6307\u5b9a \u7279\u5b9a\u7684\u4eba\u8138\u8fb9\u754c\u7684\u8bdd\n        if not static:# \u662f\u5426\u4f7f\u7528\u89c6\u9891\u7684\u7b2c\u4e00\u5e27\u4f5c\u4e3a\u53c2\u8003\n            face_det_results = face_detect(frames) # BGR2RGB for CNN face detection\n        else:\n            face_det_results = face_detect([frames[0]])\n    else:\n        print('Using the specified bounding box instead of face detection...')\n        y1, y2, x1, x2 = box\n        face_det_results = [[f[y1: y2, x1:x2], (y1, y2, x1, x2)] for f in frames] # \u88c1\u526a\u51fa\u4eba\u8138\u7ed3\u679c\n\n    for i, m in enumerate(mels):\n        idx = 0 if static else i%len(frames)\n        frame_to_save = frames[idx].copy()\n        face, coords = face_det_results[idx].copy()\n\n        face = cv2.resize(face, (img_size, img_size)) # \u91cd\u91c7\u6837\u5230\u6307\u5b9a\u5927\u5c0f\n\n        img_batch.append(face)\n        mel_batch.append(m)\n        frame_batch.append(frame_to_save)\n        coords_batch.append(coords)\n\n        if len(img_batch) >= wav2lip_batch_size:\n            img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n\n            img_masked = img_batch.copy()\n            img_masked[:, img_size\/\/2:] = 0\n\n            img_batch = np.concatenate((img_masked, img_batch), axis=3) \/ 255.\n            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n\n            yield img_batch, mel_batch, frame_batch, coords_batch\n            img_batch, mel_batch, frame_batch, coords_batch = [], [], [], []\n\n    if len(img_batch) > 0:\n        img_batch, mel_batch = np.asarray(img_batch), np.asarray(mel_batch)\n\n        img_masked = img_batch.copy()\n        img_masked[:, img_size\/\/2:] = 0\n\n        img_batch = np.concatenate((img_masked, img_batch), axis=3) \/ 255.\n        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n\n        yield img_batch, mel_batch, frame_batch, coords_batch\n\nmel_step_size = 16 \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} for inference.'.format(device))\n\n\n#\u52a0\u8f7d\u6a21\u578b\ndef _load(checkpoint_path):\n    if device == 'cuda':\n        checkpoint = torch.load(checkpoint_path)\n    else:\n        checkpoint = torch.load(checkpoint_path,\n                                map_location=lambda storage, loc: storage)\n    return checkpoint\n\ndef load_model(path):\n    model = Wav2Lip()\n    print(\"Load checkpoint from: {}\".format(path))\n    checkpoint = _load(path)\n    s = checkpoint[\"state_dict\"]\n    new_s = {}\n    for k, v in s.items():\n        new_s[k.replace('module.', '')] = v\n    model.load_state_dict(new_s)\n\n    model = model.to(device)\n    return model.eval()","f71be467":"os.mkdir('\/kaggle\/working\/temp\/')","56e94c31":"full_frames = full_frames[:len(mel_chunks)]\n\nbatch_size = wav2lip_batch_size\ngen = datagen(full_frames.copy(), mel_chunks)  # \u8fdb\u884c\u4eba\u8138\u7684\u88c1\u526a\u4e0e\u62fc\u63a5\uff0c6\u901a\u9053\n\nfor i, (img_batch, mel_batch, frames, coords) in enumerate(tqdm(gen, \n                                        total=int(np.ceil(float(len(mel_chunks))\/batch_size)))):\n    #\u52a0\u8f7d\u6a21\u578b\n    if i == 0:\n        model = load_model(checkpoint_path)\n        print (\"Model loaded\")\n\n        frame_h, frame_w = full_frames[0].shape[:-1]\n        #\u6682\u5b58\u4e34\u65f6\u89c6\u9891\n        out = cv2.VideoWriter('\/kaggle\/working\/temp\/result_without_audio.mp4',\n                                cv2.VideoWriter_fourcc(*'DIVX'), fps, (frame_w, frame_h))\n\n    img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n    mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n    \n    \n    ##### \u5c06 img_batch, mel_batch\u9001\u5165\u6a21\u578b\u5f97\u5230pred\n    ##############TODO##############\n    with torch.no_grad():\n        pred = model(mel_batch, img_batch)\n    \n    pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n\n    for p, f, c in zip(pred, frames, coords):\n        y1, y2, x1, x2 = c\n        p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n\n        f[y1:y2, x1:x2] = p\n        out.write(f)\n\nout.release()\n","e950e72c":"os.mkdir('\/kaggle\/working\/result\/')","53d20da1":"#\u5c06\u751f\u6210\u7684\u89c6\u9891\u4e0e\u8bed\u97f3\u5408\u5e76\noutfile=\"\/kaggle\/working\/result\/result.mp4\"# \u6700\u7ec8\u8f93\u51fa\u7ed3\u679c\u5230\u8be5\u6587\u4ef6\u5939\u4e0b\ncommand = 'ffmpeg -y -i {} -i {} -strict -2 -q:v 1 {}'.format(speech, '\/kaggle\/working\/temp\/result_without_audio.mp4',outfile)\nsubprocess.call(command, shell=platform.system() != 'Windows')","f293700e":"# \u4efb\u52a1\n\n\u672c\u6b21\u5b9e\u8df5\u6d89\u53ca\u5230\u5bf9Wav2Lip\u6a21\u578b\u7684\uff0c\u4ee5\u53ca\u76f8\u5173\u4ee3\u7801\u5b9e\u73b0\u3002\u603b\u4f53\u4e0a\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u90e8\u5206\uff1a\n1. \u73af\u5883\u7684\u914d\u7f6e\n2. \u6570\u636e\u96c6\u51c6\u5907\u53ca\u9884\u5904\u7406\n3. \u6a21\u578b\u7684\u8bad\u7ec3\n4. \u6a21\u578b\u7684\u63a8\u7406","4fc693bf":"#### 2. \u6570\u636e\u96c6\u7684\u5b9a\u4e49  \n\u5728\u8bad\u7ec3\u65f6\uff0c\u4f1a\u7528\u52304\u4e2a\u6570\u636e\uff1a\n1. x:\u8f93\u5165\u7684\u56fe\u7247\n2. indiv_mels: \u6bcf\u4e00\u5f20\u56fe\u7247\u6240\u5bf9\u5e94\u8bed\u97f3\u7684mel-spectrogram\u7279\u5f81\n3. mel: \u6240\u6709\u5e27\u5bf9\u5e94\u7684200ms\u7684\u8bed\u97f3mel-spectrogram\uff0c\u7528\u4e8eSyncNet\u8fdb\u884c\u5507\u97f3\u540c\u6b65\u635f\u5931\u7684\u8ba1\u7b97\n4. y:\u771f\u5b9e\u7684\u4e0e\u8bed\u97f3\u5bf9\u5e94\u7684\uff0c\u5507\u97f3\u540c\u6b65\u7684\u56fe\u7247\u3002\n","88362642":"## 2. \u6570\u636e\u96c6\u7684\u51c6\u5907\u53ca\u9884\u5904\u7406\n\n**LRS2 \u6570\u636e\u96c6\u7684\u4e0b\u8f7d**  \n\u5b9e\u9a8c\u6240\u9700\u8981\u7684\u6570\u636e\u96c6\u4e0b\u8f7d\u5730\u5740\u4e3a\uff1a<a href=\"http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/lip_reading\/lrs2.html\">LRS2 dataset<\/a>\uff0c\u4e0b\u8f7d\u8be5\u6570\u636e\u96c6\u9700\u8981\u83b7\u5f97BBC\u7684\u8bb8\u53ef\uff0c\u9700\u8981\u53d1\u9001\u7533\u8bf7\u90ae\u4ef6\u4ee5\u83b7\u53d6\u4e0b\u8f7d\u5bc6\u94a5\uff0c\u5177\u4f53\u64cd\u4f5c\u8be6\u89c1\u7f51\u9875\u4e2d\u7684\u6307\u793a\u3002\u4e0b\u8f7d\u5b8c\u6210\u540e\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u89e3\u538b\u5230\u672c\u76ee\u5f55\u7684`mvlrs_v1\/`\u6587\u4ef6\u5939\u4e0b\uff0c\u5e76\u5c06LRS2\u4e2d\u7684\u6587\u4ef6\u5217\u8868\u6587\u4ef6`train.txt, val.txt, test.txt` \u79fb\u52a8\u5230`filelists\/`\u6587\u4ef6\u5939\u4e0b\uff0c\u6700\u7ec8\u5f97\u5230\u7684\u6570\u636e\u96c6\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\u6240\u793a\u3002\n```\ndata_root (mvlrs_v1)\n\u251c\u2500\u2500 main, pretrain (\u6211\u4eec\u53ea\u4f7f\u7528main\u6587\u4ef6\u5939\u4e0b\u7684\u6570\u636e)\n|\t\u251c\u2500\u2500 \u6587\u4ef6\u5939\u5217\u8868\n|\t\u2502   \u251c\u2500\u2500 5\u4f4d\u4ee5.mp4\u7ed3\u5c3e\u7684\u89c6\u9891ID\n```\n**\u6570\u636e\u96c6\u9884\u5904\u7406**\n\u6570\u636e\u96c6\u4e2d\u5927\u591a\u6570\u89c6\u9891\u90fd\u662f\u5305\u542b\u4eba\u7684\u534a\u8eab\u6216\u8005\u5168\u8eab\u7684\u753b\u9762\uff0c\u800c\u6211\u4eec\u7684\u6a21\u578b\u53ea\u9700\u8981\u4eba\u8138\u8fd9\u4e00\u5c0f\u90e8\u5206\u3002\u6240\u4ee5\u5728\u9884\u5904\u7406\u9636\u6bb5\uff0c\u6211\u4eec\u8981\u5bf9\u6bcf\u4e00\u4e2a\u89c6\u9891\u8fdb\u884c\u5206\u5e27\u64cd\u4f5c\uff0c\u63d0\u53d6\u89c6\u9891\u7684\u6bcf\u4e00\u5e27\uff0c\u4e4b\u540e\u4f7f\u7528`face detection`\u5de5\u5177\u5305\u5bf9\u4eba\u8138\u4f4d\u7f6e\u8fdb\u884c\u5b9a\u4f4d\u5e76\u88c1\u51cf\uff0c\u53ea\u4fdd\u7559\u4eba\u8138\u7684\u56fe\u7247\u5e27\u3002\u540c\u65f6\uff0c\u6211\u4eec\u4e5f\u9700\u8981\u5c06\u6bcf\u4e00\u4e2a\u89c6\u9891\u4e2d\u7684\u8bed\u97f3\u5206\u79bb\u51fa\u6765\u3002","e57087db":"#### 4. \u547d\u4ee4\u884c\u8bad\u7ec3\n\u4e0a\u9762\u662f\u6309\u6b65\u9aa4\u8bad\u7ec3\u7684\u8fc7\u7a0b\uff0c\u5728`hq_wav2lip_train.py`\u6587\u4ef6\u4e2d\u5df2\u7ecf\u628a\u4e0a\u8ff0\u7684\u8fc7\u7a0b\u8fdb\u884c\u4e86\u5c01\u88c5\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u7684\u547d\u4ee4\u76f4\u63a5\u8fdb\u884c\u8bad\u7ec3","e17f6444":"#### 2.\u6570\u636e\u96c6\u7684\u5b9a\u4e49","a3c69ae9":"**\u751f\u6210\u5668**  \n\u7531\u4e24\u4e2aencoder: face_encoder\u548c audio_encoder, \u4e00\u4e2adecoder\uff1aface_decoder\u7ec4\u6210\u3002face encoder \u548c audio encoder \u5206\u522b\u5bf9\u8f93\u5165\u7684\u4eba\u8138\u548c\u8bed\u97f3\u7279\u5f81\u8fdb\u884c\u964d\u7ef4\uff0c\u5f97\u5230\uff081\uff0c1\uff0c512\uff09\u7684\u7279\u5f81\uff0c\u5e76\u5c06\u4e8c\u8005\u8fdb\u884c\u62fc\u63a5\u9001\u5165\u5230 face decoder\u4e2d\u53bb\u8fdb\u884c\u4e0a\u91c7\u6837\uff0c\u6700\u7ec8\u5f97\u5230\u548c\u8f93\u5165\u4e00\u6837\u5927\u5c0f\u7684\u4eba\u8138\u56fe\u50cf\u3002","cd4753a2":"## 1. \u73af\u5883\u7684\u914d\u7f6e\n- `\u5efa\u8bae\u51c6\u5907\u4e00\u53f0\u6709\u663e\u5361\u7684linux\u7cfb\u7edf\u7535\u8111\uff0c\u6216\u8005\u53ef\u4ee5\u9009\u62e9\u4f7f\u7528\u7b2c\u4e09\u65b9\u4e91\u670d\u52a1\u5668\uff08Google Colab\uff09` \n- `Python 3.6 \u6216\u8005\u66f4\u9ad8\u7248\u672c` \n- ffmpeg: `sudo apt-get install ffmpeg`\n- \u5fc5\u8981\u7684python\u5305\u7684\u5b89\u88c5\uff0c\u6240\u9700\u8981\u7684\u5e93\u540d\u79f0\u90fd\u5df2\u7ecf\u5305\u542b\u5728`requirements.txt`\u6587\u4ef6\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 `pip install -r requirements.txt`\u4e00\u6b21\u6027\u5b89\u88c5. \n- \u5728\u672c\u5b9e\u9a8c\u4e2d\u5229\u7528\u5230\u4e86\u4eba\u8138\u68c0\u6d4b\u7684\u76f8\u5173\u6280\u672f\uff0c\u9700\u8981\u4e0b\u8f7d\u4eba\u8138\u68c0\u6d4b\u9884\u8bad\u7ec3\u6a21\u578b\uff1aFace detection [pre-trained model](https:\/\/www.adrianbulat.com\/downloads\/python-fan\/s3fd-619a316812.pth) \u5e76\u79fb\u52a8\u5230 `face_detection\/detection\/sfd\/s3fd.pth`\u6587\u4ef6\u5939\u4e0b. ","456a0f5e":"**\u4e0b\u9762\u5f00\u59cb\u8bad\u7ec3\uff0c\u6700\u7ec8\u7684Loss\u53c2\u8003\u503c\u4e3a0.20\u5de6\u53f3\uff0c\u6b64\u65f6\u6a21\u578b\u80fd\u8fbe\u5230\u8f83\u597d\u7684\u5224\u522b\u6548\u679c**","867960a3":"SyncNet\u7684\u4e3b\u8981\u5305\u542b\u4e24\u4e2a\u90e8\u5206\uff1aFace_encoder\u548cAudio_encoder\u3002\u6bcf\u4e00\u4e2a\u90e8\u5206\u90fd\u7531\u591a\u4e2aConv2d\u6a21\u5757\u7ec4\u6210\uff0c\u901a\u8fc7\u6307\u5b9a\u5377\u79ef\u6838\u7684\u5927\u5c0f\u5b9e\u73b0\u5bf9\u8f93\u5165\u7684\u4e0b\u91c7\u6837\u548c\u7279\u5f81\u63d0\u53d6","368c5f15":"#### 3. \u8bad\u7ec3","7c1da40c":"### 3.2 \u8bad\u7ec3Wav2Lip\n\u9884\u8bad\u7ec3\u6a21\u578b [weight](https:\/\/iiitaphyd-my.sharepoint.com\/:u:\/g\/personal\/radrabha_m_research_iiit_ac_in\/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW)\n#### 1. \u6a21\u578b\u7684\u5b9a\u4e49\nwav2lip\u6a21\u578b\u7684\u751f\u6210\u5668\u9996\u5148\u5bf9\u8f93\u5165\u8fdb\u884c\u4e0b\u91c7\u6837\uff0c\u7136\u540e\u518d\u7ecf\u8fc7\u4e0a\u91c7\u6837\u6062\u590d\u6210\u539f\u6765\u7684\u5927\u5c0f\u3002\u4e3a\u4e86\u65b9\u4fbf\uff0c\u6211\u4eec\u5bf9\u5176\u4e2d\u91cd\u590d\u5229\u7528\u5230\u7684\u6a21\u5757\u8fdb\u884c\u4e86\u5c01\u88c5\u3002","d5c80299":"### \u4e4b\u524d\u4ee3\u7801\u6709\u95ee\u9898\uff0c\u4ee5\u6700\u65b0\u7248\u4e3a\u51c6","4e8c7ca0":"### 4. \u6a21\u578b\u7684\u63a8\u7406\n\u5f53\u6a21\u578b\u8bad\u7ec3\u5b8c\u6bd5\u540e\uff0c\u6211\u4eec\u53ea\u4f7f\u7528\u751f\u6210\u5668\u7684\u7f51\u7edc\u6a21\u578b\u90e8\u5206\u4f5c\u4e3a\u6211\u4eec\u7684\u63a8\u7406\u6a21\u578b\u3002\u6a21\u578b\u7684\u8f93\u5165\u7531\u4e00\u6bb5\u5305\u542b\u4eba\u8138\u7684\u53c2\u7167\u89c6\u9891\u548c\u4e00\u6bb5\u8bed\u97f3\u7ec4\u6210\u3002  \n\u5728\u8fd9\u91cc\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u5b98\u65b9\u63d0\u4f9b\u7ed9\u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6a21\u578b[weight](https:\/\/iiitaphyd-my.sharepoint.com\/:u:\/g\/personal\/radrabha_m_research_iiit_ac_in\/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW)\u4e0b\u8f7d\u8be5\u6a21\u578b\u5e76\u653e\u5165\u5230\u6307\u5b9a\u6587\u4ef6\u5939\u4e0b\uff0c\u4f9b\u4e4b\u540e\u7684\u63a8\u7406\u4f7f\u7528\u3002  \n\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u4e3b\u8981\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u6b65\u9aa4\uff1a\n1. \u8f93\u5165\u6570\u636e\u7684\u9884\u5904\u7406\uff0c\u5305\u542b\u4eba\u8138\u62a0\u56fe\uff0c\u89c6\u9891\u5206\u5e27\uff0c\u63d0\u53d6mel-spectrogram\u7279\u5f81\u7b49\u64cd\u4f5c\u3002\n2. \u5229\u7528\u7f51\u7edc\u6a21\u578b\u751f\u6210\u5507\u97f3\u540c\u6b65\u7684\u89c6\u9891\u5e27\u3002\n3. \u5c06\u751f\u6210\u7684\u89c6\u9891\u5e27\u51c6\u6362\u6210\u89c6\u9891\uff0c\u5e76\u548c\u8f93\u5165\u7684\u8bed\u97f3\u7ed3\u5408\uff0c\u5f62\u6210\u6700\u7ec8\u7684\u8f93\u51fa\u89c6\u9891\u3002  \n","98324328":"\u9884\u5904\u7406\u540e\u7684`lrs2_preprocessed\/`\u6587\u4ef6\u5939\u4e0b\u7684\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\n```\npreprocessed_root (lrs2_preprocessed)\n\u251c\u2500\u2500 \u6587\u4ef6\u5939\u5217\u8868\n|\t\u251c\u2500\u2500 \u4e94\u4f4d\u7684\u89c6\u9891ID\n|\t\u2502   \u251c\u2500\u2500 *.jpg\n|\t\u2502   \u251c\u2500\u2500 audio.wav\n```","77b3fd16":"## 3. \u6a21\u578b\u8bad\u7ec3\n\u6a21\u578b\u7684\u8bad\u7ec3\u4e3b\u8981\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\n1. Lip-Sync Expert Discriminator\u7684\u8bad\u7ec3\u3002\u8fd9\u91cc\u63d0\u4f9b\u5b98\u65b9\u7684\u9884\u8bad\u7ec3\u6a21\u578b [weight](https:\/\/iiitaphyd-my.sharepoint.com\/:u:\/g\/personal\/radrabha_m_research_iiit_ac_in\/EQRvmiZg-HRAjvI6zqN9eTEBP74KefynCwPWVmF57l-AYA?e=ZRPHKP)\n2. Wav2Lip \u6a21\u578b\u7684\u8bad\u7ec3\u3002\n\n### 3.1 \u9884\u8bad\u7ec3Lip-Sync Expert\n#### 1. \u7f51\u7edc\u7684\u642d\u5efa \n\u4e0a\u9762\u6211\u4eec\u5df2\u7ecf\u4ecb\u7ecd\u4e86SyncNet\u7684\u57fa\u672c\u7f51\u7edc\u7ed3\u6784\uff0c\u4e3b\u8981\u6709\u4e00\u7cfb\u5217\u7684(Conv+BatchNorm+Relu)\u7ec4\u6210\uff0c\u8fd9\u91cc\u6211\u4eec\u5bf9\u5176\u8fdb\u884c\u4e86\u4e00\u4e9b\u6539\u8fdb\uff0c\u52a0\u5165\u4e86\u6b8b\u5dee\u7ed3\u6784\u3002\u4e3a\u4e86\u65b9\u4fbf\u4e4b\u540e\u7684\u4f7f\u7528\uff0c\u6211\u4eec\u5bf9(Conv+BatchNorm+Relu)\u4ee5\u53ca\u6b8b\u5dee\u6a21\u5757\u8fdb\u884c\u4e86\u5c01\u88c5\u3002","558345fe":"### Lip-Sync Expert\nLip-sync Expert\u57fa\u4e8e **[SyncNet](https:\/\/www.robots.ox.ac.uk\/~vgg\/publications\/2016\/Chung16a\/)**\uff0c\u662f\u4e00\u79cd\u7528\u6765\u5224\u522b\u8bed\u97f3\u548c\u89c6\u9891\u662f\u5426\u540c\u6b65\u7684\u7f51\u7edc\u6a21\u578b\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff0cSyncNet\u7684\u8f93\u5165\u4e5f\u662f\u4e24\u79cd\uff1a\u8bed\u97f3\u7279\u5f81MFCC\u548c\u5634\u5507\u7684\u89c6\u9891\u5e27\uff0c\u5229\u7528\u4e24\u4e2a\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684Encoder\u5206\u522b\u5bf9\u8f93\u5165\u7684\u8bed\u97f3\u548c\u89c6\u9891\u5e27\u8fdb\u884c\u964d\u7eac\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5c06\u4e8c\u8005\u7684\u7279\u5f81\u90fd\u6620\u5c04\u5230\u540c\u4e00\u4e2a\u7eac\u5ea6\u7a7a\u95f4\u4e2d\u53bb\uff0c\u6700\u540e\u5229\u7528contrastive loss\u5bf9\u5507\u97f3\u540c\u6b65\u6027\u8fdb\u884c\u8861\u91cf\uff0c\u7ed3\u679c\u7684\u503c\u8d8a\u5927\u4ee3\u8868\u8d8a\u4e0d\u540c\u6b65\uff0c\u7ed3\u679c\u503c\u8d8a\u5c0f\u5219\u4ee3\u8868\u8d8a\u540c\u6b65\u3002\u5728Wav2Lip\u6a21\u578b\u4e2d\uff0c\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e86SyncNet\u7684\u7f51\u7edc\u7ed3\u6784\uff1a\u7f51\u7edc\u66f4\u6df1\uff1b\u52a0\u5165\u4e86\u6b8b\u5dee\u7f51\u7edc\u7ed3\u6784\uff1b\u8f93\u5165\u7684\u8bed\u97f3\u7279\u5f81\u88ab\u66ff\u6362\u6210\u4e86mel-spectrogram\u7279\u5f81\u3002","5323af87":"## Wav2Lip\n**[Wav2Lip](https:\/\/arxiv.org\/pdf\/2008.10010.pdf)** \u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u751f\u6210\u7f51\u7edc\u7684\u7531\u8bed\u97f3\u9a71\u52a8\u7684\u4eba\u8138\u8bf4\u8bdd\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002\u5982\u4e0b\u56fe\u6240\u793a\uff0cWav2Lip\u7684\u7f51\u7edc\u6a21\u578b\u603b\u4f53\u4e0a\u5206\u6210\u4e09\u5757\uff1a\u751f\u6210\u5668\u3001\u5224\u522b\u5668\u548c\u4e00\u4e2a\u9884\u8bad\u7ec3\u597d\u7684Lip-Sync Expert\u7ec4\u6210\u3002\u7f51\u7edc\u7684\u8f93\u5165\u67092\u4e2a\uff1a\u4efb\u610f\u7684\u4e00\u6bb5\u89c6\u9891\u548c\u4e00\u6bb5\u8bed\u97f3\uff0c\u8f93\u51fa\u4e3a\u4e00\u6bb5\u5507\u97f3\u540c\u6b65\u7684\u89c6\u9891\u3002\u751f\u6210\u5668\u662f\u57fa\u4e8eencoder-decoder\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u5206\u522b\u5229\u75282\u4e2aencoder: speech encoder, identity encoder\u53bb\u5bf9\u8f93\u5165\u7684\u8bed\u97f3\u548c\u89c6\u9891\u4eba\u8138\u8fdb\u884c\u7f16\u7801\uff0c\u5e76\u5c06\u4e8c\u8005\u7684\u7f16\u7801\u7ed3\u679c\u8fdb\u884c\u62fc\u63a5\uff0c\u9001\u5165\u5230 face decoder \u4e2d\u8fdb\u884c\u89e3\u7801\u5f97\u5230\u8f93\u51fa\u7684\u89c6\u9891\u5e27\u3002\u5224\u522b\u5668Visual Quality Discriminator\u5bf9\u751f\u6210\u7ed3\u679c\u7684\u8d28\u91cf\u8fdb\u884c\u89c4\u8303\uff0c\u63d0\u9ad8\u751f\u6210\u89c6\u9891\u7684\u6e05\u6670\u5ea6\u3002\u4e3a\u4e86\u66f4\u597d\u7684\u4fdd\u8bc1\u751f\u6210\u7ed3\u679c\u7684\u5507\u97f3\u540c\u6b65\u6027\uff0cWav2Lip\u5f15\u5165\u4e86\u4e00\u4e2a\u9884\u9884\u8bad\u7ec3\u7684\u5507\u97f3\u540c\u6b65\u5224\u522b\u6a21\u578b Pre-trained Lip-sync Expert\uff0c\u4f5c\u4e3a\u8861\u91cf\u751f\u6210\u7ed3\u679c\u7684\u5507\u97f3\u540c\u6b65\u6027\u7684\u989d\u5916\u635f\u5931\u3002","fa7ad742":"#### 3.\u8bad\u7ec3\n\n\u4f7f\u7528cosine_loss \u4f5c\u4e3a\u635f\u5931\u51fd\u6570","20db7acc":"**\u5224\u522b\u5668**  \n\u5224\u522b\u5668\u4e5f\u662f\u7531\u4e00\u7cfb\u5217\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7ec4\u6210\uff0c\u8f93\u5165\u4e00\u5f20\u4eba\u8138\u56fe\u7247\uff0c\u5229\u7528face encoder\u5bf9\u5176\u8fdb\u884c\u964d\u7ef4\u5230512\u7ef4\u3002"}}