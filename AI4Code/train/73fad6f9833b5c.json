{"cell_type":{"d749df77":"code","ebb7fa78":"code","01f2a98d":"code","1847e9bf":"code","a6c56bc7":"code","54a6cfc4":"markdown","344895ff":"markdown","677d491a":"markdown","f17efd3f":"markdown","740037a0":"markdown","68534f04":"markdown","1f65689e":"markdown"},"source":{"d749df77":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('..\/input\/fifa-2018-match-statistics\/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"\/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64, np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)","ebb7fa78":"import shap  # package used to calculate Shap values\n\nrow_to_show = 5\ndata_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)","01f2a98d":"my_model.predict_proba(data_for_prediction_array)","1847e9bf":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","a6c56bc7":"# use Kernel SHAP to explain test set predictions\nk_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\nk_shap_values = k_explainer.shap_values(data_for_prediction)\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","54a6cfc4":"Here is the code to get the SHAP values for a single prediction.","344895ff":"The team is 70% likely to have a player win the award. Now we visualize the SHAP values","677d491a":"# Your Turn\nSHAP values are awesome. Have fun applying them, alongside the other tools you've learned, to **[solve a full data-science scenario](https:\/\/www.kaggle.com\/kernels\/fork\/1637226)**\n","f17efd3f":"The shap_values object above is a list with two arrays. The first array in the list is the SHAP values for a negative outcome (don't win the award.) . The second array is the list of SHAP values for the positive outcome, which is how we usually think about predictions. It's cumbersome to review a raw array, but the shap package has a nice way to visualize the results.  Before doing that, here are the predicted probabilities of each outcome.","740037a0":"# ML for Insights Challenge\nThis is day 3 of the ML for Insights challenge.\n\nThe earlier parts of the challenge are:\n0. Use Cases for ML Insights: **[Tutorial](https:\/\/www.kaggle.com\/dansbecker\/use-cases-for-model-insights)**\n1. Permutation Importance: **[Tutorial](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance) - [Exercise](https:\/\/www.kaggle.com\/dansbecker\/exercise-permutation-importance)**\n2. Partial Depenence Plots: **[Tutorial](https:\/\/www.kaggle.com\/dansbecker\/partial-plots) - [Exercise](https:\/\/www.kaggle.com\/dansbecker\/exercise-partial-plots)**\n\n\n# SHAP Values\n\nYou've seen (and used) techniques to extract general insights from a machine learning model. But what if you want to break down how the model works for an individual prediction?\n\nSHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature. Where could you use this?\n\n- A model says a bank shouldn't loan someone money, and the bank is legally required to explain the basis for each loan rejection\n- A healthcare provider wants to identify what factors are driving each patient's risk of some disease so they can directly address those risk factors with targeted health interventions\n\nYou'll use SHAP Values to explain individual predictions in this lesson. In the next lesson, you'll see how these can be aggregated into powerful model-level insights.\n\n# How They Work\nSHAP values interpret the impact of having a certain value for a given feature in comparison to the prediction we'd make if that feature took some baseline value.\n\nAn example is helpful, and we'll continue the soccer\/football example from the [permutation importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance) and [partial dependence plots](https:\/\/www.kaggle.com\/dansbecker\/partial-plots) lessons.\n\n\nIn these tutorials, we predicted whether a team would have a player win the *Man of the Game* award.\n\nWe could ask:\n- How much was a prediction driven by the fact that the team scored 3 goals?\n    \nBut it's easier to give a concrete, numeric answer if we restate this as:\n- How much was a prediction driven by the fact that the team scored 3 goals, **instead of some baseline number of goals.**\n\nOf course, each team has many features. So if we answer this question for `number of goals`, we could repeat the process for all other features.\n\nSHAP values do this in a way that guarantees a nice property. When we make a prediction\n\n```sum(SHAP values for all features) = pred_for_team - pred_for_baseline_values```\n\nThat is, the SHAP values of all features sum up to explain why my prediction was different from the baseline. This allows us to decompose a prediction in a graph like this:\n\n![Imgur](https:\/\/i.imgur.com\/JVD2U7k.png)\n\n*If you want a larger view of this graph, [here is a link](https:\/\/i.imgur.com\/JVD2U7k.png)*","68534f04":"\nHow do you interpret this?\n\nWe predicted 0.7, whereas the base_value is 0.4979.  Feature values causing increased predictions are in pink, and their visual size shows the magnitude of the feature's effect.  Feature values decreasing the prediction are in blue.  The biggest impact comes from `Goal Scored` being 2.  Though the ball possession value has a meaningful effect decreasing the prediction.\n\nIf you subtract the length of the blue bars from the length of the pink bars, it equals the distance from the base value to the output.\n\nThere is some complexity to the technique, to ensure that the baseline plus the sum of individual effects adds up to the prediction (which isn't as straightforward as it sounds.) We won't go into that detail here, since it isn't critical for using the technique. [This blog post](https:\/\/towardsdatascience.com\/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d) has a longer theoretical explanation.\n\n# Code to Calculate SHAP Values\nWe calculate SHAP values using the wonderful [Shap](https:\/\/github.com\/slundberg\/shap) library.\n\nFor this example, we'll reuse the model you've already seen with the Soccer data.","1f65689e":"If you look carefully at the code where we created the SHAP values, you'll notice we reference Trees in `shap.TreeExplainer(my_model)`.  But the SHAP package has explainers for every type of model.\n\n- `shap.DeepExplainer` works with Deep Learning models. \n- `shap.KernelExplainer` works with all models, though it is slower than other Explainers and it offers an approximation rather than exact Shap values.\n\nHere is an example using KernelExplainer to get the same results you saw above."}}