{"cell_type":{"8c5dfa3b":"code","e9416b42":"code","c3e6e042":"code","32f843ae":"code","a31205f1":"code","df2513d6":"code","b369c212":"code","d0c61cd1":"code","1d367adb":"code","0ed73ed7":"code","cd3a5d11":"code","1010a643":"code","88283bb8":"code","fa0c1925":"code","f94aa516":"code","b1cbe015":"code","8c47a28e":"code","e03fea3b":"code","4719c324":"code","39ab7ee5":"code","9243ef4b":"code","2da804f9":"code","a9ef79cf":"code","7b0272d7":"code","f4085a76":"code","68a05042":"code","b6c8a1a9":"code","2b114adc":"code","67395bba":"code","f2de0d72":"code","464e3c5c":"code","cc69d6ee":"code","9671e2d2":"code","aaa633a2":"code","5be125bb":"code","a097c643":"code","ce86a4c1":"code","384a3c76":"code","46184843":"markdown","c2f3e671":"markdown","acac72d4":"markdown","f4605428":"markdown","d5cf157b":"markdown","b9d2d75a":"markdown","90d637a9":"markdown","5d8568fb":"markdown","c04d5ba4":"markdown","b6ad5873":"markdown","767ee33b":"markdown","4190c4f3":"markdown","bcdad248":"markdown","b6207a17":"markdown","48388673":"markdown","05702ddd":"markdown","0da7beed":"markdown","9fba8f21":"markdown","2d92765c":"markdown","ab37b7b8":"markdown","2d253b9e":"markdown","4a096c9e":"markdown","236583cf":"markdown","5c9ef3de":"markdown","65c16594":"markdown","7babfb67":"markdown"},"source":{"8c5dfa3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9416b42":"df=pd.read_csv('..\/input\/nyse\/prices.csv')","c3e6e042":"df.head()","32f843ae":"df.isnull().sum()","a31205f1":"df.info()","df2513d6":"df.describe()","b369c212":"df.shape","d0c61cd1":"df['symbol'].nunique()","1d367adb":"df1=df[df['symbol']=='AMZN']","0ed73ed7":"df1.head()","cd3a5d11":"df1.describe()","1010a643":"np.round(df1.median(),2)","88283bb8":"df1['date']=pd.to_datetime(df1['date'])","fa0c1925":"print(\"Minimum date value : {}\".format(df1['date'].min()))\nprint(\"Maximum date value : {}\".format(df1['date'].max()))","f94aa516":"#importing ploting libraries\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\ncolors = ['#FF9900','#000000']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})\nsns.palplot(colors, size=2)","b1cbe015":"fig=plt.figure(figsize=(20,8))\nax=sns.lineplot(data=df1, x='date',y='open')\nax=sns.lineplot(data=df1, x='date',y='close', color=colors[1]);\nfor s in ['left','right','top','bottom']:\n    ax.spines[s].set_visible(False)\n\nplt.title(\"AMAZON Stock value changes since 2010\", size=20, weight='bold')","8c47a28e":"fig=plt.figure(figsize=(20,8))\nax=sns.lineplot(data=df1, x='date',y='volume')\n#ax=sns.lineplot(data=df1, x='date',y='close', color=colors[1]);\nfor s in ['left','right','top','bottom']:\n    ax.spines[s].set_visible(False)\nplt.title(\"Google Stock volume\", size=20, weight='bold')","e03fea3b":"#integer columns\nfig=plt.figure(figsize=(20,8), tight_layout=True)\nplt.suptitle(\"Analysing the Numeric variables\", size=20, weight='bold')\nax=fig.subplot_mosaic(\"\"\"AB\n                         CC\n                         DE\"\"\")\nsns.kdeplot(df1['high'], ax=ax['A'], color=colors[0], fill=True, linewidth=2)\nsns.kdeplot(df1['low'], ax=ax['B'], color=colors[1],fill=True, linewidth=2)\nsns.kdeplot(df1['open'], ax=ax['C'], color=colors[0],fill=True, linewidth=2)\nsns.kdeplot(df1['close'], ax=ax['D'], color=colors[1],fill=True, linewidth=2)\nsns.kdeplot(df1['volume'], ax=ax['E'], color=colors[0],fill=True, linewidth=2)\nax['B'].yaxis.set_visible(False)\nax['E'].yaxis.set_visible(False)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['D'].yaxis.label.set_alpha(0.5)\nfor s in ['left','right','top','bottom']:\n    ax['A'].spines[s].set_visible(False)\n    ax['B'].spines[s].set_visible(False)\n    ax['C'].spines[s].set_visible(False)\n    ax['D'].spines[s].set_visible(False)\n    ax['E'].spines[s].set_visible(False)","4719c324":"#integer columns\nfig=plt.figure(figsize=(20,8), tight_layout=True)\nplt.suptitle(\"Boxplot the Numeric variables\", size=20, weight='bold')\nax=fig.subplot_mosaic(\"\"\"AB\n                         CC\n                         DE\"\"\")\nsns.boxplot(df1['high'], ax=ax['A'], color=colors[0])\nsns.boxplot(df1['low'], ax=ax['B'], color=colors[1])\nsns.boxplot(df1['open'], ax=ax['C'], color=colors[0])\nsns.boxplot(df1['close'], ax=ax['D'], color=colors[1])\nsns.boxplot(df1['volume'], ax=ax['E'], color=colors[1])\nax['B'].yaxis.set_visible(False)\nax['E'].yaxis.set_visible(False)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['A'].yaxis.label.set_alpha(0.5)\nax['C'].yaxis.label.set_alpha(0.5)\nax['D'].yaxis.label.set_alpha(0.5)\nfor s in ['left','right','top','bottom']:\n    ax['A'].spines[s].set_visible(False)\n    ax['B'].spines[s].set_visible(False)\n    ax['C'].spines[s].set_visible(False)\n    ax['D'].spines[s].set_visible(False)\n    ax['E'].spines[s].set_visible(False)","39ab7ee5":"df1.drop(['symbol'], axis=1, inplace=True)","9243ef4b":"# we need to predict the closing price of the stock, lets us consider 'Close' feature as the Target variable. \nsns.pairplot(df1,corner=True)","2da804f9":"df1.corr()['close']","a9ef79cf":"from scipy.stats import levene, shapiro\nint_cols=df1.select_dtypes(exclude='object').columns.to_list()\n\nfor i in int_cols:\n    _, p_value=shapiro(df1[i])\n    if p_value<0.05:\n        print(\"Feature {} is normaly distributed\".format(i))\n    else:\n        print(\"Feature {} is not normaly distributed\".format(i))\n        \n    print(\"Normalitiy test p_value for featue -  {} is {}\".format(i,np.round(p_value,3)))","7b0272d7":"fig=plt.figure(figsize=(15,8))\nsns.heatmap(df1.corr(), annot=True, cmap=[colors[0],colors[1]], linecolor='white', linewidth=2 )","f4085a76":"X=df1[['volume','open']]\ny=df1['close']","68a05042":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, shuffle=False, random_state=42)","b6c8a1a9":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test =scaler.transform(X_test)","2b114adc":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn import set_config\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\nset_config(display='diagram')\npred=model.predict(X_test)\nsc=np.round(model.score(X_test, y_test),2) * 100\nr2=np.round(r2_score(y_test,pred),2)\nmse=np.round(mean_squared_error(y_test,pred),2)\nmae=np.round(mean_squared_error(y_test,pred),2)","67395bba":"fig=plt.figure(figsize=(15,8))\np=pd.Series(pred, index=y_test.index)\nplt.plot(y_test)\nplt.plot(p)\nplt.legend(['y_test','predicted'])\nplt.title(\"Compare test and predicted values\", size=20, weight='bold')\nplt.text(x=800000, y=600,s='Accuracy score : {} %'.format(sc))\nplt.text(x=800000, y=580,s='R2 Score : {}'.format(r2))\nplt.text(x=800000, y=560,s='Mean Squared error : {}'.format(mse))\nplt.text(x=800000, y=540,s='Mean Absolute error : {}'.format(mae))","f2de0d72":"X=df1[['open','high']]\ny=df1['close']\nlength=100\n#from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, shuffle=False, random_state=42)\ntraining_set = X.iloc[:1000].values\ntest_set = X.iloc[1000:].values\n# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\ntraining_set_scaled = sc.fit_transform(training_set)\ntest_set_scaled=sc.transform(test_set)\n# Creating a data structure with 60 time-steps and 1 output\nX_train = []\ny_train = []\nfor i in range(length, len(training_set)):\n    X_train.append(training_set_scaled[i-length:i, 0])\n    y_train.append(training_set_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n\nX_test = []\ny_test = []\nfor i in range(length, len(test_set)):\n    X_test.append(test_set_scaled[i-length:i, 0])\n    y_test.append(test_set_scaled[i, 0])\nX_test, y_test = np.array(X_test), np.array(y_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","464e3c5c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout\nmodel = Sequential()\n#Adding the first LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\n# Adding a second LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a third LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a fourth LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n# Adding the output layer\nmodel.add(Dense(units = 1))\n\n# Compiling the RNN\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nmodel.fit(X_train, y_train, validation_data=(X_test,y_test),epochs = 100, batch_size = 32)","cc69d6ee":"loss = pd.DataFrame(model.history.history)\nfig=plt.figure(figsize=(15,8))\nplt.title(\"Validation loss Vs Trainiinverse_transformss\", size=20, weight='bold')\nplt.plot(loss)","9671e2d2":"pred=model.predict(X_test)\ntest=pd.DataFrame(columns=['test','pred'])\ntest['test']=y_test\ntest['pred']=pred.flatten()\ntest","aaa633a2":"fig=plt.figure(figsize=(15,8))\nplt.title(\"Test vs Predicted value\", size=20, weight='bold')\nplt.plot(test)\nplt.legend(['test','predict'])\nr2=np.round(r2_score(y_test,pred),2)\nmse=np.round(mean_squared_error(y_test,pred),2)\nmae=np.round(mean_squared_error(y_test,pred),2)\nplt.text(x=500, y=1.5,s='R2 Score : {}'.format(r2))\nplt.text(x=500, y=1.35,s='Mean Squared error : {}'.format(mse))\nplt.text(x=500, y=1.25,s='Mean Absolute error : {}'.format(mae))","5be125bb":"from fbprophet import Prophet\ndf_p = df1[['date','close']]\ndf_p.columns=['ds','y']\n\nsplit_data = df_p.index.max()-100000\ntrain = df_p.loc[df_p.index<=split_data].copy()\ntest=df_p.loc[df_p.index>split_data].copy()\ntrain.set_index('ds',inplace=True)\ntest.set_index('ds',inplace=True)\ntrain.reset_index(inplace=True)\ntest.reset_index(inplace=True)\n\n#Model creation\nmodel=Prophet()\nmodel.fit(train)\n\n#model prediction\npred=model.predict(test)","a097c643":"test_pred=pd.DataFrame(columns=['ds','test','predict','pred_lower','pred_high'], index=test.index)\ntest_pred['test']=test['y']\ntest_pred['ds']=test['ds']\ntest_pred['predict']=pred['yhat']\ntest_pred['pred_lower']=pred['yhat_lower']\ntest_pred['pred_high']=pred['yhat_upper']","ce86a4c1":"#plotting Test vs Predicted\nfig=plt.figure(figsize=(15,8))\nplt.title(\"Test Vs Prediction\", size=20, weight='bold')\nsns.lineplot(data=test_pred,x='ds',y='test')\nsns.lineplot(data=test_pred,x='ds',y='predict')\n\nr2=np.round(r2_score(test_pred['test'],test_pred['predict']),2)\nmse=np.round(mean_squared_error(test_pred['test'],test_pred['predict']),2)\nmae=np.round(mean_squared_error(test_pred['test'],test_pred['predict']),2)\nplt.text(x=mdates.datestr2num('2016-10'), y=700,s='R2 Score : {}'.format(r2))\nplt.text(x=mdates.datestr2num('2016-10'), y=680,s='Mean Squared error : {}'.format(mse))\nplt.text(x=mdates.datestr2num('2016-10'), y=660,s='Mean Absolute error : {}'.format(mae))","384a3c76":"fig=plt.figure(figsize=(15,8))\nplt.title(\"Test Vs Prediction\", size=20, weight='bold')\nsns.lineplot(data=train,x='ds',y='y')\nsns.lineplot(data=test_pred,x='ds',y='predict')\nsns.lineplot(data=test_pred,x='ds',y='test', alpha=0.5, ls='--', color='black')\n\nr2=np.round(r2_score(test_pred['test'],test_pred['predict']),2)\nmse=np.round(mean_squared_error(test_pred['test'],test_pred['predict']),2)\nmae=np.round(mean_squared_error(test_pred['test'],test_pred['predict']),2)\nplt.text(x=mdates.datestr2num('2015'), y=300,s='R2 Score : {}'.format(r2))\nplt.text(x=mdates.datestr2num('2015'), y=260,s='Mean Squared error : {}'.format(mse))\nplt.text(x=mdates.datestr2num('2015'), y=220,s='Mean Absolute error : {}'.format(mae))\nplt.legend(['Train','Predict','Test'])","46184843":"# Bivariated & Multivariated Analysis","c2f3e671":"# Check for Null Values","acac72d4":"***No Null values in the entire dataset***","f4605428":"# Dataframe shape","d5cf157b":"***Date & Symbol are in object datatype and rest are float datatype***","b9d2d75a":"# EDA","90d637a9":"# Stock Prediction","5d8568fb":"# Notebook is in progress","c04d5ba4":"***we have around 6 years of data***","b6ad5873":"***All the Median of all the numeric values are lesser than mean, so the dataset is right skewed","767ee33b":"# Hypothesis test to find the Normality in the Dataset","4190c4f3":"# Normalizing the values","bcdad248":"# Describe the data","b6207a17":"# Categorical feature analysis  \n** There is no categorical feature other than Symbol. as we have taken the Google stock as sample data, we will drop the Symbol feature.","48388673":"# Read Data","05702ddd":"# Univariated Analysis","0da7beed":"# Train Test Split","9fba8f21":"# Try with LSTM","2d92765c":"# Correlation","ab37b7b8":"# Sampling AMAZON from the dataset\nGOOGL is the NYSE stock symbol for Google stocks. let us take that as a sample for our analysis.","2d253b9e":"***There are 501 symbols in the datasets and in total there are 851264 rows with 7 columns***  \n***Data set is good for analysis. however, since there are many company details are there. we will sample it with one Company***","4a096c9e":"# Model Creation  \n## basic Linear regression model","236583cf":"# Try with FBProphet","5c9ef3de":"***There seems to be ouliers in the data set, however, those values can't be considered as outlier as they may be extrem values during peak selling days***  \n***Date column has been ingored as it is series of numbers***  \nLet us analyse more for conclusion","65c16594":"# Check Datatype of the features","7babfb67":"***feature Open, high, low are highly correlated to Target feature Close. we can use either one of the feature for prediction to avoid multicollinearity***"}}