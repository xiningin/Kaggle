{"cell_type":{"a8a1aef3":"code","1d0aed5d":"code","d4f7edfa":"code","aec52205":"code","f03f1f27":"code","518fbb88":"code","79bb1c58":"code","958e8525":"code","7851c3ee":"code","df1f7399":"code","b6c00259":"code","47331f47":"code","e1ff1073":"code","5548f956":"code","85db5419":"code","e912300d":"code","23e77f6f":"code","e3c93842":"code","5613fd98":"code","b82a049f":"code","19e81e27":"code","dda0976a":"code","9b64fc82":"code","f975ae1a":"code","765629b8":"code","a2b76ab0":"code","3eb1f9db":"code","e61fd8d3":"code","7d0a7f0f":"code","ef631a9f":"code","c7b3ffdb":"code","613a11e0":"code","799d5cc2":"code","23f056a9":"code","1fe39952":"code","df6f8350":"code","4df3c44d":"code","ba5205ad":"code","a78c1b4d":"code","b281896e":"code","6ab5f6ee":"code","8a1a8d81":"code","713e6f00":"code","4806ebb7":"code","3a8082ab":"code","c90c372e":"code","b8f007c3":"code","2322672e":"code","c9cffb6b":"code","b3dd41f7":"code","86b3344a":"code","26e21b68":"code","25e22648":"code","fb3273b5":"code","bdca2ff5":"code","11180699":"code","f7eb4478":"code","bd49baee":"code","e712fedf":"code","68a5bcbd":"markdown","af08fdc9":"markdown","0ca46cc6":"markdown","a560717f":"markdown","e79243eb":"markdown","5c10c18c":"markdown","c1bcb66e":"markdown","296702c7":"markdown","2ba88fef":"markdown","869d81a8":"markdown","9640ed62":"markdown","a866aa47":"markdown","2e98f370":"markdown","8641658c":"markdown","5576453d":"markdown","cdfe9fd6":"markdown","c8ca017c":"markdown","4020bcc0":"markdown","e8885e78":"markdown","e12f1789":"markdown","9953b21a":"markdown","00b4927b":"markdown","68ba76a7":"markdown","f03d9642":"markdown"},"source":{"a8a1aef3":"# Importing core libraries\nimport seaborn as sns\nimport scipy.stats as ss\nimport math \nimport numpy as np\nimport pandas as pd\nimport pprint\nimport joblib\n\nfrom collections import Counter\nfrom time import time\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\n# Classifiers\nfrom catboost import CatBoostClassifier, Pool\n\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer","1d0aed5d":"df_train = pd.read_csv('..\/input\/bri-data-hackathon-pa\/train.csv')\ndf_test = pd.read_csv('..\/input\/bri-data-hackathon-pa\/test.csv')\nsample_submission = pd.read_csv('..\/input\/bri-data-hackathon-pa\/sample_submission.csv')\n\n#df_train = df_train.drop(['age','year_graduated'],axis=1)\n#df_test = df_test.drop(['age','year_graduated'],axis=1)","d4f7edfa":"numbers = df_train.age\nage_numbers = [2020 - number for number in numbers]\ndf_train.age = age_numbers\n\nnum = df_test.age\nage_num = [2020 - x for x in num]\ndf_test.age = age_num","aec52205":"numbers1 = df_train.year_graduated\nage_numbers1 = [2020 - number for number in numbers1]\ndf_train.year_graduated = age_numbers1\n\nnum1 = df_test.year_graduated\nage_num1 = [2020 - x for x in num1]\ndf_test.year_graduated = age_num1","f03f1f27":"#change GPA 4.1-41 Train\ncolumn_name = 'GPA'\n\nmask1 = (df_train.GPA >= 4.1) & (df_train.GPA <= 41)\ndf_train.loc[mask1, column_name] = df_train.GPA \/ 10\n\n#change GPA 41-410 Train\n\nmask2 = (df_train.GPA >= 41) & (df_train.GPA <= 410)\ndf_train.loc[mask2, column_name] = df_train.GPA \/ 100\n\n#change GPA 4.1-41 Test\n\nmask3 = (df_test.GPA >= 4.1) & (df_test.GPA <= 41)\ndf_test.loc[mask3, column_name] = df_test.GPA \/ 10\n\n#change GPA 41-410 Test\n\nmask4 = (df_test.GPA >= 41) & (df_test.GPA <= 410)\ndf_test.loc[mask4, column_name] = df_test.GPA \/ 100","518fbb88":"#df_train['Last_achievement_%'] = df_train['Last_achievement_%'] \/ 100\n#df_test['Last_achievement_%'] = df_test['Last_achievement_%'] \/ 100","79bb1c58":"df_train['quantile_current_branch'] = pd.qcut(df_train['job_duration_in_current_branch'],\n                              q=[0, .2, .4, .6, .8, 1],\n                              labels=False,\n                            duplicates='drop')\ndf_test['quantile_current_branch'] = pd.qcut(df_test['job_duration_in_current_branch'],\n                              q=[0, .2, .4, .6, .8, 1],\n                              labels=False,\n                            duplicates='drop')","958e8525":"jblv1 = 'job_level'\n\nfilt = df_train[jblv1] == 'JG03'\ndf_train.loc[filt, jblv1] = 0\n\nfilt2 = (df_train[jblv1] == 'JG04')\ndf_train.loc[filt2, jblv1] = 1\n\nfilt3 = (df_train[jblv1] == 'JG05')\ndf_train.loc[filt3, jblv1] = 2\n\n#test\n\nj11 = 'job_level'\n\nfilt1 = df_test[j11] == 'JG03'\ndf_test.loc[filt1, j11] = 0\n\nfilt22 = (df_test[j11] == 'JG04')\ndf_test.loc[filt22, j11] = 1\n\nfilt33 = (df_test[j11] == 'JG05')\ndf_test.loc[filt33, j11] = 2\n\nfilt44 = (df_test[j11] == 'JG06')\ndf_test.loc[filt44, j11] = 2","7851c3ee":"perlv1 = 'person_level'\n\nfilt = df_train[perlv1] == 'PG01'\ndf_train.loc[filt, perlv1] = 0\nfilt2 = (df_train[perlv1] == 'PG02')\ndf_train.loc[filt2, perlv1] = 1\nfilt3 = (df_train[perlv1] == 'PG03')\ndf_train.loc[filt3, perlv1] = 2\nfilt4 = (df_train[perlv1] == 'PG04')\ndf_train.loc[filt4, perlv1] = 3\nfilt5 = (df_train[perlv1] == 'PG05')\ndf_train.loc[filt5, perlv1] = 4\nfilt6 = (df_train[perlv1] == 'PG06')\ndf_train.loc[filt6, perlv1] = 5\nfilt7 = (df_train[perlv1] == 'PG07')\ndf_train.loc[filt7, perlv1] = 6\nfilt8 = (df_train[perlv1] == 'PG08')\ndf_train.loc[filt8, perlv1] = 7\n\n#test\nfilt = df_test[perlv1] == 'PG01'\ndf_test.loc[filt, perlv1] = 0\nfilt2 = (df_test[perlv1] == 'PG02')\ndf_test.loc[filt2, perlv1] = 1\nfilt3 = (df_test[perlv1] == 'PG03')\ndf_test.loc[filt3, perlv1] = 2\nfilt4 = (df_test[perlv1] == 'PG04')\ndf_test.loc[filt4, perlv1] = 3\nfilt5 = (df_test[perlv1] == 'PG05')\ndf_test.loc[filt5, perlv1] = 4\nfilt6 = (df_test[perlv1] == 'PG06')\ndf_test.loc[filt6, perlv1] = 5\nfilt7 = (df_test[perlv1] == 'PG07')\ndf_test.loc[filt7, perlv1] = 6\nfilt8 = (df_test[perlv1] == 'PG08')\ndf_test.loc[filt8, perlv1] = 7","df1f7399":"emptype1 = 'Employee_type'\n\nfilt = df_train[emptype1] == 'RM_type_C'\ndf_train.loc[filt, emptype1] = 0\n\nfilt2 = (df_train[emptype1] == 'RM_type_B')\ndf_train.loc[filt2, emptype1] = 1\n\nfilt3 = (df_train[emptype1] == 'RM_type_A')\ndf_train.loc[filt3, emptype1] = 2\n\n#test\nfilt = df_test[emptype1] == 'RM_type_C'\ndf_test.loc[filt, emptype1] = 0\n\nfilt2 = (df_test[emptype1] == 'RM_type_B')\ndf_test.loc[filt2, emptype1] = 1\n\nfilt3 = (df_test[emptype1] == 'RM_type_A')\ndf_test.loc[filt3, emptype1] = 2","b6c00259":"edulvl1 = 'Education_level'\n\nfilt = df_train[edulvl1] == 'level_0'\ndf_train.loc[filt, edulvl1] = 0\n\nfilt2 = (df_train[edulvl1] == 'level_1')\ndf_train.loc[filt2, edulvl1] = 1\n\nfilt3 = (df_train[edulvl1] == 'level_2')\ndf_train.loc[filt3, edulvl1] = 2\n\nfilt4 = (df_train[edulvl1] == 'level_3')\ndf_train.loc[filt4, edulvl1] = 3\n\nfilt5 = (df_train[edulvl1] == 'level_4')\ndf_train.loc[filt5, edulvl1] = 4\n\nfilt6 = (df_train[edulvl1] == 'level_5')\ndf_train.loc[filt6, edulvl1] = 5\n\n#test\nfilt = df_test[edulvl1] == 'level_0'\ndf_test.loc[filt, edulvl1] = 0\n\nfilt2 = (df_test[edulvl1] == 'level_1')\ndf_test.loc[filt2, edulvl1] = 1\n\nfilt3 = (df_test[edulvl1] == 'level_2')\ndf_test.loc[filt3, edulvl1] = 2\n\nfilt4 = (df_test[edulvl1] == 'level_3')\ndf_test.loc[filt4, edulvl1] = 3\n\nfilt5 = (df_test[edulvl1] == 'level_4')\ndf_test.loc[filt5, edulvl1] = 4\n\nfilt6 = (df_test[edulvl1] == 'level_5')\ndf_test.loc[filt6, edulvl1] = 5","47331f47":"for c in df_train.columns: display(df_train[c].value_counts().to_frame())","e1ff1073":"#Inter Quartile Range(IQR)\noutlier = []\ndef iqr_outliers(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > Upper_tail or i < Lower_tail:\n            outlier.append(i)\n    print(\"Outliers:\",outlier)\niqr_outliers(df_train['GPA'])","5548f956":"def replace(GPA):\n    if GPA in outlier:\n        return np.nan\n    else:\n        return GPA\ndf_train.GPA = df_train.GPA.apply(replace)\ndf_test.GPA = df_test.GPA.apply(replace)","85db5419":"last_outlier = []\ndef last_outliers(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > Upper_tail or i < Lower_tail:\n            last_outlier.append(i)\n    print(\"Outliers:\",last_outlier)\nlast_outliers(df_train['Last_achievement_%'])","e912300d":"def last_replace(Last):\n    if Last in last_outlier:\n        return np.nan\n    else:\n        return Last\ndf_train['Last_achievement_%'] = df_train['Last_achievement_%'].apply(last_replace)\ndf_test['Last_achievement_%'] = df_test['Last_achievement_%'].apply(last_replace)","23e77f6f":"branch_outlier = []\ndef branch_outliers(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > Upper_tail or i < Lower_tail:\n            branch_outlier.append(i)\n    print(\"Outliers:\",branch_outlier)\nbranch_outliers(df_train['job_duration_in_current_branch'])","e3c93842":"def branch_replace(branch):\n    if branch in branch_outlier:\n        return np.nan\n    else:\n        return branch\ndf_train['job_duration_in_current_branch'] = df_train['job_duration_in_current_branch'].apply(branch_replace)\ndf_test['job_duration_in_current_branch'] = df_test['job_duration_in_current_branch'].apply(branch_replace)","5613fd98":"joblevel_outlier = []\ndef joblevel_outliers(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > Upper_tail or i < Lower_tail:\n            joblevel_outlier.append(i)\n    print(\"Outliers:\",joblevel_outlier)\njoblevel_outliers(df_train['job_duration_in_current_job_level'])","b82a049f":"def joblevel_replace(joblevel):\n    if joblevel in joblevel_outlier:\n        return np.nan\n    else:\n        return joblevel\ndf_train['job_duration_in_current_job_level'] = df_train['job_duration_in_current_job_level'].apply(joblevel_replace)\ndf_test['job_duration_in_current_job_level'] = df_test['job_duration_in_current_job_level'].apply(joblevel_replace)","19e81e27":"personlevel_outlier = []\ndef personlevel_outliers(df):\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > Upper_tail or i < Lower_tail:\n            personlevel_outlier.append(i)\n    print(\"Outliers:\",personlevel_outlier)\npersonlevel_outliers(df_train['job_duration_in_current_job_level'])","dda0976a":"def personlevel_replace(personlevel):\n    if personlevel in personlevel_outlier:\n        return np.nan\n    else:\n        return personlevel\ndf_train['job_duration_in_current_job_level'] = df_train['job_duration_in_current_job_level'].apply(personlevel_replace)\ndf_test['job_duration_in_current_job_level'] = df_test['job_duration_in_current_job_level'].apply(personlevel_replace)","9b64fc82":"#df_features\nbinary_var = ['gender','marital_status_maried(Y\/N)']\nnominal = ['age','year_graduated',\n           'job_duration_in_current_job_level',\n           'job_duration_in_current_person_level',\n           'job_duration_in_current_branch',           \n           'GPA',\n           'job_duration_from_training',\n           'branch_rotation',\n           'job_rotation',\n           'assign_of_otherposition',\n           'annual leave',\n           'sick_leaves',\n           'Last_achievement_%',\n           'Achievement_above_100%_during3quartal',]\nordinal = ['job_level',\n           'person_level',\n           'Employee_type',\n           'number_of_dependences',\n           'Education_level',\n          ]\ncols = binary_var + nominal + ordinal","f975ae1a":"y = df_train['Best Performance'].values\nX = df_train.drop(['Best Performance'],axis=1)\nXt = df_test","765629b8":"X.info()","a2b76ab0":"from sklearn.preprocessing import MinMaxScaler\n\n# Min-max normalization\nX[ordinal] = MinMaxScaler().fit_transform(X[ordinal])\nXt[ordinal] = MinMaxScaler().fit_transform(Xt[ordinal])","3eb1f9db":"# Transforming all the labels of all variables\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoders = [LabelEncoder() for _ in range(X.shape[1])]\n\nfor col, column in enumerate(X.columns):\n    unique_values = pd.Series(X[column].append(Xt[column]).unique())\n    unique_values = unique_values[unique_values.notnull()]\n    label_encoders[col].fit(unique_values)\n    X.loc[X[column].notnull(), column] = label_encoders[col].transform(X.loc[X[column].notnull(), column])\n    Xt.loc[Xt[column].notnull(), column] = label_encoders[col].transform(Xt.loc[Xt[column].notnull(), column])","e61fd8d3":"#X = X.fillna(-999)\n#Xt = Xt.fillna(-999)\n\nX = X.apply(lambda x: x.fillna(x.mean()),axis=0)\nXt = Xt.apply(lambda x: x.fillna(x.mean()),axis=0)","7d0a7f0f":"def frequency_encoding(column, df, df_test=None):\n    frequencies = df[column].value_counts().reset_index()\n    df_values = df[[column]].merge(frequencies, how='left', \n                                   left_on=column, right_on='index').iloc[:,-1].values\n    if df_test is not None:\n        df_test_values = df_test[[column]].merge(frequencies, how='left', \n                                                 left_on=column, right_on='index').fillna(1).iloc[:,-1].values\n    else:\n        df_test_values = None\n    return df_values, df_test_values\n\nfor column in X.columns:\n    train_values, test_values = frequency_encoding(column, X, Xt)\n    X[column+'_freq'] = train_values\n    Xt[column+'_freq'] = test_values","ef631a9f":"import category_encoders as cat_encode\n\n\nenc_x = np.zeros(X[cols].shape)\n\nfor tr_idx, oof_idx in StratifiedKFold(n_splits=5, random_state=2021, shuffle=True).split(X, y):\n    encoder = cat_encode.TargetEncoder(cols=cols, smoothing=0.3)\n    \n    encoder.fit(X[cols].iloc[tr_idx], y[tr_idx])\n    enc_x[oof_idx, :] = encoder.transform(X[cols].iloc[oof_idx], y[oof_idx])\n    \nencoder.fit(X[cols], y)\nenc_xt = encoder.transform(Xt[cols]).values\n\nfor idx, new_var in enumerate(cols):\n    new_var = new_var + '_enc'\n    X[new_var] = enc_x[:,idx]\n    Xt[new_var] = enc_xt[:, idx]","c7b3ffdb":"feature_name = list(X.columns)\n# no of maximum features we need to select\nnum_feats=50","613a11e0":"def cor_selector(X, y,num_feats):\n    cor_list = []\n    feature_name = X.columns.tolist()\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature\ncor_support, cor_feature = cor_selector(X, y,num_feats)\nprint(str(len(cor_feature)), 'selected features')","799d5cc2":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=num_feats)\nchi_selector.fit(X_norm, y)\nchi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","23f056a9":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)","1fe39952":"rfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","df6f8350":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), max_features=num_feats)\nembeded_lr_selector.fit(X_norm, y)","4df3c44d":"embeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","ba5205ad":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\nembeded_rf_selector.fit(X, y)","a78c1b4d":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","b281896e":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nmodel= LGBMClassifier(random_state=33,\n                      #early_stopping_rounds = 250,\n                      n_estimators=2000,min_data_per_group=5, # reduce overfitting when using categorical_features\n                      boosting_type='gbdt', num_leaves=170, max_depth=- 1, learning_rate=0.01, subsample_for_bin=2000, \n                      min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, \n                      colsample_bytree=.75, reg_alpha=1.3, reg_lambda=0.1,  n_jobs=- 1,cat_smooth=1.0, \n                      silent=True, importance_type='split')\n\nembeded_lgb_selector = SelectFromModel(model, max_features=num_feats)\nembeded_lgb_selector.fit(X, y)","6ab5f6ee":"embeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')","8a1a8d81":"from sklearn.feature_selection import SelectFromModel\nimport xgboost as xgb\n\nxgb_model= xgb.XGBClassifier(n_estimators=2000,\n                         max_depth=12, \n                         learning_rate=0.02, \n                         subsample=0.8,\n                         colsample_bytree=0.4, \n                         missing=-1,)\n\nembeded_xgb_selector = SelectFromModel(xgb_model, max_features=num_feats)\nembeded_xgb_selector.fit(X, y)","713e6f00":"embeded_xgb_support = embeded_lgb_selector.get_support()\nembeded_xgb_feature = X.loc[:,embeded_xgb_support].columns.tolist()\nprint(str(len(embeded_xgb_feature)), 'selected features')","4806ebb7":"\n#pd.set_option('display.max_rows', None)\n# put all selection together\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, \n                                     'Pearson':cor_support, \n                                     'Chi-2':chi_support, \n                                     'RFE':rfe_support, \n                                     'Logistics':embeded_lr_support,\n                                     'Random Forest':embeded_rf_support, \n                                     'LightGBM':embeded_lgb_support,\n                                     'XGBoost' :embeded_xgb_support\n                                    })\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df","3a8082ab":"feature_selected = feature_selection_df.loc[(feature_selection_df.Total >= 3)]\nfeatures = feature_selected.Feature.tolist()","c90c372e":"X = X.astype(np.float32)\nXt = Xt.astype(np.float32)\n\n# Defining categorical variables\ncat_features = features\n#cat_features = cols\n# Setting categorical variables to int64\nX[cat_features] = X[cat_features].astype(np.int64)\nXt[cat_features] = Xt[cat_features].astype(np.int64)","b8f007c3":"#from imblearn.over_sampling import SMOTE\n\n#smote = SMOTE(random_state = 42)\n#X, y = smote.fit_resample(X,y)\n\n\n#sns.countplot(y)","2322672e":"import gc\n_ = gc.collect()","c9cffb6b":"# Initializing a CatBoostClassifier with best parameters\n\"\"\"\nparam_cb = {'bagging_temperature': 0.8,\n               'depth': 5,\n               'iterations': 1000,\n               'l2_leaf_reg': 30,\n               'learning_rate': 0.05,\n               'random_strength': 0.8,\n            'bootstrap_type' : 'Bayesian',\n            'objective':'CrossEntropy',\n           }\n\"\"\"\n\nparam_cb = {\n        'learning_rate': 0.05,\n        'random_strength': 0.8,\n        'bagging_temperature': 0.8, \n        'l2_leaf_reg': 30,\n        'depth': 12, \n        #'max_leaves': 48,\n        'max_bin':255,\n        'iterations' : 1000,\n        #'task_type':'GPU',\n        'loss_function' : \"Logloss\",\n        'objective':'CrossEntropy',\n        'eval_metric' : \"AUC\",\n        'bootstrap_type' : 'Bayesian',\n        'use_best_model': True \n}\n","b3dd41f7":"# Setting a n-fold stratified cross-validation (note: shuffle=True)\nSEED = 2021\n#SEED = 42\nFOLDS = 100\n#FOLDS = 5\nskf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)","86b3344a":"# CV interations\nroc_auc = list()\naverage_precision = list()\noof = np.zeros(len(X))\ncv_test_preds = np.zeros(len(Xt))\nbest_iteration = list()\n#i = 1\nfor train_idx, test_idx in skf.split(X, y):\n    #print(\"\\nfold {}\".format(i))\n    X_train, y_train = X.iloc[train_idx, :], y[train_idx]\n    X_test, y_test = X.iloc[test_idx, :], y[test_idx]\n    \n    train = Pool(data=X_train, \n             label=y_train,            \n             feature_names=list(X_train.columns),\n             cat_features=cat_features)\n\n    val = Pool(data=X_test, \n               label=y_test,\n               feature_names=list(X_test.columns),\n               cat_features=cat_features)\n\n    catb = CatBoostClassifier(**param_cb,\n                          #loss_function='Logloss',\n                          #eval_metric = 'AUC',\n                          #nan_mode='Min',\n                          thread_count=4,\n                          verbose = True)\n    \n    catb.fit(train,\n             verbose_eval=100, \n             early_stopping_rounds=50,\n             eval_set=val,\n             use_best_model=True,\n             #task_type = \"GPU\",\n             plot=False)\n    \n    best_iteration.append(catb.best_iteration_)\n    preds = catb.predict_proba(X_test)\n    oof[test_idx] = preds[:,1]\n    \n    # CV test prediction\n    Xt_pool = Pool(data=Xt[list(X_train.columns)],\n               feature_names=list(X_train.columns),\n               cat_features=cat_features)\n    \n    cv_test_preds += catb.predict_proba(Xt_pool)[:,1] \/ FOLDS\n    \n    roc_auc.append(roc_auc_score(y_true=y_test, y_score=preds[:,1]))\n    average_precision.append(average_precision_score(y_true=y_test, y_score=preds[:,1]))","26e21b68":"# Storing results to disk\nsample_submission['Best Performance'] = cv_test_preds\nsample_submission.to_csv('catboost_param_cb_100fold_new.csv', index=False)","25e22648":"sample_submission.head()","fb3273b5":"sample_submission['Best Performance'].mean()","bdca2ff5":"import matplotlib.pyplot as plt\n\nplt.hist(sample_submission['Best Performance'],bins=100)\nplt.ylim((0,5000))\nplt.title('cat')\nplt.show()","11180699":"oof = pd.DataFrame({'id':X.index, 'catboost_oof': oof})\noof.to_csv(\"oof.csv\", index=False)","f7eb4478":"print(\"Average cv roc auc score %0.3f \u00b1 %0.3f\" % (np.mean(roc_auc), np.std(roc_auc)))\nprint(\"Average cv roc average precision %0.3f \u00b1 %0.3f\" % (np.mean(average_precision), np.std(average_precision)))\n\nprint(\"Roc auc score OOF %0.3f\" % roc_auc_score(y_true=y, y_score=oof.catboost_oof))\nprint(\"Average precision OOF %0.3f\" % average_precision_score(y_true=y, y_score=oof.catboost_oof))","bd49baee":"feature_dict = {'Features': catb.feature_names_, 'Importance': catb.feature_importances_}\nfeature_imp = pd.DataFrame(feature_dict).sort_values(by=['Importance'], ascending=False)\n\nplt.figure(figsize=(10,7))\ndf_imp = feature_imp.head(20)\nsns.barplot(y=df_imp['Features'], x=df_imp['Importance'], palette='light:m_r')","e712fedf":"#catb.save_model('model_one')","68a5bcbd":"# 1. Pearson correlation","af08fdc9":"embeded_rf_feature","0ca46cc6":"rfe_feature","a560717f":"# outlier","e79243eb":"**GPA**","5c10c18c":"# **MODELING**","c1bcb66e":"**person**","296702c7":"cor_feature","2ba88fef":"# **Scaling**","869d81a8":"# **SUBMISSION**","9640ed62":"# 4. Lasso: SelectFromModel","a866aa47":"# 5. Tree-based: SelectFromModel","2e98f370":"**branch**","8641658c":"sample_submission","5576453d":"# Features","cdfe9fd6":"**job**","c8ca017c":"based on this we should detect outliers from [GPA,\t\nLast_achievement_%, job_duration_in_current_branch,\njob_duration_in_current_person_level,\njob_duration_in_current_job_level]","4020bcc0":"embeded_lr_feature","e8885e78":"chi_feature","e12f1789":"# 3. Recursive Feature Elimination","9953b21a":"## Reading the Data and Preprocessing","00b4927b":"**last**","68ba76a7":"embeded_lgb_feature","f03d9642":"# 2. Chi-Square Features"}}