{"cell_type":{"9c362e4b":"code","04b0f7c6":"code","b5b7a0e1":"code","8fada19f":"code","6e54cf58":"code","46ceb594":"code","fc8249d2":"code","eb5371a7":"markdown"},"source":{"9c362e4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib.pyplot import plot\nfrom sklearn.preprocessing import MinMaxScaler\nimport datetime\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","04b0f7c6":"block= 'block_58'\noriginal_data = pd.read_csv('\/kaggle\/input\/daily_dataset\/daily_dataset\/{}.csv'.format(block))\noriginal_data.describe()","b5b7a0e1":"max_data = original_data.loc[original_data.groupby('LCLid').pipe(lambda group: group.energy_max.idxmax(skipna=True))][['LCLid','day','energy_max']]\nmin_data = original_data.loc[original_data.groupby('LCLid').pipe(lambda group: group.energy_max.idxmin(skipna=True))][['LCLid','day','energy_min']]\n# original_data.describe()\n\ndata = original_data.groupby('LCLid').agg({'energy_median': ['mean'], 'energy_mean': ['mean'], 'energy_sum': ['sum']})\ndata = data.merge(max_data, left_on='LCLid', right_on='LCLid',suffixes=('_left', '_max'))\ndata = data.merge(min_data, left_on='LCLid', right_on='LCLid',suffixes=('_max', '_min'))\ndata['day_max'] = pd.to_datetime(data['day_max']).dt.dayofweek\ndata['day_min'] = pd.to_datetime(data['day_min']).dt.dayofweek\n\n# data = pd.concat([data,pd.get_dummies(data['day_min'], prefix='day_min')],axis=1).drop(columns=['day_min'])\n# data = pd.concat([data,pd.get_dummies(data['day_max'], prefix='day_max')],axis=1).drop(columns=['day_max'])\ndata['min_max_ratio'] = data.pipe(lambda group: group.energy_min\/ group.energy_max)\n# print(original_data[(original_data['LCLid']=='MAC000094')])\n\n\ndata.head()","8fada19f":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin_min\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nX = scaler.fit_transform(data.drop(columns=['LCLid']))\n\nNc = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in Nc]\nscore = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n\nplt.plot(Nc,score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","6e54cf58":"kmeans = KMeans(n_clusters=4).fit(X)\ncentroids = kmeans.cluster_centers_\nprint(centroids)","46ceb594":"clustered_data = pd.concat([data,pd.DataFrame(kmeans.predict(X))],axis=1).sort_values(by=[0])\nclustered_data = clustered_data.rename(columns={0: 'Cluster'})\nclustered_data.head()","fc8249d2":"hourly_data = pd.read_csv('\/kaggle\/input\/hhblock_dataset\/hhblock_dataset\/{}.csv'.format(block))\n\ndef create_plot(LCLid, start_date=None, end_date=None, ax=None, title=None):\n    plot_data = hourly_data[(hourly_data['LCLid']==LCLid)].set_index('day').drop(columns=['LCLid'])\n    plot_data = plot_data.stack().reset_index().rename(columns={0: LCLid,'level_1': 'time','day':'date'})\n    plot_data['time'] = plot_data['time'].apply(lambda x: x.replace('hh_',''))\n    plot_data['datetime'] = plot_data[['date','time']].apply(lambda x: pd.to_datetime(x['date'])+datetime.timedelta(minutes=int(x['time'])*30),axis=1)\n    plot_data = plot_data.drop(columns=['date','time'])\n    \n    if start_date:\n        plot_data = plot_data[(plot_data['datetime'] >= pd.to_datetime(start_date))] \n    if end_date:\n        plot_data = plot_data[(plot_data['datetime'] <= pd.to_datetime(end_date))]  \n        \n    if len(plot_data.index>0):\n        plot_data = plot_data.set_index('datetime')\n        plot = plot_data.plot(ax=ax, title=title, figsize = (20,6))\n        print(\"Processing {LCLid}, {title}\".format(**{'LCLid':LCLid, 'title':title}))\n        return plot\n    \n    print(\"[WARNING] LCLid: {LCLid} has no data between dates: {start_date}-> {end_date}\".format(**{'LCLid': LCLid, 'start_date': start_date, 'end_date': end_date}))\n    return None\n    \n\n# hourly_data.head()\nax = None\ncluster = 0\nstart_date='2014-02-01'\nend_date='2014-02-07'\ntitle = \"Cluster: {}\".format(str(cluster))\nfor index, consumer_row in clustered_data[['LCLid','Cluster']].iterrows():\n    plot_new_cluster = cluster != consumer_row['Cluster']\n    consumer_id, cluster = consumer_row['LCLid'], consumer_row['Cluster']\n    aux = create_plot(consumer_id,start_date=start_date,end_date=end_date,ax=ax,title=title)\n    if aux:\n        ax = aux\n    if plot_new_cluster:\n        ax = None\n        title = \"Cluster: {}\".format(str(cluster))\n","eb5371a7":"The first step is to see the data we are processing. For that we can use the method describe that can summarize the dataset's info:"}}