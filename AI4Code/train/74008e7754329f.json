{"cell_type":{"f20ee388":"code","011f41a9":"code","3dd0398f":"code","bc7cf22d":"code","b6adcff3":"code","a7f20cea":"code","bea74819":"code","bebfd855":"code","093ee02c":"code","3f31bf31":"code","181fdf90":"code","cbca08ec":"code","b4cf8415":"code","661dd568":"code","6965560c":"code","ece98bd5":"code","b6fe125f":"code","f4159253":"code","7f4b8b70":"code","d941d71b":"code","4706c5a1":"code","00e3a36c":"code","90f8e693":"code","fb218ed7":"code","07e96640":"code","a748eea8":"code","faae6a03":"code","04a8ac29":"code","fac6ee9a":"code","85d3d47d":"code","2dc1527c":"code","7a0ec6d0":"code","1ed70647":"code","cc505e31":"code","74b37c1f":"code","67ca46f7":"code","2fb76a15":"code","15241c48":"code","ae2cba07":"code","1467ffa2":"code","ef7e21d4":"code","57079818":"code","021bfeec":"code","d998608a":"markdown","73c2c625":"markdown","27e09734":"markdown","8e555379":"markdown","f0430b4b":"markdown","ebee5556":"markdown","e4c0707c":"markdown","ff860637":"markdown","60d5fb68":"markdown","fb9fa53c":"markdown","d45b6616":"markdown","726ea62f":"markdown","362b7c4d":"markdown","90b3e03d":"markdown","2977d841":"markdown"},"source":{"f20ee388":"import pandas as pd\nimport re\nimport string\nfrom transformers import BertModel, BertTokenizer\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\nimport random\n\nfrom tqdm import tqdm\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\nstop=set(stopwords.words('english'))","011f41a9":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","3dd0398f":"df=pd.concat([tweet,test])","bc7cf22d":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","b6adcff3":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","a7f20cea":"df['text']=df['text'].apply(lambda x : remove_URL(x))","bea74819":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","bebfd855":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","093ee02c":"df['text']=df['text'].apply(lambda x : remove_html(x))","3f31bf31":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","181fdf90":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","cbca08ec":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","b4cf8415":"df['text']=df['text'].apply(lambda x : remove_punct(x))","661dd568":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","6965560c":"corpus=create_corpus(df)","ece98bd5":"for i in range(0, len(corpus)):\n    \n    corpus[i] = \" \".join(corpus[i])","b6fe125f":"df[\"text\"] = corpus","f4159253":"train_df = df[0:7613]\ntest_df = df[7613:]","7f4b8b70":"model = BertModel.from_pretrained('bert-base-uncased')","d941d71b":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","4706c5a1":"# sentence padding\ndef padding_zero(vec, max_length):\n    if len(vec) < max_length:\n        vec = np.pad(vec, ( (0, max_length-len(vec)), (0,0) ), 'constant')\n    elif len(vec) > max_length:\n        vec = vec[:max_length]\n\n    return vec","00e3a36c":"text_data = list(train_df[\"text\"].values)","90f8e693":"train_lst = list()\n\nfor i in tqdm(range(0, len(text_data))):\n    input_ids = torch.tensor(tokenizer.encode(text_data[i], add_special_tokens=True, max_length=100)).unsqueeze(0)  # Batch size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[0][0]\n    bert_arr = padding_zero(last_hidden_states.detach().numpy(), 10)\n    train_lst += [bert_arr]","fb218ed7":"train_arr = np.array(train_lst)","07e96640":"target_lst= list(train_df[\"target\"].values)","a748eea8":"len(train_arr), len(target_lst)","faae6a03":"class MyDataSet(torch.utils.data.Dataset):\n    def __init__(self, data, target):\n        \n        self.data = data\n        self.target = target\n        self.length = len(self.data)\n        \n    def __len__(self):\n        \n        return self.length\n    \n    def __getitem__(self, index): \n        \n        ret = self.data[index]\n        target = self.target[index]\n    \n        return ret, target","04a8ac29":"trainset = MyDataSet(train_arr, target_lst)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)","fac6ee9a":"for data in trainloader:\n    print(data[0].shape, data[1].shape)\n    break","85d3d47d":"# Model\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        \n        encoder = nn.TransformerEncoderLayer(768, 32, dim_feedforward=512) # \u6b21\u5143\u6570, Attention\u306e\u6570, # feed_forward\n        self.encoder = nn.TransformerEncoder(encoder, 4)\n        \n        self.pool = nn.MaxPool2d((128, 1))\n        self.linear = nn.Linear(7680, 2)\n    \n    def forward(self, s):\n        b = s.shape[0]\n        \n        s = s.permute(1, 0, 2)\n        s = self.encoder(s)\n        s = s.permute(1, 0, 2)\n#         s = self.pool(s)\n        s = s.reshape(b, -1)\n        s = self.linear(s)\n        s = torch.log_softmax(s, dim=1)\n        \n        return s","2dc1527c":"transModel = Model()","7a0ec6d0":"criterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(transModel.parameters(), lr=0.0001)","1ed70647":"for data in trainloader:\n    res = transModel(data[0])\n    print(res.shape)\n    break","cc505e31":"for epoch in tqdm(range(10)):\n    print(epoch)\n    \n    train_loss = 0    \n    model.train()\n    \n    for data in trainloader:\n        \n        x, t = data[0], data[1].long()\n        \n        out = transModel(x)\n        \n        loss = criterion(out, t)\n        train_loss += loss.item()\n    \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f\"{train_loss\/len(trainloader)}\")","74b37c1f":"text_data = list(test_df[\"text\"].values)","67ca46f7":"test_lst = list()\n\nfor i in tqdm(range(0, len(text_data))):\n    input_ids = torch.tensor(tokenizer.encode(text_data[i], add_special_tokens=True, max_length=100)).unsqueeze(0)  # Batch size 1\n    outputs = model(input_ids)\n    last_hidden_states = outputs[0][0]\n    bert_arr = padding_zero(last_hidden_states.detach().numpy(), 10)\n    test_lst += [bert_arr]","2fb76a15":"test_arr = np.array(test_lst)","15241c48":"transModel.eval()","ae2cba07":"res_lst = list()\nfor i in tqdm(range(0, len(test_arr))):\n    res_lst.append(np.exp(transModel(torch.FloatTensor(test_arr[i]).unsqueeze(-1).permute(2, 0, 1)).detach().numpy())[0])","1467ffa2":"fin_res_lst = list()\nfor i in range(0, len(res_lst)):\n    \n    if res_lst[i][0] > res_lst[i][1]:\n        fin_res_lst.append(0)\n    else:\n        fin_res_lst.append(1)","ef7e21d4":"sample_df = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","57079818":"sample_df[\"target\"] = fin_res_lst","021bfeec":"sample_df.to_csv('submission.csv',index=False)","d998608a":"# Make Test Data","73c2c625":"# Removing punctuations","27e09734":"# Model","8e555379":"# Train","f0430b4b":"# Bert and Padding","ebee5556":"# What's This Kernel\n* Data Cleaning<br>\n* Transformer Model<br>","e4c0707c":"# Removing emoji tags","ff860637":"# Make train data","60d5fb68":"# Begin Tranning","fb9fa53c":"# Removing HTML Tags","d45b6616":"# import Library","726ea62f":"# Removing Stopwords","362b7c4d":"# Loading the data and getting basic idea","90b3e03d":"# removing urls","2977d841":"# Reference\n\nThis kernel includes codes and ideas from kernels below.<br>\n* [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove) by [@Shahules786](https:\/\/www.kaggle.com\/shahules)"}}