{"cell_type":{"882c0f6a":"code","750ed9e5":"code","1027ebc1":"code","52990cf4":"code","84972126":"code","960b7d4d":"code","ade5f208":"code","b5df74b2":"code","a9602741":"code","3bd2d0c8":"code","6080d79e":"code","6ac81814":"code","b1287c4b":"code","2737a41d":"code","e512766e":"code","c0755e00":"code","d86fecf5":"code","70129f5a":"code","6e801e46":"code","a57ade58":"code","f201f4bc":"code","175e26aa":"code","56bff7a0":"code","8c48f85a":"markdown","f713b662":"markdown","7125786b":"markdown","d2e790cf":"markdown","87f25cca":"markdown","1a51507c":"markdown","6e465d26":"markdown","553b366a":"markdown","2fcb22b1":"markdown","b256c236":"markdown","7b384e71":"markdown","a9c7e005":"markdown"},"source":{"882c0f6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Imports needed for the Notebook\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","750ed9e5":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Taking overview of the train dataset\ntrain.head()","1027ebc1":"# Making a copy of the train dataset\noriginal_train = train.copy()\n\n# Store our test passenger IDs for easy access\nPassengerId = test['PassengerId']","52990cf4":"# Combining both the datasets\ncomb_data = [train, test]\ncomb_data_df = pd.concat([train, test])\ncomb_data_df.head()","84972126":"# Finding collective missing values in the data\ncomb_data_df.info()","960b7d4d":"# Feature that tells whether a passenger had a cabin on the Titanic or not (Putting 0 if NaN else 1)\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Adding new feature FamilySize as a combination of SibSp and Parch\nfor data in comb_data:\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n\n# Adding new feature IsAlone from FamilySize\nfor data in comb_data:\n    data['IsAlone'] = 0\n    data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n\n# Remove all NULLS in the Embarked column\nfor data in comb_data:\n    data['Embarked'] = data['Embarked'].fillna('S')\n    \n# Remove all NULLS in the Fare column\nfor data in comb_data:\n    data['Fare'] = data['Fare'].fillna(train['Fare'].median())\n\n# Remove all NULLS in the Age column\nfor dataset in comb_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    # Next line has been improved to avoid warning\n    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in comb_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in comb_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in comb_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] ;","ade5f208":"# Feature selection: remove variables no longer containing relevant information\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","b5df74b2":"train.head()","a9602741":"colormap = plt.cm.viridis\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","3bd2d0c8":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n# Since \"Survived\" is a binary class (0 or 1), these metrics grouped by the Title feature represent:\n    # MEAN: survival rate\n    # COUNT: total observations\n    # SUM: people survived\n\n# title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} ","6080d79e":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).agg(['mean', 'count', 'sum'])\n# Since Survived is a binary feature, this metrics grouped by the Sex feature represent:\n    # MEAN: survival rate\n    # COUNT: total observations\n    # SUM: people survived\n    \n# sex_mapping = {{'female': 0, 'male': 1}} ","6ac81814":"# Let's use our 'original_train' dataframe to check the sex distribution for each title.\n# We use copy() again to prevent modifications in out original_train dataset\ntitle_and_sex = original_train.copy()[['Name', 'Sex']]\n\n# Create 'Title' feature\ntitle_and_sex['Title'] = title_and_sex['Name'].apply(get_title)\n\n# Map 'Sex' as binary feature\ntitle_and_sex['Sex'] = title_and_sex['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\n# Table with 'Sex' distribution grouped by 'Title'\ntitle_and_sex[['Title', 'Sex']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n\n# Since Sex is a binary feature, this metrics grouped by the Title feature represent:\n    # MEAN: percentage of men\n    # COUNT: total observations\n    # SUM: number of men","b1287c4b":"# Define function to calculate Gini Impurity\ndef get_gini_impurity(survived_count, total_count):\n    survival_prob = survived_count\/total_count\n    not_survival_prob = (1 - survival_prob)\n    random_observation_survived_prob = survival_prob\n    random_observation_not_survived_prob = (1 - random_observation_survived_prob)\n    mislabelling_survided_prob = not_survival_prob * random_observation_survived_prob\n    mislabelling_not_survided_prob = survival_prob * random_observation_not_survived_prob\n    gini_impurity = mislabelling_survided_prob + mislabelling_not_survided_prob\n    return gini_impurity","2737a41d":"# Gini Impurity of starting node\ngini_impurity_starting_node = get_gini_impurity(342, 891)\ngini_impurity_starting_node","e512766e":"# Gini Impurity decrease of node for 'male' observations\ngini_impurity_men = get_gini_impurity(109, 577)\ngini_impurity_men","c0755e00":"# Gini Impurity decrease if node splited for 'female' observations\ngini_impurity_women = get_gini_impurity(233, 314)\ngini_impurity_women","d86fecf5":"# Gini Impurity decrease if node splited by Sex\nmen_weight = 577\/891\nwomen_weight = 314\/891\nweighted_gini_impurity_sex_split = (gini_impurity_men * men_weight) + (gini_impurity_women * women_weight)\n\nsex_gini_decrease = weighted_gini_impurity_sex_split - gini_impurity_starting_node\nsex_gini_decrease","70129f5a":"# Gini Impurity decrease of node for observations with Title == 1 == Mr\ngini_impurity_title_1 = get_gini_impurity(81, 517)\ngini_impurity_title_1","6e801e46":"# Gini Impurity decrease if node splited for observations with Title != 1 != Mr\ngini_impurity_title_others = get_gini_impurity(261, 374)\ngini_impurity_title_others","a57ade58":"# Gini Impurity decrease if node splited for observations with Title == 1 == Mr\ntitle_1_weight = 517\/891\ntitle_others_weight = 374\/891\nweighted_gini_impurity_title_split = (gini_impurity_title_1 * title_1_weight) + (gini_impurity_title_others * title_others_weight)\n\ntitle_gini_decrease = weighted_gini_impurity_title_split - gini_impurity_starting_node\ntitle_gini_decrease","f201f4bc":"cv = KFold(n_splits=10)            # Desired number of Cross Validation folds\naccuracies = list()\nmax_attributes = len(list(test))\ndepth_range = range(1, max_attributes + 1)\n\n# Testing max_depths from 1 to max attributes\n# Uncomment prints for details about each Cross Validation pass\nfor depth in depth_range:\n    fold_accuracy = []\n    tree_model = tree.DecisionTreeClassifier(max_depth = depth)\n    # print(\"Current max depth: \", depth, \"\\n\")\n    for train_fold, valid_fold in cv.split(train):\n        f_train = train.loc[train_fold] # Extract train data with cv indices\n        f_valid = train.loc[valid_fold] # Extract valid data with cv indices\n\n        model = tree_model.fit(X = f_train.drop(['Survived'], axis=1), \n                               y = f_train[\"Survived\"]) # We fit the model with the fold train data\n        valid_acc = model.score(X = f_valid.drop(['Survived'], axis=1), \n                                y = f_valid[\"Survived\"])# We calculate accuracy with the fold validation data\n        fold_accuracy.append(valid_acc)\n\n    avg = sum(fold_accuracy)\/len(fold_accuracy)\n    accuracies.append(avg)\n    # print(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\n    # print(\"Average accuracy: \", avg)\n    # print(\"\\n\")\n    \n# Just to show results conveniently\ndf = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\ndf = df[[\"Max Depth\", \"Average Accuracy\"]]\nprint(df.to_string(index=False))","175e26aa":"# Create Numpy arrays of train, test and target (Survived) dataframes to feed into our models\ny_train = train['Survived']\nx_train = train.drop(['Survived'], axis=1).values \nx_test = test.values\n\n# Create Decision Tree with max_depth = 3\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 3)\ndecision_tree.fit(x_train, y_train)\n\n# Predicting results for test dataset\ny_pred = decision_tree.predict(x_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)\n\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(decision_tree,\n                              out_file=f,\n                              max_depth = 3,\n                              impurity = True,\n                              feature_names = list(train.drop(['Survived'], axis=1)),\n                              class_names = ['Died', 'Survived'],\n                              rounded = True,\n                              filled= True )\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\".\/tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")\n\n# Code to check available fonts and respective paths\n# import matplotlib.font_manager\n# matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')","56bff7a0":"acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nacc_decision_tree","8c48f85a":"In this notebook, I will go through the whole process of creating a machine learning model using decision tree on the famous Titanic dataset, which is used by many people all over the world. It provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age and survival.\n\nDecision Trees can also help a lot when we need to understanding the data. Given their transparency and relatively low computational cost, Decision Trees are also very useful for exploring your data before applying other algorithms. They're helpful for checking the quality of engineered features and identifying the most relevant ones by visualising the resulting tree.\n\nThe main downsides of Decision Trees are their tendency to over-fit, their inability to grasp relationships between features, and the use of greedy learning algorithms (not guaranteed to find the global optimal model). Using them in a Random Forest helps mitigate some of this issues.\n\nAfter this short introduction to Decision Trees, let's see how to apply them for the Titanic challenge. \n\nWe would go through following aspects:\n\n* Learning from the data with Decision Trees\n* Dataset exploration and processing\n* Relevant features for Decision Trees\n* Gini Impurity\n* Finding best tree depth with the help of cross-validation\n* Generating and visualising the final model","f713b662":"# Feature Engineering","7125786b":"# Introduction","d2e790cf":"# Final Tree","87f25cca":"Here, we found that Age, Fare, Cabin, Embarked and Survived has missing values.","1a51507c":"### Title VS Sex","6e465d26":"# Finding best tree depth with the help of Cross Validation","553b366a":"# Gini Impurity","2fcb22b1":"# Importing Libraries","b256c236":"### Adding Features derived from already available features","7b384e71":"# Visualising the processed data","a9c7e005":"# Importing\/Loading Data"}}