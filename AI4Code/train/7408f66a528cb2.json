{"cell_type":{"ef72bec7":"code","5548c518":"code","452fb1cd":"code","221cb23e":"code","079459d0":"code","21d47536":"code","31710f0a":"code","34bad2a9":"code","a7f2a912":"code","d68b9c17":"code","e1cb4472":"code","c09632ac":"code","ccfab580":"code","057b5d4a":"code","b9f72fe7":"code","2e7544c5":"code","ebc6155f":"code","ff9b3525":"code","a941446b":"code","574c61ae":"code","799013b7":"code","b7c896d2":"code","cfa0180a":"code","762279b5":"code","f0f83074":"code","cef3c7ad":"code","7f18dc07":"code","c3c7cc5d":"code","26b70606":"code","e8c1752a":"code","369915c1":"code","42e6cc13":"code","92d11547":"code","84950f0b":"code","7110968c":"code","9358179c":"code","666f63e6":"code","23943ba9":"code","4215a19e":"code","29a8c66c":"code","9c4e9887":"code","a269befe":"code","82c6c39d":"code","29040636":"code","b128b95b":"code","c70edac3":"code","2edaf9ff":"code","2fd42df7":"code","a881e398":"code","e1e12639":"code","9ad5845b":"code","9a9f73df":"code","8b062bd7":"code","adf7dce2":"code","604a8b07":"code","b5f65a6e":"code","17b43898":"code","dc38738b":"code","e75f2c7f":"code","3a13c9d6":"code","201a9227":"code","e435de57":"code","629ef8fe":"code","5230e815":"code","66e3c1ab":"code","7d7b41ea":"markdown","8a924a3a":"markdown","93b452b9":"markdown","de9bb787":"markdown","60a445f6":"markdown","a8a87f7d":"markdown","4912ff8c":"markdown","c5de2c5e":"markdown","f1319441":"markdown","f83a9e50":"markdown","b3e71280":"markdown","5322a154":"markdown","d26bcb9b":"markdown","f4fe186e":"markdown","539f277c":"markdown","2c40e173":"markdown","deca2fdf":"markdown","fa9ccdfa":"markdown","67a0e63c":"markdown","3e8331c8":"markdown","7e894604":"markdown","d8099342":"markdown","04468f2e":"markdown","e1e0626e":"markdown","df5955fa":"markdown","c2eebfaf":"markdown","96993a18":"markdown","5b8e036e":"markdown","43309786":"markdown","9064ea90":"markdown","001f5968":"markdown","6ef7d462":"markdown","84d983ff":"markdown","d05269e7":"markdown","ee169271":"markdown","147cb64b":"markdown","30021164":"markdown","a5643696":"markdown","6e884580":"markdown","734afb43":"markdown","78512287":"markdown","6207965c":"markdown","8a9995d3":"markdown","8bf66acf":"markdown","25d5788c":"markdown","0b4b6d49":"markdown","06ed02f9":"markdown","a80d2972":"markdown","57311ce5":"markdown","7e262400":"markdown","51bd0bbe":"markdown","70e5fad5":"markdown","5f316a0c":"markdown","3062eda9":"markdown","da4d4392":"markdown","0026820a":"markdown","5dbfcc7d":"markdown","98183c7a":"markdown","a7f3fb22":"markdown","34c49a92":"markdown","7fb797e7":"markdown","78692a60":"markdown","1e4f3dda":"markdown","f1ae26cf":"markdown"},"source":{"ef72bec7":"# import libraries \n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport scipy.stats as stats\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","5548c518":"# let's start by reading the train and test dataset.\n\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","452fb1cd":"# print out the first five rows of the train datasets\n\ntrain_df.head()","221cb23e":"# print out the first five rows of the test datasets.\n\ntest_df.head()","079459d0":"plt.figure()\notl = sns.lmplot('GrLivArea', 'SalePrice',data=train_df, fit_reg=False);","21d47536":"#train_df[(train_df['SalePrice'] < 300000) & (train_df['GrLivArea'] > 4000)]\ntrain_df[(train_df['GrLivArea'] > 4000)][['SalePrice','GrLivArea']]","31710f0a":"#train_df.drop(train_df[(train_df['SalePrice'] < 300000) & (train_df['GrLivArea'] > 4000)].index,inplace=True)\ntrain_df.drop(train_df[(train_df['GrLivArea'] > 4000)].index,inplace=True)\n\nplt.figure()\nsns.lmplot('GrLivArea', 'SalePrice',data=train_df, fit_reg=False);\nplt.xlim(0,5500);\nplt.ylim(0,800000);","34bad2a9":"# check the dimensions\nprint(train_df.shape)","a7f2a912":"sns.distplot(train_df['SalePrice'])\nplt.title('SalePrice Distribution')\nplt.ylabel('Frequency')\n\nplt.figure()\nqq = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()\n\n# For normally distributed data, the skewness should be about zero. \n# A skenewss  value greater than zero means that there is more weight in the left tail of the distribution\n\nprint(\"Skewness: {:.3f}\".format(train_df['SalePrice'].skew()))","d68b9c17":"# log1p calculates log(1 + input)\n\ntrain_df['SalePrice'] = np.log1p(train_df['SalePrice'])","e1cb4472":"# let's check the result of the transformation\n\nsns.distplot(train_df['SalePrice'])\nplt.title('SalePrice Distribution')\nplt.ylabel('Frequency')\n\nplt.figure()\nqq = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()\n\nprint(\"Skewness: {:.3f}\".format(train_df['SalePrice'].skew()))","c09632ac":"plt.figure(figsize=(15,5))\n\n# correlation table\ncorr_train = train_df.corr()\n\n# select top 10 highly correlated variables with SalePrice\nnum = 10\ncol = corr_train.nlargest(num, 'SalePrice')['SalePrice'].index\ncoeff = np.corrcoef(train_df[col].values.T)\n\n# heatmap\nheatmp = sns.heatmap(coeff, annot = True, xticklabels = col.values, yticklabels = col.values, linewidth=2,cmap='PiYG', linecolor='blue')","ccfab580":"# Visualized the relationship between the target variable and top 10 features highly correlated with the target variable.\n\nsns.pairplot(train_df[col], size=3);","057b5d4a":"# seperate id from datasets and drop them.\n\ntrain_id = train_df.iloc[:,0]\ntest_id = test_df.iloc[:,0]\n\ntrain_df.drop('Id',axis=1,inplace = True)\ntest_df.drop('Id',axis=1,inplace = True)","b9f72fe7":"# seperate the target variable (SalePrice) from the train\n\ny_df = train_df['SalePrice']\ntrain_df.drop('SalePrice',axis=1,inplace=True)\n\nprint('dimension of the train:' , train_df.shape)\nprint('dimension of the test:' , test_df.shape)","2e7544c5":"# In order to avoid repeating unnecessary codes, for our convenience, let's combine the train and test set.\ndf = pd.concat([train_df, test_df]).reset_index()\n\ndf.drop(['index'],axis=1,inplace=True)","ebc6155f":"print('dimension of the dataset:' , df.shape)\ndf.head()","ff9b3525":"mc = pd.DataFrame(df.isnull().sum(),columns=['Missing Count'])\nmc = mc[mc['Missing Count']!=0]\nmc['Missing %'] = (mc['Missing Count'] \/ df.shape[0]) * 100\nmc.sort_values('Missing %',ascending=False)","a941446b":"nones = ['PoolQC', 'MiscFeature', 'Alley','Fence', 'FireplaceQu', 'GarageType','GarageFinish',\n        'GarageQual','GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n        'MasVnrType']\n\nfor none in nones:\n    df[none].fillna('None',inplace = True)\n    ","574c61ae":"zeros = ['GarageYrBlt','GarageArea','GarageCars','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n         'BsmtFullBath','BsmtHalfBath','MasVnrArea']\n\nfor zero in zeros:\n    df[zero].fillna(0, inplace = True)\n","799013b7":"Counter(df.Utilities)","b7c896d2":"df.drop('Utilities',axis=1, inplace=True)","cfa0180a":"freq = ['MSZoning','Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual','Functional']\n\nfor fr in freq:\n    df[fr].fillna(df[fr].mode()[0], inplace=True)","762279b5":"df['old_lotfrontage'] = df['LotFrontage']\n\ndf['LotFrontage'] = df.groupby(['LotArea','Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ndf['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","f0f83074":"fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\nol = sns.distplot(df['old_lotfrontage'].dropna(),ax=ax1,kde=True,bins=70)\nlf = sns.distplot(df['LotFrontage'],ax=ax2,kde=True,bins=70,color='red')\n\n# drop the old_lotfrontage as we finished the comparison\ndf.drop('old_lotfrontage',axis=1,inplace=True)","cef3c7ad":"print(\"Remaining missing values:\",df.isnull().sum().sum())","7f18dc07":"# get_dummies can convert data to 0 and 1 only if the data type is string. Among the many nominal features,\n# MSSubClass, MoSold, and YrSold are integer type so we need to convert them to string type.\n\ndf['MoSold'] = df.astype(str)\ndf['YrSold'] = df.astype(str)\ndf['MSSubClass'] = df.astype(str)\n\nnominals = ['MSSubClass','MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl',\n           'Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating','CentralAir','GarageType','MiscFeature','SaleType','SaleCondition','MoSold','YrSold']","c3c7cc5d":"from sklearn.preprocessing import LabelEncoder\n\nordinals = ['LotShape','LandSlope','OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual',\n           'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','Electrical','KitchenQual',\n            'Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence']\n\nfor ordinal in ordinals:\n    le = LabelEncoder()\n    le.fit(df[ordinal])\n    df[ordinal] = le.transform(df[ordinal])","26b70606":"# Total square feet of houses\n\ndf['totalArea'] = df['GrLivArea'] + df['TotalBsmtSF']","e8c1752a":"# Assign numeric features by excluding non numeric features\nnumeric = df.dtypes[df.dtypes != 'object'].index\n\n# Display the skewness of each column and sort the values in descending order \nskewness = df[numeric].apply(lambda x: x.skew()).sort_values(ascending=False)\n\n# Create a dataframe and show 5 most skewed features \nsk_df = pd.DataFrame(skewness,columns=['skewness'])\nsk_df['skw'] = abs(sk_df)\nsk_df.sort_values('skw',ascending=False).drop('skw',axis=1).head()","369915c1":"# As a general rule of thumb, skewness with an absolute value less than 0.5 is considered as a acceptable range of skewness for normal distribution of data\nskw_feature = skewness[abs(skewness) > 0.5].index\n\n# Transform skewed features to normal distribution by taking log(1 + input)\ndf[skw_feature] = np.log1p(df[skw_feature])\n","42e6cc13":"df = pd.get_dummies(df)\nprint(df.shape)","92d11547":"# Split the combined dataset into two: train and test\n\nX_train = df[:train_df.shape[0]]\nX_test = df[train_df.shape[0]:]\n\n#X_train, X_test, y_train, y_test = train_test_split(df,y_df, random_state = 1)\n","84950f0b":"print(\"training shape:{}, test shape:{}\".format(X_train.shape,X_test.shape))","7110968c":"# Import libraries\n\nfrom sklearn.model_selection import GridSearchCV,learning_curve, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\nfrom sklearn.linear_model import LassoCV,ElasticNetCV,Lasso,ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom mlxtend.regressor import StackingRegressor\nfrom xgboost import XGBRegressor","9358179c":"print(X_train.shape, X_test.shape,y_df.shape)","666f63e6":"scaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\n#X_train = RobustScaler().fit_transform(X_train)\nX_train = pd.DataFrame(X_train, columns = df.columns )\n\nX_test = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test, columns = df.columns)\n#X_test = RobustScaler().fit_transform(X_test)","23943ba9":"y_df.head()","4215a19e":"kfold = KFold(n_splits=20, random_state= 0, shuffle = True)","29a8c66c":"def rmsle_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_df, scoring=\"neg_mean_squared_error\", cv = kfold))\n    return(rmse)\n","9c4e9887":"KR = KernelRidge()\n\nKR_param_grid = {\n    'alpha' : [0.93],\n    'kernel' : ['polynomial'],\n    'gamma':[0.001],\n    'degree': [3],\n    'coef0': [1.5]\n}\n\nKR_CV = GridSearchCV(KR, param_grid = KR_param_grid, cv = kfold, scoring = \"neg_mean_squared_error\",n_jobs = -1, verbose = 1)\nKR_CV.fit(X_train, y_df)\nKR_best = KR_CV.best_estimator_\nprint(KR_best)\n\n# scaler, cv = 20\n# * KernelRidge(alpha=1.0, coef0=0.9, degree=2, gamma=0.004, kernel='polynomial',kernel_params=None)\n# ** KernelRidge(alpha=0.93, coef0=1.5, degree=3, gamma=0.001, kernel='polynomial',kernel_params=None)\n# *** KernelRidge(alpha=0.93, coef0=1.5, degree=3, gamma=0.001, kernel='polynomial',kernel_params=None) - 0.12514\n","a269befe":"y_submission_1 = np.expm1(KR_best.predict(X_test))","82c6c39d":"score = rmsle_cv(KR_best)\nprint(\"Kernel Ridge mean score:\", score.mean())\nprint(\"Kernel Ridge std:\", score.std())","29040636":"lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.003, 0.007, 0.009, 0.01, 0.03, 0.05, 0.07, 0.09, 0.2, 0.4, 0.6, 0.8, 1, 1.2], random_state = 1, n_jobs = -1, verbose = 1)\nlasso.fit(X_train, y_df)\nalpha = lasso.alpha_\nprint(\"Optimized Alpha:\", alpha)\n\nlasso = LassoCV(alphas = alpha * np.linspace(0.5,1.5,20), cv = kfold, random_state = 1, n_jobs = -1)\nlasso.fit(X_train, y_df)\nalpha = lasso.alpha_\nprint(\"Final Alpha:\", alpha)\n\n# scaler cv = 20\n#lasso = LassoCV(alphas = 0.00244736842105, cv = kfold, random_state = 1, n_jobs = -1, verbose = 1)\n#lasso.fit(X_train, y_df)\n\n#Final Alpha: 0.00244736842105","b128b95b":"print(\"Lasso mean score:\", rmsle_cv(lasso).mean())\nprint(\"Lasso std:\", rmsle_cv(lasso).std())","c70edac3":"y_submission_2 = np.expm1(lasso.predict(X_test))","2edaf9ff":"elnet = ElasticNetCV(alphas = [0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.003, 0.007, 0.009, 0.01, 0.03, 0.05, 0.07, 0.09, 0.2, 0.4, 0.6, 0.8, 1, 1.2] \n                ,l1_ratio = [0.1, 0.3, 0.5, 0.7, 0.9, 1]\n                ,cv = kfold, random_state = 1, n_jobs = -1)\nelnet.fit(X_train, y_df)\nalpha = elnet.alpha_\nratio = elnet.l1_ratio_\nprint(\"Optimized Alpha:\", alpha)\nprint(\"Optimized l1_ratio:\", ratio)\n\nelnet = ElasticNetCV(alphas = alpha * np.linspace(0.5,1.5,20), l1_ratio = ratio * np.linspace(0.9,1.3,6), \n                     cv = kfold, random_state = 1, n_jobs = -1)\nelnet.fit(X_train, y_df)\n\nalpha = elnet.alpha_\nratio = elnet.l1_ratio_\n\nprint(\"Final Alpha:\", alpha)\nprint(\"Final l1_ratio:\", ratio)\n\n# scaler cv = 20\n# Final Alpha: 0.0276315789474, Final l1_ratio: 0.09","2fd42df7":"print(\"ElasticNet mean score:\", rmsle_cv(elnet).mean())\nprint(\"ElasticNet std:\", rmsle_cv(elnet).std())","a881e398":"y_submission_3 = np.expm1(elnet.predict(X_test))\n# kaggle_score: 0.12302","e1e12639":"\nepsilons = [0.03]\ndegrees = [2]\ncoef0s = [1.6]\n\ngammas = ['auto']\nCs = [0.1]\nkernels = ['poly']\n\nparam_grid = dict(C=Cs, epsilon = epsilons, gamma=gammas, kernel=kernels, degree= degrees, coef0=coef0s)\nSVMR = GridSearchCV(SVR(), param_grid = param_grid, cv = kfold, scoring = \"neg_mean_squared_error\",n_jobs = -1,verbose = 1)\n\nSVMR.fit(X_train,y_df)\nSVMR_best = SVMR.best_estimator_\nprint(SVMR.best_params_)\n\n# cv = 20 \n\n# * {'kernel': 'poly', 'C': 0.1, 'gamma': 'auto', 'degree': 2, 'epsilon': 0.03, 'coef0': 1.5} - 0.12514\n# ** {'kernel': 'poly', 'C': 0.1, 'gamma': 'auto', 'degree': 2, 'epsilon': 0.03, 'coef0': 1.6} - 0.12428","9ad5845b":"print(\"SVM mean score:\", rmsle_cv(SVMR_best).mean())\nprint(\"SVM std:\", rmsle_cv(SVMR_best).std())","9a9f73df":"y_submission_4 = np.expm1(SVMR.predict(X_test))","8b062bd7":"RFC = RandomForestRegressor(random_state = 1)\n\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [88],\n              \"min_samples_leaf\": [1],\n              \"n_estimators\" :[570]\n                }\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv = kfold, scoring = \"neg_mean_squared_error\", n_jobs = -1, verbose = 1)\ngsRFC.fit(X_train,y_df)\nRFC_best = gsRFC.best_estimator_\nprint(gsRFC.best_params_)\n\n\n# cv = 20 (Scaler)\n# {'max_depth': None, 'min_samples_leaf': 1, 'max_features': 88, 'n_estimators': 600}\n# {'max_depth': None, 'min_samples_leaf': 1, 'max_features': 88, 'n_estimators': 570} - 0.13778","adf7dce2":"print(\"Random Forest mean score:\", rmsle_cv(RFC_best).mean())\nprint(\"Random Forest std:\", rmsle_cv(RFC_best).std())","604a8b07":"y_submission_5 = np.expm1(gsRFC.predict(X_test))","b5f65a6e":"XGB = XGBRegressor()\n\nxg_param_grid = {\n              'n_estimators' :[870],\n              'learning_rate': [0.04],\n              \n              'max_depth': [3],\n              'min_child_weight':[0.2],\n              \n              'gamma': [0],\n                \n              'subsample':[0.8],\n              'colsample_bytree':[0.7]\n    \n              #'reg_alpha':[0.08,0.09,0.095,0.1,0.15,0.2],\n              #'reg_lambda':[0,0.001,0.002]\n              }\n                \ngsXGB = GridSearchCV(XGB,param_grid = xg_param_grid, cv=kfold, scoring=\"neg_mean_squared_error\", n_jobs= -1, verbose = 1)\ngsXGB.fit(X_train,y_df)\nXGB_best = gsXGB.best_estimator_\nprint(gsXGB.best_params_)\n\n# cv = 20\n# {'min_child_weight': 0.5, 'learning_rate': 0.05, 'n_estimators': 850, 'max_depth': 3} - 0.12611\n# {'min_child_weight': 0.2, 'learning_rate': 0.04, 'gamma': 0, 'n_estimators': 870, 'max_depth': 3}\n# * {'max_depth': 3, 'subsample': 0.8, 'learning_rate': 0.04, 'gamma': 0, 'colsample_bytree': 0.7, 'min_child_weight': 0.2, 'n_estimators': 870} - 0.12287\n# {'gamma': 0, 'min_child_weight': 0.1, 'learning_rate': 0.04, 'n_estimators': 885, 'max_depth': 3}\n\n# {'reg_alpha': 0.1, 'reg_lambda': 0.001, 'n_estimators': 870, 'colsample_bytree': 0.7, 'subsample': 0.8, 'min_child_weight': 0.2, 'learning_rate': 0.04, 'gamma': 0, 'max_depth': 3} - 0.12531\n# ","17b43898":"print(\"XG Boost mean score:\", rmsle_cv(XGB_best).mean())\nprint(\"XG Boost std:\", rmsle_cv(XGB_best).std())","dc38738b":"y_submission_6 = np.expm1(gsXGB.predict(X_test))","e75f2c7f":"print(\"source: https:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor\/\")\nImage(url= \"https:\/\/rasbt.github.io\/mlxtend\/user_guide\/regressor\/StackingCVRegressor_files\/stacking_cv_regressor_overview.png\")","3a13c9d6":"XGB = XGBRegressor()\n\nELNET = ElasticNet(random_state = 1)\nLCV=Lasso(random_state = 1)\nSV = SVR()\nKR = KernelRidge()\nXG = XGBRegressor()\nstack = StackingRegressor(regressors = [ELNET,LCV,XG],meta_regressor = XGB)\n\nparams = {       \n              'meta-xgbregressor__n_estimators' : [740*2],#740\n              'meta-xgbregressor__learning_rate': [0.01\/2], #0.01\n              'meta-xgbregressor__min_child_weight':[0],\n              'meta-xgbregressor__gamma':[0.1],\n              'meta-xgbregressor__max_depth': [2],\n              'meta-xgbregressor__subsample':[0.65],\n              'meta-xgbregressor__colsample_bytree':[0.4],\n              'meta-xgbregressor__reg_alpha':[0],\n              'meta-xgbregressor__reg_lambda':[1],\n              \n              'lasso__alpha':[0.00244736842105],\n              'elasticnet__alpha':[0.0276315789474],\n              'elasticnet__l1_ratio':[0.09],\n              'xgbregressor__min_child_weight':[0.2],\n              'xgbregressor__n_estimators' : [870],\n              'xgbregressor__learning_rate': [0.04],\n              'xgbregressor__gamma':[0],\n              'xgbregressor__max_depth': [3],\n              'xgbregressor__subsample':[0.8],\n              'xgbregressor__colsample_bytree':[0.7]\n    \n              #'kernelridge__alpha':[0.93],\n              #'kernelridge__coef0':[1.5],\n              #'kernelridge__degree':[3],\n              #'kernelridge__gamma':[0.001],\n              #'kernelridge__kernel':['polynomial'],\n              #'kernelridge__kernel_params':[None],\n              \n              #'svr__coef0':[1.6],\n              #'svr__kernel':['poly'],\n              #'svr__epsilon':[0.03],\n              #'svr__gamma': ['auto'],\n              #'svr__degree': [2],\n              #'svr__C':[0.1]\n        }\n\ngrid = GridSearchCV(estimator = stack, param_grid=params,cv=kfold,refit=True, verbose=1,n_jobs=1,scoring=\"neg_mean_squared_error\")\ngrid.fit(X_train, y_df)\ngrid_best = grid.best_estimator_\nprint(grid_best)\n\n#StackingRegressor(meta_regressor=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0,\n#learning_rate=0.01, max_delta_step=0, max_depth=3,\n#min_child_weight=0.5, missing=None, n_estimators=770, nthread=-1,\n#objective='reg:linear', reg_alpha=0, reg_lambda=1,\n#scale_pos_weight=1, seed=0, silent=True, subsample=1) - 0.12965\n\n# StackingRegressor(meta_regressor=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n# gamma=0, learning_rate=0.02, max_delta_step=0, max_depth=1,\n# min_child_weight=0.3, missing=None, n_estimators=760, nthread=-1,\n# objective='reg:linear', reg_alpha=0, reg_lambda=1,\n# scale_pos_weight=1, seed=0, silent=True, subsample=0.3) - 0.12546\n\n#StackingRegressor(meta_regressor=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.5,\n#gamma=0, learning_rate=0.02, max_delta_step=0, max_depth=1,\n#min_child_weight=0.2, missing=None, n_estimators=760, nthread=-1,\n#objective='reg:linear', reg_alpha=0, reg_lambda=1,\n#scale_pos_weight=1, seed=0, silent=True, subsample=0.2) - 0.12493\n\n#StackingRegressor(meta_regressor=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.4,\n#gamma=0.1, learning_rate=0.01, max_delta_step=0, max_depth=2,\n#min_child_weight=0, missing=None, n_estimators=740, nthread=-1,\n#objective='reg:linear', reg_alpha=0, reg_lambda=1,\n#scale_pos_weight=1, seed=0, silent=True, subsample=0.65) - 0.12027\n\n#StackingRegressor(meta_regressor=XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.4,\n#gamma=0.1, learning_rate=0.005, max_delta_step=0, max_depth=2,\n#min_child_weight=0, missing=None, n_estimators=1480, nthread=-1,\n#objective='reg:linear', reg_alpha=0, reg_lambda=1,\n#scale_pos_weight=1, seed=0, silent=True, subsample=0.65) - 0.12026","201a9227":"print(\"Stacking mean score:\", rmsle_cv(grid_best).mean())\nprint(\"Stacking std:\", rmsle_cv(grid_best).std())","e435de57":"y_submission_st = np.expm1(grid.predict(X_test))","629ef8fe":"y_submission_avg = (y_submission_6 + y_submission_2 + y_submission_st)\/3\n\n# W: 0.11960 (y_submission_6 + y_submission_2)\/2\n# WW: 0.11948 (y_submission_6 + y_submission_2 + y_submission_st)\/3","5230e815":"#y_submission_weight = (y_submission_st *0.3340) + (y_submission_2 * 0.3331) + (y_submission_6 *0.3329) - 11.952\n# y_submission_weight = (y_submission_st *0.334) + (y_submission_2 * 0.3331) + (y_submission_6 *0.3329) - 11.952","66e3c1ab":"my_submission = pd.DataFrame()\nmy_submission['Id'] = test_id\nmy_submission['SalePrice'] = y_submission_avg\nmy_submission.to_csv('submission47.csv',index=False)","7d7b41ea":"#### 4.1.2 Imputing Missing Values\n<a id='imputing_missing'><\/a>","8a924a3a":"### 5.3 Ensemble - Stacked Regression and GridSearch <a id='stacked_regression_and_gridsearch'><\/a>\n\nStacking is an ensemble learning technique to combine multiple regression models via a meta-regressor. In our case, we will use XG Boost as a meta-regressor and use the predictions of Lasso Regression, Elasticnet and XG Boost as trainig set of stacking. The reason why I chose the two models is that these are performing the best among many algorithms. Just be aware that it is not always that combination of only the best models performs the best. Selection of which models to choose for stacking is more like art rather science. Sometimes, some models that perform not well as a single model may work well on stacking so always do some experiments with many combinations. I tried many combinations but in my case, the best single models perform the best on stacking as well. One tip for the experiment that I have just mentioned is that it usually performs better as the models will be used for stacking have different characteristics or mechanisms. Below illustration is showing how stacking works.","93b452b9":"#### 3.2.1 Distribution\n<a id='t_distribution'><\/a>","de9bb787":"#### 3.2.2 Target Variable Transformation\n<a id='t_transformation'><\/a>","60a445f6":"#### 3.1.2 Removal of the Outliers\n<a id='r_outliers'><\/a>","a8a87f7d":"As you can see the above, there are only two categories in the Utilities feature: 'AllPub' and 'NoSeWa'. Moreover, except 1 'NoSeWa and 2 NaN values, all the values are 'AllPub' ,which means the data is very imbalanced and this feature seems not that helpful in predictive modeling. I decide to remove this feature.","4912ff8c":"The best result comes from the combination of Lasso Regression, ElasticNet and XG Boost, which scores 0.12026 and best score so far.","c5de2c5e":"## 2. Load and Check Data\n<a id='load_and_check_data'><\/a>","f1319441":"### 3.5 Independent Variables (Id, SalePrice) Seperation\n<a id='seperation'><\/a>","f83a9e50":"#### Random Forest","b3e71280":"#### Groupby\n\nSince there are a lot of missing values in the LotFrontage, simply filling in missing values with median or mode may affect badly our models. We can fill missing values in LotFrontage with the median LotFrontage of similar rows according to LotArea and Neighborhood.","5322a154":"#### Lasso Regression","d26bcb9b":"As we can see the first column of the plots, it is not perfectly linear but we can say some of them are showing some positive linear pattern.","f4fe186e":"The data documentation says missing values in the above features (the elements in the list \"nones\") mean these properties do not have one of them: garage, basment, fireplace, alley access, pool, misc features, fence, or masonry veneer. ","539f277c":"### 4.1 Missing Values\n<a id='missing_values'><\/a>\n\nHandling missing data is important as many machine learning algorithms do not support data with missing values.","2c40e173":"# House Prices: Advanced Regression Techniques","deca2fdf":"### 3.6 Concatenation of train and test datasets\n<a id='concat'><\/a>","fa9ccdfa":"#### None","67a0e63c":"#### 3.1.1 Outliers Detection\n<a id='outliers_detection'><\/a>","3e8331c8":"## 3. Data Exploration & Preprocessing\n<a id='data_exp_pro'><\/a>","7e894604":"## Table of Contents","d8099342":"### 3.3 Correlation Check\n<a id='correlation'><\/a>","04468f2e":"### 3.1 Outliers\n<a id='outliers'><\/a>\n\nAs [the data document](http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt) mentioned, it is recommended removing any houses with more than 4000 square feet from the data set.","e1e0626e":"1. [Introduction](#intro)<br><br>\n2. [Load and Check Data](#load_and_check_data)<br><br>\n3. [Data Exploration and Preprocessing](#data_exp_pro)<br>- [3.1 Outliers](#outliers) <br>- [3.1.1 Outliers Detection](#outliers_detection)<br>- [3.1.2 Removal of Outliers](#r_outliers)<br>- [3.2 Target Variable](#target_variable)<br>- [3.2.1 Distribution](#t_distribution)<br>- [3.2.2 Transformation](#t_transformation)<br>- [3.3 Correlation Check](#correlation)<br>- [3.4 Pairplot](#pairplot)<br>- [3.5 Seperation](#seperation)<br>- [3.6 Concatenation of train and test datasets](#concat)<br><br>\n4. [Feature Engineering](#feature_engineering)<br>- [4.1 Missing Values](#missing_values)<br>- [4.1.1 Check For Missing Values](#check_missing)<br>- [4.1.2 Imputing Missing Values](#imputing_missing)<br>- [4.2 Transformation & Encoding](#transformation_encoding)<br>- [4.2.1 Data Type & LabelEncoder](#data_type)<br>- [4.2.2 New Feature](#new_feature)<br>- [4.2.3 Skewed Numeric Features](#numeric_features)<br>- [4.2.4 Log1p Transformation](#log1p)<br>- [4.2.5 Dummy Variables](#dummy)<br>- [4.2.6 Train Test Split](split)\n<br><br>\n5. [Modeling](#modeling)<br>- [5.1 Feature Scaling](#feature_scaling)<br>- [5.2 Simple Modeling](#simple_modeling)<br>- [5.3 Stacked Regression and GridSearch](#stacked_regression_and_gridsearch)<br> - [5.4 Ensemble - Averaging](#avg)\n<br><br>\n6. [Submission](#submission)<br><br>","df5955fa":"#### Support Vector Machine","c2eebfaf":"#### 4.1.1 Check For Missing Values\n<a id='check_missing'><\/a>\n","96993a18":"Some features are explicitly described that the missing values mean zero or we can assume that these properties have zero basement or garage or masonry veneer.","5b8e036e":"The left blue plot is the distribution of the 'LotFrontage' from original dataset after simply omitting missing values and the right red plot is the distribution of the 'LotFrontage' after filling missing values with the median ages of similar rows. As you can see, the distributions are very similar each other.","43309786":"As you can see the distribution plot and qq plot, the target variable is skewed to the right. In order to use many general linear models, we need to transform it to normal.","9064ea90":"There could be many ways to handle outliers; however, I decide to follow the data docs author's recommendation (removing any houses with more than 4000 square feet from the data set). ","001f5968":"#### Mode\n\nUnlike Utilities feature, the features below are not extremly imbalanced and consist of many categories. Also, there are only a few missing values in each feature. Therefore, we can fill missing values with most frequently occurred values.","6ef7d462":"#### XG Boost","84d983ff":"You can clearly see two outliers at the bottom right. Compared to the size of the house, they are extremely cheap.","d05269e7":"The goal of this project is to predict the house sales prices in Ames, Iowa. The first part consists of various visualization and normalization. By visualizing the distribution of features, we can gain some insight how to handle the data and determine the direction of the project. Also the distribution of target variable is transformed to normal by taking logarithm function so that they can fit many linear models better and lead better performance. \n\nThe second part is feature engineering. It comprises filling missing values, encoding and transformation of skewed features. Reading the original document written by the dataset creater is very important for the step for filling missing values since there are some useful directions how to deal with missing values. Specifically, it will help you when to fill the missing values with zero instead of None or mode (the most frequent value). The next thing to do is encoding. Dataset must be transformed to appropriate format that a machine learning model can handle. By using LabelEncoder and get_dummies, we can convert them into a numerical representation that we can apply our machine learning algorithms to. Once we finish the encoding process, some highly skewed features need to be transformed to normal as the target variable was transformed.\n\nLastly, in modeling part, there are two things to notice: stacking and ensemble (averaging). Stacking boosts accuracy of our results predicted by many single models; however, we can boost the accuracy even more by averaging the prediction of stacking and predictions of best models. At the end, I managed to score 0.11948 which is ranked top 17 percent in the competition.","ee169271":"### 4.2 Transformation & Encoding\n<a id='transformation_encoding'><\/a>","147cb64b":"The final result is 0.11948 which is placed in top 17% in the competition.","30021164":"Based on the correlation table shown, we can conjecture that the features related with __quality__ (OverallQual,FullBath, YearBuilt, YearRemodAdd) and the __size__ (GrLivArea, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF) may play an important role in prediction.","a5643696":"#### 4.2.3 Skewed Numeric Features\n<a id='numeric_features'><\/a>","6e884580":"#### Score","734afb43":"## 1. Introduction\n<a id='intro'><\/a>","78512287":"#### 4.2.5 Dummy Variable\n<a id='dummy'><\/a>\n\nA dummy variable is one that takes the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome. Since 'get_dummies' function does not affect numerical values, only nominal data type values will be converted to 0 or 1 (as we already converted ordinal types values to numeric by LabelEncoder, we do not have to worry about that). ","6207965c":"#### 4.2.2 New Feature\n<a id='new_feature'><\/a>\n\nAs we have seen the correlation table, area features such as GrLivArea or BsmtSF are highly correlated with house sales price. Creating new feature regarding the total area may help predict the target variable.","8a9995d3":"We will be evaluating below algorithms' root-mean-squared-error for train dataset. \n\n> - Ridge Regression\n> - Lasso Regression\n> - ElasticNet Regression\n> - Support Vector Machine\n> - Random Forest\n> - XG Boost","8bf66acf":"#### 4.2.6 Train Test Split\n<a id='split'><\/a>\n\nSince data cleaning and feature engineering process are finished, we need to split the combined dataset into train and test as given dataset","25d5788c":"### 5.4 Ensemble - Averaging \n<a id='avg'><\/a>\n\nThe result of stacking outperforms all the single models'. One more time, combining the stacking result with best single models would be able to boost our accuracy even more; however, this time, instead of stacking, we can use different type of ensemble, averaging. Simply, averaging is adding all the results predicted by each model and dividing by the number of models. First time, I expected weighted averaging would make a better prediction since the stacking result outperforms other models; however, on the contrary to my expectation, just simple averaing method performs the best.","0b4b6d49":"#### Kernel Ridge","06ed02f9":"## 4. Feature Engineering\n<a id='feature_engineering'><\/a>","a80d2972":"#### 4.2.1 Data Type & LabelEncoder\n<a id='data_type'><\/a>","57311ce5":"### 3.2 Target Variable\n<a id='target_variable'><\/a>","7e262400":"### 5.2 Simple Modeling\n<a id='simple_modeling'><\/a>","51bd0bbe":"#### Nominal\n\nMachine learning algorithms will require that nominal variables be converted into dummy variables (0 or 1) as all of scales are mutually exclusive (no overlap) and none of them have any numerical significance.","70e5fad5":"#### Removal","5f316a0c":"By just taking log, the shape of the distribution becomes almost normal.  ","3062eda9":"### 3.4 Pairplot\n<a id='pairplot'><\/a>\n\nOne of the best way to visualize the relationship between the target variable and many features at the same time is pairplot. In our case, instead of plotting the whole features with target variable, only chose the top 10 most highly correlated features with target variable: 'SalesPrice','OverallQual',GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', 1stFlrSF', 'FullBath','YearBuilt', 'YearRemodAdd'.","da4d4392":"#### Kfold","0026820a":"#### Zero","5dbfcc7d":"## 6. Submission\n<a id='submission'><\/a>","98183c7a":"#### Ordinal\n\nWith ordinal scales, it is the order of the values is what's important and significant, but the differences between each one is not really known. Therefore, unlike nominal, ordinal values matter the order (e.g. a > b > c). In this case, instead of using get_dummies (0 or 1), it is better to use LabelEncoder.","a7f3fb22":"#### 4.2.4 Log1p Transformation\n<a id='log1p'><\/a>","34c49a92":"- Skewnewss quantifies how symmetrical the distribution is.\n- If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n- If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n- If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n\nSource: https:\/\/help.gooddata.com\/display\/doc\/Normality+Testing+-+Skewness+and+Kurtosis","7fb797e7":"#### ElasticNet Regression","78692a60":"Even though I consider the features with more than 0.5 skewness as not normally distributed feature, you can try some different numbers to improve the result since there is no clear cut rule for the cutoff value.  ","1e4f3dda":"## 5. Modeling <a id='modeling'><\/a>","f1ae26cf":"### 5.1 Feature Scaling \n<a id='feature_scaling'><\/a>\nMany machine learning algorithms expect the scale of the input and even the output data to be equivalent. It can help in methods, particularly linear models, that weights in order to make a prediction. Among many scaling techniques, I decided to use StandardScaler since in the previous section 3.1.1 outlier detections, we have already removed the data whose GrLivArea is greater than 4000. Therefore, we would worry the information loss by removing the outliers more rather than distortion of the data shape because of outliers. If we did not delete those outliers, it would be better to choose RobustScaler."}}