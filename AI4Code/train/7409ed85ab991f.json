{"cell_type":{"d467a5dd":"code","91b0b25b":"code","c3ab2c91":"code","a3c89e2a":"code","2f667a3a":"code","49e0574a":"code","e52b0693":"code","6f5ac11e":"code","bf508721":"code","1ad887e6":"code","fa057ccb":"code","ac20d655":"code","c6d6aae0":"code","27882e17":"code","1ae4025f":"code","3b2ba94c":"code","4a42a97b":"code","d927fab2":"code","f6c584dd":"code","db0aec9b":"code","bfbc7a0b":"code","170b9ce5":"code","fbc801cd":"code","4834a5c2":"code","a24eefc1":"code","978f47a1":"code","280ea914":"code","c42cba6a":"code","95e9c889":"code","44d2899a":"code","118aa53a":"code","6fb2f0bb":"code","ce2a6e62":"code","470423b7":"code","ec446f52":"code","7a3e78d8":"code","59063671":"code","746ec271":"code","9e541b13":"code","e4dc8098":"code","daa8d8a9":"code","c9ee3af6":"code","adf18fa6":"code","ae09672d":"code","3550d45a":"code","87155c75":"code","cd3bda0d":"code","329f450b":"code","bd6f4c9d":"code","8cd18ecf":"code","29c1d4fb":"code","0f2fdf91":"code","bc6c42cd":"code","40e2081e":"code","c78b173d":"code","2a6b1402":"code","bc5e8969":"code","10103923":"code","1c5978b8":"code","c509130f":"code","5164093b":"code","4e9aab98":"code","74f0b532":"code","b2423e55":"code","44e015eb":"code","41a0bbcd":"code","ea1ee3f6":"code","e189c992":"code","699fda54":"markdown","471daa3d":"markdown","1aa9e4f2":"markdown","55eadffc":"markdown","25c852b0":"markdown","6199edef":"markdown","5332c17d":"markdown","e9b7caff":"markdown","45477109":"markdown","ddb2e5ab":"markdown","c90c6994":"markdown","481e3ce7":"markdown","5837b1ac":"markdown","65ce2b7c":"markdown","b8dabb51":"markdown","f8ab3e89":"markdown","fb460541":"markdown","fb8959cd":"markdown","bb5ba877":"markdown","7e92d08e":"markdown","c3ae2a94":"markdown","457a9a63":"markdown","834f2c8f":"markdown","b2556e14":"markdown","32baa749":"markdown","67c7f3e5":"markdown","c8e437e3":"markdown","b9f870f3":"markdown","edd3ef34":"markdown","4ee103ab":"markdown","4e5db882":"markdown","1c9d3bd5":"markdown","9d1024ae":"markdown","5586d48d":"markdown","cd40ef65":"markdown","225661ef":"markdown","81a589bb":"markdown","f6a4b5b9":"markdown","62c68dc2":"markdown","ab875c32":"markdown","b89dd563":"markdown","909c96ce":"markdown","147d764c":"markdown","81b46317":"markdown","a1a5f49d":"markdown","b4ad1047":"markdown","86ce901c":"markdown"},"source":{"d467a5dd":"from warnings import filterwarnings\nfilterwarnings(\"ignore\")","91b0b25b":"pip install skompiler","c3ab2c91":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LogisticRegression,LogisticRegressionCV\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict,ShuffleSplit,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import scale\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaseEnsemble,GradientBoostingClassifier\nfrom sklearn.svm import SVC,LinearSVC\nimport time\nfrom matplotlib.colors import ListedColormap\nfrom xgboost import XGBRegressor\nfrom skompiler import skompile\nfrom lightgbm import LGBMRegressor","a3c89e2a":"pd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)","2f667a3a":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","49e0574a":"df.shape","e52b0693":"df.describe()","6f5ac11e":"X = df.drop(\"Outcome\",axis=1)\ny= df[\"Outcome\"] #We will predict Outcome(diabetes) ","bf508721":"X_train = X.iloc[:600]\nX_test = X.iloc[600:]\ny_train = y[:600]\ny_test = y[600:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","1ad887e6":"support_vector_classifier = SVC(kernel=\"linear\").fit(X_train,y_train)","fa057ccb":"support_vector_classifier","ac20d655":"# Default C\nsupport_vector_classifier.C","c6d6aae0":"support_vector_classifier","27882e17":"y_pred = support_vector_classifier.predict(X_test)","1ae4025f":"cm = confusion_matrix(y_test,y_pred)","3b2ba94c":"cm","4a42a97b":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","d927fab2":"accuracy_score(y_test,y_pred)","f6c584dd":"print(classification_report(y_test,y_pred))","db0aec9b":"support_vector_classifier","bfbc7a0b":"accuracies= cross_val_score(estimator=support_vector_classifier,\n                            X=X_train,y=y_train,\n                            cv=10)\nprint(\"Average Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standart Deviation of Accuracies: {:.2f} %\".format(accuracies.std()*100))","170b9ce5":"support_vector_classifier.predict(X_test)[:10]","fbc801cd":"svm_params ={\"C\":np.arange(1,20)}","4834a5c2":"svm = SVC(kernel=\"linear\")\nsvm_cv = GridSearchCV(svm,svm_params,cv=8)","a24eefc1":"start_time = time.time()\n\nsvm_cv.fit(X_train,y_train)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time for Support Vector Regression cross validation: \"\n      f\"{elapsed_time:.3f} seconds\")","978f47a1":"#best score\nsvm_cv.best_score_","280ea914":"#best parameters\nsvm_cv.best_params_","c42cba6a":"svm_tuned = SVC(kernel=\"linear\",C=2).fit(X_train,y_train)","95e9c889":"svm_tuned","44d2899a":"y_pred = svm_tuned.predict(X_test)","118aa53a":"cm = confusion_matrix(y_test,y_pred)","6fb2f0bb":"cm","ce2a6e62":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","470423b7":"accuracy_score(y_test,y_pred)","ec446f52":"print(classification_report(y_test,y_pred))","7a3e78d8":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","59063671":"df.shape","746ec271":"df.describe()","9e541b13":"X = df.drop(\"Outcome\",axis=1)\ny= df[\"Outcome\"] #We will predict Outcome(diabetes) ","e4dc8098":"X_train = X.iloc[:600]\nX_test = X.iloc[600:]\ny_train = y[:600]\ny_test = y[600:]\n\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_test Shape: \",X_test.shape)\nprint(\"y_train Shape: \",y_train.shape)\nprint(\"y_test Shape: \",y_test.shape)","daa8d8a9":"support_vector_classifier = SVC(kernel=\"rbf\").fit(X_train,y_train)","c9ee3af6":"#get support vectors\nsupport_vector_classifier.support_vectors_","adf18fa6":"#get supports\nsupport_vector_classifier.support_","ae09672d":"# get number of support vectors for each class\nsupport_vector_classifier.n_support_","3550d45a":"#default gamma\nsupport_vector_classifier.gamma","87155c75":"support_vector_classifier","cd3bda0d":"y_pred = support_vector_classifier.predict(X_test)","329f450b":"cm = confusion_matrix(y_test,y_pred)","bd6f4c9d":"cm","8cd18ecf":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","29c1d4fb":"accuracy_score(y_test,y_pred)","0f2fdf91":"print(classification_report(y_test,y_pred))","bc6c42cd":"support_vector_classifier","40e2081e":"accuracies= cross_val_score(estimator=support_vector_classifier,\n                            X=X_train,y=y_train,\n                            cv=10)\nprint(\"Average Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standart Deviation of Accuracies: {:.2f} %\".format(accuracies.std()*100))","c78b173d":"support_vector_classifier.predict(X_test)[:10]","2a6b1402":"svm_params ={\"C\":[0.0001,0.001,0.01,0.1,0.5,1,3,5,7,10,40,80,100],\n             \"gamma\":[0.0001,0.001,0.01,0.1,0.5,1,5,10,30,50,100]}","bc5e8969":"svm = SVC(kernel=\"rbf\")\nsvm_cv = GridSearchCV(svm,svm_params,cv=8,n_jobs=-1,verbose=2)","10103923":"start_time = time.time()\n\nsvm_cv.fit(X_train,y_train)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"Elapsed time for Support Vector Regression with RBF kernel cross validation: \"\n      f\"{elapsed_time:.3f} seconds\")","1c5978b8":"#best score\nsvm_cv.best_score_","c509130f":"#best parameters\nsvm_cv.best_params_","5164093b":"svm_tuned = SVC(kernel=\"rbf\",C=10,gamma=0.0001).fit(X_train,y_train)","4e9aab98":"svm_tuned","74f0b532":"y_pred = svm_tuned.predict(X_test)","b2423e55":"cm = confusion_matrix(y_test,y_pred)","44e015eb":"cm","41a0bbcd":"print(\"Our Accuracy is: \", (cm[0][0]+cm[1][1])\/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0]))","ea1ee3f6":"accuracy_score(y_test,y_pred)","e189c992":"print(classification_report(y_test,y_pred))","699fda54":"### Theory","471daa3d":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate out model.","1aa9e4f2":"We now have an additional **C** hyperparameter that we can tune. As C increases, our tolerance for points outside of \u03f5 also increases. As C approaches 0, the tolerance approaches 0 and the equation collapses into the simplified one.","55eadffc":"Now we're going to split our dataset to train and test set. We will choose almost 20% of dataset as test size.","25c852b0":"### Model Tuning & Validation","6199edef":"![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fconfusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826&psig=AOvVaw29atdmY9s4wmI-rc0qQZZb&ust=1628435461495000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCKj1g_2Yn_ICFQAAAAAdAAAAABAD).","5332c17d":"## Support Vector Machines - Classifier(SVM) - Linear Kernel","e9b7caff":"The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. Kernel Functions only calculate the relationships between every pair of points as if they are in the higher dimensions, the do not actually do the transformation.  This is called Kernel Trick. In other words, we can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps us to build a more accurate classifier.","45477109":"### Model","ddb2e5ab":"In order to see all rows and columns, we will increase max display numbers of dataframe.","c90c6994":"Now we will try to tune our model by using **K-Fold Cross Validation**.","481e3ce7":"## Support Vector Machines - Classifier(SVM) - Radial Basis Kernel(RBF)","5837b1ac":"Now we will tune our model with GridSearch.","65ce2b7c":"- **true positive**: for correctly predicted event values.\n- **false positive**: for incorrectly predicted event values.\n- **true negative**: for correctly predicted no-event values.\n- **false negative**: for incorrectly predicted no-event values.","b8dabb51":"### Model Tuning & Validation","f8ab3e89":"### Prediction","fb460541":"### Model","fb8959cd":"Now we will try to tune our model by using **K-Fold Cross Validation**.","bb5ba877":"SVM uses Maximum Margin Classifiers to find maximum margin. But Maximum Margin Classifiers are super sensitive to outliers in the training data. To make a treshold that less sensitive to outliers, we allow misclassification. This makes higher bias and low variance. When we allow misclassification to the distance between supporter observation and threshold, it is called **Soft Margin**. In order to understand best soft margin, we use cross validation.","7e92d08e":"That's the formula to minimise:","c3ae2a94":"## Importing Libraries","457a9a63":"- **true positive**: for correctly predicted event values.\n- **false positive**: for incorrectly predicted event values.\n- **true negative**: for correctly predicted no-event values.\n- **false negative**: for incorrectly predicted no-event values.","834f2c8f":"- **The Elements of  Statistical Learning** - Trevor Hastie,  Robert Tibshirani, Jerome Friedman -  Data Mining, Inference, and Prediction (Springer Series in Statistics) \n\n- [**What is a Confusion Matrix in Machine Learning?**](https:\/\/machinelearningmastery.com\/confusion-matrix-machine-learning\/)\n\n- [**Support Vector Machines for Machine Learning**](https:\/\/machinelearningmastery.com\/support-vector-machines-for-machine-learning\/)\n\n- [**Support Vector Machines with Scikit-learn**](https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python)\n\n- [**Support Vector Machines by Statquest**](https:\/\/www.youtube.com\/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer)\n\n- [**Support Vector Machines with Scikit-learn**](https:\/\/www.datacamp.com\/community\/tutorials\/svm-classification-scikit-learn-python)\n\n- [**SVM by Sklearn**](https:\/\/scikit-learn.org\/stable\/modules\/svm.html)\n\n- [**Kernel Functions-Introduction to SVM Kernel & Examples**](https:\/\/data-flair.training\/blogs\/svm-kernel-functions\/)","b2556e14":"- **true positive**: for correctly predicted event values.\n- **false positive**: for incorrectly predicted event values.\n- **true negative**: for correctly predicted no-event values.\n- **false negative**: for incorrectly predicted no-event values.","32baa749":"We will use **rbf kernel** here.","67c7f3e5":"### Prediction","c8e437e3":"Now we will tune our model with GridSearch. We will tune *gamma* and *C* parameters.","b9f870f3":"But mapping to a higher dimensional space can be highly compute-intensive. Therefore this approach is not the best. We use **kernel trick** because of that. ","edd3ef34":"For a real world example, we will work with **Pima Indians Diabetes Database** data set by UCI Machine Learning.\n\nIt can be downloaded [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nWe will try to predict whether the patient has diabetes or not.","4ee103ab":"Support Vector Machine gives us the flexibility to define how much error is acceptable in our model and will find an appropriate line (or hyperplane in higher dimensions) to fit the data. The objective function of SVM is to minimize the coefficients \u2014 more specifically, the l2-norm of the coefficient vector \u2014 not the squared error. The error term is instead handled in the constraints, where we set the absolute error less than or equal to a specified margin, called the maximum error, **\u03f5 (epsilon)**. We can tune epsilon to gain the desired accuracy of our model.\n\n**Support Vectors**: Support vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n\n**Hyperplane**: A hyperplane is a decision plane which separates between a set of objects having different class memberships.\n\n**Margin**: A margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.","4e5db882":"Illustrative example:\n\n![image-2.png](attachment:image-2.png)\n\nPhoto is cited by:https:\/\/towardsdatascience.com\/an-introduction-to-support-vector-regression-svr-a3ebc1672c2","1c9d3bd5":"We also could use *svm.LinearSVC()* function directly.","9d1024ae":"## Resources","5586d48d":"**Created by Berkay Alan**\n\n**Classification with Support Vector Machines \ud83d\udcc8\ud83d\udcaf**\n\n**10 August 2021**\n\n**For more Tutorial:** https:\/\/github.com\/berkayalan\/Data-Science-Tutorials","cd40ef65":"**Some Kernels**\n\n- **Linear Kernel**: A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n\n        K(x, xi) = sum(x * xi)\n\n- **Polynomial Kernel**: A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.\n\n        K(x,xi) = 1 + sum(x * xi)^d (Where d is the degree of the polynomial)\n\n- **Radial Basis(Gaussion) Function Kernel**: The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in **infinite dimensional space**.\n\n        K(x,xi) = exp(-gamma * sum((x \u2013 xi^2))\n        \nHere gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm.\n\nI highly suggest you to read [this article](https:\/\/towardsdatascience.com\/svm-and-kernel-svm-fed02bef1200) for this part.","225661ef":"- **true positive**: for correctly predicted event values.\n- **false positive**: for incorrectly predicted event values.\n- **true negative**: for correctly predicted no-event values.\n- **false negative**: for incorrectly predicted no-event values.","81a589bb":"![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fpursuitnotes%2Fday-12-kernel-svm-non-linear-svm-5fdefe77836c&psig=AOvVaw04okfh2OVimfvUsX8lEKRm&ust=1628623796797000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCNDUns3WpPICFQAAAAAdAAAAABAD).","f6a4b5b9":"For a real world example, we will work with **Pima Indians Diabetes Database** data set by UCI Machine Learning.\n\nIt can be downloaded [here](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database).\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nWe will try to predict whether the patient has diabetes or not.","62c68dc2":"![Screen%20Shot%202021-07-20%20at%2020.29.43.png](attachment:Screen%20Shot%202021-07-20%20at%2020.29.43.png)","ab875c32":"![image.png](attachment:image.png)\n\nPhoto is cited by [here](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fconfusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826&psig=AOvVaw29atdmY9s4wmI-rc0qQZZb&ust=1628435461495000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCKj1g_2Yn_ICFQAAAAAdAAAAABAD).","b89dd563":"As we can see, we have data points that are outside the epsilon in sensitive tube. We care about the error for them and they will be measured as distance between the point and the tube. As such, we need to account for the possibility of errors that are larger than \u03f5. We can do this with slack variables.\n\nThe concept of **slack variables** is simple: for any value that falls outside of \u03f5, we can denote its deviation from the margin as \u03be.","909c96ce":"Because we are doing a classification case, we will create a **confusion matrix** in order to evaluate out model.","147d764c":"Some problems can\u2019t be solved using linear hyperplane. In such situation, SVM uses a kernel trick to transform the input space to a higher dimensional space.\n\n**SVM Kernels**\n\nThe SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. Kernel Functions only calculate the relationships between every pair of points as if they are in the higher dimensions, the do not actually do the transformation.  This is called Kernel Trick. In other words, we can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps us to build a more accurate classifier.","81b46317":"Photo is cited by: https:\/\/towardsdatascience.com\/an-introduction-to-support-vector-regression-svr-a3ebc1672c2","a1a5f49d":"**Radial Basis(Gaussion) Function Kernel**: The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in **infinite dimensional space**.\n\n        K(x,xi) = exp(-gamma * sum((x \u2013 xi^2))\n        \nHere gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm. It is specified with cross validation.\n\nWhen there is a new observation to classify, radial kernel behaves like a *Weighted Nearest Neighbour*. Because the closest observation(nearest neihgbour) have a lot of influence on how we classify the new observation. \n\nI highly suggest you to read [this article](https:\/\/towardsdatascience.com\/svm-and-kernel-svm-fed02bef1200) for this part.","b4ad1047":"Now we're going to split our dataset to train and test set. We will choose almost 20% of dataset as test size.","86ce901c":"### Theory"}}