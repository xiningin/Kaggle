{"cell_type":{"840efa8f":"code","0c4ea72a":"code","6936dfb4":"code","2bd4c428":"code","4098369b":"code","1ce66b21":"code","59a85f91":"code","eecf5a23":"code","5f38511c":"code","e7b60849":"code","d82968b9":"code","78398906":"code","b2150c78":"code","3951f7a6":"code","a6367fb7":"code","58eace77":"code","7240ff13":"code","a4b8e381":"code","1c576aed":"code","1bba8162":"code","605c2394":"code","4077a5eb":"code","c2dcc451":"code","2b562a1a":"code","98e60ded":"code","1abe2d2c":"code","df2ff512":"code","dbe4f862":"code","1db48f5b":"code","91f507e4":"code","1dcfe7d0":"code","2b775335":"code","e7cb0bc6":"code","3f262e95":"code","a3f9d3e9":"code","bccca3c4":"code","98854d8c":"code","de861253":"code","356427b3":"code","e0acbd11":"code","5e1edf09":"code","621213dc":"code","cddb509c":"code","5950fa9a":"code","0d7e4822":"code","28ec4fc6":"code","6d5384c0":"code","ec1d101d":"code","1c10c278":"code","3b37b2fb":"code","3b0f71e6":"code","81c2cba4":"code","d42b5108":"code","df579557":"code","ab9400b9":"code","7c04fb83":"code","cb426701":"code","d4aaab33":"code","0b9d2f32":"code","2566eb2a":"code","8e14b1f2":"code","3659cfb4":"code","8e8b5c76":"code","f5103c4f":"code","9aaac8c8":"code","ed9f8677":"code","9411db2b":"code","476b80b0":"code","99f7f638":"code","d0f7c388":"code","71d9c227":"code","e0976c1d":"code","7daaee46":"code","f65f6945":"code","29238878":"markdown","6112da0a":"markdown","a24725ab":"markdown","f1df81f9":"markdown","ebd198fe":"markdown","6c29e448":"markdown","681c92db":"markdown","d72271e1":"markdown","84771701":"markdown","ccc24628":"markdown","fc753c19":"markdown","8fe71085":"markdown","1cc1c1ed":"markdown","641caea2":"markdown"},"source":{"840efa8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c4ea72a":"!pip install graphviz;\n#!pip uninstall dtreeviz","6936dfb4":"## look into the dater\n#from fastbook import *\n#from kaggle import api\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n#!pip install dtreeviz\n#from dtreeviz.trees import *\nfrom IPython.display import Image, display_svg, SVG","2bd4c428":"from sklearn.metrics import confusion_matrix, roc_auc_score, plot_confusion_matrix, precision_score, recall_score, classification_report, plot_roc_curve, roc_curve\nfrom sklearn.model_selection import train_test_split","4098369b":"## pandas\npd.options.display.max_rows = 20\npd.options.display.max_columns = 12","1ce66b21":"main_path = \"..\/input\/ieee-fraud-detection-joined-tables\/\"","59a85f91":"## 1. Create df from 100k sample and then go to random 50k sample\ndf = pd.read_csv(main_path+\"tr_sample-100k.csv\")\ndf = df.sample(50000, random_state=42)\n# #df_te= pd.read_csv(main_path + \"te_tran_iden.csv\")","eecf5a23":"# ## 2. Take all samples of isFraud=True and Random Samples of isFraud=False\n# df = pd.read_csv(main_path+\"tr_tran_iden.csv\")\n# ### ratio of true and false\n# print(df.isFraud.value_counts()[1]\/len(df.isFraud) *100)\n# print(df[\"isFraud\"].value_counts())","5f38511c":"# df1 = df[df[\"isFraud\"]==False].sample(50000, random_state=42)\n# df2 = df[df[\"isFraud\"]==True]\n# df = pd.concat([df1,df2], ignore_index=True)\n# df.shape, df1.shape, df2.shape","e7b60849":"# ### ratio of true and false\n# df.isFraud.value_counts()[1]\/len(df.isFraud) *100","d82968b9":"# ## Get dataset\n# df = pd.read_csv(main_path+\"tr_sample-100k.csv\")\n# ### ratio of true and false\n# df = df.sample(50000,random_state=42)\n# df.isFraud.value_counts()\n","78398906":"# ## sort by time\n# df.sort_values(\"TransactionDT\", inplace=True)\n# df.reset_index(drop=True, inplace=True)\n# df","b2150c78":"# ## split the data into train and valid\n# df1 = df.iloc[0:int(0.8*len(df)),]\n# df2 = df.iloc[int(0.8*len(df)):len(df),]\n# df1.shape,df2.shape","3951f7a6":"# ## Mix both the ROSed train dater and valid dater\n# from imblearn.over_sampling import RandomOverSampler\n# ros = RandomOverSampler(random_state=42)\n\n# X,Y = ros.fit_resample(df1,df1[\"isFraud\"])\n# df=pd.concat([X,df2], ignore_index=True)\n# df.reset_index(drop=True, inplace=True)","a6367fb7":"#len(X)\/len(df), df.isFraud.value_counts(), df2.isFraud.value_counts()","58eace77":"## check tables for missing values in join column\ncol = \"shape\", \"columns with nans \", \"nans in TransactioniD\"\nind = \"train\",\"test\"\nlst1 = [df.shape, sum(df.isna().sum()>0), df.TransactionID.isna().sum()]\n#lst2 = [df_te.shape, sum(df_te.isna().sum()>0), df_te.TransactionID.isna().sum()]\n#pd.DataFrame([lst1,lst2], columns=col,index=ind)\nlst1","7240ff13":"## Class imbalance? Might need to upsample or downsample???? Isn't usage of false positves and true positives overcoming imbalance?\nsum(df[\"isFraud\"]==True)\/len(df)","a4b8e381":"## remove columns with nans>300k\n##[((df.isna().sum()>n\/2*1e5).sum(),n\/2) for n in range(1,13)]\n#to_drop = df.columns[df.isna().sum()>300000].to_list()\nto_drop=[] # forcing to use all variables\ndf.drop(to_drop,1, inplace=True)\ndf.shape","1c576aed":"## sort by time\ndf.sort_values(\"TransactionDT\", inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf","1bba8162":"## split to train and valid ind linearly\nmsk = np.arange(len(df))<0.8*len(df) #usually 0.8, 0.8851 for the oversampling case\nsplits = (list(np.where(msk)[0]),list(np.where(~msk)[0]))\nlen(splits[0])+len(splits[1]), len(df), len(splits[0])","605c2394":"## variables for TP\ndep_var = \"isFraud\"\nprocs = [Categorify,FillMissing]\n## Cont and Cat variables\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var)","4077a5eb":"## check if cont variables in test have nans where train doesn't\n#def na_check(col): return df.loc[:,col].isna().sum()>0,df_te.loc[:,col].isna().sum()>0\n#na_col = {col: na_check(col) for col in cont}\n#na_col_te = [col for col in cont if df.loc[:,col].isna().sum()==0 and df_te.loc[:,col].isna().sum()>0]\nna_col_te = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14']\n","c2dcc451":"df.isFraud.value_counts()","2b562a1a":"## Add a last row with nans for col:na_col_te\n#df.insert(0,df.columns,df.median())\n\ndf = pd.concat([df.median().to_frame().T, df], ignore_index=True)","98e60ded":"df.loc[df.index[0],na_col_te] = np.nan\ndf.loc[df.index[0],\"isFraud\"]=1 ## forcing it to be an int rather than in between like 0.5\ndf.loc[df.index[0],na_col_te]\ndf.shape","1abe2d2c":"df.isFraud.value_counts()","df2ff512":"to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)\nlen(to.train)+len(to.valid)","dbe4f862":"to.items.columns","1db48f5b":"to.valid.show(3)","91f507e4":"temp = to.valid.items\ntemp.isFraud.value_counts()","1dcfe7d0":"to.items.head()","2b775335":"##vocab\nto.classes[\"ProductCD\"]","e7cb0bc6":"save_pickle(\"to.pkl\",to)","3f262e95":"load_pickle(\"to.pkl\")","a3f9d3e9":"def rf(xs, y, n_estimators=40, max_samples=0.6,#20_000,#200_000,\n       max_features=0.8, min_samples_leaf=5, sample_weight=None, **kwargs):\n    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True,random_state=42).fit(xs, y,sample_weight)#, class_weight=\"balanced_subsample\").fit(xs, y)","bccca3c4":"xs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\nxs.shape","98854d8c":"# ## Oversampling SMOTE\n# from imblearn.over_sampling import SMOTE\n# sm = SMOTE(random_state=42, n_jobs=-1,k_neighbors=15)\n\n# X,Y = sm.fit_resample(xs,y)\n# m = rf(X,Y)\n#Y.value_counts(), y.value_counts(),","de861253":"y.dtype","356427b3":"# #oversampling\n# from imblearn.over_sampling import RandomOverSampler\n# ros = RandomOverSampler(random_state=42)\n\n# X,Y = ros.fit_resample(xs,y)\n# m = rf(X,Y)","e0acbd11":"## Weighting the samples\n# wt = y.value_counts()[0]\/y.value_counts()[1]\n# y.value_counts(),wt\n#sample_weight = np.array([0.88 if i == 0 else 23 for i in y])","5e1edf09":"## m = DecisionTreeRegressor(max_leaf_nodes=4)\n## m.fit(xs, y);\nm = rf(xs,y)#,sample_weight=sample_weight)\n#Y=y\n","621213dc":"def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6)","cddb509c":"m1 = m.estimators_[0]","5950fa9a":"## check the leaves are much lesser than the length of samples.\nmax([t.get_n_leaves() for t in m.estimators_]), m.max_samples*len(xs)","0d7e4822":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","28ec4fc6":"pd.DataFrame(m.feature_importances_).describe()","6d5384c0":"fi = rf_feat_importance(m,xs)\nfi[0:20]","ec1d101d":"def plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:50]);","1c10c278":"to_keep = fi[fi.imp>0.005].cols\nlen(to_keep)","3b37b2fb":"fi.imp","3b0f71e6":"xs_cp = xs.copy()\nvalid_xs.cp = valid_xs.copy()","81c2cba4":"xs= xs[to_keep]\nvalid_xs = valid_xs[to_keep]","d42b5108":"xs_imp.shape","df579557":"m=rf(xs,y)","ab9400b9":"## checking if mean of all estimators gives the random forest results\npreds_p = np.stack([t.predict_proba(valid_xs)[:,1] for t in m.estimators_])\nr_mse(m.predict_proba(valid_xs)[:,1],preds_p.mean(0))\n","7c04fb83":"## how AUC score changes with n_estimators_\nplt.plot([roc_auc_score(valid_y, preds_p[:i+1].mean(0)) for i in range(len(m.estimators_))]); # n_estimators seems fine","cb426701":"m.max_samples","d4aaab33":"## predict proba of OOB\ny_oob = m.oob_decision_function_[:,1]\ny_oob.shape","0b9d2f32":"oob_auc = roc_auc_score(y,m.oob_decision_function_[:,1])","2566eb2a":"fpr,tpr,_ = roc_curve(y,y_oob)\nplt.plot(fpr,tpr, label='AUC = ' + str(round(roc_auc_score(y,m.oob_decision_function_[:,1]), 2)))\nplt.legend(loc='lower right')","8e14b1f2":"roc_auc_score(y,y_oob)","3659cfb4":"preds_p.shape","8e8b5c76":"preds_p_std = preds_p.std(0)\npreds_p_std[0:5]","f5103c4f":"pd.DataFrame(preds_p_std).describe() ## Can look at individual rows prediction std from different trees.","9aaac8c8":"## how unbalanced is the train and the valid, they look the same to me. :)\ny.value_counts()[1]\/y.value_counts()[0] *100, valid_y.value_counts()[1]\/valid_y.value_counts()[0] *100","ed9f8677":"## Metrics functions\ndef m_rmse(m, xs, y): return r_mse(m.predict(xs), y)\n#def m_accuracy(m,xs,y): return sum(m.predict(xs)==y)\/len(xs)\ndef m_accuracy2(m,xs,y): return m.score(xs,y)\ndef m_conf(m,xs,y): \n    plot_confusion_matrix(m,xs,y)\n    return confusion_matrix(y, m.predict(xs))\ndef m_prec(m,xs,y): return precision_score(y,m.predict(xs), average=None, labels=[1,0])\ndef m_recall(m,xs,y): return recall_score(y,m.predict(xs), average=None, labels=[1,0])\ndef m_rep(m,xs,y): return classification_report(y, m.predict(xs), labels=[1,0], digits=4, output_dict=True)","9411db2b":"## See predicted column. Does it match expectations\npd.DataFrame(m.predict(xs)).describe() ","476b80b0":"## prec recal train\ntr_rep = m_rep(m,xs,y)\nprint(tr_rep[\"1\"][\"recall\"]) ## sensitivity TPR\nprint(tr_rep[\"0\"][\"recall\"]) ## TNR\nprint(1- tr_rep[\"0\"][\"recall\"]) ## 1-specificity FPR\nprint(tr_rep[\"1\"][\"precision\"])\nprint(tr_rep[\"0\"][\"precision\"])","99f7f638":"## prec recal valid\nvd_rep = m_rep(m,valid_xs,valid_y)\nprint(vd_rep[\"1\"][\"recall\"]) ## sensitivity TPR\nprint(vd_rep[\"0\"][\"recall\"]) ## TNR\nprint(1- vd_rep[\"0\"][\"recall\"]) ## 1-specificity FPR\nprint(vd_rep[\"1\"][\"precision\"])\nprint(vd_rep[\"0\"][\"precision\"])","d0f7c388":"# plot_roc_curve(m,xs,y,response_method=\"decision_function\") not working \nplt.plot(fpr,tpr, label='AUC = ' + str(round(roc_auc_score(y,m.oob_decision_function_[:,1]), 2)))\nplt.legend(loc='lower right')\n","71d9c227":"## plot ROC\nplot_roc_curve(m,xs,y)\nplot_roc_curve(m,valid_xs,valid_y)","e0976c1d":"## AUC score\nprint(roc_auc_score(y,m.predict_proba(xs)[:,1]), roc_auc_score(valid_y,m.predict_proba(valid_xs)[:,1]))\n## recall\nprint(\"\\n\",tr_rep[\"1\"][\"recall\"]) ## sensitivity TPR\nprint(tr_rep[\"0\"][\"recall\"]) ## TNR\n##valid\nprint(vd_rep[\"1\"][\"recall\"]) ## sensitivity TPR\nprint(vd_rep[\"0\"][\"recall\"]) ## TNR\n## precision\nprint(tr_rep[\"1\"][\"precision\"])\nprint(tr_rep[\"0\"][\"precision\"])\n## valid precision\nprint(vd_rep[\"1\"][\"precision\"])\nprint(vd_rep[\"0\"][\"precision\"])","7daaee46":"print(round(roc_auc_score(y,m.predict_proba(xs)[:,1]),4),\";\",round(roc_auc_score(y,m.oob_decision_function_[:,1]),4),\";\",round(roc_auc_score(valid_y,m.predict_proba(valid_xs)[:,1]),4),\";\",\n      round(tr_rep[\"1\"][\"recall\"],4),\",\", round(tr_rep[\"0\"][\"recall\"],4),\";\",\n      round(vd_rep[\"1\"][\"recall\"],4), \",\",round(vd_rep[\"0\"][\"recall\"],4),\";\",\n      round(tr_rep[\"1\"][\"precision\"],4),\",\",round(tr_rep[\"0\"][\"precision\"],4),\";\",\n      round(vd_rep[\"1\"][\"precision\"],4),\",\",round(vd_rep[\"0\"][\"precision\"],4))","f65f6945":"precision_score(y,m.predict(xs))","29238878":"## STD across trees gives confidence in the readings","6112da0a":"## Model","a24725ab":"## Feature importance","f1df81f9":"## OUT OF BAGs","ebd198fe":"## Check Dater ","6c29e448":"## oversampling (with replacement)","681c92db":"## RF metrics deep dive","d72271e1":"## Tabular Pandas","84771701":"## Output of Valid dater","ccc24628":"## Import ","fc753c19":"### Undersampling","8fe71085":"## Resources\nMore info on the dater\n1. https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203 \n1. [Notebook on RF](https:\/\/www.kaggle.com\/raviolli77\/random-forest-in-python#Training-Algorithm)\n2. [searching hyperparameters](https:\/\/www.kaggle.com\/raviolli77\/random-forest-in-python#Creating-Training-and-Test-Sets)\n3. [8 tactics to combat unbalanced datersets](https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/)\n4. [Kaggle notebook on unbalanced datersets](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)\n5. [Survey of predictive modeling with imbalnced datersets](https:\/\/arxiv.org\/abs\/1505.01658)","1cc1c1ed":"## Trimming col from the dater","641caea2":"### Random Sampling"}}