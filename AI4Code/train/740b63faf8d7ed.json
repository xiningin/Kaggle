{"cell_type":{"f888bb65":"code","9b560ba2":"code","1133e6a4":"code","b2bacab8":"code","210f0dfc":"code","e4a951d9":"code","3f322338":"code","e7e6cd6a":"code","e271d35a":"code","ed1ab67a":"markdown","3ac701cd":"markdown","34c6d890":"markdown","bd963737":"markdown","babf9eea":"markdown","62444ed6":"markdown","286be06e":"markdown","5963d1f5":"markdown","5b77306a":"markdown","77d35e3a":"markdown","6368ca13":"markdown","cf7dc1c7":"markdown","1cc7e6a3":"markdown","d1de1cd2":"markdown","c8d1808d":"markdown","96a5e888":"markdown","c9c56453":"markdown","a46fe1cb":"markdown","caef2022":"markdown"},"source":{"f888bb65":"# import the necessary packages\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom imutils import paths\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport argparse\nimport cv2\nimport os\n","9b560ba2":"# construct the argument parser and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(\"-d\", \"--dataset\", required=True,\n\thelp=\"path to input dataset\") \nap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n\thelp=\"path to output loss\/accuracy plot\")\nap.add_argument(\"-m\", \"--model\", type=str, default=\"covid19.model\",\n\thelp=\"path to output loss\/accuracy plot\")\nargs = vars(ap.parse_args())","1133e6a4":"# initialize the initial learning rate, number of epochs to train for,\n# and batch size\nINIT_LR = 1e-3\nEPOCHS = 25\nBS = 8","b2bacab8":"# Fetch images in our dataset directory, then initialize the list of data (i.e., images) and class images\nprint(\"Loading images from the dataset\")\nimagePaths = list(paths.list_images(args[\"dataset\"]))\ndata = []\nlabels = []\n\n\nfor imagePath in imagePaths:\n   # extract the class label from the filename\n    label = imagePath.split(os.path.sep)[-2]\n\n# load the image, swap color channels, and resize it to be a fixed\n# 224x224 pixels while ignoring aspect ratio\n    image = cv2.imread(imagePath)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224, 224))\n\n# update the data and labels lists\n    data.append(image)\n    labels.append(label)\n# convert the data and labels to NumPy arrays while scaling the pixel\n# intensities to the range [0, 255]\ndata = np.array(data) \/ 255.0\nlabels = np.array(labels)\n","210f0dfc":"# perform one-hot encoding on the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\nlabels = to_categorical(labels)\n\n# partition the data into training and testing splits using 80% of\n# the data for training and the remaining 20% for testing\n(trainX, testX, trainY, testY) = train_test_split(data, labels,test_size=0.20, stratify=labels, random_state=42)   \n#test_size=0.20 specifies that we reserving 20% of it for testing and the remaining 80% to for training.\n\n# initialize the training data augmentation object\ntrainAug = ImageDataGenerator(rotation_range=15,fill_mode=\"nearest\")","e4a951d9":"# load the VGG16 network, ensuring the head FC layer sets are left\n# off\nbaseModel = VGG16(weights=\"imagenet\", include_top=False,\n\tinput_tensor=Input(shape=(224, 224, 3)))\n\n# construct the head of the model that will be placed on top of the\n# the base model\nheadModel = baseModel.output\nheadModel = AveragePooling2D(pool_size=(4, 4))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(64, activation=\"relu\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(2, activation=\"softmax\")(headModel)\n\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmodel = Model(inputs=baseModel.input, outputs=headModel)\n\n# loop over all layers in the base model and freeze them so they will\n# *not* be updated during the first training process\nfor layer in baseModel.layers:\n\tlayer.trainable = False","3f322338":"#compile our model\nprint(\"Compiling the CNN model...\")\nopt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n\n# train the head of the network\nprint(\"Training head...\")\nH = model.fit_generator(\n    trainAug.flow(trainX, trainY, batch_size=BS),\n    steps_per_epoch=len(trainX) \/\/ BS,\n    validation_data=(testX, testY),\n    validation_steps=len(testX) \/\/ BS,\n    epochs=EPOCHS)","e7e6cd6a":"\n# make predictions on the testing set\nprint(\"Evaluating network...\")\npredIdxs = model.predict(testX, batch_size=BS)\n\n# for each image in the testing set we need to find the index of the label with corresponding largest predicted probability\npredIdxs = np.argmax(predIdxs, axis=1)\n\n# Display formatted classification report\nprint(classification_report(testY.argmax(axis=1), predIdxs,target_names=lb.classes_))\n# compute the confusion matrix and and use it to derive the raw\n# accuracy, sensitivity, and specificity\ncm = confusion_matrix(testY.argmax(axis=1), predIdxs)\ntotal = sum(sum(cm))\nacc = (cm[0, 0] + cm[1, 1]) \/ total\nsensitivity = cm[0, 0] \/ (cm[0, 0] + cm[0, 1])\nspecificity = cm[1, 1] \/ (cm[1, 0] + cm[1, 1])\n\n# show the confusion matrix, accuracy, sensitivity, and specificity\nprint(cm)\nprint(\"acc: {:.4f}\".format(acc))\nprint(\"sensitivity: {:.4f}\".format(sensitivity))\nprint(\"specificity: {:.4f}\".format(specificity))","e271d35a":"# plot the training loss and accuracy\nN = EPOCHS\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig(args[\"plot\"])\n\n# serialize the model to disk\nprint(\"[INFO] saving COVID-19 detector model...\")\nmodel.save(args[\"model\"], save_format=\"h5\")","ed1ab67a":"**2. Downloading the dataset**","3ac701cd":"![animated-graphic-2019-ncov.jpg](attachment:animated-graphic-2019-ncov.jpg)\n\nThe coronavirus spread so quickly between people and approaches 100,000 people worldwide. In this\nconsequence, it is very much essential to identify the infected people so that prevention of spread can\nbe taken.  \nThe detection of COVID-19 is currently a difficult task because of unavailability of diagnosis system\neverywhere, which is causing panic. Since COVID-19 attacks the epithelial cells that line our\nrespiratory tract, we can use X-rays to analyse the health of a patient\u2019s lungs.\n\nIn this project, the deep learning based methodology is used for detection of the coronavirus\ninfected patient using X-ray images.\n\n( This article is for educational puposes only. It is not medically, a highly accurate COVID-19 diagnosis system, nor has it been professionally or academically vetted ) ","34c6d890":"**4. Shortcomings of the Model**\n\nOn testing the CNN model we found the accuracy 80% of accuracy. Following reasons may account for the same:-       \n1. Maybe there\u2019s some better model for this type of classification.\n2. The dataset is very, but, very small. Unfortunately, I didn\u2019t find any other Covid-19-positive chest X-rays.\n3. It is difficult to classify images, especially for someone who isn\u2019t an expert in medicine.\n\n****![Arnav.png](attachment:Arnav.png)","bd963737":"* We are using a Batch size (BS) of 8.\n* INIT_LR is our training rate. \n* EPOCHS refers to one cycle through the full training dataset.","babf9eea":"* Out of the various optimization techniques, I chose Adam optimizer.Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum.        \n* Given that this is a two-class problem, we use binary_crossentropy loss rather than categorical crossentropy.","62444ed6":"Now, we plot the model\u2019s training accuracy\/loss.\n![ArnavPlot.png](attachment:ArnavPlot.png)\n\nBy looking at the plot, It is evident that:\n* The model is not over-fitted or under-fitted. \n* The loss decreases when epochs increase.","286be06e":"* Here, We make predictions on the testing set and generated and print out a classification report using scikit-learn\u2019s helper utility.    \n* Further, we generated confusion matrix and used it to derive the accuracy, sensitivity, and specificity, and print them out ","5963d1f5":"Initially, we fetch all images in the dataset\u2019s directory .\nThen, for each **imagePath**, we:\n\n* Extract the class label (either Covid or normal) from the path.    \n* Load the image, preprocess it by converting to RGB channel ordering, and resize it to 224\u00d7224 pixels so that it is ready for our convolutional neural network.   \n* Update our data and label lists, respectively.  \n\nThen we convert the data into NumPy arrays for usability.","5b77306a":"The COVID-19 X-ray image dataset used for this project was curated by Dr. Joseph Cohen, a postdoctoral fellow at the University of Montreal:https:\/\/github.com\/ieee8023\/covid-chestxray-dataset](http:\/\/))\n* For the COVID-19 positive X-Ray images, I parsed the \"metadata.csv\" file found in Dr. Cohen\u2019s repositoryand selected all rows that are:\n  Positive for COVID-19 (i.e., ignoring MERS, SARS, and ARDS cases).\n  \n* For the X-ray images of healthy patients, I used Kaggle\u2019s Chest X-Ray Images (Pneumonia) dataset, which were quite noisy and laden with incorrect labels, but it served as a good enough starting point for this COVID-19 detector.\n\n","77d35e3a":"**1.  Setting up the Development Environment**\n\nIn this project, a system based on deep Convoluted Neural Networks (CNNs) is developed for the identification of COVID-19 as a\nclassification task. In this study, we prepared two sets of datasets: One of the COVID19 affected X-Ray images and other of a healthy person. \n\nFor the development of the same, we will be working with the following packages:-\n1. TensorFlow : \n>                 pip install tensorflow\n2. scikit-learn: \n>                 pip install sklearn\n3. Imutils: \n>                 pip install imutils\n4. Matplotlib:\n>                 pip install matplotlib\n5. OpenCV: \n>                 pip install opencv-python","6368ca13":"**3. Building a convolutional neural network model to detect Covid-19 in X-ray images**","cf7dc1c7":"![covid19_keras_dataset.png](attachment:covid19_keras_dataset.png)\n","1cc7e6a3":"Following libraries have been used:-\n* TensorFlow and Keras to develop our CNN(convolutional neural network).\n* scikit-learn to perform processing on the data and to evaluate the model. \n* OpenCV for processing the images in the dataset. \n* Matplotlib to aid the visualisation of the results of the model.","d1de1cd2":"* One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction. The categorical value represents the numerical value of the entry in the dataset.    \n* Each encoded label consists of a two-element array with one of the elements being \u201chot\u201d (1) or \u201cnot\u201d(0). We use it as needed to classify positive or negative cases; that is, we perform a binary classification.          \n* We have performed Data Augmentation which is a strategy that enables us to significantly increase the diversity of data available for training models, without actually collecting new data. \n","c8d1808d":"We can use the dataset for deep feature extraction based on deep learning architectures such as AlexNet, VGG16, VGG19, GoogleNet, ResNet18, ResNet50,\nResNet101, InceptionV3, InceptionResNetV2, DenseNet201 and XceptionNet.             \nFor simplicity I chose VGG16 network with weights pre-trained on ImageNet.          \n\nVGG16 Documentation: [https:\/\/keras.io\/applications\/#vgg16](http:\/\/)","96a5e888":"**Reference:**  Sethy, P.K.; Behera, S.K. Detection of Coronavirus Disease (COVID-19) Based on Deep Features. Preprints 2020, 2020030300 (doi: 10.20944\/preprints202003.0300.v1).","c9c56453":"* dataset is the path of the dataset.\n* plot is an optional path to an output training history plot. The default is plot.png.\n* model is the optional path to our output Covid-19 model; by default, it will be named covid19.model.","a46fe1cb":"  **Structure of this Article**\n\nWhile creating this model, following flow of events were observed:-\n1. Setting up the Development Environment\n2. Downloading the dataset\n3. Building a convolutional neural network to detect Covid-19 in X-ray images\n4. Shortcomings of the model\n","caef2022":"Following table shows the scope of better accuracy by using different models.\n![ar.png](attachment:ar.png)\n"}}