{"cell_type":{"3388717a":"code","3e727913":"code","5590347b":"code","df0a703a":"code","1c094927":"code","c3619156":"code","de7d145f":"code","57d7b424":"code","102bac3e":"code","ecafa5c9":"code","c2875161":"code","ab1b1daa":"code","ee358755":"code","eedffeee":"code","ba7187aa":"code","91bbdd99":"code","8de933de":"code","19cb3ce8":"code","4a55f574":"markdown","b2a05c90":"markdown","f4911ef3":"markdown","b9e0182c":"markdown","59b5f060":"markdown","d9da6959":"markdown","4e44b40e":"markdown","abd8c14d":"markdown","ac9dd8e4":"markdown","4d78f9fc":"markdown","98f0adc3":"markdown","b0f1b44b":"markdown"},"source":{"3388717a":"pip install pyspark","3e727913":"pip install findspark","5590347b":"from pyspark.sql import SparkSession \nspark=SparkSession.builder.appName(\"withcol\").getOrCreate()\n\n ","df0a703a":"df=spark.read.csv(\"..\/input\/employee-salary-dataset\/Employee_Salary_Dataset.csv\",header=True,inferSchema=True)\ndf.show(5)","1c094927":"df.printSchema()","c3619156":"#Method 1:\ndf1=df.withColumn(\"Salary\",df[\"Salary\"].cast(\"String\"))\ndf1.printSchema()","de7d145f":"#Method 2:\nfrom pyspark.sql.functions import col\ndf2=df.withColumn(\"Salary\",col(\"Salary\").cast(\"String\"))\ndf2.printSchema()","57d7b424":"df1=df.withColumn(\"ID\",df[\"ID\"]+50)\ndf1.show(5)","102bac3e":"df2=df.withColumn(\"ID\",col(\"ID\") *20)\ndf2.show(5)","ecafa5c9":"df1=df.withColumn(\"New_ID\",df[\"ID\"]+10)\ndf1.show(5)","c2875161":"df2=df.withColumn(\"New_ID\",col(\"ID\") *10)\ndf2.show(5)","ab1b1daa":"from pyspark.sql.functions import lit\ndf1=df.withColumn(\"Country\", lit(\"   India   \"))\ndf1.show(5)","ee358755":"df1=df1.withColumnRenamed(\"gender\",\"sex\")\ndf1.show(5)","eedffeee":"from pyspark.sql.functions import ltrim,rtrim,trim\ndf2=df1.withColumn(\"Country\",ltrim(\"Country\")) #left trim\ndf2.show(3)","ba7187aa":"df3=df1.withColumn(\"Country\",rtrim(\"Country\")) #right trim\ndf3.show(3)","91bbdd99":"df4=df1.withColumn(\"Country\",trim(\"Country\"))\ndf4.show(3)","8de933de":"from pyspark.sql.functions import current_date\ndf1 = df.withColumn(\"current_date\",current_date())\ndf1.show(5)","19cb3ce8":"from pyspark.sql.functions import when\ndf_when = df.withColumn(\"new_col\", when(df.Salary < 50000,\"low\")\n                                 .when(df.Salary >=50 ,\"high\")\n                                 .when(df.Salary.isNull() ,\"\")\n                                 .otherwise(df.Salary))\ndf_when.show()","4a55f574":"### Timestamp","b2a05c90":"## 6.Trim a column","f4911ef3":"## 1.Changing Data types","b9e0182c":"## 2. Updating the value of an existing column","59b5f060":"### Lit function is use to add a value to the new column","d9da6959":"## 3. Create a new Column from an Existing","4e44b40e":"### I am changing the salary data type from Integer to string and putting it into another dataframe.In the first method I used the dataframe to fetch the column that I want to cast and in the second method I have used col","abd8c14d":"## 4. Create a fresh new column","ac9dd8e4":"### Using When()","4d78f9fc":"## 5. Rename a column","98f0adc3":"### you can see I have left whitespace before and after India in country column. Lets trim this","b0f1b44b":"## PySpark withColumn() is a transformation function which is used to UPDATE the value of an existing, convert the DATATYPE of an existing column, create a new column. trim columns"}}