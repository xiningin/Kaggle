{"cell_type":{"7ae283bd":"code","df281457":"code","696c77b6":"code","92492d99":"code","e125eec3":"code","1b7ad7da":"code","8e16f137":"code","cfd530a0":"code","b7ef47cb":"code","a62f7b6e":"code","55aa1173":"code","be2623ad":"code","a73d9dce":"code","da286eef":"code","f355a4a0":"code","47f425d6":"code","1eac2b6b":"code","68c276ea":"code","04633714":"code","20170d37":"code","f7bd8efb":"code","9274d151":"code","9c7c83ca":"code","5a92dc94":"code","9e5643ae":"code","c8b02d10":"code","75a54937":"code","34e46f1e":"code","60e2e5e5":"code","9dfe218e":"code","f9036eb9":"code","f5aaf549":"code","ed62c5a0":"code","8043cec6":"code","c126cd2e":"code","e1379446":"markdown","90a4ce97":"markdown","abfd7123":"markdown","2b600ac7":"markdown","72812553":"markdown","6ba8b784":"markdown","f5b44d9e":"markdown","c37b98f0":"markdown","b7d361c7":"markdown"},"source":{"7ae283bd":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport datetime as dt\nplt.style.use('ggplot')","df281457":"train = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/train.csv',parse_dates=['datetime'])\ntest = pd.read_csv('\/kaggle\/input\/bike-sharing-demand\/test.csv',parse_dates=['datetime'])","696c77b6":"train.head()","92492d99":"test.head()","e125eec3":"import missingno as msno\nmsno.matrix(train)","1b7ad7da":"msno.matrix(test)","8e16f137":"train.info()","cfd530a0":"train['year'] = train['datetime'].dt.year\ntrain['month'] = train['datetime'].dt.month\ntrain['day'] = train['datetime'].dt.day\ntrain['hour'] = train['datetime'].dt.hour\ntrain['weekday'] = train['datetime'].dt.weekday\ntrain.head()","b7ef47cb":"fig,axes = plt.subplots(2,2,figsize=(12,8))\nsns.barplot(x=train['year'],y=train['count'],ax=axes[0,0],ci=False)\nsns.barplot(x=train['month'],y=train['count'],ax=axes[0,1],ci=False)\n#sns.barplot(x=train['day'],y=train['count'],ax=axes[1,0],ci=False)\nsns.barplot(x=train['hour'],y=train['count'],ax=axes[1,0],ci=False)\nsns.barplot(x=train['weekday'],y=train['count'],ax=axes[1,1],ci=False)\naxes[0,0].set(title='Bike sharing count by year')\naxes[0,1].set(title='Bike sharing count by month')\naxes[1,0].set(title='Bike sharing count by hour')\naxes[1,1].set(title='Bike sharing count by weekday')\nplt.tight_layout()","a62f7b6e":"fig, axes = plt.subplots(nrows=2,ncols=2,figsize=(12,8))\n\nsns.boxplot(data=train,y=\"count\",orient=\"v\",ax=axes[0][0])\nsns.boxplot(data=train,y=\"count\",x=\"season\",orient=\"v\",ax=axes[0][1])\nsns.boxplot(data=train,y=\"count\",x=\"hour\",orient=\"v\",ax=axes[1][0])\nsns.boxplot(data=train,y=\"count\",x=\"workingday\",orient=\"v\",ax=axes[1][1])\n\naxes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\naxes[0][1].set(xlabel='Season', ylabel='Count',title=\"Box Plot On Count Across Season\")\naxes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\naxes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")","55aa1173":"plt.figure(figsize=(12,8))\ncorr_matrix = train[[\"temp\",\"atemp\",\"casual\",\"registered\",\"humidity\",\"windspeed\",\"count\"]].corr()\nmask = np.zeros_like(corr_matrix)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr_matrix,mask=mask,vmax=.8,square=True,annot=True)","be2623ad":"fig,(ax1,ax2,ax3) = plt.subplots(ncols=3,figsize=(12,8))\nsns.regplot(x=train['temp'],y=train['count'],ax=ax1)\nsns.regplot(x=train['humidity'],y=train['count'],ax=ax2)\nsns.regplot(x=train['windspeed'],y=train['count'],ax=ax3)","a73d9dce":"fig,(ax1,ax2,ax3,ax4,ax5) = plt.subplots(nrows=5,figsize=(15,25))\nsns.pointplot(data=train,x='hour',y='count',ax=ax1)\nsns.pointplot(data=train,x='hour',y='count',hue='workingday',ax=ax2)\nsns.pointplot(data=train,x='hour',y='count',hue='weekday',ax=ax3)\nsns.pointplot(data=train,x='hour',y='count',hue='season',ax=ax4)\nsns.pointplot(data=train,x='hour',y='count',hue='weather',ax=ax5)","da286eef":"from scipy.stats import norm, skew\nfrom scipy import stats\nplt.style.use('seaborn')\nsns.distplot(train['count'] , fit=norm)\nmu, sigma = norm.fit(train['count'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('Count distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nstats.probplot(train['count'],plot=plt)\nplt.show()","f355a4a0":"sns.distplot(np.log1p(train['count']),fit=norm)\nmu,sigma = norm.fit(np.log1p(train['count']))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')\nplt.ylabel('Frequency')\nplt.title('count distribution')\nfig=plt.figure()\nstats.probplot(np.log1p(train['count']),fit=True,plot=plt)\nplt.show()","47f425d6":"data= train.append(test)\nwindColumns = [\"season\",\"weather\",\"humidity\",\"month\",\"temp\",\"year\",\"atemp\"]\ndata['year'] = data['datetime'].dt.year\ndata['month'] = data['datetime'].dt.month\ndata['day'] = data['datetime'].dt.day\ndata['hour'] = data['datetime'].dt.hour\ndata['weekday'] = data['datetime'].dt.weekday\ndata['dayofweek'] = data['datetime'].dt.dayofweek","1eac2b6b":"from xgboost import XGBRegressor\nX = data[data['windspeed']!=0]\ny = data[data['windspeed']==0]\nwind_train_x = X[windColumns]\nwind_train_y = X['windspeed']\nwind_test_x = y[windColumns]\nwind_test_y_idx = y['windspeed'].index\nxgb=XGBRegressor()\nxgb.fit(wind_train_x,wind_train_y)\npred = xgb.predict(wind_test_x)","68c276ea":"y['windspeed'] = pred\ndata = X.append(y).sort_values('datetime')","04633714":"data[data['windspeed'] == 0]","20170d37":"plt.figure(figsize=(12,8))\nsns.countplot(np.round(data['windspeed']))\nplt.xticks(rotation=60)\nplt.title('windspeed countplot(int)')","f7bd8efb":"category_features = [\"season\",\"holiday\",\"workingday\",\"weather\",\"weekday\",\"year\"]\nfor i in category_features:\n    data[i] = data[i].astype('category')\nfinal_train = data[data['count'].notnull()]\nfinal_test = data[data['count'].isnull()]\ntrain_x = final_train.drop('count',axis=1)\ntrain_y = final_train['count']\ntest_x = final_test.drop('count',axis=1)\ndatetime = test_x.datetime","9274d151":"drop_feat = ['datetime','day','casual','registered']\ntrain_x.drop(drop_feat,axis=1,inplace=True)\ntest_x.drop(drop_feat,axis=1,inplace=True)","9c7c83ca":"dummy_train_x = pd.get_dummies(train_x)\ndummy_test_x = pd.get_dummies(test_x)","5a92dc94":"from sklearn.metrics import mean_squared_log_error\ndef rmsle(pred_y,test_y):    \n    return np.sqrt(mean_squared_log_error(test_y,pred_y))","9e5643ae":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import RidgeCV,LassoCV,ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm.sklearn import LGBMRegressor","c8b02d10":"alpha_las=[0.0005,0.0001,0.00005,0.00001]\ne_ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nlasso = make_pipeline(RobustScaler(),LassoCV(alphas=alpha_las,random_state=42,max_iter=1e7))\nridge = make_pipeline(RobustScaler(),RidgeCV(alphas = alpha_las))\nelastic = make_pipeline(RobustScaler(),ElasticNetCV(max_iter=1e7,alphas=alpha_las,l1_ratio = e_ratio))\nrf = RandomForestRegressor(bootstrap=True,max_depth=70,max_features='auto',min_samples_leaf=4,min_samples_split=10,n_estimators=2200)\ngra = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nlgbm = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nada = AdaBoostRegressor(n_estimators=2200,random_state=42,learning_rate=0.05)","75a54937":"model = [lasso,ridge,elastic,rf,gra,xgb,lgbm,ada]\nmodel_name = ['Lasso','Ridge','ElasticNet','RandomForest','GradientBoost','XGBoost','LGBM','Ada']\ntmp = pd.DataFrame(columns=['model','rmsle'])\nidx=0\nfor i,j in zip(model,model_name):\n    train_y_log = np.log1p(train_y)\n    i.fit(dummy_train_x,train_y_log)\n    pred = i.predict(dummy_train_x)\n    tmp.loc[idx,'model'] = j\n    tmp.loc[idx,'rmsle'] = rmsle(train_y_log,pred)\n    idx+=1\ntmp = tmp.sort_values(by= 'rmsle')\ntmp","34e46f1e":"from mlxtend.regressor import StackingCVRegressor\nstack = StackingCVRegressor(regressors=(gra,xgb,lgbm,ada),\n                           meta_regressor=rf,use_features_in_secondary=True)","60e2e5e5":"train_y_log = np.log1p(train_y)\nstack.fit(np.array(dummy_train_x),np.array(train_y_log))\npred = stack.predict(np.array(dummy_train_x))\ntmp.loc[idx,'model'] = 'Stack'\ntmp.loc[idx,'rmsle'] = rmsle(train_y_log,pred)\ntmp = tmp.sort_values('rmsle')","9dfe218e":"tmp['rmsle2'] = tmp['rmsle'].map('{:.4f}'.format)","f9036eb9":"plt.figure(figsize=(12,8))\nax = fig.add_subplot()\nplt.plot(tmp['model'],tmp['rmsle2'])\nfor i,j in zip(tmp['model'],tmp['rmsle2']):\n#     ax.annotate(str(j),xy=(i,j))\n    plt.text(i, j, str(j),fontsize=15)\nplt.xticks(rotation=60)\nplt.title('RMSLE score by model(train data)')\nplt.show()","f5aaf549":"def blend_model(X):\n    return (0.3*rf.predict(X))+(0.25*stack.predict(np.array(X)))+(0.2*gra.predict(X))+(0.1*xgb.predict(X))+(0.1*lgbm.predict(X))+(0.05*ada.predict(X))","ed62c5a0":"pred = blend_model(dummy_train_x)\nprint(rmsle(train_y_log,pred))","8043cec6":"# final = pd.DataFrame(columns=['datetime','count'])\n# final['datetime'] = datetime\n# final['count'] = np.exp(blend_model(dummy_test_x))\n# final.to_csv('blend bike submission.csv',index=False)\n#0.41146","c126cd2e":"final = pd.DataFrame(columns=['datetime','count'])\nfinal['datetime'] = datetime\nfinal['count'] = np.exp(stack.predict(np.array(dummy_test_x)))\nfinal.to_csv('rf bike submission.csv',index=False)\n# 0.404","e1379446":"Windspeed has too many 0 values.\n\nMy guess is that the unmeasured value goes into zero.\n\n\nI will adjust these values through xgboostRegressor later","90a4ce97":"\n- I will modify the 0 value of WindSpeed.","abfd7123":"# EDA","2b600ac7":"\nYou can see that it has been normalized even a little. Therefore, log1p should be taken for count later.","72812553":"- stacking\n    - Three models with a large Rmsle score will not be included in the stacking.","6ba8b784":"- temp and atemp have strong correlation\n- registered and conut have strong correlation.","f5b44d9e":"- Daytime use on weekends There are many use of commute hours on weekdays.\n- It is also affected by the weather, and the better the weather, the more it is used.","c37b98f0":"# Modeling","b7d361c7":"Normalization of the dependent variable is desirable for regression, so log1p is used to normalize it."}}