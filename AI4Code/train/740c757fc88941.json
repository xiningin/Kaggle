{"cell_type":{"bf01700c":"code","f428c67d":"code","55c9989f":"code","94a5e349":"code","0fb7ca25":"code","3b329e32":"code","e472a9f6":"code","f31f31ce":"code","289b7374":"code","f54175ae":"code","0149d5ea":"code","90a309d2":"code","48e96ada":"code","348cafde":"code","a494c0c2":"code","d176254d":"code","21d04704":"code","e4d68fa7":"code","3e4ea6e7":"code","1fc260e2":"code","16e06dca":"code","eb46de5c":"code","ffb2d417":"code","fed7f180":"code","63f8da0b":"code","5452f34c":"code","7d625b35":"code","3effdf03":"code","b67d8b4c":"code","63b925ef":"markdown","16e57817":"markdown","ad73c088":"markdown","03e89a52":"markdown","15236146":"markdown","f90dfc5d":"markdown","c0581742":"markdown","e0c4b438":"markdown","cdd7a89c":"markdown","ed8268fd":"markdown","8c23d64d":"markdown","98af69b6":"markdown","0050935b":"markdown","63eff5ad":"markdown","865c43c8":"markdown","2799914d":"markdown"},"source":{"bf01700c":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f428c67d":"pip install evalml","55c9989f":"import evalml\nimport numpy as np\nimport pandas as pd","94a5e349":"data = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndata.head()","0fb7ca25":"print(data.info())","3b329e32":"data.describe()","e472a9f6":"data.shape","f31f31ce":"data = data.drop(['CLIENTNUM'], axis=1)","289b7374":"for feature in data.columns:\n    if data[feature].dtype not in ['int64', 'float64']:\n        print(f'{feature}: {data[feature].unique()}')","f54175ae":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(nrows=3, ncols=1, figsize=(16, 28))\nsns.set(font_scale=1.6)\ncols_ = [\"Education_Level\", \"Marital_Status\", \"Income_Category\"]\n\nfor ind, col in enumerate(cols_):\n    sns.countplot(x=col, data=data, ax=ax[ind])","0149d5ea":"data.columns","90a309d2":"data.drop(['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1','Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'], axis = 1, inplace = True)","48e96ada":"fig, ax = plt.subplots(figsize=(20, 16))\ndf_corr = data.corr(method=\"pearson\")\nmask = np.zeros_like(np.array(df_corr))\nmask[np.triu_indices_from(mask)] = True\nax = sns.heatmap(df_corr, mask=mask, annot=True)","348cafde":"data.columns","a494c0c2":"data['Attrition_Flag'].value_counts()","d176254d":"X = data.copy()\nX = X.drop(['Credit_Limit'], axis=1) # dropping Credit Limit since it is highly correlated with Avg_Open_To_Buy\ny = X.pop('Attrition_Flag')\n\nX['Income_Category'] = X['Income_Category'].replace({'Less than $40K':0,\n                                                     '$40K - $60K':1,\n                                                     '$60K - $80K':2,\n                                                     '$80K - $120K':3,\n                                                     '$120K +':4})\nX['Card_Category'] = X['Card_Category'].replace({'Blue':0,\n                                                 'Silver':1,\n                                                 'Gold':2,\n                                                 'Platinum':3})\nX['Education_Level'] = X['Education_Level'].replace({'Uneducated':0,\n                                                     'High School':1,\n                                                     'College':2,\n                                                     'Graduate':3,\n                                                     'Post-Graduate':4,\n                                                     'Doctorate':5})","21d04704":"y = y.replace({'Existing Customer':0,\n               'Attrited Customer':1})","e4d68fa7":"from evalml.pipelines.components.transformers.imputers.simple_imputer import SimpleImputer\n\ndef preprocessing(X, y):\n    imputer = SimpleImputer(impute_strategy=\"most_frequent\", missing_values=\"Unknown\")\n    X = imputer.fit_transform(X, y)\n    return X\n\nX = preprocessing(X, y)","3e4ea6e7":"from evalml.utils import infer_feature_types","1fc260e2":"X = infer_feature_types(X, feature_types={'Income_Category': 'categorical',\n                                          'Education_Level': 'categorical'})\nX","16e06dca":"X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y, problem_type='binary',test_size=.2)","eb46de5c":"from evalml import AutoMLSearch\n\nautoml = AutoMLSearch(X_train=X_train, y_train=y_train, problem_type=\"binary\", objective=\"F1\", \n                      allowed_model_families=['random_forest' , 'xgboost', 'lightgbm'],\n                      additional_objectives=['accuracy binary'], max_batches=5)\nautoml.search()","ffb2d417":"automl.rankings","fed7f180":"best_pipeline_ = automl.best_pipeline\nautoml.describe_pipeline(automl.rankings.iloc[1][\"id\"])","63f8da0b":"best_pipeline_.fit(X_train, y_train)\npredictions = best_pipeline_.predict(X_test)","5452f34c":"from evalml.model_understanding.graphs import (\n    graph_binary_objective_vs_threshold, \n    graph_permutation_importance, \n    graph_confusion_matrix\n)\n\ngraph_binary_objective_vs_threshold(best_pipeline_, X_test, y_test, \"F1\")","7d625b35":"graph_permutation_importance(best_pipeline_, X_test, y_test, \"F1\")","3effdf03":"graph_confusion_matrix(y_test, predictions)","b67d8b4c":"from evalml.objectives.standard_metrics import AccuracyBinary, AUC, F1, PrecisionWeighted, Recall\n\nacc = AccuracyBinary()\nauc = AUC()\nf1 = F1()\npre_w = PrecisionWeighted()\nrec = Recall()\n\nprint(f\"Accuracy (Binary): {acc.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"Area Under Curve: {auc.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"F1: {f1.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"Precision (Weighted): {pre_w.score(y_true=y_test, y_predicted=predictions)}\")\nprint(f\"Recall: {rec.score(y_true=y_test, y_predicted=predictions)}\")","63b925ef":"**Education_Level, Marital_Status, and Income_Category have Unknown as a value. This is something we'll have to remember before we get to the model training, since Unknown isn't an acceptable value for any of the features.**","16e57817":"**Checking to see how prevalent Unknown is proportionally to the the other values. Based on the count plots above, it doesn't look like Unknown is the most common value, but it's frequency is high enough that we probably don't want to drop rows containing it altogether.**","ad73c088":"**Replacing the Unknown values that we saw earlier with the most frequent value encountered in that feature using SimpleImputer.**","03e89a52":"**We are getting an F1-score of 0.88 on the test set which is pretty good.**","15236146":"**Total Trans Ct is giving us the highest permutation importance score followed by Total Trans Amt.**","f90dfc5d":"**We got the best classifier with LightGBM Classifier.**","c0581742":"**We are getting (1685+273) = 1958 correct observations and (52+16) = 68 incorrect observations.**","e0c4b438":"**The target feature is imbalanced so we will consider F1-score as our metric.**","cdd7a89c":"**Encoding**","ed8268fd":"**We're also going to take a look at the correlation matrix to see if there are any features that are too closely tied to others. It looks like Avg_Open_To_Buy is perfectly correlated with Credit_Limit, so we're going to drop the latter.**","8c23d64d":"**Obtaining the complete pipeline of the best model**","98af69b6":"**Encoding the Target feature**","0050935b":"**The first thing we'll do is drop CLIENTNUM from the data since a unique client identifier will have no correlation with attrition rates. Now there's clearly some diversity in the types of features, and at first glace it looks like we don't have to worry about any null or missing values. But that seems unlikely with a dataset of this size.**","63eff5ad":"**Pipelines Review**\n\nSo a lot just happened, let's review the pipelines that were created and tested. We can see that the best performing pipeline was with the LightGBM estimator. We want to learn a little more about it, which can be done with the describe_pipeline function. Notice that the pipeline included a preprocessing step of imputation. In this case, it ended up being unnecessary because of our earlier SimpleImputer and our lack of null values for our numerical features. However AutoMLSearch comes with the built-in capacity to automatically iterate over the hyperparameters for this preprocessing step as well.","865c43c8":"**Initializing AutoMLSearch from EvalML**","2799914d":"**Splitting the dataset into 80% train and 20% test.**"}}