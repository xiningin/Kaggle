{"cell_type":{"4a0ad28e":"code","805c08fb":"code","c4a93127":"code","fa779fa7":"code","e3229c14":"code","34771549":"code","7b286a8c":"code","a9856214":"code","8d556721":"code","6a38ed4f":"code","6e0e2974":"code","7bf649b9":"code","c6f611c0":"code","aab5d176":"code","bc70fdbc":"code","2a729653":"code","081ee291":"code","6f50190d":"code","7e1d7665":"code","db4581b2":"code","60c4940e":"code","f7df3baf":"code","e27e0353":"code","79d36ff5":"code","9a6e5300":"code","92755e1e":"code","a9ed4fcc":"code","527b0a16":"code","2dacbe36":"code","89e3af92":"code","98e73a53":"code","5c61df7e":"code","404083b0":"code","2f21ea88":"code","3b260299":"code","b5880243":"code","08671f17":"code","b906eae5":"code","3a0577f7":"code","ab3f462c":"code","4724e596":"code","6a536bf1":"code","9b88d088":"code","606112f6":"code","aeb8dd83":"code","aaae1aa9":"code","ad09e4ac":"code","fdea0543":"code","04808f3e":"code","89dd5fe8":"code","003a88ff":"code","d2739661":"code","25a88949":"code","7140d67f":"code","e8f4f79c":"code","cb7ce01b":"code","315c1e5e":"code","cf4441cb":"code","12c7255f":"code","4c0da869":"code","5d7eceff":"code","9c90de1d":"code","af7732c9":"code","e4fadd42":"code","1658d0af":"code","363b08eb":"code","ed328314":"code","775aca69":"code","ba4af4c1":"code","7d37446f":"code","e7ce2cfb":"code","93eafc9e":"code","4f0e70b1":"code","c7021962":"code","2221ce23":"code","36f676e6":"code","cb142b42":"markdown","2eb05844":"markdown","1f36f71a":"markdown","e2042b0a":"markdown","73e82c8d":"markdown","5d419605":"markdown","3f42c383":"markdown","c4c22562":"markdown","380e972f":"markdown","92b52e5c":"markdown","31569952":"markdown","669e1cde":"markdown","abb31aa5":"markdown","c0e1c2f4":"markdown","916f4162":"markdown","834c6f01":"markdown","027baa43":"markdown","12171c86":"markdown","0227fb31":"markdown","16c03829":"markdown","96fe6891":"markdown","13b50e78":"markdown","21015c9b":"markdown"},"source":{"4a0ad28e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport skimage\nimport seaborn as sn\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\nfrom keras import backend as BK\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import layers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\ntf.compat.v1.set_random_seed(200)\nnp.random.seed(200)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfiles=[]\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        files.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","805c08fb":"\ntrain = tf.keras.preprocessing.image_dataset_from_directory(\n  '..\/input\/leaf-counting-dataset-by-teimouri\/training\/training',\n  validation_split=0.2,\n  subset=\"training\",\n    label_mode='categorical',\n  image_size=(128,128),\n  batch_size=128,\nseed =123)\n\nval = tf.keras.preprocessing.image_dataset_from_directory(\n  '..\/input\/leaf-counting-dataset-by-teimouri\/training\/training' ,\n  validation_split=0.2,\n  subset=\"validation\",\n    label_mode='categorical',\n  image_size=(128, 128),\n  batch_size=128,\nseed=123)","c4a93127":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    '..\/input\/leaf-counting-dataset-by-teimouri\/testing\/testing',\n    color_mode='rgb',\n    image_size=(128,128),\n  label_mode='categorical',\n  batch_size=128,\n    seed=123\n)","fa779fa7":"# training_images = []\n# testing_images = []\n# training_images_normal = []\n# testing_images_normal = []\n# labels = []\n# test_labels = []\n# # Function reading the images and assign labels through passing it the path and the label\n# def reading_data(path, label = labeling, images = \"training\") :\n#     filelist = glob.glob(path +'*.png')\n\n# # Loop through the images, read them in and check if an image is equal to your source\n#     if images == 'training':\n        \n#         for file in filelist:\n#             img = io.imread(file) \n# #             img_resized= transform.resize(img,(32,32,3))\n#             training_images.append(img)\n#             labels.append(label)\n#             training_images_normal.append(img)\n#     elif images == 'testing':     \n#         for file in filelist:\n#             img = io.imread(file)\n# #             img_resized = transform.resize(img,(32,32,3))\n#             testing_images.append(img)\n#             test_labels.append(label)\n","e3229c14":"def display_20_image(images):\n \n    plt.figure(figsize=(10, 10))\n    for images, labels in images.take(5):\n        for i in range(20):\n            ax = plt.subplot(5, 4, i + 1)\n            plt.imshow(images[i].numpy().astype(\"uint8\"))\n            plt.axis(\"off\")\n\n    plt.show()","34771549":"display_20_image(train)","7b286a8c":"X_train = np.concatenate([x for x,y in train], axis =0)\ny_train= np.concatenate([y for x,y in train], axis =0)\nX_val = np.concatenate([x for x,y in val], axis =0)\ny_val= np.concatenate([y for x,y in val], axis =0)\n","a9856214":"from tensorflow.keras.applications import VGG16\n \nvgg_model = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(128,128, 3))","8d556721":"vgg_model.layers[6].output","6a38ed4f":"for layer in vgg_model.layers:\n    layer.trainable = False\n\n# We will have to use the functional API    \n\n# last layers output\nx = vgg_model.layers[6].output\n# Flatten as before\nx= Conv2D(500, kernel_size=5, activation='relu', padding='same')(x)\nx= MaxPooling2D(pool_size=(2, 2))(x)\nx= Conv2D(300, kernel_size=3, activation='relu', padding='same')(x)\nx= MaxPooling2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\nx = Dense(300, activation = \"relu\")(x)\nx = Dense(200, activation = \"relu\")(x)\nx = Dense(5, activation='relu')(x)\n\nfrom tensorflow.keras.models import Model\nvgg_model_transfer = Model(inputs=vgg_model.input, outputs=x)\n","6e0e2974":"batchSize = 80\nnEpochs = 70\n#callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)\nopt = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\nvgg_model_transfer.compile(loss='categorical_crossentropy', \n                           optimizer=opt, \n                           metrics=['accuracy'])\nhistory = vgg_model_transfer.fit(X_train, y_train, batch_size=batchSize, epochs=nEpochs, verbose=1,\n                                 validation_data=(X_val,y_val))\n","7bf649b9":"print(vgg_model_transfer.summary())","c6f611c0":"X_test = np.concatenate([x for x,y in test_ds], axis =0)\ny_test= np.concatenate([y for x,y in test_ds], axis =0)","aab5d176":"y_predict_prob = vgg_model_transfer.predict(X_test)\ny_predict = y_predict_prob.argmax(axis=-1)","bc70fdbc":"y_predict_prob_val = vgg_model_transfer.predict(X_val)\ny_predict_val = y_predict_prob_val.argmax(axis=-1)","2a729653":"y_predict_prob_train = vgg_model_transfer.predict(X_train)\n\ny_predict_train = y_predict_prob_train.argmax(axis=-1)","081ee291":"print(classification_report(y_train.argmax(axis=-1),y_predict_train))","6f50190d":"sn.heatmap(confusion_matrix(y_train.argmax(axis=-1),y_predict_train), annot=True)","7e1d7665":"print(classification_report(y_val.argmax(axis=-1),y_predict_val));","db4581b2":"sn.heatmap(confusion_matrix(y_val.argmax(axis=-1),y_predict_val), annot=True)","60c4940e":"print(classification_report(y_test.argmax(axis=-1),y_predict))","f7df3baf":"conf_mx = confusion_matrix(y_test.argmax(axis=-1), y_predict)\nsn.heatmap(conf_mx, annot=True)","e27e0353":"# dictionary keys seems to have changed in version 2\nk = ''\nif 'accuracy' in history.history :\n    k = 'accuracy'    \n\nif 'acc' in history.history :\n    k = 'acc'\n    \nif k != '' :    \n    plt.plot(history.history[k])\n    plt.plot(history.history['val_'+k])\n    plt.title('Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    ","79d36ff5":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n","9a6e5300":"y_train_mapped = {0:1, 1:2, 2:3,3:4, 4:6}\ny_train= np.concatenate([y for x,y in train], axis =0).argmax(axis=-1)\ny_val= np.concatenate([y for x,y in val], axis =0).argmax(axis=-1)\ny_ts = y_test.argmax(axis=-1)","92755e1e":"y_t = np.array([y_train_mapped[key] for key in y_train]).astype(float)\ny_v = np.array([y_train_mapped[key] for key in y_val]).astype(float)\ny_ts = np.array([y_train_mapped[key] for key in y_ts]).astype(float)","a9ed4fcc":"vgg_regression =VGG16(\n    include_top=False, weights='imagenet', \n    input_shape=(128,128,3),classifier_activation=\"linear\"\n)","527b0a16":"for layer in vgg_regression.layers:\n    layer.trainable = False\n\n# We will have to use the functional API    \nx = vgg_regression.layers[6].output\n# last layers output\nx= Conv2D(500, kernel_size=5, activation='tanh', padding='same')(x)\nx= MaxPooling2D(pool_size=(2, 2))(x)\nx= Conv2D(300, kernel_size=3, activation='relu', padding='same')(x)\nx= MaxPooling2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\nx = Dense(300, activation = \"relu\")(x)\nx = Dense(100, activation = \"relu\")(x)\nx = Dense(1, activation='relu6')(x)\nfrom tensorflow.keras.models import Model\nvgg_regression = Model(inputs=vgg_regression.input, outputs=x)\n\n","2dacbe36":"batchSize = 80\nnEpochs = 70\ncallback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=5)\nopt = tensorflow.keras.optimizers.SGD(learning_rate=0.0001)\nvgg_regression.compile(loss='mean_squared_error', \n                           optimizer=opt, \n                           metrics=['mean_squared_error'])\nhistory_regreesion = vgg_regression.fit(X_train, y_t, batch_size=batchSize, epochs=nEpochs, verbose=1 ,validation_data=(X_val,y_v))\n","89e3af92":"print(vgg_regression.summary())","98e73a53":"y_predict = vgg_regression.predict(X_test)","5c61df7e":"y_val_predict = vgg_regression.predict(X_val)","404083b0":"y_predict_train = vgg_regression.predict(X_train)","2f21ea88":"y_predict","3b260299":"print(mean_squared_error(y_predict_train, y_t, squared=False))\n","b5880243":"mean_squared_error(y_val_predict, y_v, squared=False)","08671f17":"mean_squared_error(y_predict, y_ts, squared=False)","b906eae5":"y_predict = y_predict.astype(int)\ny_val_predict = y_val_predict.astype(int)\ny_predict_train = y_predict_train.astype(int)","3a0577f7":"print(classification_report(y_predict, y_ts))","ab3f462c":"print(classification_report(y_predict_train, y_t))","4724e596":"print(classification_report(y_predict_val, y_v))","6a536bf1":"# dictionary keys seems to have changed in version 2\nk = ''\nif 'mean_squared_error' in history_regreesion.history :\n    k = 'mean_squared_error'    \n\nif 'mse' in history_regreesion.history :\n    k = 'mse'\n    \nif k != '' :    \n    plt.plot(history_regreesion.history[k])\n    plt.plot(history_regreesion.history['val_'+k])\n    plt.title('Model MSE')\n    plt.ylabel('MSE')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","9b88d088":"X_train = np.concatenate([x for x,y in train], axis =0)\ny_train= np.concatenate([y for x,y in train], axis =0)\nX_val = np.concatenate([x for x,y in val], axis =0)\ny_val= np.concatenate([y for x,y in val], axis =0)","606112f6":" vgg_model_regualized = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(128, 128, 3))","aeb8dd83":"for layer in vgg_model_regualized.layers:\n    layer.trainable = False\n\n# We will have to use the functional API    \n\n# last layers output\nx = vgg_model_regualized.layers[6].output\nx= Conv2D(500, kernel_size=5, activation='relu', padding='same')(x)\nx= MaxPooling2D(pool_size=(2, 2))(x)\nx= Conv2D(300, kernel_size=3, activation='relu', padding='same')(x)\nx= MaxPooling2D(pool_size=(2, 2))(x)\nx = Flatten()(x)\nx = Dropout(0.3)(x)\nx = Dense(300, activation = \"relu\")(x)\nx = BatchNormalization()(x)\nx = Dense(200, activation = \"relu\")(x)\nx = Dropout(0.02)(x)\nx = Dense(5, activation='relu')(x)\nfrom tensorflow.keras.models import Model\nvgg_model_transferL = Model(inputs=vgg_model_regualized.input, outputs=x)\n","aaae1aa9":"batchSize = 80\nnEpochs = 70\nsgd = tensorflow.keras.optimizers.Adam(learning_rate=0.0001)\nvgg_model_transferL.compile(loss='categorical_crossentropy', \n                           optimizer=sgd, \n                           metrics=['accuracy'])\nhistoryL = vgg_model_transferL.fit(X_train, y_train, batch_size=batchSize, epochs=nEpochs, verbose=1,\n                                 validation_data=(X_val, y_val))\n","ad09e4ac":"print(vgg_model_transferL.summary())","fdea0543":"y_predict_probL = vgg_model_transferL.predict(X_test)\ny_predictL = y_predict_probL.argmax(axis=-1)","04808f3e":"y_predict_probL_train = vgg_model_transferL.predict(X_train)\ny_predictL_train = y_predict_probL_train.argmax(axis=-1)","89dd5fe8":"y_predict_probL_val = vgg_model_transferL.predict(X_val)\ny_predictL_val = y_predict_probL_val.argmax(axis=-1)","003a88ff":"print(classification_report(y_predictL_train, y_train.argmax(-1)))","d2739661":"sn.heatmap(confusion_matrix(y_predictL_train, y_train.argmax(-1)),annot=True)","25a88949":"print(classification_report(y_predictL_val, y_val.argmax(axis=-1)))","7140d67f":"sn.heatmap(confusion_matrix(y_predictL_val, y_val.argmax(-1)),annot=True)","e8f4f79c":"print(classification_report(y_test.argmax(axis=-1), y_predictL))","cb7ce01b":"conf_mx = confusion_matrix(y_test.argmax(axis=-1), y_predictL)\nsn.heatmap(conf_mx, annot= True)","315c1e5e":"# dictionary keys seems to have changed in version 2\nk = ''\nif 'accuracy' in historyL.history :\n    k = 'accuracy'    \n\nif 'acc' in historyL.history :\n    k = 'acc'\n    \nif k != '' :    \n    plt.plot(historyL.history[k])\n    plt.plot(historyL.history['val_'+k])\n    plt.title('Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n","cf4441cb":"plt.plot(historyL.history['loss'])\nplt.plot(historyL.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","12c7255f":"datagen = ImageDataGenerator(\n        rotation_range=10, # rotation\n        width_shift_range=0.2, # horizontal shift\n        height_shift_range=0.2, # vertical shift\n        zoom_range=0.2, # zoom\n        horizontal_flip=True, # horizontal flip\n        brightness_range=[0.2,1.2])","4c0da869":"train_generator = datagen.flow_from_directory(\n                  directory='..\/input\/leaf-counting-dataset-by-teimouri\/training\/training',\n                  target_size=(128, 128), # resize to this size\n                  color_mode=\"rgb\", # for coloured images\n                  batch_size=1, # number of images to extract from folder for every batch\n                  class_mode=\"binary\", # classes to predict\n                  seed=2020 # to make the result reproducible\n)","5d7eceff":"fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(15,15))\n\nfor i in range(4):\n\n  # convert to unsigned integers for plotting\n  image = next(train_generator)[0].astype('uint8')\n\n  # changing size from (1, 200, 200, 3) to (200, 200, 3) for plotting the image\n  image = np.squeeze(image)\n\n  # plot raw pixel data\n  ax[i].imshow(image)\n  ax[i].axis('off')","9c90de1d":"datagen = ImageDataGenerator(rescale=1\/255.,validation_split=0.2)","af7732c9":"X_train = np.concatenate([X_train,X_val])\ny_train = np.concatenate([y_train,y_val])","e4fadd42":"len(X_train)","1658d0af":"training_generator = datagen.flow(X_train, y_train, batch_size=64,subset='training',seed=7)\nvalidation_generator = datagen.flow(X_train, y_train, batch_size=64,subset='validation',seed=7)","363b08eb":"len(training_generator)","ed328314":"historyL = vgg_model_transferL.fit_generator(training_generator,steps_per_epoch=(len(X_train)*0.8)\/\/64, epochs=70, \n                                            validation_data=validation_generator, validation_steps=(len(X_train)*0.2)\/\/64)","775aca69":"print(vgg_model_transferL.summary())","ba4af4c1":"y_predict_probL = vgg_model_transferL.predict(test_ds)\ny_predictL = y_predict_probL.argmax(axis=-1)","7d37446f":"y_predict_probL_train = vgg_model_transferL.predict(X_train)\ny_predictL_train = y_predict_probL_train.argmax(axis=-1)","e7ce2cfb":"y_predict_probL_val = vgg_model_transferL.predict(X_val)\ny_predictL_val = y_predict_probL_val.argmax(axis=-1)","93eafc9e":"print(classification_report(y_train.argmax(axis=-1),y_predictL_train))\nconf_mx = confusion_matrix(y_train.argmax(axis=-1),y_predictL_train)\nsn.heatmap(conf_mx, annot= True)","4f0e70b1":"print(classification_report(y_val.argmax(axis=-1),y_predictL_val))\nconf_mx = confusion_matrix(y_val.argmax(axis=-1),y_predictL_val)\nsn.heatmap(conf_mx, annot= True)","c7021962":"print(classification_report(y_test.argmax(axis=-1),y_predictL))\nconf_mx = confusion_matrix(y_test.argmax(axis=-1),y_predictL)\nsn.heatmap(conf_mx, annot= True)","2221ce23":"# dictionary keys seems to have changed in version 2\nk = ''\nif 'accuracy' in historyL.history :\n    k = 'accuracy'    \n\nif 'acc' in historyL.history :\n    k = 'acc'\n    \nif k != '' :    \n    plt.plot(historyL.history[k])\n    plt.plot(historyL.history['val_'+k])\n    plt.title('Model Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n","36f676e6":"plt.plot(historyL.history['loss'])\nplt.plot(historyL.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","cb142b42":"# Overall Discussion\n**The model wasn't complex or big enough to learn the data so it was underfitting so when reguralization was added it didn't help the model imporving at all and after trying data augmentation it didn't help although the model to learn because the generated sample wasn't big enough for the model to learn, there must be more layers to add and to generate bigger sample this may help improving the model accuracy**","2eb05844":"# Plotting Photo","1f36f71a":"# Loading and labeling the images","e2042b0a":"**Mean Square Error for validation Data**","73e82c8d":"# Regulaization","5d419605":"**The optimizer was changed here because Adam Optimizer couldn't perform well and MSE was stucked at point and the model wasn't able to learn**","3f42c383":"# Accuracy and loss Graph Discussion \n* **Model Accuracy Started from 0.19 and kept increaseing over time reaching it's max at the end scoring 0.35 but overall the model ended up with bad accuracy recording 0.21 accruacy on training data, 0.22 on vaidation and 0.20 on testing data**\n* **Loss over epoches testing loss oscailated from 1.6 raching it's peak on epoch no 46 scoring 2.8 and then settled down to 1.7 and on training data the loss curve started from 2.2 then it ended up with the same number 1.5**\n* **Overall the model isn't learning well as it perfom bad on both Training, Validation and Testing Data**","c4c22562":"# Spliting Data into Tran and Validation ","380e972f":"# Accuracy and loss Graph Discussion \n* **Model Accuracy Started from 0.21 and reached it's peak on epoch No 59 as 0.34 but then it Fall down till it reach 0.3486 but overall the model ended up with bad accuracy recording 0.33 accruacy on training data, 0.13 on vaidation and 0.21 on testing data**\n* **Loss over epoches training loss started from 8.5 faling down till 6.9 and on testing data the loss curve started from 7.9 and kept oscaliated around the same range till it setteled down at 7.5**\n* **Overall the model isn't learning well as it perfom bad on both Training, Validation and Testing Data**","92b52e5c":"# Regression using VGG-16","31569952":"# Discussing the MSE curve and loss Curve \n* **Model started training process recordering 6.1 and started to decrease till it reached 2.4 root mean squared error and on testing data the model started with 2.9 and at epoch 45 it recorded it's heighest RMSE 5.5 but then it settled around to 3.1 but this all didn't help the model to converge or coming near to the optimal MSE to consider it as a good model**","669e1cde":"# Training and Validation Confusion Metrics in order\n","abb31aa5":"# Classification Task","c0e1c2f4":"# Choosing between the two model\n**I will choose Classification as the regression model it came out with better result however the two models is doing bad on both test and training data but classifcation came with better results even if it was slight better accuracies however away from the accuracy metric because it maybe misleading one the regression model may came out with results in the mean region of the actual points which will cause a good MSE but it will be misleading metric to use.**","916f4162":"**Mean Square Error for Test Data**","834c6f01":"# VGG-16","027baa43":"**Mean Square Error for Train Data**","12171c86":"# Converting the MSE to accuracy as rounding float outputs to int number and plot the confusion and classifcation report","0227fb31":"# Model Accuracy and Confusion Mertics Disucssion\n* **As Shown in the confusion matrix and the reoport for Training Data the model can't recognize the classes well on training data model cannot converge and learn the classes so the model final accuracy on training data accorcding to the classifcation report is 31% so this mean that the model is underfitting it cannot perform well on training data**\n* **As Shown in the confusion matrix and the reoport for valiation data the model can't recognize the classes well on validation data during the training process and recoreded bad accruacies 0.13 during the training process and model cannot converge on validation data**\n* **When it came to testing the data the model socred 18% accuracy and couldn't recocgnize the actual classes according to the confusion Matrix shown above, the model only recognized 18 right classes from 100 classes the model is obviusly underfitting and isn't doing well whether on the training, validation or testing data**","16c03829":"# Mean Squared Error breaking Down \n* **Model Scored 1.42 on training data this mean that the squared distance between the actual points and the predicted ones is far and the model is also underfitting as a regression model too**\n* **Model scored 1.77 on validation data and couldn't converge and recorded a bad RMSE**\n* **Model scored RMSE on testing data 1.87 on the testing data as bad RMSE**","96fe6891":"# Data Augmentation","13b50e78":"# Model Accuracy and Confusion Mertics Disucssion\n* **As Shown in the confusion matrix and the reoport for Training Data the model can't recognize the classes well on training data model cannot converge and learn the classes so the model final accuracy on training data accorcding to the classifcation report is 21% so this mean that the model is underfitting it cannot perform well on training data**\n* **As Shown in the confusion matrix and the reoport for valiation data the model scored 22% accuracy on validation data during the training process and recoreded bad accruacies during the training process and model cannot converge on validation data**\n* **When it came to testing the data the model socred 20% accuracy and couldn't recocgnize the actual classes according to the confusion Matrix shown above, the model only recognized 21 right classes from 100 classes the model is obviusly underfitting and isn't doing well whether on the training, validation or testing data**\n* **Data Augmentation didn't help in imorving the accuracy over the training data as the the model wasn't able to learn well but the validation data increased to 0.22 and test data accuracy decreased to 0.20**","21015c9b":"# Validating the model on Train, Test, Validation Data"}}