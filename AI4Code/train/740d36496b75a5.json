{"cell_type":{"1e63c493":"code","52d0fb51":"code","7d55b3cc":"code","fc1d33b6":"code","d4a04d81":"code","e0dd39ef":"code","c0b392d0":"code","ac7d2a91":"code","41853e52":"code","21eabdf4":"code","b08f4d94":"code","65ece671":"code","9250c382":"code","80491673":"code","4e8c795d":"code","c4b31d80":"code","cb8b79e0":"code","314333a4":"code","2da73981":"code","88cbefc1":"code","8f76010f":"code","567645a9":"code","98a2f1d2":"code","7da48a21":"code","4b213ab2":"markdown","536245d5":"markdown","fcf1659c":"markdown","12599a1a":"markdown","b7fe618a":"markdown","1810f7b3":"markdown"},"source":{"1e63c493":"!pip install xgboost","52d0fb51":"import pandas as pd\nimport numpy as np","7d55b3cc":"df_train=pd.read_csv('train.csv')","fc1d33b6":"X_train = df_train[df_train.columns[:-1]]\ny_train = df_train['SalePrice']","d4a04d81":"#Converts all int to float\ncolumn_types = dict(X_train.dtypes)\nfor k,v in column_types.items():\n    if str(v) == 'int64':\n        X_train[k] = X_train[k].astype('float64')","e0dd39ef":"#read the test DF based on train DF data types\ncolumn_types = dict(X_train.dtypes)\ndf_test=pd.read_csv('test.csv', dtype=column_types)\nX_test = df_test","c0b392d0":"#Check column types\nfor k,v in dict(X_train.dtypes == X_test.dtypes).items():\n    if v==False:\n        print(\"Column %s with diff data types\" %k)","ac7d2a91":"null_columns=X_train.columns[X_train.isnull().any()]\nX_train[null_columns].isnull().sum()","41853e52":"null_columns=X_test.columns[X_test.isnull().any()]\nX_test[null_columns].isnull().sum()","21eabdf4":"X_train = X_train.fillna(0)\nX_test = X_test.fillna(0)","b08f4d94":"#instead of one hot encoding, i'im going to convert the categorical values to numbers.\nfor k, v in dict(X_train.dtypes).items():\n    if v=='object':\n        X_train[k] = pd.factorize(X_train[k])[0] + 1\n        X_test[k] = pd.factorize(X_test[k])[0] + 1","65ece671":"#remove Ids from datasets\nX_train_set = X_train[X_train.columns[1:]]\nX_test_set = X_test[X_test.columns[1:]]","9250c382":"print('%s - %s' %(X_train_set.shape, X_test_set.shape))","80491673":"from sklearn import preprocessing\n\nX_train_set_values = X_train_set.values \nmin_max_scaler = preprocessing.MinMaxScaler()\nX_train_scaled = min_max_scaler.fit_transform(X_train_set_values)\nX_train_scaled = pd.DataFrame(X_train_scaled)\n\n#transform test set\nX_test_set_values = X_test_set.values\nX_test_scaled = min_max_scaler.transform(X_test_set_values)\nX_test_scaled = pd.DataFrame(X_test_scaled)","4e8c795d":"print('%s - %s' %(X_train_set.shape, X_test_set.shape))","c4b31d80":"import xgboost as xgb","cb8b79e0":"def report_best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","314333a4":"xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)","2da73981":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint\n\nxgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=10, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_train_set, y_train)\n\nreport_best_scores(search.cv_results_, 1)","88cbefc1":"params = {'colsample_bytree': 0.8275467623473733, 'gamma': 0.10397083143409441, 'learning_rate': 0.20031009834599744, 'max_depth': 2, 'n_estimators': 117, 'subsample': 0.9100531293444458}\nxgb_model_final = xgb.XGBRegressor(objective=\"reg:squarederror\", params=params)\nxgb_model_final.fit(X_train_scaled, y_train)","8f76010f":"y_test = xgb_model_final.predict(X_test_scaled)","567645a9":"#add predict to X_test, which has Id for submission\ny_test = pd.DataFrame(y_test)\nX_test_final = X_test\nX_test_final['SalePrice'] = y_test","98a2f1d2":"X_test_final['Id'] = X_test_final['Id'].astype('int64')\nX_test_final[['Id','SalePrice']].head()","7da48a21":"X_test_final[['Id','SalePrice']].to_csv('submission.csv',index=False)","4b213ab2":"## Model","536245d5":"## Test Phase - Final Model","fcf1659c":"### Null Columns","12599a1a":"## Feature Phase","b7fe618a":"A big problem in this project is that train and test data has different compositions. For example, there is no NA rows in train, but there is in the test, which makes pandas identify different data types.\nSo, my strategy is first convert all columns int to float and check if train-test dataframes has the same dtypes","1810f7b3":"### Train Dataframes"}}