{"cell_type":{"7bdbd0c2":"code","ede555e3":"code","7dd50a94":"code","5f290070":"code","6eb90be5":"code","92e2c4dc":"code","19d5372c":"code","25bc88b2":"code","85bc5eb7":"code","8114c1ec":"code","d62c61c4":"code","33b1a765":"code","1a62ce0a":"code","2338f4f7":"code","3e76b5b6":"code","db7e97c7":"code","7de26802":"code","9f43282b":"code","a0a18cc2":"code","7e49b3ac":"code","5d98d76d":"code","18c867f2":"code","ea55925e":"code","ef927d22":"code","e3ee1b34":"code","645aa7d5":"markdown","bd75ddbb":"markdown","613c1347":"markdown","611cb8c2":"markdown","9c48245a":"markdown","e3467032":"markdown","485c2318":"markdown","6ddc3e09":"markdown","427bef32":"markdown"},"source":{"7bdbd0c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# !pip uninstall opencv-contrib-python opencv-python -y\n# !pip install opencv-contrib-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input1'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ede555e3":"import os\nos.environ['OPENCV_IO_ENABLE_JASPER'] = 'true'\nimport cv2\nimport matplotlib.pyplot as plt","7dd50a94":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom pathlib import Path\nfrom collections import Counter\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","5f290070":"# Path to the data directory\n\n#data_dir = Path(\".\/captcha_images_v2\/\")\n#data_dir = Path(\"..\/input\/mnist-5-digit-gen\")\n#data_dir = Path(\"..\/input\/mnist-gen-with-preprocess\")\ndata_dir = Path(\"..\/input\/train870\") \n\n# Get list of all the images\nimages = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\n#for x in images:\nfor i in reversed(range(len(images))):\n    #if '*' in images[0]:\n    if len(images[i].split('\/')[-1]) < 9:\n        #images.remove(x)\n        del images[i]\ndata_dir = Path(\"..\/input\/mnist-gen-with-preprocess\")\nimages += sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\n\n#data_dir = Path(\"..\/input\/mnist-gen-with-preprocess\")\n#images = sorted(list(map(str, list(data_dir.glob(\"*.jpg\")))))\nlabels = [img.split(os.path.sep)[-1].split(\".jpg\")[0] for img in images]\ncharacters = set(char for label in labels for char in label)\n\nprint(\"Number of images found: \", len(images))\nprint(\"Number of labels found: \", len(labels))\nprint(\"Number of unique characters: \", len(characters))\nprint(\"Characters present: \", sorted(characters))\n\n# Batch size for training and validation\nbatch_size = 16\n\n# Desired image dimensions\nimg_width = 200\nimg_height = 50\n\n# Factor by which the image is going to be downsampled\n# by the convolutional blocks. We will be using two\n# convolution blocks and each block will have\n# a pooling layer which downsample the features by a factor of 2.\n# Hence total downsampling factor would be 4.\ndownsample_factor = 4\n\n# Maximum length of any captcha in the dataset\nmax_length = max([len(label) for label in labels])\n","6eb90be5":"def getDigitRect(img_arr):\n    ImgW = img_arr.shape[1]\n    ImgH = img_arr.shape[0]\n    newHeight = 100\n    Ratio = ImgH\/newHeight\n    newWidth = round(ImgW\/Ratio)\n    img_arr = cv2.resize(img_arr, (newWidth, newHeight),interpolation=cv2.INTER_AREA   ) #cv2.INTER_AREA)    \n    image_gray = img_arr\n\n    image_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n    image_gray = cv2.adaptiveThreshold(image_gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,5,5)\n    contours, _ = cv2.findContours(image_gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    sumh = 0\n    sumw = 0\n    cnt = 0\n    for i in range(len(contours)):\n            x,y,w,h = cv2.boundingRect(contours[i])\n            if h<5 or w<5:\n                continue\n            sumh += h\n            sumw += w\n            cnt += 1\n    if(cnt<=0):\n      return 0,0,ImgW,ImgH\n    avgh = sumh\/cnt\n    avgw = sumw\/cnt\n\n    sumh = 0\n    sumw = 0\n    cnt = 0\n    for i in range(len(contours)):\n            x,y,w,h = cv2.boundingRect(contours[i])\n            if h<avgh*0.7 or w<avgw*0.7:\n                continue\n            sumh += h\n            sumw += w\n            cnt += 1\n    if(cnt<=0):\n      return 0,0,ImgW,ImgH\n    avgh = sumh\/cnt\n    avgw = sumw\/cnt\n\n    x1 = y1 = 9999\n    x2 = y2 = -1\n    for i in range(len(contours)):\n            x,y,w,h = cv2.boundingRect(contours[i])\n            if h<avgh*0.7 or w<avgw*0.7:\n                continue\n            if x1>x:\n                x1 = x\n            if x2<x+w:\n                x2=x+w\n            if y1>y:\n                y1 = y\n            if y2 < y+h:\n                y2 = y+h\n    if(x1>=x2 or y1>=y2):\n      return 0,0,ImgW,ImgH\n\n    gap = (y2-y1)\/3\n    x1 -= gap\n    y1 -= gap\n    x2 += gap\n    y2 += gap\n\n    h = y2-y1\n\n    w = h * (200\/\/50)\n\n    if x1 > ((x1+x2)-w)\/2:\n        x1 = ((x1+x2)-w)\/2\n    if x2 < x1+w:\n        x2 = x1+w\n\n    x1 = max(0,round(x1*Ratio))\n    y1 = max(0,round(y1*Ratio))\n    x2 = min(ImgW,round(x2*Ratio))\n    y2 = min(ImgH,round(y2*Ratio))\n        \n    return x1,y1,x2,y2","92e2c4dc":"def drawXY(Img1,y,x,Img2):\n  #print(\"x,y=\",x,y)\n  Img1[x:x+Img2.shape[0],y:y+Img2.shape[1]] = Img2\n  return Img1  \n\ndef AutoCrop(img_arr):\n    x1,y1,x2,y2 = getDigitRect(img_arr)\n    return img_arr[y1:y2, x1:x2]\n\ndef preprocessImage(img_arr,blockSize=25,C=11):\n    ImgW = img_arr.shape[1]\n    ImgH = img_arr.shape[0]\n    newHeight = 50\n    Ratio = ImgH\/newHeight\n    newWidth = min(200,round(ImgW\/Ratio))\n    img_arr = cv2.resize(img_arr, (newWidth, newHeight),interpolation=cv2.INTER_AREA   ) #cv2.INTER_AREA)    \n\n    image_gray = cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\n    image_gray = cv2.adaptiveThreshold(image_gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,blockSize,C)#5,2)\n\n    if newWidth<200:\n        fullImage = np.zeros((50, 200), dtype=np.uint8)\n        image_gray = drawXY(fullImage,(200-newWidth)\/\/2,0,image_gray)\n    return image_gray\/255\n\ndef LoadAndPreprocessImage(fname,blockSize=21,C=9): #21 9  25 11\n    img_arr = AutoCrop(cv2.imread(fname))\n    return preprocessImage(img_arr,blockSize,C),img_arr","19d5372c":"#image_gray = LoadAndPreprocessImage('..\/input\/super-ai-engineer-2021-handwritten-digit\/TestSet1\/09a22f176f571194a03fb19ba049c1b4.jpg',3,2)\nfname = '..\/input\/super-ai-engineer-2021-handwritten-digit\/SmallTest\/288e4b2cd18e12c4e11d2c4cb74c2c30.jpg'\nimage_gray = cv2.imread(fname)\nplt.imshow(image_gray)","25bc88b2":"img_process,img_crop =  LoadAndPreprocessImage(fname)\nplt.imshow(img_crop)","85bc5eb7":"plt.imshow(img_process, cmap=\"gray\")","8114c1ec":"# Mapping characters to integers\nchar_to_num = layers.StringLookup(\n    vocabulary=list(characters), mask_token=None\n)\n\n# Mapping integers back to original characters\nnum_to_char = layers.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n)\n\ndef split_data(images, labels, train_size=0.80, shuffle=True): #0.75\n    # 1. Get the total size of the dataset\n    size = len(images)\n    # 2. Make an indices array and shuffle it, if required\n    indices = np.arange(size)\n    if shuffle:\n        np.random.shuffle(indices)\n    # 3. Get the size of training samples\n    train_samples = int(size * train_size)\n    test_size = 1 #(size-train_samples) \/\/ 2\n    # 4. Split data into training and validation sets\n    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n    x_valid, y_valid = images[indices[train_samples:-test_size]], labels[indices[train_samples:-test_size]]\n    x_test, y_test = images[indices[-test_size:]], labels[indices[-test_size:]]\n    return x_train, x_valid, x_test, y_train, y_valid, y_test\n\n\n# Splitting data into training and validation sets\nx_train, x_valid, x_test, y_train, y_valid, y_test = split_data(np.array(images), np.array(labels))\n\nprint(\"Number of train: \", len(x_train))\nprint(\"Number of valid: \", len(x_valid))\nprint(\"Number of test: \", len(x_test))\n\ndef encode_single_sample(img_path, label):\n    # 1. Read image\n    img = tf.io.read_file(img_path)\n    #print(\"shape\",img.shape)\n    # 2. Decode and convert to grayscale\n    img = tf.io.decode_png(img, channels=1)\n    # 3. Convert to float32 in [0, 1] range\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # 4. Resize to the desired size\n    img = tf.image.resize(img, [img_height, img_width])\n    # 5. Transpose the image because we want the time\n    # dimension to correspond to the width of the image.\n    img = tf.transpose(img, perm=[1, 0, 2])\n    # 6. Map the characters in label to numbers\n    \n#     img = LoadAndPreprocessImage(img_path).T\n    \n    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n    # 7. Return a dict as our model is expecting two inputs\n    return {\"image\": img, \"label\": label}\n","d62c61c4":"\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_dataset = (\n    train_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\nvalidation_dataset = (\n    validation_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_dataset = (\n    test_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)","33b1a765":"\n_, ax = plt.subplots(4, 4, figsize=(10, 5))\nfor batch in train_dataset.take(1):\n    images = batch[\"image\"]\n    labels = batch[\"label\"]\n    for i in range(16):\n        img = (images[i] * 255).numpy().astype(\"uint8\")\n        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n        ax[i \/\/ 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n        ax[i \/\/ 4, i % 4].set_title(label)\n        ax[i \/\/ 4, i % 4].axis(\"off\")\nplt.show()","1a62ce0a":"\nclass CTCLayer(layers.Layer):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.loss_fn = keras.backend.ctc_batch_cost\n\n    def call(self, y_true, y_pred):\n        # Compute the training-time loss value and add it\n        # to the layer using `self.add_loss()`.\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n        self.add_loss(loss)\n\n        # At test time, just return the computed predictions\n        return y_pred\n\n\ndef build_model():\n    # Inputs to the model\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n\n    # First conv block\n    x = layers.Conv2D(\n        32,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(x)\n    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n\n    # We have used two max pool with pool size and strides 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64. Reshape accordingly before\n    # passing the output to the RNN part of the model\n    new_shape = ((img_width \/\/ 4), (img_height \/\/ 4) * 64)\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n    x = layers.Dropout(0.2)(x)\n\n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n\n    # Output layer\n    x = layers.Dense(\n        len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n    )(x)\n\n    # Add CTC layer for calculating CTC loss at each step\n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n\n    # Define the model\n    model = keras.models.Model(\n        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n    )\n    # Optimizer\n    opt = keras.optimizers.Adam()\n    # Compile the model and return\n    model.compile(optimizer=opt)\n    return model\n\n\n# Get the model\nmodel = build_model()\nmodel.summary()","2338f4f7":"\nepochs = 50 #100\nearly_stopping_patience = 10\n# Add early stopping\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    callbacks=[early_stopping],\n)\n","3e76b5b6":"model.save('ModelTrained.h5')\n#model2 = load_model('Captcha870gen.h5')\n#model2 = build_model()\n#model2.load_weights('Captcha870genOk3.h5') #RobCpC630-435w.h5\n","db7e97c7":"# prediction_model2 = keras.models.Model(\n#     model2.get_layer(name=\"image\").input, model2.get_layer(name=\"dense2\").output\n# )\n# prediction_model2.summary()","7de26802":"\n# Get the prediction model by extracting layers till the output layer\nprediction_model = keras.models.Model(\n    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n)\nprediction_model.summary()\n\n# A utility function to decode the output of the network\ndef decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search\n    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n        :, :max_length\n    ]\n    # Iterate over the results and get back the text\n    output_text = []\n    for res in results:\n        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n        output_text.append(res)\n    return output_text","9f43282b":"\n#  Let's check results on some validation samples\nfor batch in validation_dataset.take(1):\n    batch_images = batch[\"image\"]\n    print(batch_images.shape)\n    print(batch_images[0][0][0][0])\n    batch_labels = batch[\"label\"]\n\n    preds = prediction_model.predict(batch_images)\n    pred_texts = decode_batch_predictions(preds)\n\n    orig_texts = []\n    for label in batch_labels:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        orig_texts.append(label)\n\n    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n    for i in range(len(pred_texts)):\n        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n        img = img.T\n        title = f\"Prediction: {pred_texts[i]}\"\n        ax[i \/\/ 4, i % 4].imshow(img, cmap=\"gray\")\n        ax[i \/\/ 4, i % 4].set_title(title)\n        ax[i \/\/ 4, i % 4].axis(\"off\")\nplt.show()","a0a18cc2":"for batch in test_dataset.take(1):\n    batch_images = batch[\"image\"]\n    print(batch_images.shape)\n    print(batch_images[0][0][0][0])\n    batch_labels = batch[\"label\"]\n\n    preds = prediction_model.predict(batch_images)\n    pred_texts = decode_batch_predictions(preds)\n\n    orig_texts = []\n    for label in batch_labels:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        orig_texts.append(label)\n\n    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n    for i in range(len(pred_texts)):\n        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n        img = img.T\n        title = f\"Prediction: {pred_texts[i]}\"\n        ax[i \/\/ 4, i % 4].imshow(img, cmap=\"gray\")\n        ax[i \/\/ 4, i % 4].set_title(title)\n        ax[i \/\/ 4, i % 4].axis(\"off\")\nplt.show()","7e49b3ac":"img_size = (200,50)\ndef get_imgdata(data_dir):\n    data = [] \n    for datapath in datasets: \n        path = os.path.join(data_dir, datapath)\n        #class_num = labels.index(label)\n        for img in sorted(os.listdir(path)):\n            try:\n#                 img_arr = cv2.imread(os.path.join(path, img))#, cv2.IMREAD_GRAYSCALE)\n#                 img_arr = 1-cv2.cvtColor(img_arr, cv2.COLOR_BGR2GRAY)\/255\n                img_id = os.path.splitext(img)[0]\n#                 resized_arr = cv2.resize(img_arr, img_size) # Reshaping images to preferred size\n                resized_arr,img_crop = LoadAndPreprocessImage(os.path.join(path, img))\n                data.append([resized_arr, img_id,img_crop])\n            except Exception as e:\n                print(e)\n    return np.array(data)","5d98d76d":"#datasets = [''] #MyTest\n#datatests = get_imgdata('..\/input\/mnist-5-digit-gen')\ndatasets = ['SmallTest','TestSet1','TestSet2'] #MyTest TestSet1 SmallTest\ndatatests = get_imgdata('..\/input\/super-ai-engineer-2021-handwritten-digit')\ntestimages = np.array([x.T for x in datatests[:, 0]])\npreds = prediction_model.predict(testimages)\npred_texts = decode_batch_predictions(preds)\npred_texts = np.array([ x.replace('[UNK]','') for x in pred_texts])\n\norig_texts = []\nfor label in batch_labels:\n  label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n  orig_texts.append(label)\n\n_, ax = plt.subplots(12, 4, figsize=(15, 20))\nfor i in range(len(pred_texts)):\n    if ( i >= 48):\n      break\n    img = datatests[i,2] #(batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n    title = f\"Prediction: {pred_texts[i]}\"\n    ax[i \/\/ 4, i % 4].imshow(img, cmap=\"gray\")\n    ax[i \/\/ 4, i % 4].set_title(title)\n    ax[i \/\/ 4, i % 4].axis(\"off\")\nplt.show()","18c867f2":"dfResult = pd.DataFrame(data=datatests[:,1],columns=[\"Id\"])\n\ndfResult['Predict'] = pred_texts\ndfResult\n","ea55925e":"result = pd.read_csv(\"..\/input\/super-ai-engineer-2021-handwritten-digit\/sample_submission.csv\", dtype=str)\nresult","ef927d22":"result = result.merge(dfResult, on='Id', how='left')\nresult","e3ee1b34":"result[[\"Id\",\"Predict\"]].rename(columns={'Predict': 'Predicted'}).to_csv(\"submission.csv\", index=False) ","645aa7d5":"# **Show Data**","bd75ddbb":"# **Training**","613c1347":"# **Predict**","611cb8c2":"# **Model**","9c48245a":"End","e3467032":"This code is modified from \nhttps:\/\/colab.research.google.com\/github\/keras-team\/keras-io\/blob\/master\/examples\/vision\/ipynb\/captcha_ocr.ipynb","485c2318":"# **Text Captcha Classification**\n","6ddc3e09":"# **Load captcha data**\n\n## Load the data: [Captcha Images](https:\/\/www.kaggle.com\/fournierp\/captcha-version-2-images)\nLet's download the data.\n\n","427bef32":"# **Create Dataset objects**\n"}}