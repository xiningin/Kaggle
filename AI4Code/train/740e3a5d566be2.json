{"cell_type":{"0da4680f":"code","038eb7e2":"code","2350607e":"code","f2cc8848":"code","88aa5d9b":"code","a4b818e4":"code","be518618":"code","de75d6e5":"code","f536057a":"code","7b4d2fd8":"code","0b9c9022":"code","aa602d7f":"code","003c2d1a":"code","a1d8f9ce":"code","dfa0d2ba":"code","4b743ab0":"code","fc4c78fb":"code","08050ab9":"code","629bdfb3":"code","136c7f42":"code","df2fef09":"code","ff672808":"code","acc88318":"code","b6d112cf":"markdown","e0ef727c":"markdown","516645c2":"markdown","0da67254":"markdown","2945fc83":"markdown","33697e76":"markdown"},"source":{"0da4680f":"# Import packages\nimport numpy as np # Handling matrices\nimport pandas as pd # Data processing\nimport matplotlib.pyplot as plt # Plotting\nimport seaborn as sns # Plotting \nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder # Handling categorical data and normalization\nfrom sklearn.model_selection import train_test_split, cross_val_score # Split data in train and test and CV\nfrom sklearn.metrics import roc_auc_score,precision_score,confusion_matrix, accuracy_score, roc_curve, f1_score # Several useful metrics\nfrom xgboost import XGBClassifier # XGB model\nfrom sklearn.pipeline import Pipeline # Connect processes\nfrom sklearn.compose import ColumnTransformer # Capable apply transformer to columns\n\n# Set matplotlib configuration\n%matplotlib inline\nplt.style.use('seaborn')","038eb7e2":"# Import data\ndata = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\nprint(\"This dataset contains: {} rows and {} columns\".format(data.shape[0],data.shape[1]))\ndata.head()","2350607e":"# Review the type of each feature\ndata.dtypes","f2cc8848":"# Count the type of features\ndata.dtypes.value_counts()\nprint('This dataset contains {} categorical features'.format(data.dtypes.value_counts()[0]))\nprint('This dataset contains {} numerical features'.format(data.dtypes.value_counts()[1]))\n\n# Id and target are the unique integer features","88aa5d9b":"# Analyse missing values\ndata.isna().sum()\n\n# Do not have missing values","a4b818e4":"# Identify categorical features\ncat = (data.dtypes == 'object')\ncat_cols = list(cat[cat].index)\nprint(cat_cols)\n\n# Create a handful of plots\nfor cols in cat_cols:\n    plt.figure(figsize=(8,4));\n    sns.countplot(x = data[cols]);","be518618":"# Create a list of numerical_cols\nnumerical_cols = [cname for cname in data.columns if data[cname].dtype in ['float64']]\n\n# Also, we can see how numerical features are related with the target\ndata[numerical_cols].hist(bins=15, figsize=(20, 14), layout=(7, 3));","de75d6e5":"# Analyse our target colum\ndata['target'].hist(bins=15, figsize=(12,6));\n\n# We observe that our data is unbalanced. This is an important point.","f536057a":"# Separate independent features of target\ny = data['target']\nX = data.drop(['id','target'],axis = 1)","7b4d2fd8":"# Divide data into training and validation subsets. We stratify data by output classes.\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, \n                                                                test_size=0.2,random_state = 123,stratify = y)","0b9c9022":"# Print proportion of entire dataset\nprint(\"Proportion of classes in entire data: \")\nprint(100. * y.value_counts() \/ len(y),\"\\n\")\n\n# Print proportion of train and test sets \nprint(\"Proportion of classes in train data: \")\nprint(100. * y_train.value_counts() \/ len(y_train),\"\\n\")\nprint(\"Proportion of classes in valid data: \")\nprint(100. * y_valid.value_counts() \/ len(y_valid))","aa602d7f":"# Identify categorical columns with relatively low cardinality (low number of unique values)\ncategorical_cols_O = [cname for cname in X_train.columns if X_train[cname].nunique() < 12 and \n                    X_train[cname].dtype == \"object\"]\n\n# Identify categorical columns with high cardinality\ncategorical_cols_L = [cname for cname in X_train.columns if X_train[cname].nunique() >= 12 and \n                    X_train[cname].dtype == \"object\"]\n\n# Identify numerical columns\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]","003c2d1a":"# Preprocessing\n\n# To categorical columns with low cardinality\ncategorical_O_transformer = OneHotEncoder(handle_unknown = 'ignore')\n\n# To categorical columns with high cardinality\ncategorical_L_transformer = OrdinalEncoder(handle_unknown = 'use_encoded_value',\n                                          unknown_value = -99)\n\n# To numerical columns\nnumerical_transformer = MinMaxScaler()\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat_O', categorical_O_transformer, categorical_cols_O),\n        ('cat_L', categorical_L_transformer, categorical_cols_L),\n        ('num', numerical_transformer, numerical_cols)\n    ])","a1d8f9ce":"# Creation of a model\nmodel = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=4,use_label_encoder = False,\n                     objective = \"binary:logistic\",eval_metric = \"auc\")","dfa0d2ba":"# Bundle preprocessing and modeling code in a pipeline\npipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","4b743ab0":"# Preprocessing of training data, fit model \npipeline.fit(X_train, y_train)","fc4c78fb":"# Preprocessing of validation data, get predictions\ny_pred = pipeline.predict_proba(X_valid)\n\n# Consider our output has two columns (one per each class)","08050ab9":"# Function to plot ROC curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\n# Create plot\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred[:, [1]])\nplot_roc_curve(fpr, tpr)","629bdfb3":"# Create Confusion Matrix\npred_class = y_pred[:, [1]] > 0.5\npred_class = pred_class.astype(int)\ncm = confusion_matrix(y_valid, pred_class)\nprint(\"Confusion matrix: \\n\",cm,\"\\n\")\n\n# Get accuracy\naccuracy = round(accuracy_score(y_valid,pred_class),4)\nprint(\"Accuracy: {}\".format(accuracy),\"\\n\")\n\n# Get f1 score (it is required on the Task 1 of this dataset)\nf1 = f1_score(y_valid,pred_class)\nprint(\"F1: {}\".format(f1),\"\\n\")\n","136c7f42":"# Load test data\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")\ntest.head()\n\n# Remove id\nX_test = test.drop(\"id\",axis = 1)","df2fef09":"# Prediction on the valid set\ntest_pred=pipeline.predict_proba(X_test)","ff672808":"# Create submission file\noutput = test[['id']].copy()\npositive_class = test_pred[:,[1]]\n\noutput['target'] = pd.Series(positive_class.flatten(), index=output.index)\noutput.head()","acc88318":"# write csv\noutput.to_csv(\"submissionv_XGBoost.csv\",index = False)","b6d112cf":"# 4) Write results","e0ef727c":"# 2) Create a model\n\nUsing the recommendation given by the tutorial of [Intermediate ML](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables) of Kaggle, we apply different methods to categorical features, which have less than 12 unique values. For these features we will aply One-Hot Encoding, while features with 12 or more unique values, we will apply Ordinal Encoding. ","516645c2":"# 1) Review and analysis of data","0da67254":"We can see that our categorical and numeric features have different behaviours. We have categorical features with low and high number of classes, while our numerical feature are different distributions. ","2945fc83":"# Application of XGBoost\n\nWe will perform a XGBoost model to generate an accuracy prediction to submit in the [Tabular Playground Series - Mar 2021 Competition](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021). XGBoost is one of the most useful models in Kaggle and we go to probe in a competition. ","33697e76":"# 3) Use model in test set"}}