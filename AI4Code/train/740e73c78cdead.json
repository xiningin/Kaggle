{"cell_type":{"dee78b54":"code","ee144cc9":"code","4d09d238":"code","777a36be":"code","da2e22b5":"code","d3dfea01":"code","69f1c90d":"code","73446ab3":"code","b3e07627":"code","1cae47da":"markdown","196c6a52":"markdown"},"source":{"dee78b54":"# Importing utilities and ML libraries\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport hashlib\nimport numpy as np\nimport sys\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import PCA","ee144cc9":"# Taking a first look at the data\ndata = pd.read_csv('..\/input\/bank-marketing-data-set\/bank_marketing_dataset.csv')\nlen_data = len(data)\ndata = data.drop(columns=['duration']) # dropping the \"benchmark\" feature\ndata","4d09d238":"# First of all, it's necessary to check our data for missing values and fill NAs with mode of there are any;\n# luckily, there are no NAs, but this step is essential anyway\nfor col in data.columns:\n  if data[col].isna().any():\n    mode = data[col].mode().iloc[0]\n    print(f\"{col} contains NAs, replacing with {mode}\")\n    data[col] = data[col].fillna(mode)","777a36be":"# We should convert 'yes\/no' string features to binary ones\nfor bin_col in ['default', 'housing', 'loan', 'subscribed']:\n    data[bin_col+'_bin'] = [1 if y == 'yes' else 0 for y in data[bin_col].values]\ndata = data.drop(columns=['default', 'housing', 'loan', 'subscribed'])    ","da2e22b5":"# Before clustering, we'll convert all the features to numerical type\n# Let's apply one hot encoding (if the number of unique values is relatively small)\n# or hashing if there are many uniques\ncols_to_drop = []\n\nfor col in data.columns:\n  if data[col].dtype == 'object':\n    print(f'Column {col} has {data[col].nunique()} values among {len_data}')\n\n    if data[col].nunique() < 25:\n      print(f'One-hot encoding of {col}')\n      one_hot_cols = pd.get_dummies(data[col])\n      for ohc in one_hot_cols.columns:\n        data[col + '_' + ohc] = one_hot_cols[ohc]\n    else:\n      print(f'Hashing of {col}')\n      data[col + '_hash'] = data[col].apply(lambda row: int(hashlib.sha1((col + \"_\" + str(row)).encode('utf-8')).hexdigest(), 16) % len_data)\n\n    cols_to_drop.append(col)\n    \ndata = data.drop(columns=cols_to_drop)","d3dfea01":"# There may be highly correlated features;\n# having highly correlated features won't help our model, so let's drop them\n# We drop the features that have correlation coefficient between 0.9 and 1\n# We don't include 1 as every feature has CC=1 with itself\n# Code idea is taken from\n# https:\/\/stackoverflow.com\/questions\/17778394\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ncorr = data.corr()\ncorr_top = corr.abs().unstack().sort_values(kind='quicksort')\ncorr_top = corr_top[corr_top > 0.9][corr_top < 1]\n\ncols_to_drop = [corr_top.index[i][0] for i in range(0, len(corr_top), 2)]\nprint(f\"Highly correlated features: {cols_to_drop}\")\ndata = data.drop(columns=cols_to_drop)","69f1c90d":"# Rescaling the data to make all the dimensions more or less equally ranged\nss = StandardScaler()\ndata_scaled = pd.DataFrame(columns=data.columns,\n                               data=ss.fit_transform(data))","73446ab3":"# That's how our data finally looks\ndata_scaled","b3e07627":"# We're going to create 2 clusters: the experiments with more clusters\n# 'behind the scene' didn't show the performance growth with more than 2 clusters\n# We'll apply two popular clustering algorithms (K Means and DBSCAN) and combine them with PCA\n# that tries to represent the data in smaller number of orthogonal dimensions\n# We'll add more dimensions iteratively and stop if the metric doesn't improve\n# during 5 additions\nmax_silhouette = -1 # maximum silhouette coefficient that we hsve reached\ntol_iters = 0 # number of iterations during which the metric improvement didn't happen\nalgos = ['KMeans', 'DBSCAN'] # algo names (for output)\nbest_algo = '' # best algo name (for output)\nbest_k = 0 # best number of dimensions\nearly_stopping_iters = 5 # max number of iterations with no improvement\n\nfor k in range(2, data_scaled.shape[1], 2):\n    print(f\"PCA for {k} dimensions...\")\n    data_pca = PCA(n_components=k, random_state=42).fit_transform(data_scaled)\n    print(f\"Clustering with {k} dimensions and 2 clusters...\")\n    kmc_model = KMeans(n_clusters=2, random_state=42).fit(data_pca)\n    print(\"KMeans fitted...\")\n    db_model = DBSCAN(n_jobs=-1).fit(data_pca)\n    print(\"DBSCAN fitted...\")\n    \n    kmc_sil = silhouette_score(data_pca, kmc_model.labels_, metric='euclidean', random_state=42)\n    db_sil = silhouette_score(data_pca, db_model.labels_, metric='euclidean', random_state=42)\n    \n    no_improvement = True\n    \n    for score, i in zip([kmc_sil, db_sil], range(len(algos))):\n        if score > max_silhouette:\n            print(f\"New max score for 2 clusters and {k} dimensions is {round(score, 5)} with {algos[i]}\")\n            no_improvement = False\n            max_silhouette = score\n            best_algo = algos[i]\n            best_k = k\n            tol_iters = 0\n        \n    if no_improvement:\n        print(f\"No improvement for {k} dimensions\")\n        tol_iters += 1\n        \n    if tol_iters == early_stopping_iters:\n        print(f\"Early stopping: {early_stopping_iters} iterations without improvement\")\n        break\n\nprint(\"***\")\nprint(f\"The final score is {round(max_silhouette, 5)} with {best_k} dimensions and {best_algo}\")","1cae47da":"**Greetings!**\n\nToday we'll try to apply some dimension reduction and clustering methods to group the bank clients.\n\nTo evaluate the clustering results, we'll use the **silhouette score** (https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#silhouette-coefficient) as it doesn't require us to know the labels of data in advance.","196c6a52":"The score of 0.62 ain't brilliant, but not that bad; as Sklearn says, \"The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters\". We probably have some overlapping and sparsity in clusters, but the solution looks quite simplistic and doesn't use the duration feature which is stated in data description as a \"cheat code\" :)\n\nHopefully my notebook will raise some discussion or, maybe, inspire other researchers for more advanced solutions. Thanks for your attention :)"}}