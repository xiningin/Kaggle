{"cell_type":{"ad7b44ba":"code","7805ccb5":"code","f73abedf":"code","cfae5cd0":"code","3afb64d8":"code","851e45c2":"code","a2e9277b":"code","f74c7b71":"code","3f31cd32":"code","00c9d354":"code","a3443956":"code","f8078155":"code","62857622":"code","7ca0dfa5":"code","6a8b42aa":"code","146286b2":"code","761964a1":"code","d1a3bfc4":"code","4b0cfc75":"code","9d725397":"code","68839d0f":"code","5a860814":"code","96ac0fc1":"code","99d8e660":"code","f986e8ce":"code","90e4d451":"code","f8452a43":"code","7c23f5c3":"code","92bb4e9d":"code","e02a7c5f":"code","b768ade6":"code","b92f0d41":"code","c3fbb3e0":"code","41fb6fd1":"code","88dba409":"code","387c7022":"code","22629475":"code","6cef3b66":"code","ee8299e2":"code","44f0fcf1":"code","4978e437":"code","c8edba4b":"code","f065df69":"code","58d59637":"code","91504d09":"code","96174169":"code","54f24728":"code","1623d708":"code","b9726f54":"code","5f4d4b91":"code","1afd43c4":"code","21c0f11b":"code","4a9c3c9d":"code","8d160b45":"code","1f49bb2d":"code","f08c25fc":"code","28584e77":"code","9cd25fa2":"code","038ec5ad":"code","7446dd32":"markdown","4ab17f45":"markdown","697f2d11":"markdown","95821f96":"markdown","18d37a05":"markdown","df744ff0":"markdown","7ab42d09":"markdown","71c2c02a":"markdown","9f051333":"markdown","275526b3":"markdown","21752ff3":"markdown","655761a2":"markdown","62d06d0e":"markdown","7693208f":"markdown","53f51df1":"markdown","a73d3be1":"markdown","66c8fe25":"markdown","52e08da4":"markdown","f8e15ee9":"markdown","13903116":"markdown","2da258e5":"markdown","ddcb5e03":"markdown","c6a6bfd1":"markdown","42a5117f":"markdown","667d4c82":"markdown","ba032016":"markdown","ca125f5e":"markdown","c266b29a":"markdown","0b8c8754":"markdown","a898b24d":"markdown","bac8a622":"markdown","8892fe9d":"markdown"},"source":{"ad7b44ba":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport scipy as sp\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n","7805ccb5":"data = pd.read_csv(\"\/kaggle\/input\/supermarket-sales\/supermarket_sales - Sheet1.csv\")\n","f73abedf":"data","cfae5cd0":"data.head()","3afb64d8":"data.describe()","851e45c2":"data.info()","a2e9277b":"data.value_counts()","f74c7b71":"data.shape","3f31cd32":"data.dtypes\n","00c9d354":"data.columns","a3443956":"data.isnull().sum()\n","f8078155":"data.isnull().any()\n","62857622":"data.hist(figsize=(20,14))\nplt.show()\n","7ca0dfa5":"data.corr()","6a8b42aa":"plt.figure(figsize = (12,10))\n\nsns.heatmap(data.corr(), annot =True)\n\n","146286b2":"data.columns","761964a1":"plt.figure(figsize=(14,10))\nsns.set_style(style='whitegrid')\nplt.subplot(2,3,1)\nsns.boxplot(x='Unit price',data=data)\nplt.subplot(2,3,2)\nsns.boxplot(x='Quantity',data=data)\nplt.subplot(2,3,3)\nsns.boxplot(x='Total',data=data)\nplt.subplot(2,3,4)\nsns.boxplot(x='cogs',data=data)\nplt.subplot(2,3,5)\nsns.boxplot(x='Rating',data=data)\nplt.subplot(2,3,6)\nsns.boxplot(x='gross income',data=data)\n\n\n","d1a3bfc4":"sns.pairplot(data=data)","4b0cfc75":"sns.regplot(x='Rating', y= 'gross income', data=data)","9d725397":"sns.scatterplot(x='Rating', y= 'cogs', data=data)","68839d0f":"sns.jointplot(x='Rating', y= 'Total', data=data)","5a860814":"sns.catplot(x='Rating', y= 'cogs', data=data)","96ac0fc1":"sns.lmplot(x='Rating', y= 'cogs', data=data)","99d8e660":"data.columns","f986e8ce":"plt.style.use(\"default\")\n\nsns.kdeplot(x='Rating', y= 'Unit price', data=data)","90e4d451":"sns.lineplot(x='Rating', y= 'Unit price', data=data)","f8452a43":"plt.style.use(\"default\")\nplt.figure(figsize=(5,5))\nsns.barplot(x=\"Rating\", y=\"Unit price\", data=data[170:180])\nplt.title(\"Rating vs Unit Price\",fontsize=15)\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Unit Price\")\nplt.show()\n","7c23f5c3":"data.columns","92bb4e9d":"plt.style.use(\"default\")\nplt.figure(figsize=(5,5))\nsns.barplot(x=\"Rating\", y=\"Gender\", data=data[170:180])\nplt.title(\"Rating vs Gender\",fontsize=15)\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Gender\")\nplt.show()","e02a7c5f":"plt.style.use(\"default\")\nplt.figure(figsize=(5,5))\nsns.barplot(x=\"Rating\", y=\"Quantity\", data=data[170:180])\nplt.title(\"Rating vs Quantity\",fontsize=15)\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Quantity\")\nplt.show()","b768ade6":"#lets find the categorialfeatures\nlist_1=list(data.columns)\n","b92f0d41":"list_cate=[]\nfor i in list_1:\n    if data[i].dtype=='object':\n        list_cate.append(i)\n","c3fbb3e0":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n","41fb6fd1":"for i in list_cate:\n    data[i]=le.fit_transform(data[i])\n","88dba409":"data","387c7022":"y=data['Gender']\nx=data.drop('Gender',axis=1)\n","22629475":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2)\n","6cef3b66":"print(len(x_train))\nprint(len(x_test))\nprint(len(y_train))\nprint(len(y_test))\n","ee8299e2":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","44f0fcf1":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",knn.score(x_train,y_train)*100)\n","4978e437":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\n","c8edba4b":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",svc.score(x_train,y_train)*100)\n","f065df69":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\n","58d59637":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",gnb.score(x_train,y_train)*100)\n","91504d09":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123,criterion='entropy')\n\ndtree.fit(x_train,y_train)\n","96174169":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",dtree.score(x_train,y_train)*100)\n","54f24728":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)\n","1623d708":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",rfc.score(x_train,y_train)*100)\n","b9726f54":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n","5f4d4b91":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",adb.score(x_train,y_train)*100)\n","1afd43c4":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\ngbc.fit(x_train,y_train)","21c0f11b":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",gbc.score(x_train,y_train)*100)","4a9c3c9d":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","8d160b45":"from xgboost import XGBClassifier\n\nxgb =XGBClassifier(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\n\nxgb.fit(x_train, y_train)\n","1f49bb2d":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",xgb.score(x_train,y_train)*100)\n","f08c25fc":"from sklearn.ensemble import ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators=100, random_state=0)\netc.fit(x_train,y_train)\n","28584e77":"y_pred=etc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",etc.score(x_train,y_train)*100)","9cd25fa2":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\nmodel = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)\n\n","038ec5ad":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n","7446dd32":"**3. Naive Bayes**\n\n**It is a classification technique based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.**","4ab17f45":"**LMPLOT**\n\n**The lineplot (lmplot) is one of the most basic plots. It shows a line on a 2 dimensional plane. You can plot it with seaborn or matlotlib depending on your preference. The examples below use seaborn to create the plots, but matplotlib to show.**\n","697f2d11":"**REGPLOT**\n\n**This method is used to plot data and a linear regression model fit. ... If strings, these should correspond with column names in \u201cdata\u201d. When pandas objects are used, axes will be labeled with the series name. data: This is dataframe where each column is a variable and each row is an observation.**","95821f96":"# **THANK YOU**","18d37a05":"**BARPLOT**\n\n**A barplot (or barchart) is one of the most common types of graphic. It shows the relationship between a numeric and a categoric variable. Each entity of the categoric variable is represented as a bar. The size of the bar represents its numeric value.**\n","df744ff0":"**BOXPLOT**\n\n**A boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d). ... It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.**\n\n","7ab42d09":"**8. XGBClassifier**\n\n**XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm, which attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models.**","71c2c02a":"# LOADING THE DATASET","9f051333":"# IMPORTING THE LIBRARIES","275526b3":"**KDE PLOT (DENSITY PLOT)**\n\n**KDE Plot described as Kernel Density Estimate is used for visualizing the Probability Density of a continuous variable. It depicts the probability density at different values in a continuous variable. We can also plot a single graph for multiple samples which helps in more efficient data visualization.**\n\n","21752ff3":"# TRAINING AND TESTING DATA","655761a2":"**Checking Null Value**","62d06d0e":"**JOINTPLOT**\n\n**Seaborn's jointplot displays a relationship between 2 variables (bivariate) as well as 1D profiles (univariate) in the margins. This plot is a convenience class that wraps JointGrid.**","7693208f":"**6. AdaBoostClassifier**\n\n**An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.**\n\n","53f51df1":"**7. Gradient Boosting Classifier**\n\n**Gradient boosting classifiers are a group of machine learning algorithms that combine many weak learning models together to create a strong predictive model. Decision trees are usually used when doing gradient boosting.**\n\n","a73d3be1":"**The growth of supermarkets in most populated cities are increasing and market competitions are also high. The dataset is one of the historical sales of supermarket company.**","66c8fe25":"**SCATTER PLOT**\n\n**A scatter plot (aka scatter chart, scatter graph) uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables.**\n\n","52e08da4":"# MODELS\n\n**1. KNeighborsClassifier**\n\n**By default, the KNeighborsClassifier looks for the 5 nearest neighbors. We must explicitly tell the classifier to use Euclidean distance for determining the proximity between neighboring points.**\n\n","f8e15ee9":"**HISTOGRAM**\n\n**A histogram is basically used to represent data provided in a form of some groups.It is accurate method for the graphical representation of numerical data distribution.It is a type of bar plot where X-axis represents the bin ranges while Y-axis gives information about frequency.**","13903116":"# Supermarket Sales Prediction\n","2da258e5":"**LINEPLOT**\n\n**A Line plot can be defined as a graph that displays data as points or check marks above a number line, showing the frequency of each value.**\n\n","ddcb5e03":"**PAIRPLOT**\n\n**A pairplot plot a pairwise relationships in a dataset. The pairplot function creates a grid of Axes such that each variable in data will by shared in the y-axis across a single row and in the x-axis across a single column.**\n","c6a6bfd1":"**5.Random Forest Classifier**\n\n**A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.**\n\n","42a5117f":"# **Exploratory Data Analysis**","667d4c82":"**HEATMAP**\n\n**A heatmap is a graphical representation of data that uses a system of color-coding to represent different values. Heatmaps are used in various forms of analytics but are most commonly used to show user behaviour on specific webpages or webpage templates.**\n","ba032016":"**4. DECISION TREE CLASSIFIER**\n\n**Decision trees use multiple algorithms to decide to split a node into two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes. ... The decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.**\n\n","ca125f5e":"![](https:\/\/www.growthmanifesto.com\/wp-content\/uploads\/2018\/08\/supermarket-sales.jpg)","c266b29a":"**2. SVC**\n\n**In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.**\n","0b8c8754":"**10.Bagging Classifier**\n\n**A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. ... The base estimator to fit on random subsets of the dataset.**\n\n","a898b24d":"**CATPLOT**\n\n**Catplot is a relatively new addition to Seaborn that simplifies plotting that involves categorical variables. In Seaborn version v0. 9.0 that came out in July 2018, changed the older factor plot to catplot to make it more consistent with terminology in pandas and in seaborn.**","bac8a622":"**9. ExtraTreesClassifier**\n\n**Extremely Randomized Trees Classifier(Extra Trees Classifier) is a type of ensemble learning technique which aggregates the results of multiple de-correlated decision trees collected in a \u201cforest\u201d to output it's classification result.**","8892fe9d":"**CONCLUSION :**\n    \n**ACCURACIES OF DIFFERENT MODELS ARE:**\n\n**KNeighbors Classifier= 64.75 %**\n\n**SVC= 55.50 %**\n\n**Naiye Bayes= 55.10 %**\n\n**Decision Tree Classifier= 64 %**\n\n**Random Forest Classifier= 100 %**\n\n**Ada Boost Classifier= 67 %**\n\n**Gradient Boosting Classifier= 89 %**\n\n**XGB Classifier= 64 %**\n\n**Extra Trees Classifier= 100 %**\n\n**Bagging Classifier = 51 %**\n\n\n**We got a good accuracy of about 100 % using Random Forest Classifier and Extra Trees Classifier which is quite well for the given dataset.**\n\n**The accuracy of other models can be increased further by HyperTuning.**\n\n\n\n\n\n\n\n\n\n"}}