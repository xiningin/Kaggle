{"cell_type":{"a0647346":"code","4fb3358a":"code","27287251":"code","2847b525":"code","3c952832":"code","14347a9c":"code","b2b9274a":"code","e24de30c":"code","f355bada":"code","9c96825b":"code","3c27209e":"code","977deea5":"code","09c71376":"code","29a9872b":"code","de6cb0e2":"code","c1e391c8":"code","b8abee2c":"code","223b4e1f":"code","89620856":"code","8cf0bec5":"code","7c0fd9ca":"code","4dc664d2":"code","d3b72981":"code","d5bf51f0":"code","9bd6d0e8":"code","78431844":"code","4f104176":"code","ae88ecc5":"code","5c32a7a0":"code","9201fff9":"code","3ab36894":"code","3f1d3c3b":"code","3bb7ae9a":"code","1d14041d":"code","53674a8a":"code","e69a7281":"code","26943df4":"code","7c0925fa":"code","e91ade5d":"code","d1054e04":"code","2b748f19":"code","29eb7c79":"code","be00a089":"code","1203c61e":"code","4972ab87":"code","a415daad":"code","72a3a248":"code","d6c9285a":"code","0c74e159":"markdown","820ad170":"markdown","e0ca740f":"markdown","53a32523":"markdown","64a7cfd7":"markdown","6bdabc95":"markdown","7125d8f3":"markdown","a50ce6ce":"markdown"},"source":{"a0647346":"# !pip install -Uq fastai","4fb3358a":"#from optiver_features import generate_test_df\nfrom fastai.tabular.all import *","27287251":"import cudf","2847b525":"def wap_calc(df, bid_price, bid_size, ask_price, ask_size):\n    return (df[bid_price]*df[ask_size]+df[ask_price]*df[bid_size])\/(df[bid_size]+df[ask_size])\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff()\n\ndef log_time(time_ids):\n    return np.log(time_ids)\n\n# # Calculate the realized volatility\n# def realized_volatility(series):\n#     return series.pipe(lambda x: np.sqrt(np.sum(x**2)))\n#     #series=list(series)\n#     #return cudf.sqrt(cudf.add(cudf.multiply(series,series)))\n#     #return np.sqrt(np.sum(series*series))","3c952832":"train_df = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_df['row_id'] = train_df['stock_id'].astype(str) + '-' + train_df['time_id'].astype(str)","14347a9c":"trade_train_df = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\")","b2b9274a":"trade_train_df.head()","e24de30c":"trade_train_features = trade_train_df.groupby(['stock_id', 'time_id'], as_index=False).agg({'order_count':['sum'], 'size':['mean','sum','std'], 'price':['mean','std'], 'seconds_in_bucket':['mean','std']})\ntrade_train_features['row_id'] = trade_train_features['stock_id'].astype(str) + '-' + trade_train_features['time_id'].astype(str)\ntrade_train_features.columns = ['_'.join(col).strip('_') for col in trade_train_features.columns]\ntrade_train_features = trade_train_features.drop(['stock_id','time_id'], axis=1)","f355bada":"train_df = train_df.merge(trade_train_features, how='left', on='row_id')","9c96825b":"train_book_file_list = Path(\"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\").ls()","3c27209e":"full_book_train_features = []\ngroupby_dict = {'bid_price1':['mean', 'std'], \n                'ask_price1':['mean', 'std'], \n                'bid_price2':['mean'], \n                'ask_price2':['mean'], \n                'bid_size1':['mean','sum','std', 'max'], \n                'bid_size2':['mean','sum','std', 'max'], \n                'ask_size1':['mean','sum','std', 'min', 'max'], \n                'ask_size2':['mean','sum','std', 'min', 'max'], \n                'wap1':['mean','sum','std', 'min', 'max'], \n                'wap2':['mean','sum','std', 'min', 'max'], \n                'log_return1':['mean', 'sum','std', 'min', 'max'], \n                'square_log_return1':['sum'], \n                'log_return2':['mean', 'sum','std', 'min', 'max'], \n                'square_log_return2':['sum'], \n                'wap_diff':['mean','sum','std', 'min', 'max'], \n                'seconds_in_bucket':['mean','std', 'max'], \n                'log_sib':['mean', 'std', 'max'],\n                'int_sib':['mean', 'std', 'max'],\n                'total_volume':['sum', 'mean', 'std', 'max', 'min'], \n                'price_spread':['sum', 'mean', 'std', 'max', 'min'],\n                'price_spread2':['sum', 'mean', 'std', 'max', 'min'],\n                'bid_spread':['sum', 'mean', 'std', 'max', 'min'],\n                'ask_spread':['sum', 'mean', 'std', 'max', 'min'],\n                'bid_ask_spread':['sum', 'mean', 'std', 'max', 'min'],\n                'total_volume':['sum', 'mean', 'std', 'max', 'min']\n               }\nfor fn in train_book_file_list:\n    book_train_df = cudf.read_parquet(fn)\n    book_train_df['stock_id'] = fn.name.split('=')[-1]\n    book_train_df['wap1'] = wap_calc(book_train_df, 'bid_price1', 'bid_size1', 'ask_price1', 'ask_size1')\n    book_train_df['wap2'] = wap_calc(book_train_df, 'bid_price2', 'bid_size2', 'ask_price2', 'ask_size2')\n    book_train_df['log_return1'] = log_return(book_train_df['wap1'])\n    book_train_df['log_return2'] = log_return(book_train_df['wap2'])\n    book_train_df['wap_diff'] = book_train_df['wap1'] - book_train_df['wap2']\n    book_train_df['square_log_return1'] = book_train_df['log_return1']**2\n    book_train_df['square_log_return2'] = book_train_df['log_return2']**2\n    # Calculate spread (https:\/\/www.kaggle.com\/mayangrui\/lgbm-ffnn)\n    book_train_df['price_spread'] = (book_train_df['ask_price1'] - book_train_df['bid_price1']) \/ ((book_train_df['ask_price1'] + book_train_df['bid_price1']) \/ 2)\n    book_train_df['price_spread2'] = (book_train_df['ask_price2'] - book_train_df['bid_price2']) \/ ((book_train_df['ask_price2'] + book_train_df['bid_price2']) \/ 2)\n    book_train_df['bid_spread'] = book_train_df['bid_price1'] - book_train_df['bid_price2']\n    book_train_df['ask_spread'] = book_train_df['ask_price1'] - book_train_df['ask_price2']\n    book_train_df[\"bid_ask_spread\"] = abs(book_train_df['bid_spread'] - book_train_df['ask_spread'])\n    book_train_df['total_volume'] = (book_train_df['ask_size1'] + book_train_df['ask_size2']) + (book_train_df['bid_size1'] + book_train_df['bid_size2'])\n    book_train_df['log_sib'] = log_time(book_train_df.seconds_in_bucket)\n    book_train_df['int_sib'] = book_train_df.seconds_in_bucket\/\/100\n    book_train_features = book_train_df.groupby(by=['stock_id', 'time_id'], as_index=False).agg(groupby_dict)\n    book_train_features['row_id'] = book_train_features['stock_id'].astype(str) + '-' + book_train_features['time_id'].astype(str)\n    book_train_features.columns = ['_'.join(col).strip('_') for col in book_train_features.columns]\n    book_train_features = book_train_features.drop(['stock_id','time_id'], axis=1)\n    full_book_train_features.append(book_train_features)\n\nfinal_book_train_features = cudf.concat(full_book_train_features)\nfinal_book_train_features = final_book_train_features.reset_index().rename({'index':'timeval'}, axis=1)\nfinal_book_train_features.head()\n\nfinal_book_train_features = final_book_train_features.to_pandas()","977deea5":"final_book_train_features['realized_volatility1'] = final_book_train_features.square_log_return1_sum**(1\/2)\nfinal_book_train_features['realized_volatility2'] = final_book_train_features.square_log_return2_sum**(1\/2)","09c71376":"train_df = train_df.merge(final_book_train_features, how='left', on='row_id')","29a9872b":"train_df['log_time_id'] = log_time(train_df.time_id)\ntrain_df['int_time_id0'] = train_df.time_id\/\/10000\ntrain_df['int_time_id1'] = train_df.time_id\/\/1000\ntrain_df['int_time_id2'] = train_df.time_id\/\/100","de6cb0e2":"train_df.columns","c1e391c8":"cont_columns, cat_columns = cont_cat_split(train_df, dep_var='target')","b8abee2c":"cat_columns","223b4e1f":"cat_columns.remove('row_id')\ncont_columns.remove('stock_id')\ncat_columns.append('stock_id')","89620856":"cat_columns","8cf0bec5":"splits = RandomSplitter()(train_df)","7c0fd9ca":"to = TabularPandas(train_df, procs=[FillMissing(FillStrategy.mode), Normalize, Categorify], cat_names=cat_columns, cont_names=cont_columns, y_names='target', y_block=RegressionBlock(), reduce_memory=True, splits=splits)","4dc664d2":"dls = to.dataloaders(bs=2048)","d3b72981":"def rmspe(y_pred, y_true):\n    return (torch.sqrt(torch.mean(torch.square((y_true - y_pred) \/ y_true))))","d5bf51f0":"max_y = train_df.target.max()*2","9bd6d0e8":"tc = tabular_config(embed_p=0.6, ps=0.01)","78431844":"#learn = tabular_learner(dls, layers=[4096, 2048, 1024], metrics=[rmspe], loss_func=L1LossFlat(), y_range=(0, max_y))\n#learn = tabular_learner(dls, layers=[1024,1024,512], metrics=[rmspe], loss_func=L1LossFlat(reduction='sum'), config=tc, y_range=(0, max_y))\nlearn = tabular_learner(dls, layers=[4096,2048,1024], loss_func=rmspe, config=tc, y_range=(0, max_y), opt_func=ranger)\n#learn = tabular_learner(dls, layers=[1024,2048,4096], loss_func=rmspe, config=tc, y_range=(0, max_y), opt_func=ranger)","4f104176":"learn = learn.to_fp32()","ae88ecc5":"learn.lr_find()","5c32a7a0":"learn.fit_one_cycle(25, 1e-3)","9201fff9":"preds=learn.get_preds();preds","3ab36894":"test_df = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')","3f1d3c3b":"trade_test_df = pd.read_parquet(\"..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\")","3bb7ae9a":"trade_test_features = trade_test_df.groupby(['stock_id','time_id'], as_index=False).agg({'order_count':['sum'], 'size':['mean','sum','std'], 'price':['mean','sum','std'], 'seconds_in_bucket':['mean','std']})\ntrade_test_features['row_id'] = trade_test_features['stock_id'].astype(str) + '-' + trade_test_features['time_id'].astype(str)\ntrade_test_features.columns = ['_'.join(col).strip('_') for col in trade_test_features.columns]\ntrade_test_features = trade_test_features.drop(['stock_id','time_id'], axis=1)","1d14041d":"test_df = test_df.merge(trade_test_features, how='left', on='row_id')","53674a8a":"test_book_file_list = Path(\"..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\").ls()","e69a7281":"full_book_test_features = []\nfor fn in test_book_file_list:\n    book_test_df = cudf.read_parquet(fn)\n    book_test_df['stock_id'] = fn.name.split('=')[-1]\n    book_test_df['wap1'] = wap_calc(book_test_df, 'bid_price1', 'bid_size1', 'ask_price1', 'ask_size1')\n    book_test_df['wap2'] = wap_calc(book_test_df, 'bid_price2', 'bid_size2', 'ask_price2', 'ask_size2')\n    book_test_df['log_return1'] = log_return(book_test_df['wap1'])\n    book_test_df['log_return2'] = log_return(book_test_df['wap2'])\n    book_test_df['wap_diff'] = book_test_df['wap1'] - book_test_df['wap2']\n    book_test_df['square_log_return1'] = book_test_df['log_return1']**2\n    book_test_df['square_log_return2'] = book_test_df['log_return2']**2\n    book_test_df['total_volume'] = book_test_df['ask_size1']+book_test_df['ask_size2']+book_test_df['bid_size1']+book_test_df['bid_size2']\n    # Calculate spread (https:\/\/www.kaggle.com\/mayangrui\/lgbm-ffnn)\n    book_test_df['price_spread'] = (book_test_df['ask_price1'] - book_test_df['bid_price1']) \/ ((book_test_df['ask_price1'] + book_test_df['bid_price1']) \/ 2)\n    book_test_df['price_spread2'] = (book_test_df['ask_price2'] - book_test_df['bid_price2']) \/ ((book_test_df['ask_price2'] + book_test_df['bid_price2']) \/ 2)\n    book_test_df['bid_spread'] = book_test_df['bid_price1'] - book_test_df['bid_price2']\n    book_test_df['ask_spread'] = book_test_df['ask_price1'] - book_test_df['ask_price2']\n    book_test_df[\"bid_ask_spread\"] = abs(book_test_df['bid_spread'] - book_test_df['ask_spread'])\n    book_test_df['total_volume'] = (book_test_df['ask_size1'] + book_test_df['ask_size2']) + (book_test_df['bid_size1'] + book_test_df['bid_size2'])\n    book_test_df['log_sib'] = log_time(book_test_df.seconds_in_bucket)\n    book_test_df['int_sib'] = book_test_df.seconds_in_bucket\/\/100\n    book_test_features = book_test_df.groupby(by=['stock_id', 'time_id'], as_index=False).agg(groupby_dict)\n    book_test_features['row_id'] = book_test_features['stock_id'].astype(str) + '-' + book_test_features['time_id'].astype(str)\n    book_test_features.columns = ['_'.join(col).strip('_') for col in book_test_features.columns]\n    book_test_features = book_test_features.drop(['stock_id','time_id'], axis=1)\n    full_book_test_features.append(book_test_features)\n\nfinal_book_test_features = cudf.concat(full_book_test_features)\nfinal_book_test_features = final_book_test_features.reset_index().rename({'index':'timeval'}, axis=1)\n\nfinal_book_test_features = final_book_test_features.to_pandas()","26943df4":"final_book_test_features","7c0925fa":"final_book_test_features['realized_volatility1'] = final_book_test_features.square_log_return1_sum**(1\/2)\nfinal_book_test_features['realized_volatility2'] = final_book_test_features.square_log_return2_sum**(1\/2)","e91ade5d":"test_df = test_df.merge(final_book_test_features, how='left', on='row_id')","d1054e04":"test_df['log_time_id'] = log_time(test_df.time_id)\ntest_df['int_time_id0'] = test_df.time_id\/\/10000\ntest_df['int_time_id1'] = test_df.time_id\/\/1000\ntest_df['int_time_id2'] = test_df.time_id\/\/100","2b748f19":"test_df = test_df.fillna(method='ffill')","29eb7c79":"dl = learn.dls.test_dl(test_df)","be00a089":"test_pred = learn.get_preds(dl=dl)","1203c61e":"test_pred[0].squeeze()","4972ab87":"#submit.target = test_pred[0].squeeze().tolist()\ntest_df['target'] = test_pred[0].squeeze().tolist()\ntest_df","a415daad":"submit = test_df[['row_id', 'target']].copy()","72a3a248":"submit.to_csv('submission.csv',index=False)","d6c9285a":"submit","0c74e159":"### Test Book Features","820ad170":"### Define splits","e0ca740f":"### Train Model","53a32523":"### Generate Train Book Features","64a7cfd7":"### Generate Train Trade Features","6bdabc95":"### Create Test Preds","7125d8f3":"### Categorical vs Continuous","a50ce6ce":"### Create Tabular Learner"}}