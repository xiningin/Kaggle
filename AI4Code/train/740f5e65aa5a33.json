{"cell_type":{"abf62ca2":"code","630bd9ef":"code","0b0f6f2f":"code","7bfed4c2":"code","182a968b":"code","5abcb7ff":"code","8a75ff55":"code","8a2645ae":"code","d6953635":"code","ff2e478d":"code","1738889a":"code","6631255e":"code","cc8003c4":"code","14077776":"code","6517273e":"code","b9d92835":"code","8fba3205":"code","1141fdc2":"code","be536fa3":"code","0808cf21":"code","c52f3679":"code","4c648a05":"code","73def2d1":"code","d047f03c":"code","e2eea265":"code","d3ffc0b2":"code","00bbc1a3":"code","1a7be3a3":"code","30ffe1f9":"code","781f32b3":"code","df43452f":"code","f8a5f434":"code","b5515d92":"code","59a87ecb":"code","c7c72d1a":"code","308b8b12":"code","634e6f42":"code","80226801":"code","fdec947a":"code","61317052":"code","67736a43":"code","e35e0c11":"code","762b94ea":"code","0a4b489b":"code","c5185221":"code","29fc5e1b":"code","fe9860a8":"code","c56cdb2c":"markdown","0ae6b6c8":"markdown","23fea796":"markdown","5a5b3bb7":"markdown","da3f506d":"markdown","99cd67a2":"markdown","d1ee8094":"markdown","5730c6c2":"markdown","25b7ef37":"markdown","d3f16ecc":"markdown","a1a0a1d6":"markdown","591c1277":"markdown","7e8bcc28":"markdown","6b23d5f1":"markdown","4f536e25":"markdown","e0580ef2":"markdown","16329b85":"markdown","7143fd83":"markdown","b08de389":"markdown","a4f9c8dd":"markdown"},"source":{"abf62ca2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom pandas.io.json import json_normalize \nimport itertools  # Iterating tools\nimport re  # Regular Expressions","630bd9ef":"train_data = pd.read_csv(\"..\/input\/tmdb-box-office-prediction\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/tmdb-box-office-prediction\/test.csv\")","0b0f6f2f":"train_data.info()","7bfed4c2":"test_data.info()","182a968b":"train_data.shape , test_data.shape","5abcb7ff":"pd.DataFrame(train_data.isnull().sum()).T","8a75ff55":"perc_traindata = ((pd.DataFrame(train_data.isnull().sum()).T)\/len(train_data))*100\nperc_traindata","8a2645ae":"sns.set(rc={'figure.figsize':(12,8)})\nperc_traindata.plot.bar()\nplt.xlabel(\"Columns with missing train data\")\nplt.ylabel(\"Percentage of missing train data\")","d6953635":"train_data.drop(columns=['belongs_to_collection','homepage'],axis = 1 ,inplace=True)","ff2e478d":"train_data.drop(columns=['imdb_id', 'poster_path','tagline', 'overview', 'original_title','Keywords' ,'crew'],axis = 1 ,inplace=True)","1738889a":"#Checking again the shape of train data\ntrain_data.shape","6631255e":"train_data.dtypes","cc8003c4":"#Test data null values\npd.DataFrame(test_data.isnull().sum()).T","14077776":"perc_testdata = ((pd.DataFrame(test_data.isnull().sum()).T)\/len(test_data))*100\nperc_testdata","6517273e":"sns.set(rc={'figure.figsize':(12,8)})\nperc_testdata.plot.bar()\nplt.xlabel(\"Columns with missing test data\")\nplt.ylabel(\"Percentage of missing test data\")","b9d92835":"test_data.drop(columns=['belongs_to_collection','homepage'],axis = 1 ,inplace=True)\ntest_data.drop(columns=['imdb_id', 'poster_path','tagline', 'overview', 'original_title','Keywords' ,'crew'],axis = 1 ,inplace=True)","8fba3205":"#Checking again the shape of test data\ntest_data.shape","1141fdc2":"test_data.dtypes","be536fa3":"#extract only genres column from the train dataset and create new dataset of it which contains only genres column\nnew=train_data.loc[:,[\"genres\"]]\n#fill allna with \"None\"\nnew[\"genres\"]=train_data[\"genres\"].fillna(\"None\");\nnew[\"genres\"].head(5)","0808cf21":"#extract genre function which will take input as a row [{'id': 35, 'name': 'Comedy'}] and returns\n# array of each genre name e.g. ['Comedy'] and if there the row is empty it will return ['None']\ndef extract_genres(row):\n    if row == \"None\":\n        return ['None']\n    else:\n        results = re.findall(r\"'name': '(\\w+\\s?\\w+)'\", row)\n        return results\n\n#apply extract_genres function on genres column of new dataset    \nnew[\"genres\"] = new[\"genres\"].apply(extract_genres)\nnew[\"genres\"].head(10) ","c52f3679":"#declare a dictionary \ngenres_dict = dict()\n\n# loop through all the rows of genres column and set count of the genre\nfor genre in new[\"genres\"]:\n    for elem in genre:\n        if elem not in genres_dict:\n            genres_dict[elem] = 1\n        else:\n            genres_dict[elem] += 1","4c648a05":"#generate data from from dictionary which includes count of each genre\ngenres_df = pd.DataFrame.from_dict(genres_dict, orient='index')","73def2d1":"genres_df.columns = [\"number_of_movies\"]\n#sort by number of movies descending \ngenres_df = genres_df.sort_values(by=\"number_of_movies\", ascending=False)\n#plot bar chart\ngenres_df.plot.bar()","d047f03c":"train_data['spoken_languages']","e2eea265":"new=train_data.loc[:,[\"spoken_languages\"]]\n#fill allna with \"None\"\nnew[\"spoken_languages\"]=train_data[\"spoken_languages\"].fillna(\"None\");\nnew[\"spoken_languages\"].head(5)","d3ffc0b2":"def extract_spoken(row):\n    if row == \"None\":\n        return ['None']\n    else:\n        results = re.findall(r\"'name': '(\\w+\\s?\\w+)'\", row)\n        return results\n\n#apply extract_genres function on genres column of new dataset    \nnew[\"spoken_languages\"] = new[\"spoken_languages\"].apply(extract_spoken)\nnew[\"spoken_languages\"].head(10) ","00bbc1a3":"#declare a dictionary \ngenres_dict = dict()\n\n# loop through all the rows of genres column and set count of the genre\nfor genre in new[\"spoken_languages\"]:\n    for elem in genre:\n        if elem not in genres_dict:\n            genres_dict[elem] = 1\n        else:\n            genres_dict[elem] += 1","1a7be3a3":"#generate data from from dictionary which includes count of each genre\nlanguage_df = pd.DataFrame.from_dict(genres_dict, orient='index')\nlanguage_df","30ffe1f9":"import time\nimport datetime\n\nmovietime = train_data.loc[:,[\"title\",\"release_date\",\"budget\",\"runtime\",\"revenue\"]]\nmovietime.dropna()\n\nmovietime.release_date = pd.to_datetime(movietime.release_date)\nmovietime.loc[:,\"Year\"] = movietime[\"release_date\"].dt.year\nmovietime.loc[:,\"Month\"] = movietime[\"release_date\"].dt.month\nmovietime.loc[:,\"Day_of_Week\"] = (movietime[\"release_date\"].dt.dayofweek)\nmovietime.loc[:,\"Quarter\"]  = movietime.release_date.dt.quarter \n\nmovietime = movietime[movietime.Year<2018]\nmovietime.head(6)","781f32b3":"data_plot = movietime[['revenue', 'Year']]\nmoney_Y = data_plot.groupby('Year')['revenue'].sum()\n\nmoney_Y.plot(figsize=(15,8))\nplt.xlabel(\"Year of release\")\nplt.ylabel(\"revenue\")\nplt.xticks(np.arange(1970,2020,5))\n\nplt.show()","df43452f":"f,ax = plt.subplots(figsize=(18, 10))\nplt.bar(movietime.Month, movietime.revenue, color = 'blue')\nplt.xlabel(\"Month of release\")\nplt.ylabel(\"revenue\")\nplt.show()","f8a5f434":"x = train_data[['popularity', 'runtime', 'budget']] #independent variables\ny= train_data['revenue'] #dependent variable","b5515d92":"from sklearn.metrics import mean_squared_log_error as msle\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()","59a87ecb":"from sklearn.model_selection import train_test_split\nx, X_test, y, y_test = train_test_split(x, y, test_size=0.30, random_state=1)","c7c72d1a":"x.describe()","308b8b12":"x.isnull().sum()","634e6f42":"#we fill the null values in the train data\nx.median()","80226801":"medianFiller = lambda t: t.fillna(t.median())\nx = x.apply(medianFiller,axis=0)","fdec947a":"x.isnull().sum()","61317052":"model = reg.fit(x,y)\nmodel","67736a43":"y_pred = reg.predict(x)\nfor idx, col_name in enumerate(x.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, model.coef_[0]))","e35e0c11":"intercept = model.intercept_\nprint(\"The intercept for our model is {}\".format(intercept))","762b94ea":"#The score (R^2) for in-sample and out of sample\nmodel.score(x, y)","0a4b489b":"\nx_test = test_data[['popularity', 'runtime', 'budget']]\nx_test.isnull().sum()\n#pred = reg.predict(x_test)","c5185221":"x_test.median()","29fc5e1b":"medianFiller = lambda t: t.fillna(t.median())\nx_test = x_test.apply(medianFiller,axis=0)","fe9860a8":"pred = reg.predict(x_test)\npred","c56cdb2c":"The Linear Regression gives us an accuracy of 61% <br>\n<br>\n<font size=3>Model of Test_Data","0ae6b6c8":"Some columns from which 'Prediction of Revenue' doesn't affect. <br>\nListed as : 'imdb_id', 'poster_path','tagline', 'overview', 'original_title','Keywords' ,'crew'","23fea796":"<font size =\"3\"><b> Extract a new dataframe related to movie budget, revenue, runtime","5a5b3bb7":"<font size=\"6\">To predict a movie's worldwide box office revenue","da3f506d":"now, we calculate  percentage to these missing values for both the train and test set","99cd67a2":"<font size =\"3\"><b> Shape of test data and train data","d1ee8094":"<font size =\"3\"><b>Reading train data and test data","5730c6c2":"<font size=\"5\"> LINEAR REGRESSION","25b7ef37":"<font size =\"3\"><b>Since, budget and popularity would be the main aspect to predict the revenue.   ","d3f16ecc":"<font size =\"3\"><b> Let's first extract the data from the columns which have JSON Objects <br>\n1. GENRE","a1a0a1d6":"<font size =\"3\"><b> Let's clean the data by removing columns which are less important and finding the null values","591c1277":"MONTH OF RELEASE AND REVENUE","7e8bcc28":"<b>DRAMA and COMEDY is the most common genre","6b23d5f1":"<font size =\"3\"><b>2. SPOKEN LANGUAGE","4f536e25":"Representation of these missing values % in graph","e0580ef2":"<font size =\"3\"><b> Getting information of the loaded data","16329b85":"<font size=\"3\" >We are provided with 7398 movies and a variety of metadata obtained from The Movie Database (TMDB). Movies are labeled with id. Data points include cast, crew, plot keywords, budget, posters, release dates, languages, production companies, and countries.\n\nWe are going to predict the worldwide revenue for 4398 movies in the test file.","7143fd83":"If NAN values are more than 60 , we drop that column ","b08de389":"<font size =\"3\"><b>Importing Necessary Libraries","a4f9c8dd":"GRAPH BETWEEN REVENUE AND YEAR "}}