{"cell_type":{"e4f4f728":"code","22fbbab5":"code","fe0bde14":"code","afc91db9":"code","b2d7c32e":"code","d86c1faa":"code","2faee4d7":"code","16c95d63":"code","31aa537d":"code","4f2895cc":"code","2a847704":"code","d3d35d7a":"code","9d9dec20":"code","ef9719c6":"code","c9f21e28":"code","b3635466":"code","ea659561":"code","35519688":"code","99ef58d1":"code","3fe8f376":"code","db10c869":"markdown","e72a02aa":"markdown","42863d82":"markdown","fa7ae96d":"markdown","044c43db":"markdown","aaf56770":"markdown","ca36939b":"markdown","91367f77":"markdown","c90a6cfc":"markdown","07789e47":"markdown","b8a24abe":"markdown","0dd081a0":"markdown","45425c31":"markdown","8f93b1fe":"markdown","a4d2f26b":"markdown","9405c3dd":"markdown"},"source":{"e4f4f728":"import numpy as np\nimport pandas as pd\nfrom gensim.models.phrases import Phrases, Phraser\nimport io\nimport os\nimport time\nimport fasttext.util\nimport fasttext\nimport random\n\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences \n\ndir_path = '..\/input\/poemdata\/poem_gen\/poem_gen'\ncmu_pron_file = 'cmudict-0.7b.txt'\nsimilar_phonemes_file = 'similar_phonemes.txt'\nwiki_frequency_file = 'enwiki-words-frequency.txt'\nembedding_file = '..\/input\/fasttext-embedding\/cc.en.300.vec'\nLM_file = '..\/input\/poemdata\/lstm_epoch_750-20201210T053649Z-001\/lstm_epoch_750'\ngutenberg_subset_file = '..\/input\/poemdata\/poem_data_rev.txt'","22fbbab5":"%%time\n\nsimilar_phonemes_lst = list()\nwith open(os.path.join(dir_path, similar_phonemes_file), 'r') as f:\n    for line in f.readlines():\n        tpl = tuple(line.split())\n        similar_phonemes_lst.append(tpl)\n        \nprint('list of similar phonemes loaded. %s pairs' % (len(similar_phonemes_lst)))\n\npopular_rhyme_pairs = [\n    'do',\n    'you',\n    'go',\n    'know',\n    'see',\n    'me',\n    'night',\n    'right',\n    'day',\n    'way',\n    'say',\n    'way'\n]\n\n## extract 30K most frequent words from wiki corpus\nmost_freq_words_30k = set()\nwith open(os.path.join(dir_path, wiki_frequency_file), 'r') as f:\n    for line in f.readlines():\n        word = line.split()[0].upper()\n        if word.isalpha():\n            most_freq_words_30k.add(word)\n        if len(most_freq_words_30k) == 30000:\n            break\n    common_words = ['A','I']\n    most_freq_words_30k.update(common_words)\n    print('30K most frequent words from wiki extracted')","fe0bde14":"## load word embeddings of most frequent words\n\nword_embeddings = dict()\n\ndef load_embeddings():\n    '''\n    Loads fasttext word embeddings of the 30K most frequent words\n    \n    Parameters - none\n    \n    Returns - none\n    '''\n    ## clear word embedding dict\n    global word_embeddings\n    word_embeddings = dict()\n    \n    t1 = time.time()\n    fin = io.open(embedding_file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n    n, d = map(int, fin.readline().split())\n    \n    for line in fin:\n        tokens = line.rstrip().split(' ')\n        word = tokens[0].upper()\n        wvec = np.array(list(map(float, tokens[1:])))\n        if word.isalpha():\n            if (word not in word_embeddings) and (word in most_freq_words_30k):\n                word_embeddings[word] = wvec\n        \n    print('time to load word embeddings of 30K most frequent words = %.3f sec' % (time.time()-t1))\n\nload_embeddings()","afc91db9":"## Vocabulary (stress patterns)\n\n## find stress patterns and rhyme classes of all words in the CMU\n## pronunciation dictionary\n\nstress_pattern = dict()\nrhyme_class_strict = dict()\nrhyme_valid = dict()\nrhyme_class_slant = dict()\n\nwith open(os.path.join(dir_path, cmu_pron_file), 'rb') as f:\n    lines = [x for x in f.readlines() if b';;;' not in x]\n    for line in lines:\n\n        ## ignore initial waste lines in the file\n        if line[:3] == ';;;':\n            continue\n\n        l = str(line)[2:-3]\n        l = l.split()\n        ptrn = []\n        strict_rhyme = []\n        slant_rhyme = []\n        \n        ## find stress pattern of word\n        for ch in l[1:]:\n            if ch[-1].isdigit():\n                ptrn.append(int(ch[-1]!='0'))\n                \n        ## find strict rhyme class of word\n        for ch in list(reversed(l[1:])):\n            if ch[-1].isdigit():\n                if ch[-1] != '0':\n                    strict_rhyme.append(ch[:-1] + \"1\") ## replace 2 with 1 if needed\n                    if ch[0] in ['A', 'E', 'I', 'O', 'U']: ## last stressed vowel found\n                        break\n                else:\n                    strict_rhyme.append(ch)\n            else:\n                strict_rhyme.append(ch)\n                \n        strict_rhyme = list(reversed(strict_rhyme))        \n                     \n        ## change ...100 to ...101 in stress pattern\n        ## and update strict rhyme class accordingly \n        if ptrn[-3:] == [1,0,0]:\n            ptrn[-1] = 1\n            \n            for i in reversed(range(len(strict_rhyme))):\n                if strict_rhyme[i][-1].isdigit():\n                    ch = strict_rhyme[i][:-1] + \"1\"\n                    strict_rhyme[i] = ch\n                    \n                    if ch[0] in ['A', 'E', 'I', 'O', 'U']:\n                        strict_rhyme = strict_rhyme[i:]\n                    break\n                    \n        rhyme_valid[l[0]] = 1 if 1 in ptrn else 0\n        \n        # Words whose last syllable is stressed do not participate in slant rhymes\n        ## find slant rhyme class of word\n        if rhyme_valid[l[0]] and not ptrn[-1]:\n            l1 = []\n            ## replace phoneme ER with UH R\n            for ch in l[1:]:\n                if ch[:2] == \"ER\":\n                    l1.append(\"UH\" + str(ch[2]))\n                    l1.append(\"R\")\n                else:\n                    l1.append(ch)\n            \n            slant_rhyme_class = ['']*5\n            v_found = False\n            w_found = False\n            v_index = 0\n            w_index = 0\n            \n            for i in reversed(range(len(l1))):\n                if l1[i][-1].isdigit():\n                    if not w_found:\n                        w_found = True\n                        if l1[i][-1] != '0':\n                            v_found = True\n                            w = l1[i][:-1] + \"1\"\n                            slant_rhyme_class[3] = w\n                            slant_rhyme_class[1] = w\n                            \n                            slant_rhyme_class[0] = ' '.join(l1[:i])\n                            slant_rhyme_class[4] = ' '.join(l1[i+1:])\n                            break\n                            \n                        else:\n                            slant_rhyme_class[3] = l1[i]\n                            slant_rhyme_class[4] = ' '.join(l1[i+1:])\n                            w_index = i\n                            \n                    elif w_found and not v_found:\n                        if l1[i][-1] == '0':\n                            continue\n                            \n                        v_found = True\n                        v = l1[i][:-1] + \"1\"\n                        slant_rhyme_class[0] = ' '.join(l1[:i])\n                        slant_rhyme_class[1] = v\n                        slant_rhyme_class[2] = ' '.join(l1[i+1:w_index])  \n\n        ## accept only alternating stress patterns    \n        alternating_stress = True\n        for i in range(1, len(ptrn)):\n            if ptrn[i] == ptrn[i-1]:\n                alternating_stress = False\n                \n        if alternating_stress and (l[0] in most_freq_words_30k):\n            stress_pattern[l[0]] = ptrn    \n            \n            if rhyme_valid[l[0]]:\n                rhyme_class_strict[l[0]] = strict_rhyme\n                if not ptrn[-1]:\n                    rhyme_class_slant[l[0]] = slant_rhyme_class\n\nprint('stress patterns done, %s valid words' % len(stress_pattern))\nprint('strict rhyme class done, %s valid words' % len(rhyme_class_strict))\nprint('slant rhyme class done, %s valid words' % len(rhyme_class_slant))","b2d7c32e":"## examples of stress patterns, strict rhyme classes and slant rhyme classes of some words\npronunciation_words = [\n    'VIKING',\n    'FIGHTING',\n    'COMIC'\n]\nfor w in pronunciation_words:\n    print('%s %s %s %s' % (w, stress_pattern[w], rhyme_class_strict[w], rhyme_class_slant[w]))","d86c1faa":"## topically related words\n\ndef get_related_words(topic, num=1000, fname=embedding_file):\n    '''\n    Finds words related to given topic\n    \n    Parameters:\n    topic (str) - topic whose related words are needed\n    num (int) - number of related words\n    fname (str) - file path of word embedding file\n    \n    Returns:\n    topic_vec (numpy array) - vector of the topic\n    ranked (list) - list of related words along with similarity scores\n    '''\n    t1 = time.time()\n    \n    ## find embedding of topic\n    topic = topic.upper().split()\n    topic_vec = np.zeros(300)\n    topic_words_found = dict()\n    \n    fin = io.open(embedding_file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n    n, d = map(int, fin.readline().split())\n    \n    for line in fin:\n        \n        tokens = line.rstrip().split(' ')\n        word = tokens[0].upper()\n        \n        if (word in topic) and (word not in topic_words_found):\n            \n            wvec = np.array(list(map(float, tokens[1:])))\n            topic_vec += wvec\n            topic_words_found[word] = 0\n            \n            if len(topic_words_found) == len(topic): ## all topic words found\n                break\n    \n    topic_vec \/= len(topic_words_found) ## average of vectors of individual words\n    \n    if len(topic_words_found) == 0:\n        print('Embeddings not found for any word of the topic, considering 0 vector for topic hence')\n        topic_vec = np.random.randn(300)*0.01\n\n        \n    ## find num most similar words from the 30K most frequent words' embeddings\n    similar_words = ['']*num\n    cosine_similarities = np.zeros(num)\n    \n    for word in stress_pattern: ## consider only valid stress pattern words\n        \n        wvec = word_embeddings[word]\n        cos_sim = np.sum(wvec @ topic_vec) \/ np.sqrt(np.sum(wvec**2) * np.sum(topic_vec**2))\n        \n        ## update if better than the least similar word present\n        min_idx = np.argmin(cosine_similarities)\n        if cos_sim > cosine_similarities[min_idx]:\n            similar_words[min_idx] = word\n            cosine_similarities[min_idx] = cos_sim\n    \n    ## prepare words ranked in descending order of similarity\n    ranked = []\n    for i in range(num):\n        ranked.append((similar_words[i], cosine_similarities[i]))\n    ranked = sorted(ranked, reverse=True, key = lambda o: o[1])\n        \n    print('time to find related words = %.3f' % (time.time()-t1))\n    return topic_vec, ranked\n\n\ndef similarity_scores(topic_vector, word_list):\n    '''\n    Finds similarity scores for a list of words given the embedding vector of topic\n    \n    Parameters:\n    topic_vector (numpy array) - word\/phrase embedding vector of the topic\n    word_list (list) - list of words whose similarity scores are needed\n    \n    Returns:\n    ranked (list) - list of the words along with similarity scores\n    '''\n    t1 = time.time()\n    \n    word_list = list(map(lambda o: o.upper(), word_list))\n    cosine_similarities = dict()\n    \n    for w in word_list:\n        \n        if w not in word_embeddings:\n            continue\n        \n        vec = word_embeddings[w]\n        cos_sim = np.sum(topic_vector @ vec) \/ np.sqrt(np.sum(topic_vector**2) * np.sum(vec**2))\n        cosine_similarities[w] = cos_sim\n        \n    ranked = list()\n    for w in cosine_similarities:\n        ranked.append((w, cosine_similarities[w]))\n    ranked = sorted(ranked, reverse=True, key = lambda o: o[1])\n        \n    print('time to find cosine similarity of given words = %.3f' % (time.time()-t1))\n    return ranked","2faee4d7":"## sample output of the related words function\n\nt_vec1, rel_words1 = get_related_words('bipolar disorder', 30)\nprint('Related words and scores for the topic: bipolar disorder' )\nfor w, s in rel_words1[:8]:\n    print('%s %.3f' % (w,s))","16c95d63":"# Rhyming words\ndef isRhyming(word1, word2):\n    '''\n    Checks whether 2 words are rhyming or not\n    \n    Parameters:\n    word1 (str) - First word\n    word2 (str) - Second word\n    \n    Returns:\n    bool - True if rhyming, False otherwise\n    '''\n    \n    if (word1 not in rhyme_class_slant) or (word2 not in rhyme_class_slant):\n        return False\n    \n    s1 = rhyme_class_slant[word1]\n    s2 = rhyme_class_slant[word2]\n        \n    [a1, v1, x1, w1, c1] = s1\n    [a2, v2, x2, w2, c2] = s2\n\n    if(v1 != v2 or w1 != w2 or c1 != c2):\n        return False\n    elif(a1 != '' and a2 != '' and a1==a2):\n        return False\n    else:\n        x1_s = x1.split()\n        x2_s = x2.split()\n        if len(x1_s)==1 and len(x2_s)==1:\n            if similar(x1_s[0], x2_s[0]):\n                return True\n            else:\n                return False\n        else:\n            count1 = 0\n            count2 = 0\n            for ch in x1_s:\n                if ch[0] in ['A', 'E', 'I', 'O', 'U']:\n                    count1 += 1\n            for ch in x2_s:\n                if ch[0] in ['A', 'E', 'I', 'O', 'U']:\n                    count2 += 1\n\n            if count1 != count2:\n                return False\n            \n            if (len(x1_s)==0 and len(x2_s)!=0) or (len(x1_s)!=0 and len(x2_s)==0):\n                return False\n            if (len(x1_s)==0 and len(x2_s)==0):\n                return True\n            \n            p1 = x1_s[0]\n            q1 = x1_s[-1]\n            p2 = x2_s[0]\n            q2 = x2_s[-1]\n            \n            if p1==p2 and similar(q1,q2):\n                return True\n            if similar(p1,p2) and q1==q2:\n                return True\n            return False\n\ndef similar(w1, w2):\n    '''\n    Checks whether 2 phoenemes are similar or not\n    \n    Parameters:\n    w1 (str) - First phoeneme\n    w2 (str) - Second phoenemes\n    \n    Returns:\n    bool - True if similar, False otherwise\n    '''\n    if w1==w2:\n        return True\n    tpl = (min(w1,w2), max(w1,w2))\n    return (tpl in similar_phonemes_lst)\n","31aa537d":"## example output of the isRhyming function\n\nrhyming_check_pairs = [\n    ('VIKING','FIGHTING'),\n    ('COMIC', 'IRONIC'),\n    ('HELLO','GOODBYE')\n]\n\nfor w1,w2 in rhyming_check_pairs:\n    print('%s, %s is rhyming: %s' % (w1, w2, isRhyming(w1, w2)))\n","4f2895cc":"## adding non-topical rhyming words\n\n## create dict of all strict rhyme classes\nstrict_rhyme_classes = dict()\nfor word in rhyme_class_strict:\n    tpl = tuple(rhyme_class_strict[word])\n    if tpl not in strict_rhyme_classes:\n        strict_rhyme_classes[tpl] = None\nprint('number of strict rhyme classes', len(strict_rhyme_classes))\n\n## find list of most frequent words for every strict rhyme class\nwith open(os.path.join(dir_path, wiki_frequency_file), 'r') as f:\n    for line in f.readlines():\n            \n        word = line.split()[0]\n        w = word.upper()\n        if not w.isalpha():\n            continue\n\n        if w in rhyme_class_strict:\n            rh_class = tuple(rhyme_class_strict[w])\n            if strict_rhyme_classes[rh_class] is None:\n                strict_rhyme_classes[rh_class] = (w, int(line.split()[1]))\n\nfrequent_strict_rhyme_words = list()\nfor rh_class in strict_rhyme_classes:\n    if strict_rhyme_classes[rh_class] is not None:\n        frequent_strict_rhyme_words.append(strict_rhyme_classes[rh_class])\n        \nfrequent_strict_rhyme_words = sorted(frequent_strict_rhyme_words, key=lambda o: o[1], reverse=True)[:900]\nfrequent_strict_rhyme_words = list(map(lambda o: o[0], frequent_strict_rhyme_words))\nprint('selected %s words' % len(frequent_strict_rhyme_words))","2a847704":"## rhyme word selection\n\ndef get_rhyme_pairs(word_scores, count=7):\n    '''\n    Generates rhyming pairs from given related words and their similarity scores\n    \n    Parameters:\n    word_scores (list) - List of (word,score) pairs\n    count (int) - number of pairs to obtain\n    \n    Returns:\n    rhyme_pairs (list) - List of rhyming word pairs - (word1, word2)\n    '''\n    ## ensure all distinct words\n    score_dict = dict()\n    for w,s in word_scores:\n        if (w not in score_dict) and (w in rhyme_class_slant):\n            ## bounding similarity with a minimum\n            score_dict[w]=max(s, 0.0001)\n\n    rhyme_pairs = list()\n        \n    while count>0:\n        count -= 1\n    \n        words = list(score_dict.keys())\n        word_pair_list = list()\n        sum_scores = 0\n        for i in range(len(words)):\n            for j in range(i+1, len(words)):\n                if isRhyming(words[i], words[j]):\n                    word_pair_list.append((words[i], words[j], max(score_dict[words[i]], score_dict[words[j]])))\n                    sum_scores += word_pair_list[-1][2]\n\n        rnd = random.uniform(0, sum_scores)\n        word1, word2 = None, None\n\n        for w1,w2,sc in word_pair_list:\n            ## word found (random sampling based on similarity scores)\n            if sc >= rnd:\n                word1, word2 = w1, w2\n                break\n            else:\n                rnd -= sc\n\n        rhyme_pairs.append((word1, word2))\n        del score_dict[word1]\n        del score_dict[word2]\n        \n        print(\"%s pairs obtained\" % len(rhyme_pairs))\n        \n    return rhyme_pairs","d3d35d7a":"## loading trained RNN for Language Model\nmodel = tf.keras.models.load_model(LM_file)\n\ntoken = Tokenizer()\nwith open(gutenberg_subset_file) as lmfile:\n    lmdata = lmfile.readlines()\ntoken.fit_on_texts(lmdata)\n\n## returns probability distribution for next word\n## given partial sentence\ndef LM(sent):\n    '''\n    Gives probability distribution for next word given partial sentence\n    \n    Parameters:\n    sent (list) - List of words in sentence\n    \n    Returns:\n    probs (dict) - dictionary with keys as words and values as the probability\n    '''\n    encoded = token.texts_to_sequences([''.join([w+' ' for w in sent])[:-1]])\n    encoded = pad_sequences(encoded, maxlen=19, padding='pre')\n    probs = model.predict(encoded)[0]\n    probs = {token.sequences_to_texts([[i]])[0].upper():probs[i] for i in np.argsort(probs)[-200:]}\n    return probs","9d9dec20":"## getting last word of each line for a given topic\n\ndef get_last_words(topic, rhyme_scheme):\n    '''\n    Gives the rhyming last words for every line in the poem based on topic and the rhyme scheme\n    \n    Parameters:\n    topic (str) - topic of the poem\n    rhyme_schme (str) - can be one of, 'AA', 'ABAB', 'AABB', 'ABABCDCDEFEFGG'. Decides rhyme scheme of poem\n    \n    Returns:\n    last_words (list) - list of the related rhyming words in order of their appearance in poem\n    related_words (list) - list of words related to the poem\n    '''\n    if rhyme_scheme == 'sonnet':\n        rhyme_scheme = 'ABABCDCDEFEFGG'\n    \n    topic_vector, related_words = get_related_words(topic)\n\n    non_topical_words = popular_rhyme_pairs + frequent_strict_rhyme_words\n    non_topical_words = list(filter(lambda o: o in stress_pattern, non_topical_words))\n    word_score_pairs = similarity_scores(topic_vector, non_topical_words)\n\n    valid_related_words = related_words + word_score_pairs\n\n    rhyme_pairs = get_rhyme_pairs(valid_related_words, len(rhyme_scheme)\/\/2)\n\n    ## order rhyme pairs according to rhyme scheme\n    if rhyme_scheme == 'AA':\n        last_words = list(rhyme_pairs[0])\n    elif rhyme_scheme == 'ABAB':\n        last_words = [rhyme_pairs[j%2][j\/\/2] for j in range(4)]\n    elif rhyme_scheme == 'AABB':\n        last_words = [rhyme_pairs[j\/\/2][j%2] for j in range(4)]\n    elif rhyme_scheme == 'ABABCDCDEFEFGG':\n        last_words = [None] * 14\n        last_words[-2:] = rhyme_pairs[0]\n        for i in range(3):\n            last_words[4*i : 4*i+4] = [rhyme_pairs[2*i+1+j%2][j\/\/2] for j in range(4)]\n    else:\n        last_words = ['FOLLOW', 'RULES', 'HOLLOW', 'MULES']\n            \n    return last_words, related_words","ef9719c6":"## example output of related rhyming pairs for the topic: wedding\n\nls_wrds1, _ = get_last_words('wedding','sonnet')\n\nprint('\\nList of rhyming pairs for the topic: wedding')\nindices = [0,1,4,5,8,9,12]\nfor i in indices:\n    if i<12:\n        print('%s, %s' % (ls_wrds1[i], ls_wrds1[i+2]))\n    else:\n        print('%s, %s' % (ls_wrds1[i], ls_wrds1[i+1]))","c9f21e28":"## extracting fluent path from FSA with reversed beam search\n\ndef beam_search(last_words, related_words, Meter=5, beam_width=30):\n    '''\n    Gives the entire poem by performing beam search on FSA when the list of last words of poem are provided alongwith the similarity scores\n    \n    Parameters:\n    last_words (list) - list of the related rhyming words in order of their appearance in poem\n    related_words (list) - list of words related to the poem with the similarity scores\n    Meter (int) - gives the number of alternating stress pairs for each line of the poem\n    beam_width (int) - beam_width used in the beam search algorithm\n    \n    Returns:\n    final_poem (list) - final poem generated given as a list of words\n    '''\n\n    topic_similarity = dict(related_words)\n\n    init_state = {'line': len(last_words),\n                  'syll': 0,\n                  'score': 0,\n                  'sent': []}\n    beam = [init_state]\n\n    cnt = 0\n\n    delta = 1e-100\n    repeat_penalty = 3\n    similarity_score_coefficient = 7.5\n    \n    print('Iteration: ', end='')\n\n    while beam[0]['line'] + beam[0]['syll'] > 0: # False if highest scoring poem is accepted\n        new_beam = [s for s in beam if s['line'] + s['syll'] == 0]\n        for state in beam:\n            # syllables not completed in line\n            if state['syll'] > 0:\n                prob = LM(state['sent'][19::-1])\n                \n                # find candidate words at current state\n                cands = [(w, p) for w, p in stress_pattern.items() if len(p) <= state['syll'] and (p[-1] != state['syll'] % 2)]\n                cands.sort(key = lambda wp: -prob.get(wp[0], delta) + np.exp(similarity_score_coefficient * topic_similarity.get(w, 0) - repeat_penalty * state['sent'].count(w)))\n                \n                new_beam.extend([{'line': state['line'],\n                                  'syll': state['syll']-len(p),\n                                  'score': state['score'] + np.log(prob.get(w, delta)) + similarity_score_coefficient * topic_similarity.get(w, 0) - repeat_penalty * state['sent'].count(w),\n                                  'sent': [w] + state['sent']}\n                                for w, p in cands[:beam_width]])\n\n            # syllables completed in line, move to previous line\n            elif state['line'] > 0:\n                lw = last_words[state['line'] - 1]\n                prob = LM(state['sent'][19::-1])\n                new_beam.append({'line': state['line'] - 1,\n                                 'syll': 2 * Meter - len(stress_pattern[lw]) + 1 - stress_pattern[lw][-1],\n                                 'score': state['score'] + np.log(prob.get(lw, delta)),\n                                 'sent': [lw, '.'] + state['sent']})\n\n        beam = sorted(new_beam, key = lambda s: -s['score'])[:beam_width]\n\n        print(cnt, end=' ')\n        cnt += 1\n\n    print('\\n')\n    \n    final_poem = beam[0]['sent']\n    \n    return final_poem # highest scoring accepted poem\n","b3635466":"def print_poem(topic, wl):\n    '''\n    Prints a poem as returned by beam_search\n    \n    Parameters:\n    topic (str) - poem topic\n    w1 (list) - list of words in poem\n    \n    Returns - none\n    '''\n\n    cap = True\n    cnt = 0\n\n    print(topic.capitalize(), '\\n')\n    for w in wl:\n        if w == '.':\n            print()\n            cap = True\n            cnt += 1\n            if cnt % 4 == 0:\n                print()\n        else:\n            print(' '+w.lower() if not cap else w.capitalize(), end='')\n            cap = False\n\ndef generate_poem(topic, rhyme_scheme='ABAB', seed=None, beam_width=30):\n    \n    '''\n    Runner function to generate and print poem given the topic, rhyme scheme, seed for randomness and beam_width for beam search\n    \n    Parameters:\n    topic (str) - poem topic\n    rhyme_schme (str) - can be one of, 'AA', 'ABAB', 'AABB', 'ABABCDCDEFEFGG'. Decides rhyme scheme of poem\n    seed (int) - random seed for generating distinct poems\n    beam_width (int) - beam_width used in the beam search algorithm\n    \n    Returns:\n    w1 (list) - list of words in poem\n    '''\n    \n    if not seed is None:\n        random.seed(seed)\n    last_words, related_words = get_last_words(topic, rhyme_scheme)\n    wl = beam_search(last_words, related_words, beam_width=beam_width)\n    print_poem(topic, wl)\n    return wl","ea659561":"poem1 = generate_poem('mysterious forest', 'sonnet', 1)","35519688":"poem6a = generate_poem('wedding', 'AABB', 321)","99ef58d1":"poem6 = generate_poem('wedding', 'AABB', 123)","3fe8f376":"%%time\npoem6 = generate_poem('wedding', 'AABB', seed=123, beam_width=80)","db10c869":"# Topical Poetry Generation\n\nImplementation of the publication : Ghazvininejad, Marjan, et al. \"Hafez: an interactive poetry generation system.\" Proceedings of ACL 2017, System Demonstrations. 2017.","e72a02aa":"# Vocabulary\n* Extracting list of similar phonemes (for slant rhyme)\n* Defining popular rhyme classes\n* Extracting 30k most frequent words from wiki corpus","42863d82":"# Slant Rhyme\n* Function to check if two given words rhyme (slant) or not","fa7ae96d":"We can notice in the cell below that increasing the beam width does not always improve quality of the poem as it leads to repetitive words","044c43db":"* Finding stress patterns of words\n* Accepting only those with alternating patterns\n* Finding rhyme classes, both strict classes and slant classes","aaf56770":"Loading word embedding (from fasttext) of the most frequent 30k words","ca36939b":"By changing the seed, we can generate distinct poems on the given topic and rhyme scheme","91367f77":"* Getting last words of each poem line for a given topic","c90a6cfc":"# Poems generated\n\nThe cells below are examples of calling ***generate_poem*** given different topics and rhyme schemes","07789e47":"* Importing libraries\n* Defining files to be used","b8a24abe":"# Rhyme Word Selection\n* Extracting list of rhyming word pairs from list of words","0dd081a0":"# Language Model\n* Loading LSTM-RNN language model which is trained on a subset of the Gutenberg poetry dataset\n* Function ***LM*** returns probability distribution of next word given a partial sentence","45425c31":"# Non-topical Rhyming Words\n* Creating dictionary of all strict rhyming classes containing most frequent words in those classes","8f93b1fe":"# Path Extraction\n* Applying beam search on FSA with scoring according to the language model to extract a fluent path, given the last word of each line","a4d2f26b":"* Function ***print_poem*** prints a poem as returned by ***beam_search***\n* Function ***generate_poem*** performs all required steps, given a topic and rhyme scheme, along with beam width and random seed\n* argument *rhyme_scheme* can be:\n    * 'AA'\n    * 'AABB'\n    * 'ABAB'\n    * 'ABABCDCDEFEFGG' or 'sonnet'","9405c3dd":"# Topically Related Words\n* Computing cosine similarity between words using fasttext embeddings\n* Finding words related to a given topic, using these similarities"}}