{"cell_type":{"399364ad":"code","b64890a2":"code","91f2643b":"code","4d85a087":"code","859fbcfc":"code","37cf2047":"code","867df13f":"code","02a9e06c":"code","d036fce7":"code","64504f5b":"code","c96845b9":"code","53074244":"code","2ac5c25a":"code","4ec8ec7a":"code","43b0164d":"code","aec63692":"code","2876a9ff":"code","5f659de3":"code","fc72ebf5":"code","a3c828d0":"code","268d08b6":"code","7db3ad28":"code","c7f3e8d3":"code","5da47d78":"code","53026036":"code","d4b2754b":"code","058449ce":"code","95d68096":"code","bc192fc1":"code","e19ee11e":"code","3e3455c3":"code","91e4b84c":"code","bf2b49b6":"code","fe16a4cb":"code","e174ca7d":"markdown"},"source":{"399364ad":"import gc\ngc.collect()\n\nimport pandas as pd\nimport numpy as np\n # Manually Annotated dataset from the above mentioned paper\ndf0 = pd.read_csv('..\/input\/dataset-for-french\/profilesmanualannotation.csv') ","b64890a2":"import collections\ndf1 = df0[['UserId', 'party']] #Trimming down the first dataset\nfr = pd.read_csv('..\/input\/annotatedfriends\/manualannotationFriends.csv', names=['id', 'friend']) #Dataset of Friends\nfr.drop_duplicates(inplace = True)\nFeatures = pd.read_csv('..\/input\/features\/possibleFeatures.csv' , names=['friend'])\n#mlp_features = pd.read_csv('..\/input\/mlp-importantfeatures\/temp20 (1).csv')\n#mlp_features = mlp_features['friend']\n#Features","91f2643b":"#Seperating all the parties\nfnFriends = pd.merge(df0[df0['party'] == 'fn'], fr, how = 'inner', left_on='UserId' , right_on = 'id')\nfiFriends = pd.merge(df0[df0['party'] == 'fi'], fr, how = 'inner', left_on='UserId' , right_on = 'id')\nlrFriends = pd.merge(df0[df0['party'] == 'lr'], fr, how = 'inner', left_on='UserId' , right_on = 'id')\nemFriends = pd.merge(df0[df0['party'] == 'em'], fr, how = 'inner', left_on='UserId' , right_on = 'id')\npsFriends = pd.merge(df0[df0['party'] == 'ps'], fr, how = 'inner', left_on='UserId' , right_on = 'id')","4d85a087":"fnFriendCount = fnFriends.groupby(['friend']).count()\nfiFriendCount = fiFriends.groupby(['friend']).count()\nlrFriendCount = lrFriends.groupby(['friend']).count()\nemFriendCount = emFriends.groupby(['friend']).count()\npsFriendCount = psFriends.groupby(['friend']).count()","859fbcfc":"#listDf = []\n#for i in range(6,7):\n#    listDf = []\n#    listDf = (fnFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n#    fiFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n#    lrFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n#    emFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n#    psFriendCount.nlargest(i, 'UserId').index.values.tolist())\n#    joe = pd.DataFrame({'friend' :listDf})\n#    jpott = pd.merge(joe, fr, how = 'inner' , left_on = 'friend', right_on = 'friend' )\n#    DicList = []\n#    for group, frame in jpott.groupby('id'):\n        \n#        ak = frame['friend'].tolist()\n#        dictOf = dict.fromkeys(ak , 1)\n#        DicList.append(dictOf)\n#    print(DicList[0])\n#    from sklearn.feature_extraction import DictVectorizer\n#    dictvectorizer = DictVectorizer(sparse = True)\n#    features = dictvectorizer.fit_transform(DicList)\n    #print(features)\n    #print(features.todense().shape)\n#    dataFrame = pd.SparseDataFrame(features, columns = dictvectorizer.get_feature_names(), \n#                               index = jpott['id'].unique())\n#    dataFrame.index.names = ['UserId']\n#    print(dataFrame.head(1))\n#    print(jpott['id'].unique())\n    #print(jpott['id'] == 1278286086)\n    ","37cf2047":"print(fr[fr['id']==1278286086])","867df13f":"#print(listDf)","02a9e06c":"joinedDF1 = pd.read_csv('..\/input\/top6features\/profile_manual_top6features (1).csv')\njoinedDF1.shape","d036fce7":"print(joinedDF1.friend.unique())","64504f5b":"joinedDF1.friend.unique()","c96845b9":"accuracyScore = []\ncoreFeatures = []\nlistDf = []\nfor i in range(1, 50):\n    listDf = []\n    listDf = (fnFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n    fiFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n    lrFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n    emFriendCount.nlargest(i, 'UserId').index.values.tolist() + \n    psFriendCount.nlargest(i, 'UserId').index.values.tolist())\n    joinDF = pd.DataFrame({'friend' :listDf})\n    joinedDF = pd.merge(joinDF, fr, how = 'inner' , left_on = 'friend', right_on = 'friend' )\n    \n    #print(joinedDF.head(1))\n    #merged = joinedDF1.merge(joinedDF, indicator=True, how='outer')\n    #print(merged[merged['_merge'] == 'right_only'])\n    #print(joinedDF.shape)\n    DicList = []\n    indexTobe = []\n    for group, frame in joinedDF.groupby('id'):\n        ak = frame['friend'].tolist()\n        indexTobe.append(group)\n        #break\n        dictOf = dict.fromkeys(ak , 1)\n        DicList.append(dictOf)\n    #print(DicList[0])\n    from sklearn.feature_extraction import DictVectorizer\n    dictvectorizer = DictVectorizer(sparse = True)\n    features = dictvectorizer.fit_transform(DicList)\n    features.todense().shape\n    dataFrame = pd.SparseDataFrame(features, columns = dictvectorizer.get_feature_names(), \n                               index = indexTobe)\n    #print(joinedDF['id'].unique())\n    dataFrame.index.names = ['UserId']\n    #print(dataFrame.head())\n    mergedWithParties = pd.merge(dataFrame , df0, left_on = 'UserId', right_on = 'UserId', how= 'inner')\n    mergedWithParties.drop(columns=['mediaConnection', 'gender', 'profileType'], inplace = True)\n    mergedWithParties.fillna(0, inplace = True)\n    #print('Before')\n    #print(mergedWithParties.sample(random_state = 2))\n    parties = {'fi': 1,'ps': 2,'em': 3,'lr': 4,'fn': 5,'fi\/ps': 6,'fi\/em': 7, 'fi\/lr': 8,'fi\/fn': 9, 'ps\/em': 10,\n    'ps\/lr': 11, 'ps\/fn': 12, 'em\/lr': 13,'em\/fn': 14, 'lr\/fn': 15}\n    #print(df1['party'])\n\n    mergedWithParties['party'] = mergedWithParties['party'].map(parties)\n    #print('After')\n    #print(mergedWithParties.sample(random_state = 2))\n    sanityCheck = pd.concat([mergedWithParties['UserId'],  mergedWithParties['party']], axis = 1)\n    sanityCheck2 =  pd.concat([df0['UserId'] ,df0['party']], axis = 1)\n    pd.set_option('display.max_columns', None)\n    #print(pd.concat([sanityCheck, sanityCheck2], axis = 1))\n    #sanity = sanityCheck.merge(sanityCheck2, indicator=True, how='outer')\n    #print(merged[merged['_merge'] == 'right_only'])\n    #print(merged.shape)\n    \n    mergedWithParties2 = mergedWithParties[(mergedWithParties['party']==1.0) | (mergedWithParties['party']==2.0)|\n                                      (mergedWithParties['party']==3.0)| (mergedWithParties['party']==4.0)\n                                      | (mergedWithParties['party']==5.0)]\n    \n    #print(mergedWithParties2[mergedWithParties2['party'] == 1.0])\n    #print(mergedWithParties2[mergedWithParties2['party'] == 2.0])\n    #print(mergedWithParties2[mergedWithParties2['party'] == 3.0].shape)\n    #print(mergedWithParties2[mergedWithParties2['party'] == 4.0].shape)\n    #print(mergedWithParties2[mergedWithParties2['party'] == 5.0].shape)\n    \n    from sklearn.ensemble import  RandomForestClassifier\n    from sklearn.feature_selection import SelectFromModel\n    from sklearn.model_selection import train_test_split\n\n    train, test = train_test_split(mergedWithParties2, test_size=0.2, shuffle=True)\n    featureSelector = RandomForestClassifier(n_estimators=50)\n    featureSelector.fit(train.iloc[:, :-1], train['party'])\n    accScore = featureSelector.score(test.iloc[:, :-1],pd.Series(test['party']))\n    accuracyScore.append(accScore)\n    coreFeatures.append(dictvectorizer.get_feature_names())","53074244":"print(accuracyScore)\nprint(coreFeatures)","2ac5c25a":"print(coreFeatures[15])","4ec8ec7a":"print(listDf)","43b0164d":"joinedDF = pd.read_csv('..\/input\/top6features\/profile_manual_top6features (1).csv')\n#joinedDF = pd.merge(Features, fr, left_on = 'friend', right_on = 'friend', how= 'inner')\n#joinedDF.shape","aec63692":"#mergedWithParties = pd.merge(joinedDF , df0, left_on = 'id', right_on = 'UserId', how= 'inner')\n#mergedWithParties[mergedWithParties['party'] == 'fi'].nunique()\n","2876a9ff":"DicList = []\nfor group, frame in joinedDF.groupby('id'):\n    ak = frame['friend'].tolist()\n    dictOf = dict.fromkeys(ak , 1)\n    DicList.append(dictOf)\n\nfrom sklearn.feature_extraction import DictVectorizer\ndictvectorizer = DictVectorizer(sparse = True)\nfeatures = dictvectorizer.fit_transform(DicList)\nfeatures.todense().shape","5f659de3":"dataFrame = pd.SparseDataFrame(features, columns = dictvectorizer.get_feature_names(), \n                               index = joinedDF['id'].unique())\n","fc72ebf5":"dataFrame.index.names = ['UserId']\ndataFrame.head()\n","a3c828d0":"mergedWithParties = pd.merge(dataFrame , df0, left_on = 'UserId', right_on = 'UserId', how= 'inner')","268d08b6":"mergedWithParties.head()","7db3ad28":"mergedWithParties.drop(columns=['mediaConnection', 'gender', 'profileType'], inplace = True)","c7f3e8d3":"mergedWithParties.fillna(0, inplace = True)","5da47d78":"mergedWithParties.head()","53026036":"parties = {'fi': 1,'ps': 2,'em': 3,'lr': 4,'fn': 5,'fi\/ps': 6,'fi\/em': 7, 'fi\/lr': 8,'fi\/fn': 9, 'ps\/em': 10,\n'ps\/lr': 11, 'ps\/fn': 12, 'em\/lr': 13,'em\/fn': 14, 'lr\/fn': 15}\n#print(df1['party'])\n\nmergedWithParties['party'] = mergedWithParties['party'].map(parties)\nmergedWithParties.head()","d4b2754b":"mergedWithParties2 = mergedWithParties[(mergedWithParties['party']==1.0) | (mergedWithParties['party']==2.0)|\n                                      (mergedWithParties['party']==3.0)| (mergedWithParties['party']==4.0)\n                                      | (mergedWithParties['party']==5.0)]\nmergedWithParties2.head()","058449ce":"from sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(mergedWithParties2, test_size=0.2, shuffle=True)\nfeatureSelector = RandomForestClassifier(n_estimators=50)\nfeatureSelector.fit(train.iloc[:, :-1], train['party'])","95d68096":"featureSelector.score(test.iloc[:, :-1],pd.Series(test['party']))","bc192fc1":"from sklearn.neighbors import KNeighborsClassifier\nclf2 = KNeighborsClassifier(n_neighbors = 800)\nclf2.fit(train.iloc[:, :-1], train['party'])\nclf2.score(test.iloc[:, :-1],pd.Series(test['party']))\nprint(test.shape)","e19ee11e":"Ypred = pd.Series(clf2.predict(test.iloc[:, :-1]))\n","3e3455c3":"allLabels = mergedWithParties2['party'].unique()\nprint(allLabels)","91e4b84c":"pred = list(pd.Series(clf2.predict(test.iloc[:, :-1])))\nprint(len(pred))\ntestY = list(pd.Series(test['party']))\nlabels = list(allLabels)\nfrom collections import Counter\nprint(Counter(pred))\nprint(Counter(testY))","bf2b49b6":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(pred, testY)\n","fe16a4cb":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport networkx as nx","e174ca7d":"Merging datasets of mlp-features and friends."}}