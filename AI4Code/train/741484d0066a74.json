{"cell_type":{"28f3bc2e":"code","d689caec":"code","0d28b8c5":"code","09d8c4e9":"code","2ef22ccb":"code","9b2e9c7d":"code","5a197e79":"code","f009251b":"code","55d56217":"code","6d3fdd18":"code","e530f6fa":"code","169088bb":"code","6ed8f7b1":"code","660606ea":"code","725e0c21":"code","a87f3c4e":"code","02c79541":"code","b5bfa886":"code","0cdbc7c8":"code","a1322633":"code","d78bd9c5":"code","41fc45bf":"code","370c31e1":"code","9f0bec06":"code","eaa0209f":"code","f27affbe":"code","335ddfc0":"code","7357513d":"code","a48d37f7":"code","a647c0c4":"code","e04d923a":"code","2c9f7999":"code","8fad0a4c":"code","9e125a7f":"code","d2f263dc":"code","88f96956":"code","b2204611":"code","ee0f12f1":"code","4bb3c151":"code","a0d4808b":"code","0c066b03":"code","7a9629ea":"code","364145ab":"code","90bde990":"code","5178dd90":"code","7ac46ba9":"code","5b972948":"code","514f5c81":"code","acd46aad":"code","141de1d5":"markdown","d515eb91":"markdown","86c85806":"markdown","8fbc1ea3":"markdown","1e627b91":"markdown","a65fa865":"markdown","25280481":"markdown","e773254f":"markdown","8299ac43":"markdown","9973b48e":"markdown","90459d36":"markdown","8b387f8b":"markdown","4d2c904b":"markdown","b0cc8a38":"markdown","85b2c9d2":"markdown","5e623f6c":"markdown","aa9246a0":"markdown","d6fc2fa0":"markdown","1e2969ba":"markdown","655fac15":"markdown","7bca4ed8":"markdown","cb524f27":"markdown","3bd6cd69":"markdown","3767c55d":"markdown","6e40d101":"markdown","27947056":"markdown"},"source":{"28f3bc2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d689caec":"train = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ntest = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict.csv')","0d28b8c5":"def show_the_data(data):\n    data.info()\n    print(\"\\n\\nThe columns are {}\".format(data.columns))\n    print(\"\\n\\nActual Data\\n\",data.head(2))","09d8c4e9":"def categorize(data,col):\n    numerical,category=[],[]\n    for i in col:\n        if data[i].dtype ==object:\n            category.append(i)\n        else:\n            numerical.append(i)\n    print(\"The numerical features {}:\".format(numerical))\n    print(\"The categorical features {}:\".format(category))\n    return category,numerical","2ef22ccb":"def get_correlated(cor):\n    correlated =set()\n    for i in cor.columns:\n        for j in cor.columns:\n            if cor[i][j]>0.8  and i!=j:\n                correlated.add(i)\n                correlated.add(j)\n    print(\"The Correlated columns: {}\".format(list(correlated)))\n    return correlated","9b2e9c7d":"show_the_data(train)","5a197e79":"show_the_data(test)","f009251b":"cat,num = categorize(train,train.columns)","55d56217":"print(\"There are {} categorical features in train dataset\".format(len(cat)))\nprint(\"There are {} numerical features in train dataset\".format(len(num)))","6d3fdd18":"cat1,num1 = categorize(test,test.columns)","e530f6fa":"print(\"There are {} categorical features in test dataset\".format(len(cat1)))\nprint(\"There are {} numerical features in test dataset\".format(len(num1)))","169088bb":"train.isnull().sum()","6ed8f7b1":"test.isnull().sum()","660606ea":"plt.figure(figsize = (10,10))\nplt.subplot(2,2,1)\n#train['Chance of Admit '].hist()\nsns.distplot(train['Chance of Admit '],bins=50,color='Violet',  kde_kws={\"color\": \"g\", \"lw\": 5, \"label\": \"KDE\"},hist_kws={\"linewidth\": 5,\"alpha\": 0.8 })\nplt.subplot(2,2,2)\nsns.boxplot(train['Chance of Admit '])","725e0c21":"corr = get_correlated(train.corr())","a87f3c4e":"if len(corr) == len(train.columns):\n    print(\"ALL THE FEATURES ARE HIGHELY CORRELATED!!!\")\nelse:\n    print(\"THERE ARE SOME FEATURES WITH LOW CORRELATION...\")","02c79541":"plt.figure(figsize =(10,10))\nsns.heatmap(train.corr(),annot= True,cmap = 'rocket')","b5bfa886":"data = train[corr]\ntest_data = test[corr]","0cdbc7c8":"sns.pairplot(data)","a1322633":"\ndata.describe().T","d78bd9c5":"\ndef outlier(data):\n    out1=[]\n    for col in data.columns:\n        outliers =[]\n        mean = data[col].mean()\n        std = data[col].std()\n        for i in data[col]:\n                z = (i - mean)\/std\n                if z>2:\n                    outliers.append(i)\n        out1.append(list(outliers))\n        print(\"There are {} outliers in {} feature\".format(len(outliers),col))\n    return out1","41fc45bf":"out = outlier(data)","370c31e1":"j =0\ncolumns =data.columns\nfor i in out:\n    for val in data[columns[j]]:\n        if val in i:\n            data[columns[j]]= data[columns[j]].replace(val,np.nan)\n    j =j+1","9f0bec06":"data.isnull().sum()","eaa0209f":"data.dropna(axis = 0,inplace =True)","f27affbe":"data.info()","335ddfc0":"fig = px.density_contour(data, x=\"CGPA\", y=\"Chance of Admit \")\nfig.show()","7357513d":"plt.figure(figsize = (10,10))\nsns.jointplot(data=data, x=\"TOEFL Score\", y=\"Chance of Admit \",color='Indigo', marker=\"*\", s=100)","a48d37f7":"#px.area(data, x=\"GRE Score\", y=\"Chance of Admit \")\nfig = px.scatter(data, x=\"GRE Score\", y=\"Chance of Admit \", marginal_y=\"box\", marginal_x=\"histogram\")\nfig.show()","a647c0c4":"fig = px.scatter_3d(data, x=\"CGPA\", y=\"GRE Score\", z=\"TOEFL Score\", hover_name=\"Chance of Admit \",)\nfig.show()","e04d923a":"toppers=data[data['CGPA']>=9.5].sort_values(by=['CGPA'],ascending=False)\nprint('There are {} university toppers'.format(len(toppers)))\nsns.barplot(x='CGPA',y='Chance of Admit ',data=toppers, linewidth=1.5,edgecolor=\"0.1\")","2c9f7999":"GREtoppers=data[data['GRE Score']>=330].sort_values(by=['GRE Score'],ascending=False)\nprint('There are {} GRE toppers'.format(len(GREtoppers)))\nsns.barplot(x='GRE Score',y='Chance of Admit ',data=GREtoppers, linewidth=1.5,edgecolor=\"0.1\")","8fad0a4c":"Toefltoppers=data[data['TOEFL Score']>=115].sort_values(by=['TOEFL Score'],ascending=False)\nprint('There are {} TOEFL toppers'.format(len(Toefltoppers)))\nsns.barplot(x='TOEFL Score',y='Chance of Admit ',data=Toefltoppers, linewidth=1.5,edgecolor=\"0.1\")","9e125a7f":"y  = data['Chance of Admit ']\nt_test = test_data['Chance of Admit ']\ndata.drop(['Chance of Admit '],axis =1,inplace =True)\ntest_data.drop(['Chance of Admit '],axis = 1,inplace = True)","d2f263dc":"from sklearn.preprocessing import StandardScaler as SS\nss = SS()\ndata_ss = ss.fit_transform(data)\nss1 = SS()\ntest_ss = ss.fit_transform(test_data)","88f96956":"from sklearn.linear_model import LinearRegression as LR\nfrom sklearn.tree import DecisionTreeRegressor as DTR\nfrom sklearn.ensemble import RandomForestRegressor as RFR\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nfrom sklearn.ensemble import AdaBoostRegressor as ABR\nfrom sklearn.neighbors import KNeighborsRegressor as KNR\nfrom sklearn.svm import SVR \nfrom sklearn.linear_model import Ridge as RR\nfrom sklearn.metrics import r2_score","b2204611":"key = ['Linear Regression','Decision Tree Regression','Random Forest Regression','Gradient Boosting Regression','Ada Boosting Regression','K-Neighbors Regression','Support Vector Regression','Lasso Regression']\nvalue = [LR(),DTR(),RFR(),GBR(),ABR(),KNR(),SVR(),RR()]\npred=[]\nmodels = dict(zip(key,value))\nprint(models)","ee0f12f1":"for name,algo in models.items():\n    model=algo\n    model.fit(data_ss,y)\n    predictions = model.predict(test_ss)\n    acc=r2_score(t_test, predictions)\n    pred.append(acc)\n    print(name,acc)","4bb3c151":"sns.barplot(y=key,x=pred,linewidth=1.5,orient ='h',edgecolor=\"0.1\")","a0d4808b":"n_neighbors = list(np.arange(1,6))\nweights = ['uniform','distance']\nalgorithm = ['auto','ball_tree','kd_tree','brute']\nmetric =['euclidean','manhattan','chebyshev','minkowski']\np =[1,2]\nleaf_size = list(np.arange(20,200,40))\nrandom_grid = {'n_neighbors':n_neighbors,'weights':weights,'p':p,'leaf_size':leaf_size,'algorithm':algorithm,'metric':metric}\nprint(random_grid)\n","0c066b03":"from sklearn.model_selection import RandomizedSearchCV\nrf = KNR()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(data_ss, y)","7a9629ea":"rf_random.best_estimator_","364145ab":"final = KNR(algorithm='kd_tree', leaf_size=60, metric='chebyshev',\n                    metric_params=None, n_jobs=None, n_neighbors=5, p=1,\n                    weights='distance')\nfinal.fit(data_ss,y)","90bde990":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","5178dd90":"base_model = KNR()\nbase_model.fit(data_ss, y)\nbase_accuracy = evaluate(base_model, test_ss, t_test)","7ac46ba9":"best_model = final\nbest_accuracy = evaluate(best_model, test_ss, t_test)","5b972948":"print('Improvement of {:0.2f}%.'.format( 100 * (best_accuracy - base_accuracy) \/ base_accuracy))","514f5c81":"final = best_model\npred = final.predict(test_ss)\npred[:20]","acd46aad":"sns.distplot(t_test,hist=False,label = 'Actual')\nsns.distplot(pred,hist=False, label ='Predicted')\nplt.legend(loc=\"upper left\")\nplt.xlabel('Prediction level')\n","141de1d5":"One thing about this dataset is that it doesn't have any categorical values therefore there is no need to perform categorical to numerical conversion which is one of the crucial steps in Feature Engineering","d515eb91":"Func: categorize","86c85806":"# Plotting the accuracy of each model","8fbc1ea3":"![image.png](attachment:image.png)","1e627b91":"# Lets Seperately Analyse Academic, TOEFL and GRE Toppers","a65fa865":"# Dividing the data into dependent and independent features","25280481":"Func: show_the_data","e773254f":"# Without any hyper parameter tuning we found that KNN Algorithm is good in terms of accuracy. Let's Perform Hyper Parameter Tuning on it","8299ac43":"Welcome to my new kernal, in this kernal we are going to perform a lot of interesting things and exploration on the graduation-admission dataset. So,let's dive into it --->","9973b48e":"Func: get_correlated","90459d36":"**Thanks for viewing my work. Please upvote this if you like it and share this to your friends for whom this will be helpfull!!!!.**","8b387f8b":"The good news is that we don't have any missing values so now we can exclude handling missing values step which i one another important step in feature engineering.","4d2c904b":"# Visualizing the data","b0cc8a38":"**To Delete the ouliers I will first replace all the outliers values with np.nan and will delete them using dropna method**","85b2c9d2":"# <center>3-D Visualization of CGPA, GRE Score and TOEFL Score<\/center>","5e623f6c":"As there is a improvement we will use our best model  as  our final model","aa9246a0":"![image.png](attachment:image.png)","d6fc2fa0":"# Let's try some machine learning models","1e2969ba":"# Helper Functions","655fac15":"# <center>LET'S CHECK FOR MISSING VALUES<\/center>","7bca4ed8":"NOW OUR JOB IS VERY SIMPLE\n* ONLY TAKE THE HIGHLY CORRELATED FEATURES\n* EXAMINE THEIR DISTRIBUTION AND SPREAD\n* PERFORM SCALING AND NORMALIZATION\n* CREATE MODELS\n* PERFORM CROSS VALIDATION\n* TUNE THEM","cb524f27":"# Evaluating the effect of hyperparmeter optimization","3bd6cd69":"# OUTLIER DETECTION USING Z-SCORE METHOD","3767c55d":"# <center>Visualizing the Target Feature's Distribution<\/center>","6e40d101":"# Performing Standardization","27947056":"<b>As we are having a lot of numerical features it's our main job to find out the highly correlated features so that we can perform further analysis<\/b>"}}