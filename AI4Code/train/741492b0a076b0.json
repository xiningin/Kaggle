{"cell_type":{"36f39d30":"code","6942f58e":"code","38c77d2a":"code","b607cbac":"code","29c39044":"code","b6e58925":"code","d2c9aecd":"code","ebfd4b53":"code","0e2d2ff9":"code","e7202b59":"code","7728be14":"code","dccd181e":"code","1103d488":"code","1a65cffb":"code","23d00899":"code","ff3ae110":"code","03a500e8":"code","bea49590":"code","bd26e465":"code","0b0ad497":"code","6c9cadae":"code","5784943f":"markdown","82d4c90c":"markdown","e18cfaea":"markdown","cd60dbbb":"markdown","218a0cc0":"markdown","00a57914":"markdown","a1c76016":"markdown","2e7a35d6":"markdown","79e89ca6":"markdown","265d5a86":"markdown","9ede2e04":"markdown","02c90fb8":"markdown","260111dc":"markdown","1373c329":"markdown","f6cab878":"markdown","d76da64c":"markdown","673273b4":"markdown","6c59ae23":"markdown","acb6f400":"markdown","2e1d8dad":"markdown","9c955e81":"markdown","2099da76":"markdown","600557b1":"markdown","42d0f3d5":"markdown"},"source":{"36f39d30":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style = \"whitegrid\", \n        color_codes = True,\n        font_scale = 1.5)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6942f58e":"original_training_data = pd.read_csv('..\/input\/email-data\/train.csv')\ntest = pd.read_csv('..\/input\/email-data\/test.csv')\n\n# Converting the emails to lower case as a first step to processing the text\noriginal_training_data['email'] = original_training_data['email'].str.lower()\ntest['email'] = test['email'].str.lower()\n\noriginal_training_data.head()","38c77d2a":"nulls = original_training_data[original_training_data['subject'].isnull() | original_training_data['email'].isnull()]\nprint('Null values:', len(nulls))\n\n# Replacing null values with empty strings\noriginal_training_data = original_training_data.fillna('')\nnew_nulls = original_training_data[original_training_data['subject'].isnull() | original_training_data['email'].isnull()]\nprint('Null values:', len(new_nulls))","b607cbac":"hams = original_training_data[original_training_data['spam'] == 0]\nspams = original_training_data[original_training_data['spam'] == 1]\n\nfirst_ham = hams['email'].iloc[0]\nfirst_spam = spams['email'].iloc[0]\nprint('First Ham Email')\nprint(first_ham)\nprint('First Spam Email')\nprint(first_spam)","29c39044":"from sklearn.model_selection import train_test_split\n\ntrain, val = train_test_split(original_training_data, test_size=0.1, random_state=42)","b6e58925":"def words_in_texts(words, texts):\n    '''\n    Args:\n        words (list-like): words to find\n        texts (Series): strings gto search in\n    \n    Returns:\n        NumPy array of 0s and 1s with shape (n, p) where n is the\n        number of texts and p is the number of words.\n    '''\n    indicator_array = [int(word in text) for text in texts for word in words]\n    indicator_array = np.array_split(np.array(indicator_array), len(texts))\n    return np.array(indicator_array)\n    \nwords_in_texts(['hello', 'bye', 'world'], ['hello', 'hello worldhello'])","d2c9aecd":"# Preserve ordering of labels\ntrain=train.reset_index(drop=True)\n\n# Selected words to distinguish between spam and ham emails\nwords = ['free', 'credit', 'offer', '$', 'money', 'click', 'opportunity', 'bank', 'receive', 'dear', 'best', 'deal']\n\n# Finding the proportions of spam and ham emails per word\ncontains = words_in_texts(words, train['email'])\ncontains = pd.DataFrame(data=contains, columns=words)\ncontains['label'] = train['spam']\ncontains['label'] = contains['label'].replace(0, 'Ham').replace(1, 'Spam')\ncontains = contains.melt('label')\ncontains = contains.groupby(['label', 'variable']).mean().reset_index()\ncontains\n\nplt.figure(figsize=(10,5)) \nsns.barplot(x=contains['variable'], y=contains['value'], hue=contains['label'])\nplt.title('Frequency of Words in Spam\/Ham Emails per Word')\nplt.ylim(0,1)\nplt.xticks(rotation=27);","ebfd4b53":"# Getting lengths of each email and attaching the email's label to its length\nlengths = train['email'].apply(len)\nlengths = pd.DataFrame(data={'lengths': lengths, 'label': train['spam']}).melt('label')\nlengths['label'] = lengths['label'].replace(0, 'Ham').replace(1, 'Spam')\n\nhams = lengths[lengths['label'] == 'Ham']\nspams = lengths[lengths['label'] == 'Spam']\n\nplt.xlim(0, 50000)\nsns.distplot(hams['value'], label='Ham', hist=False)\nsns.distplot(spams['value'], label='Spam', hist=False)\nplt.xlabel('Length of email body')\nplt.ylabel('Distribution')\nplt.title('Length Distributions for Ham and Spam Emails');","0e2d2ff9":"some_words = ['drug', 'bank', 'prescription', 'memo', 'private']\n\nX_train = words_in_texts(some_words, train['email'])\nY_train = train['spam']\n\nX_train[:5], Y_train[:5]","e7202b59":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\nmodel.fit(X_train, Y_train)\n\ntraining_accuracy = model.score(X_train, Y_train)\nprint(\"Training Accuracy: \", training_accuracy)","7728be14":"Y_train_predictions = model.predict(X_train)\n\nTP = np.count_nonzero((Y_train_predictions == Y_train) & (Y_train_predictions == 1))\nTN = np.count_nonzero((Y_train_predictions == Y_train) & (Y_train_predictions == 0))\nFP = np.count_nonzero((Y_train_predictions != Y_train) & (Y_train_predictions == 1))\nFN = np.count_nonzero((Y_train_predictions != Y_train) & (Y_train_predictions == 0))\n\nlogistic_predictor_precision = TP \/ (TP + FP)\nlogistic_predictor_recall = TP \/ (TP + FN)\nlogistic_predictor_far = FP \/ (FP + TN)\n\nprint('Precision:', logistic_predictor_precision)\nprint('Recall:', logistic_predictor_recall)\nprint('False Alarm Rate:', logistic_predictor_far)","dccd181e":"training = pd.read_csv('..\/input\/email-data\/train.csv')\ntraining = training.fillna('')","1103d488":"# Proportion of exclamation points\nhams = training[training['spam'] == 0]\nspams = training[training['spam'] == 1]\n\nham_punc = hams['email'].str.findall('!').str.len()\nspam_punc = spams['email'].str.findall('!').str.len()\n\nsns.distplot(ham_punc, label='Ham')\nsns.distplot(spam_punc, label='Spam')\nplt.xlim(0, 30)\nplt.ylabel(\"Proportion of !'s\")\nplt.legend()\nplt.title(\"Exclamation Point Assessment\");","1a65cffb":"# Number of words in subject\nham_sub_len = hams['subject'].str.findall('\\w+').str.len().fillna(0)\nspam_sub_len = spams['subject'].str.findall('\\w+').str.len().fillna(0)\nsns.distplot(ham_sub_len, label='Ham')\nsns.distplot(spam_sub_len, label='Spam')\nplt.xlim(-5, 25)\nplt.ylabel('Proprotion of Subject Length')\nplt.title(\"Subject Length Assessment\");\nplt.legend();","23d00899":"# Number of capital letters in subject\nsub_ham_cap = hams['subject'].str.findall('\\W+').str.len()\nsub_spam_cap = spams['subject'].str.findall('\\W+').str.len()\nsns.distplot(sub_ham_cap, label='Ham')\nsns.distplot(sub_spam_cap, label='Spam')\nplt.xlim(0, 30)\nplt.ylabel('Proportion of Capital Letters')\nplt.title(\"Capital Letter Assessment in Subject\");\nplt.legend();","ff3ae110":"# Number of capital letters in body\nham_cap = hams['email'].str.findall('\\W+').str.len()\nspam_cap = spams['email'].str.findall('\\W+').str.len()\nsns.distplot(ham_cap, label='Ham')\nsns.distplot(spam_cap, label='Spam')\nplt.xlim(0, 3500)\nplt.ylabel('Proportion of Capital Letters')\nplt.title(\"Capital Letter Assessment in Email Body\");\nplt.legend();","03a500e8":"# Reply ('RE:'), FWDs ('FWD:'), and URLs ('URL:')\nreplies = np.array(training['subject'].str.count(\"Re:\"))\nfwds = np.array(training['subject'].str.count(\"FWD:\"))\nurls = np.array(training['email'].str.count(\"URL:\"))\n\nprint('Number of Re subjects:', sum(training['subject'].str.count('Re:')))\nprint('Number of FWD subjects:', sum(training['subject'].str.count('FWD:')))\nprint('Number of URL emails:', sum(training['email'].str.count('URL:')))\n\n\n# Plotting Re:, FWD:, and URL: for Ham vs Spam emails\ndf = pd.DataFrame(data={'Re:': replies, 'FWD:': fwds, 'URL:': urls})\ndf['label'] = Y_train\ndf['label'] = df['label'].replace(0, 'Ham').replace(1, 'Spam')\ndf = df.melt('label').groupby(('label', 'variable')).mean().reset_index()\n\nsns.barplot(x=df['variable'], y=df['value'], hue=df['label'])\nplt.ylim(0, .7)\nplt.title('FWD, Re, and URL Email Proportions');","bea49590":"# HTML tags\ntag_hams = hams['email'].str.findall('<[^>]*>').str.len()\ntag_spams = spams['email'].str.findall('<[^>]*>').str.len()\n\nsns.distplot(tag_hams, label='Ham')\nsns.distplot(tag_spams, label='Spam')\nplt.xlim(0, 500)\nplt.ylabel('Proportion of HTML tags')\nplt.legend();","bd26e465":"# Finding better words that I initially believe might help distinguishing\nsome_words = ['body', 'business', 'html', 'money', 'offer', 'please', '$', 'click', 'offer', '<td', '<\/tr>', '<br']\n\nX_train = words_in_texts(some_words, training['email'])\nY_train = training['spam']\n\nmodel = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\nmodel.fit(X_train, Y_train)\n\ntraining_accuracy = model.score(X_train, Y_train)\nprint(\"Training Accuracy: \", training_accuracy)","0b0ad497":"# Finding most common words\nham_emails = original_training_data[original_training_data['spam'] == 0]['email']\nspam_emails = original_training_data[original_training_data['spam'] == 1]['email']\n\n\n# HAMS\nham_dic = {}\nfor email in ham_emails:\n    words = email.split(\" \")\n    for word in words:\n        ham_dic[word] = ham_dic[word] + 1 if (word in ham_dic) else 1\n        \nham_word_counts = pd.DataFrame.from_dict(ham_dic, columns = ['count'], orient = 'index')\nmost_common_ham_words = ham_word_counts.sort_values(by='count', ascending=False)\nmost_common_ham_words['words'] = most_common_ham_words.index\nmost_common_ham_words['ham'] = np.arange(len(most_common_ham_words))\nmost_common_ham_words = most_common_ham_words[:40]\n\n\n# SPAMS\nspam_dic = {}\nfor email in spam_emails:\n    words = email.split(\" \")\n    for word in words:\n        spam_dic[word] = spam_dic[word] + 1 if (word in spam_dic) else 1\n        \nspam_word_counts = pd.DataFrame.from_dict(spam_dic, columns = ['count'], orient = 'index')\nmost_common_spam_words = spam_word_counts.sort_values(by='count', ascending=False)\nmost_common_spam_words['words'] = most_common_spam_words.index\nmost_common_spam_words['spam'] = np.arange(len(most_common_spam_words))\nmost_common_spam_words = most_common_spam_words[:40]\n\n\n# HAMS AND SPAMS\nhams_and_spams = pd.merge(most_common_spam_words, most_common_ham_words, how = 'outer', on = 'words')\nhams_and_spams = hams_and_spams.rename(columns={'count_x': 'spam_count', 'count_y': 'ham_count'})\nhams_and_spams = hams_and_spams[['words', 'spam', 'ham']]\n\n\n# VISUAL\nplt.figure(figsize=(10,5)) \nhams_and_spams.plot.bar(x = 'words', y = ['ham', 'spam'], grid = True, figsize=(20,10), width=.5)\nplt.tick_params(axis='both', which='minor', labelsize=6)\nplt.xlabel('Most Common Words')\nplt.ylabel('Rank Determined by Value Counts')\nplt.title('Ham vs Spam Words by Ranking');","6c9cadae":"lr = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\n\n# TRAINING\ngood_words = ['business', 'html', 'money', 'offer', 'please', '<td', 'your', 'you', '!', '$', '%', 'click', \n              'they', 'as', '<\/tr>', 'faq', 'it', 'can', 'has', '^', 'all', 'on', '<font', '<a', 'from', \n              'not','subscribe', 'unsubscribe', 'credit', 'end soon', '% off', 'cheap', 'buy', 'earn',\n              'afford', 'discount', 'free', 'easy', 'claim', 'trial', 'prize', 'win', 'won', 'new', 'sale', \n              'check', 'order', 'limited', 'unlimited', 'visit', 'clearance', 'now','expire', 'member', \n              'cancel', 'cheap', 'dear', 'hello', 'save', 'earn', 'lose', 'profit','auto', 'fast', 'online', \n              '+', 'account', 'book', 'shop', 'check out', 'email','card', 'shipping', 'opportunity', 'bank', \n              'receive', 'dear', 'best', 'deal','help', 'vote', 'spam', 'unsubscribe', 'click', 'signup', 'list',\n              '=', 'fax', 'reply', 'credit card', 'im']\n\n\nreplies = np.array(original_training_data['subject'].str.count('Re:'))\nfwds = np.array(original_training_data['subject'].str.count('FWD:'))\nurls = np.array(original_training_data['subject'].str.count('URL:'))\npunc_sub = np.array(original_training_data['subject'].str.findall('!').str.len())\npunc_body = np.array(original_training_data['email'].str.findall('!').str.len())\ntags = np.array(original_training_data['email'].str.findall('<[^>]*>').str.len())\ncaps = np.array(original_training_data['subject'].str.findall('\\W+').str.len())\ndate = np.array(original_training_data['email'].str.count(\"(date:|Date:)\"))\ncom_net = np.array(original_training_data['email'].str.count(\"(.com|.net|http)\"))\n\nX_train = words_in_texts(good_words, original_training_data['email'])\nX_train = np.append(X_train, replies[:, None], axis=1)\nX_train = np.append(X_train, fwds[:, None], axis=1)\nX_train = np.append(X_train, urls[:, None], axis=1)\nX_train = np.append(X_train, punc_sub[:, None], axis=1)\nX_train = np.append(X_train, punc_body[:, None], axis=1)\nX_train = np.append(X_train, tags[:, None], axis=1)\nX_train = np.append(X_train, caps[:, None], axis=1)\nX_train = np.append(X_train, date[:, None], axis=1)\nX_train = np.append(X_train, com_net[:, None], axis=1)\n\n\nY_train = original_training_data['spam']\nmodel = lr.fit(X_train, Y_train)\n\ntraining_accuracy = model.score(X_train, Y_train)\nprint(\"Training Accuracy: \", training_accuracy)\n\n\n# TESTING\ntest = test.fillna(\"\")\nreplies_test = np.array(test['subject'].str.count('Re:'))\nfwds_test = np.array(test['subject'].str.count('FWD:'))\nurls_test = np.array(test['subject'].str.count('URL:'))\npunc_sub_test = np.array(test['subject'].str.findall('!').str.len())\npunc_body_test = np.array(test['email'].str.findall('!').str.len())\ntags_test = np.array(test['email'].str.findall('<[^>]*>').str.len())\ncaps_test = np.array(test['subject'].str.findall('\\W+').str.len())\ndate_test = np.array(test['email'].str.count(\"(date:|Date:)\"))\ncom_net_test = np.array(test['email'].str.count(\"(.com|.net|http)\"))\n\nX_test = words_in_texts(good_words, test['email'])\nX_test = np.append(X_test, replies_test[:, None], axis=1)\nX_test = np.append(X_test, fwds_test[:, None], axis=1)\nX_test = np.append(X_test, urls_test[:, None], axis=1)\nX_test = np.append(X_test, punc_sub_test[:, None], axis=1)\nX_test = np.append(X_test, punc_body_test[:, None], axis=1)\nX_test = np.append(X_test, tags_test[:, None], axis=1)\nX_test = np.append(X_test, caps_test[:, None], axis=1)\nX_test = np.append(X_test, date_test[:, None], axis=1)\nX_test = np.append(X_test, com_net_test[:, None], axis=1)\n\n\ntest_predictions = model.predict(X_test)","5784943f":"#### For now, only using five words that might be useful as features to distinguish spam\/ham emails to create a feature matrix, X_train, with our words_in_text function.","82d4c90c":"Creating a function that takes in a list of words and a pandas Series of email texts, outputting a 2-dimensional NumPy array containing one row for each email text: the row should contain either a 0 or a 1 for each word in the list, where it is 0 if the word doesn't appear in the text and 1 if the word does.","e18cfaea":"Computing the precision, recall, and false-alarm rate of the logistic regression classifier manually","cd60dbbb":"#### **Precision** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$ of emails flagged as spam that are actually spam.\n\n#### **Recall** measures the proportion $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$ of spam emails that were correctly flagged as spam.\n\n#### **False-alarm rate** measures the proportion $\\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$ of ham emails that were incorrectly flagged as spam.","218a0cc0":"## Increasing Accuracy","00a57914":"## Basic Exploratory Data Analysis\nFinding features for distinguishing spam emails from ham emails by comparing distributions.","a1c76016":"#### A class conditional density plot, comparing the distribution of the length of spam emails to the distribution of the length of ham emails in the training set.","2e7a35d6":"## Spam\/Ham Classifier","79e89ca6":"Cleaning null values","265d5a86":"### Training Validation Split","9ede2e04":"### More EDA","02c90fb8":"## Final Model","260111dc":"Because an issue of the classifier is that the words used so far are not used often within the emails, I am finding the most common words and seeing which have significantly different proportions between ham and spam emails.","1373c329":"## Basic Classification","f6cab878":"#### A bar chart comparing the proportion of spam and ham emails containing certain words that have different proportions for the two classes.","d76da64c":"## Loading the Data","673273b4":"By looking at the first ham and spam emails, the spam email has HTML tags unlike the ham email. The ham email also has a greeting for a specificf person at the end of the email (\"thanks, misha\").","6c59ae23":"## Basic Feature Engineering","acb6f400":"Printing the first ham and first spam emails to identify any initial differences.","2e1d8dad":"### The features I ended up using in my final model are helpful words, replies, fwds, urls, exclamation points (subject and email), html tags, capital letters in subject, and if date is provided.**\n\n### Result: 96% accuracy","9c955e81":"### Evaluating Classifier","2099da76":"Training a logistic regression model using X_train and Y_train to observe the classifier's accuracy","600557b1":"This model aims to predict whether an email is spam or not spam (ham). The dataset consists of email messages and their labels (0 for ham, 1 for spam). The labeled training dataset contains 8348 labeled examples, and the test set contains 1000 unlabeled examples.\n\nThe dataset contains four columns:\n1. `id`: An identifier for the training example\n2. `subject`: The subject of the email\n3. `email`: The text of the email\n4. `spam`: 1 if the email is spam, 0 if the email is ham (not spam)","42d0f3d5":"A reason why the classifier is performing poorly so far is because X_train has many rows with all 0, meaning that the words currently used are not common among the emails."}}