{"cell_type":{"f6e5f4fe":"code","0366e6e3":"code","6608e640":"code","0fb8d168":"code","194bf7d9":"code","c90c8b04":"code","652540fa":"code","30d90c26":"code","c436fd35":"code","12d4e2d5":"code","a45a26ad":"code","9124668a":"code","f877e2fa":"code","e7d10aa1":"code","f764c3ab":"code","6c7fef49":"code","85d80a4d":"markdown"},"source":{"f6e5f4fe":"import tensorflow as tf\nimport random\nimport time\nimport numpy as np\nimport glob\nimport concurrent.futures\nimport matplotlib.pyplot as plt\n%matplotlib inline","0366e6e3":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","6608e640":"train_img_paths = glob.glob('..\/input\/100-bird-species\/train\/*\/*.jpg')\nrandom.shuffle(train_img_paths)\ntrain_label_names = [path.split('\/')[4] for path in train_img_paths]\nprint('\u8bad\u7ec3\u6570\u636e\uff1a',len(train_label_names))","0fb8d168":"label_names = list(sorted(set(train_label_names)))\nlabel_to_index = dict((name,index) for index,name in enumerate(label_names))\nindex_to_label = dict((v,k) for k,v in label_to_index.items())","194bf7d9":"train_index_labels = [label_to_index[name] for name in train_label_names]","c90c8b04":"IMSIZE = 224\ndef read_img(image):\n    img = tf.keras.preprocessing.image.load_img(image, color_mode='rgb', target_size=(IMSIZE, IMSIZE))\n    return img\ndef prepare_dataset(namelist, labels):\n    start = time.time()\n    labels = np.array(labels)\n    labels = tf.convert_to_tensor(labels)\n    labels = tf.expand_dims(labels,axis=-1)\n    labels = tf.cast(labels, tf.int32)\n    imgs = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers = 16) as executor:\n        i = 0\n        for value in executor.map(read_img, namelist):\n            i+=1\n            print(\"\\rFetching: [{}\/{}]\".format(i, len(namelist)), end=\"\", flush=True)\n            imgs.append(value)\n        imgs = np.stack(imgs)\n        imgs = tf.convert_to_tensor(imgs)\n    print(\"\\nExecution time: \",time.time() - start, \"s\")\n    return imgs, labels","652540fa":"with tf.device('\/cpu:0'):\n    TRAIN_SIZE = 12000\n    VAL_SIZE = 2000\n    train_images, train_labels = prepare_dataset(train_img_paths[:TRAIN_SIZE], train_index_labels[:TRAIN_SIZE])\n    val_images, val_labels = prepare_dataset(train_img_paths[TRAIN_SIZE:VAL_SIZE+TRAIN_SIZE], train_index_labels[TRAIN_SIZE:VAL_SIZE+TRAIN_SIZE])\nprint(\"Training Image tensor shape\", train_images.shape)\nprint(\"Training Labels tensor shape\", train_labels.shape)\nprint(\"Validation Image tensor shape\", val_images.shape)\nprint(\"Validation Labels tensor shape\", val_labels.shape)","30d90c26":"def training_map(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_brightness(image, 0.1)\n    #image = tf.image.random_contrast(image, 0, 1)\n    image = tf.cast(image, tf.float32)\n    image = image \/ 255\n    return image, label\n\ndef validation_map(image, label):\n    image = tf.cast(image, tf.float32)\n    image = image \/ 255\n    return image, label","c436fd35":"AUTOTUNE = tf.data.AUTOTUNE\nwith tf.device('\/cpu:0'):\n    BATCH_SIZE = 128\n    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).map(training_map,num_parallel_calls=AUTOTUNE).shuffle(TRAIN_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n    train_dataset = train_dataset.prefetch(buffer_size = AUTOTUNE)\n    # del train_images, train_labels\n    val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).map(validation_map,num_parallel_calls=AUTOTUNE).batch(16,drop_remainder=True)\n    val_dataset = train_dataset.prefetch(buffer_size = AUTOTUNE)\n    # del val_images, val_labels\n    \nprint(\"Training Dataset tensor shape:\", train_dataset)\nprint(\"Validation Dataset tensor shape:\", val_dataset)","12d4e2d5":"def Create_model():\n    Conv_base = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling='avg')\n    Conv_base.trainable = False\n    \n    model = tf.keras.Sequential()\n    model.add(Conv_base)\n    model.add(tf.keras.layers.Dense(1024,activation='relu'))\n    model.add(tf.keras.layers.Dense(250,activation='softmax'))\n    \n    return model","a45a26ad":"strategy = tf.distribute.experimental.TPUStrategy(resolver)\nwith strategy.scope():\n    model = Create_model()\n    optimizer = tf.keras.optimizers.Adam(0.0001)\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.NONE)\n    # metrics\n    training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n    training_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('training_accuracy', dtype=tf.float32)\n    validation_loss = tf.keras.metrics.Mean('validation_loss', dtype=tf.float32)\n    validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('validation_accuracy', dtype=tf.float32)\n    \n    \ndist_train_dataset = strategy.experimental_distribute_dataset(train_dataset)\ndist_val_dataset = strategy.experimental_distribute_dataset(val_dataset)","9124668a":"model.summary()","f877e2fa":"def train_step(inputs):\n    images, labels = inputs\n    with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        per_example_loss = loss_object(labels, logits)\n        loss = tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE)\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    training_loss.update_state(loss * strategy.num_replicas_in_sync)\n    training_accuracy.update_state(labels, logits)\n    return loss \n\n\n@tf.function\ndef distributed_train_step(dist_inputs):\n    per_replica_losses = strategy.run(train_step, args=(dist_inputs,))\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)","e7d10aa1":"def val_step(inputs):\n    images, labels = inputs\n    with tf.GradientTape() as tape:\n        logits = model(images, training=False)\n        per_example_loss = loss_object(labels, logits)\n        loss = tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE)\n    validation_loss.update_state(loss * strategy.num_replicas_in_sync)\n    validation_accuracy.update_state(labels, logits)\n    return loss\n@tf.function\ndef distributed_val_step(dist_inputs):\n    per_replica_losses = strategy.run(val_step, args=(dist_inputs,))\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)","f764c3ab":"def train(epochs):\n    for epoch in range(epochs):\n              \n        start = time.time()\n        i = 0\n        print ('\\nEpoch {}\/{} '.format(epoch+1, epochs))\n        for data in dist_train_dataset:\n            loss = distributed_train_step(data)\n            \n            percent = float(i+1) * 100 \/ len(train_dataset)\n            arrow   = '-' * int(percent\/100 * 10 - 1) + '>'\n            spaces  = ' ' * (10 - len(arrow))\n            print('\\rTraining: [%s%s] %d %% - Training Loss: %f - Training ACC: %f'% (arrow, spaces, percent, training_loss.result(), training_accuracy.result()), end='', flush=True)\n            i += 1\n        i = 0\n        print(\" -\", int(time.time()-start), \"s\", end=\"\")\n        print()\n        start = time.time()\n        for data in dist_val_dataset:\n            loss = distributed_val_step(data)\n            percent = float(i+1) * 100 \/ len(val_dataset)\n            arrow   = '-' * int(percent\/100 * 10 - 1) + '>'\n            spaces  = ' ' * (10 - len(arrow))\n            print('\\rValidate: [%s%s] %d %% - Validation Loss: %f - Validation ACC: %f'% (arrow, spaces, percent, validation_loss.result(), validation_accuracy.result()), end='', flush=True)\n            i += 1\n        print(\" -\", int(time.time()-start), \"s\")\n        \n        # \u91cd\u7f6e metrics\n        training_loss.reset_states()\n        training_accuracy.reset_states()\n        validation_loss.reset_states()\n        validation_accuracy.reset_states()","6c7fef49":"train(30)","85d80a4d":"# **TrainDataset**"}}