{"cell_type":{"f9b86217":"code","8073e1f5":"code","71da2a6d":"code","65f0d6f3":"code","bed92afa":"code","1ee455b0":"code","0a552b54":"code","bfca55c1":"code","6d08f8e2":"code","c330d1eb":"code","54379ca1":"code","6745a427":"code","e5028313":"code","5bc24a64":"code","5cf7a231":"code","39cf8c24":"code","2d120a7d":"code","cf9c6cc2":"code","5fe6e7c2":"code","d045acf5":"markdown","ed8b18b1":"markdown","562f3e9d":"markdown","c406bc2c":"markdown","92c3ed44":"markdown","ea02ab3b":"markdown","803070b7":"markdown"},"source":{"f9b86217":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8073e1f5":"# imports \nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\n","71da2a6d":"train_path = \"\/kaggle\/input\/titanic\/train.csv\"\ntest_path  = \"\/kaggle\/input\/titanic\/test.csv\"\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\n\n# train_data = train.copy()\n# test_data = test.copy()","65f0d6f3":"# A quick peek to the data we are going to handle\ntrain.head(5)","bed92afa":"#Some info about the data\nprint('General information')\ntrain.info()\nprint('--------')\nprint('Percentage of NA per property sorted')\nprint('--------')\np = (train.isna().sum()\/len(train)*100).sort_values(ascending=False)\nprint(p)\nprint('--------')\nprint('Unique values for duplications and other useful info')\nprint('--------')\nu = train.nunique().sort_values()\nprint(u)","1ee455b0":"# Heatmap \nplt.figure(figsize=(12,12))\nsns.heatmap(train.corr(), annot=True, cmap='Blues')\nplt.show()","0a552b54":"sns.heatmap(train.isnull(), yticklabels=False, cbar=False)\nplt.title('Missing values distribution')\nplt.show()","bfca55c1":"# \nPclass=['class1','class2','class3']\nax=sns.countplot(data=train,x='Pclass',hue='Survived')\nplt.xticks(ticks = [0,1,2], labels = Pclass)\nplt.legend(['Deceased', 'Survived'])\nplt.show()","6d08f8e2":"# Check the values of Embarked for manual replacement\ntrain['Embarked'].value_counts()","c330d1eb":"# Check the values of Fare for manual replacement into categories\ntrain['Fare'].value_counts(bins=5)","54379ca1":"def cleanData(data):\n    \n    # Data missing and categorical to drop\n    data.drop(['Cabin','Name','Ticket'], axis=1, inplace=True)\n\n    # Data missing Case2\n    data['Age'] = data.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n    \n    # FARE Data missing in test\n    data['Fare'] = data.groupby(['Pclass','Sex'])['Fare'].transform(lambda x: x.fillna(x.median()))\n\n    # Data missing Case3\n    data.dropna(axis=0, subset=['Embarked'], inplace=True)\n    \n    # Categorical Data\n    le = preprocessing.LabelEncoder()\n    \n    # Sex\n    data['Sex'].replace({'male':0, 'female':1}, inplace=True)\n    \n    # Embarked\n    data['Embarked'].replace({'S':0, 'C':1, 'Q':2}, inplace=True)\n    \n    return data","6745a427":"clean_train = cleanData(train)\nclean_test = cleanData(test)","e5028313":"clean_train.info()\nclean_test.info()","5bc24a64":"# Set X and y\ny = train['Survived']\nX = pd.get_dummies(train.drop('Survived', axis=1))\n\n# # Polynomial features\n# features = PolynomialFeatures(degree=2)\n# X = features.fit_transform(X)\n\n# # Standard Scaler\n# sc = StandardScaler()\n# X = sc.fit_transform(X)\n\n# Split model train test data\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.2, random_state=42)\n","5cf7a231":"def fitAndPredict(model):\n    \"\"\"The following code makes faster to evaluate a model \n    automating the fit and accuracy process\"\"\"\n    \n    model.fit(X_train, y_train)\n    prediction = model.predict(X_val)\n    return accuracy_score(y_val, prediction)","39cf8c24":"#Lets some models\nmodel1 = LogisticRegression(solver='liblinear', random_state=42)\nmodel2 = GradientBoostingClassifier()\nmodel3 = RandomForestClassifier(n_estimators=50)\nmodel4 = SGDClassifier()\nmodel5 = SVC()\n\nmodels = [model1, model2, model3, model4, model5]\ni = 0\nfor model in models:\n    i +=1\n    print(\"Model \", i,\":\", model)\n    print(\"ACC: \", fitAndPredict(model))","2d120a7d":"#As long as GradientBoost is the best of the tried ones lets tune it a bit\nmodel = GradientBoostingClassifier(min_samples_split=20, min_samples_leaf=60, max_depth=4, max_features=5)\nfitAndPredict(model)","cf9c6cc2":"# Polynomial features\n# id = clean_test['PassengerId']\n# features = PolynomialFeatures(degree=2)\n# clean_test = features.fit_transform(clean_test)\n\n# # Standard Scaler\n# sc = StandardScaler()\n# clean_test = sc.fit_transform(clean_test)","5fe6e7c2":"#Deliver (After delivering tunned gradient it seems to have less punctuation than default 1)\npredict = model2.predict(clean_test)\n\n#output = pd.DataFrame({'PassengerId': id, 'Survived': predict})\noutput = pd.DataFrame({'PassengerId': clean_test.PassengerId, 'Survived': predict})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Submission saved\")","d045acf5":"Now lets see some plots that will help us to figure out some relations in the data\n","ed8b18b1":"#### Check cleaning\n\nAfter cleaning data we have to check that all is going well","562f3e9d":"### Data cleaning\n\nSo we have the following situation:\n\n#### Missing values:\n* Case 1: **'Cabin'** 77% of missing values. As long as there is 3\/4 of the data missing if we would decide to mock the data it would not be trustable as long as we are are setting it by ourselves, so the most fair way to proceed is to drop this one\n\n* Case 2: **'Age'** with 20% of missing values. With a 20% of missing values we should try to fill following some strategy in order to apply the filling closer to what would be\n\n* Case 3: **'Embarked'** with 0.2% of missing values. Less than a 0.5% of missing values let us to take a different strategy as long as filling the missing values would affect nearly nothing to results. So in this case we will drop the cases where this property is not present\n\n#### Categorical values\nWe also have categorical variables that need to be encoded or dropped\n* Case 1: **'Sex'** as long as it only has 2 possibles values we can do it manually or by a label encoder.\n\n* Case 2: **'Name'** This property doesn't give useful info so drop is the best option.\n\n* Case 3: **'Ticket'** This property doesn't give useful info. Dtrop is the best option too.\n\n* Case 4: **'Cabin'** drop by missing 70% of values, also not very useful info at first sight. Maybe with less missing could be useful as \"travellers on stern side of the boat survived more than travellers on bow side\", but 77.1% is too much missing.\n\n* Case 5: **'Embarked'** has 3 possible values. I could use one-hot but for now I feel more confident doing by hand (considering this is my first attemp on Kaggle).","c406bc2c":"#### Data visualization\nFirst we are going to charge the data and take a peek to guess what is the situation.\n","92c3ed44":"#### Imports\n\nFirst we import the resources (we will be adding more while we would need it)","ea02ab3b":"#### Modeling\n\nWith the data cleaned we proceed to train and test models.","803070b7":"\n---"}}