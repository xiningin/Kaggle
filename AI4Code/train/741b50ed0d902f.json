{"cell_type":{"4a7dd115":"code","31722441":"code","d6c322dd":"code","c6be6703":"code","7bb8bf48":"code","82b28eca":"code","1c1d75f7":"code","3d6a0975":"code","d1040e2d":"code","b80ec903":"code","5a5828ac":"code","2f09e27e":"code","3f9a4681":"code","7b5ded3e":"code","b28b435a":"code","4c617289":"code","e635fa9c":"code","0c16177e":"code","30254263":"markdown","1c6169ff":"markdown","e7420f2f":"markdown","673b245b":"markdown","a6ae58e0":"markdown","80a9bdb3":"markdown","109db9e1":"markdown","38f0018b":"markdown","8a9995e9":"markdown","0fb549d3":"markdown","d896ad48":"markdown","48925e2d":"markdown","37a16560":"markdown","7365ce04":"markdown","ab142e3f":"markdown","a4b6e998":"markdown","057bccbe":"markdown","f617d91f":"markdown","4182ea2e":"markdown","d9d0adb1":"markdown","89c9f67e":"markdown","4a54075c":"markdown","bf86676d":"markdown","a0a82bed":"markdown"},"source":{"4a7dd115":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style='white')\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nfrom sklearn import decomposition\nfrom sklearn import datasets\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Loading the dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Let's create a beautiful 3d-plot\nfig = plt.figure(1, figsize=(6, 5))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\n\nfor name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n    ax.text3D(X[y == label, 0].mean(),\n              X[y == label, 1].mean() + 1.5,\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n# Change the order of labels, so that they match\ny_clr = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_clr, \n           cmap=plt.cm.nipy_spectral)\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([]);","31722441":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n# Train, test splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, \n                                                    stratify=y, \n                                                    random_state=42)\n\n# Decision trees with depth = 2\nclf = DecisionTreeClassifier(max_depth=2, random_state=42)\nclf.fit(X_train, y_train)\npreds = clf.predict_proba(X_test)\nprint('Accuracy: {:.5f}'.format(accuracy_score(y_test, \n                                                preds.argmax(axis=1))))","d6c322dd":"# Using PCA from sklearn PCA\npca = decomposition.PCA(n_components=2)\nX_centered = X - X.mean(axis=0)\npca.fit(X_centered)\nX_pca = pca.transform(X_centered)\n\n# Plotting the results of PCA\nplt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='Setosa')\nplt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='Versicolour')\nplt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='Virginica')\nplt.legend(loc=0);","c6be6703":"# Test-train split and apply PCA\nX_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, \n                                                    stratify=y, \n                                                    random_state=42)\n\nclf = DecisionTreeClassifier(max_depth=2, random_state=42)\nclf.fit(X_train, y_train)\npreds = clf.predict_proba(X_test)\nprint('Accuracy: {:.5f}'.format(accuracy_score(y_test, \n                                                preds.argmax(axis=1))))","7bb8bf48":"for i, component in enumerate(pca.components_):\n    print(\"{} component: {}% of initial variance\".format(i + 1, \n          round(100 * pca.explained_variance_ratio_[i], 2)))\n    print(\" + \".join(\"%.3f x %s\" % (value, name)\n                     for value, name in zip(component,\n                                            iris.feature_names)))","82b28eca":"digits = datasets.load_digits()\nX = digits.data\ny = digits.target","1c1d75f7":"# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))\nplt.figure(figsize=(16, 6))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(X[i,:].reshape([8,8]), cmap='gray');","3d6a0975":"pca = decomposition.PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\n\nprint('Projecting %d-dimensional data to 2D' % X.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection');","d1040e2d":"%%time\n\nfrom sklearn.manifold import TSNE\ntsne = TSNE(random_state=17)\n\nX_tsne = tsne.fit_transform(X)\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. t-SNE projection');","b80ec903":"pca = decomposition.PCA().fit(X)\n\nplt.figure(figsize=(10,7))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\nplt.xlim(0, 63)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.axvline(21, c='b')\nplt.axhline(0.9, c='r')\nplt.show();","5a5828ac":"# Let's begin by allocation 3 cluster's points\nX = np.zeros((150, 2))\n\nnp.random.seed(seed=42)\nX[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\nX[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n\nX[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\nX[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n\nX[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\nX[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n\nplt.figure(figsize=(5, 5))\nplt.plot(X[:, 0], X[:, 1], 'bo');","2f09e27e":"# Scipy has function that takes 2 tuples and return\n# calculated distance between them\nfrom scipy.spatial.distance import cdist\n\n# Randomly allocate the 3 centroids \nnp.random.seed(seed=42)\ncentroids = np.random.normal(loc=0.0, scale=1., size=6)\ncentroids = centroids.reshape((3, 2))\n\ncent_history = []\ncent_history.append(centroids)\n\nfor i in range(3):\n    # Calculating the distance from a point to a centroid\n    distances = cdist(X, centroids)\n    # Checking what's the closest centroid for the point\n    labels = distances.argmin(axis=1)\n    \n    # Labeling the point according the point's distance\n    centroids = centroids.copy()\n    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n    \n    cent_history.append(centroids)","3f9a4681":"# Let's plot K-means\nplt.figure(figsize=(8, 8))\nfor i in range(4):\n    distances = cdist(X, cent_history[i])\n    labels = distances.argmin(axis=1)\n    \n    plt.subplot(2, 2, i + 1)\n    plt.plot(X[labels == 0, 0], X[labels == 0, 1], 'bo', label='cluster #1')\n    plt.plot(X[labels == 1, 0], X[labels == 1, 1], 'co', label='cluster #2')\n    plt.plot(X[labels == 2, 0], X[labels == 2, 1], 'mo', label='cluster #3')\n    plt.plot(cent_history[i][:, 0], cent_history[i][:, 1], 'rX')\n    plt.legend(loc=0)\n    plt.title('Step {:}'.format(i + 1));","7b5ded3e":"from sklearn.cluster import KMeans","b28b435a":"inertia = []\nfor k in range(1, 8):\n    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n    inertia.append(np.sqrt(kmeans.inertia_))\n","4c617289":"plt.plot(range(1, 8), inertia, marker='s');\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$');","e635fa9c":"from scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\n\nX = np.zeros((150, 2))\n\nnp.random.seed(seed=42)\nX[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\nX[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n\nX[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\nX[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n\nX[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\nX[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n\n# pdist will calculate the upper triangle of the pairwise distance matrix\ndistance_mat = pdist(X) \n# linkage \u2014 is an implementation if agglomerative algorithm\nZ = hierarchy.linkage(distance_mat, 'single')\nplt.figure(figsize=(10, 5))\ndn = hierarchy.dendrogram(Z, color_threshold=0.5)","0c16177e":"from sklearn import metrics\nfrom sklearn import datasets\nimport pandas as pd\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, SpectralClustering\n\n\ndata = datasets.load_digits()\nX, y = data.data, data.target\n\nalgorithms = []\nalgorithms.append(KMeans(n_clusters=10, random_state=1))\nalgorithms.append(AffinityPropagation())\nalgorithms.append(SpectralClustering(n_clusters=10, random_state=1,\n                                     affinity='nearest_neighbors'))\nalgorithms.append(AgglomerativeClustering(n_clusters=10))\n\ndata = []\nfor algo in algorithms:\n    algo.fit(X)\n    data.append(({\n        'ARI': metrics.adjusted_rand_score(y, algo.labels_),\n        'AMI': metrics.adjusted_mutual_info_score(y, algo.labels_),\n        'Homogenity': metrics.homogeneity_score(y, algo.labels_),\n        'Completeness': metrics.completeness_score(y, algo.labels_),\n        'V-measure': metrics.v_measure_score(y, algo.labels_),\n        'Silhouette': metrics.silhouette_score(X, algo.labels_)}))\n\nresults = pd.DataFrame(data=data, columns=['ARI', 'AMI', 'Homogenity',\n                                           'Completeness', 'V-measure', \n                                           'Silhouette'],\n                       index=['K-means', 'Affinity', \n                              'Spectral', 'Agglomerative'])\n\nresults","30254263":"The following algorithm is the simplest and easiest to understand among all the the clustering algorithms without a fixed number of clusters.\n\n\nThe algorithm is fairly simple:\n1. We start by assigning each observation to its own cluster\n2. Then sort the pairwise distances between the centers of clusters in descending order\n3. Take the nearest two neigbor clusters and merge them together, and recompute the centers\n4. Repeat steps 2 and 3 until all the data is merged into one cluster\n\nThe process of searching for the nearest cluster can be conducted with different methods of bounding the observations:\n1. Single linkage \n$d(C_i, C_j) = min_{x_i \\in C_i, x_j \\in C_j} ||x_i - x_j||$\n2. Complete linkage \n$d(C_i, C_j) = max_{x_i \\in C_i, x_j \\in C_j} ||x_i - x_j||$\n3. Average linkage \n$d(C_i, C_j) = \\frac{1}{n_i n_j} \\sum_{x_i \\in C_i} \\sum_{x_j \\in C_j} ||x_i - x_j||$\n4. Centroid linkage \n$d(C_i, C_j) = ||\\mu_i - \\mu_j||$\n\nThe 3rd one is the most effective in computation time since it does not require recomputing the distances every time the clusters are merged.\n\nThe results can be visualized as a beautiful cluster tree (dendogram) to help recognize the moment the algorithm should be stopped to get optimal results. There are plenty of Python tools to build these dendograms for agglomerative clustering.\n\nLet's consider an example with the clusters we got from K-means:","1c6169ff":"## Affinity Propagation\n\nAffinity propagation is another example of a clustering algorithm. As opposed to K-means, this approach does not require us to set the number of clusters beforehand. The main idea here is that we would like to cluster our data based on the similarity of the observations (or how they \"correspond\" to each other).\n\nLet's define a similarity metric such that $s(x_i, x_j) > s(x_i, x_k)$ if an observation $x_i$ is more similar to observation $x_j$ and less similar to observation $x_k$. A simple example of such a similarity metric is a negative square of distance $s(x_i, x_j) = - ||x_i - x_j||^{2}$.\n\n\nNow, let's describe \"correspondence\" by making two zero matrices. One of them, $r_{i,k}$, determines how well the $k$th observation is as a \"role model\" for the $i$th observation with respect to all other possible \"role models\". Another matrix, $a_{i,k}$ determines how appropriate it would be for $i$th observation to take the $k$th observation as a \"role model\". This may sound confusing, but it becomes more understandable with some hands-on practice.\n\nThe matrices are updated sequentially with the following rules:\n\n$$r_{i,k} \\leftarrow s_(x_i, x_k) - \\max_{k' \\neq k} \\left\\{ a_{i,k'} + s(x_i, x_k') \\right\\}$$\n\n$$a_{i,k} \\leftarrow \\min \\left( 0, r_{k,k} + \\sum_{i' \\not\\in \\{i,k\\}} \\max(0, r_{i',k}) \\right), \\ \\ \\  i \\neq k$$\n\n$$a_{k,k} \\leftarrow \\sum_{i' \\neq k} \\max(0, r_{i',k})$$","e7420f2f":"### Handwritten numbers dataset \n\nLet's look at the handwritten numbers dataset that we used before in the [3rd lesson](https:\/\/habrahabr.ru\/company\/ods\/blog\/322534\/#derevya-resheniy-i-metod-blizhayshih-sosedey-v-zadache-raspoznavaniya-rukopisnyh-cifr-mnist).","673b245b":"## Agglomerative clustering\n","a6ae58e0":"## 4. Useful links\n- Overview of clustering methods in the [scikit-learn doc](http:\/\/scikit-learn.org\/stable\/modules\/clustering.html).\n- [Q&A](http:\/\/stats.stackexchange.com\/questions\/2691\/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) for PCA with examples\n- [Notebook](https:\/\/github.com\/diefimov\/MTH594_MachineLearning\/blob\/master\/ipython\/Lecture10.ipynb) on k-means and the EM-algorithm","80a9bdb3":"We see that $J(C_k)$ decreases significantly until the number of clusters is 3 and then does not change as much anymore. This means that the optimal number of clusters is 3. ","109db9e1":"# <center>Topic 7. Unsupervised learning: PCA and clustering\n\nWelcome to the seventh part of our Open Machine Learning Course!  \n  \nIn this lesson, we will work with unsupervised learning methods such as Principal Component Analysis (PCA) and clustering. You will learn why and how we can reduce the dimensionality of the original data and what the main approaches are for grouping similar data points.  \n\n\n### Article outline\n1. Introduction\n2. PCA\n - Intuition, theories, and application issues\n - Application examples\n3. Cluster analysis\n - K-means\n - Affinity Propagation\n - Spectral clustering\n - Agglomerative clustering\n - Accuracy metrics\n4. Useful links\n\n## 1. Introduction\n\nThe main feature of unsupervised learning algorithms, when compared to classification and regression methods, is that input data are unlabeled (i.e. no labels or classes given) and that the algorithm learns the structure of the data without any assistance. This creates two main differences. First, it allows us to process large amounts of data because the data does not need to be manually labeled. Second, it is difficult to evaluate the quality of an unsupervised algorithm due to the absence of an explicit goodness metric as used in supervised learning.  \n\nOne of the most common tasks in unsupervised learning is dimensionality reduction. On one hand, dimensionality reduction may help with data visualization (e.g. t-SNA method) while, on the other hand, it may help deal with the multicollinearity of your data and prepare the data for a supervised learning method (e.g. decision trees).\n\n\n## 2. Principal Component Analysis (PCA)\n\n### Intuition, theories, and application issues\n\nPrincipal Component Analysis is one of the easiest, most intuitive, and most frequently used methods for dimensionality reduction, projecting data onto its orthogonal feature subspace.\n\n\n<img align=\"right\" src=\"https:\/\/habrastorage.org\/getpro\/habr\/post_images\/bb6\/fe7\/f06\/bb6fe7f06e114bcc9c354a1cb025b966.png\" width=\"400\">\n\n\nMore generally speaking, all observations can be considered as an ellipsoid in a subspace of an initial feature space, and the new basis set in this subspace is aligned with the ellipsoid axes. This assumption lets us remove highly correlated features since basis set vectors are orthogonal. \nIn the general case, the resulting ellipsoid dimensionality matches the initial space dimensionality, but the assumption that our data lies in a subspace with a smaller dimension allows us to cut off the \"excessive\" space with the new projection (subspace). We accomplish this in a 'greedy' fashion, sequentially selecting each of the ellipsoid axes by identifying where the dispersion is maximal.\n \n\n> \"To deal with hyper-planes in a 14 dimensional space, visualize a 3D space and say 'fourteen' very loudly. Everyone does it.\" - Geoffrey Hinton\n\n\nLet's take a look at the mathematical formulation of this process:\n\nIn order to decrease the dimensionality of our data from $n$ to $k$ with $k \\leq n$, we sort our list of axes in order of decreasing dispersion and take the top-$k$ of them.\n\nWe begin by computing the dispersion and the covariance of the initial features. This is usually done with the covariance matrix. According to the covariance definition, the covariance of two features is computed as follows: $$cov(X_i, X_j) = E[(X_i - \\mu_i) (X_j - \\mu_j)] = E[X_i X_j] - \\mu_i \\mu_j,$$ where $\\mu_i$ is the expected value of the $i$th feature. It is worth noting that the covariance is symmetric, and the covariance of a vector with itself is equal to its dispersion.\n\nTherefore the covariance matrix is symmetric with the dispersion of the corresponding features on the diagonal. Non-diagonal values are the covariances of the corresponding pair of features. In terms of matrices where $\\mathbf{X}$ is the matrix of observations, the covariance matrix is as follows:\n\n$$\\Sigma = E[(\\mathbf{X} - E[\\mathbf{X}]) (\\mathbf{X} - E[\\mathbf{X}])^{T}]$$\n\nQuick recap: matrices, as linear operators, have eigenvalues and eigenvectors. They are very convenient because they describe parts of our space that do not rotate and only stretch when we apply linear operators on them; eigenvectors remain in the same direction but are stretched by a corresponding eigenvalue. Formally, a matrix $M$ with eigenvector $w_i$ and eigenvalue $\\lambda_i$ satisfy this equation: $M w_i = \\lambda_i w_i$.\n\nThe covariance matrix for a sample $\\mathbf{X}$ can be written as a product of $\\mathbf{X}^{T} \\mathbf{X}$. According to the [Rayleigh quotient](https:\/\/en.wikipedia.org\/wiki\/Rayleigh_quotient), the maximum variation of our sample lies along the eigenvector of this matrix and is consistent with the maximum eigenvalue. Therefore, the principal components we aim to retain from the data are just the eigenvectors corresponding to the top-$k$ largest eigenvalues of the matrix.\n\nThe next steps are easier to digest. We multiply the matrix of our data $X$ by these components to get the projection of our data onto the orthogonal basis of the chosen components. If the number of components was smaller than the initial space dimensionality, remember that we will lose some information upon applying this transformation.","38f0018b":"Indeed, with t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA.","8a9995e9":"## Accuracy metrics\n\nAs opposed to classfication, it is difficult to assess the quality of results from clustering. Here, a metric cannot depend on the labels but only on the goodness of split. Secondly, we do not usually have true labels of the observations when we use clustering.\n\nThere are *internal* and *external* goodness metrics. External metrics use the information about the known true split while internal metrics do not use any external information and assess the goodness of clusters based only on the initial data. The optimal number of clusters is usually defined with respect to some internal metrics. \n\nAll the metrics described below are implemented in `sklearn.metrics`.\n\n**Adjusted Rand Index (ARI)**\n\nHere, we assume that the true labels of objects are known. This metric does not depend on the labels' values but on the data cluster split. Let $N$ be the number of observations in a sample. Let $a$ to be the number of observation pairs with the same labels and located in the same cluster, and let $b$ to be the number of observations with different labels and located in different clusters. The Rand Index can be calculated using the following formula: $$\\text{RI} = \\frac{2(a + b)}{n(n-1)}.$$ \nIn other words, it evaluates a share of observations for which these splits (initial and clustering result) are consistent. The Rand Index (RI) evaluates the similarity of the two splits of the same sample. In order for this index to be close to zero for any clustering outcomes with any $n$ and number of clusters, it is essential to scale it, hence the Adjusted Rand Index: $$\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.$$\n\nThis metric is symmetric and does not depend in the label permutation. Therefore, this index is a measure of distances between different sample splits. $\\text{ARI}$ takes on values in the $[-1, 1]$ range. Negative values indicate the independence of splits, and positive values indicate that these splits are consistent (they match $\\text{ARI} = 1$).\n\n**Adjusted Mutual Information (AMI)**\n\nThis metric is similar to $\\text{ARI}$. It is also symmetric and does not depend on the labels' values and permutation. It is defined by the [entropy](https:\/\/en.wikipedia.org\/wiki\/Entropy_(information_theory) function and interprets a sample split as a discrete distribution (likelihood of assigning to a cluster is equal to the percent of objects in it). The $MI$ index is defined as the [mutual information](https:\/\/en.wikipedia.org\/wiki\/Mutual_information) for two distributions, corresponding to the sample split into clusters. Intuitively, the mutual information measures the share of information common for both clustering splits i.e. how information about one of them decreases the uncertainty of the other one.\n\nSimilarly to the $\\text{ARI}$, the $\\text{AMI}$ is defined. This allows us to get rid of the $MI$ index's increase with the number of clusters. The $\\text{AMI}$ lies in the $[0, 1]$ range. Values close to zero mean the splits are independent, and those close to 1 mean they are similar (with complete match at $\\text{AMI} = 1$).\n\n**Homogeneity, completeness, V-measure**\n\nFormally, these metrics are also defined based on the entropy function and the conditional entropy function, interpreting the sample splits as discrete distributions: $$h = 1 - \\frac{H(C\\mid K)}{H(C)}, c = 1 - \\frac{H(K\\mid C)}{H(K)},$$\nwhere $K$ is a clustering result and $C$ is the initial split. Therefore, $h$ evaluates whether each cluster is composed of same class objects, and $c$ measures how well the same class objects fit the clusters. These metrics are not symmetric. Both lie in the $[0, 1]$ range, and values closer to 1 indicate more accurate clustering results. These metrics' values are not scaled as the $\\text{ARI}$ or $\\text{AMI}$ metrics are and thus depend on the number of clusters. A random clustering result will not have metrics' values closer to zero when the number of clusters is big enough and the number of objects is small. In such a case, it would be more reasonable to use $\\text{ARI}$. However, with a large number of observations (more than 100) and the number of clusters less than 10, this issue is less critical and can be ignored.\n\n$V$-measure is a combination of $h$, and $c$ and is their harmonic mean:\n$$v = 2\\frac{hc}{h+c}.$$\nIt is symmetric and measures how consistent two clustering results are.\n\n**Silhouette**\n\nIn contrast to the metrics described above, this coefficient does not imply the knowledge about the true labels of the objects. It lets us estimate the quality of the clustering using only the initial, unlabeled sample and the clustering result. To start with, for each observation, the silhouette coefficient is computed. Let $a$ be the mean of the distance between an object and other objects within one cluster and $b$ be the mean distance from an object to an object from the nearest cluster (different from the one the object belongs to). Then the silhouette measure for this object is $$s = \\frac{b - a}{\\max(a, b)}.$$\n\nThe silhouette of a sample is a mean value of silhouette values from this sample. Therefore, the silhouette distance shows to which extent the distance between the objects of the same class differ from the mean distance between the objects from different clusters. This coefficient takes values in the $[-1, 1]$ range. Values close to -1 correspond to bad clustering results while values closer to 1 correspond to dense, well-defined clusters. Therefore, the higher the silhouette value is, the better the results from clustering.\n\nWith the help of silhouette, we can identify the optimal number of clusters $k$ (if we don't know it already from the data) by taking the number of clusters that maximizes the silhouette coefficient.","0fb549d3":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>\nAuthor: [Sergey Korolev](https:\/\/www.linkedin.com\/in\/sokorolev\/), Software Engineer at Snap Inc. <br>\nTranslated and edited by [Egor Polusmak](https:\/\/www.linkedin.com\/in\/egor-polusmak\/), [Anastasia Manokhina](https:\/\/www.linkedin.com\/in\/anastasiamanokhina\/), [Anna Golovchenko](https:\/\/www.linkedin.com\/in\/anna-golovchenko-b0ba5a112\/), [Eugene Mashkin](https:\/\/www.linkedin.com\/in\/eugene-mashkin-88490883\/), and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/).\n\nThis material is subject to the terms and conditions of the license [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/). Free use is permitted for any non-comercial purpose with an obligatory indication of the names of the authors and of the source.\n\n<center>Original Kernel from Yury Kashnitskiy is [here](https:\/\/www.kaggle.com\/kashnitsky\/topic-7-unsupervised-learning-pca-and-clustering)","d896ad48":"Let's try this again, but, this time, let's reduce the dimensionality to 2 dimensions:","48925e2d":"Our data has 64 dimensions, but we are going to reduce it to only 2 and see that, even with just 2 dimensions, we can clearly see that digits separate into clusters.","37a16560":"The accuracy did not increase significantly in this case, but, with other datasets with a high number of dimensions, PCA can drastically improve the accuracy of decision trees and other ensemble methods.","7365ce04":"Now let's see how PCA will improve the results of a simple model that is not able to correctly fit all of the training data:","ab142e3f":"To conclude, let's take a look at how these metrics perform with the MNIST handwritten numbers dataset:","a4b6e998":"## Examples\n### Fisher's iris dataset\n\nLet's start by uploading all of the essential modules and try out the iris example from the `scikit-learn` documentation. ","057bccbe":"#### Issues\n\nInherently, K-means is NP-hard. For $d$ dimensions, $k$ clusters, and $n$ observations, we will find a solution in $O(n^{d k+1})$ in time. There are some heuristics to deal with this; an example is MiniBatch K-means, which takes portions (batches) of data instead of fitting the whole dataset and then moves centroids by taking the average of the previous steps. Compare the implementation of K-means and MiniBatch K-means in the [sckit-learn documentation](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_mini_batch_kmeans.html).\n\nThe [implemetation](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html) of the algorithm using `scikit-learn` has its benefits such as the possibility to state the number of initializations with the `n_init` function parameter, which enables us to identify more robust centroids. Moreover, these runs can be done in parallel to decrease the computation time.","f617d91f":"Now let's check out the percent of variance that can be explained by each of the selected components.","4182ea2e":"## Spectral clustering\n\nSpectral clustering combines some of the approaches described above to create a stronger clustering method.\n\nFirst of all, this algorithm requires us to define the similarity matrix for observations called the adjacency matrix. This can be done in a similar fashion as in the Affinity Propagation algorithm: $A_{i, j} = - ||x_i - x_j||^{2}$. This matrix describes a full graph with the observations as vertices and the estimated similarity value between a pair of observations as edge weights for that pair of vertices. For the metric defined above and two-dimensional observations, this is pretty intuitive - two observations are similar if the edge between them is shorter. \nWe'd like to split up the graph into two subgraphs in such a way that each observation in each subgraph would be similar to another observation in that subgraph. Formally, this is a Normalized cuts problem; for more details, we recommend reading [this paper](http:\/\/people.eecs.berkeley.edu\/~malik\/papers\/SM-ncut.pdf).","d9d0adb1":"## 2. Clustering\n\nThe main idea behind clustering is pretty straightforward. Basically, we say to ourselves, \"I have these points here, and I can see that they organize into groups. It would be nice to describe these things more concretely, and, when a new point comes in, assign it to the correct group.\" This general idea encourages exploration and opens up a variety of algorithms for clustering.\n\n<figure><img align=\"center\" src=\"https:\/\/habrastorage.org\/getpro\/habr\/post_images\/8b9\/ae5\/586\/8b9ae55861f22a2809e8b3a00ef815ad.png\"><figcaption>*The examples of the outcomes from different algorithms from scikit-learn*<\/figcaption><\/figure>\n\nThe algorithms listed below do not cover all the clustering methods out there, but they are the most commonly used ones.\n\n### K-means\n\nK-means algorithm is the most popular and yet simplest of all the clustering algorithms. Here is how it works:\n1. Select the number of clusters $k$ that you think is the optimal number.\n2. Initialize $k$ points as \"centroids\" randomly within the space of our data.\n3. Attribute each observation to its closest centroid.\n4. Update the centroids to the center of all the attributed set of observations. \n5. Repeat steps 3 and 4 a fixed number of times or until all of the centroids are stable (i.e. no longer change in step 4).\n\nThis algorithm is easy to describe and visualize. Let's take a look.","89c9f67e":"In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the `explained_variance_ratio`). Here, that means retaining 21 principal components; therefore, we reduce the dimensionality from 64 features to 21.","4a54075c":"## Choosing the number of clusters for K-means\n\nIn contrast to the supervised learning tasks such as classification and regression, clustering requires more effort to choose the optimization criterion. Usually, when working with k-means, we optimize the sum of squared distances between the observations and their centroids.\n\n$$ J(C) = \\sum_{k=1}^K\\sum_{i~\\in~C_k} ||x_i - \\mu_k|| \\rightarrow \\min\\limits_C,$$\n\nwhere $C$ \u2013 is a set of clusters with power $K$, $\\mu_k$ is a centroid of a cluster $C_k$.\n\nThis definition seems reasonable -- we want our observations to be as close to their centroids as possible. But, there is a problem -- the optimum is reached when the number of centroids is equal to the number of observations, so you would end up with every single observation as its own separate cluster.\n\nIn order to avoid that case, we should choose a number of clusters after which a function $J(C_k)$ is decreasing less rapidly. More formally,\n$$ D(k) = \\frac{|J(C_k) - J(C_{k+1})|}{|J(C_{k-1}) - J(C_k)|}  \\rightarrow \\min\\limits_k $$\n\nLet's look at an example.","bf86676d":"Here, we used Euclidean distance, but the algorithm will converge with any other metric. You can not only vary the number of steps or the convergence criteria but also the distance measure between the points and cluster centroids.\n\nAnother \"feature\" of this algorithm is its sensitivity to the initial positions of the cluster centroids. You can run the algorithm several times and then average all the centroid results.","a0a82bed":"Let's start by visualizing our data. Fetch the first 10 numbers. The numbers are represented by 8 x 8 matrixes with the color intensity for each pixel. Every matrix is flattened into a vector of 64 numbers, so we get the feature version of the data."}}