{"cell_type":{"b748782d":"code","d305eb63":"code","0aebf144":"code","83bf2adc":"code","5e287624":"code","b3ef63e4":"code","eb30d19a":"code","90441372":"code","4d0b4010":"code","3da8d0ee":"code","6e10b402":"code","72e0905f":"code","dc0c381f":"code","0362e549":"code","c100fbf6":"code","37bc2c6c":"code","09bf5bf1":"code","081582e6":"code","8d83e48d":"code","bc096cc5":"code","39731ef9":"code","ea63d852":"code","0384355b":"code","c7af5bbe":"code","b3972abf":"code","6d07e13c":"code","63c17362":"code","eacc8568":"code","2d267407":"code","5c3bd041":"code","b95431cd":"code","a45ae66e":"code","9451021d":"code","4c2dec14":"markdown","c2e4234d":"markdown","711152e7":"markdown","4786f9d1":"markdown","17f6c621":"markdown"},"source":{"b748782d":"import pandas as pd\nimport numpy as np\npd.options.display.max_columns=30\nfrom IPython.display import display, HTML, display_html\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#Disply all the cell output\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nplt.style.use('fivethirtyeight')","d305eb63":"df = pd.read_csv('..\/input\/GOOG.csv', index_col=0, parse_dates=True)\ndf.head()","0aebf144":"df[['Open','Close']].plot(figsize=(15, 5),style=['-','.'])","83bf2adc":"df['date'] = df.index\ndf['hour'] = df['date'].dt.hour\ndf['dayofweek'] = df['date'].dt.dayofweek\ndf['quarter'] = df['date'].dt.quarter\ndf['month'] = df['date'].dt.month\ndf['year'] = df['date'].dt.year\ndf['dayofyear'] = df['date'].dt.dayofyear\ndf['dayofmonth'] = df['date'].dt.day\ndf['weekofyear'] = df['date'].dt.weekofyear\n# df['Price_After_Month']=df['Adj. Close'].shift(-30)","5e287624":"cols = ['Open', 'High', 'Low', 'Adj Close', 'date']\ndf.drop(cols,1,inplace=True)","b3ef63e4":"df.head()","eb30d19a":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nX = df.drop('Close',1)\ny = df['Close']","90441372":"#Feature Importance\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=1, max_depth=10)\nmodel.fit(X,y)\nfeatures = X.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances)[-9:]  # top 10 features\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","4d0b4010":"X.drop(['Volume', 'hour', 'dayofweek', 'quarter', 'month', 'dayofyear',\n       'dayofmonth', 'weekofyear'], 1, inplace=True)","3da8d0ee":"SC1 = StandardScaler()\nX = SC1.fit_transform(X)\nX","6e10b402":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","72e0905f":"#Linear Regression\nfrom sklearn.linear_model import LinearRegression\nReg1 = LinearRegression()\nReg1.fit(X_train,y_train)\nReg1.score(X_test, y_test),mean_squared_error(y_test, Reg1.predict(X_test)), Reg1.score(X_train,y_train)","dc0c381f":"y_test[:10].values, Reg1.predict(X_test)[:10]","0362e549":"df2 = df['Close']\ndf2 = pd.DataFrame(df2)\ndf2 = df2.rename(columns = {'Close': 'ts'})\ndf2.head()","c100fbf6":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(df, ts):\n    \"\"\"\n    Test stationarity using moving average statistics and Dickey-Fuller test\n    Source: https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/\n    \"\"\"\n    \n    # Determing rolling statistics\n    rolmean = df[ts].rolling(window = 12, center = False).mean()\n    rolstd = df[ts].rolling(window = 12, center = False).std()\n    \n    # Plot rolling statistics:\n    plt.figure(figsize=(15,10))\n    orig = plt.plot(df[ts], \n                    color = 'blue', \n                    label = 'Original')\n    mean = plt.plot(rolmean, \n                    color = 'red', \n                    label = 'Rolling Mean')\n    std = plt.plot(rolstd, \n                   color = 'black', \n                   label = 'Rolling Std')\n    plt.legend(loc = 'best')\n    plt.title('Rolling Mean & Standard Deviation for %s' %(ts))\n    plt.xticks(rotation = 45)\n    plt.show(block = False)\n    plt.close()\n    \n    # Perform Dickey-Fuller test:\n    # Null Hypothesis (H_0): time series is not stationary\n    # Alternate Hypothesis (H_1): time series is stationary\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(df[ts], \n                      autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], \n                         index = ['Test Statistic',\n                                  'p-value',\n                                  '# Lags Used',\n                                  'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","37bc2c6c":"test_stationarity(df = df2, ts = 'ts')","09bf5bf1":"def plot_transformed_data(df, ts, ts_transform):\n  \"\"\"\n  Plot transformed and original time series data\n  \"\"\"\n  f, ax = plt.subplots(1,1)\n  ax.plot(df[ts])\n  ax.plot(df[ts_transform], color = 'red')\n  ax.set_title('%s and %s time-series graph' %(ts, ts_transform))\n  ax.tick_params(axis = 'x', rotation = 45)\n  ax.legend([ts, ts_transform])\n  plt.show()\n  plt.close()\n  return","081582e6":"# Transformation - log ts\ndf2['ts_log'] = df2['ts'].apply(lambda x: np.log(x))\n\n# Transformation - 7-day moving averages of log ts\ndf2['ts_log_moving_avg'] = df2['ts_log'].rolling(window = 7,\n                                                               center = False).mean()\n\n# Transformation - 7-day moving average ts\ndf2['ts_moving_avg'] = df2['ts'].rolling(window = 7,\n                                                       center = False).mean()\n\n# Transformation - Difference between logged ts and first-order difference logged ts\n# df2['ts_log_diff'] = df2['ts_log'] - df2['ts_log'].shift()\ndf2['ts_log_diff'] = df2['ts_log'].diff()\n\n# Transformation - Difference between ts and moving average ts\ndf2['ts_moving_avg_diff'] = df2['ts'] - df2['ts_moving_avg']\n\n# Transformation - Difference between logged ts and logged moving average ts\ndf2['ts_log_moving_avg_diff'] = df2['ts_log'] - df2['ts_log_moving_avg']\n\n# Transformation - Difference between logged ts and logged moving average ts\ndf2_transform = df2.dropna()\n\n# Transformation - Logged exponentially weighted moving averages (EWMA) ts\ndf2_transform['ts_log_ewma'] = df2_transform['ts_log'].ewm(halflife = 7,\n                                                                         ignore_na = False,\n                                                                         min_periods = 0,\n                                                                         adjust = True).mean()\n\n# Transformation - Difference between logged ts and logged EWMA ts\ndf2_transform['ts_log_ewma_diff'] = df2_transform['ts_log'] - df2_transform['ts_log_ewma']\n\n# Display data\ndisplay(df2_transform.head())\n\n# Plot data\nplot_transformed_data(df = df2, \n                      ts = 'ts', \n                      ts_transform = 'ts_log')\n# Plot data\nplot_transformed_data(df = df2, \n                      ts = 'ts_log', \n                      ts_transform = 'ts_log_moving_avg')\n\n# Plot data\nplot_transformed_data(df = df2_transform, \n                      ts = 'ts', \n                      ts_transform = 'ts_moving_avg')\n\n# Plot data\nplot_transformed_data(df = df2_transform, \n                      ts = 'ts_log', \n                      ts_transform = 'ts_log_diff')\n\n# Plot data\nplot_transformed_data(df = df2_transform, \n                      ts = 'ts', \n                      ts_transform = 'ts_moving_avg_diff')\n\n# Plot data\nplot_transformed_data(df = df2_transform, \n                      ts = 'ts_log', \n                      ts_transform = 'ts_log_moving_avg_diff')\n\n# Plot data\nplot_transformed_data(df = df2_transform, \n                      ts = 'ts_log', \n                      ts_transform = 'ts_log_ewma')\n\n# Plot data\nplot_transformed_data(df = df2_transform, \n                      ts = 'ts_log', \n                      ts_transform = 'ts_log_ewma_diff')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform, \n                  ts = 'ts_log')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform, \n                  ts = 'ts_moving_avg')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform, \n                  ts = 'ts_log_moving_avg')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform,\n                  ts = 'ts_log_diff')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform,\n                  ts = 'ts_moving_avg_diff')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform,\n                  ts = 'ts_log_moving_avg_diff')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform, \n                  ts = 'ts_log_ewma')\n\n# Perform stationarity test\ntest_stationarity(df = df2_transform,\n                  ts = 'ts_log_ewma_diff')","8d83e48d":"def plot_decomposition(df, ts, trend, seasonal, residual):\n  \"\"\"\n  Plot time series data\n  \"\"\"\n  f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize = (15, 5), sharex = True)\n\n  ax1.plot(df[ts], label = 'Original')\n  ax1.legend(loc = 'best')\n  ax1.tick_params(axis = 'x', rotation = 45)\n\n  ax2.plot(df[trend], label = 'Trend')\n  ax2.legend(loc = 'best')\n  ax2.tick_params(axis = 'x', rotation = 45)\n\n  ax3.plot(df[seasonal],label = 'Seasonality')\n  ax3.legend(loc = 'best')\n  ax3.tick_params(axis = 'x', rotation = 45)\n\n  ax4.plot(df[residual], label = 'Residuals')\n  ax4.legend(loc = 'best')\n  ax4.tick_params(axis = 'x', rotation = 45)\n  plt.tight_layout()\n\n  # Show graph\n  plt.suptitle('Trend, Seasonal, and Residual Decomposition of %s' %(ts), \n               x = 0.5, \n               y = 1.05, \n               fontsize = 18)\n  plt.show()\n  plt.close()\n  \n  return","bc096cc5":"df_example_transform = df2_transform\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(df_example_transform['ts_log'], freq = 365)\n\ndf_example_transform.loc[:,'trend'] = decomposition.trend\ndf_example_transform.loc[:,'seasonal'] = decomposition.seasonal\ndf_example_transform.loc[:,'residual'] = decomposition.resid\n\nplot_decomposition(df = df_example_transform, \n                   ts = 'ts_log', \n                   trend = 'trend',\n                   seasonal = 'seasonal', \n                   residual = 'residual')\n\ntest_stationarity(df = df_example_transform.dropna(), ts = 'residual')","39731ef9":"#ARIMA Model\ndef plot_acf_pacf(df, ts):\n  \"\"\"\n  Plot auto-correlation function (ACF) and partial auto-correlation (PACF) plots\n  \"\"\"\n  f, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 5)) \n\n  #Plot ACF: \n\n  ax1.plot(lag_acf)\n  ax1.axhline(y=0,linestyle='--',color='gray')\n  ax1.axhline(y=-1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax1.axhline(y=1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax1.set_title('Autocorrelation Function for %s' %(ts))\n\n  #Plot PACF:\n  ax2.plot(lag_pacf)\n  ax2.axhline(y=0,linestyle='--',color='gray')\n  ax2.axhline(y=-1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax2.axhline(y=1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax2.set_title('Partial Autocorrelation Function for %s' %(ts))\n  \n  plt.tight_layout()\n  plt.show()\n  plt.close()\n  \n  return","ea63d852":"#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf\n\n# determine ACF and PACF\nlag_acf = acf(np.array(df_example_transform['ts_log_diff']), nlags = 20)\nlag_pacf = pacf(np.array(df_example_transform['ts_log_diff']), nlags = 20)\n\n# plot ACF and PACF\nplot_acf_pacf(df = df_example_transform, ts = 'ts_log_diff')","0384355b":"def run_arima_model(df, ts, p, d, q):\n  \"\"\"\n  Run ARIMA model\n  \"\"\"\n  from statsmodels.tsa.arima_model import ARIMA\n\n  # fit ARIMA model on time series\n  model = ARIMA(df[ts], order=(p, d, q))  \n  results_ = model.fit(disp=-1)  \n  \n  # get lengths correct to calculate RSS\n  len_results = len(results_.fittedvalues)\n  ts_modified = df[ts][-len_results:]\n  \n  # calculate root mean square error (RMSE) and residual sum of squares (RSS)\n  rss = sum((results_.fittedvalues - ts_modified)**2)\n  rmse = np.sqrt(rss \/ len(df[ts]))\n  \n  # plot fit\n  plt.plot(df[ts])\n  plt.plot(results_.fittedvalues, color = 'red')\n  plt.title('For ARIMA model (%i, %i, %i) for ts %s, RSS: %.4f, RMSE: %.4f' %(p, d, q, ts, rss, rmse))\n  \n  plt.show()\n  plt.close()\n  \n  return results_","c7af5bbe":"# Note: I do the differencing in the transformation of the data 'ts_log_diff'\n# AR model with 1st order differencing - ARIMA (1,0,0)\nmodel_AR = run_arima_model(df = df_example_transform, \n                           ts = 'ts_log_diff', \n                           p = 1, \n                           d = 0, \n                           q = 0)\n\n# MA model with 1st order differencing - ARIMA (0,0,1)\nmodel_MA = run_arima_model(df = df_example_transform, \n                           ts = 'ts_log_diff', \n                           p = 0, \n                           d = 0, \n                           q = 1)\n\n# ARMA model with 1st order differencing - ARIMA (0,1,0)\nmodel_MA = run_arima_model(df = df_example_transform, \n                           ts = 'ts_log_diff', \n                           p = 0, \n                           d = 1, \n                           q = 1)","b3972abf":"from fbprophet import Prophet\nimport datetime\nfrom datetime import datetime","6d07e13c":"df_example = df2\ndef days_between(d1, d2):\n    \"\"\"Calculate the number of days between two dates.  D1 is start date (inclusive) and d2 is end date (inclusive)\"\"\"\n    d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n    d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n    return abs((d2 - d1).days + 1)\n\n# Inputs for query\n\ndate_column = 'Date'\nmetric_column = 'ts'\ntable = df_example\nstart_training_date = '2004-08-19'\nend_training_date = '2019-08-12'\nstart_forecasting_date = '2019-08-13'\nend_forecasting_date = '2019-12-31'\nyear_to_estimate = '2019'\n\n# Inputs for forecasting\n\n# future_num_points\n# If doing different time intervals, change future_num_points\nfuture_num_points = days_between(start_forecasting_date, end_forecasting_date)\n\ncap = None # 2e6\n\n# growth: default = 'linear'\n# Can also choose 'logistic'\ngrowth = 'linear'\n\n# n_changepoints: default = 25, uniformly placed in first 80% of time series\nn_changepoints = 25 \n\n# changepoint_prior_scale: default = 0.05\n# Increasing it will make the trend more flexible\nchangepoint_prior_scale = 0.05 \n\n# changpoints: example = ['2016-01-01']\nchangepoints = None \n\n# holidays_prior_scale: default = 10\n# If you find that the holidays are overfitting, you can adjust their prior scale to smooth them\nholidays_prior_scale = 10 \n\n# interval_width: default = 0.8\ninterval_width = 0.8 \n\n# mcmc_samples: default = 0\n# By default Prophet will only return uncertainty in the trend and observation noise.\n# To get uncertainty in seasonality, you must do full Bayesian sampling. \n# Replaces typical MAP estimation with MCMC sampling, and takes MUCH LONGER - e.g., 10 minutes instead of 10 seconds.\n# If you do full sampling, then you will see the uncertainty in seasonal components when you plot:\nmcmc_samples = 0\n\nholidays = None\n\ndaily_seasonality = True","63c17362":"# get relevant data - note: could also try this with ts_log_diff\ndf_prophet = df_example_transform[['ts']] # can try with ts_log_diff\n\n# reset index\ndf_prophet = df_prophet.reset_index()\n\n# rename columns\ndf_prophet = df_prophet.rename(columns = {'Date': 'ds', 'ts': 'y'}) # can try with ts_log_diff\n\n# Change 'ds' type from datetime to date (necessary for FB Prophet)\ndf_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n\n# Change 'y' type to numeric (necessary for FB Prophet)\ndf_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='ignore')\n\n# Remove any outliers\n# df.loc[(df_['ds'] > '2016-12-13') & (df_['ds'] < '2016-12-19'), 'y'] = None","eacc8568":"def create_daily_forecast(df,\n#                           cap,\n                          holidays,\n                          growth,\n                          n_changepoints = 25,\n                          changepoint_prior_scale = 0.05,\n                          changepoints = None,\n                          holidays_prior_scale = 10,\n                          interval_width = 0.8,\n                          mcmc_samples = 1,\n                          future_num_points = 10, \n                          daily_seasonality = True):\n  \"\"\"\n  Create forecast\n  \"\"\"\n  \n  # Create copy of dataframe\n  df_ = df.copy()\n\n  # Add in growth parameter, which can change over time\n  #     df_['cap'] = max(df_['y']) if cap is None else cap\n\n  # Create model object and fit to dataframe\n  m = Prophet(growth = growth,\n              n_changepoints = n_changepoints,\n              changepoint_prior_scale = changepoint_prior_scale,\n              changepoints = changepoints,\n              holidays = holidays,\n              holidays_prior_scale = holidays_prior_scale,\n              interval_width = interval_width,\n              mcmc_samples = mcmc_samples, \n              daily_seasonality = daily_seasonality)\n\n  # Fit model with dataframe\n  m.fit(df_)\n\n  # Create dataframe for predictions\n  future = m.make_future_dataframe(periods = future_num_points)\n  #     future['cap'] = max(df_['y']) if cap is None else cap\n\n  # Create predictions\n  fcst = m.predict(future)\n\n  # Plot\n  m.plot(fcst);\n  m.plot_components(fcst)\n\n  return fcst","2d267407":"fcst = create_daily_forecast(df_prophet,\n#                              cap,\n                             holidays,\n                             growth,\n                             n_changepoints,\n                             changepoint_prior_scale,\n                             changepoints, \n                             holidays_prior_scale,\n                             interval_width,\n                             mcmc_samples,\n                             future_num_points, \n                             daily_seasonality)","5c3bd041":"def calculate_mape(y_true, y_pred):\n    \"\"\" Calculate mean absolute percentage error (MAPE)\"\"\"\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef calculate_mpe(y_true, y_pred):\n    \"\"\" Calculate mean percentage error (MPE)\"\"\"\n    return np.mean((y_true - y_pred) \/ y_true) * 100\n\ndef calculate_mae(y_true, y_pred):\n    \"\"\" Calculate mean absolute error (MAE)\"\"\"\n    return np.mean(np.abs(y_true - y_pred)) * 100\n\ndef calculate_rmse(y_true, y_pred):\n    \"\"\" Calculate root mean square error (RMSE)\"\"\"\n    return np.sqrt(np.mean((y_true - y_pred)**2))\n\ndef print_error_metrics(y_true, y_pred):\n    print('MAPE: %f'%calculate_mape(y_true, y_pred))\n    print('MPE: %f'%calculate_mpe(y_true, y_pred))\n    print('MAE: %f'%calculate_mae(y_true, y_pred))\n    print('RMSE: %f'%calculate_rmse(y_true, y_pred))\n    return","b95431cd":"print_error_metrics(y_true = df_prophet['y'], y_pred = fcst['yhat'])","a45ae66e":"#RNN \n\ndef do_lstm_model(df, \n                  ts, \n                  look_back, \n                  epochs, \n                  type_ = None, \n                  train_fraction = 0.67):\n  \"\"\"\n   Create LSTM model\n   Source: https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/\n  \"\"\"\n  # Import packages\n  import numpy\n  import matplotlib.pyplot as plt\n  from pandas import read_csv\n  import math\n  from keras.models import Sequential\n  from keras.layers import Dense\n  from keras.layers import LSTM\n  from sklearn.preprocessing import MinMaxScaler\n  from sklearn.metrics import mean_squared_error\n\n  # Convert an array of values into a dataset matrix\n  def create_dataset(dataset, look_back=1):\n    \"\"\"\n    Create the dataset\n    \"\"\"\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n      a = dataset[i:(i+look_back), 0]\n      dataX.append(a)\n      dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n  # Fix random seed for reproducibility\n  numpy.random.seed(7)\n\n  # Get dataset\n  dataset = df[ts].values\n  dataset = dataset.astype('float32')\n\n  # Normalize the dataset\n  scaler = MinMaxScaler(feature_range=(0, 1))\n  dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n  \n  # Split into train and test sets\n  train_size = int(len(dataset) * train_fraction)\n  test_size = len(dataset) - train_size\n  train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n  \n  # Reshape into X=t and Y=t+1\n  look_back = look_back\n  trainX, trainY = create_dataset(train, look_back)\n  testX, testY = create_dataset(test, look_back)\n  \n  # Reshape input to be [samples, time steps, features]\n  if type_ == 'regression with time steps':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  elif type_ == 'stacked with memory between batches':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  else:\n    trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n  \n  # Create and fit the LSTM network\n  batch_size = 1\n  model = Sequential()\n  \n  if type_ == 'regression with time steps':\n    model.add(LSTM(4, input_shape=(look_back, 1)))\n  elif type_ == 'memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  elif type_ == 'stacked with memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  else:\n    model.add(LSTM(4, input_shape=(1, look_back)))\n  \n  model.add(Dense(1))\n  model.compile(loss='mean_squared_error', optimizer='adam')\n\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    for i in range(100):\n      model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n      model.reset_states()\n  else:\n    model.fit(trainX, \n              trainY, \n              epochs = epochs, \n              batch_size = 1, \n              verbose = 2)\n  \n  # Make predictions\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    trainPredict = model.predict(trainX, batch_size=batch_size)\n    testPredict = model.predict(testX, batch_size=batch_size)\n  else:\n    trainPredict = model.predict(trainX)\n    testPredict = model.predict(testX)\n  \n  # Invert predictions\n  trainPredict = scaler.inverse_transform(trainPredict)\n  trainY = scaler.inverse_transform([trainY])\n  testPredict = scaler.inverse_transform(testPredict)\n  testY = scaler.inverse_transform([testY])\n  \n  # Calculate root mean squared error\n  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n  print('Train Score: %.2f RMSE' % (trainScore))\n  testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n  print('Test Score: %.2f RMSE' % (testScore))\n  \n  # Shift train predictions for plotting\n  trainPredictPlot = numpy.empty_like(dataset)\n  trainPredictPlot[:, :] = numpy.nan\n  trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n  \n  # Shift test predictions for plotting\n  testPredictPlot = numpy.empty_like(dataset)\n  testPredictPlot[:, :] = numpy.nan\n  testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n  \n  # Plot baseline and predictions\n  plt.plot(scaler.inverse_transform(dataset))\n  plt.plot(trainPredictPlot)\n  plt.plot(testPredictPlot)\n  plt.show()\n  plt.close()\n  \n  return","9451021d":"# LSTM Network for Regression\ndo_lstm_model(df = df_prophet, \n              ts = 'y', \n              look_back = 1, \n              epochs = 5)\n\n# LSTM for Regression Using the Window Method\ndo_lstm_model(df = df_prophet, \n              ts = 'y', \n              look_back = 3, \n              epochs = 5)\n\n# LSTM for Regression with Time Steps\ndo_lstm_model(df = df_prophet, \n              ts = 'y', \n              look_back = 3, \n              epochs = 5, \n              type_ = 'regression with time steps')","4c2dec14":"Stationarity Test - Dickey-Fuller Test\n1. constant mean\n1. constant variance\n1. an autocovariance that does not depend on time","c2e4234d":"**Create Time Series Features**","711152e7":"Only the year column has high importance. So we can ignore all other columns","4786f9d1":"Ref: Pydata Tamara Louie","17f6c621":"The Mean is Increasing so this is not a stationary series"}}