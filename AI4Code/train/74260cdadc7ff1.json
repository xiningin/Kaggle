{"cell_type":{"7fe34552":"code","ef0ff71c":"code","3814c1a1":"code","b3314bf1":"code","df525eac":"code","aa71b20f":"code","e3eb9750":"code","febb96a1":"code","f086849f":"code","29403aec":"code","e86b9d3f":"markdown","751e0c28":"markdown","16c20d6b":"markdown","0f8e1a69":"markdown","00cf46d6":"markdown","01be991a":"markdown","b4637e63":"markdown","19ab8be3":"markdown","c59da3ec":"markdown","ac2c7b90":"markdown","48c114a7":"markdown","8330e06b":"markdown","62eae6ee":"markdown","1c972db5":"markdown","a1379e57":"markdown","b2c2a609":"markdown","ef780da7":"markdown","3c651402":"markdown","24d8716a":"markdown","affc25d0":"markdown","e8ca465b":"markdown","46e5aa0a":"markdown","6379c5d9":"markdown","f276fb36":"markdown","1f9b685c":"markdown","1e60cd84":"markdown","a2f55562":"markdown","f7389c70":"markdown"},"source":{"7fe34552":"import nltk\ntext = \"Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.\"\nsentences = nltk.sent_tokenize(text)\nfor sentence in sentences:\n    print(sentence)\n    print()","ef0ff71c":"for sentence in sentences:\n    words = nltk.word_tokenize(sentence)\n    print(words)\n    print()","3814c1a1":"from nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\ndef compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):\n    \"\"\"\n    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n    \"\"\"\n    print(\"Stemmer:\", stemmer.stem(word))\n    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n    print()\n\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\ncompare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\ncompare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)","b3314bf1":"from nltk.corpus import stopwords\nprint(stopwords.words(\"english\"))","df525eac":"stop_words = set(stopwords.words(\"english\"))\nsentence = \"Backgammon is one of the oldest known board games.\"\n\nwords = nltk.word_tokenize(sentence)\nwithout_stop_words = [word for word in words if not word in stop_words]\nprint(without_stop_words)","aa71b20f":"stop_words = set(stopwords.words(\"english\"))\nsentence = \"Backgammon is one of the oldest known board games.\"\n\nwords = nltk.word_tokenize(sentence)\nwithout_stop_words = []\nfor word in words:\n    if word not in stop_words:\n        without_stop_words.append(word)\n\nprint(without_stop_words)","e3eb9750":"\nimport re\nsentence = \"The development of snowboarding was inspired by skateboarding, sledding, surfing and skiing.\"\npattern = r\"[^\\w]\"\nprint(re.sub(pattern, \" \", sentence))\n","febb96a1":"with open(\"simple movie reviews.txt\", \"r\") as file:\n    documents = file.read().splitlines()\n    \nprint(documents)","f086849f":"# Import the libraries we need\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\n# Step 2. Design the Vocabulary\n# The default token pattern removes tokens of a single character. That's why we don't have the \"I\" and \"s\" tokens in the output\ncount_vectorizer = CountVectorizer()\n\n# Step 3. Create the Bag-of-Words Model\nbag_of_words = count_vectorizer.fit_transform(documents)\n\n# Show the Bag-of-Words Model as a pandas DataFrame\nfeature_names = count_vectorizer.get_feature_names()\npd.DataFrame(bag_of_words.toarray(), columns = feature_names)","29403aec":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\n\ntfidf_vectorizer = TfidfVectorizer()\nvalues = tfidf_vectorizer.fit_transform(documents)\n\n# Show the Model as a pandas DataFrame\nfeature_names = tfidf_vectorizer.get_feature_names()\npd.DataFrame(values.toarray(), columns = feature_names)","e86b9d3f":"Stop words are words which are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That\u2019s why we want to remove these irrelevant words.\nStop words usually refer to the most common words such as \u201cand\u201d, \u201cthe\u201d, \u201ca\u201d in a language, but there is no single universal list of stopwords. The list of the stop words can change depending on your application.\nThe NLTK tool has a predefined list of stopwords that refers to the most common words. If you use it for your first time, you need to download the stop words using this code: nltk.download(\u201cstopwords\u201d). Once we complete the downloading, we can load the stopwords package from the nltk.corpus and use it to load the stop words.\n","751e0c28":"# 2. Word Tokenization\nWord tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.\nHowever, we still can have problems if we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes they contain a space. In most cases, we use a library to achieve the wanted results, so again don\u2019t worry too much for the details.\n# Example:\nLet\u2019s use the sentences from the previous step and see how we can apply word tokenization on them. We can use the nltk.word_tokenize function.","16c20d6b":"A regular expression is a powerful tool and we can create much more complex patterns. If you want to learn more about regex I can recommend you to try these 2 web apps: [regexr](https:\/\/regexr.com\/), [regex101](https:\/\/regex101.com\/).","0f8e1a69":"# Some Examples\n## Cortana\n![img](https:\/\/miro.medium.com\/max\/875\/1*TXj0kr4jVrtLtmvxZFu8Lw.png)\n\n\nYou can read more for Cortana commands from [here](https:\/\/www.howtogeek.com\/225458\/15-things-you-can-do-with-cortana-on-windows-10\/)","00cf46d6":"# What is NLP (Natural Language Processing)?\nNLP is a subfield of computer science and artificial intelligence concerned with interactions between computers and human (natural) languages. It is used to apply machine learning algorithms to text and speech.\nFor example, we can use NLP to create systems like speech recognition, document summarization, machine translation, spam detection, named entity recognition, question answering, autocomplete, predictive typing and so on.\nNowadays, most of us have smartphones that have speech recognition. These smartphones use NLP to understand what is said. Also, many people use laptops which operating system has a built-in speech recognition.","01be991a":"# The Basics of NLP for Text\nIn this article, we\u2019ll cover the following topics:\n1. Sentence Tokenization\n2. Word Tokenization\n3. Text Lemmatization and Stemming\n4. Stop Words\n5. Regex\n6. Bag-of-Words\n7. TF-IDF","b4637e63":"# 1. Sentence Tokenization\nSentence tokenization (also called sentence segmentation) is the problem of dividing a string of written language into its component sentences. The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark.\nHowever, even in English, this problem is not trivial due to the use of full stop character for abbreviations. When processing plain text, tables of abbreviations that contain periods can help us to prevent incorrect assignment of sentence boundaries. In many cases, we use libraries to do that job for us, so don\u2019t worry too much for the details for now.\n\n## Example:\nLet\u2019s look a piece of text about a famous board game called backgammon.\n> Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.","19ab8be3":"# Summary\n* NLP is used to apply machine learning algorithms to text and speech.\n* NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data\n* Sentence tokenization is the problem of dividing a string of written language into its component sentences\n* Word tokenization is the problem of dividing a string of written language into its component words\n* The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n* Stop words are words which are filtered out before or after processing of text. They usually refer to the most common words in a language.\n* A regular expression is a sequence of characters that define a search pattern.\n* The bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document.\n* TF-IDF is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\n\n","c59da3ec":"# output\n![img](https:\/\/miro.medium.com\/max\/566\/1*f5e9vn4EZB8zNSLWO0dn-A.png)\nHere are our sentences. Now we can see how the bag-of-words model works.\n![img](https:\/\/miro.medium.com\/max\/453\/1*LtMJ1qSiIuEzZDqB-RQbjw.png)\n# Additional Notes on the Bag of Words Model\n![img](https:\/\/miro.medium.com\/max\/320\/1*JvmcnIYVAzxHYdrxtmMa3Q.png)\nThe complexity of the bag-of-words model comes in deciding how to design the vocabulary of known words (tokens) and how to score the presence of known words.\nDesigning the Vocabulary\nWhen the vocabulary size increases, the vector representation of the documents also increases. In the example above, the length of the document vector is equal to the number of known words.\n\nIn some cases, we can have a huge amount of data and in this cases, the length of the vector that represents a document might be thousands or millions of elements. Furthermore, each document may contain only a few of the known words in the vocabulary.\nTherefore the vector representations will have a lot of zeros. These vectors which have a lot of zeros are called sparse vectors. They require more memory and computational resources.\n\nWe can decrease the number of the known words when using a bag-of-words model to decrease the required memory and computational resources. We can use the text cleaning techniques we\u2019ve already seen in this article before we create our bag-of-words model:\n\n* Ignoring the case of the words\n* Ignoring punctuation\n* Removing the stop words from our documents\n* Reducing the words to their base form (Text Lemmatization and Stemming)\n* Fixing misspelled words\n\nAnother more complex way to create a vocabulary is to use grouped words. This changes the scope of the vocabulary and allows the bag-of-words model to get more details about the document. This approach is called n-grams.\n\nAn n-gram is a sequence of a number of items (words, letter, numbers, digits, etc.). In the context of text corpora, n-grams typically refer to a sequence of words. A unigram is one word, a bigram is a sequence of two words, a trigram is a sequence of three words etc. The \u201cn\u201d in the \u201cn-gram\u201d refers to the number of the grouped words. Only the n-grams that appear in the corpus are modeled, not all possible n-grams.","ac2c7b90":"The difference is that a stemmer operates without knowledge of the context, and therefore cannot understand the difference between words which have different meaning depending on part of speech. But the stemmers also have some advantages, they are easier to implement and usually run faster. Also, the reduced \u201caccuracy\u201d may not matter for some applications.\n# Examples\n1. The word \u201cbetter\u201d has \u201cgood\u201d as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n2. The word \u201cplay\u201d is the base form for the word \u201cplaying\u201d, and hence this is matched in both stemming and lemmatization.\n3. The word \u201cmeeting\u201d can be either the base form of a noun or a form of a verb (\u201cto meet\u201d) depending on the context; e.g., \u201cin our last meeting\u201d or \u201cWe are meeting again tomorrow\u201d. Unlike stemming, lemmatization attempts to select the correct lemma depending on the context.\n\nAfter we know what\u2019s the difference, let\u2019s see some examples using the NLTK tool.","48c114a7":"# Example\nLet\u2019s look at the all bigrams for the following sentence:\nThe office building is open today\nAll the bigrams are:\n* the office\n* office building\n* building is\n* is open\n* open today\nThe bag-of-bigrams is more powerful than the bag-of-words approach.\n\n# Scoring Words\nOnce, we have created our vocabulary of known words, we need to score the occurrence of the words in our data. We saw one very simple approach - the binary approach (1 for presence, 0 for absence).\nSome additional scoring methods are:\n* Counts. Count the number of times each word appears in a document.\n* Frequencies. Calculate the frequency that each word appears in document out of all the words in the document.\n\n# TF-IDF\nOne problem with scoring word frequency is that the most frequent words in the document start to have the highest scores. These frequent words may not contain as much \u201cinformational gain\u201d to the model compared with some rarer and domain-specific words. One approach to fix that problem is to penalize words that are frequent across all the documents. This approach is called TF-IDF.\n\nTF-IDF, short for term frequency-inverse document frequency is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus.\nThe TF-IDF scoring value increases proportionally to the number of times a word appears in the document, but it is offset by the number of documents in the corpus that contain the word.\nLet\u2019s see the formula used to calculate a TF-IDF score for a given term x within a document y.\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*V9ac4hLVyms79jl65Ym_Bw.png)\n\nNow, let\u2019s split this formula a little bit and see how the different parts of the formula work.","8330e06b":"# 2. Design the Vocabulary\n![img](https:\/\/miro.medium.com\/max\/320\/1*AFUcM9S6FwX7RNTW4zqtLQ.png)\nLet\u2019s get all the unique words from the four loaded sentences ignoring the case, punctuation, and one-character tokens. These words will be our vocabulary (known words).\nWe can use the CountVectorizer class from the sklearn library to design our vocabulary. We\u2019ll see how we can use it after reading the next step, too.\n# 3. Create the Document Vectors\n![img](https:\/\/miro.medium.com\/max\/160\/1*90Wv4B73KktRNU9NcYdpjg.png)\nNext, we need to score the words in each document. The task here is to convert each raw text into a vector of numbers. After that, we can use these vectors as input for a machine learning model. The simplest scoring method is to mark the presence of words with 1 for present and 0 for absence.\nNow, let\u2019s see how we can create a bag-of-words model using the mentioned above CountVectorizer class.\n","62eae6ee":"# Examples:\n* am, are, is => be\n* dog, dogs, dog\u2019s, dogs\u2019 => dog\nThe result of this mapping applied on a text will be something like that:\n* the boy\u2019s dogs are different sizes => the boy dog be differ size\nStemming and lemmatization are special cases of normalization. However, they are different from each other.\n\n> Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\n\nSource: https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html","1c972db5":"# Bag-of-words\n![img](https:\/\/miro.medium.com\/max\/320\/1*RPezKXGUUwla-JP52OnZxA.png)\nMachine learning algorithms cannot work with raw text directly, we need to convert the text into vectors of numbers. This is called feature extraction.\nThe bag-of-words model is a popular and simple feature extraction technique used when we work with text. It describes the occurrence of each word within a document.\nTo use this model, we need to:\n1. Design a vocabulary of known words (also called tokens)\n2. Choose a measure of the presence of known words\nAny information about the order or structure of words is discarded. That\u2019s why it\u2019s called a bag of words. This model is trying to understand whether a known word occurs in a document, but don\u2019t know where is that word in the document.\n\nThe intuition is that similar documents have similar contents. Also, from a content, we can learn something about the meaning of the document.","a1379e57":"After reading this , you\u2019ll know some basic techniques to extract features from some text, so you can use these features as input for machine learning models.","b2c2a609":"However, keep in mind that list comprehensions are faster because they are optimized for the Python interpreter to spot a predictable pattern during looping.\nYou might wonder why we convert our list into a set. Set is an abstract data type that can store unique values, without any particular order. The search operation in a set is much faster than the search operation in a list. For a small number of words, there is no big difference, but if you have a large number of words it\u2019s highly recommended to use the set type.\nIf you want to learn more about the time consuming between the different operations for the different data structures you can look at this awesome cheat sheet.\n\n# Regex\n![img](https:\/\/miro.medium.com\/max\/658\/1*l_EB11yQfbZsKLFr8ZckuQ.jpeg)\nA regular expression, regex, or regexp is a sequence of characters that define a search pattern. Let\u2019s see some basics.\n* . - match any character except newline\n* \\w - match word\n* \\d - match digit\n* \\s - match whitespace\n* \\W - match not word\n* \\D - match not digit\n* \\S - match not whitespace\n* [abc] - match any of a, b, or c\n* [^abc] - not match a, b, or c\n* [a-g] - match a character between a & g\n\n> Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python\u2019s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write '\\\\\\\\' as the pattern string, because the regular expression must be \\\\, and each backslash must be expressed as \\\\ inside a regular Python string literal.\nThe solution is to use Python\u2019s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with 'r'. So r\"\\n\" is a two-character string containing '\\' and 'n', while \"\\n\" is a one-character string containing a newline. Usually, patterns will be expressed in Python code using this raw string notation.\n\nSource: https:\/\/docs.python.org\/3\/library\/re.html?highlight=regex\n\nWe can use regex to apply additional filtering to our text. For example, we can remove all the non-words characters. In many cases, we don\u2019t need the punctuation marks and it\u2019s easy to remove them with regex.\nIn Python, the re module provides regular expression matching operations similar to those in Perl. We can use the re.sub function to replace the matches for a pattern with a replacement string. Let\u2019s see an example when we replace all non-words with the space character.","ef780da7":"# Example\nLet\u2019s see what are the steps to create a bag-of-words model. In this example, we\u2019ll use only four sentences to see how this model works. In the real-world problems, you\u2019ll work with much bigger amounts of data.\n## 1. Load the data\n![img](https:\/\/miro.medium.com\/max\/320\/1*JTi6Bnodv2sui50F96v7-Q.png)\nLet\u2019s say that this is our data and we want to load it as an array.\n\n\nI like this movie, it's funny.\n\nI hate this movie.\n\nThis was awesome! I like it.\n\nNice one. I love it.\n","3c651402":"Let\u2019s see how we can remove the stop words from a sentence.\n","24d8716a":"# Stop words\n![img](https:\/\/miro.medium.com\/max\/513\/1*kMf7dZW4jTyaq1hxjA0pgg.png)","affc25d0":"# Siri\n![img](https:\/\/miro.medium.com\/max\/875\/1*-AuKCZbXIVOhI-AgX4J8PQ.jpeg)\n\n\nSiri is a virtual assistant of the Apple Inc.\u2019s iOS, watchOS, macOS, HomePod, and tvOS operating systems. Again, you can do a lot of things with voice commands: start a call, text someone, send an email, set a timer, take a picture, open an app, set an alarm, use navigation and so on.\n\n[Here](https:\/\/www.cnet.com\/how-to\/the-complete-list-of-siri-commands\/) is a complete list of all Siri commands.\n","e8ca465b":"# Gmail\n\n![img](https:\/\/miro.medium.com\/max\/875\/1*fTPhu7PqgIbnngbWG5zFWA.gif)\n\n\nThe famous email service Gmail developed by Google is using spam detection to filter out some spam emails.","46e5aa0a":"## Term Frequency (TF): \na scoring of the frequency of the word in the current document.\n![img](https:\/\/miro.medium.com\/max\/579\/1*V3qfsHl0t-bV5kA0mlnsjQ.png)\n## Inverse Term Frequency (ITF): \na scoring of how rare the word is across documents.\n![img](https:\/\/miro.medium.com\/max\/556\/1*wvPGL02y36QL7-tdG1BT1A.png)\n## Inverse Term Frequency (ITF): \na scoring of how rare the word is across documents.\n![img](https:\/\/miro.medium.com\/max\/556\/1*wvPGL02y36QL7-tdG1BT1A.png)\n\nFinally, we can use the previous formulas to calculate the TF-IDF score for a given term like this:\n![img](https:\/\/miro.medium.com\/max\/368\/1*D2UA6xj9KqcH6amzVj5Y5g.png)\n","6379c5d9":"# Example\nIn Python, we can use the TfidfVectorizer class from the sklearn library to calculate the TF-IDF scores for given documents. Let\u2019s use the same sentences that we have used with the bag-of-words example.\n\n","f276fb36":"# Text Lemmatization and Stemming\nFor grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality.\n> The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\nSource: https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html","1f9b685c":"# Introduction to the NLTK library for Python\nNLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to many corpora and lexical resources. Also, it contains a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Best of all, NLTK is a free, open source, community-driven project.\nWe\u2019ll use this toolkit to show some basics of the natural language processing field. For the examples below, I\u2019ll assume that we have imported the NLTK toolkit.\n We can do this like this: import nltk.","1e60cd84":"# Output\n![img](https:\/\/miro.medium.com\/max\/875\/1*dPXb0hL5GluQCf9jMY1YDQ.png)\n\nAgain, I\u2019ll add the sentences here for an easy comparison and better understanding of how this approach is working.\n\n![img](https:\/\/miro.medium.com\/max\/453\/1*LtMJ1qSiIuEzZDqB-RQbjw.png)","a2f55562":"![img](https:\/\/miro.medium.com\/max\/1250\/1*CuPIUoh1nvh_r1Ssqmy8SA.jpeg)","f7389c70":"If you\u2019re not familiar with the [list comprehensions in Python](https:\/\/towardsdatascience.com\/python-basics-list-comprehensions-631278f22c40). Here is another way to achieve the same result.\n"}}