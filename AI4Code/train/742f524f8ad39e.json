{"cell_type":{"efc56214":"code","21c3c0cd":"code","3ca7b7d6":"code","9728458a":"code","5ccb0ae3":"code","56e9cf4f":"code","16123abd":"code","89378190":"code","249d0e64":"code","02ff1709":"code","cd3693cc":"code","ffd75133":"code","dbe5f787":"code","ecf60372":"code","16b97137":"code","04193a00":"code","3c3fa5da":"code","ef50d022":"code","bd00c6ce":"code","b8ed6766":"code","136602fb":"code","1568c2ce":"code","38a196e1":"code","6ece0910":"code","b6940b39":"code","9199267c":"code","f58ade39":"code","05f59f56":"code","0a0e2715":"code","681f0f4a":"code","9025f8eb":"code","0d98ceb9":"code","b7a5d438":"code","798bcb48":"code","eaf29a92":"code","78b8cce4":"code","255b2835":"markdown","f3a5702f":"markdown","b253d375":"markdown","0270f0ee":"markdown","75998750":"markdown","393f52ed":"markdown","e21af563":"markdown","176696d7":"markdown","6a887bb8":"markdown","5b19fabe":"markdown","2f0f161b":"markdown","9636a894":"markdown","633cee90":"markdown"},"source":{"efc56214":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21c3c0cd":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier # to build a classification tree \nfrom sklearn.tree import plot_tree #to draw a classification tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score #for cross validation\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import plot_confusion_matrix","3ca7b7d6":"df=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","9728458a":"df.head()","5ccb0ae3":"df.dtypes","56e9cf4f":"df.isna().sum()","16123abd":"corr=df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","89378190":"sns.boxplot(x='quality',y='free sulfur dioxide',data=df)","249d0e64":"sns.boxplot(x='quality',y='total sulfur dioxide',data=df)","02ff1709":"# dropping fixed acidity, free sulfur dioxide\ndf=df.drop(['fixed acidity','free sulfur dioxide'],axis=1)","cd3693cc":"sns.lineplot(x='quality',y='volatile acidity',data=df)","ffd75133":"X=df.iloc[:,1:9]\nY=df.iloc[:,9]","dbe5f787":"Y.unique()","ecf60372":"Y.values[Y.values < 6.5] = 0","16b97137":"Y.values[Y.values > 6.5] = 1","04193a00":"Y.unique()","3c3fa5da":"Y.value_counts()","ef50d022":"sns.histplot(data=Y)","bd00c6ce":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","b8ed6766":"DC=DecisionTreeClassifier(random_state=42)\nmodel=DC.fit(X_train,y_train)","136602fb":"plt.figure(figsize=(45,45))\nplot_tree(model,filled=True,rounded=True,feature_names=X_train.columns,class_names=[\"Bad\",\"Good\"])\nplt.show()","1568c2ce":"plot_confusion_matrix(model,X_test,y_test,display_labels=[\"Bad\",\"Good\"])","38a196e1":"path = model.cost_complexity_pruning_path(X_train, y_train) # Determine values for alpha\nccp_alphas = path.ccp_alphas                                 # extract different values for alpha\nccp_alphas = ccp_alphas[:-1]                                 # Exclude the maximum values","6ece0910":"clf_dts = []   ## Create an array to put decision trees in","b6940b39":"## Create one decision tree per alpha and store in array\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf_dt.fit(X_train, y_train)\n    clf_dts.append(clf_dt)","9199267c":"train_scores = [clf_dt.score(X_train, y_train) for clf_dt in clf_dts]\ntest_scores = [clf_dt.score(X_test, y_test) for clf_dt in clf_dts]","f58ade39":"\nfig, ax = plt.subplots()\nax.set_xlabel('alpha')\nax.set_ylabel('accuracy')\nax.set_title('Accuracy vs alphas for training and testing sets')\nax.plot(ccp_alphas, train_scores, marker='o', label='train', drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label='test', drawstyle=\"steps-post\")\nax.legend()\nplt.show()","05f59f56":"DT = DecisionTreeClassifier(random_state=42, ccp_alpha=0.01)\nA=cross_val_score(DT,X_train,y_train,scoring='accuracy',cv=5).mean()\nA","0a0e2715":"alpha_loop_values = []\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(clf_dt, X_train, y_train, cv=5)\n    alpha_loop_values.append([ccp_alpha, np.mean(scores), np.std(scores)])\n\nalpha_results = pd.DataFrame(alpha_loop_values,\n                            columns=['alpha', 'mean_accuracy', 'std'])\nalpha_results.plot(x='alpha', y='mean_accuracy', yerr='std', marker='o', linestyle='--')","681f0f4a":"best_alpha = alpha_results.iloc[alpha_results['mean_accuracy'].idxmax(),]['alpha']","9025f8eb":"best_alpha","0d98ceb9":"clf_dt_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\nclf_dt_pruned.fit(X_train, y_train)","b7a5d438":"plot_confusion_matrix(clf_dt_pruned,X_test,y_test,display_labels=[\"Bad\",\"Good\"])","798bcb48":"plt.figure(figsize=(15, 7.5))\nplot_tree(clf_dt_pruned, \n         filled=True,\n         rounded=True,\n         class_names=[\"Bad\",\"Good\"],\n         feature_names=X_train.columns);","eaf29a92":"# y_pred=model.predict(X_test)","78b8cce4":"# importance=DC.feature_importances_\n# plt.bar([x for x in range(len(importance))], importance)","255b2835":"Evaluating Final Model","f3a5702f":"In the confusion matrix, we can see that 251+22 = 273 wine are bad in taste of which 128 are correctly classified. And of the 22+25 = 22 wine are good of which 10 are correctly classified. \nIt seem that classification tree is overfit, and require pruning","b253d375":"All are float or integer datatype chance of missing value entered with random number is checked.","0270f0ee":"You can use apply with list comprehension:\nY = Y.apply(lambda x: [0 if y <= 6.5 else 1 for y in x])","75998750":"Creating a decision tree and fitting it into a training set","393f52ed":"All the columns are filled so we can confirm that our dataset does not have missing values.","e21af563":"Splittting data in training and testing","176696d7":"we can infer a trend in volatile acidity which tend to decrese quality as it increases","6a887bb8":"This is the final tree","5b19fabe":"To get the binary output we need convert good for Y>6.5 and else bad quality.","2f0f161b":"Finding best alpha","9636a894":"from graph, a good value for alpha might be 0.01 as the tesing accuracy is the highest at this value.","633cee90":"Cost Complexity Pruning is the action of selectively removing certain parts of a trees to promote its growth. Reducing the number of leaf nodes may result in slightly worse training accuracy, but greatly improve testing performance. Alpha is a pruning parameter, and the higher the alpha, the more nodes will be pruned."}}