{"cell_type":{"fa4fc5c9":"code","0f336501":"code","d3ddea61":"code","b5e939d9":"code","2522ec7d":"code","5c3c9c71":"code","ce09dac4":"code","6768daa4":"code","104a08cb":"code","4f118809":"code","4a46afd6":"code","ad20da78":"markdown","5c71d6f8":"markdown","deea3ec2":"markdown","1ee5cbb8":"markdown","f16ec338":"markdown","4e4810c9":"markdown","0b187845":"markdown","eeb508d1":"markdown","b533188b":"markdown","f349955b":"markdown"},"source":{"fa4fc5c9":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f336501":"# The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).\n#The dataset provides the patients\u2019 information\nheartDiseaseData = pd.read_csv(\"\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv\")\nheartDiseaseData.info()","d3ddea61":"#heartDiseaseData.drop([\"education\"], axis = 1, inplace = True) # Delete useless feature.\nheartDiseaseData.dropna(how=\"any\", inplace = True)  # Delete useless raw\n\nchd = heartDiseaseData.TenYearCHD.values \nfeaturees = heartDiseaseData.drop([\"TenYearCHD\"], axis = 1) # features before normalization\nfeaturees","b5e939d9":"features = (featurees - np.min(featurees))\/(np.max(featurees) - np.min(featurees)).values # features after normalization\nfeatures","2522ec7d":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, chd_train, chd_test = train_test_split(features, chd ,test_size = 0.2 , random_state = 42)\n# test_size ==> 80% of data set for Train, 20% of data set for Test\n\n\nfeatures_train = features_train.T\nfeatures_test = features_test.T\nchd_train = chd_train.T\nchd_test = chd_test.T\n\nprint(\"Changed of Features and Values place.\")\n\nprint(\"features_train: \", features_train.shape)\nprint(\"features_test \", features_test.shape)\nprint(\"chd_train: \", chd_train.shape)\nprint(\"chd_test: \", chd_test.shape)","5c3c9c71":"def initialize_weights_and_bias(dimension):\n    \n    weights = np.full((dimension,1), 0.01) \n    bias = 0.0 \n    return weights,bias\n    \ndef sigmoid(z):\n    chd_head = 1\/(1+np.exp(-z))\n    return chd_head","ce09dac4":"def forward_backward_propagation(weights, bias , features_train, chd_train):\n    #forward propagation\n    \n    z = np.dot(weights.T,features_train) + bias\n    chd_head = sigmoid(z)\n    loss = -chd_train*np.log(chd_head) - (1- chd_train)*np.log(1-chd_head)\n    cost = (np.sum(loss))\/features_train.shape[1]\n    \n    #backward propagation\n    derivative_weights = (np.dot(features_train,((chd_head-chd_train).T)))\/features_train.shape[1] \n    derivative_bias = np.sum(chd_head-chd_train)\/features_train.shape[1] \n    gradients = {\"derivative_weights\" : derivative_weights, \"derivative_bias\" : derivative_bias}\n    return cost,gradients","6768daa4":"def update(weights, bias, features_train, chd_train, learning_rate, number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterations):\n        \n        cost, gradients = forward_backward_propagation(weights, bias, features_train, chd_train)\n        cost_list.append(cost)\n        \n        weights = weights - learning_rate* gradients[\"derivative_weights\"]\n        bias = bias - learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0: # her 10 ad\u0131mda bir depolar\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i: %f \" %(i,cost))\n            \n        \n    parameters =  {\"weights\" : weights, \"bias\" : bias}\n    plt.plot(index,cost_list2)\n    plt.xticks(index, rotation = \"vertical\")\n    plt.xlabel(\"Number Of Iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","104a08cb":"def predict(weights, bias, features_test):\n    \n    z = sigmoid(np.dot(weights.T,features_test)+bias)\n    chd_prediction = np.zeros((1,features_test.shape[1]))\n    \n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5 :\n            chd_prediction[0,i] = 0\n        else:\n            chd_prediction[0,i] = 1\n            \n    return chd_prediction","4f118809":"def logistic_regression(features_train, chd_train, features_test, chd_test, learning_rate, number_of_iterations):\n    dimension = features_train.shape[0] # that is 14(features)\n    weights, bias = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(weights, bias, features_train, chd_train, learning_rate, number_of_iterations) \n    \n    chd_prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"], features_test)\n    \n    print(\"Test occuracy: {}% \".format(100-np.mean(np.abs(chd_prediction_test - chd_test))*100))\n    \nlogistic_regression(features_train, chd_train, features_test, chd_test, learning_rate = 5, number_of_iterations = 300) ","4a46afd6":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(features_train.T,chd_train.T)\nprint(\"test accuracy {}\".format(lr.score(features_test.T,chd_test.T)))","ad20da78":"# **TRAIN-TEST SPLIT**\n\nTrain Test Split data==> 80% of data set for Train, 20% of data set for Test","5c71d6f8":"**Data Source: https:\/\/www.kaggle.com\/dileep070\/heart-disease-prediction-using-logistic-regression**\n# Data's demographic:\n\u2022 Sex: male(2) or female(0) (Nominal)\n\n\u2022 Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous) \n\nBehavioral\n\n\u2022 Current Smoker: whether or not the patient is a current smoker (Nominal)\n\n\u2022 Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n\nMedical( history)\n\n\u2022 BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n\n\u2022 Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n\n\u2022 Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n\n\u2022 Diabetes: whether or not the patient had diabetes (Nominal)\n\nMedical(current)\n\n\u2022 Tot Chol: total cholesterol level (Continuous)\n\n\u2022 Sys BP: systolic blood pressure (Continuous)\n\n\u2022 Dia BP: diastolic blood pressure (Continuous)\n\n\u2022 BMI: Body Mass Index (Continuous)\n\n\u2022 Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n\n\u2022 Glucose: glucose level (Continuous)\nPredict variable (desired target)\n\n\u2022 10 year risk of coronary heart disease CHD (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)","deea3ec2":"# **LOGISTIC REGRESSION**\nMain part.\nPut it all together.","1ee5cbb8":"# **UPDATE**\nUpdate weights and bias with backward-forward propagation.","f16ec338":"# **Logistic Regression with Sklearn Library**","4e4810c9":"# **FORWARD AND BACKWARD PROPAGATION FUNCTION**\nz = bias + px1w1 + px2w2 + ... + pxn*wn\nloss function = -(1 - y) log(1- y_head) - y log(y_head)\ncost function = sum(loss value) \/ train dataset sample count","0b187845":"# **NORMALIZATION**\n\nFormula = (x -min(x))\/(max(x)-min(x)\n","eeb508d1":"# **PREDICT**\nPredict function for testing purposes.","b533188b":"# **PARAMETER INITALIZE AND SIGMOID FUNCTION**\n\nTime to start defining functions.First of all I need to initialize my weights and bias, then I will need a sigmoid function.\n\nSigmoid Function : f(x) = 1 \/ ( 1 + (e ^ -x)\nInitialize weight = 0.01 for each data\nInitialize bias = 0","f349955b":"As you can see my labels are ordered -male = 1, female = 0-. Then i need to delete that useless features."}}