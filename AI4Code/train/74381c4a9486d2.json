{"cell_type":{"1be440c0":"code","33ee8ad0":"code","4dc1688f":"code","2ce9b6a6":"code","02f0633f":"code","f0d34358":"code","340d68a4":"code","b4d3dd07":"code","de4cdcc2":"code","229cbd21":"code","70b812dd":"code","f0345881":"code","6cc22e07":"code","5be1e60f":"code","051bf0f0":"code","dda0ece3":"code","392c03cf":"code","059c6e21":"code","0c93335d":"code","3fd1ecbb":"code","3dd27974":"code","c49e8025":"code","f1ec63f4":"code","b0233b84":"code","f36c4dc9":"code","24a06d3d":"code","dabc3770":"code","af20d4da":"code","b84f0055":"code","d7af6e9a":"code","bfa6bac5":"code","daf42683":"code","98e149ff":"code","3f7fbbce":"code","b76f1f91":"code","2cedc69f":"code","66a6d978":"code","3eb22da3":"code","b7e23634":"code","37b4c22e":"code","182c7915":"code","ac115043":"code","a96e5c66":"code","6b83e1c7":"code","ac79587b":"code","dfa7ce41":"code","25ff794b":"code","d4d69931":"code","90cf492b":"code","0fd5783d":"code","e1db103c":"code","edc7fb79":"code","554dc5b6":"code","dc5038af":"code","9ec07428":"code","5e80eda3":"code","ec542577":"code","17d1fc2a":"code","a92a5365":"code","030be32e":"code","c8507e5d":"code","d3a044e6":"markdown","46e6129c":"markdown","26ae4a22":"markdown","3c0a55ba":"markdown","361e3e1e":"markdown","56ed6005":"markdown","4e34b663":"markdown","2b57289d":"markdown","4fa6e8fb":"markdown","5ecc8e6c":"markdown","2bf033f2":"markdown","6938101e":"markdown","07e2dbf1":"markdown","080c1142":"markdown","4f5cea6f":"markdown","a31b4e23":"markdown","f18200df":"markdown","37c705a6":"markdown","2c52dc58":"markdown","0978a884":"markdown","1fac0245":"markdown","cad0f35c":"markdown","1daefda2":"markdown","cc4ebd94":"markdown","1079b63d":"markdown","5780a5f5":"markdown","104ab0d2":"markdown","432ee651":"markdown"},"source":{"1be440c0":"# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33ee8ad0":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nscalar=StandardScaler()\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4dc1688f":"%%time\ndf=pd.read_csv('\/kaggle\/input\/income-classification\/income_evaluation.csv')","2ce9b6a6":"df","02f0633f":"%%time\nstart='\\033[1m'\nend='\\033[0;0m'","f0d34358":"%%time\n     \n# function for getting details of the dataframe\ndef details(df):\n    print(start+'Shape of the Dataframe:'+end,df.shape,'\\n')\n    print(start+'Summary Of the dataframe:'+end,'\\n')\n    print(df.info(),'\\n')\n    print(start+'Statistical Summary of the Dataframe:'+end,'\\n',df.describe(),'\\n')\n    print(start+'Checking for Null values in the dataframe:'+end,'\\n',df.isnull().sum(),'\\n')\n    print(start+'Columns in dataframe:'+end,df.columns)\n    \n    \n#function for getting insigt for each column values\ndef values_in_columns(df):\n    for i in df.columns:\n        print(start+'Column Name--->'+i+end)\n        print('Number of Unique Values',df[i].nunique(),'\\n')\n        print('Count of each unique value \\n',df[i].value_counts(),'\\n')\n        \n        \n        \n# function for dropping duplicates        \ndef duplicate(df):\n    if df.duplicated().sum()>0:\n        print(start+'Dataframe contains duplicate values'+end,df.duplicated().sum())\n        df.drop_duplicates(inplace=True,ignore_index=True)\n        print('Details of dataframe after droping the Duplicate rows')\n        details(df)\n    else:\n        print(\"Dataframe doesn't contains dublicate Values\")\n        \nfont = {'family': 'serif',\n            'weight': 'normal',\n            'size': 30,\n            }\n#function for ploting bar graph for each column of the dataframe\ndef plot_each_columns(data):\n    for i in data.columns:\n        if len(data[i].value_counts())>20:\n            df = data[i].value_counts()[:20]\n            plt.figure(figsize=(30,10))\n            sns.barplot(x=df.index, y=df)\n            plt.xticks(rotation=90,fontsize = 20)\n            plt.yticks(fontsize = 20)\n            plt.title(i,fontdict=font)\n            \n        else:\n            df = data[i].value_counts()\n            plt.figure(figsize=(30,10))\n            sns.barplot(x=df.index, y=df)\n            plt.xticks(rotation=90,fontsize = 20)\n            plt.yticks(fontsize = 20)\n            plt.title(i,fontdict=font)\n\n#function for plotting countplot of each with repect to any specific column for column havind unique value less than 20\ndef plot_columns(data,hue_name):\n\n    for i in data.columns:\n    \n        if len(data[i].value_counts())<20:\n            f, ax = plt.subplots(figsize=(10, 8))\n            ax = sns.countplot(x=i, hue=hue_name, data=df)\n            ax.set_title(\"Frequency distribution of \"+i+\" variable wrt \"+hue_name)\n            plt.xticks(rotation=90,fontsize = 20)\n            plt.yticks(fontsize = 20)\n            plt.show()\n\n            \n# function for plotting box plot of each numerical column\ndef boxplot_variable(data):\n    for i in numerical:  \n        f, ax = plt.subplots(figsize=(10, 8))\n        ax = sns.boxplot(x=i,  data=data)\n        ax.set_title(\"Box Plot of \"+i)\n        plt.xticks(rotation=90,fontsize = 20)\n        plt.yticks(fontsize = 20)\n        plt.show()\n\n        \n#function for plotting boxplot of each numerical column with respect to specific columns\ndef boxplot_mult(df,x,hue_name):\n    for i in numerical:  \n        f, ax = plt.subplots(figsize=(10, 8))\n        ax = sns.boxplot( x=x,y=i,hue=hue_name, data=df)\n        ax.set_title(\"Box Plot of \"+i+\" variable wrt \"+x)\n        plt.xticks(rotation=90,fontsize = 20)\n        plt.yticks(fontsize = 20)\n        plt.show()\n        \n","340d68a4":"%%time\n\ndetails(df)","b4d3dd07":"%%time\ndf.columns = df.columns.str.replace(' ', '')\ndf.columns = df.columns.str.replace('-', '_')\n\ncategorical = []\nnumerical = []\n\nfor i in df.columns:\n    if df[i].dtype=='O':\n        categorical.append(i)\n    else:\n        numerical.append(i)\n        ","de4cdcc2":"%%time\nduplicate(df)\n","229cbd21":"%%time\nvalues_in_columns(df)\n","70b812dd":"%%time\ndf.replace({' ?':None},inplace=True)\n","f0345881":"%%time\ndetails(df)","6cc22e07":"%%time\nplot_each_columns(df)","5be1e60f":"%%time\nplot_columns(df,'sex')","051bf0f0":"%%time\nplot_columns(df,'income')","dda0ece3":"%%time\nsns.pairplot(data=df, hue=\"income\",markers=[\"o\", \"D\"])\nplt.title('Distributions for each variable')\nplt.show()","392c03cf":"%%time\nsns.pairplot(data=df, hue=\"sex\",markers=[\"o\", \"D\"])\nplt.title('Distributions for each variable')\nplt.show()","059c6e21":"%%time\nboxplot_variable(df)","0c93335d":"%%time\nboxplot_mult(df,x='income',hue_name='sex')    ","3fd1ecbb":"%%time\nboxplot_mult(df,x='sex',hue_name='income')","3dd27974":"%%time\ndf.corr()","c49e8025":"%%time\ndf[\"workclass\"].fillna('ffill',inplace=True)\ndf[\"occupation\"].fillna('ffill',inplace=True)\ndf[\"native_country\"].fillna('ffill',inplace=True)","f1ec63f4":"%%time\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()","b0233b84":"for i in df[categorical]:\n    df[i] = le.fit_transform(df[i])","f36c4dc9":"df","24a06d3d":"X = df.drop('income',axis=1)\ny = df[\"income\"]","dabc3770":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=123)","af20d4da":"X_train.shape, X_test.shape","b84f0055":"# function for generating the performance report of the trained model \ndef report(model,X_train, X_test, y_train, y_test):\n    print(start+'For training Data'+end)\n    y_pred = model.predict(X_train)\n    print(classification_report(y_train,y_pred))\n    print(f'F1 Score for train data is {f1_score(y_train,y_pred)}')\n    print(f'Precision for train data is {precision_score(y_train,y_pred)}')\n    print(f'Recall for train data is {recall_score(y_train,y_pred)}')\n    print('\\n',start+'For testing Data'+end)\n    y_pred = lr.predict(X_test)\n    print(classification_report(y_test,y_pred))\n    print(f'F1 Score for test data is {f1_score(y_test,y_pred)}')\n    print(f'Precision for train data is {precision_score(y_test,y_pred)}')\n    print(f'Recall for train data is {recall_score(y_test,y_pred)}')\n","d7af6e9a":"%%time\n# Logistic Regression\nlr = LogisticRegression(solver='liblinear',random_state=123)\nlr.fit(X_train,y_train)\nreport(lr,X_train, X_test, y_train, y_test)","bfa6bac5":"%%time\n# decision tree\ndt = tree.DecisionTreeClassifier()\ndt.fit(X_train,y_train)\nreport(dt,X_train, X_test, y_train, y_test)","daf42683":"%%time\n# random forest\nrf = RandomForestClassifier(n_estimators=100,random_state=123)\nrf.fit(X_train,y_train)\nreport(rf,X_train, X_test, y_train, y_test)","98e149ff":"%%time\n# Xgboost Classifier\nxgb = XGBClassifier(random_state=42)\nxgb.fit(X_train,y_train)\nreport(xgb,X_train, X_test, y_train, y_test)","3f7fbbce":"%%time\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"none\",\"l2\"]}\nlr1=LogisticRegression(random_state=123)\nlr_cv=GridSearchCV(lr1,grid,cv=10)\nlr_cv.fit(X_train,y_train)\n","b76f1f91":"lr_cv.best_params_","2cedc69f":"%%time\n# Logistic Regression\nlr = LogisticRegression(solver='liblinear',random_state=123,C= 1000.0, penalty= 'l2')\nlr.fit(X_train,y_train)\nreport(lr,X_train, X_test, y_train, y_test)\n","66a6d978":"tree_para = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\ndt1 = tree.DecisionTreeClassifier()\ndt_cv = GridSearchCV(dt1, tree_para, cv=5)\ndt_cv.fit(X_train,y_train)\ndt_cv.best_params_","3eb22da3":"%%time\n# decision tree\ndt = tree.DecisionTreeClassifier(criterion='gini',max_depth=8)\ndt.fit(X_train,y_train)\nreport(dt,X_train, X_test, y_train, y_test)","b7e23634":"%%time\nparam_grid = { \n    'n_estimators': [100, 200],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nrf1 = RandomForestClassifier()\nrf_cv = GridSearchCV(estimator=rf1, param_grid=param_grid, cv= 5,n_jobs=10)\nrf_cv.fit(X_train, y_train)\nrf_cv.best_params_","37b4c22e":"%%time\n# random forest\nrf = RandomForestClassifier(n_estimators=100,criterion='gini',max_depth=8,random_state=123)\nrf.fit(X_train,y_train)\nreport(rf,X_train, X_test, y_train, y_test)","182c7915":"%%time\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'max_depth': [3, 4, 5]\n        }\nxgb1 = XGBClassifier(random_state=42)\nxgb_cv = GridSearchCV(xgb,params,cv=10,n_jobs=-1)\nxgb_cv.fit(X_train,y_train)\nxgb_cv.best_params_","ac115043":"%%time\n# Xgboost Classifier\nxgb = XGBClassifier(random_state=42,min_child_weight=5,gamma=0.5,max_depth=4)\nxgb.fit(X_train,y_train)\nreport(xgb,X_train, X_test, y_train, y_test)","a96e5c66":"df.income.value_counts()","6b83e1c7":"from sklearn.utils import resample","ac79587b":"# Separate majority and minority classes\ndf_majority = df[df.income==0]\ndf_minority = df[df.income==1]","dfa7ce41":"df_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=len(df_majority),    # to match majority class\n                                 random_state=123) ","25ff794b":"df_upsampled = pd.concat([df_majority, df_minority_upsampled]).sample(frac=1)","d4d69931":"X = df_upsampled .drop('income',axis=1)\ny = df_upsampled [\"income\"]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=123)\nX_train.shape, X_test.shape","90cf492b":"%%time\n# Logistic Regression\nlr = LogisticRegression(solver='liblinear',random_state=123)\nlr.fit(X_train,y_train)\nreport(lr,X_train, X_test, y_train, y_test)","0fd5783d":"%%time\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlr1=LogisticRegression(random_state=123)\nlr_cv=GridSearchCV(lr1,grid,cv=10)\nlr_cv.fit(X_train,y_train)\n","e1db103c":"lr_cv.best_params_","edc7fb79":"%%time\n# Logistic Regression\nlr = LogisticRegression(solver='liblinear',random_state=123,C= 0.01, penalty= 'l2')\nlr.fit(X_train,y_train)\nreport(lr,X_train, X_test, y_train, y_test)","554dc5b6":"%%time\n# decision tree\ndt = tree.DecisionTreeClassifier()\ndt.fit(X_train,y_train)\nreport(dt,X_train, X_test, y_train, y_test)","dc5038af":"tree_para = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\ndt1 = tree.DecisionTreeClassifier()\ndt_cv = GridSearchCV(dt1, tree_para, cv=5)\ndt_cv.fit(X_train,y_train)\ndt_cv.best_params_","9ec07428":"%%time\n# decision tree\ndt = tree.DecisionTreeClassifier(criterion='gini',max_depth=50)\ndt.fit(X_train,y_train)\nreport(dt,X_train, X_test, y_train, y_test)","5e80eda3":"%%time\n# random forest\nrf = RandomForestClassifier(n_estimators=100,random_state=123)\nrf.fit(X_train,y_train)\nreport(rf,X_train, X_test, y_train, y_test)","ec542577":"%%time\nparam_grid = { \n    'n_estimators': [100, 200],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\nrf1 = RandomForestClassifier()\nrf_cv = GridSearchCV(estimator=rf1, param_grid=param_grid, cv= 5,n_jobs=-1)\nrf_cv.fit(X_train, y_train)\nrf_cv.best_params_","17d1fc2a":"%%time\n# random forest\nrf = RandomForestClassifier(n_estimators=100,criterion='gini',max_depth=8,random_state=123)\nrf.fit(X_train,y_train)\nreport(rf,X_train, X_test, y_train, y_test)","a92a5365":"%%time\n# Xgboost Classifier\nxgb = XGBClassifier(random_state=42)\nxgb.fit(X_train,y_train)\nreport(xgb,X_train, X_test, y_train, y_test)","030be32e":"%%time\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'max_depth': [3, 4, 5]\n        }\nxgb1 = XGBClassifier(random_state=42)\nxgb_cv = GridSearchCV(xgb,params,cv=10,n_jobs=-1)\nxgb_cv.fit(X_train,y_train)\nxgb_cv.best_params_","c8507e5d":"%%time\n# Xgboost Classifier\nxgb = XGBClassifier(random_state=42,min_child_weight=5,gamma=0.5,max_depth=4)\nxgb.fit(X_train,y_train)\nreport(xgb,X_train, X_test, y_train, y_test)","d3a044e6":"### Decision Tree","46e6129c":"#### Interpretations\n1. Data is not having null values<br>\n2. Min age is 17 and Max age is 90<br>\n3. Capital loss and capital gian contains its value more zero's ","26ae4a22":"### Logistic Regression","3c0a55ba":"#### Interpretations\n1. workclass, occupation, native-country contains '?' as the value,will treat it as nan value <br>","361e3e1e":"### Random Forest","56ed6005":"### Logistics Regression","4e34b663":"## F1 Score is coming low after hyparparameter tunning then due to class imbalance lets upsample the minority class and see the results","2b57289d":"### For making the desired output string in BOLD","4fa6e8fb":"## Training model using Hyparparmeter  tuning","5ecc8e6c":"### Decision Tree","2bf033f2":"### Random Forest ","6938101e":"#### Interpretations:\n1. Capital loss and capital gain are having maximum outlier values because mostly rows in these columns are zero\n2. Median of income more than 50k is more in males than females\n3. Education num for mor than 50k is more in females\n4. Income of older people is more than younger ones","07e2dbf1":"### Changing the categorical columns into numerical","080c1142":"### XGBoost","4f5cea6f":"### XGBoost","a31b4e23":"### Random Forest","f18200df":"### Logistic Regression","37c705a6":"### Functions Used For Analysis","2c52dc58":"#### Interpretations\n1. Now there are Null values in columns we will work on it in later part","0978a884":"#### Interpretations\n1. Number of female having divorce, separated or widowed is more acc to marital status\n2. Number of females working as ADM-clerical, priv-house serv,Other services are more\n3. Most females are not in family, unmaried or wifes realtionship wise4\n4. Ratio of male and female earning less than 50k is less then Ratio of male and female earning more than 50k","1fac0245":"## Training model simply","cad0f35c":"#### Interpretations:\n1. People having Masters, Doctorate, Prof-school are having income more than 50k than less than 50k\n2. People earning more than 50k income are having Married civ spouse Martial status\n3. Income in Armed forces and priv-house serv is lowest\n","1daefda2":"### Loading the dataset","cc4ebd94":"#### Interpretations\n1. Main occupation of People are Prof-specialty, Craft repair, exec-managerial, Adm-clerical, Sales <br>\n2. Mostly people's Native country is from United States<br>\n3. People having income less than 50K is more\n4. AVg Working hours per week is 40\n5. Capital Loss having 31018 number of rows zero which is large in number so other values are treated as outliers\n6. Capital Gain having 29825 number of rows zero which is large in number so other values are treated as outliers\n7. In the dataframe percentage of Male's are approximately double of Female's\n8. Half of the male's are married\n9. Mostly Race of the people is White \n10. Approxmately 10K people are not married\n11. Mostly people are high school graduate or in some college or completed bachelors\n12. Workclass of majority of people's is Private.","1079b63d":"#### Interpretations\n1. Number of duplicate values in dataframe is 24 which is being dropped","5780a5f5":"### XGboost","104ab0d2":"###  Filling missing values using ffill","432ee651":"### Decision Tree"}}