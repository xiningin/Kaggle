{"cell_type":{"4b5b31ef":"code","30afd51c":"code","050ceeb4":"code","b418e914":"code","db68b0d7":"code","7cf7cfcf":"code","9b7e5b2b":"code","a9728edf":"code","dcdc4cbc":"code","a73c9838":"code","6b409b27":"code","7a7408f4":"code","37797d35":"code","6dde0050":"code","895c1f09":"code","e8328ff0":"code","b6c09a58":"code","d2969f84":"code","9c1a9e79":"code","03dbde9d":"code","ba699946":"code","88cf5cf1":"code","0f5af5b5":"code","b347ad5f":"code","a674c6e5":"code","d109a8dc":"code","39c467e5":"code","6a1ec553":"code","d1e2452f":"code","d122aa61":"code","c8fa5825":"code","598a7d91":"code","636d60b3":"code","02b82e8e":"code","a93c02bf":"code","b3083a42":"code","210f5970":"code","46b05914":"code","ab962fef":"code","9ad27d7d":"code","fcd8e650":"code","d89d3a52":"code","963bb652":"code","806ea3d2":"code","79d5db96":"code","f4d788ca":"markdown","4b996904":"markdown","14184905":"markdown","09bf5738":"markdown","b9b44b63":"markdown","e4006f8a":"markdown","b5872c7f":"markdown","dbe720a0":"markdown","27a6f122":"markdown","5b30e5cd":"markdown","42b0ab1a":"markdown","dc74deab":"markdown","c5b9197a":"markdown","07e4dfb3":"markdown","318ca502":"markdown","bfa9e71b":"markdown","95148a63":"markdown","8752afb9":"markdown"},"source":{"4b5b31ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","30afd51c":"from scipy import stats\nfrom sklearn.feature_extraction import DictVectorizer as DV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","050ceeb4":"# load data\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","b418e914":"# Setting PassengerID to index and separating target column\ntrain_df.set_index('PassengerId',inplace = True)\ny_train = train_df.Survived\ndel train_df['Survived']\ntest_df.set_index('PassengerId',inplace = True)","db68b0d7":"train_df.head()","7cf7cfcf":"train_df.info()","9b7e5b2b":"sns.pairplot(train_df)","a9728edf":"# Distribution of classes\ny_train.hist(bins = 3)\nplt.title('Target distribution')","dcdc4cbc":"numeric_cols = ['Age','Fare']\ncat_cols = ['Pclass','Sex','SibSp','Parch','Ticket','Cabin','Embarked']","a73c9838":"numeric_train = train_df[numeric_cols]\ncat_train = train_df[cat_cols].astype(str)\n\nnumeric_test = test_df[numeric_cols]\ncat_test = test_df[cat_cols].astype(str)","6b409b27":"# filling nans with mean for numeric and 'None' for categorial\n# train\nnumeric_train =  numeric_train.fillna(numeric_train.mean())\ncat_train = cat_train.fillna('None')\n# test\nnumeric_test = numeric_test.fillna(numeric_test.mean())\ncat_test = cat_test.fillna('None')","7a7408f4":"train_df.Ticket.str.replace('\\D','').replace('',np.nan).dropna().astype(int).hist()\nplt.title('Distribution of numeric data from Ticket feature')","37797d35":"stats.spearmanr(train_df.Ticket.str.replace('\\D','').replace('',0).astype(int),y_train)","6dde0050":"\ntrain_df.Cabin.str.replace('\\D','').replace('',np.nan).dropna().astype(int).hist()\nplt.title('Distribution of numeric data from Cabin feature')","895c1f09":"stats.spearmanr(train_df.Cabin.str.replace('\\D','').replace('',0).fillna(0).astype(int),y_train)","e8328ff0":"def get_num_info(data):\n    \"\"\"\n    Get numbers from string as int\n    \"\"\"\n    _num = data.str.replace('\\D','')\n    _num = _num.replace('',np.nan)\n    _num = _num.fillna(_num.dropna().astype(int).median())\n    return _num.astype(int)\ndef get_text_info(data):\n    \"\"\"\n    Remove all digits and special symbols, bring remaining to upper case\n    \"\"\"\n    _text = data.str.replace('\\d+', '').replace('','None') # removing all digits\n    _text = _text.str.strip() # removing spaces\n    _text = _text.str.replace('.','')\n    _text = _text.str.replace('\/','')\n    _text = _text.str.upper() # everything to upper case\n    return _text","b6c09a58":"# tickets for train\nnumeric_train['Tickets_number'] = get_num_info(cat_train.Ticket)\ncat_train['Tickets_text'] = get_text_info(cat_train.Ticket)\n# cabin for train\nnumeric_train['Cabin_number'] = get_num_info(cat_train.Cabin)\ncat_train['Cabin_text'] = get_text_info(cat_train.Cabin)\n#tickets for test\nnumeric_test['Tickets_number'] = get_num_info(cat_test.Ticket)\ncat_test['Tickets_text'] = get_text_info(cat_test.Ticket)\n#cabin for test\nnumeric_test['Cabin_number'] = get_num_info(cat_test.Cabin)\ncat_test['Cabin_text'] = get_text_info(cat_test.Cabin)\n\ndel cat_train['Ticket'],cat_train['Cabin'],cat_test['Ticket'],cat_test['Cabin']","d2969f84":"numeric_train.corrwith(y_train,method = 'spearman')","9c1a9e79":"sns.pairplot(numeric_train.join(y_train))","03dbde9d":"prob_check = cat_train.join(y_train)\nfor feature in cat_train.columns:\n    print('----------')\n    print(feature)\n    print('----------')\n    for item in cat_train[feature].unique():\n        print(item, np.round(len(prob_check[(prob_check[feature] == item)&(prob_check['Survived'] == 1)])\/len(prob_check[prob_check['Survived'] == 1]),2))     ","ba699946":"len(train_df) - train_df['Cabin'].count() ","88cf5cf1":"cat_train['Ticket_has_text'] = np.where(cat_train['Tickets_text'] == 'NONE','1','0')\ncat_test['Ticket_has_text'] = np.where(cat_test['Tickets_text'] == 'NONE','1','0')","0f5af5b5":"encoder = DV(sparse = False)\nX_cat_train = encoder.fit_transform(cat_train.drop(['Cabin_text','Tickets_text'],axis = 1).T.to_dict().values())\nX_cat_submission = encoder.transform(cat_test.drop(['Cabin_text','Tickets_text'],axis = 1).T.to_dict().values())","b347ad5f":"(X_num_known_train, \n X_num_known_test, \n y_train_known, y_test_known) = train_test_split(numeric_train[['Fare','Tickets_number']], y_train,\n                                     stratify = y_train,\n                                     test_size = 0.2, \n                                     random_state=0)\n(X_cat_known_train,\n X_cat_known_test) = train_test_split(X_cat_train,\n                                   stratify = y_train,\n                                   test_size=0.2, \n                                   random_state=0)","a674c6e5":"scaler = StandardScaler()\nX_num_known_train = scaler.fit_transform(X_num_known_train)\nX_num_known_test = scaler.transform(X_num_known_test)","d109a8dc":"X_num_sumbission = scaler.transform(numeric_test[['Fare','Tickets_number']])","39c467e5":"# Putting numerical and categorial data together\ntrain_data_known = np.hstack((X_num_known_train,X_cat_known_train))\ntest_data_known = np.hstack((X_num_known_test,X_cat_known_test))\nsubmission_data = np.hstack((X_num_sumbission,X_cat_submission))","6a1ec553":"np.shape(train_data_known)","d1e2452f":"np.shape(test_data_known)","d122aa61":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics","c8fa5825":"models = {'RandomForestClassifier': RandomForestClassifier(class_weight = 'balanced'),\n          'LogisticRegression' : LogisticRegression(class_weight = 'balanced'),\n          'KNeighborsClassifier' : KNeighborsClassifier(),\n          'GradientBoostingClassifier': GradientBoostingClassifier(),\n          'MLPClassifier': MLPClassifier(max_iter = 1000)}","598a7d91":"results = []\nfor item in models.values():\n    item.fit(train_data_known,y_train_known)\n    results.append(metrics.accuracy_score(y_test_known,item.predict(test_data_known)))","636d60b3":"list(zip(models.keys(),results))","02b82e8e":"from sklearn.model_selection import GridSearchCV","a93c02bf":"parameters = {\n    \"learning_rate\": [0.05, 0.1],\n    \"max_depth\":[10,50],\n    \"subsample\":[0.5, 1.0],\n    \"n_estimators\":[100,1000],\n    \"max_features\":[0.2,0.5,None]\n    }\nclf = GridSearchCV(GradientBoostingClassifier(), parameters, cv = 3, n_jobs=-1)\nclf.fit(train_data_known,y_train_known)","b3083a42":"clf.best_score_","210f5970":"clf.best_params_","46b05914":"gbc = GradientBoostingClassifier(learning_rate = 0.1, max_depth = 10, max_features = 0.2, n_estimators = 100, subsample = 0.5)","ab962fef":"gbc.fit(train_data_known,y_train_known)","9ad27d7d":"y_predicted = gbc.predict(test_data_known)","fcd8e650":"def plot_roc_curve(fpr,tpr):\n    plt.plot(fpr, tpr)\n    plt.plot([0, 1], [0, 1], '--', color = 'grey', label = 'random')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.legend(loc = \"lower right\")\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n","d89d3a52":"print(metrics.classification_report(y_test_known, y_predicted))","963bb652":"print(\"Accuracy:\",metrics.accuracy_score(y_test_known, y_predicted))\nfpr, tpr, _ = metrics.roc_curve(y_test_known, gbc.predict_proba(test_data_known)[:,1])\nprint('ROC AUC: ',metrics.auc(fpr, tpr))\nplot_roc_curve(fpr,tpr)","806ea3d2":"print_confusion_matrix(metrics.confusion_matrix(y_test_known, y_predicted),[0,1])","79d5db96":"submission = pd.DataFrame(data = test_df.index,columns = ['PassengerId'])\nsubmission['Survived'] = gbc.predict(submission_data)\nsubmission.to_csv('first_submission.csv',index = False)","f4d788ca":"Let's see correlation with target for this data[](http:\/\/)","4b996904":"\n\n### Let's take a closer look\nIdeas: \n* delete text in Ticket feature, convert to int and use as a numeric feature\n* use deleted text as new categorial feature\n* Do the same for Cabin feature\n* Do not use name feature for now\n","14184905":"#### Scaling numerical features","09bf5738":"#### Submission","b9b44b63":"Based on data above, let's select features that seem important and use them for first learning try.  \n#### Numeric features  \n* Correlations and plots don't show anything straightforward, but let's try 'Fare' and 'Tickets_number'\n\n#### Categorial features\n* Let's try all of them except 'Cabin_text', biggest probability here goes to rows where we just don't have cabin info\n* 'Tickets_text' can be replaced with feature that describes wether ticket has any text info or not\n\n\n\n","e4006f8a":"#### One-hot encoding for categorial features","b5872c7f":"#### Checking numeric data from Ticket feature","dbe720a0":"#### Let's check correlations with target (probability to survive for categorial features)","27a6f122":" #### Let's split data with known target to train and test with stratification for better model selection","5b30e5cd":"#### The best is GradientBoostingClassifier, let's work with it","42b0ab1a":"### Let's look on our data","dc74deab":"### Separate numeric and categorial features and fill NANs","c5b9197a":"We see that correlation is quite small, but lets try this data as numeric feature using median to fill nans, because mean is not representative (see distribution above)","07e4dfb3":"#### Checking numeric data from Cabin feature","318ca502":"Let's remember that we have lack of cabin data, so for estimation of correlation let's try filling it with zeros","bfa9e71b":"#### Conclusion\nVery small work with features was done. But this is just a 'hello world' project.\n","95148a63":"#### Let's run threw some base algorythms to see what shows good results on our data","8752afb9":"Tunning parameters"}}