{"cell_type":{"27f730eb":"code","3bfe3ee6":"code","08ba1749":"code","d99e97d9":"code","d4a3855e":"code","6d2ce9f4":"code","e8c85766":"code","497d0c1f":"code","db20ebd1":"code","a4479d9e":"markdown","da8dacef":"markdown","44a3c863":"markdown","1b2038b9":"markdown","9b06e557":"markdown","356d798b":"markdown","27e0bd13":"markdown","74234546":"markdown","84ca925f":"markdown","429c32d6":"markdown","4e5bd9e1":"markdown","409c029b":"markdown","48ef80c7":"markdown","68053fe5":"markdown","0f6ff913":"markdown"},"source":{"27f730eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualisation purposes\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree # Our model and a handy tool for visualising trees\nfrom sklearn.model_selection import train_test_split # a tool for spliting data\nfrom sklearn.ensemble import RandomForestClassifier #second model\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3bfe3ee6":"# Create a new Pandas DataFrame with our training data\nspotifydata = pd.read_csv('..\/input\/top-spotify-songs-from-20102019-by-year\/top10s.csv')\n\nprint(spotifydata.columns, end='\\n\\n\\n')\n(spotifydata.describe(include='all')) \n","08ba1749":"# Let's reduce our data to only the features we need and the target.\n# The features we chose have similar 'count' values when we describe() them\n# We need to keep the target as part of our DataFrame for now.\nselected_columns = ['top genre', 'bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch', 'pop']\n\n# Create our new training set containing only the features we want\nprepared_data = spotifydata[selected_columns]\n\n# Don't need to drop anything because there are no empty values\n\n\nprepared_data.describe(include='all') #Note there are no empty values.","d99e97d9":"# Separate out the prediction target\ny = prepared_data['top genre']\n\n# Drop the target column (axis=1) from the original dataframe and use the rest as our feature data\nX = prepared_data.drop('top genre', axis=1)\n\n# Take a look at the data again\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0) # Split data using the\nprint(f'Number of training values: {len(train_y.values)}')                # train test split in sklearn\nprint(f'Number of testing values: {len(val_y.values)}')\nX.head()","d4a3855e":"def besttdepth(train_X, val_X, train_y, val_y):\n    '''This is a function to determine the best depth for a Decision Tree Classifier. \n       This is an alternative to the mean absolute error used for regressors          '''\n    maxnodes = {i:0 for i in range(1,100)}\n    for i in maxnodes:\n        genrepred = DecisionTreeClassifier(max_depth=i)\n        genrepred.fit(train_X, train_y)\n        for j in range(len(genrepred.predict(val_X))):\n            if genrepred.predict(val_X)[j] == val_y.values[j]:\n                maxnodes[i] += 1\n    best = 0\n    score = 0\n    for i in maxnodes:\n        if maxnodes[i] > score:\n            best = i\n            score = maxnodes[i]\n    print(maxnodes)\n    return best","6d2ce9f4":"# Create a decision tree classifier with best depth\nbestd = besttdepth(train_X, val_X, train_y, val_y)\nprint(bestd)\ngenrepred = DecisionTreeClassifier(max_depth=bestd) #replace bestd with 1 to get a faster running time\n\ngenrepred.fit(train_X, train_y)","e8c85766":"# Let's plot the tree to see what it looks like!\nplt.figure(figsize = (20,10))\nplot_tree(genrepred,\n          feature_names=X.columns,\n          class_names=['acoustic pop', 'alaska indie', 'alternative r&b', 'art pop', 'atl hip hop',\n                       'australian dance', 'australian hip hop', 'australian pop', 'barbadian pop',\n                       'baroque pop', 'belgian edm', 'big room', 'boy band', 'british soul',\n                       'brostep', 'canadian contemporary r&b', 'canadian hip hop', 'canadian latin',\n                       'canadian pop', 'candy pop', 'celtic rock', 'chicago rap', 'colombian pop',\n                       'complextro', 'contemporary country', 'dance pop', 'danish pop',\n                       'detroit hip hop', 'downtempo', 'edm', 'electro', 'electro house',\n                       'electronic trap', 'electropop', 'escape room', 'folk-pop',\n                       'french indie pop', 'hip hop', 'hip pop', 'hollywood', 'house', 'indie pop',\n                       'irish singer-songwriter', 'latin', 'metropopolis', 'moroccan pop',\n                       'neo mellow', 'permanent wave', 'pop', 'tropical house'],\n          filled=True)\nplt.show()\n","497d0c1f":"# just checking percentage of correct values\nbest = 0\n\nfor i in range(len(genrepred.predict(val_X))):\n    #print(genrepred.predict(val_X)[i], '\\t', val_y.values[i])\n    if genrepred.predict(val_X)[i] == val_y.values[i]:\n        best += 1\nprint(best\/len(val_y.values))","db20ebd1":"forestgenrepred = RandomForestClassifier(random_state=1)\n\n# Train the model on the one hot encoded data\nforestgenrepred.fit(train_X, train_y)\n\n\n\nbest = 0\nval_X.head()\n\nfor i in range(len(forestgenrepred.predict(val_X))):\n    #print(genrepred.predict(val_X)[i], '\\t', val_y.values[i])\n    if forestgenrepred.predict(val_X)[i] == val_y.values[i]:\n        best += 1\nprint(best)","a4479d9e":"# Evaluate model performance\nAs shown above, the reults are:\n\n- Decision Tree Classifier (tree depth of 1): 82\/151 (54.3% success rate)\n- Random Forest Classifier: 84\/151 (55.6% success rate)\n\n\nAs seen, the Random Forest Classifier has a higher success rate, but not by much.\n\nMy guess of why the results are not improved by much are because of best tree depth of 1. This means that the random forest doesn't have many options to chose from when selecting potential trees.","da8dacef":"# Choose and Train a Model\nNow that we have data our model can digest, let's use it to train a model and make some predictions. We're going to use a __Decision Tree Classifier__ and a __Random Forest Classifier__. These models make categorical predictions instead of continuous numerical predictions, which is perfect for predicting genres. \n\nOk, let's train our model and see what it looks like.\n\n","44a3c863":"# Introduction\nIn this project I will be looking at using a Decision Tree Classifier and a Random Forest Classifier to predict what genre of music a song is. \n\nThis Data science project is on the dataset [Top Spotify songs from 2010-2019 - BY YEAR](https:\/\/www.kaggle.com\/leonardopena\/top-spotify-songs-from-20102019-by-year). I will be predicting the genre of each song, using features such as bpm, volume, length, and more.\n\nI think the best model will be the Random Forest Classifier, as it creates muliple trees to use to predict the data. I think the bpm (beats per minute) will be the strongest feature effecting the results, as some genres are generally faster than others.\n\nI will be attempting to get an accuracy of above 80%.","1b2038b9":"# Prepare the data\nIn this project, I will be trying to predict the genre, so 'top genre' will be the prediction target.\n\nBefore we can separate our prediction target 'y' (top genre) from the rest of the data, we need to do some preparation so that there aren't any rows with missing values as our machine learning model will not be able to handle them. However, my chosen data does not contain any values, so that step is not necessary. \n\n\n## Select features and target then drop missing values\nChoosing our features first will help reduce the total number of rows we need to drop (remove).\n\nI have chosen a selection of features that are:\n- Relevant to our predictions\n- Don't have many missing values, so I don't need to drop any values\n\nNote that we'll also be including the target for now.\n","9b06e557":"The dictionary printed above shows the depth of the tree compared with the number of correct predictions. A depth of 1 has the best success rate, with 82\/150 (54.6%). Generally, the deeper the tree, the lower success rate. You may also notice that the genre **dance pop** has the same percentage of values as the success rate of a tree depth of 1 - 54%.\n\n*If you can't see the dictionary, run the above cell.*","356d798b":"Suprisingly, the Random Forest Classifier only increases the correct predictions by 2 (1.3%)","27e0bd13":"# Random Forest model","74234546":"## Split data into training and testing data.\nSplitting the training set into two subsets is important because you need to have data that your model hasn't seen yet with actual values to compare to your predictions to be able to tell how well it is performing. In this example project we're skipping this step, but when you do your project you'll need to consider how you want to split your data. The [Intro to Machine Learning course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) goes through how to do this. \n\n## Separate Features From Target\nNow that we have a set of data (as a Pandas DataFrame) without any missing values, let's separate the features we will use for training from the target, as well as spliting the training data and the testing data. Note that the ratio of testing:training values is approximately 1:3.\n\n\n","84ca925f":"# Gather and explore the data\nI have added the spotify by selecting the **Add data** button, then selecting the chosen data. \n\nI chose the dataset [Top Spotify songs](https:\/\/www.kaggle.com\/leonardopena\/top-spotify-songs-from-20102019-by-year) because music is something I am intested in, and I have never seen a genre predicter before.\n\nThis data is over 600 songs long, and contains multiple potential features to use. I have decided to choose top genre as my prediction target, as it is the only catergorical piece of data, and can potentialy relate to all other points of data.\n\nMy data:\n- Is in .csv format\n- Contains mostly numerical values\n- Contains categorical targets\n- Doesn't have many missing values\n\nThe data is shown under the _input_ folder icon as you see it in the top right of this notebook.\n\nNow that we have a file containing data, let's get it into a Pandas DataFrame and take a peek.\n\nThe values in this dataset are:\n\n- Title (not using)\n- Artist (not using)\n- Top Genre (target)\n- Year (not using)\n- BPM (beats per minute; The higher the value, the faster the song)\n- nrgy (energy; The higher the value, the more energtic the song)\n- dnce (Danceability; The higher the value, the easier it is to dance to this song)\n- dB (volume; The higher the value, the louder the song)\n- live (Liveness; The higher the value, the more likely the song is a live recording)\n- val (Valence; The higher the value, the more positive the mood is in the song)\n- dur (Duration; The higher the value, the longer the song)\n- acous (Acousticness; The higher the value, the more acoustic the song is)\n- spch (Speech; The higher the value, the more spoken words in this song)\n- pop (Popularity; The higher the value, the more popular the song iss)\n\n\n\n","429c32d6":"# Conclusion\nMy best model is the Random Forest Classifier, with a success rate of 55.6%. My other model, the Decision Tree Classifier is not much worse, with a success rate of 54.3%. I didn't succeed in geting my target rate of 80%, and, with a bit more time, or a different model, I might've been able to raise that percentage. The only feature effecting the Decision Tree Classifier was the popularity, rather than bpm, as I predicted. This is because there is a correlation between dance pop as a genre and a popularity. My Hypothesis was correct, the Random Forest Classifier is better, but the diference in success was low.","4e5bd9e1":"# Hyperparameters\n\nI have changed the majority of Hyperparameters as I am going. The biggest change is the tree depth, where I chose the optimal depth for the Decision Tree Classifier, of 1. The bigger the tree depth, the less accurate the results. The results lowered by roughly 3 for every extra layer in the tree. Ulimately, I think the tree is really classifing if the genre is dance pop or not. This is because the success rate is very close to the number of dance pop entries in the data.","409c029b":"# Setting up\nThe below code contains necessary steps for setting up our machine learning environment. Key features are described in the comments.","48ef80c7":"Note that this value is extremely close to the number of dance pop genre songs in the table.","68053fe5":"As shown in the visulisation, 'pop' (popularity) has the biggest effect on determining the genre. However ","0f6ff913":"The above function is my alternative to the MAE (mean absolute error) used to decide the tree depth for regressors. This function creates a tree for every value in *max nodes*, and works out the number of correct predictions. It then returns the best tree depth.\n\nThe reason a MAE value cannot be used in this situation is because the MAE relies on the output of the tree to be a continous value, a Decision Tree Regressor. "}}