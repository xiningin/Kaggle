{"cell_type":{"1cd24816":"code","c3395d22":"code","f87723e0":"code","d8ccdec5":"code","66bcbbdf":"code","5a657483":"code","2cd69046":"code","c9fd63e6":"code","cba8b1ab":"code","b10f2c8c":"code","3518ceca":"code","18dacc00":"code","f2a42580":"code","a2cec4be":"code","f0c10d51":"code","e312a6d9":"code","686f0f65":"code","ef733cfe":"code","26ff15e1":"code","4234e5b4":"code","742fb593":"code","39e28a6b":"code","31bbe62f":"code","e5c7ab77":"code","46c4ac66":"code","6b78c84e":"code","fa11dab1":"code","edf0a8c3":"code","77de1b84":"code","b7ebdbf5":"code","d2153abb":"code","ac2101dd":"code","0cb831cb":"code","90336745":"code","2e27d4e2":"markdown","ec5a682a":"markdown","42c913d0":"markdown","5749ef6c":"markdown","6b6fc786":"markdown","995d25b0":"markdown","c46d5196":"markdown","e0efae8e":"markdown","08c5b011":"markdown","e1c64a51":"markdown","a7411117":"markdown","e8a845c5":"markdown","0578f638":"markdown"},"source":{"1cd24816":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","c3395d22":"train_df=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","f87723e0":"train_df.describe()","d8ccdec5":"train_df.isna().sum()","66bcbbdf":"train_df[\"target\"].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%', figsize=(6,6))","5a657483":"import seaborn as sns\nsns.countplot(x='target',data=train_df)","2cd69046":"train_df[train_df.keyword!='NaN'].value_counts()","c9fd63e6":"train_df=train_df.drop([\"location\",\"keyword\",\"id\"], axis=1)\ntrain_df.head()","cba8b1ab":"train_df.isna().sum()","b10f2c8c":"train_df.info()","3518ceca":"test_df=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntest_df","18dacc00":"test_df.isna().sum()","f2a42580":"test_df=test_df.drop([\"location\",\"keyword\"], axis=1)\ntest_df.head()","a2cec4be":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize","f0c10d51":"train_df.text=train_df.text.apply(lambda x: x.lower())\ntrain_df","e312a6d9":"!pip install contractions\nimport contractions","686f0f65":"def con(data):\n  expand=contractions.fix(data)\n  return expand\n\ntrain_df.text=train_df.text.apply(con)\ntrain_df['text'][0]","ef733cfe":"import re\n\ndef remove_sp(data):\n  pattern=r'[^A-Za-z0-9\\s]'\n  data=re.sub(pattern,'',data)\n  return data\n\ntrain_df.text=train_df.text.apply(remove_sp)\ntrain_df.text[0]","26ff15e1":"#import string\n#punctuations=list(string.punctuation)\n#train_df.text=train_df.text.apply(lambda x : \" \".join(x for x in x.split() if x not in punctuations))","4234e5b4":"nltk.download('stopwords')\nstopword_list=stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\n\ntrain_df.text=train_df.text.apply(lambda x : \" \".join(x for x in x.split() if x not in stopword_list))\ntrain_df['text'][5]","742fb593":"nltk.download('punkt')\ntrain_df['text']=train_df.text.apply(word_tokenize)\ntrain_df['text'][0]\n","39e28a6b":"nltk.download('wordnet')\nlemmatizer=WordNetLemmatizer()\ntrain_df['text']=train_df.text.apply(lambda x:[lemmatizer.lemmatize(word) for word in x])\ntrain_df.text","31bbe62f":"train_df.text= train_df.text.astype(str)","e5c7ab77":"train_df.head()","46c4ac66":"X=train_df.text\nY=train_df.target\nX_test=test_df.text","6b78c84e":"#from sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.pipeline import Pipeline\n#from sklearn.svm import SVC","fa11dab1":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer()\nx_train_tfidf = tfidf.fit_transform(X)\n","edf0a8c3":"np.random.seed(42)\nfrom sklearn.svm import SVC\nsvc_clf=SVC()\nsvc_clf.fit(x_train_tfidf,Y)\nsvc_clf.score(x_train_tfidf,Y)","77de1b84":"np.random.seed(42)\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf=RandomForestClassifier()\nrf_clf.fit(x_train_tfidf,Y)\nrf_clf.score(x_train_tfidf,Y)","b7ebdbf5":"from mlxtend.classifier import StackingCVClassifier\nscv=StackingCVClassifier(classifiers=[svc_clf,rf_clf],meta_classifier= rf_clf)\nscv.fit(x_train_tfidf,Y)\nscv.score(x_train_tfidf,Y)","d2153abb":"np.random.seed(42)\nfrom sklearn import linear_model\nrd_clf = linear_model.RidgeClassifier()\nrd_clf.fit(x_train_tfidf,Y)\nrd_clf.score(x_train_tfidf,Y)","ac2101dd":"\nX_test=X_test.apply(lambda x: x.lower())\nX_test=X_test.apply(con)\nX_test=X_test.apply(remove_sp)\n#test_df.text=test_df.text.apply(lambda x : \" \".join(x for x in x.split() if x not in punctuations))\nX_test=X_test.apply(lambda x : \" \".join(x for x in x.split() if x not in stopword_list))\nX_test=X_test.apply(word_tokenize)\nX_test=X_test.apply(lambda x:[lemmatizer.lemmatize(word) for word in x])\nX_test= X_test.astype(str)\nx_test_tfidf = tfidf.transform(X_test)\nX_test\nx_test_tfidf","0cb831cb":"predictions=rd_clf.predict(x_test_tfidf)\npredictions","90336745":"output = pd.DataFrame({'Id': test_df.id, 'Target': predictions})\noutput.to_csv('my_submission.csv', index=False)","2e27d4e2":"# ** Creating the features and the target variables**","ec5a682a":"# **Applying TFIDF (Term Frequency Inverse Document Frequency) Vectorizer to convert categorical features into numbers**","42c913d0":"# **Expanding the contracted and abbreviated text data**","5749ef6c":"# **Removing the punctuations and special characters**","6b6fc786":"# **Lemmatization**","995d25b0":"# **Tokenization**","c46d5196":"# **Removing Stopwords**","e0efae8e":"# **Read the test dataset**","08c5b011":"# **Import modules to attach and read the dataset**","e1c64a51":"# **Applying Support Vector Machine Classifier**","a7411117":"# **Import the required NLTK modules**","e8a845c5":"# **Read the dataset using pandas**","0578f638":"# **Lower case the text data**"}}