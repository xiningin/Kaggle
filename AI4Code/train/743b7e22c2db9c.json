{"cell_type":{"f1e715dc":"code","454f7608":"code","41e17495":"code","db1d86ce":"code","5c296281":"code","6a98a7dc":"code","668e26db":"code","ab2a22de":"code","d4977773":"code","b6e0ec05":"code","2422b936":"code","5a195c93":"code","2d917936":"code","b01319bb":"code","a8ece44c":"code","c37ce904":"code","57682854":"code","e98a958f":"code","2485173d":"code","8bf3ea7c":"code","e13b2336":"code","81ceecbe":"code","072b16af":"code","79e08321":"code","9bd77a28":"code","37bf4310":"code","e24a9c55":"code","0acb3ce7":"code","668464f4":"code","28054ddf":"code","758c8f85":"code","08b41e24":"code","86616eb4":"code","fba54e42":"code","cf2aa86a":"code","bed2975a":"code","a7095320":"code","f9e471b7":"code","eaccb223":"code","75b19531":"code","4a476b86":"code","d8f71ac0":"code","971918a8":"code","d38870d2":"code","7c042902":"code","ebcd12fd":"markdown","b2626a6f":"markdown","51937c1c":"markdown","85ae169f":"markdown","2ab53e3b":"markdown","03166814":"markdown","96e70f89":"markdown","637b0136":"markdown","4eb24d8a":"markdown","3b80e9c5":"markdown","1ce5766e":"markdown","39fc8e66":"markdown","f9bc9b03":"markdown","fb6921da":"markdown","33346e2d":"markdown","b86a4dd8":"markdown","89de2808":"markdown","c4255acb":"markdown","c9510a33":"markdown","fc2ff239":"markdown"},"source":{"f1e715dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n!pip install dython\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"max_colwidth\", None)\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom dython.nominal import associations \n\n# Scikit Learn \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","454f7608":"with open('\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt', 'r') as text_file:\n    \n    text_features_dict = {\n        'Id':[np.nan], \n        'SalePrice':[np.nan]\n    }\n    \n    for line in text_file:\n        if ':' in line and line[0] != ' ':\n            text_column = line[:line.find(\":\")]\n            if (text_column == 'Bedroom' or text_column == 'Kitchen'):\n                text_column += 'AbvGr'\n            text_features_dict[text_column] = [np.nan]\n        elif line.strip() != '':\n            text_value = line[:line.find(\"\\t\")].strip()\n            text_features_dict[text_column].append(text_value)\n            if np.nan in text_features_dict[text_column]:\n                text_features_dict[text_column].remove(np.nan) \n\ndel text_file, line, text_column, text_value\ntext_features_dict","41e17495":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntrain.head()","db1d86ce":"numerical_features_lst = []\ncategorical_features_lst  = []\nspecial_cases_features_lst  = []\n\nfor key, value in text_features_dict.items():\n    if np.nan in value:\n        numerical_features_lst.append(key)\n    elif 'NA' in value:\n        special_cases_features_lst.append(key)\n    else:\n        categorical_features_lst.append(key)\n\ndel key, value\nprint(\"Features lists lengths\\nNumerical: {}\\nCategorical:{}\\nSpecial Cases Categorical:{}\".\n      format(len(numerical_features_lst),\n             len(categorical_features_lst),\n             len(special_cases_features_lst)))","5c296281":"def missing_values(train=train, test=test):\n    train_null = train.isnull().sum()\n    test_null = test.isnull().sum()\n    return pd.DataFrame({\n        'Train': train_null, \n        'Test': test_null,\n        '% Train': round(train_null\/len(train), 2),\n        '% Test': round(test_null\/len(test), 2)\n    })","6a98a7dc":"missing_values(train[special_cases_features_lst], \n               test[special_cases_features_lst]).query(\"Train > 0 or Test > 0\")","668e26db":" # Replace categorical special cases missing values with NA \nfor column in special_cases_features_lst:\n    print(f'\\n******* {column} *******')\n    print('--- TRAIN ---')\n    print('Before:', train[column].unique(), end='\\n')\n    train[column] = train[column].fillna('NA')\n    print('After:', train[column].unique(), end='\\n\\n')\n    print('--- TEST ---')\n    print('Before:', test[column].unique(), end='\\n')\n    test[column] = test[column].fillna('NA')\n    print('After:', test[column].unique(), end='\\n\\n')\n\ndel column","ab2a22de":"missing_values(train[special_cases_features_lst],\n               test[special_cases_features_lst]).query(\"Train > 0 or Test > 0\")","d4977773":"missing_values(train[categorical_features_lst], \n               test[categorical_features_lst]).query(\"Train > 0 or Test > 0\")","b6e0ec05":"print('******* TRAIN *******')\nfor column in categorical_features_lst:\n    if np.nan in list(train[column].unique()):\n        print(f'\\n{column}:')\n        print('Before:', train[column].unique(), end='\\n')\n        train[column] = train[column].fillna(train[column].mode()[0])\n        print('After:', train[column].unique(), end='\\n')\n\nprint('\\n******* TEST *******')\nfor column in categorical_features_lst:\n    if np.nan in list(test[column].unique()):\n        print(f'\\n{column}:')\n        print('Before:', test[column].unique(), end='\\n')\n        test[column] = test[column].fillna(test[column].mode()[0])\n        print('After:', test[column].unique(), end='\\n')\n        \ndel column","2422b936":"missing_values(train[categorical_features_lst], \n               test[categorical_features_lst]).query(\"Train > 0 or Test > 0\")","5a195c93":"missing_values(train[numerical_features_lst[2:]],\n               test[numerical_features_lst[2:]]).query(\"Train > 0 or Test > 0\")","2d917936":"train_lotfront_age_mean = float(math.floor(train['LotFrontage'].mean()))\ntest_lotfront_age_mean = float(math.floor(test['LotFrontage'].mean()))\n\ntrain['LotFrontage'] = train['LotFrontage'].fillna(train_lotfront_age_mean)\ntest['LotFrontage'] = test['LotFrontage'].fillna(test_lotfront_age_mean)\n\ndel train_lotfront_age_mean, test_lotfront_age_mean","b01319bb":"train['MasVnrArea'] = train['MasVnrArea'].fillna(0.0)\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0.0)","a8ece44c":"time_columns = []\nfor column in list(train.columns):\n    if \"year\" in column.lower() or \"yr\" in column.lower():\n        time_columns.append(column)\n        \nprint(f\"Missing data in Year columns:\\n{train[time_columns].isnull().sum()}\")","c37ce904":"train[time_columns][pd.isna(train[\"GarageYrBlt\"])]\ngarage_yearbuilt_train = train['YearBuilt'][pd.isna(train[\"GarageYrBlt\"])].values\nnull_indeces_train = train.loc[:, 'GarageYrBlt'][pd.isna(train[\"GarageYrBlt\"])].index\ngarage_dict_train = dict(zip(null_indeces_train, garage_yearbuilt_train))\ngarage_dict_train","57682854":"for i in train['GarageYrBlt'].index:\n    if pd.isna(train.loc[i, 'GarageYrBlt']):\n        train.loc[i, 'GarageYrBlt'] = garage_dict_train[i]\n        print(f\"Index {i} replaced with {train.loc[i, 'GarageYrBlt']}\")","e98a958f":"test[time_columns][pd.isna(test[\"GarageYrBlt\"])]\ngarage_yearbuilt_test = test['YearBuilt'][pd.isna(test[\"GarageYrBlt\"])].values\nnull_indeces_test = test.loc[:, 'GarageYrBlt'][pd.isna(test[\"GarageYrBlt\"])].index\ngarage_dict_test = dict(zip(null_indeces_test, garage_yearbuilt_test))\nfor i in test['GarageYrBlt'].index:\n    if pd.isna(test.loc[i, 'GarageYrBlt']):\n        test.loc[i, 'GarageYrBlt'] = garage_dict_test[i]\n        print(f\"Index {i} replaced with {test.loc[i, 'GarageYrBlt']}\")","2485173d":"missing_numerical_columns_test = missing_values(\n    train[numerical_features_lst[2:]], \n    test[numerical_features_lst[2:]]).query(\"Train > 0 or Test > 0\")['Test'].index\n\nfor column in missing_numerical_columns_test:\n    test[column] = test[column].fillna(0.0)\n    \ndel missing_numerical_columns_test, column","8bf3ea7c":"missing_values(train[numerical_features_lst[2:]],\n               test[numerical_features_lst[2:]]).query(\"Train > 0 or Test > 0\")","e13b2336":"print('Missing values in train set: ', train.isnull().sum().any())\nprint('Missing values in test set: ', test.isnull().sum().any())","81ceecbe":"list(train[categorical_features_lst].select_dtypes(include=\"number\").columns)","072b16af":"for column in list(train[categorical_features_lst].select_dtypes(include=\"number\").columns):\n    train[column] = train[column].astype(object)\n    test[column] = test[column].astype(object)\n    print(f\"Train {column} --> {train[column].dtype}\")\n    print(f\"Test {column} --> {test[column].dtype}\")","79e08321":"def check_data_types(display=False):\n    different_type_columns = []\n    for column in list(set(train.columns) & set(test.columns)):\n        train_type = train[column].dtype\n        test_type = test[column].dtype\n        if train_type != test_type:\n            if display:\n                print(f'\\n*** {column} ***\\nTrain: {train_type}\\nTest: {test_type}')\n            different_type_columns.append(column)\n\n    return different_type_columns","9bd77a28":"check_data_types(display=True)","37bf4310":"for column in check_data_types():\n    test[column] = test[column].astype('int64')","e24a9c55":"corr_features_df = associations(train, compute_only=True)[\"corr\"]\nabs(corr_features_df).tail(1).T.query('SalePrice > 0.5')[:-1]","0acb3ce7":"important_features_lst = list(abs(corr_features_df).tail(1).T.query('SalePrice > 0.5')[:-1].index)","668464f4":"train_features = pd.get_dummies(train[important_features_lst], drop_first=True)\ntest_features = pd.get_dummies(test[important_features_lst], drop_first=True)\n\ntrain_features.head()","28054ddf":"train_labels_log = np.log(train['SalePrice'])\nsubmission_labels_log = np.log(sample_submission.SalePrice)\n\nprint(f\"Train:\\nFeatures --> {train_features.shape}\\nLabels --> {train_labels_log.shape}\", end='\\n'*2)\nprint(f\"Validation:\\nFeatures --> {test_features.shape}\\nLabels --> {submission_labels_log.shape}\")","758c8f85":"x_train, x_validation, y_train, y_validation = train_test_split(train_features,\n                                                                train_labels_log, \n                                                                train_size = 0.8, \n                                                                random_state = 3)\n\nprint(f\"Train:\\nFeatures --> {x_train.shape}\\nLabels --> {y_train.shape}\", end='\\n'*2)\nprint(f\"Validation:\\nFeatures --> {x_validation.shape}\\nLabels --> {y_validation.shape}\")","08b41e24":"columns_to_scale_lst = list(train[important_features_lst].select_dtypes(include='number'))\ncolumns_to_scale_lst","86616eb4":"scaler = StandardScaler()\n\nx_train.iloc[:, :len(columns_to_scale_lst)] = scaler.fit_transform(x_train.iloc[:, :len(columns_to_scale_lst)])\nx_validation.iloc[:, :len(columns_to_scale_lst)]= scaler.transform(x_validation.iloc[:, :len(columns_to_scale_lst)])\nx_train.head()","fba54e42":"models = dict()\n\nmodels[\"Ridge\"] = {\"param_grid\": {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, \n                                             0.05, 0.06, 0.07, 0.08, 1, 2, \n                                             3, 5, 8, 10, 20, 50, 100, 1000]}}\n                  \n\nmodels[\"Lasso\"] = {\"param_grid\" : {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, \n                                             0.05, 0.06, 0.07, 0.08, 1, 2, \n                                             3, 5, 8, 10, 20, 50, 100, 1000]}}\n\nmodels[\"Elastic Net\"] = {\"param_grid\" : {'alpha': [0.001, 0.01, 0.02, 0.03, 0.04, \n                                             0.05, 0.06, 0.07, 0.08, 1, 2, \n                                             3, 5, 8, 10, 20, 50, 100, 1000],\n                                         'l1_ratio': np.arange(0.0, 1.0, 0.1)}}\n\nmodels[\"Support Vector Regression\"] = {\"param_grid\" :{'C': [1, 10], \n                                                     'epsilon': [0.01, 0.1],\n                                                     'gamma': ['auto'],\n                                                     'kernel': ['rbf']}}\n\nmodels[\"Random Forest Regressor\"] = {\"param_grid\" : {'n_estimators': list(range(100, 200, 10)),\n                                                     'max_depth': list(range(4, 7)),\n                                                     'min_samples_split': list(range(2, 4))}}\n\nmodels[\"XGB Regressor\"] = {\"param_grid\" : {'n_estimators': list(range(500, 1000, 100)),\n                                           'learning_rate': [0.001, 0.01, 0.1]}}","cf2aa86a":"ridge = Ridge()\ncurrent_model = \"Ridge\"\n\ngrid_search_ridge = GridSearchCV(ridge, models[current_model][\"param_grid\"], cv = 5)\ngrid_search_ridge.fit(x_train, y_train)\nmodels[current_model][\"Predictions\"] = np.exp(grid_search_ridge.predict(x_validation))\nmodels[current_model][\"RMSE\"] = mean_squared_error(y_validation,\n                                                   np.log(models[current_model][\"Predictions\"]),\n                                                   squared=False)\nmodels[current_model][\"Score\"] = grid_search_ridge.score(x_validation, y_validation)\nmodels[current_model][\"Best parameters\"] = grid_search_ridge.best_params_\n\nprint(f\"Model: {current_model}\")\nprint(f'Score: {round(models[current_model][\"Score\"] * 100, 2)}%')\nprint(f'RMSE: {round(models[current_model][\"RMSE\"], 3)}')\nprint(f\"Best parameters:\\n {models[current_model]['Best parameters']}\")\n\ndel current_model","bed2975a":"lasso = Lasso()\ncurrent_model = \"Lasso\"\n\ngrid_search_lasso = GridSearchCV(lasso, models[current_model][\"param_grid\"], cv = 5)\ngrid_search_lasso.fit(x_train, y_train)\nmodels[current_model][\"Predictions\"] = np.exp(grid_search_lasso.predict(x_validation))\nmodels[current_model][\"RMSE\"] = mean_squared_error(y_validation,\n                                                   np.log(models[current_model][\"Predictions\"]),\n                                                   squared=False)\nmodels[current_model][\"Score\"] = grid_search_lasso.score(x_validation, y_validation)\nmodels[current_model][\"Best parameters\"] = grid_search_lasso.best_params_\n\nprint(f\"Model: {current_model}\")\nprint(f'Score: {round(models[current_model][\"Score\"] * 100, 2)}%')\nprint(f'RMSE: {round(models[current_model][\"RMSE\"], 3)}')\nprint(f\"Best parameters:\\n {models[current_model]['Best parameters']}\")\n\ndel current_model","a7095320":"elastic_net = ElasticNet()\ncurrent_model = \"Elastic Net\"\ngrid_search_elastic_net = GridSearchCV(elastic_net, models[current_model][\"param_grid\"], cv = 5)\ngrid_search_elastic_net.fit(x_train, y_train)\nmodels[current_model][\"Predictions\"] = np.exp(grid_search_elastic_net.predict(x_validation))\nmodels[current_model][\"RMSE\"] = mean_squared_error(y_validation,\n                                                   np.log(models[current_model][\"Predictions\"]),\n                                                   squared=False)\nmodels[current_model][\"Score\"] = grid_search_elastic_net.score(x_validation, y_validation)\nmodels[current_model][\"Best parameters\"] = grid_search_elastic_net.best_params_\n\nprint(f\"Model: {current_model}\")\nprint(f'Score: {round(models[current_model][\"Score\"] * 100, 2)}%')\nprint(f'RMSE: {round(models[current_model][\"RMSE\"], 3)}')\nprint(f\"Best parameters:\\n {models[current_model]['Best parameters']}\")\n\ndel current_model","f9e471b7":"svr = SVR()\ncurrent_model = \"Support Vector Regression\"\n\ngrid_search_svr = GridSearchCV(svr, models[current_model][\"param_grid\"], cv = 5)\ngrid_search_svr.fit(x_train, y_train)\nmodels[current_model][\"Predictions\"] = np.exp(grid_search_svr.predict(x_validation))\nmodels[current_model][\"RMSE\"] = mean_squared_error(y_validation,\n                                                   np.log(models[current_model][\"Predictions\"]),\n                                                   squared=False)\nmodels[current_model][\"Score\"] = grid_search_svr.score(x_validation, y_validation)\nmodels[current_model][\"Best parameters\"] = grid_search_svr.best_params_\n\nprint(f\"Model: {current_model}\")\nprint(f'Score: {round(models[current_model][\"Score\"] * 100, 2)}%')\nprint(f'RMSE: {round(models[current_model][\"RMSE\"], 3)}')\nprint(f\"Best parameters:\\n {models[current_model]['Best parameters']}\")\n\ndel current_model","eaccb223":"random_forest_reg = RandomForestRegressor()\ncurrent_model = \"Random Forest Regressor\"\n\ngrid_search_rfr = GridSearchCV(random_forest_reg, models[current_model][\"param_grid\"], cv = 5)\ngrid_search_rfr.fit(x_train, y_train)\nmodels[current_model][\"Predictions\"] = np.exp(grid_search_rfr.predict(x_validation))\nmodels[current_model][\"RMSE\"] = mean_squared_error(y_validation,\n                                                   np.log(models[current_model][\"Predictions\"]),\n                                                   squared=False)\nmodels[current_model][\"Score\"] = grid_search_rfr.score(x_validation, y_validation)\nmodels[current_model][\"Best parameters\"] = grid_search_rfr.best_params_\n\nprint(f\"Model: {current_model}\")\nprint(f'Score: {round(models[current_model][\"Score\"] * 100, 2)}%')\nprint(f'RMSE: {round(models[current_model][\"RMSE\"], 3)}')\nprint(f\"Best parameters:\\n {models[current_model]['Best parameters']}\")\n\ndel current_model","75b19531":"xgb_reg = XGBRegressor()\ncurrent_model = \"XGB Regressor\"\n\ngrid_search_xgb = GridSearchCV(xgb_reg, models[current_model][\"param_grid\"], cv = 5)\ngrid_search_xgb.fit(x_train, y_train)\nmodels[current_model][\"Predictions\"] = np.exp(grid_search_xgb.predict(x_validation))\nmodels[current_model][\"RMSE\"] = mean_squared_error(y_validation,\n                                                   np.log(models[current_model][\"Predictions\"]),\n                                                   squared=False)\nmodels[current_model][\"Score\"] = grid_search_xgb.score(x_validation, y_validation)\nmodels[current_model][\"Best parameters\"] = grid_search_xgb.best_params_\n\nprint(f\"Model: {current_model}\")\nprint(f'Score: {round(models[current_model][\"Score\"] * 100, 2)}%')\nprint(f'RMSE: {round(models[current_model][\"RMSE\"], 3)}')\nprint(f\"Best parameters:\\n {models[current_model]['Best parameters']}\")\n\ndel current_model","4a476b86":"models_performace = pd.DataFrame({\"Score\" : [round(models[model][\"Score\"] * 100, 2) for model in models.keys()],\n                           \"RMSE\" : [models[model][\"RMSE\"] for model in models.keys()],\n                           \"Best parameters\" : [models[model][\"Best parameters\"] for model in models.keys()]},\n                          index=models.keys())\nmodels_performace","d8f71ac0":"models_predictions = pd.DataFrame([{model_name : model_values[\"Predictions\"]}[model_name].round(3)\n                            for model_name, model_values in models.items()]).T\nmodels_predictions.columns = list(models.keys())\nmodels_predictions","971918a8":"test_features_to_predict = test_features.copy()\ntest_features_to_predict.iloc[:, :len(columns_to_scale_lst)] = scaler.transform(test_features_to_predict.iloc[:, :len(columns_to_scale_lst)])\ntest_features_to_predict.head()","d38870d2":"best_model_predictions = np.exp(grid_search_svr.predict(test_features_to_predict))\npd.DataFrame({\"Predictions\":best_model_predictions}, index=sample_submission.Id).head(10)","7c042902":"output = pd.DataFrame({'Id': sample_submission.Id,\n                       'SalePrice': best_model_predictions})\noutput.to_csv('submission.csv', index=False)","ebcd12fd":"### Support Vector Regression","b2626a6f":"### XGB Regressor","51937c1c":"### Predictions on test dataset","85ae169f":"### Lasso","2ab53e3b":"## Feature Scaling","03166814":"## Building Machine Learning Models\n\n### Grid Search parameters","96e70f89":"## Creating feature lists","637b0136":"## Checking the data types","4eb24d8a":"### Submission","3b80e9c5":"### Ridge","1ce5766e":"## Feature Selection","39fc8e66":"## Loading the datasets","f9bc9b03":"## Evaluation","fb6921da":"### Elastic Net","33346e2d":"### Random Forest Regressor","b86a4dd8":"## Dataset Splitting","89de2808":"## Text Parser","c4255acb":"### Numerical features","c9510a33":"## Handling missing values\n\n### Categorical features\n\nReplacing NA value in categorical features according to data description text file.","fc2ff239":"## Predictions"}}