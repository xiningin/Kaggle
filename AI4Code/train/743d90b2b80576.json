{"cell_type":{"505757c3":"code","6c2d2143":"code","fcdd2da4":"code","3b3b3616":"code","4c1b050f":"code","cf89204c":"code","ba6ed034":"code","480592b0":"code","47e201c2":"code","00b81857":"code","0d2816fb":"code","4c972a67":"code","af6b6b34":"code","dc023afc":"code","b79de118":"code","b35395d9":"code","e35447d3":"code","c5e94f75":"code","313b5946":"code","86d8f69c":"code","e3df4d3d":"code","4d920236":"code","65ec8b67":"code","da4411af":"code","5ac6c373":"code","1f245e32":"markdown","d3454fef":"markdown","40368e6f":"markdown","a4653c6c":"markdown","55f5fc74":"markdown","89530ad0":"markdown","bdc6fc87":"markdown"},"source":{"505757c3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","6c2d2143":"data = pd.read_csv('..\/input\/avocado-prices\/avocado.csv')","fcdd2da4":"data","3b3b3616":"data.drop(data.columns[0], axis=1, inplace=True)","4c1b050f":"data.info()","cf89204c":"plt.figure(figsize=(20, 10))\n\nfor i in range(len(data.columns)):\n    if data.dtypes[i] != 'object':\n        plt.subplot(3, 5, i + 1)\n        plt.boxplot(data[data.columns[i]], vert=False)\n        plt.title(data.columns[i])\n        \nplt.show()","ba6ed034":"data.isna().sum()","480592b0":"def get_uniques(df, columns):\n    return {column: list(df[column].unique()) for column in columns}","47e201c2":"categorical_columns = ['region', 'Date', 'type']\n\nget_uniques(data, categorical_columns)","00b81857":"ordinal_features = ['Date']\n\nnominal_features = ['region']\n\ntarget_column = 'type'","0d2816fb":"date_ordering = sorted(data['Date'].unique())","4c972a67":"def ordinal_encode(df, column, ordering):\n    df = df.copy()\n    df[column] = df[column].apply(lambda x: ordering.index(x))\n    return df\n\n\ndef onehot_encode(df, column):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column])\n    df = pd.concat([df, dummies], axis=1)\n    df.drop(column, axis=1, inplace=True)\n    return df","af6b6b34":"data = ordinal_encode(data, 'Date', date_ordering)","dc023afc":"data = onehot_encode(data, 'region')","b79de118":"data","b35395d9":"label_encoder = LabelEncoder()\n\ndata[target_column] = label_encoder.fit_transform(data[target_column])","e35447d3":"y = data[target_column]\nX = data.drop(target_column, axis=1)","c5e94f75":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","313b5946":"y.shape","86d8f69c":"X.shape","e3df4d3d":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)","4d920236":"inputs = tf.keras.Input(65,)\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['accuracy']\n)\n\nbatch_size = 64\nepochs = 73\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[tf.keras.callbacks.ReduceLROnPlateau()],\n    verbose=0\n)","65ec8b67":"plt.figure(figsize=(14, 10))\n\nepochs_range = range(1, epochs + 1)\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(epochs_range, train_loss, label=\"Training Loss\")\nplt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n\nplt.title(\"Training and Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.show()","da4411af":"np.argmin(val_loss) + 1","5ac6c373":"model.evaluate(X_test, y_test)","1f245e32":"# Visualization","d3454fef":"## Encoding","40368e6f":"# Preprocessing","a4653c6c":"## Splitting and Scaling","55f5fc74":"# Results","89530ad0":"# Training","bdc6fc87":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/CAKPtccHETk"}}