{"cell_type":{"fb117e2f":"code","4541896c":"code","b06f370a":"code","e4fa40a4":"code","9d076ca1":"code","59ea6bf6":"code","1201b2c3":"code","6abbec70":"code","df598276":"code","e9097464":"code","b806f379":"code","5fcae20c":"code","cb1e4d73":"code","a4942385":"code","8c254af3":"code","454e8ddb":"code","7da87c84":"code","58b40e96":"code","e7e1cb4f":"code","23a1009a":"code","428960c7":"code","dfe4a393":"code","7de8cbfe":"code","60d3bcb0":"code","22a1b377":"code","6a942fa0":"code","7ea5a628":"markdown","1d4c9288":"markdown","2c41d846":"markdown","2e6d888e":"markdown","79a40c2f":"markdown","a192b696":"markdown","92372bfb":"markdown","ba986a9a":"markdown","641ec953":"markdown","b787cc64":"markdown","51d3fb6d":"markdown","751e89be":"markdown","85b784e6":"markdown","9e45b281":"markdown"},"source":{"fb117e2f":"# Define base Folder\nimport os\ncwd = os.getcwd()\nprint(f'Current Directory: {cwd}')","4541896c":"# Regular Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.patches as patches\nfrom tabulate import tabulate\nfrom sklearn.preprocessing import OneHotEncoder\nimport missingno as msno \nimport warnings\nfrom joblib import dump, load\nwarnings.filterwarnings(\"ignore\")\n\n# Set Color Palettes for the notebook\ncustom_colors = ['#74a09e','#86c1b2','#98e2c6','#f3c969','#f2a553', '#d96548', '#c14953']\nsns.palplot(sns.color_palette(custom_colors))\n\n# Set Style\nsns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\"})\nsns.despine(left=True, bottom=True)\nmpl.rcParams['figure.dpi'] = 250\nmpl.rc('axes', labelsize=10)\nplt.rc('xtick',labelsize=10)\nplt.rc('ytick',labelsize=10)\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","b06f370a":"def evaluate_model(model, features, target):\n\n  test_p = np.argmax(model.predict(features),axis =1)\n  error = 0\n  confusion_matrix = np.zeros([5,5])\n\n  for i in range(features.shape[0]):\n    confusion_matrix[target[i],test_p[i]] += 1\n    if target[i]!=test_p[i]:\n        error +=1\n  print(\"Confusion Matrix: \\n\\n\",np.round(confusion_matrix,3))\n  print(\"\\nErrors in Dataset: \",error)\n  print(\"\\nError Percentage : \",(error*100)\/features.shape[0])\n  print(\"\\nAccuracy : \",100-(error*100)\/features.shape[0])\n  print(\"\\nVDataset Shape :\",features.shape[0])\n\n  return confusion_matrix\n\n\ndef plot_confusion_matrix(confusion_matrix, classes, title = 'Confusion Matrix'):\n  f = plt.figure(figsize=(10,8.5))\n  f.add_subplot(111)\n\n  plt.imshow(np.log2(confusion_matrix+1),cmap=\"Reds\")\n  plt.colorbar()\n  plt.tick_params(size=5,color=\"white\")\n  plt.xticks(np.arange(0,5),np.arange(0,5))\n  plt.yticks(np.arange(0,5),np.arange(0,5))\n\n  threshold = confusion_matrix.max()\/2 \n\n  for i in range(5):\n      for j in range(5):\n          plt.text(j,i,int(confusion_matrix[i,j]),horizontalalignment=\"center\",color=\"white\" if confusion_matrix[i, j] > threshold else \"black\")\n          \n  plt.xlabel(\"Predicted\")\n  plt.ylabel(\"Actual\")\n  plt.title(title)\n  plt.xticks(range(0,len(classes)), classes)\n  plt.yticks(range(0,len(classes)), classes)\n  #plt.savefig(\"Confusion_matrix1.png\")\n  plt.grid(linestyle='')\n  plt.show()\n\ndef fruit_frequency(target_feature,classes, title = \"Dataset Frequency\"):\n  unique, counts = np.unique(target_feature, return_counts=True)\n  df_freq = pd.DataFrame(index=unique,data=counts,columns=['Frequency'])\n  df_freq_per = df_freq\/len(target_feature) \n  # counts\n  fig, ax = plt.subplots(1,2,figsize=(12,6))\n  df_freq.plot(kind='bar', ax=ax[0])\n\n  ax[0].tick_params(labelsize = 13)\n  ax[0].set_xlabel(\"Fruit\",fontsize=13)\n  ax[0].set_ylabel(\"Num. Obs.\",fontsize=13)\n  ax[0].legend(loc='lower left')\n  ax[0].set_title(title,fontsize=15)\n\n\n\n  #plt.bar(counts.index,counts.values,width = 0.8,color=\"orange\")\n  df_freq_per.plot(kind='bar', ax=ax[1], color=\"orange\")\n\n  ax[1].tick_params(labelsize = 13)\n  ax[1].set_xlabel(\"Digits\",fontsize=12)\n  ax[1].set_ylabel(\"Frequency\",fontsize=13)\n  ax[1].set_title(title,fontsize=15)\n  ax[1].legend(loc='lower right')\n  #plt.savefig('digit_frequency_train.png')  \n  plt.setp(ax, xticks=df_freq.index, xticklabels=classes)\n\n  plt.tight_layout()\n  plt.show()\n","e4fa40a4":"df = pd.read_csv('..\/input\/eyantrafruitsdataset\/dataset_attr.csv')","9d076ca1":"df.shape","59ea6bf6":"df.head()","1201b2c3":"target = df.iloc[:,-1]\nfeatures = df.iloc[:,:-1] ","6abbec70":"# Create a Dictionary connecting the label with the fruit name:\nclasses = ['Apple', 'Banana', 'Orange', 'Strawberry', 'Pineapple']\nlabels = list(target.unique())\n\nlabel_dict = dict(zip(labels,classes))\n\n# Set random number:\nrn = 17031978\n\n# Image size\nimage_size = 100\n\n# Train-Test split:\ntrain_split = 0.2","df598276":"features = features\/255","e9097464":"# Extract target Labels:\ny = np.asarray(target) #.reshape([len(target),1]\nprint(f'Shape of the dataset: {y.shape}')\n\nX = np.asarray(features).reshape([-1,image_size,image_size,1])\nprint(f'Shape of the dataset: {X.shape}')","b806f379":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, index_train, index_test = train_test_split(X, y, \n                                                                             df.index,  \n                                                                             test_size = train_split,\n                                                                             random_state = rn,\n                                                                             stratify = y)\n\nX_train, X_valid, y_train, y_valid, index_train, index_valid = train_test_split(X_train, y_train, \n                                                                                index_train,  \n                                                                                test_size = train_split,\n                                                                                random_state = rn,\n                                                                                stratify = y_train)","5fcae20c":"sample = 100\nsns.set_style(\"whitegrid\",{\"grid.linestyle\":\"\"})\n\nplt.imshow(X_train[sample].reshape([image_size,image_size]), cmap='Greys')\nplt.title(f'True Label: {label_dict.get(y_train[sample])}') #[0]\nplt.tight_layout()\nplt.show()\n\nsns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\"})","cb1e4d73":"fruit_frequency(y_train,classes, title='Training Dataset Frequency')","a4942385":"fruit_frequency(y_test,classes, title='Test Dataset Frequency')","8c254af3":"fruit_frequency(y_valid,classes, title='Validation Dataset Frequency')","454e8ddb":"rows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows)) # defining a figure \n\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) # adding sub plot to figure on each iteration\n    plt.imshow(X_train[i].reshape([image_size,image_size]),cmap=\"RdGy\") \n    plt.axis(\"off\")\n    plt.title(str(label_dict.get(y_train[i])), y=-0.16,color=\"green\") #[0]","7da87c84":"from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D\nfrom tensorflow.keras import models\nfrom tensorflow.keras.optimizers import Adam,RMSprop, Nadam, SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n\nimport tensorflow as tf\nprint(tf.__version__)","58b40e96":"model = models.Sequential()","e7e1cb4f":"tf.random.set_seed = rn\n\nmodel = models.Sequential() \nmodel.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu', input_shape=(100, 100, 1)))\nmodel.add(MaxPool2D(strides=2))\nmodel.add(Conv2D(filters=64, kernel_size=(5,5), padding='same', activation='relu'))\nmodel.add(MaxPool2D(strides=2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(5, activation='softmax'))","23a1009a":"initial_lr = 0.01\nloss = \"sparse_categorical_crossentropy\"\nmodel.compile(optimizer='nadam', loss=loss ,metrics=['accuracy'])\nmodel.summary()","428960c7":"epochs = 500\nbatch_size = 16\n\nearly_cb = EarlyStopping(monitor='val_loss',patience=50)\nrestore_cb = ModelCheckpoint('cnn_3.h5', save_best_only=True, monitor='val_loss')\n#lrr = ReduceLROnPlateau(monitor='val_loss',patience=15,verbose=1,factor=0.5, min_lr=0.00001)\n\nhistory = model.fit(X_train,y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_data=(X_valid,y_valid),\n                    verbose=1,\n                    callbacks=[early_cb,restore_cb]) ","dfe4a393":"model_1 = models.load_model('cnn_3.h5')\n# best model so far\n\n# model_best = models.load_model('cnn_2.h5')","7de8cbfe":"confusion_matrix = evaluate_model(model_1,X_valid,y_valid)","60d3bcb0":"plot_confusion_matrix(confusion_matrix,classes, title='Validation-Set Confusion Matrix')","22a1b377":"confusion_matrix = evaluate_model(model_1,X_test,y_test)","6a942fa0":"plot_confusion_matrix(confusion_matrix,classes, title='Test-Set Confusion Matrix')","7ea5a628":"## **THE MODEL**\n\n### Model Using Keras\nThere are two different ways of defining the Model in Keras:\n\n- Sequential Model\n- Function API\n\nFunctional API is used to build a more complicated Model such as for multi-output Models, directed acyclic graphs, or models with shared layers. I am using the Sequential Model in this notebook to keep things simple.\nIn Sequential Model, you can add each layer sequentially.\n\n#### *Description of Model:*\n\n* 2 Convolutional Blocks\nEach block consists of 2 Conv2D layers with LeakyRelU activation layers. Then a MaxPool2D layer and finally a Dropout Layer.\n\n* Then Dense Layers and Output layer after Flatten layer.\n* MaxPool2D layer is used to reduce the size of the image. Pool size (2,2) means reducing the image from (28,28) to (14,14). Reducing the features.\n* Dropout layer drops the few activation nodes while training, which acts as regularization. Do let the model to over-fit.\n* Output layer has 10 nodes with sigmoid activation.\n\nCheck out these functions for more info:\n\n- Conv2D\n- LeakyReLU\n- MaxPool2D\n- Dropout\n- Flatten\n- Dense","1d4c9288":"Data needs to be normalized between 0 to 1:","2c41d846":"2. Change the working directory:","2e6d888e":"# FRUIT DATASET\n\n<hr>\n\n## Loading and Visualizing the Fruit Dataset\n<hr>\n\n## About Dataset\nFruit dataset has the following features:\n* Dataset size 2'443 samples of Fruit images.\n* The size of each image is 100x100 pixels.\n* Each image has only 1 color channel, i.e., grayscale image.\n* Each pixel has value in the range of [0,255] where 0 represents black, and 255 represents white.\n* Each image has a label: ['Apple', 'Banana', 'Orange', 'Strawberry', 'Pineapple'].\n\n\n## 1. Loading train.csv\n\nThis will loads the data from Kaggle dataset dataset_attr.csv into a **Dataframe**. As file type is CSV, I am loading it using [read_csv](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html) of **pandas**. ","79a40c2f":"4. Define features and target variables:","a192b696":"Restore the best model:","92372bfb":"### **Training**\n*model.fit()* is used to train the model. It takes training data, batch_size, no of epochs, validation data. There are several more parameters, and you can check the documentation here. I am taking the epochs = 20 and batch size = 16. It will return the history of training, which later can be used to analyze the performance.","ba986a9a":"### **Compiling Model**\n\nModel compilation required the selection of optimizer and loss function. Let me discuss a few important things to avoid confusion.\n\n**Optimizers**: Keras provides several optimizers that can be used by importing the optimizers and passing in compile function.\n\n- SGD (Stochastic gradient descent optimizer)\n- RMSprop\n- Adam\n- Adamax\n- Nadam\n\nThere are several more check it out here.\n\nOne of the important things is the selection of the learning rate. If the learning rate is too high, the loss may not converge, and if it is too low, the training will be slow. So it is important to select the reasonably fair value of learning rate. One of the good value to start with is 0.001. If it doesn't work, then other higher or lower values can be tried. The rest of the parameters generally works well and need not be defined. The default value works most of the time. Here, I am using Adam optimizer, but RMSprop can also be used.\n\n**Loss Functions**: Keras provides all of the well-known loss functions which work well for most of the time. But if you need to define a custom loss function, you can. Defining a custom function is out of the scope of this notebook. Let's understand the loss function for the classification tasks. I am discussing 3 loss function here. There are several more check it out here.\n\n- binary_crossentropy: This loss function is used for the binary classification task. The single-node output layer is required. 0 and 1 is used for classification.\n- categorical_crossentropy: Used for Used for Multi-class classification\n- sparse_categorical_crossentropy: Used for Multi-class classification.\n\n**Difference between categorical_crossentropy and sparse_categorical_crossentropy**:\n\nIf your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings (for three classes):\n\n[1,0,0]\n[0,1,0]\n[0,0,1]\n\nBut if your targets are integers, use sparse_categorical_crossentropy. Examples of integer encodings (for the sake of completion):\n\n1\n2\n3\n\nHere, I am using sparse_categorical_crossentropy, as the target values are integer not one-hot vector.","641ec953":"3. Import the dataset:","b787cc64":"3. Perform Basic Imports and set up the general environment:","51d3fb6d":"The Following cell define a set of Function that are used later on in the analysis:","751e89be":"5. Define some utility features and parameters for future analysis: ","85b784e6":"6. Reshape dataset","9e45b281":"### 2. Visualize Fruit dataset Frequency\n-------\nThe first and fundamental thing to check is the frequency of the classes in the dataset, as the balanced dataset is always good to start. But this is not always true, and there are several supervised learning tasks in which the classes are not balanced, also in case of anomaly detection, there is a large difference between the positive and negative class. But that is out of the scope of this notebook.\n\n1. Frequency plot for the training set"}}