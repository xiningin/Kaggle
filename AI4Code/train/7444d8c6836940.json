{"cell_type":{"f489769b":"code","1a7f4c71":"code","cfeb640e":"code","a14a12e9":"code","0589b625":"code","921690c1":"code","cac62212":"code","35b02f06":"code","dedca894":"code","0d900507":"code","faee8140":"code","9d2267f7":"code","ba6b421a":"code","d42dd865":"code","f259e29c":"code","7563b6a9":"code","da7b726b":"code","6e6e6dae":"code","d376bf1e":"code","40a173c2":"code","c067e697":"code","12d27c37":"markdown","2def8a7c":"markdown","4366c10f":"markdown","096ea10b":"markdown","c2d6c48f":"markdown","5ec4b3c2":"markdown","9ffe5997":"markdown","5640bd2c":"markdown","95efc742":"markdown","a5d8f3d2":"markdown","8fe17e40":"markdown","4d11054b":"markdown","a7b9644a":"markdown","1fa139cb":"markdown","cf92689a":"markdown","5b675af4":"markdown","d20a1d51":"markdown","baa4c4cd":"markdown"},"source":{"f489769b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a7f4c71":"# Import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom pyearth import Earth\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\nimport xgboost\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error","cfeb640e":"# Load and display dataset\ndf = pd.read_csv('..\/input\/boston-housing-dataset\/housing.csv')\nprint('The shape of dataset: ', df.shape)\ndf.head()","a14a12e9":"# Obtain data information\ndf.info()","0589b625":"# Obtain descriptive statistics\ndf.describe()","921690c1":"# Visualize each variable's distribution\ndf.hist(bins=50, figsize=(16,12))\nplt.rcParams['figure.figsize'] = [8, 4]\nplt.show()","cac62212":"# Create a sorted heatmap\n# Place the MEDV (dependent variable) on top\n# Arrange featuress in descending order of correlation with MEDV \nk = 14 # number of variables for heatmap\ncorrmat = df.corr()\ncols = corrmat.nlargest(k, \"MEDV\")[\"MEDV\"].index\ncm = np.corrcoef(df[cols].values.T)\nfig, ax = plt.subplots(figsize=(12, 10))\nsns.set(font_scale=1.2)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=\".2f\", annot_kws={\"size\": 10}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","35b02f06":"# Visualize regression between each feature and output variable\n\n# The number of variables\nk = 14\nfig = plt.figure(figsize=(16,16))\n# Correlations between each variable\ncorrmat = df.corr()\n# Take k elements in descending order of coefficient \ncols = corrmat.nlargest(k, \"MEDV\")[\"MEDV\"].index\n# Calculate correlation\nfor i in np.arange(1,k):\n    X_train = df[cols[i]]\n    ax = fig.add_subplot(5,4,i)\n    sns.regplot(x=X_train, y=df['MEDV'])\nplt.tight_layout()\nplt.show()","dedca894":"# Create X and y\nX = df.drop('MEDV', axis=1).values\ny = df['MEDV'].values","0d900507":"# Split dataset into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint('x_train and x_test shapes are {} and {}'.format(X_train.shape, X_test.shape))\nprint('y_train and y_test shapes are {} and {}'.format(y_train.shape, y_test.shape))","faee8140":"# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","9d2267f7":"# Initiate the model\nreg = LinearRegression()\n\n# Fit the model\nreg.fit(X_train, y_train)\n\n# Cross validation\nreg_cv_results_train = cross_val_score(reg, X_train, y_train, cv=5)\nprint(\"Linear Regression (Train): \", np.mean(reg_cv_results_train))\n\nreg_cv_results_test = cross_val_score(reg, X_test, y_test, cv=5)\nprint(\"Linear Regression (Test): \", np.mean(reg_cv_results_test))","ba6b421a":"# Initiate model\nridge = Ridge(alpha=0.1, normalize=True)\n\n# Fit the model\nridge.fit(X_train, y_train)\n\n# Cross validation\nridge_cv_results_train = cross_val_score(ridge, X_train, y_train, cv=5)\nprint(\"Ridge Regression (Train): \", np.mean(ridge_cv_results_train))\n\nridge_cv_results_test = cross_val_score(ridge, X_test, y_test, cv=5)\nprint(\"Ridge Regression (Test): \", np.mean(ridge_cv_results_test))","d42dd865":"# Initiate model\nlasso = Lasso(alpha=0.1, normalize=True)\n\n# Fit the model\nlasso.fit(X_train, y_train)\n\n# Cross validation\nlasso_cv_results_train = cross_val_score(lasso, X_train, y_train, cv=5)\nprint(\"Lasso Regression (Train): \", np.mean(lasso_cv_results_train))\n\nlasso_cv_results_test = cross_val_score(lasso, X_test, y_test, cv=5)\nprint(\"Lasso Regression (Test): \", np.mean(lasso_cv_results_test))","f259e29c":"# Visualize the Lasso regulation\nnames = df.drop('MEDV', axis=1).columns\n\nlasso = Lasso(alpha=0.1, normalize=True)\nlasso_coef = lasso.fit(X_train, y_train).coef_\n\n_ = plt.plot(range(len(names)), lasso_coef)\n_ = plt.xticks(range(len(names)), names, rotation=40)\n_ = plt.ylabel('Coefficients')\n\nplt.show()","7563b6a9":"# Initiate model\nmars = Earth()\n\n# Fit the model\nmars.fit(X_train, y_train)\n\n# Cross validation\nmars_cv_results_train = cross_val_score(mars, X_train, y_train, cv=5)\nprint(\"MARS (Train): \", np.mean(mars_cv_results_train))\n\nmars_cv_results_test = cross_val_score(mars, X_test, y_test, cv=5)\nprint(\"MARS (Test): \", np.mean(mars_cv_results_test))","da7b726b":"# Initiate model\ndt = DecisionTreeRegressor()\n\n# Fit the model\ndt.fit(X_train, y_train)\n\n# Cross validation\ndt_cv_results_train = cross_val_score(dt, X_train, y_train, cv=5)\nprint(\"Decision Tree (Train): \", np.mean(dt_cv_results_train))\n\ndt_cv_results_test = cross_val_score(dt, X_test, y_test, cv=5)\nprint(\"Decision Tree (Test): \", np.mean(dt_cv_results_test))","6e6e6dae":"# Initiate model\nrf = RandomForestRegressor()\n\n# Fit the model\nrf.fit(X_train, y_train)\n\n# Cross validation\nrf_cv_results_train = cross_val_score(rf, X_train, y_train, cv=5)\nprint(\"Random Forest (Train): \", np.mean(rf_cv_results_train))\n\nrf_cv_results_test = cross_val_score(rf, X_test, y_test, cv=5)\nprint(\"Random Forest (Test): \", np.mean(rf_cv_results_test))","d376bf1e":"# Initiate model\nxgb = XGBRegressor()\n\n# Fit the model\nxgb.fit(X_train, y_train)\n\n# Cross validation\nxgb_cv_results_train = cross_val_score(xgb, X_train, y_train, cv=5)\nprint(\"XGBoost (Train): \", np.mean(xgb_cv_results_train))\n\nxgb_cv_results_test = cross_val_score(rf, X_test, y_test, cv=5)\nprint(\"XGBoost (Test): \", np.mean(xgb_cv_results_test))","40a173c2":"results_table = pd.DataFrame([[np.mean(reg_cv_results_train), np.mean(reg_cv_results_test)],\n                             [np.mean(ridge_cv_results_train), np.mean(ridge_cv_results_test)],\n                             [np.mean(lasso_cv_results_train), np.mean(lasso_cv_results_test)],\n                             [np.mean(mars_cv_results_train), np.mean(mars_cv_results_test)],\n                             [np.mean(dt_cv_results_train), np.mean(dt_cv_results_test)],\n                             [np.mean(rf_cv_results_train), np.mean(rf_cv_results_test)],\n                             [np.mean(xgb_cv_results_train), np.mean(xgb_cv_results_test)]],\n                            columns=['Train R2', 'Test R2'],\n                            index=[\"Linear Regression\",\"Ridge Regression\",\"Lasso Regression\",\"MARS\",\n                                   \"Decision Tree\",\"Random Forest\", \"XGBoost Regressor\"])\npd.options.display.precision = 3\nresults_table","c067e697":"# Visualize XGBoost Regression\nxgboost.to_graphviz(xgb)","12d27c37":"RM, PTRATIO, and LSTAT are highly correlated with MEDV (coefficient > 0.5 or < -0.5)","2def8a7c":"- Develop regression models to predict house price prediction.\n- Use cross-validation to alleviate overfitting.\n- Apply various regression models:\n  - Linear Regression\n  - Ridge Regression\n  - Lasso Regression\n  - Multivariate Adaptive Regression Splines (MARS)\n  - Decision Tree Regressor\n  - Random Forest Regressor\n  - XGBoost Regressor","4366c10f":"## 3.1. Linear Regression","096ea10b":"# 3.5. Decision Tree Regression","c2d6c48f":"## 3.2. Ridge Regression","5ec4b3c2":"# 3. Regression Analysis","9ffe5997":"# 2. Data Preparation","5640bd2c":"# Boston House Price Prediction","95efc742":"RM and LSTAT are strongly correlate with MEDV.","a5d8f3d2":"## 3.7. XGBoost Regressor","8fe17e40":"## 3.6. Random Forest Classifier","4d11054b":"# 1. Load and Explore Dataset","a7b9644a":"There are no missing values.","1fa139cb":"- Coefficients of teatures other than RM, PTRATIO, and LSTAT were reduced to 0.\n- It suggests that RM, PTRATIO, and LSTAT are important features.","cf92689a":"## 3.4. Multivariate Adaptive Regression Splines (MARS) ","5b675af4":"- Consistent with EDA and Lasso, features indexed f12 (LSTAT) and f5(RM) are the most important features.","d20a1d51":"## 3.3. Lasso Regression","baa4c4cd":"- All models seem to be overfitting.\n- XGBoost achieved the best performance."}}