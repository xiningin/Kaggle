{"cell_type":{"975516b1":"code","3d1c5e8b":"code","bfc358e1":"code","7cf64e09":"code","5de439e6":"code","fd439781":"code","16d5f265":"code","f85d5b9a":"code","6d021d60":"code","f738f92f":"code","7b2a9e6d":"code","921e5e80":"code","330ab9d6":"code","b6d232b1":"code","c419d9da":"code","5dfb9947":"code","607f164e":"code","2a717107":"code","f60e25d1":"code","b19dcd86":"code","cb075bde":"code","1ae92db8":"code","e785118f":"code","cefd369f":"code","8359410c":"code","f863bfba":"code","1ffe77ce":"code","5e663a44":"code","d16cfce7":"code","0f53f61d":"code","2b897d6b":"code","332bc109":"code","c4e8ff2f":"code","2720bed6":"code","46163204":"code","96a366d3":"code","e1128952":"code","a684e981":"code","1b4929d2":"code","3d69e48b":"code","f5ddc1bc":"markdown","f8b0324c":"markdown","4cec8acc":"markdown","3fd3fcd3":"markdown","8270e034":"markdown"},"source":{"975516b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy.stats import skew, norm \nfrom warnings import filterwarnings as filt\n\nfilt('ignore')\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12, 6)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d1c5e8b":"base_dir = '\/kaggle\/input\/digit-recognizer\/'\ntraindf = pd.read_csv(f\"{base_dir}train.csv\")\ntestdf = pd.read_csv(f\"{base_dir}test.csv\")","bfc358e1":"print(f\"Shape of training df : {traindf.shape}\")\nprint(f\"Shape of testing df : {testdf.shape}\")","7cf64e09":"print(f\"null values in training df : {traindf.isnull().values.sum()}\")\nprint(f\"null values in testing df  : {testdf.isnull().values.sum()}\")","5de439e6":"np.sqrt(traindf.shape[1] - 1)","fd439781":"sns.countplot(traindf.label)","16d5f265":"def show_digit(df, n_cls = 0):\n    row = df[df.label == n_cls].sample(n = 1).drop(['label'], axis = 1)\n    image = np.array(row).reshape(28, 28)\n    plt.imshow(image)","f85d5b9a":"show_digit(traindf)","6d021d60":"from shap import initjs, force_plot, TreeExplainer\nfrom sklearn.ensemble import RandomForestClassifier","f738f92f":"def forceplt(x, y, n_cls = 0):\n    idx = y[y == n_cls].sample(n = 1).index\n    x_samp = x.loc[idx]\n    print(f'Chose the sample from the idx : {idx}')\n    model = RandomForestClassifier().fit(x, y)\n    explainer = TreeExplainer(model)\n    shap_vals = explainer.shap_values(x_samp)[n_cls]\n    exp_vals = explainer.expected_value[n_cls]\n    return force_plot(exp_vals, shap_vals, feature_names = x.columns)","7b2a9e6d":"from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score","921e5e80":"traindf.shape[0] * 0.2","330ab9d6":"x = traindf.drop(['label'], axis = 1)\ny = traindf.label\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, stratify = y)","b6d232b1":"pd.options.display.max_columns = None\nx_val.head()","c419d9da":"initjs()\nforceplt(x_val, y_val, 0)","5dfb9947":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","607f164e":"std = StandardScaler()\nsx_train = std.fit_transform(x_train)\nsx_val = std.transform(x_val)","2a717107":"pca = PCA()\npca_x_train = pd.DataFrame(pca.fit_transform(sx_train), columns = [f\"pca{i + 1}\" for i in range(len(x_train.columns))], index = x_train.index)\npca_x_val = pd.DataFrame(pca.transform(sx_val), columns = [f\"pca{i + 1}\" for i in range(len(x_val.columns))], index = x_val.index)","f60e25d1":"pca_x_train.head()","b19dcd86":"loading = pd.DataFrame(pca.explained_variance_, index = pca_x_train.columns).T\nloading ","cb075bde":"n = 15\nplt.figure(figsize = (15, 8))\nplt.barh(loading.loc[0, : f'pca{n}'].index, loading.loc[0, : f'pca{n}'])","1ae92db8":"high_info = loading.T[loading.T[0] > 10]\nhigh_info","e785118f":"pca_x_train = pca_x_train.loc[:, :'pca3']\npca_x_val = pca_x_val.loc[:, :'pca3']","cefd369f":"from sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler","8359410c":"def sample(x, y, frac = 0.5):\n    big_x, x, big_y, y = train_test_split(x, y, test_size = frac, stratify = y)\n    return x, y\n\ndef best_model(x, y, fold = 10, frac = 0):\n    if frac > 0:\n        x, y = sample(x, y, frac)\n    \n        \n    models = [SVC(), KNeighborsClassifier(), RandomForestClassifier(), XGBClassifier(verbosity = 0), LGBMClassifier()]\n    mnames = ['svm', 'knn', 'random forest', 'xgboost', 'lgbm']\n    scalers = [None, StandardScaler(), RobustScaler(), MinMaxScaler()]\n    snames = ['none', 'std', 'robust', 'minmax']\n    scores = [[] for _ in range(len(scalers))]\n    \n    print(f'Total number of iterations : {len(models) * len(scalers)}')\n    for model in models:\n        for ind, scaler in enumerate(scalers):\n            if scaler:\n                model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n                \n            cv = StratifiedKFold(n_splits=fold, shuffle = True)\n            score = cross_val_score(model, x, y, cv = cv, scoring = 'f1_micro').mean()\n            scores[ind].append(score)\n    \n    return pd.DataFrame(scores, index = snames, columns = mnames).T\n\n\ndef report(xt, yt, xtest, ytest, pred, model):\n    print(' Report '.center(60, '='))\n    print()\n    print(f\"Training score :===> {model.score(xt, yt)}\")\n    print(f\"Testing score  :===> {model.score(xtest, ytest)}\")\n    print()\n    print(classification_report(ytest, pred))\n    print()\n    sns.heatmap(confusion_matrix(ytest, pred), fmt = '.1f', annot = True)\n\n\ndef get_score(xt, yt, xtest, ytest, model, scaler = None, frac = 0, predict = True):\n    \n    if frac > 0:\n        xt, yt = sample(xt, yt, frac)\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n        \n    model.fit(xt, yt)\n    if not predict: return model\n    pred = model.predict(xtest)\n    report(xt, yt, xtest, ytest, pred, model)\n    return \n\ndef gridcv(x, y, model, params, scaler = None, fold = 10, frac = 0):\n    if frac > 0:\n        x, y = sample(x, y, frac)\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    \n    cv = StratifiedKFold(n_splits = fold, shuffle = True)\n    clf = GridSearchCV(model, param_grid = params, cv = cv, return_train_score = True, scoring = 'f1_micro')\n    clf.fit(x, y)\n    res = pd.DataFrame(clf.cv_results_).sort_values('mean_test_score', ascending = False)\n    return clf, res[['mean_train_score', 'mean_test_score', 'params']]\n\ndef plot_cv(res):\n    sns.lineplot(x = res.reset_index().index, y = res.mean_train_score)\n    sns.lineplot(x = res.reset_index().index, y = res.mean_test_score)\n    plt.legend(['train score', 'test score'])","f863bfba":"pca_x_train.shape[0] * 0.5","1ffe77ce":"best_model(pca_x_train, y_train, 5, 0.70)","5e663a44":"params = {\n    'C' : [0.001, 0.01, 0.1, 1, 5, 10, 50],\n    'kernel' : ['rbf', 'poly']\n}\n\npip_params = {f\"model__{key}\" : values for key, values in params.items()}\npip_params","d16cfce7":"p = [0.001, 0.01, 0.1, 1, 5, 10, 50, 100]","0f53f61d":"clf, result = gridcv(pca_x_train, y_train, SVC(), pip_params, StandardScaler(), 5, 0.5)","2b897d6b":"plot_cv(result)","332bc109":"result.head(3)","c4e8ff2f":"get_score(pca_x_train, y_train, pca_x_val, y_val, SVC(C = 5), StandardScaler())","2720bed6":"# model had hard time recognizing 8 , lets see some of the differnt types of 8\n\nfor i in range(5):\n    plt.figure(i)\n    show_digit(traindf, 8)","46163204":"get_score(x_train, y_train, x_val, y_val, SVC(C = 5), StandardScaler())","96a366d3":"testdf.head()","e1128952":"%%time\nclf = get_score(x_train, y_train, x_val, y_val, SVC(C = 5), StandardScaler(), predict = False)\npred = clf.predict(testdf)","a684e981":"sub = pd.read_csv(f'{base_dir}sample_submission.csv')\/loi98\nsub.head()","1b4929d2":"sub['Label'] = pred","3d69e48b":"sub.to_csv('my_submission.csv', index = False)","f5ddc1bc":"since there are more features this will take more computational time to train the model, lets apply PCA to train faster and reduce dimentions ","f8b0324c":"lets choose pca 1 , 2 and 3","4cec8acc":"since we lost lot of information by applying pca, lets just use the original data","3fd3fcd3":"woah, didnt expect this to improve this much","8270e034":"lets standardized the dataset first "}}