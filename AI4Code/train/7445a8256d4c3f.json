{"cell_type":{"59b8413a":"code","df4f5e96":"code","50c99427":"code","95b08844":"code","3a5c8454":"code","2a4984e8":"code","4f58559a":"code","8e0f5ad0":"code","3dad8f4e":"code","177d1fe7":"code","b49d740d":"code","a4d2eebe":"code","d50ec58b":"code","236aabf8":"code","d1fbf290":"code","c11ca66c":"code","856b2fd3":"markdown","dcb2bd14":"markdown","5fee0e7c":"markdown","f081a293":"markdown","ab5dc1a3":"markdown","b29667b1":"markdown","ac325873":"markdown","7caa2209":"markdown","bd5b731a":"markdown","863da0cf":"markdown","1730d1df":"markdown","40c5f14a":"markdown","0bbb84b3":"markdown","b4cf7853":"markdown","7d15cf9e":"markdown","d49f419e":"markdown","91ad268f":"markdown","3541750b":"markdown","9726f212":"markdown","816f0df2":"markdown","123e8ecc":"markdown","2d429921":"markdown","c28fb503":"markdown","002407c8":"markdown","8ebc80a2":"markdown","4034b4f2":"markdown","08c09fb9":"markdown","eda8d57e":"markdown","9e2c2c82":"markdown","086b621d":"markdown","c0b48c8b":"markdown","6b985699":"markdown","693fe413":"markdown","e947acaf":"markdown","389c1c1b":"markdown","2defa00d":"markdown","d0fbcd4b":"markdown","e747a63f":"markdown","de7066f4":"markdown","12f13143":"markdown","209b9df9":"markdown","eafa19e3":"markdown","438b90d0":"markdown","a0f20521":"markdown","a46e0853":"markdown","e58e36bb":"markdown","b9cea965":"markdown","0ab1b9c3":"markdown","1662dc14":"markdown","691988f2":"markdown"},"source":{"59b8413a":"import tensorflow as tf\nfrom tensorflow import keras","df4f5e96":"from sklearn.datasets import make_classification\n\n\nX,y =make_classification(n_samples=5000, n_informative = 8, n_features=10, n_redundant = 0, n_classes=5, random_state=1)\nprint(X.shape, y.shape)\n","50c99427":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)   #Split data into 70% training, 30%test data","95b08844":"from sklearn.preprocessing import StandardScaler    #Standardize data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","3a5c8454":"y_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)","2a4984e8":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()","4f58559a":"model.add(tf.keras.Input(shape=10,))","8e0f5ad0":"model.add(tf.keras.layers.Dense(units=30, activation='relu', )) #First hidden layer with 30 neurons, relu activation\nmodel.add(tf.keras.layers.Dense(units=15, activation='relu')) #Second hidden layer with 15 neurons, relu activation\nmodel.add(tf.keras.layers.Dense(units=5, activation='softmax')) #Output layer with 5 neuron, softmax activation","3dad8f4e":"model.summary()","177d1fe7":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","b49d740d":"keras_history = model.fit(X_train, y_train , batch_size=16, epochs=200, validation_data=(X_test, y_test))","a4d2eebe":"keras_history.history.keys()","d50ec58b":"import matplotlib.pyplot as plt\nplt.plot(keras_history.history['loss'], label='Training loss')\nplt.plot(keras_history.history['val_loss'], color='red', label='Test loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","236aabf8":"tf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.Input(shape=10,))\nmodel.add(tf.keras.layers.Dense(units=30, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01))) \nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.125))\nmodel.add(tf.keras.layers.Dense(units=15, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01))) \nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.25))\nmodel.add(tf.keras.layers.Dense(units=5, activation='softmax')) \nmodel.summary()","d1fbf290":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nkeras_history = model.fit(X_train, y_train , batch_size=16, epochs=200, validation_data=(X_test, y_test))","c11ca66c":"import matplotlib.pyplot as plt\nplt.plot(keras_history.history['loss'], label='Training loss')\nplt.plot(keras_history.history['val_loss'], color='red', label='Test loss')\nplt.legend()","856b2fd3":"Awesome! Hope you've gotten the gist of how to use Keras and how simple and straightforward it is to use.","dcb2bd14":"## Regularization","5fee0e7c":"The keras.models.Sequential class is a wrapper for the neural network model that treats the network as a sequence of layers. We assign it to a variable model.  Click [here](https:\/\/keras.io\/api\/models\/sequential\/) to check out the documentation","f081a293":"## The fit() function","ab5dc1a3":"We have created a dataset with m=1000 and n=10. Each training example belongs to one of 5 classes","b29667b1":"## Batch Normalization","ac325873":"Finally , we call the fit() function to fit our data! Keras takes care of all the mathematical computation of forward prop and backprop! The fit() function takes into input the data, true labels, batch size in which we train, the number of epochs, and the validation data(here which is our test set). It returns a Keras history object which contains various attributes","7caa2209":"## The Input Layer","bd5b731a":"## Dropout","863da0cf":"You can apply an L1, L2 or both L1 and L2 regularizers to a layer using tf.keras.regularizers. The value of $\\lambda$ is specified as an argument of the class. For example\n\n\n```\nmodel.add(tf.keras.layers.Dense(units=30, activation='relu', activity_regularizer=tf.keras.regularizers.l2(0.01)))\n```\n\n","1730d1df":"First, we call the Input() object of Keras, to instantiate a Keras tensor. Note that we only input the number of features and not the training size. So the shape argument of the Input() object is (n, None) \n\nTo add a certain object or layer to the Sequential model, we use the ***add*** function","40c5f14a":"Nothing complicated here too. The probability of a neuron being \"dropped out\" is specified. Higher the value, the \"simpler\" the neural network is. But make sure not to specify too high a dropout rate else the model will start underfitting\n\n\n\n```\nmodel.add(tf.keras.layers.Dropout(0.25))\n```\n\n","0bbb84b3":"## The Sequential class","b4cf7853":"Next, we compile the model using the ***compile()*** method. Here we mention what our loss function should be, the type of optimizer used, and what metrics to print.\n\nWe will be using the categorical cross-entropy as our loss function, with the adam optimizer and accuracy as our metric. Note that we pass our metrics as a list","7d15cf9e":"# **MAKING THE NEURAL NETWORK**","d49f419e":"We will geenerate a random n-class classification problem using scikit learn","91ad268f":"Nothing complicated here. Just do this:\n\n\n\n```\nmodel.add(tf.keras.layers.BatchNormalization())\n\n```\n\n","3541750b":"# **PUTTING IT ALL TOGETHER**","9726f212":"## Adding layers ","816f0df2":"# The dataset","123e8ecc":"> Just that many lines of code to prototype our neural network model! Simple right? We can check out the model's details by calling the ***summary()*** function","2d429921":"The history is a dictionary containing training loss, training accuracy, validation loss, validation accuracy, etc. after each epoch. Each key contains an array of the above.","c28fb503":"## Keras Optimizers","002407c8":"We will first import tensorflow. Note that Keras uses Tensorflow 2.  as backend","8ebc80a2":"[Link for BatchNorm](https:\/\/keras.io\/api\/layers\/normalization_layers\/batch_normalization\/)","4034b4f2":"Here is all of the concepts we have just learnt. Please note that all the abovementioned techniques to prevent overfitting have been implemented below, to get an idea of how they are used in Keras. In reality, all these are not necessary. ","08c09fb9":"To convert our targets into a one hot encoding we use the Keras to_categorical function","eda8d57e":"Click [here](https:\/\/keras.io\/api\/layers\/activations\/) to check out the various activations provided by keras","9e2c2c82":"Great! Now you know how to build a basic artificial neural network in Keras. However, ANN's are extremely prone to overfitting, so always remember to keep a close tab on that. We will now see how various methods are deployed in keras to prevent overfitting","086b621d":"Note: Here we pass the type of activation as an argument to the Dense class. This is the same as adding an Activation layer with the activation inside it. So the first line of code above is the same as:\n\n\n```\nmodel.add(tf.keras.layers.Dense(64))\nmodel.add(tf.keras.layers.Activation(tf.keras.activations.relu))\n```\n\n","c0b48c8b":"**Remember to tweak various hyperparamers, such as the learning rate,  dropout rate, regularization parameter, batch size, number of layers, number of neurons, etc. to get an idea of how the model performs with such changes**","6b985699":"To add a feed forward layer to a neural network, we add a dense layer, or the Dense class. Arguments to the Dense class include the number of neurons for the layer, the activation, regularizer and and other stuff which we do not need to worry about. \n\nIn this model we will make an NN with 2 hidden layers, each activated by the relu function. The last layer , which is the output layer, has 5 neurons(one belonging to each class), activated by the softmax function. \n\n[Dense layer documentation link](https:\/\/keras.io\/api\/layers\/core_layers\/dense\/)","693fe413":"# Importing Keras","e947acaf":"## The compile() function","389c1c1b":"The Keras history object has two main attributes- \n\n1. model\n2. history\n\nThe model contains all information about the weights, inputs, activations and so on\n\nFor example, to get the weights you would code:\n\n\n```\nkeras_history.model.get_weights()\n```\n\n","2defa00d":"## Model summary","d0fbcd4b":"# Split and standardization of data","e747a63f":"Keras is an open source deep learning framework for python. It is a high level API that is built upon Tensorflow. Keras makes deployment of neural networks really simple, with just a few lines of code! The syntax is clear, simple and intuitive.  We urge you to go through the documentation after this to get a better feel of Keras. The link is: https:\/\/keras.io\/api\/","de7066f4":"Click [here](https:\/\/keras.io\/api\/optimizers\/) to check out the various Keras optimizers and their syntax","12f13143":"Click [here](https:\/\/keras.io\/api\/layers\/regularizers\/#l1-class) to check out the documentation","209b9df9":"## The Keras History Object","eafa19e3":"# **IMPROVING YOUR NEURAL NETWORK**","438b90d0":"## Post training analysis","a0f20521":"# Converting the labels to one hot encodings","a46e0853":"Now we plot the training and test loss with each epoch. A key observation to make is that our model is overfitting- training accuracy is 94% while test accuracy is 87%. Notice how the training loss decreases while the test loss increases after a while","e58e36bb":"Well, we all know the pain of working with neural networks from scratch. Don't fret! - because there are multiple frameworks, like Tensorflow, Keras, Pytorch, Caffe, etc. out there to help you build a deep learning model easily. ","b9cea965":"# Deep Learning Frameworks\n\n","0ab1b9c3":"## Activations","1662dc14":"Passing optimizer='adam' to the compile method uses the default learning rate and some other parameters. But often , we want to choose the learning rate, momentum, etc. So, we call the optimizer's class and pass it as an argument to the compile method.\n\nFor example, if we want to use stochastic gradient descent with a learning rate of 0.1 and momentum 0.9 , we would code it as follows:\n\n\n```\nopt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer =opt, metrics=['accuracy'])\n```\n\n","691988f2":"# Why Keras"}}