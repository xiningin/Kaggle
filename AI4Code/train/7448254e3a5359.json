{"cell_type":{"46dacaaa":"code","a0bbd84c":"code","d5c7a02f":"code","4c1ab733":"code","f55aebd3":"code","bd894ec4":"code","d2910d86":"code","707d84c7":"code","dc44466a":"code","3a6cc15f":"code","43a0a51d":"code","9c93d898":"code","e7471e91":"code","97e8132b":"code","21be8950":"code","2254b56b":"code","afd702f6":"code","beef23e7":"code","a8ae2b13":"code","87f5e5d8":"code","c31bcb14":"code","6b4c03f4":"code","88f82a6c":"code","f6e1ad62":"code","ef356db7":"code","06f8b1c4":"code","98ba82cb":"code","6c9c3b05":"code","3ec71960":"code","1d2359f2":"code","fb0311a4":"code","5d984647":"code","d92feb7d":"code","65606494":"code","142cf06c":"code","87c6543b":"code","7d47282d":"code","001ada98":"code","bf31f030":"code","e8885dda":"code","b3517147":"code","59f249d9":"code","1f7347a1":"code","d76bde72":"code","040708f1":"code","49e5eaf5":"code","8efa4d18":"code","fa211e4d":"code","d79e935c":"code","44763d41":"code","43d137f2":"code","eebe197d":"code","aac7ad0c":"code","fbe0bd80":"code","e27a395f":"code","0c6d19b3":"code","3a74a448":"code","d6b43ed2":"code","8ca72184":"code","bf21d440":"code","725443f4":"code","7a7947b7":"code","61ceb235":"code","41fac9fb":"code","45c10dc8":"code","44c4c2c2":"code","2e1cc178":"code","dd9bf5b1":"markdown","fc06a54f":"markdown","61b93d6d":"markdown","e740e755":"markdown","7510a830":"markdown","08aec38a":"markdown","b5c6c118":"markdown","d42bc6ee":"markdown","d8a6b05b":"markdown","00175eff":"markdown","2d5b1f78":"markdown","5f394229":"markdown","594df86d":"markdown","b6920081":"markdown","bb0ea83e":"markdown","1e95e96a":"markdown","89e232e0":"markdown","ef8c2022":"markdown","72981a13":"markdown","736396b0":"markdown","b88da742":"markdown","cf4cde7c":"markdown","858ec37d":"markdown"},"source":{"46dacaaa":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","a0bbd84c":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data.head(10)","d5c7a02f":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head(10)","4c1ab733":"train_data.shape","f55aebd3":"test_data.shape","bd894ec4":"train_data.info()","d2910d86":"test_data.info()","707d84c7":"# Checking Missing values in train_data\ntrain_data.isnull().sum()","dc44466a":"# Checking Missing values in test_data\ntest_data.isnull().sum()","3a6cc15f":"train_data.columns","43a0a51d":"train_data.head()","9c93d898":"test_data.head()","e7471e91":"train_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())","97e8132b":"train_data.isnull().sum()","21be8950":"test_data.isnull().sum()","2254b56b":"sns.catplot(x = 'Embarked', kind = 'count', data = train_data)","afd702f6":"train_data['Embarked'] = train_data['Embarked'].fillna(\"S\")","beef23e7":"train_data.isnull().sum()","a8ae2b13":"train_data['Cabin'] = train_data['Cabin'].fillna(\"Missing\")\ntest_data['Cabin'] = test_data['Cabin'].fillna(\"Missing\")","87f5e5d8":"train_data.isnull().sum()","c31bcb14":"test_data.isnull().sum()","6b4c03f4":"test_data['Fare'] = test_data['Fare'].median()","88f82a6c":"train_data.isnull().sum()","f6e1ad62":"test_data.isnull().sum()","ef356db7":"## get dummy variables for Column sex and embarked since they are categorical value.\ntrain_data = pd.get_dummies(train_data, columns=[\"Sex\"], drop_first=True)\ntrain_data = pd.get_dummies(train_data, columns=[\"Embarked\"],drop_first=True)\n\n\n#Mapping the data.\ntrain_data['Fare'] = train_data['Fare'].astype(int)\ntrain_data.loc[train_data.Fare<=7.91,'Fare']=0\ntrain_data.loc[(train_data.Fare>7.91) &(train_data.Fare<=14.454),'Fare']=1\ntrain_data.loc[(train_data.Fare>14.454)&(train_data.Fare<=31),'Fare']=2\ntrain_data.loc[(train_data.Fare>31),'Fare']=3\n\ntrain_data['Age']=train_data['Age'].astype(int)\ntrain_data.loc[ train_data['Age'] <= 16, 'Age']= 0\ntrain_data.loc[(train_data['Age'] > 16) & (train_data['Age'] <= 32), 'Age'] = 1\ntrain_data.loc[(train_data['Age'] > 32) & (train_data['Age'] <= 48), 'Age'] = 2\ntrain_data.loc[(train_data['Age'] > 48) & (train_data['Age'] <= 64), 'Age'] = 3\ntrain_data.loc[train_data['Age'] > 64, 'Age'] = 4","06f8b1c4":"## get dummy variables for Column sex and embarked since they are categorical value.\ntest_data = pd.get_dummies(test_data, columns=[\"Sex\"], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=[\"Embarked\"],drop_first=True)\n\n\n#Mapping the data.\ntest_data['Fare'] = test_data['Fare'].astype(int)\ntest_data.loc[test_data.Fare<=7.91,'Fare']=0\ntest_data.loc[(test_data.Fare>7.91) &(test_data.Fare<=14.454),'Fare']=1\ntest_data.loc[(test_data.Fare>14.454)&(test_data.Fare<=31),'Fare']=2\ntest_data.loc[(test_data.Fare>31),'Fare']=3\n\ntest_data['Age']=test_data['Age'].astype(int)\ntest_data.loc[ test_data['Age'] <= 16, 'Age']= 0\ntest_data.loc[(test_data['Age'] > 16) & (test_data['Age'] <= 32), 'Age'] = 1\ntest_data.loc[(test_data['Age'] > 32) & (test_data['Age'] <= 48), 'Age'] = 2\ntest_data.loc[(test_data['Age'] > 48) & (test_data['Age'] <= 64), 'Age'] = 3\ntest_data.loc[test_data['Age'] > 64, 'Age'] = 4","98ba82cb":"# In our data the Ticket and Cabin,Name are the base less,leds to the false prediction so Drop both of them.\ntrain_data.drop(['Ticket','Cabin','Name'],axis=1,inplace=True)\ntest_data.drop(['Ticket','Cabin','Name'],axis=1,inplace=True)","6c9c3b05":"train_data.describe()","3ec71960":"train_data.Survived.value_counts()\/len(train_data)*100\n#This signifies almost 61% people in the ship died and 38% survived.","1d2359f2":"train_data.groupby(\"Survived\").mean()","fb0311a4":"train_data.groupby(\"Sex_male\").mean()","5d984647":"train_data.corr()","d92feb7d":"#Heatmap\nplt.subplots(figsize=(10,8))\nsns.heatmap(train_data.corr(),annot=True,cmap='Blues_r')\nplt.title(\"Correlation Among Variables\", fontsize = 20);","65606494":"sns.barplot(x=\"Sex_male\",y=\"Survived\",data=train_data)\nplt.title(\"Gender Distribution - Survived\", fontsize = 16)","142cf06c":"sns.barplot(x='Pclass',y='Survived',data=train_data)\nplt.title(\"Passenger Class Distribution - Survived\", fontsize = 16)","87c6543b":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","7d47282d":"X = train_data.drop(['Survived'], axis=1)\ny = train_data[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.22, random_state = 5)","001ada98":"print(len(X_train),len(X_test),len(y_train),len(y_test))","bf31f030":"#Logistic Regression\nlogReg = LogisticRegression()\nlogReg.fit(X_train,y_train)","e8885dda":"logReg_predict = logReg.predict(X_test)\nlogReg_score = logReg.score(X_test,y_test)\n# print(\"Logistic Regression Prediction :\",logReg_predict)\nprint(\"Logistic Regression Score :\",logReg_score)","b3517147":"print(\"Accuracy Score of Logistic Regression Model:\")\nprint(metrics.accuracy_score(y_test,logReg_predict))\nprint(\"\\n\",\"Classification Report:\")\nprint(metrics.classification_report(y_test,logReg_predict),'\\n')","59f249d9":"SVC_model = SVC(probability=True)\nSVC_model.fit(X_train,y_train)","1f7347a1":"SVC_predict = SVC_model.predict(X_test)\nSVC_score = SVC_model.score(X_test,y_test)\n#print(\"Support Vector Classifier Prediction :\",SVC_predict)\nprint(\"Support Vector Classifier Score :\",SVC_score)","d76bde72":"print(\"Accuracy Score of Support Vector Classifier SVC Model:\")\nprint(metrics.accuracy_score(y_test,SVC_predict))\nprint(\"\\n\",\"Classification Report:\")\nprint(metrics.classification_report(y_test,SVC_predict),'\\n')","040708f1":"decisionTreeModel = DecisionTreeClassifier(max_leaf_nodes=17, random_state=0)\ndecisionTreeModel.fit(X_train, y_train)","49e5eaf5":"decisionTree_predict = decisionTreeModel.predict(X_test)\ndecisionTree_score = decisionTreeModel.score(X_test,y_test)\n#print(\"Decision Tree Classifier Prediction :\",len(decisionTree_predict))\nprint(\"Decision Tree Classifier Score :\",decisionTree_score)","8efa4d18":"print(\"Accuracy Score of Decision Tree Classifier Model:\")\nprint(metrics.accuracy_score(y_test,decisionTree_predict))\nprint(\"\\n\",\"Classification Report:\")\nprint(metrics.classification_report(y_test,decisionTree_predict),'\\n')","fa211e4d":"Random_forest = RandomForestClassifier(n_estimators=17)\nRandom_forest.fit(X_train,y_train)","d79e935c":"randomForest_predict = Random_forest.predict(X_test)\nrandomForest_score = Random_forest.score(X_test,y_test)\n# print(\"Random Forest Prediction :\",RF_predict)\nprint(\"Random Forest Score :\",randomForest_score)","44763d41":"print(\"Accuracy Score of Random Forest Classifier Model:\")\nprint(metrics.accuracy_score(y_test,randomForest_predict))\nprint(\"\\n\",\"Classification Report:\")\nprint(metrics.classification_report(y_test,randomForest_predict),'\\n')","43d137f2":"KNN_model = KNeighborsClassifier(n_neighbors=37)\nKNN_model.fit(X_train, y_train)","eebe197d":"KNN_predict = KNN_model.predict(X_test)\nKNN_score = KNN_model.score(X_test,y_test)\n#print(\"KNN Classifier Prediction :\",KNN_predict)\nprint(\"KNN Classifier Score :\",KNN_score)","aac7ad0c":"print(\"Accuracy Score of KNN Model:\")\nprint(metrics.accuracy_score(y_test,KNN_predict))\nprint(\"\\n\",\"Classification Report:\")\nprint(metrics.classification_report(y_test,KNN_predict),'\\n')","fbe0bd80":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier(random_state=101, n_estimators=150,min_samples_split=100, max_depth=6)\ngbk.fit(X_train, y_train)","e27a395f":"gbk_predict = gbk.predict(X_test)\ngbk_score = gbk.score(X_test,y_test)\n#print(\"Gradient Boosting Prediction :\",gbk_predict)\nprint(\"Gradient Boosting Score :\",gbk_score)","0c6d19b3":"print(\"Accuracy Score of Gradient Boosting Model:\")\nprint(metrics.accuracy_score(y_test,gbk_predict))","3a74a448":"from sklearn import ensemble\nfrom sklearn.model_selection import GridSearchCV","d6b43ed2":"GridList =[ {'n_estimators' : [10, 15, 20, 25, 30, 35, 40], 'max_depth' : [5,10,15, 20]},]\nrandomForest_ensemble = ensemble.RandomForestClassifier(random_state=21, max_features= 3)\ngridSearchCV = GridSearchCV(randomForest_ensemble,GridList, cv = 5)","8ca72184":"gridSearchCV.fit(X_train,y_train)","bf21d440":"gridSearchCV_predict = gridSearchCV.predict(X_test)\ngridSearchCV_score = gridSearchCV.score(X_test,y_test)\n#print(\"Grid SearchCV Prediction :\",gridSearchCV_predict)\nprint(\"Grid SearchCV Score :\",gridSearchCV_score)","725443f4":"from tabulate import tabulate","7a7947b7":"print(tabulate([['K-Nearest Neighbour', KNN_score],['Logistic Regression',logReg_score ],['Decision Tree',decisionTree_score ],['Random Forest',randomForest_score ],['SVC', SVC_score],['Gradient Boosting', gbk_score],['Grid SearchCV',gridSearchCV_score]], headers=['Model Algorithm', 'Score']))","61ceb235":"test_data.head()","41fac9fb":"#set ids as PassengerId and predict survival \nids = test_data['PassengerId']\nprint(len(ids))\npredictions = gridSearchCV.predict(test_data)","45c10dc8":"#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })","44c4c2c2":"output.head(10) # Output preview","2e1cc178":"output.to_csv('submission.csv', index=False) # Submission csv file","dd9bf5b1":"### Modeling Data \n###### I will be modelling the data with the below models:\n- Logistic Regression\n- Support Vector Machine\n- Decision Tree Classifier\n- Random Forest Classifier\n- K-Nearest Neighbour Classifier\n- Gradient Boosting\n- Grid SearchCV","fc06a54f":"## K-Nearest Neighbours","61b93d6d":"### Survival as per classes\n- 63% of Passenger Class 1\n- 48% of Passenger Class 2\n- Only 25% of Passenger Class 3 survived","e740e755":"## Exploratory Data Analysis ","7510a830":"## Grid SearchCV","08aec38a":"### Correlation between Variables","b5c6c118":"## Decision Tree Classifier","d42bc6ee":"### Gradient Boosting","d8a6b05b":" #### The points to know from the analysis\n #### 1. 38% of people survived\n #### 2. 74% of Females survived and ~19% of Males survived ","00175eff":"### Support Vector Machine","2d5b1f78":"- Survived has positive correlation of 0.3 with Fare\n- Sex and survived have negative correlation of -0.54\n- Pclass and Survived have negative correlation of -0.34**","5f394229":"##### Female passengers have survived more than male passengers i.e Females and Children would have been the priority","594df86d":"### Logistic Regression","b6920081":"### From the above table, we can clearly see that the accuracy of the Grid SearchCV is similiar to Logistic Regression\n\n#### Lets apply this to our test data","bb0ea83e":"**Prediction Results : 0.78468**","1e95e96a":"### Check Missing Values ","89e232e0":"## Prediction\n\n#### Let's use the Gradient Boosting Classifier to predict our data","ef8c2022":"## Random Tree Classifier","72981a13":"I will keep updating the notebook with updates\n\n**If you have any recommendations and suggestions, please share in the comments below !!**\n\nLooking forward to know your views and suggestions :)\n\n**If you feel the notebook is worth it, UPVOTE !!**\n\n**Thanks for reading :)**","736396b0":"#### No missing values left so we can proceed further","b88da742":"### Load the Data ","cf4cde7c":"# Kaggle - Titanic: Machine Learning from Disaster\n\nThe competition details as below\n\n### The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). \n\n### Predicting the survival on the Titanic","858ec37d":"### Load Helpful Packages"}}