{"cell_type":{"f297b5b3":"code","4bb126f0":"code","5dbe3950":"code","900e3647":"code","bc61b3a4":"code","ce6b3312":"code","9fbcd1d2":"code","80650e79":"code","28303339":"code","38eae431":"code","1f8e15c5":"code","7e0223f2":"code","57aa8511":"code","b13230f9":"code","5b08563d":"code","111c4dab":"code","8316db77":"code","9c0f0a56":"code","be00ab4e":"code","680568d4":"markdown"},"source":{"f297b5b3":"!pip install -q tensorflow==2.5 # Need Tensorflow >= 2.5\n!pip list | grep tensorflow","4bb126f0":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import backend as K\n\nnp.set_printoptions(suppress=True, edgeitems=20, linewidth=1000)","5dbe3950":"assert float(tf.__version__[:tf.__version__.rfind('.')]) >= 2.5, 'Need TF 2.5 version'","900e3647":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bc61b3a4":"train = pd.read_csv(os.path.join(dirname, \"train.csv\"))\ntest = pd.read_csv(os.path.join(dirname, \"test.csv\"))\nsub = pd.read_csv(os.path.join(dirname, \"sample_submission.csv\"))","ce6b3312":"print(train.shape)\ntrain.head()","9fbcd1d2":"train.describe().loc[['mean', 'std'], :] # checking sparseness","80650e79":"num_train_samples = int(0.8 * len(train))\nnum_val_samples = int(0.2 * len(train))\nnum_test_samples = len(test)\n\nprint(\"num_train_samples:\", num_train_samples)\nprint(\"num_val_samples:\", num_val_samples)\nprint(\"num_test_samples:\", num_test_samples)","28303339":"sequence_length = 12 # window (lest use a half day)","38eae431":"raw_data = train.iloc[:, 1:-3].values\ntargets = train.iloc[:, -3:].values\ntargets = np.log1p(targets)\n\ntest_data = np.concatenate([raw_data[-sequence_length:-1, :], test.iloc[:, 1:].values]) # adding last window together with test\nall_data = np.concatenate([raw_data, test.iloc[:, 1:].values])\n\n# Normalizing with test data also for more accurate mean\/std\nmean = all_data.mean(axis=0)\nstd = all_data.std(axis=0)\n\nmean_target = targets.mean(axis=0)\nstd_target = targets.std(axis=0)\n\nraw_data = (raw_data - mean) \/ std\ntargets = (targets - mean_target) \/ std_target\ntest_data = (test_data - mean) \/ std","1f8e15c5":"# Datasets\nbatch_size = 32\n\ntrain_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n    raw_data,\n    targets=targets[sequence_length:],\n    sequence_length=sequence_length,\n    batch_size=batch_size,\n    shuffle=True,\n    start_index=0,\n    end_index=num_train_samples).prefetch(64)\n\nval_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n    raw_data,\n    targets=targets[sequence_length:],\n    sequence_length=sequence_length,\n    batch_size=batch_size,\n    shuffle=True,\n    start_index=num_train_samples,\n    end_index=num_train_samples + num_val_samples).prefetch(64)\n\ntest_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n    test_data,\n    targets=None,\n    sequence_length=sequence_length,\n    batch_size=batch_size)","7e0223f2":"def rmsle(y_true, y_pred):\n    msle = tf.keras.losses.MeanSquaredLogarithmicError()\n    return K.sqrt(msle(y_true, y_pred)) ","57aa8511":"def load_model():\n    tf.keras.backend.clear_session()\n    \n    inputs = tf.keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n    x = Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\")(inputs)\n    x = LSTM(128, return_sequences=True)(x)\n    x = LSTM(64, return_sequences=True)(x)\n    x = Dropout(0.5)(x)\n    x = TimeDistributed(Dense(64))(x)\n    x = GlobalAveragePooling1D()(x)\n    \n    x1 = Dense(64)(x)\n    x1 = Add()([x1, x])\n    x1 = Dropout(0.5)(x1)\n    x1 = Dense(32)(x1)\n    x1 = Concatenate()([x1, x])\n    x1 = Dense(16)(x1)\n    x1 = Dropout(0.3)(x1)\n    x1 = Dense(1, name=\"carb\")(x1)\n    \n    x2 = Dense(64)(x)\n    x2 = Add()([x2, x])\n    x2 = Dropout(0.5)(x2)\n    x2 = Dense(32)(x2)\n    x2 = Concatenate()([x2, x])\n    x2 = Dense(16)(x2)\n    x2 = Dropout(0.3)(x2)\n    x2 = Dense(1, name=\"bezn\")(x2)\n    \n    x3 = Dense(64)(x)\n    x3 = Add()([x3, x])\n    x3 = Dropout(0.5)(x3)\n    x3 = Dense(32)(x3)\n    x3 = Concatenate()([x3, x])\n    x3 = Dense(16)(x3)\n    x3 = Dropout(0.3)(x3)\n    x3 = Dense(1, name=\"nitro\")(x3)\n    \n    model = tf.keras.Model(inputs, [x1, x2, x3])\n    return model","b13230f9":"model = load_model()\n\ncallbacks = [tf.keras.callbacks.ModelCheckpoint(\"lstm.keras\", save_best_only=True)]\n\nlr = 3e-3\noptim = tf.keras.optimizers.Adam(learning_rate=lr)\nmodel.compile(optimizer=optim, loss=rmsle)","5b08563d":"history = model.fit(train_dataset,\n                    epochs=100,\n                    validation_data=val_dataset,\n                    verbose=2,\n                    callbacks=callbacks)","111c4dab":"model = tf.keras.models.load_model(\"lstm.keras\", custom_objects={'rmsle': rmsle})\nmodel.evaluate(val_dataset)","8316db77":"# PREDICT\npreds = model.predict(test_dataset)\npreds = np.hstack(preds)\npreds.shape","9c0f0a56":"sub.iloc[:, 1:] = np.expm1(preds) * std_target + mean_target\nsub.iloc[0, 1:] = train.iloc[-1, -3:].values # First prediction we got for free...\nsub.to_csv('sub.csv', index=False)","be00ab4e":"sub.head()","680568d4":"#### <font color=\"red\">Warning 1<\/font>: This is no submission material, experimenting with some fancy architectures (LSTM's, skip connections, concatenation, etc...)\n#### <font color=\"red\">Warning 2<\/font>: First notebook, as i never really tried kaggle before, so expect weird stuff from newcomers (even if my account is not that new)...\n\nSome ideas from Remek's [notebook](https:\/\/www.kaggle.com\/remekkinas\/lstm-seq2seq-encoder-decoder)! Check it out too!"}}