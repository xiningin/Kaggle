{"cell_type":{"3ff883e1":"code","7756d661":"code","d0b1e531":"code","2fd813b1":"code","7bfef0fe":"code","31f92a61":"code","3aa888f7":"code","ecb11a75":"code","05cd7107":"code","6940b6f4":"code","9554315d":"code","3b042cd2":"code","c80cbff7":"code","5da8328e":"code","d2d66d5d":"code","4f2d4137":"code","fff3a515":"code","a7350aa9":"code","3b6420d2":"code","3dc7ecc8":"code","fd1404d6":"code","ac6bd605":"code","9d6417e7":"code","3e8c6569":"code","264238e6":"code","e55467d2":"code","54ccd7fa":"code","b7a98190":"code","dbcb63af":"code","196e21ef":"code","c4b1f221":"code","01cfdcc0":"markdown","5b221074":"markdown","729875fd":"markdown","a364a74e":"markdown","a848c7eb":"markdown","5970a1e4":"markdown","684f0464":"markdown","ead61f0f":"markdown","8919e04d":"markdown","ca9a3bf4":"markdown","535ed09b":"markdown","9aecb4a5":"markdown","d9edb0d8":"markdown","af70030b":"markdown","ba7b5c2d":"markdown","6295967d":"markdown","322c5130":"markdown","e4bf34b7":"markdown","980bbf73":"markdown","a8a64320":"markdown","8c14a895":"markdown","321cc569":"markdown","ebb29612":"markdown","a3ea304c":"markdown","014be7ff":"markdown","f5ff71d9":"markdown","1ead32bd":"markdown","af427026":"markdown","a50846a9":"markdown","5ef28612":"markdown","a53624de":"markdown","391d2860":"markdown","54b27b92":"markdown","40e8acc5":"markdown","84976d99":"markdown","4a95409c":"markdown"},"source":{"3ff883e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7756d661":"df = pd.read_csv('\/kaggle\/input\/german-credit\/german_credit_data.csv')","d0b1e531":"df.shape","2fd813b1":"df.head()","7bfef0fe":"df.info()","31f92a61":"df.describe()","3aa888f7":"df.isnull().sum()","ecb11a75":"numerical = ['Credit amount','Age','Duration']\ncategorical = ['Sex','Job','Housing','Saving accounts','Checking account','Purpose']\nunused = ['Unnamed: 0']","05cd7107":"df = df.drop(columns = unused)\ndf.shape","6940b6f4":"for cat in categorical:\n    df[cat] = df[cat].fillna(df[cat].mode().values[0])","9554315d":"df.isnull().sum()","3b042cd2":"sns.pairplot(df)","c80cbff7":"fig = plt.figure(figsize = (20,15))\naxes = 320\nfor cat in categorical:\n    axes += 1\n    fig.add_subplot(axes)\n    sns.countplot(data = df, x = cat)\n    plt.xticks(rotation=30)\nplt.show()","5da8328e":"#create correlation\ncorr = df.corr(method = 'pearson')\n\n#convert correlation to numpy array\nmask = np.array(corr)\n\n#to mask the repetitive value for each pair\nmask[np.tril_indices_from(mask)] = False\nfig, ax = plt.subplots(figsize = (15,12))\nfig.set_size_inches(15,15)\nsns.heatmap(corr, mask = mask, vmax = 0.9, square = True, annot = True)","d2d66d5d":"df_cluster = pd.DataFrame()\ndf_cluster['Credit amount'] = df['Credit amount']\ndf_cluster['Age'] = df['Age']\ndf_cluster['Duration'] = df['Duration']\ndf_cluster['Job'] = df['Job']\ndf_cluster.head()","4f2d4137":"fig = plt.figure(figsize = (15,10))\naxes = 220\nfor num in numerical:\n    axes += 1\n    fig.add_subplot(axes)\n    sns.boxplot(data = df, x = num)\nplt.show()","fff3a515":"    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1, figsize=(8,8))\n    sns.distplot(df[\"Age\"], ax=ax1)\n    sns.distplot(df[\"Credit amount\"], ax=ax2)\n    sns.distplot(df[\"Duration\"], ax=ax3)\n    sns.distplot(df[\"Job\"], ax=ax4)\n    plt.tight_layout()\n    plt.legend()","a7350aa9":"df_cluster_log = np.log(df_cluster[['Age', 'Credit amount','Duration']])\n\nfig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\nsns.distplot(df_cluster_log[\"Age\"], ax=ax1)\nsns.distplot(df_cluster_log[\"Credit amount\"], ax=ax2)\nsns.distplot(df_cluster_log[\"Duration\"], ax=ax3)\nplt.tight_layout()","3b6420d2":"df_cluster_log.head()","3dc7ecc8":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ncluster_scaled = scaler.fit_transform(df_cluster_log)","fd1404d6":"from sklearn.cluster import KMeans\n\nSum_of_squared_distances = []\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(cluster_scaled)\n    Sum_of_squared_distances.append(km.inertia_)\nplt.figure(figsize=(20,5))\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","ac6bd605":"from mpl_toolkits.mplot3d import Axes3D\n\nmodel = KMeans(n_clusters=3)\nmodel.fit(cluster_scaled)\nkmeans_labels = model.labels_\n\nfig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\nax = plt.axes(projection=\"3d\")\n\nax.scatter3D(df_cluster['Age'],df_cluster['Credit amount'],df_cluster['Duration'],c=kmeans_labels, cmap='rainbow')\n\nxLabel = ax.set_xlabel('Age', linespacing=3.2)\nyLabel = ax.set_ylabel('Credit Amount', linespacing=3.1)\nzLabel = ax.set_zlabel('Duration', linespacing=3.4)\nprint(\"K-Means\")","9d6417e7":"df_clustered_kmeans = df_cluster.assign(Cluster=kmeans_labels)\ngrouped_kmeans = df_clustered_kmeans.groupby(['Cluster']).mean().round(1)\ngrouped_kmeans","3e8c6569":"import scipy.cluster.hierarchy as sch\n\nplt.figure(figsize=(20,10))\ndendrogram = sch.dendrogram(sch.linkage(cluster_scaled, method='ward'))","264238e6":"from sklearn.cluster import AgglomerativeClustering\nmodel = AgglomerativeClustering(n_clusters=4)\nmodel.fit(cluster_scaled)\nhac_labels = model.labels_\n\nfig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\nax = plt.axes(projection=\"3d\")\n\nax.scatter3D(df_cluster['Age'],df_cluster['Credit amount'],df_cluster['Duration'],c=hac_labels, cmap='rainbow')\n\nxLabel = ax.set_xlabel('Age', linespacing=3.2)\nyLabel = ax.set_ylabel('Credit Amount', linespacing=3.1)\nzLabel = ax.set_zlabel('Duration', linespacing=3.4)\nprint(\"Hierarchical Agglomerative Clustering\")","e55467d2":"df_clustered_hac = df_cluster.assign(Cluster=hac_labels)\ngrouped_hac = df_clustered_hac.groupby(['Cluster']).mean().round(1)\ngrouped_hac","54ccd7fa":"from sklearn.cluster import DBSCAN\n\nmodel = DBSCAN()\nmodel.fit(cluster_scaled)\ndbs_labels = model.labels_\n\nfig = plt.figure(num=None, figsize=(15, 10), dpi=80, facecolor='w', edgecolor='k')\nax = plt.axes(projection=\"3d\")\n\nax.scatter3D(df_cluster['Age'],df_cluster['Credit amount'],df_cluster['Duration'],c=dbs_labels, cmap='rainbow')\n\nxLabel = ax.set_xlabel('Age', linespacing=3.2)\nyLabel = ax.set_ylabel('Credit Amount', linespacing=3.1)\nzLabel = ax.set_zlabel('Duration', linespacing=3.4)\nprint(\"DBSCAN\")","b7a98190":"grouped_kmeans","dbcb63af":"df_clustered = df.assign(Cluster=kmeans_labels)","196e21ef":"df_clustered.head()","c4b1f221":"fig = plt.figure(figsize = (20,15))\naxes = 320\nfor cat in categorical:\n    axes += 1\n    fig.add_subplot(axes)\n    sns.countplot(data = df_clustered, hue=df_clustered['Cluster'], x = cat)\n    plt.xticks(rotation=30)\nplt.show()","01cfdcc0":"## 8. Summaries","5b221074":"From the heatmap above we can see that the best correlation is between credit amount and duration. But so far we will still use all the numeric features for the clustering.","729875fd":"Above are the bar plot of all the categorical feature. From the bar plot above, we can get some insight. That are:\n1. The amount of men are greater than women\n2. Most of the customer are skilled on their job\n3. Most of the customer have their own house\n4. Most of the customer have little saving account\n5. Most of the customer hav little checking account\n6. Most of the customer use credit for car","a364a74e":"From the figure above we can see that the most optimal values are 3. So we choose 3 as the k values of the k-means model.","a848c7eb":"1. After comparing three kind of clustering models, we decide to use k-means as the model \n2. The data divided into three clusters\n3. The three clusters can be used to determine the creditworthiness of the German Credit potential borrowers\n4. Each of the cluster have their own characteristics","5970a1e4":"Looking back from the centroid of the clusters, let's see the \"returning power\" of each of the centroid by dividing the Credit amount with the duration. The higher the \"returning power\".","684f0464":"### 5.2. Fit & Transform","ead61f0f":"The table above shows the centroid of each clusters that could determine the clusters rule. These are:<br>\n<br>\nCluster 0 : Higher credit amount, middle-aged, long duration customers<br>\nCluster 1 : Lower credit amount, young, short duration customers<br>\nCluster 2 : Lower credit amount, old, short duration customers","8919e04d":"## 1. Business Understanding","ca9a3bf4":"The table above shows the centroid of each clusters that could determine the clusters rule. These are:<br>\n<br>\nCluster 0 : Higher credit amount, old, long duration customers<br>\nCluster 1 : Lower credit amount, young, long duration customers<br>\nCluster 2 : Lower credit amount, old, short duration customers<br>\nCluster 3 : Lower credit amount, young, short duration customers","535ed09b":"On this model, to determine the n_clusters we can use dendogram. ","9aecb4a5":"From the dendogram above, we can see that the most optimal n_clusters is 4.","d9edb0d8":"### 6.2. Hierarchical Agglomerative Clustering","af70030b":"## 5. Feature Engineering","ba7b5c2d":"#### 5 C's of Credit\nThe five Cs of credit is a system used by lenders to gauge the creditworthiness of potential borrowers. Creditworthiness is how a lender determines that you will default on your debt obligations, or how worthy you are to receive new credit. Your creditworthiness is what creditors look at before they approve any new credit to you. The five Cs of credit are character, capacity, capital, collateral, and conditions.\n1. Character : Credit history of the customer\n2. Capacity : Assesses borrower's debt-to-income ratio\n3. Capital :Assesses borrower's seriousness level\n4. Collateral : It gives the lender the assurance that if the borrower defaults on the loan, the lender can get something back by repossessing the collateral\n5. Conditions : Conditions are the external variables that can affect credit and credit quality. This refers to national, international and local economy, the industry and the bank itself.\n\nWe will try to find the creditworthiness of the customer on the German Credit Dataset.\n\nSource :https:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=1872804","6295967d":"From the figures above we can see that there are still some outliers on the numerical features.","322c5130":"Above are the pairplot of all the numerical features.","e4bf34b7":"## 6. Models","980bbf73":"### 6.3. DBSCAN","a8a64320":"From these models, we can choose the most well segmented model, that is k-means. We use the clusters from the that model to analyze the dataset.","8c14a895":"From the figure above we can see that DBSCAN is not suitable for this dataset.","321cc569":"From the figure above we could see that the cluster segmented well.","ebb29612":"We can use logarithmic transformation to reduce the outliers and distribution skewness.","a3ea304c":"## 7. Result Analysis","014be7ff":"## 4. Visualize","f5ff71d9":"## 2. Load dataset and quick look","1ead32bd":"### 6.1. K-Means","af427026":"We can see that the skewness of the distribution is eliminatied.","a50846a9":"### 5.1. Log Transform","5ef28612":"### 1.1. Load Dataset","a53624de":"From the figure above, we can see that distributions are right-skewed.","391d2860":"We will make a clustering model based on the features we already choose. We will try three clustering models, those are:\n1. K-Means\n2. Hierarchical Agglomerative Clustering\n3. DBSCAN","54b27b92":"First, we use Elbow Method to determine the optimal k value for the k-means.","40e8acc5":"## 3. Missing values identification and handling","84976d99":"### 1.2. Quick look","4a95409c":"Above are figures of the clusters distribution on each categorical feature."}}