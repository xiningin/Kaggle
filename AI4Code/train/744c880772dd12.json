{"cell_type":{"76fb464b":"code","54773400":"code","de134dca":"code","156bde34":"code","0697b25a":"code","35fa42ba":"code","f8098367":"code","2d92fafe":"markdown"},"source":{"76fb464b":"import pandas as pd\nimport numpy as np\nimport datatable as dt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport optuna","54773400":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nss = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\ntrain = train.drop('id',axis=1)\ntest = test.drop('id',axis=1)","de134dca":"X = train.drop('target', axis=1).copy()\ny = train['target'].copy()\nX_test = test.copy()","156bde34":"params = {\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","0697b25a":"X['std'] = X.std(axis=1)\nX['min'] = X.min(axis=1)\nX['max'] = X.max(axis=1)\n\nX_test['std'] = X_test.std(axis=1)\nX_test['min'] = X_test.min(axis=1)\nX_test['max'] = X_test.max(axis=1)","35fa42ba":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nimport optuna\nkf = StratifiedKFold(n_splits=9, shuffle=True, random_state=12)\n\npreds = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    params['learning_rate']=0.0001\n    model1 = XGBClassifier(**params)\n    \n    model1.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False)\n    \n    params['learning_rate']=0.001\n    model2 = XGBClassifier(**params)\n    model2.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model1)\n    \n    params['learning_rate']=0.001\n    model3 = XGBClassifier(**params)\n    \n    model3.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_valid,y_valid)],\n              early_stopping_rounds=200,\n              verbose=False,\n              xgb_model=model2)\n    \n    pred_valid = model3.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*30)\n    \n    test_preds = model3.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","f8098367":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('.\/submission.csv', index=False)\nss.head()","2d92fafe":"**Xgboost attempt in this TPS**\nSince i found everyone doing NN. So I thought of doing something different. I had done this xgboost by my past experiences from the competitions. So there can be some techniques which some think might be used by them. "}}