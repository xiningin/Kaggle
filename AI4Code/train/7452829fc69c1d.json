{"cell_type":{"7c4c8845":"code","b2ca01d0":"code","beee303c":"code","30581676":"code","b2e130bd":"code","1ce140c2":"code","c31e2e44":"code","a1704921":"code","9b2c0b76":"code","64242078":"code","24913914":"code","dd46c522":"code","350c52d3":"code","a9a0c489":"code","e25e2295":"code","9bde4c79":"code","4928e65b":"code","05d95d45":"code","06507b55":"code","745cc223":"code","f57c2b89":"code","9f470bce":"code","ce55930f":"code","5c48738c":"code","cda2bb08":"code","437c59af":"code","528c4d0d":"code","2bf96b46":"code","11b0698d":"code","7ebe686e":"code","26aeb3b5":"code","6b0900f7":"code","bb5c9fd8":"code","ea995f6f":"code","d2103ad6":"code","a59bea66":"code","ccbcbb7e":"code","7ce92dbb":"code","88cc99c0":"code","9b73afe7":"code","984d3ed4":"code","e9cf13d6":"code","edf23b6a":"markdown","9621c8a0":"markdown","a51453df":"markdown","4c8c7db8":"markdown","8a4a9014":"markdown","3a890fec":"markdown","7f3dd41a":"markdown","339d2b2a":"markdown","5f913980":"markdown","148798c1":"markdown","5220ca8d":"markdown"},"source":{"7c4c8845":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b2ca01d0":"#import the packaged that will be use for this analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier","beee303c":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')","30581676":"#review the dataset for trainning and testing prediction models\ndf_train","b2e130bd":"#review the meterials for the final prediction\ndf_test","1ce140c2":"# review whether there is a missing value\nmsno.matrix(df_train)\nplt.show()\nprint(df_train.isna().sum())","c31e2e44":"# get the categorical columns\ncat_bool = (df_train.dtypes == 'object')\ncat_columns = df_train.columns[cat_bool].tolist()\nprint(df_train[cat_columns].head())","a1704921":"# get the numeric columns\nnum_columns = [col for col in df_train.iloc[:, :-1].columns if df_train[col].dtype in ['float64']]\nprint(df_train[num_columns].head())","9b2c0b76":"# review the correlation between numeric features\ncorr = df_train[df_train.columns[-cat_bool]].iloc[:, 1:].corr()\n\nsns.set_context(\"notebook\")\nplt.subplots(figsize=(12, 8))\ng1 = sns.heatmap(corr, cmap = 'YlOrBr', annot=True, fmt='.2f')\ng1.set_title(\"Correlation between Numeric Columns\", y=1)\nplt.xlabel('Numeric Column')\nplt.ylabel('Numeric Column')\nplt.xticks(rotation=0)\nplt.yticks(rotation=0)\nplt.show()","64242078":"# review the frequency of the variables in categorical columns\nplt.figure(figsize=(12,16))\nsns.set_context(\"notebook\")\nplt.style.use('ggplot') \nplt.subplots_adjust(hspace=0.6, wspace=0.6)\nsns.set_palette(\"Spectral\")\nfor i, cat_col in enumerate(cat_columns):\n    plt.subplot(5, 3, i+1)\n    sns.countplot(x=cat_col, data=df_train, \n                  order=df_train[cat_col].value_counts().index)\n    plt.title(cat_col)\nplt.show()","24913914":"# review the distribution of the numeric variables\nplt.figure(figsize=(12,16))\nplt.subplots_adjust(hspace=0.6, wspace=0.6)\nfor i, num_col in enumerate(num_columns):\n    plt.subplot(5, 3, i+1)\n    sns.kdeplot(df_train[num_col], color='grey')\n    plt.title(num_col)\nplt.show()","dd46c522":"# prepare a dataframe for the normalization\ndf = pd.concat([df_train, df_test])\ndf","350c52d3":"# normalize the numeric columns\nscaler=StandardScaler()\nscaler.fit(df[num_columns])\ndata_normalized=scaler.transform(df[num_columns])\ndata_normalized=pd.DataFrame(data_normalized, index=df[num_columns].index, columns=num_columns)\ndata_normalized","a9a0c489":"# put the normalized variables back to the dataframe\ndf_normalized = pd.concat([df['id'], df[cat_columns]], axis=1)\ndf_normalized = pd.concat([df_normalized, data_normalized], axis=1)\ndf_normalized = pd.concat([df_normalized, df['target']], axis=1)\ndf_normalized","e25e2295":"# review whether the mean and standard deviation are close to 0 and 1\ndf_normalized[num_columns].describe().round(2)","9bde4c79":"# label the categorical columns\nle = LabelEncoder()\ndf_normalized[cat_columns] = df_normalized[cat_columns].apply(lambda x: le.fit_transform(x))\nprint(df_normalized[cat_columns].head())","4928e65b":"df_norm = df_normalized.drop('id', axis=1)\ndf_norm","05d95d45":"# get the binary coding of the categorical columns\nmodel_OHE = ColumnTransformer([('OHE', OneHotEncoder(), cat_columns)], remainder = 'passthrough')\nencoded = model_OHE.fit_transform(df_norm)\nprint(encoded[:5, :])\nprint(df_norm.shape)\nprint(encoded.shape)","06507b55":"# make the coded array a dataframe\ndf_encoded = pd.DataFrame(encoded, columns=range(encoded.shape[1]))\ndf_encoded","745cc223":"# split the data into 2 for training the model and the data for the final rediction\ntrain = df_encoded[-df_encoded.iloc[:, -1].isna()]\ntest = df_encoded[df_encoded.iloc[:, -1].isna()]","f57c2b89":"# generat the data for training XGBRegressor\nX = train.iloc[:, :-1]\ny = train.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=44)","9f470bce":"k_fold = KFold(n_splits=3)\nmodel = xgb.XGBRegressor(objective=\"reg:squarederror\")\nxgb_params = {'colsample_bytree': [0.3], \n              'max_depth': [5], \n              'n_estimators': [100]}\nGS = GridSearchCV(model, param_grid=xgb_params, scoring='neg_mean_squared_error', cv=k_fold)\nGS.fit(X_train, y_train)\nGS_best = GS.best_estimator_\ny_pred = GS_best.predict(X_test)\nno_reduction = np.mean((y_pred - y_test)**2)**.5\nprint('RMSE:', no_reduction)\nprint('Best Parameters:', GS.best_params_)","ce55930f":"# select the same parameters as the previous model\nmodel_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\", \n                         colsample_bytree = 0.3, \n                         max_depth = 5, \n                         n_estimators = 100)\nmodel_xgb.fit(X_train, y_train)","5c48738c":"# visualize the top 40 important features when fitting XGBRegressor\nimportance = model_xgb.get_booster().get_score(importance_type='weight')\ndf_importance = pd.DataFrame.from_dict(importance, orient='index')\ndf_40 = df_importance.sort_values(by=0, ascending=False).head(40)\nfig = px.bar(df_40.reset_index(), x='index', y=0, title='Top 40 Important Elements', color=df_40[0])\nfig.update_layout(title_x=0.5, title_y=0.86)\nfig.show()","cda2bb08":"# subset the datafram with the columns that have higher importance\nlist = df_importance[df_importance[0]>50].reset_index()['index']\ndf_reduced = df_encoded[list.astype(int)]\ndf_reduced = df_reduced.assign(target=df_normalized['target'].to_list())\ndf_reduced","437c59af":"# split the data into 2 for training the model and the data for the final rediction\ntrain_i = df_reduced.iloc[:300000]\ntest_i = df_reduced.iloc[300000:]","528c4d0d":"X_i = train_i.iloc[:, :-1]\ny_i = train_i.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X_i, y_i, train_size=0.8, test_size=0.2, random_state=44)","2bf96b46":"k_fold = KFold(n_splits=3)\nmodel = xgb.XGBRegressor(objective=\"reg:squarederror\")\nxgb_params = {'colsample_bytree': [0.3], \n              'max_depth': [5], \n              'n_estimators': [100]}\nGS = GridSearchCV(model, param_grid=xgb_params, scoring='neg_mean_squared_error', cv=k_fold)\nGS.fit(X_train, y_train)\nGS_best = GS.best_estimator_\ny_pred = GS_best.predict(X_test)\nreduction_importance = np.mean((y_pred - y_test)**2)**(1\/2)\nprint('RMSE:', reduction_importance)\nprint('Best Parameters:', GS.best_params_)","11b0698d":"X = train.iloc[:, :-1]\ny = train.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=44)","7ebe686e":"# by using RFE to select the top 20 important features\nrfe = RFE(estimator=xgb.XGBRegressor(), n_features_to_select=20, step=10, verbose=1)\nrfe.fit(X_train, y_train)\nX.columns[rfe.support_]","26aeb3b5":"train_temp = train.iloc[:,:-1]\ntest_temp = test.iloc[:,:-1]\ntrain_r = train_temp.iloc[:, rfe.support_]\ntest_r = test_temp.iloc[:, rfe.support_]\n\ntrain_r = train_r.assign(target=train[70].to_list())\ntrain_r","6b0900f7":"X_r = train_r.iloc[:, :-1]\ny_r = train_r.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X_r, y_r, train_size=0.8, test_size=0.2, random_state=44)","bb5c9fd8":"k_fold = KFold(n_splits=3)\nmodel = xgb.XGBRegressor(objective=\"reg:squarederror\")\nxgb_params = {'colsample_bytree': [0.3], \n              'max_depth': [5], \n              'n_estimators': [100]}\nGS = GridSearchCV(model, param_grid=xgb_params, scoring='neg_mean_squared_error', cv=k_fold)\nGS.fit(X_train, y_train)\nGS_best = GS.best_estimator_\ny_pred = GS_best.predict(X_test)\nreduction_rfe = np.mean((y_pred - y_test)**2)**(1\/2)\nprint('RMSE:', reduction_rfe)\nprint('Best Parameters:', GS.best_params_)","ea995f6f":"# initiate the PCA model\ntrain_pca = train.iloc[:,:-1]\npca = PCA(n_components=0.8)\npca.fit(train_pca)\nlen(pca.components_)","d2103ad6":"# visualize it to see the elbow that means the efficiency of the prediciton quickly goes down\nvar = pca.explained_variance_ratio_\nplt.figure(figsize=(8,4))\nplt.plot(var)\nplt.xlabel('Component Index')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Elbow Method')\nplt.show()","a59bea66":"train_p = PCA(n_components=2).fit_transform(train_pca)\ntrain_p.shape","ccbcbb7e":"X_train, X_test, y_train, y_test = train_test_split(train_p, y_r, train_size=0.8, test_size=0.2, random_state=44)","7ce92dbb":"k_fold = KFold(n_splits=3)\nmodel = xgb.XGBRegressor(objective=\"reg:squarederror\")\nxgb_params = {'colsample_bytree': [0.3], \n              'max_depth': [5], \n              'n_estimators': [100]}\nGS = GridSearchCV(model, param_grid=xgb_params, scoring='neg_mean_squared_error', cv=k_fold)\nGS.fit(X_train, y_train)\nGS_best = GS.best_estimator_\ny_pred = GS_best.predict(X_test)\nreduction_rca = np.mean((y_pred - y_test)**2)**(1\/2)\nprint('RMSE:', reduction_rca)\nprint('Best Parameters:', GS.best_params_)","88cc99c0":"# visualize the RMSE numbers from different mothods\nbar_x = ['No Dimensionality Reduction', 'Reduction by Feature Importance', \n         'RFE (Recursive Feature Elimination)', 'PCA Elbow Method'] \nbar_y = [no_reduction, reduction_importance, reduction_rfe, reduction_rca]\nrounded_y = [round(num, 4) for num in bar_y]\n\nfig = px.bar(x=bar_x, y=bar_y, text=rounded_y, title='Feature Reduction Method Comparison',\n             color=rounded_y, color_continuous_scale='sunsetdark',\n             labels={'x':'Feature Reduction Method', 'y':'RMSE'}, height=500)\nfig.update_layout(title_x=0.5)\nfig.show()","9b73afe7":"# fit the dataframe with reducred features into XGBRegressor\nX_r = train_r.iloc[:, :-1]\ny_r = train_r.iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X_r, y_r, train_size=0.8, test_size=0.2, random_state=44)\nk_fold = KFold(n_splits=3)\nmodel = xgb.XGBRegressor(objective=\"reg:squarederror\")\nxgb_params = {'colsample_bytree': [0.3], \n              'max_depth': [5], \n              'n_estimators': [100]}\nGS = GridSearchCV(model, param_grid=xgb_params, scoring='neg_mean_squared_error', cv=k_fold)\nGS.fit(X_train, y_train)\nGS_best = GS.best_estimator_\ny_target = GS_best.predict(test_r)","984d3ed4":"print(len(y_target))","e9cf13d6":"# generate a .csv file\nfinal_result = df_test.assign(target=y_target)\nfinal_result[['id', 'target']].to_csv('submission.csv', index=False)\nsubmission = pd.read_csv('submission.csv')\nsubmission","edf23b6a":"The RMSE of this important selection method (0.724) is higher than the previous one (0.7226). Next, this analysis will use RFE method to reduce the features.","9621c8a0":"The outcome of using PCA to find the elbow is not quite good. The RMSE (0.7481) is higher than the previous results. It shows predicting by only two features might cause the lower accuracy of the prediction. However, it is still the most efficient way to predict since RMSE is not significantly different from the previous results.","a51453df":"By looking at the visualization, the RFE method is an ideal method to reduce features and also help get a good prediction without over-fitting.\n\n## 4.3 Output a Taget Prediction","4c8c7db8":"The outcome of using the RFE method is not bad. The RMSE of the prediction (0.7228) is just slightly higher than the result without making feature reduction (0.7226).","8a4a9014":"## 3.2 Exploratory Data Analysis Summary\nThe given dataset does not have missing data. Regarding the visualization, here are the insights below.\n\n1. There is a higher correlation between count0, count5, count7, count8, count9, count10, count11, and count12 columns.\n\n2. Regarding the bar charts and histograms, it seems these columns do not have a normal distribution.\n\nIn regards to viewpoint 2, it might cause noise for the prediction. This analysis will normalize the dataset and see what the improvement will be.\n\n# 4 Analyze\n## 4.1 Normalization\nIn this section, the analysis will normalize the dataset and apply the normalized data into XGBRegressor with the same parameters as here https:\/\/www.kaggle.com\/foolwuilin\/comparison-xgbregressor-lgbm-ridge-elasticnet\n\nThe step would be as follows.\n\n1. Get the binary coding for the categorical columns.\n2. Get normalized variables for the numeric columns\n3. Put two dataframes together and then fit the data into XGBRegressor and then evaluate the RMSE","3a890fec":"This heatmap shows the correlation coefficient between numeric variables. There is a higher correlation between count0, count5, count7, count8, count9, count10, count11, and count12 columns.","7f3dd41a":"# 5 Conclusion\nThis analysis normalized the data to improve the prediction that the previous analysis did. https:\/\/www.kaggle.com\/foolwuilin\/comparison-xgbregressor-lgbm-ridge-elasticnet\n\nThe normalization truly improves the RMSE. The RMSE goes down from 0.7232 to 0.7226.\n\nNext, this analysis found that there is an overfitting problem in the previous analysis since many columns are not important for the prediction. If there are too many features in the dataset, it will be not only inefficient but also cause noise. Thus, this analysis tried below three methods below to reduce features.\n\n1. Feature importance selected by XGBRegressor\n2. RFE method\n3. PCA Elbow method \n\nAfter trying these three methods, the outcome by using the RFE method is the most ideal that the RMSE (0.7228) is only slightly higher than the result (0.7226) without reducing feature. The reduced data has only 20 features that would be beneficial for the future operation of predicting huge data.\n\nAs for further analysis, here are the suggestions.\n\n1. Trying another algorithm might help the prediction.\n\n2. Keep tuning the parameters of models to seek a better result.\n\n# 6 Reference\nAcharya, S. (2021). What are RMSE and MAE? Towards Data Science. Retrieved from https:\/\/towardsdatascience.com\/","339d2b2a":"The elbow method shows using only 2 features to predict is most efficient.","5f913980":"It is clear that the importance dramatically drops down after feature 57. Thus, the analysis will drop the features that the importance is lower than 50. ","148798c1":"After the normalization, the prediction did get an improvement. The RMSE of the previous prediction is around 0.7232. By normalizing the data, the RMSE improves to 0.7226. The outcome is better. However, this analysis would like to do further analysis to see whether the model is over-fitted. If so, will the prediction be affected if some features are removed?\n\n## 4.2 Feature Reduction\nIn this section, this analysis will try several methods to reduce the features. The best scenario is some features are removed, and the prediction is still good. The 3 methods will be as follows.\n\n1. Use the feature importance of XGBRegressor to keep the relative important features\n2. Select the most useful features by RFE (Recursive Feature Elimination)\n3. By using principal component analysis (PCA) to visualize the best number of features that would help get the most efficient prediction result\u00a0","5220ca8d":"# 1 Introduction\n## 1.1 Summary\nThis analysis is a follow-up of a regression model selection from here. https:\/\/www.kaggle.com\/foolwuilin\/comparison-xgbregressor-lgbm-ridge-elasticnet\n\nIn the analysis, XGBRegressor is selected for the ideal prediction model due to lower MAE and RMSE scores. This analysis will try to improve the prediction and do feature reduction methods to reduce features in order not to over-fit the model. Ideally, it will be best that the prediction could be improved, and the number of features is also reduced.\n\nThe dataset is based on a real dataset dealing with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features. The purpose of this analysis is to get a predicted amount that helps a company do a better plan for the business.\n\n## 1.2 Analytics Tool and Dataset\nThis report uses Python as the analytics tool. The given dataset contains 10 categorical columns (cat0 - cat9) and 14 numeric columns with continuous variables (cont0 - cont13) despite ID and target columns. 300,000 pieces of data are used for training and testing the prediction model. 200,000 pieces of data are the subject to get the prediction.\n\n# 2 Prepare\n## 2.1 Analysis Plan\nThe analysis plan is to answer the questions.\n\n1. What if normalizing the numeric variables? Will the prediction be improved?\n2. How would feature reduction affect the result?\n3. Can this analysis get a good prediction with fewer features to avoid over-fitting?\n\n## 2.2 Performance Metrics\nThis analysis will use RMSE (Root Mean Squared Error) to evaluate the prediction. RMSE is usually used for evaluating Regression Models. The metric indicates the deviation from the actual values(Acharya, 2021). The lower the error numbers, the better accuracy.\n\n## 2.3  Method\nThe analysis follows the steps as below.\n\n1. Proceeding with an Exploratory Data Analysis.\n2. Normalize the data set and run the XGBRegressor with the same parameters as the previous analysis to see whether there is an improvement.\n3. Trying different feature reduction methods to review how the result is affected.\n4. Select the most ideal method to get the prediction.\n\n# 3 Process\n## 3.1 Exploratory Data Analysis\nThis section involves the preparation of the dataset, including data cleansing. Then, an EDA (Exploratory Data Analysis) will help understand the main characteristics of the dataset."}}