{"cell_type":{"a3ecbcef":"code","bca6a59f":"code","f2460603":"code","b6eb299f":"code","717826ef":"code","be4fb357":"code","d1c61b8d":"code","140882fd":"code","dd881e8f":"code","6b932e37":"code","aed51835":"code","22944937":"code","0481764d":"code","eafd6b24":"code","b79e7e56":"code","d30cf9b2":"code","8cebbe10":"code","f1b63ee6":"code","f55a8b69":"code","69b6fd15":"code","464a9d1a":"code","79c35668":"code","49c0fed8":"code","5acb0ba5":"code","53dc2e3f":"code","9958b0dc":"code","c5fd5a0b":"code","fd5c31ba":"code","e761af22":"code","cd7dcaf3":"code","c9e2c6f6":"code","4eacf1cb":"markdown","b36f6123":"markdown","c051f72a":"markdown","924871e3":"markdown","0c77b6fa":"markdown","091cfb38":"markdown","0463f0b1":"markdown","dd65f4c1":"markdown","32ae7eaa":"markdown","6b0b2709":"markdown","d6fc87be":"markdown","d08a082e":"markdown","5179b3ac":"markdown","0794926b":"markdown","a7c673cc":"markdown","b5a50472":"markdown","402ba0c2":"markdown","5d95b752":"markdown","0176b759":"markdown","bcd7e9bd":"markdown","2a3da0fd":"markdown","91b98c02":"markdown","f398856f":"markdown","e93d3415":"markdown","b011c91a":"markdown","5456c6e1":"markdown","4557b676":"markdown","355600c5":"markdown","5fb1c9db":"markdown","6919055e":"markdown","dab073bb":"markdown","4d7357ec":"markdown"},"source":{"a3ecbcef":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import zscore\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#Test Train Split\nfrom sklearn.model_selection import train_test_split\n#Feature Scaling library\nfrom sklearn.preprocessing import StandardScaler\n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n#ROC Curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","bca6a59f":"df_bank = pd.read_csv(\"..\/input\/bank-personal-loan-modellingcsv\/Bank_Personal_Loan_Modelling .csv\")","f2460603":"# 1. Read the column description and ensure you understand each attribute well  \ndf_bank.shape # Check number of columns and rows in data frame","b6eb299f":"df_bank.dtypes","717826ef":"df_bank.isnull().values.any() # If there are any null values in data set","be4fb357":"df_bank.head() # To check first 5 rows of data set","d1c61b8d":"#Using t test to test attributes('Age','Experience','Income','ZIP Code','CCAvg','Mortgage') \n# individually make any difference to customers taking the loan.\n\ncols =['Age','Experience','Income','ZIP Code','CCAvg','Mortgage']\nfor i in np.arange(len(cols)):\n    tx = df_bank[df_bank['Personal Loan'] == 1][cols[i]]  # taken Personal Loan\n    ty = df_bank[df_bank['Personal Loan'] == 0][cols[i]]  # not taken Personal Loan\n    t_stat, pval = stats.ttest_ind(tx,ty, axis = 0)\n    if pval < 0.05:\n        print(cols[i], f' individually impacts customer behaviour as the p_value {round(pval,4)} < 0.05')\n    else:\n        print(cols[i], f' individually does not impact customer behaviour as the p_value {round(pval,4)} > 0.05')","140882fd":"# Chi_square test to check if the categorical\/boolean variables individually make any difference to customers taking the loan\ncols =['Family','Education','Securities Account','CD Account','Online','CreditCard']\nfor i in np.arange(len(cols)):\n    crosstab=pd.crosstab(df_bank['Personal Loan'],df_bank[cols[i]])\n    chi, p_value, dof, expected =  stats.chi2_contingency(crosstab)\n    if p_value < 0.05:\n         print(cols[i], f' individually impacts customer behaviour as the p_value {round(p_value,4)} < 0.05')\n    else:\n         print(cols[i], f' individually does not impact customer behaviour as the p_value {round(p_value,4)} > 0.05')  ","dd881e8f":"# 2. Study the data distribution in each attribute, share your findings  (15 marks)\n\ndf_bank.describe()","6b932e37":"# studying the distribution of continuous attributes\ncols = ['Age','Experience','Income','ZIP Code','CCAvg','Mortgage']\nfor i in np.arange(len(cols)):\n    sns.distplot(df_bank[cols[i]], color='blue')\n    #plt.xlabel('Experience')\n    plt.show()\n    print('Distribution of ',cols[i])\n    print('Mean is:',df_bank[cols[i]].mean())\n    print('Median is:',df_bank[cols[i]].median())\n    print('Mode is:',df_bank[cols[i]].mode())\n    print('Standard deviation is:',df_bank[cols[i]].std())\n    print('Skewness is:',df_bank[cols[i]].skew())\n    print('Maximum is:',df_bank[cols[i]].max())\n    print('Minimum is:',df_bank[cols[i]].min())","aed51835":"# Distribution of categorical columns 'Family','Education','Securities Account','CD Account','Online'and \n#'CreditCard' individually against 'Personal Loan'\ncols =['Family','Education','Securities Account','CD Account','Online','CreditCard']\nfor i in np.arange(len(cols)):\n    sns.countplot(x= df_bank[cols[i]],data=df_bank,hue='Personal Loan')\n    plt.show()\n    # calculating counts\n    print(pd.pivot_table(data=df_bank,index='Personal Loan',columns=[cols[i]],aggfunc='size'))                                                   \n   ","22944937":"#plt.figure(figsize = (50,50))\nsns.pairplot(df_bank)\n","0481764d":"# studying correlation between the attributes\nb_corr=df_bank.corr()\nplt.subplots(figsize =(12, 7)) \nsns.heatmap(b_corr,annot=True)","eafd6b24":"# Identify the true and false for Personal Loan\nloan_true = len(df_bank.loc[df_bank['Personal Loan'] == 1])\nloan_false = len(df_bank.loc[df_bank['Personal Loan'] == 0])\n\nprint (f\"\\nCustomer percentage who accepted loan offer: {loan_true} in total {len(df_bank)} which is {loan_true\/len(df_bank)*100}%\")\nprint (f\"Customer percentage who did not accept loan offer: {loan_false} in total {len(df_bank)} which is  {loan_false\/len(df_bank)*100}%\")\n\n# Distribution of 'Personal Loan'\nsns.countplot(x= 'Personal Loan',data=df_bank)\nplt.show()\nprint('Counts when Personal Loan is:\\n',df_bank['Personal Loan'].value_counts())","b79e7e56":"# Checking the presence of outliers\nl = len(df_bank)\ncol = ['Age','Income','ZIP Code','CCAvg','Mortgage']\nfor i in np.arange(len(col)):\n    sns.boxplot(x= df_bank[col[i]], color='cyan')\n    plt.show()\n    print('Boxplot of ',col[i])\n    #calculating the outiers in attribute \n    Q1 = df_bank[col[i]].quantile(0.25)\n    Q2 = df_bank[col[i]].quantile(0.50)\n    Q3 = df_bank[col[i]].quantile(0.75)\n    print('Q1 is : ',Q1)\n    print('Q2 is : ',Q2)\n    print('Q3 is : ',Q3)\n    IQR = Q3 - Q1\n    print('IQR is:',IQR)\n    bools = (df_bank[col[i]] < (Q1 - 1.5 *IQR)) |(df_bank[col[i]] > (Q3 + 1.5 * IQR))\n    print('Out of ',l,' rows in data, number of outliers are:',bools.sum())   #calculating the number of outliers\n    n_zeros = len(df_bank.loc[df_bank[col[i]] == 0])\n    print(' Number of zeros in',col[i],'is ', n_zeros)\n    lw_tc = len(df_bank.loc[df_bank[col[i]] < (Q1 - 1.5 *IQR)])  # Total number less than Lower Whisker\n    print(' Total number less than Lower Whisker',col[i],'is ', lw_tc)\n    uw_tc = len(df_bank.loc[df_bank[col[i]] > (Q3 + 1.5 * IQR)]) # Total number more than Upper Whisker\n    print(' Total number more than Upper Whisker',col[i],'is ', uw_tc)\n    total_c = l - (n_zeros+lw_tc+uw_tc)                         # Total non zero data within IQR is\n    print(' Total non zero data within IQR is :',total_c )\n    print(' Number of records in Outlier where Personal loan was taken', len(df_bank.loc[(df_bank[col[i]] > (Q3 + 1.5 * IQR)) & (df_bank['Personal Loan'] == 1)]))","d30cf9b2":"#Dropping column 'ID' and 'Experience'\n# ID is just an identifier for the row and has no contrbution to personal loan, so we drop it.\ndf_bank.drop('ID',axis=1,inplace=True) \n","8cebbe10":"# Experience is in high correlation with 'Age', so the contribution of both these columns towards personal loan \n# would be same, hence dropping 'Experience'. Since, we are dropping 'Experience' no need to correct it for negative values.\ndf_bank.drop('Experience',axis=1,inplace=True)","f1b63ee6":"# shows 'Age' and 'Experience' have been dropped\ndf_bank.dtypes","f55a8b69":"col = ['Income','ZIP Code','CCAvg','Mortgage']\nfor i in np.arange(len(col)):\n    df_bank['zscore']= np.abs(stats.zscore(df_bank[col[i]]))\n    df_bank= df_bank[df_bank['zscore']<= 3]\n    df_bank.drop('zscore',axis=1,inplace=True) #Rows where col[i] was in outliers have been dropped ","69b6fd15":"X_bank = df_bank.drop(['Personal Loan'], axis=1)\nX = df_bank.drop(['Personal Loan'], axis=1)\ny = df_bank['Personal Loan']","464a9d1a":"# 4. Split the data into training and test set in the ratio of 70:30 respectively \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","79c35668":"#since we have just 480 records in 'Personal Loan' column, verifying to ensure there are some records with 'Personal Loan' = 1 for training and testing both.\nprint('Number of records in y_train with values 0 & 1 are:\\n',y_train.value_counts())\nprint('Number of records in y_test with values 0 & 1 are:\\n',y_test.value_counts())","49c0fed8":"# Fit model on the Train-set\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict Test-set\n#y_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n\n# Coefficient and intercept of model\ncoef = pd.DataFrame(logreg.coef_)\ncoef['intercept'] = logreg.intercept_\nprint('\\n\\nCoefficient :',coef)\n\n# Score of model\nmodel_score = logreg.score(X_test,y_test)\nprint('\\nScore of the model '+str(model_score))\n\nprint('\\nLogistic Regression Classification report:\\n',classification_report(y_test, logreg.predict(X_test)))\n\nLGRcm_matrix = metrics.confusion_matrix(y_test,logreg.predict(X_test))\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test,logreg.predict(X_test)))","5acb0ba5":"#true positives (TP): These are cases in which we predicted yes, and actually took loan.\nTP=58\n#true negatives (TN): We predicted no, and they actually did not took loan.\nTN=1289\n#false positives (FP): We predicted yes, but they don't actually took loan.(Also known as a \"Type I error.\")\nFP=22\n#false negatives (FN): We predicted no, but they actually took loan.(Also known as a \"Type II error.\")\nFN=64\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of logistic regression classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('Logistic regression Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('Logistic regression Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('Logistic regression Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('Logistic regression Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\n#Area Under the ROC Curv\nprint('Logistic regression AUC: ',round(roc_auc_score(y_test,logreg.predict(X_test))*100))\n\n\n#ROC Curve\n\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()\n\n#----------------------------------------------------------------------------------------------------------------\n# For better understanding of confusion matrix when 3 nearest Neighbours we plot it on heatmap\n\nLGRcm_matrix = metrics.confusion_matrix(y_test,logreg.predict(X_test))\n\nreport_LGR = classification_report(y_test,logreg.predict(X_test),labels=[1,0])\n","53dc2e3f":"#Converting all attributes to z-score\nXScaled = X.apply(zscore)\n\n# 2) Spliting randomly in train and test set\nX_train1,X_test1,y_train1,y_test1 = train_test_split(XScaled, y, test_size=0.30, random_state=1)","9958b0dc":"# Call Nearest Neighbour algorithm, keeping number of neighbours as 3\nNNH = KNeighborsClassifier(n_neighbors= 3 , weights = 'uniform' )\nNNH.fit(X_train1, y_train1)\n\n# Score of the Model\nprint('NNH score when 3 nearest neighbours:', NNH.score(X_test1, y_test1))\n\n\n# For every test data point, predict it's label based on 3 nearest neighbours in this model. The majority class will \n# be assigned to the test data point\n\nprint('\\nConfusion metrics when 3 nearest neighbour:\\n', metrics.confusion_matrix(y_test1,NNH.predict(X_test1)))\n\nprint('\\nAUC: ',roc_auc_score(y_test1, NNH.predict(X_test1)*100))\n\n#-------------------------------------------------------------------------------------------------------------------\n#Iteration 1:\n# Call Nearest Neighbour algorithm, keeping number of neighbours as 5\nNNH1 = KNeighborsClassifier(n_neighbors= 5 , weights = 'uniform' )\nNNH1.fit(X_train1, y_train1)\n\n# Score of the Model\nprint('\\nNNH score when 5 nearest neighbours :', NNH1.score(X_test1, y_test1))\n\n# For every test data point, predict it's label based on 5 nearest neighbours in this model. The majority class will \n# be assigned to the test data point\n\nprint('\\nConfusion metrics when 5 nearest neighbour:\\n',metrics.confusion_matrix(y_test1,NNH1.predict(X_test1)))\n\nprint('\\nAUC: ',roc_auc_score(y_test1, NNH1.predict(X_test1)*100))\n\n#-----------------------------------------------------------------------------------------------------------------\n\n#Iteration 2:\n\n# Call Nearest Neighbour algorithm, keeping number of neighbours as 9\nNNH2 = KNeighborsClassifier(n_neighbors= 9, weights = 'uniform' )\nNNH2.fit(X_train1, y_train1)\n\nprint('\\nNNH score when 9 nearest neighbours :', NNH2.score(X_test1, y_test1))\n\nprint('\\nConfusion metrics when 9 nearest neighbour:\\n',metrics.confusion_matrix(y_test1,NNH2.predict(X_test1)))\n\nprint('\\nAUC: ',roc_auc_score(y_test1, NNH2.predict(X_test1)*100))\n\n#----------------------------------------------------------------------------------------------\n#Classification reports\n\nprint('\\nClassification report for 3 nearest neighbour\\n',classification_report(y_test1,NNH.predict(X_test1)))\nprint('\\nClassification report for 5 nearest neighbour\\n',classification_report(y_test1,NNH2.predict(X_test1)))\nprint('\\nClassification report for 9 nearest neighbour\\n',classification_report(y_test1,NNH2.predict(X_test1)))\n\nreport_KNN = classification_report(y_test1,NNH.predict(X_test1))","c5fd5a0b":"#Since AUC and NNH score is more for 3 nearest neighbour calculating metrices\n\n#Confusion metrics when 3 nearest neighbour:\n# [[1307    3]\n# [  44   80]]\n\n#true positives (TP): These are cases in which we predicted yes, and actually took loan.\nTP=80\n#true negatives (TN): We predicted no, and they actually did not took loan.\nTN=1307\n#false positives (FP): We predicted yes, but they don't actually took loan.(Also known as a \"Type I error.\")\nFP=3\n#false negatives (FN): We predicted no, but they actually took loan.(Also known as a \"Type II error.\")\nFN=44\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of KNN classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('KNN Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('KNN Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('KNN Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('KNN Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\nprint('KNN AUC: ',roc_auc_score(y_test1, NNH.predict(X_test1)*100))\n\n#ROC Curve\n\nKNN_roc_auc = roc_auc_score(y_test1, NNH.predict(X_test1))\nfpr, tpr, thresholds = roc_curve(y_test1, NNH.predict_proba(X_test1)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='KNN (area = %0.2f)' % KNN_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n#plt.savefig('Log_ROC')\nplt.show()\n\n#----------------------------------------------------------------------------------------------------------------\n# For better understanding of confusion matrix when 3 nearest Neighbours we plot it on heatmap\n\nKNNcm_matrix = metrics.confusion_matrix(y_test1,NNH.predict(X_test1))\n\nreport_KNN = classification_report(y_test1,NNH.predict(X_test1),labels=[1,0])","fd5c31ba":"#Iteration 1 - Fitting all variables, cleaned and normalized data\nGNB1 = GaussianNB()\nGNB1.fit(X_train, y_train)\n\n# Score of the Model\nprint('GNB score :', GNB1.score(X_test, y_test))\n\n\n# For every test data point, predict it's label based on 3 nearest neighbours in this model. The majority class will \n# be assigned to the test data point\n\nprint('\\nConfusion metrics :\\n', metrics.confusion_matrix(y_test, GNB1.predict(X_test)))\n\nprint('\\nAUC: ',roc_auc_score(y_test, GNB1.predict(X_test)*100))","e761af22":"#CALCULATING METRICES FOR CHECING MODEL\n\n#Confusion metrics :\n# [[1224   87]\n# [  50   73]]\n\n#true positives (TP): These are cases in which we predicted yes, and actually took loan.\nTP=73\n#true negatives (TN): We predicted no, and they actually did not took loan.\nTN=1224\n#false positives (FP): We predicted yes, but they don't actually took loan.(Also known as a \"Type I error.\")\nFP=87\n#false negatives (FN): We predicted no, but they actually took loan.(Also known as a \"Type II error.\")\nFN=50\n\n\nAccuracy=(TP+TN)\/(TP+TN+FP+FN)\nprint('Accuracy of GNB classifier on test set: {:.2%}'.format(Accuracy))\n\nMisclassification_Rate=(FP+FN)\/(TP+TN+FP+FN)\nprint('GNB Misclassification Rate: It is often wrong: {:.2%}'.format(Misclassification_Rate))\n\n#Recall\nSensitivity=TP\/(FN+TP)\nprint('GNB Sensitivity: When its actually yes how often it predicts yes: {:.2%}'.format(Sensitivity))\n\nSpecificity=TN\/(TN+FP)\nprint('GNB Specificity: When its actually no, how often does it predict no: {:.2%}'.format(Specificity))\n\nPrecision=TP\/(FP+TP)\nprint('GNB Precision: When it predicts yes, how often is it correct: {:.2%}'.format(Precision))\n\nprint('GNB AUC: ',roc_auc_score(y_test, GNB1.predict(X_test)*100))\n#ROC Curve\nGNB_roc_auc = roc_auc_score(y_test, GNB1.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, GNB1.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='GNB (area = %0.2f)' % GNB_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n#plt.savefig('Log_ROC')\nplt.show()\n#-------------------------------------------------------------------------------------------------------------\n\nGNBcm_matrix = metrics.confusion_matrix(y_test, GNB1.predict(X_test))\n\nreport_GNB = classification_report(y_test,GNB1.predict(X_test),labels=[1,0])","cd7dcaf3":" \n# For better understanding of logistic regression confusion matrix we plot it on heatmap\n\nHM = pd.DataFrame(LGRcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(7,5))\nprint('Confusion matrix for logistic regression ')\nsns.heatmap(HM,annot=True, fmt='g')\nplt.show()\n\n# For better understanding of KNN confusion matrix we plot it on heatmap\nHM = pd.DataFrame(KNNcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(7,5))\nprint('KNN confusion matrix\\n')\nsns.heatmap(HM,annot=True, fmt='g') \nplt.show()\n\n# For better understanding of GNB confusion matrix we plot it on heatmap\nHM = pd.DataFrame(GNBcm_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\nplt.figure(figsize=(7,5))\nprint('GNB confusion matrix\\n')\nsns.heatmap(HM,annot=True, fmt='g') \nplt.show()","c9e2c6f6":"print(\"\\nLogisitic Regression\\n\",report_LGR)\nprint(\"\\nNaive Bayes\\n\",report_GNB)\nprint(\"\\nKNN Classifier\\n\",report_KNN)","4eacf1cb":"1. We have 13 independent and 1 dependent variable('Personal Loan')\n2. \u2018ID\u2019,\u2018Age\u2019,\u2018Experience\u2019,\u2018Income\u2019,\u2018CC_Avg\u2019,\u2018Mortgage\u2019,\u2018Zip_Code\u2019 columns have continuous data\n3. \u2018Education\u2019, \u2018Family\u2019, \u2018Personal_Loan\u2019,\u2018Securities Account\u2019,'CD_Account\u2019,\u2018Online\u2019,\u2018Credit_Card\u2019 \n    are categorical variables with numeric data","b36f6123":"Context: \n\nThis case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget. \n\nAttribute Information: \n\uf0b7 \nID           : Customer ID                                                                           \uf0b7 \nAge         : Customer's age in completed years \uf0b7 \nExperience  : #years of professional experience   \uf0b7 \nIncome   : Annual income of the customer ($000) \uf0b7 \nZIP Code  : Home Address ZIP code. \uf0b7 \nFamily  : Family size of the customer  \uf0b7 \nCCAvg   : Avg. spending on credit cards per month ($000)  \uf0b7 \nEducation  : Education Level.  1. Undergrad 2. Graduate 3. Advanced\/Professional  \uf0b7 \nMortgage  : Value of house mortgage if any. ($000) \nPersonal Loan : Did this customer accept the personal loan offered in  the last campaign? \uf0b7 \nSecurities Account : Does the customer have a securities account with the bank? \uf0b7 \nCD Account  : Does the customer have a certificate of deposit (CD)    account with the bank?  \uf0b7 \nOnline  : Does the customer use internet banking facilities? \uf0b7\nCredit card  : Does the customer use a credit card issued by    Thera Bank? \n\nObjective:\n\nThe classification goal is to predict the likelihood of a liability customer buying personal loans.","c051f72a":"#Model has improved with KNN and n_neighbours as 9, also ROC and AUC have improved","924871e3":"# KNN Model","0c77b6fa":"##So, overall Income, Family, CCAvg, Education, Mortgage, CD Account impact customer behaviour in taking loan.","091cfb38":"Interpretations from Correlation matrix -\n1.There is a high correlation between eperience and age\n2. There is medium correlation between income and average spending on credit cards per month.\n3. There is medium correlation of personal loan with Income, average spending on credit cards per month and \n   certificate of deposit (CD)  account with the bank is also to be considered. \n4. There is low correlation of personal loan with Online,securities account and CreditCard  \n\n","0463f0b1":"Interpretations from the distribution above -\n\n1. Mean age of the customers is 45 with standard deviation of 11.5.The curve is slightly negatively skewed \n2. Mean Experience is 20 and the curv is slightly negatively skewed. Experience seems to have negative values, \n   which is an error and needs correction.The curve is moderately posiitively skewed. \n3. Mean Income is 73k with standard deviation of 46.\n4. Mean of Zipcode is 93152.503 with standard deviation of 2122. The curve is negatively skewed.    \n5. The mean of Avg. spending on credit cards per month is 1.93 with standard deviation of 1.75. \n   The curve is highly positive skewed.\n6. The mean value of house mortgage is 56.5 with standard deviation of 101.71. The curve is highly positive skewed\n7. No need for further analysis of \u2018ID\u2019, \u2018ZIP_Code\u2019 & \u2018Experience\u2019 columns, since \u2018ID\u2019 and \u2018ZIP_Code\u2019 are number of series & \u2018Experience\u2019 is highly correlated with \u2018Age\u2019.\n\n","dd65f4c1":"# Based on Hypothesis we predict the attributes which have effect on customer propensity to take the Personal Loan","32ae7eaa":"Interpretations from the data description  \n1. Id is the unique identifier for each row.\n2. The youngest customer is 23 years of age and the oldest is 67.\n3. Experience shows to have a negative minimum value which is an error, approximately 50% of the customers have experience of 20 years and the oldest as 43 years\n4. Minimum income of customer is $8K and maximum is $224k.75% of the cutomers are on or below the $98k income.\n5. Mostly customers belong to the area in zip code between 91911 and 96651.\n6. Minimum there is 1 person in the family, 50% customers have 2 people in their family or less and maximum there are 4 people.\n7. 50% of customers have average spending of 1.5K or below but the mamimum goes upto 10k.\n8. 50% of the customers are either under graduates or graduates, the remaining are Advanced\/Professional\n9. 50% of the customers have no house mortgage, 75% have 101k or below and the maximum house mortgage is 635k.\n10. 90% of the customers have no personal loan\n11. 89% of the customers have no no securities account \n12. 93% of the customers have no CD account\n13. 60% of the customers use internet banking facilities.\n14. 70% of the customer does not use the credit card issed by the bank.","6b0b2709":"The outliers in \n1. Age are 0\n2. Income are 96 \n3. Zip Code is 1\n4. CCAvg are 324\n5. Mortgage are 291","d6fc87be":"![](http:\/\/)# Data Splitting ","d08a082e":"# Logistic Regression Model Fitting\n","5179b3ac":"#Choosing the best model\n\n---------------------------------Logistic Regression-----------------------------------------\n\nAccuracy of logistic regression classifier on test set: 94.00%\nLogistic regression Misclassification Rate: It is often wrong: 6.00%\nLogistic regression Sensitivity: When its actually yes how often it predicts yes: 47.97%\nLogistic regression Specificity: When its actually no, how often does it predict no: 98.32%\nLogistic regression Precision: When it predicts yes, how often is it correct: 72.84%\nLogistic regression AUC:  73.0\nLogistic Confusion metrics :\n [[1289   22]\n [  64   59]]\n-----------------------------------KNN--------------------------------------------\n\nAccuracy of KNN classifier on test set: 96.72%\nKNN Misclassification Rate: It is often wrong: 3.28%\nKNN Sensitivity: When its actually yes how often it predicts yes: 64.52%\nKNN Specificity: When its actually no, how often does it predict no: 99.77%\nKNN Precision: When it predicts yes, how often is it correct: 96.39%\nKNN AUC:  0.8214356069933514\n\nKNN Confusion Matrix\n [[1307    3]\n [  44   80]]\n\n----------------------------Gaussian Naive Bayes----------------------------------\n\nAccuracy of GNB classifier on test set: 90.45%\nGNB Misclassification Rate: It is often wrong: 9.55%\nGNB Sensitivity: When its actually yes how often it predicts yes: 59.35%\nGNB Specificity: When its actually no, how often does it predict no: 93.36%\nGNB Precision: When it predicts yes, how often is it correct: 45.62%\nGNB AUC:  0.7635671894476382\n\nGNB Confusion Matrix\n [[1224   87]\n [  50   73]]","0794926b":"# Applying Gaussian Naive Bayes","a7c673cc":"# Model Implementation\n5. Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans","b5a50472":"# KNN model performs the best, evident from results above. \n\nAs we are building model for prediction who will take personal loan, we must predict more true postive rate value(Recall aka Sensitivity) beacuse we are considering who will take personal loan and less the False Negative value.\n\nSensitivity in KNN is around 64.52% , which is more than the other models. Other metrices are also better.\nWe end up offering more loans at end of program, if we use this model and target only those predicted as yes first(since it has 96.39% Precision), chances are they will take loan.","402ba0c2":"Interpretations From the graphs above, it seems \n1. The customers with bigger family size tend to take more personal loan than family with smaller size \n2. The customers with higher education tend to take more personal loan than others \n3. The customers with no security account tend to take more personal loan than others \n4. The customers with no CD Account tend to take more personal loan than others \n5. The customers with Online access tend to take more personal loan than others\n6. The customers with no CreditCard tend to take more personal loan than others","5d95b752":"#6. Print the confusion matrix for all the models","0176b759":"# Scaling and Splitting dataset for KNN","bcd7e9bd":"7. Give your reasoning on which is the best model in this case and why it performs better? ","2a3da0fd":"1. Sensitivity is too low, which means the model needs more data to train itself. Since Personal Loan column has only\n480 records which mention the loan was issued.\n2. Specificity seems fine\n3. More the area under ROC better the model performs, its 0.73 in this case. ","91b98c02":"There are total of 5000 rows and 14 columns in the data ","f398856f":"Interpretations from pair plots -\n\n1. There is linear correlation between 'Age' and 'Experience'\n2. Very low correlation of Income with Age and Experience\n3. Income has positive correlation with CCavg and Mortgage which looks like scattered right angle triangle \n4. Age and Experience have no correlation with CCavg and Mortgage\n5. There are more families with family size of 1\n6. 'Mortgage' data is highly skewed,approximately 50% of the data is zero\n7. Personal Loan, Securities Account, CD Account, Online and CreditCard have values along 2 hortizontal lines (strip plots) with 'Age','Experience','Income','ZIP Code','CCAvg','Mortgage' respectively and vice versa when lines become vertical.\n","e93d3415":"Income, CCAvg, Mortgage impact customer behaviour in taking loan since their p value <0.05","b011c91a":"3. Get the target column distribution. \n\n'Personal Loan' is the Target column .\nWith all the analysis above, we are studying how each attribute contributes towards customer propensity to take the Personal Loan.\n\n","5456c6e1":"# # Banking Data Analysis for Personal Loan Campaign","4557b676":"# Identifying outliers","355600c5":"# We are doing a Chi_square test on individual attributes('Family','Education','Securities Account','CD Account','Online','CreditCard') to see if they individually make any difference to customers taking the loan.\n\nHo = Attribute has no effect on customer propensity to take the Personal Loan\nHa = Attribute has an effect on customer propensity to take the Personal Loan ","5fb1c9db":"# Data cleaning ","6919055e":"# We are doing a t test on individual attributes('Age','Experience','Income','ZIP Code','CCAvg','Mortgage') to see if they individually make any difference to customers taking the loan.\nHo = Attribute has no effect on customer propensity to take the Personal Loan\nHa = Attribute has an effect on customer propensity to take the Personal Loan ","dab073bb":"Removing outliers in Income, Zip Code, CCAvg and Mortgage using zscore","4d7357ec":"Family, Education,CD Account impact customer behaviour in taking loan since their p value <0.05"}}