{"cell_type":{"212d718d":"code","0a3ebc4b":"code","99161ba7":"code","5065fa89":"code","893e92b4":"code","729dd292":"code","4b8d9b66":"code","12404411":"code","917208f7":"code","b5012134":"code","c2cb88c4":"code","f3f14deb":"code","ac99f333":"code","2e6ed2b4":"code","2c1cb499":"code","dd62c1b4":"code","4ba82188":"code","fa659ae9":"code","2c05a181":"code","9a2a3b43":"code","bc6a911e":"code","ff14a65a":"code","d8fe37c0":"code","ca016a2c":"code","ceb87bd4":"code","2e99a842":"code","b9732f69":"code","80f6d177":"code","d467c999":"code","0475948f":"code","aff672fb":"code","05ee74ca":"code","4e862fff":"code","991ad346":"code","cf01ea64":"code","176bafc4":"code","cfd4ab0c":"code","38caf9b6":"code","d7a59dd6":"code","2a07edfd":"code","a319d2ec":"code","16629f2a":"code","d2d9ce74":"code","e7adeef5":"code","c5ceef04":"code","c0f96042":"code","5374248f":"code","5b2dadf3":"markdown","a5b78db3":"markdown","75375563":"markdown","87b1200d":"markdown","28ec881b":"markdown","0a6d870e":"markdown","6fcfecd6":"markdown","faff168e":"markdown","0428086e":"markdown","5b7c13f5":"markdown","7858cb54":"markdown","ae4ab728":"markdown","1cad1a7a":"markdown","08495c0d":"markdown","e55cc0b5":"markdown","7a8fcf73":"markdown","dc3cbbfb":"markdown","d6b95f95":"markdown","1656e588":"markdown","6b300ccb":"markdown","e1adff5a":"markdown","4ba9e28c":"markdown","98d66afc":"markdown","3fecbc97":"markdown","da41f6b9":"markdown","876c75c7":"markdown","08bf022c":"markdown","57188745":"markdown","4d2c7022":"markdown","3435accf":"markdown","83315544":"markdown","11d801d8":"markdown","124ee886":"markdown","5ab12332":"markdown","ad247cfe":"markdown","656066b7":"markdown","85822737":"markdown","294e5e77":"markdown","5d4568a3":"markdown","4fe22fc0":"markdown","56314a17":"markdown","705260d3":"markdown","154ce185":"markdown","d5723e99":"markdown","73bca3d3":"markdown","0a259d5a":"markdown","ce26000d":"markdown","534ea77b":"markdown","678e4ed7":"markdown","0ed1056f":"markdown","8cf9c531":"markdown","291e1f44":"markdown","5ec353ba":"markdown","fe63f643":"markdown","b5411ed3":"markdown","02c94652":"markdown","a9dc080a":"markdown","b1549066":"markdown"},"source":{"212d718d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numba import njit\n\n#VISUALIZATION\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nimport cv2 as cv\n\n#MACHINELEARNING\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom glob import glob\nfrom skimage import io\nfrom os import listdir\nimport pickle\n\nimport time\nimport copy\nfrom tqdm.notebook import tqdm\ntqdm().pandas();5\n\nprint('import complete')","0a3ebc4b":"files = listdir(\"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\")\nprint(len(files))","99161ba7":"files[0:10]","5065fa89":"patient_file = listdir(\"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\")\nprint(len(patient_file))","893e92b4":"patient_file","729dd292":"class_0 = listdir('..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\/0')\nclass_1 = listdir('..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\/1')","4b8d9b66":"from pprint import pprint\nprint('Class 0 files:\\n ')\npprint(class_0[0:10])","12404411":"print('Class 1 files:\\n ')\npprint(class_1[0:10])","917208f7":"base_path = '..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/'\npatient_ids = listdir(base_path)","b5012134":"class_0_total = 0\nclass_1_total = 0\nfrom pprint import pprint\nfor patient_id in patient_ids:\n    class_0_files = listdir(base_path + patient_id + '\/0')\n    class_1_files = listdir(base_path + patient_id + '\/1')\n\n    class_0_total += len(class_0_files)\n    class_1_total += len(class_1_files) \n\ntotal_images = class_0_total + class_1_total\n    \nprint(f'Number of patches in Class 0: {class_0_total}')\nprint(f'Number of patches in Class 1: {class_1_total}')\nprint(f'Total number of patches: {total_images}')\n","c2cb88c4":"columns = [\"patient_id\",'x','y',\"target\",\"path\"]\ndata_rows = []\ni = 0\niss = 0\nisss = 0\n\n# note that we loop through the classes after looping through the \n# patient ids so that we avoid splitting our data into [all class 0 then all class 1]\nfor patient_id in patient_ids:\n    for c in [0,1]:\n        class_path = base_path + patient_id + '\/' + str(c) + '\/'\n        imgs = listdir(class_path)\n        \n        # Extracting Image Paths\n        img_paths = [class_path + img + '\/' for img in imgs]\n        \n        # Extracting Image Coordinates\n        img_coords = [img.split('_',4)[2:4] for img in imgs]\n        x_coords = [int(coords[0][1:]) for coords in img_coords]\n        y_coords = [int(coords[1][1:]) for coords in img_coords]\n\n        for (path,x,y) in zip(img_paths,x_coords,y_coords):\n            values = [patient_id,x,y,c,path]\n            data_rows.append({k:v for (k,v) in zip(columns,values)})\n# We create a new dataframe using the list of dicts that we generated above\ndata = pd.DataFrame(data_rows)\nprint(data.shape)\ndata.head()","f3f14deb":"cancer_perc = data.groupby(\"patient_id\").target.value_counts()\/ data.groupby(\"patient_id\").target.size()\ncancer_perc = cancer_perc.unstack()\n\nfig, ax = plt.subplots(1,3,figsize=(25,5))\n\n# Plotting Frequency of Patches per Patient\nsns.distplot(data.groupby(\"patient_id\").size(), ax=ax[0], color=\"Orange\", kde=False, bins=15)\nax[0].set_xlabel(\"Number of patches\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"How many patches do we have per patient?\")\n\n# Plotting Percentage of an image that is covered by Invasive Ductile Carcinoma\nsns.distplot(cancer_perc.loc[:, 1]*100, ax=ax[1], color=\"Tomato\", kde=False, bins=15)\nax[1].set_title(\"How much percentage of an image is covered by IDC?\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"% of patches with IDC\")\n\n# Plotting number of patches that show IDC\nsns.countplot(data.target, palette='pastel', ax=ax[2]);\nax[2].set_ylabel(\"Count\")\nax[2].set_xlabel(\"no(0) versus yes(1)\")\nax[2].set_title(\"How many patches show IDC?\");","ac99f333":"# Return random sample indexes that are cancer positive and cancer negative of size 50\n# replace = False means that no duplication is allowed\npositive_tissue = np.random.choice(data[data.target==1].index.values, size=100, replace=False)\nnegative_tissue = np.random.choice(data[data.target==0].index.values, size=100, replace=False)\n\nn_rows = 10\nn_cols = 10","2e6ed2b4":"fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indexes\n        idx = positive_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        ax[row,col].imshow(img[:,:,:])\n        ax[row,col].grid(False)","2c1cb499":"fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indices\n        idx = negative_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        ax[row,col].imshow(img[:,:,:])\n        ax[row,col].grid(False)","dd62c1b4":"def get_patient_df(patient_id):\n    return data.loc[data['patient_id']== patient_id,:]","4ba82188":"n_rows = 5\nn_cols = 3\nn_imgs = n_rows*n_cols\ncolors = ['pink', 'purple']\n\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\npatient_ids = np.random.choice( data.patient_id.unique(), size=n_imgs, replace=False)\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        patient_id = patient_ids[col + n_cols*row]\n        patient_df = get_patient_df(patient_id)\n        \n        ax[row,col].scatter(patient_df.x.values, \\\n                            patient_df.y.values, \\\n                            c=patient_df.target.values,\\\n                            cmap=ListedColormap(colors), s=20)\n        ax[row,col].set_title(\"patient \" + patient_id)","fa659ae9":"def visualise_breast_tissue(patient_id, df = data,pred = False, crop_dimension = [50,50]):\n    # Plotting Settings\n    plt.xticks([])\n    plt.yticks([])\n    # Get patient dataframe\n    p_df = get_patient_df(patient_id)\n    # Get the dimensions of the breast tissue image\n    max_coord = np.max((*p_df.x,*p_df.y))\n    # Allocate an array to fill image pixels in,use uint8 type as you don't need an int over 255\n    grid = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    mask = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    # Replace array values with values of the image\n    for x,y,target,path in zip(p_df['x'],p_df['y'],p_df['target'],p_df['path']):\n        try:\n            img = io.imread(path)\n            # Replace array values with cropped image values\n            grid[y:y+crop_dimension[1],x:x+crop_dimension[0]] = img\n            # Check if target is cancerous or not\n            if target != 0:\n                # If the target is cancerous then, replace array values with the color blue\n                mask[y:y+crop_dimension[1],x:x+crop_dimension[0]] = [0,0,255]\n        except: pass\n    # if prediction is not specifies then show the image normally\n    if pred == False:\n        io.imshow(grid)\n        img = grid\n    # if prediction is specified then apply a mask to the areas that contain predicted cancerous cells\n    else:\n        # Specify the desired alpha value\n        alpha = 0.78\n        # This is step is very important, adding 2 numpy arrays sets the values to float64, which is why convert them back to uint8\n        img = (mask * (1.0 - alpha) + grid * alpha).astype('uint8')\n        io.imshow(img)\n    return img","2c05a181":"n_rows = 5\nn_cols = 3\nn_imgs = n_rows*n_cols\n\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\nfor row in range(n_rows):\n    for col in range(n_cols):\n        p_id = patient_ids[col + n_cols*row]\n        \n        img = visualise_breast_tissue(p_id, pred = True)\n        ax[row,col].grid(False)\n        ax[row,col].set_xticks([])\n        ax[row,col].set_yticks([])\n        ax[row,col].set_title(\"Breast tissue slice of patient: \" + p_id)        \n        ax[row,col].imshow(img)","9a2a3b43":"def get_classes_split(series):\n    ratio = np.round(series.value_counts()\/series.count()*100,decimals = 1)\n    return ratio","bc6a911e":"import random\n\ngroups = [df for _, df in data.groupby('patient_id')]\nrandom.shuffle(groups)\nshuffled_data = pd.concat(groups).reset_index(drop=True)","ff14a65a":"(get_classes_split(data['target']))","d8fe37c0":"from sklearn.model_selection import train_test_split\n# Set the patient ids as indices for the dataframe\nshuffled_data = data.set_index('patient_id')\n# Select all columns except the target column and store it in X\nX = shuffled_data.loc[:, shuffled_data.columns != 'target']\n# Select the target column and store it in y \ny = shuffled_data['target']\n\n# OS stands for 'Out of Sample'\nX_data, OSX_df, y_data, OSy_df = train_test_split(X, y, test_size=0.1, shuffle = False)\n# We split it even further and obtain the training and testing dataframes\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_data, y_data, test_size=0.3, shuffle = False)","ca016a2c":"print(f'y_data: {get_classes_split(y_data)}')\ndisplay(get_classes_split(y_test_df))\ndisplay(get_classes_split(OSy_df))","ceb87bd4":"# Dataframe containing cancerous images\nc_df = data.loc[data.target == 1,:]\n# Dataframe containing normal images\nn_df = data.loc[data.target == 0,:]\n\nfraction_c = np.round(0.7*c_df.shape[0]).astype(int)\nfraction_n = np.round(0.2*n_df.shape[0]).astype(int)\n\nrest_c_df = c_df.iloc[fraction_c:-1]\nrest_n_df = n_df.iloc[fraction_n:-1]\n\nc_df = c_df.iloc[0: fraction_c]\nn_df = n_df.iloc[0: fraction_n]\n\nnc = c_df.shape[0]\nnn = n_df.shape[0]\n\nnrc = rest_c_df.shape[0]\nnrn = rest_n_df.shape[0]\n\ntotal_test = nn+nc\ntotal_train = nrn + nrc\n\nprint(\"Testing Data:\") \nprint(f'percent cancerous : {round(nc\/total_test*100,1)}%')\nprint(f'percent non-cancerous : {round(nn\/total_test*100,1)}%\\n')\nprint(\"Training Data:\")\nprint(f'percent cancerous : {round(nrc\/total_train*100,1)}%')\nprint(f'percent non-cancerous : {round(nrn\/total_train*100,1)}%')","2e99a842":"n_train_df = c_df.append(n_df, sort = True).reset_index(drop=True)\nn_test_df = rest_c_df.append(rest_n_df,sort = True).reset_index(drop=True)","b9732f69":"def shuffle_patients(frame):\n    groups = [df for _, df in frame.groupby('patient_id')]\n    random.shuffle(groups)\n    shuffled_df = pd.concat(groups).reset_index(drop=True)\n    return shuffled_df","80f6d177":"n_train_df = shuffle_patients(n_train_df)","d467c999":"get_classes_split(n_train_df['target'])","0475948f":"import gc\nfrom sklearn.decomposition import IncrementalPCA\ndef rgb_to_grayscale(img_paths, batch_size = 15000):\n    # get the total number of images\n    num_of_imgs = img_paths.shape[0]\n    # initialize counter that keeps track of position of image being loaded\n    pos = 0\n    # initialize empty array in order fill in the image values\n    grid = np.zeros((num_of_imgs*2500, 3))\n\n    for img_path in tqdm(img_paths, total=num_of_imgs):\n        # Read the image into a numpy array \n        img = io.imread(img_path)\n        # reshape the image to such that the rgb values are the columns of the matrix\n        img = img.reshape(-1, 3)\n        # replace the empty array with the values inside the image\n        grid[pos: pos + img.shape[0],:] = img\n        # update position counter\n        pos += img.shape[0]\n        \n    # initialize pca to reduce rgb scale to a single dimensional scale\n    ipca = IncrementalPCA(n_components=1, batch_size=batch_size)\n    # fit pca object to the contents within the grid\n    ipca.fit(grid)\n    # delete grid to free up sum memory\n    del grid\n    gc.collect()\n    \n    return ipca","aff672fb":"img_paths = n_train_df['path']\nrgb_pca = rgb_to_grayscale(img_paths)","05ee74ca":"rgb_pca.explained_variance_ratio_.sum()","4e862fff":"# Read image\nimg = io.imread(n_train_df.path[0])\n\nfig, ax = plt.subplots(1,2,figsize = (12.5,25))\n\nax[0].grid(False)\nax[0].set_title(\"Image of Tissue Pre PCA\")        \nax[0].imshow(img)\n###########################\n# Reshape image so that pca be applied on the rgb dimension\nimg = img.reshape(-1, 3)\n# Apply PCA on image\nimg = rgb_pca.transform(img)\n# Reshape image so that image can be shown \nimg = img.reshape(50,50)\n############################\nax[1].grid(False)\nax[1].set_title(\"Image of Tissue Post PCA\")\nax[1].imshow(img)","991ad346":"def apply_rgb_pca(img,pca_object):\n    # reshape image so that pca be applied on the rgb dimension\n    img = img.reshape(-1, 3)\n    # apply PCA on image\n    img = pca_object.transform(img)\n    # flatten the image so that it can be stored in a numpy array of images\n    img = img.flatten()\n    return img","cf01ea64":"img_paths = n_train_df['path']\nimg_targets = n_train_df['target']","176bafc4":"#initialize an array containing zeros to place the images in it\nimg_array = np.zeros((img_paths.shape[0],2500))\n# loop through the images and apply PCA on them\nfor i,img_path in tqdm(enumerate(img_paths),total = img_paths.shape[0]):\n    img = io.imread(img_path)\n    try:\n        img_array[i:img.shape[0]] = apply_rgb_pca(img,rgb_pca)\n    except:\n        pass","cfd4ab0c":"def fit_pixel_pca(img_array,n_components=50,batch_size=20000):\n    # initialize pca to reduce rgb scale to a single dimensional scale\n    ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n    # fit pca object to the contents within the grid\n    ipca.fit(img_array)\n    # delete grid to free up sum memory\n    del img_array\n    gc.collect()\n    \n    return ipca","38caf9b6":"pca = fit_pixel_pca(img_array)","d7a59dd6":"reduced_array = pca.transform(img_array)","2a07edfd":"print(f'number of dimensions before applying PCA: {img_array.shape}')\nprint(f'number of dimensions after applying PCA: {reduced_array.shape}')","a319d2ec":"pca.explained_variance_ratio_.sum()","16629f2a":"reduced_array.shape","d2d9ce74":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf',gamma = 'auto' )\nsvc_n = SVC(gamma = 'auto')","e7adeef5":"svc.fit(reduced_array,n_train_df['target'].to_list())","c5ceef04":"svc.score(reduced_array,n_train_df['target'].to_list())","c0f96042":"from sklearn.metrics import confusion_matrix\n\ntn, fp, fn, tp = confusion_matrix(svc.predict(reduced_array),n_train_df['target'].to_list()).ravel()\n\nprint(f'training set: true negatives: {tn}')\nprint(f'training set: true positives: {tp}')\nprint(f'training set: false negatives: {fn}')\nprint(f'training set: false positives: {fp}')","5374248f":"# tn, fp, fn, tp = confusion_matrix(X_data,y_data).ravel()\n\n# print(f'training set: true negatives: {tn}')\n# print(f'training set: true positives: {tp}')\n# print(f'training set: false negatives: {fn}')\n# print(f'training set: false positives: {fp}')","5b2dadf3":"For a quick refresher on the different libraries used to read images and how to use them check out this medium article by [TowardDataScience](https:\/\/towardsdatascience.com\/what-library-can-load-image-in-python-and-what-are-their-difference-d1628c6623ad) ","a5b78db3":"We have a total number of about 280 sub-folders, let's take a peak into the folder and try and understand what those sub-folders are.","75375563":"We start by exploring the file structure to gain an understanding of how everything is organised","87b1200d":"### Fit the pixel PCA object","28ec881b":"## Storing the Images\nWe know that each RGB image has a dimension of (50,50,3), in order to store them in numpy arrays, we will flatten the image into a (1,7500) shape. This means that each row of our numpy array will store an image, and that our final numpy array dimensions will be about (280000,7500). We have about 280,000 rows because we have about 280,000 crops in total.\n\nWe know that each RGB image has a dimension of (50,50,3). One way of storing them is by flattening the image array into an array with a shape of (1,7500). This means that in order to store all the images we will need a numpy array with a total dimension of (280000,7500), this is because we have a total of 280,000 images. The problem with following this method comes to the limited amount of memory that we have when running a kaggle notebook. So what can we do? \n\nWe can try and reduce the number of dimensions that we are dealing with. One way of doing that is by converting our image from an RGB image into a grayscale image. An image is converted from an RGB image into grayscale by taking the weighted sum of the R,G, & B values and getting a single dimension in return. While this works, we can perform a better approximation, by using dimensionality reduction techniques such as Principle Compononent Analysis (PCA), Single Value Decomposition (SVD),& Latent Dirichlet Allocation (LDA).\n\nFor this example we will be applying PCA. \n\n## Applying PCA\n\nLet's briefly talk about PCA before delving into the application. PCA is a dimentionality reduction technique. What it is essentially doing is reducing the number of dimensions that we have by determining the covariance between the different variables that we have. \n\nThe covariance is how likely a variable is to vary relative to another variable. This means that variables with very high covariance can be redundant. Let's say I know that A will vary when B varies, then I can formulate a formula to explain the variation of B given A. \n\nLet's take another more concrete example: we know that the velocity is equal to the displacement over time, here the third variable is always explained given the other 2. PCA does the same thing, it tries to determine the relations between variables and uses them to explain each other. \n\nPCA linearly maps the features, along the axis of maximum variation, this axis is also called the eigen vector. The distance between a point and the eigen vector and a point is also known as the eigen value. To learn more about PCA 2 great articles are linked below. \n\n* [Article explaining the theoretical aspect of dimensionality reduction techniques such as PCA and SVD.](https:\/\/towardsdatascience.com\/pca-and-svd-explained-with-numpy-5d13b0d2a4d8)\n* [Article applying PCA on satellite images.](https:\/\/towardsdatascience.com\/principal-component-analysis-in-depth-understanding-through-image-visualization-892922f77d9f)\n* [Very useful answer on stackexchange that talks about the considerations that need to be taken before applying PCA](https:\/\/stats.stackexchange.com\/a\/450089\/267920)\n","0a6d870e":"#### Percentange of Variance Explained Post Dimensionality Reduction (PCA)","6fcfecd6":"Notice how the ratio of IDC patches to non-IDC patches is highly uneven. Here we approximately a 28\/70 split, which could be problematic down the road due to the overrepresentation of non-IDC samples relative to IDC samples. We need to account for it during the validation phase.","faff168e":"Each file has 2 sub-folders, labeled 1 and 0 <br\/>\nFolder 0: Non-Invasive Ductal Carcinoma (IDC) <br\/>\nFolder 1 : Invasive Ductal Carcinoma (IDC) <br\/>","0428086e":"## Exploring the data structure","5b7c13f5":"# Pre-processing\nNow that we have a function that allows us to vizualize the where the cancer occurs, we can finally begin pre-processing our data to get it ready to be fed into our classifier.  \n\nThe next steps that we are going to take are shown below:\n\n* Split Data to Training & Testing Data\n* Apply Principle Component Analysis (PCA)\n\nNotice how we split our data into testing and training sets before applying PCA to it. This is because we want to fit the data to our training data and not our testing data as that would bias our results. One more thing to consider is, that when we receive our data to be classified we are always transforming our data onto our already fitted model. Therefore applying the fit to our testing data, would not reflect what happens when classifying in real life.","7858cb54":"To simplify things we will create a function that slices our existing dataframe and retrieves the values associated with a patient id.","ae4ab728":"### Insights\n1. The number of image patches per patient varies a lot! This leads to the questions whether all images show the same resolution of tissue cells if this varies between patients.\n2. Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer. Does a tissue slice per patient cover the whole region of interest?\n3. The classes of IDC versus no IDC are imbalanced. We have to check this again after setting up a validation strategy and find a strategy to deal with class weights (in case we would like to rebalance the classes).","1cad1a7a":"## Healthy Tissue Patches Vs Cancerous Tissue Patches\nLet us now explore the visual differences between cancerous tissue cells, and healthy tissue cells. Usually partnering with a specialist is a good idea so that they can point the exact points of interest that differentiate the 2 from each other.","08495c0d":"Let's ensure that our classes split is maintained after the shuffling and splitting.","e55cc0b5":"### Settings","7a8fcf73":"### Fitting the model ","dc3cbbfb":"This is a forked version of the [Kernel](https:\/\/www.kaggle.com\/allunia\/breastcancer) by [Allunia](https:\/\/www.kaggle.com\/allunia). The kernel has been modified so that some of the functions are more computationally efficient, and shorter and easier to understand. \n\nInstead of using deep learning, we will be using Support Vector Machines (SVMs) as a classifier combined with Principle Component Analysis (PCA). \n\nSome of the techniques used and considerations taken in this kernel, were directly inspired by [Joni Juvonen's](https:\/\/www.kaggle.com\/qitvision) [kernel](https:\/\/www.kaggle.com\/qitvision\/a-complete-ml-pipeline-fast-ai).\n","d6b95f95":"## Visualizing the Breast Tissue\nEarlier we extracted the coordinates of the cropped tissue cells, we can use those coordinates to reconstruct the whole breast tissue of the patient. This way we can explore how the diseased tissue looks when compared to the healthy tissue. \n\nWe can also explore the most common places that the cancer tends to occur in. It would be interesting to plot a heatmap of the  most common areas where the cancer appears. \n\nIf position of the crop has significance then perhaps we can use it as an input feature for our model.","1656e588":"We first apply PCA on the RGB color dimension. As mentioned before, it is possible to skip this step entirely and just convert the RGB image to a grayscale, however applying PCA ensures that we retain maximum amount of information per image. After applying PCA to the color dimensions we would have reduced our total number of dimensions per flattened image from (1,7500) to (1,2500). This will help us reduce memory consumption with later operations.","6b300ccb":"# Motivation \n\n### My aunt has recently been diagnosed with breast cancer, this kernel is dedicated to her in the hopes that I can learn from this experience and one day build an improved version of this classifier.\n\n","e1adff5a":"### Cancerous Patches","4ba9e28c":"# Checkpoint \n## Saving Outputs\nNow that we have stored our images in array, let's make a checkpoint by saving the array and dataframe. The reason we want to do that, is so that we don't rerun the whole notebook when we want to continue our analysis. This way we can directly start from here. Furthermore, we will be deleting all the  variables that we created in the process to free up some more memory.\n\nWe will do this by pickling the file, in other words serializing our data so that we can read it later. Below are 2 Functions that I created to make the process of saving and loading our variables easier.","98d66afc":"The function below, takes a list of paths to the desired images. Then reads and reshapes the arrays from (50,50,3) to (2500,3). After reshaping it adds the reshaped array to the total array. The final array will have a total dimension of (2500\\*120000,3) = (300000000,3). PCA is then applied to this array and the principle components and fitted to this object. ","3fecbc97":"### Binary target visualisation per tissue slice \nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:","da41f6b9":"## Storing the image_path, patient_id, target and x & y coordinates\nWe have about 278,000 images. To feed the algorithm with image patches we will store the path of each image and create a dataframe containing all the paths. **This way we can load batches of images one by one without storing the individual pixel values of all images**. \n\nIn addition to extracting the paths, we will also extract the x & y coordinates of each of the images. Then we can use the coordinates to reconstruct the whole breast tissue of a patient. This way we can also explore how diseased tissue looks like compared to healthy ones.\n\nWe know that the path to the images is as follows: <br\/>\n> ..\/input\/breast-histopathology-images\/[patient_id]\/[class]\/[image]\n\nBelow we start by constructing a list of dictionaries and then creating a dataframe based on that list. This is much more computationally efficient than using 'append' or 'loc' methods. For more info check out this [post](https:\/\/stackoverflow.com\/questions\/10715965\/add-one-row-to-pandas-dataframe\/17496530#17496530) on stackoverlow.","876c75c7":"### Apply PCA to the pixel dimension of image array","08bf022c":"We managed to reduce our total number of pixels from 2500 to 50, pretty great if you ask me! But before we go on celebrating this feat, let's check how much of the variance we managed to capture.","57188745":"Above, we created a function using incremental PCA, which uses the exact same technique as PCA, however instead of loading the whole array in memory, it loads it in batches. After some experimentation we opted for 10,000, rows per batch, anything more than that and we risk running out of memory. Furthermore, we opted for a maximum of 120,000 rows to be fed into the function, and again this is for memory management purposes.","4d2c7022":"### Transorming RGB image paths to single scale image array using PCA","3435accf":"### Repatching the Actual Breast Tissue Image\nNow it's time to go one step deeper with our EDA. Instead of plotting the target values using the x-y coordinates, we now plot the images themselves on their respective x-y coordinates. This will help us visualize how the cancerous tissue looks like from a macro perspective.","83315544":"Now that we have set up our data, let's create a visual summary to help us draw some insights from our data.","11d801d8":"uint8 is used unsigned 8 bit integer. And that is the range of pixel. We can't have pixel value more than 2^8 -1. Therefore, for images uint8 type is used. Whereas double is used to handle very big numbers.","124ee886":"-\n--\n\n-\n\n\n-\n\n-\n\n\n-\n\n-\n\n-\n\n-\n\n--\n\n-\n\n-\n\n-\n\n-\n\n-\n\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n\n-\n\n-\n\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n\n-","5ab12332":"Each subfolder seems to be the ID of the corresponding patient","ad247cfe":"Below we take a glimpse at the files contained in each class","656066b7":"As we expect, the shape of our dataframe matches our expectations, containing a total of 5 features, and about 280,000 patches.","85822737":"## Before and After RGB PCA\nLet's visualize the image before and after PCA was performed.","294e5e77":"### Splitting Data to Cancerous and non-Cancerous data","5d4568a3":"Let's now visualize what some of the reconstructed tissue cells look like by using the function above. \n\nNote: The function can be improved by having edges on the mask, rather than overlaying a color on top. This will help us preserve the true image of the tissue cells.","4fe22fc0":"### Shuffling Our Data\nBelow we shuffle our data such that the patient order is shuffled, however the individual patient order is maintained.","56314a17":"# CHECK OUT MY [NEW & IMPROVED NOTEBOOK](https:\/\/www.kaggle.com\/amerii\/breast-cancer-classification-guide-pca-svms)","705260d3":"### How many patches do we have in total?\n\n#### Which of them are IDC patches and which are Non-IDC?\nIn order to train our model we need to feed our model each patch individually, therefore each patch will act as an input.\n\nThe snippet below loops through the entire file structure and extracts the total number of crops for each of the classes.","154ce185":"# Preparation & peek at the data structure\n\n## Loading packages","d5723e99":"### Creating New Training & Testing Dataset","73bca3d3":"# Training Model","0a259d5a":"### Insights\u00b6\n* Sometimes we can find artifacts or incomplete patches, some images are also less than 50 x 50 pxs. \n* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n* Though some of the healthy patches are very violet colored too!\n* Would be very interesting to hear what criteria are important for a [pathologist](https:\/\/en.wikipedia.org\/wiki\/Pathology).\n","ce26000d":"## Training & Testing Split\n\nThere are a few things that we must consider before splitting our data. The first being the ratio of cancerous to non cancerous images that we have. We determined earlier that our data is not split uniformly. We have many more, non-cancerous images that non-cancerous images, which may possibly bias our model. \n\nThis may also pose a problem when evaluating our model. Let's take an extreme example where we have a dataset consisting of 100 points, 90 of them are cancerous and 10 of them are non-cancerous, let's also assume that our model classified our entire dataset as cancerous, this would give an accuracy of 90%, however this is not really representitive of what is really happening. In reality it is always misclassifying our non-cancerous data.\n\nSo how do we go about solving this?\n\nWe can use the train-test split function offered in sklearn model_selection, this function has the option to split our data such that we get equally distributed training and testing sets when grouped by the classes.\n\nFor more information, check out this [article](https:\/\/towardsdatascience.com\/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50), that talks about the considerations that need to be taken before splitting your data.","534ea77b":"The function below returns the percentage of unique items we have in a class so that we can gain a better understanding of our target variables. In our case, the data will be split between 0 and 1 values. These values represent the following:\n\n* 0 -> Tissue does not contain IDC \n* 1 -> Tissue contains IDC","678e4ed7":"It seems that we have reached the files containing the pictures of each patient, \nthe structure seems to be as follows:\n\nPatient_id\/xcoordinate_of_patch\/ycoordinate_of_patch\/class_of_cancer","0ed1056f":"### Insights\n* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation.\n* Cancerous Tissue tends to appear in clusters rather than, being dispersed all over the place.\n","8cf9c531":"## Data Augmentation\nThere are couple of ways we can use to avoid overfitting; more data, augmentation, regularization and less complex model architectures. Note that if we apply augmentation here, augmentations will also be applied when we are predicting (inference). This is called test time augmentation (TTA) and it can improve our results if we run inference multiple times for each image and average out the predictions. \n\n**The augmentations we can use for this type of data:**\n- random rotation\n- random crop\n- random flip (horizontal and vertical both)\n- random lighting\n- random zoom \n- Gaussian blur \n\n\nAlso possible to make a heat map that tells you the most likely region that Invasive Ductal Carcinoma is likely to occur, and to see whether the coordinates can be used as a feature or not.\n\n## Note to self: \n#### Please disregard this\n\nUsually before applying the steps above we might be interested in engineering new features based on the our observations and domain knowledge,one of the features that we might consider adding in future versions of this kernal is including the count of surrounding cancerous cells. This would be treated as a time series analysis, we would have to iterate on it until an equilibirium is reached. So we would have micro and macro classification. Also look into using t-svd instead of pca.","291e1f44":"### Model Evaluation","5ec353ba":"### Setting model parameters","fe63f643":"## What do we know about our data?\nNow that we have a good understanding of the file structure let's try and understand how much data we are about to process.\n\n### How many patients do we have?\nIt seems that we have a total number of 280 patients. This sample size is relatively small therefore we have to be careful not to overfit our model. We need to implement our model in such a way that it maximizes generalization.\n\nEach patient has a batch of patches that were extracted, therefore the total number of patches is likely much greater than 280.","b5411ed3":"# Explarotory Data Analysis\n\n## Let's create a visual summary of our data","02c94652":"### Non-Cancerous Patches","a9dc080a":"Now that we know the initial, split of the classes, let's split our data into training and testing sets. We will split our data as follows:\n\n* ***Training Data*** : Data that we will use to train our model\n* ***Testing Data*** : Data that we will use to test our model\n* ***Out of Sample Data*** : Data that we will use to further validate our testing. Usually this is taken before preprocessing the data.\n\nWe will be using sklearn's train_test_split to split our data. \n\nShuffle is set to False, this is because we already manually shuffled our data in the previous cell.","b1549066":"### Shuffling Our Data\nBelow we shuffle our data such that the patient order is shuffled, however the individual patient order is maintained."}}