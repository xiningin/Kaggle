{"cell_type":{"322c438d":"code","be458595":"code","b98ef4c9":"code","3865a743":"code","c198fb71":"code","3d7dfcc4":"code","77b7f7ed":"code","ce744534":"markdown","4e6f960d":"markdown","5d57619b":"markdown","68f281d1":"markdown","2c9bd68d":"markdown","e072f2fa":"markdown","9d932d12":"markdown","8831b74a":"markdown","3810ac43":"markdown"},"source":{"322c438d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","be458595":"train_df=pd.read_csv(os.path.join(dirname, filenames[1]))\ntrain_df.head(3)","b98ef4c9":"from nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\n\ntrain_df['excerpt_preprocess']=train_df['excerpt'].str.lower()\ntrain_df['excerpt_preprocess']=train_df['excerpt_preprocess'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\ntrain_df['excerpt_length']=train_df['excerpt_preprocess'].str.len()","3865a743":"train_df['excerpt_preprocess']","c198fb71":"round(train_df['excerpt_length'].mean(),0) #Avg. excerpt length","3d7dfcc4":"from nltk import sent_tokenize\nimport re\n\nexcerpt_1 = train_df.loc[0,'excerpt']\nsent_1 = sent_tokenize(excerpt_1)\n\nmax_len=0\ncount=0\ntotal_length=0\npunct=\";|!|:|;|,|-|'\"\nmax_punct_len=0\n\nfor sent in sent_1:\n    punct_len=len(re.findall(punct, sent))\n    if punct_len>max_punct_len:\n        max_punct_len=punct_len\n    if len(sent)>max_len:\n        max_len=len(sent)\n    total_length+=len(sent)\n    count+=1\nprint(\"Average sent Length: \",round(total_length\/count,1))\nprint(\"Max sent Length: \",max_len)\nprint(\"Max punct Length: \",max_punct_len)","77b7f7ed":"from nltk import sent_tokenize\nimport re\n\nexcerpt_1 = train_df.loc[0,'excerpt_preprocess']\nsent_1 = sent_tokenize(excerpt_1)\n\nmax_len=0\ncount=0\ntotal_length=0\npunct=\";|!|:|;|,|-|'\"\nmax_punct_len=0\n\nfor sent in sent_1:\n    punct_len=len(re.findall(punct, sent))\n    if punct_len>max_punct_len:\n        max_punct_len=punct_len\n    if len(sent)>max_len:\n        max_len=len(sent)\n    total_length+=len(sent)\n    count+=1\nprint(\"Average sent Length: \",round(total_length\/count,1))\nprint(\"Max sent Length: \",max_len)\nprint(\"Max punct Length: \",max_punct_len)","ce744534":"Extract the first text excerpt and analyze the average length of the sentence(words in a sentence), the maximum length of the sentence, and the maximum punctuations in a sentence.\n\nThe idea is to correlate word count, sentence length, and punctuations to the text complexity. This is done initially for the raw text and then for the text where it was preprocessed(lowercased and with the frequent text removed)","4e6f960d":"Verify that the data import and explore the data by printing the top 3 rows in the dataframe.","5d57619b":"**Process 2: Clean the Data**\n\nConvert the excerpt into lower case(so that an accurate count of the words can be obtained).\nThen remove the most frequently occurring words like - 'an', 'the', and 'on'. This list of frequently occurring can be obtained from the NLTK library.","68f281d1":"## Data Cleaning & Data Stats\n\nThe goal of the notebook is to clean CommonLit Data and get some statistics of the excerpt text. The code uses NLTK, regex, and pandas manipulation to achieve this.\n\nHere are the various processes in this notebook:\n\n1. Reading Data\n2. Clean the Data\n3. Get important stats about the excerpt","2c9bd68d":"Get the average word count of each excerpt","e072f2fa":"The next steps are to get the counts for all the excerpts and then encode the text. This data will then be fed to an ML model to rank\/access the excerpt complexity.","9d932d12":"**Process 1: Reading the data**","8831b74a":"****Process 3: Get excerpt stats****\n\nGet the initial length of the dataframe","3810ac43":"Below shows the same stats, but now with the preprocessed data."}}