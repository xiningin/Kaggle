{"cell_type":{"26ad674c":"code","8e5d8d60":"code","cdee8b7c":"code","70c87ca6":"code","c84832ad":"code","07799a34":"code","569a7848":"code","5aacf8bd":"code","4e7ff0e0":"code","a7419258":"code","93f7e61b":"code","d06ea54e":"code","e56233dd":"code","8df14846":"code","9b8f3c90":"code","ec746a3c":"code","1b7c2782":"code","29fda06b":"code","cca29ef5":"code","f9c81c2d":"code","289093e1":"code","1caafea4":"markdown","f803c1c3":"markdown","a91128ba":"markdown","ca78d0c8":"markdown","73077047":"markdown","3376ed23":"markdown","e76deaf6":"markdown","8f8a1ded":"markdown","6c1e3b9c":"markdown","5b15e22c":"markdown","18cbc7ed":"markdown","945b62db":"markdown","3830d831":"markdown","7a63c73c":"markdown","c5e8bca7":"markdown","013131b3":"markdown","c2e3db93":"markdown","3deb6195":"markdown","95dfd2b0":"markdown","8a98c0ee":"markdown"},"source":{"26ad674c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e5d8d60":"df1 = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ndf2 = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\ndf3 = pd.read_csv(\"..\/input\/lish-moa\/train_targets_nonscored.csv\")\n\nprint(\"shape of train_features.csv: \", df1.shape)\nprint(\"shape of train_targets_scored.csv: \", df2.shape)\nprint(\"shape of train_targets_nonscored.csv: \", df3.shape)\n\nprint(\"Total number of datapoints: {:,}\".format(df1.shape[0]))\n\ndf1.head()","cdee8b7c":"df1.describe()","70c87ca6":"# knowing the data types of each column of `train_features.csv`\ndata_types = df1.dtypes\nunique_dtypes = data_types.unique()\nprint(\"number of dtypes in `train_features.csv`: \", len(unique_dtypes),\n      \"\\nAnd these are: \", unique_dtypes)\n\nObj   = []\nInt   = []\nFloat = []\nfor col, data_type in zip(df1.columns, data_types):\n    if data_type == 'object':Obj.append(col)        \n    elif data_type == 'int64':Int.append(col)\n    elif data_type == 'float64':Float.append(col)\nprint(\"number of object data type: \", len(Obj))\nprint(\"number of int64 data type: \", len(Int))\nprint(\"number of float64 data type: \", len(Float))\n\nassert len(Obj)+len(Int)+len(Float) == df1.shape[1]","c84832ad":"Obj, Int","07799a34":"print(\"Number of unique values in `cp_type` col is: {} and these are: {}\"\n      .format(len(df1.loc[:, \"cp_type\"].unique()), df1.loc[:, \"cp_type\"].unique()))\n\nprint(\"Number of unique values in `cp_dose` col is: {} and these are: {}\"\n      .format(len(df1.loc[:, \"cp_dose\"].unique()), df1.loc[:, \"cp_dose\"].unique()))\n\nprint(\"Number of unique values in `cp_time` col is: {} and these are: {}\"\n      .format(len(df1.loc[:, \"cp_time\"].unique()), df1.loc[:, \"cp_time\"].unique()))","569a7848":"import matplotlib.pyplot as plt\nimport seaborn as sns","5aacf8bd":"fig, axs = plt.subplots(1,3, figsize=(18,6))\n\nfig.suptitle(\"Count Plot of Categorical variables\", fontsize = 24)\nsns.countplot(x ='cp_type', data = df1, ax = axs[0])\naxs[0].set_title(\"For: cp_type\", fontsize= 16)\nsns.countplot(x ='cp_dose', data = df1, ax = axs[1])\naxs[1].set_title(\"For: cp_dose\", fontsize = 16)\naxs[1].set(ylabel = '')\nsns.countplot(x = 'cp_time', data = df1, ax = axs[2])\naxs[2].set_title(\"For: cp_time\", fontsize = 16)\naxs[2].set(ylabel = '')\n\nplt.show()","4e7ff0e0":"# cp_type, cp_time, cp_dose\ndef encode_cp_time(row):\n    val = None\n    if row == 24:val = 1\n    elif row == 48:val = 2\n    else:val = 3\n    return val\n        \ndf1[\"cp_type\"] = df1[\"cp_type\"].apply(lambda x: 0 if x=='trt_cp' else 1)\ndf1[\"cp_dose\"] = df1[\"cp_dose\"].apply(lambda x: 0 if x=='D1' else 1)\ndf1[\"cp_time\"] = df1[\"cp_time\"].apply(encode_cp_time)\n\ndf1.head()","a7419258":"from sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.decomposition import PCA","93f7e61b":"X = df1.iloc[:, 1:]\nx = StandardScaler().fit_transform(X)\nx = pd.DataFrame(data=x, columns = X.columns)\nx.head()","d06ea54e":"pca = PCA(n_components=None, svd_solver = 'full')\npca.fit_transform(x)\nvar = pca.explained_variance_ratio_","e56233dd":"for i in range(1,9):\n    print(\"Variance explained by top {} components: {}\".format(i*100, var[:i*100].sum()))","8df14846":"corrmat = X.corr()","9b8f3c90":"f, ax = plt.subplots(1,1, figsize =(18, 8))\nsns.heatmap(corrmat, ax = ax)\nplt.show()","ec746a3c":"cols = [col for col in corrmat.columns if col.startswith('c-')]\nf, ax = plt.subplots(1,1, figsize =(18, 8))\nsns.heatmap(corrmat.loc[cols, cols], ax = ax)\nplt.show()","1b7c2782":"from sklearn.metrics import log_loss\ndef total_loss(y_true, y_pred):\n    \"\"\"\n    y_true: numpy nd-array of shape (None , 206), None means any value\n    y_pred: numpy nd-array of shape (None , 206)\n    \"\"\"\n    losses = []\n    for i in range(y_true.shape[1]):losses.append(log_loss(y_true[:,i], y_pred[:,i], eps=1e-15))\n    return np.mean(losses)","29fda06b":"df_test = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\nprint(\"number of test datapoints: {:,}\".format(df_test.shape[0]))\n\ny_train = df2.iloc[:, 1:].values\ny_train_pred = np.random.random_sample(y_train.shape) \ny_test_pred = np.random.random_sample((df_test.shape[0], y_train.shape[1])) ","cca29ef5":"tr_loss = total_loss(y_train, y_train_pred)\nprint(\"train loss: \", tr_loss)","f9c81c2d":"test_df = pd.DataFrame(data = y_test_pred, columns = df2.columns[1:])\ntemp = pd.DataFrame(data=df_test.loc[:, 'sig_id'])\ntest_df = pd.concat([temp, test_df], ignore_index=False, axis=1)\ntest_df.head()","289093e1":"test_df.to_csv(\".\/submission.csv\",index=False)","1caafea4":"# Random Model","f803c1c3":"## PCA & Correlation","a91128ba":"* There is only one column for `int64` column and `3` for the `object` datatype.\n* `cp_dose` and `cp_type` are categorical variable.\n* `cp_time` has `int64` datatype, but this one is also categorical variable. \n* Let's see the number of unique values in these columns.","ca78d0c8":"* There are `875` features, first let's reduce the dimension using **PCA**.","73077047":"* From above plot, it can be see that the features that starts with `c-`  looks highly correlated to each other. These are also shwoing strong correaltion with other features also.\n* Let's see the heatmap of features that starts with `c-`.","3376ed23":"# EDA","e76deaf6":"* This will predict the random value between 0 and 1 (inclusive, i.e. 0<=prob<=1) for each label of each datapoints.\n* We'll calculate `the worst model` performance. ","8f8a1ded":"* The most of thefeatures that start with `c-` are highly correlated.\n\n**NOTE:** If you're using all features, use tree-based model or high dropout in first (just after input) in neural-network architecture.","6c1e3b9c":"### Correlation","5b15e22c":"* One hot encoding for `cp_type` and `cp_dose`.\n* `D1` == `0`, `D2`==`1`.\n* `trt_cp`== `0`, `ctl_vehicle` ==`.\n* Label encoding for`cp_time`, `0` for 24, `1` for 48 and `2` for 72.","18cbc7ed":"* I would always prefer high variance explained (between 95-99%). But for that, I have to take atleast 600 components.\n* 800 components are explainig the 99.41% (approx) variance.\n* Now, one can select 100, 200, 300, .... any number of features and see the performance of model.","945b62db":"## Encoding","3830d831":"## Data-type of features ","7a63c73c":"### PCA","c5e8bca7":"* Let's normalise the features, first","013131b3":"**Source:** https:\/\/www.geeksforgeeks.org\/exploring-correlation-in-python\/","c2e3db93":"* For both variables `cp_dose` and `cp_time` number of count is almost same for all unique values present in their columns.\n* But for `cp_type` there are very data points that corresponds to `ctl_vehicle`.\n\n**NOTE:** Use stratified spliting with `cp_type` column. This will maintain the `cp_type`'s uniques values count-ratio.  ","3deb6195":"* There are **876** columns in `train_features.csv` file and out of these there are **875** features. \n* There are **207** labels that we need to predict for each datapoint.\n* Most of features look real-valued (continuous) and few of them look are categorical type. We'll see them detail.","95dfd2b0":"* This will explore the dataset.\n* Will build a random model and see the performance. ","8a98c0ee":"**Splitting the data**\n\n```Python\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import hstack\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\ndf_train = df1.merge(df2, on = 'sig_id')\nprint(\"shape: \", df_train.shape)\ndf_train.head()\n\ntrain, val = train_test_split(df_train, test_size = 0.2, random_state=42,\n                             stratify = df_train[\"cp_type\"])\nX_train, Y_train = df_train.iloc[:, :-206], df_train.iloc[:, -206:]\nX_val, Y_val = val.iloc[:, :-206], val.iloc[:, -206:]\n\nprint(\"Number of datapoints in train-set: {:,}\".format(len(X_train)))\nprint(\"Number of datapoints in val-set: {:,}\".format(len(X_val)))\n```"}}