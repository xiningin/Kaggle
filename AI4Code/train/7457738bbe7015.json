{"cell_type":{"f03e7255":"code","ce5110ef":"code","cd8fa521":"code","4a87cce8":"code","ae1d3a9b":"code","331b4da6":"code","09167b68":"code","9c00252b":"code","e56a9f89":"code","de823cb6":"code","f8bf24f6":"code","0b354faa":"code","a60327f2":"code","8e26f112":"code","47ebdd94":"code","ffb549c7":"code","a8474159":"code","f4dca2eb":"code","0ac1ab1a":"markdown","698e8f38":"markdown","29987d1a":"markdown","567a3c8d":"markdown","2b54d4ad":"markdown","9245af2e":"markdown","a1690f4f":"markdown","3dae43ee":"markdown","c96ed700":"markdown","9a54c789":"markdown","f3ae6e9e":"markdown","a8e3dbc6":"markdown","01e5c776":"markdown","a6951111":"markdown","6ecc0532":"markdown"},"source":{"f03e7255":"import numpy as np \nimport pandas as pd ","ce5110ef":"train_embeddings = np.load('..\/input\/cnn-embeddings-generator\/embed_train_256_0.npy')\ntest_embeddings = np.load('..\/input\/cnn-embeddings-generator\/embed_test_256_0.npy')\nexternal_train_embeddings = np.load('..\/input\/cnn-embeddings-generator\/embed_ext_2019_256_0.npy')\nexternal_train_embeddings_18 = np.load('..\/input\/cnn-embeddings-generator\/embed_ext_2018_256_0.npy')\ntrain_names = np.load('..\/input\/cnn-embeddings-generator\/names_train.npy')\ntest_names = np.load('..\/input\/cnn-embeddings-generator\/names_test.npy')\nexternal_train_names = np.load('..\/input\/cnn-embeddings-generator\/names_ext_2019.npy')\ntrain_labels = np.load('..\/input\/cnn-embeddings-generator\/labels_train.npy')\nexternal_train_labels = np.load('..\/input\/cnn-embeddings-generator\/labels_ext_2019.npy')\nexternal_train_labels_18 = np.load('..\/input\/cnn-embeddings-generator\/labels_ext_2018.npy')\ntrain_embeddings.shape,test_embeddings.shape,external_train_embeddings.shape,train_names.shape,test_names.shape,external_train_names.shape","cd8fa521":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","4a87cce8":"def build_model(dim=1280,lr=0.001):\n    inp = tf.keras.layers.Input(shape=(None,dim))\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(inp)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model\n\ndef build_model1(dim=1280,lr=0.001):\n    inp = tf.keras.layers.Input(shape=(None,dim))\n    x = tf.keras.layers.Dense(300,activation='sigmoid')(inp)\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model","ae1d3a9b":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report,accuracy_score,f1_score,roc_auc_score\nskf = StratifiedKFold(n_splits=5,shuffle=True)\n\nX = np.concatenate([train_embeddings,test_embeddings],axis=0)\ny = np.zeros((train_embeddings.shape[0]+test_embeddings.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_test_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","331b4da6":"X = np.concatenate([train_embeddings,test_embeddings],axis=0)\ny = np.zeros((train_embeddings.shape[0]+test_embeddings.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_test1_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model1(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","09167b68":"import matplotlib.pyplot as plt\nplt.hist(oof)\nplt.show()","9c00252b":"X = np.concatenate([external_train_embeddings,test_embeddings],axis=0)\ny = np.zeros((external_train_embeddings.shape[0]+test_embeddings.shape[0],1))\ny[:external_train_embeddings.shape[0]] = 1\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"ext_test_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","e56a9f89":"import matplotlib.pyplot as plt\nplt.hist(oof)\nplt.show()","de823cb6":"X = np.concatenate([external_train_embeddings_18,test_embeddings],axis=0)\ny = np.zeros((external_train_embeddings_18.shape[0]+test_embeddings.shape[0],1))\ny[:external_train_embeddings_18.shape[0]] = 1\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"ext_test_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","f8bf24f6":"X = np.concatenate([train_embeddings,external_train_embeddings],axis=0)\ny = np.zeros((train_embeddings.shape[0]+external_train_embeddings.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_ext_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","0b354faa":"import matplotlib.pyplot as plt\nplt.hist(oof)\nplt.show()","a60327f2":"X = np.concatenate([train_embeddings,external_train_embeddings_18],axis=0)\ny = np.zeros((train_embeddings.shape[0]+external_train_embeddings_18.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_ext_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","8e26f112":"X = np.concatenate([external_train_embeddings,external_train_embeddings_18],axis=0)\ny = np.zeros((external_train_embeddings.shape[0]+external_train_embeddings_18.shape[0],1))\ny[:external_train_embeddings.shape[0]] = 1\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_ext_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","47ebdd94":"X = np.concatenate([train_embeddings,external_train_embeddings],axis=0)\ny = np.zeros((train_embeddings.shape[0]+external_train_embeddings.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\n\nlabels = np.concatenate([train_labels,external_train_labels],axis=0)\nX = X[labels==0]\ny = y[labels==0]\n\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_ext_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","ffb549c7":"X = np.concatenate([train_embeddings,external_train_embeddings_18],axis=0)\ny = np.zeros((train_embeddings.shape[0]+external_train_embeddings_18.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\n\nlabels = np.concatenate([train_labels,external_train_labels_18],axis=0)\nX = X[labels==0]\ny = y[labels==0]\n\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_ext_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","a8474159":"X = np.concatenate([train_embeddings,external_train_embeddings],axis=0)\ny = np.zeros((train_embeddings.shape[0]+external_train_embeddings.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\n\nlabels = np.concatenate([train_labels,external_train_labels],axis=0)\nX = X[labels==1]\ny = y[labels==1]\n\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_ext_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","f4dca2eb":"X = np.concatenate([train_embeddings,external_train_embeddings_18],axis=0)\ny = np.zeros((train_embeddings.shape[0]+external_train_embeddings_18.shape[0],1))\ny[:train_embeddings.shape[0]] = 1\n\nlabels = np.concatenate([train_labels,external_train_labels_18],axis=0)\nX = X[labels==1]\ny = y[labels==1]\n\noof = np.zeros(y.shape)\nprint(X.shape,y.shape)\n\nfor i,(train_index, test_index) in enumerate(skf.split(X, y)):\n    print(\"Fold:\",i,end = \" \")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model_path = \"train_ext_{}.h5\".format(i)\n    early_stop = EarlyStopping(monitor='val_auc',patience=20,verbose=1,mode='max')\n    reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=4,verbose=0,mode='max')\n    checkpoint = ModelCheckpoint(model_path , monitor='val_auc', verbose=0, save_best_only=True, mode='max')\n    model = build_model(lr=0.01)\n    history = model.fit(X_train,y_train,validation_data = (X_test,y_test),verbose=0,epochs=100, batch_size = 1000,\n                        callbacks=[early_stop,reduce_lr,checkpoint])\n    model.load_weights(model_path)\n    oof[test_index] = model.predict(X_test)\n    print(\"Partial Score:\",roc_auc_score(y_test,oof[test_index]))\nprint(classification_report(y, (oof>0.5).astype(int), digits=4))\nprint(roc_auc_score(y, oof))","0ac1ab1a":"## Adverserial Validation: Test vs External Data (2017-18)","698e8f38":"# It seems there is a significant difference between train and external data. What's your views??","29987d1a":"# Adverserial Validation on Non-Melanoma Samples (Train vs External 2019)\nOne of the reasons for high AUC score b\/w train\/test and external data may be difference in labels distribution. So, let's try to run adverserial validation on just non-melanoma samples","567a3c8d":"# Update: I wanted to try stratified-Group K Fold CV strategy for adverserial validation. But it seems last year data doesn't have patient information. \n\n# Please tell me if you come up with some improvement to this notebook. \n\n## As of now, Using 2019 data seems too risky to be used for training. Even 2018 data has AUC>0.9 for both train and test. But, I guess I will be using this because it gives a boost in both OOF and LB scores.","2b54d4ad":"## Adverserial Validation: Train vs Test","9245af2e":"Adverserial Validation is a technique to determine how different two distribution is. The embeddings were generated using Chris Notebook.\nhttps:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-knn-find-duplicates","a1690f4f":"# External Data (2017-18) is similar to train than (2019)","3dae43ee":"# Adverserial Validation on Melanoma Samples - Train vs External (2018)","c96ed700":"# Adverserial Validation on Melanoma Samples - Train vs External (2019)","9a54c789":"## Adverserial Validation: Train vs External Data (2018)","f3ae6e9e":"## Adverserial Validation: Test vs External Data (2019)","a8e3dbc6":"## Loading the Embeddings","01e5c776":"## Adverserial Validation: Train vs External Data","a6951111":"## Adverserial Validation: External(2019) vs External(2018)","6ecc0532":"# Adverserial Validation on Non-Melanoma Samples (Train vs External 2018)\n"}}