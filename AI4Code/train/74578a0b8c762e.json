{"cell_type":{"13f6ac2f":"code","232dd1c6":"code","5bf2ef1d":"code","38c9e084":"code","d54bd9d8":"code","14de297c":"code","407d0e6c":"code","d98f1ba3":"code","b595ec91":"code","e925a875":"code","ac94a291":"code","6b33395b":"markdown","c5552d46":"markdown","08505002":"markdown","f72c29bd":"markdown","ccde2d16":"markdown","9225625a":"markdown","c7f5864f":"markdown","8c5569ae":"markdown","0140504d":"markdown","6a8d9567":"markdown","aeef8383":"markdown","3a1a47a3":"markdown","0ffba50f":"markdown","4fc4af5f":"markdown","d8b2d90d":"markdown","966023b6":"markdown"},"source":{"13f6ac2f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler,normalize,label_binarize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score,confusion_matrix, roc_curve, auc, f1_score, precision_score,recall_score, roc_auc_score\nfrom time import time\n","232dd1c6":"df = pd.read_csv('..\/input\/3wine-classification-dataset\/wine.csv')\ndf.head()","5bf2ef1d":"sns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df,hue=\"Wine\", markers=[\"o\", \"s\", \"D\"])","38c9e084":"y = df.iloc[:,0]\nx = df.iloc[:,1:]\n\nsc = StandardScaler()\nx = sc.fit_transform(x)\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.7,random_state = 42)\nwarnings.filterwarnings('ignore')\n","d54bd9d8":"def evaluateClassifier(x,y,y_pred,y_score):\n    cm = pd.DataFrame(\n        confusion_matrix(y, y_pred),\n        columns=['Predicted Wine 1', 'Predicted Wine 2','Predicted Wine 3'],\n        index=['True Wine 1', 'True Wine 2','True Wine 3']\n    )\n    print('\\nConfusion Matrix: \\n')\n    sns.set(font_scale=1.4) # for label size\n    sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}) # font size\n    plt.show()\n    w1 = cm['Predicted Wine 1']['True Wine 1'] \/ (cm['Predicted Wine 1']['True Wine 1'] + cm['Predicted Wine 2']['True Wine 1'] + cm['Predicted Wine 3']['True Wine 1'])\n    w2 = cm['Predicted Wine 2']['True Wine 2'] \/ (cm['Predicted Wine 1']['True Wine 2'] + cm['Predicted Wine 2']['True Wine 2'] + cm['Predicted Wine 3']['True Wine 2'])\n    w3 = cm['Predicted Wine 3']['True Wine 3'] \/ (cm['Predicted Wine 1']['True Wine 3'] + cm['Predicted Wine 2']['True Wine 3'] + cm['Predicted Wine 3']['True Wine 3'])\n    print('\\nClasswise accuracy: ')\n    print('\\nWine 1: ',w1 * 100)\n    print('\\nWine 2: ',w2 * 100)\n    print('\\nWine 3: ',w3 * 100)\n    \n    indices = ['Accuracy','Precision','F1 score','Recall  score']\n    eval = pd.DataFrame([accuracy_score(y,y_pred) * 100,precision_score(y,y_pred,average = 'macro') * 100,f1_score(y,y_pred,average = 'macro') * 100,recall_score(y,y_pred,average = 'macro') * 100],columns=['Value'],index=indices)\n    eval.index.name = 'Metrics'\n    print('\\n',eval)\n    y = label_binarize(y, classes = range(1,4))\n    for i in range(3):\n        fpr, tpr, _ = roc_curve(y[:, i], y_score[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label = 'AUC = %0.2f' % roc_auc)        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.2, 1.05])\n    plt.ylim([0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()\n    plt.title('ROC curve')\n    plt.show()","14de297c":"gnb = GaussianNB()\nstart = time()\ngnb.fit(x_train,y_train)\nstop = time()\nprint('Training time: ',stop - start)\nevaluateClassifier(x_train,y_train,gnb.predict(x_train),gnb.predict_proba(x_train))\n","407d0e6c":"evaluateClassifier(x_test,y_test,gnb.predict(x_test),gnb.predict_proba(x_test))","d98f1ba3":"def dtree_grid_search(X,y,nfolds):\n    param_grid = { 'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n    dtree_model=DecisionTreeClassifier()\n    dtree_gscv = GridSearchCV(dtree_model, param_grid, cv = nfolds)\n    dtree_gscv.fit(X, y)\n    return dtree_gscv.best_params_\n\noptimal_params = dtree_grid_search(x_train,y_train,5)\n\ndtc = DecisionTreeClassifier(criterion = optimal_params['criterion'],max_depth = optimal_params['max_depth'])\nstart = time()\ndtc.fit(x_train,y_train)\nstop = time()\nprint('Training time: ',stop - start)\nevaluateClassifier(x_train,y_train,dtc.predict(x_train),dtc.predict_proba(x_train))","b595ec91":"optimal_params = dtree_grid_search(x_test,y_test,5)\n\ndtc = DecisionTreeClassifier(criterion = optimal_params['criterion'],max_depth = optimal_params['max_depth'])\ndtc.fit(x_train, y_train)\nevaluateClassifier(x_test,y_test,dtc.predict(x_test),dtc.predict_proba(x_test))","e925a875":"def knn_grid_search(X,y,nfolds):\n    param_grid = { 'n_neighbors':range(25)}\n    knn_model = KNeighborsClassifier()\n    knn_gscv = GridSearchCV(knn_model, param_grid, cv = nfolds)\n    knn_gscv.fit(X, y)\n    return knn_gscv.best_params_\n\noptimal_params = knn_grid_search(x_train,y_train,5)\n\nknn = KNeighborsClassifier(n_neighbors = optimal_params['n_neighbors'])\nstart = time()\nknn.fit(x_train,y_train)\nstop = time()\nprint('Training time: ',stop - start)\nevaluateClassifier(x_train,y_train,knn.predict(x_train),knn.predict_proba(x_train))","ac94a291":"optimal_params = knn_grid_search(x_test,y_test,5)\n\nknn = KNeighborsClassifier(n_neighbors = optimal_params['n_neighbors'])\nknn.fit(x_train, y_train)\nevaluateClassifier(x_test,y_test,knn.predict(x_test),knn.predict_proba(x_test))","6b33395b":"## On testing data","c5552d46":"> ## Conclusion\n> ### Gaussian Naive Bayes is the best classification model for the given data set.","08505002":"##  Gaussian Naive Bayes Classifier","f72c29bd":"## Data Preparation","ccde2d16":"## On testing data","9225625a":"## Function for evaluating the classifier","c7f5864f":"> ## Inferences:\n> ### 1.On testing data, Naive Bayes give the best accuracy.\n> ### 2.The F1 score is also maximum in case of Naive Bayes.\n> ### 3.In terms of training time, KNN Classifier gives the best results.","8c5569ae":"### On training data","0140504d":"## On training data","6a8d9567":"## Imports","aeef8383":"## Decision Tree Classifier with hyper-parameter tuning****","3a1a47a3":"## KNN Classifier with hyper-parameter tuning","0ffba50f":"## On training data","4fc4af5f":"## 1. Pairwise relations in dataset using seaborn","d8b2d90d":" ##### For multi-class classification, 3 wine classification dataset is used in this notebook.<br>\n \n ## In this notebook, you will find the following things:\n \n 1. How to plot pair-wise relations using seaborn's pairplot.<br>\n \n 1. Evaluating three classifiers - Gaussian Naive Bayes, Decision Tree and KNN classifier.<br>\n \n 1. You will be able to use any of the following criteria for evaluation, as per your requirements - <br><br> \na) Training time taken by the classifier<br>\nb) Metrics including overall accuracy, classwise accuracy, precision,recall,F1-score and AUC for both training an testing data.<br>\n      \n 1. How to use confusion matrix to find TPR(true positive rate) and FPR(false positive rate).<br>\n \n 1. Tuning the hyper-parameters of KNN and Decision Tree to achieve better results.<br><br>\n \nNote: This notebook is only for demonstration purpose and you are free to use any other   \n       classifier.\n       For evaluation, call the evaluateClassifier function in a similar way as shown in the notebook.<br><br>\n**Do upvote if you find it useful.So let's get started!**       \n       ","966023b6":"## On testing data"}}