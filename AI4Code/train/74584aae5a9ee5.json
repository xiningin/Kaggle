{"cell_type":{"8cb53cbf":"code","6310e147":"code","8ada4281":"code","aa143b0b":"code","1fb823b0":"code","1b13b664":"code","fece9280":"code","760da41f":"code","57532031":"code","4e9f7a24":"code","149a8498":"code","1ea41128":"code","6c483316":"code","bf052f84":"code","6fbd339d":"code","5fe53995":"code","380220a3":"code","aa32b90f":"code","bbed5d44":"code","6e01e54b":"code","2997acb5":"code","404d2c9b":"markdown"},"source":{"8cb53cbf":"from tensorflow import keras\nimport tensorflow as tf\nimport pandas as pd\nimport os\nimport re","6310e147":"male_data = pd.read_csv('..\/input\/classify-nationalities\/Indian-Male-Names.csv')\nfemale_data = pd.read_csv('..\/input\/classify-nationalities\/Indian-Female-Names.csv')","8ada4281":"female_data.head()","aa143b0b":"repl_list = ['s\/o','d\/o','w\/o','\/','&',',','-']\n\ndef clean_data(name):\n\tname = str(name).lower()\n\tname = (''.join(i for i in name if ord(i)<128)).strip()\n\tfor repl in repl_list:\n\t\tname = name.replace(repl,\" \")\n\tif '@' in name:\n\t\tpos = name.find('@')\n\t\tname = name[:pos].strip()\n\tname = name.split(\" \")\n\tname = \" \".join([each.strip() for each in name])\n\treturn name\n\ndef remove_records(merged_data):\n\tmerged_data['delete'] = 0\n\tmerged_data.loc[merged_data['name'].str.find('with') != -1,'delete'] = 1\t\n\tmerged_data.loc[merged_data['count_words']>=5,'delete']=1\n\tmerged_data.loc[merged_data['count_words']==0,'delete']=1\n\tmerged_data.loc[merged_data['name'].str.contains(r'\\d') == True,'delete']=1\n\tcleaned_data = merged_data[merged_data.delete==0]\n\treturn cleaned_data\n\nmerged_data = pd.concat((male_data,female_data),axis=0)\n\nmerged_data['name'] = merged_data['name'].apply(clean_data)\nmerged_data['count_words'] = merged_data['name'].str.split().apply(len)\n\ncleaned_data = remove_records(merged_data)\n\nindian_cleaned_data = cleaned_data[['name','count_words']].drop_duplicates(subset='name',keep='first')\nindian_cleaned_data['label'] = 'indian'\n\nlen(indian_cleaned_data)","1fb823b0":"cleaned_data.head()","1b13b664":"merged_data.race.value_counts()","fece9280":"indian_cleaned_data.head()","760da41f":"!pip install Faker","57532031":"from faker import Faker\nfake = Faker('en_US')\nfake.name()","4e9f7a24":"from faker import Faker\nimport random\nreq = 15000\nnon_indian_names = []\n\nlangs = ['ar_EG','bs_BA','de_DE','dk_DK','en_AU','en_CA','en_GB',\n'en_IN','en_NZ','en_US','it_IT','no_NO','ro_RO']\n\nfor i in range(0,req):\n\tlng_indx = random.randint(0,len(langs)-1)\n\tfake = Faker(langs[lng_indx])\n\tnon_indian_names.append(fake.name().lower())\n\nnon_indian_names_orig = list(set(non_indian_names))\n","149a8498":"len(non_indian_names_orig)","1ea41128":"non_indian_data = pd.DataFrame({'name':non_indian_names_orig})\nnon_indian_data['count_words'] = non_indian_data['name'].str.split().apply(len)\nnon_indian_data.head()","6c483316":"indian_cleaned_data['count_words'].value_counts()","bf052f84":"non_indian_data['count_words'].value_counts()","6fbd339d":"two_word_names = non_indian_data[non_indian_data['count_words']==2]['name']\none_word_req = 5000\nnames_one_two_words = [each.split()[0] for each in two_word_names[:one_word_req]] + list(two_word_names[one_word_req:])\ncount_words = [1] * one_word_req + [2] * len(two_word_names[one_word_req:])\nnot_two_words_pd  = non_indian_data[non_indian_data['count_words']!=2]\none_two_words_pd = pd.DataFrame({'name':names_one_two_words,'count_words':count_words})\nnon_indian_data = pd.concat((not_two_words_pd,one_two_words_pd),axis=0)\nnon_indian_data['count_words'].value_counts()\nnon_indian_data['label'] = 'non_indian'\nnon_indian_data = non_indian_data[non_indian_data['count_words']<5]\nnon_indian_data['count_words'].value_counts()","5fe53995":"full_data = pd.concat((non_indian_data[['name','label']],indian_cleaned_data[['name','label']]),axis=0)\nname_data = full_data.sample(frac=1)\nname_data.head()","380220a3":"name_data['label'].value_counts()","aa32b90f":"from sklearn.model_selection import train_test_split\nX = name_data['name'].astype(str)\nY = name_data['label']\ntrain_names,test_names,train_labels,test_labels = train_test_split(X,Y,test_size=0.20,random_state =42,stratify=Y)","bbed5d44":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nvectorizer = CountVectorizer()\nX_ = vectorizer.fit_transform(train_names.values.astype('U'))\nlen(vectorizer.get_feature_names())\n","6e01e54b":"model = MultinomialNB()\nmodel.fit(X_,train_labels)\n\nX_test = vectorizer.transform(test_names.values.astype('U'))\n\ntest_predicted = model.predict(X_test)\n\nprint(classification_report(test_labels,test_predicted))","2997acb5":"check_new_names = ['lalitha','tyson','shailaja','shyamala','vishwanathan','ramanujam','conan','kryslovsky',\n'ratnani','diego','kakoli','shreyas','brayden','shanon']\n\nX_new = vectorizer.transform(check_new_names)\npredictions_nb_cv = model.predict(X_new)\ntest = pd.DataFrame({'names':check_new_names,'predictions_nb_cv':predictions_nb_cv}) \ntest","404d2c9b":"## **Result not Satisfied**"}}