{"cell_type":{"1685f69e":"code","20bbb175":"code","27612270":"code","57ede972":"code","94eaf0f2":"code","c2b10e14":"code","fe18e0cc":"code","fe18577f":"code","c34e695c":"code","6720d06b":"code","4202b562":"code","b13aa6f8":"code","4572eb3e":"code","2dbfb714":"code","8f0ea82c":"code","1dfa0970":"code","f60f0473":"code","261dd57d":"code","ba1a2d0b":"code","3b334b46":"code","51a1e056":"code","fc045efc":"code","9008b21a":"code","5b3eb3d0":"code","45826a36":"code","77b54d11":"code","e64defa4":"code","8505bff7":"code","92d98081":"code","ddd3e9dd":"code","ac1eee59":"code","60651787":"code","760a7ffa":"code","eaf5fadd":"code","f414bd11":"code","a4fbedf0":"code","28b07d49":"code","bc4b55bc":"code","fd5a49c2":"code","9c40694d":"code","0deae528":"code","03f705ae":"code","cb739d36":"code","274cc9db":"code","f56daf1f":"code","b28da124":"code","48576b5c":"code","2d280c3b":"code","46547e09":"code","8a03cb21":"code","3f522148":"code","6d60118a":"code","c68487d5":"code","f8346860":"markdown","9fa1a25b":"markdown","4678a51d":"markdown","15ef7bf5":"markdown","9a072ed9":"markdown","b149c171":"markdown","c224c124":"markdown","0abdb84d":"markdown","ccd50bce":"markdown","a8a9c48c":"markdown","f113049e":"markdown","7db0a436":"markdown","20cb755e":"markdown","63801f2b":"markdown","3824a2f5":"markdown","6ffa42c9":"markdown","dc285ee1":"markdown","fa1b53a0":"markdown","b0db96d1":"markdown","38efc02c":"markdown","7e150536":"markdown","78c23555":"markdown","4ee25b08":"markdown","e85581a7":"markdown","49f39d1f":"markdown","6e530c9e":"markdown"},"source":{"1685f69e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport statsmodels.api as sm","20bbb175":"df= pd.read_csv('..\/input\/creditcard-fraud-detection\/creditcard.csv')\ndf.head()","27612270":"from scipy.stats import zscore\ndf1=df[['Time','Amount']].apply(zscore)\ndf.drop(['Time','Amount'],axis=1,inplace=True)\ndf.head()","57ede972":"df1.head()\ndf2=pd.concat((df,df1),axis=1)\ndf2.head()","94eaf0f2":"# the no of rows and columns:\ndf2.shape","c2b10e14":"# the descriptive statistics\ndf2.describe().T","fe18e0cc":"# the target\ndf2['Class'].value_counts(normalize=True)","fe18577f":"# Checking for missing Values\ndf2.isnull().sum()","c34e695c":"df3=df2.drop('Class',axis=1)","6720d06b":"# distribution of the features and target\ncols= list(df3.columns)\nfor i in cols:\n    sns.boxplot(y=i,x=df2['Class'],data=df3)\n    plt.show()","4202b562":"x=df3\ny=df2['Class']\nx_const = sm.add_constant(x)","b13aa6f8":"import imblearn\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=2)\nx, y = smote.fit_sample(x, y.ravel())\n","4572eb3e":"x.shape","2dbfb714":"y.shape","8f0ea82c":"\nx_const = sm.add_constant(x)\nx_const.shape","1dfa0970":"model1=sm.Logit(y,x_const).fit()\nmodel1.summary()","f60f0473":"y_pred_proba=model1.predict(x_const)","261dd57d":"def pro(y_pred):\n    if y_pred <0.5:\n        y_pred=0\n    elif y_pred>0.5:\n        y_pred=1\n    return y_pred","ba1a2d0b":"y_pred=y_pred_proba.apply(pro)","3b334b46":"from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\nprint('accuracy_ score :',accuracy_score(y,y_pred))","51a1e056":"print('roc_auc_score:',roc_auc_score(y,y_pred_proba))","fc045efc":" # removing the insignificant features and making a model","9008b21a":"x_const=x_const.drop(['V21','V27'],axis=1)","5b3eb3d0":"model2= sm.Logit(y,x_const).fit()\nmodel2.summary()","45826a36":"y_pred_proba = model2.predict(x_const)\ny_pred=y_pred_proba.apply(pro)","77b54d11":"print('accuracy_score:', accuracy_score(y_pred,y))","e64defa4":"print('roc_auc_score : ',roc_auc_score(y,y_pred_proba))","8505bff7":"MACHINE LEARNING MODEL","92d98081":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)","ddd3e9dd":"lr= LogisticRegression()\nlr.fit(x_train,y_train)","ac1eee59":"y_pred_train=lr.predict(x_train)\ny_pred_test=lr.predict(x_test)\ny_train_prob = lr.predict_proba(x_train)[:,1]\ny_test_proba = lr.predict_proba(x_test)[:,1]","60651787":"from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,confusion_matrix,classification_report","760a7ffa":"print('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_train_prob))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_test_proba))","eaf5fadd":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=list(x.columns)\nvif= [variance_inflation_factor(x.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","f414bd11":"x1=x.copy()","a4fbedf0":"x1=x1.drop('V7',axis=1)","28b07d49":"X_train,X_test,Y_train,Y_test = train_test_split(x1,y,test_size=0.3,random_state=True)","bc4b55bc":"lr1=LogisticRegression()\nlr1.fit(X_train,Y_train)","fd5a49c2":"y_pred_train=lr1.predict(X_train)\ny_proba_train = lr1.predict_proba(X_train)[:,1]\ny_pred_test =lr1.predict(X_test)\ny_proba_test = lr1.predict_proba(X_test)[:,1]\n","9c40694d":"print('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_proba_train))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_proba_test))","0deae528":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=list(x1.columns)\nvif= [variance_inflation_factor(x1.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","03f705ae":"x1=x1.drop('V17',axis=1)\nX_train,X_test,Y_train,Y_test = train_test_split(x1,y,test_size=0.3,random_state=True)\nlr1=LogisticRegression()\nlr1.fit(X_train,Y_train)","cb739d36":"y_pred_train=lr1.predict(X_train)\ny_proba_train = lr1.predict_proba(X_train)[:,1]\ny_pred_test =lr1.predict(X_test)\ny_proba_test = lr1.predict_proba(X_test)[:,1]\n\nprint('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_proba_train))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_proba_test))\n","274cc9db":"cols=list(x1.columns)\nvif= [variance_inflation_factor(x1.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","f56daf1f":"x1=x1.drop('V12',axis=1)\nX_train,X_test,Y_train,Y_test = train_test_split(x1,y,test_size=0.3,random_state=True)\nlr1=LogisticRegression()\nlr1.fit(X_train,Y_train)","b28da124":"y_pred_train=lr1.predict(X_train)\ny_proba_train = lr1.predict_proba(X_train)[:,1]\ny_pred_test =lr1.predict(X_test)\ny_proba_test = lr1.predict_proba(X_test)[:,1]\n\nprint('accuracy score for train :',accuracy_score(y_train,y_pred_train))\nprint('accuracy score for test :',accuracy_score(y_test,y_pred_test))\nprint('roc_auc score for train : ',roc_auc_score(y_train,y_proba_train))\nprint('roc_auc score for test : ',roc_auc_score(y_test,y_proba_test))\n","48576b5c":"cols=list(x1.columns)\nvif= [variance_inflation_factor(x1.values,i) for i in range(len(cols))]\npd.DataFrame(vif,cols)","2d280c3b":"cols = list(x.columns)\npmax = 0\nwhile (len(cols)>1):\n   \n    X_1 = x[cols]\n    X_1 = sm.add_constant(X_1)\n    model2 = sm.Logit(y,X_1).fit()\n    p = model2.pvalues     \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","46547e09":"x2=x[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V22', 'V23', 'V24', 'V25', 'V26', 'V28', 'Time', 'Amount']]\nx2_const = sm.add_constant(x2)\nmodel3= sm.Logit(y,x2_const).fit()\nmodel3.summary()","8a03cb21":"y_pred_proba = model2.predict(x2_const)\ny_pred=y_pred_proba.apply(pro)","3f522148":"print('roc_auc_score : ',roc_auc_score(y,y_pred_proba))\nprint('accuracy_score:',accuracy_score(y,y_pred))","6d60118a":"from mlxtend.feature_selection import SequentialFeatureSelector \n\nlr = LogisticRegression()\n\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 0)\n\n\n# Build step forward feature selection\nsfs = SequentialFeatureSelector(lr,k_features = 30,forward=True,\n           floating=False, scoring='r2',\n           verbose=2,\n           cv=5)\n\nsfs = sfs.fit(X_train, y_train)\n\nsfs.k_feature_names_ ","c68487d5":"import pandas as pd\ncreditcard = pd.read_csv(\"..\/input\/creditcard-fraud-detection\/creditcard.csv\")","f8346860":"The column V12 is exhibiting the highest VIF hence dropping it to check the accuracy of model","9fa1a25b":"OVERSAMPLING USING SMOTE TO HANDEL IMBALANCED DATA","4678a51d":"Two features were eliminated by Backward Elimination Technique. The features eliminated are 'V21' and 'V27'.","15ef7bf5":" All the variables\/ features  are standardised apart from amount and time , hence standardsing it","9a072ed9":"##### checking multicollinearity using vif\n","b149c171":"The accuracy score and roc-auc score shows that their is no significant increase in the performance after removing insignificant features as well","c224c124":"From the p-value we  can find that  all the remaining variables are significant","0abdb84d":"#### Backward elimination to check significant features","ccd50bce":"### Import the dataset","a8a9c48c":"We still have features exhibiting multicollinearity as per VIF but since removal of feature is decreasing the scores hence we will try other methods of Feature selection.","f113049e":"### Importing the libraries to be used","7db0a436":" The p-value of the model is <0.05 (considering alpha to be 0.05)  and hence the model is significant.\n The p-value of individual features shows that  the fetures V21 and V27 are insignificant as the p-values are greater than the alpha.","20cb755e":"Forward Selection Method for Feature Selection","63801f2b":"From the box plot ,looks like most of the features have huge amount of outliers ,hence we will first keep the ouliers and make the model","3824a2f5":"Not much improvement from the base model. The accuracy and AUC score are still the same.","6ffa42c9":"Accuracy score has slightly increases while AUC score has slightly decreased after removal of V17.","dc285ee1":"The accuracy and the AUC score of the model is decreasing hence VIF is not helping in improving the performance of the model,so lets try Backward elimination to find significant features.","fa1b53a0":" Hence this is an imbalanced dataset with majority of the class belonging to class 0. We will try to balance the data by oversampling using SMOTE","b0db96d1":"As per vif the V17 column exhibit the highest multicollinearity , hence we will drop that and have a check.","38efc02c":" V7 has the highest vif ,hence we can remove it and try modeling","7e150536":"After oversampling the number of rows have increased from 284807 to 568630.","78c23555":"The accuracy and the AUC score for both train and test is good showing no overfitting issues.","4ee25b08":"All the above methods gets almost similar accuracy and Roc-Auc score hence not much parameter tuning or feature elimination is required to improve the performance of the model","e85581a7":"According to forward selection all the features are important.Hence the accuracy will remain same as the base moel which was built using all the features.","49f39d1f":"Making the first model using stats","6e530c9e":"Hence no missing values in the data"}}