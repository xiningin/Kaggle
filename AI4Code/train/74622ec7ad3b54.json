{"cell_type":{"39cb4b8c":"code","68bc9b8f":"code","2a9fc9c3":"code","acabd906":"code","b250625b":"code","2814a6b6":"code","db705815":"code","fc5ca098":"code","6bd0f888":"code","202cf07d":"code","94fc2083":"code","5979d759":"code","56679f1e":"code","7434a29d":"code","1b027ba6":"code","d9b85d87":"code","8626d991":"code","6c0dd3f5":"code","b7a596a9":"markdown","61251b4c":"markdown","1cf3d233":"markdown","b296f54c":"markdown","4b76d3eb":"markdown","9e2daf0e":"markdown"},"source":{"39cb4b8c":"!cp -r '..\/input\/recipes' .\/","68bc9b8f":"import os\nfrom PIL import Image\nimport numpy as np\nimport random\nfrom collections import Counter\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","2a9fc9c3":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","acabd906":"input_path = \".\/recipes\"\nclasses = os.listdir(input_path)","b250625b":"images_name = []\nimages_classes_wise = []\nfor folder in os.listdir(input_path):\n    image_class = []\n    for image in os.listdir(input_path + '\/'+ folder + '\/'):\n        images_name.append(image)\n        image_class.append(image)\n    images_classes_wise.append(image_class)","2814a6b6":"print(f'Total Number of images in dataset = {len(images_name)}')\nprint(f'No. of images in {classes} are {[len(x) for x in images_classes_wise]}')","db705815":"# extensions of image\nextension = [x.split('.')[-1] for x in images_name ]\nextensions = dict(Counter(extension))\nprint(f'Types of images in dataset ={extensions}')","fc5ca098":"import os\n\nskipped = 0\n\nfor folder_name in classes:\n    folder_path = os.path.join(input_path, folder_name)\n    for fname in os.listdir(folder_path):\n        fpath = os.path.join(folder_path, fname)\n        try:\n            fobj = open(fpath, \"rb\")\n            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n        finally:\n            fobj.close()\n\n        if not is_jfif:\n            skipped += 1\n            # Delete corrupted image\n            os.remove(fpath)\n\nprint(\"Deleted %d images\" % skipped)","6bd0f888":"IMAGE_SIZE = (256, 256)\nBATCH_SIZE = 32\nINPUT_SHAPE = (256, 256, 3)\nepochs = 1000","202cf07d":"datagen = ImageDataGenerator(\n    rescale = 1.\/255,\n    samplewise_std_normalization = True,\n    rotation_range= 60,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip=True,\n    height_shift_range = 0.2,\n    fill_mode='reflect',\n    validation_split = 0.2,\n)","94fc2083":"train_dataset = datagen.flow_from_directory(\n    input_path,\n    target_size = IMAGE_SIZE,\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    subset = 'training',\n)\n\nvalidation_dataset = datagen.flow_from_directory(\n    input_path,\n    target_size = IMAGE_SIZE,\n    batch_size = BATCH_SIZE,\n    class_mode = 'categorical',\n    subset = 'validation'\n)","5979d759":"model = keras.Sequential()\n\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=INPUT_SHAPE))\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\nmodel.add(keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'))\nmodel.add(keras.layers.Conv2D(filters=64, kernel_size=(2,2), activation='relu'))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(keras.layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'))\nmodel.add(keras.layers.Conv2D(filters=128, kernel_size=(2,2), activation='relu'))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(keras.layers.Flatten())\n\nmodel.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Dense(64, activation='relu')) \nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(len(classes), activation='softmax'))\nmodel.summary()","56679f1e":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', restore_best_weights=True, patience=20)\nopt = keras.optimizers.Adam(learning_rate=0.0007)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","7434a29d":"history = model.fit_generator(train_dataset,\n                    steps_per_epoch = 600\/\/BATCH_SIZE,\n                    epochs= epochs, \n                    validation_data = validation_dataset,\n                    validation_steps = 120 \/\/ BATCH_SIZE,\n                    callbacks = [early_stopping],\n                    verbose = 1,\n                   )","1b027ba6":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","d9b85d87":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, 'y', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","8626d991":"model.save('.\/model.h5')","6c0dd3f5":"model.evaluate(validation_dataset)","b7a596a9":"# Data Analysis","61251b4c":"# Import Files","1cf3d233":"# Checking and Removing incompatible type of images","b296f54c":"# Building dataset\nData Augmentation as dataset is very small for a neural network","4b76d3eb":"In this phase we will try to understand dataset in depth.","9e2daf0e":"# Model"}}