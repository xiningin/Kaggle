{"cell_type":{"7797d1df":"code","7d54548c":"code","85430a90":"code","f182af58":"code","125c62a0":"code","a6c89484":"code","0f7f3781":"code","067f203d":"code","a15422e0":"code","89b67260":"code","e16952a0":"code","fb526232":"code","a75f3873":"code","5874353d":"code","8eb6534c":"code","4170890b":"code","ecc63143":"code","cd0fb224":"code","499cffad":"code","dd29aaea":"code","276b84ec":"code","6af12808":"code","cfbf9f13":"code","62914cda":"code","51131887":"code","a21773a5":"code","4cf7cc03":"code","0d7d703c":"code","ae67aff4":"code","24974ad4":"code","20306221":"code","4c6b08ce":"code","1ded64a4":"code","2ecc6319":"code","c47454d8":"markdown","e0d61074":"markdown","d096b09c":"markdown","098d90f0":"markdown","17c2f513":"markdown","75917bb2":"markdown","dc9e703e":"markdown","1cb10a71":"markdown"},"source":{"7797d1df":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7d54548c":"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB \n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe","85430a90":"data = pd.read_csv('..\/input\/music-genre-classification\/dataset.csv')\nprint(data.shape)\ndata.head()","f182af58":"data.describe()","125c62a0":"data.isnull().any().any()","a6c89484":"sns.countplot(data=data, y='label')","0f7f3781":"plt.subplots(3, 2, figsize=(20,10))\nfor i, col in enumerate(['chroma_stft', 'rmse', 'spectral_centroid', 'spectral_bandwidth', 'rolloff', 'zero_crossing_rate']):\n    plt.subplot(3, 2, i+1)\n    sns.violinplot(data=data, x='label', y=col)","067f203d":"plt.subplots(10, 2, figsize=(20,30))\nfor i in range(1,21):\n    plt.subplot(10, 2, i)\n    sns.violinplot(data=data, x='label', y=('mfcc'+str(i)))","a15422e0":"X = data.drop(columns=['filename', 'label'])\ny = data.label\n\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","89b67260":"nb = MultinomialNB()\nnb.fit(X_train, y_train)\n\ny_pred = nb.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\n\nnb_f1 = f1_score(y_test, y_pred, average='macro')\nnb_f1","e16952a0":"gaus = GaussianNB()\ngaus.fit(X_train, y_train)\n\ny_pred = gaus.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\n\ngaus_f1 = f1_score(y_test, y_pred, average='macro')\ngaus_f1","fb526232":"f1_scores = pd.DataFrame(data={'model': ['MultinomialNB', 'GaussianNB'],\n                               'f1_score': [nb_f1, gaus_f1]})\nf1_scores","a75f3873":"X = data.drop(columns=['filename', 'label'])\nlabel_enc = LabelEncoder()\ny = label_enc.fit_transform(data.label)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","5874353d":"#np.array(y_test).value_counts()\nnp.bincount(y_test)","8eb6534c":"rf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)","4170890b":"y_pred = rf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nrf_f1 = f1_score(y_test, y_pred, average='macro')\nrf_f1","ecc63143":"grid_params = {'n_estimators': [75, 100, 150],\n               'max_features': ['log2', 'sqrt'],\n               'min_samples_split': [2, 3, 4],\n               'min_samples_leaf': [1, 2, 3]}\nrf_grid = GridSearchCV(RandomForestClassifier(random_state=42), \n                       param_grid=grid_params,\n                       scoring='f1_macro',\n                       cv=5, verbose=1, n_jobs = -1)\nrf_grid.fit(X_train, y_train)\nrf_grid.best_params_","cd0fb224":"y_pred = rf_grid.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nrf_grid_f1 = f1_score(y_test, y_pred, average='macro')\nrf_grid_f1","499cffad":"rf_dicts = [{'model': 'RandomForestClassifier', 'f1_score': rf_f1},\n            {'model': 'RandomForestClassifier + GridSearchCV', 'f1_score': rf_grid_f1}]\nf1_scores = f1_scores.append(rf_dicts, ignore_index=True)\nf1_scores","dd29aaea":"gbc = GradientBoostingClassifier(random_state=42)\ngbc.fit(X_train, y_train)","276b84ec":"y_pred = gbc.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\ngbc_f1 = f1_score(y_test, y_pred, average='macro')\ngbc_f1","6af12808":"grid_params = {'n_estimators': [100], #[50, 100, 150],\n               'learning_rate': [0.1], #[0.06, 0.08, 0.1],\n               'max_depth': [3], #[2, 3, 4]\n              }\ngbc_grid = GridSearchCV(GradientBoostingClassifier(random_state=42, n_iter_no_change=25), \n                        param_grid=grid_params,\n                        scoring='f1_macro',\n                        cv=5, verbose=3, n_jobs = -1)\ngbc_grid.fit(X_train, y_train)\ngbc_grid.best_params_","cfbf9f13":"y_pred = gbc_grid.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\ngbc_grid_f1 = f1_score(y_test, y_pred, average='macro')\ngbc_grid_f1","62914cda":"gbc_dicts = [{'model': 'GradientBoostingClassifier', 'f1_score': gbc_f1},\n             {'model': 'GradientBoostingClassifier + GridSearchCV', 'f1_score': gbc_grid_f1}]\nf1_scores = f1_scores.append(gbc_dicts, ignore_index=True)\nf1_scores","51131887":"X = data.drop(columns=['filename', 'label'])\nlabel_enc = LabelEncoder()\ny = label_enc.fit_transform(data.label)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)","a21773a5":"xgb_model = XGBClassifier(random_state=42, eval_metric='merror', objective='multi:softmax', use_label_encoder=False)\nxgb_model.fit(X_train, y_train)","4cf7cc03":"y_pred = xgb_model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nxgb_f1 = f1_score(y_test, y_pred, average='macro')\nxgb_f1","0d7d703c":"# https:\/\/www.kaggle.com\/prashant111\/a-guide-on-xgboost-hyperparameters-tuning\nspace = {'max_depth': 4, #hp.quniform('max_depth', 2, 8, 1),\n         'gamma': 0.108925004633226, #hp.uniform('gamma', 0.05, 0.35), #hp.uniform('gamma', 0, 1), \n         'reg_alpha' : 0, #hp.quniform('reg_alpha', 0, 5, 1), #hp.quniform('reg_alpha', 0, 10, 1),\n         'reg_lambda' : 1, #hp.uniform('reg_lambda', 0, 2),\n         'min_child_weight' : 1, #hp.quniform('min_child_weight', 0, 10, 1),\n         'n_estimators': 200,\n         #'tree_method': 'gpu_hist'\n        }\nspace_plot_metric = 'gamma'\n\neval_metric = 'merror'\nxgb_objective = 'multi:softmax'\nnum_class = len(data.label.unique())\nnum_rounds = 999\nnum_folds = 5\nrandom_state = 42\nearly_stopping_rounds = 25\nevaluation=[(X_train, y_train), (X_test, y_test)]","ae67aff4":"def objective(space):\n    print(space)\n    clf = xgb.XGBClassifier(\n        max_depth=int(space['max_depth']), \n        gamma=space['gamma'],\n        reg_alpha=int(space['reg_alpha']), \n        reg_lambda=int(space['reg_lambda']), \n        min_child_weight=int(space['min_child_weight']),\n        n_estimators=space['n_estimators'], \n        #tree_method=space['tree_method'],\n        random_state=random_state,\n        eval_metric=eval_metric, objective=xgb_objective, num_class=num_class, \n        use_label_encoder=False)\n\n    clf.fit(X_train, y_train,\n            eval_set=evaluation, \n            eval_metric=eval_metric,\n            early_stopping_rounds=early_stopping_rounds,\n            verbose=False)\n    \n    y_pred = clf.predict(X_test)\n    xgb_obj_f1 = f1_score(y_test, y_pred, average='macro')\n\n    x_vals.append(space[space_plot_metric])\n    y_score.append(xgb_obj_f1)\n    print (f'SCORE: {xgb_obj_f1}')\n    return { 'loss': -xgb_obj_f1, 'status': STATUS_OK }","24974ad4":"trials = Trials()\n\nx_vals = []\ny_score = []\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 1,\n                        trials = trials)","20306221":"sns.scatterplot(x=x_vals, y=y_score)\nbest_hyperparams","4c6b08ce":"xgb_model_opt = xgb.XGBClassifier(**space,\n                                  random_state=random_state,\n                                  eval_metric=eval_metric, objective=xgb_objective, num_class=num_class,\n                                  use_label_encoder=False)\nxgb_model_opt.fit(X_train, y_train,\n                  eval_set=evaluation, \n                  eval_metric=eval_metric,\n                  early_stopping_rounds=50,\n                  verbose=False)","1ded64a4":"y_pred = xgb_model_opt.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d')\nxgb_opt_f1 = f1_score(y_test, y_pred, average='macro')\nxgb_opt_f1","2ecc6319":"xgb_dicts = [{'model': 'XGBClassifier', 'f1_score': xgb_f1},\n             {'model': 'XGBClassifier Optimized', 'f1_score': xgb_opt_f1}]\nf1_scores.append(xgb_dicts, ignore_index=True)","c47454d8":"## EDA","e0d61074":"### Using hyperopt\nThe way I ran it was by tuning one parameter at a time:\n1. Set a range for the parameter using hp.quniform or hp.uniform.\n2. Uncomment that entry in the objective() function.\n3. Set the max_evals to an appropriate value in the fmin() function.  \n   For example, I used 15 when testing the interger values, and typically to 100 or 200 for the gamma.\n4. Plot the results. If it's obvious that there's a certain range that's performing better, reduce the range and rerun fmin().\n   For example, for gamma, nearly every value below 0.5 had better results.\n5. Take the best value and set it. \n6. Rinse and repeat for all the parameters.\n  \n**Note:** The current results were on the CPU. Setting the tree_method and setting the Kaggle Accelerator did speed up the process, but ultimately gave slighly worse results\n","d096b09c":"# Music Genre Classification\n\n### Data: \n* filename\n* chroma_stft\n* rmse\n* spectral_centroid\n* spectral_bandwidth\n* rolloff\n* zero_crossing_rate\n* mfcc(1-20)\n\n### Target\n* label = blues, classical, country, disco, hiphop, jazz, metal, pop, reggae, rock\n\nWill be using a macro F1-score, as the target is well balanced\n","098d90f0":"# Baseline - NaiveBayes","17c2f513":"# Gradient Boosting","75917bb2":"# XGBBoost\nSee https:\/\/www.kaggle.com\/prashant111\/a-guide-on-xgboost-hyperparameters-tuning for details on hyperparameter tuning methods used here.","dc9e703e":"### This next cell with the GridSearchCV takes a while.","1cb10a71":"## Random Forest"}}