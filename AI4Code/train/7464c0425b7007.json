{"cell_type":{"8c5d919b":"code","1c0b99f4":"code","93e538fe":"code","012e8429":"code","539bcc33":"code","2ed6f5f2":"code","3ad18656":"code","6e43f479":"code","50610da3":"code","75bf987a":"code","04f84ef5":"code","373050a0":"code","b86e20eb":"code","4e0dfc1f":"markdown","06a4594c":"markdown","9ff5247c":"markdown","83ced42a":"markdown","e175eaab":"markdown","7691aa2f":"markdown","4d2aa41e":"markdown","f4f7e3f4":"markdown","d51c1b93":"markdown","346445e5":"markdown","cbfdf57f":"markdown"},"source":{"8c5d919b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c0b99f4":"# ID in test set (corresponds to a shop_id and item_id, predicted number of items sold\nsample_submission = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")\n# items: item_name, item_id, item_category_id\nitems = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\") \n# item_categories: item_category_name, item_category_id\nitem_categories = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\n# sales_train: date, date_block_num, shop_id, item_id, item_price, item_cnt_day**\nsales_train = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\n# shops: shop_name, shop_id\nshops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")","93e538fe":"# merge all training data together: items, item_categories, sales_train, shops\nitems_with_categories = pd.merge(items, item_categories, on=\"item_category_id\")\nsales_with_items = pd.merge(sales_train, items_with_categories, on=\"item_id\")\ntrain_merged = pd.merge(sales_with_items, shops, on=\"shop_id\")\ntrain_merged.info()","012e8429":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ntrain_merged = downcast_dtypes(train_merged)\nprint(train_merged.info())","539bcc33":"# Pandas profiling automates some early EDA\nfrom pandas_profiling import ProfileReport\nprofile = ProfileReport(train_merged.sample(frac=0.01), title=\"Training Data Profile\")","2ed6f5f2":"# show profile\nprofile","3ad18656":"test_profile = ProfileReport(test.sample(frac=0.01), title=\"Test Data Profile\")\ntest_profile","6e43f479":"train_merged['date'] = pd.to_datetime(train_merged['date'])\ntrain_merged.dtypes","50610da3":"# for now we only care about a couple of features so I'll subset the training df to make it easier to work with.\nX = train_merged[[\"date_block_num\", \"shop_id\", \"item_id\", \"item_cnt_day\"]]","75bf987a":"# aggregate daily data and construct a monthly sales data frame\nX[\"item_cnt_month\"] = X.groupby([\"date_block_num\", \"shop_id\", \"item_id\"])[\"item_cnt_day\"].transform(np.sum)\n\n# remove item_cnt_day column as it's no longer useful for previous benchmark\ndel X[\"item_cnt_day\"]\n\n# sorting by the monthly item count for each row we can see that each row still represents a single day. For a row to represent the entire month, just eliminate all duplicates across (date_block_num, shop_id, and item_id)\nX.sort_values(by=\"item_cnt_month\",ascending= False)\nprint(len(X))\n\nX = X.drop_duplicates([\"date_block_num\", \"shop_id\", \"item_id\"])\nprint(len(X))","04f84ef5":"# filter down to just Oct 2015\nlast_month = X[\"date_block_num\"] == 33\nlast_month_X = X[last_month].copy()\nlast_month_X = last_month_X.sort_values(by=[\"shop_id\", \"item_id\"])","373050a0":"# match up item_cnt_month from last month with test set - #join on shop_id, item_id\n\ncombined = pd.merge(test, last_month_X, on=[\"shop_id\", \"item_id\"], how=\"left\")\n\n# some shops didn't sell a given item in October 2015. As a result, there are lot's of NaNs.\n# make NaNs zero\ncombined = combined.fillna(0) # could impute the mean or median month count value from previous months, but that's even a bit too much for a basic benchmark\n\n#clip results between [0,20]\ncombined[\"item_cnt_month\"] = combined[\"item_cnt_month\"].clip(upper=20)\n\n# only keep necessary columns for submission\nsubmission = combined[[\"ID\", \"item_cnt_month\"]]\nsubmission.head()","b86e20eb":"# submit benchmark - achieves RMSE of 1.16777\nsubmission.to_csv(\"submission.csv\", index=False)","4e0dfc1f":"It's first important to notice that the test set really contains three key variables.\n- shop_id\n- item_id\n- ID (unique identifier for shop_id, item_id tuple.\n\nThis is key because we'll be submitting an output file that uses the unique ID for shop_id and item_id pairs. Our target variable will be the number of sales a given shop made of a given product for a particular month (in this case - November 2015)\n\nOff the bat, it looks like the test set has a slightly different distribution in terms of number of unique shops (~ 42 in test vs. ~ 60 in train) and items covered (~ 1750 in test vs. ~ **8500 in train). This is in part due to the fact that Pandas Profiler samples a small percentage of the total df. This will be worth investigating more in the future by comparing the percentage representation of data points for a given shop or product in the train set vs. the test set.","06a4594c":"Downcasting got our dataframe down to 157MB (a huge improvement). We likely won't need all these variables when we train our model so we should be able to decrease this size further once we start actually using the data.","9ff5247c":"# EDA & Previous Value Benchmark\n\nBasically, this is the first jump into the data. \n- EDA -\nTrying to build up an intuition for the data and the competition challenge. \n- Previous Value Benchmark -\nOnce, we have some intuition, I'll build a very basic benchmark using previous values as predictions. No need to train any model yet. It's helpful to have a benchmark against which to evaluate all future models.","83ced42a":"### Pandas Profiler\n\nThe Pandas Profiler does some initial basic EDA. It's helpful to use to get a first high-level overview of the data.","e175eaab":"### Compare Training and Test Set Distributions\nIt's important to check the distribution in the training data compared with the distribution in the test data. We want to know if they have been pulled from the same distribution.\n","7691aa2f":"### Key Top Level Takeaways from Profiler\n\nRight off the bat there are a few high level takeaways that help us understand the data we have a bit better.\n1. There is no missing data (good!)\n2. There are no duplicate rows (good!)\n3. There are 34 date blocks -- month intervals in the data\n4. There are around 60 shops\n5. There are around 8500 unique items for sale\n","4d2aa41e":"## Clean the Data\n\n* **Data Types**:\nThe first thing we should do is convert the date column in the training data from an \"object\" to a \"datetime\" dtype.\n\n* **Constant features**:\nThere doesn't appear to be any constant features (features with no variation across all rows)\n\n* **Duplicated features and rows**:\nAs we discovered from the Pandas Profiler, there is no missing data and there are no duplicates","f4f7e3f4":"## Load Data","d51c1b93":"# Previous Value Benchmark\n\nOk, so now we have a basic intuition and understanding of what's in our dataset. \n\nLet's build a very basic benchmark model. The assumption of the previous value benchmark approach is that the number of items sold at a particular shop doesn't vary too much from month-to-month. As a result, we can just straight up submit the monthy sales for October 2015 as a proxy for November 2015 (the test set period).\n\n### Project Tip #2 (From Coursera):\nA good exercise is to reproduce previous_value_benchmark. As the name suggest - in this benchmark for the each shop\/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.\n\nThe most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard.\n\nGenerating features like this is a necessary basis for more complex models. Also, if you decide to fit some model, don't forget to clip the target into [0,20] range, it makes a big difference.","346445e5":"## Task Background\n\nAim: We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills. You are provided with daily historical sales data.\n\nThe task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\nEvaluation: Submissions are evaluated by root mean squared error (RMSE). True target values are clipped into [0,20] range.\n\n\nYou are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\n**File descriptions**\n* sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n* test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n* sample_submission.csv - a sample submission file in the correct format.\n* items.csv - supplemental information about the items\/products.\n* item_categories.csv  - supplemental information about the items categories.\n* shops.csv- supplemental information about the shops.\n\n**Data fields**\n* ID - an Id that represents a (Shop, Item) tuple within the test set\n* shop_id - unique identifier of a shop\n* item_id - unique identifier of a product\n* item_category_id - unique identifier of item category\n* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n* item_price - current price of an item\n* date - date in format dd\/mm\/yyyy\n* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* item_name - name of item\n* shop_name - name of shop\n* item_category_name - name of item category","cbfdf57f":"### Downcast Dataframe\nOur DF will take up a lot of memory (almost 250MB!), but we can downcast the columns to types to representations that use fewer bits. \nLearned about this from another notebook in this competition: https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data. Below code for downcasting is pulled from there."}}