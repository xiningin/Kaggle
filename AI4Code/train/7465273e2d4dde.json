{"cell_type":{"ef8b6076":"code","6659099f":"code","c2668b0e":"code","66fd2160":"code","160d6f45":"code","8f513395":"code","65b3a78d":"code","36b1ec68":"code","d4c74ed0":"code","9eb36ecd":"code","8bf0c3eb":"code","bcb78d56":"code","d48af100":"code","9064a63c":"code","7e421dd2":"code","1ca37dc6":"code","11bca2a9":"code","34f74958":"code","4882bdd7":"code","4dde4a3d":"code","cf29e243":"markdown","faf5ae70":"markdown","77694f5e":"markdown","40ea7e75":"markdown","173088e3":"markdown","dde3268a":"markdown","3066c291":"markdown","039d5374":"markdown","b92d5abc":"markdown","22e29487":"markdown","0755765a":"markdown","25e5509d":"markdown","132876d3":"markdown","e5c7b4d8":"markdown","7bebbdc7":"markdown","ec003e9d":"markdown","7f19fb89":"markdown","e40c11c4":"markdown","d86784ab":"markdown","296bb69f":"markdown","6a130242":"markdown","7ea76c36":"markdown","65a07c81":"markdown","5739dcf3":"markdown","ffad7666":"markdown","435a1835":"markdown","4c6f9739":"markdown","da9192f0":"markdown","df418e7b":"markdown"},"source":{"ef8b6076":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6659099f":"df_train=pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")","c2668b0e":"print(\"***** Shape of training dataset *****\")\nprint()\nprint(df_train.shape)\n","66fd2160":"print(\"***** First five rows of training dataset *****\")\nprint()\ndf_train.head()","160d6f45":"print(\"***** dtypes of our training dataset *****\")\nprint()\nprint(df_train.dtypes)","8f513395":"print(\"***** Basic stats about our training dataset *****\")\nprint()\ndf_train.drop(['id'],axis=1).describe().T.style.bar(subset=['mean'],color='#205ff2').background_gradient(subset=['std'],cmap='coolwarm').background_gradient(subset=['50%'],cmap='coolwarm').background_gradient(subset=['75%'],cmap='coolwarm')","65b3a78d":"print(\"***** Value counts of target columns *****\")\nprint()\ndf_train['target'].value_counts()","36b1ec68":"print(\"***** Shape of test dataset *****\")\nprint()\nprint(df_test.shape)\n","d4c74ed0":"print(\"***** First five rows of test dataset *****\")\nprint()\ndf_test.head()","9eb36ecd":"print(\"***** dtypes of our test dataset *****\")\nprint()\nprint(df_test.dtypes)","8bf0c3eb":"print(\"***** Basic stats about our test dataset *****\")\nprint()\ndf_test.drop(['id'],axis=1).describe().T.style.bar(subset=['mean'],color='#205ff2').background_gradient(subset=['std'],cmap='coolwarm').background_gradient(subset=['50%'],cmap='coolwarm').background_gradient(subset=['75%'],cmap='coolwarm')","bcb78d56":"def with_hue(data,feature,ax):\n    \n    #Numnber of categories\n    num_of_cat=len([x for x in data[feature].unique() if x==x])\n    \n    bars=ax.patches\n    \n    for ind in range(num_of_cat):\n        ##     Get every hue bar\n        ##     ex. 8 X categories, 4 hues =>\n        ##    [0, 8, 16, 24] are hue bars for 1st X category\n        hueBars=bars[ind:][::num_of_cat] \n        # Get the total height (for percentages)\n        total=sum([x.get_height() for x in hueBars])\n        #Printing percentages on bar\n        for bar in hueBars:\n            percentage='{:.1f}%'.format(100 * bar.get_height()\/total)\n            ax.text(bar.get_x()+bar.get_width()\/2.0,\n                   bar.get_height(),\n                   percentage,\n                    ha=\"center\",va=\"bottom\",fontweight='bold',fontsize=14)\n    \n\n    \ndef without_hue(data,feature,ax):\n    \n    total=float(len(data))\n    bars_plot=ax.patches\n    \n    for bars in bars_plot:\n        percentage = '{:.1f}%'.format(100 * bars.get_height()\/total)\n        x = bars.get_x() + bars.get_width()\/2.0\n        y = bars.get_height()\n        ax.text(x, y,(percentage,bars.get_height()),ha='center',fontweight='bold',fontsize=14)","d48af100":"#setting theme\nsns.set_theme(context='notebook',style='white',font_scale=3)\n\n#setting the background and foreground color\nfig=plt.figure(figsize=(24,12))\nax=plt.axes()\nax.set_facecolor(\"#F2EDD7FF\")\nfig.patch.set_color(\"#F2EDD7FF\")\n\n#Dealing with spines\nfor i in ['left','top','right']:\n    ax.spines[i].set_visible(False)\n    \nax.grid(linestyle=\"--\",axis='y',color='gray')\n\n#countplot\na=sns.countplot(data=df_train,x='target',saturation=3,palette='cool')\n\nwithout_hue(df_train,'target',a)\n\nplt.title(\"Target Distribution\",weight='bold')","9064a63c":"fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(18,12))\n\n#Train Dataset\n#ax=plt.axes()\nfor i in range(0,2):\n    ax[0].set_facecolor(\"#F2EDD7FF\")\n    \n#ax.set_facecolor(\"#F2EDD7FF\")\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nsns.set_palette(['#ff355d']*75)\n\nfor i in ['top','right']:\n    ax[0].spines[i].set_visible(False)\n    \nax[0].grid(linestyle=\"--\",axis='x',color='gray')\n\ny_col=df_train.columns[1:76]\ny_col=list(y_col)\nunique=[]\nfor i in y_col:\n    unique.append(df_train[i].nunique())\n\na=sns.barplot(x=unique,y=y_col,orient='h',zorder=2,alpha=1,saturation=1,linewidth=0,ax=ax[0])\na.set_xlabel(\"Unique Values\",fontsize=6, weight='bold')\na.set_ylabel(\"Features\",fontsize=6, weight='bold')\na.tick_params(labelsize=6, width=1, length=1.5)\na.text(2,-1.9,\"Unique values of each feature in training dataset\",fontsize=10,fontweight='bold')\nbars=a.patches\nfor bar in bars:\n    x = bar.get_x() + bar.get_width()+2\n    y = bar.get_y() + bar.get_height() \/ 2 \n    a.text(x, y,bar.get_width(),ha='center',va='center',fontweight='bold',fontsize=6)\n    \n\n\n#Test Dataset\n#ax=plt.axes()\nfor i in range(0,2):\n    ax[1].set_facecolor(\"#F2EDD7FF\")\n    \n#ax.set_facecolor(\"#F2EDD7FF\")\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nsns.set_palette(['#84DE02']*75)\n\nfor i in ['top','right']:\n    ax[1].spines[i].set_visible(False)\n    \nax[1].grid(linestyle=\"--\",axis='x',color='gray')\n\ny_col=df_test.columns[1:76]\ny_col=list(y_col)\nunique_test=[]\nfor i in y_col:\n    unique_test.append(df_test[i].nunique())\n\nb=sns.barplot(x=unique_test,y=y_col,orient='h',zorder=2,alpha=1,saturation=1,linewidth=0,ax=ax[1])\nb.set_xlabel(\"Unique Values\",fontsize=6, weight='bold')\nb.set_ylabel(\"Features\",fontsize=6, weight='bold')\nb.tick_params(labelsize=6, width=1, length=1.5)\nb.text(2,-1.9,\"Unique values of each feature in test dataset\",fontsize=10,fontweight='bold')\nbars=b.patches\nfor bar in bars:\n    x = bar.get_x() + bar.get_width()+2\n    y = bar.get_y() + bar.get_height() \/ 2 \n    b.text(x, y,bar.get_width(),ha='center',va='center',fontweight='bold',fontsize=6)\n    \n\n    \n#Difference in unique values between features\nfor i in range(0,2):\n    ax[2].set_facecolor(\"#F2EDD7FF\")\n    \n#ax.set_facecolor(\"#F2EDD7FF\")\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nsns.set_palette(['#fefe22']*75)\n\nfor i in ['top','right']:\n    ax[2].spines[i].set_visible(False)\n    \nax[2].grid(linestyle=\"--\",axis='x',color='gray')\n\ny_col=df_test.columns[1:76]\ny_col=list(y_col)\nunique_diff=[]\nfor i in y_col:\n    unique_diff.append(df_train[i].nunique()-df_test[i].nunique())\n\nb=sns.barplot(x=unique_diff,y=y_col,orient='h',zorder=2,alpha=1,saturation=1,linewidth=0,ax=ax[2])\nb.set_xlabel(\"Unique Values\",fontsize=6, weight='bold')\nb.set_ylabel(\"Features\",fontsize=6, weight='bold')\nb.tick_params(labelsize=6, width=1, length=1.5)\nb.text(2,-1.9,\"Difference of unique values in each features\",fontsize=10,fontweight='bold')\nbars=b.patches\nfor bar in bars:\n    x = bar.get_x() + bar.get_width()+2\n    y = bar.get_y() + bar.get_height() \/ 2 \n    b.text(x, y,bar.get_width(),ha='center',va='center',fontweight='bold',fontsize=6)\n    \n","7e421dd2":"fig,ax=plt.subplots(nrows=1,ncols=3,figsize=(15,12))\n\n#Training dataset\nzeroes=(((df_train.iloc[:,1:76]==0).sum())\/len(df_train))*100\n\nzero=np.array(zeroes)\nhundred=[100]*75\nfig=plt.figure(figsize=(12,12))\n\n#ax=plt.axes()\nfor i in range(0,2):\n    ax[0].set_facecolor(\"#F2EDD7FF\")\n\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nsns.set_palette(['#ff355d']*75)\n\nfor i in ['top','right']:\n    ax[0].spines[i].set_visible(False)\n    \nax[0].grid(linestyle=\"--\",axis='x',color='gray')\n\nsns.barplot(y=zeroes.index,x=hundred, color='#dadada',ax=ax[0])\nbarh = sns.barplot(y=zeroes.index, x=zero,zorder=2,alpha=1,saturation=1,linewidth=0,ax=ax[0])\nbarh.tick_params(labelsize=6, width=1, length=1.5)\nbarh.set_xlabel(\"% of zeroes\",fontsize=4, weight='bold')\nbarh.set_ylabel(\"Features\",fontsize=4, weight='bold')\n\nbarh.text(2,-1.9,\"% of zeroes in training data of  each feature\",fontsize=10,fontweight='bold')\n\nbars=barh.patches\nfor bar in bars:\n    x = bar.get_x() + bar.get_width()+2\n    y = bar.get_y() + bar.get_height() \/ 2 \n    percentage=str(bar.get_width())[:5]+\"%\"\n    barh.text(x, y,percentage,ha='center',va='center',fontweight='bold',fontsize=6)\n\n    \n    \n#Test dataset\nzeroes_test=(((df_test.iloc[:,1:76]==0).sum())\/len(df_test))*100\n\nzero_test=np.array(zeroes_test)\nhundred_test=[100]*75\nfig=plt.figure(figsize=(12,12))\n\n#ax=plt.axes()\nfor i in range(0,2):\n    ax[1].set_facecolor(\"#F2EDD7FF\")\n\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nsns.set_palette(['#84DE02']*75)\n\nfor i in ['top','right']:\n    ax[1].spines[i].set_visible(False)\n    \nax[1].grid(linestyle=\"--\",axis='x',color='gray')\n\nsns.barplot(y=zeroes_test.index,x=hundred_test, color='#dadada',ax=ax[1])\nbarh = sns.barplot(y=zeroes_test.index, x=zero_test,zorder=2,alpha=1,saturation=1,linewidth=0,ax=ax[1])\nbarh.tick_params(labelsize=6, width=1, length=1.5)\nbarh.set_xlabel(\"% of zeroes\",fontsize=4, weight='bold')\nbarh.set_ylabel(\"Features\",fontsize=4, weight='bold')\n\nbarh.text(2,-1.9,\"% of zeroes in test data of each feature\",fontsize=10,fontweight='bold')\n\nbars=barh.patches\nfor bar in bars:\n    x = bar.get_x() + bar.get_width()+2\n    y = bar.get_y() + bar.get_height() \/ 2 \n    percentage=str(bar.get_width())[:5]+\"%\"\n    barh.text(x, y,percentage,ha='center',va='center',fontweight='bold',fontsize=6)\n    \n    \n    \n#difference in % of zeroes\nzeroes_test=(((df_train.iloc[:,1:76]==0).sum())\/len(df_train))*100-(((df_test.iloc[:,1:76]==0).sum())\/len(df_test))*100\n\nzero_test=np.array(zeroes_test)\nhundred_test=[100]*75\nfig=plt.figure(figsize=(12,12))\n\n#ax=plt.axes()\nfor i in range(0,2):\n    ax[2].set_facecolor(\"#F2EDD7FF\")\n\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nsns.set_palette(['#84DE02']*75)\n\nfor i in ['top','right']:\n    ax[2].spines[i].set_visible(False)\n    \nax[2].grid(linestyle=\"--\",axis='x',color='gray')\n\nsns.barplot(y=zeroes_test.index,x=hundred_test, color='#dadada',ax=ax[2])\nbarh = sns.barplot(y=zeroes_test.index, x=zero_test,zorder=2,alpha=1,saturation=1,linewidth=0,ax=ax[2])\nbarh.tick_params(labelsize=6, width=1, length=1.5)\nbarh.set_xlabel(\"% of zeroes\",fontsize=4, weight='bold')\nbarh.set_ylabel(\"Features\",fontsize=4, weight='bold')\n\nbarh.text(2,-1.9,\"Difference in % of zeroes in each feature\",fontsize=10,fontweight='bold')\n\nbars=barh.patches\nfor bar in bars:\n    x = bar.get_x() + bar.get_width()+2\n    y = bar.get_y() + bar.get_height() \/ 2 \n    percentage=str(bar.get_width())[:5]+\"%\"\n    barh.text(x, y,percentage,ha='center',va='center',fontweight='bold',fontsize=6)\n\n","1ca37dc6":"print(\"***** Distribution of each feature in training dataset *****\")\nfig,ax=plt.subplots(nrows=19,ncols=4,figsize=(20,40))\nfig.patch.set_facecolor(\"#F2EDD7FF\")\ncols=list(df_train.columns)[1:76]\nfor i in range(0,19):\n    for j in range(0,4):\n        try:\n            ax[i][j].set_facecolor(\"#F2EDD7FF\")\n            a=sns.kdeplot(data=df_train,x=df_train[cols[i*4+j]],ax=ax[i][j],hue=df_train['target'],legend=False,palette=\"cool\",fill=True)\n        \n            ax[i][j].spines['top'].set_visible(False)\n            ax[i][j].spines['left'].set_visible(False)\n            ax[i][j].spines['right'].set_visible(False)\n            a.set(xticklabels=[])  \n            a.set(xlabel=None)\n            a.set(yticklabels=[])  \n            a.set(ylabel=None)\n            #a.set(title=cols[i*4+j])\n            a.set_title(cols[i*4+j],size=8,fontweight='bold')\n        except:\n            pass\n\n","11bca2a9":"fig = plt.figure(figsize=(40,40))\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nax=plt.axes()\nax.set_facecolor(\"#F2EDD7FF\")\n\ncorr = df_train.drop('id',axis=1).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n\na=sns.heatmap(corr,\n        square=True, center=0, linewidth=0.2,\n        mask=mask) \n\na.set_title('Feature Correlation of training dataset', loc='left', fontweight='bold')\nplt.show()","34f74958":"print(\"***** Distribution of each feature in test dataset *****\")\nprint()\nfig,ax=plt.subplots(nrows=19,ncols=4,figsize=(20,40))\nfig.patch.set_facecolor(\"#F2EDD7FF\")\ncols=list(df_test.columns)[1:76]\nfor i in range(0,19):\n    for j in range(0,4):\n        try:\n            ax[i][j].set_facecolor(\"#F2EDD7FF\")\n            a=sns.kdeplot(data=df_test,x=df_test[cols[i*4+j]],ax=ax[i][j],legend=False,palette=\"cool\",fill=True)\n        \n            ax[i][j].spines['top'].set_visible(False)\n            ax[i][j].spines['left'].set_visible(False)\n            ax[i][j].spines['right'].set_visible(False)\n            a.set(xticklabels=[])  \n            a.set(xlabel=None)\n            a.set(yticklabels=[])  \n            a.set(ylabel=None)\n            #a.set(title=cols[i*4+j])\n            a.set_title(cols[i*4+j],size=8,fontweight='bold')\n        except:\n            pass\n\n","4882bdd7":"fig = plt.figure(figsize=(40,40))\nfig.patch.set_facecolor(\"#F2EDD7FF\")\nax=plt.axes()\nax.set_facecolor(\"#F2EDD7FF\")\n\ncorr = df_test.drop('id',axis=1).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n\na=sns.heatmap(corr,\n        square=True, center=0, linewidth=0.2,\n        mask=mask) \n\na.set_title('Feature Correlation of test dataset', loc='left', fontweight='bold')\nplt.show()","4dde4a3d":"fig,ax=plt.subplots(nrows=19,ncols=4,figsize=(20,40))\nfig.patch.set_facecolor(\"#F2EDD7FF\")\ncols=list(df_test.columns)[1:76]\nfor i in range(0,19):\n    for j in range(0,4):\n        try:\n            ax[i][j].set_facecolor(\"#F2EDD7FF\")\n            a=sns.stripplot(data=df_train,x=df_train['target'],y=df_test[cols[i*4+j]],ax=ax[i][j],palette=\"cool\")\n        \n            ax[i][j].spines['top'].set_visible(False)\n            ax[i][j].spines['left'].set_visible(False)\n            ax[i][j].spines['right'].set_visible(False)\n            a.set(xticklabels=[])  \n            a.set(xlabel=None)\n            a.set(yticklabels=[])  \n            a.set(ylabel=None)\n            #a.set(title=cols[i*4+j])\n            a.set_title(cols[i*4+j],size=8,fontweight='bold')\n        except:\n            pass\n\n","cf29e243":"[slide to top](#tab)\n<a id='2.8'> <\/a>\n\n### 2.8 Distribution of features according to targets","faf5ae70":"* As we can see the highest correlation between any two features is 0.14 , so that means we don't have any significant correlation between all the available features","77694f5e":"[slide to top](#tab)\n<a id='2.2'> <\/a>\n\n### 2.2 Unique value of each feature in datasets","40ea7e75":"* From the below stripplot , we can observe that none of features in our train dataset have some patterns or any type of correlation with any of classes available","173088e3":"[slide to top](#tab)\n<a id='4'> <\/a>\n\n# 4. **References \ud83d\udcab**\n\n* https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda\n\n* https:\/\/www.kaggle.com\/dwin183287\/tps-june-2021-eda","dde3268a":"[slide to top](#tab)\n<a id='2.6'> <\/a>\n\n### 2.6 Distribution of each feature in test dataset","3066c291":"## Please provide your feedback in comment section , Do give an upvote if you like my work \ud83d\ude00\ud83d\udc4d","039d5374":"#### If you like this notebook do provide your feedback and upvote if you really my work .\n#### This notebook contains only EDA of dataset , will publish my nextbook regarding predictions and different methods of dealing with this problem (Hoping for getting good results\ud83d\ude36\ud83d\ude0f)\n#### But for now do enjoy this notebook and will come back soon \ud83d\udd1c\ud83d\ude03","b92d5abc":"* Most of the values in each feature is zero\n* Mostly all the features are right skewed\n* Some features have disturbances in their kdeplot distribution such as feature_16 , feature_18 , feature_22","22e29487":"<a id='dat'><\/a>\n# **Dataset \ud83d\udccb**","0755765a":"* As we can see the highest correlation between any two features is 0.12 , so that means we don't have any significant correlation between all the available features","25e5509d":"* In my opinion , nature of train and test dataset is almost same . That means we can apply same preprocessing on both train and test dataset\n\n* All features have lot of zeroes , as we don't have any description about any dataset so we can't say anything about the features available . May be we can say that not all features from 0 t0 74 is contributing towards any class , that's why dataset creater put zeroes instead of null values\n\n* As there are a lot of features , but none of them is correlated to each other means all of them are independent of each other . \n\n","132876d3":"<html>\n<a id=\"1\"><\/a>\n<\/html>\n\n# 1. **Basic Overview \ud83d\udcfa**","e5c7b4d8":"[slide to top](#tab)\n<a id='3'> <\/a>\n\n# 3. **Conclusions \ud83e\udd1e**","7bebbdc7":"[slide to top](#tab)\n<a id='2.1'> <\/a>\n \n###  2.1 Target Distribution","ec003e9d":"*  There are 76 columns in training dataset , in which one is \"id\" column , other remaining 75 columns are our independent features\n\n*  There is no null values in the dataset , but there are lot of 0 values in each feature , is there any chance it can effect our predictions will talk about this later\n\n*  \"feature_19\" has the highest mean and standard deviation\n\n*  All the features are of \"int64\" type\n","7f19fb89":"[slide to top](#tab)\n<a id='2.7'> <\/a>\n\n### 2.7 Feature Correlation of test dataset","e40c11c4":"* The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.","d86784ab":"* Majority of features in both dataset have more than 50% of zero values\n* Percentages of zeroes in each feature of train and test dataset is almost same ","296bb69f":"* Unique values of most of the features in both 'train' and 'test' dataset are equal\n* feature_15 , feature_28 , feature_46 , feature_59 , feature_60 , feature_73  , these are the features which have difference in number of unique values\n* feature_60 have most difference in number of unique values","6a130242":"### 1.2 Test Dataset","7ea76c36":"*  There are 77 columns in training dataset , in which two are \"id\" and \"target\" columns , other remaining 75 columns are our independent features\n\n* There is no null values in the dataset , but there are lot of 0 values in each feature , is there any chance it can effect our predictions will talk about this later\n\n* \"feature_19\" has the highest mean and standard deviation\n\n* We have overall 9 classes to predict\n\n*  Data is imbalanced, class_6 (51811) and class_8 (51763) both combined covers more than 50% of data\n\n* All the features are in \"int64\" type except the \"target\" column which is in \"string\" type\n","65a07c81":"[slide to top](#tab)\n<a id='2'> <\/a>\n# 2. **Exploratory Data Analysis \ud83d\udcca**","5739dcf3":"* Most of the values in each feature is zero\n* Mostly all the features are right skewed\n* Some features have disturbances in their kdeplot distribution such as feature_16 , feature_18 , feature_22","ffad7666":"[slide to top](#tab)\n<a id='2.5'> <\/a>\n\n### 2.5 Feature Correlation of training dataset","435a1835":"<a id='1.1'><\/a>\n### 1.1 Train Dataset","4c6f9739":"<a id='tab'><\/a>\n## Table of Contents \u23e9\n* [Dataset \ud83d\udccb](#dat)\n* [Basic Overview \ud83d\udcfa](#1)\n  * [Train Dataset](#1.1)\n  * [Test Dataset](#1.2)\n\n* [Exploratory Data Analysis \ud83d\udcca](#2)\n  * [Target Distribution](#2.1)\n  * [Unique value of each feature in datasets](#2.2)\n  * [Percentage of zeroes in each feature of datasets](#2.3)\n  * [Distribution of each feature in training data](#2.4)\n  * [Feature Correlation of training dataset](#2.5)\n  * [Distribution of each feature in test dataset](#2.7)\n  * [Distribution of features according to targets](#2.8)\n  \n* [Conclusions \ud83e\udd1e](#3)\n* [References \ud83d\udcab](#4)\n  \n   ","da9192f0":"[slide to top](#tab)\n<a id='2.4'> <\/a>\n\n### 2.4 Distribution of each feature in training data","df418e7b":"[slide to top](#tab)\n<a id='2.3'> <\/a>\n\n### 2.3 Percentage of zeroes in each feature of datasets"}}