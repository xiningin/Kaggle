{"cell_type":{"3a22fc2b":"code","b9a4307d":"code","585e50ad":"code","4ef1e9ee":"code","8d59e2d7":"code","3792ff52":"code","a98b1cbc":"code","59718452":"code","a1404834":"code","d88060cc":"code","59bdf33f":"code","f2bf99b9":"code","008138b4":"code","1d8d5cbb":"code","d612f4b8":"code","b0745c43":"code","2aec4a68":"code","cfeebfb8":"code","ddb47987":"code","87c7e1b9":"code","a505eeff":"code","f06ae97f":"code","a1faeb11":"code","e37a2e28":"code","4a76bc9a":"code","79679f3c":"code","8e304513":"code","7a2ed526":"code","d27494df":"code","975e4a07":"code","2c7b0979":"code","5e94f042":"code","da53f8a4":"code","34894a46":"code","5ea1ca8d":"code","0d8ac186":"code","b7f80238":"code","90902688":"code","d5bb43a1":"code","400d536e":"code","5279198b":"code","8fbc049f":"code","f329d7d5":"markdown","cca1215e":"markdown","3e728762":"markdown","776ef16a":"markdown","7a372a2c":"markdown","2e8d256b":"markdown","d93e5a7d":"markdown","b99cb0a8":"markdown","662a7b65":"markdown","9efe90b6":"markdown","a61fcd94":"markdown","8f85e541":"markdown","a8696b4b":"markdown","edda5923":"markdown","e11d5d01":"markdown","4aa5ab47":"markdown","ac63c1a7":"markdown","352a2a65":"markdown","4875a8ee":"markdown","6a469f09":"markdown","59d6594b":"markdown","b2bff60a":"markdown","e857ff13":"markdown"},"source":{"3a22fc2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b9a4307d":"#importing necessary files\n#import files\n#load package\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#from math import sqrt\nimport seaborn as sns\nimport pandas_profiling as pf\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble","585e50ad":"# Read the data\nX_full = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n#Shape of dataset and sample\nprint(\"Shape of Dataset {}\".format(X_full.shape))\n\nX_full.head()","4ef1e9ee":"#print unique values of target variable\nprint(\"Unique values in Target Variable: {}\".format(X_full.Response.dtype))\nprint(\"Unique values in Target Variable: {}\".format(X_full.Response.unique()))\nprint(\"Total Number of unique values : {}\".format(len(X_full.Response.unique())))\n\n#distribution plot for target classes\nsns.countplot(x=X_full.Response).set_title('Distribution of rows by response categories')","8d59e2d7":"#create a funtion to createa  new target variable based on conditions\n\ndef new_target(row):\n    if (row['Response']<=7) & (row['Response']>=0):\n        val=0\n    elif (row['Response']==8):\n        val=1\n    else:\n        val=-1\n    return val\n\n\n#create a copy of original dataset\nnew_data=X_full.copy()\n\n#create a new column\nnew_data['Final_Response']=new_data.apply(new_target,axis=1)\n\n\n#print unique values of target variable\nprint(\"Unique values in Target Variable: {}\".format(new_data.Final_Response.dtype))\nprint(\"Unique values in Target Variable: {}\".format(new_data.Final_Response.unique()))\nprint(\"Total Number of unique values : {}\".format(len(new_data.Final_Response.unique())))\n\n#distribution plot for target classes\nsns.countplot(x=new_data.Final_Response).set_title('Distribution of rows by response categories')","3792ff52":"#drop the actual response column\nnew_data.drop(axis=1,labels=['Response'],inplace=True)\n\n#rename the \"Final_Response\" to \"Response\"\nnew_data.rename(columns={\"Final_Response\":\"Response\"},inplace=True)\n#print the nw column names\nprint(\"New columns: \", new_data.columns)\nX_original=X_full\nX_full=new_data","a98b1cbc":"#Are there any columns with missing values?\n\nfig, ax = plt.subplots(figsize=(20,5))  \nsns.heatmap(X_full.isnull(), cbar=False)","59718452":"missing_val_count_by_column = (X_full.isnull().sum()\/len(X_full))\nprint(missing_val_count_by_column[missing_val_count_by_column>0.3].sort_values(ascending=False))","a1404834":"# What are the diffrent datatypes available in datasource\n\ncolumns_df=pd.DataFrame({'column_names':X_full.columns,'datatypes':X_full.dtypes},index=None)\nx=columns_df.groupby(by=['datatypes']).count()\nx.reset_index(inplace=True)\nx.rename(columns={\"column_names\":\"Number_of_columns\"},inplace=True)\nlst=[]\nfor data_type in x.datatypes:\n    v=list(X_full.select_dtypes(include=data_type).columns)\n    lst.append(v)\n    x['Column_Names']=pd.Series(lst)\n    \n\nx","d88060cc":"#Lets look at only object column : [Product_Info_2]\n\n#check the values in Product_info_ column\nprint(\"Total Unique Values: \", len(X_full['Product_Info_2'].unique()))\nprint(\"Unique values in 'Product_Info_2':\", X_full['Product_Info_2'].unique())","59bdf33f":"# Exploring Numerical variables\nmisc_cols=[\"Ins_Age\",\"Ht\",\"Wt\",\"BMI\"]\n\nsns.boxplot(data=X_full[misc_cols])","f2bf99b9":"##Import libraries for classifiers\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier \nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,recall_score,accuracy_score,precision_score\nfrom sklearn.model_selection import train_test_split","008138b4":"y = X_full.Response\nX = X_full.drop(labels=['Response'],axis=1)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=.30,random_state=1)\n\n#create train and test dataset after dropping columns with null values and categorical column\n\n#drop categorical column\nX_dropped_train=X_train.drop(axis=1,labels=[\"Product_Info_2\"]).copy()\nX_dropped_valid=X_valid.drop(axis=1,labels=[\"Product_Info_2\"]).copy()\n\n#drop columns with any null\nX_dropped_train.dropna(axis=1,inplace=True)\nX_dropped_valid.dropna(axis=1,inplace=True)\n\n# print shape of dataset\nprint(\"Shape of X_train dataset {}\".format(X_dropped_train.shape))\nprint(\"Shape of X_test dataset {}\".format(X_dropped_valid.shape))\n\nprint(\"Shape of y_train dataset {}\".format(y_train.shape))\nprint(\"Shape of y_valid dataset {}\".format(y_valid.shape))","1d8d5cbb":"#set seed for same results everytime\nseed=0\n\n#declare the models\ndt=DecisionTreeClassifier(random_state=seed)\nrf=RandomForestClassifier(random_state=seed)\nlr=LogisticRegression(random_state=seed)\nadb=ensemble.AdaBoostClassifier()\nbgc=ensemble.BaggingClassifier()\ngbc=ensemble.GradientBoostingClassifier()\nxgb=XGBClassifier(random_state=seed)\n#sgdc=SGDClassifier(random_state=seed)\nsvc=SVC(random_state=seed)\n#knn=KNeighborsClassifier()\n#nb=GaussianNB()\n\n#create a list of models\nmodels=[dt,rf,lr,adb,bgc,gbc,svc,xgb]\n\ndef score_model(X_train,y_train,X_valid,y_valid):\n    df_columns=[]\n    df=pd.DataFrame(columns=df_columns)\n    i=0\n    #read model one by one\n    for model in models:\n        model.fit(X_train,y_train)\n        y_pred=model.predict(X_valid)\n        \n        #compute metrics\n        train_accuracy=model.score(X_train,y_train)\n        test_accuracy=model.score(X_valid,y_valid)\n        \n        p_score=metrics.precision_score(y_valid,y_pred)\n        r_score=metrics.recall_score(y_valid,y_pred)\n        f1_score=metrics.f1_score(y_valid,y_pred)\n        fp, tp, th = metrics.roc_curve(y_valid, y_pred)\n        \n        #insert in dataframe\n        df.loc[i,\"Model_Name\"]=model.__class__.__name__\n        df.loc[i,\"Precision\"]=round(p_score,2)\n        df.loc[i,\"Recall\"]=round(r_score,2)\n        df.loc[i,\"Train_Accuracy\"]=round(train_accuracy,2)\n        df.loc[i,\"Test_Accuracy\"]=round(test_accuracy,2)\n        df.loc[i,\"F1_Score\"]=round(f1_score,2)\n        df.loc[i,'AUC'] = metrics.auc(fp, tp)\n        \n        i+=1\n    \n    #sort values by accuracy\n    df.sort_values(by=['F1_Score'],ascending=False,inplace=True)\n    return(df)","d612f4b8":"report_no_null=score_model(X_dropped_train,y_train,X_dropped_valid,y_valid)\nreport_no_null","b0745c43":"sns.scatterplot(data=X_full,x='BMI',y='Wt',hue='Response',alpha=1)","2aec4a68":"sns.scatterplot(data=X_full,x='BMI',y='Ins_Age',hue='Response',alpha=1)","cfeebfb8":"# Exploring Numerical variables for outliers\nmisc_cols=[\"Ins_Age\",\"Ht\",\"Wt\",\"BMI\"]\n\nsns.boxplot(data=X_full[misc_cols])","ddb47987":"#function to remove outliers\ndef remove_outlier(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    return df_out\n\n\ndev=remove_outlier(X_full,'BMI')\ndev=remove_outlier(dev,'Wt')\ndev=remove_outlier(dev,'Ht')","87c7e1b9":"sns.boxplot(data=dev[misc_cols])","a505eeff":"sns.scatterplot(data=dev,x='BMI',y='Wt',hue='Response',alpha=1)","f06ae97f":"#prepare 3rd dataset\n#identifying columns with more than 30% missing values and dropping them\n\ny = dev.Response\nX = dev.drop(labels=['Response'],axis=1)\n\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=.30,random_state=1)","a1faeb11":"#dropping the columns with more than 30% missing values\nmissing_val_count_by_column = (dev.isnull().sum()\/len(dev))\nprint(missing_val_count_by_column[missing_val_count_by_column > 0.3])\ncols_to_drop=missing_val_count_by_column[missing_val_count_by_column > 0.3].index.values\n\n# Make copy to avoid changing original data \nout_enc_X_train = X_train.drop(labels=cols_to_drop,axis=1).copy()\nout_enc_X_valid = X_valid.drop(labels=cols_to_drop,axis=1).copy()","e37a2e28":"#identify all cols with medical keywords\nmedical_keyword_cols=[col for col in out_enc_X_train.columns if str(col).startswith(\"Medical_Keyword\")]\n\n#identify all cols with medical keywords\nmedical_cols=[col for col in out_enc_X_train.columns if str(col).startswith(\"Medical_History\")]\n\nout_enc_X_train['Total_MedKwrds']=out_enc_X_train[medical_keyword_cols].sum(axis=1)\nout_enc_X_train['Total_MedHist']=out_enc_X_train[medical_cols].sum(axis=1)\n\nout_enc_X_valid['Total_MedKwrds']=out_enc_X_valid[medical_keyword_cols].sum(axis=1)\nout_enc_X_valid['Total_MedHist']=out_enc_X_valid[medical_cols].sum(axis=1)","4a76bc9a":"#label encoding \nle=LabelEncoder()\nout_enc_X_train['Product_Info_2_en'] = le.fit_transform(out_enc_X_train['Product_Info_2'])\nout_enc_X_valid['Product_Info_2_en'] = le.fit_transform(out_enc_X_valid['Product_Info_2'])\n\nout_enc_X_train.drop(axis=1,labels=['Product_Info_2'],inplace=True)\nout_enc_X_valid.drop(axis=1,labels=['Product_Info_2'],inplace=True)\n\n# imputing missing values\nimputer=SimpleImputer()\n\nout_enc_X_train= imputer.fit_transform(out_enc_X_train)\nout_enc_X_valid= imputer.transform(out_enc_X_valid)","79679f3c":"#Rename the datasets\n\nX_train=out_enc_X_train\ny_train=y_train\nX_valid=out_enc_X_valid\ny_valid=y_valid\n\n#instantiate, fit and make preditions\nmodel=RandomForestClassifier(random_state=seed)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_valid)\n\n#compute metrics\ntrain_accuracy=model.score(X_train,y_train)\ntest_accuracy=model.score(X_valid,y_valid)\np_score=metrics.precision_score(y_valid,y_pred)\nr_score=metrics.recall_score(y_valid,y_pred)\nf1_score=metrics.f1_score(y_valid,y_pred)\nfp, tp, th = metrics.roc_curve(y_valid, y_pred)\nauc = metrics.auc(fp, tp)","8e304513":"print(\"Train Accuracy: {}\".format(round(train_accuracy,3)))\nprint(\"Test Accuracy: {}\".format(round(test_accuracy,3)))\nprint(\"Precision Score: {}\".format(round(p_score,3)))\nprint(\"Recall Score: {}\".format(round(r_score,3)))\nprint(\"F1 Score: {}\".format(round(f1_score,3)))\nprint(\"AUC: {}\".format(round(auc,3)))\n\nprint(\"==============Classification Report=============================\")\nprint(metrics.classification_report(y_valid,y_pred))\n\n\nprint(\"==============Confusion Matrix=============================\")\nprint(metrics.confusion_matrix(y_valid,y_pred))","7a2ed526":"# import seaborn as sns\n# import matplotlib.pyplot as plt     \n\n# ax= plt.subplot()\n# sns.heatmap(metrics.confusion_matrix(y_valid,y_pred),annot=True, ax = ax,); #annot=True to annotate cells\n\n# # labels, title and ticks\n# ax.set_xlabel('Predicted labels')\n# ax.set_ylabel('True labels')\n# ax.set_title('Confusion Matrix')\n# ax.xaxis.set_ticklabels(['Declined', 'Approved'])\n# ax.yaxis.set_ticklabels(['Declined', 'Approved']);","d27494df":"# Look at parameters used by our current forest\nfrom pprint import pprint\npprint('Parameters currently in use:\\n')\npprint(model.get_params())","975e4a07":"\nnp.arange(10,120,10)","2c7b0979":"# Lets create the parameter grid for tunning Random forest\n\n# Number of trees in random forest\nn_estimators = np.arange(100,1200,200)\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = np.arange(10,120,10)\n# Minimum number of samples required to split a node\nmin_samples_split = [20,30,50]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [10,20,30,40]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)","5e94f042":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 50 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf,\n                               param_distributions = random_grid,\n                               n_iter = 50, cv = 3, \n                               verbose=2,\n                               scoring='precision',\n                               random_state=42,\n                               n_jobs = -1)# Fit the random search model\nrf_random.fit(X_train,y_train)","da53f8a4":"rf_random.best_params_","34894a46":"#instantiate, fit and make preditions\nrf_model=RandomForestClassifier(n_estimators=700,min_samples_split=30,min_samples_leaf=10,max_features='sqrt',\n                                max_depth=80,bootstrap=False)\nrf_model.fit(X_train,y_train)\ny_pred=rf_model.predict(X_valid)\n\n#compute metrics\ntrain_accuracy_rf=rf_model.score(X_train,y_train)\ntest_accuracy_rf=rf_model.score(X_valid,y_valid)\np_score_rf=metrics.precision_score(y_valid,y_pred)\nr_score_rf=metrics.recall_score(y_valid,y_pred)\nf1_score_rf=metrics.f1_score(y_valid,y_pred)\nfp_rf, tp_rf, th_rf = metrics.roc_curve(y_valid, y_pred)\nauc_rf = metrics.auc(fp_rf, tp_rf)\n\n\nprint(\"Train Accuracy: {}\".format(round(train_accuracy_rf,3)))\nprint(\"Test Accuracy: {}\".format(round(test_accuracy_rf,3)))\nprint(\"Precision Score: {}\".format(round(p_score_rf,3)))\nprint(\"Recall Score: {}\".format(round(r_score_rf,3)))\nprint(\"F1 Score: {}\".format(round(f1_score_rf,3)))\nprint(\"AUC: {}\".format(round(auc_rf,3)))\n\nprint(\"==============Classification Report=============================\")\nprint(metrics.classification_report(y_valid,y_pred))\n\n\nprint(\"==============Confusion Matrix=============================\")\nprint(metrics.confusion_matrix(y_valid,y_pred))","5ea1ca8d":"#evaluate models\n\ndef evaluate(model,X_train,y_train,X_valid,y_valid):\n    y_pred=model.predict(X_valid)\n    #compute metrics\n    train_accuracy_rf=model.score(X_train,y_train)\n    test_accuracy_rf=model.score(X_valid,y_valid)\n    p_score_rf=metrics.precision_score(y_valid,y_pred)\n    r_score_rf=metrics.recall_score(y_valid,y_pred)\n    f1_score_rf=metrics.f1_score(y_valid,y_pred)\n    fp_rf, tp_rf, th_rf = metrics.roc_curve(y_valid, y_pred)\n    auc_rf = metrics.auc(fp_rf, tp_rf)\n    \n    print(\"Train Accuracy: {}\".format(round(train_accuracy_rf,3)))\n    print(\"Test Accuracy: {}\".format(round(test_accuracy_rf,3)))\n    print(\"Precision Score: {}\".format(round(p_score_rf,3)))\n    print(\"Recall Score: {}\".format(round(r_score_rf,3)))\n    print(\"F1 Score: {}\".format(round(f1_score_rf,3)))\n    print(\"AUC: {}\".format(round(auc_rf,3)))\n    \n    print(\"==============Classification Report=============================\")\n    print(metrics.classification_report(y_valid,y_pred))\n    \n    print(\"==============Confusion Matrix=============================\")\n    print(metrics.confusion_matrix(y_valid,y_pred))\n    \n    return (r_score_rf,p_score,f1_score,auc,train_accuracy_rf,test_accuracy_rf)","0d8ac186":"print(\"\\n ==========================Base Model==========================\")\nmodel.fit(X_train,y_train)\nbase_recall,base_precison,base_f1,base_auc,base_train_accuracy,base_test_accuracy= evaluate(model,X_train,y_train,X_valid,y_valid)\n\nprint(\"\\n ==========================Tuned Model==========================\")\nbest_random = rf_random.best_estimator_\nrandomcv_recall,randomcv_precison,randomcv_f1,randomcv_auc,randomcv_train_accuracy,randomcv_test_accuracy = evaluate(best_random, X_train,y_train, X_valid,y_valid)\n\n\nprint('RandomSearchCV Improvement in Recall of {:0.2f}%.'.format( 100 * (randomcv_recall - base_recall) \/ base_recall))\nprint('RandomSearchCV Improvement in Precision of {:0.2f}%.'.format( 100 * (randomcv_precison - base_precison) \/ base_precison))\nprint('RandomSearchCV Improvement in F1 Score of {:0.2f}%.'.format( 100 * (randomcv_f1 - base_f1) \/ base_f1))\nprint('RandomSearchCV Improvement in AUC of {:0.2f}%.'.format( 100 * (randomcv_auc - base_auc) \/ base_auc))\nprint('RandomSearchCV Improvement in Train Accuracy of {:0.2f}%.'.format( 100 * (randomcv_train_accuracy - base_train_accuracy) \/ base_train_accuracy))\nprint('RandomSearchCV Improvement in Test Accuracy of {:0.2f}%.'.format( 100 * (randomcv_test_accuracy - base_test_accuracy) \/ base_test_accuracy))","b7f80238":"names=list(X.drop(labels=cols_to_drop,axis=1).columns.values)\nnames.append('Total_MedKwrds')\nnames.append('Total_MedHist')\n\ndata={'Feature_Name':names,\n      'Feature_Importance': rf_model.feature_importances_\n     }\n\nfeature_df=pd.DataFrame(data)\n\nfeature_df.sort_values(by=['Feature_Importance'],ascending=False,inplace=True)\n\nfig, ax = plt.subplots(figsize=(15,25))\nsns.barplot(data=feature_df,y='Feature_Name',x='Feature_Importance',)","90902688":"fpr,tpr,thresholds=metrics.roc_curve(y_valid,y_pred)\n\ndef plot_roc_curve(fpr,tpr,label=None):\n    plt.plot(fpr,tpr,linewidth=2,label=label)\n    plt.plot([0,1],[0,1],'k--')\n    plt.axis([0,1,0,1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    \nplot_roc_curve(fpr,tpr)\nplt.show()","d5bb43a1":"# plot precision vs recall\n\nfrom sklearn.model_selection import cross_val_predict\n\ny_scores=cross_val_predict(rf_model,X_train,y_train,cv=3)\n\nprecisions,recalls,thresholds=metrics.precision_recall_curve(y_train,y_scores)\n\ndef plot_precision_recall_vs_threshold(precisions,recalls,threshold):\n    plt.plot(thresholds,precisions[:-1],\"b--\",label=\"Precision\")\n    plt.plot(thresholds,recalls[:-1],\"g--\",label=\"Recalls\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0,1])\n    \nplot_precision_recall_vs_threshold(precisions,recalls,thresholds)\nplt.show()","400d536e":"estimator=rf_model.estimators_[5]\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names =names,\n                class_names = 'Response',\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","5279198b":"# Lets create the parameter grid for tunning Random forest\n\n# Create the parameter grid based on the results of random search \nparam_grid = { 'n_estimators': [300,500,700,900,1000],\n               'max_features': ['sqrt'],\n               'max_depth': [80, 100, 110],\n               'min_samples_split': [20,30],\n               'min_samples_leaf': [10,20,30],\n               'bootstrap': ['False']\n}# Create a based model\nrf = RandomForestClassifier()# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid,scoring='precision', cv = 3, n_jobs = -1, verbose = 2)\n\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","8fbc049f":"print(\"\\n ==========================Tuned Model==========================\")\nbest_grid = grid_search.best_params_\ngridcv_recall,gridcv_precison,gridcv_f1,gridcv_auc,gridcv_train_accuracy,gridcv_test_accuracy = evaluate(best_random, X_train,y_train, X_valid,y_valid)\n\n\nprint('GridSearchCV Improvement in Recall of {:0.2f}%.'.format( 100 * (gridcv_recall - base_recall) \/ base_recall))\nprint('GridSearchCV in Precision of {:0.2f}%.'.format( 100 * (gridcv_precison - base_precison) \/ base_precison))\nprint('GridSearchCV in F1 Score of {:0.2f}%.'.format( 100 * (gridcv_f1 - base_f1) \/ base_f1))\nprint('GridSearchCV in AUC of {:0.2f}%.'.format( 100 * (gridcv_auc - base_auc) \/ base_auc))\nprint('GridSearchCV in Train Accuracy of {:0.2f}%.'.format( 100 * (gridcv_train_accuracy - base_train_accuracy) \/ base_train_accuracy))\nprint('GridSearchCV in Test Accuracy of {:0.2f}%.'.format( 100 * (gridcv_test_accuracy - base_test_accuracy) \/ base_test_accuracy))","f329d7d5":"# Step 5: Perform label encoding and imputation","cca1215e":"# Step 4: Combine all 'Medical_Keyword_X' into 'Total_Medkwrds' and 'Medical_History_X' into 'Total MedHist'**","3e728762":"# Step 2: Train Test Split","776ef16a":"### Observations from missing map and missing percentage:\n1. Majority of columns dont have missing values\n2. Target variable does not have any missing values\n3. Few columns have few missing values, whereas few columns have too much missing values\n4. Few medical history, Family_Hist_* columns have more than 30% missing values.","7a372a2c":"# Step1 : Remove Outliers based on IQR range","2e8d256b":"## Attempt 2: GridSearchCV","d93e5a7d":"For object columns, generally we use encoding - Label or onehot. However, the choice of encoding depends on the cardinality of the column.Higher cardinality would result in too many columns in case of Onehot encoding. \n\nIn our case we have 19 unique values. On One hot encoding - 19*59k data points would be needed to store the information. We can look at One hot encoding and label encoding as an alternative","b99cb0a8":"We have 3 types of columns: int, float, object along with missing values. As a next step, Let's have a closer look at the only object column","662a7b65":"## Observations from Target variable\n\n1. Majority of data is for Response=8 , followed by 6,7,2\n2. Lowest data is for Response=3,followed by 4\n3. Response = 1,2,5 show almost same distribution of data\n4. Target variable is \"Response\" and it has 8 classes - out of 8 data seems to be skewed towards class 8\n\n\nFor our exsercise, we will do a binary classification by altering the target variable. The new problem statement would be - Based on the attributes of customers, will the life insurance policy be approved or not i.e.yes(1) or no(0)\n\nAs a next step, we will create a new response variable, \"Final_Response\" in actual dataset","9efe90b6":"# Our Random forest is overfitting, as  a next step will tune the hyper parameters","a61fcd94":"# Problem Statement\n\nIn a one-click shopping world with on-demand everything, the life insurance application process is antiquated. Customers provide extensive information to identify risk classification and eligibility, including scheduling medical exams, a process that takes an average of 30 days.\n\nThe result? People are turned off. That\u2019s why only 40% of U.S. households own individual life insurance. Prudential wants to make it quicker and less labor intensive for new and existing customers to get a quote while maintaining privacy boundaries.\n\nBy developing a predictive model that accurately classifies rirted tosk using a more automated approach, you can greatly impact public perception of the industry.\n\nThe results will help Prudential better understand the predictive power of the data points in the existing assessment, enabling us to significantly streamline the process.\n\n\n# For learning purpose , multiclasss classification has been converted to a two class classification problem","8f85e541":"# Step 3: Remove columns with high missing value percentage****","a8696b4b":"# Step 6: Build a Random Forest model","edda5923":"# Plot important features","e11d5d01":"# Observations after scoring 1st data set - No null & No object column\n\n1. AdaBoostClassifier gives highest Recall\n2. Highest precision is given by GradientBoostingClassifier\n3. Highest AUC is given by GradientBoostingClassifier","4aa5ab47":"# Dataset II:\n\nWe will prepare a new dataset with below operations performed:\n1. Outlier Treatment for wt,bmi,Ht\n2. Product_info_2 column encoded\n3. Combining all the keywords into single columns","ac63c1a7":"We got two classes for response [0,1] and it is as per requirment.","352a2a65":"### Basic definitions of the hyperparameters:\n\n1. **n_estimators** = number of trees in the foreset\n2. **max_features** = max number of features considered for splitting a node\n3. **max_depth** = max number of levels in each decision tree\n4. **min_samples_split** = min number of data points placed in a node before the node is split\n5. **min_samples_leaf** = min number of data points allowed in a leaf node\n6. **bootstrap** = method for sampling data points (with or without replacement).Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.\n7. **classweight**=Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n8. **criterion**= The function to measure the quality of a split. Supported criteria are \u201cgini\u201d for the Gini impurity and \u201centropy\u201d for the information gain. Note: this parameter is tree-specific\n9. **n_jobs**= Number of jobs to run in parallel\n10. **OOb_score**= Whether to use out-of-bag samples to estimate the generalization accuracy.\n11. **Random_state**= seed value\n12. **Warm_start**=When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest\n13. **min_weight_fraction_leaf**=The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n14. **min_impurity_split** = Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf\n15. **min_impurity_decrease** = A node will be split if this split induces a decrease of the impurity greater than or equal to this value\n16. **max_leaf_nodes**=Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n17. ","4875a8ee":"## Attempt 1: RandomSearchCV","6a469f09":"But before we got for baseline models:\n1. We will drop the columns with any missing values - **Keep columns with all datapoints** as missing values throw errors during algorithms\n2. We will drop the categorical column** - we would need to perfom One hot encoding or label encoding, which we will do at later step","59d6594b":"# Plot ROC curve","b2bff60a":"# Plot precision recall curve","e857ff13":"# Visualizing Random forest tree diagram\n\nSteps involved in creation of tree:\n1. ** Create a model train**\n2. **Export Tree as .dot File**: This makes use of the export_graphviz function in Scikit-Learn. There are many parameters here that control the look and information displayed. Take a look at the documentation for specifics.\n3. **Convert dot to png using a system command**: running system commands in Python can be handy for carrying out simple tasks. This requires installation of graphviz which includes the dot utility. For the complete options for conversion, take a look at the documentation.\n4. **Visualize**: the best visualizations appear in the Jupyter Notebook. (Equivalently you can use matplotlib to show images).\n"}}