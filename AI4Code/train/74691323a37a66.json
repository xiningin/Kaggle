{"cell_type":{"b6fa26c4":"code","7e904a16":"code","01164acc":"code","f6278439":"code","4c5e3b5e":"code","40974e4c":"code","088801d2":"code","b605c69b":"code","494b2d55":"code","4f3a4094":"code","487dd96a":"code","a0c4ef8a":"code","d4b31259":"code","c201d61c":"code","f4184140":"code","36278d27":"code","9192222d":"code","6d360def":"code","d61d6752":"code","f8230285":"code","c005915b":"code","16447ed4":"markdown","bbbb39e2":"markdown","c60c1047":"markdown","384abf43":"markdown","b794e625":"markdown","398ddd3e":"markdown","e3a49948":"markdown","e50dc8e4":"markdown","6eb27ffb":"markdown","ce94afa2":"markdown","993b5750":"markdown","3e4f5a85":"markdown","c1f44334":"markdown","ddd36452":"markdown","65a685ac":"markdown"},"source":{"b6fa26c4":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7e904a16":"import pandas as pd\nimport seaborn as sns\nimport matplotlib \nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler","01164acc":"cols=['CRIM',\n'ZN',\n'INDUS',\n'CHAS',\n'NOX',\n'RM',\n'AGE',\n'DIS',\n'RAD',\n'TAX',\n'PTRATIO',\n'B',\n'LSTAT',\n'MEDV']\ndf=pd.read_csv(r'\/kaggle\/input\/boston-house-prices\/housing.csv',header=None,delim_whitespace=True)\ndf.columns=cols\nprice=df['MEDV']\n#df.drop('MEDV', axis=1, inplace=True)\ndf","f6278439":"pplot=sns.pairplot(df[:-1])\npplot.fig.set_size_inches(15,15)","4c5e3b5e":"scaler = preprocessing.StandardScaler()\ndf_stand = scaler.fit_transform(df)\ndf_stand=pd.DataFrame(df_stand,columns=cols)\ndf=df_stand","40974e4c":"fig, ax = plt.subplots(figsize=(10,6))\nsns.heatmap(df.corr(), center=0, cmap='BrBG',annot=True)\nax.set_title('Multi-Collinearity of Features')","088801d2":"\nprint(df.columns)\nplt.figure(num=None, figsize=(15,30), dpi=80, facecolor='w', edgecolor='k')\nplt.figure(1)\nvar=1\nfor index,feature in enumerate(list(df.columns)):\n    plt.subplot(4,4,index+1,xlabel=feature)\n    sns.boxplot(y=df[feature])\n    plt.grid()\n    #plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n    plt.subplots_adjust(top=0.92, bottom=0, left=0.10, right=0.95, hspace=0.5,wspace=0.5)\n    var+=1","b605c69b":"\nmedv_correlation={}\nfor i in df.columns:\n    if i!='MEDV':\n        corr_value=df['MEDV'].corr(df[i]) \n        medv_correlation[i]=corr_value\n        print(\"Correlation value between MEDV and {} is {}\".format(i,corr_value))\nprint(\"MAX correlated features are:\", max(medv_correlation, key=medv_correlation.get),min(medv_correlation, key=medv_correlation.get))","494b2d55":"percentage=0\nfor k, v in df.items():\n    if k=='RM':\n        Q1 = np.array(v.quantile(0.25))\n        Q3 = np.array(v.quantile(0.75))\n        IQR = Q3 - Q1\n        v_col = v[(v <= Q1 - 1.5 * IQR) | (v >= Q3 + 1.5 * IQR)]\n        percentage = np.shape(v_col)[0] * 100.0 \/ np.shape(df)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, percentage))\nprint(\"Total number of outliers for feature RM is %d  which is %.2f%% of total data points\"%(v_col.count(),percentage)) \n\ndf=df[~(df['RM'] >= Q3 + 1.5 * IQR)|(df['RM'] <=Q1 - 1.5 * IQR)]\ndf.shape","4f3a4094":"df","487dd96a":"from sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nscaler = preprocessing.StandardScaler()\ndf_stand = scaler.fit_transform(df)\ndf_stand=pd.DataFrame(df_stand,columns=cols)\ndf_stand","a0c4ef8a":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n# Calculating VIF\ns=df._get_numeric_data()\nvif = pd.DataFrame()\nvif[\"variables\"] = s.columns\n\nvif[\"VIF\"] = [variance_inflation_factor(np.array(s.values,dtype=float), i) for i in range(s.shape[1])]","d4b31259":"vif['Result'] = vif['VIF'].apply(lambda x: 'Non Collinear' if x <= 3.7 else 'Collinear')\nvif","c201d61c":"features=list(vif[vif['Result']=='Non Collinear']['variables'])\n\nprint(features)","f4184140":"for i in df.columns:\n    if i!='MEDV':\n        if i not in features:\n            df.drop(i,axis=1,inplace=True)\ndf","36278d27":"from matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\nplt.figure(1)\nvar=1\nfor index,feature in enumerate(list(df.columns)):\n    colors = (0,0,0)\n    area = np.pi*3\n    plt.subplot(4,4,index+1,xlabel=feature,ylabel='MEDV')\n    sns.regplot(y='MEDV',x=feature,data=df,marker=\"+\",ci=80)\n    plt.grid()\n    plt.subplots_adjust(top=0.92, bottom=0, left=0.10, right=0.95, hspace=0.5,wspace=0.5)\n    var+=1","9192222d":"df_plot=pd.read_csv(r'\/kaggle\/input\/boston-house-prices\/housing.csv',header=None,delim_whitespace=True)\ndf_plot.columns=cols\n\n\nfor i in df.columns:\n    if i!='MEDV':\n        if i not in features:\n            df_plot.drop(i,axis=1,inplace=True)\n\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 8), dpi=80, facecolor='w', edgecolor='k')\nplt.figure(1)\nvar=1\nfor index,feature in enumerate(list(df.columns)[:-1]):\n    counts,bin_edges= np.histogram(df_plot[feature],bins=10,density=True)\n    pdf=counts\/sum(counts)\n    #a=440+var\n    cdf=np.cumsum(pdf)\n    plt.subplot(4,4,index+1,xlabel=feature)\n    plt.plot(bin_edges[1:],pdf)\n    plt.plot(bin_edges[1:],cdf)\n    plt.grid()\n    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.5,wspace=0.5)\n    var+=1\n","6d360def":"from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\ny=df_stand['MEDV'] \ndf_stand.drop('MEDV',axis=1,inplace=True)\nX=df_stand\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","d61d6752":"regressor = LinearRegression()  \nregressor.fit(X_train, y_train) #training the algorithm","f8230285":"y_pred = regressor.predict(X_test)\nprint(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","c005915b":"sns.regplot(y_test,y_pred)","16447ed4":"Removing Outliers from the Data for the column RM which is highly correlated with MEDV(price)","bbbb39e2":"Inorder to check for outliers. We are plotting BOX plot for visualization. ","c60c1047":"We could see that there is some correlation between House Price(MEDV) and features like LSTAT,RM,CRIM. ","384abf43":"Removing the collinear features and for further analysis.\n","b794e625":"Standardizing the data inorder to compute correlation using Variance Inflation Factor method.","398ddd3e":"Clearly we understand that our price(MEDV) is getting lower as LSTAT and CRIM is increasing. And we can say that the price(MEDV) is increasing as the no. of rooms(RM) is increasing.","e3a49948":"    Feature description - \n    - CRIM     per capita crime rate by town\n    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n    - INDUS    proportion of non-retail business acres per town\n    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n    - NOX      nitric oxides concentration (parts per 10 million)\n    - RM       average number of rooms per dwelling\n    - AGE      proportion of owner-occupied units built prior to 1940\n    - DIS      weighted distances to five Boston employment centres\n    - RAD      index of accessibility to radial highways\n    - TAX      full-value property-tax rate per '$10,000'\n    - PTRATIO  pupil-teacher ratio by town\n    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    - LSTAT    '% lower status of the population'\n    - MEDV     Median value of owner-occupied homes in $1000's","e50dc8e4":"Using the heat map we could see that there is some collinearity between some features NOX, DIS, RAD, TAX. but for better interpretation i prefer implenting VIF.  ","6eb27ffb":"Analyzing the features relationship with dependent variable MEDV.","ce94afa2":"To observe correlation, I just gave a look at the pairplot for the features.","993b5750":"There are outliers in few features and we remove them below.","3e4f5a85":"Usually, It is considered that if the VIF score is <5 they are less collinearity and near to or above 10 they have high collinearity. Best score is VIF = 1 meaning they have 0 collinearity.\n\nHere since there are majority below 5, I have chosen only the ones that are less than the mean of all VIF's and termed them as Non Collinear and rest as collinear.","c1f44334":"I have checked the correlation of the features with the dependent variable(MEDV) and found RM and LSTAT are two important features. So, I chose to remove outliers from RM feature.","ddd36452":"Error for our prediction is 0.55 . The Plot for our predicted and test data is below.","65a685ac":"We could see the PDF and CDF of the features. Almost 70-80% of our dataset has CRIM less than 20 ."}}