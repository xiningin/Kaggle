{"cell_type":{"6528da91":"code","da98184c":"code","e21e7b35":"code","4fcbde3d":"code","dd8f07be":"code","2dcc6a74":"code","7fe4f111":"code","e1a32237":"code","b88a6e57":"code","7902d66c":"code","395eb6d9":"code","f1e19683":"code","c2f00eac":"code","d45e60c3":"code","c2721128":"code","fa063a88":"code","9b2e5aa6":"code","d6da9bc3":"code","bacafeb7":"code","0e804a4f":"code","625bb84c":"code","fb7294c9":"code","5d18d092":"code","960261d5":"code","227f20d4":"code","e1840468":"code","62563ab3":"code","87ddb48b":"code","edb8eac2":"markdown","dba4ba12":"markdown","c40b029b":"markdown","268fa8f6":"markdown","44975d3a":"markdown","bc308dda":"markdown","4cc29720":"markdown","fed70bad":"markdown","4e4bc234":"markdown","9ee5c6de":"markdown","ffeb404c":"markdown","f3dfeff3":"markdown","bae571c3":"markdown","141235bb":"markdown","f7c08aa7":"markdown","add9ffc9":"markdown","6bc36fc8":"markdown","8c714778":"markdown","a04452e3":"markdown","34788188":"markdown"},"source":{"6528da91":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","da98184c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e21e7b35":"# load data\ndataset = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndataset.head()","4fcbde3d":"dataset.shape","dd8f07be":"# check for infos\ndataset.info()","2dcc6a74":"# check for missing values \ndataset.isna().sum()","7fe4f111":"# we drop time feature from our dataset\ndata = dataset.copy()\ndata = data.drop('Time', axis = 1)\ndata.head()","e1a32237":"data.hist(figsize = (20,20))\nplt.show()","b88a6e57":"amount = pd.DataFrame(data['Amount'])\namount.head()","7902d66c":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ndata['Amount'] = sc.fit_transform(amount)\n#data['Amount'] = data['scaleAmount']\n\ndata.head()","395eb6d9":"data.Class.value_counts()","f1e19683":"sns.countplot(data.Class)","c2f00eac":"# Creating feature and target vectors\nX = data.drop('Class', axis = 1)\nY = data.Class\n\nX.shape, Y.shape","d45e60c3":"# spliting data into train set and test set\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state=0)","c2721128":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\n\n\ny_pred_train = logreg.predict(x_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\nprint(\"Accuracy score: \", accuracy_score(y_test,y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))\nprint('\\n')\nprint(classification_report(y_test, y_pred))","fa063a88":"from sklearn.utils import resample\n\n# separate the maority and minority class observation\ndata_major = data[data['Class'] == 0]\ndata_minor = data[data['Class'] == 1]\n\n# over-sample the minority class observations\ndata_minor_oversample = resample(data_minor, replace = True, n_samples=284315, random_state = 0)\n\n# finally combine the majority class observation and oversampled minoiry class observation\ndata_oversampled = pd.concat([data_major, data_minor_oversample])","9b2e5aa6":"# class label count after oversampled.we will see that minoity class now is proportionate to majority class\ndata_oversampled['Class'].value_counts()","d6da9bc3":"# again lets splt our over samoled data into feature and traget variables\nX = data_oversampled.drop('Class', axis = 1)\nY = data_oversampled.Class\n\n# lets split data into train and test set\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)\n\n# model building\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\n\n# Lets evaluate our model\ny_pred_train = logreg.predict(x_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\nprint(\"Accuracy score: \", accuracy_score(y_test,y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))\nprint('\\n')\nprint(classification_report(y_test, y_pred))","bacafeb7":"from sklearn.utils import resample\n\n# separate majority and minority class observation\ndata_major = data[data['Class'] == 0]\ndata_minor = data[data['Class'] == 1]\n\n# perform undersampling in majority class data\ndata_major_undersample = resample(data_major, replace = False, n_samples=492, random_state = 0)\n\n# finally concat the minority class data and undersampled majority class data\ndata_undersampled = pd.concat([data_minor, data_major_undersample])","0e804a4f":"# class lbel count after undersampling\ndata_undersampled.Class.value_counts()","625bb84c":"# lets create feature and target variables from above undersampled data\nX = data_undersampled.drop('Class', axis = 1)\nY = data_undersampled.Class\n\n# lets split data into train and test set\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.3, random_state = 0)\n\n# model building\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\n\n# Lets evaluate our model\ny_pred_train = logreg.predict(x_train)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\nprint(\"Accuracy score: \", accuracy_score(y_test,y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))\nprint('\\n')\nprint(classification_report(y_test, y_pred))","fb7294c9":"print('Initially the class distribution counts as below')\ndata.Class.value_counts()","5d18d092":"# creating feature and target vectors\nx = data.drop('Class', axis = 1)\ny = data.Class\n\nx.shape, y.shape","960261d5":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\nx_train.shape, x_test.shape","227f20d4":"print('Label counts in splited y_train before applying SMOTE algorithm:\\n')\nprint(\"counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"counts of label '0': {} \\n\".format(sum(y_train == 0))) ","e1840468":"from imblearn.over_sampling import SMOTE \nsmote = SMOTE(random_state = 0) \nx_trainN, y_trainN = smote.fit_sample(x_train, y_train.ravel()) \n\nprint('Lets see the sample size again')\nprint('\\nx_train size: ', x_trainN.shape)\nprint(' y_train size: ', y_trainN.shape)","62563ab3":"print('Label counts in splited y_train AFTER applying SMOTE algorithm:\\n')\nprint(\"counts of label '1': {}\".format(sum(y_trainN == 1))) \nprint(\"counts of label '0': {} \\n\".format(sum(y_trainN == 0))) ","87ddb48b":"# model building\nlogreg = LogisticRegression() \nlogreg.fit(x_trainN, y_trainN.ravel()) \ny_pred = logreg.predict(x_test) \n\n\n# Lets evaluate our model\ny_pred_train = logreg.predict(x_trainN)\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_trainN, y_pred_train)))\n\nprint(\"Accuracy score: \", accuracy_score(y_test,y_pred))\nprint('\\n')\nprint(confusion_matrix(y_test, y_pred))\nprint('\\n')\nprint(classification_report(y_test, y_pred))  ","edb8eac2":"### SMOTE Technique\n\n","dba4ba12":"Our data are **skewed**","c40b029b":"### Over-Sampling technique","268fa8f6":"### Lets observe how our data is distributed.","44975d3a":"Under-sampling involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm.\nIn simple word we will randomly remove data from majority class obdervation and make it equally proportionate or actionable proportional with minority class observation.","bc308dda":"Accuracy is found to be **0.95** and recall of minority is **0.92** which is quite good. So our model is performing quite well.Also test set abd training and testing accuracies are very comparable so no much issues of overfitting and underfitting.","4cc29720":"### LOgistic regression in imbalaced data\n\nWe will try to apply logistic regression in imbalanced data and see how it performs.","fed70bad":"## More update as well as improvement of above notebook will be done soon!!\n\nWE have till now only worked with logistic regression  and applied it to various class imbalanced handling techniques.Soon more **tree based algorithms, svm,**  will be added as well. And we will observe and compare performance.\n\n","4e4bc234":"**Looking above plot it clearly indicates the class imbalance **\n\nIt is always recommended to check for class imbalance in Classification problems.\n\nClass imbalance is very common in Machine Learning especially in classification.Standard accuracy measure for example accuracy_score is no longer reliable performance evaluation metrics instead we need to opt for other metrics.\n\n**Some techniques to deal with class imbalance problems in classification are:**\n1. Collecting more data\n2. Considering alternative performance metrics.\n\n    Like i said aarlier standard performance metrics like accuracy_score is no            longer reilable because they tends to classify the majority labels. Insted we \n     can go with metrics like f1-scpre, recall, precision, ROC curve etc.\n3.Resampling the dataset\n  Two common sampling methods are: under-sampling and oversampling.\n4. Use tree based algorithms: (definately worth trying)\n5. SMOTE algorithms\n   many more.....\n   \n  One thing to note is that there is no magic tricks to know which techniques like mentioned above is best one.We can say they all are good and they all are bad. They all have their own benefits and drawbacks. We need to test all or as much as possible above techniques in our dataset and classification algorithm and chosse the best one.","9ee5c6de":"Accuracy comes out to be **0.94** and recall for minority class **1** is **0.90**.\nEven though our model is performing well for undersampled data but we have loss quite amount data from majority class. So above model may not be ideal model.","ffeb404c":"Here 0 denotes non-fraudulant transaction and 1 denotes fraudulant dataset.It's the clear indication of class imbalace.","f3dfeff3":"# Credit card fraud detection\n\nWe will in this notebook will work on credit card fraud detection. One of the major issues in the cases like this is class imbalance problem. Out of all transction there will be around 1 or 2 percent of transaction which is fraudulant and remaining 98% as non-fraudulant. This creates a major issues in classification models. so will ssee various ways to handle such imbalance","bae571c3":"### Checking the target variable","141235bb":"With training and test set accuracy almost **100**% but recall score of label\/class **1** is **0.62** . So this model is not satisfying.","f7c08aa7":"**SMOTE** (synthetic minority oversampling technique) is one of the most commonly used **oversampling** methods to solve the imbalance problem.\nIt aims to balance class distribution by randomly increasing minority class examples by replicating them.\n**SMOTE** and **Oversampling** are closely related.","add9ffc9":"### Feature scaling\n\nwe can clearly see in our dataset the feature 'Amount is quite different from remaining feature. i mean its range is quite different (big one) than remaining. This can be issue for out classification algorithms so we need to scale it.  we can either scale whole dataset or only this \"Amount\" one since other features are in close ranges to each others.","6bc36fc8":"Over-sampling is the process of randomly duplicating observations from the minority class making it proportiate to majority class.\n\nApproach: we will first separate data into majority class and minority class and than we will oversample the minority class observation with replacement. and then we will combine majority class observation and oversampled minority class observation. Thats it.","8c714778":"here accuracy comes out to be **0.98** and recall forminority class 1 is **0.92**\nThis model is performing quite well.\nThere is slight case of underfitting which can be dealt by tuning hyperparameter. Since we are using logistic regression model, our hyperparameter is **'c'**","a04452e3":"### Under-Sampling Techniques","34788188":"There is no missing values on our dataset and all the features are numeric"}}