{"cell_type":{"c40a827d":"code","edee90fc":"code","f67c9112":"code","d10f5b77":"code","4b4b631e":"code","8a293bec":"code","710538e0":"code","6989cbfa":"code","7fbbb40e":"code","a90da677":"code","ecfb2ec1":"code","e3a9b455":"code","af00c742":"code","7c83ba37":"code","801bd246":"code","bad808f2":"code","5ac2429a":"code","8f20e1a6":"code","3f93d944":"code","f3bbae4e":"code","e0af7388":"code","ae852ba7":"code","e5ff6f0d":"code","0259e275":"code","f95df01f":"code","b19269b5":"markdown","2162f52e":"markdown","9dd09f41":"markdown","d14c82bd":"markdown","92db6e5f":"markdown","94f3d4da":"markdown","4d9c178e":"markdown","04c4fe18":"markdown","7c4fec6a":"markdown","c9a82184":"markdown","95574490":"markdown","7b686d72":"markdown","9bfa5751":"markdown","5cabfe14":"markdown","25fbf3ab":"markdown"},"source":{"c40a827d":"import numpy as np\nimport pandas as pd\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","edee90fc":"%%time\n# read dataframe\ndf_train = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ndf_test  = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\n\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")","f67c9112":"num_cols = [col for col in df_test.columns]\n\ndf_train[\"mean\"] = df_train[num_cols].mean(axis=1)\ndf_train[\"std\"]  = df_train[num_cols].std(axis=1)\ndf_train[\"min\"]  = df_train[num_cols].min(axis=1)\ndf_train[\"max\"]  = df_train[num_cols].max(axis=1)\n\ndf_test[\"mean\"] = df_test[num_cols].mean(axis=1)\ndf_test[\"std\"]  = df_test[num_cols].std(axis=1)\ndf_test[\"min\"]  = df_test[num_cols].min(axis=1)\ndf_test[\"max\"]  = df_test[num_cols].max(axis=1)","d10f5b77":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n    \n            # test if column can be converted to an integer\n            asint = props[col].astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props","4b4b631e":"import gc\n\ndf_train = reduce_mem_usage(df_train)\ndf_test  = reduce_mem_usage(df_test)\ngc.collect()","8a293bec":"# prepare dataframe for modeling\nX = df_train.drop(columns=[\"id\", \"target\"]).copy()\ny = df_train[\"target\"].copy()\n\ntest_data = df_test.drop(columns=[\"id\"]).copy()","710538e0":"cat_cols = X.columns[(X.nunique() < 5)]\ncon_cols = X.columns[(X.nunique() >= 5)]\ncat_cols_indices = [X.columns.get_loc(col) for col in cat_cols]\nprint(f\"cat_cols: {cat_cols}\")","6989cbfa":"import gc\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX[con_cols] = reduce_mem_usage(pd.DataFrame(columns=con_cols, data=scaler.fit_transform(X[con_cols])))\ntest_data[con_cols] = reduce_mem_usage(pd.DataFrame(columns=con_cols, data=scaler.transform(test_data[con_cols])))\ngc.collect()","7fbbb40e":"from sklearn.model_selection import train_test_split\n\ncal_X_train, cal_X_val, cal_y_train, cal_y_val = train_test_split(X, y, random_state=0, stratify=y, test_size=.95)","a90da677":"from sklearn.model_selection import train_test_split\n\ncal_X_train, cal_X_val, cal_y_train, cal_y_val = train_test_split(X, y, random_state=0, stratify=y, test_size=.75)","ecfb2ec1":"import json\nimport optuna\nimport lightgbm as lgbm\nimport optuna.integration.lightgbm as lgbo\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlgbm_params_0 = {\n  \"objective\": \"binary\",\n  \"metric\": \"auc\",\n  \"learning_rate\": 0.08,\n  \"device\": \"gpu\",\n  \"verbose\": 0, \n  \"feature_pre_filter\": False, \n  \"lambda_l1\": 9.314037635261775, \n  \"lambda_l2\": 0.10613573572440353,\n  \"num_leaves\": 7,\n  \"feature_fraction\": 0.4, \n  \"bagging_fraction\": 0.8391963650875751, \n  \"bagging_freq\": 5, \n  \"min_child_samples\": 100,\n  \"num_iterations\": 10000,\n  \"n_estimators\": 20000,\n  \"random_state\": 42\n}\n\nif lgbm_params_0 is None:\n    lgb_train = lgbm.Dataset(cal_X_train, cal_y_train, categorical_feature=cat_cols_indices)\n    lgb_valid = lgbm.Dataset(cal_X_val,   cal_y_val, categorical_feature=cat_cols_indices)\n\n    model = lgbo.train(\n        {\n            \"objective\": \"binary\",\n            \"metric\": \"auc\", \n            \"categorical_feature\": cat_cols_indices,\n            \"n_estimators\": 10_000, \n            \"learning_rate\": 0.08, \n            \"device\": \"gpu\", \n            \"verbose\": 0\n        }, \n        lgb_train, \n        valid_sets=[lgb_valid], \n        verbose_eval=False, \n        num_boost_round=100, \n        verbosity=0, \n        early_stopping_rounds=5, \n        optuna_seed=42\n    )\n\n    lgbm_params_0 = model.params\n\n    lgbm_params_0[\"n_estimators\"] = 20_000\n    lgbm_params_0[\"random_state\"] = 42\n\n    del lgbm_params_0[\"early_stopping_round\"]\n\nwith open(\"lgbm_params_0.json\".format(), \"w\") as file:\n    file.write(json.dumps(lgbm_params_0, indent=4))","e3a9b455":"import json\n\nlgbm_params_1 = lgbm_params_0.copy()\n\nlgbm_params_1[\"random_state\"] = 187\n\n\nwith open(\"lgbm_params_1.json\".format(), \"w\") as file:\n    file.write(json.dumps(lgbm_params_1, indent=4))","af00c742":"import json\n\nlgbm_params_2 = lgbm_params_0.copy()\n\nlgbm_params_2[\"random_state\"] = 256\n\n\nwith open(\"lgbm_params_2.json\".format(), \"w\") as file:\n    file.write(json.dumps(lgbm_params_2, indent=4))","7c83ba37":"import json\nimport optuna\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier, Pool\n\ndef objective(trial):\n    param = {\n        \"objective\": \"CrossEntropy\",\n        \"eval_metric\" : \"AUC\",\n        \"task_type\": \"GPU\",\n        \"grow_policy\": \"SymmetricTree\",\n        \"use_best_model\" : True,\n        \"learning_rate\": 0.08,\n        \"n_estimators\":  10_000,\n        \"random_strength\" : trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"max_bin\": 128,\n        \"cat_features\": cat_cols_indices,\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-5, 1.0),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 300),\n        \"random_state\": 42,\n    }\n    \n    model = CatBoostClassifier(**param)  \n    \n    model.fit(\n        cal_train_pool,\n        eval_set=[cal_val_pool],\n        early_stopping_rounds=5,\n        verbose=False\n    )\n    \n    preds = model.predict_proba(cal_X_val)[:,-1]\n    \n    return roc_auc_score(cal_y_val, preds)\n\n\ncatb_params_0 = {\n    \"objective\": \"CrossEntropy\",\n    \"eval_metric\" : \"AUC\",\n    \"task_type\": \"GPU\",\n    \"grow_policy\": \"SymmetricTree\",\n    \"use_best_model\" : True,\n    \"learning_rate\": 0.01,\n    \"n_estimators\":  20_000,\n    \"random_strength\" : 1.0,\n    \"max_bin\": 128,\n    \"l2_leaf_reg\": 0.0007202715557592255,\n    \"max_depth\": 4,\n    \"min_data_in_leaf\": 103,\n    \"random_state\": 42,\n}\n\nif catb_params_0 is None:\n    cal_train_pool = Pool(cal_X_train, cal_y_train, cat_features=cat_cols_indices)\n    cal_val_pool   = Pool(cal_X_val, cal_y_val, cat_features=cat_cols_indices)\n\n    cal_train_pool.quantize(max_bin=128)\n    cal_val_pool.quantize(max_bin=128)\n    \n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=optuna.pruners.MedianPruner(\n            n_startup_trials=5, n_warmup_steps=10, interval_steps=5\n        ),\n    )\n    study.optimize(objective, n_trials=20)\n    print(\"Number of finished trials:\", len(study.trials))\n    print(\"Best trial:\", study.best_trial.params)\n\n    catb_params_0 = study.best_trial.params\n\n    catb_params_0[\"n_estimators\"] = 20_000\n    catb_params_0[\"learning_rate\"] = 0.01\n    catb_params_0[\"max_bin\"] = 128\n    catb_params_0[\"random_strength\"] = 1.0\n    catb_params_0[\"random_state\"] = 42\n    catb_params_0[\"use_best_model\"] = True\n    catb_params_0[\"objective\"] = \"CrossEntropy\"\n    catb_params_0[\"grow_policy\"] = \"SymmetricTree\"\n    catb_params_0[\"eval_metric\"] = \"AUC\"\n    catb_params_0[\"task_type\"] = \"GPU\"\n    \n    del cal_train_pool\n    del cal_val_pool\n    \n    fig = optuna.visualization.plot_parallel_coordinate(study)\n    fig.show()\n\nwith open(\"catb_params_0.json\".format(), \"w\") as file:\n    file.write(json.dumps(catb_params_0, indent=4))","801bd246":"import gc\n\ngc.collect()","bad808f2":"import json\n\ncatb_params_1 = catb_params_0.copy()\n\ncatb_params_1[\"random_state\"] = 187\n\n\nwith open(\"catb_params_1.json\".format(), \"w\") as file:\n    file.write(json.dumps(catb_params_1, indent=4))","5ac2429a":"import json\n\ncatb_params_2 = catb_params_0.copy()\n\ncatb_params_2[\"random_state\"] = 256\n\n\nwith open(\"catb_params_2.json\".format(), \"w\") as file:\n    file.write(json.dumps(catb_params_2, indent=4))","8f20e1a6":"import json\nfrom xgboost import XGBClassifier\n\ndef objective(trial):\n    param = {\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"tree_method\": \"gpu_hist\",\n        \"learning_rate\": 0.02,\n        \"n_estimators\": 10_000,\n        \"random_state\": 42,\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.5, 0.8),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 2e-3, 0.008),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.51, 0.55),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 0.65),\n        \"max_depth\": 17, # trial.suggest_int(\"max_depth\", 17, 18, 1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 150, 200),\n    }\n    model = XGBClassifier(**param)  \n    \n    model.fit(\n        cal_X_train, \n        cal_y_train,\n        eval_set=[(cal_X_val, cal_y_val)],\n        early_stopping_rounds=5,\n        verbose=False\n    )\n    \n    preds = model.predict_proba(cal_X_val)[:,-1]\n    \n    return roc_auc_score(cal_y_val, preds)\n\nxgb_params_0 = {\n    \"lambda\": 0.5397422302447832,\n    \"alpha\": 0.007483070716022332,\n    \"colsample_bytree\": 0.5400956175261262,\n    \"subsample\": 0.50044109494562,\n    \"min_child_weight\": 200,\n    \"n_estimators\": 20_000,\n    \"random_state\": 42,\n    \"learning_rate\": 0.005,\n    \"tree_method\": \"gpu_hist\",\n    \"eval_metric\": \"auc\",\n    \"objective\": \"binary:logistic\"\n}\n\nif xgb_params_0 is None:\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=optuna.pruners.MedianPruner(\n            n_startup_trials=5, n_warmup_steps=10, interval_steps=5\n        ),\n    )\n    study.optimize(objective, n_trials=50)\n    print(\"Number of finished trials:\", len(study.trials))\n    print(\"Best trial:\", study.best_trial.params)\n\n    xgb_params_0 = study.best_trial.params\n\n    xgb_params_0[\"n_estimators\"] = 20_000\n    xgb_params_0[\"random_state\"] = 42\n    xgb_params_0[\"learning_rate\"] = 0.005\n    xgb_params_0[\"tree_method\"] = \"gpu_hist\"\n    xgb_params_0[\"eval_metric\"] = \"auc\"\n    xgb_params_0[\"objective\"] = \"binary:logistic\"\n    \n    fig = optuna.visualization.plot_parallel_coordinate(study)\n    fig.show()\n\nwith open(\"xgb_params_0.json\".format(), \"w\") as file:\n    file.write(json.dumps(xgb_params_0, indent=4))","3f93d944":"import json\n\nxgb_params_1 = xgb_params_0.copy()\n\nxgb_params_1[\"random_state\"] = 187\n\n\nwith open(\"xgb_params_1.json\".format(), \"w\") as file:\n    file.write(json.dumps(xgb_params_1, indent=4))","f3bbae4e":"xgb_params_2 = {\n    \"objective\": \"binary:logistic\",\n    \"learning_rate\": 8e-3,\n    \"seed\": 42,\n    \"subsample\": 0.6,\n    \"colsample_bylevel\": 0.9,\n    \"colsample_bytree\": 0.4,\n    \"n_estimators\": 20_000,\n    \"max_depth\": 8,\n    \"alpha\": 64,\n    \"lambda\": 32,\n    \"min_child_weight\": 8,\n    \"importance_type\": \"total_gain\",\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n}","e0af7388":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nmodels = [\n    (\"lgbm0\", LGBMClassifier(**lgbm_params_0)),\n    (\"lgbm1\", LGBMClassifier(**lgbm_params_1)),\n    (\"lgbm2\", LGBMClassifier(**lgbm_params_2)),\n    (\"catb0\", CatBoostClassifier(**catb_params_0)),\n    (\"catb1\", CatBoostClassifier(**catb_params_1)),\n    (\"catb2\", CatBoostClassifier(**catb_params_2)),\n    (\"xgb0\", XGBClassifier(**xgb_params_0)),\n    (\"xgb1\", XGBClassifier(**xgb_params_1)),\n    (\"xgb2\", XGBClassifier(**xgb_params_2)),\n]\n\noof_pred_tmp = dict()\ntest_pred_tmp = dict()\nscores_tmp = dict()\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    for name, model in models:\n        if name not in scores_tmp:\n            oof_pred_tmp[name] = list()\n            oof_pred_tmp[\"y_valid\"] = list()\n            test_pred_tmp[name] = list()\n            scores_tmp[name] = list()\n     \n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_valid,y_valid)],\n            early_stopping_rounds=500,\n            verbose=0\n        )\n        \n        pred_valid = model.predict_proba(X_valid)[:, -1]\n        score = roc_auc_score(y_valid, pred_valid)\n        \n        scores_tmp[name].append(score)\n        oof_pred_tmp[name].extend(pred_valid)\n        \n        print(f\"Fold: {fold + 1} Model: {name} Score: {score}\")\n        print(\"--\"*20)\n        \n        y_hat = model.predict_proba(test_data)[:, -1]\n        test_pred_tmp[name].append(y_hat)\n    oof_pred_tmp[\"y_valid\"].extend(y_valid)\n\nfor name, model in models:\n    print(f\"Overall Validation Score | {name}: {np.mean(scores_tmp[name])}\")\n    print(\"::\"*20)","ae852ba7":"# create df with base predictions on test_data\nbase_test_predictions = pd.DataFrame(\n    {name: np.mean(np.column_stack(test_pred_tmp[name]), axis=1) \n    for name in test_pred_tmp.keys()}\n)\n\n# save csv checkpoint\nbase_test_predictions.to_csv(\".\/base_test_predictions.csv\", index=False)\n\n# create simple average blend \nbase_test_predictions[\"simple_avg\"] = base_test_predictions.mean(axis=1)\n\n# create submission file with simple blend average\nsimple_blend_submission = sample_submission.copy()\nsimple_blend_submission[\"claim\"] = base_test_predictions[\"simple_avg\"]\nsimple_blend_submission.to_csv(\".\/simple_blend_submission.csv\", index=False)","e5ff6f0d":"# create training set for meta learner based on the oof_predictions of the base models\noof_predictions = pd.DataFrame(\n    {name:oof_pred_tmp[name] for name in oof_pred_tmp.keys()}\n)\n\n# save csv checkpoint\noof_predictions.to_csv(\".\/oof_predictions.csv\", index=False)\n\n# get simple blend validation score\ny_valid = oof_predictions[\"y_valid\"].copy()\ny_hat_blend = oof_predictions.drop(columns=[\"y_valid\"]).mean(axis=1)\nscore = roc_auc_score(y_valid, y_hat_blend)\n\nprint(f\"Overall Validation Score | Simple Blend: {score}\")\nprint(\"::\"*20)","0259e275":"%%time\nfrom sklearn.linear_model import LogisticRegression\n\n# prepare meta_training set\nX_meta = oof_predictions.drop(columns=[\"y_valid\"]).copy()\ny_meta = oof_predictions[\"y_valid\"].copy()\ntest_meta = base_test_predictions.drop(columns=[\"simple_avg\"]).copy()\n\nmeta_pred_tmp = []\nscores_tmp = []\n\n# create cv\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X_meta, y_meta)):\n    # create train, validation sets\n    X_train, y_train = X_meta.iloc[idx_train], y_meta.iloc[idx_train]\n    X_valid, y_valid = X_meta.iloc[idx_valid], y_meta.iloc[idx_valid]\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    \n    # validation prediction\n    pred_valid = model.predict_proba(X_valid)[:,1]\n    score = roc_auc_score(y_valid, pred_valid)\n    scores_tmp.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print(\"--\"*20)\n    \n    # test prediction based on oof_set\n    y_hat = model.predict_proba(test_meta)[:,1]\n    meta_pred_tmp.append(y_hat)\n    \n# print overall validation scores\nprint(f\"Overall Validation Score | Meta: {np.mean(scores_tmp)}\")\nprint(\"::\"*20)","f95df01f":"# average meta predictions over each fold\nmeta_predictions = np.mean(np.column_stack(meta_pred_tmp), axis=1)\n\n# create submission file\nstacked_submission = sample_submission.copy()\nstacked_submission[\"target\"] = meta_predictions\nstacked_submission.to_csv(\".\/stacked_submission.csv\", index=False)","b19269b5":"I load the data using pandas read_csv method. A better way would be to create a feather dataset... TBC","2162f52e":"# Lvl 2 - Logistic Regression","9dd09f41":"I try to collect the pools here:","d14c82bd":"Scale all non-categorical columns","92db6e5f":"# Level 1 - LGBM\/CatB\/XGB\n\nI do an KFold on all models with the previously tuned hyperparams.","94f3d4da":"Since the dataset is verrrrry large I tend to use this function a lot. It converts each column to the best fitting datatype for its range.","4d9c178e":"Hello There,\n\nin this TPS I combined a lot of notebooks i've read and code from previous TPSs to create this.","04c4fe18":"## Catboost\n\nThe barriers for the searchspace of this study where narrowed down by previous searches. I used the plot below to determine promising barriers. Remove my fixed parameters to tune yourself.","7c4fec6a":"## XGBoost\n\nAs with Catboost, the barriers for the searchspace of this study where also narrowed down by previous searches. I will look into more tunable params and diffrent boosting methods. Remove my fixed parameters to tune yourself.","c9a82184":"# Feature Engineering\n\nI add some mean, std, min, max columns. Useless features will be removed by the feature selection step later.","95574490":"## LGBM\n\nI use the lgbm integration to do this study. Nothing special here. Remove my fixed parameters to tune yourself.","7b686d72":"# Optuna Studies","9bfa5751":"I create another params preset with diffrent seed, i plan on using another study here in the future, with different boosting or something like that.","5cabfe14":"Here i find the columns with less than 5 unique values and add them to the list of categorical columns.","25fbf3ab":"I create another params preset with different seed, i plan on using another study here in the future, with different boosting or something like that."}}