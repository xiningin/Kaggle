{"cell_type":{"b34f770b":"code","197e0948":"code","fa4d520c":"code","827910f4":"code","59388be2":"code","bb72d9b8":"code","ab696105":"code","e215ee82":"code","3ef5891b":"code","bb48efb0":"code","f6bc9af9":"code","a2d2b001":"code","6b44d2c0":"code","66f3d8e5":"code","6f134555":"code","2ee15b63":"code","551c4a3c":"code","80e3046f":"code","f07e80ce":"code","7ecfaacd":"code","aa815e55":"code","cfca39fa":"code","863de041":"code","a9b31fec":"code","0d9a7e20":"code","2e738c77":"code","d7b8f07c":"code","5158ee1b":"code","a4e12c64":"code","aed4c66a":"code","29e869fd":"code","40e5af8e":"code","62ced02a":"code","e8f3aebe":"code","957520bb":"code","0e53c8eb":"code","0956d927":"code","c2f0e85b":"code","e9a5bdda":"code","374cd76e":"code","d6a3ae86":"code","b6d00e97":"code","07cccb1d":"code","3f361f7e":"code","c239125b":"code","ac4d394b":"code","1c231770":"code","944e50fa":"code","032ebae3":"code","e301804b":"code","62c19f8f":"code","91f2b482":"code","e337781f":"code","1ec622bc":"code","7106d562":"code","fb005e80":"code","984f62b5":"code","a1dde603":"code","79d94a0f":"code","ae056c37":"code","0ca8f488":"code","2569dd1b":"code","b5b4d326":"code","eb1cf974":"code","25f89981":"code","2306e351":"code","38d23ded":"code","0b4a261e":"code","e3710888":"code","bc6df4c2":"code","25b014f4":"code","47addd68":"code","3398b3bf":"markdown","74d142fa":"markdown","217480b4":"markdown","49779a26":"markdown","b9bf64df":"markdown","f36640d5":"markdown","2f0bb0d2":"markdown","dcf74951":"markdown","e950d10c":"markdown","a63bc348":"markdown","f9aa3853":"markdown","5ae031a4":"markdown","23387880":"markdown","45de4d02":"markdown","9cbdf2ad":"markdown","49ae6634":"markdown","6e4f9843":"markdown","e1cc82d2":"markdown","ae1a6933":"markdown","7524edc4":"markdown","3d350b06":"markdown","41d48c4c":"markdown","deffdfed":"markdown","43a0a5c5":"markdown","0051e351":"markdown","7c75ae9a":"markdown","7b661153":"markdown","9458bc55":"markdown","b3b9e471":"markdown","c7262aaf":"markdown","7b3253d6":"markdown","c8986ad5":"markdown","38d63777":"markdown","878ee77b":"markdown","6ee9cb1e":"markdown","51c76eb0":"markdown","b89af0b5":"markdown","3a9bdc9f":"markdown","3e4e82a1":"markdown","28d6f6fa":"markdown","85aaa388":"markdown","79166ff2":"markdown","7d59b569":"markdown","47742519":"markdown","9c273595":"markdown","0a884cc3":"markdown","c0359834":"markdown","17987a17":"markdown","10eba161":"markdown","860c6f27":"markdown","13131792":"markdown","a323a3aa":"markdown","e85d9919":"markdown","7a7d0cba":"markdown","80b30772":"markdown","447158df":"markdown"},"source":{"b34f770b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # using for plot interacting plots\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","197e0948":"#The data sets are read\ntrain_titanic=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_titanic=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n#The gender_submission data set is an expample how the final result has to look like. Is is needed neither to create the model nor to test it.\ngender_submission=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n","fa4d520c":"train_titanic.shape","827910f4":"train_titanic.info()","59388be2":"train_titanic.describe()","bb72d9b8":"print('Oldest Passenger was of:',train_titanic['Age'].max(),'Years')\nprint('Youngest Passenger was of:',train_titanic['Age'].min(),'Years')\nprint('Average Age on the ship:',train_titanic['Age'].mean(),'Years')","ab696105":"import pandas_profiling\npandas_profiling.ProfileReport(train_titanic)","e215ee82":"#Seeing how the different features are distributed\nimport matplotlib.pyplot as plt\ntrain_titanic.hist(figsize=(15,8))\nplt.figure()","3ef5891b":"#Seeing the people who survived VS deaths\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"Survived\"\ngrouped = train_titanic[col].value_counts().reset_index()\ngrouped=grouped.rename(columns={'index':'Survived', 'Survived':'count'})\ngrouped\n#plot\ntrace = go.Pie(labels=grouped['Survived'], values=grouped['count'], pull=[0.05, 0.05])\ntrace\ntitle = {'title': 'Survived(0 = No, 1 = Yes)'}\nfig = go.Figure(data = [trace],layout =title)\niplot(fig)","bb48efb0":"col = \"Sex\"\ngrouped = train_titanic[col].value_counts().reset_index()\ngrouped=grouped.rename(columns={'index':'Sex', 'Sex':'count'})\n#plot\ntrace = go.Pie(labels=grouped['Sex'], values=grouped['count'], pull=[0.05, 0.05])\ntrace\ntitle = {'title': 'Sex(male, female)'}\nfig = go.Figure(data = [trace],layout =title)\niplot(fig)\n","f6bc9af9":"grouped = train_titanic['Sex'].value_counts().reset_index()\ngrouped=grouped.rename(columns={'index':'Sex', 'Sex':'count'})\ngrouped['%']=(grouped['count']\/sum(grouped['count'])*100).round(2)\nprint('O', grouped['%'][0],'% de todos os pasaxeiros son homes e o ', grouped['%'][1], '% son mulleres')","a2d2b001":"count_sex_survived=train_titanic.groupby(by=['Sex','Survived']).count().reset_index()\nfemale=[]\nmale=[]\nn=0\nfor gender in count_sex_survived['Sex']:\n    if gender=='female':\n        female.append(count_sex_survived['PassengerId'][n])\n    else:\n        male.append(count_sex_survived['PassengerId'][n])\n    n=n+1\nsurvived_male=pd.Series({'Survived':male[1],\n                   'Dead':male[0]})\nsurvived_female=pd.Series({'Survived':female[1],\n                          'Dead':female[0]})\ndf_bar_plot=pd.DataFrame([survived_male, survived_female], index=['male', 'female'])\ndf_bar_plot.plot.bar()","6b44d2c0":"passenger_class=train_titanic.groupby(by='Pclass').count()['PassengerId']\npassenger_class.plot.bar()","66f3d8e5":"surviving_class=train_titanic.groupby(by=['Pclass', 'Survived']).count()['PassengerId'].reset_index()\nsurviving_class=surviving_class.set_index('Pclass')\nlista_1=list(surviving_class['PassengerId'][1])\nlista_2=list(surviving_class['PassengerId'][2])\nlista_3=list(surviving_class['PassengerId'][3])\n\nsurviving_class_plot=pd.DataFrame([lista_1, lista_2, lista_3], index=['class_1', 'class_2', 'class_3'])\nsurviving_class_plot.plot.bar()","6f134555":"embarked_count=train_titanic.groupby(by='Embarked').count()['PassengerId']\nembarked_count.plot.bar()","2ee15b63":"surviving_class=train_titanic.groupby(by=['Embarked', 'Survived']).count()['PassengerId'].reset_index()\nsurviving_class=surviving_class.set_index('Embarked')\nlista_1=list(surviving_class['PassengerId']['C'])\nlista_2=list(surviving_class['PassengerId']['Q'])\nlista_3=list(surviving_class['PassengerId']['S'])\n\nsurviving_class_plot=pd.DataFrame([lista_1, lista_2, lista_3], index=['C', 'Q', 'S'])\nsurviving_class_plot.plot.bar()","551c4a3c":"people_age=train_titanic.groupby(by='Age').count()['PassengerId'].reset_index()\ny=people_age['PassengerId']\nplt.plot(y)","80e3046f":"age_survivors_vs_deads=train_titanic.dropna(how='any',subset=['Age'])\nage_survivors_vs_deads=age_survivors_vs_deads.groupby(by=['Age', 'Survived']).count()['PassengerId'].reset_index()\nage_survivors_vs_deads=age_survivors_vs_deads.rename(columns={'PassengerId':'count'})\n\nage_survivors=age_survivors_vs_deads[age_survivors_vs_deads['Survived']==1]\nx_survivors=age_survivors['Age']\ny_survivors=age_survivors['count']\n\nage_deads=age_survivors_vs_deads[age_survivors_vs_deads['Survived']==0]\nx_deads=age_deads['Age']\ny_deads=age_deads['count']\nplt.plot(x_survivors, y_survivors, label='survivors')\nplt.plot(x_deads, y_deads, label='dead')\nplt.legend()\nplt.show()","f07e80ce":"fare_ticket=train_titanic.groupby(by='Fare').count()\nfare_ticket=fare_ticket.rename(columns={'PassengerId':'count'})\nplt.plot(fare_ticket['count'])","7ecfaacd":"fare_ticket=train_titanic.dropna(how='any', subset=['Fare'])\nfare_ticket=fare_ticket.groupby(by=['Fare', 'Survived']). count().reset_index()\nfare_ticket=fare_ticket.rename(columns={'PassengerId':'count'})\n\nfare_ticket_survivors=fare_ticket[fare_ticket['Survived']==1]\nx_survivor=fare_ticket_survivors['Fare']\ny_survivor=fare_ticket_survivors['count']\n\nfare_ticket_deads=fare_ticket[fare_ticket['Survived']==0]\nx_dead=fare_ticket_deads['Fare']\ny_dead=fare_ticket_deads['count']\n\nplt.plot(x_survivor, y_survivor, label='survivors')\nplt.plot(x_dead, y_dead, label='deads')\nplt.legend()\nplt.show()","aa815e55":"plt.hist(train_titanic['Age'], bins=15)","cfca39fa":"train_titanic.corr()","863de041":"fig,ax = plt.subplots(figsize=(8,7))\nax = sns.heatmap(train_titanic.corr(), annot=True, linewidths=.5, fmt='.1f')\nplt.show()","a9b31fec":"train_titanic.groupby(by='Pclass')['Survived'].mean()","0d9a7e20":"train_titanic.groupby(by='Sex')['Survived'].mean()\n\n#that's pretting amazing correlation. females are 4 times more likely to survive","2e738c77":"hist_survivors_class=train_titanic\n#Importante os par\u00e9ntesis cando usas o operadoes ('&' e '|')\nhist_survivors_class_1_0=hist_survivors_class['Age'][(hist_survivors_class['Pclass']==1) & (hist_survivors_class['Survived']==0)]\nhist_survivors_class_1_1=hist_survivors_class['Age'][(hist_survivors_class['Pclass']==1) & (hist_survivors_class['Survived']==1)]\nhist_survivors_class_2_0=hist_survivors_class['Age'][(hist_survivors_class['Pclass']==2) & (hist_survivors_class['Survived']==0)]\nhist_survivors_class_2_1=hist_survivors_class['Age'][(hist_survivors_class['Pclass']==2) & (hist_survivors_class['Survived']==1)]\nhist_survivors_class_3_0=hist_survivors_class['Age'][(hist_survivors_class['Pclass']==3) & (hist_survivors_class['Survived']==0)]\nhist_survivors_class_3_1=hist_survivors_class['Age'][(hist_survivors_class['Pclass']==3) & (hist_survivors_class['Survived']==1)]\n\nfig = plt.figure()\nfig.suptitle('Histogram Class|Survived|Age')\nfig.set_figheight(10)\nfig.set_figwidth(15)\n\nplt.subplot(2, 3, 1)\nplt.title(\"Class 1|Dead\")\nplt.hist(hist_survivors_class_1_0)\n\nplt.subplot(2, 3, 2)\nplt.title(\"Class 1|Survive\")\nplt.hist(hist_survivors_class_1_1)\n\nplt.subplot(2, 3, 3)\nplt.title(\"Class 2|Dead\")\nplt.hist(hist_survivors_class_2_0)\n\nplt.subplot(2, 3, 4)\nplt.title(\"Class 2|Survive\")\nplt.hist(hist_survivors_class_2_1)\n\nplt.subplot(2, 3, 5)\nplt.title(\"Class 3|Dead\")\nplt.hist(hist_survivors_class_3_0)\n\nplt.subplot(2, 3, 6)\nplt.title(\"Class 3|Survive\")\nplt.hist(hist_survivors_class_3_1)\n\nplt.legend()\nplt.show()\n","d7b8f07c":"boxplot_=train_titanic\nboxplot_survived_male=boxplot_['Age'][(boxplot_['Survived']==1)&(boxplot_['Sex']=='male')].dropna()\nboxplot_survived_female=boxplot_['Age'][(boxplot_['Survived']==1)&(boxplot_['Sex']=='female')].dropna()\nboxplot_dead_male=boxplot_['Age'][(boxplot_['Survived']==0)&(boxplot_['Sex']=='male')].dropna()\nboxplot_dead_female=boxplot_['Age'][(boxplot_['Survived']==0)&(boxplot_['Sex']=='female')].dropna()\n\nl=pd.DataFrame({'survived_male': boxplot_survived_male,\n                'survived_female': boxplot_survived_female,\n               'dead_male':boxplot_dead_male,\n               'dead_female':boxplot_dead_female})\n\nl.boxplot(column=['survived_male', 'survived_female','dead_male','dead_female'])","5158ee1b":"hist_survivors_class=boxplot_\nboxplot_survived_male=boxplot_['Age'][(boxplot_['Survived']==1)&(boxplot_['Sex']=='male')].dropna()\nboxplot_survived_female=boxplot_['Age'][(boxplot_['Survived']==1)&(boxplot_['Sex']=='female')].dropna()\nboxplot_dead_male=boxplot_['Age'][(boxplot_['Survived']==0)&(boxplot_['Sex']=='male')].dropna()\nboxplot_dead_female=boxplot_['Age'][(boxplot_['Survived']==0)&(boxplot_['Sex']=='female')].dropna()\n\nbox_plot_data=[boxplot_survived_male,boxplot_survived_female,boxplot_dead_male,boxplot_dead_female]\nplt.boxplot(box_plot_data,patch_artist=True,labels=['survived_male','survived_female','dead_male','dead_female'])\nplt.show()\n \nplt.show()","a4e12c64":"train_titanic","aed4c66a":"train_titanic.drop(['Name','SibSp','Ticket','Cabin'],axis=1,inplace=True)","29e869fd":"#Setting as the index column the 'PassengerId feature. This is necessary to submit the result into kaggle'\ntrain_titanic=train_titanic.set_index(['PassengerId'])","40e5af8e":"train_titanic_dummy=pd.get_dummies(train_titanic)\ntrain_titanic_dummy","62ced02a":"missing_values=train_titanic_dummy.isnull().sum()\nmissing_values_percentage=train_titanic_dummy.isnull().sum()\/train_titanic_dummy.isnull().count()\ntable_missing_values=pd.concat([missing_values, missing_values_percentage], axis=1, keys=['total', 'percentage']).sort_values(by='total',ascending=False)\ntable_missing_values\n#Onde hai m\u00e1is missing values \u00e9 en 'Cabin', despois Age e por \u00faltimo Embarked.","e8f3aebe":"#Para aqueles valores onde non hai datos, vams a meter o valor medio da columna de Age\ntrain_titanic_dummy['Age']=train_titanic_dummy['Age'].fillna(train_titanic_dummy['Age'].mean())\n#Para o cabin e o age, deixoos igual que est\u00e1n","957520bb":"#Volvemos a comprobar como queda o tema dos missing values\nmissing_values=train_titanic_dummy.isnull().sum()\nmissing_values_percentage=train_titanic_dummy.isnull().sum()\/train_titanic_dummy.isnull().count()\ntable_missing_values=pd.concat([missing_values, missing_values_percentage], axis=1, keys=['total', 'percentage']).sort_values(by='total',ascending=False)\ntable_missing_values","0e53c8eb":"from sklearn.model_selection import train_test_split #training and testing data split\ntrain,test=train_test_split(train_titanic_dummy,test_size=0.3,random_state=0, stratify=train_titanic_dummy['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\ntest_X=test[test.columns[1:]]\ntest_Y=test[test.columns[:1]]\n\n#Explicaci\u00f3 do de stratify\n#This stratify parameter makes a split so that the proportion of values in the sample produced\n#will be the same as the proportion of values provided to parameter stratify.\n#For example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% \n#of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% \n#of 1's.\n","0956d927":"#Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(train_X, train_Y)\n#Eiqui predicimos os valores de Y\nlogisticRegr_pred=logisticRegr.predict(test_X)\n#Eiqui enfrentamos os valores de Y predecidos, cos valores de Y reales (test_Y)\nlogisticRegr_accuracy=accuracy_score(test_Y, logisticRegr_pred)\nprint('The accuracy of the Logistic Regression is', logisticRegr_accuracy)","c2f0e85b":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconfusion_matrix_logistic=confusion_matrix(test_Y,logisticRegr_pred)\n\nconfusion_matrix_logistic_table=plot_confusion_matrix(confusion_matrix_logistic,\n                     colorbar=True,\n                    show_absolute=True,\n                    show_normed=True,\n                    class_names=['Survived', 'Not Survived'])","e9a5bdda":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n#fpr: False positive rate\n#tpr: True positive rate\nauc_logistic=roc_auc_score(test_Y, logisticRegr_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, logisticRegr_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('ROC curve. Area Under the Curve: %0.3f' %auc_logistic)\nplt.show()","374cd76e":"#KNeighbors Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(train_X, train_Y)\n#Eiqui predicimos os valores de Y\nknn_pred=knn.predict(test_X)\n#Eiqui enfrentamos os valores de Y predecidos, cos valores de Y reales (test_Y)\nknn_accuracy=accuracy_score(test_Y,knn_pred)\nprint('The accuracy of the K nearest neighbors is', knn_accuracy)","d6a3ae86":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconfusion_matrix_knn= confusion_matrix(test_Y, knn_pred)\n\nconfusion_matrix_knn_table= plot_confusion_matrix(confusion_matrix_knn,\n                                            show_absolute= True,\n                                            show_normed= True,\n                                            colorbar= True,\n                                           class_names=['Survived', 'Not Survived'])\n","b6d00e97":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nauc_knn= roc_auc_score(test_Y, knn_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, knn_pred)\nplt.plot(fpr,tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve_ Area under the curve %0.3f' %auc_knn)\nplt.show()","07cccb1d":"from sklearn.tree import DecisionTreeClassifier\ntree_clasiffier=DecisionTreeClassifier(max_depth=3)\ntree_clasiffier.fit(train_X, train_Y)\n#Creamos os y predictivos\ntree_pred=tree_clasiffier.predict(test_X)\ntree_accu=accuracy_score(test_Y, tree_pred)\nprint('The accuracy of the tree Classifier is', tree_accu)","3f361f7e":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconfusion_matrix_tree=confusion_matrix(test_Y, tree_pred)\n\nconfusion_matrix_tree_table=plot_confusion_matrix(confusion_matrix_tree,\n                     colorbar=True,\n                    show_absolute=True,\n                    show_normed=True,\n                    class_names=['Survived', 'Not Survived'])\n","c239125b":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n#fpr: False positive rate\n#tpr: True positive rate\nauc_tree=roc_auc_score(test_Y, tree_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, tree_pred )\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('ROC curve. Area Under the Curve: %0.3f' %auc_tree)\nplt.show()","ac4d394b":"import graphviz\nfrom sklearn import tree\n\ndot_data = tree.export_graphviz(tree_clasiffier, \n                                out_file=None,\n                                filled=True, \n                                rounded=True,  \n                                special_characters=True,\n                               feature_names=train_X.columns.values,\n                               class_names=['Survived','Not Survived']) \ngraph = graphviz.Source(dot_data)\ngraph","1c231770":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nNaiveBayes = GaussianNB()\nNaiveBayes.fit(train_X, train_Y)\n#Eiqui predicimos os valores de Y\nNaiveBayes_pred=NaiveBayes.predict(test_X)\n#Eiqui enfrentamos os valores de Y predecidos, cos valores de Y reales (test_Y)\nNaiveBayes_accuracy=accuracy_score(test_Y, NaiveBayes_pred)\nprint('The accuracy of the Naice Bayes model is', NaiveBayes_accuracy)","944e50fa":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconfusion_matrix_NaiveBayes= confusion_matrix(test_Y, NaiveBayes_pred)\nconfusion_matrix_NaiveBayes_table= plot_confusion_matrix(confusion_matrix_NaiveBayes,\n                                                   colorbar=True,\n                                                   show_absolute= True,\n                                                   show_normed= True,\n                                                  class_names=['Survived', 'Not Survived'])","032ebae3":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n#fpr: False positive rate\n#tpr: True positive rate\nauc_NaiveBayes=roc_auc_score(test_Y, NaiveBayes_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, NaiveBayes_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('ROC curve. Area Under the Curve: %0.3f' %auc_NaiveBayes)\nplt.show()","e301804b":"from sklearn.svm import SVC\n\nSupportVectorClassifier= SVC(kernel='linear', degree=1, gamma='auto')\nSupportVectorClassifier.fit(train_X, train_Y)\nSupportVectorClassifier_pred= SupportVectorClassifier.predict(test_X)\nSupportVectorClassifier_accu = accuracy_score(test_Y, SupportVectorClassifier_pred)\nprint(SupportVectorClassifier)\nprint('The accuracy of the Support Vector Classifier is', SupportVectorClassifier_accu)","62c19f8f":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconfusion_matrix_SVC= confusion_matrix(test_Y, SupportVectorClassifier_pred)\nconfusion_matrix_SVC_table= plot_confusion_matrix(confusion_matrix_SVC,\n                                                   colorbar=True,\n                                                   show_absolute= True,\n                                                   show_normed= True,\n                                                  class_names=['Survived', 'Not Survived'])","91f2b482":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n#fpr: False positive rate\n#tpr: True positive rate\nauc_SVC=roc_auc_score(test_Y, SupportVectorClassifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, SupportVectorClassifier_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('ROC curve. Area Under the Curve: %0.3f' %auc_SVC)\nplt.show()","e337781f":"from sklearn.ensemble import RandomForestClassifier\n\nRandomForestClassifier_clf = RandomForestClassifier(max_depth=5, random_state=0)\nRandomForestClassifier_clf.fit(train_X, train_Y)\nRandomForestClassifier_pred=RandomForestClassifier_clf.predict(test_X)\nRandomForestClassifier_accu= accuracy_score(test_Y, RandomForestClassifier_pred)\nprint(RandomForestClassifier_clf)\nprint('The accuracy of the Random Forest Classifier is', RandomForestClassifier_accu)\n","1ec622bc":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconfusion_matrix_RandomForest= confusion_matrix(test_Y, RandomForestClassifier_pred)\nconfusion_matrix_RandomForest_table= plot_confusion_matrix(confusion_matrix_RandomForest,\n                                                   colorbar=True,\n                                                   show_absolute= True,\n                                                   show_normed= True,\n                                                  class_names=['Survived', 'Not Survived'])","7106d562":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n#fpr: False positive rate\n#tpr: True positive rate\nauc_RandomForestClassifier=roc_auc_score(test_Y, RandomForestClassifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, RandomForestClassifier_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('ROC curve. Area Under the Curve: %0.3f' %auc_RandomForestClassifier)\nplt.show()","fb005e80":"from sklearn.neural_network import MLPClassifier\n\nMLP_Classifier = MLPClassifier(activation='logistic')\nMLP_Classifier.fit(train_X, train_Y)\nMLP_Classifier_pred=MLP_Classifier.predict(test_X)\nMLP_Classifier_accu= accuracy_score(test_Y, MLP_Classifier_pred)\nprint(MLP_Classifier)\nprint('The accuracy of the Random Forest Classifier is', MLP_Classifier_accu)","984f62b5":"from sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\nconfusion_matrix_MLPClassifier= confusion_matrix(test_Y, MLP_Classifier_pred)\nconfusion_matrix_MLPClassifier_table= plot_confusion_matrix(confusion_matrix_MLPClassifier,\n                                                   colorbar=True,\n                                                   show_absolute= True,\n                                                   show_normed= True,\n                                                  class_names=['Survived', 'Not Survived'])\n","a1dde603":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n#fpr: False positive rate\n#tpr: True positive rate\nauc_MLP=roc_auc_score(test_Y, MLP_Classifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, MLP_Classifier_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('ROC curve. Area Under the Curve: %0.3f' %auc_MLP)\nplt.show()","79d94a0f":"print('The accuracy of the Logistic Regression is', logisticRegr_accuracy)\nprint('The accuracy of the K nearest neighbors is', knn_accuracy)\nprint('The accuracy of the NaiveBayes is', NaiveBayes_accuracy)\nprint('The accuracy of the Tree Classifier is', tree_accu)\nprint('The accuracy of the Random Forest Classifier is', RandomForestClassifier_accu)\nprint('The accuracy of the SVC classifier is', SupportVectorClassifier_accu)\nprint('The accuracy of the MLP classifier is', MLP_Classifier_accu)","ae056c37":"model_accuracy= pd.Series([logisticRegr_accuracy, knn_accuracy, NaiveBayes_accuracy, \n                          tree_accu, RandomForestClassifier_accu, SupportVectorClassifier_accu, \n                          MLP_Classifier_accu], index=['Logistic regression', 'KNN', 'Naive Bayes',\n                                                      'Tree', 'Random Forest', 'SVC', 'MLP'])\nmodel_accuracy.sort_values().plot.barh(color=['pink', 'red', 'green', 'blue', 'cyan', 'yellow', 'purple'])\nplt.title('Model accuracies')","0ca8f488":"#https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/04.08-multiple-subplots.html\n\nfig = plt.figure()\nfig.suptitle('Confusion Matrix')\nfig.set_figheight(20)\nfig.set_figwidth(20)\n\nplt.subplot(3, 3, 1)\nplt.title(\"Logistic Regression\")\nsns.heatmap(confusion_matrix_logistic,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(3, 3, 2)\nplt.title(\"KNN\")\nsns.heatmap(confusion_matrix_knn,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\n\nplt.subplot(3, 3, 3)\nplt.title(\"Naive Bayes\")\nsns.heatmap(confusion_matrix_NaiveBayes,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(3, 3, 4)\nplt.title(\"Tree\")\nsns.heatmap(confusion_matrix_tree,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\n\nplt.subplot(3, 3, 5)\nplt.title(\"random forest\")\nsns.heatmap(confusion_matrix_RandomForest,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.subplot(3, 3, 6)\nplt.title(\"random forest\")\nsns.heatmap(confusion_matrix_SVC,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\n\nplt.subplot(3, 3, 7)\nplt.title(\"random forest\")\nsns.heatmap(confusion_matrix_MLPClassifier,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n\nplt.show()","2569dd1b":"fig = plt.figure()\nfig.suptitle('ROC curves')\nfig.set_figheight(20)\nfig.set_figwidth(20)\n\nplt.subplot(3, 3, 1)\nauc_logistic=roc_auc_score(test_Y, logisticRegr_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, logisticRegr_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('AUC logistic Regression %0.3f' %auc_logistic)\n\nplt.subplot(3, 3, 2)\nauc_knn= roc_auc_score(test_Y, knn_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, knn_pred)\nplt.plot(fpr,tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('AUC KNN: %0.3f' %auc_knn)\n\nplt.subplot(3,3,3)\nauc_NaiveBayes= roc_auc_score(test_Y, NaiveBayes_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, NaiveBayes_pred)\nplt.plot(fpr,tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('AUC Naive Bayes: %0.3f' %auc_NaiveBayes)\n\nplt.subplot(3, 3, 4)\nauc_tree=roc_auc_score(test_Y, tree_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, tree_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('AUC Tree Classifier: %0.3f' %auc_tree)\n\nplt.subplot(3, 3, 5)\nauc_RandomForestClassifier=roc_auc_score(test_Y, RandomForestClassifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, RandomForestClassifier_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('AUC Random Forest: %0.3f' %auc_RandomForestClassifier)\n\nplt.subplot(3, 3, 6)\nauc_SVC=roc_auc_score(test_Y, SupportVectorClassifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, SupportVectorClassifier_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('AUC Support Vector Machine: %0.3f' %auc_SVC)\n\nplt.subplot(3, 3, 7)\nauc_MLP=roc_auc_score(test_Y, MLP_Classifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, MLP_Classifier_pred)\nplt.plot(fpr, tpr)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\nplt.title('AUC Multi Layer Percepton Classifier: %0.3f' %auc_MLP)\n\nplt.show()","b5b4d326":"fig = plt.figure()\nfig.set_figheight(10)\nfig.set_figwidth(10)\n\nauc_logistic=roc_auc_score(test_Y, logisticRegr_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, logisticRegr_pred)\nplt.plot(fpr, tpr)\n\nauc_knn= roc_auc_score(test_Y, knn_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, knn_pred)\nplt.plot(fpr,tpr)\n\nauc_NaiveBayes= roc_auc_score(test_Y, NaiveBayes_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, NaiveBayes_pred)\nplt.plot(fpr,tpr)\n\nauc_tree=roc_auc_score(test_Y, tree_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, tree_pred)\nplt.plot(fpr, tpr)\n\nauc_RandomForestClassifier=roc_auc_score(test_Y, RandomForestClassifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, RandomForestClassifier_pred)\nplt.plot(fpr, tpr)\n\nauc_SVC=roc_auc_score(test_Y, SupportVectorClassifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, SupportVectorClassifier_pred)\nplt.plot(fpr, tpr)\n\nauc_MLP=roc_auc_score(test_Y, MLP_Classifier_pred)\nfpr, tpr, thresholds= roc_curve(test_Y, MLP_Classifier_pred)\nplt.plot(fpr, tpr)\n\nplt.plot([0,1], [0,1], 'r--')\n\nplt.legend(['Logistic regression', 'KNN', 'NaiveBayes', 'Tree',\n           'Random Forest', 'Support Vecto Machine', 'Multi Layer Perceptron'])\n\nplt.xlabel('False Postive Rate')\nplt.ylabel ('True Postive Rate')\n\nplt.title('ROC curves')\n\nplt.show()","eb1cf974":"train_titanic=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_titanic=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","25f89981":"train_X_submission=train_titanic_dummy[train_titanic_dummy.columns[1:]]\ntrain_Y_submission=train_titanic_dummy[train_titanic_dummy.columns[:1]]\ntest_titanic.drop(['Name','SibSp','Ticket','Cabin'],axis=1,inplace=True)\ntest_X_submission=pd.get_dummies (test_titanic)\n","2306e351":"train_X_submission.head()","38d23ded":"missing_values_submission=test_X_submission.isnull().sum()\nmissing_values_percentage_submission=test_X_submission.isnull().sum()\/test_X_submission.isnull().count()\ntable_missing_values_submission=pd.concat([missing_values_submission, missing_values_percentage_submission], axis=1, keys=['total', 'percentage']).sort_values(by='total',ascending=False)\ntable_missing_values_submission\n#Onde hai m\u00e1is missing values \u00e9 en 'Cabin', despois Age e por \u00faltimo Embarked.","0b4a261e":"#Para aqueles valores onde non hai datos, vams a meter o valor medio da columna de Age\ntest_X_submission['Age']=test_X_submission['Age'].fillna(test_X_submission['Age'].mean())\n#Para o fare tam\u00e9n metemos o valor medio\ntest_X_submission['Fare']=test_X_submission['Fare'].fillna(test_X_submission['Fare'].mean())","e3710888":"test_X_submission=test_X_submission.set_index(['PassengerId'])","bc6df4c2":"missing_values_submission=test_X_submission.isnull().sum()\nmissing_values_percentage_submission=test_X_submission.isnull().sum()\/test_X_submission.isnull().count()\ntable_missing_values_submission=pd.concat([missing_values_submission, missing_values_percentage_submission], axis=1, keys=['total', 'percentage']).sort_values(by='total',ascending=False)\ntable_missing_values_submission","25b014f4":"tree_clasiffier=DecisionTreeClassifier(max_depth=3)\ntree_clasiffier.fit(train_X_submission, train_Y_submission)\n#Creamos os y predictivos\ntest_Y_submission=tree_clasiffier.predict(test_X_submission)\noutput = pd.DataFrame({'PassengerId': test_X_submission.index.values, 'Survived': test_Y_submission})\noutput.to_csv('my_submission.csv', index=False)","47addd68":"output.shape","3398b3bf":"## 1.5 Number of people in each class","74d142fa":"## 1.10 Distribution of the passengers age depending on his\/her survival","217480b4":"## 1.12 Distribution of the fare depending on his\/her survival","49779a26":"# 2. Preparing data to create the model","b9bf64df":"## 1.11 Distribution of the fare","f36640d5":"## 3.7 Multi-layer Perceptron Classifier (MLP Classifier)","2f0bb0d2":"## 1.16 Distribution of the age along with the class and survival rate","dcf74951":"## 1.1 Seeing how the different features are distributed","e950d10c":"### 3.4.2 ROC curve - Gausian Naives Classifier\n\nFor more informtion about the ROC curve and the threshold see these links:\n\nROC curve http:\/\/benalexkeen.com\/scoring-classifier-models-using-scikit-learn\/\n\nThreshold and ROC curve https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c\n","a63bc348":"# 4. Submision of our model\nExplanation:  https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\n","f9aa3853":"### 3.3.1 Confusion Matrix - Decision Tree Classifier","5ae031a4":"### 3.2.2 ROC curve - K-Neighbors Classifier","23387880":"## 2.4 Missing values\nFor those cells with no data, we are going to complete them with the average value of the entire column. This operation is done just in the Age column as it is the one with missing values.","45de4d02":"## 3.5 Support Vector Classifier","9cbdf2ad":"### Same as above but in a boxplot graph","49ae6634":"## 4.3 Training and testing the final model - Tree Classifier","6e4f9843":"## 1.14 Average of people who save their lifes depending on the class","e1cc82d2":"### 3.7.2 ROC Curve - Multi-layer perceptron classifier","ae1a6933":"## 1.6 Surviving rate depending on the class","7524edc4":"# TITANIC DATASET. SURVIVED OR NOT SURVIVED?","3d350b06":"## 1.13 Correlation","41d48c4c":"## 3.10 ROC curve, all together\nFor more informtion about the ROC curve and the threshold see these links:   \nROC curve http:\/\/benalexkeen.com\/scoring-classifier-models-using-scikit-learn\/\n\nThreshold and ROC curve https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c","deffdfed":"## 3.9 Confusion Matrix, all together","43a0a5c5":"## 3.8 The accuracy scores of the models, all together","0051e351":"### 3.4.1 Confusion matrix - Gausian Naives Classifier","7c75ae9a":"## 2.2 Creating Dummy Variables\nhttps:\/\/towardsdatascience.com\/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40\n","7b661153":"### 3.6.2 ROC Curve - Random Forest Classifier","9458bc55":"## 3.1 Logistic Regression\n#### Nota: Nos utilizamos a Logistic Regression porque queremos un modelo de clasificaci\u00f3n (Sobrevive ou non Sobrevive). Se fose unha predicci\u00f3n de datos cont\u00ednuos pois utilizar\u00eda unha regresi\u00f3n linear ou polin\u00f3mica.","b3b9e471":"### 3.7.1 Confusion matrix - Multi-layer Perceptron Classifier","c7262aaf":"### 3.3.2 ROC curve - Decision Tree Classifier\n\nFor more informtion about the ROC curve and the threshold see these links:\n\nROC curve http:\/\/benalexkeen.com\/scoring-classifier-models-using-scikit-learn\/\n\nThreshold and ROC curve https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c\n","7b3253d6":"### 3.1.1 Confusion matrix - Logistic Regression","c8986ad5":"### 3.2.1 Confusion matrix - K-Neighbors Classifier","38d63777":"Esta p\u00e1xina e mui boa xa que che explica os hiperpar\u00e1metros do SVC, e as\u00ed podelos configurar como \u00e9 debido:  \nhttps:\/\/medium.com\/all-things-ai\/in-depth-parameter-tuning-for-svc-758215394769","878ee77b":"## 3.2 K-Neighbors Classifier\n\nNesta p\u00e1xian tes una explicaci\u00f3n dos par\u00e1metros para axustalos como \u00e9 debido:  \nhttps:\/\/medium.com\/@mohtedibf\/in-depth-parameter-tuning-for-knn-4c0de485baf6","6ee9cb1e":"## 1.2 Survivors VS deaths","51c76eb0":"## 4.2 Missing values","b89af0b5":"## 1.9 Distribution of the passengers age","3a9bdc9f":"## 1.8 Surviving rate depending on his\/her harbour","3e4e82a1":"## 3.3 Decision Tree Classifier\nVisit this website for more information related to Decision Tree Classifiers:\n\nhttps:\/\/towardsdatascience.com\/scikit-learn-decision-trees-explained-803f3812290d\n\nE nesta p\u00e1xina, tes cousas para axustar os par\u00e1metros do modelo:\nhttps:\/\/medium.com\/@mohtedibf\/indepth-parameter-tuning-for-decision-tree-6753118a03c3","28d6f6fa":"## 3.6 Random Forest Classifier\nhttps:\/\/towardsdatascience.com\/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76","85aaa388":"### 3.3.3 Ploting the tree Classifier","79166ff2":"### 3.5.1 Confusion matrix - Support Vector Classifier","7d59b569":"## 2.1 Dropping cells which they are not relevant for the model\n### A way to increase the accuracy of the models:\nIf we take a look to the matrix correlation, we can see that the features whith the highest correlatio vs Survived are: Pclass(0,3), Fare(0,3), Parch (0,1), Age (0,1). The other data has no correlation with the Survided people. Therefore, only the features whith some correlation with the \"Survived\" feature are going to be mantained in the data set. The rest of the columns are going to be deleted as they just create noise in the model.\nMoreover, I am going to also mantain the harbour where passengers embarked as whatching to the plots, I see some influence in the survival rate created by the \"Embarked\" feature","47742519":" ### 3.5.2 ROC curve - Support Vector Classifier\n \nFor more informtion about the ROC curve and the threshold see these links:\n\nROC curve http:\/\/benalexkeen.com\/scoring-classifier-models-using-scikit-learn\/\n\nThreshold and ROC curve https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c\n","9c273595":"## 1.4 Survivors women vs men","0a884cc3":"### 3.1.2 ROC curve - Logistic Regression","c0359834":"## 4.1 Data cleaning TEST data\nData cleaning of the test_titanic data (test_X_submission)","17987a17":"## 1.15 Average of people who saved their lifes depending on the sex","10eba161":"# 3. Building Machine Learning Models & Train Data\n\nThe train data set is going to be splitted in test and train in order to be able to calculate the accuracy of each model which will be tested.   \nFor more information see:  \nhttps:\/\/towardsdatascience.com\/cross-validation-in-machine-learning-72924a69872f   \nhttps:\/\/towardsdatascience.com\/balancing-bias-and-variance-to-control-errors-in-machine-learning-16ced95724db\n","860c6f27":"## 3.4 Gausian Naives Classifier","13131792":"## 1.3 Female VS male","a323a3aa":"## 2.3 Data cleaning","e85d9919":"### 3.6.1 Confusion Matrix - Random Classifier","7a7d0cba":"# 1. Looking at the data sets","80b30772":"## 1.7 Embarked Count","447158df":"## Different Classification models:\n\n![image.png](attachment:image.png)"}}