{"cell_type":{"3cca9c28":"code","01c867c5":"code","0257f4f6":"code","e95e7bb8":"code","e925ce34":"code","3db77a15":"code","754c4e88":"code","5457270c":"code","e4398dfd":"code","8f541e93":"code","2490786b":"code","2397387c":"code","a11fbbf0":"code","8bef72f9":"code","dd9302ed":"markdown","d65ae3d5":"markdown","9342291e":"markdown"},"source":{"3cca9c28":"#library and code setup\n!apt-get install openjdk-8-jdk-headless -qq > \/dev\/null\n!pip install -q pyspark\n!pip install newspaper3k\n\nimport pyspark, os\nfrom pyspark import SparkConf, SparkContext\nos.environ[\"PYSPARK_PYTHON\"]=\"python3\"\nos.environ[\"JAVA_HOME\"]=\"\/usr\/lib\/jvm\/java-8-openjdk-amd64\/\"\n","01c867c5":"#start spark local server\nimport sys, os\nfrom operator import add\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nos.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n\nimport pyspark\nfrom pyspark import SparkConf, SparkContext\n\n#connects our python driver to a local Spark JVM running on the Google Colab server virtual machine\ntry:\n  conf = SparkConf().setMaster(\"local[*]\").set(\"spark.executor.memory\", \"1g\")\n  sc = SparkContext(conf = conf)\nexcept ValueError:\n  #it's ok if the server is already started\n  pass\n\ndef dbg(x):\n  \"\"\" A helper function to print debugging information on RDDs \"\"\"\n  if isinstance(x, pyspark.RDD):\n    print([(t[0], list(t[1]) if \n            isinstance(t[1], pyspark.resultiterable.ResultIterable) else t[1])\n           if isinstance(t, tuple) else t\n           for t in x.take(100)])\n  else:\n    print(x)","0257f4f6":"# these remove files from previous runs\n!rm -rf articles\n!rm *.csv","e95e7bb8":"from datetime import date, timedelta\nimport pandas as pd\nimport os\nimport urllib.request\nimport io\n\n# generic functions to pull and write data to disk based on date\ndef get_filename(x):\n  date = x.strftime('%Y%m%d')\n  return \"{}_gdeltdata.csv\".format(date)\n\ndef intofile(filename):\n    try:\n      print(filename)\n      if not os.path.exists(filename):\n        ## list of countries to look for \n        df_list=[]\n        list_countres=['unitedstates']\n        queryURLbase = \"https:\/\/api.gdeltproject.org\/api\/v2\/doc\/doc?format=csv&query=(%22climate%20change%22%20OR%20%22global%20warming%22)\"\n        for country in list_countres:\n          queryURLbase= queryURLbase+\"%20sourcecountry:\"+country+\"%20sourcelang:eng&maxrecords=250&mode=artlist&sort=hybridrel\"\n          print(queryURLbase)\n          date = filename.split(\"_\")[0]\n          startdate = date+\"000000\" #0 hour 0 min 0 sec (start of day)\n          enddate = date+\"235959\" #23 hour 59 min 59 sec (end of day)\n          query = queryURLbase + \"&startdatetime=\" + startdate + \"&enddatetime=\" + enddate\n          data = io.StringIO(urllib.request.urlopen(query).read().decode())\n          df = pd.read_csv(data, sep=\",\")\n          df['country']=country\n         \n          \n          df_list.append(df)\n        df_full=pd.concat(df_list)\n        df_full.to_csv(filename)\n    except:\n        print(\"Error occurred\")\n\n\ndaterange_after = sc.parallelize(pd.date_range('2020 Mar 1','2020 May 24'))\ndaterange_before = sc.parallelize(pd.date_range('2019 Dec 1','2020 Feb 28'))\n\ndates_before = daterange_before.map(get_filename)\ndates_after = daterange_after.map(get_filename)\n\ndates_before.foreach(intofile)\ndates_after.foreach(intofile)","e925ce34":"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n##drop news with same headlines\n\ndata_before_urls = sqlContext.read.option(\"header\", \"true\").csv(dates_before.collect()).dropDuplicates(['Title'])\ndata_after_urls = sqlContext.read.option(\"header\", \"true\").csv(dates_after.collect()).dropDuplicates(['Title'])\n\ndbg(data_before_urls)\n","3db77a15":"import time\nfrom datetime import datetime\nfrom pyspark.sql.functions import unix_timestamp, to_timestamp,col, udf,month\nfrom pyspark.sql.types import DateType\nstart_time = time.monotonic() \n\ndef get_headlines(row):\n    return row['Title']\n\n       \nall_headlines_before = data_before_urls.rdd.map(get_headlines)\nall_headlines_after = data_after_urls.rdd.map(get_headlines)\n\n\n# withids = all_urls.zipWithUniqueId()","754c4e88":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n\nsia = SIA()\nresults = []\nheadlines=[all_headlines_before.collect(),all_headlines_after.collect()]\nmonths_before=data_before_urls.select(month(data_before_urls.Date)).rdd.flatMap(lambda x: x).collect()\nmonths_after=data_after_urls.select(month(data_after_urls.Date)).rdd.flatMap(lambda x: x).collect()\n##we take the compound sentiment score below -0.2 as above 0.2 as positive , rest will be classfied as neutral\ndates=[months_before,months_after]\nfor head,date in zip(headlines,dates):\n  if len(results)!=0:\n    df=pd.DataFrame(results)\n    df['label'] = 0\n    df.loc[df['compound'] > 0.2, 'label'] = 1\n    df.loc[df['compound'] < -0.2, 'label'] = -1\n    df.to_csv('before_sentiments.csv')\n    results=[]\n  for line,dat in zip(head,date) :\n    try:\n      pol_score = sia.polarity_scores(line)\n      pol_score['headline'] = line\n      pol_score['month']=dat\n      results.append(pol_score)\n    except:\n      print('Skipped')\n\ndf=pd.DataFrame(results)\ndf['label'] = 0\ndf.loc[df['compound'] > 0.2, 'label'] = 1\ndf.loc[df['compound'] < -0.2, 'label'] = -1\ndf.to_csv('after_sentiments.csv')\n    \n","5457270c":"before_sents = sqlContext.read.option(\"header\", \"true\").csv('before_sentiments.csv')\nafter_sents = sqlContext.read.option(\"header\", \"true\").csv('after_sentiments.csv')\n","e4398dfd":"### month wise mean sentiment on headlilnes before and after corona virus\nbefore_sents.groupby('month').agg({'compound': 'mean'}).show()","8f541e93":"after_sents.groupby('month').agg({'compound': 'mean'}).show()","2490786b":"before_sents.groupBy('label').count().show()","2397387c":"after_sents.groupBy('label').count().show()","a11fbbf0":"## the plot of percentage of news before corona breakdown\nfig, ax = plt.subplots(figsize=(8, 8))\nbefore_plot=before_sents.toPandas()\ncounts = before_plot.label.value_counts(normalize=True) * 100\n\ng=sns.barplot(x=counts.index, y=counts, ax=ax)\nax=g\n#annotate axis = seaborn axis\nfor p in ax.patches:\n             ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                 ha='center', va='center', fontsize=11, color='gray', xytext=(0, 20),\n                 textcoords='offset points')\nax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\nax.set_ylabel(\"Percentage\")\n\nplt.show()","8bef72f9":"## the plot of percentage of news after corona march to may\nfig, ax = plt.subplots(figsize=(8, 8))\nafter_plot=after_sents.toPandas()\ncounts = after_plot.label.value_counts(normalize=True) * 100\n\ng=sns.barplot(x=counts.index, y=counts, ax=ax)\nax=g\n#annotate axis = seaborn axis\nfor p in ax.patches:\n  ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n      ha='center', va='center', fontsize=11, color='gray', xytext=(0, 20),\n      textcoords='offset points')\n\nax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\nax.set_ylabel(\"Percentage\")\n\nplt.show()","dd9302ed":"## Comparing Sentiment levels in Climate change before and after Covid epidemic\n \nThe data is taken from gdelt project https:\/\/www.gdeltproject.org<br>\n\nThe GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, themes, sources, emotions, counts, quotes, images and events driving our global society every second of every day, creating a free open platform for computing on the entire world.\n\n<img src=\"https:\/\/www.gdeltproject.org\/images\/spinningglobe.gif\" align='middle' width=800 height=800>","d65ae3d5":"### Sentiment Analysis of Headlines on and after corona outbreak\n\nWe take headlines referring to global warming or climate change and compare it during period march to may and december to february","9342291e":"As you can see after the corona virus breakdown, the news heading from period march to may got a good increase in the positive news on climate change and global warming. Around 26.32 news given had a positive outlook and 25.94 has negative outlook"}}