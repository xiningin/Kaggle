{"cell_type":{"d67eb406":"code","ef903894":"code","308b1f21":"code","4f1aa8aa":"code","8f887a71":"code","ce03ca91":"code","1302d80c":"code","4ed8c729":"code","5424cab4":"code","fc387bfd":"code","f4e5f041":"code","35f1e96d":"code","9e946daf":"code","08f77192":"code","14435a1f":"code","6e67282f":"code","67fe3bdd":"code","30029a46":"code","4357b2bb":"code","bbd5b760":"code","71f4e917":"code","c8369d7a":"markdown","c47459c0":"markdown","e9d54e97":"markdown","d1411e98":"markdown","0bfa6947":"markdown","6fe20eb5":"markdown","124a612b":"markdown","aeff7fa1":"markdown","b3570c34":"markdown","0185cc4f":"markdown","8e2f6e3a":"markdown","a44818c9":"markdown"},"source":{"d67eb406":"!pip install rank_bm25 nltk","ef903894":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path, PurePath\nimport pandas as pd\nimport requests\nfrom requests.exceptions import HTTPError, ConnectionError\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom rank_bm25 import BM25Okapi\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt\")\nimport re","308b1f21":"from ipywidgets import interact\nimport ipywidgets as widgets\nimport pandas as pd\n\ndef set_column_width(ColumnWidth, MaxRows):\n    pd.options.display.max_colwidth = ColumnWidth\n    pd.options.display.max_rows = MaxRows\n    print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n    \ninteract(set_column_width, \n         ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n         MaxRows=widgets.IntSlider(min=50, max=500, step=100, value=100));","4f1aa8aa":"# Where are all the files located\ninput_dir = PurePath('..\/input\/CORD-19-research-challenge')\n\nlist(Path(input_dir).glob('*'))","8f887a71":"metadata_path = input_dir \/ 'metadata.csv'\nmetadata = pd.read_csv(metadata_path,\n                               dtype={'Microsoft Academic Paper ID': str,\n                                      'pubmed_id': str})\n\n# Set the abstract to the paper title if it is null\nmetadata.abstract = metadata.abstract.fillna(metadata.title)","ce03ca91":"len(metadata)","1302d80c":"# Some papers are duplicated since they were collected from separate sources. Thanks Joerg Rings\nduplicate_paper = ~(metadata.title.isnull() | metadata.abstract.isnull()) & (metadata.duplicated(subset=['title', 'abstract']))\nmetadata = metadata[~duplicate_paper].reset_index(drop=True)","4ed8c729":"len(metadata)","5424cab4":"def get(url, timeout=6):\n    try:\n        r = requests.get(url, timeout=timeout)\n        return r.text\n    except ConnectionError:\n        print(f'Cannot connect to {url}')\n        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n    except HTTPError:\n        print('Got http error', r.status, r.text)\n\n# Convert the doi to a url\ndef doi_url(d): \n    return f'http:\/\/{d}' if d.startswith('doi.org') else f'http:\/\/doi.org\/{d}'\n\n\nclass ResearchPapers:\n    \n    def __init__(self, metadata: pd.DataFrame):\n        self.metadata = metadata\n        \n    def __getitem__(self, item):\n        return Paper(self.metadata.iloc[item])\n    \n    def __len__(self):\n        return len(self.metadata)\n    \n    def head(self, n):\n        return ResearchPapers(self.metadata.head(n).copy().reset_index(drop=True))\n    \n    def tail(self, n):\n        return ResearchPapers(self.metadata.tail(n).copy().reset_index(drop=True))\n    \n    def abstracts(self):\n        return self.metadata.abstract.dropna()\n    \n    def titles(self):\n        return self.metadata.title.dropna()\n        \n    def _repr_html_(self):\n        return self.metadata._repr_html_()\n    \nclass Paper:\n    \n    '''\n    A single research paper\n    '''\n    def __init__(self, item):\n        self.paper = item.to_frame().fillna('')\n        self.paper.columns = ['Value']\n    \n    def doi(self):\n        return self.paper.loc['doi'].values[0]\n    \n    def html(self):\n        '''\n        Load the paper from doi.org and display as HTML. Requires internet to be ON\n        '''\n        if self.doi():\n            url = doi_url(self.doi()) \n            text = get(url)\n            return widgets.HTML(text)\n    \n    def text(self):\n        '''\n        Load the paper from doi.org and display as text. Requires Internet to be ON\n        '''\n        text = get(self.doi())\n        return text\n    \n    def abstract(self):\n        return self.paper.loc['abstract'].values[0]\n    \n    def title(self):\n        return self.paper.loc['title'].values[0]\n    \n    def authors(self, split=False):\n        '''\n        Get a list of authors\n        '''\n        authors = self.paper.loc['authors'].values[0]\n        if not authors:\n            return []\n        if not split:\n            return authors\n        if authors.startswith('['):\n            authors = authors.lstrip('[').rstrip(']')\n            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n        \n        # Todo: Handle cases where author names are separated by \",\"\n        return [a.strip() for a in authors.split(';')]\n        \n    def _repr_html_(self):\n        return self.paper._repr_html_()\n    \n\npapers = ResearchPapers(metadata)","fc387bfd":"from rank_bm25 import BM25Okapi","f4e5f041":"english_stopwords = list(set(stopwords.words('english')))\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\n\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    return tokens\n\nclass SearchResults:\n    \n    def __init__(self, \n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    \n    def __len__(self):\n        return len(self.results)\n        \n    def _repr_html_(self):\n        return self.results._repr_html_()\n\nSEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal']\n    \nclass RankBM25Index:\n    \n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        self.columns = columns\n        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.bm25 = BM25Okapi(self.index.terms.tolist())\n        \n    def search(self, search_string, n=4):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        ind = np.argsort(doc_scores)[::-1][:n]\n        results = self.corpus.iloc[ind][self.columns]\n        results['Score'] = doc_scores[ind]\n        results = results[results.Score > 0]\n        return SearchResults(results.reset_index(), self.columns + ['Score'])\n    \nbm25_index = RankBM25Index(metadata)","35f1e96d":"results = bm25_index.search('cruise ship')\nresults","9e946daf":"bm25_index.search('sars-cov-2')","08f77192":"tasks = [('What is known about transmission, incubation, and environmental stability?', \n        'transmission incubation environment coronavirus'),\n        ('What do we know about COVID-19 risk factors?', 'risk factors'),\n        ('What do we know about virus genetics, origin, and evolution?', 'genetics origin evolution'),\n        ('What has been published about ethical and social science considerations','ethics ethical social'),\n        ('What do we know about diagnostics and surveillance?','diagnose diagnostic surveillance'),\n        ('What has been published about medical care?', 'medical care'),\n        ('What do we know about vaccines and therapeutics?', 'vaccines vaccine vaccinate therapeutic therapeutics')] \ntasks = pd.DataFrame(tasks, columns=['Task', 'Keywords'])","14435a1f":"tasks","6e67282f":"def show_task(Task):\n    print(Task)\n    keywords = tasks[tasks.Task == Task].Keywords.values[0]\n    search_results = bm25_index.search(keywords, n=10)\n    return search_results\n    \nresults = interact(show_task, Task = tasks.Task.tolist());","67fe3bdd":"from transformers import pipeline\n\nnlp = pipeline(\"question-answering\")\n","30029a46":"questions =  [('What is the mean incubation time for COVID-19?', \n        'mean incubation time covid-19'),\n        ('How is COVID-19 transmitted?', 'transmission covid-19'),\n        ('How stable is COVID-19 in different environments?', 'environment covid-19'),\n        ] \nquestions = pd.DataFrame(questions, columns=['Question', 'Keywords'])","4357b2bb":"def get_answer(answers):\n    answers_sorted = sorted(answers, reverse= True,key = lambda x: (x['score']))\n    #print(\"the answer is \"+answers_sorted[0]['answer'])\n    #print(answers_sorted[0]['abstract'])\n    \n    return pd.DataFrame(answers_sorted)\n\ndef show_answer(Question):\n    print(Question)\n    keywords = questions[questions.Question == Question].Keywords.values[0]\n    search_results = bm25_index.search(keywords, n=10)\n    answers = []\n    for abstract in search_results.results['abstract']:\n        #print(abstract)\n        answer = nlp(question=\"What is the mean incubation time for COVID-19?\", context=abstract)\n        answer['abstract'] = abstract\n        #print(answer)\n        answers.append(answer)\n    return get_answer(answers)\n    \nresults = interact(show_answer, Question = questions.Question.tolist());","bbd5b760":"import torch\nbart = torch.hub.load('pytorch\/fairseq', 'bart.large.cnn')\n#bart.eval()","71f4e917":"summarizer = pipeline(\"summarization\", model=bart)\nsummarizer(\"Sam Shleifer writes the best docstring examples in the whole world.\", min_length=5, max_length=20)","c8369d7a":"# Question Answering with BERT\n\nThis notebook is an off-the-shelf approach to question answering based on the [Hugging Face QA pipeline](https:\/\/huggingface.co\/transformers\/usage.html#extractive-question-answering). The notebook utilizes the exellent notebook https:\/\/www.kaggle.com\/dgunning\/browsing-research-papers-with-a-bm25-search-engine and builds on top of the top 10 answers from this search engine.\n\nThe goal of this notebook is to try out ideas based on extractive questions answering. The BERT QA pipeline works best with specific questions such as \"What is a good example of a question answering dataset?\". The 10 research questions posed by this challenges are more high-level and would either require to break the questions into sub-questions or provide answers based on summarization techniques that aggregate various answer documents.\n\nFor the purpose of testing how well extractive question answering may work here, I split the first task in three sub-questions. The results are mixed with some promising results for questions such as *What is the mean incubation time for COVID-19?*.\n\n\n## Structure of the notebook\nThis notebook follows mostly the structure of the BM25 notebook by https:\/\/www.kaggle.com\/dgunning and adds a QA BERT pipeline:\n\n5*. Run Question Answering BERT pipeline to extract best answer snippets\n","c47459c0":"## Research Papers for each task\nHere we add a dropdown that allows for selection of tasks and show the search results.","e9d54e97":"### Drop duplicate papers","d1411e98":"### Search for sars-cov-2","0bfa6947":"# 1. Load the All Sources Metadata file\nHere we load the csv file containing the metadata for the SARS-COV-2 papers.","6fe20eb5":"# 2. Create Data Classes for the Research Dataset and Papers\nThese classes make it easier to navigate through the datasources. There is a class called **ResearchPapers** that wraps the entire dataset an provide useful functions to navigate through it, and **Paper**, that make it easier to view each paper.","124a612b":"# 5*. Asking Questions with BERT","aeff7fa1":"# 3. Creating a search index\nWe will create a simple search index that will just match search tokens in a document. First we tokenize the abstract and store it in a dataframe. Then we just match search terms against it.","b3570c34":"### Text Preprocessing\nTo prepare the text for the search index we perform the following steps\n1. Remove punctuations and special characters\n2. Convert to lowercase\n3. Tokenize into individual tokens (words mostly)\n4. Remove stopwords like (and, to))\n\nYou can tweak the code below to improve the search results","0185cc4f":"## Using a RankBM25 Search Index\n\nRankBM25 is a python library that implements algorithms for a simple search index.\nhttps:\/\/pypi.org\/project\/rank-bm25\/","8e2f6e3a":"Adjust Notebook display settings\nTo change the width of the pandas dataframe columns - useful when we a displaying text in a dataframe, we can adjust the pandas display settings.","a44818c9":"# 4. Looking at the Covid Research Tasks\nThis dataset has a number of tasks. We will try to organize the papers according to the tasks\n\n1. **What is known about transmission, incubation, and environmental stability?**\n2. **What do we know about COVID-19 risk factors?**\n3. **What do we know about virus genetics, origin, and evolution?**\n4. **What has been published about ethical and social science considerations?**\n5. **What do we know about diagnostics and surveillance?**\n6. **What has been published about medical care?**\n7. **What do we know about non-pharmaceutical interventions?**\n8. **What has been published about information sharing and inter-sectoral collaboration?**\n9. **What do we know about vaccines and therapeutics?**"}}