{"cell_type":{"7a7b88ab":"code","ad7b9d53":"code","042957ce":"code","a434ee7c":"code","24ef3d18":"code","0e1d7745":"code","4ae46a20":"code","07d060b2":"code","62c311b4":"code","902d7860":"code","d9c4c409":"code","117c460b":"code","23315709":"code","655cf031":"code","5196c6a9":"code","63588a6b":"code","9111cdb0":"code","2609c2c3":"code","7e556bee":"code","b9ff794c":"code","d25d61ce":"code","9ffe7604":"code","262d9718":"code","4f2b0df9":"code","31309504":"code","966faf4e":"code","32a4e0b3":"code","c4c4fcb4":"code","03d5ce21":"code","85add2d6":"code","34c2015b":"code","738473f2":"code","41811eea":"code","3c1300bc":"code","2d28465d":"code","95445ddb":"code","741d1c39":"code","a50a6392":"markdown","a398bd91":"markdown","72fd259a":"markdown","612349a2":"markdown","8acdeec6":"markdown","868e38c6":"markdown","8908104d":"markdown","c45a8072":"markdown","0c071172":"markdown","39940e8c":"markdown","34b9f137":"markdown","60dff4ff":"markdown","75cf4ca0":"markdown","04c42b5d":"markdown","f1c91d9b":"markdown"},"source":{"7a7b88ab":"# linear algebra\nimport numpy as numpy \n\n# data processing\nimport pandas as pandas \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn.ensemble import RandomForestClassifier\n","ad7b9d53":"test_data = pandas.read_csv(\"..\/input\/titani1\/test.csv\")\ntrain_data = pandas.read_csv(\"..\/input\/titani1\/train.csv\")","042957ce":"train_data.info()","a434ee7c":"train_data.describe()","24ef3d18":"train_data.head(15)","0e1d7745":"MissingAll = train_data.isnull().sum().sort_values(ascending=False)\npercentage1 = train_data.isnull().sum()\/train_data.isnull().count()*100\npercentage2 = (round(percentage1, 1)).sort_values(ascending=False)\nmissingValues = pandas.concat([ percentage2,MissingAll], axis=1, keys=['Percentage','Total_Value'])\nmissingValues.head(10)","4ae46a20":"dataset = [train_data, test_data]\nfor data in dataset:\n    data['relatives'] = data['SibSp'] + data['Parch']\n    data.loc[data['relatives'] > 0, 'not_alone'] = 0\n    data.loc[data['relatives'] == 0, 'not_alone'] = 1\n    data['not_alone'] = data['not_alone'].astype(int)\ntrain_data['not_alone'].value_counts()\n","07d060b2":"sns.barplot(x='Pclass', y='Survived', data=train_data)","62c311b4":"face_grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', height=2.2, aspect=1.6)\nface_grid.map(plt.hist, 'Age', alpha=.5, bins=20)\nface_grid.add_legend();","902d7860":"Grit = sns.FacetGrid(train_data, row='Embarked', height=4.5, aspect=1.6)\nGrit.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nGrit.add_legend()","d9c4c409":"\nsurvival = 'survived'\nnon_survival = 'not survived'\nfigure, axis = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nfemale = train_data[train_data['Sex']=='female']\nmale = train_data[train_data['Sex']=='male']\nax = sns.distplot(female[female['Survived']==1].Age.dropna(), bins=18, label = survival, ax = axis[0], kde =False)\nax = sns.distplot(female[female['Survived']==0].Age.dropna(), bins=40, label = non_survival, ax = axis[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(male[male['Survived']==1].Age.dropna(), bins=18, label = survival, ax = axis[1], kde = False)\nax = sns.distplot(male[male['Survived']==0].Age.dropna(), bins=40, label = non_survival, ax = axis[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","117c460b":"train_data = train_data.drop(['PassengerId'], axis=1)","23315709":"import re\nd_k = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndataset = [train_data, test_data]\n\nfor data in dataset:\n    data['Cabin'] = data['Cabin'].fillna(\"U0\")\n    data['Deck'] = data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    data['Deck'] = data['Deck'].map(d_k)\n    data['Deck'] = data['Deck'].fillna(0)\n    data['Deck'] = data['Deck'].astype(int)\n\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)","655cf031":"dataset = [train_data, test_data]\n\nfor data in dataset:\n    m = train_data[\"Age\"].mean()\n    s = test_data[\"Age\"].std()\n    null =data[\"Age\"].isnull().sum()\n    # Calculate Random values between mean and standard deviation\n    r_age = numpy.random.randint(m - s, m + s, size = null)\n    \n    slice_age = data[\"Age\"].copy()\n    slice_age[numpy.isnan(slice_age)] = r_age\n    data[\"Age\"] = slice_age\n    data[\"Age\"] = train_data[\"Age\"].astype(int)\ntrain_data[\"Age\"].isnull().sum()","5196c6a9":"train_data.info()","63588a6b":"dataset = [train_data, test_data]\n\nfor data in dataset:\n    data['Fare'] = data['Fare'].fillna(0)\n    data['Fare'] = data['Fare'].astype(int)","9111cdb0":"train_data['Embarked'].describe()","2609c2c3":"c_v = 'S'\ndataset = [train_data, test_data]\n\nfor data in dataset:\n    data['Embarked'] = data['Embarked'].fillna(c_v)","7e556bee":"\ndataset = [train_data, test_data]\ncategory = {\"male\": 0, \"female\": 1}\nfor data in dataset:\n    data['Sex'] = data['Sex'].map(category)","b9ff794c":"dataset = [train_data, test_data]\ncommon_title = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor data in dataset:\n    \n    data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n\n    data['Title'] = data['Title'].map(common_title)\n    \n    data['Title'] = data['Title'].fillna(0)\ntrain_data = train_data.drop(['Name'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)","d25d61ce":"\ndataset = [train_data, test_data]\nport_value= {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor data in dataset:\n    data['Embarked'] = data['Embarked'].map(port_value)","9ffe7604":"train_data['Ticket'].describe()","262d9718":"train_data = train_data.drop(['Ticket'], axis=1)\ntest_data = test_data.drop(['Ticket'], axis=1)","4f2b0df9":"train_data.head(10)","31309504":"dataset = [train_data, test_data]\n\nfor data in dataset:\n    data.loc[ data['Fare'] <= 7.91, 'Fare'] = 0\n    data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\n    data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare']   = 2\n    data.loc[(data['Fare'] > 31) & (data['Fare'] <= 99), 'Fare']   = 3\n    data.loc[(data['Fare'] > 99) & (data['Fare'] <= 250), 'Fare']   = 4\n    data.loc[ data['Fare'] > 250, 'Fare'] = 5\n    data['Fare'] = data['Fare'].astype(int)","966faf4e":"dataset = [train_data, test_data]\nfor data in dataset:\n    data['Age'] = data['Age'].astype(int)\n    data.loc[ data['Age'] <= 11, 'Age'] = 0\n    data.loc[(data['Age'] > 11) & (data['Age'] <= 18), 'Age'] = 1\n    data.loc[(data['Age'] > 18) & (data['Age'] <= 22), 'Age'] = 2\n    data.loc[(data['Age'] > 22) & (data['Age'] <= 27), 'Age'] = 3\n    data.loc[(data['Age'] > 27) & (data['Age'] <= 33), 'Age'] = 4\n    data.loc[(data['Age'] > 33) & (data['Age'] <= 40), 'Age'] = 5\n    data.loc[(data['Age'] > 40) & (data['Age'] <= 66), 'Age'] = 6\n    data.loc[ data['Age'] > 66, 'Age'] = 6","32a4e0b3":"dataset = [train_data, test_data]\nfor data in dataset:\n    data['Fare_Per_Person'] = data['Fare']\/(data['relatives']+1)\n    data['Fare_Per_Person'] = data['Fare_Per_Person'].astype(int)\n\nfor data in dataset:\n    data['Age_Class']= data['Age']* data['Pclass']\ntrain_data.head(10)","c4c4fcb4":"test_set  = test_data.drop(\"PassengerId\", axis=1).copy()\ntrain_set1=train_data.drop(\"Survived\", axis=1)\ntrain_set2=train_data[\"Survived\"]","03d5ce21":"random_forest_model1 = RandomForestClassifier(n_estimators=100)\nrandom_forest_model1.fit(train_set1, train_set2)\n\nprediction_t = random_forest_model1.predict(test_set)\n\nrandom_forest_model1.score(train_set1, train_set2)\naccuracy_random_forest_model = round(random_forest_model1.score(train_set1, train_set2) * 100, 2)\nprint(accuracy_random_forest_model)","85add2d6":"feature_importances = pandas.DataFrame({'feature':train_set1.columns,'importance':numpy.round(random_forest_model1.feature_importances_,3)})\nfeature_importances = feature_importances.sort_values('importance',ascending=False).set_index('feature')\nfeature_importances.head(15)","34c2015b":"feature_importances.plot.bar()","738473f2":"train_data  = train_data.drop(\"not_alone\", axis=1)\ntest_data  = test_data.drop(\"not_alone\", axis=1)\n\ntrain_data  = train_data.drop(\"Parch\", axis=1)\ntest_data  = test_data.drop(\"Parch\", axis=1)","41811eea":"random_forest_model2 = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest_model2.fit(train_set1, train_set2)\nprediction_t2 = random_forest_model2.predict(test_set)\n\nrandom_forest_model2.score(train_set1, train_set2)\n\naccuracy_random_forest = round(random_forest_model2.score(train_set1, train_set2) * 100, 2)\nprint(round(accuracy_random_forest,2,), \"%\")","3c1300bc":"parameter_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrandomforest = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nmodel = GridSearchCV(estimator=randomforest, param_grid=parameter_grid, n_jobs=-1)\nmodel.fit(train_set1, train_set2)\n","2d28465d":"model.best_params_","95445ddb":"random_forest_model = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 5, \n                                       min_samples_split = 2,   \n                                       n_estimators=1500, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest_model.fit(train_set1, train_set2)\nprediction = random_forest_model.predict(test_set)\n\nrandom_forest_model.score(train_set1, train_set2)\n\nprint( round(random_forest_model.oob_score_, 4)*100, \"%\")","741d1c39":"testingdata=pandas.read_csv(\"..\/input\/titani1\/test.csv\")\nprediction = random_forest_model.predict(test_set)\nprediction = pandas.DataFrame(prediction, columns=['Survived'])\nprediction = pandas.concat((testingdata.iloc[:, 0], prediction), axis = 1)\n\nprediction.to_csv('Submission.csv',sep=\",\",index=False)","a50a6392":"# Importing Libraries\n","a398bd91":"**01. SibSp and Parch**","72fd259a":"**Training Random Forest Again**","612349a2":"# Loading Data From files","8acdeec6":"# Creating Categories","868e38c6":"**03. Embarked, Pclass and Sex**","8908104d":"# Creating New Features","c45a8072":"# Building Model","0c071172":"**02. Pclass**","39940e8c":"# Exploring Or Analyzing Data From Dataset","34b9f137":"**Feature Importance**","60dff4ff":"**Hyper Parameter Tunning**","75cf4ca0":"# Analyzing Features That Contribute For High Survival Rate","04c42b5d":"# Data Preprocessing","f1c91d9b":"**05. Age & Sex**"}}