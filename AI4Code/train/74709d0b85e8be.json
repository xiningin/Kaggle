{"cell_type":{"95ee2d2b":"code","3014e21c":"code","1596a7d2":"code","2e4b59aa":"code","300ad2bf":"code","6700cd99":"code","56b22eea":"code","67080483":"code","5de41b5e":"code","edb5a1cc":"code","12fa5a49":"code","dc61ae11":"code","9d7e280b":"code","3395972f":"code","22f21b73":"code","e27ec2ec":"code","a2bcda00":"code","6c45f68c":"markdown","2fd0b856":"markdown","394d824f":"markdown","2f8155a6":"markdown","475ec637":"markdown","71e57458":"markdown","42579dca":"markdown","01be9bb9":"markdown","634fcbe0":"markdown","27e4db69":"markdown","de84f944":"markdown","118f53ff":"markdown","df6013c0":"markdown","b233ffdf":"markdown","0160eb7b":"markdown","9caee65d":"markdown","8666b7e8":"markdown","6b5f0c4b":"markdown","0e61c670":"markdown"},"source":{"95ee2d2b":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom scipy.stats import norm, skew\n\nfrom tqdm import tqdm_notebook as tqdm\nfrom copy import copy\nfrom multiprocessing import Pool\n\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)","3014e21c":"use_experimental = False\n\ntrain_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\n\nindices_fake = np.load('..\/input\/list-of-fake-samples-and-public-private-lb-split\/synthetic_samples_indexes.npy')\nindices_pub = np.load('..\/input\/list-of-fake-samples-and-public-private-lb-split\/public_LB.npy')\nindices_pri = np.load('..\/input\/list-of-fake-samples-and-public-private-lb-split\/private_LB.npy')\nindices_real = np.concatenate([indices_pub, indices_pri])\n\nfeatures = [c for c in train_df.columns if c not in ['ID_code', 'target']]\ntarget_train = train_df['target']\nX_train = train_df\nX_test = test_df.loc[indices_real,:]\nX_test['target'] = np.zeros(X_test.shape[0])\nX_fake = test_df.loc[indices_fake,:]\nX_fake['target'] = np.zeros(X_test.shape[0])\ntrain_length = X_train.shape[0]\ntarget_test = X_test['target']\ntarget_fake = X_fake['target']\n\nif use_experimental:\n    np.random.seed(42)    \n    indices = np.arange(train_length)\n    train_length = 150000\n    np.random.shuffle(indices)\n    indices_train = indices[:train_length]\n    indices_test = indices[train_length:]\n    # Swapped order to not overwrite X_train to soon\n    X_test = X_train.iloc[indices_test,:]\n    X_fake = X_train.iloc[indices_test,:]\n    target_fake = X_fake['target']\n    X_train = X_train.iloc[indices_train,:]\n    target_train = X_train['target']\n    target_test = X_test['target']\n\nX_all = pd.concat([X_train, X_test])\nprint(X_all.shape)","1596a7d2":"import scipy.ndimage\n\nsigma_fac = 0.001\nsigma_base = 4\n\neps = 0.00000001\n\ndef get_count(X_all, X_fake):\n    features_count = np.zeros((X_all.shape[0], len(features)))\n    features_density = np.zeros((X_all.shape[0], len(features)))\n    features_deviation = np.zeros((X_all.shape[0], len(features)))\n\n    features_count_fake = np.zeros((X_fake.shape[0], len(features)))\n    features_density_fake = np.zeros((X_fake.shape[0], len(features)))\n    features_deviation_fake = np.zeros((X_fake.shape[0], len(features)))\n    \n    sigmas = []\n\n    for i,var in enumerate(tqdm(features)):\n        X_all_var_int = (X_all[var].values * 10000).round().astype(int)\n        X_fake_var_int = (X_fake[var].values * 10000).round().astype(int)\n        lo = X_all_var_int.min()\n        X_all_var_int -= lo\n        X_fake_var_int -= lo\n        hi = X_all_var_int.max()+1\n        counts_all = np.bincount(X_all_var_int, minlength=hi).astype(float)\n        zeros = (counts_all == 0).astype(int)\n        before_zeros = np.concatenate([zeros[1:],[0]])\n        indices_all = np.arange(counts_all.shape[0])\n        # Geometric mean of twice sigma_base and a sigma_scaled which is scaled to the length of array \n        sigma_scaled = counts_all.shape[0]*sigma_fac\n        sigma = np.power(sigma_base * sigma_base * sigma_scaled, 1\/3)\n        sigmas.append(sigma)\n        counts_all_smooth = scipy.ndimage.filters.gaussian_filter1d(counts_all, sigma)\n        deviation = counts_all \/ (counts_all_smooth+eps)\n        indices = X_all_var_int\n        features_count[:,i] = counts_all[indices]\n        features_density[:,i] = counts_all_smooth[indices]\n        features_deviation[:,i] = deviation[indices]\n        indices_fake = X_fake_var_int\n        features_count_fake[:,i] = counts_all[indices_fake]\n        features_density_fake[:,i] = counts_all_smooth[indices_fake]\n        features_deviation_fake[:,i] = deviation[indices_fake]\n        \n    features_count_names = [var+'_count' for var in features]\n    features_density_names = [var+'_density' for var in features]\n    features_deviation_names = [var+'_deviation' for var in features]\n\n    X_all_count = pd.DataFrame(columns=features_count_names, data = features_count)\n    X_all_count.index = X_all.index\n    X_all_density = pd.DataFrame(columns=features_density_names, data = features_density)\n    X_all_density.index = X_all.index\n    X_all_deviation = pd.DataFrame(columns=features_deviation_names, data = features_deviation)\n    X_all_deviation.index = X_all.index\n    X_all = pd.concat([X_all,X_all_count, X_all_density, X_all_deviation], axis=1)\n    \n    X_fake_count = pd.DataFrame(columns=features_count_names, data = features_count_fake)\n    X_fake_count.index = X_fake.index\n    X_fake_density = pd.DataFrame(columns=features_density_names, data = features_density_fake)\n    X_fake_density.index = X_fake.index\n    X_fake_deviation = pd.DataFrame(columns=features_deviation_names, data = features_deviation_fake)\n    X_fake_deviation.index = X_fake.index\n    X_fake = pd.concat([X_fake,X_fake_count, X_fake_density, X_fake_deviation], axis=1)    \n\n    features_count = features_count_names\n    features_density = features_density_names\n    features_deviation = features_deviation_names\n    return X_all, features_count, features_density, features_deviation, X_fake\n\nX_all, features_count, features_density, features_deviation, X_fake = get_count(X_all, X_fake)\nprint(X_all.shape)","2e4b59aa":"# # Also try to encode counts themselves\n\n# weighting = 500\n# n_splits = 2\n\n# features_to_encode = features_count\n\n# def get_encoding(X_all):\n#     arr_all_int = (X_all[features_to_encode].values * 10000).round().astype(int)\n#     arr_target_train = target_train.values \n#     arr_target_test = target_test.values\n    \n#     preds_oof = np.zeros((train_length, len(features)))\n#     preds_test = np.zeros((arr_all_int.shape[0] - train_length, len(features)))\n#     preds_train = np.zeros((train_length, len(features)))\n    \n#     for v ,var in enumerate(tqdm(features_to_encode)):\n#         lo = arr_all_int[:,v].min()\n#         arr_all_var = arr_all_int[:,v] - lo\n#         hi = arr_all_var.max() + 1\n#         arr_train_var = arr_all_var[:train_length]\n#         arr_test_var = arr_all_var[train_length:]\n#         folds = StratifiedKFold(n_splits=n_splits, shuffle=True)\n#         for train_idx, val_idx in folds.split(arr_train_var, arr_target_train):\n#             X_tr = arr_train_var[train_idx]\n#             y_tr = arr_target_train[train_idx]\n#             X_val = arr_train_var[val_idx]\n#             y_val = arr_target_train[val_idx]\n#             X_ts = arr_test_var\n#             arr1 = X_tr[y_tr == 1]        \n#             mean = arr1.shape[0] \/ X_tr.shape[0]\n#             hits = np.bincount(arr1, minlength=hi).astype(float)\n#             base = np.bincount(X_tr, minlength=hi).astype(float)\n#             lamb = base \/ (base + weighting)\n#             expected_target = (hits \/ (base+0.00000001)) * lamb + (1-lamb) * mean              \n\n#             prediction_oof = expected_target[X_val]\n#             prediction_test = expected_target[X_ts]\n#             prediction_train = expected_target[X_tr]\n#             preds_oof[val_idx,v] += prediction_oof \n#             preds_test[:,v] += prediction_test \n#             preds_train[train_idx,v] += prediction_train \n#         score_running_oof = roc_auc_score(arr_target_train, preds_oof.mean(axis=1))\n#         score_running_test = roc_auc_score(arr_target_test, preds_test.mean(axis=1))\n#         score_running_train = roc_auc_score(arr_target_train, preds_train.mean(axis=1))\n        \n#     preds_all = np.concatenate([preds_oof, preds_test], axis=0)\n#     feature_names = [var+'_encoding' for var in features_to_encode]\n#     X_all_encoding = pd.DataFrame(columns=feature_names, data = preds_all)\n#     X_all_encoding.index = X_all.index\n#     X_all = pd.concat([X_all,X_all_encoding], axis=1)\n#     return X_all, feature_names\n\n# X_all, features_encoding = get_encoding(X_all)\n\n# print(X_all.shape)","300ad2bf":"# def compress(arr, indices, thresh = 100000):\n#     old_length = arr.shape[0] \n#     fac = old_length \/\/ thresh\n#     if fac < 1:\n#         return arr, indices\n#     new_length = int(np.ceil(old_length \/ fac))+2\n#     new_arr = np.zeros(new_length * fac)\n#     new_arr[fac:old_length+fac] = arr\n#     new_arr = new_arr.reshape(new_length, fac).sum(axis=1)\n#     index_shift = indices[0]-0.5*(fac-1)-1\n#     new_indices = np.arange(new_length) * fac + index_shift\n#     return new_arr, new_indices    \n\n# def transform_grouper(counts_orig, indices_orig, min_elems=10):\n#     first_ind, first_count, last_ind, last_count = indices_orig[0], counts_orig[0], indices_orig[-1], counts_orig[-1]\n#     indices = indices_orig[1:-1]\n#     counts = counts_orig[1:-1]\n#     new_indices = [np.array([first_ind])]\n#     cur_indices = []\n#     cur_counts = 0\n#     for i, count in enumerate(counts):\n#         cur_indices.append(indices[i])\n#         cur_counts += count\n#         if cur_counts >= min_elems:\n#             new_indices.append(np.array(cur_indices))\n#             cur_counts = 0\n#             cur_indices = []\n#     if cur_indices:\n#         new_indices.append(np.array(cur_indices))\n#     new_indices.append(np.array([last_ind]))\n#     return new_indices\n\n# def transform_space(counts, indices_grouped):\n#     new_counts = []\n#     new_indices = []\n#     for arr_index in indices_grouped:\n#         count = counts[arr_index]\n#         count_sum = np.sum(count)\n#         if count_sum > 0:\n#             new_indices.append(np.sum(arr_index*count)\/count_sum)\n#         else:\n#             new_indices.append(np.mean(arr_index))\n#         new_counts.append(count_sum)\n#     return np.array(new_counts), np.array(new_indices)\n\n# def get_hist(arr, indices_grouped):\n#     hi = indices_grouped[-1][0] + 1\n#     counts = np.bincount(arr, minlength=hi)\n#     indices = np.arange(counts.shape[0])\n#     counts, indices = transform_space(counts, indices_grouped)\n#     return counts \/ counts.sum(), indices\n\n# def get_density_func(kde, indices, sigma=0.0001, resolution=2000):\n#     kde_smooth = scipy.ndimage.filters.gaussian_filter1d(kde,sigma*len(indices))\n#     return scipy.interpolate.interp1d(indices, kde_smooth, kind='linear')\n\n# def get_p_x_t(x, t, density_funcs):\n#     return density_funcs[t](x) \n\n# def get_p_1_x(x, density_funcs, prior=0.1, eps=0.00000000000000001):\n#     p_x_0 = get_p_x_t(x,0,density_funcs)\n#     p_x_1 = get_p_x_t(x,1,density_funcs)\n#     p_x = (p_x_1*prior + p_x_0*(1-prior)+eps)\n#     return p_x_1*prior \/ p_x, p_x\n\n# import scipy.ndimage\n# import scipy\n\n# n_splits = 2\n\n# def get_NB_predictor(X_all):   \n#     arr_all_int = (X_all[features].values * 10000).round().astype(int)\n#     arr_target_train = target_train.values \n#     arr_target_test = target_test.values\n#     preds_oof = np.zeros((train_length, len(features)))\n#     preds_test = np.zeros((arr_all_int.shape[0] - train_length, len(features)))\n#     preds_train = np.zeros((train_length, len(features)))\n#     for v, var in enumerate(features):\n#         lo = arr_all_int[:,v].min()\n#         arr_all_var = arr_all_int[:,v] - lo\n#         hi = arr_all_var.max() + 1\n#         arr_train_var = arr_all_var[:train_length]\n#         arr_test_var = arr_all_var[train_length:]\n#         counts_all = np.bincount(arr_all_var, minlength=hi)\n#         indices_all = np.arange(counts_all.shape[0])\n#         min_elems = [20]\n#         sigmas = [0.05,0.02,0.005,0.002]\n#         preds_oof_temp = np.zeros((preds_oof.shape[0], len(sigmas), len(min_elems)))\n#         preds_test_temp = np.zeros((preds_test.shape[0], len(sigmas), len(min_elems)))\n#         preds_train_temp = np.zeros((preds_train.shape[0], len(sigmas), len(min_elems)))\n#         for j,min_elem in enumerate(min_elems):\n#             indices_grouped = transform_grouper(counts_all, indices_all, min_elems=min_elem)            \n#             kfold = StratifiedKFold(n_splits=n_splits, shuffle=False, random_state = np.random.randint(10000))\n#             for train_idx, val_idx in kfold.split(arr_train_var, arr_target_train):\n#                 X_tr = arr_train_var[train_idx]\n#                 y_tr = arr_target_train[train_idx]\n#                 X_val = arr_train_var[val_idx]\n#                 y_val = arr_target_train[val_idx]\n#                 X_ts = arr_test_var\n#                 arr0 = X_tr[y_tr == 0]\n#                 arr1 = X_tr[y_tr == 1]\n#                 prior = arr1.shape[0] \/ (arr1.shape[0] + arr0.shape[0]) \n#                 kde0, indices0 = get_hist(arr0, indices_grouped)\n#                 kde1, indices1 = get_hist(arr1, indices_grouped)\n#                 for i,sigma in enumerate(sigmas): \n#                     density_func0 = get_density_func(kde0, indices0, sigma=sigma)\n#                     density_func1 = get_density_func(kde1, indices1, sigma=sigma)\n#                     prediction_oof, confidence = get_p_1_x(X_val, [density_func0, density_func1], prior=prior)\n#                     prediction_test, confidence = get_p_1_x(X_ts, [density_func0, density_func1], prior=prior)\n#                     prediction_train, confidence = get_p_1_x(X_tr, [density_func0, density_func1], prior=prior)\n#                     preds_oof_temp[val_idx,i,j] += prediction_oof \n#                     preds_test_temp[:,i,j] += prediction_test \n#                     preds_train_temp[train_idx,i,j] += prediction_train \n#         scores = np.zeros((len(sigmas), len(min_elems)))\n#         for i in range(len(sigmas)):\n#             for j in range(len(min_elems)):\n#                 scores[i,j] = log_loss(arr_target_train, preds_oof_temp[:,i,j])\n#         sorted_indices = np.dstack(np.unravel_index(np.argsort(scores.ravel()), (len(sigmas), len(min_elems))))[0]\n#         preds_oof[:,v] = preds_oof_temp[:,sorted_indices[0,0], sorted_indices[0,1]]\n#         preds_test[:,v] = preds_test_temp[:,sorted_indices[0,0], sorted_indices[0,1]]\n#         preds_train[:,v] = preds_train_temp[:,sorted_indices[0,0], sorted_indices[0,1]]\n#         score_running_oof = roc_auc_score(arr_target_train, preds_oof.mean(axis=1))\n#         score_running_test = roc_auc_score(arr_target_test, preds_test.mean(axis=1))\n#         score_running_train = roc_auc_score(arr_target_train, preds_train.mean(axis=1))\n#         print(var, sorted_indices[0], score_running_oof, score_running_test, score_running_train)\n#     preds_all = np.concatenate([preds_oof, preds_test], axis=0)\n#     feature_names = [var+'_pred' for var in features]\n#     X_all_pred = pd.DataFrame(columns=feature_names, data = preds_all)\n#     X_all_pred.index = X_all.index\n#     X_all = pd.concat([X_all,X_all_pred], axis=1)\n#     return X_all, feature_names\n        \n# X_all, features_pred = get_NB_predictor(X_all)\n\n# print(X_all.shape)","6700cd99":"features_to_scale = [features, features_count]\n\nfrom sklearn.preprocessing import StandardScaler\n\ndef get_standardized(X_all, X_fake):\n    scaler = StandardScaler()\n    features_to_scale_flatten = [var for sublist in features_to_scale for var in sublist]\n    scaler.fit(X_all[features_to_scale_flatten])\n    features_scaled = scaler.transform(X_all[features_to_scale_flatten])\n    features_scaled_fake = scaler.transform(X_fake[features_to_scale_flatten])\n    X_all[features_to_scale_flatten] = features_scaled\n    X_fake[features_to_scale_flatten] = features_scaled_fake\n    return X_all, X_fake\n\nX_all, X_fake = get_standardized(X_all, X_fake)\n\nprint(X_all.shape)","56b22eea":"# features_to_rot = [features, features_count]\n# angles = [np.pi\/4]\n\n# def get_rotated(X_all):\n#     list_features_rot = []\n#     list_X_all_rot = [] \n#     for j ,angle in enumerate(angles):\n#         list_rot_0 = []\n#         list_rot_1 = []\n#         feature_names_0 = []\n#         feature_names_1 = []\n#         c,s = np.cos(angle), np.sin(angle)\n#         rot_mat = np.array([[c,-s],[s,c]])\n#         for i in tqdm(range(len(features))):\n#             vars_to_rot = [feat[i] for feat in features_to_rot]\n#             arr = X_all[vars_to_rot].values\n#             arr_rot = np.dot(arr, rot_mat)\n#             list_rot_0.append(arr_rot[:,0])\n#             list_rot_1.append(arr_rot[:,1])\n#             feature_names_0.append('var_%d_angle_%d_rot_0' %(i,j))        \n#             feature_names_1.append('var_%d_angle_%d_rot_1' %(i,j))        \n#         arr_rot_0 = np.stack(list_rot_0).transpose()\n#         arr_rot_1 = np.stack(list_rot_1).transpose()\n#         list_features_rot.append(feature_names_0)\n#         list_features_rot.append(feature_names_1)\n#         X_all_rot_0 = pd.DataFrame(columns=feature_names_0, data = arr_rot_0)\n#         X_all_rot_0.index = X_all.index\n#         X_all_rot_1 = pd.DataFrame(columns=feature_names_1, data = arr_rot_1)\n#         X_all_rot_1.index = X_all.index\n#         list_X_all_rot.append(X_all_rot_0)\n#         list_X_all_rot.append(X_all_rot_1)\n\n#     X_all_rot = pd.concat(list_X_all_rot, axis=1)\n#     X_all = pd.concat([X_all, X_all_rot], axis=1)\n#     return X_all, feature_names_0, feature_names_1\n\n# X_all, feature_names_rot_0, feature_names_rot_1 = get_rotated(X_all)\n\n# print(X_all.shape)","67080483":"# features_to_pca = [features, features_count]\n\n# from sklearn.decomposition import PCA\n\n# def get_pca(X_all):\n#     list_X_all_pca = [] \n#     list_pca_0 = []\n#     list_pca_1 = []\n#     feature_names_0 = []\n#     feature_names_1 = []\n#     for i in tqdm(range(len(features))):\n#         vars_to_pca = [feat[i] for feat in features_to_rot]\n#         arr = X_all[vars_to_pca].values\n#         pca = PCA(n_components = 2)\n#         arr_pca = pca.fit_transform(arr)\n#         list_pca_0.append(arr_pca[:,0])\n#         list_pca_1.append(arr_pca[:,1])\n#         feature_names_0.append('var_%d_pca_0' %i)        \n#         feature_names_1.append('var_%d_pca_1' %i)        \n#     arr_pca_0 = np.stack(list_pca_0).transpose()\n#     arr_pca_1 = np.stack(list_pca_1).transpose()\n#     X_all_pca_0 = pd.DataFrame(columns=feature_names_0, data = arr_pca_0)\n#     X_all_pca_0.index = X_all.index\n#     X_all_pca_1 = pd.DataFrame(columns=feature_names_1, data = arr_pca_1)\n#     X_all_pca_1.index = X_all.index\n#     list_X_all_pca.append(X_all_pca_0)\n#     list_X_all_pca.append(X_all_pca_1)\n\n#     X_all_pca = pd.concat(list_X_all_pca, axis=1)\n#     X_all = pd.concat([X_all, X_all_pca], axis=1)\n#     return X_all, feature_names_0, feature_names_1\n\n# X_all, feature_names_pca_0, feature_names_pca_1 = get_pca(X_all)\n\n# print(X_all.shape)","5de41b5e":"X_train = X_all.iloc[:train_length,:]\nX_test = X_all.iloc[train_length:,:]\ndel X_all\nimport gc\ngc.collect()\nprint(X_train.shape, X_test.shape)","edb5a1cc":"features_used = [features, features_count]","12fa5a49":"params = {\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 1,\n    'learning_rate': 0.08,\n    'max_depth': -1,\n    'metric':'binary_logloss',\n    'num_leaves': 4,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'reg_alpha': 2,\n    'reg_lambda': 0,\n    'verbosity': 1,\n    'max_bin':256,\n}\n\n# reg_alpha\nreg_alpha_values = [0.75, 1, 2, 3]\nreg_alpha_var = [3, 0, 2, 3, 2, 0, 1, 1, 3, 2, 2, 0, 2, 0, 2, 2, 2, 1, 1, 2, 1, 2, 3, 3, 2, 1, 3, 1, 3, 2, 2, 3, 1, 1, 3, 2, 0, 1, 0, 2, 1, 1, 2, 3, 0, 3, 3, 3, 2, 0, 3, 1, 3, 1, 1, 0, 2, 2, 0, 0, 0, 1, 2, 1, 0, 1, 3, 2, 0, 2, 1, 2, 0, 0, 1, 3, 3, 1, 2, 3, 3, 2, 0, 1, 2, 3, 3, 2, 3, 3, 0, 0, 3, 0, 1, 0, 1, 0, 2, 3, 1, 0, 3, 1, 3, 2, 3, 1, 3, 3, 3, 1, 3, 2, 3, 2, 1, 0, 1, 2, 0, 3, 0, 3, 0, 3, 2, 1, 0, 0, 2, 2, 2, 0, 1, 0, 0, 2, 3, 2, 2, 1, 1, 0, 1, 2, 2, 2, 1, 0, 2, 3, 2, 3, 1, 1, 3, 1, 1, 2, 1, 2, 0, 3, 1, 3, 3, 2, 0, 1, 3, 3, 0, 1, 0, 3, 1, 3, 1, 3, 0, 3, 0, 3, 1, 0, 0, 0, 3, 0, 3, 0, 0, 2, 0, 3, 1, 0, 3, 2]\n\n# max_bin\nmax_bin_values = [256, 512, 1024]\nmax_bin_var = [0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 1, 0, 2, 0, 0, 0, 1, 2, 0, 0, 1, 0, 2]\n\n# learning_rate\nlearning_rate_values = [0.06, 0.08, 0.12]\nlearning_rate_var = [2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0, 2, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 0, 2, 0, 2, 0, 2, 1, 0, 0, 1, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1]\n\n# num_leaves\nnum_leaves_values = [3, 4, 5]\nnum_leaves_var = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 0, 2, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2]\n\n","dc61ae11":"n_folds = 5\nearly_stopping_rounds=10\nsettings = [4]\nnp.random.seed(47)\n\nsettings_best_ind = []\n\ndef train_trees():\n    preds_oof = np.zeros((len(X_train), len(features)))\n    preds_test = np.zeros((len(X_test), len(features)))\n    preds_train = np.zeros((len(X_train), len(features)))\n    preds_fake = np.zeros((len(X_fake), len(features)))\n\n    features_used_flatten = [var for sublist in features_used for var in sublist]\n    X_train_used = X_train[features_used_flatten]\n    X_test_used = X_test[features_used_flatten]\n    X_fake_used = X_fake[features_used_flatten]\n\n    for i in range(len(features)):\n        params['max_bin'] = max_bin_values[max_bin_var[i]]\n        params['learning_rate'] = learning_rate_values[learning_rate_var[i]]\n        params['reg_alpha'] = reg_alpha_values[reg_alpha_var[i]]\n        params['num_leaves'] = num_leaves_values[num_leaves_var[i]]\n        features_train = [feature_set[i] for feature_set in features_used] \n        print(f'Training on: {features_train}')\n        folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=np.random.randint(100000))\n        list_folds = list(folds.split(X_train_used.values, target_train.values))\n        preds_oof_temp = np.zeros((preds_oof.shape[0], len(settings)))\n        preds_test_temp = np.zeros((preds_test.shape[0], len(settings)))\n        preds_train_temp = np.zeros((preds_train.shape[0], len(settings)))\n        preds_fake_temp = np.zeros((preds_fake.shape[0], len(settings)))\n\n        scores = []\n        for j, setting in enumerate(settings):\n            # setting is used for hyperparameter tuning, here you can add sometinh like params['num_leaves'] = setting\n            print('\\nsetting: ', setting)\n            for k, (trn_idx, val_idx) in enumerate(list_folds):\n                print(\"Fold: {}\".format(k+1), end=\"\")\n                trn_data = lgb.Dataset(X_train_used.iloc[trn_idx][features_train], label=target_train.iloc[trn_idx])\n                val_data = lgb.Dataset(X_train_used.iloc[val_idx][features_train], label=target_train.iloc[val_idx])\n\n                # Binary Log Loss\n                clf = lgb.train(params, trn_data, 2000, valid_sets=[trn_data, val_data], verbose_eval=False, early_stopping_rounds=early_stopping_rounds) \n\n                prediction_val1 = clf.predict(X_train_used.iloc[val_idx][features_train])\n                prediction_test1 = clf.predict(X_test_used[features_train])\n                prediction_train1 = clf.predict(X_train_used.iloc[trn_idx][features_train])\n                prediction_fake1 = clf.predict(X_fake_used[features_train])\n\n                # Predictions\n                s1 = roc_auc_score(target_train.iloc[val_idx], prediction_val1)\n                s1_log = log_loss(target_train.iloc[val_idx], prediction_val1)\n                print(' - val AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1, s1_log*1000), end='')\n\n                # Predictions Test\n                if use_experimental:\n                    s1_test = roc_auc_score(target_test, prediction_test1)\n                    s1_log_test = log_loss(target_test, prediction_test1)\n                    print(' - test AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1_test, s1_log_test*1000), end='')\n\n                # Predictions Train\n                s1_train = roc_auc_score(target_train.iloc[trn_idx], prediction_train1)\n                s1_log_train = log_loss(target_train.iloc[trn_idx], prediction_train1)\n                print(' - train AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1_train, s1_log_train*1000), end='')\n                if use_experimental:\n                    print('',clf.feature_importance(), end='')\n\n                print('')\n\n\n                preds_oof_temp[val_idx,j] += np.sqrt(prediction_val1 - prediction_val1.mean() + 0.1) \n                preds_test_temp[:,j] += np.sqrt(prediction_test1 - prediction_test1.mean() + 0.1) \/ n_folds\n                preds_train_temp[trn_idx,j] += np.sqrt(prediction_train1 - prediction_train1.mean() + 0.1) \/ (n_folds-1)\n                preds_fake_temp[:,j] += np.sqrt(prediction_fake1 - prediction_fake1.mean() + 0.1) \/ n_folds\n\n            score_setting = roc_auc_score(target_train, preds_oof_temp[:,j])\n            score_setting_log = 1000*log_loss(target_train, np.exp(preds_oof_temp[:,j]))\n            scores.append(score_setting_log)\n            print(\"Score:  - val AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting, score_setting_log), end='')\n            if use_experimental:\n                score_setting_test = roc_auc_score(target_test, preds_test_temp[:,j])\n                score_setting_log_test = 1000*log_loss(target_test, np.exp(preds_test_temp[:,j]))  \n                print(\" - test AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting_test, score_setting_log_test), end='')\n\n            score_setting_train = roc_auc_score(target_train, preds_train_temp[:,j])\n            score_setting_log_train = 1000*log_loss(target_train, np.exp(preds_train_temp[:,j]))\n            print(\" - train AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting_train, score_setting_log_train))\n\n        best_ind = np.argmin(scores)\n        settings_best_ind.append(best_ind)\n        preds_oof[:,i] = preds_oof_temp[:,best_ind]\n        preds_test[:,i] = preds_test_temp[:,best_ind]\n        preds_train[:,i] = preds_train_temp[:,best_ind]\n        preds_fake[:,i] = preds_fake_temp[:,best_ind]\n\n\n        print('\\nbest setting: ', settings[best_ind])\n        preds_oof_cum = preds_oof[:,:i+1].mean(axis=1)\n        print(\"Cum CV val  : {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_train, preds_oof_cum), 1000*log_loss(target_train, np.exp(preds_oof_cum))))\n        if use_experimental:        \n            preds_test_cum = preds_test[:,:i+1].mean(axis=1)\n            print(\"Cum CV test : {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_test, preds_test_cum), 1000*log_loss(target_test, np.exp(preds_test_cum))))\n        preds_train_cum = preds_train[:,:i+1].mean(axis=1)\n        print(\"Cum CV train: {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_train, preds_train_cum), 1000*log_loss(target_train, np.exp(preds_train_cum))))\n        print('*****' * 10 + '\\n')\n        \n    return preds_oof, preds_test, preds_train, preds_fake\n\npreds_oof, preds_test, preds_train, preds_fake = train_trees()","9d7e280b":"preds_oof_cum = np.zeros(preds_oof.shape[0])\nif use_experimental:\n    preds_test_cum = np.zeros(preds_test.shape[0])\npreds_train_cum = np.zeros(preds_train.shape[0])\nfor i in range(len(features)):\n    preds_oof_cum += preds_oof[:,i]\n    preds_train_cum += preds_train[:,i]\n    print(\"var_{} Cum val: {:<8.5f}\".format(i,roc_auc_score(target_train, preds_oof_cum)), end=\"\")\n    if use_experimental:\n        preds_test_cum += preds_test[:,i]\n        print(\" - test : {:<8.5f}\".format(roc_auc_score(target_test, preds_test_cum)), end=\"\")\n    print(\" - train: {:<8.5f}\".format(roc_auc_score(target_train, preds_train_cum)))","3395972f":"print(settings)\nprint(settings_best_ind)","22f21b73":"from scipy.interpolate import interp1d\nfrom scipy.ndimage.filters import gaussian_filter\nimport matplotlib.pyplot as plt \n\nfeatures_to_show = np.arange(20)\nplt.figure(figsize = (20,20))\n\nfor i in features_to_show:\n    var = 'var_'+str(i)\n    signal = X_test[var].values\n    logits = preds_test[:,i]\n    func = interp1d(signal, logits)\n    space = np.linspace(signal.min(), signal.max(), 4000)\n    activations = func(space)\n    activations_smooth = gaussian_filter(activations, 10)\n    \n    func_smooth = interp1d(space, activations_smooth)\n    logits_smooth = func_smooth(signal)\n    plt.subplot(5,4,i+1)\n    plt.plot(space, activations)\n    plt.plot(space, activations_smooth)","e27ec2ec":"import keras\n\nn_splits = 7\nnum_preds = 5\nepochs = 60\nlearning_rate_init = 0.02\nbatch_size = 4000\n\nnum_features = len(features)\n\ndef get_features(preds, df):\n    list_features = [preds, df[features].values, df[features_count].values, df[features_deviation], df[features_density]]\n    list_indices = []\n    for i in range(num_features):\n        indices = np.arange(num_preds)*num_features + i\n        list_indices.append(indices)\n    indices = np.concatenate(list_indices)\n    feats = np.concatenate(list_features, axis=1)[:,indices]\n    return feats \n\ndef get_model_3():\n    inp = keras.layers.Input((num_features*num_preds,))\n    x = keras.layers.Reshape((num_features*num_preds,1))(inp)\n    x = keras.layers.Conv1D(32,num_preds,strides=num_preds, activation='elu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv1D(24,1, activation='elu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv1D(16,1, activation='elu')(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Conv1D(4,1, activation='elu')(x)\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.Reshape((num_features*4,1))(x)\n    x = keras.layers.AveragePooling1D(2)(x)\n    x = keras.layers.Flatten()(x)\n    x = keras.layers.BatchNormalization()(x)\n    out = keras.layers.Dense(1, activation='sigmoid')(x)\n    return keras.Model(inputs=inp, outputs=out)\n\n\ndef lr_scheduler(epoch):\n    if epoch <= epochs*0.8:\n        return learning_rate_init\n    else:\n        return learning_rate_init * 0.1\n\ndef train_NN(features_oof, features_test, features_train, features_fake):\n    \n    folds = StratifiedKFold(n_splits=n_splits)\n\n    preds_nn_oof = np.zeros(features_oof.shape[0])\n    preds_nn_test = np.zeros(features_test.shape[0])\n    preds_nn_fake = np.zeros(features_fake.shape[0])\n\n    for trn_idx, val_idx in folds.split(features_oof, target_train):\n        features_oof_tr = features_oof[trn_idx, :]\n        target_oof_tr = target_train.values[trn_idx]\n        features_oof_val = features_oof[val_idx, :]\n        target_oof_val = target_train.values[val_idx]\n\n        optimizer = keras.optimizers.Adam(lr = learning_rate_init, decay = 0.00001)\n        model = get_model_3()\n        callbacks = []\n        callbacks.append(keras.callbacks.LearningRateScheduler(lr_scheduler))\n        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n        model.fit(features_oof_tr, target_oof_tr, validation_data=(features_oof_val, target_oof_val), epochs=epochs, verbose=2, batch_size=batch_size, callbacks=callbacks)\n\n        preds_nn_oof += model.predict(features_oof, batch_size=2000)[:,0]\n        preds_nn_test += model.predict(features_test, batch_size=2000)[:,0]\n        preds_nn_fake += model.predict(features_fake, batch_size=2000)[:,0]\n\n        print(roc_auc_score(target_train, preds_nn_oof))\n        if use_experimental:\n            print(roc_auc_score(target_test, preds_nn_test))\n            print(roc_auc_score(target_test, preds_test.mean(axis=1)))\n\n    preds_nn_oof \/= n_splits\n    preds_nn_test \/= n_splits\n    preds_nn_fake \/= n_splits\n    return preds_nn_oof, preds_nn_test, preds_nn_fake\n\n\nfeatures_oof = get_features(preds_oof, X_train)\nfeatures_test = get_features(preds_test, X_test)\nif not use_experimental:\n    del X_test\nfeatures_train = get_features(preds_train, X_train)\nif not use_experimental:\n    del X_train\nfeatures_fake = get_features(preds_fake, X_fake)\nif not use_experimental:\n    del X_fake\n    del preds_oof\n    del preds_fake\n    del preds_train\n    del preds_test\n\nprint(get_model_3().summary())\n    \npreds_nn_oof, preds_nn_test, preds_nn_fake = train_NN(features_oof, features_test, features_train, features_fake)\n\nprint(roc_auc_score(target_train, preds_nn_oof))\nif use_experimental:\n    print('test AUC: ', roc_auc_score(target_test, preds_nn_test))","a2bcda00":"preds_oof_final = preds_nn_oof\npreds_test_final = preds_nn_test\npreds_fake_final = preds_nn_fake\n\nprint('oof  : ', roc_auc_score(target_train, preds_oof_final))\nif use_experimental:\n    print('test : ', roc_auc_score(target_test, preds_test_final))\n    print('train: ', roc_auc_score(target_fake, preds_fake_final))\n\nif not use_experimental:\n    sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n    predictions_all = np.zeros(test_df.shape[0])\n    predictions_all[indices_real] = preds_test_final\n    predictions_all[indices_fake] = preds_fake_final\n    sub[\"target\"] = predictions_all\n    sub.to_csv(\"submission.csv\", index=False)\n    print(sub.head(20))","6c45f68c":"## Training Summary","2fd0b856":"## Training\n\nA little discussion I had with Chua in this markdown cell. Thats probably not the most efficient way of communicating :D.\n\nChua:\n\nOne interesting finding is that any var with AUC lower than 0.504 does not help the model in any way. I checked the naive bayes kernel and they were the 'saw-like' probability curves. Went but to magic ecdf kernel and they were those that have ecdf that lined up a bit too perfectly. Considering that we are training features one by one. I think it may make sense to actually remove them all together.\n\nNawid:\n\nI think your explanation is generally correct, however I would not exclude them already during training especially because you do it separately for every fold, I believe this leads to some sort of overfitting thus leading to differences in CV and LB, I also think that it is better practice to remove them **after** the trees are trained for example with a Lasso model. Moreover I believe that using AUC as validation metric for early stopping leads to overfitting CV. I therefore removed this part of the code and only train a single model. Similarly a very high number of early stopping rounds can also lead to overfitting.","394d824f":"# Imports","2f8155a6":"## Counts, Density, Deviation\nHere I calculate the unique counts of each faeture seaparately. Based on that I also calculate the density by smoothing the counts and also the deviation as counts\/density.","475ec637":"# LGBM \nMany public kernels indicated that the features are independent, conditional on the target. For this reason I train seperate trees for each feature and their respective counts. Using a simple average (of the square root) of all tree predictors achieves around 0.9225 \/ 0.9205 on public\/private LB.","71e57458":"## Standardize\nI standardize all the features (or supposedly so, apparently I forgot density and deviation being in time trouble). Which is important for later NN usage.","42579dca":"# Santander Customer Transaction Prediction\nHi, this kernel produces a top 5 submission with public\/private LB of 0.92569\/0.92446 running in 1.5 hours on kaggle servers. It was written in a very short time thus containing some mistakes which aren't edited. This achievement wouldn't have been possible without the huge insights and inspiration I gained from many great kernels \/ discussions, thank you! I also want to thank my teammates interneuron and Chua Cheng Hong for helping me getting the most out of this kernel and collectively achieving 3rd place in this competition. ","01be9bb9":"## NB predictor (unused)","634fcbe0":"## Rotated features (unused)\n","27e4db69":"## Generating submission","de84f944":"# CNN \nThe CNN model is the main reason for our high placement as we hit a wall using solely trees and couldn't improve upon 0.922 LB. At some point while playing around with the trees I noticed two things:\n1. Averaging the predictors might not be the best solution. At first I just averaged the predictors, then I observed that averaging the logits of predictors improved the final score massively, however i couldn't really explain why. Averaging logits here is equivalent with multiplying the probabilities p(t|x) which is not what happens in Naive Bayes, there you multiply p(x|t). I concluded that the boost comes from the concativity of the logarithm, which kind of translates to the predictors having lower confidence predicting t=1 and higher confidence predicting t=0, I further investigated concave functions and found the square root to work even better. \n\n2. The predictors are very noisy around the tails of the feature distributions and this pattern is likely related to the counts or the features themselves. Also the pattern seems to be similar across different features. \n\nGiven these observations I tried using a CNN to learn these patterns. The original idea here was that it should learn when to trust the tree predictors given the feature and its counts. After reading some other top team solutions I think the CNN maybe picked up something else from the features.\n\nI choose the architecure in a way which would ensure feature independence up until the last dense layer. In order to minimize overfitting and utilize the similarity of patterns across different var_x I used convolutional layers. The convolutions are performed across different var_x and at any point the filters only have a single var and their respective features in their field of view. Batch normalization is a great regularizer here and very crucial for the success of the model. The model has a total of 2.8K trainable parameters which is sufficiently low to prevent overfitting. I verified this by splitting train data into train \/ test with `use_experimental = True` at the top of the kernel and using test AUC as a gauge. The final prediction is the average of the 7 CNNs trained on every fold.","118f53ff":"## EDA on predictors\nI plotted the predictions (sorted by feature), of the trees seaparately for the first 20 vars (x-axis corresponds to the z-score). The predictions are very noisy at the tails of the distributions, therefore I also tried using smoothed predictions (orange line) with no success however.","df6013c0":"# Feature Engineering","b233ffdf":"## Params\nParameters of the LGBM model. I choose l1 regularization \/ max_bin \/ learning rate and num_leaves seaprately for each of the 200 var_x through earlier hyperparam search.","0160eb7b":"## Setting up Dataframes\nAfter performing FE on `X_all`, I split it back into train\/test and delete the obsolete dataframe. The latter is a reoccuring theme in this kernel and was necessary as I often experienced memory overflow. This is also the reason why I wrote most of the code inside of functions. Shoutout to kaggle however for providing fast GPUs!","9caee65d":"## PCA (unused)","8666b7e8":"## Target encoding (unused)\n","6b5f0c4b":"# Loading Data\nAt this point I filter out the fakes (shoutout to YaG320) and concatenate train and test for future FE. Setting `use_experimental = True` splits the Train data into train \/ test which was useful for later NN training indicating whether a model is overfitting. I wasn't sure if the fakes are going to be used for final score evaluation, so I also applied all the transformations to them and kept them in a separate dataframe. ","0e61c670":"## Training"}}