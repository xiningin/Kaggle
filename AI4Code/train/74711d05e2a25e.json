{"cell_type":{"2e6ba8e2":"code","184abcba":"code","4d44dd8b":"code","2a397adc":"code","a06e29ff":"code","60487f32":"code","1a3b3935":"code","b99ea68f":"code","2ee1dd60":"code","347e7a2b":"code","ad63f77f":"code","9fb94319":"code","5176202c":"code","6fa935f3":"code","62446946":"code","8f0a894a":"code","40d38144":"code","375dcfe3":"code","27d49fbc":"code","8b7b6efc":"code","a2621fa5":"code","48f5a5b4":"code","d56e7cad":"code","b154fbb2":"markdown","cba15296":"markdown","fc055a91":"markdown","9af033d6":"markdown","9bff7b4b":"markdown","df10e1b7":"markdown","b79d3939":"markdown","236e5a1a":"markdown","c6876fbf":"markdown","726d9abe":"markdown","3ec22432":"markdown","94330433":"markdown","d519b035":"markdown","dd1331e2":"markdown","5460dafa":"markdown","824bdc0f":"markdown","26e496b8":"markdown","f04a40de":"markdown","573386cf":"markdown","f36c8fe4":"markdown","0ca6a877":"markdown"},"source":{"2e6ba8e2":"import copy\nimport random\nfrom IPython.display import clear_output\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\nfrom bokeh.layouts import column, row\noutput_notebook()","184abcba":"from IPython.display import Image\nimport os\nImage(\"..\/input\/rl-images\/rl_agent.png\", width=600, height=800)","4d44dd8b":"state = [0 for i in range(9)]\nstate","2a397adc":"state[0] = 1   # X plays action 0\nstate[3] = -1  # O plays action 3\nstate","a06e29ff":"def render(s):\n    board = ['.' for i in range(9)]\n    for i in range(9):\n        if s[i] == 1: board[i] = 'X'\n        if s[i] == -1: board[i] = 'O'\n        if s[i] == 0: board[i] = ' '\n    print(board[0], \"|\", board[1], \"|\", board[2])\n    print(\"----------\")\n    print(board[3], \"|\", board[4], \"|\", board[5])\n    print(\"----------\")\n    print(board[6], \"|\", board[7], \"|\", board[8])\n    \nrender(state)","60487f32":"class ttt:\n        \n    def __init__(self):\n        self.state = self.reset()\n        \n    def reset(self):\n        return [0 for i in range(9)]\n\n    def game_over(self, s):\n        done = False\n        reward = 0\n        if (s[0] + s[1] + s[2]  == 3 or s[3] + s[4] + s[5]  == 3 or s[6] + s[7] + s[8]  == 3 or\n            s[0] + s[3] + s[6]  == 3 or s[1] + s[4] + s[7]  == 3 or s[2] + s[5] + s[8]  == 3 or\n            s[0] + s[4] + s[8]  == 3 or s[2] + s[4] + s[6]  == 3):\n            done = True\n            reward = 1\n        if (s[0] + s[1] + s[2]  == -3 or s[3] + s[4] + s[5]  == -3 or s[6] + s[7] + s[8]  == -3 or\n            s[0] + s[3] + s[6]  == -3 or s[1] + s[4] + s[7]  == -3 or s[2] + s[5] + s[8]  == -3 or\n            s[0] + s[4] + s[8]  == -3 or s[2] + s[4] + s[6]  == -3):\n            done = True\n            reward = -1\n        if sum(1 for i in s if i != 0)==9 and not done:\n            done = True\n        return done, reward\n\n    def step(self, state, action, player):\n        next_state = state.copy()\n        if player == 0: next_state[action] = 1\n        else: next_state[action] = -1\n        done, reward = self.game_over(next_state)\n        return next_state, done, reward","1a3b3935":"random.seed(1)\nenv = ttt()           # initialize the environment\nstate = env.reset()   # reset the game board\n\nprint(\"Start game\")\nrender(state)         # display the game board\nprint(\" \")\n\ndone = False\nwhile not done:       # loop to play one game\n    action = random.choice([i for i in range(len(state)) if state[i] == 0]) # Player X's move\n    next_state, done, reward = env.step(state, action, 0)\n    if not done:                                                            # Player O's move\n        omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n        next_state, done, reward = env.step(next_state, omove, 1)\n    state = next_state.copy()\n    print(\"Action:\", action, \"Reward:\", reward)\n    render(state)\n    print(\" \")","b99ea68f":"def play_v_random (games, Qvalues, render_game=False):\n    results = [0 for i in range(games)]\n    for i in range(games):\n        state = env.reset()\n        done = False\n        while not done:\n            xq = [Qvalues.get((tuple(state), i)) for i in range(9) if Qvalues.get((tuple(state), i)) is not None]\n            if len(xq) == 0: \n                action = random.choice([i for i in range(len(state)) if state[i] == 0])\n            else:\n                idx = [i for i in range(9) if Qvalues.get((tuple(state), i)) is not None]\n                action = idx[xq.index(max(xq))]\n            next_state, done, reward = env.step(state, action, 0)\n            if not done:\n                omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                next_state, done, reward = env.step(next_state, omove, 1)\n            state = next_state.copy()\n            if render_game:\n                print(\"Action:\", action, \"Reward:\", reward)\n                render(state)\n                print(\" \")\n        results[i] = reward\n    return results","2ee1dd60":"random.seed(0)\nalpha = 0.05            # learning rate\ngamma = 0.95            # discount factor\nQvalues = {}            # Q-value dictionary\niterations = 500000\n\nresults = play_v_random(1000, Qvalues)\nprint(\"X Won: {:.1%}\\tO Won: {:.1%}\\tTies: {:.1%}\".format(sum(1 for i in results if i == 1)\/1000, \n                                                         sum(1 for i in results if i == -1)\/1000, \n                                                         sum(1 for i in results if i == 0)\/1000))\n\nfor iteration in range(iterations):    # loop to play a bunch of games\n    state = env.reset()\n    next_state = state.copy()\n    done = False\n    epsilon = max(1 - iteration\/(iterations*0.8), 0.01)\n    while not done:                    # loop to play one game\n        if random.random() < epsilon:  # epsilon greedy policy for player X\n            action = random.choice([i for i in range(len(state)) if state[i] == 0])\n        else:\n            xq = [Qvalues.get((tuple(state), i)) for i in range(9) if Qvalues.get((tuple(state), i)) is not None]\n            if len(xq) == 0: action = random.choice([i for i in range(len(state)) if state[i] == 0])\n            else:\n                idx = [i for i in range(9) if Qvalues.get((tuple(state), i)) is not None]\n                action = idx[xq.index(max(xq))]\n        next_state, done, reward = env.step(state, action, 0)\n        if not done:                  # random policy for player O\n            omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n            next_state, done, reward = env.step(next_state, omove, 1)\n        if not done:\n            key = (tuple(state), action)\n            if key not in Qvalues: Qvalues[key] = reward\n            next_idx = [i for i in range(9) if Qvalues.get((tuple(next_state), i)) is not None]\n            if len(next_idx) > 0: next_value = max([Qvalues.get((tuple(next_state), i)) for i in next_idx])\n            else: next_value = 0\n        else: next_value = reward\n        # update the Q-value for the state-action pair\n        Qvalues[key] *= 1 - alpha\n        Qvalues[key] += alpha * (reward + gamma * next_value) \n        state = next_state.copy()\n        \n    if iteration % 50000 == 0:\n        results = play_v_random(1000, Qvalues)\n        print(\"X Won: {:.1%}\\tO Won: {:.1%}\\tTies: {:.1%}\".format(sum(1 for i in results if i == 1)\/1000, \n                                                                 sum(1 for i in results if i == -1)\/1000, \n                                                                 sum(1 for i in results if i == 0)\/1000))","347e7a2b":"random.seed(1)\nplay_v_random(1, Qvalues, render_game=True)","ad63f77f":"x = [i for i in range(len(Qvalues))]\nq = q = list(Qvalues.values())\nq.sort()\np = figure(title=\"Q Values\", plot_height=300)\np.circle(x, q)\nshow(p)","9fb94319":"!pip3 install ann_visualizer","5176202c":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nnetwork = Sequential()\n\n# Hidden Layer #1\nnetwork.add(Dense(units=10, activation='relu', input_dim=6))\n\n# Hidden Layer #2\nnetwork.add(Dense(units=10, activation='relu'))\n\n# Output Layer\nnetwork.add(Dense(units=6, activation='softmax')) # softmax for categorical output (converts to probabilities)\n\nfrom ann_visualizer.visualize import ann_viz\n\nann_viz(network, title=\"Example Fully Connected Network\")","6fa935f3":"Image(\"..\/input\/network\/network.png\", width = 1000, height = 400)","62446946":"x = [i\/10 for i in range(-10,10)]\ny = [val if val>=0 else 0 for val in x]\n\np = figure(title=\"ReLu Activation\", plot_height=300)\np.line(x, y)\nshow(p)","8f0a894a":"import numpy as np\n\nweights = [i for i in range(-6, 7)]\n\nprob = np.exp(weights)\/sum(np.exp(weights))\nprint(prob)\nprint(sum(prob)) # sum to one?","40d38144":"p = figure(title=\"Softmax Activation\", plot_height=300)\np.line(weights, prob)\nshow(p)","375dcfe3":"network.trainable_weights","27d49fbc":"import tensorflow as tf\n\nclass DQNagent:\n    \n    def __init__(self, state_size, action_size, iterations):\n        self.gamma = 0.95                                    # discount factor\n        self.state_size = state_size                         # 9 for tic-tac-toe\n        self.action_size = action_size                       # 9 for tic-tac-toe\n        self.iterations = iterations\n        self.model = self.build_model()\n        self.optimizer = tf.keras.optimizers.SGD(lr=0.02)    # lr = learning rate (= alpha)\n        self.loss_fn = tf.keras.losses.mean_squared_error\n\n    def build_model(self):\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Dense(self.state_size**2, activation=\"relu\", input_shape=[self.state_size]),\n            tf.keras.layers.Dense(self.state_size**2, activation=\"relu\"),\n            tf.keras.layers.Dense(self.action_size)\n        ])\n        return model\n    \n    def train_model(self, state_history, action_history, next_state_history, rewards, dones):\n        next_Q_values = self.model.predict(np.array(next_state_history))                         # 1. the forward pass\n        max_next_Q_values = np.max(next_Q_values, axis=1)\n        target_Q_values = rewards + (1 - 1*np.array(dones)) * self.gamma * max_next_Q_values\n        target_Q_values = tf.reshape(target_Q_values, [len(rewards), 1])\n        mask = tf.one_hot(action_history, 9)\n        with tf.GradientTape() as tape:\n            all_Q_values = self.model(np.array(state_history))\n            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n            loss = tf.reduce_mean(self.loss_fn(target_Q_values, Q_values))                      # 2. measure the error\n        grads = tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))              # 3. the reverse pass\n        \n    def play_ttt(self):\n        for iteration in range(self.iterations):    # outer loop to play the game a bunch of times\n            state = env.reset()\n            next_state = state.copy()\n            done = False\n            dones = []\n            state_history = []\n            state_history.append(state)\n            action_history = []\n            rewards = []\n            epsilon = max(1 - iteration\/(self.iterations*0.8), 0.01)\n            while not done:                          # inner loop to play one game\n                if random.random() < epsilon:        # epsilon-greedy policy\n                    action = random.choice([i for i in range(len(state)) if state[i] == 0])\n                else:\n                    action = np.argmax(self.model.predict(np.array(state)[np.newaxis])[0])\n                action_history.append(action)\n                next_state, done, reward = env.step(state, action, 0)\n                if done: \n                    state_history.append(next_state)\n                    dones.append(done)\n                    rewards.append(reward)\n                if not done:\n                    omove = random.choice([i for i in range(len(next_state)) if next_state[i] == 0])\n                    next_state, done, reward = env.step(next_state, omove, 1)\n                    state = next_state.copy()\n                    state_history.append(next_state)\n                    dones.append(done)\n                    rewards.append(reward)\n            next_state_history = state_history[1:len(state_history)]\n            state_history = state_history[0:len(action_history)]\n            self.train_model(state_history, action_history, next_state_history, rewards, dones)\n        return self.model","8b7b6efc":"def dqn_v_random (model, games, render_game=False):\n    results = [0 for i in range(games)]\n    for i in range(games):\n        board = env.reset()\n        done = False\n        while not done:\n            xmoves = model.predict(np.array(board)[np.newaxis])[0]\n            xmoves[np.where(np.array(board)!=0)[0]] = -1\n            xmove = np.argmax(xmoves)\n            board[xmove] = 1\n            done, reward = env.game_over(board)\n            if not done:\n                omove = random.choice([i for i in range(len(board)) if board[i] == 0])\n                board[omove] = -1\n                done, reward = env.game_over(board)\n        results[i] = reward\n        if render_game:\n            print(\"Action:\", action, \"Reward:\", reward)\n            render(state)\n            print(\" \")\n    return results","a2621fa5":"tf.random.set_seed(1234)\nrandom.seed(1234)\n\nm1 = DQNagent(9,9,1).play_ttt()\n\nresults = dqn_v_random(m1, 1000)\n\nprint(\"X Won: {:.1%}\\tO Won: {:.1%}\\tTies: {:.1%}\".format(sum(1 for i in results if i == 1)\/1000, \n                                                                 sum(1 for i in results if i == -1)\/1000, \n                                                                 sum(1 for i in results if i == 0)\/1000))","48f5a5b4":"tf.keras.backend.clear_session()\ntf.random.set_seed(1234)\nrandom.seed(1234)\n\nm100 = DQNagent(9,9,100).play_ttt()\n\nresults = dqn_v_random(m100, 1000)\n\nprint(\"X Won: {:.1%}\\tO Won: {:.1%}\\tTies: {:.1%}\".format(sum(1 for i in results if i == 1)\/1000, \n                                                                 sum(1 for i in results if i == -1)\/1000, \n                                                                 sum(1 for i in results if i == 0)\/1000))","d56e7cad":"tf.keras.backend.clear_session()\ntf.random.set_seed(1234)\nrandom.seed(1234)\n\nm1000 = DQNagent(9,9,1000).play_ttt()\n\nresults = dqn_v_random(m1000, 1000)\n\nprint(\"X Won: {:.1%}\\tO Won: {:.1%}\\tTies: {:.1%}\".format(sum(1 for i in results if i == 1)\/1000, \n                                                                 sum(1 for i in results if i == -1)\/1000, \n                                                                 sum(1 for i in results if i == 0)\/1000))","b154fbb2":"These calculations are done for each node in each layer - including the output layer. However, since the output layer represents a categorical variable, I need to convert those values into probabilities. Then we simply choose the action with the highest probability. The softmax activation function in the output layer converts weights into probabilities by:\n\n$$S(y_i) = \\frac{e^{y_i}}{\\sum\\limits_{j}{e^{y_j}}}$$","cba15296":"When the model is defined, all weights are randomly initialized. Here are the weights for the two hidden layers and the output layer in our example network.","fc055a91":"## Actions\n\nFor the first player (X), there are nine possible actions. An action of 0 represents X playing in the upper left board position, an action of 1 represents the upper middle position, etc. \n\nIf X plays action 0, and O plays action 3, the state becomes:","9af033d6":"## Thoughts\n\n* There are a lot more positive Q-values than negative, which we'd expect since the policy seeks positive rewards. \n* Is there a q-value for every possible state - did we miss some?\n* After playing 500,000 games, the policy improved, but it's really not that great.\n* What if the state space was continuous (infinite)?\n* What if the action spapce was continuous?\n\n## Conclusion\n\nThe general idea of Q-learning seems promising, but we need a method that can scale to non-trivial environments.\n\n# Deep Q-Learning\n\nThe general idea is the same, but there's a twist.\n* We seek a function $Q^*(s,a) \\rightarrow r$ that will tell us what our return will be if we took an action in a given state. \n* We then construct a policy $\\pi^*$ to maximize our return: $\\pi^* = \\underset{a}{argmax}Q^*(s,a)$. \n* We don't know what the true $Q^*(s,a)$ function is, so we estimate it using a **neural network**, which are **universal function approximators**.\n\n## What's a Neural Network?\n\nA neural network consists of multiple layers of interconnected nodes. A *sequential network* is the simplest structure and the kind we'll use here. The first layer is the *input layer* and is followed by one or more *hidden layers* that feed the *output layer*. For the tic-tac-toe environment the input layer will have 9 nodes to represent the board state (one node per board position). The output layer will also have 9 nodes to represent the 9 actions (place a mark on position 0, position 1, etc.). \n\nA simplified version of our network is as follows.","9bff7b4b":"So here we go. I'll have X play O before any Q-learning, update the Q-values based on 50,000 games, have X play O again, update Q-values some more, and so on for 10 iterations. By the end, the algorithm will have seen 500,000 games. Recall that the output is X wins, O wins, and ties and that X plays O 1,000 times, so move the decimal to the left once for the percent of wins.","df10e1b7":"## Train The Model On 1 Game\n\n","b79d3939":"What happens at a single hidden node:\n* It takes 6 values (called weights) as inputs to the node.\n* It sums the weights.\n* It applies an activation function (in this case relu = rectified linear) to produce an output weight.\n\nThere are many activation functions. Rectified linear activation used in this example takes the form:\n\n$$f(z) = \\begin{Bmatrix} z, z\\ge0 \\\\ 0, z<0 \\end{Bmatrix} $$\n\nand plotted looks like:","236e5a1a":"## Agent: The Learning System\n\n* The agent knows nothing about the rules or the reward system.\n* The agent only knows the state, valid actions, and the resulting reward after choosing an action.\n    + Note it is not strictly necessary to limit the agent to valid actions - I could have it learn what actions are valid by giving a negative reward.\n    \nHow to tie state, action, and reward together to achieve learning?\n\n### Q-Learning Methodology\n\nDefine:\n* *s* is the state.\n* *a* is the action.\n* *(s, a)* is a state-action pair.\n* *r* is the reward resulting from *(s, a)*.\n* *Q* is a \"quality value\" to determine the best action for a given *(s, a)*. Higher is better.\n\nQ-learning works by an agent (initially) acting randomly in the environment, keeping track of rewards, and gradually updating its estimates of the Q-values. Eventually, the Q-value estimates will be accurate enough so that, given a state, the optimal policy will be to pick the action with the highest associated Q-value.\n\nThe algorithm:\n\n$$Q(s, a) \\xleftarrow[\\alpha]{} r + \\gamma \\cdot \\underset{a'}{max} Q(s', a')$$\n\nThe math in English:\n\n>For each state-action pair (s, a), this algorithm keeps track of a running average of the rewards r the agent gets upon leaving the state s with action a, plus the sum of discounted future rewards it expects to get. To estimate this sum, we take the maximum of the Q-Value estimates for the next state s\u2032, since we assume that the target policy would act optimally from then on.\n\n>Aur\u00e9lien G\u00e9ron\n\nA couple of new terms snuck in there:\n* alpha controls the learning rate (if 1, then no effect).\n* gamma discounts the future rewards (if 1, then no effect).\n\nBoth of these are hyperparameters we can adjust to stabilize performance.\n\n### Back To The Example\n\nExplicity representing the Q-values for the game:\n* Move 1: Q([0,0,0,0,0,0,0,0,0], 2) = 0\n* Move 2: Q([0,-1,1,0,0,0,0,0,0], 4) = 0\n* Move 3: Q([-1,-1,1,0,1,0,0,0,0], 7) = 0\n* Move 4: Q([-1,-1,1,0,1,0,0,1,-1], 5) = 0\n* Move 5: Q([-1,-1,1,0,1,-1,1,1,-1], 3) = 1\n\nIf this same sequence of moves occurred in game 2, then the non-zero Q-value from Move 5 gets back-propogated to Move 4.\n\nAssuming alpha = 0.01, and gamma = 0.95:\n* Move 1: Q([0,0,0,0,0,0,0,0,0], 2) = 0\n* Move 2: Q([0,-1,1,0,0,0,0,0,0], 4) = 0\n* Move 3: Q([-1,-1,1,0,1,0,0,0,0], 7) = 0\n* Move 4: Q([-1,-1,1,0,1,0,0,1,-1], 5) = (alpha)(0) + (gamma)(1) = 0.95\n* Move 5: Q([-1,-1,1,0,1,-1,1,1,-1], 3) = 1\n\nThat's it! It ends up being just a big book-keeping exercise to keep track of all the state-action pairs and their associated Q-values. \n\n### Epsilon-Greedy Policy\n\nEarlier I mentioned that the agent intially acts randomly, and then transitions to choosing actions based on Q-values. I control this by implimenting an epsilon greedy policy. \n\n1. Before each action by X, draw a random number and compare it to a variable represented by epsilon. \n2. If the random number is less than epsilon, then Player X's next action is random. \n3. Otherwise, Player X's next action is determined by the Q-values.\n\nWhen the algorithm starts, epsilon is 1.0. As training progresses, epsilon gradually decreases to 0.01.\n\n## Train the Agent\n\nWhile training, I want the agent to play against a random opponent to monitor progress.","c6876fbf":"## Example of Playing One Game\n\nWe haven't defined the agent yet, so we'll have both players choose random actions.","726d9abe":"## Train On 100 Games","3ec22432":"## Train On 1,000 Games","94330433":"# Introduction\n\nI am a novice in the field of Reinforcement Learning (RL). In an attempt to gain a more thorough understanding of machine learning (including RL) and brush up on Python, I found the excellent book <a href=\"https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow<\/a> by Aur\u00e9lien G\u00e9ron and dug in. Chapter 18 of G\u00e9ron's book provides the foundation for this notebook.\n\n# Machine Learning Categories\n\nCommon Supervised Learning Algorithms:\n* k-Nearest Neighbors \n* Support Vector Machines (SVM)\n* Random Forests <font color=\"red\"> (last PD Session) <\/font>\n* Neural Networks\n\nCommon Unsupervised Learning Algorithms:\n* K-Means (for clustering)\n* One-class SVM and Isolation Forest (for anomaly and novelty detection)\n* Principal Component Analysis (PCA) (for visualization and dimensionality reduction)\n\nCommon Reinforcement Learning Techniques:\n* Policy Gradients\n* Q-Learning and Deep Q-Learning <font color=\"red\"> (this PD Session) <\/font>\n* Markov Decision Processes\n\n# Reinforcement Learning\n\nReinforcement learning is applied to a very different type of problem than supervised\/unsupervised learning.\n* In supervised\/unsupervised learning, a model is trained to make an accurate **prediction**.\n* In reinforcement learning, a model is trained to **act in an environment**.\n\n## Framework and Terminology\n\n* *Agent*: the learning system.\n* An agent exists in an *environment*.\n* The agent makes an *observation* of the environment's *state*.\n* Based on the state, the agent chooses an *action*.\n* Based on the action, the agent receives a *reward* - either positive for \"good\" actions, or negative for \"bad\" actions.\n* The agent must then learn what the best *policy* is for the environment.\n\nThis framework can be depicted by:","d519b035":"Summary:\n* X win rate increases\n* O win rate decreases\n* Ties increase\n\nLet's watch the agent play a game.","dd1331e2":"Need a new function for the DQN agent to play against a random opponent.","5460dafa":"## How the Model Is Trained\n\nAt this point, we have:\n* Inputs\n* A bunch of interconnected weights and activations functions\n* Outputs\n* A means to convert the output into probabilities\n\nHow do we train the model to estimate *Q(s, a)*? Using a back propogation training algorithm!\n\nThis will be very similar to what we did in Q-Learning. Recall:\n* Move 1: Q([0,0,0,0,0,0,0,0,0], 2) = 0\n* Move 2: Q([0,-1,1,0,0,0,0,0,0], 4) = 0\n* Move 3: Q([-1,-1,1,0,1,0,0,0,0], 7) = 0\n* Move 4: Q([-1,-1,1,0,1,0,0,1,-1], 5) = (alpha)(0) + (gamma)(1) = 0.95\n* Move 5: Q([-1,-1,1,0,1,-1,1,1,-1], 3) = 1\n\nTo train the neural network, we'll play one game at a time and record the states, actions, and rewards. We then follow a three step process:\n1. The forward pass. \n\n    a. The states are taken as inputs into the neural network input layer and passed to the first hidden layer.\n    \n    b. Calculate the output of each node in the hidden layer using the activation function, and pass those values to the second hidden layer.\n    \n    c. Calculate the output of each node in the second hidden layer and pass the values to the output layer.\n    \n    d. Make predictions based on the output layer values.\n    \n2. Measure the prediction error (desired output versus model output) using a loss function (we'll use mean squared error).\n\n3. The reverse pass.\n\n    a. Go through each layer in reverse to measure the error contribution from each weight.\n    \n    b. Adjust the weights to reduce the error using an optimizing function (we'll use stochastic gradient descent).\n\n## Putting It All Together In A Class `DQNagent`","824bdc0f":"## Reward System\n\nFor this example, I'll develop a game playing policy for X using the following reward system:\n* X wins: reward = 1 \n* O wins: reward = -1\n* Else: reward = 0 \n\n## Environment\n\n","26e496b8":"This is not the way we're used to looking at a tic-tac-toe board, so I'll write a function `render()` to render the board.","f04a40de":"# Tic-Tac-Toe Example\n\n* Agent: Q-learning\n* Environment: the rules and gameplay of tic-tac-toe\n\n## State\n\nThe state is the game board, which consists of nine possible locations to play that are either empty, contain an X, or contain a O. In Python, I will represent the state as an list of length nine where 0 represents an empty space, 1 represents an X, and -1 represents an O. \n\nAt the beginning of the game, the board is empty, so it will be represented as:","573386cf":"### Q-Learning Results After 500,000 Games\n\nX Won: 73.3%    O Won: 2.4%    Ties: 24.3%\n\n# Other Examples\n\n| Open AI Environment | State Space  | Action Space | Link |\n|:---------------|:------------:|:------------:|-----|\n| Cartpole       | 4 Continuous | 2 Discrete   | Not yet published |\n| Mountain Car   | 2 Continuous | 3 Discrete   | https:\/\/jfking50.github.io\/mountaincar\/ |\n| Lunar Lander   | 8 Continuous | 4 Discrete   | https:\/\/jfking50.github.io\/lunar\/ |\n| Bipedal Walker | 24 Continuous | 4 Continuous | Not yet published |\n\n# Combat Model Applications\n\n* Train an ISR asset to collect intel at standoff\n* Train a BCT to maneuver to an objective\n* Train a Corps to defeat a threat","f36c8fe4":"X won this game but clearly missed an opportunity to win the game on the 4th move. Perhaps X was (incorrectly) blocking O on the top row. Notice that the policy learned that the optimal opening move is the center position (action 4). \n\nLet's look at the Q-values.","0ca6a877":">Image from Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (Aur\u00e9lien G\u00e9ron)."}}