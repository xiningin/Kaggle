{"cell_type":{"dee87bb5":"code","2f71fff5":"code","0f327132":"code","e8ed95fe":"code","01c26957":"code","621d33f8":"code","2683fd6c":"code","f036fcdd":"code","e116923e":"code","d6e8d166":"code","7b6557d7":"code","3529e62d":"code","c0225c1a":"code","066b1362":"code","9ca2f69b":"code","40b7d6de":"code","7ec67e4a":"code","27f3dc64":"code","8a0a8912":"code","c11eb802":"code","1137c452":"code","1e372026":"code","3dab878a":"code","13ed5f0a":"code","db267fd6":"code","3412e81f":"code","9912f5e2":"code","63846f61":"code","db33c110":"code","1131f67a":"code","0f681392":"code","a4a0c366":"code","fe61af61":"code","9d105a0c":"code","e16aeb30":"code","4dc19b66":"code","18a6450d":"code","dd311042":"code","517064da":"code","88f9d00a":"code","aadf22b4":"code","91c01b66":"code","b8a4f418":"code","fee983c8":"code","790774b8":"code","25a3f99a":"code","982f1d29":"code","80f634f3":"code","3e5cb991":"code","2c96a2b6":"code","0179504d":"code","2db7ce4c":"code","f0148e59":"code","bb4b46e1":"code","131def77":"code","fe6af009":"code","f6459df9":"code","dbda44f9":"code","3e8639d2":"code","79ec55c9":"code","0ce0314f":"code","7835f120":"code","9bb05424":"code","d579de9a":"code","9c63b91b":"code","25370795":"code","6c5e4cd2":"code","10d85c60":"code","f979a125":"markdown"},"source":{"dee87bb5":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge, HuberRegressor, GammaRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\nfrom sklearn.feature_selection import RFE\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score","2f71fff5":"# Read train and test data\ntrain_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_raw = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","0f327132":"# Check the shape of train and test data\nprint(train_raw.shape)\nprint(test_raw.shape)","e8ed95fe":"# Inspect train data using head() to get a general idea\n# We can see that there are some columns with NaN values which we will deal later\ntrain_raw.head()","01c26957":"# Check datatypes for all columns\/features\n# We'll only check for train data as test data has same features except the target feature SalePrice\n# We can observe that there are int, float and string values present in the dataset\n# We'll seperate numeric and string features, to be used later\nnumeric_cols = []\nstring_cols = []\nfor col in train_raw.columns:\n    print(col,\" : \", train_raw[col].dtype)\n    if (train_raw[col].dtype in [\"int\",\"float\"]):\n        numeric_cols.append(col)\n    elif (train_raw[col].dtype in [\"object\"]):\n        string_cols.append(col)\n\n# Check numeric and string features\nprint(\"\\nNumeric Features: \",len(numeric_cols), \"\\n\", numeric_cols, \"\\n\\n\")\nprint(\"\\nString Features: \", len(string_cols), \"\\n\",string_cols)","621d33f8":"# We'll perform the same data preprocessing for both train and test data but seperately to avoid data leak\n\n# Check for missing values in numeric features for train data\nnumeric_missing_cols_train = []\nfor col in numeric_cols:\n    if(train_raw[col].isnull().sum() > 0):\n        print(col, \" : \", train_raw[col].isnull().sum())\n        numeric_missing_cols_train.append(col)\n\n# Numeric columns with missing values for train data\nprint(\"Numeric columns train data: \",numeric_missing_cols_train,\"\\n\")\n\n# Check for missing values in numeric features(except target feature SalePrice) for test data\nnumeric_missing_cols_test = []\nfor col in [numeric_col for numeric_col in numeric_cols if numeric_col != \"SalePrice\"]:\n    if(test_raw[col].isnull().sum() > 0):\n        print(col, \" : \", test_raw[col].isnull().sum())\n        numeric_missing_cols_test.append(col)\n\n# Numeric columns with missing values for test data\nprint(\"Numeric columns test data: \",numeric_missing_cols_test)","2683fd6c":"# Check for zero values in numeric features\n# We'll deal with these zero values later on\n# Not all zero values are useless\nprint(\"Train features with zero values: \\n\",train_raw[numeric_cols].isin([0]).sum(),\"\\n\\n\")\nprint(\"Test features with zero values: \\n\",test_raw[[numeric_col for numeric_col in numeric_cols if numeric_col != \"SalePrice\"]].isin([0]).sum())","f036fcdd":"# Explore numeric columns with missing values for train data\nprint(train_raw[numeric_missing_cols_train])\n\n# Check the relationship of these features with the target variable SalePrice using jointplot\n# We can observe some large values which can be considered as outliers provided that we are dealing with regression\nfor i, feature in enumerate(numeric_missing_cols_train, 1):\n    sns.jointplot(x=feature, y='SalePrice', data=train_raw, kind = 'reg', height = 5)\nplt.show()","e116923e":"# Check for missing values in string features for train data\nstring_missing_cols_train = []\nfor col in string_cols:\n    if(train_raw[col].isnull().sum() > 0):\n        print(col, \" : \", train_raw[col].isnull().sum())\n        string_missing_cols_train.append(col)\n        \n# String columns with missing data for train data\nprint(\"String columns test data: \",string_missing_cols_train, \"\\n\")\n\n# Check for missing values in string features for test data\nstring_missing_cols_test = []\nfor col in string_cols:\n    if(test_raw[col].isnull().sum() > 0):\n        print(col, \" : \", test_raw[col].isnull().sum())\n        string_missing_cols_test.append(col)\n        \n# String columns with missing data for test data\nprint(\"String columns test data: \",string_missing_cols_test)","d6e8d166":"# Find correlation among the numeric features of the train data\n# Corr() ignores NaN and non-numeric values\ncor = train_raw.corr()\ncorr_target = abs(cor['SalePrice'])\n\nrelevant_feature_names = []\nfor col, value in corr_target[corr_target>0.3].iteritems():\n    relevant_feature_names.append(col)\n\nrelevant_feature_names","7b6557d7":"# Copy train_raw and test_raw for preprocessing\ntrain = train_raw.copy()\ntest = test_raw.copy()","3529e62d":"# Handling numeric missing values for train data\n# LotFrontage  :  259\n# MasVnrArea  :  8\n# GarageYrBlt  :  81\n\n# Inspect Garage features to see if GarageYrBlt values are missing \n# because of No Garage value in their corresponding Garage features\n\n# Get all row indexes that have missing values for GarageYrBlt feature \ngarage_row_list = train.index[train['GarageYrBlt'].isnull()].tolist()\n# Get col indexes for Garage features\ngarage_col_list = [train.columns.get_loc(col) for col in ['GarageType', 'GarageYrBlt', 'GarageFinish','GarageArea', 'GarageQual']]\nprint(\"Garage feature rows: \", garage_row_list)\nprint(\"Garage feature cols: \", garage_col_list)\n# We can observe below that GarageYrBlt feature has missing values\n# because its corresponding Garage features have No Garage(NaN) as their values \nprint(train.iloc[garage_row_list, garage_col_list])","c0225c1a":"# Impute 0 for missing values of numerical feature GarageYrBlt as the corresponding GarageType value is No Garage (NaN)\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)","066b1362":"# Check min, max, mean, mode values for numeric features with missing values\nprint(\"Minimum: \",train['LotFrontage'].min(), train['MasVnrArea'].min(), train['GarageYrBlt'].min())\nprint(\"Average: \",train['LotFrontage'].mean(), train['MasVnrArea'].mean(), train['GarageYrBlt'].mean())\nprint(\"Mode: \",train['LotFrontage'].mode()[0], train['MasVnrArea'].mode()[0], train['GarageYrBlt'].mode()[0])\nprint(\"Maximum: \",train['LotFrontage'].max(), train['MasVnrArea'].max(), train['GarageYrBlt'].max())\n\n# Use mean value to impute the mising values in two numeric columns: LotFrontage, MasVnrArea\n# We'll handle GarageYrBlt later\nfor col in [nmc_train for nmc_train in numeric_missing_cols_train if nmc_train != \"GarageYrBlt\"]:\n    train[col] = train[col].fillna(train[col].mean())","9ca2f69b":"# Handling numeric missing values for test data\n# LotFrontage  :  227\n# MasVnrArea  :  15\n# BsmtFinSF1  :  1\n# BsmtFinSF2  :  1\n# BsmtUnfSF  :  1\n# TotalBsmtSF  :  1\n# BsmtFullBath  :  2\n# BsmtHalfBath  :  2\n# GarageYrBlt  :  78\n# GarageCars  :  1\n# GarageArea  :  1\n\n# Inspect Garage features to see if GarageYrBlt values are missing \n# because of No Garage value in their corresponding Garage features\n\n# Get all row indexes that have missing values for GarageYrBlt feature \ngarage_row_list = test.index[test['GarageYrBlt'].isnull()].tolist()\ngarage_row_list = garage_row_list + test.index[test['GarageCars'].isnull()].tolist()\ngarage_row_list = garage_row_list + test.index[test['GarageArea'].isnull()].tolist()\n# Get col indexes for Garage features\ngarage_col_list = [test.columns.get_loc(col) for col in \n                   ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageCars', 'GarageArea']]\nprint(\"Garage feature rows: \", garage_row_list)\nprint(\"Garage feature cols: \", garage_col_list)\n# We can observe below that GarageYrBlt feature has missing values\n# because its corresponding Garage features have No Garage(NaN) as their values \n# The GarageCars, GarageArea are missing but \n# their corresponding GarageFinish, GarageQual, GarageCond features have No Garage(NaN) values\nprint(test.iloc[garage_row_list, garage_col_list])\n\n# Corresponding No basement values can be observed below\nbsmt_row_list = test.index[test['BsmtFinSF1'].isnull()].tolist() + \\\n                test.index[test['BsmtFullBath'].isnull()].tolist()\nprint(\"BsmtFinSF1\", bsmt_row_list) \nprint(test.iloc[bsmt_row_list, [31,30,34,36,37,38,47,48]])","40b7d6de":"# Use mean value to impute the mising values in two numeric columns: LotFrontage, MasVnrArea\nfor col in ['LotFrontage', 'MasVnrArea']:\n    test[col] = test[col].fillna(test[col].mean())","7ec67e4a":"# Impute 0 for missing values of numerical feature GarageYrBlt as the corresponding GarageType value is No Garage(NaN)\nfor col in ['GarageYrBlt', 'GarageCars', 'GarageArea']:\n    test[col] = test[col].fillna(0)","27f3dc64":"# Impute 0 for missing values of numerical Bsmt features as the corresponding Bsmt values are No Basement(NaN)\nfor col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    test[col] = test[col].fillna(0)","8a0a8912":"# As per the given description of the dataset, not all columns \n# with NaN values are considered as missing values\n# NaN is used as an option in categorical data for most of the string features\n# Observe unique values for all string features to check how many are categorical\n# Observation suggest all columns are categorical so we can apply an encoding scheme\n# And consider NaN values in the categorical features as an option which is \n# mentioned in the data description\nprint(\"Feature Name\", \"Unique Values\")\nfor col in string_cols:\n    print(col, train[col].unique())","c11eb802":"# Handling string missing values for train data\n# Alley  :  1369 \"NA-No alley access\"\n# MasVnrType  :  8 \"None-None\"\n# BsmtQual  :  37 \"NA-No basement\"\n# BsmtCond  :  37 \"NA-No basement\"\n# BsmtExposure  :  38 \"NA-No basement\"\n# BsmtFinType1  :  37 \"NA-No basement\"\n# BsmtFinType2  :  38 \"NA-No basement\"\n# Electrical  :  1\n# FireplaceQu  :  690, \"NA-No Fireplace\"\n# GarageType  :  81 \"NA-No Garage\"\n# GarageFinish  :  81 \"NA-No Garage\"\n# GarageQual  :  81 \"NA-No Garage\"\n# GarageCond  :  81 \"NA-No Garage\"\n# PoolQC  :  1453 \"NA-No Pool\"\n# Fence  :  1179 \"NA-No Fence\"\n# MiscFeature  :  1406 \"NA-None\"\n\n# Replace all NaN values in the string features with None except Electrical\nfor col in [strmc for strmc in string_missing_cols_train if strmc != \"Electrical\"]:\n    train[col] = train[col].fillna(\"None\")","1137c452":"# Since Electrical has only 1 missing value, we can use mode to replace it which is SBrkr\ntrain['Electrical'] = train[\"Electrical\"].fillna(train[\"Electrical\"].mode()[0])","1e372026":"# Intermediate copy\ntrain1 = train.copy()\ntest1 = test.copy()","3dab878a":"# Handling string missing values for train data\n# MSZoning  :  4\n# Alley  :  1352 \"NA-No alley access\"\n# Utilities  :  2\n# Exterior1st  :  1\n# Exterior2nd  :  1\n# MasVnrType  :  16 \"None-None\"\n# BsmtQual  :  44 \"NA-No basement\"\n# BsmtCond  :  45 \"NA-No basement\"\n# BsmtExposure  :  44 \"NA-No basement\"\n# BsmtFinType1  :  42 \"NA-No basement\"\n# BsmtFinType2  :  42 \"NA-No basement\"\n# KitchenQual  :  1\n# Functional  :  2 (Assume typical unless deductions are warranted)\n# FireplaceQu  :  730 \"NA-No Fireplace\"\n# GarageType  :  76 \"NA-No Garage\"\n# GarageFinish  :  78 \"NA-No Garage\"\n# GarageQual  :  78 \"NA-No Garage\"\n# GarageCond  :  78 \"NA-No Garage\"\n# PoolQC  :  1456 \"NA-No Pool\"\n# Fence  :  1169 \"NA-No Fence\"\n# MiscFeature  :  1408 \"NA-None\"\n# SaleType  :  1\n\n# Replace all NaN values in the string features with None \n# except MSZoning, Utilities, Exterior1st, Exterior2nd, KitchenQual, Functional, SaleType\nfor col in [strmc for strmc in string_missing_cols_test if strmc not in \n            ['MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType']]:\n    test1[col] = test1[col].fillna(\"None\")","13ed5f0a":"# Impute Typ value for missing Functional values based on data description\ntest1['Functional'] = test1['Functional'].fillna('Typ')","db267fd6":"# Impute mode value for remaining categorical features\ntest1['MSZoning'] = test1['MSZoning'].fillna(test1['MSZoning'].mode()[0])\ntest1['Utilities'] = test1['Utilities'].fillna(test1['Utilities'].mode()[0])\ntest1['Exterior1st'] = test1['Exterior1st'].fillna(test1['Exterior1st'].mode()[0])\ntest1['KitchenQual'] = test1['KitchenQual'].fillna(test1['KitchenQual'].mode()[0])\ntest1['SaleType'] = test1['SaleType'].fillna(test1['SaleType'].mode()[0])\n# Use the 2nd most frequent option: MetalSd \ntest1['Exterior2nd'] = test1['Exterior2nd'].fillna('MetalSd')","3412e81f":"# Encode categorical features using LabelEncoder\nle = LabelEncoder()\nfor col in string_cols:\n    train1[col] = le.fit_transform(train1[col])\n    test1[col] = le.fit_transform(test1[col])\n\nprint(train1[string_cols].head())\nprint(test1[string_cols].head())","9912f5e2":"train1 = train1.astype(\"int64\")\ntest1 = test1.astype(\"int64\")","63846f61":"# Find correlation after handling categorical values\ncor1 = train1.corr()\ncorr_target1 = abs(cor1['SalePrice'])\n\nrelevant_feature_names1 = []\nfor col, value in corr_target1[corr_target1>0.3].iteritems():\n    relevant_feature_names1.append(col)\n\nrelevant_feature_names1","db33c110":"# Set _has_statsmodels to False to plot features that have data not well represented using gaussian distribution\n# In this dataset, these features have a lot of zero values or a lot of indexes with same values\n# These features don't work well with regression\n# Data is not continous so it is treated as categorical\n# sns.distributions._has_statsmodels = False\n\n# sns.jointplot(x='BsmtFinSF2', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='LowQualFinSF', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='BsmtHalfBath', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='KitchenAbvGr', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='EnclosedPorch', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='3SsnPorch', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='ScreenPorch', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='PoolArea', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='MiscVal', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n\n# plt.show()","1131f67a":"# Joint plots for numerical features\n# sns.jointplot(x='Id', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='MSSubClass', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='LotFrontage', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='LotArea', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='OverallQual', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='OverallCond', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='YearBuilt', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='YearRemodAdd', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='MasVnrArea', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='BsmtFinSF1', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='BsmtUnfSF', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='TotalBsmtSF', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='1stFlrSF', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='2ndFlrSF', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='GrLivArea', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='BsmtFullBath', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='FullBath', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='HalfBath', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='BedroomAbvGr', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='TotRmsAbvGrd', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='Fireplaces', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='GarageYrBlt', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='GarageCars', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='GarageArea', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='WoodDeckSF', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='OpenPorchSF', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='MoSold', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n# sns.jointplot(x='YrSold', y='SalePrice', data=train1[numeric_cols], kind = 'reg', height = 5)\n\n# plt.show()","0f681392":"# Numeric Features that show linearity with SalePrice\n# We can remove outliers from these columns to improve regression result\n\n# ['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'MasVnrArea', 'BsmtFinSF1','BsmtUnfSF', 'TotalBsmtSF', \n#  '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n#  'GarageArea', 'WoodDeckSF', 'OpenPorchSF']","a4a0c366":"# Submission['SalePrice'] will be used as y_test.\n# This is done to calculate the test metrics and get an idea about the results.\n# The answers of the metrics do not represent their actual values as the real y_test may be different\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission.head()","fe61af61":"# Check the variance of all numerical features\n# Also check the normalized variance\n# We can observe that the normalized variance looks better\npd.DataFrame([train1[numeric_cols].var(),np.log1p(train1[numeric_cols]).var()])","9d105a0c":"# Standard Scalar. If data is not scaled we can get negative score\n\n# Add SalePrice to test data for scaling\n# test_temp = test1.copy()\n# test_temp['SalePrice'] = submission['SalePrice']\n\nsscaler = StandardScaler()\nscaled_train = pd.DataFrame(sscaler.fit(train1.drop([\"SalePrice\"], axis = 1))\n                            .transform(train1.drop([\"SalePrice\"], axis = 1)),\n                            columns=train1.drop([\"SalePrice\"], axis = 1).columns)\nscaled_test = pd.DataFrame(sscaler.transform(test1), columns=test1.columns)","e16aeb30":"# Minmax Scalar. If data is not scaled we can get negative score\n\n# Add SalePrice to test data for scaling\n# test_temp = test1.copy()\n# test_temp['SalePrice'] = submission['SalePrice']\n\nmmscaler = MinMaxScaler()\nscaled_train = pd.DataFrame(mmscaler.fit(train1.drop([\"SalePrice\"], axis = 1))\n                            .transform(train1.drop([\"SalePrice\"], axis = 1)),\n                            columns=train1.drop([\"SalePrice\"], axis = 1).columns)\nscaled_test = pd.DataFrame(mmscaler.transform(test1), columns=test1.columns)","4dc19b66":"# Check the variance of all numerical features\n# Also check the scaled variance\npd.DataFrame([train1[numeric_cols].var(),(scaled_train[[col for col in numeric_cols if col!=\"SalePrice\"]]).var()])","18a6450d":"# Test 8- Normalized data\n# X_train = np.log1p(train1[['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'MasVnrArea', 'BsmtFinSF1','BsmtUnfSF', 'TotalBsmtSF', \n#  '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n#  'GarageArea', 'WoodDeckSF', 'OpenPorchSF']])\n# y_train = np.log1p(train1[\"SalePrice\"])\n# X_test = np.log1p(test1[['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'MasVnrArea', 'BsmtFinSF1','BsmtUnfSF', 'TotalBsmtSF', \n#  '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n#  'GarageArea', 'WoodDeckSF', 'OpenPorchSF']])\n# y_test = np.log1p(submission['SalePrice'])","dd311042":"# Test 6- Scale\nX_train = scaled_train.drop([\"Id\"], axis = 1)\ny_train = np.log1p(train1[\"SalePrice\"])\nX_test = scaled_test.drop([\"Id\"], axis = 1)\ny_test = np.log1p(submission['SalePrice'])","517064da":"# Test 5- Linear numeric features from joint plot\n# X_train = train1[['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'MasVnrArea', 'BsmtFinSF1','BsmtUnfSF', 'TotalBsmtSF', \n#  '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n#  'GarageArea', 'WoodDeckSF', 'OpenPorchSF']]\n# y_train = np.log1p(train1[\"SalePrice\"])\n\n# X_test = test1[['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'MasVnrArea', 'BsmtFinSF1','BsmtUnfSF', 'TotalBsmtSF', \n#  '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', \n#  'GarageArea', 'WoodDeckSF', 'OpenPorchSF']]\n# y_test = np.log1p(submission['SalePrice'])","88f9d00a":"# Test 4- Correlation numerical and categorical\n# X_train = train1[['LotFrontage','OverallQual','YearBuilt','YearRemodAdd','MasVnrArea','ExterQual','Foundation','BsmtQual','BsmtExposure','BsmtFinSF1','TotalBsmtSF','HeatingQC','1stFlrSF','2ndFlrSF','GrLivArea','FullBath','KitchenQual','TotRmsAbvGrd','Fireplaces','GarageType','GarageFinish','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF']].astype('int64')\n# y_train = np.log1p(train1[\"SalePrice\"].astype('int64'))\n# X_test = test1[['LotFrontage','OverallQual','YearBuilt','YearRemodAdd','MasVnrArea','ExterQual','Foundation','BsmtQual','BsmtExposure','BsmtFinSF1','TotalBsmtSF','HeatingQC','1stFlrSF','2ndFlrSF','GrLivArea','FullBath','KitchenQual','TotRmsAbvGrd','Fireplaces','GarageType','GarageFinish','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF']].astype('int64')\n# y_test = np.log1p(submission['SalePrice'].astype('int64'))","aadf22b4":"# Test 3- Feature importance\n# X_train = train1[['PoolArea','MasVnrType','BsmtFinType2','LowQualFinSF','BsmtFinSF2','PoolQC']].astype('int64')\n# y_train = np.log1p(train1[\"SalePrice\"].astype('int64'))\n# X_test = test1[['PoolArea','MasVnrType','BsmtFinType2','LowQualFinSF','BsmtFinSF2','PoolQC']].astype('int64')\n# y_test = np.log1p(submission['SalePrice'].astype('int64'))","91c01b66":"# Test 2- Correlation only numerical\n# X_train = train1[['LotFrontage','OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF','OpenPorchSF']].astype('int64')\n# y_train = np.log1p(train1[\"SalePrice\"].astype('int64'))\n# X_test = test1[['LotFrontage','OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF','OpenPorchSF']].astype('int64')\n# y_test = np.log1p(submission['SalePrice'].astype('int64'))","b8a4f418":"# Test 1- Only handle missing values\n# X_train = train1.drop([\"Id\",\"SalePrice\"], axis = 1)\n# y_train = np.log1p(train1[\"SalePrice\"])\n# X_test = test1.drop([\"Id\"], axis = 1)\n# y_test = np.log1p(submission['SalePrice'])","fee983c8":"# Model Building\nlr = LinearRegression()\n\n# lr_selector = RFE(lr, n_features_to_select=20)\n# lr_selector = lr_selector.fit(X_train, y_train)\n\n# lr_train_score = lr_selector.score(X_train, y_train)\n# lr_test_score = lr_selector.score(X_test, y_test)\n# lr_train_predict = lr_selector.predict(X_train)\n# lr_test_predict = lr_selector.predict(X_test)\n\nlr.fit(X_train, y_train)\n\nlr_train_predict = lr.predict(X_train)\nlr_test_predict = lr.predict(X_test)\nlr_train_score = r2_score(y_train,lr_train_predict)\nlr_test_score = r2_score(y_test,lr_test_predict)\n# lr_train_score = lr.score(X_train, y_train)\n# lr_test_score = lr.score(X_test, y_test)\n","790774b8":"# Get the selected column names\nfor i in range(X_train.shape[1]):\n    if(lr_selector.support_[i] == True):\n        print('Column: %d, %s, Selected %s, Rank: %.3f' % (i,X_train.columns[i], lr_selector.support_[i], lr_selector.ranking_[i]))","25a3f99a":"rr = Ridge()\nrr_selector = RFE(rr, n_features_to_select=20)\nrr_selector = rr_selector.fit(X_train, y_train)\n\nrr_train_score = rr_selector.score(X_train, y_train)\nrr_test_score = rr_selector.score(X_test, y_test)\nrr_train_predict = rr_selector.predict(X_train)\nrr_test_predict = rr_selector.predict(X_test)\n\n# rr.fit(X_train, y_train)\n\n# rr_train_score = rr.score(X_train, y_train)\n# rr_test_score = rr.score(X_test, y_test)\n# rr_train_predict = rr.predict(X_train)\n# rr_test_predict = rr.predict(X_test)","982f1d29":"# Get the selected column names\nfor i in range(X_train.shape[1]):\n    if(rr_selector.support_[i] == True):\n        print('Column: %d, %s, Selected %s, Rank: %.3f' % (i,X_train.columns[i], rr_selector.support_[i], rr_selector.ranking_[i]))","80f634f3":"lar = Lasso()\nlar_selector = RFE(lar, n_features_to_select=20)\nlar_selector = lar_selector.fit(X_train, y_train)\n\nlar_train_score = lar_selector.score(X_train, y_train)\nlar_test_score = lar_selector.score(X_test, y_test)\nlar_train_predict = lar_selector.predict(X_train)\nlar_test_predict = lar_selector.predict(X_test)\n\n# lar.fit(X_train, y_train)\n\n# lar_train_score = lar.score(X_train, y_train)\n# lar_test_score = lar.score(X_test, y_test)\n# lar_train_predict = lar.predict(X_train)\n# lar_test_predict = lar.predict(X_test)","3e5cb991":"# Get the selected column names\nfor i in range(X_train.shape[1]):\n    if(lar_selector.support_[i] == True):\n        print('Column: %d, %s, Selected %s, Rank: %.3f' % (i,X_train.columns[i], lar_selector.support_[i], lar_selector.ranking_[i]))","2c96a2b6":"bar = BayesianRidge(n_iter=1000)\nbar_selector = RFE(bar, n_features_to_select=20)\nbar_selector = bar_selector.fit(X_train, y_train)\n\nbar_train_score = bar_selector.score(X_train, y_train)\nbar_test_score = bar_selector.score(X_test, y_test)\nbar_train_predict = bar_selector.predict(X_train)\nbar_test_predict = bar_selector.predict(X_test)\n\n# bar.fit(X_train, y_train)\n\n# bar_train_score = bar.score(X_train, y_train)\n# bar_test_score = bar.score(X_test, y_test)\n# bar_train_predict = bar.predict(X_train)\n# bar_test_predict = bar.predict(X_test)","0179504d":"# Get the selected column names\nfor i in range(X_train.shape[1]):\n    if(bar_selector.support_[i] == True):\n        print('Column: %d, %s, Selected %s, Rank: %.3f' % (i,X_train.columns[i], bar_selector.support_[i], bar_selector.ranking_[i]))","2db7ce4c":"hr = HuberRegressor(max_iter=3000)\n# hr_selector = RFE(hr, n_features_to_select=20)\n# hr_selector = hr_selector.fit(X_train, y_train)\n\n# hr_train_score = hr_selector.score(X_train, y_train)\n# hr_test_score = hr_selector.score(X_test, y_test)\n# hr_train_predict = hr_selector.predict(X_train)\n# hr_test_predict = hr_selector.predict(X_test)\n\nhr.fit(X_train, y_train)\n\nhr_train_score = hr.score(X_train, y_train)\nhr_test_score = hr.score(X_test, y_test)\nhr_train_predict = hr.predict(X_train)\nhr_test_predict = hr.predict(X_test)","f0148e59":"rfr = RandomForestRegressor(n_estimators=100, max_depth=5, n_jobs=-1, random_state=0)\nrfr_selector = RFE(rfr, n_features_to_select=15)\nrfr_selector = rfr_selector.fit(X_train, y_train)\n\nrfr_train_score = rfr_selector.score(X_train, y_train)\nrfr_test_score = rfr_selector.score(X_test, y_test)\nrfr_train_predict = rfr_selector.predict(X_train)\nrfr_test_predict = rfr_selector.predict(X_test)\n\n# rfr.fit(X_train, y_train)\n\n# rfr_train_score = rfr.score(X_train, y_train)\n# rfr_test_score = rfr.score(X_test, y_test)\n# rfr_train_predict = rfr.predict(X_train)\n# rfr_test_predict = rfr.predict(X_test)","bb4b46e1":"# Get the selected column names\nfor i in range(X_train.shape[1]):\n    if(rfr_selector.support_[i] == True):\n        print('Column: %d, %s, Selected %s, Rank: %.3f' % (i,X_train.columns[i], rfr_selector.support_[i], rfr_selector.ranking_[i]))","131def77":"importances = rfr.feature_importances_\nindices = np.argsort(importances)\nindices = indices[1:10]\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","fe6af009":"abr = AdaBoostRegressor(n_estimators=100, random_state=0)\nabr_selector = RFE(abr, n_features_to_select=15)\nabr_selector = abr_selector.fit(X_train, y_train)\n\nabr_train_score = abr_selector.score(X_train, y_train)\nabr_test_score = abr_selector.score(X_test, y_test)\nabr_train_predict = abr_selector.predict(X_train)\nabr_test_predict = abr_selector.predict(X_test)\n\n# abr.fit(X_train, y_train)\n\n# abr_train_score = abr.score(X_train, y_train)\n# abr_test_score = abr.score(X_test, y_test)\n# abr_train_predict = abr.predict(X_train)\n# abr_test_predict = abr.predict(X_test)","f6459df9":"# Get the selected column names\nfor i in range(X_train.shape[1]):\n    if(abr_selector.support_[i] == True):\n        print('Column: %d, %s, Selected %s, Rank: %.3f' % (i,X_train.columns[i], abr_selector.support_[i], abr_selector.ranking_[i]))","dbda44f9":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01, max_depth=3, loss='huber', random_state=0)\n# gbr_selector = RFE(gbr, n_features_to_select=20)\n# gbr_selector = gbr_selector.fit(X_train, y_train)\n\n# gbr_train_score = gbr_selector.score(X_train, y_train)\n# gbr_test_score = gbr_selector.score(X_test, y_test)\n# gbr_train_predict = gbr_selector.predict(X_train)\n# gbr_test_predict = gbr_selector.predict(X_test)\n\ngbr.fit(X_train, y_train)\n\ngbr_train_score = gbr.score(X_train, y_train)\ngbr_test_score = gbr.score(X_test, y_test)\ngbr_train_predict = gbr.predict(X_train)\ngbr_test_predict = gbr.predict(X_test)","3e8639d2":"xgbr = xgb.XGBRegressor(objective ='reg:squarederror', learning_rate = 0.01,\n                max_depth = 3, n_estimators = 6000)\n# xgbr_selector = RFE(xgbr, n_features_to_select=20)\n# xgbr_selector = xgbr_selector.fit(X_train, y_train)\n\n# xgbr_train_score = xgbr_selector.score(X_train, y_train)\n# xgbr_test_score = xgbr_selector.score(X_test, y_test)\n# xgbr_train_predict = xgbr_selector.predict(X_train)\n# xgbr_test_predict = xgbr_selector.predict(X_test)\n\nxgbr.fit(X_train, y_train)\n\nxgbr_train_score = xgbr.score(X_train, y_train)\nxgbr_test_score = xgbr.score(X_test, y_test)\nxgbr_train_predict = xgbr.predict(X_train)\nxgbr_test_predict = xgbr.predict(X_test)","79ec55c9":"# Takes a bit of time to execute\n# estimators = [('lr',lr),('rr',rr),('lar',lar),('bar',bar), ('hr',hr), ('abr',abr), ('gbr',gbr), ('xgbr',xgbr)]\nestimators = [('lr',lr),('rr',rr),('lar',lar),('bar',bar), ('abr',abr)]\n# estimators = [('lr',lr),('rr',rr),('bar',bar),('rfr',rfr), ('abr',abr),('gbr',gbr)]\n# estimators = [('bar',bar),('rfr',rfr),('gbr',gbr)]\nsr = StackingRegressor(estimators=estimators,final_estimator=rfr)\nsr.fit(X_train, y_train)\n\nsr_train_score = sr.score(X_train, y_train)\nsr_test_score = sr.score(X_test, y_test)\nsr_train_predict = sr.predict(X_train)\nsr_test_predict = sr.predict(X_test)","0ce0314f":"# Min blended results\ndef min_blended_predictions(y_test, sr_test_predict, lr_test_predict, rr_test_predict,\n                           lar_test_predict, bar_test_predict, hr_test_predict, rfr_test_predict, abr_test_predict,\n                           xgbr_test_predict, gbr_test_predict):\n    blended_prediction = []\n    diff_list_index = []\n    diff1 = abs(sr_test_predict)\n    diff2 = abs(lr_test_predict)\n    diff3 = abs(rr_test_predict)\n    diff4 = abs(lar_test_predict)\n    diff5 = abs(bar_test_predict)\n    diff6 = abs(hr_test_predict)\n    diff7 = abs(rfr_test_predict)\n    diff8 = abs(abr_test_predict)\n    diff9 = abs(xgbr_test_predict)\n    diff10 = abs(gbr_test_predict)\n    for i in range(0, len(diff1)):\n        diff_list = [diff1[i],diff2[i],diff3[i],diff4[i],diff5[i],diff6[i],diff7[i],diff8[i],diff9[i],diff10[i]]\n        diff_index = diff_list.index(min(diff_list))\n        if(diff_index == 0):\n            blended_prediction.append(sr_test_predict[i])\n        elif(diff_index == 1):\n            blended_prediction.append(lr_test_predict[i])\n        elif(diff_index == 2):\n            blended_prediction.append(rr_test_predict[i])\n        elif(diff_index == 3):\n            blended_prediction.append(lar_test_predict[i])\n        elif(diff_index == 4):\n            blended_prediction.append(bar_test_predict[i])\n        elif(diff_index == 5):\n            blended_prediction.append(hr_test_predict[i])\n        elif(diff_index == 6):\n            blended_prediction.append(rfr_test_predict[i])\n        elif(diff_index == 7):\n            blended_prediction.append(abr_test_predict[i])\n        elif(diff_index == 8):\n            blended_prediction.append(xgbr_test_predict[i])\n        elif(diff_index == 9):\n            blended_prediction.append(gbr_test_predict[i])\n        diff_list_index.append(diff_index)\n    return blended_prediction","7835f120":"mb_train_predict = min_blended_predictions(y_train, sr_train_predict, lr_train_predict, rr_train_predict,\n                           lar_train_predict, bar_train_predict, hr_train_predict, rfr_train_predict, abr_train_predict,\n                           xgbr_train_predict, gbr_train_predict)\n\nmb_test_predict = min_blended_predictions(y_test, sr_test_predict, lr_test_predict, rr_test_predict,\n                           lar_test_predict, bar_test_predict, hr_test_predict, rfr_test_predict, abr_test_predict,\n                           xgbr_test_predict, gbr_test_predict)","9bb05424":"print(\"Train RMSLE Blended\", np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(mb_train_predict))))\nprint(\"Test RMSLE Blended\", np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(mb_test_predict))))","d579de9a":"regressor_comparison_df = pd.DataFrame({'metrics': ['Train Score', 'Test Score', 'Train RMSE','Test RMSE','Train MSLE','Test MSLE','Train RMSLE', 'Test RMSLE'],\n                             'Stacking Regression':[sr_train_score, sr_test_score, mean_squared_error(np.expm1(y_train), np.expm1(sr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(sr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(sr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(sr_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(sr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(sr_test_predict)))],\n                             'Linear Regression':[lr_train_score, lr_test_score, mean_squared_error(np.expm1(y_train), np.expm1(lr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(lr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(lr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(lr_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(lr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(lr_test_predict)))],\n                             'Ridge Regression':[rr_train_score, rr_test_score, mean_squared_error(np.expm1(y_train), np.expm1(rr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(rr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(rr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(rr_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(rr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(rr_test_predict)))],\n                             'Lasso Regression':[lar_train_score, lar_test_score,mean_squared_error(np.expm1(y_train), np.expm1(lar_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(lar_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(lar_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(lar_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(lar_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(lar_test_predict)))],\n                             'Bayseian Regression':[bar_train_score, bar_test_score,mean_squared_error(np.expm1(y_train), np.expm1(bar_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(bar_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(bar_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(bar_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(bar_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(bar_test_predict)))],\n                             'Huber Regression':[hr_train_score, hr_test_score,mean_squared_error(np.expm1(y_train), np.expm1(hr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(hr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(hr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(hr_test_predict)),np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(hr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(hr_test_predict)))],\n                             'Random Forest Regression':[rfr_train_score, rfr_test_score,mean_squared_error(np.expm1(y_train), np.expm1(rfr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(rfr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(rfr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(rfr_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(rfr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(rfr_test_predict)))],\n                             'AdaBoost Regression':[abr_train_score, abr_test_score,mean_squared_error(np.expm1(y_train), np.expm1(abr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(abr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(abr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(abr_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(abr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(abr_test_predict)))],\n                             'XGB Regression':[xgbr_train_score, xgbr_test_score,mean_squared_error(np.expm1(y_train), np.expm1(xgbr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(xgbr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(xgbr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(xgbr_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(xgbr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(xgbr_test_predict)))],\n                             'GB Regression':[gbr_train_score, gbr_test_score,mean_squared_error(np.expm1(y_train), np.expm1(gbr_train_predict), squared=False), mean_squared_error(np.expm1(y_test), np.expm1(gbr_test_predict), squared=False), mean_squared_log_error(np.expm1(y_train), np.expm1(gbr_train_predict)), mean_squared_log_error(np.expm1(y_test), np.expm1(gbr_test_predict)), np.sqrt(mean_squared_log_error(np.expm1(y_train), np.expm1(gbr_train_predict))), np.sqrt(mean_squared_log_error(np.expm1(y_test), np.expm1(gbr_test_predict)))]\n                             })\nregressor_comparison_df","9c63b91b":"final_SalePrice_prediction = mb_test_predict","25370795":"# Display first 10 actual and prediction SalePrice of train and test data for final prediction\npd.concat([np.expm1(y_train[0:10]),\n           pd.Series(np.expm1(mb_train_predict[0:10])),\n          np.expm1(y_test[0:10]),\n          pd.Series(np.expm1(final_SalePrice_prediction[0:10]))], \n          axis=1, \n          keys=[\"Train Actual Value\", \"Train Predicted Value\", \"Test Actual Value\", \"Test Predicted Value\"])","6c5e4cd2":"# Update SalePrice column of submission dataframe with final prediction values\nsubmission['SalePrice'] = np.expm1(final_SalePrice_prediction)\nsubmission.head()","10d85c60":"# Save the final SalePrice submission file in the output directory of kaggle\nsubmission.to_csv(\"HousePriceSubmission.csv\", index=False)","f979a125":"**To this point, all missing values have been removed. Now we'll work on encoding categorical features for both train and test data**"}}