{"cell_type":{"f7db8d1d":"code","7c1f28b5":"code","8e43fb73":"code","bc21311f":"code","12a34a44":"code","1b7765d1":"code","af8a9b27":"code","515b2c1d":"code","461cdf00":"code","ec6ed98b":"code","c74a49e0":"code","50acdd1a":"markdown","19344451":"markdown","71cf54df":"markdown","b3b94c65":"markdown","476716df":"markdown","003d8a33":"markdown","1650d62e":"markdown","2e07f267":"markdown","c3d8d81e":"markdown"},"source":{"f7db8d1d":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\nfrom keras.models import Sequential\nfrom keras.layers import InputLayer, Conv2D, MaxPool2D, Flatten, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import to_categorical\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping\nimport keras","7c1f28b5":"train_set = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_set = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","8e43fb73":"X_train = train_set.drop(\"label\", axis=1)\ny_train = train_set[\"label\"]\n\nX_train = X_train \/ 255.\nX_test = test_set \/ 255.\n\nX_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)\n\n# label encoded to one hot vectors\ny_train = to_categorical(y_train, num_classes=10)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)","bc21311f":"def build_model(hidden_layers=1, feature_maps=16, kernel_size=3, n_neurons=32, dropout=0.1):\n    model = Sequential([])\n    model.add(InputLayer(input_shape=(28, 28, 1)))\n    for n in range(hidden_layers):\n        model.add(Conv2D((n + 1) * feature_maps, kernel_size, activation=\"relu\", padding=\"same\"))\n        model.add(Conv2D((n + 1) * feature_maps, kernel_size, activation=\"relu\", padding=\"same\"))\n        model.add(MaxPool2D())\n        model.add(Dropout(dropout))   \n    model.add(Flatten())\n    model.add(Dense(n_neurons))\n    model.add(Dropout(dropout))\n    model.add(Dense(10, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    return model","12a34a44":"keras.backend.clear_session()\nnp.random.seed(42)\n\nkeras_clf = KerasClassifier(build_model)\n\nparams = {\n    \"hidden_layers\": [1, 2, 3],\n    \"feature_maps\": [16, 24, 32],\n    \"n_neurons\": [64, 128, 256],\n    \"dropout\": [0.2, 0.3, 0.4]\n}\nbatch_size = 64\n\nsearch_cv = RandomizedSearchCV(keras_clf, params, n_iter=15, cv=3, verbose=2)\nsearch_cv.fit(X_train, y_train, epochs=30,\n              validation_data=(X_valid, y_valid),\n              callbacks=EarlyStopping(patience=7),\n              batch_size=batch_size,\n              verbose=0)","1b7765d1":"model = search_cv.best_estimator_.model\nmodel.summary()","af8a9b27":"search_cv.best_score_","515b2c1d":"n_epochs = 30\ns = n_epochs * len(X_train) \/\/ batch_size # number of steps in n_epochs epochs\nlearning_rate = keras.optimizers.schedules.ExponentialDecay(0.001, s, 0.1)\n\nimg_generator = ImageDataGenerator(\n    rotation_range=0.1,\n    zoom_range=0.1,\n    width_shift_range=0.1, \n    height_shift_range=0.1\n)\n\nmodel.compile(optimizer=Adam(learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = model.fit(img_generator.flow(X_train, y_train, batch_size=batch_size, seed=42),\n                   epochs=n_epochs, validation_data=(X_valid, y_valid),\n                   callbacks=[EarlyStopping(patience=7)])","461cdf00":"train_set = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_set = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nX_train = train_set.drop(\"label\", axis=1)\ny_train = train_set[\"label\"]\nX_train = X_train \/ 255.\nX_test = test_set \/ 255.\nX_train = X_train.values.reshape(-1, 28, 28, 1)\nX_test = X_test.values.reshape(-1, 28, 28, 1)\ny_train = to_categorical(y_train, num_classes=10)\n\nnets = 8\nensemble = [search_cv.best_estimator_.model for _ in range(nets)]\nhistory = [0] * nets\n\nfor j, clf in enumerate(ensemble):\n    X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_train, y_train, test_size=0.15, random_state=42)   \n    clf.compile(optimizer=Adam(learning_rate), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    history[j] = clf.fit(img_generator.flow(X_train2, y_train2, batch_size=batch_size, seed=42),\n                         epochs=n_epochs, validation_data=(X_valid2, y_valid2), verbose=0)\n    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n          j+1,n_epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))","ec6ed98b":"from scipy.stats import mode\n\ny_pred = np.empty([nets, len(X_test)])\n\nfor clf_index, clf in enumerate(ensemble):\n    y_pred[clf_index] = np.argmax(clf.predict(X_test), axis=1)\n\ny_pred_majority_votes, n_votes = mode(y_pred, axis=0)","c74a49e0":"results = y_pred_majority_votes.reshape([-1]).astype(int)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"MNIST-CNN-ENSEMBLE.csv\",index=False)","50acdd1a":"# Loading, preprocessing, and split the data","19344451":"We will use randomized search rather than grid search because there are many hyperparameters and the model may perform slightly better so it's not worth the computational cost.\n\nIn order to do RandomizedSearchCV we need to wrap our keras model using a KerasClassifier class.","71cf54df":"# Import libraries","b3b94c65":"A typical CNN architecture generally stack few convolutional layers and pooling layer, then repeat the operation several times.\nLet's start by creating a function that build and compile our keras model.","476716df":"# Build the model","003d8a33":"# Data augmentation\n\nWe will use the Keras' `ImageDataGenerator` class to apply on-the-fly data augmentation. Note that this class only returns the randomly transformed training data.\nTo learn more about data augmentation and the Keras' `ImageDataGenerator` class please read:\n* The blog post [Keras ImageDataGenerator and Data Augmentation\n](https:\/\/www.pyimagesearch.com\/2019\/07\/08\/keras-imagedatagenerator-and-data-augmentation\/)\n* This stackoverflow question: https:\/\/stackoverflow.com\/questions\/51677788\/data-augmentation-in-pytorch","1650d62e":"In this notebook, I am going to try to build a model that reaches above 0.995 scores on kaggle's leaderboard (top 10%). To do so, we first need to pick the best CNN architecture using RandomizedSearchCV. Also we have to try data augmentation, dropout, and use a learning schedule.","2e07f267":"# Ensemble Learning\n\nHere we are going to try to combine 8 CNN to have a better classifier. We predict the class that gets the most votes.\n\nHopefully, this will give us a slightly better accuracy than our first model.","c3d8d81e":"# References\n\nThe code of this notebook was inspired by the following good kernels and tutorials:\n\n* [How to choose CNN Architecture MNIST](https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist#Experiment-1)\n* [25 Million Images! [0.99757] MNIST](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist\/data#Accuracy=99.75%-using-25-Million-Training-Images!!)\n* [Introduction to CNN Keras - 0.997 (top 6%)](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6#3.-CNN)\n* Chapter 10 from the book Hands-on machine learning (Aur\u00e9lien G\u00e9ron)."}}