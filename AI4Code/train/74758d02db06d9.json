{"cell_type":{"b971fd3b":"code","8a6a84bd":"code","00d1efd7":"code","3f57bf6f":"code","6420f820":"code","0f759afe":"code","8063dab7":"code","39dd9041":"code","69142675":"code","7578fc46":"code","36b0ede7":"code","773f134d":"code","87d2fea8":"code","5ab0efdc":"code","f4cf31ea":"code","5945d290":"code","fc29e3b4":"code","d5e338b3":"code","9702e9cc":"markdown","e127993d":"markdown","5dd53a35":"markdown","fb6c57e0":"markdown","278ece94":"markdown","b81c0d45":"markdown"},"source":{"b971fd3b":"### imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split","8a6a84bd":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","00d1efd7":"from tqdm import tqdm\ndf_test=test.drop(['ID_code'], axis=1)\ndf_test = df_test.values\nunique_samples = []\nunique_count = np.zeros_like(df_test)\nfor feature in tqdm(range(df_test.shape[1])):\n    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\ndf_test_real = df_test[real_samples_indexes].copy()\ndf_test_real=pd.DataFrame(df_test_real)\ndf_test_real=df_test_real.add_prefix('var_')\ndf_test_real.head()","3f57bf6f":"train_value=train.drop(['ID_code', 'target'], axis=1)\ndf_combined=pd.concat([train_value, df_test_real])","6420f820":"df_combined.shape","0f759afe":"for i in range(25):\n    var='var_'+str(i)\n    if i%25==0:\n        print (i)\n    dictionary=df_combined[var].value_counts().to_dict()\n    train['count_'+var]=train[var].map(dictionary)\n    train['test_'+var]=train[var]*np.log2(train['count_'+var]+1)\n    train['test1_'+var]=train[var]\/np.log2(train['count_'+var]+1)\n    train['test2_'+var]=train[var]*(-np.log2(train['count_'+var]+1))\n    train.drop('count_'+var, inplace=True, axis=1)\n    dictionary1=df_test_real[var].value_counts().to_dict()\n    test['count_'+var]=test[var].map(dictionary)\n    test['test_'+var]=np.log2(test['count_'+var]+1)*test[var]\n    test['test1_'+var]=test[var]\/np.log2(test['count_'+var]+1)\n    test['test2_'+var]=test[var]*(-np.log2(test['count_'+var]+1))\n    test.drop('count_'+var, inplace=True, axis=1)","8063dab7":"ID_code=test['ID_code']\nX_test = test.drop(['ID_code'],axis = 1)\n#X_test=X_test[X_test.columns[:200].append(X_test.columns[400:])]\nX_test.head()","39dd9041":"y=train['target']\nX = train.drop(['target', 'ID_code'], axis=1)\n#X=X[X.columns[:200].append(X.columns[400:])]\nX.head()","69142675":"features = [c for c in X.columns if c not in ['ID_code', 'target']]","7578fc46":"features","36b0ede7":"from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb","773f134d":"from sklearn.metrics import make_scorer, accuracy_score,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nauc_scorer = make_scorer(roc_auc_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=auc_scorer)\ngrid_obj = grid_obj.fit(X.iloc[:1000,:], y[:1000])\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X.iloc[:1000,:], y[:1000])","87d2fea8":"predictions = clf.predict(X.iloc[:1000,:])\nprint(roc_auc_score(y[:1000], predictions))","5ab0efdc":"from sklearn.metrics import make_scorer, accuracy_score,roc_auc_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nauc_scorer = make_scorer(roc_auc_score)\n\n# Run the grid search\n\nrandom_search_obj = RandomizedSearchCV(clf, param_distributions=parameters,\n                                   n_iter=100, cv=5)\n\n#grid_obj = GridSearchCV(clf, parameters, scoring=auc_scorer)\nrandom_search_obj = random_search_obj.fit(X.iloc[:1000,:], y[:1000])\n\n# Set the clf to the best combination of parameters\nclf = random_search_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X.iloc[:1000,:], y[:1000])","f4cf31ea":"predictions = clf.predict(X.iloc[:1000,:])\nprint(roc_auc_score(y[:1000], predictions))","5945d290":"from hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n\n### make a scorer fn\nauc_scorer = make_scorer(roc_auc_score)\n\nX = X.iloc[:1000,:]\ny = y[:1000]\n\n### define obj\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n    score = cross_val_score(clf, X, y, scoring=auc_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\n### define search space\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),  ### quniform defines how values will be sampled\n    'max_depth': hp.quniform('max_depth', 1, 10, 1)            ### there are other parameteric distributions also available\n}\n\n### put together\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=3)   ### increase for best results","fc29e3b4":"print(\"Hyperopt estimated optimum {}\".format(best))","d5e338b3":"bestParams = {'n_estimators': best['n_estimators'],\n              'max_depth': best['max_depth']\n             }\nbestModel = RandomForestClassifier(n_estimators = int(best['n_estimators']),max_depth= int(best['max_depth']))\n\nbestModel = bestModel.fit(X,y)\n\npredictions = bestModel.predict(X)\nprint(roc_auc_score(y, predictions))","9702e9cc":"### What do you conclude from this analysis? ","e127993d":"### GridSearchCV","5dd53a35":"### Exercise: Validate the above score using k-fold CV","fb6c57e0":"### Bayesian Optimization\n\n- Link to original paper: https:\/\/papers.nips.cc\/paper\/4443-algorithms-for-hyper-parameter-optimization.pdf","278ece94":"- Observe how the randomsearchCV takes less time to execute but produces inferior results in this case.\n- Generally, it is okay to use randomsearchCV to save time and get approximately okayish results","b81c0d45":"### RandomSearchCV"}}