{"cell_type":{"a662bc92":"code","473c411c":"code","b7b58b64":"code","1630165f":"code","ec7a9172":"code","3691b4c5":"code","61d1f167":"code","45e23f64":"code","cf997e82":"code","b79ea3c2":"code","bb64033f":"code","8c424814":"code","5b76a7cc":"code","0e93afe7":"code","91eddbfd":"code","538f4735":"code","0afd6665":"code","b908bb6e":"markdown","bbef35a3":"markdown","edecc462":"markdown","b80cb511":"markdown","f779efcd":"markdown","08dc80a1":"markdown","ffb9ec41":"markdown","30e0ec15":"markdown","739097f4":"markdown","b66c4e80":"markdown","d96c19c3":"markdown","91a16285":"markdown","ce95d9c5":"markdown"},"source":{"a662bc92":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import linear_model\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","473c411c":"data = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\nprint(data.head())\nprint(data.shape)\nprint(data.dtypes)","b7b58b64":"data = data.drop(['id', 'date'], axis = 1)","1630165f":"import seaborn as sns\nsns.distplot( a = data[\"price\"], hist = True, kde = True, \n             kde_kws={\"color\": \"g\", \"alpha\":0.3, \"linewidth\": 5, \"shade\":True}\n            )\n","ec7a9172":"sns.lmplot(x = \"price\", y = \"sqft_living\", data = data, fit_reg = False)","3691b4c5":"p = sns.pairplot(data[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], palette='afmhot',height=1.4)\np.set(xticklabels=[])","61d1f167":"data.head()","45e23f64":"y = data.price.values\ndata = data.drop(['price'], axis = 1)\nX = data.to_numpy()\ncolnames = data.columns","cf997e82":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\nlinridge = Ridge(alpha = 20.0).fit(X_train, y_train)","b79ea3c2":"print(\"Training R Squared : {}\".format(linridge.score(X_train, y_train)))\nprint(\"Testing R Squared : {}\".format(linridge.score(X_test, y_test)))","bb64033f":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nclf = Ridge().fit(X_train_scaled, y_train)\nR_squared = clf.score(X_test_scaled, y_test)","8c424814":"R_squared","5b76a7cc":"print('Ridge Regression: Effect of alpha regularization paramater')\nfor this_alpha in [0, 1,  10, 20, 30, 50, 100, 250, 500]:\n    linridge = Ridge(alpha = this_alpha).fit(X_train, y_train)\n    r2_train = linridge.score(X_train, y_train)\n    r2_test = linridge.score(X_test, y_test)\n    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n    print('Alpha = {}\\n\\\n    num abs(coeff) > 1.0: {}, \\\n    r-squared training: {}, rsquared test: {}\\n '.format(this_alpha, num_coeff_bigger, r2_train, r2_test))","0e93afe7":"from sklearn.linear_model import Lasso\nlinlasso = Lasso(alpha= 1.0, max_iter = 10000).fit(X_train_scaled, y_train)\nprint(\"Training R Squared : {}\".format(linlasso.score(X_train_scaled, y_train)))\nprint(\"Testing R Squared : {}\".format(linlasso.score(X_test_scaled, y_test)))","91eddbfd":"print('Lasso Regression: Effect of alpha regularization paramater')\nfor alpha in [0.5, 1, 2,3,4,5,10,20,50]:\n    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n    r2_train = linlasso.score(X_train_scaled, y_train)\n    r2_test = linlasso.score(X_test_scaled, y_test)\n    \n    print('Alpha = {}\\n\\\n    Features kept: {}, r-squared training: {},\\\n    r-squared testing: {}\\n'.format(alpha, np.sum(linlasso.coef_ !=0), r2_train, r2_test))","538f4735":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree = 2)\nX_F1_poly = poly.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y, random_state = 0)","0afd6665":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression().fit(X_train, y_train)\nprint('poly degree 2 Linear model R squared training:{}'.format(linreg.score(X_train, y_train)))\nprint('poly degree 2 Linear model R squared test:{}'.format(linreg.score(X_test, y_test)))","b908bb6e":"Ridge regression uses the same least-squares criterion, but with one difference. During the training phase, it adds a penalty for feature weights. Let's see the comparison. ","bbef35a3":"Price seems to have directrelation with most of the variables","edecc462":"Now i will get rid of the variables not required in the modeling: id and date.","b80cb511":"Finding the best parameters","f779efcd":"We can see much improved R-Squared with polynomial model as it allows a better fit","08dc80a1":"Alpha = 5 gives the highest R-Squared on test data. ","ffb9ec41":"Lets perform Regularization and find the best R-squared for different alpha values! ","30e0ec15":"The LASSO Regression uses L1 regularization type to reduce error. LASSO is said to be used with lesser variables but with medium\/large effects. Ridge instead is used with many variables but small\/medium sized effects. Default alpha is 1.0 in Lasso","739097f4":"Lets do some Exploratory Analysis","b66c4e80":"We can observe the relation of price with different variables in one go too!","d96c19c3":"DEPLOYING POLYNOMIAL REGRESSION\nAdding extra polynomial features allows us a much richer set of complex functions that we can use to fit to the data. So you can think of this intuitively as allowing polynomials to be fit to the training data instead of simply a straight line, but using the same least-squares criterion that minimizes mean squared error.","91a16285":"at Alpha = 20, the R-Squared Test is the highest. ","ce95d9c5":"As we see, the R-Squared has gone down. The normalization is not effective if we have broad range of variables being proved by this example. "}}