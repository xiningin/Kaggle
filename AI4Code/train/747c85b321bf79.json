{"cell_type":{"fa9c16af":"code","0eb56864":"code","0b860e20":"code","688c2125":"code","29424d64":"code","21c60fc6":"code","b5b2b757":"code","8dd34c15":"code","3fde63e0":"code","70f6fc5b":"code","ab266edf":"code","e21f3642":"code","0e14a1e8":"code","756e7fe9":"code","6fcf4994":"code","5a226f21":"code","d9bc92ac":"code","c90f4c87":"code","40b52b5c":"code","b1567a7c":"code","b3aa7683":"code","a3926533":"code","9502472e":"code","bcd20538":"code","0fa9fbd3":"code","5caa68c5":"code","21d783e3":"code","74ca57d8":"code","85045a45":"code","f89ff76f":"code","fa808ef9":"markdown","3c7ba3d5":"markdown"},"source":{"fa9c16af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n!pip install transformers\n!pip install bert\n!pip install tensorflow-gpu","0eb56864":"max_seq_length = 128  # Your choice here.\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport bert\nimport math\nimport transformers\nfrom sklearn.model_selection import StratifiedKFold","0b860e20":"train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ntrain.head()","688c2125":"train['text']=train['text'].fillna(\" \")\ntest['text']=test['text'].fillna(\" \")\ntrain['selected_text']=train['selected_text'].fillna(\" \")","29424d64":"for i in range(len(train['sentiment'])):\n    if train['sentiment'][i]=='positive':\n        train['sentiment'][i]=0\n    elif train['sentiment'][i]=='neutral':\n        train['sentiment'][i]=1\n    elif train['sentiment'][i]=='negative':\n        train['sentiment'][i]=2\n        \nfor i in range(len(test['sentiment'])):\n    if test['sentiment'][i]=='positive':\n        test['sentiment'][i]=0\n    elif test['sentiment'][i]=='neutral':\n        test['sentiment'][i]=1\n    elif test['sentiment'][i]=='negative':\n        test['sentiment'][i]=2\ntrain.head()","21c60fc6":"train_x = train['text'].tolist()\n# train_x = np.array(train_x, dtype=object)[:, np.newaxis]\ntrain_y = train['selected_text'].tolist()\n\ntest_x = test['text'].tolist()\n# test_x = np.array(test_x, dtype=object)[:, np.newaxis]\n# test_y = test['selected_text'].tolist()","b5b2b757":"sentiment=train['sentiment'].tolist()","8dd34c15":"import re\nfrom sklearn.feature_extraction import text\nstop_words = text.ENGLISH_STOP_WORDS","3fde63e0":"for i in range(len(train_x)):\n    train_x[i]=re.sub(r'[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,.\/<>?]', ' ',train_x[i])\n    train_x[i]=re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', ' ', train_x[i], flags=re.MULTILINE)","70f6fc5b":"for i in range(len(test_x)):\n    test_x[i]=re.sub(r'[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,.\/<>?]',' ',test_x[i])\n    test_x[i]=re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', ' ', test_x[i], flags=re.MULTILINE)","ab266edf":"PRE_TRAINED_MODEL_NAME = 'roberta-large'\ntokenizer = transformers.RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","e21f3642":"def map_example_to_dict(input_ids, attention_masks,type_ids):\n  return [\n      tf.convert_to_tensor(input_ids),\n      tf.convert_to_tensor(attention_masks),\n      tf.convert_to_tensor(type_ids)\n  ]","0e14a1e8":"train_token=[]\ntest_token=[]\nfor i in range(len(train_x)):\n    train_token.append(tokenizer.encode_plus(train_x[i],pad_to_max_length=True,max_length=max_seq_length,return_token_type_ids=True))\nfor i in range(len(test_x)):\n    test_token.append(tokenizer.encode_plus(test_x[i],pad_to_max_length=True,max_length=max_seq_length,return_token_type_ids=True))\nprint(train_token[0])","756e7fe9":"input_ids=[]\nattention_mask=[]\ntype_ids=[]\nfor i in train_token:\n    input_ids.append(tf.reshape(i['input_ids'],(-1,max_seq_length)))\n    attention_mask.append(tf.reshape(i['attention_mask'],(-1,max_seq_length)))\n    type_ids.append(tf.reshape(i['token_type_ids'],(-1,max_seq_length)))\ntrain_input=map_example_to_dict(input_ids,attention_mask,type_ids)\nprint(len(train_input[0]))\n#print(len(train_input[0][0]))","6fcf4994":"input_ids=[]\nattention_mask=[]\ntype_ids=[]\nfor i in test_token:\n    input_ids.append(tf.reshape(i['input_ids'],(-1,max_seq_length)))\n    attention_mask.append(tf.reshape(i['attention_mask'],(-1,max_seq_length)))\n    type_ids.append(tf.reshape(i['token_type_ids'],(-1,max_seq_length)))\ntest_input=map_example_to_dict(input_ids,attention_mask,type_ids)\nprint(len(test_input[0]))\n#print(len(test_input[0][0]))","5a226f21":"ids = train_input[0]\nmasks = train_input[1]\ntoken_ids=train_input[2]\n\nids = tf.reshape(ids, (-1, max_seq_length,))\nprint(\"Input ids shape: \", ids.shape)\nmasks = tf.reshape(masks, (-1, max_seq_length,))\nprint(\"Input Masks shape: \", masks.shape)\ntoken_ids = tf.reshape(token_ids, (-1, max_seq_length,))\nprint(\"Token Ids shape: \", token_ids.shape)\n\nids=ids.numpy()\nmasks = masks.numpy()\ntoken_ids=token_ids.numpy()","d9bc92ac":"test_ids = test_input[0]\ntest_masks = test_input[1]\ntest_token_ids=test_input[2]\n\ntest_ids = tf.reshape(test_ids, (-1, max_seq_length,))\nprint(\"Input ids shape: \", test_ids.shape)\ntest_masks = tf.reshape(test_masks, (-1, max_seq_length,))\nprint(\"Input Masks shape: \", test_masks.shape)\ntest_token_ids = tf.reshape(test_token_ids, (-1, max_seq_length,))\nprint(\"Token Ids shape: \", test_token_ids.shape)\n\ntest_ids=test_ids.numpy()\ntest_masks = test_masks.numpy()\ntest_token_ids=test_token_ids.numpy()","c90f4c87":"ct = train.shape[0]\nstart_tokens = np.zeros((ct,max_seq_length),dtype='int32')\nend_tokens = np.zeros((ct,max_seq_length),dtype='int32')\n\nfor k in range(train.shape[0]):\n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    print(offsets[0])\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n            \n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","40b52b5c":"VALID_DATA=0.1\nvalid_size=int(len(ids)*VALID_DATA)\n\nvalid_ids=ids[0:valid_size]\nids=ids[valid_size:]\n\nvalid_masks=masks[0:valid_size]\nmasks=masks[valid_size:]\n\nvalid_token_ids=token_ids[0:valid_size]\ntoken_ids=token_ids[valid_size:]\n\nvalid_start_tokens=start_tokens[0:valid_size]\nstart_tokens=start_tokens[valid_size:]\n\nvalid_end_tokens=end_tokens[0:valid_size]\nend_tokens=end_tokens[valid_size:]","b1567a7c":"print(len(valid_ids))\nprint(len(ids))","b3aa7683":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","a3926533":"bert_model = transformers.TFRobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\nbert_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),loss='categorical_crossentropy')","9502472e":"def build_model():\n    input_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=np.int32)\n    attention_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=np.int32)\n    token_type = tf.keras.layers.Input(shape=(max_seq_length,), dtype=np.int32)\n    bert_layer = bert_model([input_ids, attention_mask,token_type])[0]\n\n    x1 = tf.keras.layers.Dropout(0.1)(bert_layer) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n\n    x2 = tf.keras.layers.Dropout(0.1)(bert_layer) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.Model(inputs=[input_ids, attention_mask,token_type], outputs=[x1,x2])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),loss='categorical_crossentropy',metrics=['categorical_accuracy'])\n    return model","bcd20538":"earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.0, patience=3)","0fa9fbd3":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((ids.shape[0],max_seq_length))\noof_end = np.zeros((ids.shape[0],max_seq_length))\npreds_start = np.zeros((test_ids.shape[0],max_seq_length))\npreds_end = np.zeros((test_ids.shape[0],max_seq_length))","5caa68c5":"model=build_model()\nmodel.fit([ids, masks, token_ids], [start_tokens, end_tokens], \n        epochs=1, batch_size=8, verbose=DISPLAY,\n        validation_data=([valid_ids,valid_masks,valid_token_ids], \n        [valid_start_tokens, valid_end_tokens]))\npreds = model.predict([test_ids,test_masks,test_token_ids],verbose=DISPLAY)\npreds_start += preds[0]\npreds_end += preds[1]\n\n# DISPLAY FOLD JACCARD\nall = []\nfor k in range(len(test_ids)):\n    a = np.argmax(oof_start[k,])\n    b = np.argmax(oof_end[k,])\n    if a>b: \n        st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n    else:\n        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[a-1:b])\n    all.append(jaccard(st,train.loc[k,'selected_text']))\njac.append(np.mean(all))\n#print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\nprint()","21d783e3":"# skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n# for fold,(idxT,idxV) in enumerate(skf.split(ids,sentiment)):\n\n#     print('#'*25)\n#     print('### FOLD %i'%(fold+1))\n#     print('#'*25)\n    \n#     tf.keras.backend.clear_session()\n#     model=build_model()\n#     sv = tf.keras.callbacks.ModelCheckpoint(\n#         '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n#         save_weights_only=True, mode='auto', save_freq='epoch')\n        \n#     model.fit([ids[idxT,], masks[idxT,], token_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n#         epochs=1, batch_size=8, verbose=DISPLAY,\n#         validation_data=([ids[idxV,],masks[idxV,],token_ids[idxV,]], \n#         [start_tokens[idxV,], end_tokens[idxV,]]))\n    \n#     #print('Loading model...')\n#     #model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n#     #print('Predicting OOF...')\n#     #oof_start[idxV,],oof_end[idxV,] = model.predict([\\ids[idxV,],masks[idxV,],token_ids[idxV,]],verbose=DISPLAY)\n    \n#     print('Predicting Test...')\n#     preds = model.predict([test_ids,test_masks,test_token_ids],verbose=DISPLAY)\n#     preds_start += preds[0]\/skf.n_splits\n#     preds_end += preds[1]\/skf.n_splits\n    \n#     # DISPLAY FOLD JACCARD\n#     all = []\n#     for k in idxV:\n#         a = np.argmax(oof_start[k,])\n#         b = np.argmax(oof_end[k,])\n#         if a>b: \n#             st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n#         else:\n#             text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n#             enc = tokenizer.encode(text1)\n#             st = tokenizer.decode(enc[a-1:b])\n#         all.append(jaccard(st,train.loc[k,'selected_text']))\n#     jac.append(np.mean(all))\n#     print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n#     print()","74ca57d8":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","85045a45":"all = []\nfor k in range(test_ids.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc[a-1:b])\n    all.append(st)","f89ff76f":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","fa808ef9":"# Create Model","3c7ba3d5":"# Create Start\/End Tokens"}}