{"cell_type":{"efb62ea2":"code","a58e15b0":"code","93430953":"code","bf0783c9":"code","24ee3f8e":"code","06877f70":"code","ba45a2dd":"code","1a33561d":"code","9338d5ab":"code","ecbf20bd":"code","629d56b1":"code","e8dbe51d":"code","6cd40a39":"code","bfc27cd0":"code","9a3f4560":"code","c2b79ca5":"code","472078b6":"code","8158d9a5":"code","16404a5d":"code","37d30695":"code","3e129fe2":"code","67cf9247":"code","4a69aa01":"code","d7f840a7":"code","9cf9028e":"code","85681b91":"code","d0990517":"code","9d7744b5":"code","a5d7709c":"code","55a7d57c":"code","e07d9981":"code","cfcd0b7e":"code","09a77539":"code","fbf11c02":"code","30f13a2e":"code","ae34b325":"code","f89b0fe7":"code","1f88be2f":"code","9654cd3b":"code","bd2e73de":"code","7b7d5624":"code","c620ec22":"code","06e37ba5":"code","a614a1ec":"code","8592cce8":"code","623db159":"code","dc876571":"code","42f520d3":"code","9502d3d1":"code","ea70952e":"code","b1b1c5bb":"code","20aeab97":"code","b1a99a2c":"code","afb9c016":"code","3728d036":"code","fc273217":"code","77e3e00f":"code","e8dbdadd":"code","18c2a4fd":"code","d7c8cd75":"code","c73ae6c0":"code","ffee4847":"code","d3315a47":"code","21b78064":"code","41e5bb52":"code","948a3164":"code","dc33e592":"code","80fc2f18":"code","b206915a":"code","3e3362f9":"code","1ed8168c":"code","8efaaa90":"code","b4e07f3d":"code","80bb9b96":"code","3fd287f0":"code","19882941":"code","dedf4f46":"code","fe91175d":"code","38ed9550":"code","7eaf609f":"code","12bf8b22":"code","8e004ed7":"code","8fc32809":"code","4be0d975":"code","d2aa09cc":"code","fe95c629":"code","d0f11ab2":"code","b75d2d40":"code","63422cb7":"code","4ede0e50":"code","647b1177":"code","6395e9c9":"code","22199c4c":"code","632acae2":"code","f9dfe3c8":"code","b825df6e":"code","78f6f744":"code","40582ea3":"code","a5574fa9":"code","ea84dbfb":"code","1343ef75":"code","5936b67d":"code","d628d076":"code","4d0bfad3":"code","bd68f25d":"code","13bd9947":"markdown","15e1dc7c":"markdown","f7450d68":"markdown","2f408a3c":"markdown","1ce1f089":"markdown","472577bf":"markdown","a7631978":"markdown","a145a3fe":"markdown","80c78e39":"markdown","fc49e97a":"markdown","c55247b1":"markdown","76161dc0":"markdown","c4729557":"markdown","b3599794":"markdown","ce80c512":"markdown","706d9796":"markdown","d27ab90c":"markdown","b6fcb162":"markdown","e21d7bb3":"markdown","ce415bc4":"markdown","0a474933":"markdown","82449479":"markdown","fda6761d":"markdown","04ed1891":"markdown","88aba344":"markdown","7734d67c":"markdown","8b96936a":"markdown","3384be5e":"markdown","b063711c":"markdown","1701b2b5":"markdown","5431f52b":"markdown","29f3e528":"markdown","4cac13ff":"markdown","f80c5788":"markdown","7b19ced1":"markdown","ab215aef":"markdown","f373ba8e":"markdown","25154a9f":"markdown","4a2f5f78":"markdown","e2e5d3c6":"markdown","81dbafb0":"markdown","09dfa2d8":"markdown","99fa0afe":"markdown","ddd112af":"markdown","2c89fab2":"markdown","e4dc0b47":"markdown","daf4c2ad":"markdown"},"source":{"efb62ea2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\n\nimport os\n\nimport kaggle_uploader\n","a58e15b0":"from tqdm.auto import tqdm\ntqdm.pandas()","93430953":"kaggle_uploader.__version__","bf0783c9":"CORES=4","24ee3f8e":"!mkdir output","06877f70":"!ls \/kaggle\/input\/CORD-19-research-challenge\/","ba45a2dd":"class COVDoc:\n    def __init__(self, filepath: str):\n        self.doc_id = None\n        self.filepath = filepath\n        self.lang = None\n        self.file_type = None\n        self.paragraph_tokens = []\n        self.paragraph_texts = []\n        self.processed_paragraphs = []\n#        self.processed_spacy_paragraphs = []\n        self.processed_nltk_paragraphs = []\n        \n    #load_text is used to lazy-load the actual text when needed\n    def load_text(self):\n        with open(self.filepath) as f:\n            d = json.load(f)\n            for paragraph in d[\"body_text\"]:\n                self.paragraph_texts.append(paragraph[\"text\"].lower())\n","1a33561d":"import json\n\ndef describe_doc(doc_path):\n    with open(doc_path) as f:\n        d = json.load(f)\n        print(d.keys())\n        print(f\"number of paragraphs: {len(d['body_text'])}\")\n        print()\n        for idx, paragraph in enumerate(d[\"body_text\"]):\n            print()\n            print(f\"section {idx+1}: title=\", end=\"\")\n            print(f'{paragraph[\"section\"]}: {len(paragraph[\"text\"])} chars')\n","9338d5ab":"import nltk, re, string, collections\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nimport spacy","ecbf20bd":"df_metadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\ndf_metadata[df_metadata[\"has_pmc_xml_parse\"] == True].head()","629d56b1":"df_metadata.isnull().sum()","e8dbe51d":"mask = df_metadata[\"sha\"].isnull() & df_metadata[\"pmcid\"].isnull()\ndfnulls = df_metadata[mask]\ndfnulls.shape","6cd40a39":"df_metadata[df_metadata[\"full_text_file\"].isnull()]","bfc27cd0":"df_metadata.shape","9a3f4560":"df_metadata[\"full_text_file\"].unique()","c2b79ca5":"def load_docs_(base_path, file_type):\n    if not base_path.endswith(\"\/\"):\n        base_path = base_path + \"\/\"\n    loaded_docs = []\n    count_pdf = 0\n    count_pmc = 0\n    file_paths_pdf = glob.glob(base_path+\"pdf_json\/*.json\")\n    file_paths_pmc = glob.glob(base_path+\"pmc_json\/*.json\")\n    file_names_pdf = [os.path.basename(path) for path in file_paths_pdf]\n    for filepath in tqdm(file_paths_pdf):\n        filename_sha = os.path.basename(filepath).split(\".\")[0]\n        #print(filename_sha)\n        df_sha = df_metadata[df_metadata[\"sha\"] == filename_sha]\n        if df_sha.shape[0] > 0:\n            has_pmc = df_sha[\"has_pmc_xml_parse\"].to_list()[0]\n            if has_pmc:\n                count_pmc += 1\n                pmc_id = df_sha[\"pmcid\"].to_list()[0]\n                filepath = f\"{base_path}pmc_json\/{pmc_id}.xml.json\"\n            else:\n                count_pdf += 1\n        else:\n            count_pdf += 1\n        doc = COVDoc(filepath)\n        doc.file_type = file_type\n        loaded_docs.append(doc)\n    print(f\"loaded {count_pdf} PDF files, {count_pmc} PMC files of type {file_type}\")\n    return loaded_docs","472078b6":"file_paths_pdf_all = []\nfile_paths_pmc_all = []\nall_docs = []\n\ndef load_doc_paths(base_path, file_type):\n    if not base_path.endswith(\"\/\"):\n        base_path = base_path + \"\/\"\n    file_paths_pdf = glob.glob(base_path+\"pdf_json\/*.json\")\n    file_paths_pmc = glob.glob(base_path+\"pmc_json\/*.json\")\n    file_paths_pdf_all.extend(file_paths_pdf)\n    file_paths_pmc_all.extend(file_paths_pmc)","8158d9a5":"medx_basepath = \"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\"\nload_doc_paths(medx_basepath, \"medx\")","16404a5d":"comuse_basepath = \"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/\"\nload_doc_paths(comuse_basepath, \"comuse\")","37d30695":"custom_basepath = \"\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/\"\nload_doc_paths(custom_basepath, \"custom\")\n","3e129fe2":"noncom_basepath = \"\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/\"\nload_doc_paths(noncom_basepath, \"noncom\")","67cf9247":"arxiv_basepath = \"\/kaggle\/input\/CORD-19-research-challenge\/arxiv\/arxiv\/\"\nload_doc_paths(arxiv_basepath, \"arxiv\")","4a69aa01":"len(file_paths_pdf_all)","d7f840a7":"len(file_paths_pmc_all)","9cf9028e":"def find_docs_in_metadata():\n    pmc_count = 0\n    pmc_missed = 0\n    pdf_count = 0\n    pdf_missed = 0\n    missed = 0\n    total = 0\n    for idx, row in tqdm(df_metadata.iterrows(), total=df_metadata.shape[0]):\n        pmcid = row[\"pmcid\"]\n        found_path = None\n        if isinstance(pmcid, str):\n            for filepath in file_paths_pmc_all:\n                if pmcid in filepath:\n                    found_path = filepath\n                    #print(filepath)\n                    pmc_count += 1\n                    break\n            if found_path is None:\n                #print(pmcid)\n                pmc_missed += 1\n        if found_path is None:\n            sha = row[\"sha\"]\n            if isinstance(sha, str):\n                for filepath in file_paths_pdf_all:\n                    #print(sha)\n                    if sha in filepath:\n                        found_path = filepath\n                        #print(filepath)\n                        pdf_count += 1\n                        break\n            if found_path is None:\n                pdf_missed += 1\n        if found_path is None:\n            missed += 1\n        else:\n            doc = COVDoc(filepath)\n            filetype = filepath.split(\"\/\")[-3]\n            doc.file_type = filetype\n            doc.doc_id = row.cord_uid\n            all_docs.append(doc)\n        total += 1\n    print(f\"finished: pmc={pmc_count}, pdf={pdf_count}, missed={missed}, pmc_missed={pmc_missed}, pdf_missed={pdf_missed}\")\n","85681b91":"for filepath in file_paths_pmc_all:\n    if \"PMC2114261\" in filepath:\n        print(filepath)\n","d0990517":"find_docs_in_metadata()","9d7744b5":"#for doc in all_docs:\n#    if \"PMC2114261\" in doc.filepath:\n#        print(doc.doc_id)\n#        print(doc.filepath)\n#        doc.load_text()\n#        print(doc.paragraph_texts)","a5d7709c":"df_metadata[df_metadata[\"cord_uid\"] == \"4sw25blb\"]","55a7d57c":"all_docs[0].doc_id","e07d9981":"all_docs[0].filepath","cfcd0b7e":"#all_docs[0].load_text()","09a77539":"#all_docs[0].paragraph_texts","fbf11c02":"def show_nltk_bigrams_for(docs):\n    tokens = []\n    for doc in docs:\n        for paragraph in doc.paragraph_texts:\n            doc_tokens = nltk.word_tokenize(paragraph)\n            tokens.extend(doc_tokens)\n    bigrams = ngrams(tokens, 2)\n    bigram_freq = collections.Counter(bigrams)\n    print(bigram_freq.most_common(100))","30f13a2e":"snippets_to_delete = [\n    \"The copyright holder for this preprint\",\n    \"doi: medRxiv preprint\",\n    \"doi: bioRxiv preprint\",\n    \"medRxiv preprint\",\n    \"bioRxiv preprint\",\n    \"cc-by-nc-nd 4.0\",\n    \"cc-by-nd 4.0\",\n    \"cc-by-nc 4.0\",\n    \"cc-by 4.0\", \n    \"international license\", \n    \"is made available under a\",\n    \"(which was not peer-reviewed)\",\n    \"the copyright holder for this preprint\",\n    \"who has granted medrxiv a license to display the preprint in perpetuity\",\n    \"author\/funder\",\n    \"all rights reserved\",\n    \"no reuse allowed without permission\",\n    \"all authors declare no competing interests\",\n    \"the authors declare no competing interests\",\n    \"no funding supported the project authors\",\n    \"his article is a US Government work\",\n    \"It is not subject to copyright under 17 USC 105 and is also made available for use under a CC0 license\",\n    \"Images were analyzed and processed using \",\n    \"ImageJ\",\n    \"(http:\/\/imagej.nih.gov\/ij)\", \n    \"Adobe Photoshop\"\n    \"CC 2017\",\n    \"All images were assembled in Adobe Illustrator\",\n]\nsnippets_to_delete = [snippet.lower() for snippet in snippets_to_delete]","ae34b325":"from urllib.parse import urlparse\n\ndef extract_urls(text):\n    \"\"\"Return a list of urls from a text string.\"\"\"\n    out = []\n    for word in text.split(' '):\n        word = word.replace(\"(\", \"\")\n        word = word.replace(\")\", \"\")\n        #tried with URL-parse library, it got complicated so really just look for HTTP\n#        thing = urlparse(word.strip())\n#        if thing.scheme:\n        if word.startswith(\"http:\/\") or word.startswith(\"https:\/\/\"):\n            out.append(word)\n    return out","f89b0fe7":"#testing the function\nextract_urls(\"(https:\/\/voice.baidu.com\/act\/newpneumonia\/newpneumonia\/?from=osari_pc_1)\")","1f88be2f":"from collections import defaultdict\nimport hashlib\nfrom langdetect import detect\n\n#get document language. I made another kernel to list these and about 98% are english, so I keep only the English ones\n#otherwise, I considered also translating the remaining ones but it seemed quite expensive looking at the cloud pricings for translate\ndef get_lang(doc):\n    try:\n        lang = detect(doc)\n    except Exception as e: \n        #some documents are broken as in no text, just garbage. langdetect throws an exception for those\n        lang = \"broken\"\n    return lang\n\n#replace URLs with \"urlX\", remove common template strings\ndef process_docs(docs):\n    print(f\"starting process_docs for {len(docs)} docs\")\n    url_counts = collections.Counter()\n    with tqdm(total=len(docs)) as pbar:\n        for doc in docs:\n            #NOTE: here we finally load the actual document content \/ paragraphs\n            doc.load_text()\n            total_text = \"\"\n            for paragraph in doc.paragraph_texts:\n                processed_paragraph = process_text(paragraph, url_counts)\n                del paragraph\n                doc.processed_paragraphs.append(processed_paragraph)\n                total_text += processed_paragraph\n            #need to save memory, this is not be used later (for now) so delete now\n            del doc.paragraph_texts\n            doc.lang = get_lang(total_text)\n            del total_text\n            pbar.update(1)\n    print(\"finished process_docs\")\n    return url_counts\n        \ndef process_text(doc_text, url_counts):\n    #sort url so longest is first, otherwise broken URL replacement if one is subset of another\n    urls = extract_urls(doc_text)\n\n    moded_text = doc_text.lower()\n    urls_to_replace = []\n    for url in urls:\n        if not url.startswith(\"http\"):\n            continue\n        urls_to_replace.append(url)\n        url_counts[url] += 1\n    del urls\n\n    for snippet in snippets_to_delete:\n        moded_text_2 = moded_text.replace(snippet, \"\")\n        del moded_text\n        moded_text = moded_text_2\n\n    urls_to_replace.sort(key = len, reverse=True)\n    for url in urls_to_replace:\n        url_hash = hashlib.sha256(url.lower().encode('utf-8')).hexdigest();\n        moded_text_2 = moded_text.replace(url, f\"URL{url_hash}\")\n        del moded_text\n        moded_text = moded_text_2\n    del urls_to_replace\n    return moded_text\n","9654cd3b":"!pip install pyenchant\n!apt install libenchant-dev -y","bd2e73de":"import enchant\n\nd = enchant.Dict(\"en_US\")","7b7d5624":"#print(\"loading spacy nlp\")\n#nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n","c620ec22":"#some things to try if memory is still an issue:\n#https:\/\/github.com\/explosion\/spaCy\/issues\/3618\ndef spacy_process(docs_to_process, nlp):\n    print(\"starting spacy process\")\n    #nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'ner'])\n    processed_doc_tokens = []\n    with tqdm(total=len(docs_to_process)) as pbar:\n        for doc in docs_to_process:\n            for paragraph in doc.processed_paragraphs:\n                spacy_paragraph = nlp(paragraph)\n                processed_spacy_paragraph = []\n                for token in spacy_paragraph:\n                    if token.is_stop:\n                        continue\n                    if token.pos_ == \"PUNCT\":\n                        continue\n                    if token.pos_ == \"NUM\":\n                        #tried to insert _NUM_ for numbers but it dominates quite a bit. just enable this if want it\n                        #processed_med_tokens.append(\"_NUM_\")\n                        continue\n                    token_text = token.lemma_.strip()\n                    #spacy seems to have some logic to understand for example that anti-viral is a compound word, but it reports different tokens still\n                    #maybe combine them somehow later but for now try it this way..\n                    if len(token_text) <= 1:\n                        continue\n                    processed_spacy_paragraph.append(token_text)\n                del spacy_paragraph\n                del paragraph\n                doc.processed_spacy_paragraphs.append(processed_spacy_paragraph)\n            del doc.processed_paragraphs\n            pbar.update(1)\n    del nlp\n    return","06e37ba5":"replacements = {'cells': 'cell', 'cases': 'case', 'used': 'use', 'using': 'use', 'results': 'result', '2modifications': 'modifications','2substitution': 'substitution','2\u2032omethyltransferase': 'omethyltransferase','2\u2032omethyltransferases': 'omethyltransferase','3adjacent': 'adjacent','3blocking': 'blocking','3coordinate': 'coordinate','3processing': 'processing','5coding': 'coding','5sequence': 'sequence','6phosphate': 'phosphate','abiotic': 'biotic','accuraty': 'accuracy','acidemia': 'academia','adapte': 'adapt','adaptor': 'adapter','adenovirus2': 'adenovirus2','adenoviruses': 'adenovirus','advective': 'adjective','aetiological': 'etiological','ageing': 'aging','aliquote': 'aliquot','alltogether': 'altogether','ambiguus': 'ambiguous','ammonis': 'ammonia','anaesthesia': 'anesthesia','anaesthetize': 'anesthetize','analyse': 'analyze','analysed': 'analyzed','analyte': 'analyze','antarctica': 'antarctic','apathogenic': 'pathogenic','artefact': 'artifact','arteritis': 'arthritis','beest': 'best','begining': 'beginning','behaviour': 'behavior','behavioural': 'behavioral','benchmarke': 'benchmark','binominal': 'binomial','biomedicals': 'biomedical','bulletin6': 'bulletin','caesarean': 'cesarean','capitalise': 'capitalize','carboxyl': 'carbonyl','catalyse': 'catalyze','categorisation': 'categorization','categorise': 'categorize','centralised': 'centralized','chaperone': 'chaperon','characterisation': 'characterization','characterise': 'characterize','characterised': 'characterized','checke': 'check','children1': 'children','chimaera': 'chimera','chimaeric': 'chimeric','circos': 'circus','cirrhosus': 'cirrhosis','cohorte': 'cohort','colinear': 'collinear','collisson': 'collision','colonisation': 'colonization','colour': 'color','colourless': 'colorless','coltd': 'cold','comfirmed': 'confirmed','compacta': 'compact','completetly': 'completely','completness': 'completeness','complexe': 'complex','compostion': 'composition','compounde': 'compound','concentrator5': 'concentrator','conceptualise': 'conceptualize','confluency': 'confluence','conjunctival': 'conjunctiva','contraining': 'containing','convertion': 'conversion','coronaviruses': 'coronavirus','corresponde': 'correspond','criterial': 'criteria','crosstalke': 'crosstalk','crosstalks': 'crosstalk','crystalize': 'crystallize','crystallise': 'crystallize','customisable': 'customizable','customise': 'customize','cyano': 'cyan','cysteines': 'cysteine','cytokines': 'cytokine','cytopathogenicity': 'cytopathogenic','cytotox': 'cytotoxin','cytotoxicities': 'cytotoxin','cytotoxicity': 'cytotoxin','cytotoxins': 'cytotoxin','datasets9': 'datasets','defence': 'defense','derivatize': 'derivative','descendent': 'descendant','destabilise': 'destabilize','detectible': 'detectable','detectr': 'detector','diabete': 'diabetes','dialyzed': 'dialyze','diameter6': 'diameter','diarrhoea': 'diarrhea','differece': 'difference','difine': 'define','dimeter': 'diameter','disc1': 'disc','discernable': 'discernible','discretised': 'discretized','discretize': 'discretized','distinguishs': 'distinguish','distrubution': 'distribution','doublestranded': 'doublestrand','dromedarius': 'dromedaries','ebiosciences': 'biosciences','effectived': 'effective','elegans': 'elegant','elimilate': 'eliminate','elongase': 'elongate','emphasise': 'emphasize','endeavour': 'endeavor','england1': 'england','enrichr': 'enrich','enrolment': 'enrollment','ensembl': 'ensemble','enspire': 'inspire','epithelia': 'epithelial','epitopes': 'epitope','equilocal': 'equivocal','esensor': 'sensor','estimaed': 'estimate','estimated\u03b3': 'estimated','estimatie': 'estimate','euclidiean': 'euclidean','evalulate': 'evaluate','evaporite': 'evaporate','exclusivly': 'exclusively','exportin5': 'exporting','expresss': 'express','factor2': 'factor','fast5': 'fast','favour': 'favor','favourable': 'favorable','favourably': 'favorably','flagellar': 'flagella','fluorescens': 'fluorescent','formalise': 'formalize','frameshifted': 'frameshift','frameshifter': 'frameshift','frameshifting': 'frameshift','frameshifts': 'frameshift','fulfil': 'fulfill','gastropoda': 'gastropod','geneious': 'generous','generalisation': 'generalization','generalise': 'generalize','generalised': 'generalized','genometric': 'geometric','genomics': 'genomic','grida': 'grid','harbour': 'harbor','hepes': 'herpes','heptatitis': 'hepatitis','heterogeneity': 'heterogenity','heterogenous': 'heterogeneous','holliday': 'holiday','homogenous': 'homogeneous','homolog': 'homology','hospitalisation': 'hospitalization','hospitalise': 'hospitalize','hospitalised': 'hospitalized','hybridisation': 'hybridization','hydrolyse': 'hydrolyze','hydrolysing': 'hydrolyzing','hypothesise': 'hypothesize','ifectious': 'infectious','imager': 'image','immunisation': 'immunization','immuno': 'immune','immunoassays': 'immunoassay','immunoblotting': 'immunoblot','imperiale': 'imperial','inadvertantly': 'inadvertently','incease': 'increase','incremente': 'increment','indictor': 'indicator','individuals5': 'individuals','individual\u0125': 'individual','industralized': 'industrialized','infec': 'infect','infecte': 'infect','infecteds': 'infected','infection1': 'infection','infection2': 'infection','infections8': 'infections','influenzae': 'influenza','initialise': 'initialize','instal': 'install','instituitional': 'institutional','instututional': 'institutional','interferon\u03b3': 'interferon','interleukin2': 'interleukin','interleukin6': 'interleukin','interleukin8': 'interleukin','internalisation': 'internalization','interspecie': 'interspecies','intinity': 'infinity','isotype': 'isotope','judgement': 'judgment','labeld': 'labeled','labelling': 'labeling','labour': 'labor','leucocyte': 'leukocyte','libarary': 'library','licence': 'license','lindependent': 'independent','localisation': 'localization','localised': 'localized','logistic\u03b1': 'logistics','loop1': 'loop','lysates': 'lysate','makino': 'making','marginalise': 'marginalize','mathematica': 'mathematical','maximisation': 'maximization','maximise': 'maximize','mcherry': 'cherry','mclean': 'clean','measurment': 'measurement','medicine4': 'medicine','mega6': 'mega','metagenomes': 'metagenome','methylated': 'methylate','microbiol': 'microbial','minima': 'minimal','minimise': 'minimize','mobilisation': 'mobilization','modeller': 'modeler','modelling': 'modeling','modulatory': 'modulator','moleculare': 'molecular','monocytes': 'monocyte','morbidit': 'morbidity','multinomialq': 'multinomial','multiplexe': 'multiplex','multiplexed': 'multiplex','nanoparticles': 'nanoparticle','na\u00efve': 'naive','neat1': 'neat','neighbour': 'neighbor','neighbourhood': 'neighborhood','neighbouring': 'neighboring','networkx': 'network','neutralisation': 'neutralization','neutralise': 'neutralize','neutraliza': 'neutralize','neutrophils': 'neutrophil','normalisation': 'normalization','normalise': 'normalize','normalised': 'normalized','notationx': 'notation','notation\u00fb': 'notation','npopulation': 'population','nucleases': 'nuclease','nucleolin': 'nucleoli','nucleoside': 'nucleotide','nucleosides': 'nucleotides','oesophagus': 'esophagus','offcial': 'official','omethyltransferases': 'omethyltransferase','oppsitely': 'oppositely','optimem': 'optimum','optimisation': 'optimization','optimise': 'optimize','organisation': 'organization','organise': 'organize','overlapa': 'overlap','overrepresente': 'overrepresented','paediatric': 'pediatric','pagel': 'page','parainfluenza3': 'parainfluenza','parameterisation': 'parameterization','parametrisation': 'parameterization','parametrise': 'parametrize','patients6': 'patients','penalise': 'penalize','peneumonia': 'pneumonia','peptidase4': 'peptidase','peroxydase': 'peroxidase','personel': 'personnel','phenylalanin': 'phenylalanine','phospho': 'phosphor','phylogenetically': 'phylogenetic','phylogenetics': 'phylogenetic','physico': 'physics','physicochemical': 'physiochemical','plateaue': 'plateau','pneumoniae': 'pneumonia','polioviruses': 'poliovirus','polymere': 'polymer','populations\u1e61': 'populations','popultion': 'population','predition': 'prediction','prioritise': 'prioritize','prisma': 'prism','programme': 'programmer','promotor': 'promoter','prospero': 'prosper','protozoal': 'protozoa','provence': 'province','pselection': 'selection','punctate': 'punctuate','quencher1': 'quencher','quilty': 'guilty','radiograph': 'radiography','randomised': 'randomized','rate\u03bc': 'rate','realisation': 'realization','realise': 'realize','reanalyse': 'reanalyze','recognise': 'recognize','recptor': 'receptor','reduc': 'reduce','refolde': 'refold','regularisation': 'regularization','regulary': 'regularly','remodeler': 'remodel','remodelling': 'remodeling','renumbere': 'renumber','replicase': 'replicate','represen': 'represent','representa': 'represent','reprograme': 'reprogram','reprograming': 'reprogramming','ressources': 'resources','restricta': 'restrict','reteste': 'retest','ribsomal': 'ribosomal','satisfie': 'satisfied','scheme1': 'scheme','scheme2': 'scheme','scientifica': 'scientific','scrutinise': 'scrutinize','sensitisation': 'sensitization','sensitise': 'sensitize','sensitised': 'sensitized','sequela': 'sequel','sequencher': 'sequencer','serie': 'series','signalling': 'signaling','simillar': 'similar','singlestranded': 'singlestrand','specialise': 'specialize','specialised': 'specialized','specically': 'specially','specrometry': 'spectrometry','spektrophotometer': 'spectrophotometer','stabilise': 'stabilize','standard8': 'standard','standardise': 'standardize','standardised': 'standardized','statiscical': 'statistical','statistially': 'statistically','stereotypy': 'stereotype','stimualate': 'stimulate','stirling': 'stirring','strain3': 'strain','striatum': 'stratum','studies9': 'studies','subprocesse': 'subprocess','subsampled': 'subsample','subspecie': 'subspecies','suceptible': 'susceptible','summarise': 'summarize','superpositione': 'superposition','sympatry': 'sympathy','synchronise': 'synchronize','syndrom': 'syndrome','synthesise': 'synthesize','syringae': 'syringe','tetherin': 'tethering','therminator': 'terminator','thresholde': 'threshold','time\u03c9': 'time','tlymphocyte': 'lymphocyte','transduce': 'transducer','transduced': 'transducer','transducer': 'transduce','transfect': 'transfection','transfectants': 'transfectant','transfected': 'transfection','transfecting': 'transfection','transfections': 'transfection','transferases': 'transferase','transferrable': 'transferable','transferrin': 'transferring','transferrins': 'transferring','translocations': 'translocation','transmid': 'transmit','transmsission': 'transmission','traveller': 'traveler','travelling': 'traveling','treshold': 'threshold','tryple': 'triple','tubercolosis': 'tuberculosis','tumour': 'tumor','unappreciate': 'unappreciated','unassemble': 'unassembled','uncoate': 'uncoated','underle': 'underlie','underpowere': 'underpowered','underreporte': 'underreported','undiagnose': 'undiagnosed','unlabelled': 'unlabeled','unpaire': 'unpaired','unrecognised': 'unrecognized','unsupervise': 'unsupervised','upregulated': 'upregulate','upregulates': 'upregulate','upregulations': 'upregulate','urbanisation': 'urbanization','using\u00f1': 'use','using': 'use', 'utilisation': 'utilization','utilise': 'utilize','vaccinees': 'vaccinee','ventilatory': 'ventilator','viremic': 'viremia','virions': 'virion','virus1': 'virus','viruse': 'virus','viruses3': 'viruses','visualisation': 'visualization','visualise': 'visualize','vitros': 'vitro','wellcome': 'welcome','wildtypes': 'wildtype','\u00b5order': 'order','\u00b5slide': 'slide','\u03b4pressure': 'pressure', 'studies': 'study'}\n","a614a1ec":"\"10\".isnumeric()","8592cce8":"stop_words = set(stopwords.words('english'))\nstop_words.update([\"et\", \"al\", \"fig\", \"eg\", \"ie\", \"2\u2032\", \"usepackage\", \"setlength\", \"also\", \"may\", \"figure\", \"one\", \"two\", \"new\", \"however\"])\n#stop_words","623db159":"replace_chars = string.punctuation#.replace(\"_\", \"\")\ntranslator = str.maketrans('', '', replace_chars)\nreplace_chars","dc876571":"lemmatizer = nltk.stem.WordNetLemmatizer()\nlemmatizer.lemmatize(\"was\")","42f520d3":"from nltk.tokenize import sent_tokenize, word_tokenize\n\ndef nltk_process(docs):\n    print(\"nltk process..\")\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    nltk_paragraph_tokens = []\n    #count how many times each recognized word occurs\n    known_words_e = collections.Counter()\n    #count how many times each unrecognized word occurs. good for looking for typos and domain words by frequency\n    unknown_words_e = collections.Counter()\n\n    with tqdm(total=len(docs)) as pbar:\n        print(\".\", end=\"\")\n        for doc in docs:\n            doc_tokens = []\n            #go through the previously Spacy preprocessed words\n#            for spacy_paragraph in doc.processed_spacy_paragraphs:\n            for paragraph in doc.processed_paragraphs:\n                processed_nltk_paragraph = []\n                paragraph_token = word_tokenize(paragraph)\n\n                for token in paragraph_token:\n                    #remove special chars as defined before (the \"translator\" variable)\n                    token = token.translate(translator)\n                    token = token.strip()\n                    if token in stop_words:\n                        #check words here before NLTK \"lemmatizes\" some of them, such as was->wa or has->ha\n                        continue\n                    token = lemmatizer.lemmatize(token)\n                    #replace using my custom mapping where applicable\n                    if token in replacements:\n                        token = replacements[token]\n                    if token in stop_words:\n                        #check stop words here a second time, just to be sure...\n                        continue\n                    if len(token) <= 1:\n                        #drop single letters and empty words\n                        continue\n                    if token.isnumeric():\n                        continue\n                    #d is the dictionary defined before, so check if the dictionary knows about it\n                    if not d.check(token):\n                        unknown_words_e[token] += 1\n                    else:\n                        known_words_e[token] += 1\n\n                    processed_nltk_paragraph.append(token)\n                doc_tokens.append(processed_nltk_paragraph)\n            nltk_paragraph_tokens.append(doc_tokens)\n            del doc.processed_paragraphs\n            #del doc.processed_spacy_paragraphs\n            pbar.update(1)\n    return nltk_paragraph_tokens, known_words_e, unknown_words_e","9502d3d1":"nltk.stem.WordNetLemmatizer().lemmatize('children')","ea70952e":"# few words I looked up from the first docs I processed:\n#anti-203 = anti campaign for proposal 203 for marihuana?\n# hbss = sickle hemoglobin ?\n# qpcr = Real-time polymerase chain reaction\n#impinger = tool for airborne sampling\n# trizol = TRIzol is a chemical solution used in the extraction of DNA, RNA, and proteins from cells. ( wikipedia )","b1b1c5bb":"def show_top_ngrams(tokens, top_size, *ns):\n    for n in ns:\n        print()\n        print(f\"{n}-GRAMS:\")\n        ng = ngrams(tokens, n)\n        ngram_freq = collections.Counter(ng)\n        for line in ngram_freq.most_common(top_size):\n            print(f\"{line[1]}: {line[0]}\")\n ","20aeab97":"#ptfe = Polytetrafluoroethylene\n#pvc = Polyvinyl chloride\n#skc biosampler = https:\/\/skcltd.com\/products2\/bioaerosol-sampling\/biosampler.html","b1a99a2c":"from gensim.models import Phrases\nimport gensim\nimport os\nimport json\n\n#https:\/\/stackoverflow.com\/questions\/53694381\/print-bigrams-learned-with-gensim\n#https:\/\/datascience.stackexchange.com\/questions\/25524\/how-does-phrases-in-gensim-work\n\ndef create_gensim_ngram_models(params):\n    filetypes, paragraph_lists, filenames, doc_ids = params\n    tokens = []\n    total_paragraphs = 0\n    for paragraph_list in paragraph_lists:\n        tokens.extend(paragraph_list)\n        total_paragraphs += len(paragraph_list)\n        \n    print(\"creating bigram\")\n    bigram = Phrases(tokens, min_count=5, threshold=100)\n    print(\"creating trigram\")\n    trigram = Phrases(bigram[tokens], threshold=100)\n    print(\"creating bigram-model\")\n    bigram_mod = gensim.models.phrases.Phraser(bigram) \n    print(\"creating trigram-model\")\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n\n    print(\"processing docs\")\n    for idx, paragraph_list in enumerate(tqdm(paragraph_lists)):\n        paragraph_tokens = []\n        paragraph_tokens_extended = []\n        for paragraph in paragraph_list:\n            gensim_tokens = trigram_mod[bigram_mod[paragraph]]\n            extended = []\n            for gensim_token in gensim_tokens:\n                extended.append(gensim_token)\n                if \"_\" in gensim_token:\n                    extended.extend(gensim_token.split(\"_\"))\n            paragraph_tokens.append(gensim_tokens)\n            paragraph_tokens_extended.append(extended)\n            del paragraph\n\n        doc_id = doc_ids[idx]\n        filetype = filetypes[idx]\n        filename = os.path.basename(f\"{filenames[idx]}\")\n        filename = os.path.splitext(f\"{filename}\")[0]\n        \n        doc = doc_id+\"\\n\"\n        doc_json_list = []\n        for paragraph in paragraph_tokens:\n            doc += \" \"+\" \".join(paragraph)\n            doc_json_list.append({\"text\": paragraph})\n        doc_json = {\"doc_id\": doc_id, \"body_text\": doc_json_list}\n        \n        whole_filename = f'output\/whole\/{filetype}\/{filename}.txt'\n        paragraph_filename = f'output\/paragraphs\/{filetype}\/{filename}.json'\n        os.makedirs(os.path.dirname(whole_filename), exist_ok=True)\n        os.makedirs(os.path.dirname(paragraph_filename), exist_ok=True)\n        with open(whole_filename, 'w') as f:\n            f.write(f\"{doc}\\n\")\n        with open(paragraph_filename, 'w') as f:\n            json.dump(doc_json, f, indent=4)\n            \n        del paragraph_list\n        del paragraph_tokens\n        del paragraph_tokens_extended\n        del doc\n        del doc_json\n        del doc_json_list\n    return #gensim_doc_tokens, gensim_doc_tokens_extended\n\ndef no_pool_gensim(filetypes, paragraphs, filenames, doc_ids):\n    params = (filetypes, paragraphs, filenames, doc_ids)\n    return create_gensim_ngram_models(params)\n\ndef pool_gensim(filetypes, paragraphs, filenames, doc_ids):\n    pool_data = (filetypes, paragraphs, filenames, doc_ids)\n    with Pool(processes=1) as pool:\n        result = pool.map(create_gensim_ngram_models, [pool_data])\n        #gensim_doc_tokens, gensim_doc_tokens_extended = result[0]\n        pool.terminate()\n        pool.close()\n        pool.join()\n    return #gensim_doc_tokens, gensim_doc_tokens_extended\n\n","afb9c016":"from multiprocessing import Pool\nimport psutil\n\ndef process_map_slice(idx_slice):\n    idx = idx_slice[0]\n    print(f\"processing slice {idx}\")\n    docs = idx_slice[1]\n    print(f\"processing slice {idx}: process_docs {len(docs)}\")\n    url_counts = process_docs(docs)\n    docs = [doc for doc in docs if doc.lang == \"en\"]\n    #disabled spacy processing due to memory issues. it was a bit slow too for this purpose\n    #print(f\"processing slice {idx}: spacy_process\")\n    #nlp = idx_slice[2]\n    #spacy_process(docs, nlp)\n    print(f\"processing slice {idx}: nltk_process\")\n    processed_nltk_paragraphs, known_words_e, unknown_words_e = nltk_process(docs)\n    doc_ids = [doc.doc_id for doc in docs]\n    filepaths = [doc.filepath for doc in docs]\n    del docs\n    del idx_slice\n    print(f\"processing slice {idx}: done\")\n    return (idx, processed_nltk_paragraphs, known_words_e, unknown_words_e, url_counts, doc_ids, filepaths)\n\ndef map_reduce(docs_to_process):\n    #split the given docs to CORES number of subsets so can spawn a process per core to process the subset\n    slice_list = np.array_split(docs_to_process, CORES)\n    processed_nltk_paragraphs = []\n    #track recognized words for later checking against unrecognized (by dictionary check)\n    #helpful for fixing common typos etc\n    known_words_e = collections.Counter()\n    unknown_words_e = collections.Counter()\n    url_counts = collections.Counter()\n    doc_ids = []\n    filepaths = []\n    print(\"creating pool\")\n    with Pool(processes=CORES) as pool:\n        import gc\n        gc.collect()\n        idx_slices = []\n        for idx, doc_slice in enumerate(slice_list):\n            idx_slices.append((idx, doc_slice))\n        results = pool.map(process_map_slice, idx_slices)\n        pool.terminate()\n        pool.close()\n        pool.join()\n        del pool\n        print(\"pool finished\")\n        del doc_slice\n        del slice_list\n        del idx_slices\n    results.sort(key=lambda tup: tup[0])\n    prev_idx = -1\n    print(\"starting merge\")\n    #print(psutil.virtual_memory())\n    for result in results:\n        idx_slice, processed_nltk_paragraphs2, known_words_e_2, unknown_words_e_2, url_counts_2, doc_ids_2, filepaths_2 = result            \n        assert idx_slice > prev_idx, f\"Prev must be < current: {prev_idx} < {idx_slice} fails\"\n        prev_idx = idx_slice\n        processed_nltk_paragraphs.extend(processed_nltk_paragraphs2)\n        known_words_e += known_words_e_2\n        unknown_words_e += unknown_words_e_2\n        url_counts += url_counts_2\n        doc_ids.extend(doc_ids_2)\n        filepaths.extend(filepaths_2)\n        del processed_nltk_paragraphs2\n        del known_words_e_2\n        del unknown_words_e_2\n        del url_counts_2\n        del result\n    del results\n    #filepaths = [doc.filepath for doc in docs_to_process]\n    del docs_to_process\n    print(\"finished map-reduce\")\n    print(psutil.virtual_memory())\n    return processed_nltk_paragraphs, known_words_e, unknown_words_e, url_counts, filepaths, doc_ids\n","3728d036":"#!cat \/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/0a43046c154d0e521a6c425df215d90f3c62681e.json","fc273217":"import gc\n\ngc.get_threshold()","77e3e00f":"import random\n\n#some of the article sets contain longer docs, we want to process in parallel so make sure every process gets different lengths by shuffling the list\nrandom.shuffle(all_docs)\n\n#results = map_reduce(all_docs)\n#processed_nltk_paragraphs, known_words_e, unknown_words_e, url_counts, all_filenames = results","e8dbdadd":"memory_saving_list = np.array_split(all_docs, 1)\n#memory_saving_list = [all_docs]\nfile_types = [doc.file_type for doc in all_docs]\n#doc_ids = [doc.doc_id for doc in all_docs]\n\nprocessed_nltk_paragraphs = []\nknown_words_e = collections.Counter()\nunknown_words_e = collections.Counter()\nurl_counts = collections.Counter()\nfilepaths = []\ndoc_ids = []\nfor doc_list in memory_saving_list:\n#    doc_list = doc_list[:100]\n    processed_nltk_paragraphs_2, known_words_e_2, unknown_words_e_2, url_counts_2, filepaths_2, doc_ids_2 = map_reduce(doc_list)\n    processed_nltk_paragraphs.extend(processed_nltk_paragraphs_2)\n    known_words_e += known_words_e_2\n    unknown_words_e += unknown_words_e_2\n    url_counts += url_counts_2\n    filepaths.extend(filepaths_2)\n    doc_ids.extend(doc_ids_2)\n    for doc in doc_list:\n        del doc\n    del doc_list\n    #break\n\ndel processed_nltk_paragraphs_2\ndel known_words_e_2\ndel unknown_words_e_2\ndel url_counts_2    \n\ndel memory_saving_list\n#del all_docs\n","18c2a4fd":"doc_ids[0]","d7c8cd75":"#processed_nltk_paragraphs[0]","c73ae6c0":"filetypes = file_types","ffee4847":"#show_nltk_bigrams_for(all_docs)","d3315a47":"unknown_words_e.most_common()[:20]","21b78064":"from operator import itemgetter\n#https:\/\/stackoverflow.com\/questions\/17243620\/operator-itemgetter-or-lambda\/17243726#17243726\n\nunknown_list = list(unknown_words_e.items())\nunknown_list.sort(key=itemgetter(1), reverse=True)\nunknown_str = \"\"\nfor unknown in unknown_list:\n    unknown_str += f\"{unknown[0]}: {unknown[1]}\\n\"\nwith open(f'output\/unknown.txt', 'w') as f:\n    f.write(unknown_str)","41e5bb52":"!ls -l output","948a3164":"!df -h .","dc33e592":"#unknown_list","80fc2f18":"def test_remove_threshold(n, counter):\n    count_kept = 0\n    count_removed = 0\n    for word_count in reversed(counter.most_common()):\n        if word_count[1] > n:\n            #if more than N instances of word, do not remove but check later\n            count_kept += 1\n            continue\n        count_removed += 1\n    return count_kept, count_removed\n","b206915a":"list_kept = []\nlist_removed = []\nfor x in range(0, 50):\n    count_kept, count_removed = test_remove_threshold(x, unknown_words_e)\n    list_kept.append(count_kept)\n    list_removed.append(count_removed)\ndf = pd.DataFrame()\ndf[\"kept\"] = list_kept\ndf[\"removed\"] = list_removed\ndf.plot()\n","3e3362f9":"pd.set_option('display.max_rows', 50)\ndf","1ed8168c":"words_to_remove = set()\nwords_to_check = []\n\n#TODO: test threahold values\n\ncount = 0\ncount2 = 0\nfor word_count in reversed(unknown_words_e.most_common()):\n    if word_count[1] > 30:\n        #if more than N instances of word, do not remove but check later\n        #TODO: smaller threshold for typo check?\n        words_to_check.append(word_count)\n        count2 += 1\n        continue\n\n    words_to_remove.add(word_count[0])    \n    count += 1\nprint(f\"selected {count} unknown words for removal\")\nprint(f\"kept {count2} unknown words\")\n\ndef remove_infrequent_known_words():\n    count = 0\n    count2 = 0\n    for word_count in reversed(known_words_e.most_common()):\n        if word_count[1] > 2:\n            #more than N instances of word, stop removing\n            count2 += 1\n            continue\n        words_to_remove.add(word_count[0])\n        count += 1\n    print(f\"selected {count} known words for removal\")\n    print(f\"kept {count2} known words\")\n\n\n    \nremove_infrequent_known_words()\n    \nwords_to_check.sort(key=itemgetter(1), reverse=True)\n","8efaaa90":"len(words_to_check)","b4e07f3d":"size_before = 0\nsize_after = 0\nnew_list_list = []\nfor paragraph_list in tqdm(processed_nltk_paragraphs):\n    new_list = []\n    for paragraph in paragraph_list:\n        size_before += len(paragraph)\n        new_paragraph = [token for token in paragraph if token not in words_to_remove]\n        size_after += len(new_paragraph)\n        new_list.append(new_paragraph)\n        del paragraph\n    new_list_list.append(new_list)\n\ndel processed_nltk_paragraphs\nprocessed_nltk_paragraphs = new_list_list\nprint(f\"size before:{size_before}\")\nprint(f\"size after: {size_after}\")\ndiff = size_before - size_after\nprint(f\"reduced by: {diff}\")\n    ","80bb9b96":"!pip install weighted-levenshtein\n!pip install python-Levenshtein","3fd287f0":"from weighted_levenshtein import lev, osa, dam_lev\nimport Levenshtein\n\n#just to see it works\nprint(lev('BANANAS', 'BANDANAS'))\nprint(Levenshtein.distance(\"BANANAS\", \"BANDANAS\"))","19882941":"import time\n\ndef find_closest_matches(unknown_words, known_words):\n    count = 0\n    count_diff = 0\n    count_short = 0\n    count_scored = 0\n    file_str = \"\"\n    epoch_time = int(time.time()\/60)\n    for unknown in tqdm(unknown_words):\n        if len(unknown[0]) < 5:\n            count_short += 1\n            #skip saving words that are shorter than 5 chars\n            continue\n#            if unknown[1] < 20:\n#                #less than 10 instances of word, stop\n#                break\n        for known in known_words:\n            unknown_word = unknown[0]\n            known_word = known\n            diff = len(unknown_word) - len(known_word)\n            diff = abs(diff)\n            if diff > 1:\n                count_diff += 1\n                continue\n            count_scored += 1\n            score = Levenshtein.distance(unknown_word, known_word)\n            if score == 1:\n                line = f\"{unknown[1]}: '{unknown[0]}': '{known}',\"\n                file_str += f\"{line}\\n\"\n                if count < 50:\n                    print(line)\n                count += 1\n    with open(f'output\/closest.txt', 'w') as f:\n        f.write(file_str)\n\n    print(f\"count={count}, count_diff={count_diff}, count_short={count_short}, count_scored={count_scored}\")","dedf4f46":"find_closest_matches(words_to_check, known_words_e)\n#find_closest_matches(unknown_words_e, known_words_e)\n","fe91175d":"!ls -l output","38ed9550":"url_counts.most_common(20)","7eaf609f":"del words_to_check\ndel words_to_remove\ndel known_words_e\ndel unknown_words_e\ndel all_docs\ndel unknown_list\ndel unknown_str\n","12bf8b22":"%%time\nno_pool_gensim(filetypes, processed_nltk_paragraphs, filepaths, doc_ids)\n","8e004ed7":"!ls output\/paragraphs\/custom_license | head -n 10","8fc32809":"!du output\n","4be0d975":"#!head output\/paragraphs\/custom\/00016663c74157a66b4d509d5c4edffd5391bbe0.json","d2aa09cc":"!ls output","fe95c629":"!ls output\/paragraphs\/noncomm_use_subset | wc -l","d0f11ab2":"!ls output\/whole\/noncomm_use_subset | wc -l","b75d2d40":"!ls output\/paragraphs\/biorxiv_medrxiv | wc -l","63422cb7":"!ls -l output\/whole\/custom_license | head -n 10","4ede0e50":"!ls -l output\/paragraphs\/comm_use_subset | head -n 10\n","647b1177":"#!head output\/whole\/comuse\/1a465d982030d8f361dc914ff2defa359fdbe5f9.txt","6395e9c9":"!apt install zip -y","22199c4c":"%%time\n#!tar zcf output.tgz output\n","632acae2":"%%time\n!zip -r -q output.zip output","f9dfe3c8":"!ls -l output","b825df6e":"!ls -l","78f6f744":"!mkdir upload_dir\n!mv output.zip upload_dir","40582ea3":"!ls upload_dir","a5574fa9":"os.path.abspath(\".\/upload_dir\")","ea84dbfb":"import kaggle_uploader\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\napi_secret = user_secrets.get_secret(\"kaggle api key\")\n\nkaggle_uploader.resources = []\nkaggle_uploader.init_on_kaggle(\"donkeys\", api_secret)\nkaggle_uploader.base_path = \".\/upload_dir\"\nkaggle_uploader.title = \"COVID NLP Preprocessed\"\nkaggle_uploader.dataset_id = \"covid-nlp-preprocess\"\nkaggle_uploader.user_id = \"donkeys\"\nkaggle_uploader.add_resource(\"output.zip\", \"zipped preprocessing data\")\nkaggle_uploader.update(\"new version\")\n","1343ef75":"import collections\n\nword_count = collections.Counter()\nfor doc in processed_nltk_paragraphs:\n    for paragraph in doc:\n        word_count.update(paragraph)\nlen(word_count)","5936b67d":"word_count.most_common(30)","d628d076":"!ls output\/paragraphs\/biorxiv_medrxiv | head -n 10","4d0bfad3":"!head output\/paragraphs\/biorxiv_medrxiv\/00060fb61742ff60e4e3ba4648c74a34cfe9560d.json","bd68f25d":"!rm -rf output","13bd9947":"See if the bigramming works:","15e1dc7c":"Levenshtein allows calculating word distances, so if distance is 1 keep it in a list to see if it is a type and could be added to the \"replacements\" mapping.","f7450d68":"Remove all infrequent words:","2f408a3c":"If you look for a lot of common terms and bi-grams you quickly notice many URL's, and many broken URL's as well. I extract them and replace with URLxxxx so I can compare if there are some broadly shared URL's across all documents, and get rid of them from topic models etc.","1ce1f089":"Gensim bigrams, trigrams, and practically quad-grams:","472577bf":"I used show_nltk_bigrams_for() to view potential bigrams:","a7631978":"NLTK based function to show most common ngrams, used it for basic data exploration:","a145a3fe":"### NLTK processing","80c78e39":"# Processing function definitions","fc49e97a":"I was using Spacy for lemmatization. Dropping unnecessary parts of the pipeline speeds it up by a lot, and saves some memory. But it had a lot of issues with memory management, so I disabled it and use only NLTK preprocessing. Which works fine actually..","c55247b1":"## Loading documents, selecting PDF vs PMC","76161dc0":"I use multiprocessing later in the kernel to process much of the data. It help process data faster but also can help manage memory better. Well, I still had troubles with Spacy and had to remove it due to memory issues but anyway. \n\nThe CORES variable simply defines the number of parallel processes spawned later in multiprocessing.","c4729557":"## Multi-processing over the documents with above functions","b3599794":"Build the bigrams, trigrams, (and potentially quad-grams) with Gensim:","ce80c512":"Load the documents from the given path. Processing sequence:\n\n- Read a list of all PDF based JSON files\n- Parse file SHA from the filenames\n- Check metadata if there is an entry for that SHA\n- If the SHA is there, read the PMC XML JSON content instead. It should be better quality.\n- If SHA or PMC ID not found, read the PDF JSON and use that\n","706d9796":"## Look at common, infrequent, and unknown words and their relations","d27ab90c":"Snippets to delete are some template values from the articles. Mainly from the MedX\/BioX since I started with those. But these are removed before anything else is done, so further templates would just require adding lines here:","b6fcb162":"Perform the actual comparison unknwon vs all known words:","e21d7bb3":"Install library to do English dictionary checks, to identify possible domain specific and misspelled words:","ce415bc4":"I collected a set of words that seem misspelled by running the kernel previously many times. This list is a mapping of words to their \"correct\" spelling. Note that this mapping is all from my head, so might be all wrong. Intent is to provide better input for later techniques such as topic models, where a single words matches across documents.","0a474933":"is_numeric() is Python string function to check if string is numeric. Here I just test it before using it:","82449479":"## N-Grams","fda6761d":"The following could be used to show that plain bi-grams from unprocessed text are quite the garbase. So this is why I run bigrams later on full pre-processed et, to get better\/cleaner results.","04ed1891":"The translator is to remove punctunation (and any other chars we might want to add to replace_chars).","88aba344":"### Find closest known words for unknown words","7734d67c":"Describe doc is a utility function I used to get an initial idea of some of the document data:","8b96936a":"### Typo fixing","3384be5e":"The above graph and the table below illustrate how a large number of very infrequent words exist in this corpus. Most of these actually only found once in all the documents. Likely some processing artefacts or other errors.","b063711c":"# Preprocessing paragraphs and sections\n\nThis notebook does \n - preprocesses each document into two versions:\n     - all body sections preprocessed and merged into one text file\n     - all body sections preprocessed and listed separately, per document, in a single JSON as in the input dataset\n - preprocessing steps applied\n     - tokenization (NLTK)\n     - lemmatization (NLTK)\n     - n-grams (Gensim)\n     - stop-word removal (NLTK based + added some of my own from unrecognized words)\n     - punctuation and special characters removal\n     - removal of some article templates text, such as copyrights \n - outputs produced\n     - under output\/whole\/... the preprocessed documents with all body texts in one text file\n     - under output\/paragraphs\/... the processed documents with each original body section separately listed in JSON format\n     - output\/closest.txt, a list of unrecognized (by dictionary) words and the closest match to known words (using Levenshtein word-distance)\n     - output\/unknown.txt, list of all unrecognized words sorted by frequency\n     - the above output is compressed into output.zip and output.tar.gz for use as kernel input sources\n\nUpdate May 1st, 2020:\n- Update input dataset to Kaggle version from May 1st\n- Add the new set of arxiv\n\nUpdate April 25th, 2020:\n- Update input dataset to Kaggle version from April 24th\n\nUpdate April 19th, 2020:\n- Fixed also filepaths being off similar to doc paths. Was causing downstream notebooks to use wrong input.\n- Updated input dataset to Kaggle version from April 17th\n\nUpdate April 18th, 2020: \n- Added stop word removal before and after lemmatization. NLTK lemmatizer does some weird things (e.g., was->wa, has->ha)\n- Fixed saved doc id's being off when docs are removed\n\nI made this notebook to provide input for further kernels to try techniques such as topic models.","1701b2b5":"Check some most common words and their closes matches:","5431f52b":"Find closest matching words from the collected known words for each word in the list of unknown words. I use the closest matchest to check for potential common typos. More frequent typos or same thing with slightly different typing (if not a real typo, e.g., British vs US English) are most useful for unifying.","29f3e528":"Well, \"use\" is a bit of a give or take.. But I leave it in.","4cac13ff":"### Spacy processing","f80c5788":"Write list of unknown words to file, in order of most frequent. Can check for common typos later to add to replacement list.","7b19ced1":"Previously I defined a function for Spacy specific preprocessing. This one is for NLTK specific pre-processing. So why do another? Because Spacy lemmatizes well, but it also seems to think too many words are just fine. I think because it uses some vocabulary from large-scale web-scraping. And the internet contains all sorts of weird acronyms and spellings.. So I use also NLTK, which is based on more \"rigorous\" English texts to also identify \"unknown\" words to store and present later. For example, for collecting domain specific words and to identify misspellings.\n\n... And for now I disabled Spacy so this is it.","ab215aef":"Create a list of infrequent words to remove, and a list of words to check for spelling:","f373ba8e":"The output was zipped, so delete the directory with individual files. Kaggle notebooks seem to be unable to host 1000+ output files when used as input to other notebooks.","25154a9f":"Some basic preprocessing such as removing URL's, detecting document language, removing template texts:","4a2f5f78":"Now to use multi-processing to process the docs in different parts. Good thing is that it runs in separate processes, so when the processes finish they are terminated and all memory released. Otherwise memory control in Python is quite a challenge (well, it still seems to be a challenge, but maybe this helps..):","e2e5d3c6":"Final check to see if missed some very common stopwords or such:","81dbafb0":"### Basic preprocessing","09dfa2d8":"Here https:\/\/doi.org\/10.1101\/19011940 seems to be the top actual intact URL left. It refers to paper titled \"Using Digital Surveillance Tools for Near Real-Time Mapping of the Risk of International Infectious Disease Spread: Ebola as a Case Study\".\n\nAnd thats all folks. You can just add this kernel as a data source if you like to play with the preprocessed files. Just click on \"+Add data\" on the right-side sidebar top if you like.","99fa0afe":"The following does Spacy specific pre-processing. Spacy does a good job of lemmatizing a lot of even somewhat obscure seeming words that NLTK seems to miss. Also using Spacy to ID words that are stopwords, punctuation, and purely numeric. Removing those.\n\n(this is disabled for memory issues, but the NLTK version is fine)","ddd112af":"Split the data into N subsets and run in separate multiprocessing Pool sessions. I tried to save memory with this but Spacy kept eating it all up. After removing Spacy I can easily process everything at once, so this loop is not really needed but leaving it here if need it later.","2c89fab2":"# Package the output","e4dc0b47":"COVDoc is just a simple data structure to hold different forms of the documens while preprocessing them:","daf4c2ad":"I picked threshold of 30 to keep ~50k words and remove ~1.2M words \/ tokens. Well at the time of writing this anyway.. The size of the vocabulary easily multiplies the memory and processing requirements, so reasonable filtering seems useful."}}