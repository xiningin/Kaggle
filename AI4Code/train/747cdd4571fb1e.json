{"cell_type":{"e6ba6610":"code","a1747cb1":"code","702b0f5e":"code","b6866bf5":"code","2fa6ea39":"code","5cd8489d":"code","3a561da8":"code","908d34f3":"code","e57133ff":"code","b66b13ff":"code","eac3b10f":"code","5ffeb472":"code","f235f6d5":"code","dad62032":"code","e290d9de":"code","ec148f72":"code","cd9cf39b":"code","de4c8169":"code","78e4efa2":"code","1a94dd07":"code","c2a55ec4":"code","c7cbf9d4":"code","88b55dbf":"code","0bba54f1":"code","d2ed2ae7":"code","ba03fef7":"code","f140fd01":"code","92491101":"code","8b90504f":"code","d8a30559":"code","c5f26299":"code","c149a38b":"code","b0c67dc8":"code","8e2c7a99":"code","f458bfe4":"code","91070046":"code","458f1d13":"code","bca45ba0":"code","349eea09":"code","cd12cf76":"code","d12baecf":"code","aed67780":"code","de16ad9e":"code","1fbc6837":"code","1e8f5328":"code","8b67203b":"code","9067c60a":"code","c38d319c":"code","d26b02a2":"code","344524bf":"code","bd59148c":"code","5f91ae8f":"code","4bab5da1":"code","ed181edd":"code","f4dd17c3":"code","ee9031d3":"code","d12ade10":"code","77cfff76":"code","ac327a14":"code","854ed109":"code","3cb0eeb5":"markdown","213e27a7":"markdown","d930e4d5":"markdown","92fdb12e":"markdown","68fabf88":"markdown","842ce2a1":"markdown","d083f669":"markdown","39439863":"markdown","60a76e5d":"markdown","3e236fbf":"markdown"},"source":{"e6ba6610":"# Pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n\n# Display up to 60 columns of a dataframe\npd.set_option('display.max_columns', 60)\n\n# Matplotlib visualization\nimport matplotlib.pyplot as plt\nplt.style.use('grayscale')\n%matplotlib inline\n\n# Seaborn for visualization\nimport seaborn as sns\nimport plotly.express as px\nsns.set(font_scale = 2)\n\n# Splitting data into training and testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import KFold # for cross validation\nfrom sklearn.model_selection import GridSearchCV # for tuning parameter\nfrom sklearn.model_selection import RandomizedSearchCV  # Randomized search on hyper parameters.\nfrom sklearn.preprocessing import StandardScaler # for normalization\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report,balanced_accuracy_score,f1_score,recall_score,accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn import metrics # for the check the error and accuracy of the model\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n","a1747cb1":"data = pd.read_csv('..\/input\/creditcarddefaultdata\/data.csv')","702b0f5e":"data.head()","b6866bf5":"data.info()","2fa6ea39":"data.describe()","5cd8489d":"print(\" Shape of  dataframe: \", data.shape)\n# Drop duplicates\ndata.drop_duplicates()\nprint(data.shape)\n","3a561da8":"null= data.isnull().sum().sort_values(ascending=False)\ntotal =data.shape[0]\npercent_missing= (data.isnull().sum()\/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\npd.DataFrame(missing_data)","908d34f3":"plt.figure(figsize=(18,8))\nplt.subplot(1,2,1)\ndata.Y.value_counts().plot.pie(explode=[0,0.1])\nplt.subplot(1,2,2)\nsns.countplot(data.Y)\nplt.suptitle(\"Target Distribution\", size=20)\nplt.show()","e57133ff":"count_df = pd.DataFrame(data['Y'].value_counts())\ncount_df.style.background_gradient(cmap='vlag')","b66b13ff":"Count_Nodefault_transacation = len(data[data['Y']==0])\nCount_default_transacation = len(data[data['Y']==1]) \nPercentage_nodefault = Count_Nodefault_transacation\/(Count_default_transacation+Count_Nodefault_transacation)\nprint('% of no defaults       :', Percentage_nodefault*100)\nprint('Number of no defaults     :', Count_Nodefault_transacation)\nPercentage_of_default= Count_default_transacation\/(Count_default_transacation+Count_Nodefault_transacation)\nprint('% of defaults         :',Percentage_of_default*100)\nprint('Number of defaults    :', Count_default_transacation)","eac3b10f":"def plot_distribution(feature,color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Distribution of %s\" % feature)\n    sns.distplot(data[feature].dropna(),color=color, kde=True,bins=30)\n    plt.show()\n\ndef plot_box(feature, color):\n    plt.figure(figsize=(10,6))\n    plt.title(\"Box Plot of %s\" % feature)\n    sns.boxplot(data[feature].dropna(),color=color)\n    plt.show()","5ffeb472":"plot_distribution('X1','g')","f235f6d5":"plot_distribution('X12','r')","dad62032":"plot_distribution('X16','y')","e290d9de":"plot_box('X12',color='b')","ec148f72":"plot_box('X1','b')","cd9cf39b":"data","de4c8169":"plt.figure(figsize=(20,8))\nsns.boxenplot(x = 'X5', y = 'X1',data = data, color='g')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('values', fontsize=12)\nplt.title('Boxplot for X5')\nplt.show()","78e4efa2":"cnt_srs = data['X5'].value_counts()\n\nplt.figure(figsize=(20,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='g')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('values', fontsize=12)\nplt.title('Count for X5')\nplt.show()","1a94dd07":"cnt_srs = data['X3'].value_counts()\n\nplt.figure(figsize=(20,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='r')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('values', fontsize=12)\nplt.title('Count for X3')\nplt.show()","c2a55ec4":"cnt_srs = data['X2'].value_counts()\n\nplt.figure(figsize=(20,8))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='y')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('values', fontsize=12)\nplt.title('Count for X2')\nplt.show()","c7cbf9d4":"fig = px.scatter(data, x=\"X1\", y=\"X20\", trendline=\"ols\",color = 'X2',color_continuous_scale='ylgn',template = 'simple_white')\nfig.show()","88b55dbf":"fig = px.scatter(data, x=\"X1\", y=\"X12\", marginal_y=\"violin\",color = 'X3',marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\nfig.show()","0bba54f1":"data.head()","d2ed2ae7":"df_corr = data[['X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21','X22', 'X23', 'Y']]","ba03fef7":"sns.pairplot(df_corr,hue = 'Y')","f140fd01":"corr = df_corr.corr(method='spearman')\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, annot=True, vmax=.8, square=True,cmap = 'rainbow_r');","92491101":"## Zoomed out correlation map\nk = 10\ncols = corr.nlargest(k, 'Y')['Y'].index\ncm = np.corrcoef(data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True,cmap = 'rainbow_r', fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()\n","8b90504f":"# Binned Age Feature\nbins = [20, 29, 39, 49, 59, 69, 81]\nbins_names = [1, 2, 3, 4, 5, 6]\ndata['age_binned'] = pd.cut(data['X5'], bins, labels=bins_names)\ndata['age_binned'] = pd.to_numeric(data['age_binned'])\nprint(data.sample())","d8a30559":"data.info()","c5f26299":"# Replacing all Negative values with 0\ndata['X6'] = data['X6'].apply(lambda x : x if x > 0 else 0)\ndata['X7'] = data['X6'].apply(lambda x : x if x > 0 else 0)\ndata['X8'] = data['X6'].apply(lambda x : x if x > 0 else 0)\ndata['X9'] = data['X6'].apply(lambda x : x if x > 0 else 0)\ndata['X10'] = data['X6'].apply(lambda x : x if x > 0 else 0)\ndata['X11'] = data['X6'].apply(lambda x : x if x > 0 else 0)\nprint(data.head(10))","c149a38b":"data[data.X1 > 300000][['X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21','X22', 'X23', 'Y']]","b0c67dc8":"data","8e2c7a99":"data['Percetage_amount_1'] = (data.X1 - data.X12) \/ data.X1\ndata['Percetage_amount_2'] = (data.X1 - data.X13) \/ data.X1\ndata['Percetage_amount_3'] = (data.X1 - data.X14) \/ data.X1\ndata['Percetage_amount_4'] = (data.X1 - data.X15) \/ data.X1\ndata['Percetage_amount_5'] = (data.X1 - data.X16) \/ data.X1\ndata['Percetage_amount_6'] = (data.X1 - data.X17) \/ data.X1\nprint(data.sample())","f458bfe4":"cols_to_drop = ['X5']\ndata_preprocess = data.drop(cols_to_drop,1)","91070046":"data_preprocess.to_csv('preprocess_data.csv',index=False)\nprint(f\"Train Rows and columns {data_preprocess.shape[0]}, {data_preprocess.shape[1]}\")","458f1d13":"X = data_preprocess.drop('Y', axis=1)\ny =  data_preprocess['Y']","bca45ba0":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)","349eea09":"print(\"Shape of X_train :\", X_train.shape)\nprint(\"Shape of X_test :\", X_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","cd12cf76":"from imblearn.combine import SMOTETomek\noversample = SMOTETomek(random_state=42)\nX_train_resam, y_train_resam = oversample.fit_resample(X_train, y_train)\ncounter = Counter(y_train)\ncounter_ = Counter(y_train_resam)\n\nprint('before oversampling',counter)\nprint('after oversampling' ,counter_)","d12baecf":"## Scaling the Data with Robust Scaler due to oultiers observed in EDA\nscaler = RobustScaler()\nX_train_resam = scaler.fit_transform(X_train_resam)\nX_test = scaler.transform(X_test)","aed67780":"clfs = []\n\nclfs.append((\"LogRegression\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGB\",\n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DTC\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RFClassifier\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GBClassifier\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier())]))) \n\n\nscoring = 'f1_weighted'\nn_folds = 10\nmsgs = []\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=None)\n    cv_results = cross_val_score(model, X, y, \n                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    msgs.append(msg)\n    print(msg)","de16ad9e":"fig = plt.figure(figsize=(10,5))\nsns.set_context('notebook', font_scale=1.1)\nfig.suptitle('Algorithm Comparison - F1 (cv=10)')\nax = fig.add_subplot(111)\nplt.boxplot(results, showmeans=True)\nax.set_xticklabels(names)\nax.set_ylabel('F1')\nax.set_ylim([0.75,1])\nplt.box(False)","1fbc6837":"lr = LogisticRegression()\nlr.fit(X_train_resam, y_train_resam)\ny_pred_lr = lr.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_lr))\nprint(\"F1 Score \",f1_score(y_test, y_pred_lr))\nprint(\"Recall Score \",recall_score(y_test, y_pred_lr))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_lr))\n\nprint()\nprint(classification_report(y_test, y_pred_lr))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,lr.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","1e8f5328":"knn = KNeighborsClassifier()\nknn.fit(X_train_resam, y_train_resam)\ny_pred_knn = knn.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_knn))\nprint(\"F1 Score \",f1_score(y_test, y_pred_knn))\nprint(\"Recall Score \",recall_score(y_test, y_pred_knn))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_knn))\n\nprint()\nprint(classification_report(y_test, y_pred_knn))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,knn.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","8b67203b":"dt = DecisionTreeClassifier()\ndt.fit(X_train_resam, y_train_resam)\ny_pred_dt = dt.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_dt))\nprint(\"F1 Score \",f1_score(y_test, y_pred_dt))\nprint(\"Recall Score \",recall_score(y_test, y_pred_dt))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_dt))\n\nprint()\nprint(classification_report(y_test, y_pred_dt))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,dt.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","9067c60a":"rf = RandomForestClassifier(n_estimators=100,random_state=42)\nrf.fit(X_train_resam, y_train_resam)\ny_pred_rf = rf.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_rf))\nprint(\"F1 Score \",f1_score(y_test, y_pred_rf))\nprint(\"Recall Score \",recall_score(y_test, y_pred_rf))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_rf))\n\nprint()\nprint(classification_report(y_test, y_pred_rf))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,rf.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","c38d319c":"gb = GradientBoostingClassifier(n_estimators=100,random_state=42)\ngb.fit(X_train_resam, y_train_resam)\ny_pred_gb = gb.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_gb))\nprint(\"F1 Score \",f1_score(y_test, y_pred_gb))\nprint(\"Recall Score \",recall_score(y_test, y_pred_gb))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_gb))\n\nprint()\nprint(classification_report(y_test, y_pred_gb))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,gb.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","d26b02a2":"xgb = XGBClassifier(n_estimators=100,random_state=42)\nxgb.fit(X_train_resam, y_train_resam)\ny_pred_xgb = xgb.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_xgb))\nprint(\"F1 Score \",f1_score(y_test, y_pred_xgb))\nprint(\"Recall Score \",recall_score(y_test, y_pred_xgb))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_xgb))\n\nprint()\nprint(classification_report(y_test, y_pred_xgb))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,xgb.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","344524bf":"import lightgbm as ltb\nlgbm = ltb.LGBMClassifier()\nlgbm.fit(X_train_resam, y_train_resam)\ny_pred_lgb = lgbm.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_lgb))\nprint(\"F1 Score \",f1_score(y_test, y_pred_lgb))\nprint(\"Recall Score \",recall_score(y_test, y_pred_lgb))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_lgb))\n\nprint()\nprint(classification_report(y_test, y_pred_lgb))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,lgbm.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","bd59148c":"!pip install optuna -q","5f91ae8f":"import optuna","4bab5da1":"def objective(trial):\n    \n    param = {\n        'loss': 'deviance', \n        'random_state': 101,\n        'n_estimators' : trial.suggest_int('n_estimators', 100, 500),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.001,0.008,0.01,0.02,0.03,0.04,0.05]),\n        'max_depth': trial.suggest_categorical('max_depth', [1,2,3,4,5]),\n        'min_samples_leaf' : trial.suggest_int('min_samples_leaf', 1, 10)}\n    model = GradientBoostingClassifier(**param)  \n    \n    model.fit(X_train_resam,y_train_resam)\n    \n    preds = model.predict(X_test)\n    score = f1_score(y_test,preds)\n    \n    return score","ed181edd":"%%time\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","f4dd17c3":"optuna.visualization.plot_param_importances(study)","ee9031d3":"tuned_params=study.best_params   \ntuned_params","d12ade10":"tuned_params['random_state'] = 101","77cfff76":"tuned_gb = GradientBoostingClassifier(**tuned_params)\ntuned_gb.fit(X_train_resam, y_train_resam)\ny_pred_tuned_gb = tuned_gb.predict(X_test)\n\nprint(\"Accuracy of model \",accuracy_score(y_test, y_pred_tuned_gb))\nprint(\"F1 Score \",f1_score(y_test, y_pred_tuned_gb))\nprint(\"Recall Score \",recall_score(y_test, y_pred_tuned_gb))\nprint(\"Balanced Accuracy Score \",balanced_accuracy_score(y_test, y_pred_tuned_gb))\n\nprint()\nprint(classification_report(y_test, y_pred_tuned_gb))\nplt.figure(figsize=(4,3))\nConfMatrix = confusion_matrix(y_test,tuned_gb.predict(X_test))\nsns.heatmap(ConfMatrix,annot=True, cmap=\"Blues\", fmt=\"d\", \n            xticklabels = ['Non-default', 'Default'], \n            yticklabels = ['Non-default', 'Default'])\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title(\"Confusion Matrix\")","ac327a14":"## Feature Importance Plot \nfeat_imp = pd.Series(tuned_gb.feature_importances_, index=data_preprocess.drop(['Y'], axis=1).columns)\nfeat_imp.nlargest(20).plot(kind='bar', figsize=(8,10))","854ed109":"## Persisting Final Model\nimport pickle\nfilename = '_gb.pkl'\npickle.dump(tuned_gb, open(filename, 'wb'))","3cb0eeb5":"> A number of 6,636 out of 30,000 (or 22%) of defaulters . The data has unbalance with respect of the target value\n\n","213e27a7":"<img src = \"https:\/\/www.indiewire.com\/wp-content\/uploads\/2018\/12\/r4offmlfqwjhmvhysomm.jpg\">\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content:<\/h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#one_\" role=\"tab\" aria-controls=\"profile\">1.Exploratory Data Analysis<span class=\"badge badge-primary badge-pill\"><\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#one_\" role=\"tab\" aria-controls=\"profile\">2.Data Preprocessing<span class=\"badge badge-primary badge-pill\"><\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#one_\" role=\"tab\" aria-controls=\"profile\">3.Predictive Models<span class=\"badge badge-primary badge-pill\"><\/span><\/a>","d930e4d5":"## Phase III Predictive Models\n### Model Selection\n### Hyperparameter Tuning\n### Persist Final Model","92fdb12e":"## Bivariate Analysis","68fabf88":"## Phase II - Data Preprocessing\n#### Feature Engineering\n#### Over Sampling - Imbalanced Dataset with SMOTETomek","842ce2a1":"### Conclusion\n\n* Offical Scoring Metrics  - Balanced Accuracy, F1 Score and Recall Score.\n* After Model Selection Final Model Picked up <b> Gradient Boosting Classifier <\/b>.\n* Please Refer Doc for more info.\n\n#### Hyperparameter Tuning Using Optuna.\n* [Reference](https:\/\/optuna.org\/)","d083f669":"## Multivariate Analysis","39439863":"<img src = \"https:\/\/drinkwhen.ca\/wp-content\/uploads\/2020\/04\/Catch-Me-If-You-Can-Drinking-Game.gif\">","60a76e5d":"## Phase I - Exploratory Data Analysis\n### Univariate \n### Bivariate \n### Multivariate","3e236fbf":"## Univariate Analysis"}}