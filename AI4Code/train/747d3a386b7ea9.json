{"cell_type":{"62c64dc2":"code","dce97417":"code","95272d3b":"code","5f73f1fd":"code","7a8cb21e":"code","750ba097":"code","a12f2093":"code","e984299c":"code","43448c8d":"code","a8843076":"code","5af9f0aa":"code","948b4532":"code","35b11503":"code","af7fd06a":"code","b6e3330f":"code","75e064ed":"code","2ed6c19c":"code","e8b62d77":"code","c76383ee":"code","ea22a569":"code","455c65b7":"code","7bad5ee1":"code","f113d19b":"code","b93840b0":"code","f64a2a10":"code","6eae9c25":"code","9bbfa3aa":"code","9d1b2e47":"code","c1aff1fd":"markdown","57f15806":"markdown","d0ac644e":"markdown","c0bcd03b":"markdown","3336a85b":"markdown","87769f7e":"markdown","6be70bc8":"markdown","654377f5":"markdown","96faaba2":"markdown","842c13c8":"markdown","282d0a68":"markdown","21d39999":"markdown","83bbe913":"markdown","924c38e4":"markdown","2593e73e":"markdown","ae3c1586":"markdown","254ed955":"markdown","e3a3bcd1":"markdown","bab50c67":"markdown","8ff1c851":"markdown","0ffce587":"markdown","0c3351e1":"markdown","16d80b3c":"markdown"},"source":{"62c64dc2":"# Standard Libraries\nimport os\nimport random\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\n# Libraries For Audio Files\nimport soundfile as sf\nimport librosa\nimport librosa.display\nimport IPython.display as display\n\n# Keras Libraries For The Neural Network\nfrom keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dce97417":"path = '\/kaggle\/input\/ravdess-emotional-speech-audio\/'\nos.listdir(path)[0:2]","95272d3b":"emotions = {1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad', 5: 'angry', 6: 'fearful', 7: 'disgust', 8: 'surprised'}","5f73f1fd":"def read_wav_file(path, file):\n    \"\"\" Read wav audio file and return numpy array and samplerate\"\"\"\n    \n    data, samplerate = sf.read(path+file)\n    return data, samplerate\n\ndef display_waveplot(data, sr):\n    \"\"\" Plot the wave plot by numpy array and samplerate \"\"\"\n    \n    plt.figure(figsize=(14, 5))\n    librosa.display.waveplot(data, sr=sr)\n    plt.grid()\n    plt.show()\n    \ndef plot_spectrogram(data, samplerate):\n    \"\"\" Plot spectrogram with mel scaling \"\"\"\n    \n    sr = samplerate\n    spectrogram = librosa.feature.melspectrogram(data, sr=sr)\n    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n    librosa.display.specshow(log_spectrogram, sr=sr, x_axis='time', y_axis='mel')","7a8cb21e":"actors = ['Actor_'+str(i).zfill(2) for i in range(1, 25)]","750ba097":"columns = ['File', 'Modality', 'Vocal', 'Emotion', 'Intensity', 'Statement', 'Repetition', 'Actor']\ndf = pd.DataFrame(columns = columns)\nfiles = []\nfor actor in actors:\n    files.extend(os.listdir(path+actor))\ndf['File'] = files\nfile = files[0]\nfor i in range(len(files)):\n    file = files[i]\n    integer_list = list(map(int, file.split('.')[0].split('-')))\n    df.loc[i, df.columns[1:]] = integer_list\n\ndf.head()","a12f2093":"row = 0\nfile = df.loc[row, 'File']\npath_file = path+'Actor_'+str(df.loc[row, 'Actor']).zfill(2)+'\/'\nfile","e984299c":"display.Audio(path_file+file)","43448c8d":"data, sr = read_wav_file(path_file, file)\nprint('Lenght Data Array:', len(data))\nprint('Samplerate:', sr)\nprint('Lenght Audio:', len(data)\/sr)","a8843076":"display_waveplot(data, sr)","5af9f0aa":"plot_spectrogram(data, sr)","948b4532":"for row in df.index:\n    file = df.loc[row, 'File']\n    path_file = path+'Actor_'+str(df.loc[row, 'Actor']).zfill(2)+'\/'\n    data, sr = read_wav_file(path_file, file)\n    df.loc[row, 'Lenght_Data_Array'] = len(data)\n    df.loc[row, 'Samplerate'] = sr\ndf['Seconds'] = df['Lenght_Data_Array']\/df['Samplerate'] ","35b11503":"df['Emotion'].value_counts().sort_index()","af7fd06a":"df['Intensity'].value_counts().sort_index()","b6e3330f":"df['Statement'].value_counts()","75e064ed":"df['Seconds'].hist(bins=20)\nplt.show()","2ed6c19c":"labels= pd.DataFrame(0, index=df.index, columns=emotions.values())\nfor row in labels.index:\n    labels.loc[row, labels.columns[df.loc[row, 'Emotion']-1]]=1\nlabels['File'] = df['File']","e8b62d77":"labels.head()","c76383ee":"list_IDs_train, list_IDs_val = train_test_split(list(df.index), test_size=0.33, random_state=2021)","ea22a569":"batch_size = 32","455c65b7":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, labels, batch_size):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.labels = labels\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.list_IDs))\n        self.data_lenght = 254000\n        self.num_labels = 8\n    \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_*self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        X = X.reshape((self.batch_size, 100, 2540\/\/2))\n        return X, y\n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, self.data_lenght\/\/2))\n        y = np.zeros((self.batch_size, self.num_labels))\n        for i, ID in enumerate(list_IDs_temp):\n            file = self.labels.loc[i, 'File']\n            actor = file.split('.')[0].split('-')[-1]\n            path_file = self.path+'Actor_'+str(actor+'\/')\n            audio_file, audio_sr = read_wav_file(path_file, file)\n            lenght = len(audio_file)\n            audio_file_fft = np.abs(np.fft.fft(audio_file)[: lenght\/\/2])\n            # scale data\n            audio_file_fft = (audio_file_fft-audio_file_fft.mean())\/audio_file_fft.std()\n            X[i, :(lenght\/\/2)] = audio_file_fft\n            y[i, ] = self.labels.loc[ID, self.labels.columns[:-1]].values\n        return X, y","7bad5ee1":"train_generator = DataGenerator(path, list_IDs_train, labels, batch_size)\nval_generator = DataGenerator(path, list_IDs_val, labels, batch_size)","f113d19b":"epochs = 2\nlernrate = 2e-3","b93840b0":"model = Sequential()\nmodel.add(Conv1D(64, input_shape=(100, 2540\/\/2,), kernel_size=5, strides=4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=(4)))\nmodel.add(Conv1D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(8, activation='sigmoid'))","f64a2a10":"model.compile(optimizer = Adam(lr=lernrate),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])","6eae9c25":"model.summary()","9bbfa3aa":"history = model.fit_generator(generator=train_generator, validation_data=val_generator, epochs = epochs, workers=4)","9d1b2e47":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\nfig.subplots_adjust(hspace = .2, wspace=.2)\naxs = axs.ravel()\nloss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\naxs[0].plot(epochs, loss, 'bo', label='loss_train')\naxs[0].plot(epochs, loss_val, 'ro', label='loss_val')\naxs[0].set_title('Value of the loss function')\naxs[0].set_xlabel('epochs')\naxs[0].set_ylabel('value of the loss function')\naxs[0].legend()\naxs[0].grid()\nacc = history.history['binary_accuracy']\nacc_val = history.history['val_binary_accuracy']\naxs[1].plot(epochs, acc, 'bo', label='accuracy_train')\naxs[1].plot(epochs, acc_val, 'ro', label='accuracy_val')\naxs[1].set_title('Accuracy')\naxs[1].set_xlabel('Epochs')\naxs[1].set_ylabel('Value of accuracy')\naxs[1].legend()\naxs[1].grid()\nplt.show()","c1aff1fd":"**Interpretation**\n* Emotion: 02 = calm.\n* Emotional intensity: 01 = normal.\n* Statement: 02 = \"Dogs are sitting by the door\".\n* Repetition: 02 = 2nd repetition.\n\nListen to the sound file:","57f15806":"# Define Model","d0ac644e":"# Exploratory Data Analysis","c0bcd03b":"# Libraries","3336a85b":"# Audio Data Generator\nWe use a Data Generator to load the data on demand.","87769f7e":"# Parameters","6be70bc8":"Plot the spectogram with mel scaling","654377f5":"# Encode Target Labels","96faaba2":"# Train And Validation Data","842c13c8":"# Feature Engineering\nWe extend the data frame df by the features\n* Lenght_Data_Array\n* Samplerate","282d0a68":"The features emotion and intensity are not evenly distributed:","21d39999":"The audio files dosen't have the same lenght:","83bbe913":"# Filenames\n**File naming convention**\n\nEach of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n\n**Filename identifiers**\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nIn this dataset we have\n* Modality = 03 and\n* Vocal channel = 01\n\nfor all samples.\n\n**Filename example: 03-01-06-01-02-01-12.wav**\n\n1. Audio-only (03)\n2. Speech (01)\n3. Fearful (06)\n4. Normal intensity (01)\n5. Statement \"dogs\" (02)\n6. 1st Repetition (01)\n7. 12th Actor (12)\n8. Female, as the actor ID number is even.","924c38e4":"# Overview\nThere are 24 actors. For each of the actores there are existing 60 wave files.","2593e73e":"# Intro\nWelcome to the [RAVDESS Emotional speech audio Emotional speech dataset](https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio).\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/107620\/256618\/6b13d2f1d0b8d8a00b5aac95477c79a2\/dataset-cover.png)\n\nWe recommend [this notebook](https:\/\/www.kaggle.com\/drcapa\/recognizesongapp-fromscratch-tutorial) for handling audio data. For a facial emotion classification we recommend [this notebook](https:\/\/www.kaggle.com\/drcapa\/facial-expression-eda-cnn).\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span>","ae3c1586":"The features statement and repetition are evenly distributed:","254ed955":"# Analyse Training","e3a3bcd1":"# Path","bab50c67":"Next we load the wave file:","8ff1c851":"# A Sample File\nWe take focus on the first sample of the data frame to show how to handle and interpret the file.","0ffce587":"Waveplot of the data array:","0c3351e1":"We create a data frame with all meta data informations:","16d80b3c":"# Functions"}}