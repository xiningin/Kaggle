{"cell_type":{"d16a3ca2":"code","3ed9dc34":"code","b7ff655f":"code","382e84ba":"code","4304b351":"code","36b3e4a9":"code","abe67d13":"code","bc4f2cfa":"code","17371b21":"code","f3e19918":"code","9f55d133":"code","25074835":"code","5b178cae":"code","3c5208fe":"code","3b60dbb7":"code","8eb1f9f9":"code","5b859a8b":"code","2418cc8c":"code","da79e061":"code","e2269e6b":"code","90251492":"code","5235bf6f":"code","3a9d95e8":"code","c0d46158":"code","b0a0ba96":"code","493e4fd7":"code","b5143ff7":"code","8443b90a":"code","75b46512":"code","7a027052":"code","20cd1a29":"code","e96d4d51":"code","c3bbe4c2":"code","f099cf4b":"code","754c9825":"code","421eb264":"code","5e23f73b":"code","ea01158d":"code","71bfd598":"code","c392f551":"code","5626e17b":"code","48f9322a":"markdown","8aa92908":"markdown","e295f75e":"markdown","91b8b9f2":"markdown","89ddc262":"markdown","3ea34008":"markdown","897bf76d":"markdown","8ba96c0b":"markdown","e10deb9c":"markdown","7f409817":"markdown","91791427":"markdown","1ff0b051":"markdown","3ebee812":"markdown","feae76b6":"markdown","3e7d8858":"markdown","3978d5d0":"markdown","2e5c8d70":"markdown","2da0adda":"markdown","5194b4fa":"markdown","a90c5529":"markdown","2b0702bc":"markdown","19565d75":"markdown","f9e4114f":"markdown","60f518ff":"markdown","855d6211":"markdown"},"source":{"d16a3ca2":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nfigure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport json\nimport os\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Imputer\nfrom scipy.stats import skew \nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n# models\nfrom xgboost import XGBRegressor\nimport warnings\n# Ignore useless warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n\n# Avoid runtime error messages\npd.set_option('display.float_format', lambda x:'%f'%x)\n\n# make notebook's output stable across runs\nnp.random.seed(42)","3ed9dc34":"# Read CSVs\nfetch_from = '..\/input\/train.csv'\ntrain = pd.read_csv(fetch_from)\n\nfetch_from = '..\/input\/test.csv'\ntest = pd.read_csv(fetch_from)","b7ff655f":"# How many datapoints in the training set?\ntrain.shape","382e84ba":"# How many datapoints in the test set?\ntest.shape","4304b351":"# Look at sample datapoints in the training set\ntrain.sample(5)","36b3e4a9":"# Look at sample datapoints in the test set\ntest.sample(5)","abe67d13":"# How many missing values does the dataset have?\ntrain.isnull().sum().sum()","bc4f2cfa":"# Which columns have the most missing values?\ndef missing_data(df):\n    total = df.isnull().sum()\n    percent = (df.isnull().sum()\/train.isnull().count()*100)\n    missing_values = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in df.columns:\n        dtype = str(df[col].dtype)\n        types.append(dtype)\n    missing_values['Types'] = types\n    missing_values.sort_values('Total',ascending=False,inplace=True)\n    return(np.transpose(missing_values))\nmissing_data(train)","17371b21":"# Let's plot these missing values(%) vs column_names\nmissing_values_count = (train.isnull().sum()\/train.isnull().count()*100).sort_values(ascending=False)\nplt.figure(figsize=(15,10))\nbase_color = sns.color_palette()[0]\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nsns.barplot(missing_values_count[:10].index.values, missing_values_count[:10], color = base_color)","f3e19918":"train.describe()","9f55d133":"test.describe()","25074835":"train.hist(bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","5b178cae":"test.hist(bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","3c5208fe":"train.describe(include='O')","3b60dbb7":"test.describe(include='O')","8eb1f9f9":"train_eda = train.copy()\nlabel_col = 'SalePrice'","5b859a8b":"col_name = 'HouseStyle'\nfreq_table = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"count\")      # Name the count column\nfreq_table_per = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"percentage\", normalize=True)\nfreq_table['percentage'] = freq_table_per['percentage']\nfreq_table.sort_values(by='count', ascending=False)","2418cc8c":"# First, we need to create a function to decide how many blocks to allocate to each category\ndef percentage_blocks(df, var):\n    \"\"\"\n    Take as input a dataframe and variable, and return a Pandas series with\n    approximate percentage values for filling out a waffle plot.\n    \"\"\"\n    # compute base quotas\n    percentages = 100 * df[var].value_counts() \/ df.shape[0]\n    counts = np.floor(percentages).astype(int) # integer part = minimum quota\n    decimal = (percentages - counts).sort_values(ascending = False)\n    # add in additional counts to reach 100\n    rem = 100 - counts.sum()\n    for cat in decimal.index[:rem]:\n        counts[cat] += 1\n    return counts\n\n# Second, plot those counts as boxes in the waffle plot form\nwaffle_counts = percentage_blocks(train_eda, col_name)\nprev_count = 0\n# for each category,\nfor cat in range(waffle_counts.shape[0]):\n    # get the block indices\n    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n    # and put a block at each index's location\n    x = blocks % 10 # use mod operation to get ones digit\n    y = blocks \/\/ 10 # use floor division to get tens digit\n    plt.bar(x = x, height = 0.9, width = 0.9, bottom = y)\n    prev_count += waffle_counts[cat]\n\n# Third, we need to do involve aesthetic cleaning to polish it up for interpretability. We can take away the plot border and ticks, since they're arbitrary, but we should change the limits so that the boxes are square. We should also add a legend so that the mapping from colors to category levels is clear.\n# aesthetic wrangling\nplt.legend(waffle_counts.index, bbox_to_anchor = (1, 0.5), loc = 6)\nplt.axis('off')\nplt.axis('square')","da79e061":"# Area depicts the distribution of points. > width = > the number of points\nbase_color = sns.color_palette()[0]\nplt.figure(figsize=(20,15))\nplt.xticks(rotation=45)\nsns.boxplot(data = train_eda, x = 'HouseStyle', y = 'SalePrice', color = base_color);","e2269e6b":"col_name = 'HouseStyle'\nfreq_table = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"count\")      # Name the count column\nfreq_table_per = pd.crosstab(index=train_eda[col_name],  # Make a crosstab\n                              columns=\"percentage\", normalize=True)\nfreq_table['percentage'] = freq_table_per['percentage']\nfreq_table.sort_values(by='count', ascending=False)","90251492":"# Relative frequency variation - Plotting absolute counts on axis and porportions on the bars\n# Barchart sorted by frequency\nbase_color = sns.color_palette()[0]\ncat_order = train_eda[col_name].value_counts().index\nplt.figure(figsize=(15,10))\nplt.xticks(rotation = 90)\nsns.countplot(data = train_eda, x = col_name, order = cat_order, color = base_color);\n\n# add annotations\nn_points = train_eda.shape[0]\ncat_counts = train_eda[col_name].value_counts()\nlocs, labels = plt.xticks() # get the current tick locations and labels\n\n# loop through each pair of locations and labels\nfor loc, label in zip(locs, labels):\n\n    # get the text property for the label to get the correct count\n    count = cat_counts[label.get_text()]\n    pct_string = '{:0.1f}%'.format(100*count\/n_points)\n\n    # print the annotation just below the top of the bar\n    plt.text(loc, count+4, pct_string, ha = 'center', color = 'black')","5235bf6f":"# Area depicts the distribution of points. > width = > the number of points\nbase_color = sns.color_palette()[0]\nplt.figure(figsize=(20,15))\nplt.xticks(rotation=45)\nsns.boxplot(data = train_eda, x = 'Neighborhood', y = 'SalePrice', color = base_color);","3a9d95e8":"col_name = 'GrLivArea'\nhist_kws={\"alpha\": 0.3}\nplt.figure(figsize=(15,10))\n# Trim long-tail\/other values\n# plt.xlim(0, 1200)\nsns.distplot(train_eda[col_name], hist_kws=hist_kws);","c0d46158":"col_name = 'OverallQual'\nhist_kws={\"alpha\": 0.3}\nplt.figure(figsize=(15,10))\n# Trim long-tail\/other values\n# plt.xlim(0, 1200)\nsns.distplot(train_eda[col_name], hist_kws=hist_kws);","b0a0ba96":"col_name = 'OverallQual'\nbase_color = sns.color_palette()[0]\nplt.figure(figsize=(20,15))\nplt.xticks(rotation=45)\nsns.boxplot(data = train_eda, x = col_name, y = 'SalePrice', color = base_color)","493e4fd7":"# Which features are the most correlated to our target variable, SalePrice?\ncorr_matrix = train_eda.corr()\nplt.subplots(figsize=(15,10))\nsns.heatmap(corr_matrix, vmax=1.0, square=True, cmap=\"Blues\")","b5143ff7":"# Get the top 10 most correlated features\ncorr_matrix = train_eda.corr()\ncorr_matrix[label_col].sort_values(ascending=False)[:10]","8443b90a":"from pandas.plotting import scatter_matrix\n\nattributes = [label_col, \"OverallQual\", \"LotArea\", \"BedroomAbvGr\", \"GrLivArea\"]\nscatter_matrix(train_eda[attributes], figsize=(15, 15));","75b46512":"col_name = 'GarageArea'\ntrain_eda.plot(kind=\"scatter\", x=label_col, y=col_name, alpha=0.2, figsize=(15,10))\n# changing axis labels to only show part of the graph\nplt.axis([0, 400000, 0, 1200])","7a027052":"train_eda.plot(kind=\"scatter\", x=label_col, y=\"GarageArea\", alpha=0.4,\n             s=train_eda[\"BedroomAbvGr\"], label=\"BedroomAbvGr\", figsize=(20,15),\n             c=\"YrSold\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\nplt.axis([0, 400000, 0, 1200])\nplt.legend();","20cd1a29":"train_fe = train.copy()\ntest_fe = test.copy()","e96d4d51":"train_fe = train_fe[train_fe.GrLivArea < 4300]\ntrain_fe.reset_index(drop=True, inplace=True)\n\ntrain_fe[\"SalePrice\"] = np.log1p(train_fe[\"SalePrice\"])\ny = train_fe.SalePrice.reset_index(drop=True)\ntrain_features = train_fe.drop(['SalePrice'], axis=1)\ntest_features = test.copy()\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)\n\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)","c3bbe4c2":"features['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nfeatures.update(features[objects].fillna('None'))","f099cf4b":"# Convert categorical variables stored as numbers to strings\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","754c9825":"# Remove Id\ntrain_ID = train_fe['Id']\ntest_ID = test_fe['Id']\n\ntrain_fe.drop(['Id'], axis=1, inplace=True)\ntest_fe.drop(['Id'], axis=1, inplace=True)\n\n# Create new features\nfeatures = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\n# Fix skewed features\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(X):, :]\n\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()","421eb264":"X.head()","5e23f73b":"X.info()","ea01158d":"features_train = train.copy()\nfeatures_train.dropna(axis=0, subset=['SalePrice'], inplace=True)\nlabel = features_train.SalePrice\nfeatures_train = features_train.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\ntrain_X, test_X, train_y, test_y = train_test_split(features_train.as_matrix(), label.as_matrix(), test_size=0.25)\n\nmy_imputer = Imputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.transform(test_X)","71bfd598":"xgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006, random_state=42);\n\nxgb.fit(train_X, train_y)\npredictions = xgb.predict(test_X)","c392f551":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\ndef mae(y, y_pred):\n    return mean_absolute_error(predictions, test_y)","5626e17b":"print('MAE score on train data:')\nprint(mae(predictions, test_y))","48f9322a":"## Reconcile feature types","8aa92908":"The training and test sets seem to have consistently distributed features. But just to make sure this is the case, let's plot features from both the training and test sets!","e295f75e":"The distributions of features across the training and test data do indeed seem consistent. This is excellent!","91b8b9f2":"# Overview of training and test datasets","89ddc262":"## What's missing?","3ea34008":"## Remove outliers","897bf76d":"Before we go forward, let's make a copy of the training set!","8ba96c0b":"# Feature Engineering and Prep","e10deb9c":"Note: .corr() only captures linear relationships, so it's not the most reliable way to detect correlations. So let's plot a few of these features on a scatterplot matrix!","7f409817":"# What're the most correlated features?","91791427":"# Next steps:\n- Try a bunch of other models, using GridSearch and CrossValidation\n- A combo of XGRegressor, CatBoostRegressor, RandomForestRegressor, SVR and RidgeRegression worked best for me\n\n## If you like this kernel, please give it an upvote. Thank you! :)","1ff0b051":"# Time to deep dive into the data!","3ebee812":"# Train a basic XGBoost to serve as a benchmark","feae76b6":"Alright, we have some understanding of the features in our training set! Let's see how useful they might be in predicting our SalePrice!","3e7d8858":"## Fill missing values","3978d5d0":"Before we mvoe forward, let's make a copy of the training set!","2e5c8d70":"Getting started with Kaggle can be an intimidating prospect!\n\nSo I wrote this kernel to help you get started quickly. You can fork it, plug in a dataset you wanna you\u2019ve been itching to explore and be doing analysis in 5 mins.\n\n## If you like this kernel, please give it an upvote. Thank you! :)\n\nHappy Kagglin!","2da0adda":"How good a predictor is GarageArea for SalePrice?","5194b4fa":"Let's add some more context to our GarageArea vs SalePrice plot! Let's color it by the year sold (YrSold) and change the size accoding to the number of bedrooms above ground (BedroomAbvGr).","a90c5529":"## Closer look at a few of these variables","2b0702bc":"## Other Feature Engineering","19565d75":"## Categorical Features","f9e4114f":"## Defining our scoring metrics","60f518ff":"Defining a basic XGBoost model.","855d6211":"## Numerical Features"}}