{"cell_type":{"adfa8701":"code","9c48c1e4":"code","c0ff8c71":"code","c6cfae5a":"code","09744fa1":"code","5bd59421":"code","58b49710":"code","645dbece":"code","4ad2bd90":"code","0a46f136":"code","168f0153":"code","219047a9":"code","4003e472":"code","67eddcb9":"code","8c446b1c":"code","6577742a":"code","430475e3":"code","367d31b8":"code","73cfa390":"code","529fe06b":"code","730858d1":"code","7cae0261":"code","fda5aa7a":"code","7e38419b":"code","b793e946":"code","dbfc2632":"code","97927bd6":"code","5b17944e":"code","65c8c1af":"code","38c5fa4a":"code","4a57efa1":"code","37f419e5":"code","0de6c898":"code","2f224cd8":"code","cefdbf33":"code","bddb430a":"code","5c10c5b2":"code","ff4979d2":"code","b08df41c":"code","ecd49fbb":"code","90582f3f":"code","b253178f":"code","10ddc3b4":"code","7b17886f":"code","0d4bda4e":"code","8a64d63d":"code","7fb0e2fd":"code","93d110dd":"markdown","28a47365":"markdown","73e598ac":"markdown","c715b5e0":"markdown","fdb7720d":"markdown","362764c8":"markdown","098e3229":"markdown","e8532931":"markdown"},"source":{"adfa8701":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9c48c1e4":"import sklearn \n\nfrom sklearn.model_selection import train_test_split","c0ff8c71":"# Load Yelp business data\nbiz_f = open('..\/input\/yelp_academic_dataset_business.json')\nbiz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\nbiz_f.close()","c6cfae5a":"biz_df.shape","09744fa1":"# Load Yelp reviews data\nreview_file = open('..\/input\/yelp_academic_dataset_review.json')\nreview_df = pd.DataFrame([json.loads(next(review_file)) for x in range(biz_df.shape[0])])\nreview_file.close()","5bd59421":"review_df.shape","58b49710":"biz_df.dropna(subset=['categories'], inplace=True)","645dbece":"# Pull out only Nightlife and Restaurants businesses\ntwo_biz = biz_df[biz_df.apply(lambda x: 'Nightlife' in x['categories'] or 'Restaurants' in x['categories'], axis=1)]","4ad2bd90":"two_biz.head()","0a46f136":"# Join with the reviews to get all reviews on the two types of business\ntwo_biz_reviews = two_biz.merge(review_df, on='business_id', how='inner')","168f0153":"two_biz_reviews.shape","219047a9":"# Trim away the features we won't use\ntwo_biz_reviews = two_biz_reviews[['business_id', 'name', 'stars_y', 'text', 'categories']]","4003e472":"two_biz_reviews.shape","67eddcb9":"# Create the target column--True for Nightlife businesses, and False otherwise\ntwo_biz_reviews['target'] = two_biz_reviews.apply(lambda x: 'Nightlife' in x['categories'], axis=1)","8c446b1c":"two_biz_reviews.head()","6577742a":"two_biz_reviews.target.value_counts(normalize=True)","430475e3":"# Create a class-balanced subsample to play with\nnightlife = two_biz_reviews[two_biz_reviews.apply(lambda x: 'Nightlife' in x['categories'], axis=1)]\nrestaurants = two_biz_reviews[two_biz_reviews.apply(lambda x: 'Restaurants' in x['categories'], axis=1)]\n\nnightlife_subset = nightlife.sample(frac=0.74, random_state=123)\nrestaurant_subset = restaurants.sample(frac=0.35, random_state=123)\n\ncombined = pd.concat([nightlife_subset, restaurant_subset])\n\n# Split into training and test datasets\ntraining_data, test_data = train_test_split(combined,  train_size=0.7, random_state=123)","367d31b8":"training_data.shape","73cfa390":"training_data.target.value_counts(normalize=True)","529fe06b":"test_data.shape","730858d1":"test_data.target.value_counts(normalize=True)","7cae0261":"from sklearn.feature_extraction.text import CountVectorizer","fda5aa7a":"# Represent the review text as a bag-of-words \nbow_transform = CountVectorizer()\nX_tr_bow = bow_transform.fit_transform(training_data['text'])\n\nX_te_bow = bow_transform.transform(test_data['text'])\nlen(bow_transform.vocabulary_)","7e38419b":"X_tr_bow[:1]","b793e946":"X_tr_bow.shape","dbfc2632":"from sklearn.feature_extraction.text import TfidfTransformer","97927bd6":"# Create the tf-idf representation using the bag-of-words matrix\ntfidf_trfm = TfidfTransformer(norm=None)\nX_tr_tfidf = tfidf_trfm.fit_transform(X_tr_bow)\n\nX_te_tfidf = tfidf_trfm.transform(X_te_bow)","5b17944e":"from sklearn.preprocessing import normalize","65c8c1af":"# Just for kicks, l2-normalize the bag-of-words representation\nX_tr_l2 = normalize(X_tr_bow, axis=0)\nX_te_l2 = normalize(X_te_bow, axis=0)","38c5fa4a":"# test and train targets\ny_tr = training_data['target']\ny_te = test_data['target']","4a57efa1":"from sklearn.linear_model import LogisticRegression","37f419e5":"def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n    ### Helper function to train a logistic classifier and score on test data\n    m = LogisticRegression(solver='lbfgs', C=_C).fit(X_tr, y_tr)\n    s = m.score(X_test, y_test)\n    print ('Test score with', description, 'features:', s)\n    return m","0de6c898":"m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')","2f224cd8":"m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized')","cefdbf33":"m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf')","bddb430a":"from sklearn.model_selection import GridSearchCV","5c10c5b2":"# Specify a search grid, then do a 5-fold grid search for each of the feature sets\nparam_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}","ff4979d2":"# Tune classifier for bag-of-words representation\nbow_search = GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\nbow_search.fit(X_tr_bow, y_tr)","b08df41c":"bow_search.best_params_","ecd49fbb":"# Tune classifier for L2-normalized word vector\nl2_search = GridSearchCV(LogisticRegression(), cv=5,param_grid=param_grid_)\nl2_search.fit(X_tr_l2, y_tr)","90582f3f":"l2_search.best_params_","b253178f":"# Tune classifier for tf-idf\ntfidf_search = GridSearchCV(LogisticRegression(), cv=5,param_grid=param_grid_)\ntfidf_search.fit(X_tr_tfidf, y_tr)","10ddc3b4":"tfidf_search.best_params_","7b17886f":"import pandas as pd\n# Let's check out one of the grid search outputs to see how it went\nsearch_results = pd.DataFrame.from_dict({\n    'bow': bow_search.cv_results_['mean_test_score'],\n    'tfidf': tfidf_search.cv_results_['mean_test_score'],\n    'l2': l2_search.cv_results_['mean_test_score']\n})\nsearch_results.index = param_grid_['C']\nsearch_results\n# Average cross validation classifier accuracy scores","0d4bda4e":"# find which parameter is the best for each model\nsearch_results.apply(lambda column: column.idxmax(), axis=0)\n# higher values of C correspond to less regularization","8a64d63d":"# Our usual matplotlib incantations. Seaborn is used here to make the plot pretty.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nax = sns.boxplot(data=search_results, width=0.4)\nax.set_ylabel('Accuracy', size=14)\nax.tick_params(labelsize=14)","7fb0e2fd":"# Train a final model on the entire training set, using the best hyperparameter \n# settings found previously. Measure accuracy on the test set.\nm1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow', _C=bow_search.best_params_['C'])\nm2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized', _C=l2_search.best_params_['C'])\nm3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf', _C=tfidf_search.best_params_['C'])","93d110dd":"## Scaling Bag-of-Words with Tf-Idf Transformation","28a47365":"## Classification with Logistic Regression","73e598ac":"## Tuning Logistic Regression with Regularization","c715b5e0":"If you have made it so far, thanks for examining! ","fdb7720d":"# Chapter 4. The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf","362764c8":" Imbalanced datasets are problematic for modeling because the model will expend most of its effort fitting to the larger class. Since we have plenty of data in both classes, a good way to resolve the problem is to downsample the larger class (restaurants) to be roughly the same size as the smaller class (nightlife).","098e3229":"This chapter is one of the chapters of the book, _Feature Engineering for Machine Learning_. Since I do not have enough resource on my local machine, I have ended up creating this kernel to practice the source code of the chapter 4 while reading the book. You can also examine all the main chapters' code over the original GitHub repository of the book: https:\/\/github.com\/alicezheng\/feature-engineering-book","e8532931":"## Creating a Classification Dataset"}}