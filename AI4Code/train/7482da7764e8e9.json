{"cell_type":{"e65fdf97":"code","28799be3":"code","65dc1dcb":"code","2fc65b3a":"code","cf7b48c1":"code","a5a582c7":"code","ba80eaf6":"code","3d529427":"code","c736b078":"code","4176b52f":"code","b25502a8":"markdown","71c19b69":"markdown","b6c759b1":"markdown","31b24ad0":"markdown","175298a4":"markdown","fe522e90":"markdown","f760c5cd":"markdown","3d2a43d2":"markdown","088e45a1":"markdown","04ad8551":"markdown","bc619d14":"markdown","870c632e":"markdown"},"source":{"e65fdf97":"import numpy as np # Linear algebra.\nimport pandas as pd # Data processing.\nimport matplotlib.pyplot as plt # Visualize\nimport math\nfrom keras.models import Sequential # Create Model\nfrom keras.layers import Dense # Neurons\nfrom keras.layers import LSTM # Long Short Term Memory\nfrom sklearn.preprocessing import MinMaxScaler # Normalize\nfrom sklearn.metrics import mean_squared_error # Loss Function\nfrom sklearn.model_selection import train_test_split","28799be3":"data = pd.read_csv(\"..\/input\/Tesla.csv - Tesla.csv.csv\") # Import data\ndata.head()","65dc1dcb":"df = data.iloc[:,1].values # We use \"Open\" column.\nplt.plot(df)\nplt.xlabel(\"Time\")\nplt.ylabel(\"Open\")\nplt.show()","2fc65b3a":"df = df.reshape(-1,1)\n\nscaler = MinMaxScaler(feature_range = (0,1)) # Normalize data\ndf = scaler.fit_transform(df)\nnp.max(df)","cf7b48c1":"# Test - Train Split\ntrain_size = int(len(df) * 0.75) # % 75 Train\ntest_size = len(df) - train_size # % 25 Test\nprint(\"Train Size :\",train_size,\"Test Size :\",test_size)\n\ntrain = df[0:train_size,:]\ntest = df[train_size:len(df),:]","a5a582c7":"time_stemp = 10\n\ndatax = []\ndatay = []\nfor i in range(len(train)-time_stemp-1):\n    a = train[i:(i+time_stemp), 0]\n    datax.append(a)\n    datay.append(train[i + time_stemp, 0])\ntrainx = np.array(datax)\ntrainy = np.array(datay)\n\n\ndatax = []\ndatay = []\nfor i in range(len(test)-time_stemp-1):\n    a = test[i:(i+time_stemp), 0]\n    datax.append(a)\n    datay.append(test[i + time_stemp, 0])\ntestx = np.array(datax)\ntesty = np.array(datay)\n\ntrainx = np.reshape(trainx, (trainx.shape[0], 1, trainx.shape[1])) # For Keras\ntestx = np.reshape(testx, (testx.shape[0], 1,testx.shape[1])) # For Keras\nprint(trainx.shape)\ntestx.shape","ba80eaf6":"epochs = 200\nmodel = Sequential()\nmodel.add(LSTM(10, input_shape = (1, time_stemp)))\nmodel.add(Dense(1)) # Output Layer\nmodel.compile(loss = \"mean_squared_error\", optimizer = \"adam\")\nhistory = model.fit(trainx,trainy, epochs = epochs, batch_size = 50, verbose=0)\n# As you can see, Loss is very little","3d529427":"epoch = np.arange(0, epochs, 10)\nlosses = []\nfor i in epoch:\n    if i % 10 == 0:\n        losses.append(history.history[\"loss\"][i])\n        \ndata = {\"epoch\":epoch,\"loss\":losses}\ndata = pd.DataFrame(data) # Create dataframe for visualize with plotly\n\n# Visualize\nimport plotly.express as px\n\nfig = px.line(data,x=\"epoch\",y=\"loss\",width = 1200, height = 500)\nfig.show()\n# I choose plotly for visualize because it's interactive","c736b078":"train_predict = model.predict(trainx)\ntest_predict = model.predict(testx)\n\ntrain_predict = scaler.inverse_transform(train_predict)\ntest_predict = scaler.inverse_transform(test_predict)\ntrainy = scaler.inverse_transform([trainy])\ntesty = scaler.inverse_transform([testy])\n\ntrain_score = math.sqrt(mean_squared_error(trainy[0], train_predict[:,0])) # mean_squared_error -> Loss Function\nprint(\"Train Score : %2.f RMSE\" % (train_score))\ntest_score = math.sqrt(mean_squared_error(testy[0], test_predict[:,0]))\nprint(\"Test Score : %2.f RMSE\" % (test_score))","4176b52f":"# empty_like -> Return a new array with the same shape and type as a given array.\ntrain_predict_plot = np.empty_like(df)\ntrain_predict_plot[:,:] = np.nan\ntrain_predict_plot[time_stemp:len(train_predict)+time_stemp, :] = train_predict\n\ntest_predict_plot = np.empty_like(df)\ntest_predict_plot[:, :] = np.nan\ntest_predict_plot[len(train_predict)+(time_stemp*2)+1:len(df)-1, :] = test_predict\n\nplt.plot(scaler.inverse_transform(df),color = \"red\",label = \"Real\")\nplt.plot(train_predict_plot,label = \"Train Predict\",color = \"yellow\",alpha = 0.7)\nplt.plot(test_predict_plot,label = \"Test Predict\",color = \"green\", alpha = 0.7)\nplt.legend()\nplt.xlabel(\"Time\")\nplt.ylabel(\"Open Value\")\nplt.show()","b25502a8":"<a id=\"8\"><\/a> <br>\n<font color='red'>\n### Prediction","71c19b69":"<a id=\"7\"><\/a> <br>\n<font color='red'>\n### Create Model","b6c759b1":"<font color='red'>\n## Long Short Term Memory (LSTM) Example\n<font color='green'>\n- LSTM is used in time-varying data sets.\n\n<font color='red'>\n**Content:**\n<font color='black'>\n1. [Imports](#1)\n    1. [Import Data](#2)\n2. [Preprocessing Data](#3)\n    1. [Normalize Data](#4)\n    2. [Test - Train Split](#5)\n    3. [Prepare Data](#6)\n3. [Create Model](#7)\n    1. [Visualize Losses](#16)\n4. [Prediction](#8)\n5. [Visualize](#9)\n6. [Conclusion](#10)","31b24ad0":"<a id=\"10\"><\/a> <br>\n<font color='red'>\n### Conclusion\n<font color='black'>\n- As you can see, Our model is very successful.\n- If you encounter an error or would like to make a suggestion, please do not forget to comment\n- If you don't understand something, please don't forget to ask.","175298a4":"<a id=\"5\"><\/a> <br>\n<font color='blue'>\n**Train - Test Split**\n<font color='black'>\n- We will split the data into 75% train and 25% test.","fe522e90":"<a id=\"4\"><\/a> <br>\n<font color='blue'>\n#### Normalize Data","f760c5cd":"<a id=\"1\"><\/a> <br>\n<font color='red'>\n### Imports","3d2a43d2":"<a id=\"6\"><\/a> <br>\n<font color='blue'>\n**Prepare Data**","088e45a1":"<a id=\"9\"><\/a> <br>\n<font color='red'>\n### Visualize","04ad8551":"<a id=\"3\"><\/a> <br>\n<font color='red'>\n### Preprocessing Data","bc619d14":"<a id=\"16\"><\/a> <br>\n<font color='blue'>\n**Visualize Losses**","870c632e":"<a id=\"2\"><\/a> <br>\n<font color='blue'>\n**Import Data**"}}