{"cell_type":{"dc42f953":"code","373b9978":"code","e4f2a77d":"code","787bd4a5":"code","4fa716d7":"code","66b13fbf":"code","56667d6d":"code","71a54ec4":"code","d95f043e":"code","8814327f":"code","0ecb6204":"code","748771c7":"code","29a753cc":"code","55db3e38":"code","8b4683f6":"code","bb6c1907":"code","8df7aa58":"code","e02ee00d":"code","6106cf72":"code","8298519c":"code","1bf5c537":"code","dfb2def1":"code","e2a4ad0c":"code","873f672e":"code","62333184":"code","5ab43cf4":"code","0fafda31":"code","fd5edf10":"code","c0316530":"code","1a3d545e":"code","40d4c19d":"code","feee659d":"code","d6d7261c":"code","ae214daa":"code","7e4863e7":"code","8b714ea1":"code","e1d9bcce":"code","c1477d08":"code","356e020c":"code","8a93d656":"code","7dab75b4":"code","a2859799":"code","afc41904":"code","fe221ff1":"markdown","028e18d4":"markdown","e26d7356":"markdown","9df534a8":"markdown","05754426":"markdown","67000a83":"markdown","f6e0c589":"markdown","26a959b8":"markdown","e0e1df11":"markdown","6464d0df":"markdown","67f1247a":"markdown","02b984d6":"markdown","936b28ee":"markdown","e33e9d29":"markdown","4cb9db01":"markdown","fa610b2c":"markdown","9d395d02":"markdown","e745bf41":"markdown","2cc304cb":"markdown"},"source":{"dc42f953":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport gc, sys\ngc.enable()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","373b9978":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ndel train_transaction, train_identity\ngc.collect()","e4f2a77d":"train.shape","787bd4a5":"train.head().T","4fa716d7":"%%time\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\ndel test_transaction, test_identity\ngc.collect()","66b13fbf":"test.shape","56667d6d":"# From https:\/\/www.kaggle.com\/nroman\/lgb-single-model-lb-0-9419\n\nselected_features = [\n    'TransactionAmt', 'ProductCD',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n    'P_emaildomain', 'R_emaildomain',\n    'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14',\n    'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15',\n    'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n    'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12',\n    'V13', 'V17', 'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36',\n    'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V51',\n    'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63',\n    'V64', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V78',\n    'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n    'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128',\n    'V130', 'V131', 'V138', 'V139', 'V140', 'V143', 'V145', 'V146', 'V147', 'V149',\n    'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161', 'V162',\n    'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173',\n    'V175', 'V176', 'V177', 'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189',\n    'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207',\n    'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219',\n    'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229',\n    'V231', 'V233', 'V234', 'V238', 'V239', 'V242', 'V243', 'V244', 'V245', 'V246',\n    'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261', 'V262',\n    'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273',\n    'V274', 'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285',\n    'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303', 'V304', 'V306', 'V307',\n    'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324',\n    'V326', 'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338',\n    'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09', 'id_11', 'id_12', 'id_13', 'id_14',\n    'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33', 'id_36', 'id_37',\n    'id_38', 'DeviceType', 'DeviceInfo'\n]\n\nlen(selected_features)","71a54ec4":"columns_to_drop = list(set(train.columns) - set(selected_features) - set(['isFraud', 'TransactionDT']))\nlen(columns_to_drop)","d95f043e":"# train = train.drop(columns_to_drop, axis=1)\n# test = test.drop(columns_to_drop, axis=1)","8814327f":"# train['null_columns'] = train.isna().sum(axis=1)\n# test['null_columns'] = test.isna().sum(axis=1)","0ecb6204":"train = train.fillna(-999)\ntest = test.fillna(-999)","748771c7":"# columns with 90% null\n# many_null_columns_train = [c for c in train.columns if train[c].isnull().sum() \/ train.shape[0] > 0.9]\n\nmany_same_values_columns_train = [c for c in train.columns if train[c].value_counts(normalize=True).values[0] > 0.9]\n\n# value_counts(dropna=False, normalize=True)  ^^^\n# commented code not needed because of 'fillna(-999)'\n\nmany_same_values_columns_test = [c for c in test.columns if test[c].value_counts(normalize=True).values[0] > 0.9]\n\ncolumns_to_drop = list(set(many_same_values_columns_train + many_same_values_columns_test))\ncolumns_to_drop.remove('isFraud')\nlen(columns_to_drop)","29a753cc":"# train = train.drop(columns_to_drop, axis=1)\n# test = test.drop(columns_to_drop, axis=1)","55db3e38":"import re\n\nos = ['Windows', 'iOS', 'Android', 'Mac OS', 'Linux']\nbrowser = ['chrome', 'mobile safari', 'ie', 'safari', 'edge', 'firefox']\ndevice = ['Windows', 'iOS', 'MacOS', 'SM', 'SAMSUNG', 'Moto', 'LG']\n\ndef to_pattern(x: str, patterns):\n    for p in patterns:\n        t = re.compile('^(' + p + ').*')\n        if t.match(x):\n            return p\n    return 'other'\n\ndef make_os_feature(df):\n    return df['id_30'].map(lambda x: to_pattern(str(x), os))\n\ndef make_browser_feature(df):\n    return df['id_31'].map(lambda x: to_pattern(str(x), browser))\n\ndef make_device_feature(df):\n    return df['DeviceInfo'].map(lambda x: to_pattern(str(x), device))\n\n# train['os'] = make_os_feature(train)\n# train['browser'] = make_browser_feature(train)\n# train['device'] = make_device_feature(train)\n\n# test['os'] = make_os_feature(test)\n# test['browser'] = make_browser_feature(test)\n# test['device'] = make_device_feature(test)","8b4683f6":"train['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)","bb6c1907":"def make_aggregates(df, feature_to_group_by, feature):\n    out = pd.DataFrame(index=df.index)\n    \n    # filter for -999 is needed because NaN values were filled by this constant\n    agg = df[df[feature] != -999].groupby([feature_to_group_by])[feature]\n    \n    new_feature = feature + '_' + feature_to_group_by\n    out[new_feature + '_mean'] = df[feature] \/ agg.transform('mean')\n    out[new_feature + '_std' ] = df[feature] \/ agg.transform('std')\n    \n    return out\n\ndef merge_aggregates(df, feature_to_group_by, feature):\n    return df.merge(make_aggregates(df, feature_to_group_by, feature), how='left', left_index=True, right_index=True)\n\n\n# for pair in [('card1', 'TransactionAmt'), ('card1', 'D15'), ('addr1', 'TransactionAmt')]:\n#    train = merge_aggregates(train, pair[0], pair[1])\n#    test = merge_aggregates(test, pair[0], pair[1])","8df7aa58":"# Based on https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\n\ndef make_count_1(feature, df):\n    temp = df[feature].value_counts(dropna=False)\n    new_feature = feature + '_count'\n    df[new_feature] = df[feature].map(temp)\n\ndef make_count_2(feature, train, test):\n    temp = pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False)\n    new_feature = feature + '_count'\n    train[new_feature] = train[feature].map(temp)\n    test[new_feature] = test[feature].map(temp)\n\n\nfor feature in ['id_01', 'id_31', 'id_33', 'id_36']:\n    make_count_1(feature, train)\n    make_count_1(feature, test)\n    \nfor feature in ['card1', 'id_36']:\n    make_count_2(feature, train, test)","e02ee00d":"# Based on https:\/\/www.kaggle.com\/nroman\/lgb-single-model-lb-0-9419\n\ndef features_interaction(df, feature_1, feature_2):\n    return df[feature_1].astype(str) + '_' + df[feature_2].astype(str)\n\nfeatures_interactions = [\n    'id_02__id_20',\n    'id_02__D8',\n    'D11__DeviceInfo',\n    'DeviceInfo__P_emaildomain',\n    'P_emaildomain__C2',\n    'card2__dist1',\n    'card1__card5',\n    'card2__id_20',\n    'card5__P_emaildomain',\n    'addr1__card1'\n]\n\nfor new_feature in features_interactions:\n    feature_1, feature_2 = new_feature.split('__')\n    train[new_feature] = features_interaction(train, feature_1, feature_2)\n    test[new_feature] = features_interaction(test, feature_1, feature_2)","6106cf72":"import matplotlib.pyplot as plt","8298519c":"# From https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\n\ndef make_day_feature(df, offset=0.58, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6.\n    \"\"\"\n    days = df[tname] \/ (3600 * 24)\n    encoded_days = np.floor(days - 1 + offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23.\n    \"\"\"\n    hours = df[tname] \/ (3600)\n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","1bf5c537":"plt.hist(train['TransactionDT'] \/ (3600 * 24), bins=1800)\nplt.xlim(70, 78)\nplt.xlabel('Days')\nplt.ylabel('Number of transactions')\nplt.ylim(0,1000)","dfb2def1":"train['Weekday'] = make_day_feature(train)","e2a4ad0c":"plt.plot(train.groupby('Weekday').mean()['isFraud'])\nplt.ylim(0, 0.04)\nplt.xlabel('Encoded day')\nplt.ylabel('Fraction of fraudulent transactions')","873f672e":"train['Hour'] = make_hour_feature(train)","62333184":"plt.plot(train.groupby('Hour').mean()['isFraud'])\nplt.xlabel('Encoded hour')\nplt.ylabel('Fraction of fraudulent transactions')","5ab43cf4":"test['Weekday'] = make_day_feature(test)\ntest['Hour'] = make_hour_feature(test)","0fafda31":"# Based on https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499\n\nemail_vendors = {\n    'gmail': 'google',\n    'att.net': 'att',\n    'twc.com': 'spectrum',\n    'scranton.edu': 'other',\n    'optonline.net': 'other',\n    'hotmail.co.uk': 'microsoft',\n    'comcast.net': 'other',\n    'yahoo.com.mx': 'yahoo',\n    'yahoo.fr': 'yahoo',\n    'yahoo.es': 'yahoo',\n    'charter.net': 'spectrum',\n    'live.com': 'microsoft',\n    'aim.com': 'aol',\n    'hotmail.de': 'microsoft',\n    'centurylink.net': 'centurylink',\n    'gmail.com': 'google',\n    'me.com': 'apple',\n    'earthlink.net': 'other',\n    'gmx.de': 'other',\n    'web.de': 'other',\n    'cfl.rr.com': 'other',\n    'hotmail.com': 'microsoft',\n    'protonmail.com': 'other',\n    'hotmail.fr': 'microsoft',\n    'windstream.net': 'other',\n    'outlook.es': 'microsoft',\n    'yahoo.co.jp': 'yahoo',\n    'yahoo.de': 'yahoo',\n    'servicios-ta.com': 'other',\n    'netzero.net': 'other',\n    'suddenlink.net': 'other',\n    'roadrunner.com': 'other',\n    'sc.rr.com': 'other',\n    'live.fr': 'microsoft',\n    'verizon.net': 'yahoo',\n    'msn.com': 'microsoft',\n    'q.com': 'centurylink',\n    'prodigy.net.mx': 'att',\n    'frontier.com': 'yahoo',\n    'anonymous.com': 'other',\n    'rocketmail.com': 'yahoo',\n    'sbcglobal.net': 'att',\n    'frontiernet.net': 'yahoo',\n    'ymail.com': 'yahoo',\n    'outlook.com': 'microsoft',\n    'mail.com': 'other',\n    'bellsouth.net': 'other',\n    'embarqmail.com': 'centurylink',\n    'cableone.net': 'other',\n    'hotmail.es': 'microsoft',\n    'mac.com': 'apple',\n    'yahoo.co.uk': 'yahoo',\n    'netzero.com': 'other',\n    'yahoo.com': 'yahoo',\n    'live.com.mx': 'microsoft',\n    'ptd.net': 'other',\n    'cox.net': 'other',\n    'aol.com': 'aol',\n    'juno.com': 'other',\n    'icloud.com': 'apple',\n    -999: 'undefined'\n}\n\nus_emails = ['gmail', 'net', 'edu']\n\ndef transform_email(df, column):\n    out = pd.DataFrame(index=df.index)\n    \n    # vendor\n    out[column + '_vendor'] = df[column].map(email_vendors)\n    \n    # suffix\n    out[column + '_suffix'] = df[column].map(lambda x: str(x).split('.')[-1])\n    \n    # US\n    out[column + '_us'] = out[column + '_suffix'].map(lambda x: 'us' if str(x) in us_emails else 'other')\n    \n    return out","fd5edf10":"# temp = transform_email(train, 'P_emaildomain')\n# train = train.merge(temp, how='left', left_index=True, right_index=True)\n# temp = transform_email(train, 'R_emaildomain')\n# train = train.merge(temp, how='left', left_index=True, right_index=True)\n# del temp","c0316530":"# temp = transform_email(test, 'P_emaildomain')\n# test = test.merge(temp, how='left', left_index=True, right_index=True)\n# temp = transform_email(test, 'R_emaildomain')\n# test = test.merge(temp, how='left', left_index=True, right_index=True)\n# del temp","1a3d545e":"train = train.sort_values('TransactionDT').drop('TransactionDT', axis=1)\ntest = test.sort_values('TransactionDT').drop('TransactionDT', axis=1)","40d4c19d":"from sklearn.preprocessing import LabelEncoder\n\n\ndef encode_categorial_features_fit(df, columns_to_encode):\n    encoders = {}\n    for c in columns_to_encode:\n        if c in df.columns:\n            encoder = LabelEncoder()\n            encoder.fit(df[c].astype(str).values)\n            encoders[c] = encoder\n    return encoders\n\ndef encode_categorial_features_transform(df, encoders):\n    out = pd.DataFrame(index=df.index)\n    for c in encoders.keys():\n        if c in df.columns:\n            out[c] = encoders[c].transform(df[c].astype(str).values)\n    return out\n\n\ncategorial_features_columns = [\n    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n    'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31',\n    'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n    'DeviceType', 'DeviceInfo', 'ProductCD', 'P_emaildomain', 'R_emaildomain',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n    'P_emaildomain_vendor', 'P_emaildomain_suffix', 'P_emaildomain_us',\n    'R_emaildomain_vendor', 'R_emaildomain_suffix', 'R_emaildomain_us' # ,\n    # 'os', 'browser', 'device'\n] + features_interactions\n\ncategorial_features_encoders = encode_categorial_features_fit(\n    pd.concat([train, test], join='outer', sort=False),\n    categorial_features_columns)","feee659d":"temp = encode_categorial_features_transform(train, categorial_features_encoders)\ncolumns_to_drop = list(set(categorial_features_columns) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","d6d7261c":"temp = encode_categorial_features_transform(test, categorial_features_encoders)\ncolumns_to_drop = list(set(categorial_features_columns) & set(test.columns))\ntest = test.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","ae214daa":"# From https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","7e4863e7":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","8b714ea1":"y_train = train['isFraud'].copy()\nx_train = train.drop('isFraud', axis=1)\ndel train\n\nx_test = test.copy()\ndel test","e1d9bcce":"from sklearn.model_selection import KFold, train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nimport optuna","c1477d08":"def objective(trial):\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'boosting_type': 'gbdt',\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'num_threads': 4,\n        \n        'num_leaves': trial.suggest_int('num_leaves', 10, 1000),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 0.001, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 1, 100),\n        'bagging_fraction' : trial.suggest_loguniform('bagging_fraction', .5, .99),\n        'feature_fraction' : trial.suggest_loguniform('feature_fraction', .5, .99),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 0.1, 2),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 0.1, 2)\n    }\n    \n    scores = []\n    \n    cv = KFold(n_splits=5)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n        \n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n        \n        lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n        lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n        \n        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n        y = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n        \n        score = roc_auc_score(y_train_valid.astype('float32'), y)\n        print('Fold score:', score)\n        scores.append(score)\n    \n    average_score = sum(scores) \/ len(scores)\n    print('Average score:', average_score)\n    return average_score\n\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=40)","356e020c":"# study.best_trial","8a93d656":"params = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'is_unbalance': False,\n    'boost_from_average': True,\n    'num_threads': 4,\n    \n#     'num_leaves': study.best_trial.params['num_leaves'],\n#     'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n#     'min_child_weight': study.best_trial.params['min_child_weight'],\n#     'max_depth': study.best_trial.params['max_depth'],\n#     'bagging_fraction' : study.best_trial.params['bagging_fraction'],\n#     'feature_fraction' : study.best_trial.params['feature_fraction'],\n#     'lambda_l1': study.best_trial.params['lambda_l1'],\n#     'lambda_l2': study.best_trial.params['lambda_l2']\n    \n    'num_iterations': 5000,\n    'learning_rate': 0.006,\n    'early_stopping_round': 100,\n    \n    'num_leaves': 396,\n    'min_data_in_leaf': 39,\n    'min_child_weight': 0.06046322469906681,\n    'max_depth': 95,\n    'bagging_fraction' : 0.7119300452429695,\n    'feature_fraction' : 0.6121880522551549,\n    'lambda_l1': 0.30642889371923837,\n    'lambda_l2': 0.6028242575804484\n}","7dab75b4":"n_splits=5\n\ny = np.zeros(x_test.shape[0])\n\nfeature_importances = []\n\ncv = KFold(n_splits=n_splits)\nfor train_idx, valid_idx in cv.split(x_train, y_train):\n    \n    x_train_train = x_train.iloc[train_idx]\n    y_train_train = y_train.iloc[train_idx]\n    x_train_valid = x_train.iloc[valid_idx]\n    y_train_valid = y_train.iloc[valid_idx]\n    \n    lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n    lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n    \n    lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100, early_stopping_rounds=100)\n    \n    y_part = lgb_model.predict(x_test.astype('float32'), num_iteration=lgb_model.best_iteration)\n    y += y_part \/ n_splits\n    \n    feature_importances.append(lgb_model.feature_importance())","a2859799":"import seaborn as sns\n\nfeature_importance_df = pd.concat([\n    pd.Series(x_train.columns),\n    pd.Series(np.mean(feature_importances, axis=0))], axis=1)\nfeature_importance_df.columns = ['featureName', 'importance']\n\n# get top 100 features sorted by importance descending\ntemp = feature_importance_df.sort_values(by=['importance'], ascending=False).head(100)\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"importance\", y=\"featureName\", data=temp)\nplt.show()","afc41904":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\nsubmission['isFraud'] = y\nsubmission.to_csv('lightgbm.csv')","fe221ff1":"## Remove timestamp","028e18d4":"## Make day and hour features","e26d7356":"## Transform emails","9df534a8":"## Features interaction","05754426":"## Transform id_30, id_31","67000a83":"## Fill 'null' values with constant","f6e0c589":"## 'card1', ... count","26a959b8":"## Remove features that have more than 90% of same values","e0e1df11":"## Keep selected features only","6464d0df":"> ## Load data","67f1247a":"# EDA, feature engineering, LightGBM baseline\n\n**Feature engineering:**\n* select ~280 features from 432 (-)\n* count 'null' values per row (-)\n* fill 'null' values with constant (+)\n* remove features that have more than 90% of same values (-)\n* make 'os', 'browser', 'device' from 'id_30', 'id_31' (-)\n* transform 'TransactionAmt' (+)\n* feature aggregates (-)\n* 'card1', ... count (+)\n* features interaction (+)\n* make 'day', 'hour' features (+)\n* make 'vendor', 'suffix', 'us' from email features (-)\n* remove timestamp (+)\n\n**Model:**\n* LightGBM\n* Optuna to get optimal parameters\n* 5 fold cross-validation","02b984d6":"## Submit predictions","936b28ee":"## Encode categorial features","e33e9d29":"## Count 'null' values per row","4cb9db01":"## Free memory","fa610b2c":"## Transform 'TransactionAmt'","9d395d02":"## Feature aggregates","e745bf41":"## Extract target variable","2cc304cb":"> ## LightGBM"}}