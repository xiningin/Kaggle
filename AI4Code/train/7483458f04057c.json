{"cell_type":{"5fc9b294":"code","8d6558b8":"code","2726db6e":"code","7b6421ae":"code","03d357ab":"code","bfa6b7fd":"code","d5db534b":"code","41526675":"code","4bd0f956":"code","0352434d":"code","ad02268d":"code","f8352acf":"code","67efd428":"code","6f63d47e":"code","6001d94e":"code","9c9cd3bb":"code","38aa4c56":"code","1cff4ffe":"code","d153b092":"code","aac77ba7":"code","95cf0bf0":"code","2b76d11f":"code","9ea06f2a":"code","86fdf857":"code","a607cd92":"code","447e390c":"code","c8f8bbc7":"code","7902ca62":"code","e518248b":"code","cb024cec":"code","8f555cbc":"code","e50bcd68":"code","ebc2946a":"code","210980e5":"code","a24a6127":"markdown","dab919fb":"markdown","e8a8349c":"markdown","419de637":"markdown","b7792d8f":"markdown","8d8d2fd3":"markdown","3106e68e":"markdown","22cc5fb1":"markdown","00f51304":"markdown","e97f42ae":"markdown"},"source":{"5fc9b294":"!pip install torchsummary","8d6558b8":"import os\nimport pickle\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.utils.data as data_utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import nn\n\nfrom torchsummary import summary","2726db6e":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n!wget https:\/\/www.cs.toronto.edu\/~kriz\/cifar-10-python.tar.gz\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7b6421ae":"!tar -xvf \/kaggle\/working\/cifar-10-python.tar.gz","03d357ab":"!rm -rf \/kaggle\/working\/cifar-10-python.tar.gz\n!ls \/kaggle\/working\/cifar-10-batches-py","bfa6b7fd":"DATA_DIR = '\/kaggle\/working\/cifar-10-batches-py'","d5db534b":"def unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict","41526675":"metadata = unpickle(os.path.join(DATA_DIR, 'batches.meta'))[b'label_names']\nmetadata = [m.decode('utf-8') for m in metadata]\nmetadata","4bd0f956":"data_batch_1 = unpickle(os.path.join(DATA_DIR, 'data_batch_1'))\nlist(data_batch_1.keys())","0352434d":"print(f\"Batch label: {data_batch_1[b'batch_label']}\")\nprint(f\"Shape of Labels: {len(data_batch_1[b'labels'])}\")\nprint(f\"Actual image data shape: {data_batch_1[b'data'].shape}\")\nprint(f\"Filenames: {len(data_batch_1[b'filenames'])}\")","ad02268d":"def load_data(data_type='TRAIN'):\n    X, Y = [], []\n    if data_type == 'TRAIN':\n        for i in range(5):\n            batch = unpickle(os.path.join(DATA_DIR, f'data_batch_{i+1}'))\n            X.append(batch[b'data'])\n            Y.append(batch[b'labels'])\n    else:\n        test_batch = unpickle(os.path.join(DATA_DIR, f'test_batch'))\n        X.append(test_batch[b'data'])\n        Y.append(test_batch[b'labels'])\n\n    return torch.from_numpy(np.concatenate(np.array(X), axis=0)), torch.from_numpy(np.concatenate(np.array(Y), axis=0))","f8352acf":"X_train, Y_train = load_data()\nX_test, Y_test = load_data('TEST')","67efd428":"print(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of Y_train: {Y_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')\nprint(f'Shape of Y_test: {Y_test.shape}')","6f63d47e":"X_train, X_val, Y_train, Y_val = train_test_split(X_train.cpu().detach().numpy(), Y_train.cpu().detach().numpy(), test_size=0.1, random_state=666)","6001d94e":"# Convert to PyTorch tensor\nX_train = torch.from_numpy(X_train)\nX_val = torch.from_numpy(X_val)\nY_train = torch.from_numpy(Y_train)\nY_val = torch.from_numpy(Y_val)","9c9cd3bb":"print(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of Y_train: {Y_train.shape}')\nprint(f'Shape of X_test: {X_val.shape}')\nprint(f'Shape of Y_test: {Y_val.shape}')","38aa4c56":"IMG_SIZE = 32\nCHANNELS = 3\nW, H = IMG_SIZE, IMG_SIZE","1cff4ffe":"# Visualize few samples of training dataset\nfig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\ncount=0\nfor row in ax:\n    for col in row:\n        col.imshow(torch.stack([X_train[count, :][:1024].reshape(IMG_SIZE, IMG_SIZE), X_train[count, :][1024: 2048].reshape(IMG_SIZE, IMG_SIZE), X_train[count, :][2048:].reshape(IMG_SIZE, IMG_SIZE)], axis=2))\n        col.set_title(metadata[Y_train[count]])\n        count += 1\nplt.show()","d153b092":"sns.set(rc={'figure.figsize':(13,8)})\nax = sns.distplot(Y_train, kde=False)\nax.set(xlabel='Labels', ylabel='# of records', title='Distribution of targets')\nplt.show()","aac77ba7":"class CFAR10Dataset(Dataset):\n    \"\"\"\n    Custom CIFAR-10 dataset\n    \"\"\"\n    def __init__(self, X, Y, transforms=None):\n        self.X = X\n        self.Y = Y\n        self.transforms = transforms\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        image = torch.stack([self.X[index, :][:1024].reshape(IMG_SIZE, IMG_SIZE), self.X[index, :][1024: 2048].reshape(IMG_SIZE, IMG_SIZE), self.X[index, :][2048:].reshape(IMG_SIZE, IMG_SIZE)], axis=2).permute(2, 1, 0).float()\/255\n        if self.transforms:\n            image = self.transforms(image)\n        return image, self.Y[index]","95cf0bf0":"batch_size=3000","2b76d11f":"transformations = transforms.Compose([transforms.CenterCrop(100),\n                                      transforms.ToTensor()])","9ea06f2a":"train_dataset = CFAR10Dataset(X_train, Y_train)\ntrain_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)","86fdf857":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),                           \n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.BatchNorm2d(num_features=32),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), padding=1),\n            nn.BatchNorm2d(num_features=64),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.BatchNorm2d(num_features=128),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(3, 3), padding=1),\n            nn.ReLU(True),\n            nn.BatchNorm2d(num_features=256),\n            nn.MaxPool2d(kernel_size=(2, 2)),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(5, 5), padding=2),\n            nn.ReLU(True),\n            nn.Dropout2d(p=0.3)\n        )\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(in_features=1024, out_features=1024)\n        self.fc1_dropout = nn.Dropout2d(p=0.3)\n        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n        self.fc3 = nn.Linear(in_features=512, out_features=10)\n        \n        \n    def forward(self, X):\n        output = self.layer1(X)\n        output = self.layer2(output)\n        output = self.layer3(output)\n        output = self.layer4(output)\n        output = self.flatten(output)\n        output = self.fc1(output)\n        output = self.fc1_dropout(output)\n        output = self.fc2(output)\n        output = self.fc3(output)\n        return output","a607cd92":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","447e390c":"model = Model().to(device)\n\n# Print summary of our model\nsummary(model, input_size=(CHANNELS, IMG_SIZE, IMG_SIZE))","c8f8bbc7":"LEARNING_RATE = 0.1\nEPOCHS = 100\nCLASSES = 10\nCUTMIX_ALPHA = 1","7902ca62":"model = nn.DataParallel(model)\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()","e518248b":"X_val.shape\nX_val_list = []\nfor x in X_val:\n    X_val_list.append((torch.stack([x[:1024].reshape(IMG_SIZE, IMG_SIZE), x[1024: 2048].reshape(IMG_SIZE, IMG_SIZE), x[2048:].reshape(IMG_SIZE, IMG_SIZE)], axis=2).permute(2, 1, 0).float()\/255))\nX_val = torch.stack(X_val_list)","cb024cec":"def get_accuracy(preds, actual):\n    assert len(preds) == len(actual)\n    \n    total = len(actual)\n    _, predicted = torch.max(preds.data, axis=1)\n    correct = (predicted == actual).sum().item()\n    return correct \/ total","8f555cbc":"def shuffle_minibatch(x, y):\n    assert x.size(0)== y.size(0)\n    indices = torch.randperm(x.size(0))\n    return x[indices], y[indices]","e50bcd68":"total_steps = len(train_loader)\nloss_list, train_acc_list, val_acc_list = [], [], []\nplot_flag = False\nfor epoch in range(EPOCHS):\n    for i, (x_train, y_train) in enumerate(train_loader):\n        x_train = x_train.to(device)\n        y_train = y_train.to(device)\n        \n        cutmix_decision = np.random.rand()\n        if cutmix_decision > 0.50:\n            # Cutmix: https:\/\/arxiv.org\/pdf\/1905.04899.pdf\n            x_train_shuffled, y_train_shuffled = shuffle_minibatch(x_train, y_train)\n            lam = np.random.beta(CUTMIX_ALPHA, CUTMIX_ALPHA)\n            cut_rat = np.sqrt(1. - lam)\n            cut_w = np.int(W * cut_rat)\n            cut_h = np.int(H * cut_rat)\n\n            # uniform\n            cx = np.random.randint(W)\n            cy = np.random.randint(H)\n\n            bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n            bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n            bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n            bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n            x_train[:, :, bbx1:bbx2, bby1:bby2] = x_train_shuffled[:, :, bbx1:bbx2, bby1:bby2]\n            lam = 1 - (bbx2 - bbx1) * (bby2 - bby1) \/ (W * H)\n        \n        # Forward pass\n        y_preds = model(x_train)\n        \n        # Calculate loss\n        if cutmix_decision > 0.50:\n            loss = criterion(y_preds, y_train) * lam + criterion(y_preds, y_train_shuffled) * (1. - lam)\n        else:\n            loss = criterion(y_preds, y_train)\n        if i+1 == total_steps:\n            loss_list.append(loss)\n        \n        # Backpropagate\n        optimizer.zero_grad()  # Reason: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch\n        loss.backward()\n        optimizer.step()\n        \n        # Calculate the accuracy\n        train_acc = get_accuracy(y_preds, y_train)\n        if i+1 == total_steps:\n            train_acc_list.append(train_acc)\n            \n        # Calculate validation accuracy\n        X_val = X_val.to(device)\n        Y_val = Y_val.to(device)\n        \n        # Predict on validation set\n        val_preds = model(X_val)\n        \n        val_acc = get_accuracy(val_preds, Y_val)\n        if i+1 == total_steps:\n            val_acc_list.append(val_acc)\n\n        if (i + 1) % 10 == 0:\n            print('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}, Train accuracy: {:.2f}%, Validation accuracy: {:.2f}%'\n                  .format(epoch + 1, EPOCHS, i + 1, total_steps, loss.item(),\n                          train_acc * 100, val_acc * 100))\n    print()","ebc2946a":"plt.style.use('ggplot')\nplt.figure()\nplt.plot(np.arange(0, EPOCHS), loss_list, label='train_loss')\n\nplt.title('Loss')\nplt.xlabel('# of epochs')\nplt.ylabel('Loss')\nplt.legend(loc='upper right')\nplt.show()","210980e5":"plt.style.use('ggplot')\nplt.figure()\nplt.plot(np.arange(0, EPOCHS), train_acc_list, label='train_accuracy')\nplt.plot(np.arange(0, EPOCHS), val_acc_list, label='val_accuracy')\n\nplt.title('Accuracy')\nplt.xlabel('# of epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper right')\nplt.show()","a24a6127":"### Define data augmentations using pytorch transforms","dab919fb":"## Creating custom PyTorch data generator and Data Loader\n\nReferences: \n\n* https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\n* https:\/\/github.com\/utkuozbulak\/pytorch-custom-dataset-examples\n* https:\/\/stackoverflow.com\/questions\/41924453\/pytorch-how-to-use-dataloaders-for-custom-datasets\n","e8a8349c":"## Exploratory Data Analysis","419de637":"https:\/\/discuss.pytorch.org\/t\/multi-class-classifier\/29901\/2","b7792d8f":"## Let's explore the data format\n\nThe complete details of format of the data is given [here](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html). ","8d8d2fd3":"### Distribution of class","3106e68e":"There are 5000 records for all 10 classes of images.","22cc5fb1":"The return dict contains following keys:\n\n* **batch_label:** The label of batch\n* **labels:** Labels of given images in `data` key for training\n* **data:** Flattened colored images for training\n* **filename:** Names of file from the image is read (Useless in our case)","00f51304":"From the above results, we've 50k training images and 10k testing images. Training images will be further splitted into training and validation images.","e97f42ae":"## Basic CNN Model using PyTorch\n\nLet's create a CNN model. Architecture reference: https:\/\/www.kaggle.com\/kaushal2896\/bengali-graphemes-starter-eda-multi-output-cnn"}}