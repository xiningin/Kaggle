{"cell_type":{"bc8f52d1":"code","3479b19c":"code","ab9684d9":"code","d30bc18b":"code","5de1c019":"code","e35c8aec":"code","ad02e115":"code","0ab86a8e":"code","2111ce0a":"code","f0a2c18e":"code","615c5525":"code","239de4bb":"code","11921478":"code","337dba46":"code","5b1cde00":"code","007027a4":"code","812f803c":"code","59161188":"code","f36c9566":"markdown","855d603d":"markdown","5eb453a4":"markdown","d0d91f73":"markdown","f3f66fd5":"markdown","6068baa8":"markdown","c190730c":"markdown","bad6dff5":"markdown","9c0fe520":"markdown","19b71fc2":"markdown","bf69fd09":"markdown"},"source":{"bc8f52d1":"import pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.utils.multiclass import unique_labels\nfrom xgboost import XGBClassifier\n\nimport time\nimport pickle\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nfrom lwoku import get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search","3479b19c":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","ab9684d9":"prediction = pd.read_csv('..\/input\/tactic-02-stack-classifiers\/submission_lg.csv', index_col='Id', engine='python')\ntrain_frequency = y_train.value_counts()\ntest_frequency = prediction['Cover_Type'].value_counts()\nweight = (test_frequency \/ test_frequency.min()).to_dict()\n\nfig = make_subplots(rows=1, cols=2)\nfig.add_trace(go.Bar(\n                     name='Train',\n                     x=train_frequency.index,\n                     y=train_frequency\n                    ),\n              row=1,\n              col=1)\nfig.add_trace(go.Bar(\n                     name='Predicted test',\n                     x=test_frequency.index,\n                     y=test_frequency\n                    ),\n              row=1,\n              col=2)\nfig.update_xaxes(title_text='Cover type', dtick = 1)\nfig.update_yaxes(title_text='Frequency')\nfig.update_layout(width=2 * 360 + 100,\n                  height=360,\n                  title='Frequency of cover type in train and predicted test',\n                  hovermode='closest',\n                  template='none'\n                 )\nfig.show()","d30bc18b":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_liblinear.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_li_w_clf = clf.best_estimator_\nlr_li_w_clf.class_weight = weight\nlr_li_w_clf","5de1c019":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_saga.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_sa_w_clf = clf.best_estimator_\nlr_sa_w_clf.class_weight = weight\nlr_sa_w_clf","e35c8aec":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_sag.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_sg_w_clf = clf.best_estimator_\nlr_sg_w_clf.class_weight = weight\nlr_sg_w_clf","ad02e115":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_lbfgs.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_lb_w_clf = clf.best_estimator_\nlr_lb_w_clf.class_weight = weight\nlr_lb_w_clf","0ab86a8e":"with open('..\/input\/tactic-03-hyperparameter-optimization-lr\/clf_newton-cg.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlr_w_clf = clf.best_estimator_\nlr_w_clf.class_weight = weight\nlr_w_clf","2111ce0a":"# with open('..\/input\/tactic-03-hyperparameter-optimization-svc\/clf_weighted.pickle', 'rb') as fp:\n#     svc_w_clf = pickle.load(fp)\n# svc_w_clf.class_weight = weight\n# svc_w_clf","f0a2c18e":"with open('..\/input\/tactic-03-hyperparameter-optimization-xtra-trees\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nxt_w_clf = clf.best_estimator_\nxt_w_clf.class_weight = weight\nxt_w_clf","615c5525":"with open('..\/input\/tactic-03-hyperparameter-optimization-rf\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nrf_w_clf = clf.best_estimator_\nrf_w_clf.class_weight = weight\nrf_w_clf","239de4bb":"weight_0 = weight.copy()\nfor k in range(1, 8):\n    weight_0[k - 1] = weight_0.pop(k)\nweight_0","11921478":"with open('..\/input\/tactic-03-hyperparameter-optimization-lightgbm\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlg_w_clf = clf.best_estimator_\nlg_w_clf.class_weight = weight_0   \nlg_w_clf","337dba46":"models = [\n          ('lr_li-w', lr_li_w_clf),\n          ('lr_sa-w', lr_sa_w_clf),\n          ('lr_sg-w', lr_sg_w_clf),\n          ('lr_lb-w', lr_lb_w_clf),\n          ('lr-w', lr_w_clf),\n#           ('svc-w', svc_w_clf),\n          ('xt-w', xt_w_clf),\n          ('rf-w', rf_w_clf),\n          ('lg-w', lg_w_clf),\n]","5b1cde00":"results = pd.DataFrame(columns = ['Model',\n                                  'Accuracy',\n                                  'Fit time',\n                                  'Predict test set time',\n                                  'Predict train set time'])\n\nfor name, model in models:\n    print(name)\n\n    # Fit\n    t0 = time.time()\n    model.fit(X_train, y_train)\n    t1 = time.time()\n    t_fit = (t1 - t0)\n    \n    # Predict test set\n    t0 = time.time()\n    y_test_pred = pd.Series(model.predict(X_test), index=X_test.index)\n    t1 = time.time()\n    t_test_pred = (t1 - t0)\n\n    # Predict train set\n    t0 = time.time()\n    y_train_pred = pd.Series(get_prediction(model, X_train, y_train), index=X_train.index)\n    accuracy = accuracy_score(y_train, y_train_pred)\n    t1 = time.time()\n    t_train_pred = (t1 - t0)\n\n    # Submit\n    y_train_pred.to_csv('train_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    y_test_pred.to_csv('submission_' + name + '.csv', header=['Cover_Type'], index=True, index_label='Id')\n    print('\\n')\n    \n    results = results.append({\n        'Model': name,\n        'Accuracy': accuracy,\n        'Fit time': t_fit,\n        'Predict test set time': t_test_pred,\n        'Predict train set time': t_train_pred\n    }, ignore_index = True)","007027a4":"results = results.sort_values('Accuracy', ascending=False).reset_index(drop=True)\nresults.to_csv('results.csv', index=True, index_label='Id')\nresults","812f803c":"tactic_03_results = pd.read_csv('..\/input\/tactic-03-hyperparameter-optimization\/results.csv', index_col='Id', engine='python')\ntactic_03_results","59161188":"comparison = pd.DataFrame(columns = ['Model',\n                                     'Accuracy',\n                                     'Fit time',\n                                     'Predict test set time',\n                                     'Predict train set time'])\n\ndef get_increment(df1, df2, model, column):\n    model1 = model.split('-', 1)[0]\n    v1 = float(df1[df1['Model'] == model1][column])\n    v2 = float(df2[df2['Model'] == model][column])\n    return '{:.2%}'.format((v2 - v1) \/ v1)\n\nfor model in results['Model']:\n    accuracy = get_increment(tactic_03_results, results, model, 'Accuracy')\n    fit_time = get_increment(tactic_03_results, results, model, 'Fit time')\n    predict_test_set_time = get_increment(tactic_03_results, results, model, 'Predict test set time')\n    predict_train_set_time = get_increment(tactic_03_results, results, model, 'Predict train set time')\n    comparison = comparison.append({\n        'Model': model,\n        'Accuracy': accuracy,\n        'Fit time': fit_time,\n        'Predict test set time': predict_test_set_time,\n        'Predict train set time': predict_train_set_time\n    }, ignore_index = True)    \n\ncomparison","f36c9566":"# LightGBM\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)","855d603d":"# Class weight\n","5eb453a4":"# Random forest classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. RF](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-rf)","d0d91f73":"# Extra-trees classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. Xtra-trees](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-xtra-trees)","f3f66fd5":"# Logistic Regression classifier\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LR](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lr).","6068baa8":"# C-Support Vector Classification\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. SVC](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-svc)","c190730c":"# Prepare data","bad6dff5":"# Compare","9c0fe520":"# Introduction\n\nThe aim of this notebook is to compare if the models with weighted classes score more than without weighting: [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).\n\nThe results are collected at [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary).","19b71fc2":"The frequency of the cover type in the train set is not proportional to the real data.\n\nAn estimate is used to know the frequency of the test data. The predicted test in [Tactic 02. Stack classifiers](https:\/\/www.kaggle.com\/juanmah\/tactic-02-stack-classifiers) has a 77.31 % of accuracy.\n\nAnother method could be submitting a submission file with the same value for all the samples.\nThis would result in knowing the real frequency of that value.\nThen, all frequencies can be found by submitting for each cover type.","bf69fd09":"### Model list"}}