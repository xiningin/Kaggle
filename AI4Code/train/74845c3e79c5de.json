{"cell_type":{"8791756a":"code","7ca0ab69":"code","2273ba88":"code","2f66c76b":"code","a15d08c6":"code","72596b7c":"code","44bc8b42":"code","7be14739":"code","ce50f67c":"code","9ec6c779":"code","43c72025":"code","dc722dfd":"code","3d937d71":"code","311b4692":"code","53783ec4":"code","2e0aeb39":"code","8a01ecd3":"code","84e97b37":"code","01f39503":"code","b65f640f":"code","62d2b4e7":"code","cd5856f7":"code","2f9a98bd":"code","b156a552":"code","73591c7e":"code","a2ffd344":"code","dc399b96":"code","65df6a2b":"code","024df7cc":"code","bc5a0f8f":"code","a9266083":"code","a7cbf296":"code","bd3f61f3":"code","f801ca1c":"code","0fba1fec":"code","f322cbd8":"code","e3d22572":"code","3739141b":"code","61d86c34":"code","3ff226af":"code","f93eff22":"code","48c9dcbf":"code","9176b871":"code","862ab737":"code","765442fc":"code","8b393b29":"code","f4ccf73a":"code","96934c26":"code","f13c356e":"code","5735ad71":"code","f27419bd":"code","f1a6a1d9":"code","625b3434":"code","9cd4cabc":"code","91be90b0":"code","0bf27ba8":"code","0a1fff76":"code","e0b9e500":"code","2914eb4b":"code","a7617368":"code","dcab9f06":"code","2fea1f0a":"code","3a1e09c2":"code","0cf77dd6":"code","22e0afca":"code","f222a940":"code","15d04471":"code","270acb6c":"code","c5574546":"code","b3bc45fe":"code","7e7fa414":"code","b1dc424e":"code","80f1a16e":"code","f83c5c4d":"code","9b13182d":"code","cebf46a1":"code","4b9d61dd":"code","d7918d2e":"code","bd49c3af":"code","ba57f41a":"code","8449f1b2":"code","407a4cc4":"code","41cf59b0":"code","8fdeb8ff":"code","7d1b035b":"code","5b51fd92":"code","9b2b4a14":"code","6447f611":"code","90e9b92d":"code","42632e01":"code","6a4314bb":"code","648ff07f":"code","06e99ae0":"code","0ebd1734":"code","5ed23083":"code","5748a663":"code","1a290a01":"code","12126268":"code","ec73ea4c":"code","bbab1f8b":"code","c0b9016c":"code","06fa6f15":"code","a4c09a80":"code","031e8762":"code","ee4d0e52":"code","6040cfd4":"code","bd3424f6":"code","001dbe01":"code","90391346":"code","9741a792":"code","8c1c1f88":"code","3a35b8b0":"code","40be1da6":"code","400b22d2":"code","9911bbf1":"code","6a124dcb":"code","8cc36d3b":"code","d8435f6b":"code","0b4a6ee4":"code","072fbbb0":"code","9aae36fc":"code","1e6bbd3f":"code","d50d9e4e":"code","a267319e":"code","9095b157":"code","a9df223b":"code","5adac6b5":"code","deb69cdf":"code","df5cf172":"code","8ee896d6":"code","e1c1ea3f":"code","a6f6d4a2":"code","fb58ee3e":"code","8c0b64c0":"code","189da55d":"code","f1284d88":"code","07f7b833":"code","fde6dec2":"code","66e95eb3":"code","0cc6696c":"code","724a1d7c":"code","b8be31ad":"code","2ec75796":"code","c76343da":"code","e8d7dad4":"markdown","aa5cea18":"markdown","0a88b183":"markdown","61e865e1":"markdown","a1795343":"markdown","f81631b7":"markdown","65bc9cef":"markdown","79773001":"markdown","c2a0c9c3":"markdown","e63e20a7":"markdown","f8c01a19":"markdown","924c62b6":"markdown","20e15076":"markdown","4e5215f8":"markdown","9652a2f5":"markdown","9e05abd8":"markdown","0dcdf991":"markdown","c536835b":"markdown","c6d50675":"markdown","c37b3ea9":"markdown","5e9ba020":"markdown","3578cf67":"markdown","5ffffc4c":"markdown","85931407":"markdown","9b6749bf":"markdown","0ceb11e9":"markdown","89a0d3be":"markdown","bf362648":"markdown","71889826":"markdown","90d68092":"markdown","e9ce792b":"markdown","40436391":"markdown","330c6d17":"markdown","3209be38":"markdown","3f6579de":"markdown","72f77d7f":"markdown","7c1c0dd1":"markdown","a37077c1":"markdown","63bebbe0":"markdown","8df213e9":"markdown","a51e89ab":"markdown"},"source":{"8791756a":"import os\n\nfrom sklearn.metrics import classification_report\n\n\ndef classifcation_report_train_test(y_train, y_train_pred, y_test, y_test_pred):\n\n    print('''\n            =========================================\n               CLASSIFICATION REPORT FOR TRAIN DATA\n            =========================================\n            ''')\n    print(classification_report(y_train, y_train_pred))\n\n    print('''\n            =========================================\n               CLASSIFICATION REPORT FOR TEST DATA\n            =========================================\n            ''')\n    print(classification_report(y_test, y_test_pred))","7ca0ab69":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')","2273ba88":"# Load the Bank.csv\ndata=pd.read_csv(\"..\/input\/bank-personal-loan-campaign\/Bank.csv\",skiprows=4,skipfooter=3,na_values=['Null'])","2f66c76b":"# Check the head and tail of data\ndata.head(6)","a15d08c6":"data.tail(6)","72596b7c":"data=data.drop(columns=['ID','ZIP Code'],axis=1)\ndata.head()","44bc8b42":"#converting the column names to Lower case\ndata.columns = map(str.lower, data.columns)\ndata.columns","7be14739":"#replace spaces in column names with _\ndata.columns = [x.replace(' ', '_') for x in data.columns]\ndata.columns","ce50f67c":"data.head()","9ec6c779":"#check for null values\ndata.apply(lambda x : sum(x.isnull()))","43c72025":"#find unique levels in each column\\\ndata.apply(lambda x: len(x.unique()))","dc722dfd":"\ndef myfunc(x):\n    return len(x.unique())\n\ndata.apply(myfunc)","3d937d71":"data.apply(lambda x: len(x.unique())).sort_values()","311b4692":"# check the statistics of Dataframe\ndata.describe()","53783ec4":"data.describe().T","2e0aeb39":"#Check is there any customers with negative experience, if yes remove those rows from data \n#data[data['experience']<0].count()\nprint('People Having Negative Experience:',data[data['experience'] < 0]['experience'].count())\nprint('People Having Positive Experience:',data[data['experience'] > 0]['experience'].count())","8a01ecd3":"#Check is there any customers with negative experience, if yes remove those rows from data \ndata.drop(data[data['experience']<0].index,inplace=True)","84e97b37":"data.experience.value_counts()","01f39503":"df = data.copy()\ndf.head()","b65f640f":"for col in df.columns:\n    if len(df[col].unique()) < 10:\n        print(col, df[col].unique())","62d2b4e7":"# Check the no of levels of all categorical columns\n\nnum_col=['age','experience','income',\"ccavg\",'mortgage']\ncat_col=df.columns.difference(num_col)\ncat_col","cd5856f7":"df[cat_col] = df[cat_col].apply(lambda x: x.astype('category'))\ndf[num_col] = df[num_col].apply(lambda x: x.astype('float'))\ndf.dtypes","2f9a98bd":"df[cat_col]","b156a552":"for i in df[cat_col]:\n    #print([i],':',df[cat_col[i]].unique())\n    print(i,':',df[i].nunique())\n    \n","73591c7e":"num_data = data.loc[:,num_col]\ncat_data = data.loc[:,cat_col]\nnum_data.head()","a2ffd344":"# check is there any NA values present, and if present impute them \ncat_data.isna().sum()","dc399b96":"num_data.isna().sum()","65df6a2b":"#num_data.fillna()\nnum_data.fillna(num_data['age'].mean(), inplace = True) ","024df7cc":"num_data.isna().sum()","bc5a0f8f":"full_data = pd.concat([num_data,cat_data],axis=1)\nfull_data.head()","a9266083":"# check the Personal loan statistics with plot which is suited\nfull_data['personal_loan'].value_counts().plot(kind='bar')","a7cbf296":"# Influence of income and education on personal loan and give the observations from the plot\nsns.boxplot(x='education',y='income',hue='personal_loan',data=full_data)","bd3f61f3":"# Influence of Credict card usage and Personal Loan  and give observations from the plot\nsns.boxplot(y='ccavg',x='personal_loan',data=full_data)","f801ca1c":"# Influence of education level on persoanl loan and give the insights\nfull_data.groupby(['education','personal_loan']).size().plot(kind='bar')\nplt.xticks(rotation=30)                                                      ","0fba1fec":"# Influence of family size on persoanl loan and suggest the insights\nsns.countplot(x='family',data=full_data,hue='personal_loan',palette='Set3')","f322cbd8":"# Influence of deposit account on personal loan and give the insights\nsns.countplot(x='cd_account',data=full_data,hue='personal_loan')","e3d22572":"# Influence of Security account on personal loan and give the insights\nsns.countplot(x=\"securities_account\", data=full_data,hue=\"personal_loan\")","3739141b":"# Influence of Credit card on Persoanl Loan and give insights\nsns.countplot(x=\"creditcard\", data=full_data,hue=\"personal_loan\")","61d86c34":"#median\nprint('Non-Loan customers: ',full_data[full_data.personal_loan == 0]['ccavg'].median()*1000)\nprint('Loan customers    : ', full_data[full_data.personal_loan == 1]['ccavg'].median()*1000)","3ff226af":"#mean\nprint('Non-Loan customers: ',full_data[full_data.personal_loan == 0]['ccavg'].mean()*1000)\nprint('Loan customers    : ', full_data[full_data.personal_loan == 1]['ccavg'].mean()*1000)","f93eff22":"#family income personalloan realtionship\nsns.boxplot(x='family',y='income',data=full_data,hue='personal_loan')","48c9dcbf":"# Correlation with heat map\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = data.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})","9176b871":"# Correlation with heat map\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorr = data.corr()\nsns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 2.5})\nplt.figure(figsize=(13,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","862ab737":"#mask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True","765442fc":"# creating` dummies for columns which have more than two levels\nfor col in cat_data.columns:\n    if len(cat_data[col].unique()) > 2:\n        print(col, cat_data[col].unique())","8b393b29":"df = pd.get_dummies(data=full_data, columns=['family', 'education'], drop_first=True)\ndf.head()","f4ccf73a":"# Train test split\n## Split the data into X_train, X_test, y_train, y_test with test_size = 0.20 using sklearn\nX = df.copy().drop(\"personal_loan\",axis=1)\ny = df[\"personal_loan\"]\n\n## Split the data into X_train, X_test, y_train, y_test with test_size = 0.20 using sklearn\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n## Print the shape of X_train, X_test, y_train, y_test\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","96934c26":"X_train.head()","f13c356e":"X_train.isna().sum()","5735ad71":"# after split check the proportion of target levels - train\nprint(y_train.value_counts(normalize=True)) ","f27419bd":"# after split check the proportion of target levels - test\nprint(y_test.value_counts(normalize=True))","f1a6a1d9":"#you can also plot this \n\nimport matplotlib.pyplot as plt\ny_test.value_counts(normalize=True).plot(kind='bar')","625b3434":"# Implement ***SVM CLASSIFIER*** with grid search ","9cd4cabc":"# Predict","91be90b0":"# Apply the follwing models and show a data frame with the all the model performances\n#    1. Logistic Regression - We haven't given the code, you need to explore!\n#    2. Decision trees\n#    3. K-nn \n    \n# Please ensure you experiment with multiple hyper parameters for the each of the above algorithms\n","0bf27ba8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","0a1fff76":"lrc = LogisticRegression()\n\nlrc.fit(X_train,y_train)\n\ny_train_pred_lrc_be = lrc.predict(X_train)\ny_test_pred_lrc_be = lrc.predict(X_test)","e0b9e500":"svc = SVC()\n\nsvc.fit(X_train,y_train)\n\ny_train_pred_svc_be = svc.predict(X_train)\ny_test_pred_svc_be = svc.predict(X_test)","2914eb4b":"dtc = DecisionTreeClassifier()\n\ndtc.fit(X_train,y_train)\n\ny_train_pred_dt_be = dtc.predict(X_train)\ny_test_pred_dt_be = dtc.predict(X_test)","a7617368":"knn = KNeighborsClassifier()\n\nknn.fit(X_train,y_train)\n\ny_train_pred_knn_be = dtc.predict(X_train)\ny_test_pred_knn_be = dtc.predict(X_test)\n#classifcation_report_train_test(y_train,y_train_pred_knn_be,y_test, y_test_pred_knn_be)","dcab9f06":"naive_model = GaussianNB()\nnaive_model.fit(X_train, y_train)\n\ny_train_pred_nv_be = naive_model.predict(X_train)\ny_test_pred_nv_be =naive_model.predict(X_test)","2fea1f0a":"from sklearn.metrics import recall_score\n\nprint(\"Recall of DecisionTrees:\",recall_score(y_test, y_test_pred_dt_be))\nprint(\"Recall of LogisticRegression:\",recall_score(y_test, y_test_pred_lrc_be))\nprint(\"Recall of SupportVectorMachines:\",recall_score(y_test, y_test_pred_svc_be))\nprint(\"Recall of KNearestNeighbours:\",recall_score(y_test, y_test_pred_knn_be))\nprint(\"Recall of naiibeys:\",recall_score(y_test, y_test_pred_nv_be))","3a1e09c2":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, lrc,scoring='recall')\nplt.show()","0cf77dd6":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, svc,scoring='recall')\nplt.show()","22e0afca":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, dtc,scoring='recall')\nplt.show()","f222a940":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, knn,scoring='recall')\nplt.show()","15d04471":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, naive_model,scoring='recall')\nplt.show()","270acb6c":"#Scale the numeric attributes\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(X_train[num_col])\n\nX_train[num_col] = scaler.transform(X_train[num_col])\nX_test[num_col] = scaler.transform(X_test[num_col])\nX_train.head()","c5574546":"#Build svc Classifier\nfrom sklearn.svm import SVC","b3bc45fe":"## Create an SVC object and print it to see the default arguments\n\nsvc = SVC(class_weight='balanced')\nsvc","7e7fa414":"svc.fit(X_train,y_train)","b1dc424e":"y_train_pred_svc = svc.predict(X_train)\ny_test_pred_svc = svc.predict(X_test)","80f1a16e":"#from sklearn.metrics import classifcation_report_train_test\nclassifcation_report_train_test(y_train, y_train_pred_svc, y_test, y_test_pred_svc)","f83c5c4d":"!pip install mlxtend","9b13182d":"## Use Grid Search for parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nsvc_grid = SVC(class_weight='balanced')\n \n\nparam_grid = {\n\n'C': [0.001, 0.01, 0.1, 1, 10],\n'gamma': [0.001, 0.01, 0.1, 1], \n'kernel':['linear', 'rbf']}\n\n \nsvc_cv_grid = GridSearchCV(estimator = svc_grid, param_grid = param_grid, cv = 10)","cebf46a1":"svc_cv_grid.fit(X_train, y_train)","4b9d61dd":"# Display the best estimator\nsvc_cv_grid.best_estimator_","d7918d2e":"#predicting using best_estimator\ny_train_pred_svc_best = svc_cv_grid.best_estimator_.predict(X_train)\ny_test_pred_svc_best = svc_cv_grid.best_estimator_.predict(X_test)","bd49c3af":"#classification reprot by using base function created on first cell\nclassifcation_report_train_test(y_train, y_train_pred_svc_best, y_test, y_test_pred_svc_best)","ba57f41a":"## Use Grid Search for parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nsvc_grid = SVC(class_weight='balanced')\n \n\nparam_grid = {\n\n'C': [0.6,0.7,0.8,0.9,1,1.5],\n'gamma': [1,2,3,4,5], \n'kernel':['linear', 'rbf']}\n\n \nsvc_cv_grid2 = GridSearchCV(estimator = svc_grid, param_grid = param_grid, cv = 10)","8449f1b2":"svc_cv_grid2.fit(X_train, y_train)","407a4cc4":"svc_cv_grid2.best_estimator_","41cf59b0":"y_train_pred_svc_best2 = svc_cv_grid2.best_estimator_.predict(X_train)\ny_test_pred_svc_best2 = svc_cv_grid2.best_estimator_.predict(X_test)","8fdeb8ff":"classifcation_report_train_test(y_train, y_train_pred_svc_best2, y_test, y_test_pred_svc_best2)","7d1b035b":"## Use Grid Search for parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nsvc_grid = SVC(class_weight='balanced')\n \n\nparam_grid = {\n\n'C': [0.6,0.7,0.8,0.9,1,1.5],\n'gamma': [0.6,0.7,0.8],  \n'kernel':['linear', 'rbf']}\n\n \nsvc_cv_grid3 = GridSearchCV(estimator = svc_grid, param_grid = param_grid, cv = 10)","5b51fd92":"svc_cv_grid3.fit(X_train, y_train)","9b2b4a14":"svc_cv_grid3.best_estimator_","6447f611":"y_train_pred_svc_best1 = svc_cv_grid3.best_estimator_.predict(X_train)\ny_test_pred_svc_best1 = svc_cv_grid3.best_estimator_.predict(X_test)","90e9b92d":"classifcation_report_train_test(y_train, y_train_pred_svc_best1, y_test, y_test_pred_svc_best1)","42632e01":"## Use Grid Search for parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nsvc_grid = SVC(class_weight='balanced')\n \n\nparam_grid = {\n\n'C': [0.9,1,1.2,1.3,1.4],\n'gamma': [0.6,0.7,0.8], \n'kernel':['linear', 'rbf']}\n\n \nsvc_cv_grid4 = GridSearchCV(estimator = svc_grid, param_grid = param_grid, cv = 10)","6a4314bb":"svc_cv_grid4.fit(X_train, y_train)","648ff07f":"svc_cv_grid4.best_estimator_","06e99ae0":"svc_cv_grid4.best_params_","0ebd1734":"y_train_pred_svc_best2 = svc_cv_grid4.best_estimator_.predict(X_train)\ny_test_pred_svc_best2 = svc_cv_grid4.best_estimator_.predict(X_test)","5ed23083":"classifcation_report_train_test(y_train, y_train_pred_svc_best2, y_test, y_test_pred_svc_best2)","5748a663":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, svc_cv_grid4.best_estimator_,scoring='recall')\n","1a290a01":"knn1 = KNeighborsClassifier(n_neighbors= 3 , weights = 'uniform', metric='euclidean')\nknn1.fit(X_train, y_train)    \npredicted = knn1.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, predicted)\nprint(acc)","12126268":"knn1 = KNeighborsClassifier(n_neighbors= 5 , weights = 'uniform', metric='euclidean')\nknn1.fit(X_train, y_train)    \npredicted = knn1.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc = recall_score(y_test, predicted)\nprint(acc)","ec73ea4c":"knn2 = KNeighborsClassifier(n_neighbors= 7, weights = 'uniform', metric='euclidean')\nknn2.fit(X_train, y_train)    \npredicted2 = knn2.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc = recall_score(y_test, predicted2)\nprint(acc)","bbab1f8b":"grid_params ={\n    'n_neighbors':[3,5,7,9],\n    'weights':['uniform','distance'],\n    'metric':['euclidean','manhattan','minkowski']\n}\ngs_results=GridSearchCV(KNeighborsClassifier(),grid_params,verbose=1,cv=5,n_jobs=-1)\ngs_results.fit(X_train,y_train)","c0b9016c":"gs_results.best_estimator_","06fa6f15":"gs_results.best_params_","a4c09a80":"gs_results.best_score_","031e8762":"knn_predict_train=gs_results.best_estimator_.predict(X_train)\nknn_predict_test=gs_results.best_estimator_.predict(X_test)","ee4d0e52":"classifcation_report_train_test(y_train, knn_predict_train, y_test, knn_predict_test)","6040cfd4":"error_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","bd3424f6":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nplt.grid()\nplt.show()","001dbe01":"from sklearn.tree import DecisionTreeClassifier","90391346":"depths = np.arange(1, 21)\ndepths","9741a792":"#num_leafs = [1, 5, 10, 20, 50, 100]\n#criterion = ['gini', 'entropy']","8c1c1f88":"param_grid = { 'criterion':['gini','entropy'],'max_depth': depths}\ndtree_model=DecisionTreeClassifier()","3a35b8b0":"gs = GridSearchCV(estimator=dtree_model, param_grid=param_grid, cv=10)","40be1da6":"gs = gs.fit(X_train, y_train)","400b22d2":"gs.best_estimator_","9911bbf1":"gs.best_params_","6a124dcb":"dt_predict_train=gs.best_estimator_.predict(X_train)\ndt_predict_test=gs.best_estimator_.predict(X_test)","8cc36d3b":"classifcation_report_train_test(y_train, dt_predict_train, y_test, dt_predict_test)","d8435f6b":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, gs.best_estimator_,scoring='recall')\nplt.show()","0b4a6ee4":"from sklearn import tree\ndt1=tree.DecisionTreeClassifier(max_depth=6)\ndt1.fit(X_train,y_train)","072fbbb0":"dt1.get_depth","9aae36fc":"dt1.get_params","1e6bbd3f":"#dt1.predict(X_test)\ndt1_predict_train=dt1.predict(X_train)\ndt1_predict_test=dt1.predict(X_test)","d50d9e4e":"classifcation_report_train_test(y_train, dt1_predict_train, y_test, dt1_predict_test)","a267319e":"tree.plot_tree(dt1.fit(X_train, y_train)) ","9095b157":"!pip install pydotplus","a9df223b":"dt1.feature_importances_","5adac6b5":"f_imp = pd.Series(dt1.feature_importances_, index = X_train.columns)","deb69cdf":"## Sort importances  \nf_imp_order= f_imp.nlargest(n=10)\nf_imp_order","df5cf172":"## Plot Importance\n%matplotlib inline\nf_imp_order.plot(kind='barh')\nplt.show()","8ee896d6":"# Grid search cross validation\nfrom sklearn.linear_model import LogisticRegression\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(X_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","e1c1ea3f":"logreg2=LogisticRegression(C=10,penalty=\"l2\")\nlogreg2.fit(X_train,y_train)\nprint(\"score\",logreg2.score(X_test,y_test))\n","a6f6d4a2":"reg_predict_train=logreg2.predict(X_train)\nreg_predict_test=logreg2.predict(X_test)\nclassifcation_report_train_test(y_train, reg_predict_train, y_test, reg_predict_test)","fb58ee3e":"# Create first pipeline for base without reducing features.\n\n#pipe = Pipeline([('classifier' , RandomForestClassifier())])\n# pipe = Pipeline([('classifier', RandomForestClassifier())])\n\n# Create param grid.\nfrom sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier(random_state=42)\nparam_grid = { \n    'n_estimators': [240,245,250],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [12,13,14],\n    'criterion' :['gini', 'entropy']\n}\n# Create grid search object\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\n","8c0b64c0":"CV_rfc.best_estimator_\n\n","189da55d":"rf_predict_train=CV_rfc.best_estimator_.predict(X_train)\nrf_predict_test=CV_rfc.best_estimator_.predict(X_test)\nclassifcation_report_train_test(y_train, rf_predict_train, y_test, rf_predict_test)","f1284d88":"from mlxtend.plotting import plot_learning_curves\nplot_learning_curves(X_train, y_train, X_test, y_test, CV_rfc.best_estimator_,scoring='recall')\nplt.show()","07f7b833":"CV_rfc.best_estimator_.feature_importances_","fde6dec2":"feat_importances = pd.Series(CV_rfc.best_estimator_.feature_importances_, index = X_train.columns)","66e95eb3":"feat_importances_ordered = feat_importances.nlargest(n=10)\nfeat_importances_ordered","0cc6696c":"## Plot Importance\n%matplotlib inline\nfeat_importances_ordered.plot(kind='barh')\nplt.show()","724a1d7c":"from sklearn.metrics import recall_score\n\nscores = pd.DataFrame(columns=['Model','Train_Recall','Test_Recall'])\n\ndef get_metrics(train_actual,train_predicted,test_actual,test_predicted,model_description,dataframe):\n    \n    train_recall   = recall_score(train_actual,train_predicted)\n    test_recall   = recall_score(test_actual,test_predicted)\n    dataframe = dataframe.append(pd.Series([model_description,train_recall,\n                                            test_recall],\n                                           index=scores.columns ), ignore_index=True)\n    return(dataframe)","b8be31ad":"scores = get_metrics(y_train,y_train_pred_dt_be,y_test,y_test_pred_dt_be,'DecisionTrees basic model',scores)\nscores = get_metrics(y_train,y_train_pred_lrc_be,y_test,y_test_pred_lrc_be,'LogisticRegression basic model',scores)\nscores = get_metrics(y_train, y_train_pred_svc_be,y_test, y_test_pred_svc_be,'SupportVectorMachines basic model',scores)\nscores = get_metrics(y_train, y_train_pred_knn_be,y_test, y_test_pred_knn_be,'KNearestNeighbours basic model',scores)\nscores = get_metrics(y_train, y_train_pred_nv_be,y_test, y_test_pred_nv_be,'naiibeys basic model',scores)\nscores = get_metrics(y_train,dt_predict_train,y_test,dt_predict_test,'Decision Tree with GridSearchCV()',scores)\nscores = get_metrics(y_train,reg_predict_train,y_test,reg_predict_test,'logistic regression with GridSearchCV()',scores)\nscores = get_metrics(y_train,y_train_pred_svc_best2,y_test,y_test_pred_svc_best2,'SVC using GridSearchCV()',scores)\nscores = get_metrics(y_train,knn_predict_train,y_test,knn_predict_test,'KNN using GridSearchCV(),Where k=5',scores)\nscores = get_metrics(y_train,rf_predict_train,y_test,rf_predict_test,'random forest using GridSearchCV',scores)\n\n","2ec75796":"scores","c76343da":"scores.insert(3, \"Best Tuning Parametrs\",['','','','','','{criterion: gini, max_depth: 6}', '{C: 10.0, penalty: l2}', '{C: 1, gamma: 0.7, kernel: rbf}', '{metric: euclidean, n_neighbors: 3, weights: distance}','max_depth=13,max_features=auto,n_estimators=240,criterion=entropy'], True)\nscores\n#df.insert(2, \"Age\", [21, 23, 24, 21], True)","e8d7dad4":"# Another way to find unique levels in each column","aa5cea18":"Observation : It seems the customers whose education level is 1 is having more income. However customers who has taken the personal loan have the same income levels","0a88b183":"\n# KNN tuning using GridSearchCV","61e865e1":"# Standardization","a1795343":"# Logistic Regression With GridSearchCV()","f81631b7":"Observation : Family size does not have any impact in personal loan. But it seems families with size of 3 are more likely to take loan. When considering future campaign this might be good association.","65bc9cef":"# Learning Curves","79773001":"Vectors of data represented as lists, numpy arrays, or pandas Series objects passed directly to the x, y, and\/or hue parameters.\nThe x, y, and hue variables will determine how the data are plotted","c2a0c9c3":"<h4><font color='#F55905'> <u>BUSINESS PROBLEM:<\/u><\/font><center><br><br><font color='#15657F'>REDUCE THE CAMPAIGN COST FOR THE PRODUCT(PERSONAL LOAN) OF IDBI BANK<\/font><\/center><\/h4>\n\nThis case is about a bank (IDBI Bank) which has a growing customer base. Majority of these customers are liability customers (depositors) with varying size of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). \n\n   ***A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success.  This has encouraged the retail marketing department to devise campaigns to better target marketing to increase the success ratio with a minimal budget.***\n   \nThe department wants to build a model that will help them identify the potential customers who have a higher probability of purchasing the loan. \nThis will increase the success ratio while at the same time reduce the cost of the campaign.\n\nThe file given below contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). \nAmong these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign\n\n<h4><font color='#F55905'><u>MACHINE LEARNING PROBLEM:<\/u><\/font><center><br><br><font color='#15657F'>USE A CLASSIFICATION MODEL TO PREDICT THE LIKELYHOOD OF A LIABILITY CUSTOMER BUYING PERSONAL LOANS<\/font><\/center><\/h4>","e63e20a7":"# Decision Tree Tuning using GridSearchCV()","f8c01a19":"# For Better Understanding Transpose The Above Matrix (by using T) ","924c62b6":"# Trial 4","20e15076":"Let's go ahead and use the elbow method to pick a good K Value:","4e5215f8":"C : Create regularization hyperparameter space","9652a2f5":"Observation : families with income less than 100K are less likely to take loan,than families with high income","9e05abd8":"# RF Classifier","0dcdf991":"## Imputations","c536835b":"<h4><font color='#F55905'><u>ATTRIBUTE DESCRIPTION:<\/u><\/font><\/h4>","c6d50675":"l1,l2 : Create regularization penalty space","c37b3ea9":"The variable ID,ZIP Code does not add any interesting information. There is no association between a person's customer ID ,ZIP Code and loan.So We Can Drop ID ,ZipCode","5e9ba020":"\n# Trial 3","3578cf67":"Observed : \nIncome and CCAvg is moderately correlated.\nAge and Experience is highly correlated","5ffffc4c":"Observation : personal_loan =1, having more 'ccavg' than personal_loan=0\n     .the outliers are more in personal_loan=0 than 1","85931407":"We got max_depth=6 .So, use this max_depth value in DTClassifier ","9b6749bf":"Observation : Customers who does not have cd_account , does not have loan as well. This seems to be majority. But almost all customers who has cd_account has loan as well","0ceb11e9":"Observation: The graph show persons who have personal loan have a higher credit card average.","89a0d3be":"# Why Grid Search is not performed for Naive Bayes Classifier?","bf362648":"there are 52 records with negative experience.so we don't want negative experience better remove those from  data","71889826":"palette : Colors to use for the different levels of the hue variable. Should be something that can be interpreted by color_palette().","90d68092":"<table cellspacing=\"0\" border=\"0\">\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">ID<\/td>\n\t\t<td align=\"left\">Customer ID<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Age<\/td>\n\t\t<td align=\"left\">Customer's age in completed years<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Experience<\/td>\n\t\t<td align=\"left\">#years of professional experience<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Income<\/td>\n\t\t<td align=\"left\">Annual income of the customer<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">ZIPCode<\/td>\n\t\t<td align=\"left\">Home Address ZIP code.<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Family<\/td>\n\t\t<td align=\"left\">Family size of the customer<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">CCAvg Avg.<\/td>\n\t\t<td align=\"left\">spending on credit cards per month<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Education<\/td>\n\t\t<td align=\"left\">Education Level. 1: Undergrad; 2: Graduate; 3: Advanced\/Professional<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Mortgage<\/td>\n\t\t<td align=\"left\">Value of house mortgage if any<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Personal Loan<\/td>\n\t\t<td align=\"left\">Did this customer accept the personal loan offered in the last campaign?<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Securities Account<\/td>\n\t\t<td align=\"left\">Does the customer have a securities account with the bank?<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">CD Account<\/td>\n\t\t<td align=\"left\">Does the customer have a certificate of deposit (CD) account with the bank?<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">Online<\/td>\n\t\t<td align=\"left\">Does the customer use internet banking facilities?<\/td>\n\t<\/tr>\n\t<tr>\n\t\t<td height=\"17\" align=\"left\">CreditCard<\/td>\n\t\t<td align=\"left\">Does the customer uses a credit card issued by UniversalBank?<\/td>\n\t<\/tr>\n<\/table>","e9ce792b":"Observation : at k=3 we are getting less error.....and we can also get k=1 less error but it is overfitting case.So,better to avoid k=1.","40436391":"Observation : Majority of customers who does not have loan have securities account","330c6d17":"# trial 2 Hyperparameter tuning","3209be38":"# Choosing a K Value","3f6579de":"<h4><font color='#F55905'><u> EVALUATION METRIC:<\/u><\/font><\/h4> <br>Recall for the positive class (Correctly classify people who opt for personal loan)","72f77d7f":"# KNN random tuning","7c1c0dd1":" make an array of depths to choose from, say 1 to 20","a37077c1":"# trial 1 Hyperpameter tuning","63bebbe0":"   there is no any hyperparameter to tune","8df213e9":"## Visualisations","a51e89ab":"## Few important parameters of read_csv\n\n### skiprows = Line numbers to skip  or we can specify the set of row names to exclude \n#####                    way 1: pd.read_csv('Bank.csv',skiprows=4)\n#####                   way 2: pd.read_csv('Bank.csv',skiprows=[0,1,2,3,6])\n\n\n### skipfooter = number of lines to skip from the bottom\n#####  pd.read_csv('Bank.csv',skiprows=[0,1,2,3,6],skipfooter=0)\n\n\n### nrows = no of rows to read from a file. useful for big datasets\n\n### na_values = additional strings to recognise as NA\/NAN values\n#### pd.read_csv('Bank.csv',skiprows=[0,1,2,3],na_values={'Age':['Null']},nrows=10) its for feature specific\n#### pd.read_csv('Bank.csv',skiprows=[0,1,2,3],na_values=['Null'],nrows=10)\n\n\n### index_col = used to set the column as a row label\n#### pd.read_csv('Bank.csv',skiprows=[0,1,2,3],na_values=['Null'],nrows=10,index_col='ID')\n\n### usecols = used to subset the set of columns to read \n#### pd.read_csv('Bank.csv',skiprows=[0,1,2,3],na_values=['Null'],nrows=10,usecols=['Age','Income'])"}}