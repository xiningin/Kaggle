{"cell_type":{"01581962":"code","76bbcf53":"code","ff50e313":"code","b856c532":"code","3b00c22e":"code","83db5ba2":"code","8a091099":"code","6de97e90":"code","1c1dd867":"code","2028203b":"code","93cd8c6b":"code","41d99332":"code","0becfd01":"code","5a83629a":"code","014ffb92":"code","2dab7bc0":"code","9d80d2ad":"code","3ed1a26b":"code","669d8ba2":"code","ea6652a1":"code","cfd0f3c7":"code","d0a9d5c4":"code","c3f3fda5":"code","49aaa5dc":"code","6fcb7995":"code","3ac8734e":"code","ce78950c":"code","257b2b38":"code","52a76af9":"code","060ddb64":"code","16b2d328":"code","2971f888":"code","465ded8b":"code","91764a1c":"code","57250a8b":"code","a9ff13b2":"code","c22ce626":"code","9c87a9be":"code","c26e24f1":"code","2c9aecf3":"code","ec517db9":"code","7cdd2b6f":"code","824997d5":"code","c791bbc1":"code","babc7443":"code","6416d280":"code","c348169b":"code","1d6819ad":"code","ababdd34":"markdown","1fd4576d":"markdown","0ea8df41":"markdown","cb62da16":"markdown","e424a2e4":"markdown","d603d984":"markdown","73f17993":"markdown","a2fa5b10":"markdown","a868a3f8":"markdown","98dfa6e5":"markdown","45d95cd6":"markdown","60a7ac99":"markdown"},"source":{"01581962":"# kaggle titanic python solution and tutorial\nimport numpy as np \nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# color palletes\nmale_female_pal = ['#3489d6', '#e64072']\nsurvival_pal = ['#2a2a2a', '#ff0000']\nsns.set_palette(survival_pal)\nsns.set_style(\"whitegrid\")\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, LabelBinarizer, scale, Normalizer, PowerTransformer, MaxAbsScaler\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.svm import SVC, NuSVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport lightgbm as lgb\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","76bbcf53":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head()","ff50e313":"# * Survival - Survival (0=No, 1=Yes)\n# * Pclass - Ticket class \n#            - A proxy for socio-economic status (SES) \n#            - (1=1st=Upper, 2=2nd=Middle, 3=3rd=Lower) \n# * Sex - Sex (Male of Female)\n# * Age - Age in years\n#      - Age is fractional if less than 1. \n#      - If the age is estimated, is it in the form of xx.5\n# * Sibsp - of siblings \/ spouses aboard the Titanic \n#      - Sibling = brother, sister, stepbrother, stepsister\n#      - Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n# * Parch - of parents \/ children aboard the Titanic \n#      - Parent = mother, father\n#      - Child = daughter, son, stepdaughter, stepson\n#      - Some children travelled only with a nanny, therefore parch=0 for them.\n# * Ticket - Ticket number \n# * Fare - Passenger fare \n# * Cabin - Cabin number \n# * Embarked - Port of Embarkation \n#              - C = Cherbourg, \n#              - Q = Queenstown, \n#              - S = Southampton","b856c532":"train.info()","3b00c22e":"# test.info()","83db5ba2":"train.describe(include='all')","8a091099":"# test.describe(include='all')","6de97e90":"# Looking for missing values\n\n# print(train.shape)\n# print(train.isnull().sum())\n\n# print(test.shape)\n# print(test.isnull().sum())","1c1dd867":"# Filling Embarked with most frequent value\n\nprint(train['Embarked'].value_counts())\nmost_freq = train['Embarked'].value_counts().index[0]\ntrain['Embarked'].fillna(most_freq, inplace=True)","2028203b":"# Filling Fare with mean value\n\nmean_fare = test['Fare'].mean()\ntest['Fare'].fillna(mean_fare, inplace=True)","93cd8c6b":"train.info()","41d99332":"# converting to catogorical values into categorical columns \n\ncat_cols = ['Pclass', 'Sex', 'Embarked']\nfor i in cat_cols:\n    train[i] = train[i].astype('category')\n    test[i] = test[i].astype('category')\n    \n# train.info()","0becfd01":"# No. of people survived\n\nsns.countplot(x='Survived', data=train)\nplt.show()","5a83629a":"# Correlation between columns\n\nplt.figure(figsize=(8, 6))\ndf_corr = train.drop('PassengerId', axis=1).corr()\nsns.heatmap(df_corr, annot=True, fmt='.2f', cmap='RdBu', vmax=0.8, vmin=-0.8)\nplt.show()","014ffb92":"# Pairplot\n\nplt.figure(figsize=(8, 8))\nsns.pairplot(train.drop('PassengerId', axis=1), hue=\"Survived\", palette=survival_pal)\nplt.plot()","2dab7bc0":"# How being in different categories resulted in the survival ?\n\ncat_cols = ['Pclass', 'Sex', 'Embarked']\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 4))\nfor ind, val in enumerate(cat_cols):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])\n    plt.legend(['Not Survived', 'Survived'])","9d80d2ad":"# Did people hold on to their families ?\n\ncat_cols = ['SibSp', 'Parch']\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 4))\nfor ind, val in enumerate(cat_cols):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])\n    plt.legend(['Not Survived', 'Survived'])","3ed1a26b":"# Class - Gender - Survival\n\ng = sns.FacetGrid(train, col='Embarked', size=4)\ng.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', order=[1, 2, 3], hue_order=['male', 'female'], palette=male_female_pal)\ng.add_legend()\nplt.show()","669d8ba2":"# Does the title has more information\n\n# extracting the titel\ntrain[\"Title\"] = train[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)\ntest[\"Title\"] = test[\"Name\"].str.extract('([A-Za-z]+)\\.',expand=False)\n\n# replacing similar titles\nfor i in [train, test]:\n    i['Title'] = i['Title'].replace('Mr', 'Mr')\n    i['Title'] = i['Title'].replace(('Mme', 'Ms'), 'Mrs')\n    i['Title'] = i['Title'].replace('Mlle', 'Miss')\n    i['Title'] = i['Title'].replace(('Capt', 'Col', 'Major', 'Dr','Rev'), 'Officer')\n    i['Title'] = i['Title'].replace(('Jonkheer', 'Don', 'Sir', 'Countess','Dona', 'Lady'), 'Royalty')\n    \n# Titile vs Age Distribution\nsns.set_palette('Paired')\nplt.figure(figsize=(15, 6))\nax = sns.kdeplot(train[train['Title']=='Mr']['Age'], shade=True, label='Mr')\nax = sns.kdeplot(train[train['Title']=='Mrs']['Age'], shade=True, label='Mrs')\nax = sns.kdeplot(train[train['Title']=='Miss']['Age'], shade=True, label='Miss')\nax = sns.kdeplot(train[train['Title']=='Master']['Age'], shade=True, label='Master')\nax = sns.kdeplot(train[train['Title']=='Officer']['Age'], shade=True, label='Officer')\nax.set_xlim(-10, 90)\nax.set_xlabel('Age')\nax.set_title('Distribution of Age of based on Title')\nplt.show()","ea6652a1":"plt.figure(figsize=(10, 6))\nsns.countplot(x=\"Embarked\", hue=\"Title\", data=train)\nplt.show()","cfd0f3c7":"# Mean of of age wrt Title\n\ntr = train[['Age', 'Title']]\nts = test[['Age', 'Title']]\ntr_ts = pd.concat([tr, ts])\n\nprint(tr_ts.groupby('Title').mean())","d0a9d5c4":"# Filling missing age with respect Title mean is better idea than just with mean of whole age column\n\nt_age = {\n    'Master':5.5,\n    'Miss':21.8,\n    'Mr':32.3,\n    'Mrs':36.9,\n    'Officer':46.3,\n    'Royality':41.2\n}\n\ntrain['Age'] = train['Age'].fillna(train['Title'].map(t_age))\ntest['Age'] = test['Age'].fillna(test['Title'].map(t_age))","c3f3fda5":"new = train['Age']\nprint(new.describe())\n\n# scale, Normalizer, PowerTransformer, MaxAbsScaler\n\nprint('\\n'+'MinMaxScaler'+'-'*40+'\\n')\nmm = MinMaxScaler()\nsc = pd.DataFrame(mm.fit_transform(new.values.reshape(-1,1)))\nprint(sc.mean())\nprint(sc.describe())\n\nprint('\\n'+'StandardScaler'+'-'*40+'\\n')\nmm = StandardScaler()\nsc = pd.DataFrame(mm.fit_transform(new.values.reshape(-1,1)))\nprint(sc.mean())\nprint(sc.describe())\n\nprint('\\n'+'scale'+'-'*40+'\\n')\nmm = scale(X = new.values.reshape(-1,1))\nsc = pd.DataFrame(mm)\nprint(sc.mean())\nprint(sc.describe())\n\nprint('\\n'+'MinMaxScaler'+'-'*40+'\\n')\nmm = MinMaxScaler()\nsc = pd.DataFrame(mm.fit_transform(new.values.reshape(-1,1)))\nprint(sc.mean())\nprint(sc.describe())\n\nprint('\\n'+'MaxAbsScaler'+'-'*40+'\\n')\nmm = MaxAbsScaler()\nsc = pd.DataFrame(mm.fit_transform(new.values.reshape(-1,1)))\nprint(sc.mean())\nprint(sc.describe())","49aaa5dc":"# Age and Fare vs Survival\n\nsns.set_palette(survival_pal)\n    \nfig, ax = plt.subplots(1, 2, figsize=(20, 5))\nfor ind, val in enumerate(['Age', 'Fare']):\n    ax[ind] = sns.kdeplot(train[train['Survived']==0][val], shade=True, ax=ax[ind])#.set_xlabel(val).set_label('Not Survived')\n    ax[ind] = sns.kdeplot(train[train['Survived']==1][val], shade=True, ax=ax[ind])#.set_xlabel(val).set_label('Survived')\n    ax[ind].set_xlabel(val)\n    ax[ind].legend(['Not Survived', 'Survived'])","6fcb7995":"# plt.figure(figsize=(60,5))\n# ax = sns.countplot(x='Age', hue='Survived', data=train)\n# plt.legend(['Not Survived', 'Survived'])\n# plt.show()\n\n# plt.figure(figsize=(200, 5))\n# ax = sns.countplot(x='Fare', hue='Survived', data=train)\n# plt.legend(['Not Survived', 'Survived'])\n# plt.show()","3ac8734e":"# Binning Age and Fare\n\ntrain['age_cat'] = pd.cut(train['Age'], \n                          bins = [0, 0.99, 7, 23, 58, 100],\n                          labels = [\"infant\", \"child\", \"young\", \"adult\", \"senior\"],\n                          include_lowest=True)\ntest['age_cat'] = pd.cut(test['Age'], \n                         bins = [0, 0.99, 7, 23, 58, 100],\n                         labels = [\"infant\", \"child\", \"young\", \"adult\", \"senior\"],\n                         include_lowest=True)\n\ntrain['fare_cat'] = pd.cut(train['Fare'], \n                           bins = [0, 12, 40, 80, 1000],\n                           labels = ['least', 'low', 'mid', 'high'],\n                           include_lowest=True)\ntest['fare_cat'] = pd.cut(test['Fare'], \n                           bins = [0, 12, 40, 80, 1000],\n                           labels = ['least', 'low', 'mid', 'high'],\n                           include_lowest=True)","ce78950c":"# Extracting Cabin Type from Cabin name\n\nc_train_type = train['Cabin'].str[0]\ntrain['c_type'] = c_train_type\ntrain['c_type'] = train['c_type'].fillna('unknown')\n\nc_test_type = test['Cabin'].str[0]\ntest['c_type'] = c_test_type\ntest['c_type'] = test['c_type'].fillna('unknown')","257b2b38":"# Age, Fare, Cabin category vs Survival\n\nfig, ax = plt.subplots(1, 3, figsize=(24, 5))\nfor ind, val in enumerate(['age_cat', 'fare_cat', 'c_type']):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])","52a76af9":"# Family member count and Family Size and is alone\n\ntrain['fam_count'] = train['SibSp']+train['Parch']\ntest['fam_count'] = test['SibSp']+test['Parch']\n\nsize = {\n    0:'alone',\n    1:'small',\n    2:'small',\n    3:'small',\n    4:'large',\n    5:'large',\n    6:'large',\n    7:'large',\n    10:'large'\n}\n\ntrain['fam_size'] = train['fam_count'].map(size)\ntest['fam_size'] = test['fam_count'].map(size)\n\ntrain['is_alone'] = train['fam_size']=='alone'\ntest['is_alone'] = test['fam_size']=='alone'","060ddb64":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\nfor ind, val in enumerate(['fam_count', 'fam_size', 'is_alone']):\n    sns.countplot(x=val, hue='Survived', data=train, ax=ax[ind])","16b2d328":"# dataframe\n\ntrain.head()","2971f888":"# Scaling Age and Fare\n\nmm = MinMaxScaler()\nfor i in ['Age', 'Fare']:\n    train[i] =  mm.fit_transform(train[i].values.reshape(-1,1))\n    test[i] =  mm.fit_transform(test[i].values.reshape(-1,1))","465ded8b":"# Label Binerizer Sex\n\nlb = LabelBinarizer()\nfor i in ['Sex', 'is_alone']:\n    train[i] =  lb.fit_transform(train[i])\n    test[i] =  lb.fit_transform(test[i])","91764a1c":"# Label Encoding Pclass\n\nen = LabelEncoder()\ntrain['Pclass'] =  en.fit_transform(train['Pclass'])\ntest['Pclass'] =  en.fit_transform(test['Pclass'])","57250a8b":"# Create dummies for nominal categorical columns\n\ndef create_dummies(df, column_name):\n    dummies = pd.get_dummies(df[column_name], prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    df.drop(column_name, axis=1, inplace=True)\n    return df\n\n# for i in ['Sex', 'SibSp', 'Parch', 'Embarked', 'Title', 'age_cat', 'fare_cat', 'c_type', 'fam_count', 'fam_size']:\n#     train = create_dummies(train, i)\n#     test = create_dummies(test, i)","a9ff13b2":"# Droping columns\n\ntrain.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","c22ce626":"# dataframe\n\ntrain.head()","9c87a9be":"# Final correlation heatmap\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(train.drop('PassengerId', axis=1).corr(), annot=True, fmt='.1f', cmap='RdBu', vmax=0.8, vmin=-0.8)\nplt.show()","c26e24f1":"# X = train.drop(['Survived', 'PassengerId'], axis=1)\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'fam_count', 'is_alone']\nX = train[features]\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","2c9aecf3":"# naive bayes\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred = nb.predict(X_test)\n\nprint(accuracy_score(y_pred, y_test))\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\nperm = PermutationImportance(nb, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","ec517db9":"# Logistic regression\n\nlr = LogisticRegression(C = 1, penalty= 'l2', solver= 'liblinear')\n\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint(accuracy_score(y_pred, y_test))\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\nperm = PermutationImportance(lr, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","7cdd2b6f":"# svm\n\nmodel = SVC()\n\nhyperparameters = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [1, 0.1, 0.01],\n    'kernel': ['rbf', 'linear']\n}\n\ngrid = GridSearchCV(model, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nsvc = grid.best_estimator_\ny_pred = svc.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_estimator_)\nprint(grid.best_score_)\n\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\nperm = PermutationImportance(svc, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","824997d5":"# K Nearest Neighbours\n\nmodel = KNeighborsClassifier()\n\nhyperparameters = {\n    \"n_neighbors\" : range(1,20,2),\n    'weights' : ['uniform', 'distance'],\n    'p' : [1, 2]\n}\n\ngrid = GridSearchCV(model, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nknn = grid.best_estimator_\ny_pred = knn.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_score_)\n\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\n\nperm = PermutationImportance(knn, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","c791bbc1":"# Decision Tree\n\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nhyperparameters = {\"criterion\": [\"entropy\", \"gini\"],\n                   \"max_depth\": [3, 5, 7, 10],\n                   \"max_features\": [\"log2\", \"sqrt\", 'auto'], \n                   'min_samples_leaf' : [2, 3, 4, 5],\n                   'min_samples_split' : [2, 3, 4, 5]\n}\n\ngrid = GridSearchCV(model, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\ndt = grid.best_estimator_\ny_pred = dt.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_score_)\n\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\nperm = PermutationImportance(dt, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","babc7443":"# Random Forest\n\nmodel = RandomForestClassifier()\n\nhyperparameters = {\"criterion\": [\"entropy\", \"gini\"],\n                   \"max_depth\": [5, 10],\n                   \"max_features\": [\"log2\", \"sqrt\"],\n                   'min_samples_leaf' : [2, 3, 4, 5],\n                   'min_samples_split' : [2, 3, 4, 5],\n                   \"n_estimators\": [6, 9]\n}\n\ngrid = GridSearchCV(model, param_grid=hyperparameters, cv=10)\ngrid.fit(X, y)\n\nbest_params = grid.best_params_\nbest_score = grid.best_score_\n\nrf = grid.best_estimator_\ny_pred = rf.predict(X_test)\n\nprint(grid.best_params_)\nprint(grid.best_score_)\n\n#print(classification_report(y_pred, y_test))\nplt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_pred, y_test), annot=True, cbar=False, fmt='1d', cmap='Blues')\n\nperm = PermutationImportance(rf, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","6416d280":"# test.head()","c348169b":"# test.isnull().sum()","1d6819ad":"holdout_ids = test[\"PassengerId\"]\nholdout_features = test[features]\nholdout_predictions = lr.predict(holdout_features)\n\nsubmission = pd.DataFrame({\"PassengerId\": holdout_ids, \n                           \"Survived\": holdout_predictions})\nprint(submission.head())\n\nsubmission.to_csv(\"submission.csv\",index=False)","ababdd34":"# Preprocessing","1fd4576d":"### Missing values","0ea8df41":"# Holdout Prediction","cb62da16":"# EDA","e424a2e4":"# kaggle titanic python solution and tutorial\n# Importing Libraries","d603d984":"# Getting Training and Testing Datasets","73f17993":"# Visual Exploration","a2fa5b10":"# Features \/ Columns","a868a3f8":"### Datatypes","98dfa6e5":"# Preprocessing","45d95cd6":"# Train - Test data splitting","60a7ac99":"# Models"}}