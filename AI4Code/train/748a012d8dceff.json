{"cell_type":{"f9f8aba3":"code","96be3dc2":"code","13349b8d":"code","542819ef":"markdown","119935a8":"markdown","165f8a28":"markdown","b13956d5":"markdown","7a7f6fe5":"markdown","b9737bf7":"markdown","bb6fe3b1":"markdown","d0381245":"markdown","7a95640d":"markdown","5d056b10":"markdown","1f8b56a5":"markdown","c79f02fb":"markdown","b75fb090":"markdown","383409c1":"markdown","730691e2":"markdown"},"source":{"f9f8aba3":"class MY_Neural_Network:\n    def __init__(self,layers,x,y,learning_rate=0.001):\n        \n        self.w1 = np.random.rand(layers[1],layers[0])\n        self.b1 = np.random.rand(layers[1],1)\n        \n        self.w2 = np.random.rand(layers[2],layers[1])\n        self.b2 = np.random.rand(layers[2],1)\n        \n        self.w3 = np.random.rand(layers[3],layers[2])\n        self.b3 = np.random.rand(layers[3],1)\n        \n        self.input = x.T\n        self.y = y\n        \n        self.output = np.zeros(y.shape)\n        \n        self.alpha = learning_rate","96be3dc2":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))\n\ndef relu(x):\n    return np.maximum(0,x)\n    \nclass MY_Neural_Network:\n    def __init__(self,layers,x,y,learning_rate=0.001):\n        \n        self.w1 = np.random.rand(layers[1],layers[0])\n        self.b1 = np.random.rand(layers[1],1)\n        \n        self.w2 = np.random.rand(layers[2],layers[1])\n        self.b2 = np.random.rand(layers[2],1)\n        \n        self.w3 = np.random.rand(layers[3],layers[2])\n        self.b3 = np.random.rand(layers[3],1)\n        \n        self.input = x.T\n        self.y = y\n        \n        self.output = np.zeros(y.shape)\n        \n        self.alpha = learning_rate\n        \n    \n    def forwardpropogation(self):\n        \n        self.z1 = np.dot(self.w1,self.input)+self.b1\n        self.a1 = relu(self.z1)\n        \n        self.z2 = np.dot(self.w2,self.a1)+self.b2\n        self.a2 =  relu(self.z2)\n        \n        self.z3 = np.dot(self.w3,self.a2)+self.b3\n        self.output = sigmoid(self.z3)","13349b8d":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))\n    \ndef derivative_sigmoid(x):\n    return x*(1-x)\n    \ndef relu(x):\n    return np.maximum(0,x)\n    \ndef derivative_relu(x):\n    for i in range(x.shape[0]):\n        for j in range(x.shape[1]):\n            if x[i][j] >0:\n                x[i][j]=1\n            else:\n                x[i][j]=0\n    return x\n\nclass MY_Neural_Network:\n    def __init__(self,layers,x,y,learning_rate=0.001):\n        \n        self.w1 = np.random.rand(layers[1],layers[0])\n        self.b1 = np.random.rand(layers[1],1)\n        \n        self.w2 = np.random.rand(layers[2],layers[1])\n        self.b2 = np.random.rand(layers[2],1)\n        \n        self.w3 = np.random.rand(layers[3],layers[2])\n        self.b3 = np.random.rand(layers[3],1)\n        \n        self.input = x.T\n        self.y = y\n        \n        self.output = np.zeros(y.shape)\n        \n        self.alpha = learning_rate\n        \n    \n    def forwardpropogation(self):\n        \n        self.z1 = np.dot(self.w1,self.input)+self.b1\n        self.a1 = relu(self.z1)\n        \n        self.z2 = np.dot(self.w2,self.a1)+self.b2\n        self.a2 = relu(self.z2)\n        \n        self.z3 = np.dot(self.w3,self.a2)+self.b3\n        self.output = sigmoid(self.z3)\n    \n    def backwardpropogation(self):\n\n        self.dz3 = self.output-self.y\n        self.dw3 = 1\/self.y.shape[1]*np.dot(self.dz3,self.a2.T)\n        self.db3 = 1\/self.y.shape[1]*np.sum(self.dz3,axis=1,keepdims=True)\n        \n        self.dz2 = np.dot(self.dw3.T,self.dz3)*derivative_relu(self.z2)\n        self.dw2 = 1\/self.y.shape[1]*np.dot(self.dz2,self.a1.T)\n        self.db2 = 1\/self.y.shape[1]*np.sum(self.dz2,axis=1,keepdims=True)\n        \n        self.dz1 = np.dot(self.dw2.T,self.dz2)*derivative_relu(self.z1)\n        self.dw1 = 1\/self.y.shape[1]*np.dot(self.dz1,self.input.T)\n        self.db1 = 1\/self.y.shape[1]*np.sum(self.dz1,axis=1,keepdims=True)\n        \n        self.w1=self.w1-self.alpha*self.dw1\n        self.b1=self.b1-self.alpha*self.db1\n        \n        self.w2=self.w2-self.alpha*self.dw2\n        self.b2=self.b2-self.alpha*self.db2\n        \n        self.w3=self.w3-self.alpha*self.dw3\n        self.b3=self.b3-self.alpha*self.db3\n        \n        return self.w1,self.b1,self.w2,self.b2,self.w3,self.b3\n        \n    def cost(self):\n        cost=-1\/self.y.shape[1]*np.sum((self.y*np.log(self.output)+(1-self.y)*np.log(1-self.output)))\n        return cost","542819ef":"# Forward propagation\n\nEach layer\u2019s input is modified using weights and bias of the respective layer. The result is then fed into the activation function and is forwarded to the next layer.\n\n![1_YHhh5J5AZWfBG9WTfTJDXg.png](attachment:1_YHhh5J5AZWfBG9WTfTJDXg.png)\n\nFor all the hidden layers, we\u2019ll be using the ReLU activation function, and for the output layer, we\u2019ll be using the sigmoid activation function since it\u2019s a binary classification. We can use **softmax regression** in cases of multi-class classification.\n\n**Note:** *tanH *and *LeakyReLU* are the other activation functions that can be used.\n\n**Note:** We perform dot product on the weights of the layer and inputs to the layer. So, we always have to keep track of the matrix dimensions. Transpose the inputs wherever needed.","119935a8":"# Loss function\n\nWith the randomly initialized weights, we\u2019ll be predicting the output and then calculating error in the predictions. This error\/deviation of predicted values from true values is called cost\/loss of neural network.\n\nBelow is the loss function **(binary cross-entropy)** of the neural network.\n\n![1_mBs5vzfPHFBRenrac-iVLQ.jpeg](attachment:1_mBs5vzfPHFBRenrac-iVLQ.jpeg)\n\nThere are many loss functions available. Since we\u2019re dealing with classification, binary cross-entropy is used for binary classification and sparse categorical cross-entropy is used for multi-class classification.","165f8a28":"## What Is a Neural Network?\n\nA neural network is a collection of layers of neurons that takes data as input, trains itself to recognize patterns in the data, and predicts output for similar kinds of data.\nArtificial neural networks are loosely modeled on biological neural networks.\nIn layman\u2019s terms, as a tiny amount of information passes from one neuron to another in biological neural networks, information passes between neurons of different layers in artificial neural networks.","b13956d5":"![1_GN76DsIK-0P1XtqwrN06OQ.jpeg](attachment:1_GN76DsIK-0P1XtqwrN06OQ.jpeg)","7a7f6fe5":"# How to Know the Global Minimum?\n \nRun the forward propagation, backward propagation functions for some iterations, and calculate the cost for each iteration. The iteration in which the cost is low is the global minimum.","b9737bf7":"At the global minimum, we\u2019ll be having the best set of weights and bias and the cost will also be minimum.\n\nOur goal is to find out the best set of weights and bias that can be used for further predictions.","bb6fe3b1":"**Note:** The rate at which the cost reaches the global minimum is determined by the learning rate(alpha). If the learning rate is high, then step size will be high and the cost may never reach the global minimum. If the learning rate is low, it takes a lot of time to reach the global minimum. So, the learning rate should be optimum.","d0381245":"# Gradient descent\n\nWe have to keep updating the weights and bias of the neural network until the cost of the neural network is minimum. Technically, until the cost reaches a global minimum.\n![1_zhMRJeNe8zmgKsJHHcuVjA.png](attachment:1_zhMRJeNe8zmgKsJHHcuVjA.png)","7a95640d":"# Let\u2019s Build Our Neural Network\nLet\u2019s consider a three-layered neural network(two hidden layers and one output layer).\n\n![1_3fA77_mLNiJTSgZFhYnU0Q.png](attachment:1_3fA77_mLNiJTSgZFhYnU0Q.png)\n\nWe have to create a class for our neural network and initialize the weights and bias randomly for all three layers.","5d056b10":"**Every artificial neural network has the following components:**\n\n**Input layer:** In structured data, the number of nodes in the input layer will be equal to the number of features. In unstructured data, such as images, the number of nodes will be equal to the size of the image.\n\n**Weights and bias:** The inputs to each neuron\/node are modified using weights and are summed with bias.\n\n**Hidden layers:** The size of the hidden layers (number of hidden layers and the number of neurons in each hidden layer) will be arbitrary depending on the type of data we\u2019re dealing with. Neurons in the hidden layers are the core processing units of the neural network. Each neuron in the hidden layer holds information about a specific sub-component of the data. For example, in an image recognition task, each neuron in the network will have some information about the edges in the image.\n\n**Output layer:** The number of nodes in the output layer depends on the number of classes in the label column.\n\n**Activation functions:** These are the mathematical functions that are applied to every neuron in the hidden layers and the output layer. There are many activation functions available, of which we\u2019ll be using *sigmoid* and *ReLU*.","1f8b56a5":"![1_nAUzoWNPMkIYUeDlDRfdGw.png](attachment:1_nAUzoWNPMkIYUeDlDRfdGw.png)","c79f02fb":"# Backward propagation\n\nWe\u2019ll now propagate backward to find out the slope of the cost function to update our weights and bias.\n\nWe can\u2019t calculate the slope of cost function directly w.r.t weights and bias. We need to proceed sequentially as shown in the below image.\n\n**Note:** In the below derivation I\u2019ve used the sigmoid activation function. You can change the activation function.","b75fb090":"\nTo know about more topics refer my \n\nmedium blog - https:\/\/medium.com\/@srujan.krish97\n \nkaggle - https:\/\/www.kaggle.com\/krishnasrujan\/account\n\nThank You for upvoting.\n    ","383409c1":"The below figure explains it.\n\n![1_Q52v-kT0Ner-A5r4FChEUw.png](attachment:1_Q52v-kT0Ner-A5r4FChEUw.png)","730691e2":"![1_yRz_mxKkVInaxwxByUmqwA.jpeg](attachment:1_yRz_mxKkVInaxwxByUmqwA.jpeg)"}}