{"cell_type":{"b88436bb":"code","06428d81":"code","726b1903":"code","a9bfd0e4":"code","de29d01d":"code","e0c40b9b":"code","fe4a996e":"code","192edfd5":"code","5cd8c024":"code","d80ff33e":"code","ec078bfd":"code","4c3fca25":"code","c169fdc4":"code","8e81bd4c":"code","863a319e":"code","610a4e95":"code","6a640d60":"code","5d249e60":"code","9ba2de22":"markdown","75a7e6a6":"markdown","90390237":"markdown","16008ff4":"markdown","468e30b8":"markdown","c160a40d":"markdown","4eefbace":"markdown","ec9742a2":"markdown","c0f3868e":"markdown","0b2cc5ae":"markdown","c4f4045c":"markdown","0d2f09b3":"markdown","8bf882f7":"markdown","c54d5194":"markdown"},"source":{"b88436bb":"import numpy as np\nimport pandas as pd","06428d81":"pd.set_option('display.max_colwidth', None)\n\ntrain = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntrain['titleUcase'] = train['title'].str.upper()","726b1903":"import spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nfor item in train['title'].head(11):\n    print('\\n',item, '\\n', [(x.text, x.label_) for x in nlp(item).ents if len(x.text)>0])","a9bfd0e4":"!python -m spacy download xx\n\nimport xx_ent_wiki_sm\nnlp = spacy.load('xx')","de29d01d":"for item in train['title'].head(11):\n    print('\\n',item, '\\n', [(x.text, x.label_) for x in nlp(item).ents if len(x.text)>0])","e0c40b9b":"unit = ['GR', 'GM', 'KG', 'KILO', 'MG', 'LITRE', 'ML', 'PC', 'INCH', 'YARD', 'CM', 'MM', 'METRE', 'MICRO', 'GB', 'MB', 'TB', 'KB', 'THN']","fe4a996e":"import re\n\ndef findunit(row):\n    numunit=0\n    for item in unit:\n        quantity = re.findall('[0-9.-]+' + '\\s*'+ item, row.titleUcase)\n\n        if len(quantity) > 0:\n            q1 = quantity[0].replace(item,\"\")\n            ##if more than one same unit, consider just 1 more instance\n            ##Also ensure no more than 4 entries against all units combined\n            if ((len(quantity) > 1) and (numunit < 3)):\n                q2 = quantity[1].replace(item,\"\")   \n                train.loc[train.posting_id == row.posting_id,['quantity_'+str(numunit),'unit_'+str(numunit),'quantity_'+str(numunit+1),'unit_'+str(numunit+1)]]=q1,item,q2,item\n                numunit+=2\n            else:\n                train.loc[train.posting_id == row.posting_id, ['quantity_' + str(numunit), 'unit_' + str(numunit)]] = q1, item   \n                numunit+=1\n\n        ##Return if 4 cols across all units is filled\n        if numunit >= 4:\n            return\n\n##Not efficient but for 30K rows we should be ok. Takes about 2 mins\ntrain.apply(findunit, axis = 1)\n\nprint('Number of records with units:',train['unit_0'].notna().sum())","192edfd5":"cols=['quantity_0','unit_0','quantity_1','unit_1','quantity_2','unit_2','quantity_3','unit_3']\ntrain['quantam'] = train[cols].fillna('').agg(' '.join, axis=1)\n\n##7 spaces means there is no unit. Replace string for readability\ntrain['quantam'] = train['quantam'].replace('       ' , 'NO UNIT' )","5cd8c024":"print('Sample rows without unit:\\n************************')\nfor row in train.head(20).itertuples():\n    if row.quantam == 'NO UNIT':\n        print(row.title)","d80ff33e":"print('Sample rows with unit:\\n*********************')\nfor row in train.head(10).itertuples():\n    if row.quantam != 'NO UNIT':\n        print(row.title, '\\n',row.quantam, '\\n')","ec078bfd":"##i\/p = 1)base row 2)all units in that row 3)row to be matched against\n##o\/p = True if rows are matched else False\ndef compare(row1, row1units, row2):\n    if str(row1.quantam) == str(row2.quantam):\n        return True\n    else:\n        ##get all units in row2\n        row2units=[]\n        for i in range(4):\n            if ~pd.isna(row2['unit_'+str(i)].values):\n                row2units.append(str(row2['unit_'+str(i)].values))\n                \n        ##Proceed only if row1 & row2 dont have duplicate units\n        if (len(set(row1units)) == len(row1units)) and (len(set(row2units)) == len(row2units)):\n            ##No duplicate units. Iterate thru each unit of row 1\n            ##if a match is found in row2, then quantities should be same\n            for unit in row1units:\n                for i in range(4):\n                    if str(row2['unit_'+ str(i)].values) == unit:\n                        ##units match. Get quantity from row1 (should have inputted a map)\n                        for j in range(4):\n                            if (str(row1['unit_'+ str(j)].values)) == unit:\n                                row1quantity = row1['quantity_'+ str(j)].values[0]\n                                row2quantity = row2['quantity_'+ str(i)].values[0]\n                                ##add all cleanup and other processing here\n                                row1quantity.replace(' ' , '')   \n                                row2quantity.replace(' ' , '')\n                                if (len(re.findall('[0-9]', row1quantity)) == 0) | (len(re.findall('[0-9]', row2quantity))==0):\n                                    continue\n                                ##if quantity contains '-' do a string comparision else numeric\n                                if ('-' in row1quantity) or ('-' in row2quantity):\n                                    if row2quantity != row1quantity:\n                                        return False\n                                elif float(row2quantity) != float(row1quantity):\n                                    return False\n            ##no match in any unit. We can't say that quantities differ\n            return True\n        else:\n            ##Duplicate units. Maybe this is a range. For now return True\n            return True\n        \n\n##i\/p = list of posting_ids with first being the ref ID\n##o\/p = updated list after removal of IDs that done belong\ndef unmatch(ids):\n    ##First ID is the reference row\n    row1 = train.loc[train.posting_id == ids[0]]\n    removeids = []\n    if row1.quantam.values != 'NO UNIT':\n        ##get all units in row1\n        row1units=[]\n        for i in range(4):\n            if ~pd.isna(row1['unit_'+str(i)].values):\n                row1units.append(str(row1['unit_'+str(i)].values))\n        \n        ##compare row1 with all the other rows\n        for i in range(1,len(ids)):\n            row2 = train.loc[train.posting_id == ids[i]]\n            if not(compare(row1,row1units, row2)):\n                removeids.append(i)\n    return [v for i, v in enumerate(ids) if i not in removeids]","4c3fca25":"out = pd.read_csv('..\/input\/chris-rapids\/submission_Chris.csv')\nout.head(4)","c169fdc4":"out['ids'] = out.posting_id + ' ' + out.matches\nout.head(4)","8e81bd4c":"def removedup(inp):\n    temp = list()\n    op = [x for x in inp if not (x in temp or temp.append(x))]\n    return op\n    \nout['postprocess'] = out.apply(lambda x: unmatch(removedup(x.ids.split(' '))), axis=1)","863a319e":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score\n\ntmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain['target'] = train.label_group.map(tmp)\ntrain['matches'] = out['matches']\ntrain['newmatches'] = out['postprocess']\ntrain['f1'] = train.apply(getMetric('newmatches'),axis=1)\n\nprint('New CV Score =', train.f1.mean() )","610a4e95":"train['differs'] = train.apply(lambda x: list(set(x.matches.split(' ')).difference(x['newmatches'])), axis=1)","6a640d60":"differences = []\ntrain.apply(lambda x: [differences.append(x.posting_id) if len(x.differs)>0 else None], axis=1)\nlen(differences)","5d249e60":"cols_needed = ['quantam', 'matches', 'newmatches', 'differs']\nprint('First row: Base row Title and label group which has a wrong prediction against it')\nprint('Second row: This is mis-classified (mis-clustered?) row paired to base row above')\nprint('Label groups should be different for above 2 rows. If they are same, we investigate')\nprint('***********************************************************************************\\n')\n\nyay = 0   ##num instances where our code is correct. \ngosh = 0  ##num instances where our code is incorrect. Pl dont use such names in prod systems :)\n\nfor item in differences[:9]:\n    diff_ids = train[(train.posting_id==item)]['differs'].to_numpy(copy=True)\n    for ids in diff_ids[0]:\n        row1 = train.loc[train.posting_id==item]\n        row2 = train.loc[train.posting_id==ids]\n        if row1.label_group.values[0] != row2.label_group.values[0]:\n            yay+=1\n        else:\n            gosh+=1\n        print(row1[['title', 'label_group']].values, '\\n', row2[['title', 'label_group']].values, '\\n')\n        \nprint('yay=',yay,'gosh=',gosh)","9ba2de22":"Unfortunately looks like set() does not maintain order. We want our first item to be the reference posting_id. We create a small function removedup() which does that - For each x, we add x to output if x not in the list 'temp'. Each time we also append 'x' to 'temp', so if a duplicate turns up it wont get added. We rely on append always returning None unconditionally. So it is an ugly hack but suffices for our quick purpose investigation here.\n\nWe feed this to our unmatch() function. It takes a couple of mins to run","75a7e6a6":"Hmm. That is a fair bit of difference. 1000 rows with potentially more than 1 diff in each row. This is a sizeable chunk. Let us investigate. We compare all rows that are returned as a un-matched by our code above. Let us print each pair of titles side by side along with the labels. Ideally these should have different labelgroups. This should increases our score. If they have the same score our CV decreases and we should investigate why there un-matched.","90390237":"It often helps to think from a contrarian point of view. This could open up unexpected avenues of pursuit. So in the current competition, while the focus is on **MATCHING** products, we can try taking a step back to see conditions when products can be **UN-MATCHED**. \n\n* The first criteria for \u2018un-matching\u2019 could be the **PRICE**. If the price is not within an acceptable range, we probably know that the products are different. We can\u2019t have a Rp@6999 product compare with a Rp@999 product though the rest of the text title and the images may be quite similar. A quick check at the training data shows that the price info seems to have been scrubbed for most of the records barring very few.\n\n* The second could be the **ORG NAME**. We just can\u2019t have a Sony 32 inch TV HD color hi pixel whatsoever match with a Samsung 32 inch TV HD color hi pixel whatsoever. This could be a criteria for un-matching in current dataset. However a bulk of the items listed are small ones and don\u2019t have a tangible org name associated. So success could be limited. Nonetheless this could work to some extent.\n\n* We could have a whole range of \u2018**CONTRASTS**\u2019 lookup tables. We could have sex as one of the entry. An xxx perfume strong cologne fragrance for males could be different than an exactly similar (text, image) product but one designed for females. We could have color contrasts. White Apple iPhone 10 256 GB versus Black Apple iPhone 10 256 GB.  There could be several such interesting rows of contrasting criteria. A look-up table could hold these contrasting pairs and then we could ensure that they are not present individually in the two product titles being compared\n\n* Then based on the product **CATEGORY** (yes we may have to do some topic labelling as train data does not have this) we could have some strong category specific differentiators. For e.g. for edibles, we could have \u2018organic\u2019 versus \u2019natural\u2019 versus \u2018non-organic\u2019 or just the absence of the word \u2018organic\u2019. So 100% grape juice Tropicana sweet real grapes should be different from 100% organic grape juice Tropicana sweet real grapes. Last I checked the train data had about 250 organic products, so this could help to some extent. Based on the product category, other interesting contrasts can be identified and used to un-match products.\n\n* We could also have **APHANUMERIC CODES**. For e.g. org1 LR220X-5A lighting lamp manita may not be equal to org1 LR220Y-5A lighting lamp manita. Or to cover a more simpler case anti-wrinkle anti-uv cream SPF-250 is different from anti-wrinkle anti-uv cream SPF-50.\n\n* Lastly we have the **QUANTITY**. This could be a very important indicator for un-matching. For sure - \u201cL\u2019Oreal Paris Total Repair 5 Shampoo Hair Care - 450 ml (Perawatan Rambut Rusak dan Mudah Patah)\u201d is different from \u201c\\L\u2019Oreal Paris Total Repair 5 Shampoo Hair Care - 900 ml (Perawatan Rambut Rusak dan Mudah Patah)\u201d even though the rest of the text and the entire image may nearly be the same. \n\nLet us see if we can leverage existing frameworks to quickly give out this kind of info","16008ff4":"The 'matches' look to be sorted. Let us merge the 2 cols. This way the 1st ID in 'matches' is the reference ID. We will use set() to remove the duplicate","468e30b8":"Let us use Spacy. Not sure if this can be called SOTA","c160a40d":"Time to check if what we wrote works. Let us take the output of Chris's starter kit where he uses Rapids to categorize labels using an unsupervised approach. It is a simple effective notebook and acheieves a good score of 70+. Chris has also introduced the Rapids FW here and it is blazingly fast. Thanks Chris\/Nvidia for this nice FW. Do check it out in case you have not already done so.","4eefbace":"Interesting. For the first 10 mismatches (out of 1008) We have 22 yays and 4 gosh's. Let us analyze the 4 gosh'es to see what is happeninig.  \nIn case of label group: 2660605217, the text scraped by shopppe seems to have picked up some text from the next item - so it is a noise. Then we have 3 cases where the quantities are clearly different though the rest of the text is same. No way this can be same product. More noise. \n\nSo a 100% success of our un-match logic as far as these 10 records are concerned. If so our CV should have scored much much better. If we go all the way and do it for entire train data we get **1684 yays and 516 gosh's**. So there is a fair bit of correct post-processing, so need to investigate why CV is not going up further. Maybe this weekend I will do that. Also our code is pretty basic and we havent taken care of any edge conditions or looked at improving speed\/elegance etc. Defintiely, many things can be improved.\n\nThis idea (& a few others - in particular generating more training records from this limited data - esp for products with low samples) has been on the back of my mind for a while now but work constraints didnt allow me to put pen to paper (or pressure to the keyboard keys until now). I could see some discussions related to this as well. I believe it was Chris who referred to the importance of numerical values in one of the discussions couple of weeks back followed by another recent discussion by Sarthak. In fact un-matching is an entire post-processing piece by itself and covers lot more than prices or numericals (see my discussion link on this). The results seem promising with slight improvement in CV and ideally the test scores too should improve with PP. I dont have time to submit or participate in this comp but if anyone gets a better test score, please do share the results and your observations\n\nAlso this PP exposes the noisiness of the train data. There are pairs that should have not been matched under label group but have been matched. I hope the test data is much cleaner. \n\nHow else can un-matching be leveraged? Well this is a time-constrained comp so if we are comparing a row against every other row in the test DB, it could consume vast amounts of time. The \u2018un-matching\u2019 criteria could be used to generate clusters beyond which matching may not be required and this should reduce time taken which be used for other activities. \n\nAll said and done, even if it does not matter to the comp scores, this seems to me to be the right way to approach the business problem. This is one of those domains where the model needs to be confident on its predictions. If it is not very confident, it is ok not to predict (and leave it to be manually hand-labelled as similar or dissimilar). Special efforts need to be taken to eliminate the false positives and hence the un-matching logic could hold the key to actual applications of this tech. In fact it is interesting that we get a few rows in train data which are pretty obviously mis-grouped. The usage of un-matching could have prevented that easily. \n\nShoppe matching is an insanely interesting competition.  If there is one competition that has it all - image, text, OCR, mutiple approaches to tackle - it would be this. I am sure we will have an exciting finish. This is also a very relevant industry problem and I would like to thank the organisers and Kaggle staff for hosting this competition. The kernels by ragnar and chris and the discussion topics by rohitsingh & tensorgirl & knownothing and several others have been very useful and educative for me.","ec9742a2":"A bit disappointing, you would agree. Let us check if the indonesian version does any better. We will have to create a link first","c0f3868e":"Let us now check the new CV. We use below snippet from Chris's original code. The original CV score is 0.7248. The kernel is at: https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700","0b2cc5ae":"No big difference.\n\nLet us try to create a quick custom code to retreieve quantity (weight, volume, nums) from the title. Note that some rows could have more than one quantity.","c4f4045c":"We could still have 'G' or 'L' and much much more units. For a quick POC above should suffice","0d2f09b3":"A good 25% have units. This is sizeable\n\nLet us aggregate these units into one column for a quick comparision later","8bf882f7":"The new score is 0.7255... a modest improvement. Should we be disappointed? I guess not. For one we may see more success in the test data, secondly this feature could be benificial in other ways - e.g. if we are short on processor time, this can be used for quick checks to derive smaller clusters. More importantly when this AI is applied to the retail biz domain, there is simply no way that one cannot NOT use this imp feature. As discussed this is the **second most important signal in differentiating products (the first would be the price range)**. \n\nFor curiosity, let us check out the differences","c54d5194":"Looks ok. Now let us write a simple logic to un-match products based on the units. Let us say we are given a list containing the posting_id's"}}