{"cell_type":{"af9400f4":"code","30668ffa":"code","e106326c":"code","63347363":"code","35d3cc4e":"code","028bb426":"code","30527a67":"code","c25eb44c":"code","525f0e0d":"code","89ec69f4":"code","16838dd1":"code","c4754a28":"code","fea8e0f8":"code","cd6911ca":"code","f5f0a3fd":"code","71e19cfd":"code","3dd4c3a9":"code","8b59293f":"code","b87d16eb":"code","28361e2b":"code","fc18e3ce":"code","f3342e42":"code","c75aa2d0":"code","9d0c3fc0":"code","67143737":"code","58adc605":"code","ca8d1870":"code","fa16d7c5":"code","d83c50c8":"code","e6731388":"code","93335917":"code","f0a5b629":"code","8c76fa5b":"code","e2525b7f":"code","66301adf":"code","c8074cf1":"markdown","45c32fa9":"markdown","9b184298":"markdown","703f6df5":"markdown","12aa6a75":"markdown","221cb55d":"markdown","1183688e":"markdown","81f90f81":"markdown","8e097a9c":"markdown","ca561c1b":"markdown","d8e2e326":"markdown","35784294":"markdown","bf1c1717":"markdown","6522f8e8":"markdown","2941f682":"markdown","54fef3b1":"markdown","3731014e":"markdown","881c17b5":"markdown","09a482ca":"markdown","ffc393ab":"markdown"},"source":{"af9400f4":"# Importing basic libraries for EDA\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# Importing the os library to get the path for the data file (not used in the EDA).\nimport os\n\n# The following magic command lets matplotlib display images in the cells outputs.\n%matplotlib inline\n\n# Setting seaborn style\nsb.set(style=\"darkgrid\")","30668ffa":"# Getting the list of entries in the current directory\n#  The data file 'kc_house_data.csv' has to be in the same folder as this notebook\n#  for it to appear in the following list","e106326c":"os.listdir('..\/input\/housesalesprediction\/')","63347363":"# Reading the data .csv file\ndata = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","35d3cc4e":"# List of the columns names\ndata.columns","028bb426":"# Checking the head (top 5) rows of the dataframe and whowing all the columns\npd.set_option(\"display.max_columns\", len(data.columns))\ndata.head()","30527a67":"data.info()","c25eb44c":"# Helper function to print the values count of features\ndef feature_val_count(data, feature_name):\n    s = data[feature_name].value_counts()\n    return print(f\"The Value counts of the feature {feature_name}: \\n {s}\")","525f0e0d":"# Checking the values count of the features to determine their nature\nfeature_val_count(data, 'condition')","89ec69f4":"# Checking the basic statistics for each feature(column) [Count, Mean, Standard Deviation, Minimum, Quartiles, and Maximum]\ndata.describe()","16838dd1":"data.head()","c4754a28":"type(data.date[0])","fea8e0f8":"data.date = pd.to_datetime(data.date, infer_datetime_format=True)","cd6911ca":"data.head()","f5f0a3fd":"# Checking that the date type changed correctly\ntype(data.date[0])","71e19cfd":"# Creating a list of the years, and months extracted form the date feature.\nYears = list(pd.DatetimeIndex(data.date).year)\nMonths = list(pd.DatetimeIndex(data.date).month)","3dd4c3a9":"# Creating baplot for the sales bount by Year and by Month\n# Creating a boxplot for the monthly sales count distribution\n\nfig = plt.figure(figsize=(20,6))\ngrid = plt.GridSpec(2, 2, width_ratios=(1, 2), height_ratios=(1,5), hspace=0.2, wspace=0.2)\nLeft_ax = fig.add_subplot(grid[:, 0])\nRight_top = fig.add_subplot(grid[0, 1])\nRight_bot = fig.add_subplot(grid[1, 1], xticklabels=['Jan','Feb','Mar','May','Avr','Jun','Jul','Aou','Sep','Oct','Nov','Dec'])\n\nsb.countplot(x=Years, palette='mako', ax=Left_ax)\nLeft_ax.set_title('House sales count by Year', fontdict={'fontsize':15})\nsb.countplot(x=Months, palette='mako', ax=Right_bot)\nsb.boxplot(x=Months, ax=Right_top)\nRight_top.set_title('House sales count by Month', fontdict={'fontsize':15});","8b59293f":"# Sorting the data by date and extracting some basics statistics aout the price feature\n# Calculating the Upper and Lower whiskers of the boxplot\n\ndata_sorted = data.sort_values(by='date')\n\nmedian = np.median(data.price)\nupper_quartile = np.percentile(data.price, 75)\nlower_quartile = np.percentile(data.price, 25)\n\niqr = upper_quartile - lower_quartile\nupper_whisker = data.price[data.price<=upper_quartile+1.5*iqr].max()\nlower_whisker = data.price[data.price>=lower_quartile-1.5*iqr].min()","b87d16eb":"print('\\033[1m' + 'Price feature statistics:\\n')\n\ndisplay(data_sorted.price.describe())\nprint('')\nprint(f'Upper Whisker: {upper_whisker}')\nprint(f'Lower Whisker: {lower_whisker}')","28361e2b":"n_outliers = (data_sorted.price>upper_whisker).sum()\nper_outlizers = n_outliers\/len(data_sorted.price)*100\nprint(f'Number of outliers: {n_outliers}')\nprint(f'Percentage of outliers: {per_outlizers:.2f}%')","fc18e3ce":"# Plotting the price feature using 3 different types of plots to better visualize the distribution\n\nplt.figure(figsize=(20,8))\nsb.scatterplot(x=range(len(data_sorted.price)) ,y=data_sorted.price, alpha=0.4)\nplt.plot((0, len(data.price)), (lower_whisker, lower_whisker), 'm--',linewidth=3)\nplt.plot((0, len(data.price)), (upper_whisker, upper_whisker), 'r--',linewidth=3)\nplt.legend(['Lower Whisker', 'Upper Whisker', 'House Price'])\nplt.title('Scatter plot of the house price feature', fontdict={'fontsize':15})\n\nplt.figure(figsize=(20,8))\nplt.subplot(121)\nsb.histplot(data=data.price, bins=140)\nplt.title('Distribution of the house prices', fontdict={'fontsize':15})\n\nplt.subplot(122)\nsb.boxplot(x=data.price)\nplt.title('Boxplot of the house prices', fontdict={'fontsize':15});","f3342e42":"plt.figure(figsize=(20,6))\nplt.subplot(121)\nsb.countplot(x=data.bedrooms, palette='mako' )\nplt.title('Number of bedrooms distribution', fontdict={'fontsize':15})\nplt.subplot(122)\nsb.countplot(y=data.bathrooms, palette='mako' )\nplt.title('Number of bathrooms distribution', fontdict={'fontsize':15});","c75aa2d0":"sqft_des = pd.DataFrame(data=[data.sqft_living.describe(),data.sqft_lot.describe()])","9d0c3fc0":"pd.DataFrame((data.sqft_basement>0).value_counts()).transpose()","67143737":"plt.figure(figsize=(20,15))\nplt.subplot(321)\nsb.histplot(x=data.sqft_living, kde=True, bins= 110)\nsb.histplot(x=data.sqft_living15, kde=True, bins= 110, color='red')\nplt.legend(['sqft_living','sqft_living15'])\nplt.title('Living area distribution', fontdict={'fontsize':15})\nplt.subplot(322)\nax = sb.histplot(x=data.sqft_lot)\nax = sb.histplot(x=data.sqft_lot15, color='red')\nplt.legend(['sqft_living','sqft_living15'])\nplt.title('Lot area distribution', fontdict={'fontsize':15})\nax.set_xscale('log')\nplt.subplot(323)\nsb.boxplot(x=data.sqft_living)\nplt.subplot(324)\nax2 = sb.boxplot(x=data.sqft_lot)\nax2.set_xscale('log')\nplt.subplot(325)\nsb.histplot(x=data.sqft_above)\nplt.subplot(326)\nax3 = sb.histplot(x=data[data.sqft_basement>0]['sqft_basement'])\n#ax3.set_xscale('log')\nplt.tight_layout()\n\nbasement_bool = pd.DataFrame((data.sqft_basement>0).value_counts()).reset_index()\nplt.figure(figsize=(8,5))\nax = sb.barplot(y=basement_bool['sqft_basement'], x=basement_bool['index'], palette='mako')\nax.set(ylabel='Count', xlabel='Basement');","58adc605":"plt.figure(figsize=(20,20))\nplt.subplot(321)\nsb.countplot(x=data.floors, palette='mako')\nplt.title('Distribution of houses with respect to floor count', fontdict={'fontsize':15})\nplt.subplot(322)\nsb.countplot(x=data.waterfront, palette='mako')\nplt.title('Number of houses with\/without a water front', fontdict={'fontsize':15})\nplt.subplot(323)\nsb.countplot(x=data.view, palette='mako')\nplt.title('Distribution of the views count', fontdict={'fontsize':15})\nplt.subplot(324)\nsb.countplot(x=data.condition, palette='mako')\nplt.title('Houses condition distribution', fontdict={'fontsize':15});","ca8d1870":"plt.figure(figsize=(15,20))\nplt.subplot(121)\nsb.countplot(y=data.yr_built, palette='mako')\nplt.title('Distribution of yr_built feature', fontdict={'fontsize':15})\nplt.subplot(122)\nsb.countplot(y=data[data.yr_renovated>0]['yr_renovated'], palette='mako')\nplt.title('Distribution of yr_renovated feature for renovated houses', fontdict={'fontsize':15})\n\nplt.figure(figsize=(8,5))\nyr_renov_bool = pd.DataFrame((data.yr_renovated>0).value_counts()).reset_index()\nsb.barplot(y=yr_renov_bool['yr_renovated'], x=yr_renov_bool['index'], palette='mako')\nax.set(ylabel='Count', xlabel='Renovated');","fa16d7c5":"df = data[['long','lat']].copy()\ndf['loc']='USA'\ndf.rename(columns={'long':'lon'}, inplace=True)","d83c50c8":"fig = go.Figure(data=px.scatter_geo(\n        lon = df['lon'],\n        lat = df['lat'],\n        center={'lat':df['lat'].mean(), 'lon':df['lon'].mean()},\n        width=700,\n        height=600,\n        opacity=0.5\n        ))\n\nfig.update_layout(\n        title = 'Houses Location in USA-King County',\n        geo_scope='usa'\n    )\nfig.show()","e6731388":"sb.pairplot(data=data[['price','bedrooms','bathrooms','sqft_living','sqft_lot','grade','sqft_above','sqft_basement','yr_built']], palette='mako');","93335917":"plt.figure(figsize=(18,13))\nplt.title('Heatmap correlation of the most important features', fontsize=18)\nsb.heatmap(data=data.iloc[:,1:].corr(), annot=True);","f0a5b629":"plt.figure(figsize=(20,20))\nplt.suptitle('Relation between categorical variables and the target variable', y=0.91, fontsize=20)\nplt.subplot(421)\nsb.barplot(x=data.bedrooms, y=data.price, palette='mako')\nplt.subplot(422)\nsb.barplot(x=data.waterfront, y=data.price, palette='mako')\nplt.subplot(423)\nsb.barplot(x=data.grade, y=data.price, palette='mako')\nplt.subplot(424)\nsb.barplot(x=data.floors, y=data.price, palette='mako')\nplt.subplot(425)\nsb.barplot(x=data.condition, y=data.price, palette='mako')\nplt.subplot(426)\nsb.barplot(x=data.view, y=data.price, palette='mako')\nplt.subplot(414)\nsb.barplot(x=data.bathrooms, y=data.price, palette='mako')","8c76fa5b":"sb.lmplot(x='sqft_living', y='price', hue='waterfront', data=data, palette='mako', height=8, aspect=1.7);","e2525b7f":"sb.lmplot(x='sqft_lot', y='price', hue='waterfront', data=data, palette='mako', height=8, aspect=1.7, ci=0);","66301adf":"df = data[['long','lat','price','grade','condition','yr_built']].copy()\ndf.rename(columns={'long':'lon'}, inplace=True)\n\nfig = go.Figure(data=px.scatter_geo(\n        lon = df['lon'],\n        lat = df['lat'],\n        center={'lat':df['lat'].mean(), \n                'lon':df['lon'].mean()\n               },\n        size=df['price'],\n        color=df['grade'],\n        symbol=df['condition'],\n        animation_frame=df['yr_built'],\n        width=700,\n        height=600,\n        opacity=0.5,\n        color_continuous_scale=\"deep\"\n        ))\n\nfig.update_layout(\n        title = 'Houses prices map',\n        geo_scope='usa'\n    )\nfig.layout.legend.y = 1.05\nfig.layout.legend.x = 1.035\nfig.layout.coloraxis.colorbar.y = 0.25\nfig.show()","c8074cf1":"In the image below the house **condition** is coded in symbols and the **grade** variable in colors.","45c32fa9":"As shown in the image above, there are homes with three-quarters and half of a bathroom, and this means:\nA 1.5 bath would mean one full bathroom, and one half bathroom. A 0.5 bathroom is called a half bath. It doesn't mean half bath in terms of its size in square feet. A half bath offers a sink and a toilet but no shower or bathtub. This type of math notations for bathrooms are commonly used in USA and that's why it appears in this dataset.","9b184298":"**Remark:** \nNotice that the distribution of prices is extremeply right skewed, and that we have 1146 outlires out of 21613 entries.\nAlmost 94.7% of the house prices are below 1127500.","703f6df5":"Usually the 'id' variable is ignored because it has no meaning and it is only used to index each row with a unique identifier.","12aa6a75":"#### floors, waterfront, view, condition, and grade","221cb55d":"## Multivariate Analysis\n\nStarting with bivariate analysis, and since we have a target variable which is the house prices, then we can limit the bivariate analysis to the 'price' vs All the other significant features. But first, let's take a quick look on the pair scatter plot of the numer","1183688e":"### Feature Naure:\nFrom the type of the features and their values count, we can determine the nature of each feature:\n- **Qualitative:**\n  - **Nominal:** id, waterfront, zipcode\n  - **Ordinal:** date, view, condition\n- **Quantitative:**\n  - **Discrete:** bedrooms, sqft_living, sqft_lot, sqft_above, sqft_basement, yr_built, yr_renovated, sqft_living15, sqft_lot15\n  - **Continuous:** price, floors, lat, long,","81f90f81":"#### Sqft_living, sqft_lot, sqft_living15, sqft_lot15, sqft_above, and sqft_basement.\n\nsqft_living15 & sqft_lot15: Living room area and lot area in 2015, implying that there was some renovations.","8e097a9c":"#### Date","ca561c1b":"### Conclusion of univariate analysis\n\nMany of the categorical features on the dataset are heavely unbalanced like 'condition', 'view', 'waterfront', and 'floors', which may be the cause of the extreme skeweness of the distribution of hous prices and areas. These speculations can be further inspected by carrying out a multivariate analysis, which is the object of the following sections.","d8e2e326":"#### Bedrooms & Bathrooms","35784294":"**Remark:**\nNotice is that the date type is 'str', so we need to convert it to a timestamp variable, which is achieved using the pandas method .to_date_time()","bf1c1717":"In this notebook, I presented a simple and methodical way of performing an EDA for structured and clean data. In practice, data are collected in raw state and needs more cleaning work. The presented EDA was not aiming for a specific task even though we have concidered the price feature as a target variable for a classification task, but in case we're going to build a model there are more analysis to be made. For instance, we can further inspect the drop in price for the houses that have 6.5-7.5 bathrooms, and we can also think about binning some features and rechck whether or not a pattern has emmerged.","6522f8e8":"## Conclusion","2941f682":"#### Price","54fef3b1":"#### yr_built and yr_renovated","3731014e":"# Exploratory Data Analysis for Kc_House_Data\n\nThis is a tutorial notebook on how to perform EDA for a dataset.\n\nThe chosen data for this tutorial is House Sales in King County, USA, available on [Kaggle](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction).\n\nCheck the blog post [How to perform EDA for machine learning?](https:\/\/mlwithhamza.blogspot.com\/2021\/07\/how-to-perform-eda-for-machine-learning.html) for more informations about the used EDA method in this notebook.","881c17b5":"#### Lon, and Lat\n\nThe best practice in dealing with longitude & Latitude variables is to plot them on a map to visualize the distribution (scatter) of positions on a real scale. And this is valid for both univariate and multivariate analysis. ","09a482ca":"## Univariate Analysis\n\nIn this step of the EDA, each variable is examined and assessed by itself. Usually this step dosen't provide valuable insight, however it helps understanding each feature better by visualizing its distribution and examining it's statistics.","ffc393ab":"## Overall View"}}