{"cell_type":{"44173129":"code","ac69ec91":"code","5aeff114":"code","ce9084c1":"code","bb3cbd4a":"code","98cc3a03":"code","19e9c465":"code","51b45b84":"code","c1baa1fd":"code","dd20452d":"code","c79ed72a":"code","35e15306":"code","77972af8":"code","edcdd75d":"code","0bf08765":"code","f30c823e":"markdown","7009e40f":"markdown","c147ab44":"markdown","1ab61f05":"markdown","6c557d76":"markdown","c9d5f754":"markdown"},"source":{"44173129":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nplt.rcParams[\"figure.dpi\"] = 300\nplt.rcParams[\"font.size\"] = 14\nplt.rcParams['font.family'] = ['sans-serif']\nplt.rcParams['font.sans-serif'] = ['DejaVu Sans']\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\nplt.rcParams['image.cmap'] = 'gray' # grayscale looks better\nfrom itertools import cycle\nprop_cycle = plt.rcParams['axes.prop_cycle']\ncolors = prop_cycle.by_key()['color']","ac69ec91":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nfrom skimage.io import imread, imsave\nfrom IPython.display import clear_output\nfrom skimage.util import montage\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\nfrom skimage.color import label2rgb\nimport h5py","5aeff114":"with h5py.File('..\/input\/packaging-images-and-features\/out_cartoons.h5', 'r') as h:\n    val_dict = {}\n    for k in h.keys():\n        print(k, h[k].shape)\n        val_dict[k] = list(h[k])\n    cartoon_df = pd.DataFrame(val_dict)\n    del val_dict\ncartoon_df.sample(3)","ce9084c1":"val_count = {}\nfor c_col in cartoon_df.columns:\n    if c_col not in ['image']:\n        val_count[c_col] = cartoon_df[c_col].value_counts().index.max()+1\nprint(sum(val_count.values()), 'states')\nval_count","bb3cbd4a":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(cartoon_df, \n                                     random_state=0, \n                                     test_size=0.25) # stratify=cartoon_df[val_count.keys()],","98cc3a03":"from keras import layers, models\nfrom keras.losses import mse, binary_crossentropy\nfrom keras.utils import plot_model as pm_raw\ndef plot_model(*args, **kwargs):\n    try:\n        return pm_raw(*args, **kwargs)\n    except Exception as e:\n        print(e)\nfrom keras import backend as K","19e9c465":"original_shape = train_df['image'].shape[1:]\noriginal_dim = np.prod(original_shape)\n\n# parameters\nARGS_MSE = False\nEPOCHS = 100\nBATCH_SIZE = 64\nTARGET_DIM_SIZE = 128\nTARGET_LATENT_SIZE = 2\nrecon_loss_func = mse if ARGS_MSE else binary_crossentropy","51b45b84":"# reparameterization trick\n# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n# z = z_mean + sqrt(var) * epsilon\n\ndef sampling(args):\n    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n    # Arguments\n        args (tensor): mean and log of variance of Q(z|X)\n    # Returns\n        z (tensor): sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    batch = K.shape(z_mean)[0]\n    dim = K.int_shape(z_mean)[1]\n    # by default, random_normal has mean = 0 and std = 1.0\n    epsilon = K.random_normal(shape=(batch, dim))\n    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n\n\ndef z_log_loss(_, x):\n    return K.exp(x)-x\n\n\ndef z_mean_loss(_, x):\n    return K.square(x)","c1baa1fd":"def build_dcgan_vae(input_shape,\n                    intermediate_dim=TARGET_DIM_SIZE,\n                    latent_dim=TARGET_LATENT_SIZE,\n                    cnn_blocks=5,\n                    cnn_depth=8):\n    # VAE model = encoder + decoder\n    # build encoder model\n    # dcgan style CNN\n    raw_inputs = layers.Input(shape=input_shape, name='encoder_input')\n    cur_x = raw_inputs\n    for i in range(cnn_blocks):\n        cur_x = layers.Conv2D(cnn_depth*2**i,\n                              (3, 3),\n                              activation='linear',\n                              padding='same',\n                              strides=(2, 2),\n                              use_bias=False)(cur_x)\n        cur_x = layers.BatchNormalization()(cur_x)\n        cur_x = layers.LeakyReLU(0.2)(cur_x)\n    inputs = layers.Flatten()(cur_x)\n    x = layers.Dense(intermediate_dim, activation='relu')(inputs)\n    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n\n    # use reparameterization trick to push the sampling out as input\n    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n    z = layers.Lambda(sampling, output_shape=(\n        latent_dim,), name='z')([z_mean, z_log_var])\n\n    # instantiate encoder model\n    encoder = models.Model(raw_inputs, [z_mean, z_log_var, z], name='encoder')\n    plot_model(encoder, to_file='vae_dcgan_encoder.png', show_shapes=True)\n\n    # build decoder model\n    latent_inputs = layers.Input(shape=(latent_dim,), name='z_sampling')\n    x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n    int_shape = (input_shape[0]\/\/2**cnn_blocks,\n                 input_shape[1]\/\/2**cnn_blocks,\n                 cnn_depth*2**cnn_blocks)\n    ds_features = layers.Dense(np.prod(int_shape).astype(int),\n                               activation='relu', name='decoding_blcok')(x)\n    ds_features = layers.Reshape(int_shape)(ds_features)\n    cur_x = ds_features\n    for i in range(cnn_blocks):\n        cur_x = layers.UpSampling2D((2, 2))(cur_x)\n        cur_x = layers.Conv2D(cnn_depth*2**(cnn_blocks-i),\n                              (3, 3),\n                              padding='same',\n                              activation='linear',\n                              use_bias=False\n                              )(cur_x)\n        cur_x = layers.BatchNormalization()(cur_x)\n        cur_x = layers.LeakyReLU(0.2)(cur_x)\n\n    formed_output = layers.Conv2D(\n        input_shape[2], (1, 1), activation='sigmoid')(cur_x)\n\n    # instantiate decoder model\n    decoder = models.Model(latent_inputs, formed_output, name='decoder')\n    plot_model(decoder, to_file='vae_dcgan_decoder.png', show_shapes=True)\n\n    # we have to reconfigure the model to instrument the output well\n    # instantiate VAE model\n    def rename_tensor(last_tensor, name): return layers.Lambda(\n        lambda y: y, name=name)(last_tensor)\n    enc_z_mean, enc_z_log_var, enc_z = [rename_tensor(c_lay, c_name) for c_name, c_lay in zip(\n        ['enc_z_mean', 'enc_z_log_var', 'enc_z'], encoder(raw_inputs))]\n\n    outputs = decoder(enc_z)\n    vae = models.Model(inputs=[raw_inputs],\n                       outputs=[outputs, enc_z_mean,\n                                enc_z_log_var, enc_z],\n                       name='vae_dcgan')\n    vae.summary()\n\n    vae.compile(optimizer='adam',\n                loss={'enc_z_mean': z_mean_loss,\n                      'enc_z_log_var': z_log_loss, 'decoder': recon_loss_func},\n                loss_weights={'decoder': np.prod(\n                    input_shape), 'enc_z_log_var': 0.5, 'enc_z_mean': 0.5},\n                metrics={'decoder': 'mae'}\n                )\n    plot_model(vae, to_file='vae_dcgan.png', show_shapes=True)\n\n    return encoder, decoder, vae","dd20452d":"safe_shape = lambda x: np.stack(x.values, 0)[:, :, :, :]\/255.0\ndef make_bundle(in_df):\n    out_dict = {\n            'decoder': safe_shape(in_df['image']),\n            'enc_z_mean': np.zeros((in_df.shape[0], 2)),\n            'enc_z_log_var': np.zeros((in_df.shape[0], 2)),     \n        }\n    return (\n        {'encoder_input': safe_shape(in_df['image'])},\n        out_dict\n    )\ntrain_bundle = make_bundle(train_df)\nvalid_bundle = make_bundle(test_df)\noriginal_shape = train_bundle[0]['encoder_input'].shape[1:]\nprint(original_shape)","c79ed72a":"encoder, decoder, vae = build_dcgan_vae(original_shape, cnn_depth=16, cnn_blocks=5)\nencoder.summary()","35e15306":"base_dcgan_vae_history = vae.fit(\n    x=train_bundle[0],\n    y=train_bundle[1],\n    validation_data=valid_bundle,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE)\nclear_output()","77972af8":"def show_training_results(**named_model_histories):\n    model_out = list(named_model_histories.items())\n    test_keys = [k for k in model_out[0]\n                 [1].history.keys() if not k.startswith('val_')]\n    fig, m_axs = plt.subplots(\n        2, len(test_keys), figsize=(4*len(test_keys), 10))\n    for c_key, (c_ax, val_ax) in zip(test_keys, m_axs.T):\n        c_ax.set_title('Training: {}'.format(c_key.replace('_', ' ')))\n        val_ax.set_title('Validation: {}'.format(c_key.replace('_', ' ')))\n        for model_name, model_history in model_out:\n            c_ax.plot(model_history.history[c_key], label=model_name)\n            val_key = 'val_{}'.format(c_key)\n            if val_key in model_history.history:\n                val_ax.plot(\n                    model_history.history[val_key], '-', label=model_name)\n\n        c_ax.legend()\n        val_ax.legend()\n\n\ndef plot_results(models,\n                 data,\n                 batch_size=128,\n                 model_name=\"vae_mnist\"):\n    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n    # Arguments\n        models (tuple): encoder and decoder models\n        data (tuple): test data and label\n        batch_size (int): prediction batch size\n        model_name (string): which model is using this function\n    \"\"\"\n\n    encoder, decoder = models\n    x_test, y_vars = data\n    os.makedirs(model_name, exist_ok=True)\n\n    filename = os.path.join(model_name, \"vae_mean.png\")\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(x_test,\n                                   batch_size=batch_size)\n    col_count = np.sqrt(len(y_vars)).astype(int)\n    fig, m_ax = plt.subplots(col_count, col_count, figsize=(30, 30))\n    for c_ax, (y_name, y_test) in zip(m_ax.flatten(), y_vars.items()):\n        for k in np.unique(y_test):\n            c_ax.plot(z_mean[y_test == k, 0], z_mean[y_test == k, 1],\n                     '.', label='{:2.0f}'.format(k))\n            c_ax.set_title(y_name)\n        c_ax.legend()\n        c_ax.set_xlabel(\"z[0]\")\n        c_ax.set_ylabel(\"z[1]\")\n    \n    fig.savefig(filename)\n\n    filename = os.path.join(model_name, \"digits_over_latent.png\")\n    # display a 10x10 2D manifold of digits\n    n = 10\n    digit_size_x = original_shape[0]\n    digit_size_y = original_shape[1]\n    digit_shape_c = original_shape[2]\n    figure = np.zeros((digit_size_x * n, digit_size_y * n, digit_shape_c))\n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n\n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(\n                digit_size_x, digit_size_y, digit_shape_c)\n            figure[i * digit_size_x: (i + 1) * digit_size_x,\n                   j * digit_size_y: (j + 1) * digit_size_y] = digit\n\n    plt.figure(figsize=(10, 10), dpi=300)\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.imshow(figure[:, :].squeeze(), cmap='Greys_r')\n    plt.savefig(filename)\n    imsave(filename+'_raw.png', figure)\n    plt.show()\n\n    features_test = encoder.predict(x_test)[-1]\n    plt.hist2d(features_test[:, 0],\n               features_test[:, 1],\n               bins=30)\n\n    fig, ax1 = plt.subplots(1, 1, figsize=(12, 10))\n    ax1.plot(features_test[:, 0])\n    ax1.plot(features_test[:, 1])","edcdd75d":"show_training_results(\n                      dcgan = base_dcgan_vae_history\n                     )","0bf08765":"plot_results((encoder, decoder),\n             (safe_shape(test_df['image']), \n              {c_col: test_df[c_col] for c_col in val_count.keys()}),\n             batch_size=BATCH_SIZE,\n             model_name=\"vae_dcgan\")","f30c823e":"# Build Model","7009e40f":"## DCGan-Style VAE","c147ab44":"# Overview\nThe goal of the kernel is to make a VAE which is able to generate different cartoon faces and see how well it matches with the existing categories and components","1ab61f05":"# Build Model","6c557d76":"# Results\nHere we show the output of the trained model","c9d5f754":"# Load and Organize Data"}}