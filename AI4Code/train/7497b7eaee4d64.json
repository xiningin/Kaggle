{"cell_type":{"6091d432":"code","870bb7d4":"code","78418a2d":"code","2410acad":"code","dfc07dfa":"code","4f24af46":"code","dcd1cf71":"code","e7788fb0":"code","6011a48a":"code","df8ee3a2":"code","89cbab68":"code","a3363c6e":"code","4ebec5d0":"code","8aa69d20":"code","2c2a22d0":"code","3a90430e":"code","4aaf1c75":"code","fadeafda":"code","6447b936":"code","194bca65":"code","e3444894":"code","4c5d810b":"code","af2eba74":"code","c48d04d1":"code","de31552c":"code","1b79ac83":"code","3cbf448c":"code","4018cebf":"code","dd8a303b":"code","a9762b9c":"code","c6ea13cd":"code","46062054":"code","64499e59":"code","9e43ae87":"code","4adfdb3f":"code","349a9d62":"code","2491cddb":"code","dd5bb41b":"code","5f236d86":"code","6c6b8bbb":"code","34247db1":"code","01ba112f":"code","d5efc267":"code","8c074b1e":"code","74e92f33":"code","8d3467fd":"code","71ab618c":"code","75fcb54c":"code","99ae1a51":"code","4f705232":"code","b39365b6":"code","31a26087":"code","5fa6ad91":"code","6818f92a":"code","1ffb2824":"code","f115ee27":"code","0330522f":"code","ff0eeb1b":"code","dbada05d":"code","07981110":"code","f408f751":"code","cd549bbe":"code","ee1f223a":"code","7d2b598d":"code","d30ed0bb":"code","4a287436":"code","388bd0e0":"code","9512de09":"code","60cca77e":"code","eafcabb1":"code","e10c6e04":"code","b15511bc":"code","148a5d12":"code","99f76d9e":"code","1cd9ac4b":"code","7e49b976":"code","cb852c4e":"code","ce371425":"code","2cfb4476":"code","ff1f07d5":"code","30cadcf1":"code","1209d418":"code","0c9b673c":"code","4631f91e":"code","802095f9":"code","d2d9461c":"markdown","7fe2f152":"markdown","923e525d":"markdown","b75cf0bc":"markdown","3507ab17":"markdown","7527c7aa":"markdown","774be08d":"markdown","80f8b23d":"markdown","9617818c":"markdown","bb632cdf":"markdown","97097d1b":"markdown","0be9deff":"markdown","7b47ddf3":"markdown","ae0fcbd3":"markdown","6532bfb0":"markdown","934a0d0d":"markdown","f8989994":"markdown","3f999374":"markdown","795d2d2d":"markdown","0dbf38a1":"markdown","a1fee6bf":"markdown","24bb75fc":"markdown","1ee58f1d":"markdown","62ccd319":"markdown","ea72905d":"markdown","78545526":"markdown","d9de0847":"markdown","fc47a87d":"markdown","e6cde73c":"markdown","eb557999":"markdown","64073952":"markdown","293f8b71":"markdown","8cc015bf":"markdown","67d2f4c0":"markdown","4a5a19cf":"markdown","a91111c2":"markdown","eaf5c0c8":"markdown","328434f3":"markdown","6acd8fb6":"markdown","53cb527a":"markdown","cc2314d2":"markdown","7a851555":"markdown","035a1aa0":"markdown","1a02e59e":"markdown","5e148514":"markdown","3075ec91":"markdown","7f26b2ce":"markdown","be9f9672":"markdown","81b98c0a":"markdown","3ed7ce95":"markdown","aba063d3":"markdown","870b3e40":"markdown","555e446c":"markdown","be09eed2":"markdown","f3f079b2":"markdown","1b657e3a":"markdown","24a56aaa":"markdown","58e7f03b":"markdown","1eceafd5":"markdown","67b95ed5":"markdown","99b6bbb9":"markdown","b1863e71":"markdown","0935a888":"markdown","2ac23e09":"markdown","08d5c125":"markdown","6c908db8":"markdown","de9fff9c":"markdown","33fd1f8c":"markdown","d075c15d":"markdown","8de899d2":"markdown","917913ad":"markdown","25b5d3f7":"markdown","11aeaf27":"markdown","ef1d3519":"markdown","19f054f4":"markdown","c3aa0668":"markdown","f6fdc8d0":"markdown","c953b1d0":"markdown","2a9d97a1":"markdown","0e56713c":"markdown","fcaed166":"markdown","d7101f16":"markdown","9ffca9c3":"markdown","4d7a81d0":"markdown","b468f766":"markdown","5f2582ce":"markdown","ae0af02c":"markdown","12119952":"markdown","00ead9ac":"markdown","34ac2534":"markdown","f547628e":"markdown","fb0ca511":"markdown","4a934423":"markdown","f8c9faf2":"markdown","23bc6269":"markdown","64b899a6":"markdown"},"source":{"6091d432":"import numpy as np                      # Implements multi-dimensional array and matrices\nimport pandas as pd                     # For data manipulation and analysis\nimport pandas_profiling\nimport matplotlib.pyplot as plt         # Plotting library for Python programming language and it's numerical mathematics extension NumPy\nimport seaborn as sns                   # Provides a high level interface for drawing attractive and informative statistical graphics\n%matplotlib inline\nsns.set()\n\nfrom subprocess import check_output     # To check output","870bb7d4":"#Reading House Data\nhd = pd.read_csv('https:\/\/raw.githubusercontent.com\/insaid2018\/Term-2\/master\/Projects\/house_data.csv')","78418a2d":"hd.head(10)                                # Display the first five rows of the dataset.","2410acad":"hd.tail(10)","dfc07dfa":"hd.shape                                          # Shape of the Dataset","4f24af46":"hd.describe()    # Descriptive statistics for the numerical variables in the Dataset.","dcd1cf71":"hd.sample(10)                            # Displays a random 10 rows from the dataset.","e7788fb0":"hd.info()                         # Displays Data_Types and Null_Values in the Dataset.","6011a48a":"import pandas_profiling                          # Get a quick overview for all the variables using pandas_profiling.\nprofile = pandas_profiling.ProfileReport(hd)\nprofile.to_file('House_Data_before_preprocessing.html')   # HTML file will be downloaded to local workspace.","df8ee3a2":"DrpLst=[]  #DropList for droping the columns which are having >5% missing Values.\nfor i in hd.columns:\n    if((hd[i].count())\/1460)<0.95:\n        DrpLst.append(i)","89cbab68":"DrpLst","a3363c6e":"for i in hd.columns:                   #DropList for droping the columns which are having >80% Zero Values.\n    if(((hd[i]==0).sum())\/1460)>0.80:\n        if(i not in DrpLst):\n            DrpLst.append(i)","4ebec5d0":"len(DrpLst)  #There are 19 columns, which are having higher missing Values and Zeros.","8aa69d20":"#hd.shape\nDrpLst","2c2a22d0":"DrpLst.append('Id')  #Dropping the column Id\nhd = hd.drop(DrpLst, axis=1)","3a90430e":"#Code to seperate the Dataset on the basis of 'dtypes'\nhdN=pd.DataFrame()\nhdB=pd.DataFrame(hd['CentralAir'])\nhdC=pd.DataFrame()\n\nfor i in hd.columns:\n    if (hd[i].dtype=='int64') | (hd[i].dtype=='float64'):\n        hdN[i]=hd[i]\n    elif (hd[i].dtype=='object') & (i!='CentralAir'):\n        hdC[i]=hd[i]","4aaf1c75":"Lst=['YearBuilt','YearRemodAdd','MoSold','YrSold'] #List of columns which are categorical but, present in Numerical Data.\n\n#Adding the Lst columns from Numerical(hdN) to Categorical(hdC) Data\nfor i in Lst:\n    hdC[i]=hdN[i]\n\n\n#Dropping Lst columns from Numerical(hdN) Data\nhdN=hdN.drop(Lst,axis=1)","fadeafda":"hdN.columns","6447b936":"#Replacing Missing Values with the Mean \n\nfor i in hdN.columns:\n    if((hdN[i].count())!=1460):\n        hdN[i]= hdN[i].replace(np.NaN , hdN[i].mean())\n\n#hdN.isnull().sum() ","194bca65":"hdN.astype('int')","e3444894":"hdN.info()","4c5d810b":"hdC.nunique()  #Finding the nunique values   ","af2eba74":"for i in hdC.columns:\n    if((hdC[i].count())!=1460):\n        hdC[i] = hdC[i].fillna(hdC[i].mode()[0])","c48d04d1":"hdC.info()","de31552c":"hdC.columns","1b79ac83":"#Reading House Data\n#hd = pd.read_csv('https:\/\/raw.githubusercontent.com\/insaid2018\/Term-2\/master\/Projects\/house_data.csv')","3cbf448c":"def FtEng(df,lst,k,pfx):\n    for col in lst:\n        pf = pfx\n        ln = len(df[col])\n        vlc = pd.DataFrame(df[col].value_counts())\n        vlc['cats']=vlc.index\n        \n        \n        for r in vlc[col]:\n            if (r<(k*ln)):\n                pf = pf + vlc[vlc[col]==r]['cats'][0]\n\n\n\n        if (len(pf)<20):\n            for t in vlc[col]:\n                if (t<(k*ln)):\n                    u = vlc[vlc[col]==t]['cats'][0]\n                    df.loc[df[col]==u, col] = pf\n            print('The new category of '+ col +' is '+ pf)\n\n\n        elif (len(pf)>20):\n            for e in vlc[col]:\n                if (e<(k*ln)):\n                    v = vlc[vlc[col]==e]['cats'][0]\n                    df.loc[df[col]==v, col] = 'Others'\n            print('The new category of '+ col +' is Others')\n\n    return df\n\n            #print(u)\n    ","4018cebf":"CatColList = ['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n              'Condition1','BldgType','HouseStyle','RoofStyle']\n\nhdC_Dist = FtEng(df=hdC,lst=CatColList, k=0.10, pfx='Skw_')","dd8a303b":"hdC.shape","a9762b9c":"hdC_Dist.shape","c6ea13cd":"from sklearn.preprocessing import LabelEncoder\nhdC_Dist = hdC_Dist.apply(LabelEncoder().fit_transform) # fitting on hdC set of data","46062054":"#hdC_Dist.info()  ","64499e59":"from sklearn.preprocessing import LabelEncoder\nhdC = hdC.apply(LabelEncoder().fit_transform) # fitting on hdC set of data","9e43ae87":"hdC.info()          # Checking the Dtype","4adfdb3f":"hdB=(pd.get_dummies(hdB, prefix='CentralAir', drop_first=True)).astype('int')","349a9d62":"hdB.info()","2491cddb":"hdM_Dist = pd.concat([hdN,hdB,hdC_Dist], axis=1)","dd5bb41b":"hdM = pd.concat([hdN,hdB,hdC], axis=1)","5f236d86":"hdM","6c6b8bbb":"#Function for Outlier removal using Inter Qunatile Range\ndef OutlrRem(df,col):\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3-Q1\n    LwrR = df[col].min()\n    UprR = Q3 + (1.5*IQR)\n    df = df.loc[(df[col]>LwrR)&(df[col]<UprR)]\n    return df","34247db1":"hdM.nunique().nlargest(12)","01ba112f":"hdM.shape  #Checking the shape before Outlier Removal","d5efc267":"OutLst = ['LotArea','GrLivArea','BsmtUnfSF','1stFlrSF','TotalBsmtSF','SalePrice',\n          'BsmtFinSF1','GarageArea','2ndFlrSF','MasVnrArea','WoodDeckSF','OpenPorchSF']\nfor j in OutLst:\n    OutlrRem(hdM,j)\n","8c074b1e":"hdM.shape #Checking the shape after Outlier Removal","74e92f33":"import pandas_profiling\nprof = pandas_profiling.ProfileReport(hdM)\nprof.to_file('House_Data_after_preprocessing.html')","8d3467fd":"plt.figure(figsize=(50,50))\n#plt.title('House built over the years')\nabc = sns.catplot(\"YearBuilt\", data=hd, aspect=5, kind=\"count\", color='Skyblue')\nabc.set_xticklabels(rotation=90)","71ab618c":"plt.figure(figsize=(5,5))\n#plt.title('House built over the years')\nabc = sns.catplot(\"YrSold\", data=hd, aspect=2, kind=\"count\", color='Skyblue')\nabc.set_xticklabels(rotation=90)","75fcb54c":"c=sns.scatterplot(x=\"LotArea\", y=\"SalePrice\",data=hdM)","99ae1a51":"#hd = hd[hd['LotArea']<45000]  # For Visualization and understanding hd(raw) and hdM(processed) are used.\nhdM = hdM[hdM['LotArea']<45000]","4f705232":"c=sns.scatterplot(x=\"LotArea\", y=\"SalePrice\",hue='MSZoning',data=hd) #Outliers are removed.\nplt.legend(bbox_to_anchor=(1.05, 0.9), loc=2, borderaxespad=0.)","b39365b6":"plt.figure(figsize=(20,20))\nplt.tight_layout()\nsns.countplot('MSZoning',data=hd, ax=plt.subplot2grid((4,3), (0,0)))\nsns.countplot('SaleCondition',data=hd,ax=plt.subplot2grid((4,3), (0,1)))\nsns.countplot('BldgType',data=hd, ax=plt.subplot2grid((4,3), (0,2)))\nsns.countplot('HouseStyle',data=hd, ax=plt.subplot2grid((4,3), (1,0)))\nsns.countplot('RoofStyle',data=hd, ax=plt.subplot2grid((4,3), (1,1)))\nsns.countplot('ExterQual',data=hd,ax=plt.subplot2grid((4,3), (1,2)))\nsns.countplot('ExterCond',data=hd,ax=plt.subplot2grid((4,3), (2,0)))\nsns.countplot('Foundation',data=hd,ax=plt.subplot2grid((4,3), (2,1)))\nsns.countplot('BsmtQual',data=hd,ax=plt.subplot2grid((4,3), (2,2)))\nsns.countplot('KitchenQual',data=hd,ax=plt.subplot2grid((4,3), (3,0)))\nsns.countplot('Electrical',data=hd,ax=plt.subplot2grid((4,3), (3,1)))\nsns.countplot('SaleType',data=hd,ax=plt.subplot2grid((4,3), (3,2)))\n\nplt.show() ","31a26087":"sns.jointplot(\"SalePrice\",\"YearRemodAdd\", data=hdM, kind='hex', color='DarkBlue')","5fa6ad91":"#hdM=hdM[hdM['SalePrice']<400000]\nhd=hd[hd['SalePrice']<400000]  # Using 'hd' dataset here such that to show the year with encoding in the plot","6818f92a":"sns.jointplot(\"SalePrice\",\"YearRemodAdd\", data=hd, kind='hex', color='DarkBlue')","1ffb2824":"g = sns.PairGrid(hd, y_vars=['SalePrice'], x_vars=['GarageArea','TotalBsmtSF','OverallCond','OverallQual'], height=4)\ng.map(sns.regplot)\nplt.show()","f115ee27":"hd['Neighborhood'].unique()","0330522f":"plt.figure(figsize=(15,5))\nc = sns.scatterplot(x=\"Neighborhood\", y=\"SalePrice\",hue='YearBuilt',data=hd)\nplt.legend(bbox_to_anchor=(1.05, 0.9), loc=2, borderaxespad=0.)\nc.set_xticklabels(( 'CollgCr','Veenker', 'Crawfor', 'NoRidge', 'Mitchel', 'Somerst','NWAmes', \n                   'OldTown', 'BrkSide', 'Sawyer', 'NridgHt', 'NAmes','SawyerW', 'IDOTRR', \n                   'MeadowV', 'Edwards', 'Timber', 'Gilbert','ClearCr', 'NPkVill', 'StoneBr', \n                   'Blmngtn', 'BrDale', 'SWISU','Blueste'),rotation=90);","ff0eeb1b":"plt.figure(figsize=(50,50))\nsns.heatmap(hdM.corr(), annot=True)","dbada05d":"t = 0.8 #Thershold for Correlation\ncr = hdM.corr()\nhdCor = cr[(cr>t)|(cr<(-t))]\nplt.figure(figsize=(20,20))\nsns.heatmap(hdCor, annot=True)","07981110":"from sklearn.preprocessing import StandardScaler\ndef scaling(x):\n    sclr = StandardScaler().fit(x)  #Fitting the data \n    hdtf = sclr.transform(x)        #Transforming the data\n    return hdtf\n\ndef postscaling(y,z):\n    hdSc=pd.DataFrame(y)\n    hdSc.columns = z.columns\n    return hdSc","f408f751":"hdSc = postscaling(scaling(hdM),hdM) #Executing the function to check for errors.","cd549bbe":"hdSc = postscaling(scaling(hdM_Dist),hdM_Dist)","ee1f223a":"def splitX(hdSc):\n    X = pd.DataFrame()\n    X = hdSc.loc[:,hdSc.columns!='SalePrice']\n    return X\n\ndef splitY(hdSc):\n    Y = pd.DataFrame()\n    Y = hdSc['SalePrice']\n    return Y\n    ","7d2b598d":"X = splitX(hdSc)\nY = splitY(hdSc)\nprint('Shape of X is ', X.shape)   #Printing X Shape\nprint('Shape of X is ', Y.shape)   #Printing Y Shape","d30ed0bb":"from sklearn.model_selection import train_test_split\ndef splitTrTt(X,Y):\n    return train_test_split(X,Y, random_state=1, test_size=0.25)","4a287436":"X_train, X_test, Y_train, Y_test = splitTrTt(X,Y)  #Calling the function to check for errors","388bd0e0":"#Printing the shapes of split train and test data.\ndef printshape():\n    print('X_Train Shape : ',X_train.shape)\n    print('X_Test Shape : ',X_test.shape)\n    print('X_Train Shape : ',Y_train.shape)\n    print('X_Test Shape : ',Y_test.shape)\n\nprintshape()","9512de09":"from sklearn.linear_model import LinearRegression\ndef lnrReg(X_train, Y_train, X_test):\n    lr = LinearRegression()                  #Instantiating the Model\n    lr = lr.fit(X_train, Y_train)            #Training the Model with X_Train and Y_Train\n    Y_pred_train = lr.predict(X_train)       #Prediction on X_Train\n    Y_pred_test = lr.predict(X_test)         #Prediction on Y_Train\n    return Y_pred_train,Y_pred_test\n","60cca77e":"Y_pred_train,Y_pred_test = lnrReg(X_train, Y_train, X_test)\n\nfrom sklearn import metrics\nprint('Train Acc' + format(metrics.mean_absolute_error(Y_train,Y_pred_train),'.5f'))\n\nprint('Test Acc' + format(metrics.mean_absolute_error(Y_test,Y_pred_test),'.5f'))","eafcabb1":"Y_pred_train,Y_pred_test = lnrReg(X_train, Y_train, X_test)\n\nfrom sklearn import metrics\nprint('Train Acc' + format(metrics.mean_squared_error(Y_train,Y_pred_train),'.5f'))\n\nprint('Test Acc' + format(metrics.mean_squared_error(Y_test,Y_pred_test),'.5f'))","e10c6e04":"from sklearn.model_selection import GridSearchCV\n\ndef LinGridCV(X_train, Y_train,X_test):\n    parameters = {'normalize':[True,False], 'copy_X':[True,False],'fit_intercept':[True,False]}\n    lr = LinearRegression()\n    lr = GridSearchCV(lr,parameters,cv=15)\n    lr.fit(X_train, Y_train)\n    \n    lin = lr.best_estimator_\n    print('Best Linear Regressor GSCV Estimator is',lin)\n    lin = lin.fit(X_train, Y_train)\n    Y_pred_train_LinGCV = lin.predict(X_train)\n    Y_pred_test_LinGCV = lin.predict(X_test)\n    return Y_pred_train_LinGCV,Y_pred_test_LinGCV\n","b15511bc":"from sklearn.tree import DecisionTreeRegressor\ndef DecTreReg(X_train, Y_train,X_test):\n    dtr = DecisionTreeRegressor(random_state=3)\n    dtr = dtr.fit(X_train, Y_train)\n    Y_pred_train_dtr = dtr.predict(X_train)\n    Y_pred_test_dtr = dtr.predict(X_test)\n    return Y_pred_train_dtr,Y_pred_test_dtr","148a5d12":"Y_pred_train_dtr,Y_pred_test_dtr = DecTreReg(X_train, Y_train, X_test)\n\nfrom sklearn import metrics\nprint('Train Acc' + format(metrics.mean_absolute_error(Y_train,Y_pred_train_dtr),'.5f'))\n\nprint('Test Acc' + format(metrics.mean_absolute_error(Y_test,Y_pred_test_dtr),'.5f'))","99f76d9e":"def dtrGridCV(X_train, Y_train,X_test):\n    parameters = {'criterion':['mse','mae'],'max_depth': range(2,20),\n                             'max_features': ['auto', 'sqrt', 'log2', None]}\n    dt = DecisionTreeRegressor()\n    dt = GridSearchCV(dt,parameters, cv=10)\n    dt.fit(X_train, Y_train)\n    \n    dct = dt.best_estimator_\n    print('Best Decision Tree GSCV Estimator is',dct)\n    dct = dct.fit(X_train, Y_train)\n    Y_pred_train_dtrGCV = dct.predict(X_train)\n    Y_pred_test_dtrGCV = dct.predict(X_test)\n    return Y_pred_train_dtrGCV,Y_pred_test_dtrGCV","1cd9ac4b":"from sklearn.ensemble import RandomForestRegressor\ndef RanForReg(X_train, Y_train,X_test):\n    rfr = RandomForestRegressor(random_state=4)\n    rfr = rfr.fit(X_train, Y_train)\n    Y_pred_train_rfr = rfr.predict(X_train)\n    Y_pred_test_rfr = rfr.predict(X_test)\n    return Y_pred_train_rfr,Y_pred_test_rfr","7e49b976":"Y_pred_train_rfr,Y_pred_test_rfr = RanForReg(X_train, Y_train, X_test)\n\nfrom sklearn import metrics\nprint('Train Acc' + format(metrics.mean_absolute_error(Y_train,Y_pred_train_rfr),'.5f'))\n\nprint('Test Acc' + format(metrics.mean_absolute_error(Y_test,Y_pred_test_rfr),'.5f'))","cb852c4e":"Y_pred_train_rfr,Y_pred_test_rfr = RanForReg(X_train, Y_train, X_test)\n\nfrom sklearn import metrics\nprint('Train Acc' + format(metrics.mean_squared_error(Y_train,Y_pred_train_rfr),'.5f'))\n\nprint('Test Acc' + format(metrics.mean_squared_error(Y_test,Y_pred_test_rfr),'.5f'))","ce371425":"def rfrGridCV(X_train, Y_train,X_test):\n\n    param_grid = {'n_estimators':[50,100,150,200],'criterion':['mse', 'mae'],'max_depth': range(3,8),\n                  'min_samples_leaf':range(2,4),'max_features': ['auto', 'sqrt', 'log2', None]}\n    rf = RandomForestRegressor(random_state=5)\n    rf = GridSearchCV(estimator=rf,param_grid=param_grid, cv=10)\n    rf.fit(X_train, Y_train)\n    \n    rfc = rf.best_estimator_\n    print('Best Random Forest GSCV Estimator is',rfc)\n    rfc = rfc.fit(X_train, Y_train)\n    Y_pred_train_rfcGCV = rfc.predict(X_train)\n    Y_pred_test_rfcGCV = rfc.predict(X_test)\n    return Y_pred_train_rfcGCV,Y_pred_test_rfcGCV","2cfb4476":"def Modl(x):\n    \n    #Scaling\n    hdSc = postscaling(scaling(x),x)\n    \n    \n    X = splitX(hdSc)  #X Split\n    Y = splitY(hdSc)  #Y Split\n\n    \n    \n    #Train-Test Split\n    X_train, X_test, Y_train, Y_test = splitTrTt(X,Y)\n    Y_pred_train,Y_pred_test = lnrReg(X_train, Y_train,X_test)\n    Y_pred_train_dtr,Y_pred_test_dtr = DecTreReg(X_train, Y_train,X_test)\n    Y_pred_train_LinGCV, Y_pred_test_LinGCV = LinGridCV(X_train, Y_train,X_test)\n    Y_pred_train_dtrGCV, Y_pred_test_dtrGCV = dtrGridCV(X_train, Y_train,X_test)\n    Y_pred_train_rfr,Y_pred_test_rfr = RanForReg(X_train, Y_train,X_test)\n    Y_pred_train_rfcGCV,Y_pred_test_rfcGCV = rfrGridCV(X_train, Y_train,X_test)\n    \n    \n    a = []\n    c = [(Y_pred_train,Y_pred_test),(Y_pred_train_dtr,Y_pred_test_dtr),\n         (Y_pred_train_LinGCV, Y_pred_test_LinGCV),(Y_pred_train_dtrGCV, Y_pred_test_dtrGCV),\n         (Y_pred_train_rfr,Y_pred_test_rfr),(Y_pred_train_rfcGCV,Y_pred_test_rfcGCV)]\n\n    \n    #For loop to iterate over the models for calculation of metrics\n    from sklearn import metrics\n    for i,j in c: \n        metlst = [format(metrics.mean_absolute_error(Y_train,i),'.5f'),\n                  format(metrics.mean_absolute_error(Y_test,j),'.5f'),\n                  format(metrics.mean_squared_error(Y_train,i),'.5f'),\n                  format(metrics.mean_squared_error(Y_test,j),'.5f'),\n                  format(np.sqrt(metrics.mean_squared_error(Y_train,i)),'.5f'),\n                  format(np.sqrt(metrics.mean_squared_error(Y_test,j)),'.5f'),\n                  format(metrics.r2_score(Y_train,i),'.5f'),\n                  format(metrics.r2_score(Y_test,j),'.5f')]\n        a.append(metlst)\n\n    #DataFrame for storing all the measuring values from models    \n    b = pd.DataFrame(a,\n                     columns=['MAE_Train', 'MAE_Test','MSE_Train', 'MSE_Test','RMSE_Train', 'RMSE_Test','R2_Train', 'R2_Test'],\n                     index = ['Linear Regression','DecisionTree Reg','Linear Grid SearchCV','DecisionTree Reg GSCV',\n                              'RandomForest Reg', 'RandomForest Reg GSCV']).astype('float')\n    return b\n","ff1f07d5":"#Modl(hdM)               #Running the function above with hdM Data\n# Results are as in below block","30cadcf1":"# Model Instantiating \nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()               # Linear Regressor with default parameters\nlr = lr.fit(X_train,Y_train)\n\n\ncols=X_train.columns.to_list()         # Passing columns of X_train to list called 'cols'\ncols.insert(0,'Intercept')             # Inserting 'Intercept' at the top for 'cols' list\ncoef = lr.coef_.tolist()               # Passing coefficients of the Default to list called 'cols'\ncoef.insert(0, lr.intercept_)          # Inserting Intercept value to 'coef' list\n\n\ncoefs = pd.DataFrame(coef, columns=['Coefficients'], index=cols)  #Creating DataFrame for better plotting","1209d418":"coefs.plot(kind='barh', figsize=(12, 12))","0c9b673c":"def Mdl(x):  \n  #Scaling\n  hdSc = postscaling(scaling(x),x)\n      \n  X = splitX(hdSc)  #X Split\n  Y = splitY(hdSc)  #Y Split\n\n      \n  #Train-Test Split\n  X_train, X_test, Y_train, Y_test = splitTrTt(X,Y)\n  \n  #Model Implementation\n  Y_pred_train,Y_pred_test = lnrReg(X_train, Y_train,X_test)\n  Y_pred_train_rfr,Y_pred_test_rfr = RanForReg(X_train, Y_train,X_test)\n\n  from sklearn import metrics\n  print('Linear Regression Model Output')\n  print('MAE :', format(metrics.mean_absolute_error(Y_test,Y_pred_test),'.5f'))\n  print('MSE :', format(metrics.mean_squared_error(Y_test,Y_pred_test),'.5f'))\n  print('RMSE :' , format(np.sqrt(metrics.mean_squared_error(Y_test,Y_pred_test)),'.5f'))\n  print('R2 Score :' , format(metrics.r2_score(Y_test,Y_pred_test),'.5f'))\n\n  print('RandomForest Regressor Model Output')\n  print('MAE :', format(metrics.mean_absolute_error(Y_test,Y_pred_test_rfr),'.5f'))\n  print('MSE :', format(metrics.mean_squared_error(Y_test,Y_pred_test_rfr),'.5f'))\n  print('RMSE :' , format(np.sqrt(metrics.mean_squared_error(Y_test,Y_pred_test_rfr)),'.5f'))\n  print('R2 Score :' , format(metrics.r2_score(Y_test,Y_pred_test_rfr),'.5f'))","4631f91e":"hdCoef1 = hdM.drop(['MoSold','SaleType','PavedDrive','Electrical','HeatingQC','Heating','BsmtFinType2',\n                    'Foundation','ExterCond','Condition1','LotConfig','Utilities','Street','MSZoning',\n                    'CentralAir_Y','OpenPorchSF','GarageArea','BedroomAbvGr'], axis=1)\nMdl(hdCoef1)","802095f9":"hdCoef2 = hdM.drop(['Heating','BsmtFinType2','ExterCond','HouseStyle','LandContour','LotShape','HalfBath','FullBath'], axis=1)\nMdl(hdCoef2)","d2d9461c":"### 3.6.2 Categorical","7fe2f152":"- Functions for splitting the Data into X and Y before doing Train-Test Split","923e525d":"<a id=section202><\/a>\n### 2.2 Importing Dataset:","b75cf0bc":"- Splitting the data X and Y into Train-Test Split with Test size of 25%.","3507ab17":"### 7.3.2 Random Forest Regressor GridSearchCV Model\n- Random Forest Regressor with GridSearchCV for train and prediction.\n- The model below would perform GridSearchCV within the parameters provided such as, **{'n_estimators':[50,100,150,200],'criterion':['mse', 'mae'],'max_depth': range(3,8),'min_samples_leaf':range(2,4),'max_features': ['auto', 'sqrt', 'log2', None]}**\n- And then, considers the best estimator for predictions.","7527c7aa":"### 3.3.2 Dealing with zeros:\n","774be08d":"- Function to iterate over the above models and predict values.\n- The predicted values are stored into Data Frame for comparision.\n- MAE, MSE, RMSE and R-Squared metrics are used to measure the performance of the model.\n- Not only the predictions on Test but, Predictions on Train are also considered for better insights.","80f8b23d":"<a id=section505><\/a>\n### 5.5 Understanding the impact of Remodelling on the SalePrice","9617818c":"<a id=section701><\/a>\n### 7.1 Linear Regression Model","bb632cdf":"<a id=section7><\/a>\n## 7. Model Implementation","97097d1b":"- From above, It can been seen that there are some outliers in LotArea. These houses are of very large area. Since, the count is very less and can overfit the model, removing them.","0be9deff":"- The plot is too complicated and confusing to understand. Hence, we shall concenrate only on the columns with higher correlation.","7b47ddf3":"# Dist (UnSkew Testing)","ae0fcbd3":"- Next, I will do descriptive statistics for numerical variables.\n- This will help in finding distribution, Standard Deviations and min-max of numerical columns","6532bfb0":"<a id=section3><\/a>\n## 3. Data Profiling:\n\n- Firstly, I will __understand the dataset__ using various pandas functionalities.\n- Then, with the help of __pandas profiling__ I will find which columns of the dataset need preprocessing.\n- In __preprocessing__, I will deal with erronous and missing values. \n- Then after, I will do __pandas profiling__ again, to see how preprocessing has transformed the dataset.","934a0d0d":"<a id=section5><\/a>\n## 5. Exploratory Data Analysis","f8989994":"<a id=section9><\/a>\n## 9. Interpreting Model Coefficients","3f999374":"Here, I have done Pandas Profiling before preprocessing our dataset, so I have named the html file as __House_Data_before_preprocessing.html__. Looking at the file will help developing useful insights from the dataset. <br\/>\n\nBelow are insights from the preprofiling:\n- There are columns with missing values on average of 4-8%.\n- Also, some columns comprises of >90 zeros.\n- Columns should be divided based on dtypes.\n- Such as, 'Numerical and Float' of one set, 'Categorical' of another set and 'Boolean' of seperate set... for better handling.\n- Categorical Columns should be carefully handled during encoding.\n- Columns with no insights needs to be addressed.\n- All the columns with 'Basement' characteristics are positively correlated to one another. \n- Same goes for 'Garage' involved columns as well.","795d2d2d":"### 3.3.3 Dropping Missing Values and Zeros","0dbf38a1":"### 3.3.1 Dealing with missing Values:","a1fee6bf":"### Observations:\n- The houses of Residential Low zone are spread all over the SalePrice.\n- SalePrice for most of the houses in Residential Medium zone are in the range: 50,000 - 1,80,000 dollars.\n- The houses above $1,60,000 saleprice are of Floating Village zone.\n- Houses above 20,000 sq.ft. Lot Area are of Residential Low Density Zone.","24bb75fc":"Although, Most of the aspects of cleaning, handling, transforming and many... in processing data are covered in the project. There is still room for experiments. Hence, Below are some key points to be considered for future improvements.\n- Impute missing values(>5%) of the dropped columns.\n- Check importance of >80% zero value columns when predicting Sale Price.\n- Try to reduced the skewness in columns with feature engineering techniques.\nEx: Merge lower categories of a column into single category.\n- Use Random SearchCV while passing data to make the model more generalized.\n- Try other Regression Models to check for better predictions.","1ee58f1d":"<a id=section1><\/a>\n## 1. Problem statement\n\nThe dataset consists of 1460 houses sale information which are built over the period 1872-2010. The data consists of various variables for each house such as, MSZoning, LotArea, Street, Utilties, saletype, yearsold and many. The information from these variables will help in creating a Machine Learning Model, which would predict the Sale Price when an information of a new house with its characteristics are passed.","62ccd319":"<a id=section402><\/a>\n### 4.2 Dummifying Variables of the Boolean(hdB) Data\n- Dummyfication would make N categories in a column to N-1 number of columns.\n- In this case, No as 0 and Yes as 1.\n- Since, drop_first is True only the second column such as, CentralAir_Yes will be returned. ","ea72905d":"- Plotting the coefficient for each column and the intercept.","78545526":"- Appending the above created list with the column names which are having >80% zeros values. Considering that columns with >80% zeros would not help in making the predictions.","d9de0847":"<a id=section502><\/a>\n### 5.2 How many Houses were sold per year?","fc47a87d":"- Now dropping the 19 columns from the data set which are having >5% missing values and >80% zeros.\n- Also, Dropping the 'Id' column which is non-intrepretable.","e6cde73c":"- Calling the functions for execution and error check.","eb557999":"<a id=section2><\/a>\n## 2. Data Loading and Description:\n\nThe dataset comprises of __1460 observations of 81 columns__. Below is the table showing names of all the columns and respective description.","64073952":"- From Above, It can been seen that there are some outliers in SalePrice. These houses are of high price aroun 700000. Since, the count is very less and they can overfit the model, removing them.","293f8b71":"- The selected columns are ['MSZoning','BldgType','HouseStyle','RoofStyle','ExterQual','ExterCond','Foundation','BsmtQual', 'KitchenQual','Electrical','SaleType','SaleCondition']","8cc015bf":"- Above experiment shows:\n - In Case-1, Models are under performing.\n - In Case-2, Linear Regression model is giving slightly improved metrics but, RandomForest Regressor results didn't get better.","67d2f4c0":"- The values in each column is of different scales.\n- And this can lead to problems when fitting.\n- Scaling will help to Normalize data to single scaler.\n- Here, StandardScaler is used for scaling.","4a5a19cf":"### 7.1.1 Linear Regression Deafult Model\n- Linear Regression is choosen as a primary model for train and prediction.\n- The model below is basic meaning without any parameters.","a91111c2":"## Table of Contents\n \n1. [Problem Statement](#section1)<\/br>\n    - 1.1 [Introduction](#section101)<br\/>\n    \n<br>\n\n2. [Data Loading and Description](#section2)<\/br>\n    - 2.1 [Importing Packages](#section201)<br\/>\n    - 2.2 [Importing Dataset](#section202)<br\/>\n    \n<br>\n\n3. [Data profiling](#section3)<\/br>\n    - 3.1 [Understanding Data](#section301)<br\/>\n    - 3.2 [Data Preprofiling](#section302)<br\/>\n    - 3.3 [Data Preprocessing](#section303)<br\/>\n    - 3.4 [Seperating columns with respect to data types (Num, Cat and Bool)](#section304)<br\/>\n    - 3.5 [Copying columns from Numerical Column to Categrical](#section305)<br\/>\n    - 3.6 [Treating Missing Values](#section306)<br\/> \n  \n<br>\n\n4. [Data Transformation](#section4)<\/br>\n    - 4.1 [How many movies are released per year over the decade 2006-2016?](#section401)<br\/>\n    - 4.2 [Label Encoding Categorical(hdC) Data](#section402)<br\/>\n    - 4.3 [Dummifying Variables of the Boolean(hdB) Data](#section403)<br\/>\n    - 4.4 [Merging all the columns](#section404)<br\/>\n    - 4.5 [Outliers Detection and Handling](#section405)<br\/>\n    - 4.6 [Pandas Post-profiling](#section406)<br\/>\n\n<br>\n\n5. [Exploratory Data Analysis](#section5)<\/br>\n    - 5.1 [Count of the Houses Built in each years](#section501)<br\/>\n    - 5.2 [How many Houses were sold per year?](#section501)<br\/>\n    - 5.3 [Plotting the variation in Sale Price with change in the Lot Area and MSZoning of the house](#section503)<br\/>\n    - 5.4 [Understanding the distribution of data for selected columns](#section504)<br\/>\n    - 5.5 [Understanding the impact of Remodelling on the SalePrice](#section505)<br\/>\n    - 5.6 [Relationship between Price and Other selected columns](#section506)<br\/>\n    - 5.7 [Which Neighborhood house are expensive?](#section507)<br\/>\n    - 5.8 [Finding Correlation between the columns](#section508)<br\/>\n\n<br>\n\n6. [Data Preparation for Model Input](#section6)<\/br>\n    - 6.1 [Scaling the Data](#section601)<br\/>\n    - 6.2 [Splitting the Data](#section602)<br\/>\n\n<br>\n\n7. [Model Implementation](#section7)<\/br>    \n    - 7.1 [Linear Regression Model](#section701)<br\/>\n    - 7.2 [DecisionTree Regressor Model](#section702)<br\/>            \n    - 7.3 [RandomForest Regressor Model](#section703)<br\/>\n\n<br>\n\n8. [Metrics for Model Performance](#section8)<\/br>\n\n<br>\n\n9. [Interpreting Model Coefficients](#section9)<\/br>\n    - 9.1 [Plotting Coefficients of the Columns](#section901)<br\/>\n    - 9.2 [Cases for tuning the Model](#section902)<br\/>\n\n<br>\n\n10. [Conclusion](#section10)<\/br>  \n","eaf5c0c8":"- Inorder to handle the data better, finding the Data Types and Null Values. ","328434f3":"<a id=section901><\/a>\n### 9.1 Plotting Coefficients of the columns\n- Let us understand more about the model and each columns influence on Sale Price of the house.\n- For that, the best performed model(Linear Regression Default Model) from above table is fit and coefficients are extracted.\n- The coefficient against each column is collected in data frame.\n- Plotting the values from data frame will show the influence or dependency of each column.","6acd8fb6":"- And, Display a random 10 rows to see variations in the Data.","53cb527a":"| Column Name        | Description                                                                              |\n| -------------------|:----------------------------------------------------------------------------------------:| \n|SalePrice    | the property's sale price in dollars. This is the target variable that you're trying to predict.|\n|MSSubClass   | The building class|\n|MSZoning     | The general zoning classification|\n|LotFrontage  | Linear feet of street connected to property|\n|LotArea      | Lot size in square feet|\n|Street       | Type of road access|\n|Alley        | Type of alley access|\n|LotShape     | General shape of property|\n|LandContour  | Flatness of the property|\n|Utilities    | Type of utilities available|\n|LotConfig    | Lot configuration|\n|LandSlope    | Slope of property|\n|Neighborhood | Physical locations within Ames city limits|\n|Condition1   | Proximity to main road or railroad|\n|Condition2   | Proximity to main road or railroad (if a second is present)|\n|BldgType     | Type of dwelling|\n|HouseStyle   | Style of dwelling|\n|OverallQual  | Overall material and finish quality|\n|OverallCond  | Overall condition rating|\n|YearBuilt    | Original construction date|\n|YearRemodAdd | Remodel date|\n|RoofStyle    | Type of roof|\n|RoofMatl     | Roof material|\n|Exterior1st  | Exterior covering on house|\n|Exterior2nd  | Exterior covering on house (if more than one material)|\n|MasVnrType   | Masonry veneer type|\n|MasVnrArea   | Masonry veneer area in square feet|\n|ExterQual    | Exterior material quality|\n|ExterCond    | Present condition of the material on the exterior|\n|Foundation   | Type of foundation|\n|BsmtQual     | Height of the basement|\n|BsmtCond     | General condition of the basement|\n|BsmtExposure | Walkout or garden level basement walls|\n|BsmtFinType1 | Quality of basement finished area|\n|BsmtFinSF1   | Type 1 finished square feet|\n|BsmtFinType2 | Quality of second finished area (if present)|\n|BsmtFinSF2   | Type 2 finished square feet|\n|BsmtUnfSF    | Unfinished square feet of basement area|\n|TotalBsmtSF  | Total square feet of basement area|\n|Heating      | Type of heating|\n|HeatingQC    | Heating quality and condition|\n|CentralAir   | Central air conditioning|\n|Electrical   | Electrical system|\n|1stFlrSF     | First Floor square feet|\n|2ndFlrSF     | Second floor square feet|\n|LowQualFinSF | Low quality finished square feet (all floors)|\n|GrLivArea    | Above grade (ground) living area square feet|\n|BsmtFullBath | Basement full bathrooms|\n|BsmtHalfBath | Basement half bathrooms|\n|FullBath     | Full bathrooms above grade|\n|HalfBath     | Half baths above grade|\n|Bedroom      | Number of bedrooms above basement level|\n|Kitchen      | Number of kitchens|\n|KitchenQual  | Kitchen quality|\n|TotRmsAbvGrd | Total rooms above grade (does not include bathrooms)|\n|Functional   | Home functionality rating|\n|Fireplaces   | Number of fireplaces|\n|FireplaceQu  | Fireplace quality|\n|GarageType   | Garage location|\n|GarageYrBlt  | Year garage was built|\n|GarageFinish | Interior finish of the garage|\n|GarageCars   | Size of garage in car capacity|\n|GarageArea   | Size of garage in square feet|\n|GarageQual   | Garage quality|\n|GarageCond   | Garage condition|\n|PavedDrive   | Paved driveway|\n|WoodDeckSF   | Wood deck area in square feet|\n|OpenPorchSF  | Open porch area in square feet|\n|EnclosedPorch| Enclosed porch area in square feet|\n|3SsnPorch    | Three season porch area in square feet|\n|ScreenPorch  | Screen porch area in square feet|\n|PoolArea     | Pool area in square feet|\n|PoolQC       | Pool quality|\n|Fence        | Fence quality|\n|MiscFeature  | Miscellaneous feature not covered in other categories|\n|MiscVal      | $Value of miscellaneous feature|\n|MoSold       | Month Sold|\n|YrSold       | Year Sold|\n|SaleType     | Type of sale|\n|SaleCondition| Condition of sale|","cc2314d2":"### Observations from Data Profiling:\n\n- There are missing values(\/NaN) in the data.\n- Dataset consists of different dtypes.\n- Skewness is observed in some columns.\n\nNow, I have inputs for starting off with pre-profiling.","7a851555":"- Concatenating all the data sets that were seperated based on dtype for better handling. \n- The Final dataset:hdM to be passed to the Model.","035a1aa0":"- The metrics obtained from different version of models shows that **Random Forest Regressor Default** Model gave the better results.\n- On the otherhand, **Linear Regression: Basic and Grid SearchCV** Models gave same results.\n- Key point is, Default models are better performers than parameters tuned Grid SearchCV ones.\n- Linear Regression or RandomForest Regressor Model with default parameters could be used for predictions based on time consumption factor.","1a02e59e":"### Observations:\n- Nearly 23% houses were sold in 2009 and it is the highest.\n- Lowest number of houses were sold in the year 2010.","5e148514":"### 7.2.1 Decision Tree Regressor Default Model\n- DecisionTree Regressor is choosen as a secondary model for train and prediction.\n- The model below is basic meaning without any parameters.","3075ec91":"<a id=section504><\/a>\n### 5.4 Understanding the distribution of data for selected columns ","7f26b2ce":"<a id=section506><\/a>\n### 5.6 Relationship between Price and ['GarageArea','TotalBsmtSF','OverallCond','OverallQual']","be9f9672":"<a id=section902><\/a>\n### 9.2 Cases for tuning the Model\n- Let us divide the tuning into different cases based on thresholds.\n- Below is the function for checking the metrics for each case.","81b98c0a":"### Observations:\n- The Graph shows that houses of the Neighborhood: 'NridgHt' are sold more. At the same, 'SalePrice' is from above average price to highest price and those houses are latest built.\n- Only the houses of neighborhood:'Crawfor' are sold at higher price even though, they are built before 1950.\n- All the houses built after 1950 are sold above 150000(approx.).","3ed7ce95":"- Dealing with missing Values.\n- Dealing with zeros.\n- Splitting column data for more insights.\n- Dropping the coulumn unsoughtful for data insights.","aba063d3":"### Observations:\n- In MSZoning Column, the RL category is of >75% and the RM is of 15%.\n- In SaleCondition, Normal sale is >75%, followed by approx. 10% of Abnorml and Partial each.\n- In Building Type column, 1Fam is again >75%.\n- The column: HouseStyle is also skewed but better than above columns. Because, 1Story is nearly 50% followed by 2Story of 23%.\n- RoofStyle is Gable category for 75% houses.\n- External Quality and External Condition have high similar such as, TA of 60% and Gd of around 20%.\n- Basement Quality and Kitchen Quality are highly similar.\n- Electrical and Sale Type are highly skewed, such as SBrkr and WD categories are of 80% respectively.","870b3e40":"- Treating missing values with the mode after group by of 'YearBuilt' Column.","555e446c":"### Observations:\n- Number of houses sold that are built in 2006 are >60, which is the highest.\n- Houses Built during the peroid 1978-1991 are lower than previous years, so do the sales.\n- 268 out of 1460 houses sold were built during the years: 2003-2007. \n","be09eed2":"<a id=section503><\/a>\n### 5.3 Plotting the variation in Sale Price with change in the Lot Area and MSZoning of the house","f3f079b2":"- No change in the rows shows that there are no outliers in the Data.","1b657e3a":"Best Linear Regressor GSCV Estimator is LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n\nBest Decision Tree GSCV Estimator is DecisionTreeRegressor(ccp_alpha=0.0, criterion='mae', max_depth=8,\n                      max_features='sqrt', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, presort='deprecated',\n                      random_state=None, splitter='best')\n                      \n\nBest Random Forest GSCV Estimator is RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=2,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=150, n_jobs=None, oob_score=False,\n                      random_state=5, verbose=0, warm_start=False)\n                                     \n|     |MAE_Train|MAE_Test|MSE_Train|MSE_Test|RMSE_Train|RMSE_Test|R2_Train|R2_Test|\n|-----|:-------:|:------:|:-------:|:------:|:--------:|:-------:|:------:|:-----:|\n|Linear Regression|0.24518|0.24761|0.15789|0.15816|0.39735|0.39769|0.83844|0.85142|\n|DecisionTree Reg|0.00000|0.31187|0.00000|0.19404|0.00000|0.44050|1.00000|0.81771|\n|Linear Grid SearchCV|0.24518|0.24761|0.15789|0.15816|0.39735|0.39769|0.83844|0.85142|\n|DecisionTree Reg GSCV|0.18998|0.34592|0.10917|0.27036|0.33041|0.51996|0.88829|0.74601|\n|RandomForest Reg|0.08362|0.20027|0.02121|0.09132|0.14563|0.30219|0.97830|0.91421|\n|RandomForest Reg GSCV|0.17308|0.22712|0.07388|0.14217|0.27181|0.37706|0.92440|0.86643|","24a56aaa":"<a id=section303><\/a>\n### 3.3 Pre processing:","58e7f03b":"### Observations:\n\n#### SalePrice vs GarageArea & TotalBsmtSF:\n- An important observation from first two graphs is that SalePrice is low when there is no Garage and Basement.\n- GarageArea and TotalBasementSquareFeet are linearly related to SalePrice individually.\n- There are few outliers in both the graphs.\n\n#### SalePrice vs OverallCond:\n- It is very strange that the SalePrice is low even when the Over All Condition is High.\n- House condition is negative related with Price.\n\n#### SalePrice vs OverallQual:\n- As the Over all Quality increases, Sale Price increases.\n- Very few outliers are seen.\n","1eceafd5":"<a id=section8><\/a>\n## 8. Metrics for Model Performance","67b95ed5":"<a id=section10><\/a>\n## 10. Conclusion","99b6bbb9":"**Observations post running all the Models**","b1863e71":"## Feature Engineering\n### Combining the less weigted categories in a column to single category to address the skewnes.","0935a888":"### 7.2.2 Decision Tree Regressor Grid SearchCV Model\n- DecisionTree Regressor with GridSearchCV for train and prediction.\n- The model below would perform GridSearchCV within the parameters provided such as, **{'criterion':['mse','mae'],'max_depth': range(2,20),'max_features': ['auto', 'sqrt', 'log2', None]}**\n- And then, considers the best estimator for predictions.","2ac23e09":"<a id=section6><\/a>\n## 6. Data Preparation for Model Input","08d5c125":"#### **Case-2:** Dropping the columns with Coefficient values within the threshold: <0.01 and >-0.01","6c908db8":"### 3.6.1 Numerical\n- Replacing the missing values in a column with the mean of values of that column.","de9fff9c":"   From the metrics obtained by performing experiments with different Models, It can be concluded that **Linear Regression** and **Random Forest Regressor** are better fit for this data. And, Removing the Columns with coefficient values <0.01 and >-0.01 has shown slight improvements in the metrics.\n","33fd1f8c":"### 7.3.1 Random Forest Regressor Default Model\n- Random Forest Regressor with default parameters for train and prediction.","d075c15d":"<a id=section702><\/a>\n### 7.2 Decision Tree Regressor","8de899d2":"<a id=section501><\/a>\n### 5.1 Count of the Houses Built in each years","917913ad":"#### **Case-1:** Dropping the columns with Coefficient values within the threshold: <0.02 and >-0.02\n ","25b5d3f7":"<a id=section602><\/a>\n### 6.2 Splitting the Data","11aeaf27":"- From the Coefficients Plot, It is understood that some of the columns are having very low coefficient values.\n- And, these columns could create noise for the model.\n- Hence, data will be fed to the model by dropping the columns based on different thresholds.\n- This will help in fine tuning the Model.","ef1d3519":"<a id=section405><\/a>\n### 4.5 Pandas Post-profiling","19f054f4":"<a id=section305><\/a>\n### 3.5 Copying columns from Numerical Column to Categrical.\n- The list of columns with years information is to be copied to categorical set(hdC).\n- because, when replacing NaN values with the mean values during the treatment. \n- Missing or Nan Values in the Years involved columns would be filled with the mean.","c3aa0668":"<a id=section508><\/a>\n### 5.8 Finding correlation between the columns","f6fdc8d0":"### Observations:\n- Density of the houses remodelled after 2000 is high.\n- SalePrice is nearly normal distributed.\n- On the otherhand, YearRemodAdd is unevenly distributed. And it makes sense because, remodelling depends on the condition of the house.\n- There were very few houses remodelled during 1980-1990.","c953b1d0":"<a id=section404><\/a>\n### 4.4 Outliers Detection and Handling\n- Treating the outliers of the data using Inter Quantile Range.\n- IQR is one of the most effective techniques in removing the outliers.\n- The values over the quantile range are treated as outliers and removed.","2a9d97a1":"<a id=section703><\/a>\n### 7.3 Random Forest Regressor","0e56713c":"<a id=section601><\/a>\n### 6.1 Scaling the Data","fcaed166":"### 7.1.2 Linear Regression GridSearchCV Model\n- Linear Regression with GridSearchCV for train and prediction.\n- The model below would perform GridSearchCV within the parameters provided such as, **{'normalize':[True,False], 'copy_X':[True,False],'fit_intercept':[True,False]}**\n- And then, considers the best estimator for predictions.","d7101f16":"<a id=section403><\/a>\n### 4.3 Merging all the columns","9ffca9c3":"<a id=section301><\/a>\n### 3.1 Understanding Data:\n- Displaying the first five rows of the data to understand variables.\n- Finding the shape of dataset.","4d7a81d0":"<a id=section302><\/a>\n### 3.2 Pre profiling:\n\n- With pandas profiling, an __interactive HTML report__ gets generated, which gives breif description about the columns of the dataset like the __counts, Histograms and Correlations__. Detailed information about __each column__, __coorelation between different columns__ and a __sample__ of dataset.<br\/>\n- It also gives __visual interpretation__ of each column in the data.\n- Spread of the data is better understood by the distribution plot. \n- Grannular level analysis of each column.","b468f766":"# Dist (UnSkew Testing)","5f2582ce":"<a id=section306><\/a>\n### 3.6 Treating Missing Values","ae0af02c":"<a id=section304><\/a>\n### 3.4 Seperating columns with respect to data types (Num, Cat and Bool).\n\n- Firstly, data should be seperated according to the dtypes before begining with preprocessing of the data.\n- This helps in treating the missing values accordingly.","12119952":"## Observations:\n\nBelow are pair of columns which are correlated more than 80% threshold:\n- [OverallQual and CentralAir_Y]\n- [GrLivArea and TotalRmsAbvGr]\n- [SalePrice and OverallQual]\n- [Garage Area and Garage Cars]\n- [Exterior1st and Exterior2nd]\n\nIn fact, one among the pairs can be dropped in order to reduce the computing time of the model. For now, no changes are done to the final 'hdM' dataset.\n","00ead9ac":"<a id=section4><\/a>\n## 4 Data Transformation\n\n- Transformation of categorical and boolean columns are required in order to input data to the model.\n- In this project, Categorical columns is transformed using Label Encoder.\n- And Boolean Data is transformed using dummyification.","34ac2534":"- Creating a list for dropping the columns which are having >5% of missing values.","f547628e":"- Finding the column with highest unique values. So that, we could group by and then, fill the NaN values with mode of the values of that column.","fb0ca511":"<a id=section507><\/a>\n### 5.7 Which Neighborhood house are expensive?","4a934423":"<a id=section101><\/a>\n### 1.1 Introduction:\n\nThe Exploratory Data Analysis is to practice Python skills learned till now on a structured data set including loading, inspecting, wrangling, exploring, and drawing conclusions from data. The notebook has observations with each step in order to explain thoroughly how to approach the data set. Based on the observation some questions are arised and answered in the notebook for the reference though not all of them are explored in the analysis.\n\nAfter EDA, The Data is treated for outliers and then, split into Train and Test sets of 75% and 25% respectively. The Models used to train and predict are Linear Regression and DecisionTree Regressor with each model in two ways such as, Basic(default) and GridSearchCV(best_estimator). The predictions from models are collected into DataFrame for comparisions and performance tuning.","f8c9faf2":"<a id=section201><\/a>\n### 2.1 Importing Packages:","23bc6269":"<a id=section401><\/a>\n### 4.1 Label Encoding Categorical(hdC) Data:\n- In simple words, Label Encoder will label the categories of a column in numerical representation.\n- Post this, all the columns will be of 'int64'.","64b899a6":"# <center>House Price Prediction<\/center> \n#### <center>ML Foundation Project<\/center>"}}