{"cell_type":{"fc02a3ce":"code","7b2b2828":"code","b2d689d2":"code","6c80680f":"code","44c10759":"code","568ed9d7":"code","f2e67583":"code","084c0d83":"code","f640c570":"code","d8eec7de":"code","ae35d230":"code","b05531fa":"code","bfd64f41":"code","18f0ef9e":"code","89e75b63":"code","9449a1fe":"code","3347a9c6":"code","3002c95a":"code","e1b71c93":"code","8e012da3":"code","0ef67f87":"code","0885f554":"code","1da92a6b":"code","ef7d2456":"code","4b7e3c47":"code","142bd90a":"code","e70ac632":"code","77568435":"code","acfc31c2":"code","db6852a7":"code","0feeb115":"code","972f87dd":"code","9a1c410d":"code","15af5d23":"code","98e6c5e8":"code","83a02b4c":"code","209071b3":"code","42e06e4c":"code","fdfd59e4":"code","651bbff2":"code","29f1a93c":"code","03a96729":"code","72fa8a2e":"code","565da3e7":"code","c9e7380e":"code","67e7b1de":"code","132497b4":"code","9016b96c":"code","3b875695":"code","a728dea7":"code","ea89b356":"code","b2a60a4d":"code","4e68b186":"code","61a71937":"code","7e3672ac":"code","2e3d8485":"code","0342543a":"code","8e6b262a":"code","cf33b476":"code","8f04ba36":"code","8a8473a5":"code","764f2132":"code","d84f339e":"code","fe13b907":"code","16cd7a96":"code","6cc24d5c":"code","f9919600":"code","82cfd0e0":"code","434e0e2f":"code","f79d61ec":"code","29dbc590":"code","5937bbf4":"code","b0505a7f":"code","67734dd3":"code","ddadbb1a":"code","d4287095":"code","88694ccd":"code","1d0a8d3a":"code","5ee03af4":"code","193cbb28":"code","ffd59733":"code","8597b0cf":"code","9827c4f0":"code","e35cb993":"code","431f97b6":"code","65d4008d":"code","b0f46736":"code","c93b3bdc":"code","07993e9e":"code","eac0a6b1":"code","d479690f":"code","afefff18":"code","fbaf72fc":"code","fe6a0c35":"code","45fa8071":"code","6b1fc1de":"code","fed3d5a4":"code","42d668da":"code","05fe33a6":"markdown"},"source":{"fc02a3ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7b2b2828":"import pandas as pd","b2d689d2":"metadata=pd.read_excel('..\/input\/All_Reviews.xlsx')","6c80680f":"metadata.head(3)","44c10759":"metadata['Reviews'].head(5)","568ed9d7":"metadata['Reviews'] = metadata['Reviews'].fillna('')","f2e67583":"metadata['Reviews'].head(10)","084c0d83":"No_of_seleced_doc=7500","f640c570":"df1=metadata['Reviews'].iloc[0:No_of_seleced_doc]","d8eec7de":"df1.head(20)","ae35d230":"df1.shape","b05531fa":"print(\"There are {} review comments from {} different categories, such as {}... \\n\".format(df1.shape[0],len(metadata.Category.unique()),\", \".join(metadata.Category.unique()[0:5])))","bfd64f41":"from PIL import Image","18f0ef9e":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","89e75b63":"text = \" \".join(str(review) for review in metadata['Reviews'])\nprint (\"There are {} words in the combination of all reviews.\".format(len(text)))","9449a1fe":"stopwords = set(STOPWORDS)\nstopwords.update([\"Nan\",\"Negative\",\"etc\",\"got\",\"credit card\",\"card\",\"will\"])","3347a9c6":"wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=100).generate(text)","3002c95a":"import matplotlib.pyplot as plt\n%matplotlib inline","e1b71c93":"plt.figure(figsize=(10,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","8e012da3":"import os","0ef67f87":"import gensim","0885f554":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))","1da92a6b":"data_words = list(sent_to_words(df1))","ef7d2456":"print(data_words[0])","4b7e3c47":"print(data_words[:2])","142bd90a":"import re, nltk, spacy","e70ac632":"nlp = spacy.load('en', disable=['parser', 'ner'])","77568435":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n    return texts_out","acfc31c2":"data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","db6852a7":"print(data_words[:2])","0feeb115":"print(data_lemmatized[:2])","972f87dd":"import sklearn","9a1c410d":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","15af5d23":"tfidf_vectorizer=TfidfVectorizer(analyzer='word',       \n                             min_df=10,                        # minimum read occurences of a word \n                             stop_words='english',             # remove stop words\n                             lowercase=True,                   # convert all words to lowercase\n                             token_pattern='[a-zA-Z0-9]{2,}', )","98e6c5e8":"tfidf=tfidf_vectorizer.fit_transform(data_lemmatized)","83a02b4c":"tfidf.shape","209071b3":"tfidf_feature_names = tfidf_vectorizer.get_feature_names()","42e06e4c":"#tfidf_feature_names","fdfd59e4":"tf_vectorizer=CountVectorizer(analyzer='word',       \n                             min_df=10,                        # minimum read occurences of a word \n                             stop_words='english',             # remove stop words\n                             lowercase=True,                   # convert all words to lowercase\n                             token_pattern='[a-zA-Z0-9]{2,}',  # num chars > 2\n                            )","651bbff2":"tf=tf_vectorizer.fit_transform(data_lemmatized)","29f1a93c":"tf.shape","03a96729":"tf_feature_names = tf_vectorizer.get_feature_names()","72fa8a2e":"#tf_feature_names","565da3e7":"tf","c9e7380e":"print(\"Sparsicity in tf matrix: \", ((tf > 0).sum()\/tf.size)*100, \"%\")","67e7b1de":"data_dense = tf.todense()","132497b4":"print(\"Sparsicity: \", ((data_dense > 0).sum()\/data_dense.size)*100, \"%\")","9016b96c":"from sklearn.decomposition import NMF, LatentDirichletAllocation","3b875695":"no_topics = 5","a728dea7":"nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n","ea89b356":"lda_model = LatentDirichletAllocation(n_components=no_topics,               # Number of topics\n                                      max_iter=10,               # Max learning iterations\n                                      learning_method='batch',   \n                                      random_state=100,          # Random state\n                                      batch_size=128,            # n docs in each learning iter\n                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n                                      n_jobs = -1,               # Use all available CPUs\n                                     )","b2a60a4d":"lda_output = lda_model.fit_transform(tfidf)","4e68b186":"print(lda_model) ","61a71937":"print(\"Log Likelihood: \", lda_model.score(tf))","7e3672ac":"print(\"Perplexity: \", lda_model.perplexity(tf))","2e3d8485":"search_params = {'n_components': [3, 4, 6, 7], 'learning_decay': [.4,.45, .55]}\n","0342543a":"lda = LatentDirichletAllocation()","8e6b262a":"from sklearn.model_selection import GridSearchCV","cf33b476":"model = GridSearchCV(lda, param_grid=search_params)\n","8f04ba36":"model.fit(tf)","8a8473a5":"best_lda_model = model.best_estimator_","764f2132":"print(\"Best Model's Params: \", model.best_params_)","d84f339e":"print(\"Best Log Likelihood Score: \", model.best_score_)","fe13b907":"print(\"Model Perplexity: \", best_lda_model.perplexity(tf))","16cd7a96":"#gscore=model.fit(tf).cv_results_","6cc24d5c":"print(model.scorer_)","f9919600":"import pyLDAvis","82cfd0e0":"import pyLDAvis.sklearn","434e0e2f":"pyLDAvis.enable_notebook()","f79d61ec":"panel = pyLDAvis.sklearn.prepare(best_lda_model, tf, tf_vectorizer,mds='tsne') #no other mds function like tsne used.","29dbc590":"panel","5937bbf4":"lda_output = lda_model.transform(tfidf)","b0505a7f":"topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]","67734dd3":"docnames = [\"Doc\" + str(i) for i in range(len(data_lemmatized))]","ddadbb1a":"import numpy as np","d4287095":"df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n","88694ccd":"dominant_topic = np.argmax(df_document_topic.values, axis=1)\n","1d0a8d3a":"df_document_topic['dominant_topic'] = dominant_topic\n","5ee03af4":"def color_green(val):\n    color = 'green' if val > .1 else 'black'\n    return 'color: {col}'.format(col=color)","193cbb28":"def make_bold(val):\n    weight = 700 if val > .1 else 400\n    return 'font-weight: {weight}'.format(weight=weight)","ffd59733":"df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n","8597b0cf":"df_document_topics","9827c4f0":"df_document_topic.info()","e35cb993":"panel = pyLDAvis.sklearn.prepare(lda_model, tf, tf_vectorizer,mds='tsne') #no other mds function like tsne used.\n","431f97b6":"panel","65d4008d":"df_topic_keywords = pd.DataFrame(lda_model.components_\/lda_model.components_.sum(axis=1)[:,np.newaxis])\n","b0f46736":"df_topic_keywords.columns = tfidf_vectorizer.get_feature_names()\n","c93b3bdc":"df_topic_keywords.index = topicnames","07993e9e":"df_topic_keywords.head(15)","eac0a6b1":"def show_lda_topics(lda_model=lda_model, n_words=20):\n    keywords = np.array(df_topic_keywords.columns)\n    topic_keywords = []\n    for topic_weights in lda_model.components_:\n        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n        topic_keywords.append(keywords.take(top_keyword_locs))\n    return topic_keywords","d479690f":"topic_keywords = show_lda_topics(lda_model=lda_model, n_words=15)","afefff18":"df_topic_keywords = pd.DataFrame(topic_keywords)","fbaf72fc":"df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]","fe6a0c35":"df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n","45fa8071":"df_topic_keywords","6b1fc1de":"no_top_words = 8","fed3d5a4":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print (\"Topic %d:\" % (topic_idx))\n        print (\" \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))","42d668da":"display_topics(nmf, tfidf_feature_names, no_top_words)","05fe33a6":"The data has been collected from 3 different cards websites. The column specifying old\/new is for internal purposes and doesn't hold any significance with respect to the data. For a supervised learning problem, we define precisely the problem we are going to solve. Then we check wether the results of the modelling, seem to solve the problem we defined.\n\nBut here we are solving an unsupervised learning problem, which often are much more exploratory. We may have a notion that we could cluster banks or cards or reviews, we would understand the business better and therefore be able to improve something. However, we may not have a precise formulation.\n\nFor clustering, specifically, it is often difficult to understand what the clustering reveals. Even when the clustering does not seem to revealinteresting information, it often is not clear how to use that to make better decisions. Therefore for clustering, additional creativity and business knowledge must be applied in the evaluation and interpretation of the data mining process."}}