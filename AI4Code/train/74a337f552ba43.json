{"cell_type":{"3b99db66":"code","fe5dba58":"code","60f7331f":"code","8dbaf7c2":"code","b4ad06be":"code","59c55261":"code","55732153":"code","c93692e3":"code","67d4fb5a":"code","1c6465a1":"code","d021d543":"code","3b5ab7a4":"code","6c31d158":"code","5f9864a5":"code","51f82553":"code","88fbf4b9":"code","df654abb":"markdown","8fe21a91":"markdown","6e89ebb2":"markdown","5f790e2a":"markdown","38c4b0c9":"markdown","e49a8320":"markdown","1b887889":"markdown","9e36e050":"markdown","c2206e29":"markdown","ef5316f5":"markdown","b890cbbd":"markdown","8410bcaa":"markdown"},"source":{"3b99db66":"# Author List:\n# Marc Thurow\n# Alexander Schmitz\n# Artur Leinweber\n# Mathias Bredereck\n\n# TODO:\n# - Visualisierung\n# - Random Forest Algo, um das Overfitting zu verringern","fe5dba58":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nepsilon = 10e-10","60f7331f":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndef plotData(tiles, n, data, title, x_min, x_max, y_min, y_max):\n    getColor = {-1:'red', 1:'blue'}\n\n    x_pos = data[\"x\"].values\n    y_pos = data[\"y\"].values\n    color = data[\"Category\"].apply(lambda value: getColor[value])\n\n    tile = tiles[1][n]\n    \n    tile.set_aspect('equal')\n    tile.set_title(title)\n    tile.set_xlabel(\"x\")\n    tile.set_ylabel(\"y\")\n    tile.set_xlim(x_min, x_max)\n    tile.set_ylim(y_min, y_max)\n    tile.scatter(x = x_pos, y = y_pos, c = color)","8dbaf7c2":"class DecisionTree:\n    \n    def __init__(self):\n        pass\n    \n    def calculateEntropy(self, probabilities):\n        \n        entropy = 0\n        for p in probabilities:\n            entropy -= p*math.log(p,2)\n        return entropy\n    \n    def calculateInformationGain(self, rootEntropy, childPropabilities, childEntropies):\n        \n        gain = rootEntropy\n        for p, e in zip(childPropabilities, childEntropies):\n            gain -= p*e\n        return gain\n    \n    def calculateProbablity(data, attributes, goalAttribute):\n        pass\n    \n    def splitDataOnAttribute(self, data, attributes, attribute):\n        pass\n    \n    def getBestSplit(self, E_Root, attributes, data, goalAttribute):\n        pass\n    \n    def createDecisionTree(self, data, attributes, goalAttribute):\n        pass\n    \n    def predict(tree, featureVector, goalAttribute):\n        pass","b4ad06be":"class ContinuousDecisionTree(DecisionTree):\n    \n    def __init__(self):\n        super().__init__()\n        \n    def _calculateEntropy(self, probabilities):\n        \n        entropy = super().calculateEntropy(probabilities)\n        return entropy\n    \n    def _calculateInformationGain(self, rootEntropy, childPropabilities, childEntropies):\n        \n        gain = super().calculateInformationGain(rootEntropy, childPropabilities, childEntropies)\n        return gain\n    \n    def _calculateProbablity(self, data, attributes):\n        \n        attributeIndex = len(attributes) -1\n        attributeValues = {}\n        \n        for row in data:\n            value = row[attributeIndex]\n            if not value in attributeValues:\n                attributeValues[value] = 1 \/ len(data)\n            else:\n                attributeValues[value] += (1 \/ len(data))\n            \n        return list(attributeValues.values())\n    \n    def splitDataOnAttribute(self, data, attributes, attribute):\n        pass\n    \n    def splitOnAttributeValue(self, data, splitAfterIndex):\n        \n        data1 = data[0 : splitAfterIndex+1]\n        data2 = data[splitAfterIndex+1 :]\n        \n        return data1, data2\n    \n    def _getBestSplit(self, E_Root, attributes, data):\n        \n        bestSplit = None\n        decisionThreshold = None\n        attributeIndex = None\n        \n        #find best split throughout all dims\n        gain = 0\n        for i, attribute in enumerate(attributes):\n            \n            if i == len(attributes) - 1:\n                continue\n            newData = sorted(data, key=lambda point : point[i])\n            \n            for j, row in enumerate(newData):\n                \n                if j == len(newData) -1:\n                    continue\n                \n                splittedNewData = self.splitOnAttributeValue(newData, j)\n                childEntropies = []\n                childProbs = []\n\n                for split in splittedNewData:\n                    \n                    probs = self._calculateProbablity(split, attributes)\n                    childEntropy = self._calculateEntropy(probs)\n                    childEntropies.append(childEntropy)\n                    \n                    childProbs.append(len(split) \/ len(newData))\n                    \n                newGain = self._calculateInformationGain(E_Root, childProbs, childEntropies)\n                \n                if newGain > gain:\n                    \n                    gain = newGain\n                    bestSplit = splittedNewData\n                    attributeIndex = i\n                    decisionAxis = np.array(splittedNewData[0])[:,attributeIndex]\n                    decisionThreshold = ( np.max(decisionAxis)  )\n                    \n        \n        return bestSplit, decisionThreshold, attributeIndex\n    \n    def createDecisionTree(self, data, attributes, goalAttribute):\n        \n        decisionTree = {}\n        \n        probabilities = self._calculateProbablity(data, attributes)\n        E_Root = self._calculateEntropy(probabilities)\n        \n        if np.abs(E_Root) > 0 + epsilon:\n        \n            bestSplit, decisionThreshold, attributeIndex = self._getBestSplit(E_Root, attributes, data)\n            \n            subtrees = []\n            for dataParts in bestSplit:\n                \n                newData = np.array(dataParts[:])\n                \n                subtree = self.createDecisionTree(newData, attributes, goalAttribute)\n                subtrees.append(subtree)\n                \n            decisionTree[attributes[attributeIndex]] = [decisionThreshold, subtrees]\n            \n        else:\n            leaf = {goalAttribute : data[0][-1]}\n            decisionTree = leaf\n            \n        return decisionTree\n    \n    def predict(self, tree, featureVector, attributes, goalAttribute):\n        \n        classPredict = None\n        leafFound = False\n        \n        currentTreeView = tree\n        \n        while not leafFound:\n            \n            currentNodeAttribute = list(currentTreeView.keys())[0]\n            \n            verticesNodes = list(currentTreeView.values())\n            \n            if currentNodeAttribute == goalAttribute: #leaf found\n                classPredict = verticesNodes[0] #just the class to find\n                leafFound = True\n\n            if not leafFound:\n            \n                attributeIndex = attributes.index(currentNodeAttribute)\n                featureAttribute = featureVector[attributeIndex]\n                \n                childNodeIndex = 0\n                if featureAttribute <= verticesNodes[0][0]:\n                    childNodeIndex = 0\n                else:\n                    childNodeIndex = 1\n                \n                #now go for the child node\n                currentTreeView = verticesNodes[0][1][childNodeIndex]\n        \n        return classPredict","59c55261":"def read_data(filename):\n    f = open(filename)\n    data_line = False\n    data = []\n    for l in f:\n        l = l.strip() \n        if data_line:\n            content = [float(x) for x in l.split(',')]\n            if len(content) == 3:\n                data.append(content)\n        else:\n            if l.startswith('@DATA'):\n                data_line = True\n    return data\n\ndataC = read_data(r\"..\/input\/kiwhs-comp-1-complete\/train.arff\")","55732153":"import numpy as np\nimport pandas as pd\n\ndata = pd.DataFrame({'x':[item[0] for item in dataC], 'y':[item[1] for item in dataC], 'Category':[item[2] for item in dataC]})","c93692e3":"from sklearn.model_selection import train_test_split\n\ntrain_data, test_data = train_test_split(data, random_state = 0, test_size = 0.2)","67d4fb5a":"tiles = plt.subplots(1, 3, figsize=(30, 10))\n\nplotData(tiles, 0, data, \"data\", -5, 5, -5, 5)\nplotData(tiles, 1, train_data, \"train data\", -5, 5, -5, 5)\nplotData(tiles, 2, test_data, \"test data\", -5, 5, -5, 5)","1c6465a1":"goalAttribute = 'class'\nattributesC = [\"x\",\"y\",\"class\"]\ncdt = ContinuousDecisionTree()\ntree = cdt.createDecisionTree(train_data.values, attributesC, goalAttribute)\n\nprint(\"well done\")","d021d543":"accuracy = 0\nprediction_data = pd.DataFrame(columns = ['x', 'y', 'Category'])\n\nfor index, data in test_data.iterrows():\n    prediction = cdt.predict(tree, [data['x'], data['y']], attributesC, 'class')\n    category = data['Category']\n    prediction_data = prediction_data.append(pd.Series({'x' : data['x'], 'y' : data['y'], 'Category' : prediction}), ignore_index=True)\n    if prediction == category:\n        accuracy += 1\naccuracy \/= len(test_data)    \n\nprint(accuracy)","3b5ab7a4":"tiles = plt.subplots(1, 2, figsize=(20, 10))\n\nplotData(tiles, 0, test_data, \"test data\", -5, 5, -5, 5)\nplotData(tiles, 1, prediction_data, \"prediction data\", -5, 5, -5, 5)","6c31d158":"def read_data(filename):\n    f = open(filename)\n    data_line = False\n    data = []\n    for l in f:\n        l = l.strip() \n        if data_line:\n            content = [float(x) for x in l.split(',')]\n            if len(content) == 3:\n                data.append(content)\n        else:\n            if l.startswith('@DATA'):\n                data_line = True\n    return data\n\ndataC = read_data(r\"..\/input\/kiwhs-comp-1-complete\/train.arff\")","5f9864a5":"train_data = pd.DataFrame({'x':[item[0] for item in dataC], 'y':[item[1] for item in dataC], 'Category':[item[2] for item in dataC]})","51f82553":"goalAttribute = 'class'\nattributesC = [\"x\",\"y\",\"class\"]\ncdt = ContinuousDecisionTree()\ntree = cdt.createDecisionTree(train_data.values, attributesC, goalAttribute)\n\nprint(\"well done\")","88fbf4b9":"test_data = pd.read_csv(r\"..\/input\/kiwhs-comp-1-complete\/test.csv\")\ntest_prediction = pd.DataFrame(columns = ['Id (String)', 'Category (String)'])\n\nfor index, data in test_data.iterrows():\n    prediction = cdt.predict(tree, [data['X'], data['Y']], attributesC, 'class')\n    test_prediction = test_prediction.append(pd.Series({'Id (String)' : int(data['Id']), 'Category (String)' : int(prediction)}), ignore_index=True)\n    \ntest_prediction.to_csv(r\"predict.csv\", index=False)","df654abb":"\n\n\nWettbewerb\n==========","8fe21a91":"Datenstrukturen und Algorithmus zur Umsetzung des Entscheidungsbaum-Klassifizierers\n=====================================================================","6e89ebb2":"## Klassifizierung der Testdaten f\u00fcr den Wettbewerb","5f790e2a":"## Einlesen der Trainingsdaten","38c4b0c9":"## Evaluierung: Klassifizierung der Testdaten und Vergleich mit dem Soll-Ergebnis","e49a8320":"## Training: Aufbau des Entscheidungsbaumes","1b887889":"## Aufteilen des Datensatzes in Trainings-Daten und Test-Daten","9e36e050":"Wir implementieren einen bin\u00e4ren Entscheidungsbaum zur L\u00f6sung des Klassifikationsproblems. Vorbild hierf\u00fcr war der (nicht bin\u00e4re) Entscheidungsbaum aus der Vorlesung.\nDie Suche nach dem besten Split erfolgt anschaulich, indem f\u00fcr s\u00e4mtliche Koordinatenachsen \u00fcber diese jeweilige Achse iteriert wird und f\u00fcr jeden Iterations-\nschritt der Information Gain f\u00fcr eine Aufteilung der Daten an dieser Stelle ermittelt wird, wodurch der Beste dann ausgew\u00e4hlt werden kann.\nF\u00fcr die jeweiligen Datenanteile kann erneut eine beste Aufteilung nach diesem Schema gefunden werden. Anschlie\u00dfend wird ein einfacher dict-basierter Baum durch\nAusw\u00e4hlen der besten Anteile rekursiv aufgebaut, wobei immer dann ein spezieller Leaf-Knoten gebildet wird, wenn die Entropie zu 0 wird.\n\nErwartung:\nDer Baum wird vollst\u00e4ndig aufgebaut bzw. jeder Punkt im Trainingsdatensatz wird klassifiziert. Hierdurch steckt in dem Baum die Annahme, dass die partielle Verteilung\nder Punkte im Trainingsdatensatz (bezogen auf eine Verteilungsfunktion f\u00fcr alle m\u00f6glichen Punkte) bereits vollst\u00e4ndig ist, was bei Testdaten zu einer statistisch schlechten Klassifikation\nan Entscheidungsgrenzen f\u00fchren kann. Nat\u00fcrlich ist die Klassifikation auch dann schlecht, wenn der Trainingsdatensatz keine seltenen Gebiete f\u00fcr einzelne Klassen enth\u00e4lt, wenn es diese gibt,\ndenn dann gibt es generell keinen Grund f\u00fcr eine \"korrekte\" Klassifikation in diesen Gebieten anhand der Trainingsdaten.\n\nAufgrund der Verteilung der Punkte in den Trainingsdaten ist zu erwarten, dass eben nur an den Entscheidungsgrenzen Ungenauigkeiten auftauchen. Wenn man das erwartet,\ndr\u00fcckt man aber auch eine Erwartung an die Verteilungsfunktion aus. Unsere L\u00f6sung schw\u00e4chelt also daran, dass der Baum die Verteilungshypothese nicht direkt entwickelt, oder nicht optimal entwickelt.\n\nWir erwarten einen Score von 90%, vielleicht etwas dr\u00fcber.\n\nDamit gewinnt man zwar nicht die Competition, aber die Aufgabe macht Spa\u00df und man Luft f\u00fcr Verbesserungen.\n\nVerbessungsideen f\u00fcr die n\u00e4chste Aufgabe:\nBei hinreichend vielen Trainingsdaten k\u00f6nnte man mehrere B\u00e4ume mit jeweiligen Teilen der Daten trainieren und die B\u00e4ume irgendwie kombinieren.\nIm Internet findet man dazu auch einen Ansatz der \"Random Forests\" hei\u00dft, dessen Implementierung praktisch nicht schwierig erscheint.\n\nIntuitiv versucht man Entscheidungsgrenzen im Kopf zu zeichnen, wenn man sch\u00e4tzen m\u00f6chte, was ein Entscheidungsbaum mit den Daten machen k\u00f6nnte.\nEine Visualisierung w\u00fcrde das Verst\u00e4ndnis f\u00f6rdern und bei der Verbesserung der Funktionalit\u00e4t des Klassifikators helfen.","c2206e29":"## Training: Aufbau des Entscheidungsbaumes","ef5316f5":"Evaluation\n========","b890cbbd":"## Einlesen eines Datensatzes zum Trainieren und Evaluieren","8410bcaa":"## Visualisierung des Datensatzes"}}