{"cell_type":{"79ec81bf":"code","b159027e":"code","9c43ccf7":"code","c70db72a":"code","b6b2def1":"code","1e71a332":"code","c49a0c67":"code","a20dacb4":"code","11d5c597":"code","bf9a2e59":"code","aba53e60":"code","0fb81ff8":"code","eeca1138":"code","79fdbed9":"code","188b52a9":"code","4a4a4c15":"code","e1cf00cf":"code","882321ac":"code","7047d660":"code","aa83f38b":"code","6168ae7c":"code","11438da6":"code","22de9f65":"code","a98c268f":"code","f5fe0151":"code","851dbaa5":"code","959f6f14":"code","9ebbe145":"code","e94ff740":"code","d0afcbc8":"code","a257a123":"code","adeb9044":"code","9a046aa5":"code","54dd22f0":"code","ac0b2b57":"code","0fc5ace6":"code","e079b41b":"code","3d132c08":"markdown","2ce4b477":"markdown","4466c408":"markdown"},"source":{"79ec81bf":"import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nprint('All modules & libraries imported!')","b159027e":"train= pd.read_csv('..\/input\/titanic\/train.csv', index_col=  'PassengerId')\ntest= pd.read_csv('..\/input\/titanic\/test.csv', index_col=  'PassengerId')\ndf= pd.concat([train, test], axis= 0)\ndf.head()","9c43ccf7":"print(df.info()) # get the datatypes of each column\nprint(df.isna().sum()) # 177 missing values for Age, 687 for Cabin, 2 for Embarked","c70db72a":"# first column Pclass is passenger class wihtout any missing values & proper datatype. Nothing to do\n# second column Name requires some feature engineering. We can extract titles (Mr\/Mrs etc) from name\ndf['Title']= '' # create empty column for storing the appropriate titles\n\ndf['Title'][df['Name'].str.contains('Mr. ')]= 'Mr'\ndf['Title'][df['Name'].str.contains('Mrs. ')]= 'Mrs'\ndf['Title'][df['Name'].str.contains('Miss. ')]= 'Miss'\ndf['Title'][df['Name'].str.contains('Mlle. ')]= 'Miss'\ndf['Title'][df['Name'].str.contains('Ms. ')]= 'Miss'\ndf['Title'][df['Name'].str.contains('Master. ')]= 'Master'\ndf['Title'][df['Name'].str.contains('Don. ')]= 'Mr'\ndf['Title'][df['Name'].str.contains('Dona. ')]= 'Madam'\ndf['Title'][df['Name'].str.contains('Rev. ')]= 'Rev'\ndf['Title'][df['Name'].str.contains('Dr. ')]= 'Mr'\ndf['Title'][df['Name'].str.contains('Mme. ')]= 'Madam'\ndf['Title'][df['Name'].str.contains('Capt. ')]= 'Mr'\ndf['Title'][df['Name'].str.contains('Col. ')]= 'Mr'\ndf['Title'][df['Name'].str.contains('Major. ')]= 'Mr'\ndf['Title'][df['Name'].str.contains('Countess. ')]= 'Madam'\ndf['Title'][df['Name'].str.contains('Sir. ')]= 'Mr'\ndf['Title'][df['Name'].str.contains('Jonkheer. ')]= 'Master'\n\n# column Name is not of use to us anymore so we drop it\ndf.drop(['Name'], axis= 1, inplace= True)\ndf.head()","b6b2def1":"# column Age has some missing values. Let us handle them\nage_mean= df['Age'].mean()\nage_sd= df['Age'].std()\nc= df['Age'].isna().sum()\nage_random=  np.random.randint(age_mean-age_sd, age_mean+age_sd, c)\ndf['Age'][np.isnan(df['Age'])] = age_random\ndf.isna().sum()","1e71a332":"# column Embarked has 2 missing values. Those can be imputed by the mode value\ndf.fillna({'Embarked': 'S'}, inplace= True)\ndf.isna().sum()","c49a0c67":"# column Fare has 1 missing value. The passenger in question was a male of about 61 years age,\n# was a 3rd class passenger, & had no spouse\/sibling\/children\/parent aboard the ship. He was not\n# allotted any cabin, embarked from Southampton & had ticket number 3701. We try to find the\n# most likely fare for a person with these above characteristics. Passenger having ticket number\n# 345364 displays all identical characteristics, & has fare 6.2375. We impute this value.\ndf[(df['Embarked']=='S')&(df['Pclass']==3)&(df['SibSp']== 0)&(df['Parch']== 0)&(df['Age']<=61)&(df['Age']>=59)&(df['Sex']==1)]\ndf['Fare'][(df['Fare'].isna())]= 6.2375\ndf.isna().sum()","a20dacb4":"# Regarding the absence of values in column Cabin of course we cannot make any estimation\n# as to which passenger was alootted which cabin. We perform a different sort of feature\n# engineering here. We create another column; if Cabin value is present then the new column\n# has 1 else 0 value. Then we drop Cabin column.\ndf['Has_Cabin']= ''\ndf['Has_Cabin'][df['Cabin'].isna()]= 0\ndf['Has_Cabin'][df['Cabin'].notna()]= 1\ndf.drop('Cabin', axis= 1, inplace= True)\ndf.isna().sum() # no more missing values in dataframe","11d5c597":"# Column SibSp specifies number of siblings & spouses travelling with said passenger.\n# Column Parch specifies number of parents or children of said passenger.\n# We can feature engineer a new column called Family that is SibSp+Parch instead of\n# retaining both of them.\ndf['Family']= df['Parch'] + df['SibSp']\ndf.drop(['Parch', 'SibSp'], axis= 1, inplace= True)\ndf['Has']= ''\ndf['Has'][(df['Family']==0)]= 0\ndf['Has'][(df['Family']!=0)]= 1\ndf.head()","bf9a2e59":"# we need to categorize age. Column Age having value below 18 should be a category. 19-39 another,\n# 40-60 another, 61 to 80 another. We add an additional column Age_scale, then drop Age.\ndf['Age_scale']= ''\ndf['Age_scale'][(df['Age']<=18)]= 0\ndf['Age_scale'][(df['Age']>18)&(df['Age']<=39)]= 1\ndf['Age_scale'][(df['Age']>39)&(df['Age']<=60)]= 2\ndf['Age_scale'][(df['Age']>60)&(df['Age']<=80)]= 3\ndf['Age_scale']= df['Age_scale'].apply(pd.to_numeric)\ndf.drop(['Age'], axis= 1, inplace= True)","aba53e60":"# lastly we need to drop the column Ticket.\ndf.drop('Ticket', axis= 1, inplace= True)\ndf.isna().sum()","0fb81ff8":"df['Fare_range']= ''\ndf['Fare_range'][(df['Fare'] <= 7.91)] = 0\ndf['Fare_range'][(df['Fare'] > 7.91) & (df['Fare'] <= 14.454)] = 1\ndf['Fare_range'][(df['Fare'] > 14.454) & (df['Fare'] <= 31)]   = 2\ndf['Fare_range'][(df['Fare'] > 31)] = 3\ndf['Fare_range']= df['Fare_range'].apply(pd.to_numeric)\ndf.drop(['Fare'], axis= 1, inplace= True)\n#train['Fare_range']= train['Fare_range'].astype(int)","eeca1138":"# we need to integer encode the column Sex\nle= LabelEncoder()\ndf['Sex']= le.fit_transform(df['Sex'])\ndf['Embarked']= le.fit_transform(df['Embarked'])\ndf['Title']= le.fit_transform(df['Title'])\n\n# now we need to one-hot encode the columns Embarked, Pclass, Title\n#df= pd.get_dummies(df, prefix=['Embarked'], columns= ['Embarked'])\n#df= pd.get_dummies(df, prefix=['Title'], columns= ['Title'])","79fdbed9":"# let us separate the df dataframe into train & test dataframes now\ntest= df[df['Survived'].isna()]\ntrain= df[df['Survived'].notna()]","188b52a9":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()\n# Most survivors were from 1st passenger class","4a4a4c15":"train[['Family', 'Survived']].groupby(['Family'], as_index=False).mean()","e1cf00cf":"train[['Fare_range', 'Survived']].groupby(['Fare_range'], as_index=False).mean()","882321ac":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","7047d660":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()\n# Most survivors were women","aa83f38b":"train[['Age_scale', 'Survived']].groupby(['Age_scale'], as_index=False).mean()\n# Most survivors were under the age of 18","6168ae7c":"# let us examine the correlation matrix first to check for any extreme cases, multicollinearity etc.\npd.options.display.max_columns = None\ntrain.corr()\n# we can safely assume that high multicollinearity is absent in our data","11438da6":"lr= LogisticRegression()\nsvm= SVC()\nlsvm= LinearSVC()\nsgdc= SGDClassifier()\nrf= RandomForestClassifier(n_estimators= 1000)\nperc= Perceptron(shuffle= True)\nskf= StratifiedKFold(n_splits= 10, shuffle= True)\ndt= DecisionTreeClassifier()\nknn= KNeighborsClassifier()","22de9f65":"target= train['Survived']\ntrain.drop(['Survived'], axis= 1, inplace= True)\ntest.head()","a98c268f":"train.head()","f5fe0151":"scores= {'name':[], 'cl': [], 'r2': []}\nfrom sklearn import model_selection\nresults= model_selection.cross_validate(lr, train, target, cv= skf, scoring= ['accuracy', 'r2'])\nscores['name'].append('Logistic Regression')\nscores['cl'].append(results['test_accuracy'].mean())\nscores['r2'].append(results['test_r2'].mean())\nprint('Classification accuracy using Logistic Regression: %.4f' % (results['test_accuracy'].mean()))\nprint('R squared value using Logistic Regression: %.4f' % (results['test_r2'].mean()))","851dbaa5":"results= model_selection.cross_validate(rf, train, target, cv= skf, scoring= ['accuracy', 'r2'])\nscores['name'].append('Random Forest')\nscores['cl'].append(results['test_accuracy'].mean())\nscores['r2'].append(results['test_r2'].mean())\nprint('Classification accuracy using Random Forest: %.4f' % (results['test_accuracy'].mean()))\nprint('R squared value using Random Forest: %.4f' % (results['test_r2'].mean()))","959f6f14":"results= model_selection.cross_validate(svm, train, target, cv= skf, scoring= ['accuracy', 'r2'])\nscores['name'].append('SVM')\nscores['cl'].append(results['test_accuracy'].mean())\nscores['r2'].append(results['test_r2'].mean())\nprint('Classification accuracy using Support Vector Machines: %.4f' % (results['test_accuracy'].mean()))\nprint('R squared value using  Support Vector Machines: %.4f' % (results['test_r2'].mean()))","9ebbe145":"results= model_selection.cross_validate(perc, train, target, cv= skf, scoring= ['accuracy', 'r2'])\nscores['name'].append('Perceptron')\nscores['cl'].append(results['test_accuracy'].mean())\nscores['r2'].append(results['test_r2'].mean())\nprint('Classification accuracy using Perceptron: %.4f' % (results['test_accuracy'].mean()))\nprint('R squared value using Perceptron: %.4f' % (results['test_r2'].mean()))","e94ff740":"results= model_selection.cross_validate(sgdc, train, target, cv= skf, scoring= ['accuracy', 'r2'])\nscores['name'].append('Stochastic Gradient Descent')\nscores['cl'].append(results['test_accuracy'].mean())\nscores['r2'].append(results['test_r2'].mean())\nprint('Classification accuracy using Stochastic Gradient Descent: %.4f' % (results['test_accuracy'].mean()))\nprint('R squared value using Stochastic Gradient Descent: %.4f' % (results['test_r2'].mean()))","d0afcbc8":"results= model_selection.cross_validate(dt, train, target, cv= skf, scoring= ['accuracy', 'r2'])\nscores['name'].append('Decision Tree')\nscores['cl'].append(results['test_accuracy'].mean())\nscores['r2'].append(results['test_r2'].mean())\nprint('Classification accuracy using Decision Tree: %.4f' % (results['test_accuracy'].mean()))\nprint('R squared value using Decision Tree: %.4f' % (results['test_r2'].mean()))","a257a123":"results= model_selection.cross_validate(knn, train, target, cv= skf, scoring= ['accuracy', 'r2'])\nscores['name'].append('KNN')\nscores['cl'].append(results['test_accuracy'].mean())\nscores['r2'].append(results['test_r2'].mean())\nprint('Classification accuracy using K Nearest Neighbors: %.4f' % (results['test_accuracy'].mean()))\nprint('R squared value using K Nearest Neighbors: %.4f' % (results['test_r2'].mean()))","adeb9044":"scores_df= pd.DataFrame(scores)\nscores_df.sort_values(by=['r2'], ascending= False)\n# we see from the below dataframe that SVM is the best classifier currenty for our purposes","9a046aa5":"test.drop('Survived', axis= 1, inplace= True)","54dd22f0":"test.head()","ac0b2b57":"svm= SVC()\nsvm.fit(train, target)\nscore_svc= svm.predict(test)","0fc5ace6":"sub = pd.DataFrame({\n        \"PassengerId\": test.index,\n        \"Survived\": score_svc\n})\nsub['Survived']= sub['Survived'].astype(int)\nsub.head()\nsub.to_csv('titanic1.csv', index=False)","e079b41b":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\ndef create_link(df, title = \"Download\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_link(sub)","3d132c08":"* Above dataframe gives us the classification accuracies & R squared values corresponding to each model.\n* Perceptron has the worst fitting apparent from it's negative R squared value, which, is statistically\nimpossible but commonly occurs in real world datasets in machine learning. This occurs when the\nmeans\/data distribution of the testing & training sets are completely different.\n* SVM not only gives usthe best classification accuracy, but also yields best model fit. Hence we choose SVM.","2ce4b477":"<h2> Encoding the data <\/h2>","4466c408":"<h2> Analyzing the Data <\/h2>"}}