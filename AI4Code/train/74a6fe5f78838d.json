{"cell_type":{"f7459bc5":"code","52fd250a":"code","0ca51e36":"code","eac3fd31":"code","a99bedda":"code","3ecf910d":"code","f6c7288f":"code","54ec1e9e":"code","5cedfe49":"code","7fbc5d25":"code","5f35cbbe":"code","3a3f7e80":"code","49aef5f3":"code","a832904b":"code","74bd24c5":"code","1e78c8b9":"markdown","0ac3e17d":"markdown","3506bddc":"markdown","0fab1879":"markdown","5d07f35a":"markdown","654adcd2":"markdown","d68e40b2":"markdown"},"source":{"f7459bc5":"import warnings\nwarnings.filterwarnings('ignore')\n!pip install jieba\n!wget -nc \"https:\/\/codeload.github.com\/weiyunchen\/nlp\/zip\/master\"\n!unzip -o master\nimport pandas as pd\nimport os\nimport time\nfrom tqdm import tqdm_notebook\nimport re\nimport time\nimport copy\nimport random\nimport jieba\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\n","52fd250a":"data = pd.read_csv('..\/input\/DMSC.csv', index_col=0)\n\n# \u6309\u8bc4\u5206\u5206\u6210\u4e24\u7c7b\uff0c1\u52062\u5206\u4e3a\u8d1f\u9762\u8bc4\u4ef7,345\u6b63\u9762\ndata['Star']=((data.Star+0.5)\/3.5+1).astype(int)","0ca51e36":"Movie = data['Movie_Name_CN'].value_counts()\nMovie","eac3fd31":"sample_df = data.groupby(['Movie_Name_CN', 'Star']).apply(\n    lambda x: x.sample(n=int(2125056\/(28*200)), replace=True, random_state=0))\n\nsample_df.shape","a99bedda":"from sklearn.model_selection import train_test_split\n\n\ncomments = sample_df.values[:, 7]\nstar = sample_df.values[:, 6]\n\nx_train, x_test, y_train, y_test, = train_test_split(\n    comments, star, test_size=0.2, random_state=0)\n\nlen(y_train), len(y_test), len(x_train), len(x_test)","3ecf910d":"# \u6e05\u7406\u975e\u4e2d\u6587\u5b57\u7b26\ndef clean_str(line):\n    line.strip('\\n')\n    line = re.sub(r\"[^\\u4e00-\\u9fff]\", \"\", line)\n    line = re.sub(\n        \"[0-9a-zA-Z\\-\\s+\\.\\!\\\/_,$%^*\\(\\)\\+(+\\\"\\')]+|[+\u2014\u2014\uff01\uff0c\u3002\uff1f\u3001~@#\uffe5%\u2026\u2026&*\uff08\uff09<>\\[\\]:\uff1a\u2605\u25c6\u3010\u3011\u300a\u300b;\uff1b=?\uff1f]+\", \"\", line)\n    return line.strip()\n\n\n# \u52a0\u8f7d\u505c\u7528\u8bcd\nwith open('nlp-master\/stopwords.txt') as f:\n    stopwords = [line.strip('\\n') for line in f.readlines()]\n\n\ndef cut(data, labels, stopwords):\n    result = []\n    new_labels = []\n    for index in tqdm_notebook(range(len(data))):\n        comment = clean_str(data[index])\n        label = labels[index]\n        # \u5206\u8bcd\n        seg_list = jieba.cut(comment, cut_all=False, HMM=True)\n        seg_list = [x.strip('\\n')\n                    for x in seg_list if x not in stopwords and len(x) > 1]\n        if len(seg_list) > 1:\n            result.append(seg_list)\n            new_labels.append(label)\n    # \u8fd4\u56de\u5206\u8bcd\u7ed3\u679c\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\n    return result, new_labels\n\n# \u5206\u522b\u5bf9\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\u5206\u8bcd\ntrain_cut_result, train_labels = cut(x_train, y_train, stopwords)\ntest_cut_result, test_labels = cut(x_test, y_test, stopwords)","f6c7288f":"# TfidfVectorizer \u4f20\u5165\u539f\u59cb\u6587\u672c\ntrain_data = [' '.join(x) for x in train_cut_result]\ntest_data = [' '.join(x) for x in test_cut_result]\n\nn_dim = 20000\n\n# \u6570\u636e\u7684TF-IDF\u4fe1\u606f\u8ba1\u7b97\n# sublinear_tf=True \u65f6\u751f\u6210\u4e00\u4e2a\u8fd1\u4f3c\u9ad8\u65af\u5206\u5e03\u7684\u7279\u5f81\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5927\u69821~2\u4e2a\u767e\u5206\u70b9\nvectorizer = TfidfVectorizer(\n    max_features=n_dim, smooth_idf=True, sublinear_tf=True)\n\n# \u5bf9\u8bad\u7ec3\u6570\u636e\u8bad\u7ec3\ntrain_vec_data = vectorizer.fit_transform(train_data)\n\n# \u8bad\u7ec3\u5b8c\u6210\u4e4b\u540e\u5bf9\u6d4b\u8bd5\u6570\u636e\u8f6c\u6362\ntest_vec_data = vectorizer.transform(test_data)","54ec1e9e":"vectorizer.get_feature_names()[:10]","5cedfe49":"# \u8f93\u51fa\u7684\u7c7b\u522b\u4e3a 2\nn_categories = 2\n# \u5b66\u4e60\u7387\u8fc7\u5927\u4f1a\u5bfc\u81f4 loss \u9707\u8361\nlearning_rate = 0.001\n# \u635f\u5931\u51fd\u6570\ncriterion = nn.CrossEntropyLoss()\n# \u8fed\u4ee3\u6b21\u6570\nepochs = 3\n# \u6bcf\u6b21\u8fed\u4ee3\u540c\u65f6\u52a0\u8f7d\u7684\u4e2a\u6570\nbatch_size = 100","7fbc5d25":"class TxtDataset(Dataset):\n    def __init__(self, VectData, labels):\n        # \u4f20\u5165\u521d\u59cb\u6570\u636e\uff0c\u7279\u5f81\u5411\u91cf\u548c\u6807\u7b7e\n        self.VectData = VectData\n        self.labels = labels\n\n    def __getitem__(self, index):\n        # DataLoader \u4f1a\u6839\u636e index \u83b7\u53d6\u6570\u636e\n        # toarray() \u662f\u56e0\u4e3a VectData \u662f\u4e00\u4e2a\u7a00\u758f\u77e9\u9635\uff0c\u5982\u679c\u76f4\u63a5\u4f7f\u7528 VectData.toarray() \u5360\u7528\u5185\u5b58\u592a\u5927\uff0c\u52ff\u5c1d\u8bd5\n        return self.VectData[index].toarray(), self.labels[index]-1\n\n    def __len__(self):\n        return len(self.labels)\n\n# \u7ebf\u4e0b\u5185\u5b58\u8db3\u591f\u5927\u53ef\u4ee5\u8003\u8651\u589e\u5927 num_workers\uff0c\u5e76\u884c\u8bfb\u53d6\u6570\u636e\n# \u52a0\u8f7d\u8bad\u7ec3\u6570\u636e\u96c6\ntrain_dataset = TxtDataset(train_vec_data, train_labels)\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=1\n                              )\n# \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\ntest_dataset = TxtDataset(test_vec_data, test_labels)\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=batch_size,\n                             shuffle=False,\n                             num_workers=1\n                             )","5f35cbbe":"class TxtModel(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(TxtModel, self).__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_size, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(1024, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(1024, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(1024, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(512, output_size)\n        )\n\n    def forward(self, x):\n        output = self.classifier(x.double())\n        return output.squeeze(1)","3a3f7e80":"# \u5b9a\u4e49\u6a21\u578b\u548c\u4f18\u5316\u5668\nmodel = TxtModel(n_dim, 2)\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# \u6bcf\u4e24\u4ee3\u8870\u51cf\u5b66\u4e60\u7387\nexp_lr_scheduler = lr_scheduler.StepLR(\n    optimizer, step_size=int(epochs\/2), gamma=0.1)\n \nmodel = model.double()\n\n# \u4fdd\u5b58\u51c6\u786e\u5ea6\u6700\u9ad8\u7684\u6a21\u578b\nbest_model = copy.deepcopy(model)\nbest_accuracy = 0.0\n\nfor epoch in range(epochs):\n    exp_lr_scheduler.step()\n    model.train()\n    loss_total = 0\n    st = time.time()\n    # train_dataloader \u52a0\u8f7d\u6570\u636e\u96c6\n    for data, label in tqdm_notebook(train_dataloader):\n        output = model(data)\n        # \u8ba1\u7b97\u635f\u5931\n        loss = criterion(output, label)\n        optimizer.zero_grad()\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n        optimizer.step()\n        loss_total += loss.item()\n\n    # \u8f93\u51fa\u635f\u5931\u3001\u8bad\u7ec3\u65f6\u95f4\u7b49\n    print('epoch {}\/{}:'.format(epoch, epochs))\n    print('training loss: {}, time resumed {}s'.format(\n        loss_total\/len(train_dataset), time.time()-st))\n\n    model.eval()\n\n    loss_total = 0\n    st = time.time()\n\n    correct = 0\n    for data, label in test_dataloader:\n        output = model(data)\n        loss = criterion(output, label)\n        loss_total += loss.item()\n\n        _, predicted = torch.max(output.data, 1)\n        correct += (predicted == label).sum().item()\n    # \u5982\u679c\u51c6\u786e\u5ea6\u53d6\u5f97\u6700\u9ad8\uff0c\u5219\u4fdd\u5b58\u51c6\u786e\u5ea6\u6700\u9ad8\u7684\u6a21\u578b\n    if correct\/len(test_dataset) > best_accuracy:\n        best_model = copy.deepcopy(model)\n\n    print('testing loss: {}, time resumed {}s, accuracy: {}'.format(\n        loss_total\/len(test_dataset), time.time()-st, correct\/len(test_dataset)))","49aef5f3":"import json\nimport requests\n# 26266893 \u4e3a\u56fd\u4ea7\u79d1\u5e7b\u4f73\u4f5c\u300a\u6d41\u6d6a\u5730\u7403\u300b\uff0c\u5728\u6b64\u4ee5\u300a\u6d41\u6d6a\u5730\u7403\u300b\u7684\u5f71\u8bc4\u4e3a\u4f8b\nres = requests.get(\n    'https:\/\/api.douban.com\/v2\/movie\/subject\/26266893\/comments?apikey=0df993c66c0c636e29ecbb5344252a4a')\ncomments = json.loads(res.content.decode('utf-8'))['comments']\ncomments","a832904b":"def predict_comments(comments):\n    test_comment = random.choice(comments)\n# \u9009\u62e9\u5176\u4e2d\u4e00\u6761\u5206\u7c7b\uff0c\u5e76\u53bb\u9664\u975e\u4e2d\u6587\u5b57\u7b26\n    content = clean_str(test_comment['content'])\n    rating = test_comment['rating']['value']\n# \u5bf9\u8bc4\u8bba\u5206\u8bcd\n    seg_list = jieba.cut(content, cut_all=False, HMM=True)\n# \u53bb\u6389\u505c\u7528\u8bcd\u548c\u65e0\u610f\u4e49\u7684\n    cut_content = ' '.join([x.strip('\\n')\n                        for x in seg_list if x not in stopwords and len(x) > 1])\n\n# \u8f6c\u5316\u4e3a\u7279\u5f81\u5411\u91cf\n    one_test_data = vectorizer.transform([cut_content])\n\n# \u8f6c\u5316\u4e3a pytorch \u8f93\u5165\u7684 Tensor \u6570\u636e\uff0csqueeze(0) \u589e\u52a0\u4e00\u4e2a batch \u7ef4\u5ea6\n    one_test_data = torch.from_numpy(one_test_data.toarray()).unsqueeze(0)\n# \u4f7f\u7528\u51c6\u786e\u5ea6\u6700\u597d\u7684\u6a21\u578b\u9884\u6d4b\uff0csoftmax \u5904\u7406\u8f93\u51fa\u6982\u7387\uff0c\u53d6\u5f97\u6700\u5927\u6982\u7387\u7684\u4e0b\u6807\u518d\u52a0 1 \u5219\u4e3a\u9884\u6d4b\u7684\u6807\u7b7e\n    pred = torch.argmax(F.softmax(best_model(one_test_data), dim=1)) + 1\n    if rating<3:\n        rat='\u5dee\u8bc41'\n    else:\n        rat='\u597d\u8bc42'\n    print('\u8bc4\u8bba\u5185\u5bb9: ',content)\n    print('\u5173\u952e\u5b57: ',cut_content)\n    print('\u89c2\u4f17\u8bc4\u4ef7: ',rat)\n    print('\u9884\u6d4b\u8bc4\u4ef7: ',pred)","74bd24c5":"for i in range(5):\n    print('\u89c2\u540e\u611f: ',i)\n    print(predict_comments(comments))","1e78c8b9":"\u65e0\u610f\u4e2d\u770b\u5230\u4e86\u8fd9\u4e2a\u6570\u636e\u96c6\uff0c\u6b63\u597d\u7ec3\u4e00\u7ec3NLP\u3002\u6211\u5c06\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5efa\u7acbNLP\u7684\u6a21\u578b\u5e76\u8fdb\u884c\u6d4b\u8bd5\uff0c\u672c\u5b9e\u9a8c\u6682\u65f6\u5206\u4e3a\u4ee5\u4e0b\u51e0\u90e8\u5206\uff1a\n- NN\n- LSTM","0ac3e17d":"### \u6d4b\u8bd5\u6a21\u578b","3506bddc":"\u53ef\u4ee5\u770b\u5230\uff0c\u6211\u4eec\u7528\u7b80\u5355\u7684\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u7684\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u7531\u4e8ekaggle\u7ebf\u4e0a\u5e73\u53f0\u7684\u914d\u7f6e\u6709\u9650\uff0c\u6211\u4eec\u53ea\u662f\u968f\u673a\u62bd\u53d6\u4e86\u5f88\u5c11\u7684\u6570\u636e\u6765\u5efa\u6a21\u3002\u5728\u4f7f\u7528\u5168\u90e8200\u591a\u4e07\u6761\u6570\u636e\u5efa\u6a21\u65f6\uff0c\u5728\u6211\u81ea\u5df1\u7684\u7535\u8111\u4e0a\u5f97\u5230\u7684\u6a21\u578b\u9884\u6d4b\u6548\u679c\u6bd4\u7ebf\u4e0a\u7684\u6548\u679c\u597d\u975e\u5e38\u591a\uff0c\u51c6\u786e\u7387\u53ef\u4ee5\u8fbe\u523090%\u5de6\u53f3\u3002\u540e\u7eed\u6211\u4eec\u8fd8\u4f1a\u5efa\u7acb\u590d\u6742\u7684\u6a21\u578b\u3002","0fab1879":"# \u8c46\u74e3\u7535\u5f71\u60c5\u611f\u5206\u6790","5d07f35a":"## 1\u3001NN","654adcd2":"\u7531\u4e8ekaggle\u7ebf\u4e0a\u73af\u5883\u7684\u8ba1\u7b97\u80fd\u529b\u6709\u9650\uff0c\u82e5\u4f7f\u7528\u6570\u636e\u96c6\u7684\u5168\u90e8\u6570\u636e\u9700\u8981\u8bad\u7ec3\u975e\u5e38\u4e45\u7684\u65f6\u95f4\uff0c\u56e0\u6b64\u7ebf\u4e0a\u6a21\u578b\u7684\u4e00\u4e9b\u53c2\u6570\u8bbe\u7f6e\u4ec5\u4ec5\u662f\u4e3a\u4e86\u5feb\u901f\u8dd1\u5b8c\u6a21\u578b\u548c\u7b80\u5355\u770b\u4e0b\u6548\u679c\uff0c\u4e0d\u5177\u6709\u5b9e\u7528\u7684\u610f\u4e49\u3002\u6211\u5c06\u9644\u4e0a\u6211\u5728\u7ebf\u4e0b\u6d4b\u8bd5\u7684\u4e00\u4e9b\u56fe\u7247\u548c\u4fe1\u606f\u3002\n\n\u7ebf\u4e0b\u4f7f\u7528\u4e86\u5b8c\u6574\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u7684\u51c6\u786e\u7387\u4e3a90%\u5de6\u53f3\uff1a\n![](https:\/\/upload-images.jianshu.io\/upload_images\/15406304-8e87834cdd967e70.png?imageMogr2\/auto-orient\/strip%7CimageView2\/2\/w\/1240)","d68e40b2":"\u7ebf\u4e0b\u7535\u8111\u7684\u914d\u7f6e\u5982\u4e0b\uff1a\n![](https:\/\/upload-images.jianshu.io\/upload_images\/15406304-d1031654488a5dc3.png?imageMogr2\/auto-orient\/strip%7CimageView2\/2\/w\/1240)\n![](https:\/\/upload-images.jianshu.io\/upload_images\/15406304-7d4426c0b3558251.png?imageMogr2\/auto-orient\/strip%7CimageView2\/2\/w\/1240)"}}