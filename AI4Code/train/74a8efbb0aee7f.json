{"cell_type":{"dc05696d":"code","318fdedb":"code","44d29fdb":"code","cbb9ac68":"code","db07d376":"code","959443cb":"code","fe02928a":"code","bc4aab91":"code","baa5115c":"code","0212359e":"code","baf65ad0":"code","d7939068":"code","60eb7f06":"code","8dd1ac47":"code","a36f9a57":"code","a9fa4549":"code","e40aba6f":"code","b51b3f60":"code","8618fa47":"code","ffd198a0":"code","90f51059":"code","e9b83e7a":"code","bda3bba0":"code","912a7da0":"code","d6b6c02b":"code","e1aadfdd":"code","937e4028":"code","1d92f41f":"code","6dbaec9b":"code","9d22bf69":"code","5664c87b":"code","92fcfa0d":"code","7ec12457":"code","f4c3de61":"code","b1271b37":"code","e31bb59b":"code","6b8a75e2":"code","1418b81d":"code","da086c84":"code","136fca7b":"code","15f0f329":"code","796dd033":"code","38d22f1e":"code","0e7b11fc":"code","4a8f6ce9":"code","e79842c5":"code","4cb2d3d7":"code","812e8ba7":"code","7c6c866f":"code","f4b6b5d1":"code","4b1eb67a":"code","d342908d":"code","de77a64d":"code","30921c1d":"code","d154c949":"code","8ad0402c":"code","b9116a36":"code","c1744b16":"code","ec2afd46":"code","4d8758a0":"code","99d68ab1":"code","44f11555":"code","b12bd121":"code","db31263a":"code","ace96457":"code","d81bea4e":"code","8bc533ed":"code","70431732":"code","f71c9cff":"code","40623470":"code","28c96e66":"code","fe6ed40c":"code","6bff8485":"code","8e19c2e6":"code","e1c1a55a":"code","c4b41fc5":"code","6c2ea070":"code","2a92b171":"code","281df8aa":"code","507d0d70":"code","42ffb2a1":"code","df77b587":"code","899b8d86":"code","3a4e737b":"code","061498bf":"code","5c8d87ad":"code","4fa2e344":"code","895ee21f":"code","ed301d92":"code","2d097368":"code","606849c0":"code","1c6ae6d2":"code","7f84a037":"code","8fca669c":"code","a20787a0":"code","d73e08e9":"code","0ebb8f84":"code","2e8eafe1":"code","f220e025":"code","600fdb18":"code","d741bf31":"code","89d35ae2":"code","27e95fe2":"code","ead5dc88":"code","2ba54c65":"markdown","fa263f49":"markdown","913bc0d9":"markdown","e5ac4111":"markdown","8497073a":"markdown","f362f93e":"markdown","a280b0a8":"markdown","f5ad7458":"markdown","4d5f4375":"markdown","8f22df66":"markdown","d8d1da0b":"markdown","af9ab767":"markdown","7f020ef5":"markdown","047695e0":"markdown","dab17b2b":"markdown","8c939bb5":"markdown","e9763286":"markdown","4a57cf8b":"markdown","214578a5":"markdown","6cbede35":"markdown","729b4950":"markdown","7ce865de":"markdown","61c44de2":"markdown","ebc148bd":"markdown","171ba6b3":"markdown","b1cde00e":"markdown","4f5cff9b":"markdown","1aa24a92":"markdown","7a1b4e41":"markdown","7d3d4d6c":"markdown","4f22f24a":"markdown","64a63021":"markdown","cc369642":"markdown","cd5c367f":"markdown","367be584":"markdown","a58beee5":"markdown","836655ba":"markdown","6bc6a94d":"markdown","1440ebbb":"markdown","6983b706":"markdown","44f243df":"markdown","f1deafab":"markdown","a5bb44a0":"markdown","e9fae21a":"markdown","c354fc7c":"markdown","393587b1":"markdown","d050015f":"markdown","41f1778e":"markdown","8bd9f02c":"markdown","034fa19d":"markdown","1de2e9eb":"markdown","96ab0d50":"markdown","2a455073":"markdown"},"source":{"dc05696d":"import numpy as np \nimport pandas as pd \nimport os\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostRegressor\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)","318fdedb":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', parse_dates=['first_active_month'])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=['first_active_month'])\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')","44d29fdb":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","cbb9ac68":"e = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='train')\ne","db07d376":"train['feature_1'] = train['feature_1'].astype('category')\ntrain['feature_2'] = train['feature_2'].astype('category')\ntrain['feature_3'] = train['feature_3'].astype('category')\ntrain.head()","959443cb":"train.info()","fe02928a":"fig, ax = plt.subplots(1, 3, figsize = (16, 6))\nplt.suptitle('Violineplots for features and target');\nsns.violinplot(x=\"feature_1\", y=\"target\", data=train, ax=ax[0], title='feature_1');\nsns.violinplot(x=\"feature_2\", y=\"target\", data=train, ax=ax[1], title='feature_2');\nsns.violinplot(x=\"feature_3\", y=\"target\", data=train, ax=ax[2], title='feature_3');","bc4aab91":"fig, ax = plt.subplots(1, 3, figsize = (16, 6));\ntrain['feature_1'].value_counts().sort_index().plot(kind='bar', ax=ax[0], color='teal', title='feature_1');\ntrain['feature_2'].value_counts().sort_index().plot(kind='bar', ax=ax[1], color='brown', title='feature_2');\ntrain['feature_3'].value_counts().sort_index().plot(kind='bar', ax=ax[2], color='gold', title='feature_3');\nplt.suptitle('Counts of categiories for features');","baa5115c":"test['feature_1'] = test['feature_1'].astype('category')\ntest['feature_2'] = test['feature_2'].astype('category')\ntest['feature_3'] = test['feature_3'].astype('category')","0212359e":"d1 = train['first_active_month'].value_counts().sort_index()\nd2 = test['first_active_month'].value_counts().sort_index()\ndata = [go.Scatter(x=d1.index, y=d1.values, name='train'), go.Scatter(x=d2.index, y=d2.values, name='test')]\nlayout = go.Layout(dict(title = \"Counts of first active\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Count'),\n                  ),legend=dict(\n                orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","baf65ad0":"test.loc[test['first_active_month'].isna(), 'first_active_month'] = test.loc[(test['feature_1'] == 5) & (test['feature_2'] == 2) & (test['feature_3'] == 1), 'first_active_month'].min()","d7939068":"plt.hist(train['target']);\nplt.title('Target distribution');","60eb7f06":"print('There are {0} samples with target lower than -20.'.format(train.loc[train.target < -20].shape[0]))","8dd1ac47":"max_date = train['first_active_month'].dt.date.max()\ndef process_main(df):\n    date_parts = [\"year\", \"weekday\", \"month\"]\n    for part in date_parts:\n        part_col = 'first_active_month' + \"_\" + part\n        df[part_col] = getattr(df['first_active_month'].dt, part).astype(int)\n            \n    df['elapsed_time'] = (max_date - df['first_active_month'].dt.date).dt.days\n    \n    return df","a36f9a57":"train = process_main(train)\ntest = process_main(test)","a9fa4549":"historical_transactions = pd.read_csv('..\/input\/historical_transactions.csv')\ne = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='history')\ne","e40aba6f":"print(f'{historical_transactions.shape[0]} samples in data')\nhistorical_transactions.head()","b51b3f60":"# let's convert the authorized_flag to a binary value.\nhistorical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)","8618fa47":"print(f\"At average {historical_transactions['authorized_flag'].mean() * 100:.4f}% transactions are authorized\")\nhistorical_transactions['authorized_flag'].value_counts().plot(kind='barh', title='authorized_flag value counts');","ffd198a0":"autorized_card_rate = historical_transactions.groupby(['card_id'])['authorized_flag'].mean().sort_values()\nautorized_card_rate.head()","90f51059":"autorized_card_rate.tail()","e9b83e7a":"historical_transactions['installments'].value_counts()","bda3bba0":"historical_transactions.groupby(['installments'])['authorized_flag'].mean()","912a7da0":"historical_transactions['installments'] = historical_transactions['installments'].astype('category')","d6b6c02b":"historical_transactions['purchase_date'] = pd.to_datetime(historical_transactions['purchase_date'])","e1aadfdd":"plt.title('Purchase amount distribution.');\nhistorical_transactions['purchase_amount'].plot(kind='hist');","937e4028":"for i in [-1, 0]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = historical_transactions.loc[historical_transactions['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")","1d92f41f":"plt.title('Purchase amount distribution for negative values.');\nhistorical_transactions.loc[historical_transactions['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');","6dbaec9b":"map_dict = {'Y': 0, 'N': 1}\nhistorical_transactions['category_1'] = historical_transactions['category_1'].apply(lambda x: map_dict[x])\nhistorical_transactions.groupby(['category_1']).agg({'purchase_amount': ['mean', 'std', 'count'], 'authorized_flag': ['mean', 'std']})","9d22bf69":"historical_transactions.groupby(['category_2']).agg({'purchase_amount': ['mean', 'std', 'count'], 'authorized_flag': ['mean', 'std']})","5664c87b":"map_dict = {'A': 0, 'B': 1, 'C': 2, 'nan': 3}\nhistorical_transactions['category_3'] = historical_transactions['category_3'].apply(lambda x: map_dict[str(x)])\nhistorical_transactions.groupby(['category_3']).agg({'purchase_amount': ['mean', 'std', 'count'], 'authorized_flag': ['mean', 'std']})","92fcfa0d":"for col in ['city_id', 'merchant_category_id', 'merchant_id', 'state_id', 'subsector_id']:\n    print(f\"There are {historical_transactions[col].nunique()} unique values in {col}.\")","7ec12457":"def aggregate_historical_transactions(trans, prefix):\n    trans['purchase_month'] = trans['purchase_date'].dt.month\n    trans['installments'] = trans['installments'].astype(int)\n    trans['month_diff'] = ((datetime.datetime.today() - trans['purchase_date']).dt.days)\/\/30\n    trans['month_diff'] += trans['month_lag']\n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']). \\\n                                        astype(np.int64) * 1e-9\n    trans = pd.get_dummies(trans, columns=['category_2', 'category_3'])\n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean', 'sum'],\n        'category_2_2.0': ['mean', 'sum'],\n        'category_2_3.0': ['mean', 'sum'],\n        'category_2_4.0': ['mean', 'sum'],\n        'category_2_5.0': ['mean', 'sum'],\n        'category_3_1': ['sum', 'mean'],\n        'category_3_2': ['sum', 'mean'],\n        'category_3_3': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'month_lag': ['min', 'max'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'city_id': ['nunique'],\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n\n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n\n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n\n    return agg_trans","f4c3de61":"def aggregate_per_month(history):\n    grouped = history.groupby(['card_id', 'month_lag'])\n    history['installments'] = history['installments'].astype(int)\n    agg_func = {\n            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            'installments': ['count', 'sum', 'mean', 'min', 'max', 'std'],\n            }\n\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True)\n    \n    return final_group\n\nfinal_group = aggregate_per_month(historical_transactions) ","b1271b37":"%%time\ndel d1, d2, autorized_card_rate\ngc.collect()\nhistorical_transactions = reduce_mem_usage(historical_transactions)\nhistory = aggregate_historical_transactions(historical_transactions, prefix='hist_')\ngc.collect()\ntrain = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\ndel history","e31bb59b":"del historical_transactions\ngc.collect()","6b8a75e2":"new_merchant_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv')\ne = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='new_merchant_period')\ne","1418b81d":"print(f'{new_merchant_transactions.shape[0]} samples in data')\nnew_merchant_transactions.head()","da086c84":"# let's convert the authorized_flag to a binary value.\nnew_merchant_transactions['authorized_flag'] = new_merchant_transactions['authorized_flag'].apply(lambda x: 1 if x == 'Y' else 0)","136fca7b":"print(f\"At average {new_merchant_transactions['authorized_flag'].mean() * 100:.4f}% transactions are authorized\")\nnew_merchant_transactions['authorized_flag'].value_counts().plot(kind='barh', title='authorized_flag value counts');","15f0f329":"card_total_purchase = new_merchant_transactions.groupby(['card_id'])['purchase_amount'].sum().sort_values()\ncard_total_purchase.head()","796dd033":"card_total_purchase.tail()","38d22f1e":"new_merchant_transactions['installments'].value_counts()","0e7b11fc":"new_merchant_transactions.groupby(['installments'])['purchase_amount'].sum()","4a8f6ce9":"new_merchant_transactions['installments'] = new_merchant_transactions['installments'].astype('category')","e79842c5":"plt.title('Purchase amount distribution.');\nnew_merchant_transactions['purchase_amount'].plot(kind='hist');","4cb2d3d7":"for i in [-1, 0]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] < i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount less than {i}.\")\nfor i in [0, 10, 100]:\n    n = new_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] > i].shape[0]\n    print(f\"There are {n} transactions with purchase_amount more than {i}.\")","812e8ba7":"plt.title('Purchase amount distribution for negative values.');\nnew_merchant_transactions.loc[new_merchant_transactions['purchase_amount'] < 0, 'purchase_amount'].plot(kind='hist');","7c6c866f":"map_dict = {'Y': 0, 'N': 1}\nnew_merchant_transactions['category_1'] = new_merchant_transactions['category_1'].apply(lambda x: map_dict[x])\nnew_merchant_transactions.groupby(['category_1']).agg({'purchase_amount': ['mean', 'std', 'count']})","f4b6b5d1":"new_merchant_transactions.groupby(['category_2']).agg({'purchase_amount': ['mean', 'std', 'count']})","4b1eb67a":"map_dict = {'A': 0, 'B': 1, 'C': 2, 'nan': 3}\nnew_merchant_transactions['category_3'] = new_merchant_transactions['category_3'].apply(lambda x: map_dict[str(x)])\nnew_merchant_transactions.groupby(['category_3']).agg({'purchase_amount': ['mean', 'std', 'count']})","d342908d":"for col in ['city_id', 'merchant_category_id', 'merchant_id', 'state_id', 'subsector_id']:\n    print(f\"There are {new_merchant_transactions[col].nunique()} unique values in {col}.\")","de77a64d":"new_merchant_transactions['purchase_date'] = pd.to_datetime(new_merchant_transactions['purchase_date'])","30921c1d":"def aggregate_historical_transactions(trans, prefix):\n    trans['purchase_month'] = trans['purchase_date'].dt.month\n    trans['month_diff'] = ((datetime.datetime.today() - trans['purchase_date']).dt.days)\/\/30\n    trans['month_diff'] += trans['month_lag']\n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).astype(np.int64) * 1e-9\n    trans['installments'] = trans['installments'].astype(int)\n    trans = pd.get_dummies(trans, columns=['category_2', 'category_3'])\n    agg_func = {\n        'category_1': ['sum', 'mean'],\n        'category_2_1.0': ['mean', 'sum'],\n        'category_2_2.0': ['mean', 'sum'],\n        'category_2_3.0': ['mean', 'sum'],\n        'category_2_4.0': ['mean', 'sum'],\n        'category_2_5.0': ['mean', 'sum'],\n        'category_3_1': ['sum', 'mean'],\n        'category_3_2': ['sum', 'mean'],\n        'category_3_3': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std'],\n        'purchase_month': ['mean', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp, 'max', 'min'],\n        'month_lag': ['min', 'max'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'city_id': ['nunique'],\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n\n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n\n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n\n    return agg_trans","d154c949":"%%time\ngc.collect()\nnew_transactions = reduce_mem_usage(new_merchant_transactions)\nhistory = aggregate_historical_transactions(new_merchant_transactions, prefix='new')\ndel new_merchant_transactions\ngc.collect()\ntrain = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\ndel history\ngc.collect()","8ad0402c":"train = pd.merge(train, final_group, on='card_id')\ntest = pd.merge(test, final_group, on='card_id')\ngc.collect()\ndel final_group","b9116a36":"merchants = pd.read_csv('..\/input\/merchants.csv')\ne = pd.read_excel('..\/input\/Data_Dictionary.xlsx', sheet_name='merchant')\ne","c1744b16":"print(f'{merchants.shape[0]} merchants in data')\nmerchants.head()","ec2afd46":"# encoding categories.\nmap_dict = {'Y': 0, 'N': 1}\nmerchants['category_1'] = merchants['category_1'].apply(lambda x: map_dict[x])\nmerchants.loc[merchants['category_2'].isnull(), 'category_2'] = 0\nmerchants['category_4'] = merchants['category_4'].apply(lambda x: map_dict[x])","4d8758a0":"merchants['merchant_category_id'].nunique(), merchants['merchant_group_id'].nunique()","99d68ab1":"plt.hist(merchants['numerical_1']);\nplt.title('Distribution of numerical_1');","44f11555":"np.percentile(merchants['numerical_1'], 95)","b12bd121":"plt.hist(merchants.loc[merchants['numerical_1'] < 0.1, 'numerical_1']);\nplt.title('Distribution of numerical_1 less than 0.1');","db31263a":"min_n1 = merchants['numerical_1'].min()\n_ = sum(merchants['numerical_1'] == min_n1) \/ merchants['numerical_1'].shape[0]\nprint(f'{_ * 100:.4f}% of values in numerical_1 are equal to {min_n1}')","ace96457":"plt.hist(merchants['numerical_2']);\nplt.title('Distribution of numerical_2');","d81bea4e":"plt.hist(merchants.loc[merchants['numerical_2'] < 0.1, 'numerical_2']);\nplt.title('Distribution of numerical_2 less than 0.1');\nmin_n1 = merchants['numerical_1'].min()\n_ = sum(merchants['numerical_1'] == min_n1) \/ merchants['numerical_1'].shape[0]\nprint(f'{_ * 100:.4f}% of values in numerical_1 are equal to {min_n1}')","8bc533ed":"(merchants['numerical_1'] != merchants['numerical_2']).sum() \/ merchants.shape[0]","70431732":"merchants['most_recent_sales_range'].value_counts().plot('bar');","f71c9cff":"d = merchants['most_recent_sales_range'].value_counts().sort_index()\ne = merchants.loc[merchants['numerical_2'] < 0.1].groupby('most_recent_sales_range')['numerical_1'].mean()\ndata = [go.Bar(x=d.index, y=d.values, name='counts'), go.Scatter(x=e.index, y=e.values, name='mean numerical_1', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Counts of values in categories of most_recent_sales_range\",\n                        xaxis = dict(title = 'most_recent_sales_range'),\n                        yaxis = dict(title = 'Counts'),\n                        yaxis2=dict(title='mean numerical_1', overlaying='y', side='right')),\n                   legend=dict(orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","40623470":"d = merchants['most_recent_purchases_range'].value_counts().sort_index()\ne = merchants.loc[merchants['numerical_2'] < 0.1].groupby('most_recent_purchases_range')['numerical_1'].mean()\ndata = [go.Bar(x=d.index, y=d.values, name='counts'), go.Scatter(x=e.index, y=e.values, name='mean numerical_1', yaxis='y2')]\nlayout = go.Layout(dict(title = \"Counts of values in categories of most_recent_purchases_range\",\n                        xaxis = dict(title = 'most_recent_purchases_range'),\n                        yaxis = dict(title = 'Counts'),\n                        yaxis2=dict(title='mean numerical_1', overlaying='y', side='right')),\n                   legend=dict(orientation=\"v\"))\npy.iplot(dict(data=data, layout=layout))","28c96e66":"plt.hist(merchants['avg_sales_lag3'].fillna(0));\nplt.hist(merchants['avg_sales_lag6'].fillna(0));\nplt.hist(merchants['avg_sales_lag12'].fillna(0));","fe6ed40c":"for col in ['avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12']:\n    print(f'Max value of {col} is {merchants[col].max()}')\n    print(f'Min value of {col} is {merchants[col].min()}')","6bff8485":"plt.hist(merchants.loc[(merchants['avg_sales_lag12'] < 3) & (merchants['avg_sales_lag12'] > -10), 'avg_sales_lag12'].fillna(0), label='avg_sales_lag12');\nplt.hist(merchants.loc[(merchants['avg_sales_lag6'] < 3) & (merchants['avg_sales_lag6'] > -10), 'avg_sales_lag6'].fillna(0), label='avg_sales_lag6');\nplt.hist(merchants.loc[(merchants['avg_sales_lag3'] < 3) & (merchants['avg_sales_lag3'] > -10), 'avg_sales_lag3'].fillna(0), label='avg_sales_lag3');\nplt.legend();","8e19c2e6":"merchants['avg_purchases_lag3'].nlargest()","e1c1a55a":"merchants.loc[merchants['avg_purchases_lag3'] == np.inf, 'avg_purchases_lag3'] = 6000\nmerchants.loc[merchants['avg_purchases_lag6'] == np.inf, 'avg_purchases_lag6'] = 6000\nmerchants.loc[merchants['avg_purchases_lag12'] == np.inf, 'avg_purchases_lag12'] = 6000","c4b41fc5":"plt.hist(merchants['avg_purchases_lag3'].fillna(0));\nplt.hist(merchants['avg_purchases_lag6'].fillna(0));\nplt.hist(merchants['avg_purchases_lag12'].fillna(0));","6c2ea070":"plt.hist(merchants.loc[(merchants['avg_purchases_lag12'] < 4), 'avg_purchases_lag12'].fillna(0), label='avg_purchases_lag12');\nplt.hist(merchants.loc[(merchants['avg_purchases_lag6'] < 4), 'avg_purchases_lag6'].fillna(0), label='avg_purchases_lag6');\nplt.hist(merchants.loc[(merchants['avg_purchases_lag3'] < 4), 'avg_purchases_lag3'].fillna(0), label='avg_purchases_lag3');\nplt.legend();","2a92b171":"train.head()","281df8aa":"for col in train.columns:\n    if train[col].isna().any():\n        train[col] = train[col].fillna(0)","507d0d70":"for col in test.columns:\n    if test[col].isna().any():\n        test[col] = test[col].fillna(0)","42ffb2a1":"y = train['target']","df77b587":"col_to_drop = ['first_active_month', 'card_id', 'target']","899b8d86":"for col in col_to_drop:\n    if col in train.columns:\n        train.drop([col], axis=1, inplace=True)\n    if col in test.columns:\n        test.drop([col], axis=1, inplace=True)","3a4e737b":"train['feature_3'] = train['feature_3'].astype(int)\ntest['feature_3'] = test['feature_3'].astype(int)","061498bf":"categorical_feats = ['feature_1', 'feature_2']\n\nfor col in categorical_feats:\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","5c8d87ad":"train.head()","4fa2e344":"for col in ['newpurchase_amount_max', 'newpurchase_date_max', 'purchase_amount_max_mean']:\n    train[col + '_to_mean'] = train[col] \/ train[col].mean()\n    test[col + '_to_mean'] = test[col] \/ test[col].mean()","895ee21f":"X = train\nX_test = test","ed301d92":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)","2d097368":"def train_model(X=X, X_test=X_test, y=y, params=None, folds=folds, model_type='lgb', plot_feature_importance=False):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n                    verbose=1000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n            \n        if model_type == 'xgb':\n            train_data = xgb.DMatrix(data=X_train, label=y_train)\n            valid_data = xgb.DMatrix(data=X_valid, label=y_valid)\n\n            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n            y_pred_valid = model.predict(xgb.DMatrix(X_valid), ntree_limit=model.best_ntree_limit)\n            y_pred = model.predict(xgb.DMatrix(X_test), ntree_limit=model.best_ntree_limit)\n            \n        if model_type == 'rcv':\n            model = RidgeCV(alphas=(0.1, 1.0, 10.0, 100.0), scoring='neg_mean_squared_error', cv=3)\n            model.fit(X_train, y_train)\n            print(model.alpha_)\n\n            y_pred_valid = model.predict(X_valid)\n            score = mean_squared_error(y_valid, y_pred_valid) ** 0.5\n            print(f'Fold {fold_n}. RMSE: {score:.4f}.')\n            print('')\n            \n            y_pred = model.predict(X_test)\n            \n        if model_type == 'cat':\n            model = CatBoostRegressor(iterations=20000,  eval_metric='RMSE', **params)\n            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n\n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_squared_error(y_valid, y_pred_valid) ** 0.5)\n        \n        prediction += y_pred    \n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","606849c0":"params = {'num_leaves': 54,\n         'min_data_in_leaf': 79,\n         'objective': 'regression',\n         'max_depth': 7,\n         'learning_rate': 0.018545526395058548,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8354507676881442,\n         \"bagging_freq\": 3,\n         \"bagging_fraction\": 0.8126672064208567,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         'min_child_weight': 5.343384366323818,\n         'reg_alpha': 1.1302650970728192,\n         'reg_lambda': 0.3603427518866501,\n         'subsample': 0.8767547959893627,}","1c6ae6d2":"oof_lgb, prediction_lgb, feature_importance = train_model(params=params, model_type='lgb', plot_feature_importance=True)","7f84a037":"submission['target'] = prediction_lgb\nsubmission.to_csv('lgb.csv', index=False)","8fca669c":"xgb_params = {'eta': 0.01, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True, 'nthread': 4}\noof_xgb, prediction_xgb = train_model(params=xgb_params, model_type='xgb')","a20787a0":"submission['target'] = prediction_xgb\nsubmission.to_csv('xgb.csv', index=False)","d73e08e9":"oof_rcv, prediction_rcv = train_model(params=None, model_type='rcv')","0ebb8f84":"submission['target'] = prediction_rcv\nsubmission.to_csv('rcv.csv', index=False)","2e8eafe1":"cat_params = {'learning_rate': 0.01,\n              'depth': 9,\n              'l2_leaf_reg': 10,\n              'bootstrap_type': 'Bernoulli',\n              #'metric_period': 500,\n              'od_type': 'Iter',\n              'od_wait': 50,\n              'random_seed': 11,\n              'allow_writing_files': False}\noof_cat, prediction_cat = train_model(params=cat_params, model_type='cat')","f220e025":"submission['target'] = (prediction_lgb + prediction_xgb + prediction_rcv + prediction_cat) \/ 4\nsubmission.to_csv('blend.csv', index=False)","600fdb18":"train_stack = np.vstack([oof_lgb, oof_xgb, oof_rcv, oof_cat]).transpose()\ntrain_stack = pd.DataFrame(train_stack)\ntest_stack = np.vstack([prediction_lgb, prediction_xgb, prediction_rcv, prediction_cat]).transpose()\ntest_stack = pd.DataFrame(test_stack)","d741bf31":"oof_lgb_stack, prediction_lgb_stack = train_model(X=train_stack, X_test=test_stack, params=params, model_type='lgb')","89d35ae2":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = prediction_lgb_stack\nsample_submission.to_csv('stacker_lgb.csv', index=False)","27e95fe2":"oof_rcv_stack, prediction_rcv_stack = train_model(X=train_stack, X_test=test_stack, params=None, model_type='rcv')","ead5dc88":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = prediction_rcv_stack\nsample_submission.to_csv('stacker_rcv.csv', index=False)","2ba54c65":"## historical_transactions\nUp to 3 months' worth of historical transactions for each card_id","fa263f49":"We can see that these ranges have different counts and different mean value of numerical_1 even after removing outliers.","913bc0d9":"### avg_sales_lag","e5ac4111":"### date","8497073a":"We even have infinite values...","f362f93e":"Interesting. Most common number of installments are 0 and 1 which is expected. But -1 and 999 are strange. I think that these values were used to fill in missing values.","a280b0a8":"And they have 1 unique value: -33.21928095.\nThis seems to be a special case. Maybe it would be reasonable to simply exclude these samples. We'll try later.","f5ad7458":"### avg_purchases_lag","4d5f4375":"In contrast with historical data, **all** transactions here were authorized!","8f22df66":"#### Code for training models","d8d1da0b":"Distribution of these values is quite similar and most values are between 0 and 2.","af9ab767":"Also there is one line with a missing data in test. I'll fill in with the first data, having the same values of features.","7f020ef5":"### Feature engineering","047695e0":"These two plots show an important idea: while different categories of these features could have various counts, the distribution of target is almost the same. This could mean, that these features aren't really good at predicting target - we'll need other features and feature engineering.\nAlso it is worth noticing that mean target values of each catogory of these features is near zero. This could mean that data was sampled from normal distribution.","dab17b2b":"In fact more than a half values are equal to minimum value. A very skewered distribution.","8c939bb5":"It seems that there are some cards, for which most of transactions were declined. Were this fraud transactions?","e9763286":"These two variables seem to be quite similar.","4a57cf8b":"## new_merchant_transactions \nTwo months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.","214578a5":"### installments","6cbede35":"> most_recent_sales_range \tmost_recent_purchases_range \tavg_sales_lag3 \tavg_purchases_lag3 \tactive_months_lag3 \tavg_sales_lag6 \tavg_purchases_lag6 \tactive_months_lag6 \tavg_sales_lag12 \tavg_purchases_lag12 \tactive_months_lag12","729b4950":"We have a date column, three anonymized categorical columns and target.","7ce865de":"All categories are quite different","61c44de2":"This looks really strange!","ebc148bd":"It seems that almost all transactions have purchase amount in range (-1, 0). Quite a strong normalization and high outliers, which will need to be processed.","171ba6b3":"### target","b1cde00e":"### purchase_amount\nSadly purchase_amount is normalized. Let's have a look at it nevertheless.","4f5cff9b":"## Main data exploration\nLet's have a look at data","1aa24a92":"### purchase_amount\nSadly purchase_amount is normalized. Let's have a look at it nevertheless.","7a1b4e41":"### Feature engineering","7d3d4d6c":"### most_recent_sales_range","4f22f24a":"### Basic LGB model","64a63021":"These two variables are very similar. In fact for 90% merchants they are the same.","cc369642":"It seems that almost all transactions have purchase amount in range (-1, 0). Quite a strong normalization and high outliers, which will need to be processed.","cd5c367f":"### Features 1, 2, 3","367be584":"Well, 95% of values are less than 0.1, we'll need to deal with outliers.","a58beee5":"### numerical_1","836655ba":"## merchants\nAggregate information for each merchant_id","6bc6a94d":"### installments","1440ebbb":"### Categories","6983b706":"### most_recent_purchases_range","44f243df":"On the other hand it seems that `999` could mean fraud transactions, considering only 3% of these transactions were approved. One more interesting thing is that the higher the number of installments is, the lower is the approval rate.","f1deafab":"### Processing data for modelling","a5bb44a0":"#### Cards with lowest and highest percentage of authorized transactions","e9fae21a":"It seems that there are some cards, for which most of transactions were declined. Were this fraud transactions?","c354fc7c":"For now I won't use merchants data in models.","393587b1":"All categories are quite different","d050015f":"## General information\n\nThis kernel is dedicated to EDA of Elo Merchant Category Recommendation competition as well as feature engineering.\n\nIn this dataset we can see clients who use Elo and their transactions. We need to predict the loyalty score for each card_id.\n\nWork in progress.\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/10445\/logos\/thumb76_76.png?t=2018-10-24-17-14-05)","41f1778e":"### Feature engineering","8bd9f02c":"Interesting. Most common number of installments are 0 and 1 which is expected. But -1 and 999 are strange. I think that these values were used to fill in missing values.","034fa19d":"Trends of counts for train and test data are similar, and this is great.\nWhy there is such a sharp decline at the end of the period? I think it was on purpose. Or maybe new cards are taken into account only after fulfilling some conditions. ","1de2e9eb":"### Numerical_2","96ab0d50":"#### Cards with lowest and highest total purchase amount","2a455073":"### Categories"}}