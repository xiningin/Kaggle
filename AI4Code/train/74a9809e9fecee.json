{"cell_type":{"8fb4dd6e":"code","0068a68f":"code","76bc7791":"code","6559a9e3":"code","7efff3c7":"code","7e3d9c60":"code","63a46295":"code","d11a67d5":"code","901895ce":"code","c50a9d28":"code","d3d94061":"code","0de47fba":"code","6474eee8":"code","9a9d92e7":"code","dfb1db83":"code","c77ff709":"code","06c4fc8e":"code","13d37db8":"code","ea0e246d":"code","da62cb70":"code","5558366c":"code","14a0a593":"code","06c99082":"code","cf640d4f":"code","df16c886":"code","2ca53973":"code","010f8ad5":"markdown","edf2231c":"markdown","9b5e7fa0":"markdown","92682b59":"markdown","159078af":"markdown","0114bc07":"markdown","71b921e6":"markdown","431e8bbd":"markdown","dd5c3ec7":"markdown"},"source":{"8fb4dd6e":"import os\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVR, LinearSVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport optuna\nimport lightgbm as lgb\nfrom mlens.ensemble import SuperLearner\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 5);\nsns.set_style('whitegrid')","0068a68f":"seed=13\nimputer_neighbors=7\nclusters=9","76bc7791":"train_df = pd.read_csv('..\/input\/tt-GameLevel\/train.csv')\ntest_df = pd.read_csv('..\/input\/tt-GameLevel\/test_without_target.csv')\nsub_df = pd.read_csv('..\/input\/tt-GameLevel\/sample_submission.csv')\ntrain_df.head(3)\ntrain_df.describe()","6559a9e3":"train_df.isna().sum()","7efff3c7":"fig, ax = plt.subplots(1,2)\nsns.distplot(train_df.level_4.value_counts(), ax=ax[0], label='lev 4 train');\nsns.distplot(train_df.level_5.value_counts(), ax=ax[1], label='lev 5 train');\nsns.distplot(test_df.level_4.value_counts(), ax=ax[0], label='lev 4 test');\nsns.distplot(test_df.level_5.value_counts(), ax=ax[1], label='lev 5 test');\nax[0].legend();\nax[1].legend();\nplt.tight_layout();","7e3d9c60":"sorted(train_df.target.unique())","63a46295":"sns.heatmap(train_df.corr(), cmap='coolwarm')","d11a67d5":"train_df.age.unique()\ntrain_df[train_df.age.isna()]","901895ce":"plt.bar(train_df.target.value_counts().index, train_df.target.value_counts());","c50a9d28":"plt.scatter(train_df['duration_ms'].values, train_df['target'])","d3d94061":"fig, ax = plt.subplots(2, 2);\nconc = pd.concat([train_df, test_df]);\nsns.boxplot(conc['duration_ms'], ax=ax[0][0]);\nsns.boxplot(conc['age'], ax=ax[0][1]);\nsns.boxplot(conc['player_4'], ax=ax[1][0]);\nsns.boxplot(conc['# tries'], ax=ax[1][1]);\nplt.tight_layout();","0de47fba":"\"\"\"\n\nClipping did not work either :)\n\ndef clip_outliers(df, columns=['# tries', 'player_4', 'duration_ms']):\n    for idx in df.index:\n        for col in columns:\n            df.loc[idx, col] = df[col].quantile(0.75)\n    return df\n\nconc = \n\"\"\"","6474eee8":"sns.heatmap(train_df.groupby('age').mean().corr());","9a9d92e7":"train_df['age'].max()\ntest_df['age'].max()","dfb1db83":"from sklearn.manifold import TSNE\nfrom sklearn.impute import KNNImputer\n\ntrain_cols = list(train_df.drop(['target', 'Unnamed: 0'], axis=1).columns)\n\nimputer = KNNImputer(n_neighbors=imputer_neighbors)\nimupter = imputer.fit(pd.concat([train_df[train_cols], test_df.drop('Unnamed: 0', axis=1)]))\ndata = imupter.transform(pd.concat([train_df[train_cols], test_df.drop('Unnamed: 0', axis=1)]))\n\ntsne = TSNE()\ntsne_data = tsne.fit_transform(data)","c77ff709":"train_df['duration_ms'] = np.log10(train_df['duration_ms'])\/10\ntest_df['duration_ms'] = np.log10(test_df['duration_ms'])\/10\n\ntrain_df['nans_orig'] = train_df.isna().sum(axis=1)\ntest_df['nans_orig'] = test_df.isna().sum(axis=1)","06c4fc8e":"vif_test_df = train_df.drop('Unnamed: 0', axis=1)\nfor c in vif_test_df.columns:\n    vif_test_df.loc[:, c] = vif_test_df[c].fillna(vif_test_df[c].mean())\nfor i, c in enumerate(vif_test_df.columns):\n    vif = variance_inflation_factor(vif_test_df.values, i)\n    print(c, vif)","13d37db8":"plt.scatter(tsne_data[:, 0], tsne_data[:, 1])","ea0e246d":"train_df.loc[:, 'tsne_0'] = [x[0] for x in tsne.embedding_[:len(train_df)]]\ntrain_df.loc[:, 'tsne_1'] = [x[1] for x in tsne.embedding_[:len(train_df)]]\ntest_df.loc[:, 'tsne_0'] = [x[0] for x in tsne.embedding_[len(train_df):]]\ntest_df.loc[:, 'tsne_1'] = [x[1] for x in tsne.embedding_[len(train_df):]]\n\nz_duration = stats.zscore(conc['duration_ms'])\nz_p4 = stats.zscore(conc['player_4'])\nz_tires = stats.zscore(conc['# tries'])\n\ntrain_df.loc[:, 'z_duration'] = z_duration[:len(train_df)]\ntest_df.loc[:, 'z_duration'] = z_duration[len(train_df):]\ntrain_df.loc[:, 'z_p4'] = z_p4[:len(train_df)]\ntest_df.loc[:, 'z_p4'] = z_p4[len(train_df):]\ntrain_df.loc[:, 'z_tires'] = z_tires[:len(train_df)]\ntest_df.loc[:, 'z_tires'] = z_tires[len(train_df):]","da62cb70":"l_gb = lgb.LGBMClassifier(n_estimators=100, max_depth=13, verbose=1)\n\nlreg = LogisticRegression(solver='lbfgs', C=0.1, max_iter=1e3)\nrf = RandomForestClassifier(n_estimators=500, max_depth=20)\nknn = KNeighborsClassifier(9)\n\nkmeans = KMeans(clusters)","5558366c":"stable_cols = ['player_0', 'player_1', 'player_2', 'player_3']\nX = train_df.drop(['target', 'Unnamed: 0', 'player_4'], axis=1)\ny = train_df.target\n\nkmeans = kmeans.fit(X[stable_cols], y)\nX['kmeans_reg'] = kmeans.predict(X[stable_cols])","14a0a593":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=seed)\n\ndef opt_lgm(trial, rounds=10):\n    \n    param = {\"objective\": \"multiclass\",\n        'metric': 'multi_logloss',\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.1)\n        }\n    param['objective'] = 'multiclass'\n    param['importance_type'] = 'gain'\n    param['n_estimators'] = 500\n\n\n    model = lgb.LGBMClassifier(**param)\n    model = model.fit(X_train, y_train)\n    \n    valid_prediction = model.predict(X_test)\n    recall = recall_score(y_test, valid_prediction, average='micro') \n    return recall\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(opt_lgm, n_trials=100)\nparams = study.best_params","06c99082":"params['objective'] = 'multiclass'\nparams['importance_type'] = 'gain'\nparams['n_estimators'] = 500\n\nmodel = lgb.LGBMClassifier(**params)\n\nkf = KFold(n_splits=5)\n\ni=0\nfor train_index, test_index in kf.split(X):\n\n    X_train, X_test = X.loc[train_index, :], X.loc[test_index, :]\n    y_train, y_test = y[train_index], y[test_index]\n    clf = model.fit(X_train, y_train)\n    preds = clf.predict(X_test)\n    print('accuracy:', accuracy_score(y_test, preds),\n          'precision:', precision_score(y_test, preds, average='micro'),\n          'recall', recall_score(y_test, preds, average='micro'))\n    \n    sub_preds = test_df.drop(['Unnamed: 0', 'player_4'], axis=1)\n    sub_preds['kmeans_reg'] = kmeans.predict(sub_preds[stable_cols])\n    sub_df[f'pred_{i}'] = clf.predict(sub_preds)\n    i+=1","cf640d4f":"sub_df","df16c886":"sub_df['Predicted'] = sub_df.drop(['Id', 'Predicted'], axis=1).mode(axis=1)[0]\nsub_df['Predicted'] = sub_df['Predicted'].astype('int')\nsub_df = sub_df[['Id', 'Predicted']]\nsub_df.head(3)","2ca53973":"sub_df.to_csv('submission.csv', index=False)","010f8ad5":"And next up we have just training, hyperparameter tunning and actual prediction. Of course clusters might be helpful here either though I did not notice any significant difference with or without them. Let's just keep it.","edf2231c":"First I tried to min-max scale age. For some misterious reason that worsened results. Niether other transormation for this field applied. Weird but true.\n\nNow we have a few columns with no NaN values, others are not that lucky. So ideas I tried to impute NaNs with KNN algo. That seemed better then not for XGB but for other learners I used for some additional insights.","9b5e7fa0":"As we can see there is no strong correlation between any value and target. We need to be smarter.","92682b59":"To my surprise also only one feature removed player_4 shown some improvement on results. Weirdly enough VIF for duration column was also huge but removing it as a feature caused model miss something important.","159078af":"Here and there we can see outliers too. Keep that in mind as go.","0114bc07":"Surely we want to see first distribution of the values among test and training test. Let's take a look.","71b921e6":"Where am I?\n\nThis challenge turned out to be more interesting than I initially thought. Not because of the nature of the problem but because of the data. I struggled to engineer meaning features and ratios and so on here but this turned out to be not as predictive ones. I am getting ahead of myself, let's see what we have here.\n","431e8bbd":"Something that did not work with duration column: min-max scaling, it worsened the results. log10 though helped capture something.","dd5c3ec7":"We do not have too much meaningful features here, at least to extend we can manipulate them enough to get something useful (okay, I should say I did not discovered them by triyng some combos of feature interaction). But we still can engineer something regarding the data distributions and data \"dispersion\". That is what I turned to next."}}