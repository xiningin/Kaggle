{"cell_type":{"74ed6dd7":"code","1a425cc9":"code","624151e4":"code","54a0ae99":"code","494084b6":"code","18f16ae2":"code","81a643b6":"code","82b17a16":"code","260abab7":"code","31881d20":"code","37020b85":"code","bf4eb292":"code","7ee63696":"code","6f216fe7":"code","a8c97db9":"code","8d7128a0":"code","e8810d75":"code","c5700abb":"code","a12e0ac7":"code","27dce06b":"code","2e4d6034":"code","5f18e511":"code","9d391478":"code","2ef7fe97":"code","5f3e6121":"code","6ed4fa16":"markdown","35f052b9":"markdown","544aff0a":"markdown","0ce17865":"markdown","af5663c1":"markdown","51a98995":"markdown","6d7bcc83":"markdown","19f1bc4d":"markdown","88be531b":"markdown","6af4ece2":"markdown","30b002de":"markdown","5e9515a1":"markdown","5700abd5":"markdown"},"source":{"74ed6dd7":"!pip install -q nnAudio\n!pip install timm","1a425cc9":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom PIL import Image\nfrom matplotlib import pyplot as plt","624151e4":"train = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/training_labels.csv')\ntest = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\n\ndef get_file_path(image_id: str, training: bool = True) -> str:\n    folder = \"train\" if training else \"test\"\n    return \"..\/input\/g2net-gravitational-wave-detection\/{}\/{}\/{}\/{}\/{}.npy\".format(\n        folder, image_id[0], image_id[1], image_id[2], image_id)\n\n\ntrain['file_path'] = train['id'].apply(get_file_path)\ntest['file_path'] = test['id'].apply(lambda x: get_file_path(x, training=False))\n\ndisplay(train.head())\ndisplay(test.head())","54a0ae99":"from typing import Tuple, Callable\nimport random\n\ndef visualize_sample(\n    _id: str, \n    target: np.int64, \n    colors: Tuple[str, str, str]=(\"black\", \"red\", \"green\"), \n    signal_names: Tuple[str, str, str]=(\"LIGO Hanford\", \"LIGO Livingston\", \"Virgo\")\n) -> None:\n    path = get_file_path(_id)\n    x = np.load(path)\n    plt.figure(figsize=(16, 7))\n    for i in range(3):\n        plt.subplot(4, 1, i + 1)\n        plt.plot(x[i], color=colors[i])\n        plt.legend([signal_names[i]], fontsize=12, loc=\"lower right\")\n        \n        plt.subplot(4, 1, 4)\n        plt.plot(x[i], color=colors[i])\n    \n    plt.subplot(4, 1, 4)\n    plt.legend(signal_names, fontsize=12, loc=\"lower right\")\n\n    plt.suptitle(f\"id: {_id} target: {target}\", fontsize=16)\n    plt.show()","494084b6":"for i in random.sample(train.index.tolist(), 3):\n    _id = train.iloc[i][\"id\"]\n    target = train.iloc[i][\"target\"]\n\n    visualize_sample(_id, target)","18f16ae2":"import librosa\nimport librosa.display\n\ndef visualize_sample_spectogram(\n    _id: str, \n    target: np.int64,\n    signal_names: Tuple[str, str, str]=(\"LIGO Hanford\", \"LIGO Livingston\", \"Virgo\")\n) -> None:\n    x = np.load(get_file_path(_id))\n    plt.figure(figsize=(16, 5))\n    for i in range(3):\n        X = librosa.stft(x[i] \/ x[i].max())\n        Xdb = librosa.amplitude_to_db(abs(X))\n        plt.subplot(1, 3, i + 1)\n        librosa.display.specshow(Xdb, sr=2048, x_axis=\"time\", y_axis=\"hz\", vmin=-30, vmax=50) \n        plt.colorbar()\n        plt.title(signal_names[i], fontsize=14)\n\n    plt.suptitle(f\"id: {_id} target: {target}\", fontsize=16)\n    plt.show()","81a643b6":"for i in random.sample(train.index.tolist(), 3):\n    _id = train.iloc[i][\"id\"]\n    target = train.iloc[i][\"target\"]\n\n    visualize_sample_spectogram(_id, target)","82b17a16":"import torch\nfrom nnAudio.Spectrogram import CQT1992v2\n\ndef apply_qtransform(\n        waves: np.ndarray, transform=CQT1992v2(sr=2048, fmin=20, fmax=1024, hop_length=64)\n) -> torch.Tensor:\n    waves = np.hstack(waves)\n    waves = waves \/ np.max(waves)\n    waves = torch.from_numpy(waves).float()\n    image = transform(waves)\n    return image","260abab7":"for i in range(3):\n    waves = np.load(train.loc[i, 'file_path'])\n    image = apply_qtransform(waves)\n    target = train.loc[i, 'target']\n    plt.figure(figsize=(10, 8))\n    plt.imshow(image[0])\n    plt.title(f\"target: {target}\")\n    plt.show()","31881d20":"train['target'].hist()","37020b85":"class Args:\n    debug=False\n    apex=False\n    print_freq=500\n    num_workers=4\n#     model_name='swin_base_patch4_window12_384'\n    model_name='swin_large_patch4_window12_384'\n    scheduler='CosineAnnealingWarmRestarts' \n    epochs=3\n    T_0=3 # CosineAnnealingWarmRestarts\n    lr=1e-4\n    n_fold=5\n    min_lr=1e-6\n    batch_size=32\n    weight_decay=1e-6\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    qtransform_params={\"sr\": 2048, \"fmin\": 20, \"fmax\": 1024, \"hop_length\": 32, \"bins_per_octave\": 8}\n    seed=42\n    target_size=1\n    target_col='target'\n    trn_fold=[0] #[# cross validation]\n    train=True\n\nif Args.debug:\n    Args.epochs = 1\n    train = train.sample(n=10000, random_state=Args.seed).reset_index(drop=True)","bf4eb292":"OUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\n\ndef get_score(y_true, y_pred):\n    score = roc_auc_score(y_true, y_pred)\n    return score\n\n\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=Args.seed)","7ee63696":"import os\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision.models as models\nfrom torch.optim import Adam, SGD\nfrom torch.nn.parameter import Parameter\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","6f216fe7":"def seed_everything(seed: int) -> None:\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","a8c97db9":"class G2NetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, dir_, augmentations=None, ext=\".npy\"):\n        self.df = df\n#         self.file_names = df['file_path'].values\n        self.dir_ = dir_\n        self.file_ids = df['id'].values\n        self.targets = df[Args.target_col].values\n#         self.wave_transform = CQT1992v2(**Args.qtransform_params)\n        self.augmentations = augmentations\n        self.ext = ext\n        self.pad = nn.ReplicationPad2d((0, 0, 2, 3))\n    \n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        file_id = self.file_ids[idx]\n        target = self.targets[idx]\n        signals = list()\n        path = os.path.join(self.dir_, file_id + self.ext)\n        signal = np.load(path).astype('float32')\n#         signals.append(path)\n        signal = torch.from_numpy(signal).float().unsqueeze(0).unsqueeze(0)\n        \n        signal = self.pad(signal).squeeze(0)\n        signal = signal.repeat((3, 1, 1))\n        return signal, torch.tensor(target).float()\n    \n#     def apply_qtransform(self, waves: np.ndarray, transform: Callable) -> torch.Tensor:\n#         waves = np.hstack(waves)\n#         waves \/= np.max(waves)\n#         waves = torch.from_numpy(waves).float()\n#         image = transform(waves)\n#         return image","8d7128a0":"def get_augmentations(*, data):\n    \n    if data == 'train':\n        return A.Compose([\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return A.Compose([\n            ToTensorV2(),\n        ])","e8810d75":"train_dataset = G2NetDataset(\n    train,\n    \"..\/input\/g2net-n-mels-128-train-images\",\n    get_augmentations(data='train'),\n)\n\nfor i in range(5):\n    plt.figure(figsize=(16,12))\n    image, target = train_dataset[i]\n    print(image.shape)\n#     plt.imshow(image[0])\n#     plt.title(f'label: {label}')\n#     plt.show()","c5700abb":"class G2NetCustomModel(nn.Module):\n    def __init__(self, cfg, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.model = timm.create_model(self.cfg.model_name, img_size=(32, 128), pretrained=pretrained, in_chans=3)\n        self.n_features = self.model.head.in_features\n        self.model.head = nn.Linear(self.n_features, self.cfg.target_size)\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n\nm = G2NetCustomModel(Args)\nx = torch.ones((1, 3, 32, 128))\nprint(m(x))","a12e0ac7":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n        \ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","27dce06b":"def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    if Args.apex:\n        scaler = GradScaler()\n        \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    \n    model.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        \n        if Args.apex:\n            with autocast():\n                y_preds = model(images)\n                loss = criterion(y_preds.view(-1), labels)\n        else:\n            y_preds = model(images)\n            loss = criterion(y_preds.view(-1), labels)\n        \n        # backprop\n        losses.update(loss.item(), batch_size)\n        if Args.gradient_accumulation_steps > 1:\n            loss = loss \/ Args.gradient_accumulation_steps\n        if Args.apex:\n            scaler.scale(loss).backward()\n        else:\n            loss.backward()\n            \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), Args.max_grad_norm)\n        if (step + 1) % Args.gradient_accumulation_steps == 0:\n            if Args.apex:\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % Args.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  'Grad: {grad_norm:.4f}  '\n                  'LR: {lr:.6f}  '\n                  .format(epoch+1, step, len(train_loader), \n                          remain=timeSince(start, float(step+1)\/len(train_loader)),\n                          loss=losses,\n                          grad_norm=grad_norm,\n                          lr=scheduler.get_lr()[0]))\n    return losses.avg","2e4d6034":"def valid_fn(valid_loader, model, criterion, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    \n    # switch to evaluation mode\n    model.eval()\n    preds = []\n    start = end = time.time()\n    \n    for step, (images, labels) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        # compute loss\n        with torch.no_grad():\n            y_preds = model(images)\n        loss = criterion(y_preds.view(-1), labels)\n        losses.update(loss.item(), batch_size)\n        # record accuracy\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n        if Args.gradient_accumulation_steps > 1:\n            loss = loss \/ Args.gradient_accumulation_steps\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % Args.print_freq == 0 or step == (len(valid_loader)-1):\n            print('EVAL: [{0}\/{1}] '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(step, len(valid_loader),\n                          loss=losses,\n                          remain=timeSince(start, float(step+1)\/len(valid_loader))))\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","5f18e511":"def train_loop(folds, fold):\n    \n    LOGGER.info(f\"========== fold: {fold} training ==========\")\n\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n    valid_labels = valid_folds[Args.target_col].values\n\n    train_dataset = G2NetDataset(train_folds, \"..\/input\/g2net-n-mels-128-train-images\", augmentations=get_augmentations(data='train'))\n    valid_dataset = G2NetDataset(valid_folds, \"..\/input\/g2net-n-mels-128-train-images\", augmentations=get_augmentations(data='train'))\n\n    train_loader = torch.utils.data.DataLoader(train_dataset,\n                              batch_size=Args.batch_size, \n                              shuffle=True, \n                              num_workers=Args.num_workers, pin_memory=False, drop_last=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, \n                              batch_size=Args.batch_size * 2, \n                              shuffle=False, \n                              num_workers=Args.num_workers, pin_memory=False, drop_last=False)\n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if Args.scheduler=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=Args.factor, patience=Args.patience, verbose=True, eps=Args.eps)\n        elif Args.scheduler=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=Args.T_max, eta_min=Args.min_lr, last_epoch=-1)\n        elif Args.scheduler=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=Args.T_0, T_mult=1, eta_min=Args.min_lr, last_epoch=-1)\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    model = G2NetCustomModel(Args, pretrained=False)\n    model.to(device)\n\n    optimizer = Adam(model.parameters(), lr=Args.lr, weight_decay=Args.weight_decay, amsgrad=False)\n    scheduler = get_scheduler(optimizer)\n\n    # ====================================================\n    # loop\n    # ====================================================\n    criterion = nn.BCEWithLogitsLoss()\n\n    best_score = 0.\n    best_loss = np.inf\n    \n    for epoch in range(Args.epochs):\n        \n        start_time = time.time()\n        \n        # train\n        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # eval\n        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            scheduler.step()\n\n        # scoring\n        score = get_score(valid_labels, preds)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n        \n        if score > best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        OUTPUT_DIR+f'{Args.model_name}_fold{fold}_best_score.pth')\n        \n        if avg_val_loss < best_loss:\n            best_loss = avg_val_loss\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n            torch.save({'model': model.state_dict(), \n                        'preds': preds},\n                        OUTPUT_DIR+f'{Args.model_name}_fold{fold}_best_loss.pth')\n    \n    valid_folds['preds'] = torch.load(OUTPUT_DIR+f'{Args.model_name}_fold{fold}_best_score.pth', \n                                      map_location=torch.device('cpu'))['preds']\n\n    return valid_folds\n    ","9d391478":"def main():\n\n    def get_result(result_df):\n        preds = result_df['preds'].values\n        labels = result_df[Args.target_col].values\n        score = get_score(labels, preds)\n        LOGGER.info(f'Score: {score:<.4f}')\n    \n    if Args.train:\n        # train \n        oof_df = pd.DataFrame()\n        for fold in range(Args.n_fold):\n            if fold in Args.trn_fold:\n                _oof_df = train_loop(train, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                LOGGER.info(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n        # CV result\n        LOGGER.info(f\"========== CV ==========\")\n        get_result(oof_df)\n        # save result\n        oof_df.to_csv(OUTPUT_DIR+'df_of.csv', index=False)","2ef7fe97":"Fold = StratifiedKFold(n_splits=Args.n_fold, shuffle=True, random_state=Args.seed)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train[Args.target_col])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ndisplay(train.groupby(['fold', 'target']).size())","5f3e6121":"main()","6ed4fa16":"# Torch Custom Dataset","35f052b9":"# Modelling","544aff0a":"# Args","0ce17865":"# Swin Transformer\n\n* From the [paper](https:\/\/arxiv.org\/pdf\/2103.14030.pdf)\n\n![swin_vs_vit.png](attachment:2e1f30b4-ee23-4254-bbcd-64db00a3a855.png)\n\n* Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. \n\n* The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. \n\n* These qualities of Swin Transformer make it compatible with a broad range of vision tasks\n\n\n## What this notebook is about\n\n* Thanks for all the [EDA](https:\/\/www.kaggle.com\/ihelon\/g2net-eda-and-modeling). Being new to time series I learned a lot from this.","af5663c1":"# CV Folds Split","51a98995":"One good thing about simulated that, it is balanced","6d7bcc83":"## QTransform","19f1bc4d":"# Train","88be531b":"# Basic EDA","6af4ece2":"Each .npy file has 3 time series (1 for each detector) and each spans 2 sec and is sampled at 2,048 Hz.","30b002de":"# Utils","5e9515a1":"## Sepectogram with librosa","5700abd5":"# Load files"}}