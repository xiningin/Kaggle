{"cell_type":{"3908fcff":"code","ebbb1859":"code","b3b59f70":"code","0f44446c":"code","49f700a1":"code","5d1f60ee":"code","7a179f78":"code","f3804cfd":"code","41942bd5":"code","ca24a23c":"code","77cc66c3":"code","4612ca8d":"code","eacc73db":"code","ef8dd60e":"code","7863cd05":"code","b7450694":"markdown","7208f2a3":"markdown","a4b86d27":"markdown","0293ca0b":"markdown","989fec5d":"markdown","7033f27c":"markdown","8effd145":"markdown","fc550827":"markdown","7a81460b":"markdown","9bb708ec":"markdown","28e0c08c":"markdown","dd252462":"markdown"},"source":{"3908fcff":"# Load a few helpful modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\nprint(f'Using PyTorch v{torch.__version__}')","ebbb1859":"import pandas as pd\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","b3b59f70":"# Construct the transform\nimport torchvision.transforms as transforms\nfrom   PIL import Image\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n    ])\n\n# Get the device we're training on\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_digits(df):\n    \"\"\"Loads images as PyTorch tensors\"\"\"\n    # Load the labels if they exist \n    # (they wont for the testing data)\n    labels = []\n    start_inx = 0\n    if 'label' in df.columns:\n        labels = [v for v in df.label.values]\n        start_inx = 1\n        \n    # Load the digit information\n    digits = []\n    for i in range(df.pixel0.size):\n        digit = df.iloc[i].astype(float).values[start_inx:]\n        digit = np.reshape(digit, (28,28))\n        digit = transform(digit).type('torch.FloatTensor')\n        if len(labels) > 0:\n            digits.append([digit, labels[i]])\n        else:\n            digits.append(digit)\n\n    return digits","0f44446c":"# Load the training data\ntrain_X = get_digits(train)\n\n# Some configuration parameters\nnum_workers = 0    # number of subprocesses to use for data loading\nbatch_size  = 64   # how many samples per batch to load\nvalid_size  = 0.2  # percentage of training set to use as validation\n\n# Obtain training indices that will be used for validation\nnum_train = len(train_X)\nindices   = list(range(num_train))\nnp.random.shuffle(indices)\nsplit     = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# Define samplers for obtaining training and validation batches\nfrom torch.utils.data.sampler import SubsetRandomSampler\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# Construct the data loaders\ntrain_loader = torch.utils.data.DataLoader(train_X, batch_size=batch_size,\n                    sampler=train_sampler, num_workers=num_workers)\nvalid_loader = torch.utils.data.DataLoader(train_X, batch_size=batch_size, \n                    sampler=valid_sampler, num_workers=num_workers)\n\n# Test the size and shape of the output\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(type(images))\nprint(images.shape)\nprint(labels.shape)","49f700a1":"# Generate 20 random image indices\nrand_indx = np.random.randint(num_train, size=20)\n\n# Construct the subplot space\nfig, axes = plt.subplots(4, 5, figsize=(15, 15))\n\n# Plot the images\nfor i,indx in enumerate(rand_indx):\n    # Get the image\n    img = train_X[indx][0][0]\n    \n    # Get the appropriate subplot\n    x  = i%5         # Subplot x-coordinate\n    y  = int(i\/5)    # Subplot y-coordinate\n    ax = axes[y][x]\n    ax.imshow(img, cmap='gray')\n\n    # Format the plot\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax.set_title(f\"{train_X[indx][1]}\")","5d1f60ee":"# Import the necessary modules\nimport torch.nn as nn\n\ndef calc_out(in_layers, stride, padding, kernel_size, pool_stride):\n    \"\"\"\n    Helper function for computing the number of outputs from a\n    conv layer\n    \"\"\"\n    return int((1+(in_layers - kernel_size + (2*padding))\/stride)\/pool_stride)\n\n# define the CNN architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        # Some helpful values\n        inputs      = [1,32,64,64]\n        kernel_size = [5,5,3]\n        stride      = [1,1,1]\n        pool_stride = [2,2,2]\n\n        # Layer lists\n        layers = []\n\n        self.out   = 28\n        self.depth = inputs[-1]\n        for i in range(len(kernel_size)):\n            # Get some variables\n            padding = int(kernel_size[i]\/2)\n\n            # Define the output from this layer\n            self.out = calc_out(self.out, stride[i], padding,\n                                kernel_size[i], pool_stride[i])\n\n            # convolutional layer 1\n            layers.append(nn.Conv2d(inputs[i], inputs[i+1], kernel_size[i], \n                                       stride=stride[i], padding=padding))\n            layers.append(nn.ReLU())\n            \n            # convolutional layer 2\n            layers.append(nn.Conv2d(inputs[i+1], inputs[i+1], kernel_size[i], \n                                       stride=stride[i], padding=padding))\n            layers.append(nn.ReLU())\n            # maxpool layer\n            layers.append(nn.MaxPool2d(pool_stride[i],pool_stride[i]))\n            layers.append(nn.Dropout(p=0.2))\n\n        self.cnn_layers = nn.Sequential(*layers)\n        \n        print(self.depth*self.out*self.out)\n        \n        # Now for our fully connected layers\n        layers2 = []\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(self.depth*self.out*self.out, 512))\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(512, 256))\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(256, 256))\n        layers2.append(nn.Dropout(p=0.2))\n        layers2.append(nn.Linear(256, 10))\n\n        self.fc_layers = nn.Sequential(*layers2)\n\n    def forward(self, x):\n        x = self.cnn_layers(x)\n        x = x.view(-1, self.depth*self.out*self.out)\n        x = self.fc_layers(x)\n        return x\n    \n# create a complete CNN\nmodel = Net()\nmodel","7a179f78":"import torch.optim as optim\n\n# specify loss function\ncriterion = nn.CrossEntropyLoss()\n\n# specify optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.0005)","f3804cfd":"# number of epochs to train the model\nn_epochs = 25 # you may increase this number to train a final model\n\nvalid_loss_min = np.Inf # track change in validation loss\n\n# Additional rotation transformation\n#rand_rotate = transforms.Compose([\n#    transforms.ToPILImage(),\n#    transforms.RandomRotation(20),\n#    transforms.ToTensor()\n#])\n\n# Get the device\nprint(device)\nmodel.to(device)\ntLoss, vLoss = [], []\nfor epoch in range(n_epochs):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    #########\n    # train #\n    #########\n    model.train()\n    for data, target in train_loader:\n        # move tensors to GPU if CUDA is available\n        data   = data.to(device)\n        target = target.to(device)\n        \n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ############\n    # validate #\n    ############\n    model.eval()\n    for data, target in valid_loader:\n        # move tensors to GPU if CUDA is available\n        data   = data.to(device)\n        target = target.to(device)\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n    \n    # calculate average losses\n    train_loss = train_loss\/len(train_loader.dataset)\n    valid_loss = valid_loss\/len(valid_loader.dataset)\n    tLoss.append(train_loss)\n    vLoss.append(valid_loss)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        valid_loss_min = valid_loss","41942bd5":"# Plot the resulting loss over time\nplt.plot(tLoss, label='Training Loss')\nplt.plot(vLoss, label='Validation Loss')\nplt.legend();","ca24a23c":"model.load_state_dict(torch.load('model_cifar.pt'));","77cc66c3":"# track test loss\ntest_loss     = 0.0\nclass_correct = [0]*10\nclass_total   = [0]*10\n\nmodel.eval()\n\n# For generating confusion matrix\nconf_matrix = np.zeros((10,10))\n\n# iterate over test data\nfor data, target in valid_loader:\n    # move tensors to GPU if CUDA is available\n    data   = data.to(device)\n    target = target.to(device)\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = model(data)\n    # calculate the batch loss\n    loss = criterion(output, target)\n    # update test loss \n    test_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(output, 1)    \n    # compare predictions to true label\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if device == \"cpu\" else np.squeeze(correct_tensor.cpu().numpy())\n    # calculate test accuracy for each object class\n    for i in range(target.size(0)):\n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n        \n        # Update confusion matrix\n        conf_matrix[label][pred.data[i]] += 1\n\n# average test loss\ntest_loss = test_loss\/len(valid_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\n\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %3s: %2d%% (%2d\/%2d)' % (\n            i, 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %3s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","4612ca8d":"import seaborn as sns\nplt.subplots(figsize=(10,9))\nax = sns.heatmap(conf_matrix, annot=True, vmax=20)\nax.set_xlabel('Predicted');\nax.set_ylabel('True');","eacc73db":"# Define the test data loader\ntest        = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest_X      = get_digits(test)\ntest_loader = torch.utils.data.DataLoader(test_X, batch_size=batch_size, \n                                          num_workers=num_workers)","ef8dd60e":"# Create storage objects\nImageId, Label = [],[]\n\n# Loop through the data and get the predictions\nfor data in test_loader:\n    # Move tensors to GPU if CUDA is available\n    data = data.to(device)\n    # Make the predictions\n    output = model(data)\n    # Get the most likely predicted digit\n    _, pred = torch.max(output, 1)\n    \n    for i in range(len(pred)):        \n        ImageId.append(len(ImageId)+1)\n        Label.append(pred[i].cpu().numpy())\n\nsub = pd.DataFrame(data={'ImageId':ImageId, 'Label':Label})\nsub.describe","7863cd05":"# Write to csv file ignoring index column\nsub.to_csv(\"submission.csv\", index=False)","b7450694":"Let's generate a confusion matrix to investigate when a number is mis-classified, which number is it most likely classified as.","7208f2a3":"We now have a 'submission.csv' file that we can submit to the competition.","a4b86d27":"# Make Final Predictions\nNow that we have a trained model with a reasonable amount of accuracy, let's try to make our final predictions for submission to the competition.","0293ca0b":"# Constructing the model\nHere I will construct the PyTorch CNN model that I will ultimately train on the MNIST image dataset. I'm going to opt for using a Sequential model just for the ease of construction and editing it later on.","989fec5d":"# Visualize digits\nLet's take a look at a random sample of 20 digits to see what they look like","7033f27c":"# Train the model\nNow that we have a working model, we need to train it.","8effd145":"# Import the data\nThe first thing we need to do is import the data into the appropriate format","fc550827":"... and see how well it does on our validation data.","7a81460b":"# Make some predictions\nNow let's load the best fit model...","9bb708ec":"# MNIST with PyTorch CNNs\nThis notebook analyzes the MNIST images from the beginners competition using convolutional neural networks (CNNs) implemented in PyTorch.","28e0c08c":"There are some non-surprises in there, such as the confusion between `4` and`9` since they are very similar in shape.","dd252462":"That looks pretty good! Now that we have our data and we know it's in an appropriate format, we can move on to building our model."}}