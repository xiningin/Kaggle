{"cell_type":{"101e37e3":"code","2d26c4ba":"code","a53dc0b4":"code","70a72d82":"code","c1b50676":"code","7ce0dd3d":"code","f29e215d":"code","7c5502b0":"code","1ae49272":"code","fd35bc83":"code","1578c1d6":"code","8642346b":"code","68ac9b29":"code","57f9e7be":"code","95b9c97d":"code","c7ab4f59":"code","f3605476":"code","8d8f4ed3":"code","c79e2649":"code","c58e63a2":"code","e15e8c45":"code","6d0bd2d4":"code","4f963104":"code","6b377f44":"code","1aca39d3":"code","d3fd58ea":"code","1227e62d":"code","b86772f3":"code","0ca969e9":"code","3da9f616":"code","3a32e75d":"code","80d4bf38":"code","663ef9d8":"markdown","84519254":"markdown","c008c24b":"markdown","7646a16f":"markdown","0aba6ab8":"markdown","d2e92f4e":"markdown","9a0a7264":"markdown"},"source":{"101e37e3":"import numpy as np\nimport os\nimport pandas as pd\nimport preprocessor as pp\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sentence_transformers import SentenceTransformer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler","2d26c4ba":"df = pd.read_excel('.\/text_classification_dataset.csv')\ndf.head()","a53dc0b4":"df['type'].value_counts()","70a72d82":"# removing emoticons and urls\npp.set_options(pp.OPT.URL, pp.OPT.EMOJI, pp.OPT.NUMBER)  # to remove urls, numbers and emoticons while cleaning tweets\nstop_words = stopwords.words(\"english\")\nstop_words.append('@')\nstop_words.append('#')\nlist_of_tweets = []\nfor row in range(len(df)):\n    cleaned_tweet = pp.clean(df.iloc[row,0])\n    tweet = word_tokenize(cleaned_tweet.lower())\n    list_of_tweets.append(tweet)\n    df.iloc[row,0] = \" \".join([w for w in tweet if w not in stop_words])\n    \ndf.head()","c1b50676":"# create tf-idf values of each word as features\nvectorizer = TfidfVectorizer(tokenizer=word_tokenize)\ntf_idf_vectors = vectorizer.fit_transform(df['text'])","7ce0dd3d":"# encoding the label and splitting it into train_test_set\nle = LabelEncoder()\ndf['type'] = le.fit_transform(df['type'])\nX_train, X_test, y_train, y_test = train_test_split(tf_idf_vectors.toarray(),df['type'].to_numpy(), test_size=0.3, random_state=42)","f29e215d":"# applying pca on high dimensional tfidf maxtrix\npca = PCA(n_components=768)\npca_tfidf = pca.fit_transform(tf_idf_vectors.toarray())\npca_X_train, pca_X_test, y_train, y_test = train_test_split(pca_tfidf,df['type'].to_numpy(), test_size=0.3, random_state=42)","7c5502b0":"# bert embeddings as features\n\ndef embed_text(text):\n    return np.array(embed.encode(text))\n\nembed = SentenceTransformer('bert-base-nli-mean-tokens')\nbert_embeds = embed_text(df['text'])\nbert_X_train, bert_X_test, y_train, y_test = train_test_split(bert_embeds,df['type'].to_numpy(), test_size=0.3, random_state=42)","1ae49272":"# fitting gaussian models with bert embeddings\n\ngnb = GaussianNB()\ngnb.fit(bert_X_train, y_train)\nscore_train = gnb.score(bert_X_train, y_train)\nscore_test = gnb.score(bert_X_test, y_test)\nprint(\"\\nTrain set score for GaussianNB:\", score_train)\nprint(\"\\nTest set score for GaussianNB:\", score_test)","fd35bc83":"# fitting gaussian models with pca tf-idf features\n\ngnb = GaussianNB()\ngnb.fit(pca_X_train, y_train)\nscore_train = gnb.score(pca_X_train, y_train)\nscore_test = gnb.score(pca_X_test, y_test)\nprint(\"\\nTrain set score for GaussianNB:\", score_train)\nprint(\"\\nTest set score for GaussianNB:\", score_test)","1578c1d6":"# fitting gaussian models with tf-idf features\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\nscore_train = gnb.score(X_train, y_train)\nscore_test = gnb.score(X_test, y_test)\nprint(\"\\nTrain set score for GaussianNB:\", score_train)\nprint(\"\\nTest set score for GaussianNB:\", score_test)\n\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)\nscore_train = gnb.score(X_train, y_train)\nscore_test = gnb.score(X_test, y_test)\nprint(\"\\nTrain set score for MultinomialNB:\", score_train)\nprint(\"\\nTest set score for MultinomialNB:\", score_test)","8642346b":"# fitting logistic regression model with bert embeddings\n\nlr = LogisticRegression()\nlr.fit(bert_X_train, y_train)\nscore_train = lr.score(bert_X_train, y_train)\nscore_test = lr.score(bert_X_test, y_test)\nprint(\"\\nTrain set score for LogisticRegression:\", score_train)\nprint(\"\\nTest set score for LogisticRegression:\", score_test)","68ac9b29":"# fitting logistic regression model with pca tf-df features\n\nlr = LogisticRegression()\nlr.fit(pca_X_train, y_train)\nscore_train = lr.score(pca_X_train, y_train)\nscore_test = lr.score(pca_X_test, y_test)\nprint(\"\\nTrain set score for LogisticRegression:\", score_train)\nprint(\"\\nTest set score for LogisticRegression:\", score_test)","57f9e7be":"# fitting logistic regression model with tf-idf features\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nscore_train = lr.score(X_train, y_train)\nscore_test = lr.score(X_test, y_test)\nprint(\"\\nTrain set score for LogisticRegression:\", score_train)\nprint(\"\\nTest set score for LogisticRegression:\", score_test)","95b9c97d":"# fitting svm classification model with bert embeddings\n\nsvc = SVC(gamma='auto')\nsvc.fit(bert_X_train, y_train)\nscore_train = svc.score(bert_X_train, y_train)\nscore_test = svc.score(bert_X_test, y_test)\nprint(\"\\nTrain set score for SVM:\", score_train)\nprint(\"\\nTest set score for SVM:\", score_test)","c7ab4f59":"# fitting svm classification model with pca tf-idf features\n\nsvc = SVC(gamma='auto')\nsvc.fit(pca_X_train, y_train)\nscore_train = svc.score(pca_X_train, y_train)\nscore_test = svc.score(pca_X_test, y_test)\nprint(\"\\nTrain set score for SVM:\", score_train)\nprint(\"\\nTest set score for SVM:\", score_test)","f3605476":"# fitting svm classification model with tf-idf features\n\nsvc = SVC(gamma='auto')\nsvc.fit(X_train, y_train)\nscore_train = svc.score(X_train, y_train)\nscore_test = svc.score(X_test, y_test)\nprint(\"\\nTrain set score for SVM:\", score_train)\nprint(\"\\nTest set score for SVM:\", score_test)","8d8f4ed3":"# fitting RandomForest classification model with bert embeddings\n\nrfc = RandomForestClassifier()\nrfc.fit(bert_X_train, y_train)\nscore_train = rfc.score(bert_X_train, y_train)\nscore_test = rfc.score(bert_X_test, y_test)\nprint(\"\\nTrain set score for RandomForestClassifier:\", score_train)\nprint(\"\\nTest set score for RandomForestClassifier:\", score_test)","c79e2649":"# fitting RandomForest classification model with with pca tf-idf features\n\nrfc = RandomForestClassifier()\nrfc.fit(pca_X_train, y_train)\nscore_train = rfc.score(pca_X_train, y_train)\nscore_test = rfc.score(pca_X_test, y_test)\nprint(\"\\nTrain set score for RandomForestClassifier:\", score_train)\nprint(\"\\nTest set score for RandomForestClassifier:\", score_test)","c58e63a2":"# fitting RandomForest classification model with tf-idf features\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\nscore_train = rfc.score(X_train, y_train)\nscore_test = rfc.score(X_test, y_test)\nprint(\"\\nTrain set score for RandomForestClassifier:\", score_train)\nprint(\"\\nTest set score for RandomForestClassifier:\", score_test)","e15e8c45":"# creating LSTM network\nclass LSTM(nn.Module):\n    def __init__(self, embedding_dim, n_hidden, n_outputs, n_hlayers=1, vocab_size=3000):\n        super(LSTM, self).__init__()\n        self.n_hidden = n_hidden\n        self.n_hlayers = n_hlayers\n        self.embed = nn.Embedding(vocab_size+1,embedding_dim)\n        self.layer1 = nn.LSTM(embedding_dim, n_hidden, n_hlayers, dropout=0.5, batch_first=True)\n        self.layer2 = nn.Linear(n_hidden, 15)\n        self.layer3 = nn.Linear(15, n_outputs)\n        self.dropout = nn.Dropout()\n    \n    def forward(self, x):\n        x = x.long()\n        x = self.embed(x)\n        batch_size, hidden_dim = x.shape[0], self.n_hidden\n        h,c = torch.zeros((self.n_hlayers,batch_size, hidden_dim)), torch.zeros((self.n_hlayers,batch_size, hidden_dim))\n        x, hc = self.layer1(x,(h,c))\n        out = self.layer3(self.layer2(x))\n        return out[:,-1]      ","6d0bd2d4":"# function to pad featues to make them equal size\ndef pad_sequence(reviews_int, seq_length=200):\n    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n    for ii, review in enumerate(reviews_int):\n        features[ii,-len(review):] = np.array(review)[:seq_length]\n    return features","4f963104":"# our evaluate func\ndef evaluate(model, dl, criterion):\n    total_loss = 0\n    num_correct = 0\n    for x,y in dl:\n        output = model(x)\n        loss = criterion(output,y)\n        total_loss += loss.item()\n        pred = torch.argmax(output, dim=1)\n        correct = pred.eq(y.view_as(pred))\n        #correct = np.squeeze(correct.numpy())\n        num_correct += np.sum(correct.numpy())\n    return total_loss, num_correct\/len(dl.dataset)","6b377f44":"# our train func\ndef train(model, train_dl, val_dl, epochs, optimizer, criterion):\n    train_loss_list = []\n    for e in range(epochs):\n        tot_train_loss = 0\n        for x,y in train_dl:\n            output = model(x)\n            loss = criterion(output,y)\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss_list.append(loss.item())\n            tot_train_loss += loss.item()\n        print(\"After {} epcoh:\".format(e+1), \"Train_Loss:\", tot_train_loss,\"Val_Loss and Accuracy:\", evaluate(model, val_dl, criterion))","1aca39d3":"# creating vocab2int and mapping tokens to int\nwords = vectorizer.get_feature_names()\nvocab2int ={word:ii for ii, word in enumerate(words,1)}\ntweets = []\nfor row in df.iterrows():\n    tweets.append([vocab2int[word] for word in word_tokenize(row[1]['text'].lower())])","d3fd58ea":"# pad tweets to keep the lenght of all tweets to 35\nseq_length=35\nfeatures = pad_sequence(tweets, seq_length)\nassert len(features) == len(tweets)\nassert len(features[0]) == seq_length","1227e62d":"# loading glove embeddings\nembedding_dict = {}\nwith open('.\/glove.6B.100d.txt',encoding=\"utf8\")as f:\n    for line in f:\n        split = line.split()\n        word = split[0]\n        embeddings = np.array(split[1:], dtype='float32')\n        embedding_dict[word] = embeddings","b86772f3":"# creating embedding matrix to initialize embedding layer\nembed_dim = 100\nembedding_matrix = np.zeros((len(words)+1, embed_dim))\nembedding_matrix[0] = embedding_dict['pad']\n \nfor word, index in vocab2int.items():\n    try:\n        embedding_matrix[index] = embedding_dict[word]\n    except:\n        embedding_matrix[index] = embedding_dict['unk']","0ca969e9":"# prepaing dataset and dataloader to feed data to our lstm network\nlstm_X_train, lstm_X_test, y_train, y_test = train_test_split(features,df['type'].to_numpy(), test_size=0.3, random_state=42)\ntrain_dataset = TensorDataset(torch.from_numpy(lstm_X_train), torch.from_numpy(y_train).type(torch.LongTensor))\ntrain_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True)\nval_dataset = TensorDataset(torch.from_numpy(lstm_X_test), torch.from_numpy(y_test).type(torch.LongTensor))\nval_loader = DataLoader(val_dataset, batch_size = 32)","3da9f616":"\"embed_dim\" in dir()","3a32e75d":"if \"embed_dim\" not in dir():\n    embed_dim = 300\n\nn_hidden = 50 # no of hidden dim\nn_output = len(le.classes_) # no of class labels\nn_hlayer = 1 # no of lstm layers\nvocab_size = len(words) # size of vocabulary\nmodel = LSTM(embed_dim, n_hidden, n_output, n_hlayer, vocab_size)\ntry:\n    model.embed.weight.data.copy_(torch.from_numpy(embedding_matrix))\nexcept:\n    pass\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr=0.001, weight_decay=0.001)","80d4bf38":"train(model, train_loader, val_loader, 25, optimizer, criterion)","663ef9d8":"### Naive Bayes","84519254":"### SVM","c008c24b":"### Preprocessing Tweets","7646a16f":"### Random Forest","0aba6ab8":"Next two cells help us use pre-trained Glove Embeddings instead of training embeddings from scratch. This helps us get better performance. One can also skip these embeddings by skipping the next cells and running rest of the cells as it is.","d2e92f4e":"### LSTM","9a0a7264":"### Logistic Regression"}}