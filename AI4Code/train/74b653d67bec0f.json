{"cell_type":{"b85a7d7d":"code","22a11840":"code","f5e35939":"code","51379043":"code","c232125c":"code","802dfad4":"code","ea2bbbca":"code","ac8906cf":"code","da28f6be":"code","8b61e1f0":"code","e48fe2fd":"code","23e95daa":"code","f2ed509e":"code","e9ad263e":"code","0173d28b":"code","dae9a3ee":"code","707b1669":"code","f84e96a5":"code","a7041ba8":"code","3b52a24f":"code","ef6f506b":"code","56197aea":"code","d8d22e93":"markdown","e94084c3":"markdown","1c14b376":"markdown","b0b6d281":"markdown","3cc3394f":"markdown","8cd5f0f4":"markdown","1b21f84e":"markdown","2b277043":"markdown","6bda8f0a":"markdown"},"source":{"b85a7d7d":"import sys\nsys.path.append('..\/input\/rich-text-formatting')","22a11840":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport random\n\nfrom transformers import pipeline\n\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\n\nfrom rich import print as _pprint","f5e35939":"def cprint(string):\n    \"\"\"\n    Utility function for beautiful colored printing.\n    \"\"\"\n    _pprint(f\"[black]{string}[\/black]\")","51379043":"train_file = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\ntest_file = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv\")","c232125c":"train_file.head()","802dfad4":"train_file.info()","ea2bbbca":"train_file.describe()","ac8906cf":"test_file.head()","da28f6be":"test_file.info()","8b61e1f0":"test_file.describe()","e48fe2fd":"sample_sub.head()","23e95daa":"cprint(\"Total Training Examples: [green]{}[\/green]\".format(train_file.shape[0]))\ncprint(\"Total Testing Examples: [green]{}[\/green]\".format(test_file.shape[0]))","f2ed509e":"train_file['language'].value_counts()","e9ad263e":"language_name = train_file['language'].value_counts().index.tolist()\nlanguage_val = train_file['language'].value_counts().tolist()\n\nfig = px.bar(\n    x=language_name,\n    y=language_val,\n    title=\"Training Samples by Language\",\n    labels={\n        'x': 'Language',\n        'y': 'Sample count'\n    },\n    color=language_val\n)\nfig.show()","0173d28b":"fig = px.pie(\n    names=language_name,\n    values=language_val,\n    title=\"Training Samples by Language - Pie Chart\",\n    color_discrete_sequence=px.colors.sequential.RdBu_r,\n)\nfig.show()","dae9a3ee":"hindi = train_file[train_file['language']=='hindi']['context'].str.len()\ntamil = train_file[train_file['language']=='tamil']['context'].str.len()\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=list(hindi), name='Hindi Context'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(tamil), name='Tamil Context'),\n    row=1, \n    col=2,\n)\n\nfig.update_layout(height=400, width=800, title_text=\"Character Count by Language\")\niplot(fig)","707b1669":"hindi = train_file[train_file['language']=='hindi']['context'].str.split().map(lambda x: len(x))\ntamil = train_file[train_file['language']=='tamil']['context'].str.split().map(lambda x: len(x))\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=list(hindi), name='Hindi Context'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(tamil), name='Tamil Context'),\n    row=1, \n    col=2,\n)\n\nfig.update_layout(height=400, width=800, title_text=\"Word Count Distribution by Language\")\niplot(fig)","f84e96a5":"hindi = train_file[train_file['language']=='hindi']['context'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\ntamil = train_file[train_file['language']=='tamil']['context'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\n\nfig = ff.create_distplot([hindi, tamil], ['Hindi', 'Tamil'])\nfig.update_layout(height=500, width=800, title_text=\"Average Word Length Distribution by Language\")\niplot(fig)","a7041ba8":"hindi = train_file[train_file['language']=='hindi']['context'].apply(lambda x: len(set(str(x).split()))).to_list()\ntamil = train_file[train_file['language']=='tamil']['context'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([hindi, tamil], ['Hindi', 'Tamil'])\nfig.update_layout(height=500, width=800, title_text=\"Unique Word Count Distribution by Language\")\niplot(fig)","3b52a24f":"model = \"..\/input\/bbmcfs\/bert-base-multilingual-cased-finetuned-squad\"\nqna = pipeline('question-answering', model=model, tokenizer=model, device=0)\n\npredictions = []\n\nfor question, context in test_file[[\"question\", \"context\"]].to_numpy():\n    result = qna(context=context, question=question)\n    predictions.append(result[\"answer\"])","ef6f506b":"submission = pd.DataFrame()\nsubmission['id'] = test_file['id']\nsubmission['PredictionString'] = predictions\nsubmission.to_csv(\"submission.csv\", index=None)\n\nsubmission.head()","56197aea":"cprint(\"[red]Under Work! More stuff coming soon[\/red] \u26a0\")","d8d22e93":"<div class=\"alert alert-info\">\n    <h1 align='center'>3. Baseline Model using \ud83e\udd17<\/h1>\n<\/div>","e94084c3":"<div class=\"alert alert-warning\">\n    <h1 align='center'>2. Data Loading and EDA \ud83d\udcb9<\/h1>\n<\/div>","1c14b376":"<div class=\"alert alert-success\">\n    <h1 align='center'>1. Introduction and Imports \ud83d\udcd4<\/h1>\n<\/div>\n<center>\nLet's get started with this new text competition! \n<br>    \n    With nearly 1.4 billion people, India is the second-most populated country in the world. Yet Indian languages, like Hindi and Tamil, are underrepresented on the web. \n<br>\n    Popular Natural Language Understanding (NLU) models perform worse with Indian languages compared to English, the effects of which lead to subpar experiences in downstream web applications for Indian users.\n<\/center>","b0b6d281":"<div class=\"alert alert-warning\">\n    <h3 align='center'>2.1 Training Samples by Language<\/h3>\n<\/div>","3cc3394f":"<div class=\"alert alert-success\">\n    <h3 align='center'>1.3 Evaluation Metric \u2712<\/h1>\n<\/div>\n\n<center>\nIn this competition, our submissions will be judged on the Jaccard Score metric.\n<\/center>\n\n$$\nscore = \\frac{1}{n} \\sum_{i=1}^n jaccard( gt_i, dt_i ) \n$$","8cd5f0f4":"<div class=\"alert alert-success\">\n    <h3 align='center'>1.1 What is our task? \ud83c\udfaf<\/h1>\n<\/div>\n\nOk, so in this competition we will be predicting answers to questions in Hindi and Tamil. The answers are drawn directly from a limited context \n\nDuring inference, we will be provided with hiddent test set that will be about the same size as our training set.\n\nThis is a Research Code-Competition.\n\nThe submission file should consist of 2 rows:\n* `id`: Unique ID\n* `PredictionString`: Predicted String","1b21f84e":"<center>\n    <strong>If you found this notebook useful, you can leave an upvote!<\/strong>\n<\/center>","2b277043":"<div class=\"alert alert-warning\">\n    <h3 align='center'>2.2 EDA on Context<\/h3>\n<\/div>","6bda8f0a":"<div class=\"alert alert-success\">\n    <h3 align='center'>1.2 How does the Data look like? \ud83d\uddc3<\/h1>\n<\/div>\n\nThe data provided to us in this competition consists mainly of 2 `.csv` files (`train.csv` and `test.csv`).\n\nBelow is the breakdown of the `.csv` files;\n\n* \ud83d\udcc4 `train.csv` - The training set, containing context, questions, and answers. Also includes the start character of the answer for disambiguation.\n\n\n* \ud83d\udcc4 `test.csv` - The test set, containing context and questions.\n\n\n* \ud83d\udcc4 `sample_submission.csv` - The Sample submission file in the format we are expected to follow."}}