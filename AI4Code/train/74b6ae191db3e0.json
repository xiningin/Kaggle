{"cell_type":{"4f8f104f":"code","8cba4e6b":"code","5a048694":"code","e55be6b4":"code","92af6587":"code","0971fbf2":"code","af63b82b":"code","02b7d482":"code","16fc2205":"code","7cc88624":"code","7187dd53":"code","ed7c09ec":"code","01c9ccb3":"code","e6190e3b":"code","8f23d4e4":"code","53fe1885":"code","bb2803a6":"code","bb0be1d5":"code","fc68bb90":"code","5dc64886":"code","8673943c":"code","d44b7516":"code","678365e1":"code","3e45f635":"code","83675849":"code","a22b542d":"code","dd4f8c0c":"code","02440925":"code","1c6e9d13":"code","fe61276a":"code","d271a994":"code","9f94276c":"code","38193930":"code","8456f4c4":"code","efd39133":"code","72317c3e":"code","1637183c":"code","d4e2ef78":"code","1256908c":"code","23777dcd":"code","d49b35bb":"code","5275904f":"code","d25fe1e3":"code","07262d43":"code","f98107d3":"code","78bfe79c":"code","cde84530":"code","6a4f8375":"code","33dd3dce":"code","9a210081":"code","0864bfd5":"code","b19192fa":"code","dedb43f4":"code","57096937":"code","a8f89f16":"code","28d14a63":"code","4859d6a1":"code","5048b504":"code","3d35dcc1":"code","c75912a8":"code","bbb70686":"code","3d316d32":"code","e6ea9bbe":"code","d0f02e93":"code","5fcce700":"code","380b53b4":"markdown","4ceb8423":"markdown","d0fb9618":"markdown","5bc66985":"markdown","e56b6d91":"markdown","fa9af91c":"markdown","39935fc5":"markdown","b30f3bfd":"markdown","d2b6a85f":"markdown","14f56870":"markdown","16a64234":"markdown","ffc0fe83":"markdown","21ea06bb":"markdown","d6398230":"markdown","a00f27bd":"markdown","ef5f131d":"markdown","a9f4fff8":"markdown","955d97ae":"markdown","3efabfc6":"markdown","76a209a9":"markdown","0783cb93":"markdown","87929519":"markdown","3ef12ec8":"markdown","7fcc9302":"markdown","1798929d":"markdown","4014cdc1":"markdown","40db746b":"markdown","f8e95bdf":"markdown","33111e38":"markdown","10ed12e0":"markdown","4f9f94d2":"markdown","175bd0de":"markdown","a44054d6":"markdown","060334e4":"markdown","6e4ddd84":"markdown","ac7f1b8b":"markdown","942ade77":"markdown","563c275e":"markdown","74acd94e":"markdown","4fe05ba9":"markdown","91f42814":"markdown","1f8122ef":"markdown","96f5cbc1":"markdown","68ded9a9":"markdown","036e78b3":"markdown","8945e575":"markdown","b48546b9":"markdown","c128d5ef":"markdown","21d9c371":"markdown","3433f95b":"markdown","c23426b4":"markdown","91f0fd4a":"markdown","2bb86e79":"markdown","a77bad94":"markdown","6d8d0f65":"markdown","d34c4745":"markdown","698d440e":"markdown","fbae1756":"markdown","9e0178e2":"markdown","587ccaa7":"markdown","896ba39a":"markdown","d745d177":"markdown"},"source":{"4f8f104f":"!pip install textstat","8cba4e6b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom collections import defaultdict,Counter\nfrom wordcloud import WordCloud, STOPWORDS\nfrom plotly.subplots import make_subplots\nfrom nltk.tokenize import word_tokenize\nimport plotly.figure_factory as ff\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import stopwords\nimport plotly.graph_objects as go\nfrom textblob import TextBlob\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom nltk.util import ngrams\nimport plotly.offline as py\nimport plotly.express as px\nfrom statistics import *\nfrom plotly import tools\nimport seaborn as sns\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport textstat\nimport string\nimport json\nimport nltk\nimport gc\n\n","5a048694":"py.init_notebook_mode(connected=True)\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))\nplt.style.use('seaborn')\n","e55be6b4":"train=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ntest=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\ntarget=train['sentiment']","92af6587":"print('There are {} rows and {} cols in train set'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} cols in test set'.format(test.shape[0],test.shape[1]))","0971fbf2":"train.head(3)","af63b82b":"import re\ndef basic_cleaning(text):\n    text=re.sub(r'https?:\/\/www\\.\\S+\\.com','',text)\n    text=re.sub(r'[^A-Za-z|\\s]','',text)\n    return text\n\ndef clean(df):\n    for col in ['text','selected_text']:\n        df[col]=df[col].astype(str).apply(lambda x:basic_cleaning(x))\n    return df\n\ncolors=['blue','green','red']\nsent=train.sentiment.unique()","02b7d482":"fig=make_subplots(1,2,subplot_titles=('Train set','Test set'))\nx=train.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['blue','green','red'],name='train'),row=1,col=1)\nx=test.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['blue','green','red'],name='test'),row=1,col=2)","16fc2205":"df=pd.concat([train,test])\ndf['text']=df['text'].astype(str)\ndf['seleceted_text']=df['selected_text'].astype(str)","7cc88624":"vals=[]\nfor i in range(0,3):\n    x=df[df['sentiment']==sent[i]]['text'].str.len()\n    vals.append(x)\n\nfig = ff.create_distplot(vals, sent,show_hist=False)\nfig.update_layout(title=\"Distribution of number of characters in tweets\")\nfig.show()","7187dd53":"vals=[]\nfor i in range(0,3):\n    x=df[df['sentiment']==sent[i]]['selected_text'].dropna().str.len()\n    vals.append(x)\n\nfig = ff.create_distplot(vals, sent)\nfig.update_layout(title=\"Distribution of number of characters in selected text\")\nfig.show()","ed7c09ec":"\nsent=df.sentiment.unique()\nfig,ax= plt.subplots(1,3,figsize=(12,6))\nfor i in range(0,3):\n    df[df['sentiment']==sent[i]]['text'].str.split().str.len().hist(ax=ax[i],color=colors[i])\n    ax[i].set_title(sent[i])\nfig.suptitle(\"Distribution of number of No: Words in Tweets\", fontsize=14)","01c9ccb3":"\nsent=train.sentiment.unique()\nfig,ax= plt.subplots(1,3,figsize=(12,6))\nfor i in range(0,3):\n    train[train['sentiment']==sent[i]]['selected_text'].str.split().str.len().hist(ax=ax[i],color=colors[i])\n    ax[i].set_title(sent[i])\nfig.suptitle(\"Distribution of number of No: Words in Selected text\", fontsize=14)","e6190e3b":"def preprocess_news(df,stop=stop,n=1,col='text'):\n    '''Function to preprocess and create corpus'''\n    new_corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for text in df[col]:\n        words=[w for w in word_tokenize(text) if (w not in stop)]\n       \n        words=[lem.lemmatize(w) for w in words if(len(w)>n)]\n     \n        new_corpus.append(words)\n        \n    new_corpus=[word for l in new_corpus for word in l]\n    return new_corpus\n","8f23d4e4":"\nfig,ax=plt.subplots(1,3,figsize=(15,7))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]\n    corpus_train=preprocess_news(new,{})\n    \n    dic=defaultdict(int)\n    for word in corpus_train:\n        if word  in stop:\n            dic[word]+=1\n            \n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    x,y=zip(*top)\n    ax[i].bar(x,y,color=colors[i])\n    ax[i].set_title(sent[i],color=colors[i])\n\nfig.suptitle(\"Common stopwords in different sentiments\")","53fe1885":"df['punc']=df['text'].apply(lambda x : [c for c in x if c in string.punctuation])\nfig,ax=plt.subplots(1,3,figsize=(10,6))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]['punc'].map(lambda x: len(x))\n    sns.distplot(new,color=colors[i],ax=ax[i])\n    ax[i].set_title(sent[i],color=colors[i])\n    \nfig.suptitle(\"Number of Punctuations in tweets\") ","bb2803a6":"fig,ax=plt.subplots(1,3,figsize=(12,7))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]['text'].map(lambda x: len(set(x.split())))\n    sns.distplot(new.values,ax=ax[i],color=colors[i])\n    ax[i].set_title(sent[i])\nfig.suptitle(\"Distribution of number of unique words\")","bb0be1d5":"fig,ax=plt.subplots(1,3,figsize=(12,7))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]['selected_text'].astype(str).map(lambda x: len(set(x.split())))\n    sns.distplot(new.values,ax=ax[i],color=colors[i])\n    ax[i].set_title(sent[i])\nfig.suptitle(\"Distribution of number of unique words\")","fc68bb90":"fig,ax=plt.subplots(1,3,figsize=(15,10))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]['punc']\n    punc=[p for pun in new.values for p in pun]\n    counter=Counter(punc).most_common(10)\n    x,y=zip(*counter)\n    ax[i].bar(x,y,color=colors[i])\n    ax[i].set_title(sent[i],color=colors[i])\n    \nfig.suptitle(\"Punctuations in tweets\")   \n    \n    ","5dc64886":"df=clean(df)","8673943c":"\nfig,ax=plt.subplots(1,3,figsize=(20,12))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]\n    corpus_train=preprocess_news(new,n=3)\n    counter=Counter(corpus_train)\n    most=counter.most_common()\n    x=[]\n    y=[]\n    for word,count in most[:20]:\n        if (word not in stop) :\n            x.append(word)\n            y.append(count)\n    sns.barplot(x=y,y=x,ax=ax[i],color=colors[i])\n    ax[i].set_title(sent[i],color=colors[i])\nfig.suptitle(\"Common words in tweet text\")","d44b7516":"fig,ax=plt.subplots(1,3,figsize=(20,12))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]   \n    corpus=preprocess_news(new,n=3,col='selected_text')\n    counter=Counter(corpus)\n    most=counter.most_common()\n    x=[]\n    y=[]\n    for word,count in most[:20]:\n        if (word not in stop) :\n            x.append(word)\n            y.append(count)\n    sns.barplot(x=y,y=x,ax=ax[i],color=colors[i])\n    ax[i].set_title(sent[i],color=colors[i])\nfig.suptitle(\"Common words in selected text\")","678365e1":"def get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n),stop_words=stop).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:20]","3e45f635":"fig,ax=plt.subplots(1,2,figsize=(15,10))\nfor i in range(2):\n    new=df[df['sentiment']==sent[i+1]]['selected_text']\n    top_n_bigrams=get_top_ngram(new,2)[:20]\n    x,y=map(list,zip(*top_n_bigrams))\n    sns.barplot(x=y,y=x,ax=ax[i],color=colors[i+1])\n    ax[i].set_title(sent[i+1])\n    \nfig.suptitle(\"Common bigrams in selected text\")","83675849":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None,ax=None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=30, \n        scale=3,\n        random_state=1 \n        )\n    \n    wordcloud=wordcloud.generate(str(data))\n    ax.imshow(wordcloud,interpolation='nearest')\n    ax.axis('off')\n    #plt.show()","a22b542d":"fig,ax=plt.subplots(1,3,figsize=(20,12))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]['text']\n    show_wordcloud(new,ax=ax[i])\n    ax[i].set_title(sent[i],color=colors[i])","dd4f8c0c":"fig,ax=plt.subplots(1,3,figsize=(20,12))\nfor i in range(3):\n    new=df[df['sentiment']==sent[i]]['selected_text'].dropna()\n    show_wordcloud(new,ax=ax[i])\n    ax[i].set_title(sent[i],color=colors[i])\n    ","02440925":"#utility functions:\ndef plot_readability(a,b,c,title,bins=0.4,colors=colors):\n    trace1 = ff.create_distplot([a,b,c],sent, bin_size=bins, colors=colors, show_rug=False)\n    trace1['layout'].update(title=title)\n    py.iplot(trace1, filename='Distplot')\n    table_data= [[\"Statistical Measures\",\"neu\",'pos','neg'],\n                [\"Mean\",mean(a),mean(b),mean(c)],\n                [\"Standard Deviation\",pstdev(a),pstdev(b),pstdev(c)],\n                [\"Variance\",pvariance(a),pvariance(b),pvariance(c)],\n                [\"Median\",median(a),median(b),median(c)],\n                [\"Maximum value\",max(a),max(b),max(c)],\n                [\"Minimum value\",min(a),min(b),min(c)]]\n    trace2 = ff.create_table(table_data)\n    py.iplot(trace2, filename='Table')\n","1c6e9d13":"tqdm.pandas()\nfre_neu = np.array(df[\"text\"][df[\"sentiment\"] == sent[0]].progress_apply(textstat.flesch_reading_ease))\nfre_pos = np.array(df[\"text\"][df[\"sentiment\"] == sent[1]].progress_apply(textstat.flesch_reading_ease))\nfre_neg = np.array(df[\"text\"][df[\"sentiment\"] == sent[2]].progress_apply(textstat.flesch_reading_ease))\n\nplot_readability(fre_neu,fre_pos,fre_neg,\"Flesch Reading Ease\",20)","fe61276a":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n","d271a994":"def plot_jaccard(sentiment,ax):\n    jacc=[]\n    text=train[train['sentiment']==sentiment].dropna()['text'].values.tolist()\n    selected=train[train['sentiment']==sentiment].dropna()['selected_text'].values.tolist()\n    for i,k in zip(text,selected):\n        jacc.append(jaccard(i,k))\n    ax.hist(jacc,bins=10,color='blue',alpha=0.4)\n    ax.set_title(sentiment)\n    ","9f94276c":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\nplot_jaccard('positive',ax=ax1)\nplot_jaccard('negative',ax2)\nplot_jaccard('neutral',ax3)\nfig.suptitle('jaccard similarity of text and selected text')","38193930":"train['jaccard']=train.apply(lambda x : jaccard(x.text,x.selected_text),axis=1)\npositive=train[(train['sentiment']=='positive') & (train['jaccard']>0.9)]","8456f4c4":"positive.head()","efd39133":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,7))\nx=train[train['sentiment']=='positive']['text'].str.len()\ny=train[(train['sentiment']=='positive')]['jaccard'].values.tolist()\nax1.scatter(x,y,color='green',alpha=.4)\nax1.set_xlabel('text length')\nax1.set_ylabel('jaccard similarity with selected text')\nax1.set_title(\"text length vs jaccard similarity\")\nx=train[train['sentiment']=='positive']['text'].apply(lambda x : len(x.split()))\nax2.scatter(x,y,color='green',alpha=.4)\nax2.set_xlabel('text length')\nax2.set_ylabel('jaccard similarity with selected text')\nax2.set_title(\"no: of words vs jaccard similarity\")","72317c3e":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,7))\nx=train[train['sentiment']=='negative']['text'].str.len()\ny=train[(train['sentiment']=='negative')]['jaccard'].values.tolist()\nax1.scatter(x,y,color='red',alpha=.4)\nax1.set_xlabel('text length')\nax1.set_ylabel('jaccard similarity with selected text')\nax1.set_title(\"text length vs jaccard similarity\")\nx=train[train['sentiment']=='negative']['text'].apply(lambda x : len(x.split()))\nax2.scatter(x,y,color='red',alpha=.4)\nax2.set_xlabel('text length')\nax2.set_ylabel('jaccard similarity with selected text')\nax2.set_title(\"no: of words vs jaccard similarity\")","1637183c":"def get_sent(text):\n    testimonial = TextBlob(str(text))\n    return testimonial.sentiment.polarity\n\nplt.figure(figsize=(10,7))\ntrain['polarity']=train['selected_text'].apply(lambda x : get_sent(x))\nsns.boxplot(x='sentiment', y='polarity', data=train)\nplt.gca().set_title('Sentiment vs Polarity of selected text')\nplt.show()\n\n","d4e2ef78":"import scipy\ncorr=[]\nfor i in sent:\n    text_pos=train[train['sentiment']==i]['text'].astype(str).map(lambda x : len(x.split()))\n    sel_pos=train[train['sentiment']==i]['selected_text'].astype(str).map(lambda x : len(x.split()))\n    corr.append(scipy.stats.pearsonr(text_pos,sel_pos)[0])\nplt.bar(sent,corr,color='blue',alpha=.7)\nplt.gca().set_title(\"pearson corr between no: words in text and selected text\")\nplt.gca().set_ylabel(\"correlation\")","1256908c":"from transformers import *\nimport tensorflow as tf\nimport tokenizers","23777dcd":"MAXLEN=128\nPATH = '..\/input\/tf-roberta\/'\ntokenizer=tokenizers.ByteLevelBPETokenizer(vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True)\n\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\n\n","d49b35bb":"ct=train.shape[0]\ninput_ids=np.ones((ct,MAXLEN),dtype='int32')\nattention_mask=np.zeros((ct,MAXLEN),dtype='int32')\ntoken_type_ids=np.zeros((ct,MAXLEN),dtype='int32')\nstart_tokens=np.zeros((ct,MAXLEN),dtype='int32')\nend_tokens=np.zeros((ct,MAXLEN),dtype='int32')\n\n\nfor k in tqdm(range(ct)):\n    \n    text1=\" \"+\" \".join(train.loc[k,\"text\"].split())\n    text2=\" \".join(train.loc[k,\"selected_text\"].split())\n    idx=text1.find(text2)\n    chars=np.zeros(len(text1))\n    chars[idx:idx+len(text2)]=1\n    if (text1[idx-1]==\" \"):\n        chars[idx-1]=1\n    enc=tokenizer.encode(text1)\n    \n    offsets=enc.offsets\n    \n    toks=[]\n    for i,(a,b) in enumerate(offsets):\n        sm=np.sum(chars[a:b])\n        if sm > 0:\n            toks.append(i)\n            \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1\n    \n    \n","5275904f":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAXLEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAXLEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAXLEN),dtype='int32')\n\nfor k in tqdm(range(test.shape[0])):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1\n\n","d25fe1e3":"def build_model():\n    ids = tf.keras.layers.Input((MAXLEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAXLEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAXLEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","07262d43":"model=build_model()","f98107d3":"import os\nmodels=os.listdir(\"..\/input\/tfroberta5fold6epochs\")\nmodel_file_path=\"..\/input\/tfroberta5fold6epochs\/\"","78bfe79c":"start_token=np.zeros((ct,MAXLEN),dtype='float64')\nend_token=np.zeros((ct,MAXLEN),dtype='float64')\n\nfor model_path in models:\n    model.load_weights(model_file_path+model_path)\n    pred=model.predict([input_ids_t,attention_mask_t,token_type_ids_t])\n    \n    start_token+=pred[0]\/len(models)\n    end_token+=pred[1]\/len(models)\n\n","cde84530":"start_token=np.argmax(start_token,axis=1)\nend_token=np.argmax(end_token,axis=1)\nselected_text=[]\nfor k in range(ct):\n    \n    a=start_token[k]\n    b=end_token[k]\n    if a<b:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        selected_text.append(tokenizer.decode(enc.ids[a-1:b]))\n    else:\n        if test.loc[k,'sentiment']==\"neutral\":\n            selected_text.append(test.loc[k,'text'])\n        else:\n            text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            selected_text.append(tokenizer.decode(enc.ids[b-1:a]))\n            \n    ","6a4f8375":"test['selected_text']=selected_text\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.head(5)","33dd3dce":"del model\ngc.collect()","9a210081":"cols=['textID','text','sentiment','selected_text']\ntrain_df=train[cols].copy()\ndel train\ntest_df=test.copy()\ndel test\ngc.collect()","0864bfd5":"\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nimport gc\nimport os","b19192fa":"train_df=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")","dedb43f4":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","57096937":"tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')  ## change it to commit\n\n# Save the loaded tokenizer locally\nsave_path = '\/kaggle\/working\/distilbert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased\/vocab.txt', lowercase=True)\nfast_tokenizer","a8f89f16":"x_train = fast_encode(train_df.text.astype(str), fast_tokenizer, maxlen=128)\nx_test = fast_encode(test_df.text.astype(str),fast_tokenizer,maxlen=128)","28d14a63":"transformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')","4859d6a1":"def create_targets(df):\n    df['t_text'] = df['text'].apply(lambda x: tokenizer.tokenize(str(x)))\n    df['t_selected_text'] = df['selected_text'].apply(lambda x: tokenizer.tokenize(str(x)))\n    def func(row):\n        x,y = row['t_text'],row['t_selected_text'][:]\n        for offset in range(len(x)):\n            d = dict(zip(x[offset:],y))\n            #when k = v that means we found the offset\n            check = [k==v for k,v in d.items()]\n            if all(check)== True:\n                break \n        return [0]*offset + [1]*len(y) + [0]* (len(x)-offset-len(y))\n    df['targets'] = df.apply(func,axis=1)\n    return df\n\ntrain_df = create_targets(train_df)\n\nprint('MAX_SEQ_LENGTH_TEXT', max(train_df['t_text'].apply(len)))\nprint('MAX_TARGET_LENGTH',max(train_df['targets'].apply(len)))\nMAX_TARGET_LEN=108","5048b504":"train_df['targets'] = train_df['targets'].apply(lambda x :x + [0] * (MAX_TARGET_LEN-len(x)))\ntargets=np.asarray(train_df['targets'].values.tolist())\n","3d35dcc1":"lb=LabelEncoder()\nsent_train=lb.fit_transform(train_df['sentiment'])\nsent_test=lb.fit_transform(test_df['sentiment'])","c75912a8":"def new_model(transformer_layer):\n    \n    inp = Input(shape=(128, ))\n    inp2= Input(shape=(1,))\n    \n    embedding_matrix=transformer_layer.weights[0].numpy()\n\n    x = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n\n    x = CuDNNLSTM(150, return_sequences=True,name='lstm_layer',)(x)\n    x = CuDNNLSTM(100, return_sequences=False,name='lstm_layer-2',)(x)\n    \n    y =Dense(10,activation='relu')(inp2)\n    x= concatenate([x,y])\n    \n    x = Dense(MAX_TARGET_LEN,activation='sigmoid')(x)\n\n    model = Model(inputs=[inp,inp2], outputs=x)\n\n    model.compile(loss='binary_crossentropy',\n                      optimizer='adam')\n\n\n    #prinnt(model.summary())\n    \n    return model","bbb70686":"\nmodel=new_model(transformer_layer)\nhistory=model.fit([x_train,sent_train],targets,epochs=3)","3d316d32":"predictions=model.predict([x_test,sent_test])","e6ea9bbe":"\n\ndef convert_output(sub,predictions):\n    preds=[]\n    for i,row in enumerate(sub['text']):\n\n        text,target=row.lower(),predictions[i].tolist()\n        target=np.round(target).tolist()\n        try:\n            start,end=target.index(1),target[::-1].index(1)\n            text_list=tokenizer.tokenize(text)\n            text_list=text_list+((108-len(text_list))*['pad'])\n            start_w,end_w=text_list[start],text_list[-end]\n            start=text.find(start_w.replace(\"#\",'',1))    ## remove # to match substring\n            end=text.find(end_w.replace(\"#\",''),start)\n            #pred=' '.join([x for x in text_list[start:-end]])\n            pred=text[start:end]\n        except:\n            pred=text\n        \n        preds.append(pred)\n        \n    return preds\n","d0f02e93":"prediction_text=convert_output(test_df,predictions)","5fcce700":"len(prediction_text)","380b53b4":"- We load the Distilbert pretained tokenizer (uncased) and save it to directory.\n- Reload and use BertWordPieceTokenizer.\n- An implementation of a tokenizer consists of the following pipeline of processes, each applying different transformations to the textual information:\n![](https:\/\/miro.medium.com\/max\/1400\/1*7uy9X3eE1rVmqV08yKrDgg.png)","4ceb8423":"- Mean score of tweets is ~70,so it is considered as fairly easy to read.","d0fb9618":"- Now we load the pretrained bert ('uncased') transformer layer.\n- This is used for creating the representations and training our corpus.","5bc66985":"hmm..the distribution seems to be quite similar,let us check for selected text","e56b6d91":"## <font size='4' color='red'>Tensorflow roberta model<\/font>","fa9af91c":"Let's do the same for selected_text and see the distribution","39935fc5":"## <font size='5' color='red'>Contents<\/font>\n\n* [introduction](#1)\n\n* [Loading required packages](#2)\n\n* [Getting basic Ideas](#3) \n\n    * [Basic Cleaning]()\n    \n* [Exploratory Data analysis](#4)\n\n    * [Class distribution]()\n    * [Distribution of length of tweets]()\n    * [Distribution of number of words in tweets]()\n    * [Distribution of target]()\n    * [Common Stopwords]()\n    * [Common words in tweets w\/o stopwords]()\n    * [Common bigrams in tweeets]()\n    * [WordClouds of tweets]()\n    * [Readability index](#5)\n    \n* [Hints for post-processing](#9)\n\n    * [analyzing jaccard similarity]()\n\n\n\n* [Bert-Lstm Model](#7)\n    * [Data preparation]()\n    * [Model]()\n    * [Post processing]()\n    * [Making our submission]()\n\n## <font size='4' color='red'>If you find this kernel useful,consider doing an upvote..this will motivate me create more content<\/font>","b30f3bfd":"- Now we need to make each output of the same length to feed it to the neural network.\n- For that we find the maxlength of the target and pad all other target to this length.","d2b6a85f":"## <font size='4' color='green'>Loading Required packages<\/font><a id='2'><\/a>\n","14f56870":"### <font size='3' color='blue'>WordCloud for tweets<\/font>","16a64234":"## <font color='blue' size='4'>Which are the most common words in selected text?<\/font>","ffc0fe83":"- Well,you can see the difference.\n- `'positve` tweets are filled with positive words like `great day`,'`happy day` etc.\n- `negative` tweets have bigrams like `im sorry`, and many words starting with `dont`.","21ea06bb":"## <font size='4' color='red'>Hints for post processing<\/font><a id='9'><\/a>","d6398230":"### <font size='3' color='blue'>Distribution of no: punctuations in tweets<\/font>","a00f27bd":"- The function below is used to convert the output to text format.\n","ef5f131d":"- It is evident that there are more positive words like `good`,`thanks` etc on positive tweets.\n- It can be observed that there are a lot of negative words in negative tweets.","a9f4fff8":"### <font size='3' color='blue'>What is the distribution of selected text?<\/font>","955d97ae":"## <font size='4' color='blue'>Which are the most common words?<\/font> ","3efabfc6":"1. `the` dominated in the list of common stopwords,followed by `and` and '`you`","76a209a9":"- This is a multi-input model (comment+sentiment label).\n- I have made a simple LSTM model\n- concatenated both the inputs ","0783cb93":"### <font size='3' color='blue'>Wordcloud for selected text<\/font>","87929519":"- Now the comment text is prepared and encoded using this tokenizer easily.\n- We here set the maxlen=128,(limit)","3ef12ec8":"- There is a class imbalance between labels,neutral class dominates over the other class tweets.\n- The distributions of labels follows the same trend in train and test.\n\n**But our task is not to predict these labels,but to predict the selected text which can help us figure out the sentiment**","7fcc9302":"- Many many thanks to one of the most wonderful kaggler @chrisdeotte for his [kernel](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\/data).Take a second to upvote his work.\n- I have done an inference with this by training 6 epochs and 5 folds.\n\n","1798929d":"- The selected test is of much larger length in neutral sentiment tweets.Why is this ? We wil find out later..\n- The distribution of selected texts in positive and negative are almost the same.","4014cdc1":"### <font size='3' color='blue'>Check for bigrams in selected text<\/font>","40db746b":"### <font size='3' color='blue'>What is the distribution of Number of words in a tweet?<\/font>","f8e95bdf":"### Data preparation","33111e38":"This code is lifted from [kernel](https:\/\/www.kaggle.com\/gskdhiman\/bert-baseline-starter-kernel#Training).\n- In this section we create the representaion for the selected text from tweet text.\n- The representation is created such that the positions of tokens which is selcted from text is represented with 1 and others with 0.\n- for example,consider the tweet `\" I have a cute dog\"` and selected text `\"cute dog\"`\n- This produces the ouput as ` [0,0,0,1,1]`","10ed12e0":"- The above trend repreats here ,here it is more evident and clear.\n- Let's dig bit further for positive ane negetive words...\n","4f9f94d2":"Same here !\nI think in this situation we should check the distribution of sentiment polarity of selected text.See if it actually matches with the label or if there are any outliers.","175bd0de":"Couldn't find any evidence or relation between them in positive tweets.Let's check for negative tweets","a44054d6":"## <font size='4' color='green'>Exploratory Data Analysis<\/font><a id='4'><\/a>","060334e4":"sub=pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nsub['selected_text']=prediction_text\nsub.to_csv('submission.csv',index=False)\nsub.head()","6e4ddd84":"## <font size='5' color='green'>Introduction<\/font><a id='1'><\/a>","ac7f1b8b":"### <font size='3' color='blue'>Number of unique words in tweets<\/font>","942ade77":"In this section I am trying to build a multi-input model using BERT embedding for predicting the target keyphrases.\nI will explain my approach step by step,this is a naive approach and will be improved later.","563c275e":"## <font color='green' size='4'>Readability features<\/font><a id='5'><\/a>\n\nReadability is the ease with which a reader can understand a written text. In natural language processing, the readability of text depends on its content. It focuses on the words we choose, and how we put them into sentences and paragraphs for the readers to comprehend.\n9.1 The Flesch Reading Ease formula\n\n   - In the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. The formula for the Flesch reading-ease score (FRES) test is \n![](data:image\/png;base64,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)   \n   \n   \n   \n  -  90-100 - Very Easy\n  -  80-89 - Easy\n  -  70-79 - Fairly Easy\n  -  60-69 - Standard\n  -  50-59 - Fairly Difficult\n  -  30-49 - Difficult\n  -  0-29 - Very Confusing\n","74acd94e":"Yes,this  follows the same trend as of number of characters.","4fe05ba9":"- Let's first take a look at the class distribution of sentiment label","91f42814":"## <font size='4' color='blue'>Work in progress !!!<\/font> \n## <font size='4' color='red'>Do an upvote if you think this was helpful<\/font> ","1f8122ef":"- Make predictions on test","96f5cbc1":"- Fitting the model ","68ded9a9":"- The number of words in tweets ranges from 1 to 30,5-10 being the most common choice.","036e78b3":"### <font size='3' color='blue'>Which are the most common stopwords?<\/font>","8945e575":"- We need to use the sentiment as a feature,for this encode it using LabelEncode.","b48546b9":"- Below function is from this [kernel](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras) by @xhlulu,this is used to encode the sentences easily and quickly using distilbert tokenizer.","c128d5ef":"## Making our submission","21d9c371":"- Are these examples due to some manual errors? There seems to be positive keywords in them like \"love\",\"cute\",\"welcome\" etc..\n- let's examine if this has anything to do with text length","3433f95b":"## Post processing","c23426b4":"## Model","91f0fd4a":"## <font size='4' color='green'>Getting Basic Ideas<\/font><a id='3'><\/a>","2bb86e79":"- The number of character in tweets ranges from 1 to 160 (max).\n- More tweets contains less than 60 characters.\n","a77bad94":"- All the distributions are left skewed.\n- There happens to be more punctuations in positive tweets.","6d8d0f65":"### <font size='3' color='blue'>What's the distribution of length of tweet?<\/font>","d34c4745":"### <font size='3' color='blue'>Common punctuations in tweets<\/font>","698d440e":"### <font size='3' color='blue'>What is distribution of number of words in selected text?<\/font>","fbae1756":"Wow,the results look pretty satisfactory,but there are outliers especially in the negative tweets.\n\nlet's check the correlation between the number of words in tweet and selected text\n","9e0178e2":"- here we can easily observe that the jaccard similarity of most of samples with negative sentiment is close to 1,which means that most of the text is in selected text.\n- Also there are some examples in positive and negative tweets with similarity of 1 (entire text).We will investigate them too.","587ccaa7":"- For negative and positive tweets this distributions is similar,this should be because of selected keywords from tweets.\n- In case of neutral tweets it is spread and there aren't a specefic keyword to select.\n- This may cause neutral class to be difficult to predict.","896ba39a":"## <font size='4' color='blue'>BERT Embeddings with LSTM Model<\/font><a id='5'><\/a>","d745d177":"With all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\nHelp build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment?\n![](https:\/\/media.giphy.com\/media\/xUPGcEOEllmvFAvako\/giphy.gif)"}}