{"cell_type":{"e01fda60":"code","ad894882":"code","ccde35de":"code","9c9cbbef":"code","f3e5f1ac":"code","ab553791":"code","bbd2d5a3":"code","6997230f":"code","04c872e1":"code","d5620738":"code","39ef9a1f":"markdown","83e83c15":"markdown","bff46623":"markdown","14d149be":"markdown","2be6e0af":"markdown","549e3f0b":"markdown","69ca69c7":"markdown","2f35b0aa":"markdown","f3894aa6":"markdown","129c6b6c":"markdown","9c0a79bf":"markdown","2bf9f41b":"markdown","b61a5de2":"markdown","ba0e4d9e":"markdown","c0eb3da1":"markdown","0b99a806":"markdown","518dfbd3":"markdown","30fa5222":"markdown","b1008710":"markdown","af2cf7b1":"markdown","8ffb44ea":"markdown","54ab74f0":"markdown","c5d3c4bd":"markdown"},"source":{"e01fda60":"# !pip install gplearn\nimport gplearn\nprint('ok')","ad894882":"import os\nimport shutil\nimport copy\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom gplearn.functions import make_function\nfrom gplearn.genetic import SymbolicRegressor\nfrom sklearn.model_selection import KFold\nprint('ok')","ccde35de":"print(os.listdir('..\/..'))\nprint(os.listdir('..\/input'))\nprint(os.listdir('..\/input\/gppyfiles'))","9c9cbbef":"DATA_DIR = '..\/input\/gplearn-data'\nsubmission = pd.read_csv(os.path.join('..\/input\/LANL-Earthquake-Prediction', 'sample_submission.csv'), index_col='seg_id')\nscaled_train_X = pd.read_csv(os.path.join(DATA_DIR, 'scaled_train_X_AF0.csv'))\nscaled_test_X = pd.read_csv(os.path.join(DATA_DIR, 'scaled_test_X_AF0.csv'))\ntrain_y = pd.read_csv(os.path.join(DATA_DIR, 'train_y_AF0.csv'))\npredictions = np.zeros(len(scaled_test_X))\nprint('ok')","f3e5f1ac":"from scipy.stats import pearsonr\ny = train_y['time_to_failure'].values\npcol = []\npcor = []\npval = []\nfor col in scaled_train_X.columns:\n    pcol.append(col)\n    pcor.append(abs(pearsonr(scaled_train_X[col], y)[0]))\n    pval.append(abs(pearsonr(scaled_train_X[col], y)[1]))\n\ndf = pd.DataFrame(data={'col': pcol, 'cor': pcor, 'pval': pval}, index=range(len(pcol)))\ndf.sort_values(by=['cor', 'pval'], inplace=True)\ndf.dropna(inplace=True)\ndf = df.loc[df['pval'] <= 0.05]\n\ndrop_cols = []\n\nfor col in scaled_train_X.columns:\n    if col not in df['col'].tolist():\n        drop_cols.append(col)\n\nscaled_train_X.drop(labels=drop_cols, axis=1, inplace=True)\nscaled_test_X.drop(labels=drop_cols, axis=1, inplace=True)\nprint(scaled_train_X.shape, scaled_test_X.shape)","ab553791":"# in an IDE this works as a lambda - not so in colab - have not tried a lambda on kaggle\nfrom gplearn.functions import make_function\n\ndef th(x):\n    return np.tanh(x)\n\ngptanh = make_function(th, 'tanh', 1)\nprint('ok')","bbd2d5a3":"# import os\n# import sys\n# sys.path.append('..\/input\/gppyfiles')\n# from gppyfiles.gplearn_tanh import gptanh\nfunction_set = ['add', 'sub', 'mul', 'div', 'inv', 'abs', 'neg', 'max', 'min', gptanh]  # 'sqrt', 'log', \nprint('ok')","6997230f":"predictions = np.zeros(len(scaled_test_X))\n\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\n\nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"Feature\"] = scaled_train_X.columns\nprint('ok')","04c872e1":"sample_wts = np.sqrt(np.array([x - 10.0 if x > 10.0 else 0 for x in y]) + 1.0)\nprint(y[0:16])\nprint(sample_wts[-8:])\nprint('ok')","d5620738":"# still needs cleanup\n# GENS = 500\n# MAE_THRESH = 2.5\n# MAX_NO_IMPROVE = 50\n# np.random.seed(666)\n# maes = []\n# gens = []\n\n# for fold_, (trn_idx, val_idx) in enumerate(folds.split(scaled_train_X, train_y.values)):\n#   print('working fold %d' % fold_)\n#   X_tr, X_val = scaled_train_X.iloc[trn_idx], scaled_train_X.iloc[val_idx]\n#   y_tr, y_val = train_y['time_to_failure'].values[trn_idx].ravel(), train_y['time_to_failure'].values[val_idx].ravel()\n#   sample_wts_tr = sample_wts[trn_idx]\n#   np.random.seed(5591 + fold_)\n#   best = 1e10\n#   count = 1\n#   imp_count = 0\n#   best_mdl = None\n#   best_iter = 0\n  \n#   gp = SymbolicRegressor(population_size=2000,\n#                        generations=count,\n#                        tournament_size=50,  # consider 20, was 50\n#                        parsimony_coefficient=0.0001,  # oops: 0.0001?\n#                        const_range=(-16, 16),  # consider +\/-20, was 100\n#                        function_set=function_set,\n#                        # stopping_criteria=1.0,\n#                        # p_hoist_mutation=0.05,\n#                        # max_samples=.875,  # was in\n#                        # p_crossover=0.7,\n#                        # p_subtree_mutation=0.1,\n#                        # p_point_mutation=0.1,\n#                        init_depth=(6, 16),\n#                        warm_start=True,\n#                        metric='mean absolute error', verbose=1, random_state=42, n_jobs=-1, low_memory=True)\n\n#   for run in range(GENS):\n#       mdl = gp.fit(X_tr, y_tr, sample_weight=sample_wts_tr)\n#       pred = gp.predict(X_val)\n#       mae = np.sqrt(mean_absolute_error(y_val, pred))\n\n#       if mae < best and imp_count < MAX_NO_IMPROVE:\n#           best = mae\n#           count += 1\n#           gp.set_params(generations=count, warm_start=True)\n#           imp_count = 0\n#           best_iter = run\n#           if mae < MAE_THRESH:\n#               best_mdl = copy.deepcopy(mdl)\n#       elif imp_count < MAX_NO_IMPROVE:\n#           count += 1\n#           gp.set_params(generations=count, warm_start=True)\n#           imp_count += 1\n#       else:\n#           break\n\n#       print('GP MAE: %.4f, Run: %d, Best Run: %d, Fold: %d' % (mae, run, best_iter, fold_))\n\n#   maes.append(best)\n#   gens.append(run)\n      \n#   print('Finish - GP MAE: %.4f, Run: %d, Best Run: %d' % (mae, run, best_iter))\n          \n#   preds = best_mdl.predict(scaled_test_X)\n#   print(preds[0:12])\n#   predictions += preds \/ folds.n_splits\n\n# try:\n#     print(maes)\n#     print(np.mean(maes))\n#     print(gens)\n# except:\n#     print('oops')\n# submission.time_to_failure = predictions\n# submission.to_csv('submission_gplearn_AF0_1.csv')\n# print(submission.head(12))\nprint('ok')","39ef9a1f":"### Notes","83e83c15":"Import our tanh function and create a custom function set for our gplearn session that includes it.","bff46623":"Optional: Reduce feature set by eliminating apparently useless features using Pearsons.","14d149be":"Have no clue how to match the quality of results obtained by Scirpus in genetic programming!  Tutorials for gplearn appear to be sadly lacking on the web.  It is not so clear on how to best change hyperparameters.","2be6e0af":"The code was originally run with a feature set generated by code from my Masters Degree project.  Please see my [Masters Final Project \u2013 Model; LB~1.392](https:\/\/www.kaggle.com\/vettejeep\/masters-final-project-model-lb-1-392) for that feature generation code.  This notebook is not a part of my degree project, it is just an interest area.","549e3f0b":"### Introduction","69ca69c7":"## Attempt at Genetic Programming with GPLearn","2f35b0aa":"Switched this over to [\"Andrews Features\"](https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples).  Still dicey as whether it will train in 9 hours, so the training code is commented out.  I do not have a leader board score with this yet.","f3894aa6":"Make a 'tanh' function for gplearn.  I suspect it needs to be in a top level module to pickle properly in the notebook, so create as a separate file (gplearn_tanh.py) and import. Here is code for the import file. ","129c6b6c":"Optional sample weighting.  I tried to up-weight the samples above 10 seconds because they are uncommon, in the hope the model would fit them better.","9c0a79bf":"The gplearn library is at https:\/\/gplearn.readthedocs.io\/en\/stable\/.  It is a simple pip install & appears to be on Kaggle already.","2bf9f41b":"Genetic program model, main code loop.","b61a5de2":"Set up the folds for cross validation.","ba0e4d9e":"### Impressions of gplearn","c0eb3da1":"Get gplearn, install if needed.  Kaggle appears to have it installed.","0b99a806":"Lacks built-in 'tanh' function, but it is easy to make our own.  Tanh has helped toy regression models that I have tried out.","518dfbd3":"This is an attempt at genetic programming with gplearn in Python.  It is a learning exercise, not a serious attempt at the leader board.  The leader board score was 1.561, so there is a long way to go before I have enough knowledge of genetic programming to consider my efforts with gplearn being competitive with boosted trees.  Still, 1.561 seems to be a reasonable start and clearly indicates that the alorithm is learning.  Hopefully this can serve as a community learning exercise for interested Kagglers.  Maybe we can improve it together.  All sugestion for improvement is requested and appreciated.  Please feel free to fork and re-post with your improvements. ","30fa5222":"Imports.","b1008710":"The gplearn implementation of early stopping seems useless, so the call to fit() is implemented in a loop with warm start and a simple early stop based upon a failure to improve metric.","af2cf7b1":"### Coding Attempt","8ffb44ea":"#### Vettejeep","54ab74f0":"Load data files.","c5d3c4bd":"Trains slowly but seems to consume few resources.  Locally I trained three instances at once without a problem."}}