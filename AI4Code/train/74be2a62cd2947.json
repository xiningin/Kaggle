{"cell_type":{"0d92798b":"code","661e55c8":"code","497863d6":"code","32f032bd":"code","00227d11":"code","1bca0ccb":"code","2d6c9c64":"code","ae531504":"code","c848f2fd":"code","d67f0831":"code","984a8166":"code","9dca9be2":"code","52b9ed78":"code","aa91b7a3":"code","137135c5":"code","268a38e4":"markdown","c65586a0":"markdown","9ff7754d":"markdown","fdec3859":"markdown","9cd4cf4f":"markdown","1c210ced":"markdown","a296e521":"markdown","ed9ccbb3":"markdown","22efe8c9":"markdown","7fb4c356":"markdown","f55c68f1":"markdown","5d72ee09":"markdown","2e42b5e8":"markdown","b58c7aac":"markdown","d0e0745f":"markdown","5bf9bd0a":"markdown","fe4a12b0":"markdown"},"source":{"0d92798b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom tensorflow.keras import models, layers, optimizers, losses, metrics\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg19 import VGG19\nimport os\nimport shutil\n%matplotlib inline","661e55c8":"paths = {\n    'training': '\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/train\/',\n    'validation': '\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/val\/',\n    'test': '\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/test\/'\n}\ncategories = {0: 'normal', 1: 'pneumonia'}\ndataset = pd.DataFrame()\nfor cat_number, cat_name in categories.items():\n    for split, path in paths.items():\n        combined_path = os.path.join(path, cat_name.upper())\n        files = os.listdir(combined_path)\n        for i, file in enumerate(files):\n            files[i] = os.path.join(combined_path, file)\n        tmp = pd.DataFrame()\n        tmp['filename'] = files\n        tmp['class'] = cat_number\n        dataset = dataset.append(tmp)\ndataset = dataset.sample(frac=1).reset_index(drop=True) # Shuffle Dataset\ndisplay(dataset.head())","497863d6":"pneumonia_samples = dataset[dataset['class']==1]['filename'].iloc[:5]\nnormal_samples = dataset[dataset['class']==0]['filename'].iloc[:5]\n\nsamples = pneumonia_samples.append(normal_samples).to_list()\n\n_, ax = plt.subplots(2,5, figsize=(30,10))\nfor i in range(10):\n    img = imread(samples[i])\n    ax[i\/\/5, i%5].imshow(img, cmap='gray')\n    if i<5:\n        ax[i\/\/5, i%5].set_title(\"Pneumonia\")\n    else:\n        ax[i\/\/5, i%5].set_title(\"Normal\")\n    ax[i\/\/5, i%5].axis('off')\n    ax[i\/\/5, i%5].set_aspect('auto')\nplt.show()","32f032bd":"# Plot Image Distribution\ncategories = ['Normal', 'Pneumonia']\nfrequencies = list(dataset['class'].value_counts())[::-1]\nplt.bar(categories, frequencies)\nplt.xlabel(\"Categories\")\nplt.ylabel(\"Count\")\nplt.title(f'Data Distribution')\nplt.show()","00227d11":"# Create train, test, validation splits\ntrain_split = dataset.iloc[:round(len(dataset) * 0.8)]\nval_split = dataset.iloc[round(len(dataset) * 0.8): round(len(dataset) * 0.9)]\ntest_split = dataset.iloc[round(len(dataset) * 0.9):]\n\ndirectories = {'train': train_split, 'validation': val_split, 'test': test_split}\n\nfor directory, split in directories.items():\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n    os.mkdir(directory)\n    normal_path = os.path.join(directory, 'normal')\n    pneumonia_path = os.path.join(directory, 'pneumonia')\n    os.mkdir(normal_path)\n    os.mkdir(pneumonia_path)\n    for index, entry in split.iterrows():\n        file = os.path.basename(entry['filename'])\n        src = entry['filename']\n        dst = os.path.join(normal_path, file) if entry['class'] == 0 else os.path.join(pneumonia_path, file)\n        shutil.copyfile(src, dst)","1bca0ccb":"METRICS = [\n    metrics.BinaryAccuracy(name='accuracy'),\n    metrics.Precision(name='precision'),\n    metrics.Recall(name='recall'),\n    metrics.AUC(name='auc'),\n    metrics.AUC(name='prc', curve='PR')\n]\n\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation = layers.LeakyReLU(), input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n    layers.Conv2D(64, (3, 3), activation = layers.LeakyReLU()),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n    layers.Conv2D(64, (3, 3), activation = layers.LeakyReLU()),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(512, activation=layers.LeakyReLU()),\n    layers.Dropout(0.25),\n    layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizers.Adam(learning_rate=0.00175),\n    loss='binary_crossentropy',\n    metrics=METRICS\n)\nmodel.summary()","2d6c9c64":"train_datagen = ImageDataGenerator(\n    rescale=1\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True\n)\nvalidation_datagen = ImageDataGenerator(rescale=1\/255)\ntest_datagen = ImageDataGenerator(rescale=1\/255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    'train',\n    target_size=(28, 28),\n    batch_size=32,\n    color_mode='grayscale',\n    class_mode='binary'\n)\nvalidation_generator = validation_datagen.flow_from_directory(\n    'validation\/',\n    target_size=(28, 28),\n    batch_size=32,\n    color_mode='grayscale',\n    class_mode='binary'\n)\ntest_generator = test_datagen.flow_from_directory(\n    'test\/',\n    target_size=(28, 28),\n    batch_size=32,\n    color_mode='grayscale',\n    shuffle=False,\n    class_mode='binary'\n)\n\nhistory = model.fit(\n    train_generator,\n    epochs=30,\n    validation_data=validation_generator,\n)\nmodel.save('models\/model_1.h5')","ae531504":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.title(\"Accuracy over Time\")\nplt.legend()\nplt.show()\n\nplt.plot(history.history['precision'], label='precision')\nplt.plot(history.history['val_precision'], label = 'val_precision')\nplt.xlabel('Epochs')\nplt.ylabel('Precision')\nplt.ylim(0, 1)\nplt.title(\"Precision over Time\")\nplt.legend()\nplt.show()\n\nplt.plot(history.history['recall'], label='recall')\nplt.plot(history.history['val_recall'], label = 'val_recall')\nplt.xlabel('Epochs')\nplt.ylabel('Recall')\nplt.ylim(0, 1)\nplt.title(\"Recall over Time\")\nplt.legend()\nplt.show()\n\nplt.plot(history.history['auc'], label='auc')\nplt.plot(history.history['val_auc'], label = 'val_auc')\nplt.xlabel('Epochs')\nplt.ylabel('AUC')\nplt.ylim(0, 1)\nplt.title(\"AUC over Time (ROC Curve)\")\nplt.legend()\nplt.show()\n\nplt.plot(history.history['prc'], label='prc')\nplt.plot(history.history['val_prc'], label = 'val_prc')\nplt.xlabel('Epochs')\nplt.ylabel('AUC')\nplt.ylim(0, 1)\nplt.title(\"AUC over Time (PRC Curve)\")\nplt.legend()\nplt.show()\n\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.ylim(0, 1)\nplt.title(\"Loss over Time\")\nplt.legend()\nplt.show()","c848f2fd":"y_pred = np.round(model.predict(test_generator)).flatten()\ny_true = test_generator.classes\ndisplay(ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred), display_labels=['Normal', 'Pneumonia']).plot())","d67f0831":"# Evaluating model on the test set\nmodel.evaluate(test_generator)","984a8166":"# Using VGG19\nconvolutional_base = VGG19(\n    include_top=False,\n    input_shape=(32, 32, 3),\n)\nconvolutional_base.trainable = False\nvgg_model = models.Sequential([\n    convolutional_base,\n    layers.Flatten(),\n    layers.Dense(512, activation=layers.LeakyReLU()),\n    layers.Dropout(0.25),\n    layers.Dense(1, activation='sigmoid')\n])\n\nvgg_model.compile(\n    optimizers.Adam(learning_rate=0.00175),\n    loss='binary_crossentropy',\n    metrics=METRICS\n)\n\ndisplay(vgg_model.summary())","9dca9be2":"train_generator_vgg = train_datagen.flow_from_directory(\n    'train',\n    target_size=(32, 32),\n    batch_size=32,\n    class_mode='binary'\n)\nvalidation_generator_vgg = validation_datagen.flow_from_directory(\n    'validation\/',\n    target_size=(32, 32),\n    batch_size=32,\n    class_mode='binary'\n)\ntest_generator_vgg = test_datagen.flow_from_directory(\n    'test\/',\n    target_size=(32, 32),\n    shuffle=False,\n    batch_size=32,\n    class_mode='binary'\n)\n\nvgg_history = vgg_model.fit(\n    train_generator_vgg,\n    epochs=30,\n    validation_data=validation_generator_vgg,\n)\nvgg_model.save('models\/vgg_19_cnn.h5')","52b9ed78":"plt.plot(vgg_history.history['accuracy'], label='accuracy')\nplt.plot(vgg_history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.title(\"Accuracy over Time\")\nplt.legend()\nplt.show()\n\nplt.plot(vgg_history.history['precision'], label='precision')\nplt.plot(vgg_history.history['val_precision'], label = 'val_precision')\nplt.xlabel('Epochs')\nplt.ylabel('Precision')\nplt.ylim(0, 1)\nplt.title(\"Precision over Time\")\nplt.legend()\nplt.show()\n\nplt.plot(vgg_history.history['recall'], label='recall')\nplt.plot(vgg_history.history['val_recall'], label = 'val_recall')\nplt.xlabel('Epochs')\nplt.ylabel('Recall')\nplt.ylim(0, 1)\nplt.title(\"Recall over Time\")\nplt.legend()\nplt.show()\n\nplt.plot(vgg_history.history['auc'], label='auc')\nplt.plot(vgg_history.history['val_auc'], label = 'val_auc')\nplt.xlabel('Epochs')\nplt.ylabel('AUC')\nplt.ylim(0, 1)\nplt.title(\"AUC over Time (ROC Curve)\")\nplt.legend()\nplt.show()\n\nplt.plot(vgg_history.history['prc'], label='prc')\nplt.plot(vgg_history.history['val_prc'], label = 'val_prc')\nplt.xlabel('Epochs')\nplt.ylabel('AUC')\nplt.ylim(0, 1)\nplt.title(\"AUC over Time (PRC Curve)\")\nplt.legend()\nplt.show()\n\nplt.plot(vgg_history.history['loss'], label='loss')\nplt.plot(vgg_history.history['val_loss'], label = 'val_loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.ylim(0, 1)\nplt.title(\"Loss over Time\")\nplt.legend()\nplt.show()","aa91b7a3":"y_pred = np.round(vgg_model.predict(test_generator_vgg)).flatten()\ny_true = test_generator_vgg.classes\ndisplay(ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred), display_labels=['Normal', 'Pneumonia']).plot())","137135c5":"vgg_model.evaluate(test_generator_vgg)","268a38e4":"## 3.2 Evaluating Results","c65586a0":"We evaluated the model by observing its performance metrics with increasing number of epochs. Our performance metrics were accuracy, precision, recall, area under the curve (AUC) of the ROC (Recieving Operator Characteristic) curve as well as the PRC (Precision Recall Curve).","9ff7754d":"## 2.3 Evaluating Model Results","fdec3859":"## 2.1 Defining Model Architecture\nOur classifier will be a Convolutional Neural Network (CNN). CNN's are great for image classification since they can learn local patterns in the image, and they can recognize a pattern if it appears in multiple locations of the image. CNN's can learn the spatial hierarchies of patterns, starting with very low level patterns and building up to learn general patterns in the image. The input to our network is a 28x28 grayscale image, leading to an input shape of (28, 28, 1).\n\nThe patterns learned from using convolutional layers alone will be small in comparison to the original image, which is not enough to classify x-ray images as that of a pneumonia patient vs a normal patient. Another problem is that the feature maps (the outputs of a convolutional layer) has a very large number of coefficients per sample. Unless the neural network is big enough, having this many coefficients would result in massive overfitting. To combat these isues, we use max pooling layers in addition to our convolutional layers. After our convolutions and max pooling layers, we flatten our output and pass it along to dense layers to complete our classification. We also introduce dropout layers to lower the risk of overfitting. All the layers are constructed with LeakyReLU as the activation function.\n\nSince our task is a binary classification task, our final layer is a Dense layer with a single neuron with a sigmoid activation, which outputs the probability that the image is of the positive class (a pneumonia image in our case). For this reason, we also use binary crossentropy as our loss function.","9cd4cf4f":"# 2. Constructing and Training our Model","1c210ced":"# 3. Modifying Architectures of Pretrained ConvNets","a296e521":"As shown in the histogram, the dataset is very imbalanced. The dataset is heavily biased towards the pneumonia class, with roughly 3 times as many pneumonia chest images as normal chest images. This is not very surprising, given that medical data is typically imbalanced. Given this heavy imbalance of pneumonia cases, we want to make sure to adjust our classifier for this imbalance. Otherwise, we may end up having a model that outputs a high amount of false positives, which is not very useful in the medical industry.\n\nNext, we'll split our images into training, validation, and testing splits. We will then create directories for training, validation, and testing, each with subdirectories for normal and pneumonia images and copy the appropriate files over to those directories. Run the following cell to generate the splits.","ed9ccbb3":"To illustrate the model's performance in distinguishing pneumonia vs normal cases, we plotted the confusion matrix below. We tested our model on the test set to evaluate our model's performance on unseen data.","22efe8c9":"Our model did very well on the test set, and it agrees with the confusion matrix plotted above, showing that our model is good at detecting pneumonia images as well as distinguishing pneumonia x-rays from the normal chest x-rays. It also performed very well despite the class imbalance.","7fb4c356":"## 3.2 Training our Model\n\nWe fit the model with 30 epochs as before. We had to make some modifications to our image generators. For one, we had to make is resizing the input images to 32x32 rather than 28x28 due to the input shape required for VGG-19. We also had to generate them as RGB images to match the input shape of VGG-19.","f55c68f1":"## 2.2 Model Training\nWe constructed the train, validation, and test directories earlier so we could use Keras's ImageDataGenerator. We rescale all images by a factor of 1\/255 for normalization purposes. We also augment the training data by passing shear and zoom range arguments as well as horizontal and vertical flip arguments so that the network has more variety samples to train on. We train the model for 30 epochs and save our model.","5d72ee09":"# 1. Data Visualization and Preprocessing\nThe data has already been split into training, validation, and testing. However, we want to split the data in a 80% train, 10% validation, and 10% test ratio, and the data is currently not split in that ratio. In order to split it easier and to be able to calculate some statistics about the dataset as a whole, we will load the file paths of every image along with their class into a pandas DataFrame. \n\nRun the following cell to create the DataFrame and to examine the contents.","2e42b5e8":"We used the same metrics as before, and we display the same types of charts as we did for our custom model.","b58c7aac":"## 3.1 Modifying VGG-19 CNN\n\nThere are many models that have been pretrained and are ready for out of the box use. The advantage of this is that if the model has already been trained on a very generalized dataset, then we may be able to apply it to our dataset and still get good results without having to design an architecture and train the weights ourselves. We chose to modify the VGG-19 CNN, which was pretrained on the ImageNet dataset. Since it was trained on a very generalized dataset, it might be able to adapt to our dataset and outperform the model we previously made. \n\nWe keep the convolutional base and discard the top of the model (ie the final Dense Layers) since those weights are not useable for our purpose. We freeze the weights of the convolutional base to prevent them from being overwritten during training. The top layer is replaced with the same architecture as the dense layer portion of our custom architecture used previously.","d0e0745f":"The classifications of some of these images are more obvious than others. Our aim is to create and train a classifier to distinguish between pneumonia and normal chest x-ray images, especially when the distinction is not obvious. Before we make our classifier, let's examine the distribution of our data to determine the ratio of pneumonia to normal images. The following cell generates a histogram to visualize the class distribution.","5bf9bd0a":"Our modified VGG-19 did not perform as well as our custom model. It  This could be due to a variety of reasons, such as the possibility that the pretrained weights did not generalize as well to our model as it did to ImageNet. Another possibility lies in our data preprocessing steps. We converted the images to RGB format when the images themselves were grayscale. This may have introduced extra noise into our dataset and caused the negative impact on our model's performance. Our modified VGG-19 also shows evidence of suffering from the class imbalance. In the confusion matrix above, we see that the model classified roughly half of the true normal cases as normal and the other half as pneumonia, indicating that the model didn't really know how to classify normal images. However, it performed better than our custom model when it came to accurately predicting pneumonia cases. Unfortunately, it came at a cost of more false positives.\n\nOur Results indicate that our custom model is better suited to detect pneumonia cases than our modified VGG-19.","fe4a12b0":"The following cell displays 5 chest x-rays from patients with pneumonia and another 5 chest x-rays from normal patients. Run the following cell to view 5 pneumonia images and 5 normal images"}}