{"cell_type":{"e9abbe94":"code","adc6ccbf":"code","583c029e":"code","2ac03d1b":"code","e87564a7":"code","f58081b2":"code","276ed68b":"code","7c95d3db":"code","ee4baa66":"code","cba23253":"markdown","401849aa":"markdown","92525742":"markdown","a054410f":"markdown"},"source":{"e9abbe94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","adc6ccbf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nimport datatable as dt\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","583c029e":"%%time\ntrain_data_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\nfeature_datatable =  dt.fread('..\/input\/jane-street-market-prediction\/features.csv')\nexample_test = dt.fread('..\/input\/jane-street-market-prediction\/example_test.csv')\nexample_test = example_test.to_pandas()\ntrain_data = train_data_datatable.to_pandas()","2ac03d1b":"train_data['weight_x_resp'] = train_data['weight']*train_data['resp']\n\n\na =1+(train_data.groupby('date')['weight_x_resp'].mean())\nb = 1+(train_data.groupby('date')['resp'].mean())\n\n\nc = (1+(train_data.groupby('date')['weight_x_resp'].mean())).cumprod()\nd =(1+(train_data.groupby('date')['resp'].mean())).cumprod()\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_figheight(5)\nfig.set_figwidth(20)\nfig.suptitle('Relative and culmulative')\nax1.plot(a)\nax1.plot(b)\n\nax2.plot(c)\nax2.plot(d)\nfig.legend(labels = ['weight_x_resp','resp']);","e87564a7":"feature_names = ['feature_'+str(i) for i in range(130)]\n# feature_names = [ 'feature_1', 'feature_2', 'feature_6', 'feature_9',\n#        'feature_10', 'feature_16', 'feature_20', 'feature_29', 'feature_37',\n#        'feature_38', 'feature_39', 'feature_40', 'feature_51', 'feature_52',\n#        'feature_53', 'feature_54', 'feature_69', 'feature_70', 'feature_71',\n#        'feature_83', 'feature_100', 'feature_109', 'feature_112',\n#        'feature_122', 'feature_123', 'feature_124', 'feature_126',\n#        'feature_128', 'feature_129','weight']\n\nx = train_data.loc[:, feature_names].values\n# x = StandardScaler().fit_transform(x) # normalizing the features\n# normalised_train = pd.DataFrame(x,columns=feature_names)\n# pca_train = PCA(n_components=2)\n# principalComponents_train = pca_train.fit_transform(x)","f58081b2":"\nbst = xgb.Booster()  # init model\nbst.load_model('\/kaggle\/input\/k\/yidancai\/stock-market-prediction\/xgb2.model')  # load data","276ed68b":"# the difference between version 8 and 7 is the threshold, version 7 : 0, version 8, 0.1\nto_action = lambda t: 1 * (t > 0.1)","7c95d3db":"\n\ntest_data =  example_test.loc[:, feature_names]\ntest_matrix = xgb.DMatrix(test_data)\n\n# y_preds = np.round(bst.predict(test_matrix))\ny_preds = to_action(np.multiply(bst.predict(test_matrix),test_data['weight']))\nprint(y_preds)","ee4baa66":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\n\nfor (test_df, sample_prediction_df) in iter_test:\n    if test_df['weight'].item() > 0:\n        test = test_df.loc[:, feature_names]\n        test = test.fillna(0)\n#         print(X_test.shape)\n        test_matrix = xgb.DMatrix(test)\n        y_preds = to_action(np.multiply(bst.predict(test_matrix),test_df['weight']))\n        sample_prediction_df.action = y_preds.astype(int)  #make your 0\/1 prediction here\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)","cba23253":"# visualization \ngraph network \n[\ud83c\udf10EDA: Tag Network Analysis (networkx + gephi)\ud83c\udf10](https:\/\/www.kaggle.com\/quillio\/eda-tag-network-analysis-networkx-gephi)\n\n[Jane Street: t-SNE using RAPIDS cuML](https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-t-sne-using-rapids-cuml)","401849aa":"A few questions need to be answer:\n\n1. From the training dataset, how do we choose the actions? The actions will be derived from the resp and weights data, how do we use these data? (may need some domain knowledge, there are also many notebooks around trying to de-anonymize the feactures and tags)\n\n2. How to use the feature file? Can we use it for reducing the dimension? What are other ways needed to preprocess the data?\n\n3. What visualization tools can we use to understand the data?\n\n4. What target value do we want to predict directly? resps or action? Which machine learning models shall we use?\n\n5. Given the constraints on running time, how can we speed up?","92525742":"Feature Engineering \n\nThe xgboost model takes quite a long time when testing using CPU, so reducing features is considered.\n\nUsing PCA, reference [Jane : EDA & Feature Selection](https:\/\/www.kaggle.com\/shahules\/jane-eda-feature-selection)\n\nor select some features, Reference\n[Jane Day 242 Feature Generation and Selection](https:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection)","a054410f":"[Target Engineering; CV; \u270a fast.ai Multi-Target](https:\/\/www.kaggle.com\/marketneutral\/target-engineering-cv-fast-ai-multi-target) shows the overall trend of resp and resp * weight\n\nAlso notice cumulative resp and return (resp*weight) for feature_0 =1 and feature_0 = -1 are different [An observation about feature_0](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/204963)"}}