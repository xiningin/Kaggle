{"cell_type":{"84571b1c":"code","bc199c87":"code","990fef1c":"code","7608d132":"code","4edc49f8":"code","9159e320":"code","689bffa3":"code","9a9aba6a":"code","c8b1a933":"code","ace5e0c6":"code","62930df2":"code","ba75c6a1":"code","8e317038":"code","9c318ada":"code","a0a9f0d2":"markdown","ad073a0c":"markdown","8a231c4e":"markdown","d0263dc5":"markdown"},"source":{"84571b1c":"from IPython.core.display import display, HTML, Javascript\n\ndef ApplyCustomCSS():\n    return HTML(\"<style>\"+open(\"..\/input\/customcss\/custom_kaggle_forCommit.css\", \"r\").read()+\"<\/style>\")\n\nApplyCustomCSS()","bc199c87":"! pip install pyspellchecker \n! pip install textstat\nimport numpy as np\nimport pandas as pd\nimport warnings as w\nfrom scipy import spatial\nfrom tqdm.notebook import tqdm\nfrom textblob import TextBlob\nimport random, math, cv2, os, string, re, gc, nltk, textstat\nfrom collections import Counter\nfrom spellchecker import SpellChecker\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nw.filterwarnings('ignore')\n\nSEED = 100\n\nsample_sub = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain.__name__ = \"Train\"\ntest.__name__ = \"Test\"\n\n# nltk.download('stopwords')\n\ntrain.head()","990fef1c":"length = train.excerpt.apply(lambda x: len(x))\nprint('\\033[36m'+f\"The training set size is {train.shape[0]} with {train.shape[1]} columns\"+'\\033[0m')\nprint('\\033[32m'+f\"The training set size is {test.shape[0]} with {test.shape[1]} columns\"+'\\033[0m')\nprint('\\033[33m'+f\"The training set maximum excerpt length is {max(length)} and with minimum length of {min(length)}\"+'\\033[0m')\ntrain.target.describe()","7608d132":"for df in [train, test]:\n    print('\\033[36m'+df.__name__+'\\033[0m')\n    for col in df.columns:\n        print('\\033[32m'+f\"The {col} in {df.__name__} has \"+'\\033[0m'+'\\033[33m'+f\"{df[col].isnull().sum()} null values\"+'\\033[0m'+'\\033[32m'+f\" against {df.shape[0]}\"+'\\033[0m')\n    print(\"\")","4edc49f8":"def plot_table(df, title_text, footer_text = 'May 7, 2021', fig_background_color = 'skyblue', fig_border = 'steelblue'):\n    column_headers = df.columns\n    row_headers = df.index\n\n    cell_text = df.values\n\n    rcolors = plt.cm.BuPu(np.full(len(row_headers), 0.1))\n    ccolors = plt.cm.BuPu(np.full(len(column_headers), 0.1))\n\n    plt.figure(linewidth=2,\n               edgecolor=fig_border,\n               facecolor=fig_background_color,\n               tight_layout={'pad':1},\n\n              )\n\n    the_table = plt.table(cellText=cell_text,\n                          rowLabels=row_headers,\n                          rowColours=rcolors,\n                          rowLoc='right',\n                          colColours=ccolors,\n                          colLabels=column_headers,\n                          loc='center')\n\n    the_table.scale(1, 1.5)\n\n    ax = plt.gca()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    plt.box(on=None)\n\n    plt.suptitle(title_text)\n\n    plt.figtext(0.95, 0.05, footer_text, horizontalalignment='right', size=6, weight='light')\n    plt.draw()\n\ndef add_new_columns(text_row):\n    text = text_row.excerpt\n    processed_text = text_row.processed_excerpt\n    \n    punctuations_ = [word for word in text if word in string.punctuation]\n    stopwords_ = [word for word in text.split() if word in set(nltk.corpus.stopwords.words(\"english\"))]\n    tokens = nltk.word_tokenize(text)\n    sent_ = nltk.sent_tokenize(text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  \n                               u\"\\U0001F300-\\U0001F5FF\"  \n                               u\"\\U0001F680-\\U0001F6FF\"  \n                               u\"\\U0001F1E0-\\U0001F1FF\"  \n                               \"]+\", flags=re.UNICODE)\n    emojis_ = re.findall(emoji_pattern, text)\n    misspelled_words = SpellChecker().unknown(processed_text.split())\n    \n    return pd.Series({\"num_of_punctuations\" : len(punctuations_), \n                      \"num_of_stopwords\" : len(stopwords_), \n                      \"num_of_words\" : len(tokens),\n#                       \"num_of_emojis\" : len(emojis_),\n                      \"num_of_sentences\" : len(sent_),\n                      \"num_of_misspelled_words\" : len(misspelled_words),\n                      \"num_of_characters\" : len(processed_text),\n                      \"num_of_unique_words\" : len(set(tokens))})  \n\ndef clean(text):\n    text = str(text).lower()\n    text = ''.join([k for k in text if k not in string.punctuation]) # try by not removing punctuation as they are an important part of an essay \n    text = ' '.join([word for word in text.split() if word not in set(nltk.corpus.stopwords.words(\"english\"))])\n    \n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  \n                               u\"\\U0001F300-\\U0001F5FF\"  \n                               u\"\\U0001F680-\\U0001F6FF\"  \n                               u\"\\U0001F1E0-\\U0001F1FF\"  \n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text  \n\ndef n_gram_freq(text, n_grams, min_freq):\n    result = {}\n    tokens = nltk.word_tokenize(text)\n    \n    if n_grams == 2:\n        ngram_toks = nltk.bigrams(tokens)\n        \n    elif n_grams == 3:\n        ngram_toks = nltk.trigrams(tokens)\n        \n    else:\n        ngram_toks = text.split()\n        \n    for key, val in Counter(ngram_toks).items():\n        if n_grams == 1:\n            result[key] = val\n            continue\n        if val > min_freq:\n            result[' '.join(key)] = val\n    \n    return result\n\ndef plot_wordcloud(text_df, title, more_than_unigram = False, ):\n    if more_than_unigram:\n        text_df = {\"_\".join(words.split()):freq for words, freq in text_df.items()}\n        wordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2560, height=1440).generate_from_frequencies(text_df)\n        \n    else:\n        wordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2560, height=1440).generate(text_df)\n\n    ax = plt.subplots(figsize=(15, 15), facecolor='w')\n    plt.title(title, font=\"Serif\", fontsize = 30, color = 'blue')\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.tight_layout(pad=0)\n    plt.show()\n    \ntqdm.pandas()\ntrain['processed_excerpt'] = train['excerpt'].progress_apply(clean)\ntest[\"processed_excerpt\"] = test['excerpt'].progress_apply(clean)\ntext = \" \".join([record for record in train['excerpt']])\nprocessed_text = \" \".join([record for record in train['processed_excerpt']])\n\ntrain[[\"num_of_punctuations\",\n       \"num_of_stopwords\",\n       \"num_of_words\",\n       \"num_of_sentences\",\n       \"num_of_misspelled_words\",\n       \"num_of_characters\",\n       \"num_of_unique_words\"]] = train.progress_apply(add_new_columns, axis = 1)\ntest[[\"num_of_punctuations\",\n       \"num_of_stopwords\",\n       \"num_of_words\",\n       \"num_of_sentences\",\n       \"num_of_misspelled_words\",\n       \"num_of_characters\",\n       \"num_of_unique_words\"]] = test.progress_apply(add_new_columns, axis = 1)","9159e320":"sns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.pairplot(train[[\"num_of_punctuations\",\n                    \"num_of_stopwords\",\n                    \"num_of_words\",\n                    \"num_of_sentences\",\n                    \"num_of_misspelled_words\",\n                    \"num_of_characters\", \n                    \"target\"]])\n#             corner=True)\n#             plot_kws=dict(marker=\"*\", linewidth=1))\n#             diag_kws=dict(fill=False))","689bffa3":"fig, ax = plt.subplots(1,2,figsize=(15,6))\nsns.distplot(train['target'], color = \"#3F88C5\",ax=ax[0], kde = True, kde_kws=dict(color = \"#136F63\", linewidth=3))\nsns.distplot(train['standard_error'], color = \"#136F63\", kde = True,ax=ax[1], kde_kws=dict(color = \"#3F88C5\", linewidth=3))\nax[0].set_title(\"Target Distribution\",font=\"Serif\")\nax[1].set_title(\"Standard Error Distribution\",font=\"Serif\")\nplt.show()","9a9aba6a":"# Unigram wordcloud and \nunigram_freq = n_gram_freq(processed_text, 1, 2)\nuni_df = pd.DataFrame(data = unigram_freq.items(), columns = [\"Unigram_word\",\"Count\"]).sort_values(by = \"Count\", ascending = False)\nuni_df.reset_index(drop = True, inplace = True)\nuni_df[\"Count\"] = uni_df[\"Count\"].astype(str)\n\nplot_table(uni_df.head(10), \"Top 10 most Occuring words\")\nplot_wordcloud(processed_text, \"UniGram WordCloud\", more_than_unigram = False)","c8b1a933":"# Bigram wordcloud \nbigram_freq = n_gram_freq(processed_text, 2, 2)\nbi_df = pd.DataFrame(data = bigram_freq.items(), columns = [\"Bigram_word\",\"Count\"]).sort_values(by = \"Count\", ascending = False)\nbi_df.reset_index(drop = True, inplace = True)\nbi_df[\"Count\"] = bi_df[\"Count\"].astype(str)\n\nplot_table(bi_df.head(10), \"Top 10 most Occuring BiGrams\")\nplot_wordcloud(n_gram_freq(processed_text, 2, 2), \"BiGram WordCloud\", more_than_unigram = True)","ace5e0c6":"# Trigram wordcloud \ntrigram_freq = n_gram_freq(processed_text, 3, 1)\ntri_df = pd.DataFrame(data = trigram_freq.items(), columns = [\"Trigram_word\",\"Count\"]).sort_values(by = \"Count\", ascending = False)\ntri_df.reset_index(drop = True, inplace = True)\ntri_df[\"Count\"] = tri_df[\"Count\"].astype(str)\n\nplot_table(tri_df.head(10), \"Top 10 most Occuring TriGrams\")\nplot_wordcloud(n_gram_freq(processed_text, 3, 1), \"TriGram WordCloud\", more_than_unigram = True)","62930df2":"train['polarity']=train['processed_excerpt'].apply(lambda x:TextBlob(x).sentiment.polarity)\ntest['polarity']=test['processed_excerpt'].apply(lambda x:TextBlob(x).sentiment.polarity)\n\nprint(\"3 Random excerpts with Highest Polarity:\")\nfor index,excerpt in enumerate(train.iloc[train['polarity'].sort_values(ascending=False)[:3].index]['processed_excerpt']):\n    print('Excerpt {}:\\n'.format(index+1),excerpt)","ba75c6a1":"train['dale_chall_score']=train['processed_excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\ntrain['flesh_reading_ease']=train['processed_excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\ntrain['gunning_fog']=train['processed_excerpt'].apply(lambda x: textstat.gunning_fog(x))\n\ntest['dale_chall_score']=test['processed_excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\ntest['flesh_reading_ease']=test['processed_excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\ntest['gunning_fog']=test['processed_excerpt'].apply(lambda x: textstat.gunning_fog(x))\n\nprint('Dale Chall Score ',train['dale_chall_score'].mean())\n\nprint('Flesch Reading Score ',train['flesh_reading_ease'].mean())\n\nprint('Gunning Fog Index ',train['gunning_fog'].mean())\n","8e317038":"def find_best_model(models_list, x_train, y_train, x_test, y_test, use_text = True):\n    if use_text:\n        print(\"\\t Predicting with processed text\")\n    else:\n        print(\"\\t Predicting with information from processed text\")\n    for mod in models_list:\n        if use_text:\n            model = make_pipeline(\n                TfidfVectorizer(binary=True, ngram_range=(1,1)),\n                mod)\n        else:\n            model = mod\n            \n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        MSE = mean_squared_error(y_test, y_pred)\n        \n        print(\"\\t Model Name:\", mod.__name__)\n        print(\"MSE: \", MSE, end = \"\\n\")\n    print(\"--*\"*10 )\n\nridge = Ridge(fit_intercept = True, normalize = False)\nlr = LinearRegression()\nmlpreg = MLPRegressor(hidden_layer_sizes=(3,), activation='relu',\n         solver='adam', alpha=0.001, batch_size='auto',\n         learning_rate='adaptive', learning_rate_init=0.01,\n         power_t=0.5, max_iter=1000, shuffle=True, random_state=9,\n         tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n         nesterovs_momentum=True, early_stopping=False,\n         validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n         epsilon=1e-08)\n\n\nridge.__name__ = \"Ridge Regression\"\nlr.__name__ = \"Linear Regression\"\nmlpreg.__name__ = \"MLPRegressor\"\nmodels = [ridge,lr,mlpreg]\n\nX1 = train[\"processed_excerpt\"]\ny1 = train['target']\nX2 = train[[\"num_of_punctuations\", \"num_of_stopwords\", \"num_of_words\", \"dale_chall_score\", \"flesh_reading_ease\", \"gunning_fog\",\n               \"num_of_unique_words\", \"num_of_sentences\", \"num_of_misspelled_words\", \"num_of_characters\", \"polarity\"]]\ny2 = train['target']\n\nx_train1, x_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.33, random_state=2)\nx_train2, x_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.33, random_state=2)\n\nfind_best_model(models, x_train1, y_train1, x_test1, y_test1, use_text = True)\nfind_best_model(models, x_train2, y_train2, x_test2, y_test2, use_text = False)","9c318ada":"def get_final_preds(mod,X,y, use_text, features = None):\n    if use_text:\n        model = make_pipeline(\n            TfidfVectorizer(binary=True, ngram_range=(1,1)),\n            mod)\n        model.fit(X, y)\n        preds = model.predict(test[\"processed_excerpt\"])\n    else:\n        mod.fit(X[features], y)\n        preds = mod.predict(test[features])\n\n    return preds\n\ntest_pred = get_final_preds(mlpreg, train, train['target'], False, features = [\"num_of_punctuations\", \"num_of_stopwords\", \"num_of_words\", \"dale_chall_score\", \"flesh_reading_ease\", \"gunning_fog\",\n               \"num_of_unique_words\", \"num_of_sentences\", \"num_of_misspelled_words\", \"num_of_characters\", \"polarity\"])\nsubmission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['target'] = test_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","a0a9f0d2":"## Data Information with EDA\n\n#### \ud83d\udcd3 Data:\n* train.csv - the training set<br>\n* test.csv - the testing set\n\n#### **Columns** :-\n* **id** - unique ID for excerpt\n* **url_legal** - URL of source\n* **license** - license of source material\n* **excerpt** - text to predict reading ease of\n* **target** - reading ease\n* **standard_error** - measure of spread of scores among multiple raters for each excerpt\n\n\n#### Target \ud83c\udfaf : Rating the complexity of the reading passages for 3-12 standards.","ad073a0c":"<h4><center>\ud83d\udd25\ud83d\udd25\u2620\ufe0f If you find this notebook useful and resourceful, do leave behind a upvote. I will be updating this notebook on a regular basis, so please check back once new version comes up. \u2620\ufe0f\ud83d\udd25\ud83d\udd25<\/center>","8a231c4e":"<h1><center>CommonLit EDA + Literature<\/center><\/h1>\n                                                      \n<center><img src = \"https:\/\/images.unsplash.com\/photo-1507842217343-583bb7270b66?ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8bGlicmFyeXxlbnwwfHwwfHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=500&q=60\" width = \"750\" height = \"500\"\/><\/center>\n","d0263dc5":"## Error Evaluation Martix\n\n<center><img src = \"https:\/\/www.gstatic.com\/education\/formulas2\/355397047\/en\/root_mean_square_deviation.svg\" width = \"550\" height = \"150\"\/><\/center>\n\n\n**RMSD** &emsp; =&ensp;root-mean-square deviation\/error<br>\n**i** &emsp; &emsp; &emsp; =&ensp;variable i<br>\n**N** &emsp; &emsp; &emsp;=&ensp;number of non-missing data points<br>\n**x**<sub>i<\/sub> &emsp; &emsp; &emsp;=&ensp;actual observations time series<br>\n**$\\hat{x}$<sub>i<\/sub>** &emsp; &emsp; &ensp; =&ensp;estimated time series<br>\n\n> The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.\n\n> RMSD is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSD is better than a higher one. However, comparisons across different types of data would be invalid because the measure is dependent on the scale of the numbers used.\n\n> RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers."}}