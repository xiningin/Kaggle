{"cell_type":{"09ba93d2":"code","652e867e":"code","878e563d":"code","c62f9fcc":"code","30d74ba7":"code","a6859e6e":"code","d88e6401":"markdown","fe9860cc":"markdown","e7403570":"markdown","4646515a":"markdown"},"source":{"09ba93d2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.losses import *\nfrom keras.callbacks import *\nfrom keras.optimizers import *\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\ny = df['label'].values\nX = df.drop(['label'], axis=1).values.reshape(-1, 28, 28)\/255.","652e867e":"def make_vae(latent_size, kl_coeff=1.0):\n    encoder_input = Input((28,28,))\n    x = Reshape((28,28,1))(encoder_input)\n    x = ZeroPadding2D(padding=2)(x)\n    for n in [32,64,128,128,128]:\n        x = Conv2D(n, kernel_size=3, padding='same')(x)\n        x = Activation('relu')(x)\n        x = BatchNormalization()(x)\n        x = MaxPooling2D(pool_size=2)(x)\n    x = Flatten()(x)\n    z_mean = Dense(latent_size)(x)\n    z_log_sigma = Dense(latent_size)(x)\n    def sampling(args):\n        z_mean, z_log_sigma = args\n        epsilon = K.random_normal(shape=K.shape(z_mean),\n                                  mean=0., stddev=1)\n        return z_mean + K.exp(z_log_sigma) * epsilon\n\n    z = Lambda(sampling, output_shape=(latent_size,))([z_mean, z_log_sigma])\n    \n    encoder = Model(encoder_input, z_mean)\n    encoder.summary()\n\n    decoder = Sequential()\n    decoder.add(InputLayer((latent_size,)))\n    decoder.add(Dense(128))\n    decoder.add(Reshape((1,1,128)))\n    for n in [128,128,64,32,1]:\n        decoder.add(UpSampling2D(size=2))\n        decoder.add(Conv2D(n, kernel_size=3, padding='same'))\n        decoder.add(Activation('relu'))\n        decoder.add(BatchNormalization())\n    decoder.add(Conv2D(1, kernel_size=3, padding='same'))\n    decoder.add(Activation('sigmoid'))\n    decoder.add(Cropping2D(cropping=2))\n    decoder.add(Reshape((28,28,)))\n    decoder.summary()\n    \n    vae_output = decoder(z_mean)\n    vae = Model(encoder_input, vae_output)\n    vae.summary()\n    \n    def vae_loss(y_true, y_pred):\n        recon_loss = K.sum(K.square(y_true-y_pred), axis=[1,2])\n        kl_loss = - 0.5 * K.sum(1 + 2*z_log_sigma - K.square(z_mean) - K.square(K.exp(z_log_sigma)), axis=-1)\n        return recon_loss + kl_coeff*kl_loss\n    \n    return encoder, decoder, vae, vae_loss","878e563d":"encoder, decoder, vae, vae_loss = make_vae(2, kl_coeff=1.)\n\nvae.compile(optimizer='adam', loss=vae_loss, metrics=[])\n\nepochs=100\nvae.fit(X, X, \n        epochs=epochs,\n        callbacks=[EarlyStopping(monitor='loss', patience=2)],\n        verbose=2)","c62f9fcc":"plt.figure(figsize=(10,2))\nX_pred = vae.predict(X[:10])\n\nfor i in range(10):\n    plt.subplot(2,10,i+1)\n    plt.imshow(X[i], cmap='gray')\n    plt.axis('off')\n\nfor i in range(10):\n    plt.subplot(2,10,10+i+1)\n    plt.imshow(X_pred[i], cmap='gray')\n    plt.axis('off')","30d74ba7":"encoded = encoder.predict(X)\nencoded_t = encoded.T\nplt.scatter(encoded_t[0], encoded_t[1], c=y);","a6859e6e":"X,Y = np.meshgrid(np.arange(-1, 1, 0.05), np.arange(-1, 1, 0.05))\nX = np.ravel(X)\nY = np.ravel(Y)\nlatents = np.stack([X, Y], axis=1)\ngenerated = decoder.predict(latents)\n\nplt.figure(figsize=(20,20))\nfor i in range(40):\n    for j in range(40):\n        plt.subplot(40,40,40*(39-i)+j+1)\n        plt.imshow(generated[40*i+j])\n        plt.axis('off')","d88e6401":"## VAE Reconstruction ","fe9860cc":"# Variational Autoencoder\n\nVAEs makes the latent space smooth by sampling from the latent space and makes the whole latent space a normal distribution by adding KLP loss.\n\n## Ref\n\n1. Keras VAE tutorial: [Building Autoencoders in Keras](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)\n\n1. Keras VAE implementation: [git repo](https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/variational_autoencoder.py)\n\n1. Irhum Shafkat's intuitive insights: [Intuitively Understanding Variational Autoencoders](http:\/\/louistiao.me\/posts\/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial\/)\n\n1. Agustinus Kristiadi gets the math correct: [variational-autoencoder](https:\/\/wiseodd.github.io\/techblog\/2016\/12\/10\/variational-autoencoder\/)","e7403570":"## Generating New Image by Sampling from Latent Space","4646515a":"## Visulizing Latent Space\n\nCompared to that of CAE, VAE's latent space has less gap and is normally distributed."}}