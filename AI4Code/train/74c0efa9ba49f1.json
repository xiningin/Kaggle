{"cell_type":{"5ddb2c38":"code","36454dbc":"code","42127dcf":"code","7b75c900":"code","3fff995e":"code","34082907":"code","4e7f10f8":"code","4e5c80db":"code","36ad6d5e":"code","78c841a2":"code","2d685d72":"code","3a1f9dfb":"code","efc34f39":"code","c23c3aa8":"markdown","e3ad1de5":"markdown","1abfbf7a":"markdown","4f36002e":"markdown","1e970457":"markdown","6e0928e7":"markdown","c7ebfdb7":"markdown","5bb76caf":"markdown","3549b8b7":"markdown","48325152":"markdown","c432f471":"markdown","5df58627":"markdown","528ec26d":"markdown","a96ba01e":"markdown","85a44c23":"markdown"},"source":{"5ddb2c38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36454dbc":"import pandas as pd\nimport statsmodels.api as sm","42127dcf":"df = pd.read_csv(\"\/kaggle\/input\/tvradionewspaperadvertising\/Advertising.csv\")\ndf.head()","7b75c900":"X = df[['TV', 'Radio', 'Newspaper']]   # independent variables => predict sales value based on these features\ny = df[['Sales']]                        # dependent variables\n\nX.head()","3fff995e":"y.head()","34082907":"X = sm.add_constant(X)\nX.head()","4e7f10f8":"model = sm.OLS(y, X).fit()\nmodel.summary()","4e5c80db":"import matplotlib.pyplot as plt\nX.iloc[:, 1:].corr()","36ad6d5e":"df_salary = pd.read_csv(\"\/kaggle\/input\/salary-data-with-age-and-experience\/Salary_Data.csv\")\ndf_salary.head()","78c841a2":"X = df_salary[['YearsExperience', 'Age']]\ny = df_salary[['Salary']]\n\nX.head()","2d685d72":"y.head()","3a1f9dfb":"X = sm.add_constant(X)\nmodel = sm.OLS(y, X).fit()\nmodel.summary()","efc34f39":"X.iloc[:, 1:].corr()","c23c3aa8":"### Confirming the multicollinearity between Age and YearsOfExperience by plotting the correlation table","e3ad1de5":"In this scenario, According to the summary,\n\n* B0 = coeff of const = -6661.9872\n\n* B1 = coeff of YearsofExperience = 6153.3533\n* [this coeff value means that if we change the YearsOfExperience(i.e. input feature) by 1 unit, the change in salary(i.e. output) will be 6153.3533]\n\n* B2 = coeff of Age = 1836.0136\n* [thus if we change the Age(i.e. input feature) by 1 unit(1 year), the change in salary(i.e. output) will be 6153.3533]\n\n\n* R-squared value = 0.960 => very close to 1 => the model has fitted very well\n\n* P value of const = 0.773\n* P value of YearsOfExperience = 0.014\n* P value of Age = 0.165\n\n=> for Age => the P-value is >0.05 => Age and YearsOfExperience may have some kind of correlation\n\n* std error of const = 0.308\n* std error of YearsOfExperience = 2337.092\n* std error of Age = 1285.034\n\nHere we can see the std errors of both YearsOfExperience and Age are very very high, thus indicating that there is a huge Multicollinearity among them","1abfbf7a":"## Splitting the data into independent and dependent features","4f36002e":"### In this case the independent features are 'Years of Experience' and the 'Age' and we have to predict the dependent variable 'Salary' based on these two independent features","1e970457":"# Example Dataset 2 => Salary Dataset with age and YOE","6e0928e7":"## In this Case, we will use the Multiple Linear Regression technique 'Ordinary Least Squared'\n\n* Equation of Linear Regression best fit line for this dataset is: y = B0 * 1 + B1 * (TV) + B2 * (Radio) + B3 * (Newspaper)\n\n* Whenever computing Ordinary Least Squared (OLS) => we need to compute B0(i.e. the intercept) also.\n* But we dont have the B0 value here => So we will add a column for B0 value and all values in that column will be equal to 1\n\n* To add a constant value column for B0 with all values = 1 => We will use the statsmodel library","c7ebfdb7":"* TV => expenditure done on TV advertisements\n* Radio => expenditure done on radio advertisements\n* Newspaper => expenditure done on newspaper advertisements\n\n* Sales => The final sales amount collected with the help of expenditures done\n\n\nLooking at our current dataset, (TV, Radio and Newspaper) are the independent features and the Sales is the output feature which we have to predict","5bb76caf":"### Fit an Ordinary Least Squared Model with intercept on TV and Radio. We will again be using the statsmodel library for this as it has the function OLS for creating the model. Inside the OLS method we have to give endog(output feature) and exog values(input features) as parameters","3549b8b7":"# Example Dataset 1 => Advertising Dataset","48325152":"## Plot independent features in terms of correlation","c432f471":"#### With the help of this Correlation Table \/ Matrix we can imply that age and yearsofexperience have 98% correlation (very highly correlated). This implies that taking one of these features will be more than enough to predict the salary.\n\n#### Now the Question is which of the input features (YearsOfExperience and Age) to keep and which one to drop for the final prediction of salary\n\n### Remedy for this Multicollinearity problem:\n\n* Solution 1 : Dont do anything, keep things as it is and don't care about multicollinearity and take all the input features to create the model\n\n* Solution 2 : Check the P values for Age and YearsOfExperience. P value of Age > P value of YearsOfExperience. Thus drop the 'Age' feature. This will not have much effect on the model as the correlation is about 98%. Thus the whole model can be trained just by considering the feature 'YearsOfExperience'","5df58627":"Through this table, we can see the correlation values among the various independent features:\n\n* Between TV and Radio => 0.054809\n* Between Radio and Newspaper => 0.354104\n* Between TV and Newspaper => 0.056648\n\nThis implies that none of the correlation values are >0.5. Thus indicating that there is not much correlation between the independent features and thus no multicollinearity issue among the independent features","528ec26d":"### Fitting the OLS(Ordinary Least Squared) model, similar to the previous dataset","a96ba01e":"This summary helps us understand whether there is multicollinearity \/ high correlation between the independent features.\n\nAccording to the summary,\n\n* B0 = coeff of const = 4.6251\n\n* B1 = coeff of TV = 0.0544 \n* [coeff value means that if we change the TV expenditure(i.e. input feature) by 1 unit, the change in sales(i.e. output) will be 0.0544]\n\n* B2 = coeff of Radio = 0.1070\n* B3 = coeff of Newspaper = 0.0003 \n* B3 coeff => << 0.005 => this shows that we are making an unnecessary expenditure on Newspaper. Thus we can reduce that unnecessary expenditure done on Newspaper. Thus while creating the model, we can just drop this feature.\n\n\n* R-squared value = 0.903 => very close to 1 => the model has fitted very well\n\n* P value of const = 0\n* P value of TV = 0\n* P value of Radio = 0\n* P value of Newspaper = 0.954\n\n=> Except the feature 'Newspaper' (P-value = 0.954) , all the P values are less than 0.05\n\n* std error of const = 0.308\n* std error of TV = 0.001\n* std error of Radio = 0.008\n* std error of Newspaper = 0.006\n\nstd error => high number(>0.5) if there is multicollinearity among the independent varibles.\nBut here, the std error are small numbers thus indicating there is no multicollinearity among the independent variables","85a44c23":"# What kind of problem we have to solve\n\n\nLets say you have two independent features -> Age and experience and you have to predict the salary based on those two values.\n\nLinear Regression Best-fit line formula => Salary = B0 + B1 * (Age) + B2 * (Experience)\n\nhere, B0 = intercept, B1 and B2 are coefficients \/ slopes\n\nNow, if you see, there can be a possibility that the age and experience variable themselves have a high correlation value (>90%) i.e. age and experience are internally correlated with each other. This affects the output 'salary', the features 'age' and 'experience' will be almost same thus implying that we are providing the same information to the output feature 'salary' which we want to compute.\n\nThis is the problem that we have to resolve."}}