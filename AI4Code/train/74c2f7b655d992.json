{"cell_type":{"c84ce47f":"code","99e82e19":"code","48d02347":"code","c630f853":"code","013b9808":"code","22f79593":"code","48513ccf":"code","f9324a5e":"code","2ade6ea0":"code","5c5c779d":"code","7b1c4e2d":"code","b4ea2396":"code","dade5d92":"code","214395ae":"code","1f213328":"code","7c7cbbbd":"code","472397e2":"code","c6d87978":"code","dd2bdc84":"code","55f820d7":"code","9031a797":"code","dec0a07f":"code","a0391d61":"code","3873e485":"code","f4e1ed3c":"code","0abfaadf":"code","ef1ed01c":"code","f7351177":"code","d37fd9d5":"code","69eb1e62":"code","95486838":"code","27c4242b":"code","b5a8b805":"code","79559743":"code","fb8a9bbb":"code","ca1cbc02":"code","29bc7f38":"code","f3a7beab":"code","dffe245d":"markdown","0724a8d9":"markdown","a419ee52":"markdown","2fedaf52":"markdown","fea65f9c":"markdown","84ab53f4":"markdown","28782e90":"markdown","f8c759fc":"markdown","2443b73a":"markdown","84324b71":"markdown","98616d83":"markdown","b847b496":"markdown","513a0f4a":"markdown","08a065b5":"markdown","fb961b3f":"markdown"},"source":{"c84ce47f":"import os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport zlib\nimport itertools\nimport sklearn\nimport itertools\nimport scipy\nimport skimage\nfrom skimage.transform import resize\nimport csv\nfrom tqdm import tqdm\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, learning_curve,KFold,cross_val_score,StratifiedKFold\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix\nimport keras\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import models, layers, optimizers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.utils import class_weight\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop\nfrom keras.models import Sequential, model_from_json\nfrom keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,MaxPooling2D,AveragePooling2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras import backend as K\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.applications.inception_v3 import InceptionV3\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom keras.applications.mobilenet import MobileNet\n\n# import the necessary packages\nfrom keras.applications import ResNet50\nfrom keras.applications import InceptionV3\nfrom keras.applications import Xception # TensorFlow ONLY\nfrom keras.applications import VGG16\nfrom keras.applications import VGG19\nfrom keras.applications import imagenet_utils\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nimport numpy as np\nimport argparse\nimport cv2\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n%matplotlib inline\nimport keras\nfrom keras import backend as K\nfrom keras.layers.core import Dense, Activation\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.applications import imagenet_utils\nfrom keras.layers import Dense,GlobalAveragePooling2D\nfrom keras.applications import MobileNet\nfrom keras.applications.mobilenet import preprocess_input\nimport numpy as np\nfrom IPython.display import Image\nfrom keras.optimizers import Adam\nimport warnings\nwarnings.filterwarnings(\"ignore\")","99e82e19":"imageSize=150\ntrain_dir = \"..\/input\/blur-dataset\/\"\ntest_dir =  \"..\/input\/blur-dataset\/\"","48d02347":"\nfrom tqdm import tqdm\ndef get_data(folder):\n    \"\"\"\n    Load the data and labels from the given folder.\n    \"\"\"\n    X = []\n    y = []\n    for folderName in os.listdir(folder):\n        if not folderName.startswith('.'):\n            if folderName in ['Defocus']:\n                label = 0\n            elif folderName in ['Motion']:\n                label = 1\n            elif folderName in ['Sharp']:\n                label = 2\n            else:\n                label = 3\n            for image_filename in tqdm(os.listdir(folder + folderName)):\n                img_file = cv2.imread(folder + folderName + '\/' + image_filename)\n                if img_file is not None:\n                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n                    img_arr = np.asarray(img_file)\n                    X.append(img_arr)\n                    y.append(label)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    return X,y\nX_train, y_train = get_data(train_dir) # Un-comment to use full dataset: Step 1 of 2\nX_test, y_test= get_data(test_dir)\n\n","c630f853":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.4) # Re-comment to use full dataset: Step 2 of 2\n\n# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nfrom keras.utils.np_utils import to_categorical\ny_trainHot = to_categorical(y_train, num_classes = 4)\ny_testHot = to_categorical(y_test, num_classes = 4)","013b9808":"def plotHistogram(a):\n    \"\"\"\n    Plot histogram of RGB Pixel Intensities\n    \"\"\"\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.imshow(a)\n    plt.axis('off')\n    histo = plt.subplot(1,2,2)\n    histo.set_ylabel('Count')\n    histo.set_xlabel('Pixel Intensity')\n    n_bins = 30\n    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\nplotHistogram(X_train[1])","22f79593":"multipleImages = glob('..\/input\/blur-dataset\/Blur_dataset\/Blur\/**')\ndef plotThreeImages(images):\n    r = random.sample(images, 3)\n    plt.figure(figsize=(16,16))\n    plt.subplot(131)\n    plt.imshow(cv2.imread(r[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(r[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(r[2])); \nplotThreeImages(multipleImages)","48513ccf":"print(\"Motion Blurr\")\nmultipleImages = glob('..\/input\/blur-dataset\/motion_blurred\/**')\ni_ = 0\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.subplots_adjust(wspace=0, hspace=0)\nfor l in multipleImages[:25]:\n    im = cv2.imread(l)\n    im = cv2.resize(im, (128, 128)) \n    plt.subplot(5, 5, i_+1) #.set_title(l)\n    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n    i_ += 1","f9324a5e":"print(\"Defocus Blurr\")\nmultipleImages = glob('..\/input\/blur-dataset\/defocused_blurred\/**')\ni_ = 0\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.subplots_adjust(wspace=0, hspace=0)\nfor l in multipleImages[:25]:\n    im = cv2.imread(l)\n    im = cv2.resize(im, (128, 128)) \n    plt.subplot(5, 5, i_+1) #.set_title(l)\n    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n    i_ += 1","2ade6ea0":"print(\"Sharp\")\nmultipleImages = glob('..\/input\/blur-dataset\/sharp\/**')\ni_ = 0\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.subplots_adjust(wspace=0, hspace=0)\nfor l in multipleImages[:25]:\n    im = cv2.imread(l)\n    im = cv2.resize(im, (128, 128)) \n    plt.subplot(5, 5, i_+1) #.set_title(l)\n    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n    i_ += 1","5c5c779d":"#print(\"CNV\")\n#multipleImages = glob('..\/input\/kermany2018\/oct2017\/OCT2017 \/train\/CNV\/**')\n#i_ = 0\n#plt.rcParams['figure.figsize'] = (10.0, 10.0)\n#plt.subplots_adjust(wspace=0, hspace=0)\n#for l in multipleImages[:25]:\n #   im = cv2.imread(l)\n  #  im = cv2.resize(im, (128, 128)) \n   # plt.subplot(5, 5, i_+1) #.set_title(l)\n    #plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n    #i_ += 1","7b1c4e2d":"map_characters = {0: 'Defocus', 1: 'Motion', 2: 'Sharp'}\ndict_characters=map_characters\nimport seaborn as sns\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)","b4ea2396":"# Helper Functions  Learning Curves and Confusion Matrix\n\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nclass MetricsCheckpoint(Callback):\n    \"\"\"Callback that saves metrics after each epoch\"\"\"\n    def __init__(self, savepath):\n        super(MetricsCheckpoint, self).__init__()\n        self.savepath = savepath\n        self.history = {}\n    def on_epoch_end(self, epoch, logs=None):\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        np.save(self.savepath, self.history)\n\ndef plotKerasLearningCurve():\n    plt.figure(figsize=(10,5))\n    metrics = np.load('logs.npy')[()]\n    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n        l = np.array(metrics[k])\n        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n        y = l[x]\n        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n    plt.legend(loc=4)\n    plt.axis([0, None, None, None]);\n    plt.grid()\n    plt.xlabel('Number of epochs')\n    plt.ylabel('Accuracy')\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (5,5))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndef plot_learning_curve(history):\n    plt.figure(figsize=(8,8))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('.\/accuracy_curve.png')\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.savefig('.\/loss_curve.png')","dade5d92":"map_characters1 = {0: 'Defocus', 1: 'Motion', 2: 'Sharp'}\nclass_weight1 = class_weight.compute_class_weight('balanced', np.unique(y_test), y_test)\nweight_path1 = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n#weight_path2 = '..\/input\/keras-pretrained-models\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n#weight_path3 = '..\/input\/keras-pretrained-models\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n#weight_path4 = '..\/input\/keras-pretrained-models\/xception_weights_tf_dim_ordering_tf_kernels_notop.h5'\npretrained_model_1 = VGG16(weights = weight_path1, include_top=False, input_shape=(imageSize, imageSize, 3))\n#pretrained_model_2 = InceptionV3(weights = weight_path2, include_top=False, input_shape=(imageSize, imageSize, 3))\n#pretrained_model_3 = ResNet50(weights = weight_path3, include_top=False, input_shape=(imageSize, imageSize, 3))\n#pretrained_model_4 = Xception(weights = weight_path4, include_top=False, input_shape=(imageSize, imageSize, 3))\n\noptimizer1 = keras.optimizers.Adam()\n#optimizer2 = keras.optimizers.RMSprop(lr=0.0001)\n#optimizer3 = keras.optimizers.SGD()\n#optimizer4 = keras.optimizers.Adagrad()\n#optimizer5 = keras.optimizers.Adadelta()\ndef pretrainedNetwork(xtrain,ytrain,xtest,ytest,pretrainedmodel,pretrainedweights,classweight,numclasses,numepochs,optimizer,labels):\n    base_model = pretrained_model_1 # Topless\n    # Add top layer\n    x = base_model.output\n    x = Flatten()(x)\n    predictions = Dense(numclasses, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    # Train top layer\n    for layer in base_model.layers:\n        layer.trainable = False\n    model.compile(loss='categorical_crossentropy', \n                  optimizer=optimizer, \n                  metrics=['accuracy'])\n    callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n    model.summary()\n    # Fit model\n    history = model.fit(xtrain,ytrain, epochs=numepochs, class_weight=classweight, validation_data=(xtest,ytest), verbose=1,callbacks = [MetricsCheckpoint('logs')])\n    # Evaluate model\n    score = model.evaluate(xtest,ytest, verbose=0)\n    print('\\nKeras CNN - accuracy:', score[1], '\\n')\n    y_pred = model.predict(xtest)\n    print('\\n', sklearn.metrics.classification_report(np.where(ytest > 0)[1], np.argmax(y_pred, axis=1), target_names=list(labels.values())), sep='') \n    Y_pred_classes = np.argmax(y_pred,axis = 1) \n    Y_true = np.argmax(ytest,axis = 1) \n    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n    plotKerasLearningCurve()\n    plt.show()\n    plot_learning_curve(history)\n    plt.show()\n    plot_confusion_matrix(confusion_mtx, classes = list(labels.values()))\n    plt.show()\n    return model\n#pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,4,83,optimizer1,map_characters1)","214395ae":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,4,4,optimizer1,map_characters1)","1f213328":"# Deal with imbalanced class sizes below\n# Make Data 1D for compatability upsampling methods\nX_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\nX_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\nX_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\nX_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\nY_train = y_train\nY_test = y_test\n#ros = RandomOverSampler(ratio='auto')\nros = RandomUnderSampler(ratio='auto')\nX_trainRos, Y_trainRos = ros.fit_sample(X_trainFlat, Y_train)\nX_testRos, Y_testRos = ros.fit_sample(X_testFlat, Y_test)\n# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_trainRosHot = to_categorical(Y_trainRos, num_classes = 4)\nY_testRosHot = to_categorical(Y_testRos, num_classes = 4)\n# Make Data 2D again\nfor i in range(len(X_trainRos)):\n    height, width, channels = imageSize,imageSize,3\n    X_trainRosReshaped = X_trainRos.reshape(len(X_trainRos),height,width,channels)\nfor i in range(len(X_testRos)):\n    height, width, channels = imageSize,imageSize,3\n    X_testRosReshaped = X_testRos.reshape(len(X_testRos),height,width,channels)\n# Plot Label Distribution\ndfRos = pd.DataFrame()\ndfRos[\"labels\"]=Y_trainRos\nlabRos = dfRos['labels']\ndistRos = lab.value_counts()\nsns.countplot(labRos)\nprint(dict_characters)","7c7cbbbd":"class_weight1 = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\nprint(\"Old Class Weights: \",class_weight1)\nclass_weight2 = class_weight.compute_class_weight('balanced', np.unique(Y_trainRos), Y_trainRos)\nprint(\"New Class Weights: \",class_weight2)","472397e2":"# Shuffle data to permit further subsampling\nfrom sklearn.utils import shuffle\nX_trainRosReshaped, Y_trainRosHot = shuffle(X_trainRosReshaped, Y_trainRosHot, random_state=13)\nX_testRosReshaped, Y_testRosHot = shuffle(X_testRosReshaped, Y_testRosHot, random_state=13)","c6d87978":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4, 4,optimizer1,map_characters1)","dd2bdc84":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4, 4,optimizer2,map_characters1)","55f820d7":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4, 4,optimizer3,map_characters1)","9031a797":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4, 4,optimizer4,map_characters1)","dec0a07f":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4, 4,optimizer5,map_characters1)","a0391d61":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_2,weight_path2,class_weight2,4,2,optimizer2,map_characters1)","3873e485":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4,2,optimizer3,map_characters1)","f4e1ed3c":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4,2,optimizer4,map_characters1)","0abfaadf":"pretrainedNetwork(X_trainRosReshaped[:10000], Y_trainRosHot[:10000], X_testRosReshaped[:2000], Y_testRosHot[:2000],pretrained_model_1,weight_path1,class_weight2,4,2,optimizer5,map_characters1)","ef1ed01c":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_2,weight_path2,class_weight1,4,8,optimizer1,map_characters1)","f7351177":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_2,weight_path2,class_weight1,4,8,optimizer2,map_characters1)","d37fd9d5":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_2,weight_path2,class_weight1,4,8,optimizer3,map_characters1)","69eb1e62":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_2,weight_path2,class_weight1,4,8,optimizer4,map_characters1)","95486838":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_3,weight_path2,class_weight1,4,8,optimizer1,map_characters1)","27c4242b":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_3,weight_path2,class_weight1,4,8,optimizer2,map_characters1)","b5a8b805":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_3,weight_path2,class_weight1,4,8,optimizer3,map_characters1)","79559743":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_3,weight_path2,class_weight1,4,8,optimizer4,map_characters1)","fb8a9bbb":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_4,weight_path2,class_weight1,4,8,optimizer1,map_characters1)","ca1cbc02":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_4,weight_path2,class_weight1,4,8,optimizer2,map_characters1)","29bc7f38":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_4,weight_path2,class_weight1,4,8,optimizer3,map_characters1)","f3a7beab":"pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_4,weight_path2,class_weight1,4,8,optimizer4,map_characters1)","dffe245d":"# *Step 2: Load Data*","0724a8d9":"# *Step 3: Vizualize Data*","a419ee52":"# **> *Step 1: Import Modules***","2fedaf52":"# Optimizers  SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop","fea65f9c":" We were able to detect retina damage in OCT images with an accuracy rate of approximately 95%.  That is much better than random chance given 4 different categories and it is of a comparable accuracy to that of medical professionals.","84ab53f4":"# *Step 4: Plot Function","28782e90":"Transfer learning w\/ InceptionV3 Convolutional Network","f8c759fc":"# *****Step 7: Evaluate Final Model*","2443b73a":"The min\/max pixel values are already scaled between 0 and 1","84324b71":"*Step 6: Evaluate Undersampling Strategy*","98616d83":"Optimizers","b847b496":"# *****Step 5: Evaluate Classification Models*","513a0f4a":"# VGG16 Optimizer 1****","08a065b5":"To Do: (1) Try to get accuracy up from 95% to 99%; (2) Add data augmentation; (3) Add ROC curve","fb961b3f":"The goal is to get rid of the class imbalance issues.  Oversampling with data augmentation (e.g. [SMOTE](http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/over_sampling.html)) would be preferable to undersampling but undersampling is faster.\n\nThis step is only important if you are using the full dataset that has the imbalanced class sizes.  For the purpose of this analysis I am using an abbreviated dataset that does not have imbalanced class sizes (see get_data function) so really the undersampling step is not necessary."}}