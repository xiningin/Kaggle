{"cell_type":{"c28de30f":"code","7cd0b430":"code","a541e944":"code","65b4f39e":"code","7706caca":"code","919b5c85":"code","535ecdfa":"code","e6cb7091":"code","49950f99":"code","8958eb94":"code","017445cb":"code","44fc4b1e":"code","2f6e802b":"code","14c76213":"code","8ec3d0dc":"code","ba54e594":"code","d084518e":"code","58449962":"code","84832c97":"code","1ebc4367":"code","8f521e24":"code","22c70483":"code","a1063b4d":"code","f14df9db":"code","ffa95427":"code","2ff1d958":"code","727932d2":"code","3c3650c6":"code","2ee686f6":"code","d5c4f46e":"code","ff67da51":"code","06d6c685":"code","bf3f902b":"code","949611f6":"code","ed01f31e":"markdown","f5643d27":"markdown","55016bfd":"markdown","815bca01":"markdown","38452677":"markdown","6e402c12":"markdown","48b4b1a1":"markdown","76bc2066":"markdown","f014dc6f":"markdown","8690b606":"markdown","3c3ca9ea":"markdown","87886bd6":"markdown","b9532710":"markdown","4bbb1269":"markdown","02915681":"markdown","c1c15b25":"markdown","cdbd3f52":"markdown","7a1fa474":"markdown","e68f4ca9":"markdown","df0f4c37":"markdown","000ac040":"markdown"},"source":{"c28de30f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#Count vectorizer for N grams\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport spacy\nimport nltk\n# Nltk for tekenize and stopwords\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nnltk.download('wordnet')\nnltk.download('stopwords')\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom fuzzywuzzy import fuzz","7cd0b430":"with open(\"\/kaggle\/input\/usa-presidential-debate-2020\/presidential_debate_transcript.txt\",\"r\",encoding=\"utf-8\") as f:\n    debate = f.read()\ndebate[:2000]","a541e944":"def find_year(text):\n    line=re.findall(r\"\\b(19[40][0-9]|20[0-1][0-9]|2020)\\b\",text)\n    return line\n\ndef find_nonalp(text):\n    line = re.findall(\"[^A-Za-z0-9 ]\",text)\n    return line\n\ndef find_number(text):\n    line=re.findall(r'[0-9]+',text)\n    return \" \".join(line)\ndef find_punct(text):\n    line = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*', text)\n    string=\"\".join(line)\n    return list(string)\ndef ngrams_top(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    \n    return df\ndef only_words(text):\n    text=text.lower()\n    line=re.findall(r'\\b[^\\d\\W]+\\b', text)    \n    return \" \".join(line)\n\ndef non_stop_word_fn(text):\n    stop_words = set(stopwords.words('english'))\n    new_stopwords = ['president','biden','joe','trump','please','mr','then','you','your','can','we','more','on','at','vice','gentlemen','say','like','said','sir','saying','going','that','this','ph']\n    new_stopwords_list = stop_words.union(new_stopwords)\n    word_tokens = word_tokenize(text) \n    non_stop_words = [w for w in word_tokens if not w in new_stopwords_list] \n    stop_words= [w for w in word_tokens if w in new_stopwords_list] \n    return \" \".join(non_stop_words)\n\ndef search_string(text,key):\n    return bool(re.search(r''+key+'', text))","65b4f39e":"#from spacy import displacy\n#displacy.serve(doc, style=\"ent\")","7706caca":"doc = nlp(str(debate))\nPERSON =[]\nfor ent in doc.ents:\n    #print(doc)\n    if ent.label_ == \"PERSON\":\n        if ent.text not in PERSON:\n            PERSON.append(ent.text)\nPERSON","919b5c85":"doc = nlp(str(debate))\nMONEY =[]\nfor ent in doc.ents:\n    #print(doc)\n    if ent.label_ == \"MONEY\":\n        if ent.text not in MONEY:\n            MONEY.append(ent.text)\nMONEY","535ecdfa":"doc = nlp(str(debate))\nORGS =[]\nfor ent in doc.ents:\n    #print(doc)\n    if ent.label_ == \"ORG\":\n        if ent.text not in ORGS:\n            ORGS.append(ent.text)\nORGS","e6cb7091":"doc = nlp(str(debate))\nGPES =[]\nfor ent in doc.ents:\n    #print(doc)\n    if ent.label_ == \"GPE\":\n        if ent.text not in GPES:\n            GPES.append(ent.text)\nGPES","49950f99":"speaker_list = [\"BIDEN\", \"TRUMP\", \"WALLACE\", \"CROSSTALK\"]\ngroup = 0\ntexts = debate.split(\"\\n\")\nprint(f\"paragraphs: {len(texts)}\")\n\ndebate_list = []\n\nfor text in texts:\n    if text:\n        #print(\"Text: \",text)\n        dialogue = text.split(\":\")\n        #print(\"Len: \",len(dialogue))\n        if len(dialogue) == 1:\n            if dialogue[0].strip() == '(CROSSTALK)':\n                debate_list.append((speaker_list[3], \"\"))\n            else:    \n                previous_dialogue = debate_list[-1]\n                name = previous_dialogue[0]                  \n                #print(\"previous_dialogue: \",name)\n                debate_list.append((name, dialogue[0]))\n        else:\n            if dialogue[0] not in speaker_list:\n                previous_dialogue = debate_list[-1]\n                debate_list.append((previous_dialogue[0], dialogue[1]))\n            else:    \n                debate_list.append((dialogue[0], dialogue[1]))\n                \n \ndf = pd.DataFrame(debate_list, columns=('Speaker', 'Text'))\n\nprint(df.head())","8958eb94":"df['presence_year'] = df['Text'].map(find_year)\ndf['presence_number'] = df['Text'].map(find_number)\ndf['presence_punct'] = df['Text'].map(find_punct)\ndf['only_words'] = df['Text'].map(only_words)\ndf['words'] = df['only_words'].map(non_stop_word_fn)\ndf['words_len']=df['only_words'].str.split().map(lambda x: len(x))","017445cb":"df['search_India']=df['Text'].apply(lambda x : search_string(x,'India'))\ndf[df['search_India']]['Text'].values\n","44fc4b1e":"df['search_Kamala']=df['Text'].apply(lambda x : search_string(x,'Kamala Harris'))\ndf[df['search_Kamala']]['Text'].values","2f6e802b":"emotion = pd.read_csv('..\/input\/emodata\/Emotion-Lexicon.txt', sep='\\t')\n\nelements = emotion['word'].values\n     \nfear = 0\ndisgust = 0\nsadness = 0\njoy = 0\ntrust = 0\nanticipation = 0\nsurprise = 0\nanger = 0\n    \nbiden  = df[df['Speaker'] == 'BIDEN']\nbiden_speech = \" \".join(biden['words'])\nbiden_speech_list =   biden_speech.split()                 \nfor  wd in biden_speech_list:\n    for (j, element) in enumerate(elements):             \n        if fuzz.ratio(element, wd) >= 94:            \n               \n            if emotion['emotion'][j] == 'trust':\n                trust += 1\n            elif emotion['emotion'][j] == 'surprise':\n                surprise += 1\n            elif emotion['emotion'][j] == 'anticipation':\n                anticipation += 1\n            elif emotion['emotion'][j] == 'joy':\n                joy += 1\n            elif emotion['emotion'][j] == 'anger':\n                anger += 1\n            elif emotion['emotion'][j] == 'fear':\n                fear += 1\n            elif emotion['emotion'][j] == 'disgust':\n                disgust += 1\n            elif emotion['emotion'][j] == 'sadness':\n                sadness += 1             \n    \nprint(\"done...\")","14c76213":"print(\"Num of 'Anticipation': \",anticipation )\nprint(\"Num of 'Surprise': \",surprise )\nprint(\"Num of 'Trust': \",trust)\nprint(\"Num of 'Joy': \",joy)\nprint(\"Num of 'Anger': \",anger)\nprint(\"Num of 'Fear': \",fear)\nprint(\"Num of 'Sadness': \",sadness)\nprint(\"Num of 'Disgust': \",disgust)","8ec3d0dc":"emotion = pd.read_csv('..\/input\/emodata\/Emotion-Lexicon.txt', sep='\\t')\n\nelements = emotion['word'].values\n     \nfear1 = 0\ndisgust1 = 0\nsadness1 = 0\njoy1 = 0\ntrust1 = 0\nanticipation1 = 0\nsurprise1 = 0\nanger1 = 0\n    \ntrump  = df[df['Speaker'] == 'TRUMP']\ntrump_speech = \" \".join(trump['words'])\ntrump_speech_list =   trump_speech.split()                 \nfor  wd in trump_speech_list:\n    for (j, element) in enumerate(elements):             \n        if fuzz.ratio(element, wd) >= 94:                            \n            if emotion['emotion'][j] == 'trust':\n                trust1 += 1\n            elif emotion['emotion'][j] == 'surprise':\n                surprise1 += 1\n            elif emotion['emotion'][j] == 'anticipation':\n                anticipation1 += 1\n            elif emotion['emotion'][j] == 'joy':\n                joy1 += 1\n            elif emotion['emotion'][j] == 'anger':\n                anger1 += 1\n            elif emotion['emotion'][j] == 'fear':\n                fear1 += 1\n            elif emotion['emotion'][j] == 'disgust':\n                disgust1 += 1\n            elif emotion['emotion'][j] == 'sadness':\n                sadness1 += 1\n                 \nprint(\"done...\")","ba54e594":"print(\"Num of 'Anticipation': \",anticipation1 )\nprint(\"Num of 'Surprise': \",surprise1 )\nprint(\"Num of 'Trust': \",trust1 )\nprint(\"Num of 'Joy': \",joy1 )\nprint(\"Num of 'Anger': \",anger1 )\nprint(\"Num of 'Fear': \",fear1 )\nprint(\"Num of 'Sadness': \",sadness1 )\nprint(\"Num of 'Disgust': \",disgust1 )","d084518e":"from prettytable import PrettyTable\ntable = PrettyTable() \nprint('Comparison table of emotions across Trump & Biden debate dialogues')\ntable.add_column('Person',['Anticipation','Trust','Surprise','Joy', 'Anger','Disgust','Fear','Sadness'])\ntable.add_column('Joe Biden',[anticipation, trust, surprise, joy,anger, fear, sadness,disgust])\ntable.add_column('Donald Trump ',[anticipation1, trust1, surprise1, joy1, anger1, fear1, sadness1,disgust1])\nprint(table)","58449962":"# make the dataframe\nemo = pd.DataFrame({'person':['Joe Biden','Joe Biden', 'Joe Biden','Joe Biden','Joe Biden','Joe Biden','Joe Biden','Joe Biden','Donald Trump','Donald Trump','Donald Trump','Donald Trump','Donald Trump','Donald Trump','Donald Trump','Donald Trump'],\n        'count': [fear,anger,sadness, disgust, anticipation, surprise, trust,joy, fear1,anger1,sadness1, disgust1, anticipation1, surprise1, trust1,joy1],\n        'emotion':['fear', 'anger', 'sadness', 'disgust', 'anticipation', 'surprise', 'trust','joy', 'fear', 'anger', 'sadness', 'disgust', 'anticipation', 'surprise', 'trust','joy'],\n        \n    })    \nemo","84832c97":"import plotly.express as px\n\nfig = px.bar(emo, x=\"person\", y=\"count\", color=\"emotion\", title=\"Emotions expressed in debate\")\nfig.show()","1ebc4367":"from wordcloud import WordCloud\nfrom textblob import TextBlob\nfrom textblob import Word\nimport matplotlib.pyplot as plt\n\ndef generate_wordcloud(speakers, df):\n    '''\n     Generates wordcloud files in the \/data folder\n     one for each name in the speakers list\n     the df dataframe has the following columns:\n        'year', 'party', 'debate', 'candidate', 'text'\n    '''\n    for p in speakers:\n        #print(\"Generating: %s\" % p)\n        \n        wordcloud = WordCloud( max_words=400, width=1500, height=1000, background_color='white')\n        text    =  \"\".join(df[df.Speaker == p]['Text'])\n        blob    = TextBlob(text)\n        nouns   = [ word for word, pos in blob.tags if pos == 'NN' ]\n\n        wordcloud.generate(' '.join(nouns))\n        fig = plt.figure(1, figsize=(10,10))\n        plt.axis('off')\n        title = \"Word Cloud for \"+ p\n        fig.suptitle(title, fontsize=20)\n        plt.imshow(wordcloud)\n        plt.show()        \n\ngenerate_wordcloud(speaker_list, df)","8f521e24":"!pip install -i https:\/\/test.pypi.org\/simple\/ summa","22c70483":"from summa.summarizer import summarize\nbiden_text = df[df['Speaker'] == 'BIDEN']['Text']\nprint(summarize(\" \".join(biden_text.values), words=300))\nprint(\" = \"* 60)\nprint(\"\\n\")\ntrump_text = df[df['Speaker'] == 'TRUMP']['Text']\nprint(summarize(\" \".join(trump_text.values), words=300))\nprint(\" = \"* 60)\nprint(\"\\n\")\nwallace_text = df[df['Speaker'] == 'WALLACE']['Text']\nprint(summarize(\" \".join(wallace_text.values), words=300))","a1063b4d":"from summa import keywords\nwords_spoken = \" \".join(df['words'].values)\nprint(keywords.keywords(words_spoken))","f14df9db":"biden_speech = df[df['Speaker'] == 'BIDEN']['words']\ntrump_speech = df[df['Speaker'] == 'TRUMP']['words']\nwallace_speech = df[df['Speaker'] == 'WALLACE']['words']","ffa95427":"print(\"Biden Speech top 1 grams\")\nprint(ngrams_top(biden_speech,(1,1),n=10))\nprint(\"\\nBiden Speech top 2 grams\")\nprint(ngrams_top(biden_speech,(2,2),n=10))\nprint(\"\\nBiden Speech top 3 grams\")\nprint(ngrams_top(biden_speech,(3,3),n=10))","2ff1d958":"print(\"Trump Speech top 1 grams\")\nprint(ngrams_top(trump_speech,(1,1),n=10))\nprint(\"\\nTrump Speech top 2 grams\")\nprint(ngrams_top(trump_speech,(2,2),n=10))\nprint(\"\\nTrump Speech top 3 grams\")\nprint(ngrams_top(trump_speech,(3,3),n=10))","727932d2":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.cluster import KMeans\n\ndef dolemma(text):\n    words = []\n    stemmer = WordNetLemmatizer()\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            words.append(stemmer.lemmatize(token, pos='v'))\n        words.append(token)\n    return \" \".join(words)\n  \n\ndf['lemma'] = df['words'].map(dolemma)\nvocab = \" \".join(df['lemma'].values)\ntfidfconvert = TfidfVectorizer(stop_words = 'english' ,ngram_range=(1,3),  max_features=1000)\n#print(tfidfconvert.vocabulary_)\ntransformed = tfidfconvert.fit_transform(df['lemma'])\nwords = tfidfconvert.get_feature_names()\n\nmodelkmeans = KMeans(n_clusters=10, init='k-means++', n_init=20)\nmodelkmeans.fit(transformed)\n\ncommon_words = modelkmeans.cluster_centers_.argsort()[:,-1:-11:-1]\nfor num, centroid in enumerate(common_words):\n    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))","3c3650c6":"Sum_of_squared_distances = []\nK = range(1,140)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(transformed)\n    Sum_of_squared_distances.append(km.inertia_)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,10)) \nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","2ee686f6":"def make_bigrams(text):\n    return bigram_mod[text] \n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    doc = nlp(\" \".join(texts)) \n    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n\n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(df['words'], min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(df['words'], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\ndata_words_bigrams = bigram_mod[df['words']]\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:1])","d5c4f46e":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(id2word[0])\n\n# Human readable format of corpus (term-frequency)\n[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","ff67da51":"# Build LDA model\nlda_model = gensim.models.LdaModel(corpus=corpus, id2word=id2word,\n                                       num_topics=10, random_state=100,\n                                       chunksize=100, passes=10, per_word_topics=True)\n\nfrom pprint import pprint\n#Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","06d6c685":"lda_model = gensim.models.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=12, \n                                           random_state=100,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha=.1,\n                                           eta=0.1)\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","bf3f902b":"pyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","949611f6":"Further work: Different analysis of the transcripts can be carried out by just counting words, word frequencies, length of sentences, frequency of certain pronouns (e.g., I, I\u2019m), etc. We can choose to focus on one measure, the candidate\u2019s influence in each debate by measuring how many times each candidate took turn speaking.","ed01f31e":"**If you like my notebook, please upvote and encourage me.**","f5643d27":"<a id=\"10\"><\/a>\n<font color=\"darkblue\" size=+3>Extract Topics<\/font>","55016bfd":"<a id=\"7\"><\/a>\n<font color=\"darkblue\" size=+3>N-Grams<\/font>","815bca01":"<font color=\"teal\" size=+2>India,China and Russia is talked about in the same breath and that too twice !!!<\/font>","38452677":"\n<font color=\"darkblue\" size=+3>Imports and Installations<\/font>","6e402c12":"<font color=\"teal\" size=+2>Donald trump is scoring LOW in positive qualities and scoring HIGH on negative qualities compared to Joe Biden.<\/font>","48b4b1a1":"<a id=\"3\"><\/a>\n<font color=\"darkblue\" size=+3>Various Spacy NER<\/font>","76bc2066":"<a id=\"2\"><\/a>\n<font color=\"darkblue\" size=+3>Utility Methods<\/font>","f014dc6f":"<font color=\"teal\" size=+3>A bunch of names came in debate. Important ones were Kamala Harris, Obama, Nancy Pelosi, Bernie Sanders<\/font>","8690b606":"<a id=\"6\"><\/a>\n<font color=\"darkblue\" size=+3>Extracting Summary<\/font>","3c3ca9ea":"I am using dataset \"NRC-Emotion-Intensity\" from - https:\/\/saifmohammad.com\/WebPages\/AffectIntensity.htm\n\nThe words are lemma\/stem based, so I am using fuzzywuzzy( >= 94) to choose closest words and get a count of 4 mentioned postive emotions - anticipation, joy, trust, surprise and counts of 4 mentioned negative emotions - anger, disgust, sadness and fear.\n\nWe are getting most viewed episodes and less viewed episodes and looking at their emotional appeal. Below is Plutchik\u2019s Wheel of Emotions","87886bd6":"<a id=\"5\"><\/a>\n<font color=\"darkblue\" size=+3>Word clouds<\/font>","b9532710":"<font color=\"teal\" size=+3>Topics covered from COVID to Ku KLUX KLAN...Interesting !!!<\/font>","4bbb1269":"<font color=\"teal\" size=+2>A bunch of countries were talked about and lot of US states as well !!!<\/font>","02915681":"<font size=\"+4\" color=\"magenta\" ><b> <center><u>Analysis of US Presidential Debate Sep 2020 <\/u><\/center><\/b><\/font>","c1c15b25":"<font color=\"teal\" size=+3>'Trillion' talks. Need to double down on this.<\/font>","cdbd3f52":"<a id=\"4\"><\/a>\n<font color=\"darkblue\" size=+3>Extracting punctuation, numbers<\/font>","7a1fa474":"<a id=\"9\"><\/a>\n<font color=\"darkblue\" size=+3>K-Means<\/font>","e68f4ca9":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of contents<\/h3>\n\n* [Notebook](#note)\n    * [1. Data](#1)\n    * [2. Utility Functions](#2)\n    * [3. Spacy's NER](#3)\n    * [4. Extracting punctuation, numbers](#4)\n    * [5. Word Cloud](#5)\n    * [6. Emotional Analysis](#6)\n    * [7. N-Grams](#7)\n    * [8. Extracting Summary](#8)\n    * [9. K-Means](#9)\n    * [10. Extracting Topics](#10)\n    ","df0f4c37":"<a id=\"1\"><\/a>\n<font color=\"darkblue\" size=+3>Data<\/font>","000ac040":"<a id=\"5\"><\/a>\n<font color=\"darkblue\" size=+3>Emotional Analysis<\/font>"}}