{"cell_type":{"74d18365":"code","fa8315e2":"code","db0997b2":"code","d7a1da77":"code","e06c68c5":"code","72ef9247":"code","84185f99":"code","ab83d0d7":"code","e54024ac":"code","93a6d6dd":"code","18ba65e1":"code","b26b9201":"code","1e764e95":"code","9f013bb5":"code","09203a0a":"code","ae111ec7":"code","c0226ed6":"code","16e1d541":"code","987863ec":"code","01d55b16":"code","725e345a":"code","73da4d7b":"code","e80a6dcf":"code","b48d5b05":"code","02886540":"code","d174721d":"code","75f4884c":"code","30f09cf1":"code","00dcb6ca":"code","97b3343b":"code","88e3d47a":"code","e7a7ea30":"code","10c806f4":"code","2acc9ff0":"code","a6a4e058":"code","9b2eb24f":"code","cd668621":"code","92bf7e8d":"code","d0e19880":"code","dbe72513":"code","45b7d764":"code","08585e89":"code","0f89fc70":"markdown","b7a718c8":"markdown","9e3d4acb":"markdown","4cd68ff8":"markdown","601082ee":"markdown","8f9073a8":"markdown","9c5ada24":"markdown","a4e00718":"markdown","64a7f3b4":"markdown","2303fbf9":"markdown","97bec803":"markdown","6be7d429":"markdown","918c42f8":"markdown","f717cc08":"markdown","52eb4af4":"markdown","75d81e2d":"markdown","de24882b":"markdown"},"source":{"74d18365":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set(style=\"white\")\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, normalize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score","fa8315e2":"# Load csv as pandas frame and drop useless columns\ndata = pd.read_csv('..\/input\/churn-for-bank-customers\/churn.csv', index_col='RowNumber')\\\n    .drop(['Surname', 'CustomerId'], axis=1)\ndata","db0997b2":"data.info()","d7a1da77":"data['Geography'].value_counts()","e06c68c5":"data['Gender'].value_counts()","72ef9247":"# converts categorical features to integers\ndef label_encoder(data_: pd.DataFrame(), columns_name_: list):\n    le = LabelEncoder()\n    for i in columns_name_:\n        le.fit(data_[i])\n        data_[i] = le.transform(data_[i])\n    return data_","84185f99":"data = label_encoder(data, ['Geography', 'Gender'])\ndata","ab83d0d7":"print(data.CreditScore.describe())\n\nplt.title('CreditScore')\nplt.hist(data.CreditScore)\nplt.show()","e54024ac":"print(data.Balance.describe())\n\nplt.title('Balance')\nplt.hist(data.Balance)\nplt.show()","93a6d6dd":"print(data.EstimatedSalary.describe())\n\nplt.title('EstimatedSalary')\nplt.hist(data.EstimatedSalary)\nplt.show()","18ba65e1":"plt.title('Exited')\n\nsns.barplot(x=data['Exited'].value_counts().keys(),\n            y=data['Exited'].value_counts().values)","b26b9201":"plt.title('Tenure')\n\nsns.barplot(x=data['Tenure'].value_counts().keys(),\n            y=data['Tenure'].value_counts().values)","1e764e95":"plt.title('Gender')\n\nsns.barplot(x=['male', 'female'],\n            y=data['Gender'].value_counts().values)","9f013bb5":"plt.title('NumOfProducts')\n\nsns.barplot(x=data['NumOfProducts'].value_counts().keys(),\n            y=data['NumOfProducts'].value_counts().values)","09203a0a":"plt.title('HasCrCard')\n\nsns.barplot(x=data['HasCrCard'].value_counts().keys(),\n            y=data['HasCrCard'].value_counts().values)","ae111ec7":"plt.title('IsActiveMember')\n\nsns.barplot(x=data['IsActiveMember'].value_counts().keys(),\n            y=data['IsActiveMember'].value_counts().values)","c0226ed6":"print('Min:', data['Age'].min(),\n      '\\nMax:', data['Age'].max())\nval_count = data['Age'].value_counts()\nplt.title('Age')\nplt.plot([i for i in range(data['Age'].min(), data['Age'].max() + 1)],\n         [val_count[i] if i in val_count else 0 for i in range(data['Age'].min(), data['Age'].max() + 1)])","16e1d541":"sns.barplot(x=['France', 'Germany', 'Spain'],\n            y=[*data['Geography'].value_counts().values])","987863ec":"# correlation table\n\ncorr = data.corr()\nf, ax = plt.subplots(figsize=(10, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=None, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","01d55b16":"sns.pairplot(data, hue=\"Exited\", palette=\"husl\")","725e345a":"# split data on train, val and test\nx_tr, x_te, y_tr, y_te = train_test_split(\n    data.iloc[:, :-1], data['Exited'], random_state=42, test_size=0.2, shuffle=True)\nx_tr, x_val, y_tr, y_val = train_test_split(\n    x_tr, y_tr, random_state=42, test_size=0.2, shuffle=True)\nprint(x_tr.shape)\nprint(x_val.shape)\nprint(x_te.shape)","73da4d7b":"final_score = {}","e80a6dcf":"# calculate auc (if possible), accuracy, f-metric and recall; return pandas frame\ndef calc_score(y_true, y_pred, y_pred_proba=None):\n    return pd.DataFrame(data={'metrics': ['auc', 'acc', 'f1', 'recall'],\n                              'single model': [roc_auc_score(y_true, y_pred_proba).round(3)\\\n                                               if y_pred_proba is not None else '-',\n                                               accuracy_score(y_true, y_pred).round(3),\n                                               f1_score(y_true, y_pred).round(3),\n                                               recall_score(y_true, y_pred).round(3)]})","b48d5b05":"import copy\n\n# coss-validation\ndef kfold(model, split: int, X, y, x_test):\n    \"\"\"\n    :param model: sklearn model\n    :param split: number of folds\n    :param X: train data\n    :param y: target\n    :param x_test: test data, which need to predict\n    :return: np.array with predictions on x_test\n    \"\"\"\n    pred_cross_val = []\n    # init KFold\n    kf = KFold(n_splits=split, shuffle=False)\n    kf.get_n_splits(X)\n    for train_index, test_index in kf.split(X):\n        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        # init new model with same parameters\n        model_ = copy.copy(model)\n        model_.fit(X_train, y_train)\n        pred_cross_val.append(model_.predict(x_test))\n    # mean prediction\n    pred_cross_val = np.array(pred_cross_val).mean(axis=0)\n    pred_cross_val = np.around(pred_cross_val)\n    return pred_cross_val","02886540":"# LogReg model\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_tr, y_tr)\n# calculate score for single model on validation sample\nscore = calc_score(y_val, lr.predict(x_val))\n\n# predict and calculate score for cross-validation model on validation sample\npred_cross_val = kfold(LogisticRegression(), 2, x_tr.values, y_tr.values, x_val)\n# add column 'cross. val. model' in score\nscore['cross. val. model'] = calc_score(y_val, pred_cross_val)['single model']\n\n# predict and calculate score for cross-validation model on test sample\npred_cross_val = kfold(LogisticRegression(), 2, x_tr.values, y_tr.values, x_te)\nfinal_score['LogisticRegression'] = calc_score(y_te, pred_cross_val)['single model'][1]\nscore","d174721d":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(10)\nknn.fit(x_tr, y_tr)\n# calculate score for single model on validation sample\nscore = calc_score(y_val, knn.predict(x_val))\n\n# predict and calculate score for cross-validation model on validation sample\npred_cross_val = kfold(KNeighborsClassifier(10), 2, x_tr.values, y_tr.values, x_val)\nscore['cross. val. model'] = calc_score(y_val, pred_cross_val)['single model']\n\n# predict and calculate score for cross-validation model on test sample\npred_cross_val = kfold(KNeighborsClassifier(10), 2, x_tr.values, y_tr.values, x_te)\nfinal_score['KNeighborsClassifier'] = calc_score(y_te, pred_cross_val)['single model'][1]\nscore","75f4884c":"from catboost import CatBoostClassifier\n\ncatb = CatBoostClassifier(learning_rate=0.1, boosting_type='Ordered', verbose=0)\ncatb.fit(x_tr, y_tr, eval_set=(x_val, y_val), use_best_model=True)\n\n# predict and calculate score for single model on validation sample\nscore = calc_score(y_val, catb.predict(x_val), catb.predict_proba(x_val)[:, 1])\n\n# predict and calculate score for cross-validation model on test sample\nfinal_score['CatBoostClassifier'] = calc_score(y_te, catb.predict(x_te),\n                                        catb.predict_proba(x_te)[:, 1])['single model'][1]\n\nscore","30f09cf1":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(learning_rate=0.1, n_estimators=100)\nlgbm.fit(x_tr, y_tr)\n\n# predict and calculate score for single model on validation sample\nscore = calc_score(y_val, lgbm.predict(x_val), lgbm.predict_proba(x_val)[:, 1])\n\n# predict and calculate score for cross-validation model on test sample\nfinal_score['LGBMClassifier'] = calc_score(y_te, lgbm.predict(x_te),\n                                              lgbm.predict_proba(x_te)[:, 1])['single model'][1]\nscore","00dcb6ca":"pred = []\npred_proba = []\npred.append(catb.predict(x_te))\npred_proba.append(catb.predict_proba(x_te)[:, 1])\npred.append(lgbm.predict(x_te))\npred_proba.append(lgbm.predict_proba(x_te)[:, 1])\n# mean prediction\npred = np.array(pred).mean(axis=0).round()\npred_proba = np.array(pred_proba).mean(axis=0)\n\n# calculate ensemble score\ncalc_score(y_te, pred, pred_proba)","97b3343b":"import keras\nfrom keras.layers import Dense, Dropout, LeakyReLU\nfrom keras import Sequential\nfrom keras.metrics import Accuracy, AUC\nfrom keras.optimizers import Adam","88e3d47a":"# plot loss and auc on each epoch\ndef ploting(history):\n    # print(history.history.keys())\n    ac = []\n    for i in history.history.keys():\n        ac.append(i)\n    loss = history.history[ac[0]]\n    val_loss = history.history[ac[2]]\n    acc = history.history[ac[1]]\n    val_acc = history.history[ac[3]]\n    epochs = range(1, len(loss) + 1)\n    fig = plt.figure(figsize=(10, 10))\n    ax1 = fig.add_subplot(2, 1, 1)\n    ax2 = fig.add_subplot(2, 1, 2)\n    ax1.plot(epochs, loss, 'bo', label='Training loss')\n    ax1.plot(epochs, val_loss, 'b', label='Validation loss', color='r')\n    ax1.set_title('Training and validation loss')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n    ax2.plot(epochs, acc, 'bo', label='Training acc')\n    ax2.plot(epochs, val_acc, 'b', label='Validation acc', color='r')\n    ax2.set_title('Training and validation accuracy')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('AUC')\n    ax2.legend()\n    for ax in fig.axes:\n        ax.grid(True)\n    plt.show()","e7a7ea30":"# normalize and split data on train, val and test\nscaler = StandardScaler()\nscaler.fit(data.iloc[:, :-1])\n\nx_tr, x_te, y_tr, y_te = train_test_split(\n    data.iloc[:, :-1], data['Exited'], random_state=42, test_size=0.2, shuffle=True)\nx_tr, x_val, y_tr, y_val = train_test_split(\n    x_tr, y_tr, random_state=42, test_size=0.2, shuffle=True)\nprint(x_tr.shape)\nprint(x_val.shape)\nprint(x_te.shape)\n\nx_tr = scaler.transform(x_tr)\nx_val = scaler.transform(x_val)\nx_te = scaler.transform(x_te)\n\ny_tr = y_tr.values.reshape(-1, 1)\ny_val = y_val.values.reshape(-1, 1)\ny_te = y_te.values.reshape(-1, 1)","10c806f4":"activation = LeakyReLU(alpha=0.2)\n\n# create model\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=x_tr.shape[-1], activation=activation))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation=activation))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(16, activation=activation))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(8, activation=activation))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nprint(model.summary())\n\n# compile model\nmodel.compile(optimizer=Adam(lr=0.001),\n              loss='binary_crossentropy',\n              metrics=[AUC()])\n\n# fit model\nhist = model.fit(x_tr, y_tr,\n          batch_size=64, epochs=150,\n          validation_data=(x_val, y_val), verbose=2)","2acc9ff0":"ploting(hist)","a6a4e058":"score = calc_score(y_val,\n                      model.predict(x_val).reshape(-1).round(),\n                      model.predict(x_val).reshape(-1))\nfinal_score['NeuroClassifier'] = calc_score(y_te,\n                model.predict(x_te).reshape(-1).round(),\n                model.predict(x_te).reshape(-1))['single model'][1]\nscore","9b2eb24f":"import keras\nfrom keras.layers import (Dropout, LeakyReLU, Conv1D,\n                          MaxPooling1D, GlobalAveragePooling1D, BatchNormalization)\nfrom keras import Sequential\nfrom keras.metrics import Accuracy, AUC\nfrom keras.optimizers import Adam","cd668621":"activation = 'sigmoid'\n\nmodel = Sequential()\nmodel.add(Conv1D(64, 3, input_shape=(10, 1), padding='same', activation=activation))\nmodel.add(MaxPooling1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Conv1D(64, 3, padding='same', activation=activation))\nmodel.add(MaxPooling1D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(GlobalAveragePooling1D())\n#model.add(Dropout(0.25))\nmodel.add(Dense(1, activation='sigmoid'))\n\nprint(model.summary())\n\nmodel.compile(optimizer=Adam(lr=0.001),\n              loss='binary_crossentropy',\n              metrics=[AUC()])\n\nprint(x_tr.reshape(-1, x_tr.shape[1], 1).shape)\nprint(x_val.reshape(-1, x_tr.shape[1], 1).shape)\nhist = model.fit(x_tr.reshape(-1, x_tr.shape[1], 1), y_tr,\n          batch_size=256, epochs=200,\n          validation_data=(x_val.reshape(-1, x_val.shape[1], 1),\n                           y_val))","92bf7e8d":"ploting(hist)","d0e19880":"score = calc_score(y_val,\n                      model.predict(x_val.reshape(-1, x_tr.shape[1], 1)).reshape(-1).round(),\n                      model.predict(x_val.reshape(-1, x_tr.shape[1], 1)).reshape(-1))\nfinal_score['CNNClassifier'] = calc_score(y_te,\n                model.predict(x_te.reshape(-1, x_tr.shape[1], 1)).reshape(-1).round(),\n                model.predict(x_te.reshape(-1, x_tr.shape[1], 1)).reshape(-1))['single model'][1]\nscore","dbe72513":"final_score","45b7d764":"# sort models' score\nimport operator\nsort_dict = sorted(final_score.items(), key=operator.itemgetter(1), reverse=True)\nsort_dict","08585e89":"# rating\ndef visualize(column=0):\n    y = [x[1] for x in sort_dict]\n    labels = [x[0] for x in sort_dict]\n    shift = 0.78\n    plt.figure(figsize=(15, 10))\n    graph = sns.barplot(x=(np.asarray(y) - shift), y=labels,\n                        palette=sns.color_palette(\"RdYlGn_r\", len(y)),\n                        edgecolor=\".2\", linewidth=2)\n    plt.xticks([i \/ 100 for i in range(0, 11)], [\"%.2f\" % (i \/ 100 + shift) for i in range(0, 11)])\n    for i, v in enumerate(y):\n        graph.text(v - shift - 0.009, i + 0.05, \"%.4f\" % v, color='darkslategray', fontweight='bold', size=14)\n    plt.title('Rating accuracy')\n    plt.show()\n\nvisualize(0)","0f89fc70":"# Train models and predict","b7a718c8":"## Test Neural Networks","9e3d4acb":"## Test sklearn models","4cd68ff8":"# Score","601082ee":"### Predict","8f9073a8":"## Split data","9c5ada24":"# Load data","a4e00718":"### Predict","64a7f3b4":"## Gradboost ensemble","2303fbf9":"## Test gradboost","97bec803":"# Prepare data","6be7d429":"## Test Convolution Neural Networks","918c42f8":"## Data visualization","f717cc08":"### Create model","52eb4af4":"### Create model","75d81e2d":"### Prepare data for NN","de24882b":"# Models' rating"}}