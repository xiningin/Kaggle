{"cell_type":{"2efdc50a":"code","1f06baf0":"code","93049e3c":"code","1f24801c":"code","fc89575b":"code","5cacb567":"code","a9ee1ed4":"code","2f4bc11b":"code","8845f19c":"code","d505b1a9":"code","88eb8854":"code","86fdc876":"code","5d8688be":"code","002b9dce":"code","1b9452a5":"code","ab838ce3":"code","2e5eb57d":"code","51926f14":"code","2dfba99b":"code","4d272fad":"code","1bf92573":"code","f35ed7e6":"code","e8040b4e":"code","507966b3":"code","171ce642":"code","49ecaf4f":"code","76ffefa3":"code","417bb6a7":"code","c93bdccd":"code","3b39c9ef":"code","42484b0b":"code","7cc33dc8":"code","9e8c3339":"code","d7ffaedb":"code","5c2ca2e7":"code","c381fcad":"code","23a93c57":"code","4a14fda6":"code","fde9ed07":"code","da6bb262":"code","5bd9730f":"code","3840b4d4":"code","0697ccca":"code","8b2e12d6":"code","981751ab":"code","2b07e6e1":"code","b2665c5f":"code","7001a1bd":"code","fc2ce760":"markdown","777c9aaf":"markdown","c6074bb1":"markdown","f1299ed1":"markdown","63bd4a10":"markdown","25695c54":"markdown","e8cd48ad":"markdown","72342725":"markdown","c104fa2a":"markdown","4c5590e5":"markdown","42c1d1aa":"markdown","b820cae2":"markdown","a6180f37":"markdown","e98dae16":"markdown","99448fbf":"markdown","4d447998":"markdown","0b7b84a4":"markdown"},"source":{"2efdc50a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f06baf0":"# Reading .csv file \nimport pandas as pd\ndataFake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv', encoding='ISO-8859-1')\ndataReal = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv', encoding='ISO-8859-1')","93049e3c":"# To get the dimentions of the dataset.\nprint(\"Real Data Dimentions\", dataReal.shape)\nprint(\"Fake Data Dimentions\", dataFake.shape)","1f24801c":"# To get the column names of the dataset\nprint(\"Real Data Column\", dataReal.columns)\nprint(\"Fake Data Column\", dataFake.columns)","fc89575b":"# to print the consise summary of the data set\nprint(\"Real Data Summaary\")\nprint( dataReal.info())","5cacb567":"# to print the consise summary of the data set\nprint(\"Fake Data Column Summary\")\nprint(dataFake.info())","a9ee1ed4":"# to compute a summary of statistics pertaining to the DataFrame columns\nprint(\"Real Data Summaary\")\ndataReal.describe()","2f4bc11b":"# to compute a summary of statistics pertaining to the DataFrame columns\nprint(\"Fake Data Column Summary\")\ndataFake.describe()","8845f19c":"# to return the first five rows of the data frame\ndataReal.head()","d505b1a9":"# to return the first five rows of the data frame\ndataFake.head()","88eb8854":"dataFake['class'] = 0 \ndataReal['class'] = 1","86fdc876":"dataFake.drop(['date', 'subject'], axis=1, inplace=True)\ndataReal.drop(['date', 'subject'], axis=1, inplace=True)","5d8688be":"dataFake['text'] = dataFake['title'] + dataFake['text']\ndataFake.drop('title', axis=1, inplace=True)","002b9dce":"dataReal['text'] = dataReal['title'] + dataReal['text']\ndataReal.drop('title', axis=1, inplace=True)","1b9452a5":"dataFake.dropna(inplace=True)","ab838ce3":"dataReal.dropna(inplace=True)","2e5eb57d":"dataFake.drop_duplicates()","51926f14":"dataReal.drop_duplicates()","2dfba99b":"import re","4d272fad":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt ","1bf92573":"dataFake['tidy_tweet'] = np.vectorize(remove_pattern)(dataFake['text'], \"@[\\w]*\")","f35ed7e6":"dataReal['tidy_tweet'] = np.vectorize(remove_pattern)(dataReal['text'], \"@[\\w]*\")","e8040b4e":"dataFake['tidy_tweet'] = dataFake['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")","507966b3":"dataReal['tidy_tweet'] = dataReal['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")","171ce642":"dataFake['tidy_tweet'] = dataFake['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","49ecaf4f":"dataReal['tidy_tweet'] = dataReal['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n","76ffefa3":"def cleaning_URLs(data):\n    return re.sub('((www.[^s]+)|(https?:\/\/[^s]+))',' ',data)","417bb6a7":"dataFake['tidy_tweet'] = dataFake['tidy_tweet'].apply(lambda x: cleaning_URLs(x))","c93bdccd":"dataReal['tidy_tweet'] = dataReal['tidy_tweet'].apply(lambda x: cleaning_URLs(x))","3b39c9ef":"import string","42484b0b":"import nltk  \nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\nfrom nltk.stem import WordNetLemmatizer","7cc33dc8":"stopwords_english = stopwords.words('english') ","9e8c3339":"print(stopwords_english)","d7ffaedb":"tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)","5c2ca2e7":"tweet_tokens_fake = tokenizer.tokenize(str(dataFake['tidy_tweet']))","c381fcad":"tweet_tokens_real = tokenizer.tokenize(str(dataReal['tidy_tweet']))","23a93c57":"tweet_tokens_fake","4a14fda6":"tweet_tokens_real","fde9ed07":"clean_data_fake = []\n\nfor word_fake in tweet_tokens_fake: # Go through every word in your tokens list\n    if (word_fake not in stopwords_english and  # remove stopwords\n        word_fake not in string.punctuation):  # remove punctuation\n        clean_data_fake.append(word_fake)\n\nprint('Data after removing of stop words')\nprint(clean_data_fake)","da6bb262":"clean_data_real = []\n\nfor word_real in tweet_tokens_real: # Go through every word in your tokens list\n    if (word_real not in stopwords_english and  # remove stopwords\n        word_real not in string.punctuation):  # remove punctuation\n        clean_data_real.append(word_real)\n\nprint('Data after removing of stop words')\nprint(clean_data_real)","5bd9730f":"# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ndata_stem_fake = [] \n\nfor word in clean_data_fake:\n    stem_word_fake = stemmer.stem(word)  # stemming word\n    data_stem_fake.append(stem_word_fake)  # append to the list\n\nprint('Data after Stemming')\nprint(data_stem_fake)","3840b4d4":"# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ndata_stem_real = [] \n\nfor word in clean_data_real:\n    stem_word_real = stemmer.stem(word)  # stemming word\n    data_stem_real.append(stem_word_real)  # append to the list\n\nprint('Data after Stemming')\nprint(data_stem_real)","0697ccca":"# instantiate Lemmatization Class\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Create an empty list to store the stems\ndata_lemm_fake = [] \n\nfor word in clean_data_real:\n    lemm_word_fake =wordnet_lemmatizer.lemmatize(word)  # Lemmatization word\n    data_lemm_fake.append(lemm_word_fake)  # append to the list\n\nprint('Data after Lemmatization')\nprint(data_lemm_fake)","8b2e12d6":"# instantiate Lemmatization Class\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Create an empty list to store the stems\ndata_lemm_real = [] \n\nfor word in clean_data_real:\n    lemm_word_real =wordnet_lemmatizer.lemmatize(word)  # Lemmatization word\n    data_lemm_real.append(lemm_word_real)  # append to the list\n\nprint('Data after Lemmatization')\nprint(data_lemm_real)","981751ab":"# packages\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.spatial import distance","2b07e6e1":"vectorizer = CountVectorizer()\nreal_vector = vectorizer.fit_transform(dataFake)\nfake_vector = vectorizer.fit_transform(dataReal)\ntext_to_vector_fake = real_vector.toarray()[0].tolist()\ntext_to_vector_real= fake_vector.toarray()[1].tolist()","b2665c5f":" cosine = distance.cosine(text_to_vector_fake, text_to_vector_real)","7001a1bd":"print('Similarity of Real and fake tweets are equal to ',round((1-cosine)*100,2),'%')","fc2ce760":"# Removal of Stopwords","777c9aaf":"# Load Data","c6074bb1":"Cleaning Tweets, Remove twitter handles (@user), Remove special characters, numbers, punctuations, Remove Short Words, Remove URLs, Removing Hashtag","f1299ed1":"Removing Short Words","63bd4a10":"Droping columns date and subject","25695c54":"Remove twitter handles (@user)","e8cd48ad":"Remove special characters, numbers, punctuations","72342725":"# EXPLORATORY DATA ANALYSIS","c104fa2a":"Combine the title and text column","4c5590e5":"Drop rows with any missing values","42c1d1aa":"Create a new column Class which holds *0* for *Fake* and *1* for *Real* Tweets","b820cae2":"# Stemming","a6180f37":"Removing duplicates","e98dae16":"# Tokenization","99448fbf":"# Lemmatization","4d447998":"# Pre-processing of Tweets","0b7b84a4":"Remove URLs"}}