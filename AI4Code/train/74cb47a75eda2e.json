{"cell_type":{"9a2780c9":"code","84789325":"code","29194073":"code","ae450ed6":"code","89cb72d2":"code","7c5a45b2":"code","278980da":"code","d5ae9298":"code","1adf7b4b":"code","876937fa":"code","fba6c3e1":"code","d9344a00":"code","7aff807f":"code","ec015a9e":"code","810102d6":"code","abbb8767":"code","782ab31f":"code","307af09a":"code","af465da5":"markdown","0d30a206":"markdown","8d26795e":"markdown","4b935eca":"markdown","743e8b3b":"markdown","30e2ae58":"markdown","8fc3c01f":"markdown","0e7763d5":"markdown","bd3099c9":"markdown","271167b6":"markdown","140a686c":"markdown"},"source":{"9a2780c9":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","84789325":"train = pd.read_csv('..\/input\/train.csv.zip')\ntest = pd.read_csv('..\/input\/test.csv.zip')\nsubm = pd.read_csv('..\/input\/sample_submission.csv.zip')","29194073":"train.head()","ae450ed6":"train['comment_text'][0]","89cb72d2":"train['comment_text'][2]","7c5a45b2":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","278980da":"lens.hist();","d5ae9298":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","1adf7b4b":"len(train),len(test)","876937fa":"COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","fba6c3e1":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","d9344a00":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[COMMENT])\ntest_term_doc = vec.transform(test[COMMENT])","7aff807f":"trn_term_doc, test_term_doc","ec015a9e":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","810102d6":"x = trn_term_doc\ntest_x = test_term_doc","abbb8767":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","782ab31f":"preds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","307af09a":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","af465da5":"Example","0d30a206":"We'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset.","8d26795e":"There are a few empty comments that we need to get rid of, otherwise sklearn will complain.","4b935eca":"This creates a *sparse matrix* with only a small number of non-zero elements.","743e8b3b":"1. And finally, creating a submission file","30e2ae58":"Here's the basic naive bayes feature equation:","8fc3c01f":"## Looking at the data\n\nThe training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict.","0e7763d5":"## Building the model\n\nWe'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper.","bd3099c9":"Fit a model for one dependent at a time:","271167b6":"## Introduction\n\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine)","140a686c":"The length of the comments varies a lot."}}