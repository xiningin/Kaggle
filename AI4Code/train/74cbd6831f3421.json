{"cell_type":{"862ba90a":"code","22242d49":"code","a5a1e9a3":"code","9fc76cea":"code","64a8b140":"code","a5ce73c8":"code","16119f63":"code","3d3e4ea3":"code","87d7bc94":"code","e85869cf":"code","358b61ae":"code","ac1d7a67":"code","4569a612":"code","65cf9055":"code","17620969":"code","99719b2b":"code","ae0ca28c":"code","f24fd33d":"markdown","10514d6c":"markdown","e508814c":"markdown","cc4026df":"markdown","ad385971":"markdown","32f7ffec":"markdown","a5ea845d":"markdown","46cd9a3b":"markdown","7c57fb05":"markdown","b3d4fa2c":"markdown"},"source":{"862ba90a":"import math\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pandas_profiling as pp\nimport missingno as msno\nfrom sklearn.model_selection import train_test_split\n\npd.set_option('display.max_columns', None) \n\n# Read the datahome-data-for-ml-course\/\nX_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)","22242d49":"print('missing value:(missing count, total element,percentage)')\n[{x:(X_full[x].isna().sum(),len(X_full.index),math.ceil(X_full[x].isna().sum()*100\/len(X_full.index)*100)\/100)} for x in X_full.columns[X_full.isna().any()]]","a5a1e9a3":"msno.heatmap(X_full)","9fc76cea":"# pp.ProfileReport(pd.concat([y,X_full], axis=1))","64a8b140":"# This is skew cols we saw when profiling the data \n# skew_cols = ['1stFlrSF','BsmtUnfSF','GrLivArea','LotArea','OpenPorchSF','TotalBsmtSF']\n# skew_cols = ['BsmtUnfSF','GrLivArea','LotArea','OpenPorchSF']\nskew_cols = []\n\nprint(X_full['Exterior1st'].unique())\nprint(X_full['Exterior2nd'].unique())\nprint(X_full['Condition1'].unique())\nprint(X_full['Condition2'].unique())\nprint(X_full['FullBath'].unique())\nprint(X_full['HalfBath'].unique())\nprint(X_full['SaleType'].unique())\nX_full[['Exterior1st','Exterior2nd','Condition1','Condition2','YearBuilt','YearRemodAdd','YrSold','GarageYrBlt']].head()","a5ce73c8":"# Convert numeric cols to categories\nX_full['MSSubClass'] = X_full['MSSubClass'].apply(str)\nX_test_full['MSSubClass'] = X_test_full['MSSubClass'].apply(str)\n\nX_full['MoSold'] = X_full['MoSold'].apply(str)\nX_test_full['MoSold'] = X_test_full['MoSold'].apply(str)\n\n# Changing OverallCond into a categorical variable\n# X_full['OverallCond'] = X_full['OverallCond'].astype(str)\n# X_test_full['OverallCond'] = X_test_full['OverallCond'].astype(str)","16119f63":"# Merge 'Exterior1st', 'Exterior2nd' to 'Exterior'\nX_full['Exterior'] =  X_full.apply(lambda x: x['Exterior1st'] if (pd.isnull(x['Exterior2nd'])) else str(x['Exterior1st'])+'-'+str(x['Exterior2nd']), axis=1)\nX_test_full['Exterior'] =  X_test_full.apply(lambda x: x['Exterior1st'] if (pd.isnull(x['Exterior2nd'])) else str(x['Exterior1st'])+'-'+str(x['Exterior2nd']), axis=1)\nX_full.drop(['Exterior1st', 'Exterior2nd'],axis=1,inplace=True)\nX_test_full.drop(['Exterior1st', 'Exterior2nd'],axis=1,inplace=True)\n\n# Merge 'Condition1', 'Condition2' to 'Condition'\nX_full['Condition'] =  X_full.apply(lambda x: x['Condition1'] if (pd.isnull(x['Condition2'])) else str(x['Condition1'])+'-'+str(x['Condition2']), axis=1)\nX_test_full['Condition'] =  X_test_full.apply(lambda x: x['Condition1'] if (pd.isnull(x['Condition2'])) else str(x['Condition1'])+'-'+str(x['Condition2']), axis=1)\nX_full.drop(['Condition1', 'Condition2'],axis=1,inplace=True)\nX_test_full.drop(['Condition1', 'Condition2'],axis=1,inplace=True)\n\n# Caculate YearRemodAdd and YrSold\n# X_full['YearRemodAdd'] = X_full.apply(lambda x: x['YearRemodAdd'] - x['YearBuilt'], axis=1)\n# X_full['GarageYrBlt'] = X_full.apply(lambda x: x['GarageYrBlt'] - x['YearBuilt'], axis=1)\n# X_full['YrSold'] = X_full.apply(lambda x: x['YrSold'] - x['YearBuilt'], axis=1)\n\n# X_test_full['YearRemodAdd'] = X_test_full.apply(lambda x: x['YearRemodAdd'] - x['YearBuilt'], axis=1)\n# X_test_full['GarageYrBlt'] = X_test_full.apply(lambda x: x['GarageYrBlt'] - x['YearBuilt'], axis=1)\n# X_test_full['YrSold'] = X_test_full.apply(lambda x: x['YrSold'] - x['YearBuilt'], axis=1)\n\n# Generate total square\nX_full['TotalSF'] = X_full['TotalBsmtSF'] + X_full['1stFlrSF'] + X_full['2ndFlrSF']\nX_test_full['TotalSF'] = X_test_full['TotalBsmtSF'] + X_test_full['1stFlrSF'] + X_test_full['2ndFlrSF']\nX_full.drop(columns=['TotalBsmtSF','1stFlrSF','2ndFlrSF'],axis=1,inplace=True)\nX_test_full.drop(columns=['TotalBsmtSF','1stFlrSF','2ndFlrSF'],axis=1,inplace=True)\n\n# # Generate total bathroom\nX_full['TotalBathroom'] = 1*X_full['FullBath'] + X_full['HalfBath']\nX_test_full['TotalBathroom'] = 1*X_test_full['FullBath'] + X_test_full['HalfBath']\nX_full.drop(['FullBath', 'HalfBath'],axis=1,inplace=True)\nX_test_full.drop(['FullBath', 'HalfBath'],axis=1,inplace=True)\n\n# # Generate BsmtBath\n# X_full['BsmtBath'] = X_full['BsmtHalfBath'] + 1.1*X_full['BsmtFullBath']\n# X_test_full['BsmtBath'] = X_test_full['BsmtHalfBath'] + 1.1*X_test_full['BsmtFullBath']\n# X_full.drop(['BsmtHalfBath', 'BsmtFullBath'],axis=1,inplace=True)\n# X_test_full.drop(['BsmtHalfBath', 'BsmtFullBath'],axis=1,inplace=True)\n\n# Generate TotalPorch\n# X_full['TotalPorch'] = X_full['EnclosedPorch'] + X_full['SsnPorch'] + X_full['ScreenPorch']\n# X_test_full['TotalPorch'] = X_test_full['EnclosedPorch'] + X_test_full['SsnPorch'] + X_test_full['ScreenPorch']\n# X_full.drop(['EnclosedPorch','SsnPorch','ScreenPorch'],axis=1,inplace=True)\n# X_test_full.drop(['EnclosedPorch','SsnPorch','ScreenPorch'],axis=1,inplace=True)\n\n# Generate Bsmt\n# X_full['Bsmt'] = X_full['BsmtCond'] + X_full['BsmtExposure'] + X_full['BsmtFinType1'] + X_full['BsmtFinType2']\n# X_test_full['Bsmt'] = X_test_full['BsmtCond'] + X_test_full['BsmtExposure'] + X_test_full['BsmtFinType1'] + X_full['BsmtFinType2']\n# X_full['BsmtFinType2'] =  X_full['BsmtFinType1'] + X_full['BsmtFinType2']\n# X_test_full['BsmtFinType2'] =  X_test_full['BsmtFinType1'] + X_test_full['BsmtFinType2']\n# X_full.drop(['BsmtFinType1','BsmtFinType2'],axis=1,inplace=True)\n# X_test_full.drop(['BsmtFinType1','BsmtFinType2'],axis=1,inplace=True_HouseStyle\n\n\n# Generagte BldgType_HouseStyle\n# X_full['BldgType_HouseStyle'] = X_full['BldgType'] + '_' + X_full['HouseStyle']\n# X_test_full['BldgType_HouseStyle'] = X_test_full['BldgType'] + '_' + X_test_full['HouseStyle']\n# X_full.drop(['BldgType','HouseStyle'],axis=1,inplace=True)\n# X_test_full.drop(['BldgType','HouseStyle'],axis=1,inplace=True)\n\n# Merge PoolArea and PoolQC\n# X_full['PoolQC'] = X_full['PoolQC'].map({'Ex':3,'Gd':2,'Fa':1})\n# X_full['PoolQC'] = X_full['PoolQC'].fillna(0)\n# X_test_full['PoolQC'] = X_test_full['PoolQC'].map({'Ex':3,'Gd':2,'Fa':1})\n# X_test_full['PoolQC'] = X_test_full['PoolQC'].fillna(0)\n\n# X_full.drop(columns=['PoolArea', 'PoolQC'],axis=1,inplace=True)\n# X_test_full.drop(columns=['PoolArea', 'PoolQC'],axis=1,inplace=True)\n\n# # Calculate log of SalePrice\n# y = np.log(y)\n\n# Drop columns that have too many missing value\nX_full.drop(columns=['Alley','MiscFeature','PoolQC','PoolArea'],axis=1,inplace=True)\nX_test_full.drop(columns=['Alley','MiscFeature','PoolQC','PoolArea'],axis=1,inplace=True)\n","3d3e4ea3":"# pp.ProfileReport(pd.concat([y,X_full], axis=1))","87d7bc94":"# select caterical columns\ncategorical_cols = [cname for cname in X_full.columns if X_full[cname].dtype == \"object\"]\n# Select numerical columns\nnumerical_cols = [cname for cname in X_full.columns if X_full[cname].dtype in ['int64', 'float64']]\nX_full[numerical_cols].head(5)","e85869cf":"print(sorted({x:X_full[x].nunique() for x in categorical_cols}.items(), key=lambda x: x[1],reverse=True))\nX_full[categorical_cols].head(5)","358b61ae":"print(sorted({x:X_full[x].nunique() for x in numerical_cols}.items(), key=lambda x: x[1],reverse=True))\nprint(X_full['MSSubClass'].unique())\nX_full[numerical_cols].head(5)","ac1d7a67":"# Break off validation set from training data\n# X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n#                                                                 train_size=0.8, test_size=0.2,\n#                                                                 random_state=0)\n\ncategorical_small_variety_cols = [cname for cname in X_full.columns if\n                    X_full[cname].nunique() <= 15 and\n                    X_full[cname].dtype == \"object\"]\n\ncategorical_large_variety_cols = [cname for cname in X_full.columns if\n                    X_full[cname].nunique() > 15 and\n                    X_full[cname].dtype == \"object\"]\n\n# categorical_l_cols = [cname for cname in X_full.columns if\n#                     X_full[cname].nunique() > 10 and \n#                     X_full[cname].nunique() <= 15 and \n#                     X_full[cname].dtype == \"object\"]\ncategorical_label_cols = []\n\nprint('numerical_cols: ',numerical_cols)\nprint('categorical_cols: ',categorical_cols)\nprint('categorical_label_cols: ',categorical_label_cols )\nprint('categorical_small_variety_cols: ', categorical_small_variety_cols)\nprint('categorical_large_variety_cols: ',categorical_large_variety_cols)","4569a612":"from sklearn.pipeline import Pipeline, TransformerMixin\nfrom sklearn.neighbors import LocalOutlierFactor\n\nclass OutlierExtractor(TransformerMixin):\n    def __init__(self, **kwargs):\n        \"\"\"\n        Create a transformer to remove outliers. A threshold is set for selection\n        criteria, and further arguments are passed to the LocalOutlierFactor class\n\n        Keyword Args:\n            neg_conf_val (float): The threshold for excluding samples with a lower\n               negative outlier factor.\n\n        Returns:\n            object: to be used as a transformer method as part of Pipeline()\n        \"\"\"\n        try:\n            self.threshold = kwargs.pop('neg_conf_val')\n        except KeyError:\n            self.threshold = -10.0\n        pass\n        self.kwargs = kwargs\n\n    def transform(self, X):\n        \"\"\"\n        Uses LocalOutlierFactor class to subselect data based on some threshold\n\n        Returns:\n            ndarray: subsampled data\n\n        Notes:\n            X should be of shape (n_samples, n_features)\n        \"\"\"\n        x = np.asarray(X)\n        lcf = LocalOutlierFactor(**self.kwargs)\n        lcf.fit(X)\n        return x[lcf.negative_outlier_factor_ > self.threshold, :]\n\n    def fit(self, *args, **kwargs):\n        return self","65cf9055":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nimport category_encoders as ce\nfrom xgboost import XGBRegressor\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_num', SimpleImputer(strategy='median')),\n#     ('remove_outlier', OutlierExtractor())\n])\n\n# Preprocessing for categorical data\ncategorical_onehot_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_onehot', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\ncategorical_label_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_label', SimpleImputer(strategy='most_frequent')),\n    ('label', ce.OrdinalEncoder())\n    \n])\n\ncategorical_count_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_count', SimpleImputer(strategy='most_frequent')),\n    ('count', ce.TargetEncoder(handle_missing='count'))\n#     ('count', ce.CountEncoder(min_group_size = 1,handle_unknown=0,handle_missing='count'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(verbose=False,\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cox_box', PowerTransformer(method='yeo-johnson', standardize=False),skew_cols),\n        ('cat_label', categorical_label_transformer, categorical_label_cols),\n        ('cat_onehot', categorical_onehot_transformer, categorical_small_variety_cols),\n        ('cat_count', categorical_count_transformer, categorical_large_variety_cols),\n    ])\n\ntrain_pipeline = Pipeline(verbose=False,steps=[\n                    ('preprocessor', preprocessor),   \n                    ('scale', StandardScaler(with_mean=True,with_std=True)),\n                    ('model', XGBRegressor(random_state=0))\n                    ])","17620969":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.ensemble import RandomForestRegressor\nfrom IPython.display import display\n\n# Split dataset to train and test\nX_train, X_valid, y_train, y_valid = train_test_split(X_full[numerical_cols], y,train_size=0.8, test_size=0.2,random_state=0)\n\n# Define pipeline to do transformation\ntransform_pipeline = Pipeline(verbose=False,steps=[\n                    ('imputer_num', SimpleImputer(strategy='median')),\n                    ('scale', StandardScaler(with_mean=True,with_std=True)),\n                    ])\n\n# Transform data\ntransform_pipeline.fit(X_train,y_train)\npi_X_train = pd.DataFrame(transform_pipeline.transform(X_train))\npi_X_valid = pd.DataFrame(transform_pipeline.transform(X_valid))\npi_X_train.columns = X_train.columns\npi_X_valid.columns = X_valid.columns\n\n# Define a model and calculate permutation importance of all numeric columns\npi_model = RandomForestRegressor(n_estimators=700,max_depth=4,random_state=0)\npi_model.fit(pi_X_train,y_train)\nperm = PermutationImportance(pi_model, random_state=1).fit(pi_X_valid, y_valid)\neli5.show_weights(perm, feature_names = pi_X_valid.columns.tolist(),top=100)","99719b2b":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'model__nthread':[2], #when use hyperthread, xgboost may become slower\n              'model__learning_rate': [0.04, 0.05], #so called `eta` value\n              'model__max_depth': range(3,5,1),\n#               'model__importance_type': ['weight', 'gain', 'cover'],\n#               \"model__min_child_weight\" : [ 1 ],\n#               \"model__gamma\"            : [ 0.0],\n              \"model__colsample_bytree\" : [ 0.2 ],\n              'model__silent': [1],\n              'model__n_estimators': [700], #number of trees\n#               'model__n_estimators': range(595,600,1), #number of trees\n#               'model__n_estimators': range(550,1000,5), #number of trees\n             }\nsearched_model = GridSearchCV(estimator=train_pipeline,param_grid = param_grid, scoring=\"neg_mean_absolute_error\", cv=5, error_score='raise', verbose = 1)\nsearched_model.fit(X_full,y)\n\nprint(searched_model.best_estimator_)\nprint(searched_model.best_score_)\n","ae0ca28c":"preds_test = searched_model.predict(X_test_full)\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test_full.index,'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\noutput","f24fd33d":"# 1. Read train and test data\n- Read train and test data which is store in csv to pandas dataframe.\n- Create X and y as input for the model.","10514d6c":"# 6. Create pipeline","e508814c":"# 8. Train the model\n- Use XGBoost to train to model\n- Using GridSearchCV to search for best hyper parameter of XGBoost\n\nPS: You can add more parameters. I keep small range of params to make it run faster in Kaggle.","cc4026df":"# 5. Choose type for each feature\n- All columns with dtype int64, float64 will be numeric feature and others will be categories\n- Seperate categories into large variety and small variety","ad385971":"# 3. Change some numeric columns to categories since it looks like categories\n- MSSubClass: The building class. This must be category, not numeric although it is storing as numeric\n- MoSold: Month sold. Thinking about this: is it true that house price in Feb is double of Jan and price of Mar is triple of Jan? If it is not then we must change it to categories and must use OneHot Encoding to transform it.","32f7ffec":"# 2. Explore data\n- Explore missing data using missingno package\n- Use pandas_profiling package to have an overview of data. Howevers, since it run a little slow, I comment it. If you need to rerun, uncomment and run it.\n- Explore in detail some features that we can do feature engineering on them.","a5ea845d":"# 7. Calculate importance of each feature to iterate feature engineering\n- Use Permutation Importance to calculate how important of each feature and use it in the iteration of feature engineering","46cd9a3b":"# 4. Do feature engineering\n**This step is an iteration. After calculate permuation importance, we try to explore new feature. The iteration end when we satisfy with the score :)**\n- Exterior2nd is depends on Exterior1st so we merge Exterior1st and Exterior2nd to Exterior.\n- Condition2 is depends on Condition1 so we merge Condition1 and Condition2 to Condition.\n- Explore new feature which is importance for the model:\n  + TotalSF: Total square feet of the whole house\n  + TotalBathroom: Total bathroom of the whole house (include full bathroomo and half bathroom).\n","7c57fb05":"# 9. Predict using the best model\n- Predict the model with best hyper parameter\n- Create result to submit to leaderboard","b3d4fa2c":"![banner](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png)\n# Preface\nThis kernel aims to show how I solve the \"House price competition\" and move to top 3% on the leaderboard. For each step, I comment out in detail why and how to do. If you need more explainations, please feel free to comment below and I will soon response. If you see that is kernel is helpful, please help me upvote it and share it with your friend (if they need). Thanks in advance!"}}