{"cell_type":{"97a0d94e":"code","b379e412":"code","81930cef":"code","1dc6a68a":"code","a0e8069a":"code","b7e39ec5":"code","b2c3a8ce":"code","43ed277b":"code","f7c68824":"code","8080dc4c":"code","8efb600b":"code","6e3ca650":"code","27eabdd7":"code","4cc319f8":"code","870c4b0c":"code","e4a9d611":"code","72a37d99":"code","b41fbc45":"code","1fedb9ad":"code","4f1bd42d":"code","c75be0ac":"code","d9548ae0":"code","600de1fb":"code","def6d32c":"code","0dd075f7":"code","59bccf0b":"code","86638510":"code","b4269e37":"code","a3eaf82b":"code","fff6d728":"code","6851b7f0":"code","728d3cbc":"code","01692958":"code","262a5db2":"code","73ceffd0":"code","0fe07185":"code","d9688afe":"code","6486fdaa":"code","782e3564":"code","e2cdb9b1":"code","41e23e04":"code","5458cfac":"code","8f65ad96":"markdown","aee99b1b":"markdown","9adba364":"markdown","eb93cf18":"markdown","1b6fb491":"markdown","27522560":"markdown"},"source":{"97a0d94e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom pathlib import Path\n\nfrom fastai import *\nfrom fastai.tabular import *\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom tqdm import tqdm","b379e412":"import numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA","81930cef":"path=Path('..\/input')\nfe_path=path\/'feature-engineering-msmw'\ncomp_path=path\/'microsoft-malware-prediction'\nhp_path=path\/'lgbm-hyperopt'","1dc6a68a":"cats=np.load(fe_path\/'categories.npy').item()\nmeans,stds=np.load(fe_path\/'means.npy').item(),np.load(fe_path\/'stds.npy').item()","a0e8069a":"index='MachineIdentifier'\ndep_var='HasDetections'","b7e39ec5":"cont_names=list(means.keys())\ncat_names=list(cats.keys())","b2c3a8ce":"feature_imp=pd.read_pickle(hp_path\/'feature_importance')","43ed277b":"idx=np.argsort(feature_imp.mean(axis=1))","f7c68824":"frac=0.01\nfrac_important=np.cumsum(feature_imp.mean(axis=1)[idx])\/np.max(np.sum(feature_imp.mean(axis=1)[idx]))\nidx_important=np.where(frac_important>frac)[0]","8080dc4c":"feature_cols=feature_imp.iloc[idx[idx_important]].index\ncat_names=[cat for cat in cat_names if cat in feature_cols]\ncont_names=[cont for cont in cont_names if cont in feature_cols]\n\nfeatures=[index]+cat_names+cont_names+[dep_var]","8efb600b":"import matplotlib.pyplot as plt\nplt.figure(figsize=(16,16))\np1 = plt.barh( idx_important,feature_imp.mean(axis=1)[idx[idx_important]], xerr=feature_imp.std(axis=1)[idx[idx_important]],orientation ='horizontal')\nplt.yticks(idx_important, feature_imp.mean(axis=1).index[idx[idx_important]]);\nplt.xscale('log')","6e3ca650":"df_trn=pd.read_hdf(fe_path\/'train.h5',columns=features)","27eabdd7":"df_trn.set_index('MachineIdentifier',inplace=True)","4cc319f8":"#df_trn=df_trn.iloc[:int(0.2*len(df_trn))]","870c4b0c":"N_trn=int(0.9*len(df_trn))\nvalid_idx =range(N_trn,len(df_trn))# np.argsort(df_trn.OSVersion_Elapsed)[N_trn:]\nbs=1024","e4a9d611":"data = TabularDataBunch.from_df('.', df_trn, dep_var, valid_idx=valid_idx, cat_names=cat_names,bs=bs)#,test_df=df_test)","72a37d99":"model = TabularModel(data.get_emb_szs(), n_cont=len(data.cont_names), out_sz=2, layers=[2000,4000,1000], ps=None, emb_drop=0.5,\n                         y_range=[0,1], use_bn=True)","b41fbc45":"def auc_score(y_pred,y_true,tens=True):\n    score=roc_auc_score(y_true,torch.sigmoid(y_pred)[:,1])\n    if tens:\n        score=tensor(score)\n    else:\n        score=score\n    return score","1fedb9ad":"data.show_batch()","4f1bd42d":"learn=Learner(data, model,metrics=[auc_score,accuracy])","c75be0ac":"learn.lr_find(num_it=1000)\nlearn.recorder.plot()","d9548ae0":"learn.save('untrained')","600de1fb":"learn.fit_one_cycle(10, 1e-2)","def6d32c":"learn.save('embeddings-stage-1')","0dd075f7":"learn.recorder.plot_losses()","59bccf0b":"learn.recorder.plot_metrics()","86638510":"plt.plot(learn.recorder.losses)","b4269e37":"plt.plot(learn.recorder.lrs)","a3eaf82b":"(x_cat,x_cont),y=data.one_batch(detach=False)\n\nimport matplotlib.gridspec as gridspec\n\nn_plot=int(np.sqrt(len(cat_names)))-1\nlearn.load('untrained')\nplt.figure(figsize=(16,16))\ngs = gridspec.GridSpec(n_plot,n_plot)\nfor i in range(n_plot):\n    for j in range(n_plot):\n        ax = plt.subplot(gs[i,j])\n        x_embed=to_np(learn.model.embeds[j+i*n_plot](x_cat[:,j+i*n_plot]))\n        X_embedded = PCA(n_components=2).fit_transform(x_embed)\n        color=to_np(x_cat[:,j+i*n_plot])\n        color=color\/np.max(color)\n        ax.scatter(X_embedded[:,0],X_embedded[:,1],c=color)\n        ax.set_title(cat_names[j+i*n_plot])","fff6d728":"(x_cat,x_cont),y=data.one_batch(detach=False)\n\nimport matplotlib.gridspec as gridspec\n\nn_plot=int(np.sqrt(len(cat_names)))-1\nlearn.load('embeddings-stage-1')\n\nplt.figure(figsize=(16,16))\ngs = gridspec.GridSpec(n_plot,n_plot)\nfor i in range(n_plot):\n    for j in range(n_plot):\n        ax = plt.subplot(gs[i,j])\n        x_embed=to_np(learn.model.embeds[j+i*n_plot](x_cat[:,j+i*n_plot]))\n        X_embedded = PCA(n_components=2).fit_transform(x_embed)\n        color=to_np(x_cat[:,j+i*n_plot])\n        color=color\/np.max(color)\n        ax.scatter(X_embedded[:,0],X_embedded[:,1],c=color)\n        ax.set_title(cat_names[j+i*n_plot])","6851b7f0":"y_pred,y_true=learn.get_preds()","728d3cbc":"scr=to_np(auc_score(y_pred,y_true))\n","01692958":"f\"{scr:0.3}\"","262a5db2":"learn.show_results(rows=20)","73ceffd0":"df_sub=pd.read_csv(comp_path\/'sample_submission.csv').set_index('MachineIdentifier')","0fe07185":"features.remove(dep_var)","d9688afe":"chk_size=1000*1024\ndf_test_iter=pd.read_hdf(fe_path\/'test.h5',iterator=True, chunksize=chk_size, columns=features)","6486fdaa":"for df_test in tqdm(df_test_iter):\n    df_test.set_index('MachineIdentifier',inplace=True)\n    for col in cat_names:\n        df_test[col]=df_test[col].cat.codes\n    x_cat_np=df_test.loc[:,cat_names].values.astype(np.int64)+1\n    x_cont_np=df_test.loc[:,cont_names].values.astype(np.float32)\n    for idxs in np.array_split(range(df_test.shape[0]),chk_size\/\/bs):\n        x_cat=to_device(tensor(x_cat_np[idxs,:]),'cuda')\n        x_cont=to_device(tensor(x_cont_np[idxs,:]),'cuda')\n        pred=learn.model(x_cat,x_cont)\n        df_sub.loc[df_test.index[idxs],'HasDetections']=to_np(pred)[:,1]","782e3564":"import seaborn as sns","e2cdb9b1":"sns.distplot(df_sub)","41e23e04":"df_sub.head()","5458cfac":"df_sub.to_csv('submission')","8f65ad96":"Validation set is therefore about 10% of the whole dataset","aee99b1b":"# [Tabular learning](https:\/\/docs.fast.ai\/tabular.html) \nI wanted to go a slightly different route from the LGBM that seems to be favorite here.\nI don't expect too great results but maybe I can average with LGBM results later to get a few precent more out of my other models.\n## Deep in this case means embeddings+2 layers\nNot very deep I know I know, but lets see maybe Ill add some if that increases performance.\nThe Idea is to use the continuous variables as they are. \nThe categorical variables are in a first layer transformed in a lower dimensional space via embeddings with dropout.\nThe embeddings also use droput and the continuous variables batchnorm.\nAfter that blocks of BatchNorm, Dropout, Linear and ReLU.\nThe Embedding sizes I chose to sqrt(n_cat) or a maximum of 100.\n\nThe preprocessing is done in a different kernel. Just standard stuff and I added a column for the date that can be decoded from OsBuildLab.\nI plan to add new features when I set the pipeline for my lgbm and this model up.\nIn this kernel, I still need to add the prediction for the test case so I can submit this.\n\nI would additionally like to explore the embeddings a bit more since one can make pretty plots of the relationship of for instance dates of weekdays vs weekends, or gaming vs non gaming.\n","9adba364":"Pretty stupid way of defining the metric but I couldn't make it work otherwise to work with the fastai library","eb93cf18":"Based on the categorical and continuous variables a model of a certain size is created. Although its a 0 or 1 case out_sz has to be 2. ","1b6fb491":"Now combining the data and model with a learner object.\nThis one defines metric, loss function, a learning rate scheduler and handles all details about model saving including handling the predictions on a specific dataset (for now validation but I'll add test set soon)","27522560":"This part creates the dataset as well as a fancy pytorch dataloaderfor training and validation set."}}