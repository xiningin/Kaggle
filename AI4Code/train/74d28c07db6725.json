{"cell_type":{"70d36a5e":"code","0325cecf":"code","9b53fbe8":"code","6cc33e3e":"code","29dbee24":"code","e192be0a":"code","fae43284":"code","0e4f294f":"code","34cd7b34":"code","e1d9c515":"code","4a59fc64":"code","3d4850b8":"code","d94d8dc6":"code","52cd1f6d":"code","293c6ff5":"code","48b971d5":"code","ca551419":"code","a37f5708":"code","5d328723":"code","e79cadaa":"code","43d0cd69":"code","0d5f1e32":"code","6e329e80":"code","4c238e0e":"code","2de0a067":"code","f1687db6":"code","bf112967":"code","ee53a842":"code","de62892c":"code","3d9cd320":"code","d23c97f6":"code","defa2154":"code","8cc67d5d":"code","1d927796":"code","fba06732":"code","96a8b491":"code","209af354":"code","d16ea758":"code","3636834d":"markdown","864e58e9":"markdown","fb5043b0":"markdown","30f55532":"markdown","3f560eaa":"markdown","4c5adc65":"markdown","036832e6":"markdown","cc435eb8":"markdown","9fc3df14":"markdown","c64e017b":"markdown","bde75c1d":"markdown","5f3e89f1":"markdown","0c92eef8":"markdown","378e18e2":"markdown","65684794":"markdown","a73935f8":"markdown","f4176abb":"markdown","db7eeea1":"markdown","d791b7c7":"markdown","4c68c415":"markdown","39b0d9b3":"markdown","3e068b49":"markdown","83e9815d":"markdown","7a4b7722":"markdown","0b66900c":"markdown"},"source":{"70d36a5e":"# Main libraries usage\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Sklearn libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import  OneHotEncoder, MinMaxScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import  DecisionTreeRegressor\nfrom sklearn.metrics import  mean_squared_error\nfrom sklearn.compose import  ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import  Pipeline\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.base import BaseEstimator, TransformerMixin\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0325cecf":"# Main functions\n\n\ndef stratified_splitting(data, split_attr):\n    \"\"\"\n    The function to split data based on specific attribute \n    to ensure that we include all the categories of the data in our test set.\n    \"\"\"\n    split = StratifiedShuffleSplit(n_splits=1, test_size=.2, random_state=42)\n    for tr_index, tes_index in split.split(data, data[split_attr]):\n        tr_stratified_set = data.loc[tr_index]\n        tes_stratified_set = data.loc[tes_index]\n    return tr_stratified_set, tes_stratified_set \n\n\ndef compare_random_stratified_split(data, random_set, stratified_set, split_attr):\n\toverall          = data[split_attr].value_counts() \/ len(data)\n\trandom_split     = random_set[split_attr].value_counts() \/ len(random_set)\n\tstratified_split = stratified_set[split_attr].value_counts() \/ len(stratified_set)\n\n\t# Sort values\n\toverall.sort_values(ascending=False)\n\trandom_split.sort_values(ascending=False)\n\tstratified_split.sort_values(ascending=False)\n\n\toverall_Vs_random_error     = np.abs(overall - random_split)\n\toverall_Vs_stratified_error = np.abs(overall - stratified_split)\n\n\t# As it all numpy array I will transform to pandas dataframe\n\tdict_result = {\"overall\": overall, \"random_split\": random_split, \"stratified_split\": stratified_split,\n              \"overall_Vs_random_error\": overall_Vs_random_error,\n               \"overall_Vs_stratified_error\": overall_Vs_stratified_error}\n\terror_result = pd.DataFrame(dict_result, columns=dict_result.keys())\n\treturn error_result\n\n\ndef visualize_plot(data,plot_kind, x_axis, y_axis, point_size=50, color=\"gray\", colorbar=False):\n\tplt.figure(figsize=(10,10))\n\tplt.style.use('fivethirtyeight')\n\tdata.plot(kind=plot_kind, x=x_axis, y=y_axis, s=point_size, \n\t\tc=color, cmap=plt.get_cmap(\"jet\"), colorbar=colorbar)\n\n\treturn True\n\ndef split_seprate(data):\n\thappiness_report_num = data.drop(['Happiness Score', 'Region', 'Country'], axis=1)\n\thappiness_report_cat = data[['Region']]\n\thappiness_report_labels = data[['Happiness Score']]\n\n\treturn happiness_report_num, happiness_report_cat, happiness_report_labels\n\n\ndef prepare_data(data, target='Happiness Score', number_of_instances=10):\n\n\tsome_data = data.iloc[:number_of_instances].copy()\n\tsome_data_labeled = some_data[[target]]\n\tsome_data = some_data.drop(target, axis=1)\n\n\treturn some_data,  some_data_labeled\n\n\n\nclass AddCombinedAttributes(BaseEstimator, TransformerMixin):\n\n\tdef __init__(self, add_GDP_for_family = True):\n\t\tself.add_GDP_for_family = add_GDP_for_family\n\tdef fit(self, X, y=None):\n\t\treturn self\n\tdef transform(self, X):\n\t\tif self.add_GDP_for_family:\n\t\t\tGDP_for_family = X[:, 2] \/ (X[:, 3]+1)\n\t\t\tX = np.c_[X, GDP_for_family]\n\t\treturn X","9b53fbe8":"world_happiness_report = pd.read_csv(\"..\/input\/world-happiness\/2015.csv\")\nworld_happiness_report.head()","6cc33e3e":"world_happiness_report.info()","29dbee24":"world_happiness_report.describe()","e192be0a":"world_happiness_report['Country'].value_counts()[:10]","fae43284":"world_happiness_report['Region'].value_counts()","0e4f294f":"world_happiness_report['Happiness Rank'].value_counts()","34cd7b34":"world_happiness_report.hist(bins=50, figsize=(15, 12))","e1d9c515":"world_happiness_report['Region'].value_counts()","4a59fc64":"# test_size the size of testing set\n# random_state to generate the same splitting each time run the code\nrandom_train_set, random_test_set = train_test_split(world_happiness_report, test_size=.2, random_state=42)","3d4850b8":"# display number of instances per set\nprint(len(random_train_set), len(random_test_set))","d94d8dc6":"# this stratified_splitting function from configs file \ntr_stratified_set, tes_stratified_set = stratified_splitting(world_happiness_report, \"Region\")\n# As we can see same number of instance per set\nprint(len(tr_stratified_set), len(tes_stratified_set))","52cd1f6d":"\n# this compare_random_stratified_split function from main functions cell\nerror_result = compare_random_stratified_split(world_happiness_report, random_test_set, \n                                              tes_stratified_set, \"Region\")\nerror_result.fillna(0, inplace=True)\nerror_result","293c6ff5":"happiness_report = tr_stratified_set.copy()\nhappiness_report.head()","48b971d5":"_ = visualize_plot(happiness_report, \"scatter\", \"Health (Life Expectancy)\", \"Economy (GDP per Capita)\",\n                  50, \"Happiness Score\", True)","ca551419":"correlation_metrix = happiness_report.corr()\ncorrelation_metrix['Happiness Score'].sort_values(ascending=False)","a37f5708":"print(happiness_report.columns)\nattributes = [\"Happiness Score\", \"Happiness Rank\", \"Health (Life Expectancy)\", \"Economy (GDP per Capita)\"]\n\n\nplt.style.use('default')\npd.plotting.scatter_matrix(happiness_report[attributes], figsize=(16,10))","5d328723":"happiness_report['GDP_for_family'] =  happiness_report['Economy (GDP per Capita)'] \/ happiness_report['Family']\n\n# Now lets looking in the correlation again\nhappiness_corr = happiness_report.corr()\nhappiness_corr['Happiness Score'].sort_values(ascending=False)","e79cadaa":"happiness_report = tr_stratified_set.copy()\nhappiness_report_num, happiness_report_cat, happiness_report_labels = split_seprate(happiness_report)","43d0cd69":"imputer = SimpleImputer(strategy='mean')\nimputer.fit(happiness_report_num.values)\nimputer.statistics_","0d5f1e32":"X_training = imputer.transform(happiness_report_num)\n\nX_training.shape # numpy array","6e329e80":"# Feature Scaling\nmin_max_scaling = MinMaxScaler()\nmin_max_scaling.fit(X_training)\nX_training = min_max_scaling.transform(happiness_report_num)","4c238e0e":"\n# Return to pandas data frame with imputed data\n\nhappiness_report_num = pd.DataFrame(X_training, columns=happiness_report_num.columns,\n                                   index=happiness_report_num.index)\nhappiness_report_num.head()","2de0a067":"# happiness_report_cat = pd.DataFrame(happiness_report_cat, columns=['Region'])\none_hot_encode = OneHotEncoder()\nhappiness_report_cat_1hot = one_hot_encode.fit_transform(happiness_report_cat)\nprint(happiness_report_cat_1hot.shape)\nprint(one_hot_encode.categories_)\nhappiness_report_cat_1hot","f1687db6":"happiness_report_cat_1hot.toarray()","bf112967":"print(happiness_report_num.values.shape)\nattr_adder = AddCombinedAttributes()\nadd_extra_attr = attr_adder.transform(happiness_report_num.values)\nadd_extra_attr.shape","ee53a842":"happiness_report = tr_stratified_set.copy()\nhappiness_report = happiness_report.drop('Happiness Score' , axis=1)\nnum_attr_names = happiness_report_num.columns\ncat_attr_name = ['Region']","de62892c":"numeric_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('attr_combination', AddCombinedAttributes()),\n    ('min_max_scaler', MinMaxScaler()),\n])","3d9cd320":"full_pipeline = ColumnTransformer([\n    (\"num_pipeline\", numeric_pipeline,  num_attr_names),\n    (\"cat_pipeline\", OneHotEncoder(),  cat_attr_name)\n])\n\nhappiness_report_prepared = full_pipeline.fit_transform(happiness_report)","d23c97f6":"happiness_report_prepared.shape","defa2154":"lin_reg = LinearRegression()\nlin_reg.fit(happiness_report_prepared, happiness_report_labels)\n\n# Now let check result on some data points, but first pass data to the pipeline\nsome_data,  some_data_labeled = prepare_data(tr_stratified_set)\nsome_data_prepared = full_pipeline.transform(some_data)\npredict_some_data = lin_reg.predict(some_data_prepared)\n\nprint(\"Predict Values\\n\", predict_some_data)\n\nprint(\"Actual\", some_data_labeled)","8cc67d5d":"happiness_report_predict = lin_reg.predict(happiness_report_prepared)\nlin_mse = mean_squared_error(happiness_report_predict, happiness_report_labels)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","1d927796":"# Decision Tree\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(happiness_report_prepared, happiness_report_labels)\nhappiness_report_predict = tree_reg.predict(happiness_report_prepared)\ntree_mse = mean_squared_error(happiness_report_predict, happiness_report_labels)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","fba06732":"scores = cross_val_score(tree_reg, happiness_report_prepared, happiness_report_labels,\n                        scoring=\"neg_mean_squared_error\", cv=3)\ntree_rmse_scores = np.sqrt(-scores)\ntree_rmse_scores","96a8b491":"scores = cross_val_score(lin_reg, happiness_report_prepared, happiness_report_labels,\n                        scoring=\"neg_mean_squared_error\", cv=3)\ntree_rmse_scores = np.sqrt(-scores)\ntree_rmse_scores","209af354":"test_data,  test_labeled = prepare_data(tes_stratified_set)\ntest_data_prepared = full_pipeline.transform(test_data)\npredict_test_data = lin_reg.predict(test_data_prepared)\nlin_mse = mean_squared_error(predict_test_data, test_labeled)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","d16ea758":"# Note !\n\nAs we can see we have a small error on the test set, and this can lead us to launch model to work on our system.","3636834d":"## Some Assumption\n\nIt more powerful when you trying to get insights from the data is to make some assumption which can lead you to good result at the end.\n\nAs we can in first graph how the **Happiness Score** going on to increase with the strong positive correlation between **Economy (GDP per Capita)** and ** Health (Life Expectancy)**.\n\nAlso, may be the **Freedom** has its relation with the **Dystopia Residual)**, as when the Dystopia Residual decrease it means that the life free is going to be high because all of us trying to life in Utopia, and the **Dystopia Residual** is represent the Dystopia Happiness, so it increase with the freedom.\n\n\nAlso, as there are two similr attribute which are **Happiness Rank** and **Happiness Score**, dicover these two attributes may be also help us to get more insights, and as we can see in the graph below there is a strong negative correlation.\n","864e58e9":"## Create a Test set\n\nAfter what we have discussed and present we need to go deeper in our discovery process and visualization, but first, we need to set aside part of the data for testing and use the rest of the data to discover and go deeper with the analysis.\n\nWe will split the data using two method, then comapre this pliting to the whole data, and check how the we consider all samples in the testing as well as in training.\n\nThe Region attribute can help us in splitting data, to consider each group are provided.","fb5043b0":"## Discover and visualize to get insights\n\nThe histogram is helpful, but as we need to get more insights we need to go deeper, and the visualization helps us to get more insights since the brain is very weel capture information from images and graphs, also instead of representing each attribute on its own, now the time to discover the relation ship between attributes, or some attributes with the target attribute.\n\nNow we have a train and test set so let us keep the test set aside and not touched till we decide to launch the model, also take a copy of the traing set to go in depth of discovering and keep the original data.","30f55532":"## Note !\n\nThe new ones **GDP_for_family** has a positive correlation of *.35* with **Happiness Score**, so we can train the model with and also without.\n\n# Prepare the Data for Machine Learning Algorithms\n\nWe have passed through different ways of discovery and analysis to get insights about the data we dealing with and what are the most representative attributes of our data, also we have added a new attribute to look at in the training of our model.\n\nFrom this step we need to make our work as automated as we can because it will not work just with the data we have, it will work with the test set we keep aside and for the new data when the system goes a life, also it may be used for other similar dataset or some of the functions can be used.\n\n# Notes !\n\nFirst, we need to split the learning attributes and target variable.\n\nSecond, separate the categories attributes from the attributes of the numbers.\n","3f560eaa":"# Numeric Attributes handling\n\nWe do not have any missing values in our training, it (126) instance for all attributes, but the thing does not go like this, the test set may include some missing values or even when system goes live it may return some missing values for some attributes so we need to handle like this case and save the value we will replace with once we have missing value.\n\nAlso because some attributes have different ranges like **Happiness Rank**, from most of the other ones, and because of some models working well with data in a specific range, we will apply MaxMin features scaling to restrict that all values are between range 0 and 1.\n\nSome of steps we can do with missing values:\n\n- Remove the attribute itself\n- Remove corresponding rows (instances)\n- Replace the missing value with the mean, standard deviation, or with 0 value as the case required","4c5adc65":"# Attribute Combination class\n\nWe have discussed earlier about attribute combination, but here we need to make this process automated, and also some attributes maybe useful to remove from the dataset like **Standard Error **, it less effective the target variable as we see.\n\nBut we need to make the process alongside sklearn functionality.","036832e6":"# Introduction\n\n\n\nThe World Happiness Report is a landmark survey of the state of global happiness.\n\nThe data trying to analyze which country has higher happiness or life satisfaction and which of them have to be like Dystopia with the world\u2019s lowest incomes, lowest life expectancy, lowest generosity, most corruption, least freedom, and least social support.\n\nOther attributes are provided for the dataset to give us other insights and we will see the effect of each of these attributes.","cc435eb8":"# sparse matrix\n\nBecause most of this matrix are 0, sklearn keep just the location of **nonZeros** to save your memory, and to back to the numpy array just us **toarray** method assocated with the object.","9fc3df14":"# Important Note !\n\nEven of the big difference error between random and stratified Vs overall based on the **Region** attributes, it help you to consider that we should collect more instances for North America and Australia and New Zealand, as they just 2  instances.","c64e017b":"# Note !\n\nAs we can see we have a small error on the test set, and this can lead us to launch model to work on our system.","bde75c1d":"# Note !\n\nWe have 9 numeric attributes and with AddCombinedAttributes it will be 10, and also because we have categorized attributes with 10 discrete values, then it matrix of 10*number of instances, then the ColumnTransformer combine the output from each pipeline and return one matrix.","5f3e89f1":"## Discover some attributes unique values\n\nBecause some attributes are categorical and the other ones may be categorical also even displaying as numbers like **Happiness Rank**, so we need to know the unique values and for each of these unique what is the frequent numbers, and actually this can help us in the stage of splitting data.  \n\nAlso, some attributes may be better to remove from the dataset because it can cause misleading learning, like **Country**, it just displaying names and no information it holds.\n\nAlso like **Region** may hold some information about the life satisfaction from those who live in Africa from those in Europe so we can trying attribute combination, like remove it and add it to check its effect on learning.\n\nAlso like **Happiness Rank**, is going from very different values so, it will be good to consider as a number attribute.","0c92eef8":"## Note!!\n\n**As we can see the strong relationship between the target and the other two ones, while these two and the target has a strong negative correlation with the *Happiness Score*.**\n\n## Attribute Combination\n\nIt worse trying to add a relationship between two attributes to extract new ones, but even if that multiple attributes have strong positive or negative relation but its not reflect any information to extract new ones from.\n\nMaybe there is a one I will try to add which is related to **Economy (GDP per Capita)' Vs 'Family'**.\n\n","378e18e2":"# Important Note !\n\nThe **Region** can help us during the stage of splitting data to train and test to consider all categories are provided inside the train and test with close ratio.","65684794":"## Some statistics about numbers attributes\n\nSince most of the columns (attributes or features) are numbers it will be helpful to display some of statics related to these attributes, like what the mean, median, what the geatest values and lowest or what most values less than 25% or 50% another name is (first quarter range and third quarter range).\n\nInsights you can get from this,\n- maybe you need to apply feature scaling because different ranges for each attribute.\n- maybe some numbers attribute like categorical because of small discrete values.\n\n\nWe can see that most attributes values are not far away from mean in the standard deviation but maybe because these attributes are just small numbers like 25% of **Happieness Rank** less than 40.","a73935f8":"# Better Evaluation\n\nThere is no big difference between the two models in error, but it seems that we have overfitting the dataset very well, actually, this may back to the small number of instances we have but also we have to make our evaluation better than we have.\n\n## Cross Validation\n\nits a helpful method that helps us to train on part of the training set and evaluate our result on another part from also training, not just that it helps you to make different evaluation, train the same models across different iteration for each time pick a part for training and part for evaluation, then the same process pick another part for training and another for testing, ending by that you have trained on the whole training and also evaluate on the whole training set.","f4176abb":"## Scatter Matrix\n\nEven of graphs above give us a lot of insight we need more insights, but it not helpfull to graphs each of these attributes agnist each other it consume a lot of time and also we need one graph represent all in 1, but to represent each attribute against each other it means that, we have 10 number attributes it will produce 10*10 graph, so instead we display the most effected ones like what we see in the above numbers.","db7eeea1":"# Select & Train the model\n\nThings now are simple than we may assume, we have to go through the dataset from a different perspective like in correlation, different graph representation attributes combination and other, its time to check how the model will make on the training data.\n\nLet's trying a Linear Regression model since we are dealing with continuous variable.","d791b7c7":"# Categorize attributes\n\njust one attribute is categorized as the **Region** attribute and it may reflect some information not like country because it just 1 country for each instance, but the **Region** is 10 region overall which can reflect those who live in some region have happiness score large than others in another region.\n\nBut not this the point, most of the models accept only numbers so we need to transfer this attribute to the number that the model can deal with, and there are different ways to transform, and this based on the kine of categorized attribute you have, is it ordinal or random one that the arrangements may cause misleading to the model.\n\nThe **Region** is not ordinal so will use the **One Hot** method, also there is another way we can deal with embedding.","4c68c415":"\n# Display first 5 rows\n\nThe first step is to represent some rows from the data just to know what attributes look like then move on to discover more about them.\n","39b0d9b3":"# Summary\n\nAs we can see it seems that we have better result of the two models, but LinearRegression looks better than DecisionTreeRegressor and the assumption we said about overfitting seems to be not here, as the result is close to each other from training to validation, but one last thing to decide that is what we have kept aside which is the **Test Set**.","3e068b49":"\n# Some information about the data and attributes\n\nWe can see that all of the attributes have no NAN values, even of that, we need to get a specific value for each attribute, because when the system go-live, maybe some of the value of the attribute will not be provided, in the case like this you have to apply the saved value for this attribute.\n\nWe also can notice that most of the attributes are numbers except the country and region.\n\nTwo attributes that are close to each other are the Happiness Rank and Happiness score, so we will use the score as your target variable since we trying to learn about regression.\n\nAlso, the memory usage is small enough to fit our memory.\n","83e9815d":"## Histogram\n\nA simple graph that you can use is the histogram to display different ranges of data with their frequency, it will help you to understand more about the data you dealing with.\n\nWe can apply it to the whole data or just for some attributes we interest in.\n\nThe histogram helps us to discover like normal distribution of each attribute, or those who are skewed to right or left, which can help us know which attributes have outliers, which may cause misleading in learning and also may these outliers need to discover of this attribute have its effect on the target.\n\nAll of these steps help us to get insights and intuition about the data we dealing with.\n\nAs we can see from graphs below:\n\n- Economy (GDP per Capita) range from .0 to 1.5.\n\nYou can ask the business owner about these values because there is no income for some people like 1.5 and so on, so find which numbers you multiply by is helpful to know the actual values like should we multiply by 10,000 to back the original values.\n\nEven of machine learning model work best with small ranges of numbers, but we need to get a whole overview about the data and its attributes.\n\n- Happiness Score, it from 2 to 7.\n\nAlso, we can ask about these values and what about the prediction when the system goes live maybe we go beyond these values or should we consider 7 is the maximum, like these question help us to understand in depth about the data we dealing with.","7a4b7722":"# PipeLine\n\nWe have moved through different stages and for each one, we have explained why and trying first to make the process, for now, we have prepared most of the things but again it will be helpful to go through again these steps but in the simple pipeline that works for **numbers** and **categories** separately.\n\n## Numeric Pipeline\n\nWe have apply these stages for numeric attributes:\n- MinMaxScaler\n- SimpleImputer\n- CombinedAttributes\n\n## Categorize Pipeline\n\nJust we have made one function to convert the category to a number\n\n## Compine the Two pipelines\n\nAs we have two pipelines for numbers and categories we will introduce one pipeline to combine them.","0b66900c":"## Correlation\n\nAs we get a lot of insights from which attributes have its own linearity of effect the target variables, from strong positive correlation to strong negative correlation. But more insights is to get numbers that represent the correlation between each of these attributes with the target.\n\nAnd as we can see that GDP per Capita as well as Life Expectancy are most effect the Happiness Score, and how far away Happiness Rank is strong negative as it, and others like **Standard Error** may be need to removed as it have a small negative affect on target so we can trying to train the model with and without."}}