{"cell_type":{"76c50689":"code","260adf4a":"code","21c89429":"code","aee60596":"code","55c4a6aa":"code","37206133":"code","210b8308":"code","03dada38":"code","d8cc74aa":"code","a1a921a5":"code","4349cc9e":"code","c09ac83a":"code","688c732c":"code","26e18d90":"code","2e047c6d":"code","7ff285db":"code","4f41dec6":"code","c1bdff1e":"code","660898c5":"code","d2b8c9e9":"code","2eb2fd2d":"code","23bbae51":"code","ecfa7c80":"code","132dd9ef":"code","8c4740fe":"code","e13e394f":"code","3bb299f9":"code","9e216107":"code","19894537":"code","d8df352a":"code","a79437e7":"code","76625a24":"code","07f93652":"code","7d84d6c8":"code","39b5bc0e":"code","10e7f9c6":"markdown","14a1af94":"markdown","1ef60fa1":"markdown","a8a50d8c":"markdown","e540fb6f":"markdown","e7368a8a":"markdown","280b7f30":"markdown","ba8a74b9":"markdown","6f7f1044":"markdown","7b16b9e1":"markdown","912fdaf6":"markdown","358ba5b2":"markdown","f7e017c7":"markdown","43a577ab":"markdown","c999cec8":"markdown","77de7dc6":"markdown","b6790185":"markdown","611d50f9":"markdown","4180e120":"markdown","52341491":"markdown","9f7b5678":"markdown","9b951573":"markdown","011e45b7":"markdown","12f1549c":"markdown","ff50d931":"markdown","bb648886":"markdown","6489246b":"markdown","3fdcf53e":"markdown"},"source":{"76c50689":"# We're going to use type hinting\nfrom typing import List, Union, Dict\n\n# Modelling. Warnings will be used to silence various model warnings for tidier output\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import export_graphviz\nfrom sklearn.exceptions import DataConversionWarning\nimport warnings\n\n# Data handling\/display\nimport graphviz\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\n\n# IBM's fairness tooolbox:\nfrom aif360.datasets import BinaryLabelDataset  # To handle the data\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric  # For calculating metrics\nfrom aif360.explainers import MetricTextExplainer  # For explaining metrics\nfrom aif360.algorithms.preprocessing import Reweighing  # Preprocessing technique\n\nsns.set()\nsns.set_context(\"talk\")","260adf4a":"# Load data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntest.loc[:, 'Survived'] = 0 ","21c89429":"# Preprocessing will be done using a sklearn pipeline. We need these bits to make the transformers and connect them.\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\n\n# For the logistic regression model\nfrom sklearn.preprocessing import StandardScaler","aee60596":"class SelectCols(TransformerMixin):\n    \"\"\"Select columns from a DataFrame.\"\"\"\n    def __init__(self, cols: List[str]) -> None:\n        self.cols = cols\n\n    def fit(self, x: None) -> \"SelectCols\":\n        \"\"\"Nothing to do.\"\"\"\n        return self\n\n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Return just selected columns.\"\"\"\n        return x[self.cols]    ","55c4a6aa":"sc = SelectCols(cols=['Sex', 'Survived'])\nsc.transform(train.sample(5))","37206133":"class LabelEncoder(TransformerMixin):\n    \"\"\"Convert non-numeric columns to numeric using label encoding. \n    Handles unseen data on transform.\"\"\"\n    def fit(self, x: pd.DataFrame) -> \"LabelEncoder\":\n        \"\"\"Learn encoder for each column.\"\"\"\n        encoders = {}\n        for c in x:\n            # Make encoder using pd.factorize on unique values, \n            # then convert to a dictionary\n            v, k = zip(pd.factorize(x[c].unique()))\n            encoders[c] = dict(zip(k[0], v[0]))\n\n        self.encoders_ = encoders\n\n        return self\n\n    def transform(self, x) -> pd.DataFrame:\n        \"\"\"For columns in x that have learned encoders, apply encoding.\"\"\"\n        x = x.copy()\n        for c in x:\n            # Ignore new, unseen values\n            x.loc[~x[c].isin(self.encoders_[c]), c] = np.nan\n            # Map learned labels\n            x.loc[:, c] = x[c].map(self.encoders_[c])\n\n        # Return without nans\n        return x.fillna(-2).astype(int)","210b8308":"le = LabelEncoder()\nle.fit_transform(train[['Pclass', 'Sex']].sample(5))","03dada38":"le.encoders_","d8cc74aa":"class NumericEncoder(TransformerMixin):\n    \"\"\"Remove invalid values from numerical columns, replace with median.\"\"\"\n    def fit(self, x: pd.DataFrame) -> \"NumericEncoder\":\n        \"\"\"Learn median for every column in x.\"\"\"\n        # Find median for all columns, handling non-NaNs invalid values and NaNs\n        # Where all values are NaNs (after coercion) the median value will be a NaN.\n        self.encoders_ = {\n            c: pd.to_numeric(x[c],\n                             errors='coerce').median(skipna=True) for c in x}\n\n        return self\n\n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"For each column in x, encode NaN values are learned \n        median and add a flag column indicating where these \n        replacements were made\"\"\"\n\n        # Create a list of new DataFrames, each with 2 columns\n        output_dfs = []\n        for c in x:\n            new_cols = pd.DataFrame()\n            # Find invalid values that aren't nans (-inf, inf, string)\n            invalid_idx = pd.to_numeric(x[c].replace([-np.inf, np.inf],\n                                                     np.nan),\n                                        errors='coerce').isnull()\n\n            # Copy to new df for this column\n            new_cols.loc[:, c] = x[c].copy()\n            # Replace the invalid values with learned median\n            new_cols.loc[invalid_idx, c] = self.encoders_[c]\n            # Mark these replacement in a new column called \n            # \"[column_name]_invalid_flag\"\n            new_cols.loc[:, f\"{c}_invalid_flag\"] = invalid_idx.astype(np.int8)\n\n            output_dfs.append(new_cols)\n\n        # Concat list of output_dfs to single df\n        df = pd.concat(output_dfs,\n                       axis=1)\n\n        # Return wtih an remaining NaNs removed. These might exist if the median\n        # is a NaN because there was no numeric data in the column at all.\n        return df.fillna(0)","a1a921a5":"ne = NumericEncoder()\nne.fit_transform(train[['Age', 'Fare']].sample(5))","4349cc9e":"ne.encoders_","c09ac83a":"## Constructing the pipeline\n\n# LabelEncoding fork: Select object columns -> label encode\npp_object_cols = Pipeline([('select', SelectCols(cols=['Sex', 'Survived', \n                                                       'Cabin', 'Ticket', \n                                                       'SibSp', 'Embarked',\n                                                       'Parch', 'Pclass',\n                                                       'Name'])),\n                           ('process', LabelEncoder())])\n\n# NumericEncoding fork: Select numeric columns -> numeric encode\npp_numeric_cols = Pipeline([('select', SelectCols(cols=['Age', \n                                                        'Fare'])),\n                            ('process', NumericEncoder())])\n\n\n# We won't use the next part, but typically the pipeline would continue to \n# the model (after dropping 'Survived' from the training data, of course). \n# For example:\npp_pipeline = FeatureUnion([('object_cols', pp_object_cols),\n                            ('numeric_cols', pp_numeric_cols)])\n\nmodel_pipeline = Pipeline([('pp', pp_pipeline),\n                           ('mod', LogisticRegression())])\n# This could be run with model.pipeline.fit_predict(x), and passed to a \n# gridsearch object ","688c732c":"train_, valid = train_test_split(train,\n                                 test_size=0.3)\n\n# .fit_transform on train\ntrain_pp = pd.concat((pp_numeric_cols.fit_transform(train_), \n                      pp_object_cols.fit_transform(train_)),\n                     axis=1)\n\n# .transform on valid\nvalid_pp = pd.concat((pp_numeric_cols.transform(valid), \n                      pp_object_cols.transform(valid)),\n                     axis=1)\nvalid_pp.sample(5)","26e18d90":"# .transform on test\ntest_pp = pd.concat((pp_numeric_cols.transform(test), \n                     pp_object_cols.transform(test)),\n                    axis=1)\ntest_pp.sample(5)","2e047c6d":"target = 'Survived'\nx_columns = [c for c in train_pp if c != target]\nx_train, y_train = train_pp[x_columns], train_pp[target]\nx_valid, y_valid = valid_pp[x_columns], valid_pp[target]\nx_test = test_pp[x_columns]","7ff285db":"sub = pd.read_csv('..\/input\/gender_submission.csv')","4f41dec6":"biased_lr = LogisticRegression()\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', FutureWarning)\n\n    biased_lr.fit(x_train, y_train)\n    \nprint(f\"Logistic regression validation accuracy: {biased_lr.score(x_valid, y_valid)}\")\n\nsub.loc[:, 'Survived'] = biased_lr.predict(x_test).astype(int)\nsub.to_csv('biased_lr.csv', \n           index=False)","c1bdff1e":"biased_rfc = RandomForestClassifier(n_estimators=100, \n                                    max_depth=4)\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', FutureWarning)\n    \n    biased_rfc.fit(x_train, y_train)\n    \nprint(f\"Random forest validation accuracy: {biased_rfc.score(x_valid, y_valid)}\")\n\nsub.loc[:, 'Survived'] = biased_rfc.predict(x_test).astype(int)\nsub.to_csv('biased_rfc.csv', \n           index=False)","660898c5":"train_pp_bld = BinaryLabelDataset(df=pd.concat((x_train, y_train),\n                                               axis=1),\n                                  label_names=['Survived'],\n                                  protected_attribute_names=['Sex'],\n                                  favorable_label=1,\n                                  unfavorable_label=0)\n\nprivileged_groups = [{'Sex': 1}]\nunprivileged_groups = [{'Sex': 0}]","d2b8c9e9":"class MetricAdditions:\n    def explain(self,\n                disp: bool=True) -> Union[None, str]:\n        \"\"\"Explain everything available for the given metric.\"\"\"\n\n        # Find intersecting methods\/attributes between MetricTextExplainer and provided metric.\n        inter = set(dir(self)).intersection(set(dir(self.metric)))\n\n        # Ignore private and dunder methods\n        metric_methods = [getattr(self, c) for c in inter if c.startswith('_') < 1]\n\n        # Call methods, join to new lines\n        s = \"\\n\".join([f() for f in metric_methods if callable(f)])\n\n        if disp:\n            print(s)\n        else:\n            return s\n        \n        \nclass MetricTextExplainer_(MetricTextExplainer, MetricAdditions):\n    \"\"\"Combine explainer and .explain.\"\"\"\n    pass","2eb2fd2d":"# Create the metric object\nmetric_train_bld = BinaryLabelDatasetMetric(train_pp_bld,\n                                            unprivileged_groups=unprivileged_groups,\n                                            privileged_groups=privileged_groups)\n\n# Create the explainer object\nexplainer = MetricTextExplainer_(metric_train_bld)\n# Explain relevant metrics\nexplainer.explain()","23bbae51":"rw = Reweighing(unprivileged_groups=unprivileged_groups,\n                privileged_groups=privileged_groups)\ntrain_pp_bld_f = rw.fit_transform(train_pp_bld)","ecfa7c80":"pd.DataFrame({'Sex': x_train.Sex,\n              'Survived': y_train,\n              'Original_weight': np.ones(shape=(x_train.shape[0],)),\n              'new_weight': train_pp_bld_f.instance_weights}).sample(15)","132dd9ef":"# Create the metric object\nmetric_train_bld = BinaryLabelDatasetMetric(train_pp_bld_f,\n                                            unprivileged_groups=unprivileged_groups,\n                                            privileged_groups=privileged_groups)\n\n# Create the explainer object\nexplainer = MetricTextExplainer_(metric_train_bld)\n# Explain relevant metrics\nexplainer.explain()","8c4740fe":"unbiased_lr = LogisticRegression()\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', FutureWarning)\n        \n    unbiased_lr.fit(x_train, y_train,\n                    sample_weight=train_pp_bld_f.instance_weights)\n\n\nsub.loc[:, 'Survived'] = unbiased_lr.predict(x_test).astype(int)\nsub.to_csv('unbiased_lr.csv', \n           index=False)","e13e394f":"unbiased_rfc = RandomForestClassifier(n_estimators=100,\n                                      max_depth=4)\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', FutureWarning)\n    \n    unbiased_rfc.fit(x_train, y_train,\n                     sample_weight=train_pp_bld_f.instance_weights)\n\n\nsub.loc[:, 'Survived'] = unbiased_rfc.predict(x_test).astype(int)\nsub.to_csv('unbiased_rfc.csv', \n           index=False)","3bb299f9":"print(f\"Logistic regression validation accuracy: {unbiased_lr.score(x_valid, y_valid)}\")\nprint(f\"Random forest validation accuracy: {unbiased_rfc.score(x_valid, y_valid)}\")","9e216107":"pd.DataFrame({'LogReg': [0.77033, 0.69377], 'RFC': [0.79425, 0.69856]}, \n             index=['baised', 'unbiased'])","19894537":"def plot_auc(y_true: np.ndarray, preds: Dict[str, np.ndarray],\n             title: str='', \n             ax=None) -> None:\n    \n    leg = []\n    for k, p in preds.items():\n        fpr, tpr, _ = roc_curve(y_true, p)\n        ax = sns.lineplot(x=fpr, \n                          y=tpr,\n                          ci=None,\n                          ax=ax\n                         )\n        leg.append(f\"{k}: {round(auc(fpr, tpr), 2)}\")\n    \n    ax.legend(leg)\n    ax.set_xlabel('FPR')\n    ax.set_ylabel('TPR')\n    sns.lineplot(x=[0, 1],\n                 y=[0, 1],\n                 color='gray',\n                 ax=ax)\n    \n    ax.set_title(title)\n    \nprint('Accuracy:')\ndisplay(pd.DataFrame({'LogReg': [biased_lr.score(x_valid, y_valid), \n                                 unbiased_lr.score(x_valid, y_valid)],\n                      'RFC': [biased_rfc.score(x_valid, y_valid),\n                              unbiased_rfc.score(x_valid, y_valid)]}, \n                     index =['Unfair', 'Fair']))\n\nprint('AUC:')\nfig, ax = plt.subplots(nrows=1, \n                       ncols=2,\n                       figsize=(16, 6))\nplot_auc(y_valid, \n         {'biased': biased_lr.predict_proba(x_valid)[:, 1],\n          'unbiased': unbiased_lr.predict_proba(x_valid)[:, 1]},\n         title='LR',\n         ax=ax[0]) \nplot_auc(y_valid, \n         {'biased': biased_rfc.predict_proba(x_valid)[:, 1],\n          'unbiased': unbiased_rfc.predict_proba(x_valid)[:, 1]},\n         title='RFC',\n         ax=ax[1]) \nplt.show()","d8df352a":"def feature_importance(mod: Union[LogisticRegression, RandomForestClassifier],\n                       names: List[str],\n                       scale=None) -> pd.DataFrame:\n    \"\"\"Return feature importance for LR or RFC models in a sorted DataFrame.\"\"\"\n    if type(mod) == LogisticRegression:\n        imp = np.abs(mod.coef_.squeeze()) \/ scale\n        var = np.zeros(shape=imp.shape)\n    elif type(mod) == RandomForestClassifier:\n        imps = np.array([fi.feature_importances_ for fi in mod.estimators_])\n        imp = imps.mean(axis=0)\n        var = imps.std(axis=0)\n\n    return pd.DataFrame({'feature': names,\n                         'importance': imp,\n                         'std': var}).sort_values('importance',\n                                                  ascending=False)\n\ndef plot_feature_importance(**kwargs) -> None:\n    ax = sns.barplot(**kwargs)\n    for l in ax.get_xticklabels():\n        l.set_rotation(90)","a79437e7":"fig, ax = plt.subplots(nrows=1, \n                       ncols=2,\n                       figsize=(16, 6))\n\nplot_feature_importance(x='feature', \n                        y='importance', \n                        data=feature_importance(biased_lr,\n                                                names=x_train.columns.tolist(),\n                                                scale=x_train.std()),\n                       ax=ax[1])\n_ = ax[0].set_title('LR')\nplot_feature_importance(x='feature', \n                        y='importance', \n                        data=feature_importance(biased_rfc,\n                                                names=x_train.columns.tolist()),\n                       ax=ax[0])\n_ = ax[1].set_title('RFC')","76625a24":"fig, ax = plt.subplots(nrows=1, \n                       ncols=2,\n                       figsize=(16, 6))\n\nplot_feature_importance(x='feature', \n                        y='importance', \n                        data=feature_importance(unbiased_lr,\n                                                names=x_train.columns.tolist(),\n                                                scale=x_train.std()),\n                        ax=ax[1])\n_ = ax[0].set_title('LR')\nplot_feature_importance(x='feature', \n                        y='importance', \n                        data=feature_importance(unbiased_rfc,\n                                                names=x_train.columns.tolist()),\n                        ax=ax[0])\n_ = ax[1].set_title('RFC')","07f93652":"model = biased_rfc\n# model = unbiased_rfc\nt = 1\n\ntree_graph = export_graphviz(biased_rfc.estimators_[t],\n                             filled= True,\n                             rounded=True,\n                             leaves_parallel=True,\n                             class_names=['Lost', 'Survived'],\n                             feature_names=x_train.columns) \ndisplay(graphviz.Source(tree_graph))","7d84d6c8":"def calc_metrics(mod, x: pd.DataFrame, y_true: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Calculate fairness metrics at each model threshold.\"\"\"\n    \n    # Create a BinaryLabelDataset (as before training)\n    bld = BinaryLabelDataset(df=pd.concat((x, y_true),\n                                               axis=1),\n                             label_names=['Survived'],\n                             protected_attribute_names=['Sex'],\n                             favorable_label=1,\n                             unfavorable_label=0)\n\n    privileged_groups = [{'Sex': 1}]\n    unprivileged_groups = [{'Sex': 0}]\n    \n    # Create a second set to hold the predicted labels\n    bld_preds = bld.copy(deepcopy=True)\n    preds = mod.predict_proba(x)[:, 1]\n\n    balanced_accuracy = []\n    disp_impact = []\n    avg_odd_diff = []\n    \n    # For threshold values between 0 and 1:\n    thresh = np.linspace(0.01, 0.99, 100)\n    for t in thresh:\n        \n        # Apply threshold and set labels in bld for predictions\n        bld_preds.labels[preds > t] = 1\n        bld_preds.labels[preds <= t] = 0\n\n        # Calculate the metrics for this threshold\n        valid_metric = ClassificationMetric(bld, bld_preds, \n                                            unprivileged_groups=unprivileged_groups,\n                                            privileged_groups=privileged_groups)\n\n        # Save the balanced accuracy of the model, and the metrics\n        balanced_accuracy.append(0.5 * (valid_metric.true_positive_rate()\n                                        + valid_metric.true_negative_rate()))\n        avg_odd_diff.append(valid_metric.average_odds_difference())\n        disp_impact.append(np.abs(valid_metric.disparate_impact() - 0.5))\n\n    # Return as df indexed by threshold\n    metrics = pd.DataFrame({'balanced_accuray': balanced_accuracy,\n                            'disparate_impact': disp_impact,\n                            'avg_odds_diff': avg_odd_diff},\n                          index=thresh)\n    return metrics\n\n\ndef plot_metrics(metrics: pd.DataFrame, \n                 title: str='', **kwargs) -> None:\n    \"\"\"Plot the metrics df from calc_metrics with seaborn.\"\"\"\n    ax = sns.lineplot(data=metrics, \n                      **kwargs)\n    ax.set_title(title)\n    ax.set_xlabel('Classification threshold')\n\n# Plot for LR\nfig, ax = plt.subplots(nrows=1, \n                       ncols=2,\n                       figsize=(16, 6))\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', RuntimeWarning)\n    \n    plot_metrics(calc_metrics(biased_lr, x_valid, y_valid),\n                ax=ax[0],\n                title=\"LR: Biased\")\n    \n    plot_metrics(calc_metrics(unbiased_lr, x_valid, y_valid),\n                ax=ax[1],\n                title='\"Fair\"')","39b5bc0e":"# Plot for RFC\nfig, ax = plt.subplots(nrows=1, \n                       ncols=2,\n                       figsize=(16, 6))\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', RuntimeWarning)\n    \n    plot_metrics(calc_metrics(biased_rfc, x_valid, y_valid),\n                ax=ax[0],\n                title=\"RFC: Biased\")\n    \n    plot_metrics(calc_metrics(unbiased_rfc, x_valid, y_valid),\n                ax=ax[1],\n                title='\"Fair\"')","10e7f9c6":"### \"Unfair\" models","14a1af94":"### Example output","1ef60fa1":"## Model training\n\nThe new weights are passed to the classifier for training along with the unmodified data. Because the training data is unmodified, we don't need to do anything to the test set when making predictions.\n\n","a8a50d8c":"# Benchmark models\n\n## Prepare the data\nSplit the features and targets, and create a training and validation set.","e540fb6f":"There are similar results for the random forest although disparate impact looks a bit worse in the fair model (note this may vary between notebook runs - the validation set here is only 223 rows!)","e7368a8a":"### Example output","280b7f30":"### Test set performance\n\nThese values examples taken from manually submitting the output files and comapring against the test set. ","ba8a74b9":"The new weights are stored in .instance_weights, for example:","6f7f1044":"Sex is certainly a less important feature in both models, but hasn't completely eliminated.\n\nNote that for the forest models, the actual trees can be inspected using the block of code below. Overall, Sex will appear less frequently as a split in the unbiased RFC models (which is essentially what is quantified in the feature importance graphs above). Each forest contains 100 individual trees, so only a single example is plotted here. Modify **model** and **t** below to inspect other trees. ","7b16b9e1":"### Example output","912fdaf6":"## Train models\n\nTrain LogisticRegression and a RandomForestClassifier. Also dump out the test predictions to see how to do on the leaderboard.\n\nwarnings is just used here to shut up some FutureWarnings.","358ba5b2":"## Metrics\n\nAIF comes with a metrics and an explainer class that runs relevant metrics different types of datasets. Here we'll add a method to explain everything relevant to our banary problem with a single call to .explain().","f7e017c7":"### Validation performance","43a577ab":"Recalucalting the fairness metrics with the new row weights should show zero difference between the avaerage outcomes for the priviliged and unpriviliged groups.","c999cec8":"# Preprocessing\n\nReweighing requires processed numerical data, so we need to pre-process the raw Titanic data before trying to apply it.\n\nWe'll use a sklearn pipeline to do pre-processing of the data. For those not familiar with sklearn pipelines, they can be used to chain processing, feature engineering and model steps together. This allows for hyperparameters search over for the full pipeline, rather than just the model. They work with sklearn's object orientated fit\/transform\/predict paradigm.\n\nWe're not going to use them to their full potential here, but will create custom transformers and a pipeline to do just the initial pre-processing. The transformers will each define .fit() and .transform() methods; .fit() will be used to learn from the training data, and .transform() will be applied to the training and test data.\n\nThere will be two forks to the pipeline, one to handle object\/string features, and the other to handle numeric features. The outputs of these will be combined to create the features for modelling\n\nThe following cells define classes we need. For convenience, these will work with DataFrames.\n","77de7dc6":"## LabelEncoder\n\nTo deal with the object\/string columns. Does simple label encoding and handles unseen data (ie. categorical levels seen in .transform() that weren't previously seen in .fit()).","b6790185":"# Did it work?\n\nBased on both the validation set and test set accuracy, it looks like we've lost some performance by trying to remove the effectiveness of the Sex feature (is anyone surprised?). ","611d50f9":"For the LogisticRegression model, the biased model is very unfair at the optimal balanced accuracy threhold around 0.5. Disparte impact and average odds diference both indicate bais in the predictions.Notice that at prediction thesholds of 0 and 1, where everyone is treated the same, the metrics indicate no bias. Balanced accuracy is 50% in these cases as it corrects for the samping bias (between survived and no survived).\n\nFor the \"fair\" model avg odds appears reasonable, and disparat impact indicates less bias at the optimal classification threshold around 0.4 indicated","4180e120":"## Constructing the pipeline\n\nThe two forks are joined as individual pipelines, then their output is concatenated using a feature union.","52341491":"# Fairness metrics\nModel accuracy depends on the classification threshold applied to the predicted probabilities. Similarly, fairness varies with this threshold; members of the same groups may can similar predication probabilities, assuming the group is predictive, so can be separated by the selected threshold.\n\nThe following code will calculate the balanced accuracy of the model at several threshold steps. Disparate impact and average odds difference are also calculated as measures of fairness (see also: https:\/\/github.com\/IBM\/AIF360\/blob\/master\/examples\/demo_reweighing_preproc.ipynb).\n\n**Balanced accuracy** (accuracy balanced for target proportion) = (TP \/ P + TN \/ N) \/ 2  \n**Average odds difference** - closer to 0 is better    \n**Disparate impact** - closer to 0.5 is better","9f7b5678":"We'll consider statistical mean difference here as it has a simple interpretation. It's the difference in mean outcomes when the data is split by the un\/privileged groups we defined. If it's not 0, one group has better outcomes that the other - in this case men are more likely to have an unfavourable outcome.\n\n## Reweighing\n\nReweighing won't change the training data at all, rather it will learn new weights for each training row that will make the mean difference in outcomes between the specified groups 0.\n\nSpecifically here:\n  - Examples of men who don't survive will be downweighted\n  - Examples men who survive will be upweighted\n  - Examples of women who survive will be downweighted\n  - Examples of women who die will be upweighted\n  \nPractically, reweighing works like an sklearn transformer (on the BinaryLabelDataset)\n","9b951573":"Also here: https:\/\/github.com\/garethjns\/Kaggle-Titanic\/blob\/master\/titanicsexism-fairness-in-ml.ipynb\n\n# TitanicSexism (fairness in ML)\n\n*Women and children first?! We'll never get that past the regulators!*\n\n## Fairness in machine learning\nThere's a growing awareness in machine learning of the problems that can be caused by unfair models, and the difficulty involved in creating fair models from biased data. Even when legally protected characteristics like sex, age, and race aren't explicitly used, other features can act as proxies and subtly leak bias. Understanding and dealing sources of bias and their interactions in models is currently an unsolved problem, but tools are becoming more readily available to approach it\n\nExisting approaches for bias mitigation can be applied at different stage in ML pipelines - in pre-processing, during model training, and after prediction. This kernel will demonstrate applying [reweighing](http:\/\/doi.org\/10.1007\/s10115-011-0463-8), a relatively simple pre-processing technique that learns weights for training data that can be used during training to reduce bias.\n\nClearly Titanic models typically rely too heavily on sex as a feature, which is unacceptable in our modern and enlightened world. To try and address this problem we'll;\n - Assess the bias in the Titainc dataset\n - Apply reweighing in pre-processing using the new [IBM AIF 360 toolbox](https:\/\/github.com\/IBM\/AIF360)\n - Train benchmark and \"fair\" models, using both linear (logistic regression) and non-parametric (random forest) models\n - Compare the performance of the models\n - Try different techniques for comparing the fairness of the benchmark and \"fair\" models, and compare the effectiveness of the reweighing approach between logistic regression and random forest.","011e45b7":"## Running the pipeline\n\npp_pipeline.fit_transform() can called on the training set, however for the sake of sticking with DataFrames, we'll avoid calling the FeatureUnion and do it like this instead. ","12f1549c":"### \"Fair\" models","ff50d931":"## SelectCols\nThis is a helper class. It simply holds a list of column names, which it will return when it finds them in a DataFrame passed to .transform is called (each transformer in a Pipeline calls .fit_transform() in turn).","bb648886":"# \"Fair\" models\n\nBefore applying any fairness processing or calculating metrics, we need to set up a  BinaryLabelDataset object (which is part of AIF360) to hold the data. Then we'll specify sex as a protected characteristic, and men (label: 1) as the underprivileged group.","6489246b":"# Conclusions (WIP)\n  - There's no free lunch (or hunch) here, attempting to remove sex as a predictor harms model performance (try submitting the output files to compare against the test set). This isn't a particularly suprising finding given that, for the Titianic dataset, gender alone produces a pretty good model.\n  - Less bias is removed from the random forest model, due to interactions with other features. For example, if a certain age range contains only women who surive then a tree split can be created on age that acts as a proxy for sex. Reweighing be more effective for linear models because this is avoided.\n  \n# Next steps\n  - Other techniques?","3fdcf53e":"But have we really created a fairer model? This is a harder question to answer.\n\n## Feature importance\n\nThe most superficial way to try and figure out what the clasifier is thinking is to plot the feature importance."}}