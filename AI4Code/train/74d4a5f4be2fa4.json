{"cell_type":{"f974570e":"code","28c3b2f0":"code","56b40bda":"code","f3679569":"code","85ca0287":"code","046ea4f4":"code","067ab753":"code","198e863e":"code","60c30b42":"code","627ac0df":"code","db9b008e":"code","baabc046":"code","b78e05ac":"code","3f0e2f8e":"code","a9eb5cbd":"code","9e742489":"code","6be5cda5":"code","c054e4ed":"code","91d17b6e":"code","87c99931":"code","996224e5":"code","e9c0bb02":"markdown","69020915":"markdown","4a238b76":"markdown","0158a010":"markdown","94e9de3b":"markdown","2964232c":"markdown","95bd640d":"markdown","754ed7e9":"markdown","9ee7b4c6":"markdown","bc760e6a":"markdown","259e22fe":"markdown","8338f2f1":"markdown","f85dff43":"markdown","fdc2d38d":"markdown","b0cdbab1":"markdown","f8d7b742":"markdown"},"source":{"f974570e":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks","28c3b2f0":"X_train = np.load('..\/input\/street-view-house-numbers-svhn-dataset-numpy\/X_train.npy')\ny_train = np.load('..\/input\/street-view-house-numbers-svhn-dataset-numpy\/y_train.npy')\nX_test = np.load('..\/input\/street-view-house-numbers-svhn-dataset-numpy\/X_test.npy')\ny_test = np.load('..\/input\/street-view-house-numbers-svhn-dataset-numpy\/y_test.npy')","56b40bda":"X_train.shape","f3679569":"y_train = np.where(y_train==10, 0, y_train)\ny_test = np.where(y_test==10, 0, y_test)","85ca0287":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(13, 5))\ncolumns = 5\nrows = 2\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(rows*columns):\n    j = np.random.randint(X_train.shape[3], size=1)[0]\n    img = X_train[:,:,:,j]\n    # create subplot and append to ax    \n    ax.append( fig.add_subplot(rows, columns, i+1) )\n    ax[-1].set_title(str(y_train[j][0]))  # set title\n    plt.tick_params(length=0)\n    plt.axis('off')\n    plt.imshow(img)","046ea4f4":"X_train_gray = X_train.mean(axis=2, keepdims=True)\nX_test_gray = X_test.mean(axis=2, keepdims=True)","067ab753":"fig = plt.figure(figsize=(13, 5))\ncolumns = 5\nrows = 2\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(rows*columns):\n    j = np.random.randint(X_train_gray.shape[3], size=1)[0]\n    img = X_train_gray[:,:,0,j]\n    # create subplot and append to ax    \n    ax.append( fig.add_subplot(rows, columns, i+1) )\n    ax[-1].set_title(str(y_train[j][0]))  # set title\n    plt.tick_params(length=0)\n    plt.axis('off')\n    plt.imshow(img)","198e863e":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,Dropout,BatchNormalization\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ndef get_new_model(input_shape):\n    model = Sequential([\n        Dense(312, activation='relu', input_shape=input_shape, name='dense_1'),\n        Flatten(name='flatten_2'),\n        Dropout(0.3),\n        BatchNormalization(),\n        Dense(550, activation='relu', name='dense_3'),\n        Flatten(name='flatten_4'),\n        Dropout(0.3),\n        BatchNormalization(),\n        Dense(724, activation='relu', name='dense_5'),\n        Flatten(name='flatten_6'),\n        Dropout(0.3),\n        BatchNormalization(),\n        Dense(444, activation='relu', name='dense_7'),\n        Flatten(name='flatten_8'),\n        Dropout(0.3),\n        BatchNormalization(),\n        Dense(10, activation='softmax', name='dense_13')\n    ])\n    \n    model.compile(optimizer='adam',\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n    \n    return model\n\nmodel = get_new_model(X_train_gray[:,:,:,0].shape)","60c30b42":"model.summary()","627ac0df":"def get_early_stopping():\n    callback = tf.keras.callbacks.EarlyStopping(\n                    min_delta=0.0001,\n                    monitor=\"val_loss\",\n                    patience=8,\n                    verbose=0,\n                    )\n    return callback","db9b008e":"early_stopping = get_early_stopping()","baabc046":"callbacks = [early_stopping]\n\ndef train_model(model, X_train_gray, y_train, epochs1=45, callbacks=callbacks):\n    history = model.fit(np.moveaxis(X_train_gray, -1, 0), y_train, epochs=epochs1, \n                  validation_split=0.27 , callbacks=callbacks,batch_size=62)\n    return history \n\nhistory = train_model(model, X_train_gray[:,:,:,:], y_train)","b78e05ac":"try:\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\nexcept KeyError:\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\nplt.title('Accuracy vs. epochs MLP model')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show() ","3f0e2f8e":"model.evaluate(np.moveaxis(X_test_gray, -1, 0),y_test)","a9eb5cbd":"model = Sequential([\nConv2D(64, kernel_size = 3, activation='relu', input_shape = X_train_gray[:,:,:,0].shape),\nBatchNormalization(),\nConv2D(64, kernel_size = 3, activation='relu'),\nBatchNormalization(),\nConv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'),\nBatchNormalization(),\nDropout(0.4),\n\nConv2D(64, kernel_size = 3, activation='relu'),\nBatchNormalization(),\nConv2D(64, kernel_size = 3, activation='relu'),\nBatchNormalization(),\nConv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'),\nBatchNormalization(),\nDropout(0.4),\n\nConv2D(128, kernel_size = 4, activation='relu'),\nBatchNormalization(),\nFlatten(),\nDropout(0.4),\nDense(512, activation='relu'),\nDropout(0.4),\nDense(10, activation='softmax')])\n    \nmodel.compile(optimizer='adamax',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    ","9e742489":"callbacks = [early_stopping]\n\ndef train_model(model, X_train_gray, y_train, epochs1=45, callbacks=callbacks):\n    history = model.fit(np.moveaxis(X_train_gray, -1, 0), y_train, epochs=epochs1, \n                  validation_split=0.27 , callbacks=callbacks,batch_size=58)\n    return history \n\nhistory = train_model(model, X_train_gray[:,:,:,:], y_train)","6be5cda5":"his=pd.DataFrame(history.history)","c054e4ed":"his.head()","91d17b6e":"his.loc[:,['accuracy','val_accuracy']].plot()\nplt.show() ","87c99931":"his.loc[:,['loss','val_loss']].plot()\nplt.show() ","996224e5":"model.evaluate(np.moveaxis(X_test_gray, -1, 0),y_test)","e9c0bb02":"# Transform the dataset: reducing channels","69020915":"We can see that the best accuracy we got, 94.71!!!","4a238b76":"We can see that the best accuracy we got was 86.68%, let's try the CNN model","0158a010":"The label '0' is as '10' in the original dataset, so we are gonna replace this value","94e9de3b":"# Approach to solve this problem will be to:\n### 1. Load the *data*\n### 2. Visualize the images\n### 2. Build the CNN\/MLP model(as the dataset is already in *numpy* format","2964232c":"Now we reduce the images' channels to make the processing faster. To do this, we take the mean value between channels","95bd640d":"Now, we build an Miltilayer Perceptron (MLP) classifier","754ed7e9":"Now we can see that even that the CNN model has by far less parameters, it ouperformed the MLP model","9ee7b4c6":"# And let's see our model","bc760e6a":"# loading the data","259e22fe":"Let's build a Convolutional Neural Network model to see the accuracy","8338f2f1":"# Building an MLP classifier","f85dff43":"And then display 10 random images and its labels","fdc2d38d":"Let's start the training process","b0cdbab1":"The EarlyStopping module stop the training process if the validation accuracy doesn't improve after the patience argument value","f8d7b742":"# Building a CNN model"}}