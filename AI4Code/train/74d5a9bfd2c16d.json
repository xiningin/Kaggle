{"cell_type":{"7ba470f7":"code","09e3f9db":"code","b78387a8":"code","adcedc1e":"code","9a123d7a":"code","740691b9":"code","9f9aa4bc":"code","599f5e14":"code","cc24850e":"code","65f2c153":"code","6fecacae":"markdown","d06784f1":"markdown","2da82de3":"markdown","376cbe31":"markdown","90672e43":"markdown","02ab176e":"markdown","aaac2bd8":"markdown","911c0ebc":"markdown"},"source":{"7ba470f7":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","09e3f9db":"df = pd.read_csv('..\/input\/adult-census-income\/adult.csv')\ndf.income.value_counts()","b78387a8":"df.head()","adcedc1e":"df['income'].isnull().sum()\n","9a123d7a":"# creating folds here we are using StratifiedKFold\nfrom sklearn import model_selection\ndf['kfold'] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.income.values\n\nkf = model_selection.StratifiedKFold(n_splits=5)\n\nfor fold, (train_, valid_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[valid_, 'kfold'] = fold\n    \ndf.to_csv(\".\/adult_folds.csv\", index=False)","740691b9":"df_fold = pd.read_csv('.\/adult_folds.csv')\ndf_train = df[df.kfold != 0].reset_index(drop=True)\ndf_train.income.isnull().sum()","9f9aa4bc":"from sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import preprocessing\ndef run(fold):\n    # load the full training data with folds\n    df = pd.read_csv(\".\/adult_folds.csv\")\n    \n    # list of numerical columns\n    num_cols = [\n        \"fnlwgt\",\n    \"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"\n    ]\n    # drop numerical columns\n    df = df.drop(num_cols, axis=1)\n    # map targets to 0s and 1s\n    target_mapping = { \"<=50K\": 0,\n                        \">50K\": 1 \n                     }\n    df.loc[:, \"income\"] = df.income.map(target_mapping)\n    # all columns are features except income and kfold columns\n    features = [\n    f for f in df.columns if f not in (\"kfold\", \"income\")\n    ]\n\n    # fill all NaN values with NONE\n    # note that I am converting all columns to \"strings\"\n    # it doesnt matter because all are categories \n    for col in features:\n        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\") # get training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True) # initialize OneHotEncoder from scikit-learn\n    ohe = preprocessing.OneHotEncoder()\n    \n    All_data = pd.concat(\n    [df_train[features], df_valid[features]],\n        axis=0\n    )\n    \n    ohe.fit(All_data[features])\n    \n    x_train = ohe.transform(df_train[features])\n    x_valid = ohe.transform(df_valid[features])\n    \n    model = linear_model.LogisticRegression(solver='liblinear')\n    \n    model.fit(x_train, df_train.income.values)    \n    valid_preds = model.predict_proba(x_valid)[:, 1]\n    \n    # get roc auc score\n    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n    \n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \nif __name__ == \"__main__\" :\n    for fold_ in range(5):\n        run(fold_)\n    ","599f5e14":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport xgboost as xgb\n\ndef Xgboost_fold(fold):\n    # load the full training data with folds\n    df = pd.read_csv(\".\/adult_folds.csv\")\n   \n    # list of numerical columns\n    num_cols = [\n        \"fnlwgt\",\n        \"age\",\n        \"capital.gain\",\n        \"capital.loss\",\n        \"hours.per.week\"\n    ]\n\n    # drop numerical columns\n    df = df.drop(num_cols, axis=1)\n\n    # map targets to 0s and 1s\n    target_mapping = { \"<=50K\": 0,\n                        \">50K\": 1 \n                     }\n    df.loc[:, \"income\"] = df.income.map(target_mapping)\n\n    # all columns are features except kfold & income columns\n    features = [\n    f for f in df.columns if f not in (\"kfold\", \"income\")\n    ]\n\n    # fill all NaN values with NONE\n    # note that I am converting all columns to \"strings\"\n    # it doesnt matter because all are categories \n    for col in features:\n        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n    \n    #now its time to label encode the features\n    for col in features:\n        lbl = preprocessing.LabelEncoder()\n        \n        # fit label encoder on all data\n        lbl.fit(df[col])\n\n        # transform all the data\n        df.loc[:, col] = lbl.transform(df[col])\n        \n    # get training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True) # get training data\n    \n    x_train = df_train[features].values\n    # get validation data\n\n    x_valid = df_valid[features].values\n\n    # initialize xgboost model\n    model = xgb.XGBClassifier( n_jobs=-1)\n\n    # fit model on training data (ohe)\n    model.fit(x_train, df_train.income.values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC # we will use the probability of 1s\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get roc auc score\n    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    \n    \nif __name__ == \"__main__\":\n    for fold_ in range(5):\n        Xgboost_fold(fold_)\n    ","cc24850e":"def Xgboost_fold(fold):\n    # load the full training data with folds\n    df = pd.read_csv(\".\/adult_folds.csv\")\n   \n    # list of numerical columns\n    num_cols = [\n        \"fnlwgt\",\n        \"age\",\n        \"capital.gain\",\n        \"capital.loss\",\n        \"hours.per.week\"\n    ]\n\n    # map targets to 0s and 1s\n    target_mapping = { \"<=50K\": 0,\n                        \">50K\": 1 \n                     }\n    df.loc[:, \"income\"] = df.income.map(target_mapping)\n\n    # all columns are features except kfold & income columns\n    features = [\n    f for f in df.columns if f not in (\"kfold\", \"income\")\n    ]\n\n    # fill all NaN values with NONE\n    # note that I am converting all columns to \"strings\"\n    # it doesnt matter because all are categories \n    for col in features:\n        # do not encode the numerical columns\n        if col not in num_cols:\n            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n    \n    #now its time to label encode the features\n    for col in features:\n        if col not in num_cols:\n            lbl = preprocessing.LabelEncoder()\n        \n            # fit label encoder on all data\n            lbl.fit(df[col])\n\n            # transform all the data\n            df.loc[:, col] = lbl.transform(df[col])\n        \n    # get training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True) # get training data\n    \n    x_train = df_train[features].values\n    # get validation data\n\n    x_valid = df_valid[features].values\n\n    # initialize xgboost model\n    model = xgb.XGBClassifier( n_jobs=-1)\n\n    # fit model on training data (ohe)\n    model.fit(x_train, df_train.income.values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC # we will use the probability of 1s\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get roc auc score\n    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    \n    \nif __name__ == \"__main__\":\n    for fold_ in range(5):\n        Xgboost_fold(fold_)\n    ","65f2c153":"import itertools  \ndef feature_engineering(df, cat_cols):\n    \"\"\"\n    This function is used for feature engineering\n    :param df: the pandas dataframe with train\/test data :param cat_cols: list of categorical columns\n    :return: dataframe with new features\n    \"\"\"\n    # this will create all 2-combinations of values\n    # in this list\n    # for example:\n    # list(itertools.combinations([1,2,3], 2)) will return\n    # [(1, 2), (1, 3), (2, 3)]\n    combi = list(itertools.combinations(cat_cols, 2))\n    for c1, c2 in combi:\n        df.loc[:,\n              c1 + \"_\" + c2\n              ] = df[c1].astype(str) + \"_\" + df[c2].astype(str)\n        return df\n    \ndef Xgboost_fold(fold):\n    # load the full training data with folds\n    df = pd.read_csv(\".\/adult_folds.csv\")\n   \n    # list of numerical columns\n    num_cols = [\n        \"fnlwgt\",\n        \"age\",\n        \"capital.gain\",\n        \"capital.loss\",\n        \"hours.per.week\"\n    ]\n\n    # map targets to 0s and 1s\n    target_mapping = { \"<=50K\": 0,\n                        \">50K\": 1 \n                     }\n    df.loc[:, \"income\"] = df.income.map(target_mapping)\n    \n    # list of categorical columns for feature engineering\n    cat_cols = [\n        c for c in df.columns if c not in num_cols\n        and c not in (\"kfold\", \"income\")\n    ]\n    \n    # add new features\n    df = feature_engineering(df, cat_cols)\n    \n    # all columns are features except kfold & income columns\n    features = [\n    f for f in df.columns if f not in (\"kfold\", \"income\")\n    ]\n\n    # fill all NaN values with NONE\n    # note that I am converting all columns to \"strings\"\n    # it doesnt matter because all are categories \n    for col in features:\n        # do not encode the numerical columns\n        if col not in num_cols:\n            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n    \n    #now its time to label encode the features\n    for col in features:\n        if col not in num_cols:\n            lbl = preprocessing.LabelEncoder()\n        \n            # fit label encoder on all data\n            lbl.fit(df[col])\n\n            # transform all the data\n            df.loc[:, col] = lbl.transform(df[col])\n        \n    # get training data using folds\n    df_train = df[df.kfold != fold].reset_index(drop=True) # get validation data using folds\n    df_valid = df[df.kfold == fold].reset_index(drop=True) # get training data\n    \n    x_train = df_train[features].values\n    # get validation data\n\n    x_valid = df_valid[features].values\n\n    # initialize xgboost model\n    model = xgb.XGBClassifier( n_jobs=-1,\n                             max_depth=7)\n\n    # fit model on training data (ohe)\n    model.fit(x_train, df_train.income.values)\n\n    # predict on validation data\n    # we need the probability values as we are calculating AUC # we will use the probability of 1s\n    valid_preds = model.predict_proba(x_valid)[:, 1]\n\n    # get roc auc score\n    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n\n    # print auc\n    print(f\"Fold = {fold}, AUC = {auc}\")\n    \n    \n    \nif __name__ == \"__main__\":\n    for fold_ in range(5):\n        Xgboost_fold(fold_)","6fecacae":"This is a very good AUC for a model which is that simple!<\/br><\/br>\nLet's try the label encoded xgboost without tuning any of hyperparameters now.","d06784f1":"After creating a training data with folds csv file","2da82de3":"This seems quit good already. <\/br>\nI have tried increase max_depth to 7 and n_estimatores to 200 but it doesn't improve<\/br>\nyou can try by yourself","376cbe31":"We see that there are 7841 instances with income greater than 50k USD. This is ~24% of the total number of samples.<\/br>\nThe evalution we use for this dataset is AUC.","90672e43":"Whoa!!!<\/br><\/br>\nThat's an excellent score!<\/br><\/br>\nNow, we can try to add some features. We will take all the categorical columns and create all combinations of degree two. Take a look at feature_engineering function in the snippet below to know how this is done.","02ab176e":"Step 1-> Always making Cross-validation.","aaac2bd8":"Now, let's try to include <b>numerical featues in the xgboost model<\/b> without parameter tuning.","911c0ebc":"It seems that even without changing any hyperparameters and just by adding a bunch of features, we can improve our fold scores a bit by increasing max_depth to 7 helps."}}