{"cell_type":{"2a7267dc":"code","bc7fcee0":"code","78f8e437":"code","354dc30c":"code","ac4f6dba":"code","ba6c1345":"code","3fe42c82":"code","6fe7ba4d":"code","33288e10":"code","056dc753":"code","daa2f732":"code","1eb3b140":"code","7136861f":"code","a3cb6cf7":"code","dc70630f":"code","dbfa287a":"code","7070ae8c":"code","16676bc0":"code","50d5c2fb":"code","66d8cf66":"markdown","a54a0b3c":"markdown","419ee579":"markdown","86eca442":"markdown","37c2673d":"markdown","7cee036d":"markdown","c2289411":"markdown","588f69a9":"markdown","bff68f58":"markdown","221367d3":"markdown","4a2ee994":"markdown"},"source":{"2a7267dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc7fcee0":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","78f8e437":"train_df.head()","354dc30c":"test_df['target'] = sub['target']\ntest_df.head()","ac4f6dba":"train_df.info()","ba6c1345":"test_df.info()","3fe42c82":"# For now drop the columns that may not be relevant\n\ntrain_df.drop(['id', 'keyword', 'location'], axis=1, inplace=True)\ntest_df.drop(['id', 'keyword', 'location'], axis=1, inplace=True)","6fe7ba4d":"train_df.head()","33288e10":"test_df.head()","056dc753":"df = pd.concat([train_df, test_df], ignore_index = True)\ndf.head()","daa2f732":"'''\nText data requires preparation before you can start using it for predictive modeling. The text preprocessing steps include\nbut are not limited to:\n\n1. Removing punctuation\n2. Tokenizing the text into words\n3. Removing stopwords\n4. Lemmatizing the word tokens\n'''\n\nstopwords = nltk.corpus.stopwords.words('english')\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text_clean = \"\".join([char for char in text if char not in string.punctuation])\n    text_clean = re.split('\\W+', text.lower())\n    text_clean = [word for word in text_clean if word not in stopwords]\n    text_clean = \" \".join([lemmatizer.lemmatize(i, 'v') for i in text_clean])\n    return text_clean","1eb3b140":"# Cleaning the 'text' column\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\ndf.head()","7136861f":"# Initialize the Sentiment Analyzer function\nsid = SentimentIntensityAnalyzer()\n\n# Computing the sentiment score of the training data 'text' column\ndf['polarity'] = df['text'].apply(lambda x: sid.polarity_scores(x)['compound'])\n'''\nConverting the polarity score into classes:\nclass 0: Neutral\nclass 1: Positive\nclass 2: Negative\n'''\ndf.loc[df['polarity'] > 0, 'polarity'] = 1\ndf.loc[df['polarity'] < 0, 'polarity'] = 2\ndf.head()","a3cb6cf7":"# TF-IDF vectorization on the training data\n\ntfidf_vect = TfidfVectorizer()\nX_tfidf = tfidf_vect.fit_transform(df['text'])\n\nx_features = pd.concat([df['polarity'], pd.DataFrame(X_tfidf.toarray())], axis=1)\ny_features = df['target']\nx_features.head()","dc70630f":"x_train, x_test, y_train, y_test = train_test_split(x_features, df['target'], test_size=0.3)","dbfa287a":"rf = RandomForestClassifier()\nrf_model = rf.fit(x_train, y_train)\ny_pred = rf_model.predict(x_test)\nprecision, recall, fscore, support = score(y_test, y_pred, average = 'binary')\nprint(\"Precision: {} \\nRecall: {} \\nAccuracy: {}\".format(round(precision, 3), round(recall, 3),\n                                                        round((y_pred == y_test).sum()\/len(y_pred), 3)))\n","7070ae8c":"def train_RF(n_est, depth):\n        rf = RandomForestClassifier(n_estimators = n_est, max_depth = depth, n_jobs = -1)\n        rf_model = rf.fit(x_train, y_train)\n        y_pred = rf_model.predict(x_test)\n        precision, recall, fscore, support = score(y_test, y_pred, average = 'binary')\n        print('Est: {} \/ Depth: {} ------ Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(n_est, depth, round(precision,3),\n                                                                                           round(recall, 3), \n                                                                                        round((y_pred == y_test).sum()\/len(y_pred), 3)))","16676bc0":"for n_est in [10, 150, 300]:\n    for depth in [30, 60, 90, None]:\n        train_RF(n_est, depth)","50d5c2fb":"sub['target'] = y_pred\nsub.to_csv('submission.csv', index=False)\nsub.head(3)","66d8cf66":"<a id = \"sec-2c\"> <\/a>\n### After Initial Cleaning","a54a0b3c":"<a id = \"sec-5a\"> <\/a>\n### TF-IDF Vectorization","419ee579":"<a id = \"sec-2b\"> <\/a>\n### Drop the irrelavant columns","86eca442":"<a id = \"sec-one\"> <\/a>\n## Reading the Datasets","37c2673d":"<a id = \"sec-three\"> <\/a>\n## Text Preprocessing","7cee036d":"<a id = \"sec-5b\"> <\/a>\n### Random Forest Model","c2289411":"<a id = \"sec-four\"> <\/a>\n## Sentiment Analysis using NLTK","588f69a9":"<a id = \"sec-2b\"> <\/a>\n### Check for Null Values","bff68f58":"<a id = \"sec-5c\"> <\/a>\n### Random Forest Model with Grid Search","221367d3":"<a id = \"sec-five\"> <\/a>\n## Classification Models\n1. [TF-IDF Vectorization](#sec-5a)\n2. [Random Forest Model](#sec-5b)\n3. [Random Forest Model with Grid Search](#sec-5c)","4a2ee994":"<a id = \"sec-two\"> <\/a>\n## Exploring the Datasets"}}