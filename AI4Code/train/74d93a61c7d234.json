{"cell_type":{"b1535586":"code","ed43a289":"code","df713d9a":"code","65ab245a":"code","238a03ca":"code","196bdf65":"code","ab686e4c":"code","5276ee51":"code","4a215021":"code","2bec1079":"code","6cb88308":"code","df6f99bc":"code","5a4d3006":"code","b7f38cda":"code","47059a7c":"code","e885f0bc":"code","c2417520":"code","d24dda7c":"code","cbbae25a":"code","7cd9a390":"markdown","1c09331d":"markdown"},"source":{"b1535586":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed43a289":"import pandas as pd\nimport re\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = stopwords.words('english')","df713d9a":"df = pd.read_excel(r\"\/kaggle\/input\/challenge-dataset\/Challenge_File.xlsx\")\n\nprint(df.head())\nprint(df.shape)\nprint(df.columns)","65ab245a":"print((df.isnull()==True).sum())\nprint(df['Determination Result'].value_counts())\nprint(df[\"Determination Result\"].unique())","238a03ca":"financial_firm_list = ['lender','agency','broker','financial','financial firm','debt collector','forex dealer','smsf','recovery agent','company', 'fsp', 'life insurer','firm','service provider', 'financial firms', 'cfd provider', 'bank', 'trustee', 'credit union', 'services provider','trustee and insurer','insurer','facilitator','payment provider','creditor','insurer and the trustee','collection agency','trustee and the insurer', 'credit provider', 'fs', 'issuer', 'platform provider', 'firms','stockbroker', 'crb', 'opsp', 'assignee']\ncomplainant_list = ['complainant','customer', 'complainants','applicants', 'applicant']\nboth_decision = ['both parties','fsp \/ applicant','both bank and complainant', 'both life insurer and complainant', 'both fsp and applicant']\nDict = {0: financial_firm_list, 1: both_decision, 2: complainant_list}\n\ndef target_labels(label):\n    for key in Dict.keys():\n        if label.lower() in Dict[key]:\n            return key\n    else:\n        return 4\n        \ndf[\"target labels\"] = df[\"Determination Result\"].apply(target_labels)\n\ndf[\"target labels\"].value_counts()","196bdf65":"abbreviations = { \n    \"$\" : \" dollar \",    \"\u20ac\" : \" euro \",    \"4ao\" : \"for adults only\",    \"a.m\" : \"before midday\",    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",    \"acct\" : \"account\",    \"adih\" : \"another day in hell\",    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",    \"asap\" : \"as soon as possible\",    \"asl\" : \"age, sex, location\",    \"atk\" : \"at the keyboard\",    \"ave.\" : \"avenue\",    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\",     \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",    \"b4n\" : \"bye for now\",    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",    \"bak\" : \"back at keyboard\",    \"bbbg\" : \"bye bye be good\",    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",    \"bbl\" : \"be back later\",    \"bbs\" : \"be back soon\",    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",    \"blvd\" : \"boulevard\",    \"bout\" : \"about\",    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",    \"brt\" : \"be right there\",    \"bsaaw\" : \"big smile and a wink\",    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",    \"c\/o\" : \"care of\",    \"cet\" : \"central european time\",    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",    \"csl\" : \"can not stop laughing\",    \"cu\" : \"see you\",    \"cul8r\" : \"see you later\",    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",    \"cya\" : \"see you\",    \"cyt\" : \"see you tomorrow\",    \"dae\" : \"does anyone else\",    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",    \"dm\" : \"direct message\",    \"dwh\" : \"during work hours\",    \"e123\" : \"easy as one two three\",    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",    \"embm\" : \"early morning business meeting\",    \"encl\" : \"enclosed\",    \"encl.\" : \"enclosed\",    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",    \"fawc\" : \"for anyone who cares\",    \"fb\" : \"facebook\",    \"fc\" : \"fingers crossed\",    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\",     \"ft.\" : \"feet\",    \"ft\" : \"featuring\",    \"ftl\" : \"for the loss\",    \"ftw\" : \"for the win\",    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",    \"g9\" : \"genius\",    \"gahoy\" : \"get a hold of yourself\",    \"gal\" : \"get a life\", \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",    \"gg\" : \"good game\",    \"gl\" : \"good luck\",    \"glhf\" : \"good luck have fun\",    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",    \"gn\" : \"good night\",    \"g.o.a.t\" : \"greatest of all time\",    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",    \"gps\" : \"global positioning system\",    \"gr8\" : \"great\",    \"gratz\" : \"congratulations\",    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",    \"hp\" : \"horsepower\",    \"hr\" : \"hour\",    \"hrh\" : \"his royal highness\",    \"ht\" : \"height\",    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",    \"icq\" : \"i seek you\",    \"icymi\" : \"in case you missed it\",    \"idc\" : \"i do not care\",    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",    \"idk\" : \"i do not know\",    \"ie\" : \"that is\",    \"i.e\" : \"that is\",    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",    \"iirc\" : \"if i remember correctly\",    \"ilu\" : \"i love you\",    \"ily\" : \"i love you\",    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",    \"imu\" : \"i miss you\",    \"iow\" : \"in other words\",    \"irl\" : \"in real life\",    \"j4f\" : \"just for fun\",    \"jic\" : \"just in case\",    \"jk\" : \"just kidding\",    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",    \"lb\" : \"pound\",    \"lbs\" : \"pounds\",    \"ldr\" : \"long distance relationship\",    \"lmao\" : \"laugh my ass off\",    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",    \"ltd\" : \"limited\",    \"ltns\" : \"long time no see\",    \"m8\" : \"mate\",    \"mf\" : \"motherfucker\",    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",    \"mofo\" : \"motherfucker\",    \"mph\" : \"miles per hour\",    \"mr\" : \"mister\",    \"mrw\" : \"my reaction when\",    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",    \"nagi\" : \"not a good idea\",    \"nbc\" : \"national broadcasting company\",    \"nbd\" : \"not big deal\",    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",    \"nhs\" : \"national health service\",    \"nrn\" : \"no reply necessary\",    \"nsfl\" : \"not safe for life\",    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",    \"nvr\" : \"never\",    \"nyc\" : \"new york city\",    \"oc\" : \"original content\",    \"og\" : \"original\",    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",    \"omdb\" : \"over my dead body\",   \"omg\" : \"oh my god\",    \"omw\" : \"on my way\",    \"p.a\" : \"per annum\",    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",    \"poc\" : \"people of color\",    \"pov\" : \"point of view\",    \"pp\" : \"pages\",    \"ppl\" : \"people\",    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",    \"pt\" : \"point\",    \"ptb\" : \"please text back\",    \"pto\" : \"please turn over\",    \"qpsa\" : \"what happens\",     \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",    \"rlrt\" : \"real life retweet\",     \"rofl\" : \"rolling on the floor laughing\",    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",    \"rt\" : \"retweet\",    \"ruok\" : \"are you ok\",    \"sfw\" : \"safe for work\",     \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",    \"sq\" : \"square\",    \"srsly\" : \"seriously\",     \"ssdd\" : \"same stuff different day\",    \"tbh\" : \"to be honest\",    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",    \"tfw\" : \"that feeling when\",    \"thks\" : \"thank you\",    \"tho\" : \"though\",    \"thx\" : \"thank you\",    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",    \"tl;dr\" : \"too long i did not read\",    \"tldr\" : \"too long i did not read\",    \"tmb\" : \"tweet me back\",    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",    \"u\" : \"you\",    \"u2\" : \"you too\",    \"u4e\" : \"yours for ever\",    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",    \"w\/o\" : \"without\",    \"w8\" : \"wait\",    \"wassup\" : \"what is up\",    \"wb\" : \"welcome back\",    \"wtf\" : \"what the fuck\",    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",    \"wuf\" : \"where are you from\",    \"wuzup\" : \"what is up\",    \"wywh\" : \"wish you were here\",    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",    \"ynk\" : \"you never know\",    \"zzz\" : \"sleeping bored and tired\"\n}\n\nContraction = {\n    \"isn't\": \"is not\",    \"doesn't\": \"does not\",    \"don't\": \"do not\",    \"can't've\": \"cannot have\",    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",    \"cause\": \"because\",    \"could've\": \"could have\",    \"couldn't\": \"could not\",    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",    \"hadn't\": \"had not\",    \"hadn't've\": \"had not have\",    \"hasn't\": \"has not\",    \"haven't\": \"have not\",\n    \"I'll've\": \"I will have\",    \"I'm\": \"I am\",    \"I've\": \"I have\",    \"i'd\": \"i would\",    \"he'd\": \"he would\",    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",    \"he'll've\": \"he he will have\",    \"he's\": \"he is\",    \"how'd\": \"how did\",    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",    \"how's\": \"how is\",    \"I'd\": \"I would\",    \"I'd've\": \"I would have\",    \"I'll\": \"I will\",    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",    \"i'll've\": \"i will have\",    \"i'm\": \"i am\",    \"i've\": \"i have\",    \"isn't\": \"is not\",    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",    \"it'll\": \"it will\",    \"it'll've\": \"it will have\",    \"it's\": \"it is\",    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",    \"mayn't\": \"may not\",    \"might've\": \"might have\",    \"mightn't\": \"might not\",    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",    \"mustn't\": \"must not\",    \"mustn't've\": \"must not have\",    \"needn't\": \"need not\",    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",    \"oughtn't\": \"ought not\",    \"oughtn't've\": \"ought not have\",    \"shan't\": \"shall not\",    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",    \"she'd\": \"she would\",    \"she'd've\": \"she would have\",    \"she'll\": \"she will\",    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",    \"should've\": \"should have\",    \"shouldn't\": \"should not\",    \"shouldn't've\": \"should not have\",    \"so've\": \"so have\",\n    \"so's\": \"so as\",    \"that'd\": \"that would\",    \"that'd've\": \"that would have\",    \"that's\": \"that is\",    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",    \"there's\": \"there is\",    \"they'd\": \"they would\",    \"they'd've\": \"they would have\",    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",    \"they're\": \"they are\",    \"they've\": \"they have\",    \"to've\": \"to have\",    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",    \"we'd've\": \"we would have\",    \"we'll\": \"we will\",    \"we'll've\": \"we will have\",    \"we're\": \"we are\",\n    \"we've\": \"we have\",    \"weren't\": \"were not\",    \"what'll\": \"what will\",    \"what'll've\": \"what will have\",    \"what're\": \"what are\",\n    \"what's\": \"what is\",    \"what've\": \"what have\",    \"when's\": \"when is\",    \"when've\": \"when have\",    \"where'd\": \"where did\",\n    \"where's\": \"where is\",    \"where've\": \"where have\",    \"who'll\": \"who will\",    \"who'll've\": \"who will have\",    \"who's\": \"who is\",\n    \"who've\": \"who have\",    \"why's\": \"why is\",    \"why've\": \"why have\",    \"will've\": \"will have\",    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",    \"would've\": \"would have\",    \"wouldn't\": \"would not\",    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",    \"y'all'd\": \"you all would\",    \"y'all'd've\": \"you all would have\",    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",    \"you'd\": \"you would\",    \"you'd've\": \"you would have\",    \"you'll\": \"you will\",    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",    \"you've\": \"you have\"\n}","ab686e4c":"def preprocess(text):\n    t=[]\n    words=str(text).split()\n    no_contraction = [Contraction[w.lower()] if w.lower() in Contraction.keys() else w.lower() for w in words]\n    no_abbreviation = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in no_contraction]\n    new_str = re.sub(r'[^a-zA-Z ]+','', ' '.join(no_abbreviation) )\n    words=word_tokenize(new_str)\n    text = [w.lower() for w in words if not w in stop_words]\n    return ' '.join(text)\n\ndf['text']=df['Determination'].apply(preprocess)\n\n\ndef remove_stopwords(text):\n    t=[]\n    words=word_tokenize(text)\n    text = [w.lower() for w in words if not w in stop_words ]\n    return ' '.join(text)\n\ndf['text']=df['text'].apply(remove_stopwords)","5276ee51":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical","4a215021":"tokenizer=Tokenizer()\ntokenizer.fit_on_texts(df['text'])\nprint('number of total words:',len(tokenizer.word_index))","2bec1079":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['text'], df['target labels'], test_size=0.2, random_state=0)","6cb88308":"length=500\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train,maxlen=length)\n\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test,maxlen=length)\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","df6f99bc":"x = df['text']\ny = df['target labels']","5a4d3006":"length=500\nx = tokenizer.texts_to_sequences(x)\nx = pad_sequences(x,maxlen=length)\n\ny = to_categorical(y)","b7f38cda":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,Embedding,Dropout,Bidirectional\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.optimizers import Adam","47059a7c":"model=Sequential()\nmodel.add(Embedding(len(tokenizer.word_index)+1,output_dim=64, trainable=True,input_length=length))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(5,activation='softmax'))\n\nmodel.summary()","e885f0bc":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\naccuracy=model.fit(X_train,y_train,epochs=10,batch_size=256,validation_split=0.1)","c2417520":"y_predict = model.predict(X_test)","d24dda7c":"import numpy as np\ny_predict = np.argmax(y_predict, axis=1)\ny_test = np.argmax(y_test, axis = 1)","cbbae25a":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_predict, y_test))\nprint(confusion_matrix(y_predict, y_test))","7cd9a390":"**text to number**","1c09331d":"**model building**"}}