{"cell_type":{"0bbb01c9":"code","435f98ae":"code","e075bd92":"code","a1b5afb1":"code","1a729417":"code","157e1a0b":"code","ad7815b6":"code","8e5c420a":"code","e51f9227":"code","6a55bed7":"code","be26fbe7":"code","9ae0e2bf":"code","bfe837ad":"code","98204d7a":"code","de5ad24a":"code","5e7a650a":"code","d7ab6325":"code","efeee1f5":"code","2de893ed":"code","ab0a1cbe":"code","8522b58e":"code","f8ad296d":"code","28fe32de":"markdown","0193228a":"markdown","98bde475":"markdown","9010af8c":"markdown","2c97c949":"markdown","22f3942b":"markdown","e4e10f85":"markdown","ea83733b":"markdown","a4c27df8":"markdown","3a4c7358":"markdown","723a1926":"markdown","7b9997d5":"markdown","7cf65723":"markdown"},"source":{"0bbb01c9":"import numpy as np\nimport pandas as pd\n\n# Plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Feature Engineering\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.feature_selection import mutual_info_classif\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\n# Neural Network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\n# Cross-Validation\nfrom sklearn.model_selection import StratifiedKFold","435f98ae":"train_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","e075bd92":"train_data.head()","a1b5afb1":"train_data.describe()","1a729417":"# Get train data without the target and ids\nX = train_data.iloc[:, 1:-1].copy()\n# Get the target\ny = train_data.target.copy()\n# Create test X, drop ids.\ntest_X = test_data.iloc[:, 1:].copy()\n\n# It takes time to handle all of the data.\n# So, I am using a smaller portion of the data\n# while debugging\/testing.\n#X = train_data.iloc[0:10000, 1:-1].copy()\n#y = train_data.target[0:10000].copy()\n#test_X = test_data.iloc[0:10000, 1:].copy()","157e1a0b":"def make_mi_scores(mi_scores, X, y):\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\")\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","ad7815b6":"mi_scores = mutual_info_classif(X, y)\nmi_scores_classif = make_mi_scores(mi_scores, X, y)","8e5c420a":"plt.figure(dpi=100, figsize=(20, 16))\nplot_mi_scores(mi_scores_classif[mi_scores_classif > 1e-4])","e51f9227":"selected_features = mi_scores_classif[mi_scores_classif > 1e-4].index.tolist()\nselected_features = [f'f{feature}' for feature in selected_features]\nprint(f\"Selected Features: {selected_features}\")","6a55bed7":"# The number 2 is just a threshold to split\ndata = X[selected_features].copy()\nh_skew = data.loc[:,data.skew() >= 2].columns  # with Skewed\nl_skew = data.loc[:,data.skew() < 2].columns   # Bimodal","be26fbe7":"# Skewed distrubutions\nX['median_h'] = X[h_skew].median(axis=1)\ntest_X['median_h'] = test_X[h_skew].median(axis=1)\n\nX['var_h'] = X[h_skew].var(axis=1)\ntest_X['var_h'] = test_X[h_skew].var(axis=1)\n\n# Bimodal distributions\nX['mean_l'] = X[l_skew].mean(axis=1)\ntest_X['mean_l'] = test_X[l_skew].mean(axis=1)\n\nX['std_l'] = X[l_skew].std(axis=1)\ntest_X['std_l'] = test_X[l_skew].std(axis=1)\n\nX['median_l'] = X[l_skew].median(axis=1)\ntest_X['median_l'] = test_X[l_skew].median(axis=1)\n\nX['skew_l'] = X[l_skew].skew(axis=1)\ntest_X['skew_l'] = test_X[l_skew].skew(axis=1)\n\nX['max_l'] = X[l_skew].max(axis=1)\ntest_X['max_l'] = test_X[l_skew].max(axis=1)\n\nX['var_l'] = X[l_skew].var(axis=1)\ntest_X['var_l'] = test_X[l_skew].var(axis=1)","9ae0e2bf":"# Scaling and Nomalization\ntransformer_high_skew = make_pipeline(\n    StandardScaler(), \n    MinMaxScaler(feature_range=(0, 1))\n)\n\ntransformer_low_skew = make_pipeline(\n    StandardScaler(),\n    MinMaxScaler(feature_range=(0, 1))\n)\n\nh_skew = X.loc[:, X.skew() >= 2].columns\nl_skew = X.loc[:, X.skew() < 2].columns\n\npreprocessor = make_column_transformer(\n    (transformer_high_skew, l_skew),\n    (transformer_low_skew, h_skew)\n)","bfe837ad":"# Set seeds\nmy_seed = 1\nnp.random.seed(my_seed)\ntf.random.set_seed(my_seed)","98204d7a":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001,           # Minimium amount of change to count as an improvement\n    patience=5,               # How many epochs to wait before stopping\n    restore_best_weights=True)","de5ad24a":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ReduceLROnPlateau\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.2,                # Factor by which the learning rate will be reduced\n    patience=5,                # Number of epochs with no improvement\n    min_lr=0.001)              # Lower bound on the learning rate","5e7a650a":"EPOCHS = 100\nBATCH_SIZE = 512\nN_SPLITS = 15\nCALLBACKS = [early_stopping]","d7ab6325":"model = keras.Sequential([\n    layers.Dense(100, activation='swish', input_shape=[X.shape[1]]),\n    layers.Dropout(0.3),\n    layers.Dense(64, activation='swish'),\n    layers.Dropout(0.3),\n    layers.Dense(32, activation='swish'),\n    layers.Dropout(0.3),\n    # For a binary classification function use sigmoid\n    layers.Dense(1, activation='sigmoid')])","efeee1f5":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['AUC'])","2de893ed":"from sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer","ab0a1cbe":"fold = 0\ntest_predictions = np.zeros(test_X.shape[0])\nskf = StratifiedKFold(n_splits=N_SPLITS, random_state=48, shuffle=True)\nscores = {fold:None for fold in range(skf.n_splits)}\nfor train_idx, test_idx in skf.split(X, y):\n    train_X, val_X = X.loc[train_idx], X.loc[test_idx]\n    train_y, val_y = y.loc[train_idx], y.loc[test_idx]\n\n    # Preprocessing\n    test  = test_X.copy()\n    \n    train_X = preprocessor.fit_transform(train_X)\n    val_X = preprocessor.transform(val_X)\n    test = preprocessor.transform(test)\n\n    # Model\n    history = model.fit(\n        train_X, train_y,\n        validation_data=(val_X, val_y),\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        callbacks=CALLBACKS,        # Put your callbacks in a list\n        verbose=0)                  # Turn off training log\n\n    scores[fold] = (history.history)\n    print(f\"Fold {fold + 1} \\t\\t AUC: {np.max(scores[fold]['val_auc'])}\")\n\n    # Get the average values from each fold to the prediction\n    test_predictions += model.predict(test, batch_size=BATCH_SIZE).reshape(1,-1)[0] \/ skf.n_splits\n    fold += 1\n\noverall_auc = [np.max(scores[fold]['val_auc']) for fold in range(skf.n_splits)]\nprint('Overall Mean AUC: ', np.mean(overall_auc))","8522b58e":"# Credits to https:\/\/www.kaggle.com\/mlanhenke\/tps-11-nn-baseline-keras?scriptVersionId=79830528\nfig, ax = plt.subplots(3, 5, figsize=(20, 15))\nax = ax.flatten()\n\nfor fold in range(skf.n_splits):\n    df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n    min_train = np.round(np.min(df_eval['train_loss']),5)\n    min_valid = np.round(np.min(df_eval['valid_loss']),5)\n    delta = np.round(min_valid - min_train,5)\n    \n    sns.lineplot(\n        x=df_eval.index,\n        y=df_eval['train_loss'],\n        label='train_loss',\n        ax = ax[fold]\n    )\n\n    sns.lineplot(\n        x=df_eval.index,\n        y=df_eval['valid_loss'],\n        label='valid_loss',\n        ax = ax[fold]\n    )\n    \n    ax[fold].set_ylabel('')\n    ax[fold].set_xlabel(f\"Fold {fold+1}\\nmin_train: {min_train}\\nmin_valid: {min_valid}\\ndelta: {delta}\", fontstyle='italic')\n\nsns.despine()","f8ad296d":"# Run the code to save predictions in the format used for competition scoring\noutput = pd.DataFrame({'id': test_data.id, 'target': test_predictions})\noutput.to_csv('submission.csv', index=False)\noutput","28fe32de":"# Importing Libraries and Loading datasets","0193228a":"The dataset is splitted according to the distributions of each column.  \nThis idea is taken from, https:\/\/www.kaggle.com\/javiervallejos\/simple-nn-with-good-results-tps-nov-21, https:\/\/www.kaggle.com\/adityasharma01\/simple-nn-tps-nov-21  \nThe difference is, that notebook is using all columns to create new basic columns while this notebook is creating those columns from selected features.","98bde475":"## Training","9010af8c":"## Model","2c97c949":"# Selected Features","22f3942b":"# Pre-processing","e4e10f85":"# Evaluation","ea83733b":"## Callbacks","a4c27df8":"# Feature Engineering\n","3a4c7358":"# Explore Data","723a1926":"## Mutual Information\n\nFor now, I am using mutual information so select some features.  \nThe reason is simple, I am reading some tutorials https:\/\/www.kaggle.com\/ryanholbrook\/mutual-information :) and I am trying to find a way that I can implement what I have learned.","7b9997d5":"# Modelling\n\n\nI will use my setup from https:\/\/www.kaggle.com\/sfktrkl\/tps-nov-2021-nn?scriptVersionId=80095054","7cf65723":"# Submission"}}