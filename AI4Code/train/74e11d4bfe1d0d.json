{"cell_type":{"ef594624":"code","dc198c7b":"code","a56ba66c":"code","fbf81d55":"code","c924db57":"code","c78ae592":"code","3cdf4755":"code","53d8df2e":"code","288dbffc":"code","24ecc3eb":"code","74d032cb":"code","9e156d36":"code","9823c15e":"code","b8063c58":"code","f8626db3":"code","9eb2a4d3":"code","ce48dee3":"code","1ecc02ea":"code","fa196cc4":"code","168b659a":"code","ab6c5d3a":"code","dd25d7d8":"code","7f0b61d2":"code","ed50c0cb":"code","b3b19dbb":"code","8c86c80d":"code","fe959eef":"code","5d3d609c":"code","081a29fe":"code","9777368d":"code","5b14c1a4":"code","74d9a9af":"code","53715732":"code","d22aeff2":"code","09d6a2bb":"code","e1941552":"code","5149d034":"code","6b2f3d88":"code","ba1acb7e":"code","5deb09da":"code","1f3328c9":"code","a00954b1":"code","178ceeb1":"code","1251aac7":"code","5945c31c":"code","b23efcbe":"code","f09d8c63":"code","3ffac03d":"code","0e47be72":"code","193db539":"code","e3fe4268":"code","4fef22e9":"code","43a1cf4b":"code","5e42028b":"code","1081989d":"code","08f7e178":"code","1ab5e6de":"code","99863e5e":"code","71a2dd86":"code","abd33f40":"code","3bbee4b0":"code","1cad0194":"code","2fffd19b":"code","34e8ebe2":"markdown","13a0afa7":"markdown","f12f6765":"markdown","84d32a68":"markdown","0d7dcb03":"markdown","a7e079cc":"markdown","07275db3":"markdown","83646696":"markdown","eb46cc0f":"markdown","e64d0a66":"markdown","683988bc":"markdown","35a3eac3":"markdown","752b313d":"markdown","e3dfcb0d":"markdown","35030db4":"markdown","52eed482":"markdown","d96474d7":"markdown","5d6aa8d8":"markdown","ea29046f":"markdown","cbaceb44":"markdown","7c798ccb":"markdown","34cb7e9e":"markdown","8a263397":"markdown","584d7e70":"markdown","33a8e981":"markdown","1f7eca79":"markdown","e848229b":"markdown","65400d2e":"markdown","04650344":"markdown","ee7777ba":"markdown","046cf696":"markdown","c05c96d1":"markdown","20b3e09b":"markdown","a01c8ed7":"markdown","ffb9f618":"markdown","61831398":"markdown","d40e301b":"markdown","5a0b846a":"markdown","bd5f78ae":"markdown"},"source":{"ef594624":"import numpy as np\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer ","dc198c7b":"df = pd.read_json(\"..\/input\/train.json\")\ntestset = pd.read_json(\"..\/input\/test.json\")","a56ba66c":"df.head()","fbf81d55":"testset.head()","c924db57":"df.isnull().sum()","c78ae592":"testset.isnull().sum()","3cdf4755":"df.cuisine.unique()","53d8df2e":"df.ingredients = df.ingredients.astype('str')\ntestset.ingredients = testset.ingredients.astype('str')","288dbffc":"df.ingredients[0]","24ecc3eb":"testset.ingredients[0]","74d032cb":"df.ingredients = df.ingredients.str.replace(\"[\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"]\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"'\",\" \")\ndf.ingredients = df.ingredients.str.replace(\",\",\" \")","9e156d36":"testset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\",\",\" \")","9823c15e":"df.ingredients[0]","b8063c58":"testset.ingredients[0]","f8626db3":"df.ingredients = df.ingredients.str.lower()\ntestset.ingredients = testset.ingredients.str.lower()","9eb2a4d3":"df.ingredients = df.ingredients.apply(lambda x: word_tokenize(x))\ntestset.ingredients = testset.ingredients.apply(lambda x: word_tokenize(x))","ce48dee3":"lemmatizer = WordNetLemmatizer()","1ecc02ea":"def lemmat(wor):\n    l = []\n    for i in wor:\n        l.append(lemmatizer.lemmatize(i))\n    return l","fa196cc4":"df.ingredients = df.ingredients.apply(lemmat)\ntestset.ingredients = testset.ingredients.apply(lemmat)","168b659a":"df.ingredients[0]","ab6c5d3a":"testset.ingredients[0]","dd25d7d8":"type(df.ingredients[0])","7f0b61d2":"df.ingredients = df.ingredients.astype('str')\ndf.ingredients = df.ingredients.str.replace(\"[\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"]\",\" \")\ndf.ingredients = df.ingredients.str.replace(\"'\",\" \")\ndf.ingredients = df.ingredients.str.replace(\",\",\" \")","ed50c0cb":"testset.ingredients = testset.ingredients.astype('str')\ntestset.ingredients = testset.ingredients.str.replace(\"[\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"]\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\"'\",\" \")\ntestset.ingredients = testset.ingredients.str.replace(\",\",\" \")","b3b19dbb":"type(df.ingredients[0])","8c86c80d":"df.ingredients[0]","fe959eef":"#vect = HashingVectorizer ()\nvect = TfidfVectorizer()","5d3d609c":"features = vect.fit_transform(df.ingredients)","081a29fe":"features","9777368d":"#vect.get_feature_names()","5b14c1a4":"testfeatures = vect.transform(testset.ingredients)","74d9a9af":"testfeatures","53715732":"encoder = LabelEncoder()\nlabels = encoder.fit_transform(df.cuisine)","d22aeff2":"X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)","09d6a2bb":"print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","e1941552":"#logreg = LogisticRegression(C=10,solver='lbfgs', multi_class='multinomial',max_iter=400)\n#logreg.fit(X_train,y_train)","5149d034":"#print(\"Logistic Regression accuracy\",logreg.score(X_test, y_test))","6b2f3d88":"#logreg.predict(X_test)","ba1acb7e":"#from sklearn import linear_model\n#sgd = linear_model.SGDClassifier()\n#sgd.fit(X_train, y_train)","5deb09da":"#print(\"SGD classifier accuracy\",sgd.score(X_test, y_test))","1f3328c9":"#from sklearn.svm import LinearSVC\n#linearsvm = LinearSVC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False, max_iter = 1500)\n#linearsvm.fit(X_train, y_train)","a00954b1":"#print(\"Linear SVM accuracy\", linearsvm.score(X_test, y_test))","178ceeb1":"#labelsNN = df.cuisine","1251aac7":"#labelsNN = pd.get_dummies(labelsNN)","5945c31c":"#labelsNN = labelsNN.values","b23efcbe":"#labelsNN[0]","f09d8c63":"#from scipy.sparse import csr_matrix\n#sparse_dataset = csr_matrix(features)\n#featuresNN = sparse_dataset.todense()","3ffac03d":"#featuresNN[0]","0e47be72":"#X_trainNN, X_testNN, y_trainNN, y_testNN = train_test_split(featuresNN, labelsNN, test_size=0.2)","193db539":"#print(X_trainNN.shape, X_testNN.shape, y_trainNN.shape, y_testNN.shape)","e3fe4268":"#numfeat = X_trainNN.shape[1]","4fef22e9":"#import keras\n#from keras.layers import *","43a1cf4b":"#model = keras.models.Sequential()\n#model.add(Dense(300,input_dim = numfeat,activation = 'relu'))\n#model.add(Dense(500,activation = 'relu'))\n#model.add(Dense(400,activation = 'relu'))\n#model.add(Dense(20,activation='softmax'))\n#model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['categorical_accuracy'])\n#model.fit(X_trainNN,y_trainNN,epochs=50,shuffle=True, verbose =2,batch_size=500)","5e42028b":"#print(\"Accuracy with KERAS\" ,model.evaluate(X_testNN,y_testNN)[1])","1081989d":"#linearsvmfinal = LinearSVC(C=1.0,random_state=0,multi_class='crammer_singer',dual = False, max_iter = 1500)\n#linearsvmfinal.fit(features,labels)","08f7e178":"import lightgbm as lgb","1ab5e6de":"gbm = lgb.LGBMClassifier(objective=\"mutliclass\",n_estimators=10000,num_leaves=512)\ngbm.fit(X_train,y_train,verbose = 300)","99863e5e":"pred = gbm.predict(testfeatures)","71a2dd86":"#pred = linearsvmfinal.predict(testfeatures)","abd33f40":"predconv = encoder.inverse_transform(pred)","3bbee4b0":"sub = pd.DataFrame({'id':testset.id,'cuisine':predconv})","1cad0194":"output = sub[['id','cuisine']]","2fffd19b":"output.to_csv(\"outputfile.csv\",index = False)","34e8ebe2":"Lets vectorize our testset as well, we only tranform it with already fitted model","13a0afa7":"I have tried both Keras and tensorflow (Of course the backend is same), but Keras code looks simpler and clear.","f12f6765":"Observe that olives converted to olive, tomatoes to tomato etc, many words are now in their root form.","84d32a68":"Lemmatization converted it back to list, so change to str again and remove the unncessary words.","0d7dcb03":"Lets visualize some random features.","a7e079cc":"Our labels are ready, now we need the features, we have already created the features above but it was sparse matrix, which neural network doesnt like, so convert to dense arrays.","07275db3":"# What's cooking kernel !","83646696":"I have trained with KERAS on my pc for few times and achieved max accuracy of 0.81.","eb46cc0f":"Check for any null values.","e64d0a66":"Lets LEMMATIZE the data now (Since i believe that dataset might have different representation of same words, like the olives and olive, tomatoes and tomato, which represent the same word)","683988bc":"# NOTES:\n1) You can achieve better accuracy by ensembling the model, i will update this very soon.\n2) Neural Network has even scored an accuracy of 0.81 but the computation is very time taking.\n3) I have not used my time on visualizing the dataset.(which is not needed for this submission).\n4) Please comment for any questions, doubts or suggestions.\n\n THANK YOU\n \n# please UPVOTE, if you like.","35a3eac3":"# Data Modeling","752b313d":"Lets remove those unnecessary symbols, which might be problem when tokenizing and lemmatizing","e3dfcb0d":"Now our data looks good for vectorization.","35030db4":"Load the dataset","52eed482":"Convert it to one hot formatting, there are many ways to do, i prefer to do this way.","d96474d7":"A sequential NN with 300,500 and 400 nodes in first,second and third layers resp.","5d6aa8d8":"# KERAS","ea29046f":"Now, we have achieved almost similar accuracies in all the above models, I dont prefer NN's on this data as it is computationally very expensive.","cbaceb44":"Convert the ingredients to string.","7c798ccb":"# NEURAL NETWORK'S","34cb7e9e":"Lets split the dataset into training and testing parts","8a263397":"So, now our features has 2826 features, which are created by the process of vectorization.","584d7e70":"Lets TOKENIZE the data now. (the processing of splitting into individual words)","33a8e981":"Now, lets try our luck with neural networks.","1f7eca79":"Convert everything to lower ( I think they are already in lower case, but to be on safe side).","e848229b":"Here's how the features look like.","65400d2e":"# PREDICTION","04650344":"For Neural Networks we need to have the dense array's as inputs and preferably one hot encoding for lables.\nSo, lets create lables.","ee7777ba":"Check different types of cuisines","046cf696":"I prefer just using the logisticRegression or linearsvm for predictions, but linearSVC also has almost same results.\nI'm not predict using Keras or Tensorflow, since it needs an extra two steps to convert the labels, which I dont want to waste my time on.","c05c96d1":"# END","20b3e09b":"Split the dataset.","a01c8ed7":"Lets create our labels now, which is obviously cuisine column. Lets labelencode it so that they convert to numerical lables, which usually might give better prediction results. Not a necessary step tho","ffb9f618":"The loss is categorical cross entropy and the optimizer is adam with default learning rate.\nWe can tweak a lot of parameters like the no of nodes, epochs, batchsize etc to improve accuracy.","61831398":"# Text Data processing","d40e301b":"Here's how the one hot encoding looks like.","5a0b846a":"Check the shapes, to make sure.","bd5f78ae":"Convert it to arrays, you can do by values method or np.array() both are same"}}