{"cell_type":{"673d79f8":"code","4a31b448":"code","0b7e3d21":"code","ab2a28fc":"code","bef08bfd":"code","3862bf2c":"code","b1a13423":"code","95196447":"code","ee17ab9e":"code","8cfb0fbc":"code","dcbc60de":"code","d6fb7c3a":"code","f1a7199d":"code","da78756c":"code","b27c88b8":"code","19f43dc1":"code","e6489d68":"code","b87e4f33":"code","75f5a820":"code","c92b1baa":"code","f4bc440d":"code","74301471":"code","c794ad50":"code","e4a13a90":"code","3ba3a0b5":"code","c04f5904":"code","025c02f6":"code","2b5a4a40":"code","642397b4":"code","66d27a0a":"code","233a1771":"code","5e73e16d":"code","b30041d1":"code","3ae76f89":"code","8fe50d59":"code","bc341646":"code","cfa84b3c":"code","bd9881fd":"code","d8d2edd6":"code","b9adca8b":"code","1da0a598":"code","00523d35":"code","99244af1":"code","7b49edbc":"code","a368d09c":"code","8a723dcd":"markdown"},"source":{"673d79f8":"# importing necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport os\n","4a31b448":"data = pd.read_csv(\"..\/input\/heartbeat\/mitbih_train.csv\", header=None)\ndf = pd.DataFrame(data)","0b7e3d21":"df.head()","ab2a28fc":"df.isnull().sum()","bef08bfd":"#df.value_counts()","3862bf2c":"classes = []\nsns.countplot(x=187, data = df) ","b1a13423":"class_1 = df[df[187]==1.0]\nclass_2 = df[df[187]==2.0]\nclass_3 = df[df[187]==3.0]\nclass_4 = df[df[187]==4.0]\nclass_0 = df[df[187]==0.0].sample(n = 8000)","95196447":"new_df = pd.concat([class_0, class_1, class_2, class_3, class_4])","ee17ab9e":"new_df.head()","8cfb0fbc":"df.tail()","dcbc60de":"sns.countplot(x=187, data = new_df) ","d6fb7c3a":"index = 0\n\nfig, ax = plt.subplots(nrows = 1, ncols = 5, figsize=(25,6))\n\nfor i in range(5):\n  ax[i].plot(new_df[new_df[187]==float(i)].sample(1).iloc[0,:186])\n  ax[i].set_title('Class: '+str(i))\n","f1a7199d":"#now lets split data in test train pairs\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(new_df.drop([187], axis=1), new_df[187], test_size = 0.1)","da78756c":"X_train = np.array(X_train).reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test = np.array(X_test).reshape(X_test.shape[0], X_test.shape[1], 1)","b27c88b8":"from keras.utils.np_utils import to_categorical\n\ny_train=keras.utils.np_utils.to_categorical(y_train)\ny_test=keras.utils.np_utils.to_categorical(y_test)","19f43dc1":"from tensorflow.keras import Sequential,utils\nfrom tensorflow.keras.layers import Flatten, Dense, Conv1D, MaxPool1D, Dropout , Conv2D","e6489d68":"model = Sequential()\n\nmodel.add(Conv1D(filters=32, kernel_size=(3,), padding='same', activation='relu', input_shape = (X_train.shape[1],1)))\nmodel.add(Conv1D(filters=64, kernel_size=(3,), padding='same', activation='relu')) \nmodel.add(Conv1D(filters=128, kernel_size=(5,), padding='same', activation='relu'))    \n\nmodel.add(MaxPool1D(pool_size=(3,), strides=2, padding='same'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units = 512, activation='relu'))\nmodel.add(Dense(units = 1024, activation='relu'))\n\nmodel.add(Dense(units = 5, activation='softmax'))\n\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","b87e4f33":"callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]","75f5a820":"history=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50 , callbacks=callbacks, batch_size=32)\nmodel.load_weights('best_model.h5')","c92b1baa":"# Prediction\n\ny_pred = model.predict(X_test)","f4bc440d":"plt.figure(0)\n\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","74301471":"loss,accuracy=model.evaluate(X_test,y_test)\n\nprint('Test loss:', loss)\nprint('Test accuracy:', accuracy)\n","c794ad50":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nY_pred = model.predict(X_test)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_test,axis = 1) \n\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Oranges\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","e4a13a90":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_true, Y_pred_classes))","3ba3a0b5":"train_df=pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_train.csv',header=None)\ntest_df=pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_test.csv',header=None)","c04f5904":"train_df = train_df.sample(frac=1).reset_index(drop=True)\n\ntrain_df, val_df = train_test_split(train_df, test_size = 0.20)","025c02f6":"train_df[187]=train_df[187].astype(int)\nequilibre=train_df[187].value_counts()\nprint(equilibre)","2b5a4a40":"df=train_df.groupby(187,group_keys=False).apply(lambda train_df : train_df.sample(1))\ndf","642397b4":" plt.plot(df.iloc[0,:186])","66d27a0a":"train_df[187]=train_df[187].astype(int)\nequilibre=train_df[187].value_counts()\nprint(equilibre)","233a1771":"plt.pie(equilibre, labels=['N','S','V','F','Q'], colors=['red','green','blue','skyblue','orange'],autopct='%1.1f%%',radius=1.5,textprops={'fontsize': 13})\nplt.show()","5e73e16d":"from sklearn.utils import resample\ndf_1=train_df[train_df[187]==1]\ndf_2=train_df[train_df[187]==2]\ndf_3=train_df[train_df[187]==3]\ndf_4=train_df[train_df[187]==4]\ndf_0=(train_df[train_df[187]==0]).sample(n=48483,random_state=42,replace=True)\n\ndf_1_upsample=resample(df_1,replace=True,n_samples=30000,random_state=123)\ndf_2_upsample=resample(df_2,replace=True,n_samples=30000,random_state=124)\ndf_3_upsample=resample(df_3,replace=True,n_samples=30000,random_state=125)\ndf_4_upsample=resample(df_4,replace=True,n_samples=30000,random_state=126)\n\ntrain_df=pd.concat([df_0,df_1_upsample,df_2_upsample,df_3_upsample,df_4_upsample])","b30041d1":"train_df[187]=train_df[187].astype(int)\nequilibre=train_df[187].value_counts()\nprint(equilibre)","3ae76f89":"df=train_df.groupby(187,group_keys=False).apply(lambda train_df : train_df.sample(1))\ndf","8fe50d59":"plt.plot(df.iloc[0,:186] )\nplt.plot(df.iloc[1,:186] )\nplt.plot(df.iloc[2,:186] )\nplt.plot(df.iloc[3,:186] )\nplt.plot(df.iloc[4,:186] )","bc341646":"train_target=train_df[187]\nY_train=to_categorical(train_target)\n\nval_target=val_df[187]\nY_val=to_categorical(val_target)\n\n\nX_train=train_df.iloc[:,:186].values\nX_val=val_df.iloc[:,:186].values\n\nX_train = X_train.reshape(len(X_train), X_train.shape[1],1)\nX_val = X_val.reshape(len(X_val), X_val.shape[1],1)","cfa84b3c":"test_target=test_df[187]\nY_test=to_categorical(test_target)\nX_test=test_df.iloc[:,:186].values\nX_test = X_test.reshape(len(X_test), X_test.shape[1],1)","bd9881fd":"import keras\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input,Dense, Convolution1D, MaxPool1D, Flatten, Dropout\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\ndef get_model():\n        im_shape=(X_train.shape[1],1)\n        inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n        \n\n        \n        x0 = keras.layers.Conv1D(128,8, activation='relu',input_shape=im_shape)(inputs_cnn)  \n        x0= keras.layers.BatchNormalization()(x0)\n        pool0=MaxPool1D(pool_size=(4), strides=(2), padding=\"same\")(x0)\n\n        \n\n        x1 = keras.layers.Conv1D(128,8, activation='relu',input_shape=im_shape)(pool0) \n        x1= keras.layers.BatchNormalization()(x1)\n        pool1=MaxPool1D(pool_size=(4), strides=(2), padding=\"same\")(x1)\n\n        \n        x2 = keras.layers.Conv1D(128,6, activation='relu',input_shape=im_shape)(pool1) \n        x2 = keras.layers.BatchNormalization()(x2)\n        pool2=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(x2)\n\n        \n        x3 = keras.layers.Conv1D(128,6, activation='relu',input_shape=im_shape)(pool2) \n        x3 = keras.layers.BatchNormalization()(x3)\n        pool3=MaxPool1D(pool_size=(3), strides=(2), padding=\"same\")(x3)\n\n        \n        x4 = keras.layers.Conv1D(128,4, activation='relu',input_shape=im_shape)(pool3) \n        x4 = keras.layers.BatchNormalization()(x4)\n        pool4=MaxPool1D(pool_size=(3), strides=(1), padding=\"same\")(x4)\n\n        \n        x5 = keras.layers.Conv1D(128,2, activation='relu',input_shape=im_shape)(pool4) \n        x5 = keras.layers.BatchNormalization()(x5)\n        pool5=MaxPool1D(pool_size=(3), strides=(1), padding=\"same\")(x5)\n\n        \n        x6 = keras.layers.Conv1D(128,2, activation='relu',input_shape=im_shape)(pool5) \n        x6 = keras.layers.BatchNormalization()(x6)\n        pool6=MaxPool1D(pool_size=(2), strides=(1), padding=\"same\")(x6)\n\n        \n        x7 = keras.layers.Conv1D(128,2, activation='relu',input_shape=im_shape)(pool6) \n        x7 = keras.layers.BatchNormalization()(x7)\n        pool7=MaxPool1D(pool_size=(2), strides=(1), padding=\"same\")(x7)\n    \n    \n        x = Flatten()(pool7)\n        x = keras.layers.BatchNormalization()(x)\n        \n        dense_1 = Dense(32, activation='relu')(x)\n        dense_2 = Dense(32, activation='relu')(dense_1)\n\n        out = keras.layers.Dense(5, activation='softmax')(dense_2)\n        return keras.Model(inputs=inputs_cnn, outputs=out)\n\n\n    ","d8d2edd6":"model = get_model()\n\n\n\nmodel.compile(optimizer = 'adam' ,loss= \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nmodel.summary()\n","b9adca8b":"callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]","1da0a598":"history=model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=50 , callbacks=callbacks, batch_size=526)\nmodel.load_weights('best_model.h5')","00523d35":"loss,accuracy=model.evaluate(X_val,Y_val)\n\nprint('Test loss:', loss)\nprint('Test accuracy:', accuracy)\n","99244af1":"plt.figure(0)\n\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","7b49edbc":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nY_pred = model.predict(X_test)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(Y_test,axis = 1) \n\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Oranges\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","a368d09c":"from sklearn.metrics import classification_report\n\nprint(classification_report(Y_true, Y_pred_classes))","8a723dcd":"## ECG Heartbeat Categorization\n\n> This dataset is composed of two collections of heartbeat signals derived from two famous datasets in heartbeat classification, the MIT-BIH Arrhythmia Dataset and The PTB Diagnostic ECG Database."}}