{"cell_type":{"7d31445c":"code","fa027f58":"code","089c83fe":"code","22b5cf05":"code","75492827":"code","f59ca090":"code","c0a6c42f":"code","b0e3b006":"code","14280fb5":"code","459ad95d":"code","cd920e56":"code","1718f7b2":"code","0ed4ab7c":"code","194fea57":"code","94812c74":"code","96a4ffc1":"code","89cec766":"code","f5752f46":"code","adf58841":"code","90707fa9":"code","6d82e88f":"code","f0f7e98e":"code","aef96dd7":"code","3a792079":"markdown","9f286267":"markdown"},"source":{"7d31445c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fa027f58":"# reading both training and test data\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","089c83fe":"df.head()","22b5cf05":"# columns dropped from both training and test data\ncols_to_drop = ['Name','PassengerId','Ticket','Cabin']\ndf_cleaned = df.drop(cols_to_drop,axis = 1)\ntest_cleaned = test_df.drop(cols_to_drop , axis = 1)","75492827":"# calculating missing values in training data\nprint(df_cleaned.isnull().sum())\nprint(print(test_cleaned.isnull().sum()))","f59ca090":"# missing values being imputed\ndf_cleaned.fillna(df_cleaned.mean(), inplace= True)\ndf_cleaned['Embarked'].fillna(df['Embarked'].mode()[0],inplace = True) \n\n\n# for test data\ntest_cleaned.fillna(test_cleaned.mean(), inplace= True)\n","c0a6c42f":"print(df_cleaned.isnull().sum())\nprint(test_cleaned.isnull().sum())","b0e3b006":"X = df_cleaned.drop('Survived' , axis = 1)\ny = df_cleaned['Survived']","14280fb5":"# dummy variables for both training and test data\nX = pd.get_dummies(X,columns=['Sex','Embarked'], drop_first= True)\ntest_cleaned = pd.get_dummies(test_cleaned,columns=['Sex','Embarked'] , drop_first=True)","459ad95d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Stratifying target variable to get balance in training and test data\n\nX_train,X_test,y_train,y_test = train_test_split(X , y , test_size = 0.30 , stratify = y)","cd920e56":"classifier = RandomForestClassifier()\nclassifier.fit(X_train,y_train)\ny_preds = classifier.predict(X_test)\nprint(accuracy_score(y_test,y_preds))","1718f7b2":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()\n    ]\nfor classifier in classifiers:\n    \n   \n    classifier.fit(X_train, y_train)   \n    y_preds = classifier.predict(X_test)\n    print(classifier)\n    \n    print(\"model score: %.3f\" % accuracy_score(y_test , y_preds))\n","0ed4ab7c":"classifier = GradientBoostingClassifier()\nclassifier.fit(X_train,y_train)\ny_preds = classifier.predict(X_test)\naccuracy = accuracy_score(y_test,y_preds)\naccuracy","194fea57":"importances = classifier.feature_importances_\n\nfeat_importances = pd.Series(classifier.feature_importances_, index=X.columns)\nfeat_importances.nlargest(8).plot(kind='barh')","94812c74":"import scipy\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\ncorr = np.round(scipy.stats.spearmanr(X).correlation, 4)\ncorr_condensed = scipy.spatial.distance.squareform(1 - corr)\nz = linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = dendrogram(z, labels=X.columns, orientation='left', leaf_font_size=16)\nplt.show()","96a4ffc1":"to_keep = ['Pclass', 'Age', 'SibSp', 'Fare', 'Sex_male','Embarked_S']\nX = X[to_keep]","89cec766":"X_train,X_test,y_train,y_test = train_test_split(X , y , test_size = 0.30 , stratify = y)\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    NuSVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()\n    ]\nfor classifier in classifiers:\n    \n   \n    classifier.fit(X_train, y_train)   \n    y_preds = classifier.predict(X_test)\n    print(classifier)\n    \n    print(\"model score: %.3f\" % accuracy_score(y_test , y_preds))","f5752f46":"parameters = {\n    \n    \"max_features\":['auto'],\n    'learning_rate': [0.01,0.03,0.05], #so called `eta` value\n    'max_depth': [3,5],\n    'min_samples_leaf': [2,3,],\n    'subsample': [0.4,0.7,0.9],\n    'n_estimators': [50,100,150]#number of trees, change it to 1000 for better results\n              \n\n              }","adf58841":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\n\nfrom sklearn.model_selection import GridSearchCV\nCV = GridSearchCV(gbc, parameters, cv = 5)","90707fa9":"CV.fit(X_train, y_train)  \npredictions = CV.predict(X_test)\naccuracy_score(y_test , predictions)\nprint(CV.best_params_)    \nprint(CV.best_score_)","6d82e88f":"test_cleaned = test_cleaned[to_keep]","f0f7e98e":"# predictions for test data\npredictions = CV.predict(test_cleaned)","aef96dd7":"# Save test predictions to file\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index = False)\nprint('Your submission was sucessfully saved!')","3a792079":"# The columns PassengerId,Ticket,Cabin to be dropped. Name column was also dropped to keep it simple.\n","9f286267":"This is a quick and simple random forset implementation(as Jeremy Howard from fastai says implement a random forst as soon as possible.) without much diggin deep. \n* Dropped few columns for example 'Name','PassengerId','Ticket','Cabin'\n* imputed missed values by mean and mode for categorical variables\n* performance comparison of different classification models\n* It achieves accuracy score of 78.6 on kaggle test data.\n* This is my first ever attempt please be easy on me. Any suggestions welcome.\n\n\n"}}