{"cell_type":{"4e87e22b":"code","0e64bd64":"code","c54fed6b":"code","6309ca50":"code","1574f1a7":"code","904c5243":"code","b03137ae":"code","294a31a4":"code","b453781c":"code","dcbb1fdb":"code","d8238166":"code","a2e10fe0":"code","cf461c92":"code","c6f72e1a":"code","bfdf31d5":"code","7b70067e":"code","a874d141":"code","49c8480d":"code","03b1930f":"code","fef17849":"code","86f4aa81":"code","5b28e5c9":"code","cd8c495f":"code","79a05280":"code","b4c0d19e":"code","2f4cb1c8":"code","717a1479":"code","ec882d8c":"markdown","f8f28cb7":"markdown","a5d0c6d8":"markdown","8dbe3b27":"markdown","ea6b9b66":"markdown","587afe52":"markdown","e642a049":"markdown","eda16cd1":"markdown","a4380fec":"markdown","769cb779":"markdown","92ea4aae":"markdown","38dc8b42":"markdown","6d46569e":"markdown","be0a2b97":"markdown","43c973d8":"markdown","067ae26b":"markdown","e766e1fe":"markdown","4c3be49f":"markdown","7ec65480":"markdown","a2df54a4":"markdown","e4992e69":"markdown","0eb13714":"markdown","1f205636":"markdown","a02be53e":"markdown"},"source":{"4e87e22b":"# Any results you write to the current directory are saved as output.\n# import necessary libraries.note that i will be importing necessary libraries if need all along\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport seaborn as sns#visualization\nsns.set(style=\"ticks\", color_codes=True)\nimport matplotlib.ticker as mtick # For specifying the axes tick format \nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization","0e64bd64":"#Import dataset into and display head of your dataset\n\nchurn = pd.read_csv('..\/input\/Churn_Modelling.csv')\nchurn.head()","c54fed6b":"#drop unnecessary attributes and display new dataset\nchurn = churn.drop([\"RowNumber\",\"CustomerId\",\"Surname\"], axis =1)\n","6309ca50":"#Describe how big is out dataset helps to understand how big will be our analysis and requirements.\nprint(\"Rows : \",churn.shape[0])\nprint(\"Columns  : \",churn.shape[1])","1574f1a7":"#I check if there is any NaN values that can bring biased scenario, all column attributes should return false to verify this \nchurn.isnull().any()","904c5243":"#count our unique values without duplication of same figure\n\nprint (\"\\nUnique values :  \\n\",churn.nunique())","b03137ae":"#what are our data types\nchurn.dtypes","294a31a4":"#Mean=> the are a lot of average calculations in statistics so i used mean the check the average possibility of attributtes to impact the situation\nchurn.groupby(['Exited']).mean()","b453781c":"#Let's convert all the categorical variables into dummy variables\ndf = pd.get_dummies(churn)\ndf.head()","dcbb1fdb":"plt.figure(figsize=(10,4))\ndf.corr()['Exited'].sort_values(ascending = False).plot(kind='bar')","d8238166":"plt.figure(figsize = (20,10))\nsns.heatmap((df.loc[:, ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary','Exited','Geography_France','Geography_Germany','Geography_Spain','Gender_Female','Gender_Male']]).corr(),\n            annot=True,linewidths=.5);","a2e10fe0":"# Passing labels and values\nlab = churn[\"Exited\"].value_counts().keys().tolist()\nval = churn[\"Exited\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  0.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .2\n              )\nlayout = go.Layout(dict(title = \"Customer churn\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","cf461c92":"#Categorical attirbutes churn rate\nfig, axs = plt.subplots(2, 2, figsize=(15, 8))\nsns.countplot(x= churn.Geography, hue = 'Exited' ,data=churn, ax =axs[0][0])\nsns.countplot(x=churn.Gender, hue = 'Exited' ,data=churn, ax=axs[1][0])\nsns.countplot(x=churn.HasCrCard, hue = 'Exited' ,data=churn, ax=axs[0][1])\nsns.countplot(x=churn.IsActiveMember, hue = 'Exited' ,data=churn, ax=axs[1][1])\nplt.ylabel('count')\n","c6f72e1a":"fig, axarr = plt.subplots(3, 2, figsize=(15, 8))\nsns.boxplot(y='CreditScore',x = 'Exited', hue = 'Exited',data = churn, ax=axarr[0][0])\nsns.boxplot(y='Age',x = 'Exited', hue = 'Exited',data = churn , ax=axarr[0][1])\nsns.boxplot(y='Tenure',x = 'Exited', hue = 'Exited',data = churn, ax=axarr[1][0])\nsns.boxplot(y='Balance',x = 'Exited', hue = 'Exited',data = churn, ax=axarr[1][1])\nsns.boxplot(y='NumOfProducts',x = 'Exited', hue = 'Exited',data = churn, ax=axarr[2][0])\nsns.boxplot(y='EstimatedSalary',x = 'Exited', hue = 'Exited',data = churn, ax=axarr[2][1])","bfdf31d5":"df['BalanceEstimatedSalaryRatio'] = df.Balance\/(df.EstimatedSalary)\ndf['TenureOverAge'] = df.Tenure\/(df.Age)\ndf['CreditScoreOverAge'] = df.CreditScore\/(df.Age)\ndf.head()\n","7b70067e":"con_v=['CreditScore',  'Age', 'Tenure', 'Balance','NumOfProducts', 'EstimatedSalary','BalanceEstimatedSalaryRatio','TenureOverAge','CreditScoreOverAge']\nminVec = df[con_v].min().copy()\nmaxVec = df[con_v].max().copy()\ndf[con_v] = (df[con_v]-minVec)\/(maxVec-minVec)\ndf.head()","a874d141":"# Create Train & Test Data\nfrom sklearn.model_selection import train_test_split\ny = df['Exited'].values\nx = df.drop(columns = ['Exited'])\nx_train, x_test, y_train, y_test = train_test_split(x, y,random_state=0)","49c8480d":"# Scaling all the variables to a range of 0 to 1\nfrom sklearn.preprocessing import MinMaxScaler\nfeatures = x.columns.values\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(x)\nx = pd.DataFrame(scaler.transform(x))\nx.columns = features","03b1930f":"# Running logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nmodel = LogisticRegression()\nresult = model.fit(x_train, y_train)\nprediction_test = model.predict(x_test)\nprint (metrics.accuracy_score(y_test, prediction_test))# Print the prediction accuracy","fef17849":"# getting the weights of all the variables on regression model\nweights = pd.Series(model.coef_[0],\n                 index=x.columns.values)\nweights.sort_values()[-13:].plot(kind = 'barh')\nweights.sort_values(ascending = False)\n","86f4aa81":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,\n                                  random_state =50, max_features = \"auto\",\n                                  max_leaf_nodes = 30)\nmodel_rf.fit(x_train, y_train)\n# Make predictions\nprediction_test = model_rf.predict(x_test)\nprobs = model_rf.predict_proba(x_test)\nprint (metrics.accuracy_score(y_test, prediction_test))# Print the prediction accuracy","5b28e5c9":"importances = model_rf.feature_importances_\nweights = pd.Series(importances,\n                 index=x.columns.values)\nweights.sort_values()[-13:].plot(kind = 'barh')\nweights.sort_values(ascending = False)","cd8c495f":"from sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nmodel.svm = SVC(kernel='linear') \nmodel.svm.fit(x_train,y_train)\npreds = model.svm.predict(x_test)\nmetrics.accuracy_score(y_test, preds)# Print the prediction accuracy","79a05280":"from sklearn.neighbors import KNeighborsClassifier\nclassifiers = [\n    KNeighborsClassifier(5),    \n]\n# iterate over classifiers\nfor item in classifiers:\n    classifier_name = ((str(item)[:(str(item).find(\"(\"))]))\n    print (classifier_name)\n    # Create classifier, train it and test it.\n    clf = item\n    clf.fit(x_train, y_train)\n    pred = clf.predict(x_test)\n    score = clf.score(x_test, y_test)\n    print (round(score,3),\"\\n\", \"- - - - - \", \"\\n\") # Print the prediction accuracy\n    \n","b4c0d19e":"# apply on Random forest body and we will directly display five first customers as per our model\nx_test[\"prob_true\"] = prediction_test\ndf_risky = x_test[x_test[\"prob_true\"] > 0.9]\ndisplay(df_risky.head()[[\"prob_true\"]])\n","2f4cb1c8":"df_risky.shape","717a1479":"df_risky.head()\n","ec882d8c":"**EDA (Exploratory Data Analysis)**","f8f28cb7":"Before exploring deep our dataset  we can see attributes correlation with churn as we have seen that mean results are not so informative, so i first change dummies varibles which are Geography and Gender in our dataset so that we can get to define correlation of attributes to churn way easy.","a5d0c6d8":"The above Mean calculation is one of the ways to understand the average of our data, this helps us to investigate any to note into our study whenever it is high or low measures it can noted so that all along the study helps us to avoid biases. the difference between Exited and un \nExited are not that remarkable to give us a big picture of our study so any case can appear. But to note that:\n\n* Customers with low creditscore tend to churn,reasonable!\n* on average older customers are the most to churn,questionable?\n* customers with high balance are churning probably they are getting attracted by other banks offer to raise the wealth and this corresponds with their estimatedsalary also\n* tenure,creditcard and being active mean are not explicitly helping in this case to hightlight anything big","8dbe3b27":"As said understanding the relation of things, people and business metrics all plays a big to understand those small obvious patterns  before the core exploratory phase,thats why  some of the attributes  have almost no impact into our results such as Surname,CustomerId and RowNumber as long as we have an index(counts) it will serve as an ID.","ea6b9b66":"**Support Vecor Machine (SVM)**","587afe52":"\nWhat does Categorical attributes Highlights:\n\n* Geographical location can determine the success of your business and can be a great tool to know how to play with your market as france show a huge number of customers with low churn.\n\n* Apparently it is possible that customers without credit card are churning and it is obvious that the ones with it are not churning much.\n\n* Female customers are churning than male this would be a factor of several things that can't be described without additional informations, also a great deal to consider gender so that the retention plan prepare promotions or offers based on affected gender,\n*  Not a suprise that inactive customers are churning than active ones","e642a049":"**Data Exploration**\n\nThis section is the core part of understanding the problem and channel late to right features, as said before we need to establish possible relations in our attributes and this is where to strongest part of trading off comes in to secure the best predictions.\n\nNote: **Exited ** will always play the role of a target","eda16cd1":"**Head to Head attributes relations**\n\nCategotical attributes\n and Continuous attributes\n       on impacting\n     Exited, here is not about the counts but a specifically churned and unchurned vs the service. ","a4380fec":"How huge are our attributes? in the meantime it is quite easier to spot that Geography,Gender,HasCrCard and IsActiveMember are categorical attributes that can corresponds to (yes\/no) or 1\/0 to define its state. so the rest of the attributes should be continuous attributes.","769cb779":"\n**Logistic Regrssion**","92ea4aae":"****Algorithms Modelling****\n\nThis study is task is a binary case study and obviosly as a supervised classification learning i would like to chose these models below, we will explore its results as we proceed.","38dc8b42":"**Predicting Customer Churn at a Bank Practice**\n\n\nIn This study we are going to do data mining with a bank dataset targeting customers churn problematic and i will be describing each and every step  with comments in the cell or out when needed.\n\nData science practice require much attention to get value of insights, it is based on unlocking hidden patterns or opportunities from a bunch of data accumulated over time, before we tackle the mining and predictions  as a data scientist i have to get a hint on 2 key points:\n\n1. Business understanding or domain knowledge:What cause a customer to churn by considering  relation of things and people towards the business\n2. Analytical approach: Break it into pieces to understand what is required in terms of data,understanding and modelling results with a specific approach.","6d46569e":"Let's check the exactitude correlation with figures ","be0a2b97":"**Random Forest**","43c973d8":"The above graphs gives a hint on our journey:\n* Age,\n* Geography_germany,\n*  Balance \n* gender_female \n* Estimatedsalary \n\nall seems to have high correlation with the churn, remember this is exploratory many factors play along to change the situation in prediction outcomes.\n\n","067ae26b":"We are going to create new features from what we have and based on the relationship of attributes and prepare the existing ones to be ready to predict our next client possibly to churn and these stage is normally standardize head to head attributes as decided,\nfor balance and Estmatedsalary this is quantitative relation then we will find its ratio and for tenure and creditscore over age .\n\nNote that i am using df dataframe because i have already converted categorical attributes to dummy variables in other to process it excellently.\n","e766e1fe":"Our dataset have 20.4% on churned customers this means we will try to predict , thats why i selected to use default est_train_split model which deliver 25% test set and 75% traing set.because it covers the churned figure which prevent biases inside the model itself.","4c3be49f":"**Feature Engineering and Preparation**","7ec65480":" As seen before the mean and correlation maps outcomes relate to these continuous results. but they are more detailed in terms of churned and unchurned distribution with estimated numbers .\n\nOver all conclusion is that all attributes have its impact to the performance for instance Tenure and CreditScore are functions of age the more you get older the more the relation of a customer and a bank become stronger as sign of loyalty, and of course balances depends most of the case by your salary. these attributes are going to help us to engineer more case scenario by brin up new features that will help to punish negativity into predictions.\n","a2df54a4":"\n**20.4 Exited,\n79.6% un Exited**\nThis is a loss over a certain time yet there is no clear factor for churning maybe it is the nature of the business better be alert. but also it gives hope because the default test dataset is always 25% which will help to punish gradually.","e4992e69":"* **Continous attributes churn rate**\n\nFor continuous attribute i will have to normalize its values in order to compare its churn it won't be possible to plot Balance and age in the same plot yet they have very different figures.","0eb13714":"**Model testing and validate on testing data**\n\nHere i will provide first customers to churn as per our model predictions and we provide a risky dataframe which include a column(pro_true) for those to probably churning in the near future based on our selected model.\n\n**After observing our models results Random forest beat the others!**","1f205636":"**Thank You.**","a02be53e":"**Conclusions**\n\nIn this study i have been dealing with a bank dataset with the aim to predict customers who may churn in the near future to prepare any retention plan after discovering patterns hidden in the dataset, at the beginning there was no clue of what it is happening because the patterns from mean and correlations map couldn't easily provide any clear insight thats where machine learning shows off what it can that our brains can't apparently. we have seen that churning customers in the past were 20,4% this is slightly not a trouble as long as it is low figure compared to the default sample of test data in model testing and this why i have taken 25% default set so that it can cover any biases behind.\n\nRandom forest beat other algorithms because it has the highest accuracy of 0.87 means 87%, referred to no free lunch theorem in machine learning that every machine learning is valid anyway it all depends on the situations and parameters passed to it to bring good results, in our case we can defend Random forest because it has shown its capabilities compared to others as long as we passed same data and same concept to all algorithms, on top of that random forest has a topology of multiple trees and this helps to avoid overfitting the model which is the main problem sometimes in machine learning. note that this model can perform way better if we feed it with more historical datasets this will help the model to capture many and necessary information to provide more accuracy in predictions."}}