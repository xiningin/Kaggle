{"cell_type":{"a7c65d77":"code","9b899dd4":"code","fcbf7900":"code","6f26bae5":"code","d36ac90a":"code","5b88aa97":"code","3dbe7dfb":"code","189c65b3":"code","68451f31":"code","7f529ef5":"code","ebaabb2c":"code","9e190658":"code","98872253":"code","dbe54cfa":"code","b6537496":"code","5dbca182":"code","6b667c9a":"code","da5d4b1d":"code","8531e74d":"code","7591bc1e":"code","2f13d7b5":"code","a74c2d60":"code","b5e1aedc":"code","14c4cffb":"code","399d0f5a":"code","87a52d91":"code","65378501":"code","d62e6663":"code","e6f42b34":"code","0ee70888":"code","4081544b":"code","48c3fced":"code","e4e84c47":"code","21eb5430":"code","a0277d41":"code","c36ca562":"code","1bd1cbd7":"markdown","827ef425":"markdown","9df7140f":"markdown","bafbb723":"markdown","19fa6a55":"markdown","e3a9648c":"markdown","43120ca6":"markdown","152be2eb":"markdown","f258c4cc":"markdown","9b309520":"markdown","941692a4":"markdown","cf6a8e0a":"markdown","4e854e6b":"markdown","3c8e0f6a":"markdown","1b8ee33f":"markdown","be48edcd":"markdown","35d659f6":"markdown","20d3c3b4":"markdown","c01b57d2":"markdown","0f82d5d0":"markdown"},"source":{"a7c65d77":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9b899dd4":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport random","fcbf7900":"df = pd.read_csv(\"\/kaggle\/input\/impact-of-covid19-pandemic-on-the-global-economy\/transformed_data.csv\")\ndf.head()","6f26bae5":"df.isna().sum()","d36ac90a":"import missingno as msno \n\nmsno.bar(df)\nplt.show()","5b88aa97":"df[\"HDI\"].dtype","3dbe7dfb":"df[\"HDI\"] = df[\"HDI\"].fillna(df[\"HDI\"].mean())","189c65b3":"df.dtypes","68451f31":"import plotly.express as px\n\nfig = px.box(df, y=\"POP\")\nfig.show()","7f529ef5":"#Trimming\n\n#calculate the IQR\nIQR = df[\"POP\"].quantile(0.75) - df[\"POP\"].quantile(0.25)\n\n#calculate the boundries\nlower = df[\"POP\"].quantile(0.25) - (IQR * 1.5)\nupper = df[\"POP\"].quantile(0.75) + (IQR * 1.5)\n\n# find the outliers\noutliers = np.where(df[\"POP\"] > upper, True, np.where(df[\"POP\"] < lower, True, False))\n\n# remove outliers from data.\ndf_trimming = df.loc[~(outliers)] ","ebaabb2c":"import plotly.express as px\n\nfig = px.box(df_trimming, y=\"POP\", title = \"Trimming\")\nfig.show()","9e190658":"# Imputing\n\ndef detect_outlier(data):\n        outliers = []\n        threshold = -2\n        mean = np.mean(data)\n        std = np.std(data)\n        for y in data:\n            z_score = (y - mean) \/ std\n            if z_score < threshold:\n                outliers.append(y)\n        return outliers\n\nresult = list(set(detect_outlier(df[\"POP\"])))\nprint(f'Outlier: {result}')","98872253":"df_impute = df\n\ndf_impute[\"POP\"] = df_impute[\"POP\"].apply(lambda x : df_impute[\"POP\"].mean() if x in result else x)","dbe54cfa":"fig = px.box(df_impute, y=\"POP\", title = \"Imputing\")\nfig.show()","b6537496":"df_binning = df.copy(deep = True)\n\ndf_binning[\"TD\"].value_counts()","5dbca182":"pd.cut(df_binning[\"TD\"], bins = 5)","6b667c9a":"pd.cut(df_binning[\"TD\"], bins = 5, labels = [\"Bin_1\", \"Bin_2\", \"Bin_3\", \"Bin_4\", \"Bin_5\"])","da5d4b1d":"df_binning[\"TD\"] = pd.cut(df_binning[\"TD\"], bins = 5, labels = [\"Bin_1\", \"Bin_2\", \"Bin_3\", \"Bin_4\", \"Bin_5\"])\ndf_binning.sample(5)","8531e74d":"df_log = df.copy(deep = True)\ndf_log[\"GDPCAP\"] = df_log[\"GDPCAP\"].apply(lambda x : x * 10000)\ndf_log[\"GDPCAP\"]","7591bc1e":"np.log1p(df_log[\"GDPCAP\"])","2f13d7b5":"df_log[\"GDPCAP\"] = np.log1p(df_log[\"GDPCAP\"])\ndf.sample(5)","a74c2d60":"# Extras\n\n#Let's find the log and log1p of a small positive number plus one\nprint(\"Log -->\", np.log(1e-100 + 1))\nprint(\"Log1p -->\", np.log1p(1e-100 + 1))","b5e1aedc":"df_encode = df.copy(deep = True)\n\ndf_encode[\"COUNTRY\"].value_counts()","14c4cffb":"from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()\n\nres = ohe.fit_transform(df_encode[\"COUNTRY\"].values.reshape(-1,1))\n\nres.toarray()","399d0f5a":"ohe.inverse_transform(res)","87a52d91":"ohe.get_feature_names()","65378501":"y = pd.get_dummies(df_encode[\"COUNTRY\"], prefix='Country')\ny.head()","d62e6663":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nprint(le.fit_transform(df_encode[\"COUNTRY\"]))","e6f42b34":"df_encode[\"COUNTRY\"] = le.fit_transform(df_encode[\"COUNTRY\"])\n\nprint(le.inverse_transform(df_encode[\"COUNTRY\"]))","0ee70888":"df_encode.sample(5)","4081544b":"df_group = df.copy(deep = True)","48c3fced":"df.pivot(index = 'DATE', columns = 'CODE')","e4e84c47":"pd.DataFrame(df_group.groupby(\"COUNTRY\")[[\"HDI\", \"TC\", \"TD\"]].mean()).reset_index()","21eb5430":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\ny = scaler.fit_transform(df[\"HDI\"].values.reshape(-1, 1))\n\npd.DataFrame({\"HDI\": df[\"HDI\"], \"Normalized HDI\": y.flatten()})","a0277d41":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\ny = scaler.fit_transform(df[\"HDI\"].values.reshape(-1, 1))\n\npd.DataFrame({\"HDI\": df[\"HDI\"], \"Standardized HDI\": y.flatten()})","c36ca562":"# Extras\n\nfrom scipy.stats import zscore\n\nzscore(df[\"HDI\"])","1bd1cbd7":"## 2. Handling Outliers\n![](https:\/\/editor.analyticsvidhya.com\/uploads\/12469out.png)\n\nThe best way to detect outliers is to demonstrate the data visually. All other statistical methodologies are open to making mistakes, whereas visualizing the outliers gives a chance to take a decision with high precision.","827ef425":"### 7.1 Normalization","9df7140f":"![](https:\/\/miro.medium.com\/max\/1922\/1*Mn5NoddG6Hlqqld171x-Xg.jpeg)","bafbb723":"## 5.Encoding\nEncoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\n\nThis method changes your categorical data, which is challenging to understand for algorithms, to a numerical format and enables you to group your categorical data without losing any information.","19fa6a55":"### 2.2. Imputing outliers","e3a9648c":"### 7.2 Standardization","43120ca6":"## 3. Binning\nBinning can be applied on both categorical and numerical data.\n\nThe main motivation of binning is to make the model more robust and prevent overfitting. However, it has a cost on the performance. Every time you bin something, you sacrifice information and make your data more regularized.","152be2eb":"## 4. Log Transform\nLogarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. Here are the benefits of using log transform:\n\n* It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal\n* It also decreases the effect of the outliers due to the normalization of magnitude differences and the model become more robust\n* The data you apply log transform to must have only positive values, otherwise you receive an error","f258c4cc":"## 6. Grouping\n\n**Categorical Grouping**<br>\nUsing a pivot table or grouping based on aggregate functions using lambda.\n\n**Numeric Grouping**<br>\nNumerical columns are grouped using sum and mean functions in most of the cases.","9b309520":"### 5.3 Label Encoder","941692a4":"### 5.2 get_dummies()","cf6a8e0a":"### 2.1. Trimming","4e854e6b":"## 7. Scaling\n\nIn most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In order for a symmetric dataset, scaling is required.\n\n**Normalization**<br>\nNormalization (or min-max normalization) scales all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers<br><br>\n**Standardization**<br>\nStandardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.","3c8e0f6a":"## Methods for handling outliers\n**Now that we understand how to detect outliers in a better way, it\u2019s time to engineer them. We\u2019re going to explore a few different techniques and methods to achieve that:**\n\n* **Trimming**: Simply removing the outliers from our dataset.\n* **Imputing**: We treat outliers as missing data, and we apply missing data imputation techniques.\n* **Discretization**: We place outliers in edge bins with higher or lower values of the distribution.\n* **Censoring**: Capping the variable distribution at the maximum and minimum values.","1b8ee33f":"## 1. Imputation\nMissing values are one of the most common problems you can encounter when you prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, etc. Whatever the reason, missing values affect the performance of machine learning models.","be48edcd":"### 6.1 Categorical Grouping","35d659f6":"## List of Feature Engineering Techniques\n* **Imputation**\n* **Handling Outliers**\n* **Binning**\n* **Log Transform**\n* **Encoding**\n* **Grouping Operations**\n* **Scaling**","20d3c3b4":"### 6.2 Numerical Grouping","c01b57d2":"### References:\n\nhttps:\/\/heartbeat.fritz.ai\/hands-on-with-feature-engineering-techniques-dealing-with-outliers-fcc9f57cb63b\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/7-feature-engineering-techniques-machine-learning\/\n\nhttps:\/\/stackoverflow.com\/questions\/49538185\/what-is-the-purpose-of-numpy-log1p\/49538384","0f82d5d0":"### 5.1 One-Hot Encoding"}}