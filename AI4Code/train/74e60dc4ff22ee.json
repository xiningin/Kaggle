{"cell_type":{"2a965807":"code","e328fbbd":"code","7d8ac7e8":"code","0b3ca8b6":"code","8aa63834":"code","ccdae306":"code","de4d7d3b":"code","9a72bb90":"code","7b3e2d8b":"code","7c39d75a":"code","abdb2c88":"code","f1d16808":"code","8646780c":"code","b218e201":"code","bb35358f":"code","0f3b33ea":"code","136a7272":"code","3a62643e":"code","be6e1f77":"code","7b933045":"code","ed320983":"code","da2d9bdd":"code","8d510479":"markdown","d147dffc":"markdown","d670ed24":"markdown","7ed0780a":"markdown","9c4da60e":"markdown","1c115f30":"markdown","b63364a2":"markdown","a538f5d2":"markdown","a9066009":"markdown"},"source":{"2a965807":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n!apt-get install -y -qq libboost-all-dev","e328fbbd":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","7d8ac7e8":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","0b3ca8b6":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","8aa63834":"# Latest Pandas version\n!pip install -q 'pandas==0.25' --force-reinstall","ccdae306":"import lightgbm as lgb\nimport pandas as pd \nprint(\"LGBM version:\", lgb.__version__)\nprint(\"Pandas version:\", pd.__version__)","de4d7d3b":"%reload_ext autoreload\n%autoreload 2\nimport os\nimport platform\nprint(\"Python Version:\", platform.python_version())\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport time\nnotebookstart = time.time()\n\nfrom contextlib import contextmanager\nimport gc\nimport pprint\n\nimport numpy as np\n\n# Viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA\n\nseed = 24\nnp.random.seed(seed)\n\npd.set_option('display.max_columns', 500)\npd.options.display.max_rows = 999\npd.set_option('max_colwidth', 500)\n\nprint(\"LGBM version:\", lgb.__version__)","9a72bb90":"print(\"Define DF Schema..\")\n\ntarget_var = 'isFraud'\n\nschema = {\n    \"TransactionDT\":       \"int32\",\n    \"TransactionAmt\":    \"float32\",\n    \"ProductCD\":          \"object\",\n    \"card1\":               \"int16\",\n    \"card2\":             \"float32\",\n    \"card3\":             \"float32\",\n    \"card4\":              \"object\",\n    \"card5\":             \"float32\",\n    \"card6\":              \"object\",\n    \"addr1\":             \"float32\",\n    \"addr2\":             \"float32\",\n    \"dist1\":             \"float32\",\n    \"dist2\":             \"float32\",\n    \"P_emaildomain\":      \"object\",\n    \"R_emaildomain\":      \"object\",\n    \"C1\":                \"float32\",\n    \"C2\":                \"float32\",\n    \"C3\":                \"float32\",\n    \"C4\":                \"float32\",\n    \"C5\":                \"float32\",\n    \"C6\":                \"float32\",\n    \"C7\":                \"float32\",\n    \"C8\":                \"float32\",\n    \"C9\":                \"float32\",\n    \"C10\":               \"float32\",\n    \"C11\":               \"float32\",\n    \"C12\":               \"float32\",\n    \"C13\":               \"float32\",\n    \"C14\":               \"float32\",\n    \"D1\":                \"float32\",\n    \"D2\":                \"float32\",\n    \"D3\":                \"float32\",\n    \"D4\":                \"float32\",\n    \"D5\":                \"float32\",\n    \"D6\":                \"float32\",\n    \"D7\":                \"float32\",\n    \"D8\":                \"float32\",\n    \"D9\":                \"float32\",\n    \"D10\":               \"float32\",\n    \"D11\":               \"float32\",\n    \"D12\":               \"float32\",\n    \"D13\":               \"float32\",\n    \"D14\":               \"float32\",\n    \"D15\":               \"float32\",\n    \"M1\":                 \"object\",\n    \"M2\":                 \"object\",\n    \"M3\":                 \"object\",\n    \"M4\":                 \"object\",\n    \"M5\":                 \"object\",\n    \"M6\":                 \"object\",\n    \"M7\":                 \"object\",\n    \"M8\":                 \"object\",\n    \"M9\":                 \"object\",\n    \"V1\":                \"float32\",\n    \"V2\":                \"float32\",\n    \"V3\":                \"float32\",\n    \"V4\":                \"float32\",\n    \"V5\":                \"float32\",\n    \"V6\":                \"float32\",\n    \"V7\":                \"float32\",\n    \"V8\":                \"float32\",\n    \"V9\":                \"float32\",\n    \"V10\":               \"float32\",\n    \"V11\":               \"float32\",\n    \"V12\":               \"float32\",\n    \"V13\":               \"float32\",\n    \"V14\":               \"float32\",\n    \"V15\":               \"float32\",\n    \"V16\":               \"float32\",\n    \"V17\":               \"float32\",\n    \"V18\":               \"float32\",\n    \"V19\":               \"float32\",\n    \"V20\":               \"float32\",\n    \"V21\":               \"float32\",\n    \"V22\":               \"float32\",\n    \"V23\":               \"float32\",\n    \"V24\":               \"float32\",\n    \"V25\":               \"float32\",\n    \"V26\":               \"float32\",\n    \"V27\":               \"float32\",\n    \"V28\":               \"float32\",\n    \"V29\":               \"float32\",\n    \"V30\":               \"float32\",\n    \"V31\":               \"float32\",\n    \"V32\":               \"float32\",\n    \"V33\":               \"float32\",\n    \"V34\":               \"float32\",\n    \"V35\":               \"float32\",\n    \"V36\":               \"float32\",\n    \"V37\":               \"float32\",\n    \"V38\":               \"float32\",\n    \"V39\":               \"float32\",\n    \"V40\":               \"float32\",\n    \"V41\":               \"float32\",\n    \"V42\":               \"float32\",\n    \"V43\":               \"float32\",\n    \"V44\":               \"float32\",\n    \"V45\":               \"float32\",\n    \"V46\":               \"float32\",\n    \"V47\":               \"float32\",\n    \"V48\":               \"float32\",\n    \"V49\":               \"float32\",\n    \"V50\":               \"float32\",\n    \"V51\":               \"float32\",\n    \"V52\":               \"float32\",\n    \"V53\":               \"float32\",\n    \"V54\":               \"float32\",\n    \"V55\":               \"float32\",\n    \"V56\":               \"float32\",\n    \"V57\":               \"float32\",\n    \"V58\":               \"float32\",\n    \"V59\":               \"float32\",\n    \"V60\":               \"float32\",\n    \"V61\":               \"float32\",\n    \"V62\":               \"float32\",\n    \"V63\":               \"float32\",\n    \"V64\":               \"float32\",\n    \"V65\":               \"float32\",\n    \"V66\":               \"float32\",\n    \"V67\":               \"float32\",\n    \"V68\":               \"float32\",\n    \"V69\":               \"float32\",\n    \"V70\":               \"float32\",\n    \"V71\":               \"float32\",\n    \"V72\":               \"float32\",\n    \"V73\":               \"float32\",\n    \"V74\":               \"float32\",\n    \"V75\":               \"float32\",\n    \"V76\":               \"float32\",\n    \"V77\":               \"float32\",\n    \"V78\":               \"float32\",\n    \"V79\":               \"float32\",\n    \"V80\":               \"float32\",\n    \"V81\":               \"float32\",\n    \"V82\":               \"float32\",\n    \"V83\":               \"float32\",\n    \"V84\":               \"float32\",\n    \"V85\":               \"float32\",\n    \"V86\":               \"float32\",\n    \"V87\":               \"float32\",\n    \"V88\":               \"float32\",\n    \"V89\":               \"float32\",\n    \"V90\":               \"float32\",\n    \"V91\":               \"float32\",\n    \"V92\":               \"float32\",\n    \"V93\":               \"float32\",\n    \"V94\":               \"float32\",\n    \"V95\":               \"float32\",\n    \"V96\":               \"float32\",\n    \"V97\":               \"float32\",\n    \"V98\":               \"float32\",\n    \"V99\":               \"float32\",\n    \"V100\":              \"float32\",\n    \"V101\":              \"float32\",\n    \"V102\":              \"float32\",\n    \"V103\":              \"float32\",\n    \"V104\":              \"float32\",\n    \"V105\":              \"float32\",\n    \"V106\":              \"float32\",\n    \"V107\":              \"float32\",\n    \"V108\":              \"float32\",\n    \"V109\":              \"float32\",\n    \"V110\":              \"float32\",\n    \"V111\":              \"float32\",\n    \"V112\":              \"float32\",\n    \"V113\":              \"float32\",\n    \"V114\":              \"float32\",\n    \"V115\":              \"float32\",\n    \"V116\":              \"float32\",\n    \"V117\":              \"float32\",\n    \"V118\":              \"float32\",\n    \"V119\":              \"float32\",\n    \"V120\":              \"float32\",\n    \"V121\":              \"float32\",\n    \"V122\":              \"float32\",\n    \"V123\":              \"float32\",\n    \"V124\":              \"float32\",\n    \"V125\":              \"float32\",\n    \"V126\":              \"float32\",\n    \"V127\":              \"float32\",\n    \"V128\":              \"float32\",\n    \"V129\":              \"float32\",\n    \"V130\":              \"float32\",\n    \"V131\":              \"float32\",\n    \"V132\":              \"float32\",\n    \"V133\":              \"float32\",\n    \"V134\":              \"float32\",\n    \"V135\":              \"float32\",\n    \"V136\":              \"float32\",\n    \"V137\":              \"float32\",\n    \"V138\":              \"float32\",\n    \"V139\":              \"float32\",\n    \"V140\":              \"float32\",\n    \"V141\":              \"float32\",\n    \"V142\":              \"float32\",\n    \"V143\":              \"float32\",\n    \"V144\":              \"float32\",\n    \"V145\":              \"float32\",\n    \"V146\":              \"float32\",\n    \"V147\":              \"float32\",\n    \"V148\":              \"float32\",\n    \"V149\":              \"float32\",\n    \"V150\":              \"float32\",\n    \"V151\":              \"float32\",\n    \"V152\":              \"float32\",\n    \"V153\":              \"float32\",\n    \"V154\":              \"float32\",\n    \"V155\":              \"float32\",\n    \"V156\":              \"float32\",\n    \"V157\":              \"float32\",\n    \"V158\":              \"float32\",\n    \"V159\":              \"float32\",\n    \"V160\":              \"float32\",\n    \"V161\":              \"float32\",\n    \"V162\":              \"float32\",\n    \"V163\":              \"float32\",\n    \"V164\":              \"float32\",\n    \"V165\":              \"float32\",\n    \"V166\":              \"float32\",\n    \"V167\":              \"float32\",\n    \"V168\":              \"float32\",\n    \"V169\":              \"float32\",\n    \"V170\":              \"float32\",\n    \"V171\":              \"float32\",\n    \"V172\":              \"float32\",\n    \"V173\":              \"float32\",\n    \"V174\":              \"float32\",\n    \"V175\":              \"float32\",\n    \"V176\":              \"float32\",\n    \"V177\":              \"float32\",\n    \"V178\":              \"float32\",\n    \"V179\":              \"float32\",\n    \"V180\":              \"float32\",\n    \"V181\":              \"float32\",\n    \"V182\":              \"float32\",\n    \"V183\":              \"float32\",\n    \"V184\":              \"float32\",\n    \"V185\":              \"float32\",\n    \"V186\":              \"float32\",\n    \"V187\":              \"float32\",\n    \"V188\":              \"float32\",\n    \"V189\":              \"float32\",\n    \"V190\":              \"float32\",\n    \"V191\":              \"float32\",\n    \"V192\":              \"float32\",\n    \"V193\":              \"float32\",\n    \"V194\":              \"float32\",\n    \"V195\":              \"float32\",\n    \"V196\":              \"float32\",\n    \"V197\":              \"float32\",\n    \"V198\":              \"float32\",\n    \"V199\":              \"float32\",\n    \"V200\":              \"float32\",\n    \"V201\":              \"float32\",\n    \"V202\":              \"float32\",\n    \"V203\":              \"float32\",\n    \"V204\":              \"float32\",\n    \"V205\":              \"float32\",\n    \"V206\":              \"float32\",\n    \"V207\":              \"float32\",\n    \"V208\":              \"float32\",\n    \"V209\":              \"float32\",\n    \"V210\":              \"float32\",\n    \"V211\":              \"float32\",\n    \"V212\":              \"float32\",\n    \"V213\":              \"float32\",\n    \"V214\":              \"float32\",\n    \"V215\":              \"float32\",\n    \"V216\":              \"float32\",\n    \"V217\":              \"float32\",\n    \"V218\":              \"float32\",\n    \"V219\":              \"float32\",\n    \"V220\":              \"float32\",\n    \"V221\":              \"float32\",\n    \"V222\":              \"float32\",\n    \"V223\":              \"float32\",\n    \"V224\":              \"float32\",\n    \"V225\":              \"float32\",\n    \"V226\":              \"float32\",\n    \"V227\":              \"float32\",\n    \"V228\":              \"float32\",\n    \"V229\":              \"float32\",\n    \"V230\":              \"float32\",\n    \"V231\":              \"float32\",\n    \"V232\":              \"float32\",\n    \"V233\":              \"float32\",\n    \"V234\":              \"float32\",\n    \"V235\":              \"float32\",\n    \"V236\":              \"float32\",\n    \"V237\":              \"float32\",\n    \"V238\":              \"float32\",\n    \"V239\":              \"float32\",\n    \"V240\":              \"float32\",\n    \"V241\":              \"float32\",\n    \"V242\":              \"float32\",\n    \"V243\":              \"float32\",\n    \"V244\":              \"float32\",\n    \"V245\":              \"float32\",\n    \"V246\":              \"float32\",\n    \"V247\":              \"float32\",\n    \"V248\":              \"float32\",\n    \"V249\":              \"float32\",\n    \"V250\":              \"float32\",\n    \"V251\":              \"float32\",\n    \"V252\":              \"float32\",\n    \"V253\":              \"float32\",\n    \"V254\":              \"float32\",\n    \"V255\":              \"float32\",\n    \"V256\":              \"float32\",\n    \"V257\":              \"float32\",\n    \"V258\":              \"float32\",\n    \"V259\":              \"float32\",\n    \"V260\":              \"float32\",\n    \"V261\":              \"float32\",\n    \"V262\":              \"float32\",\n    \"V263\":              \"float32\",\n    \"V264\":              \"float32\",\n    \"V265\":              \"float32\",\n    \"V266\":              \"float32\",\n    \"V267\":              \"float32\",\n    \"V268\":              \"float32\",\n    \"V269\":              \"float32\",\n    \"V270\":              \"float32\",\n    \"V271\":              \"float32\",\n    \"V272\":              \"float32\",\n    \"V273\":              \"float32\",\n    \"V274\":              \"float32\",\n    \"V275\":              \"float32\",\n    \"V276\":              \"float32\",\n    \"V277\":              \"float32\",\n    \"V278\":              \"float32\",\n    \"V279\":              \"float32\",\n    \"V280\":              \"float32\",\n    \"V281\":              \"float32\",\n    \"V282\":              \"float32\",\n    \"V283\":              \"float32\",\n    \"V284\":              \"float32\",\n    \"V285\":              \"float32\",\n    \"V286\":              \"float32\",\n    \"V287\":              \"float32\",\n    \"V288\":              \"float32\",\n    \"V289\":              \"float32\",\n    \"V290\":              \"float32\",\n    \"V291\":              \"float32\",\n    \"V292\":              \"float32\",\n    \"V293\":              \"float32\",\n    \"V294\":              \"float32\",\n    \"V295\":              \"float32\",\n    \"V296\":              \"float32\",\n    \"V297\":              \"float32\",\n    \"V298\":              \"float32\",\n    \"V299\":              \"float32\",\n    \"V300\":              \"float32\",\n    \"V301\":              \"float32\",\n    \"V302\":              \"float32\",\n    \"V303\":              \"float32\",\n    \"V304\":              \"float32\",\n    \"V305\":              \"float32\",\n    \"V306\":              \"float32\",\n    \"V307\":              \"float32\",\n    \"V308\":              \"float32\",\n    \"V309\":              \"float32\",\n    \"V310\":              \"float32\",\n    \"V311\":              \"float32\",\n    \"V312\":              \"float32\",\n    \"V313\":              \"float32\",\n    \"V314\":              \"float32\",\n    \"V315\":              \"float32\",\n    \"V316\":              \"float32\",\n    \"V317\":              \"float32\",\n    \"V318\":              \"float32\",\n    \"V319\":              \"float32\",\n    \"V320\":              \"float32\",\n    \"V321\":              \"float32\",\n    \"V322\":              \"float32\",\n    \"V323\":              \"float32\",\n    \"V324\":              \"float32\",\n    \"V325\":              \"float32\",\n    \"V326\":              \"float32\",\n    \"V327\":              \"float32\",\n    \"V328\":              \"float32\",\n    \"V329\":              \"float32\",\n    \"V330\":              \"float32\",\n    \"V331\":              \"float32\",\n    \"V332\":              \"float32\",\n    \"V333\":              \"float32\",\n    \"V334\":              \"float32\",\n    \"V335\":              \"float32\",\n    \"V336\":              \"float32\",\n    \"V337\":              \"float32\",\n    \"V338\":              \"float32\",\n    \"V339\":              \"float32\",\n    \"id_01\":             \"float32\",\n    \"id_02\":             \"float32\",\n    \"id_03\":             \"float32\",\n    \"id_04\":             \"float32\",\n    \"id_05\":             \"float32\",\n    \"id_06\":             \"float32\",\n    \"id_07\":             \"float32\",\n    \"id_08\":             \"float32\",\n    \"id_09\":             \"float32\",\n    \"id_10\":             \"float32\",\n    \"id_11\":             \"float32\",\n    \"id_12\":              \"object\",\n    \"id_13\":             \"float32\",\n    \"id_14\":             \"float32\",\n    \"id_15\":              \"object\",\n    \"id_16\":              \"object\",\n    \"id_17\":             \"float32\",\n    \"id_18\":             \"float32\",\n    \"id_19\":             \"float32\",\n    \"id_20\":             \"float32\",\n    \"id_21\":             \"float32\",\n    \"id_22\":             \"float32\",\n    \"id_23\":              \"object\",\n    \"id_24\":             \"float32\",\n    \"id_25\":             \"float32\",\n    \"id_26\":             \"float32\",\n    \"id_27\":              \"object\",\n    \"id_28\":              \"object\",\n    \"id_29\":              \"object\",\n    \"id_30\":              \"object\",\n    \"id_31\":              \"object\",\n    \"id_32\":             \"float32\",\n    \"id_33\":              \"object\",\n    \"id_34\":              \"object\",\n    \"id_35\":              \"object\",\n    \"id_36\":              \"object\",\n    \"id_37\":              \"object\",\n    \"id_38\":              \"object\",\n    \"DeviceType\":         \"object\",\n    \"DeviceInfo\":         \"object\",\n    \"is_fraud\":\t\t\t  \"int8\"\n}","7b3e2d8b":"@contextmanager\ndef timer(name):\n    \"\"\"\n    Time Each Process\n    \"\"\"\n    t0 = time.time()\n    yield\n    print('\\n[{}] done in {} Minutes\\n'.format(name, round((time.time() - t0)\/60,2)))\n\ndef fraud_preprocessing(debug = None):\n    print(\"Starting Pre-Processing..\")\n    with timer(\"Load Tables\"):\n        train_transaction = pd.read_csv('..\/input\/train_transaction.csv',\n                                        index_col='TransactionID', nrows= debug, dtype = schema)\n        test_transaction = pd.read_csv('..\/input\/test_transaction.csv',\n                                       index_col='TransactionID', nrows= debug, dtype = schema)\n\n        train_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\n        test_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\n    with timer(\"Merge Tables\"):\n        train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n        test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\n        print(\"Train Shape: {} Rows, {} Columns\".format(*train.shape))\n        print(\"Test Shape: {} Rows, {} Columns\".format(*test.shape))\n\n        y = train[target_var].copy()\n        del train_transaction, train_identity, test_transaction, test_identity\n\n        traindex = train.index\n        testdex = test.index\n\n        df = pd.concat([train.drop(target_var,axis=1),test],axis = 0)\n        del train, test\n        \n    with timer(\"Feature Engineering\"):\n        print(\"** crickets **\")\n\n    with timer(\"Label Encode\"):\n        categorical_cols = []\n        # Label Encoding\n        for f in df.columns:\n            if df[f].dtype=='object': \n                categorical_cols += [f]\n                lbl = preprocessing.LabelEncoder()\n                df[f] = lbl.fit_transform(df[f].astype(str))\n#                 df[f] = df[f].astype('category')\n        print(\"Total Shape: {} Rows, {} Columns\".format(*df.shape))\n                \n    return df, y, traindex, testdex, categorical_cols","7c39d75a":"debug = None\ndf, y, traindex, testdex, cat_cols = fraud_preprocessing(debug = debug)\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv',\n                                index_col='TransactionID',\n                                nrows = debug)\n\nX = df.loc[traindex,:]\nfeat_names = X.columns.tolist() \ntest = df.loc[testdex,:]\ndel df ; gc.collect();","abdb2c88":"print(\"None Fraud: {}%, Fraud: {}%\".format(*y.value_counts(normalize=True)))\nprint(\"Randomness Score AUC: {}\".format(\n    metrics.roc_auc_score(y,np.array([y.value_counts(normalize=True)[0]]*y.shape[0]))))","f1d16808":"metric = 'auc'\nsplit_size = .5\nn_estimators = 5000\n\n# Parameters From\n# https:\/\/www.kaggle.com\/vincentlugat\/ieee-lgb-bayesian-opt\nparams = {\n 'boosting_type': 'gbdt',\n 'device': 'gpu',\n 'feature_fraction': 0.8,\n 'is_unbalance': False,\n 'learning_rate': 0.09,\n 'max_depth': 20,\n 'metric': 'auc',\n 'min_data_in_leaf': 1,\n 'nthread': -1,\n 'num_boost_round': 231,\n 'num_leaves': 4095,\n 'objective': 'binary',\n 'reg_alpha': 1.0,\n 'reg_lambda': 1.0,\n 'seed': 24,\n 'subsample': 0.4,\n 'subsample_for_bin': 500,\n 'tree_learner': 'serial',\n 'verbose': -1}","8646780c":"feature_subset = feat_names\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X[feature_subset], y, test_size=split_size,\n    random_state=seed, shuffle=True,stratify=y)\n\nprint(\"Light Gradient Boosting Multi-Class Classifier: \")\nlgtrain = lgb.Dataset(X_train, y_train, categorical_feature = cat_cols, free_raw_data=False)\nlgvalid = lgb.Dataset(X_valid, y_valid, categorical_feature = cat_cols, free_raw_data=False)\n\nevals_result = {} \nwith timer(\"LGBM\"):\n    lgb_model = lgb.train(\n            params,\n            lgtrain,\n            valid_sets=[lgtrain, lgvalid],\n            valid_names=['train','valid'],\n            verbose_eval=300,\n            num_boost_round = n_estimators,\n            early_stopping_rounds=100,\n            evals_result=evals_result\n            )\n    \n# -------------------------------------------------------\nprint('Plot metrics during training...')\nf,ax = plt.subplots(figsize = [6,5])\nlgb.plot_metric(evals_result, metric=metric, ax = ax)\nplt.show()\n# -------------------------------------------------------","b218e201":"with timer(\"LGBM Feature Importance\"):\n    # Importance to DataFrame\n    model_importance = pd.DataFrame(\n                 [X_train.columns.tolist(),\n                  lgb_model.feature_importance(importance_type = 'gain').tolist(),\n                  lgb_model.feature_importance(importance_type = 'split').tolist()]).T\n    model_importance.columns = ['feature','gain','split']\n\n    print(\"Feature Importance\")\n    # Feature Importance Plot\n    f, ax = plt.subplots(1,2,figsize=[20,10])\n    lgb.plot_importance(lgb_model, max_num_features=50, ax=ax[0], importance_type='gain')\n    ax[0].set_title(\"Light GBM GAIN Feature Importance\")\n\n    lgb.plot_importance(lgb_model, max_num_features=50, ax=ax[1], importance_type='split')\n    ax[1].set_title(\"Light GBM SPLIT Feature Importance\")\n    plt.tight_layout(pad=1)\n    plt.savefig('sales_propensity_feature_import.png')\n    \n    del X_train, X_valid, y_train, y_valid, lgb_model; gc.collect();","bb35358f":"pca = PCA().fit(X.fillna(0).values)\n\nf, ax = plt.subplots(1,2,figsize =[10,5])\nax[0].plot(np.cumsum(pca.explained_variance_ratio_), 'k-')\nax[0].set_xlabel('number of components')\nax[0].set_ylabel('cumulative explained variance')\nax[0].set_title(\"PCA - All Features\")\n\nax[1].plot(np.cumsum(pca.explained_variance_ratio_[:20]),'xg-')\nax[1].set_xlabel('number of components')\nax[1].set_ylabel('cumulative explained variance')\nax[1].set_title(\"PCA - Top 20 Features\")\n\nplt.tight_layout(pad=0)\nplt.show()\n\ndel pca; gc.collect();","0f3b33ea":"def feature_count_trade_off(feat_list, step, importance_type = 'perm_score'):\n    \"\"\"\n    feat_list:\n        list of features to test - Iteratively refit model after shaving off worst features with step size\n    importance_type:\n        \"perm_score\": Feature Permutation\n        \"gain\": Decision Tree Gain\n        \"split\": Decisino Tree Split\n    \"\"\"\n    \n    feature_size_experiment = []\n    feature_subset = feat_list\n    intervals =  [2] + list(np.logspace(2, 8.5, num=20, base=2.0).astype(int).clip(0,X.shape[1]))\n    intervals = intervals[::-1]\n    print(\"Number of Experiments: {}\".format(len(intervals)))\n    print(\"Intervals to test: {}\".format(intervals))\n    for i in intervals:\n        feature_subset = feature_subset[:i]\n        sbst_X_train, sbst_X_valid, y_train, y_valid = train_test_split(\n            X[feature_subset], y, test_size=split_size,\n            random_state=seed, shuffle=True,stratify=y)\n        \n        sb_cat_cols = [x for x in cat_cols if x in feature_subset]\n        lgtrain = lgb.Dataset(sbst_X_train, y_train, categorical_feature = sb_cat_cols)\n        lgvalid = lgb.Dataset(sbst_X_valid, y_valid, categorical_feature = sb_cat_cols)\n\n        experiment_lgb = lgb.train(\n                params,\n                lgtrain,\n                valid_sets=[lgtrain, lgvalid],\n                valid_names=['train','valid'],\n                num_boost_round = n_estimators,\n                verbose_eval=0,\n                early_stopping_rounds=50\n                )\n        \n        val_pred = experiment_lgb.predict(sbst_X_valid)\n        \n        # Get Metrics\n        loss = metrics.log_loss(y_valid, val_pred)\n        auc = metrics.roc_auc_score(y_valid, val_pred)\n        feature_num = len(feature_subset)\n\n        # Importance to DataFrame\n        model_importance = pd.DataFrame(\n                     [sbst_X_valid.columns.tolist(),\n                      experiment_lgb.feature_importance(importance_type = 'gain').tolist(),\n                      experiment_lgb.feature_importance(importance_type = 'split').tolist()]).T\n        model_importance.columns = ['feature','gain','split']\n        \n        # Pass list to next iteration\n        if importance_type == 'perm_score':\n            model_importance = model_importance.sort_values(by='perm_score', ascending=False) # ascending True if minimization metric\n            best_features_after_iteration = model_importance['feature'].values\n        elif importance_type == 'gain':\n            model_importance = model_importance.sort_values(by='gain', ascending=False)\n            best_features_after_iteration = model_importance['feature'].values\n        elif importance_type == 'split':\n            model_importance = model_importance.sort_values(by='split', ascending=False)\n            best_features_after_iteration = model_importance['feature'].values\n            \n        feature_size_experiment.append(\n            [\n             loss,\n             feature_num,\n             experiment_lgb.best_score['train'][metric],\n             experiment_lgb.best_score['valid'][metric],\n             experiment_lgb.best_iteration,\n             feature_subset,\n             np.mean(model_importance[importance_type].values),\n             np.std(model_importance[importance_type].values)\n            ]\n        )\n            \n        feature_subset = best_features_after_iteration\n        \n    # Full Data\n    feature_size_experiment_df = pd.DataFrame(feature_size_experiment,\n                                              columns = ['valid_logloss',\n                                                         'feature_count',\n                                                         'train_{}'.format(metric),\n                                                         'valid_{}'.format(metric),\n                                                         'boosting_round',\n                                                         'feature_set',\n                                                         'average_importance_type',\n                                                         'std_avg_importance_type'\n                                                        ])\n    \n    # Plot\n    f, ax = plt.subplots(1,4,figsize=[17,6])\n    ax[0].plot(feature_size_experiment_df.feature_count, feature_size_experiment_df['valid_{}'.format(metric)], 'xr-')\n    ax[0].set_title(\"AUC vs. Feature Count\")\n    ax[0].set_xlabel(\"Feature Count\")\n    ax[0].set_ylabel(\"AUC\")\n    ax[1].plot(feature_size_experiment_df.feature_count, feature_size_experiment_df.valid_logloss, 'o-')\n    ax[1].set_title(\"Logloss vs. Feature Count\")\n    ax[1].set_xlabel(\"Feature Count\")\n    ax[1].set_ylabel(\"Logloss\")\n    ax[2].plot(feature_size_experiment_df.feature_count, feature_size_experiment_df.boosting_round, 'xg-')\n    ax[2].set_title(\"Boosting Rounds vs. Feature Count\")\n    ax[2].set_xlabel(\"Feature Count\")\n    ax[2].set_ylabel(\"Boosting Rounds\")\n    ax[3].errorbar(feature_size_experiment_df.feature_count, feature_size_experiment_df['average_importance_type'],\n               yerr=feature_size_experiment_df['std_avg_importance_type'], label='both limits (default)', fmt =  'r-')\n    ax[3].set_title(\"Average {} Importance \\nvs. Row Count in Thousands\".format(importance_type))\n    ax[3].set_xlabel(\"Boosting Rounds\")\n    ax[3].set_ylabel(\"Avg {}\".format(importance_type))\n    \n    plt.tight_layout(pad=0)\n    \n    return feature_size_experiment_df","136a7272":"IMPORTANCE = 'split'\nwith timer(\"Feature Selection Experiment - RANKED BY {}\".format(IMPORTANCE)):\n    perm_expr = feature_count_trade_off(feat_list = model_importance.sort_values(by=IMPORTANCE, ascending=False)['feature'].values,\n                                        step = 25,\n                                        importance_type = IMPORTANCE)\n    plt.savefig('{}_perm_score_experiment.png'.format(IMPORTANCE))\n    plt.show()","3a62643e":"# Output Table:\ndisplay(perm_expr.iloc[-25:])\nperm_expr.to_csv(\"full_output_table_{}.csv\".format(IMPORTANCE))","be6e1f77":"final_features = [\n\"TransactionDT\",\n\"card1\",\n\"TransactionAmt\",\n\"addr1\",\n\"card2\",\n\"P_emaildomain\",\n\"dist1\",\n\"D15\",\n\"id_31\",\n\"D4\",\n\"id_02\",\n\"card5\",\n\"D10\",\n\"D2\",\n\"D11\",\n\"C13\",\n\"id_19\",\n\"id_20\",\n\"D1\",\n\"D8\",\n\"D5\",\n\"M4\",\n\"D3\",\n\"M5\",\n\"M6\",\n\"C2\",\n\"C1\",\n\"D9\",\n\"dist2\",\n\"id_33\",\n\"id_13\",\n\"C14\",\n\"id_06\",\n\"V307\",\n\"C6\",\n\"id_05\",\n\"V310\",\n\"C11\",\n\"M8\",\n\"R_emaildomain\",\n\"M9\",\n\"C9\",\n\"M3\",\n\"id_01\",\n\"V313\",\n\"D14\",\n\"V127\",\n\"V130\",\n\"card6\",\n\"V62\",\n\"V315\",\n\"id_30\",\n\"V314\",\n\"V264\",\n\"C5\",\n\"M7\",\n\"V83\",\n\"V76\",\n\"V308\",\n\"M2\",\n\"V317\",\n\"V87\",\n\"V283\",\n\"V20\",\n\"V306\",\n\"D6\",\n\"V78\",\n\"V312\"]\n\ncat_cols = [x for x in cat_cols if x in final_features]","7b933045":"%%time\n# SOURCE: https:\/\/www.kaggle.com\/artkulak\/ieee-fraud-simple-baseline-0-9383-lb\n# Added Convergence Plot\nEPOCHS = 3\nkf = KFold(n_splits = EPOCHS, shuffle = True)\ny_preds = np.zeros(sample_submission.shape[0])\ny_oof = np.zeros(X.shape[0])\nf,ax = plt.subplots(figsize = [8,5])\nfor i, (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n    evals_result = {}\n    lgtrain = lgb.Dataset(X.iloc[tr_idx, :][final_features], y.iloc[tr_idx], categorical_feature = cat_cols)\n    lgvalid = lgb.Dataset(X.iloc[val_idx, :][final_features], y.iloc[val_idx], categorical_feature = cat_cols)\n    clf = lgb.train(\n            params,\n            lgtrain,\n            valid_sets=[lgtrain, lgvalid],\n            valid_names=['train','valid'],\n            verbose_eval=300,\n            early_stopping_rounds=100,\n            evals_result=evals_result,\n            )\n    y_pred_train = clf.predict(X.iloc[val_idx, :][final_features])\n    y_oof[val_idx] = y_pred_train\n    print('ROC AUC {}\\n'.format(roc_auc_score(y.iloc[val_idx], y_pred_train)))\n    y_preds += clf.predict(test.loc[:,final_features]) \/ EPOCHS\n    \n    evals_result['train_{}'.format(i)] = evals_result.pop('train')\n    evals_result['valid_{}'.format(i)] = evals_result.pop('valid')\n    lgb.plot_metric(evals_result, metric=metric, ax = ax)\n\nplt.title(\"GPU LGBM Metric Convergence over {} Folds\".format(EPOCHS))\nplt.show()","ed320983":"# When doing feature selection, make sure you use the same subset on test set.\n# LGBM will not break, but it will give you broken predictions.. -_-\nassert X[final_features].shape[1] == test[final_features].shape[1]\n\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('{}_feats_{}fold_lgbm_gpu.csv'.format(len(final_features),EPOCHS))","da2d9bdd":"print(\"Notebook Runtime: %0.2f Hours\"%((time.time() - notebookstart)\/60\/60))","8d510479":"#### Principle Component Analysis\nHow features explain the majority of the variance amongst these features?\n\n[Jake VanderPlas's PythonDataScienceHandbook](https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html)","d147dffc":"#### Run Submission set on Final Features:","d670ed24":"### LGBM Model","7ed0780a":"# Tree Split Feature Selection - LGBM GPU EarlyStop  \n_By Nick Brooks_\n\nV1 - 29\/07\/2019 - First Commit <br>\nV2 - 03\/08\/2019 - PCA, Metric Convergence, Submission with 21 features <br>\nV3 - 05\/08\/2019 - Fix GPU implementation <br>\n\n**Motivation:** <br>\nHow much of these features have actual signal? How does predictive power react when the number of features is decreased? What can PCA tell us about the  amount of variance in the features? Does reducing the number of features lead to smoother convergence?\n\n**Methodology:** <br>\n*Tree-Split Feature Selection* - Experiment with Iteratively removing feature using Gradient Boosting Ensemble split importance. LGBM is trained with a single validation fold and shuffles the train \/ validation set each iteration.\n\nVery hyped for the GPU speed boost to Gradient Boosting Algorithmns, this will enable many fun experiments.\n\n**Other Links:** <Br>\n[ELI5 Permutation Importance](https:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html)\n    \n**My Other Fraud Notebooks:** <br>\nhttps:\/\/www.kaggle.com\/nicapotato\/auc-performance-vs-training-size-gpu-catboost <br>\nhttps:\/\/www.kaggle.com\/nicapotato\/fraud-shap-xgboost <br>","9c4da60e":"#### Prepare Data","1c115f30":"## Iterative Split Feature Importance, Feature Selection with Early Stopping","b63364a2":"#### GPU Installation from [kirankunapuli](https:\/\/www.kaggle.com\/kirankunapuli\/)\nSource: https:\/\/www.kaggle.com\/kirankunapuli\/ieee-fraud-lightgbm-with-gpu\/comments","a538f5d2":"**Reflection:** <br>\nIdeally I wanted to use feature permutation to do this iterative feature selection, but it is too computationally expensive (even on CPU kernel)","a9066009":"#### Submit"}}