{"cell_type":{"1f8fdf00":"code","5ac3313d":"code","dc19702f":"code","b8b81227":"code","e5613abf":"code","1e5f1e8e":"code","bc83fd53":"code","e64a6475":"code","26c7b9d0":"code","6a81a55e":"code","db1baceb":"code","cb148d84":"code","97a4f839":"code","c4402c91":"code","fb685c63":"code","e02d15c2":"code","ba25dc4a":"code","894cbaf6":"code","6d99c255":"code","ae666a75":"code","cae84138":"code","90b7b90b":"code","98466631":"code","94a1a0ec":"code","3df2a7d9":"code","4ab2db88":"code","c04ca44b":"code","c6ed80f9":"code","cdecb847":"code","da304daf":"code","4fb2a2e9":"code","a5f96bd1":"code","35d76897":"code","e70e15d9":"code","db1a8fdb":"code","e14d0877":"code","efe766e9":"code","fd3a20e5":"code","22f08abb":"code","7ce07895":"code","4a6cdc03":"code","6a193779":"code","daac80aa":"code","987446e3":"code","14926d15":"code","9270cccb":"code","fbd46926":"code","05feb347":"code","ca70120f":"code","29c8e89b":"markdown","d866eeef":"markdown","dad36536":"markdown","7016097f":"markdown","d0df4c6c":"markdown","cc0a1056":"markdown","2e1bfceb":"markdown","202adf50":"markdown","b2f209ee":"markdown","8e408392":"markdown","a7ccade5":"markdown","cd80e0c2":"markdown","f52148cf":"markdown","a9d8244f":"markdown","1c9fc790":"markdown","083e4f18":"markdown","27af8079":"markdown","b2d6cd07":"markdown","858b7158":"markdown","2effc4c0":"markdown","e15790f5":"markdown","d3d10112":"markdown","31a0b796":"markdown","f401a130":"markdown","d9db8b22":"markdown","d92a06fe":"markdown","f91ab670":"markdown","d508f4a8":"markdown","6ff93eeb":"markdown","b11912bc":"markdown","23649055":"markdown","f6046545":"markdown","533ba4ab":"markdown"},"source":{"1f8fdf00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ac3313d":"os.listdir()","dc19702f":"import torch\nimport torchtext\nimport torch.nn as nn\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F","b8b81227":"import re\nimport random\nfrom sklearn.model_selection import train_test_split","e5613abf":"data_df = pd.read_csv('\/kaggle\/input\/boardgamegeek-reviews\/bgg-15m-reviews.csv',usecols=[ \"rating\", \"comment\"])[[\"comment\", \"rating\"]]","1e5f1e8e":"data_df.head()","bc83fd53":"df = data_df[data_df['comment'].notna()]\n### https:\/\/stackoverflow.com\/questions\/13413590\/how-to-drop-rows-of-pandas-dataframe-whose-value-in-a-certain-column-is-nan","e64a6475":"df.head()","26c7b9d0":"df['rating'] = df['rating'].apply(lambda x: round(x))\ndf['comment'] = df['comment'].apply(lambda x: x.lower())","6a81a55e":"df['rating'].unique()","db1baceb":"### https:\/\/stackoverflow.com\/questions\/29576430\/shuffle-dataframe-rows\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head(10)","cb148d84":"pattern = re.compile(\"[^a-zA-Z ]+\")\ndf[\"comment\"] = df['comment'].map(lambda x: pattern.sub('', x))\ndf.head(10)","97a4f839":"# drop rows with comment length <= 10\ndf = df[df['comment'].map(len) > 10]\nprint(len(df))\ndf = df.reset_index(drop=True)","c4402c91":"df.head(10)","fb685c63":"df['comment'].map(len).max()","e02d15c2":"#rn out of RAM!\n#training_df, testing_df =train, test = train_test_split(df, test_size=0.30)\ntraining_df, testing_df = df.loc[:0.5*len(df)], df.loc[0.75*len(df):]","ba25dc4a":"training_df.tail()","894cbaf6":"testing_df.tail()","6d99c255":"del data_df\ndel df","ae666a75":"training_df.to_csv(\"training.csv\", index=False)\ntesting_df.to_csv(\"testing.csv\", index=False)","cae84138":"os.listdir()","90b7b90b":"tokenizer = lambda x: x.split()","98466631":"TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer, lower=True, include_lengths=True, batch_first=True, fix_length=200)\nLABEL = torchtext.data.LabelField(dtype=torch.float)","94a1a0ec":"fields = [('comment',TEXT),('rating', LABEL)]","3df2a7d9":"train_data = torchtext.data.TabularDataset(\"training.csv\",\"csv\", fields, skip_header=True)","4ab2db88":"test_data = torchtext.data.TabularDataset(\"testing.csv\",\"csv\", fields, skip_header=True)","c04ca44b":"train_data.examples[0].comment, train_data.examples[0].rating","c6ed80f9":"del training_df\ndel testing_df","cdecb847":"# TEXT.build_vocab(train_data, vectors=torchtext.vocab.GloVe(name='6B', dim=100,cache = 'output\/kaggle\/working\/vector_cache'))\n# TEXT.build_vocab(train_data, vectors=\"glove.6B.100d\") #some url error. Due to permissions I believe\nTEXT.build_vocab(train_data, vectors=torchtext.vocab.Vectors(\"\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt\", cache = '..\/output\/working\/vector_cache'))\nLABEL.build_vocab(train_data)","da304daf":"word_embeddings = TEXT.vocab.vectors\nword_embeddings.shape","4fb2a2e9":"train_data, valid_data = train_data.split()","a5f96bd1":"train_iter, valid_iter, test_iter = torchtext.data.BucketIterator.splits((train_data, valid_data, test_data),\n                                                               batch_size=32,\n                                                               sort_key=lambda x: len(x.comment),\n                                                               repeat=False,\n                                                               shuffle=True)","35d76897":"vocab_size = len(TEXT.vocab)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(vocab_size, device)","e70e15d9":"word_embeddings.shape","db1a8fdb":"torch.save(word_embeddings, \"word_embeddings.pt\")","e14d0877":"import dill","efe766e9":"with open(\"TEXT.Field\", \"wb\") as f:\n    dill.dump(TEXT, f)","fd3a20e5":"class ClassifierModel(nn.Module):\n    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n        super(ClassifierModel, self).__init__()\n        \"\"\"\n        output_size : 2 = (pos, neg)\n        \"\"\"\n        self.batch_size = batch_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_length = embedding_length\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)  # Initiale the look-up table.\n        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assign pre-trained GloVe word embedding.\n        self.lstm = nn.LSTM(embedding_length, hidden_size)\n        self.label = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input_sentence, batch_size=None):\n        \"\"\" \n        final_output.shape = (batch_size, output_size)\n        \"\"\"\n        input = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n        input = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n        if batch_size is None:\n            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n        else:\n            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n        final_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n\n        return final_output","22f08abb":"def clip_gradient(model, clip_value):\n    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n    for p in params:\n        p.grad.data.clamp_(-clip_value, clip_value)","7ce07895":"def train_model(model, train_iter, epoch):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    model.to(device)\n    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n    steps = 0\n    model.train()\n    for idx, batch in enumerate(train_iter):\n        text = batch.comment[0]\n        target = batch.rating\n        target = torch.autograd.Variable(target).long()\n        if torch.cuda.is_available():\n            text = text.cuda()\n            target = target.cuda()\n        if (text.size()[0] != 32):# One of the batch has length different than 32.\n            continue\n        optim.zero_grad()\n        prediction = model(text)\n        loss = loss_fn(prediction, target)\n        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n        acc = 100.0 * num_corrects\/len(batch)\n        loss.backward()\n        clip_gradient(model, 1e-1)\n        optim.step()\n        steps += 1\n        \n        if steps % 100 == 0:\n            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n        \n        total_epoch_loss += loss.item()\n        total_epoch_acc += acc.item()\n        \n    return total_epoch_loss\/len(train_iter), total_epoch_acc\/len(train_iter)","4a6cdc03":"def eval_model(model, val_iter):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    model.eval()\n    with torch.no_grad():\n        for idx, batch in enumerate(val_iter):\n            text = batch.comment[0]\n            if (text.size()[0] != 32):\n                continue\n            target = batch.rating\n            target = torch.autograd.Variable(target).long()\n            if torch.cuda.is_available():\n                text = text.cuda()\n                target = target.cuda()\n            prediction = model(text)\n            loss = loss_fn(prediction, target)\n            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n            acc = 100.0 * num_corrects\/len(batch)\n            total_epoch_loss += loss.item()\n            total_epoch_acc += acc.item()\n\n    return total_epoch_loss\/len(val_iter), total_epoch_acc\/len(val_iter)","6a193779":"batch_size = 32\noutput_size = 11\nhidden_size = 256\nembedding_length = 100\nmodel = ClassifierModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)","daac80aa":"#architecture\nprint(model)\n\n#No. of trianable parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \nprint(f'The model has {count_parameters(model):,} trainable parameters')","987446e3":"learning_rate = 0.001\nloss_fn = F.cross_entropy","14926d15":"for epoch in range(5):\n    train_loss, train_acc = train_model(model, train_iter, epoch)\n    val_loss, val_acc = eval_model(model, valid_iter)\n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')","9270cccb":"test_loss, test_acc = eval_model(model, test_iter)\nprint(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')","fbd46926":"torch.save(model.state_dict(), 'saved_weights.pt')","05feb347":"test_sent = \"This game is interesting\"\ntest_sent = TEXT.preprocess(test_sent)\ntest_sent = [[TEXT.vocab.stoi[x] for x in test_sent]]\ntest_sent = np.asarray(test_sent)\ntest_sent = torch.LongTensor(test_sent)\ntest_tensor = Variable(test_sent)\ntest_tensor = test_tensor.cuda()\nmodel.eval()\noutput = model(test_tensor, 1)\nout = F.softmax(output, 1)\nout","ca70120f":"print(\"rating\",torch.argmax(out[0]).item())","29c8e89b":"## Why use embedding vectors?\/What does the embeddings do?","d866eeef":"LSTM (Long Short Term Memory) is recurrent neural network model and is mostly used for processing sequential data. Like in our case the text data is sequential by nature. Hence LSTM is usefull for NLP tasks. It is also a powerful model compared to Vanilla RNN. There are different variants of LSTM which can be experimented with","dad36536":"## References","7016097f":"## Training and Evaluation","d0df4c6c":"Lets see if we loaded the data properly","cc0a1056":"## Import required libraries","2e1bfceb":"- Overfitting\n1. The model training accuracy and loss are closely related to the validation accuracy and loss\n2. The model does not overfit. Also I had to use less amount of data due to resource limits","202adf50":"After 10 epochs on the dataset, average training accuracy was around 39.7% and validation accuracy about 36%. Surely this numbers can be increased by tuning the hyperparameters defined above, and training more.","b2f209ee":"The Embedding vectors defines the relations between different words based on several features. For example King is related to Queen just like a Man is related to Women. Another generic example is oange and apple, both are fruits and the relation is defined by embeddings.","8e408392":"## Prepare the dataset for pytorch torchtext\nData should be tokenized and numeric","a7ccade5":"## What is LSTM?","cd80e0c2":"#### https:\/\/pytorch.org\/text\/stable\/data.html\n#### https:\/\/pytorch.org\/tutorials\/beginner\/transformer_tutorial.html\n#### https:\/\/pytorch.org\/tutorials\/beginner\/text_sentiment_ngrams_tutorial.html\n#### https:\/\/github.com\/prakashpandey9\/Text-Classification-Pytorch\/blob\/master\/main.py\n#### https:\/\/www.analyticsvidhya.com\/blog\/2020\/01\/first-text-classification-in-pytorch\/\n#### https:\/\/towardsdatascience.com\/use-torchtext-to-load-nlp-datasets-part-ii-f146c8b9a496","f52148cf":"## create validation set","a9d8244f":"thats done. Lets find the maximum comment length","1c9fc790":"## Save the weights","083e4f18":"## Contribution and Findings","27af8079":"- Hyperparameters\n1. The values for embedding vectors and their dimensions can increase the number of parameters required by the program.\n2. Batch Size can be 16, 32, 64, .. In this notebook I have used 32.\n3. The number of layers in the model can be increased but not necessarily may have better results.\n4. The input length has been fixed at 200 characters but can be increased. The smaller text will be padded by default.","b2d6cd07":"## Evaluation","858b7158":"## Mohammed Furkhan, Shaikh","2effc4c0":"## Model","e15790f5":"We have removed the missing values from the dataset. Lets clean it up so we get only english characters.\nI'm also gonna round-up the ratings because I'm trying to solve the problem as classification problem and not regression.","d3d10112":"You see that there are some characters other than english alphabets and numbers.","31a0b796":"## Iterators for training and evaluation","f401a130":"We should be able to see the class values below","d9db8b22":"That's a lot and we dont need all of it to predict a rating. So we will fix the length during training.","d92a06fe":"Lets also drop rows which are having very few characters or words","f91ab670":"## for gradients","d508f4a8":"## Load The dataset and preprocess\nOnly require comment and rating columns from the reviews csv file","6ff93eeb":"## Create word embeddings","b11912bc":"## check on custom input text","23649055":"There seems to missing values in comment column. Lets remove all those rows.","f6046545":"## Create training and testing datasets","533ba4ab":"1. Data cleaning and preparation\n2. Explicityly based on torchtext and self preprocessed dataset.\n3. Different WordEmbedding Vectors and parameters\n4. Optimized the hyperparameters empirically\n5. Classifier based on 11 classes 0 - 10\n6. Deploying on cloud\n7. Faster processing using sclied data"}}