{"cell_type":{"23f06bf4":"code","1857c296":"code","b76204e1":"code","4a49b85e":"code","e8ca9ee3":"code","35977d0e":"code","2430ef8b":"code","ec1f8960":"code","16009a28":"code","69ff8426":"code","3881f10a":"code","9a81b8f7":"code","78ab0f49":"code","08e64709":"code","0507be15":"code","60c389c3":"code","ef35d499":"code","24d5f585":"code","f73712ff":"code","22755acc":"code","87d9b42a":"code","59680069":"markdown","2453b431":"markdown","8888db1d":"markdown","32290f45":"markdown","2a91423f":"markdown","a24bf344":"markdown","e314f7b6":"markdown","6c002cf3":"markdown","f8fb2dd3":"markdown","5214885d":"markdown"},"source":{"23f06bf4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\n\n\ndf = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\ndf1 = pd.read_csv(\"..\/input\/ensambleyay\/train_pred_xgb.csv\")\ndf2 = pd.read_csv(\"..\/input\/ensambleyay\/train_pred_lgbm.csv\")\ndf3 = pd.read_csv(\"..\/input\/ensambleyay\/train_pred_nn.csv\")\n\ndf1.rename(columns={\"pred_1\": \"pred_xgb\"}, inplace=True)\ndf2.rename(columns={\"pred_1\": \"pred_lgbm\"}, inplace=True)\ndf3.rename(columns={\"pred_1\": \"pred_nn\"}, inplace=True)\n\ndf_test1 = pd.read_csv(\"..\/input\/ensambleyay\/test_xgb.csv\")\ndf_test2 = pd.read_csv(\"..\/input\/ensambleyay\/test_lgbm.csv\")\ndf_test3 = pd.read_csv(\"..\/input\/ensambleyay\/test_nn.csv\")\n\ndf_test1.rename(columns={\"claim\": \"pred_xgb\"}, inplace=True)\ndf_test2.rename(columns={\"claim\": \"pred_lgbm\"}, inplace=True)\ndf_test3.rename(columns={\"claim\": \"pred_nn\"}, inplace=True)\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\n\n# df = df.merge(df1, on=\"id\", how=\"left\", rsuffix=\"_xgb\")\n# df = df.merge(df2, on=\"id\", how=\"left\", rsuffix=\"_lgbm\")\n# df = df.merge(df3, on=\"id\", how=\"left\", rsuffix=\"_nn\")\n\n# df_test = df_test.merge(df_test1, on=\"id\", how=\"left\", rsuffix=\"_xgb\")\n# df_test = df_test.merge(df_test2, on=\"id\", how=\"left\", rsuffix=\"_lgbm\")\n# df_test = df_test.merge(df_test3, on=\"id\", how=\"left\", rsuffix=\"_nn\")\n\ndf.head(10)","1857c296":"df_test.head(10)","b76204e1":"# from sklearn.linear_model import LinearRegression\n# from xgboost import XGBRegressor\n# useful_features = [\"pred_xgb\", \"pred_lgbm\"]\n# test = df_test[useful_features]\n\n# final_preds = []\n\n# X_train = df[useful_features]\n# y_train = df.claim\n\n# model = LinearRegression()\n# model.fit(X_train, y_train)\n\n# # model = XGBRegressor(n_estimators=100)\n# # model.fit(X_train, y_train)\n\n# predictions = model.predict(test)","4a49b85e":"# useful_features = [\"pred_xgb\", \"pred_lgbm\"]\n# test = df_test[useful_features]\n# X = df[useful_features]\n# y = df.claim\n\n# kfold = StratifiedKFold(n_splits = 10, random_state=42, shuffle=True)\n# preds = [0.0]*test.shape[0]\n# for idx in kfold.split(X=X, y=y):\n#     train_idx, val_idx = idx[0], idx[1]\n#     X_train = X.iloc[train_idx]\n#     y_train = y.iloc[train_idx]\n#     X_val = X.iloc[val_idx]\n#     y_val = y.iloc[val_idx]\n    \n#     model = LinearRegression()\n#     model.fit(X_train, y_train)\n    \n#     val_preds = model.predict(X_val)\n#     print(roc_auc_score(y_val, val_preds))\n#     predictions = model.predict(test)\n#     print(predictions.shape, predictions[:10])\n#     preds += predictions \/ kfold.n_splits\n# print(\"final\", preds[:10])","e8ca9ee3":"useful_features = [\"pred_xgb\", \"pred_lgbm\"]\ntest = df_test[useful_features]\nX = df[useful_features]\ny = df.claim","35977d0e":"plt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\npca = PCA()\nX_pca = pca.fit_transform(X)\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)\n\nprint(X.head())\nprint(X_pca.head())","2430ef8b":"loadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings","ec1f8960":"plot_variance(pca)","16009a28":"# mi_scores = make_mi_scores(X_pca, y, discrete_features=False)\n# mi_scores","69ff8426":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nestimator = HistGradientBoostingRegressor()\nparam_grid = {\"max_iter\" : [100, 200, 300, 400, 500, 600], \n              \"learning_rate\" : [1e-1, 1e-2, 1e-3]\n             }\n\nmy_scorer = make_scorer(roc_auc_score)\ngrid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5, verbose=10, scoring = my_scorer)\ngrid.fit(X, y)\n\nprint(grid.best_score_ , grid.best_params_)","3881f10a":"from sklearn.svm import SVC\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndef plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n    # Get Test Scores Mean and std for each grid search\n    scores_mean = cv_results['mean_test_score']\n    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n\n    scores_sd = cv_results['std_test_score']\n    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n\n    # Plot Grid search scores\n    _, ax = plt.subplots(1,1)\n\n    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n    for idx, val in enumerate(grid_param_2):\n        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n\n    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n    ax.set_xlabel(name_param_1, fontsize=16)\n    ax.set_ylabel('CV Average Score', fontsize=16)\n    ax.legend(loc=\"best\", fontsize=15)\n    ax.grid('on')\n\n# Calling Method \nplot_grid_search(grid.cv_results_, [100, 200, 300, 400, 500, 600] , [1e-1, 1e-2, 1e-3], 'max_iters', 'learning rate')","9a81b8f7":"# useful_features = [\"pred_xgb\", \"pred_lgbm\", \"pred_nn\"]\nuseful_features = [\"pred_xgb\", \"pred_lgbm\"]\ntest = df_test[useful_features]\nX = df[useful_features]\ny = df.claim\n\nkfold = StratifiedKFold(n_splits = 10, random_state=41, shuffle=True)\npreds = [0.0]*test.shape[0]\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nfor idx in kfold.split(X=X, y=y):\n    train_idx, val_idx = idx[0], idx[1]\n    X_train = X.iloc[train_idx].reset_index(drop=True)\n    y_train = y.iloc[train_idx].reset_index(drop=True)\n    X_val = X.iloc[val_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)\n    \n    model = HistGradientBoostingRegressor(max_iter = 200, learning_rate = 0.01)\n    model.fit(X_train, y_train)\n    val_preds = model.predict(X_val)\n    predictions = model.predict(test)\n    print(roc_auc_score(y_val, val_preds), predictions[:10])\n    \n    final_test_predictions.append(predictions)\n    final_valid_predictions.update(dict(zip(val_idx, val_preds)))\n    \nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_L2_1.csv\", index=False)\n\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsubmission.claim = np.mean(np.column_stack(final_test_predictions), axis=1)\nsubmission.to_csv(\"test_L2_1.csv\", index=False)\n\nprint(\"final\", predictions[:10])","78ab0f49":"estimator = XGBRegressor(tree_method=\"gpu_hist\",\n                         predictor=\"gpu_predictor\",\n                        random_state = 41)\n\nparam_grid = {\"n_estimators\" : [50, 100, 150, 200, 250, 300], \n              \"learning_rate\" : [1e-1, 1e-2, 1e-3]\n             }\n\nmy_scorer = make_scorer(roc_auc_score)\ngrid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5, verbose=10, scoring = my_scorer)\ngrid.fit(X, y)\n\nprint(grid.best_score_ , grid.best_params_)","08e64709":"plot_grid_search(grid.cv_results_, [50, 100, 150, 200, 250, 300], [1e-1, 1e-2, 1e-3], 'n_estimators', 'learning rate')","0507be15":"useful_features = [\"pred_xgb\", \"pred_lgbm\"]\ntest = df_test[useful_features]\nX = df[useful_features]\ny = df.claim\n\nkfold = StratifiedKFold(n_splits = 10, random_state=41, shuffle=True)\npreds = [0.0]*test.shape[0]\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nfor idx in kfold.split(X=X, y=y):\n    train_idx, val_idx = idx[0], idx[1]\n    X_train = X.iloc[train_idx].reset_index(drop=True)\n    y_train = y.iloc[train_idx].reset_index(drop=True)\n    X_val = X.iloc[val_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)\n    \n    model = XGBRegressor(n_estimators=150,\n                         learning_rate = 0.01,\n                         random_state = 41,\n                         tree_method=\"gpu_hist\",\n                         predictor=\"gpu_predictor\",\n                         verbosity=0, n_jobs=-1)\n    model.fit(X_train, y_train)\n    val_preds = model.predict(X_val)\n    predictions = model.predict(test)\n    print(roc_auc_score(y_val, val_preds), predictions[:10])\n    \n    final_test_predictions.append(predictions)\n    final_valid_predictions.update(dict(zip(val_idx, val_preds)))\n    \nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_L2_2.csv\", index=False)\n\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsubmission.claim = np.mean(np.column_stack(final_test_predictions), axis=1)\nsubmission.to_csv(\"test_L2_2.csv\", index=False)\n\nprint(\"final\", predictions[:10])","60c389c3":"df = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n\ndf1 = pd.read_csv(\"train_pred_L2_1.csv\")\ndf2 = pd.read_csv(\"train_pred_L2_2.csv\")\n\ndf1.rename(columns={\"pred_1\": \"pred_hist\"}, inplace=True)\ndf2.rename(columns={\"pred_1\": \"pred_xgb2\"}, inplace=True)\n\ndf_test1 = pd.read_csv(\"test_L2_1.csv\")\ndf_test2 = pd.read_csv(\"test_L2_2.csv\")\n\ndf_test1.rename(columns={\"claim\": \"pred_hist\"}, inplace=True)\ndf_test2.rename(columns={\"claim\": \"pred_xgb2\"}, inplace=True)\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")","ef35d499":"df.head(10)","24d5f585":"df_test.head(10)","f73712ff":"from sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\n# useful_features = [\"pred_xgb\", \"pred_lgbm\", \"pred_nn\"]\nuseful_features = [\"pred_hist\", \"pred_xgb2\"]\ntest = df_test[useful_features]\n\nfinal_preds = []\n\nX_train = df[useful_features]\ny_train = df.claim\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# model = XGBRegressor(n_estimators=100)\n# model.fit(X_train, y_train)\n\npredictions = model.predict(test)","22755acc":"# useful_features = [\"pred_xgb\", \"pred_lgbm\", \"pred_nn\"]\nuseful_features = [\"pred_hist\", \"pred_xgb2\"]\ntest = df_test[useful_features]\nX = df[useful_features]\ny = df.claim\n\nkfold = StratifiedKFold(n_splits = 10, random_state=42, shuffle=True)\npredictions = [0.0]*test.shape[0]\nfor idx in kfold.split(X=X, y=y):\n    train_idx, val_idx = idx[0], idx[1]\n    X_train = X.iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    X_val = X.iloc[val_idx]\n    y_val = y.iloc[val_idx]\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    \n    val_preds = model.predict(X_val)\n    print(roc_auc_score(y_val, val_preds))\n    preds = model.predict(test)\n    print(preds.shape, predictions[:10])\n    predictions += preds \/ kfold.n_splits\nprint(\"final\", preds[:10])","87d9b42a":"# submission = pd.read_csv(\"..\/input\/submissions-ensemble\/submission_nn.csv\")\nsample_submission.claim = predictions\nsample_submission.to_csv(\"submission_ens_stack3.csv\", index=False)\nsample_submission","59680069":"**The first model is a Histogram Gradient Boosted Regreesor, a lightweight model for making predictions on only two features. The visualization for the parameter searching is also generated.**","2453b431":"# **Looking at the predictions**\nIf we take a peek at the variance coverage, we can see that there not much difference between the two predictions.","8888db1d":"# **Choosing the models**\nEven though the initial plan was to use three models, the vanilla dense neural network model always dropped the score. So, the new plan was to discard the predictions made by neural networks. These will be the Level-0 models.","32290f45":"# **Straight To Blending**\nTried two models for blending - LinearRegression and XGBRegressor. \nLinearRegressor won.","2a91423f":"# **GridSearch for Level-1 models**","a24bf344":"# **Final Blending**","e314f7b6":"# **GridSearch and visualization for Level-1 XGBRegressor**","6c002cf3":"# **Blending but 10-fold**\nThis cell predicts with 10 weaker models trained on 10 folds rather than one model that trains on the entire training set. This can avoid overfitting.","f8fb2dd3":"**With the optimal parameters, validation and test predictions are made with a 10-fold distribution for the final blending.**","5214885d":"# **Final Blending but 10-fold**"}}