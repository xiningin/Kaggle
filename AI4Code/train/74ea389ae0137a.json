{"cell_type":{"da78e520":"code","28c67033":"code","ff7fb973":"code","9b3e9269":"code","6f890a84":"code","b549bd28":"code","42b2a3b2":"code","8e2d5950":"code","f8bf0115":"code","4b149e41":"code","cf24a7ed":"code","7bab4fd6":"code","440f9d3d":"code","5bf44c84":"code","a6490bd8":"code","3f3a8ac0":"markdown"},"source":{"da78e520":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize,RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import f1_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28c67033":"# Import the tweet data\n\ntweet_data_train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntweet_data_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet_data_train.head()","ff7fb973":"tweet_data_train.info()","9b3e9269":"# This function will contain the entire code to create the feature column from the existing.\n\ndisaster_words=['forest','fire','forest fire','earthquake',\n                'landslide','typhoon','hurricane','attack',\n                'ablaze','rains','avalanche','rescue','help',\n                'hurt','god','hell','died','injured','succumbed',\n                'mayhem','torrential','devastation','terror',\n                'stuck','storm','unpleasant','havoc','terrorist',\n                'tsunami','wildfire','hailstorm','snowfall',\n                'sinkhole','pelting','war','riot','weapon',\n                \n                'forests','fires','forest fires','earthquakes',\n                'landslides','typhoons','hurricanes','attacks',\n                'ablazes','rain','avalanches','rescues','terrors'\n                ,'storms','terrorists','tsunamis','wildfires',\n                'hailstorms','snowfalls','sinkholes','peltings',\n                'riots','weapons',\n                \n               '#forest','#fire','#forest fire','#earthquake',\n                '#landslide','#typhoon','#hurricane','#attack',\n                '#ablaze','#rains','#avalanche','#rescue','#help',\n                '#hurt','#god','#hell','#died','#injured','#succumbed',\n                '#mayhem','#torrential','#devastation','#terror',\n                '#stuck','#storm','#unpleasant','#havoc','#terrorist',\n                '#tsunami','#wildfires','#wildfire','#hailstorm','#snowfall',\n                '#sinkhole','#pelting','#war','#riot','#weapon'\n               \n               '#forests','#fires','#forest fires','#earthquakes',\n                '#landslides','#typhoons','#hurricanes','#attacks',\n                '#ablazes','#rain','#avalanches','#rescues','#terrors'\n                ,'#storms','#terrorists','#tsunamis','#wildfires',\n                '#hailstorms','#snowfalls','#sinkholes','#peltings',\n                '#riots','#weapons']\nkeywords=list(tweet_data_train['keyword'])+list(tweet_data_test['keyword'])\ndisaster_words+=keywords\ndisaster_words=list(set(disaster_words))\ndef make_feature_col(x):\n    \n    x['Number of Disaster Words']=pd.Series()\n    tokenizer = RegexpTokenizer(r'\\w+')\n    to_be_removed = set(stopwords.words('english'))\n    c=0\n    for i in range(len(x)):\n        \n        tokens=tokenizer.tokenize(x['text'][i].lower())\n        new_tokens=[word for word in tokens if not word in to_be_removed]\n        for j in range(len(new_tokens)):\n            if(new_tokens[j] in disaster_words):\n                c+=1\n        x['Number of Disaster Words'].iloc[i]=c\n        c=0\n    return x","6f890a84":"! pip install wordcloud","b549bd28":"from wordcloud import WordCloud\nunique_string=(\" \").join(disaster_words[1:])\nwordcloud = WordCloud(width = 1000, height = 500).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","42b2a3b2":"tweet_data_train=make_feature_col(tweet_data_train)\ntweet_data_train.head()","8e2d5950":"new_tweet_data_train=tweet_data_train.loc[:,['id','Number of Disaster Words','target']]","f8bf0115":"new_tweet_data_train","4b149e41":"# Test Data\n\ntweet_data_test=make_feature_col(tweet_data_test)\ntweet_data_test=tweet_data_test.loc[:,['id','Number of Disaster Words']]\ntweet_data_test.head()\n","cf24a7ed":"# Model Building\n\nX=new_tweet_data_train['Number of Disaster Words']\nX=np.array(X).reshape(-1,1)\ny=new_tweet_data_train['target']\n\n\ndtc = DecisionTreeClassifier(max_depth=10000)\nrfc = RandomForestClassifier()\ndtc.fit(X, y)\nrfc.fit(X,y)\n\nprint('Accuracy of Decision Tree Classifier on training set: {:.2f}'\n     .format(dtc.score(X, y)))\nprint('Accuracy of Random Forest Classifier on training set: {:.2f}'\n     .format(rfc.score(X, y)))\n\nXTest=tweet_data_test['Number of Disaster Words']\n","7bab4fd6":"pred_dtc=dtc.predict(np.array(XTest).reshape(-1,1))\nprint(pred_dtc)\n\npred_rfc=rfc.predict(np.array(XTest).reshape(-1,1))\nprint(pred_rfc)","440f9d3d":"len(pred_dtc)\nlen(pred_rfc)","5bf44c84":"output_dtc=pd.DataFrame({'id':tweet_data_test['id'],'target':pred_dtc})\noutput_rfc=pd.DataFrame({'id':tweet_data_test['id'],'target':pred_rfc})","a6490bd8":"output_rfc.to_csv('submission.csv',index=False)","3f3a8ac0":"This code is to predict whether the tweet is a natural disaster tweet or not. Normally a human mind decides this by using some pre-fed words and phrases and their intensity. For Eg. if someone says \"shock\" we consider it as a state of mind but when someone says \"Electric Shock\" we perceive it differently due to its intensity.\n\nThus for the machine to understand this we need to feed in that intelligence, especially those pre-fed words using which it can decide as to what is the tweet's classification type.\n<br>\nComing to the code, we will complete the notebook in phases :\n<br>\nPhase 1 - Cleaning and Preprocessing<br>\nPhase 2 - Model Building<br>\nPhase 3 - Model Testing\n<br><br>\nPHASE 1 <br>\nThe data is cleaned and processed by:\n* Removing Nulls\n* Removing Duplicates\n* Checking and Formatting Column DataTypes\n* Reshaping the data if needed\n* Word Tokenization and Count Vector\n* Tweet Understanding using the vector\n* New Dataset creation\n* Sanity Checks on the new dataset\n<br>\n\nPHASE 2<br>\nThe model used to classify will be the Logistic Regression.\n<br><br>\nPHASE 3<br>\nThe model is run on test data and evaulated for its primary metric.<br><br>\n\nIn Tweet Understanding we find the number of disaster words in the tweet using a pre-fed dataset and make a new column as our feature vector."}}