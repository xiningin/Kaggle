{"cell_type":{"b5b2ecee":"code","68ef0267":"code","4340a34a":"code","85bc50e3":"code","60e12dc1":"code","f447752e":"code","aaa567ac":"code","c74c3f1e":"code","2939efeb":"code","68345fe5":"code","9eb34773":"code","8053f0de":"code","c2c38fb7":"code","2375d1e4":"code","488e4f5e":"code","4e39f3b4":"code","39e95095":"code","f0da4da2":"code","bce6288e":"code","5602d0ad":"code","d19b61dd":"code","7e5df8d5":"code","e44d1dda":"code","5a93a2ba":"code","e9b24f59":"code","66989164":"code","f4fd2918":"code","32126e33":"code","2642a407":"code","6d42ec60":"markdown","b6d8f03d":"markdown","b11c0a4f":"markdown","5e2db74f":"markdown","0f5e34e6":"markdown","96899e50":"markdown","b26a68cf":"markdown","30f1254a":"markdown","a231b3fe":"markdown","1a7a7321":"markdown","1e7045b1":"markdown","226530d2":"markdown","99f3dd2e":"markdown","430a5e28":"markdown","906ac46b":"markdown","23f962da":"markdown","5a2690d8":"markdown","610a7743":"markdown","cbe48b6e":"markdown","ade998e7":"markdown"},"source":{"b5b2ecee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport json\nfrom PIL import Image\n\nBASE_DIR = \"..\/input\"\n\nIMAGES_TRAIN_DIR = f\"{BASE_DIR}\/train\"\nIMAGES_TEST_DIR = f\"{BASE_DIR}\/test\"\nTRAIN_CSV = f\"{BASE_DIR}\/train.csv\"\nLABEL_DESCRIPTIONS = f\"{BASE_DIR}\/label_descriptions.json\"\n# Any results you write to the current directory are saved as output.","68ef0267":"#print(os.listdir(\"..\/input\"))\ntrain_df = pd.read_csv(TRAIN_CSV)\n\nwith open(LABEL_DESCRIPTIONS) as f:\n    image_info = json.load(f)\ncategories = pd.DataFrame(image_info['categories'])\nattributes = pd.DataFrame(image_info['attributes'])\nprint(\"There are descriptions for\", categories.shape[0],\"categories and\", attributes.shape[0], \"attributes\")\n\ntrain_df['hasAttributes'] = train_df.ClassId.apply(lambda x: x.find(\"_\") > 0)\ntrain_df['CategoryId'] = train_df.ClassId.apply(lambda x: x.split(\"_\")[0]).astype(int)\ntrain_df = train_df.merge(categories, left_on=\"CategoryId\", right_on=\"id\")","4340a34a":"categories.head()","85bc50e3":"attributes.head()","60e12dc1":"print(\"Fraction of mask annotations with any attributes within train data:\", train_df.hasAttributes.mean())","f447752e":"subset = train_df[~train_df.hasAttributes]\nsupercategory_names = np.unique(subset.supercategory)\nplt.figure(figsize=(10, 10))\ng = sns.countplot(x = 'supercategory', data=subset, order=supercategory_names)\nax = g.axes\ntl = [x.get_text() for x in ax.get_xticklabels()]    \nax.set_xticklabels(tl, rotation=90)\nfor p, label in zip(ax.patches, supercategory_names):\n    c = subset[(subset['supercategory'] == label)].shape[0]\n    ax.annotate(str(c), (p.get_x(), p.get_height() + 1000))\nplt.title(\"Supercategories with no attributes\")\nplt.show()\n","aaa567ac":"subset = train_df[train_df.hasAttributes]\nsupercategory_names = np.unique(subset.supercategory)\ng = sns.countplot(x = 'supercategory', data=subset, order=supercategory_names)\nax = g.axes\ntl = [x.get_text() for x in ax.get_xticklabels()]    \nax.set_xticklabels(tl, rotation=90)\nfor p, label in zip(ax.patches, supercategory_names):\n    c = subset[(subset['supercategory'] == label)].shape[0]\n    ax.annotate(str(c), (p.get_x()+0.3, p.get_height() + 50))\nplt.title(\"Supercategories with any attributes\")\nplt.show()","c74c3f1e":"supercategory_names = train_df[['supercategory', 'name']].groupby('supercategory').agg(\n    lambda x: x.unique().shape[0]).reset_index().sort_values(\"name\", ascending=False).set_index('name')\nsupercategory_names","2939efeb":"def buildPlot(**kwargs):\n    data = kwargs['data']\n    g = sns.countplot(y=\"name\", data=data)\n    g.set_yticklabels(data['name'].unique())#, rotation=90)\n    \nidx = train_df.supercategory.isin(['decorations', 'garment parts', 'upperbody'])\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)\n","68345fe5":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[4].values)\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","9eb34773":"total = train_df.ImageId.unique().shape[0]\nprint(f\"There are {total} images in train dataset.\")\nimages_with_shoes = train_df[train_df.name==\"shoe\"].ImageId.unique().shape[0]\nimages_with_legs = train_df[train_df.supercategory==\"legs and feet\"].ImageId.unique().shape[0]\nprint(f\"However, only {images_with_legs} images have associated legs and feet annotation, and only {images_with_shoes} have any shoes on it.\")","8053f0de":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[3].values)\ng = sns.FacetGrid(data=train_df[idx], row=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","c2c38fb7":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[2].values)\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","2375d1e4":"idx = train_df.supercategory.isin(supercategory_names.supercategory.loc[1].values)\ng = sns.FacetGrid(data=train_df[idx], col=\"supercategory\", sharey=False)\ng = g.map_dataframe(buildPlot)","488e4f5e":"#extract all available attributes and create separate table\ncat_attributes = []\nfor i in train_df[train_df.hasAttributes].index:\n    item = train_df.loc[i]\n    xs = item.ClassId.split(\"_\")\n    for a in xs[1:]:\n        cat_attributes.append({'ImageId': item.ImageId, 'category': int(xs[0]), 'attribute': int(a)})\ncat_attributes = pd.DataFrame(cat_attributes)\n\ncat_attributes = cat_attributes.merge(\n    categories, left_on=\"category\", right_on=\"id\"\n).merge(attributes, left_on=\"attribute\", right_on=\"id\", suffixes=(\"\", \"_attribute\"))","4e39f3b4":"# helper objects and methods\nscat_x, count_x = np.unique(cat_attributes['supercategory'], return_counts=True)\ncategories_by_x = {\n    x: dict(cat_attributes[cat_attributes['supercategory'] == x][['name', 'category']].drop_duplicates().values)\n    for x in scat_x}\nscat_y, count_y = np.unique(cat_attributes['supercategory_attribute'], return_counts=True)\ncategories_by_y = {\n    y: dict(cat_attributes[cat_attributes['supercategory_attribute'] == y][['name_attribute', 'attribute']].drop_duplicates().values) \n    for y in scat_y}\nvals = cat_attributes.groupby(['category', 'attribute']).count().reset_index(drop=True).values[:,0]\nscale_min, scale_max = vals.min(), vals.max()\n\ndef get_scatter_data(x, y, cat, attr):\n    ids_x = {cat[k]: i for i, k in enumerate(cat)}\n    ids_y = {attr[k]: i for i, k in enumerate(attr)}\n    data = np.zeros((len(cat), len(attr)), dtype=np.uint)\n    for k, v in zip(x, y):\n        data[ids_x[k], ids_y[v]]+=1\n    ii, jj = np.where(data > 0)\n    sizes = [data[i, j] for i, j in zip(ii, jj)]\n    return ii, jj, sizes\n\ndef drawPunchcard(**kwargs):\n    data = kwargs['data']\n    x = data[\"category\"]\n    y = data[\"attribute\"]\n    supercategory_x = data[\"supercategory\"].values[0]\n    cat = categories_by_x[supercategory_x]\n    supercategory_y = data[\"supercategory_attribute\"].values[0]\n    attr = categories_by_y[supercategory_y]\n    ii, jj, sizes = get_scatter_data(\n        x, y, \n        cat, \n        attr)\n    g = sns.scatterplot(ii, jj, size=sizes, sizes=(20, 200), hue=np.log(sizes)+1)\n    g.set_xticks(np.arange(len(cat)))\n    g.set_xticklabels(list(cat), rotation=90)\n    g.set_yticks(np.arange(len(attr)))\n    g.set_yticklabels(list(attr))","39e95095":"sns.color_palette(\"bright\")\nsns.set(font_scale=1.0)\nsns.set_style(\"white\")\nwidth_ratios=[len(categories_by_x[x]) for x in categories_by_x]\nheight_ratios=[len(categories_by_y[x]) for x in categories_by_y]\ng = sns.FacetGrid(data=cat_attributes, col=\"supercategory\",  row=\"supercategory_attribute\", \n                  #margin_titles=True, \n                  gridspec_kws={'height_ratios': height_ratios, 'width_ratios': width_ratios},\n                  sharex=\"col\", sharey=\"row\",\n                  col_order=list(categories_by_x),\n                  row_order=list(categories_by_y))#.set_titles('{col_name}', '{row_name}')\ng = g.map_dataframe(drawPunchcard).set_titles('{col_name}', '{row_name}')\ng.fig.set_size_inches(10, 20) \nfor ax, cat_name in zip(g.axes, list(categories_by_y)):\n    ax[-1].set_ylabel(cat_name, labelpad=10, rotation=-90)\n    ax[-1].yaxis.set_label_position(\"right\")","f0da4da2":"images = train_df[['ImageId', \"Width\", \"Height\"]].drop_duplicates()\nprint(\"Number of unique triplets (ImageId, Width, Height):\", images.shape[0])\nprint(\"Unique image names: \", images['ImageId'].unique().shape[0])","bce6288e":"def read_image_dimensions(path):\n    \"returns real width and height\"\n    with Image.open(path) as image:\n        dimensions = image.size\n    return dimensions\n\nimages_with_incorrect_size = {}\nfor ImageId, width, height in images.values:\n    image_path = os.path.join(IMAGES_TRAIN_DIR, ImageId)\n    (real_width, real_height) = read_image_dimensions(image_path)\n    if real_width != width or real_height!=height:\n        images_with_incorrect_size[ImageId] = (real_width, real_height)","5602d0ad":"print(\"Number of images with incorrect dimensions:\", len(images_with_incorrect_size))","d19b61dd":"for ImageId in images_with_incorrect_size:\n    (width, height) = images_with_incorrect_size[ImageId]\n    idx = train_df['ImageId'] == ImageId\n    print(ImageId, train_df.loc[idx, \"Width\"].values[0], train_df.loc[idx, \"Height\"].values[0], \"Real dimensions:\", width, height)\n    train_df.loc[idx, \"Width\"] = width\n    train_df.loc[idx, \"Height\"] = height","7e5df8d5":"df = train_df[[\"ImageId\", \"EncodedPixels\", \"ClassId\"]].drop_duplicates()\ngrouped_df = df.groupby([\"EncodedPixels\", \"ImageId\"]).count().reset_index()\ngrouped_df = grouped_df[grouped_df.ClassId > 1]\nprint(\"Number of images with duplicated EncodedPixels:\", grouped_df.shape[0])","e44d1dda":"duplicated_data = df[df.ImageId.isin(grouped_df.ImageId) & df.EncodedPixels.isin(grouped_df.EncodedPixels)].sort_values([\"ImageId\", \"EncodedPixels\"])\nduplicated_data.to_csv(\"images_with_duplicated_masks.csv\", index=None) # you can look at these images, if you want","5a93a2ba":"duplicates = dict()\nxlabels, ylabels = set(), set()\n\nfor (ImageId, EncodedPixels), x in duplicated_data.groupby([\"ImageId\", \"EncodedPixels\"]):\n    pair = tuple(sorted(x.ClassId.values))\n    s,e = pair\n    xlabels.add(s)\n    ylabels.add(e)\n    if not pair in duplicates:\n        duplicates[pair] = 0\n    duplicates[pair] +=1\n\nxlabels = {x: i for i, x in enumerate(sorted(xlabels))}\nylabels = {x: i for i, x in enumerate(sorted(ylabels))}\nmatrix = np.zeros((len(ylabels), len(xlabels)), dtype=np.int)\nfor (s, e) in duplicates:\n    matrix[ylabels[e], xlabels[s]] = duplicates[(s, e)]","e9b24f59":"plt.figure(figsize=(10,10))\nannot = np.array([[(str(x) if x >0 else \"\") for x in line]for line in matrix])\nsns.heatmap(matrix, annot=annot, fmt=\"s\",xticklabels=sorted(list(xlabels)), yticklabels=sorted(list(ylabels)), square=True, cbar=False)","66989164":"categories[categories.id.isin((32, 35))]","f4fd2918":"def sum_mask_pixels(encoded_pixels):\n    pixels = [np.int(x) for x in encoded_pixels.split(\" \")]\n    return np.sum(pixels[1::2])\n\ndef compute_mask_percentage(row):\n    s = sum_mask_pixels(row['EncodedPixels'])\n    return 1.0* s\/row[\"Width\"]\/row[\"Height\"]\n\ntrain_df['mask_fraction'] = train_df.EncodedPixels.apply(sum_mask_pixels).astype(np.float)\ntrain_df['mask_fraction'] = train_df['mask_fraction']\/train_df[\"Width\"]\/train_df[\"Height\"]","32126e33":"plt.figure(figsize=(10, 8))\ng = sns.stripplot(y=\"mask_fraction\", data=train_df, x=\"supercategory\")\nlabels = [x.get_text() for x in g.get_xticklabels()]\ng = g.set_xticklabels(labels, rotation=90)","2642a407":"def draw_images(data=None,**kwargs):\n    plt.axis(\"off\")\n    path = os.path.join(IMAGES_TRAIN_DIR, data['ImageId'].values[0])\n    with Image.open(path) as image:\n        data = np.asarray(image)\n    plt.imshow(data)\n\nsubset = train_df[train_df.mask_fraction > 0.7]\ngrid = sns.FacetGrid(subset, col=\"name\", col_wrap=4)\ngrid.map_dataframe(draw_images)","6d42ec60":"# Connecting categories and attributes","b6d8f03d":"Next we fix annotation file.","b11c0a4f":"# Conclusions and other ideas\n1. It is quite clear that for the draft segmentation model it is sufficient to use mask's category identifier and to ignore mask's attributes.\n2. It is unclear (at least for me) if the train dataset attributes are given for all possible images, or are these attributes  provided for only a small subset of images. Do we have weakly-labeled data or fully-labeled data?\n3. Train dataset contains a very diverse collection of images, with a lot of small details. There are also images which contain only 1 apparel, already clipped. \n4. Almost a half of the images have no shoes annotation. It means that human body pictured on them (if there is any) is probably clipped. \n5. We can split train dataset to several parts. One of these parts might contain clipped clothes, the other might contain full human body. And maybe it will improve segmentation results for both subsets. But maybe not.","5e2db74f":"The most common duplicated mask is marked with image categories with ids 32 and 35.","0f5e34e6":"Supercategories might be the key to answering following questions:\n1. How many mask annotations have any associated attributes?\n2. How often specific attributes (or attribute groups) appear within category's supercategory?\n3. Is there a way to filter train data somehow?\n4. etc.","96899e50":"Relative counts for 3 supercategories (`decorations`, `garment parts`, `upperbody`) are shown below:","b26a68cf":"# Masking","30f1254a":"We named this parameter `mask_fraction`. We use supercategories to compare how `mask_fraction` distribution differs from supercategory to supercategory:","a231b3fe":"Pockets and zipper are usually located nearby, and they are relatively small. There also might be pockets with zipper. Maybe not a mistake after all?","1a7a7321":"These sample images are all clipped. They have particular clothes shown on it, but there is no human on the photo.","1e7045b1":"## 1. Check Width and Height correctness\nThere are no images with different width and height parameters in `train.csv` file. That doesn't mean that there are no errors. Just in case we'll check all dimensions for train images, and compare them with the ones provided in annotation file.","226530d2":"# Supercategories","99f3dd2e":"As you can see from previous 2 plots, number of mask annotations with any attribute is relatively small. Only masks related to 4 supercategories (well, 3, if we neglect 3 mask annotations related to `garment parts`) have any associated attribute. For every other supercategory, we might ignore them.","430a5e28":"## 2. Check if there are any duplicates\nWe also check if there are any image masks with several accociated classes (we might want to ignore them while training our segmentation model).","906ac46b":"Both attributes and categories contain additional `supercategory` column, which might be the source of insights related to our data. Cloths from the same supercategory are similar in some sense. Attribute's supercategory denotes that it describes some specific property (I.e., length, style).","23f962da":"# Closer look at train data","5a2690d8":"Now we will take a closer look at train dataset and check how many categories are presented there. What supercategories are varied the most?","610a7743":"In the dataset the most common `wholebody` cloth type is `dress`. `Shoe` dominates in 2nd most common supercategory (`legs and feet`), that means that it might be worth to search for shoes on the photos :)","cbe48b6e":"The biggest masks (`mask_fraction` > 0.7) are associated only with 3 supercategories - `lowerbody`, `upperbody` and `wholebody`. Masks associated with `neck`, `arms and hands`, `closures` are almost always small (< 0.1 of total image pixels).\n\nLet's take a look at some big masks (`mask_fraction` > 0.7):","ade998e7":"For every mask in train dataset, we convert number of masks pixels to a fraction of total image pixels:"}}