{"cell_type":{"c6dc1acc":"code","635ec8c0":"code","ca3482ea":"code","e305cb87":"code","b59fe130":"code","46b197dd":"code","47253fb6":"code","5cbbd5a0":"code","db416732":"code","7b722412":"code","257b698e":"code","bd1d46c9":"code","7c5a0aea":"code","50b4f1b2":"code","521628bf":"code","36ad0c79":"code","37cab7e8":"code","2c6f07cf":"code","977325f9":"code","db9e5cf6":"code","79c8f172":"code","8487a3a6":"code","dc4d89cc":"code","a6dd4eb9":"code","bd5a74d7":"code","9de0da8f":"code","b2f91aa0":"code","727fc93f":"code","5571d8ef":"code","0710f997":"code","e3daaaef":"code","eabf074b":"markdown","b91739c8":"markdown","ff395c53":"markdown","d035fb73":"markdown","861718f0":"markdown","429927c1":"markdown","a58339e4":"markdown","bb222d02":"markdown","957d314b":"markdown","e1839375":"markdown","6f53827b":"markdown","5c94d28a":"markdown","414739d5":"markdown","ecfcfc63":"markdown","e5d83f87":"markdown","7576e5ee":"markdown","3aa67209":"markdown","0f0bbf8b":"markdown","b1e8e34d":"markdown","26928230":"markdown","d554e163":"markdown","3a5bad41":"markdown","830427bf":"markdown","b10d1cae":"markdown","bcc74ec1":"markdown","fde2b40a":"markdown","2bbcdc9b":"markdown","7d830299":"markdown","c74831fc":"markdown","5df2d523":"markdown","bb8bdb3e":"markdown","ed4b13e2":"markdown","4b8f87a0":"markdown","258103af":"markdown","97f6c146":"markdown","eaa3e73a":"markdown","5042e77c":"markdown","6dc86e60":"markdown","728b6196":"markdown","15380149":"markdown"},"source":{"c6dc1acc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file = os.path.join(dirname, filename)\n        print('File path obtained.')\n\n# Need to specifiy encoding for the data to load correctly\ndf = pd.read_csv(file,encoding='ISO-8859-1')\nprint('Dataset loaded.')\ndf.head()","635ec8c0":"df.info()","ca3482ea":"# Find nas, drop duplicate rows, drop Unnamed: 8 column\n\ndef find_nas(df_in):\n    # Returns series with % of na for each column in df_in\n    cols = df_in.columns.values\n    na_per = []\n    for x in cols:\n        na_per.append((df_in[x].isna().sum()\/len(df_in.index))*100)\n\n    ser_out = pd.Series(na_per, index = cols)\n    ser_out = ser_out.sort_values(ascending = True)\n    return ser_out\n\ndef make_na_plot(na_ser):\n    # Creates a bar plot for the % of na values given from na_ser\n    blue = '#008fd5'\n    na_ser.plot(kind = 'barh', figsize = (10,5), color = blue)\n    \n    plt.title('% of na values', fontsize=24)\n    plt.xlabel('% na', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.ylabel('Columns', fontsize=18)\n    plt.yticks(fontsize=14)\n\n    plt.show()\n    \ndef basic_clean(data):\n    # Drop Unnamed: 8 column, remove duplicate rows and find nas\n    \n    # Drop column since it has no info\n    data.drop(['Unnamed: 8'], axis = 1, inplace = True)\n    print('Unnamed: 8 column dropped.')\n    \n    # Number of duplicate rows\n    num_dup = len(data)-len(data.drop_duplicates())\n    print('Number of duplicate rows on the dataset = {}\/{} ({}%)'.format(num_dup, len(data), (num_dup\/len(data))*100))\n    # Remove duplicate rows\n    data.drop_duplicates(inplace = True)\n    data.reset_index(inplace = True, drop = True)\n    print('Duplicates have been dropped. {} rows left.\\n'.format(len(data)))\n    \n    # Number of nas\n    make_na_plot(find_nas(data))\n    \n    return data\n    \n\ndf = basic_clean(df)","e305cb87":"# Rows with null Descriptions also have null CustomerIDs\nprint(df.Description.isna().sum() == df.loc[df.Description.isna()].CustomerID.isna().sum())","b59fe130":"# Drop nas and change data types\n\ndef drop_na(data):\n    data.dropna(inplace = True)\n    print('NAs droped. ', len(data.index), ' non-NA values left.')\n    return data\n\ndef change_dt(data):\n    # Change data types to more appropriate ones\n    data['CustomerID'] = data['CustomerID'].astype(int).astype(str) # CustomerID is float, change to object\n    data['InvoiceDate'] =  pd.to_datetime(data['InvoiceDate']) # InvoiceDate should be datetime\n    print('Date types converted.\\n')\n    data.info()\n    return data\n\ndf = drop_na(df)\ndf = change_dt(df)","46b197dd":"# See all the different entries of the Country column\nprint(df['Country'].value_counts())\n\n# Drop all rows with 'Unspecified' in Country\nunspec_country_id = list(df[df['Country'] == 'Unspecified'].index)\ndf.drop(unspec_country_id, inplace = True)\ndf.reset_index(inplace = True, drop = True)\nprint('\\nDeleted all rows that have \"Unspecified\" as their Country entry.')","47253fb6":"# See all the different entries of the column\nprint(df['CustomerID'].value_counts())","5cbbd5a0":"# Check how many letters, numbers, spaces or other characters the CustomerID values have\n\ndef check_str(s):\n    # Returns number of numbers, letters, spaces or other char in s\n    numbers = sum(c.isdigit() for c in s)\n    letters = sum(c.isalpha() for c in s)\n    spaces  = sum(c.isspace() for c in s)\n    others  = len(s) - numbers - letters - spaces\n    return numbers, letters, spaces, others\n\ndef check_col_vals(col):\n    # # Returns number of numbers, letters, spaces or other char in column col\n    dft = df.copy()\n    # Check what kinds of chars are in InvoiceNo\n    dft['nlso'] = dft[col].apply(check_str)\n    print('Format: (# numbers, # letters, # spaces, # other string chars)')\n    print(dft['nlso'].value_counts())\n    print('-'*70)\n    \ncheck_col_vals('CustomerID')","db416732":"# See all the different entries of the column\nprint(df['StockCode'].value_counts())","7b722412":"# Check how many letters, numbers, spaces or other characters the StockCode values have\ncheck_col_vals('StockCode')","257b698e":"# Take a look at StockCode values with \" \" in them\ndft = df.copy()\ndft['space'] = dft.StockCode.apply(lambda x: 1 if \" \" in x else 0)\ndft.loc[dft.space == 1].head()","bd1d46c9":"print(df['Description'].value_counts())","7c5a0aea":"a = len(df[df['Description'] == 'WHITE HANGING HEART T-LIGHT HOLDER'])\nb = len(df[df['Description'] == 'CREAM HANGING HEART T-LIGHT HOLDER'])\nprint('WHITE HANGING HEART T-LIGHT HOLDER: {}'.format(a))\nprint('CREAM HANGING HEART T-LIGHT HOLDER: {}'.format(b))\nprint('The same StockCode has been used for both of the above products.')","50b4f1b2":"# Compare value_count series length for StockCode and Description\ndef compare_val_counts(df_in, col1, col2):\n    # Compares the length of the value_count() series of col1 and col2\n    \n    # The value count series for the Description column\n    descr_df = df_in[col1].value_counts()\n    descr_df = descr_df.reset_index()\n    descr_df.columns = [col1, col1[0].lower() + 'vc']\n\n    # The value count series for the StokCode column\n    stock_df = df_in[col2].value_counts()\n    stock_df = stock_df.reset_index()\n    stock_df.columns = [col2, col2[0].lower() + 'vc']\n\n    print('Length of Description Series: {} \\nLength of StockCode Series: {} \\nDifference in length (D-S): {}'.format(len(descr_df), len(stock_df), len(descr_df)-len(stock_df)))\n\n\ncompare_val_counts(df, 'Description', 'StockCode')","521628bf":"# Take a look at duplicated values for StockCode and Description columns\ndef find_dup(df_in, col):\n    \"\"\"    \n    Works with col == 'Description' or 'StockCode'.\n    Find which values of col (say 'Description') are used for more than one value of StockCode.\n    Return them in a df together with their StockCode. (Replace 'Description' with 'StockCode' for col == 'StockCode').\n    \"\"\"\n    df_doub = df_in.copy()\n    # Keep only 'Description' and 'StockCode'\n    df_doub.drop(columns = ['InvoiceNo', 'Quantity', 'InvoiceDate', 'UnitPrice', 'CustomerID','Country'], inplace = True)\n\n    # Get rid of all the rows that are the same\n    df_doub.drop_duplicates(inplace = True) \n\n    # Find all duplicate values in col\n    df_doub = df_doub[df_doub.duplicated([col], keep = False)]\n\n    # Sort the df to see them clearly\n    df_doub.sort_values(col, inplace = True)\n    return df_doub\n\ndef print_dupinfo(df_in):\n    # Prints the lengths of the value_count() series for the StockCode and Description columns\n    # Also returns the top 5 entries of the df containing which values are duplicated for each column\n    df1 = find_dup(df_in, 'StockCode')\n    df2 = find_dup(df_in, 'Description')\n\n    print('The number of StockCodes that are used more than once is {}.'.format(len(df1.drop_duplicates('StockCode'))))\n    print('The number of Descriptions that are used more than once is {}.'.format(len(df2.drop_duplicates('Description'))))\n\n    print('-'*70)\n    print(df1.head())\n    print('-'*70)\n    print(df2.head())\n    print('-'*70)\n\n\nprint_dupinfo(df)","36ad0c79":"\"\"\" \nThe problem is that we have the same col1 values for different col2 values when there should be a 1-1 correspondance between the values of col1 and col2.\nSo what we want to do is this: take all the col2 values that correspond to a specific col1 value, and replace them all with the first col2 value that appears for that\nspecific col1 value.\n\"\"\"\ndef repl_dup(df_in, col1, col2):\n    # Replace col2 values that correspond to the same col1 values by the col2 value that appears first for each col1 value\n    df_out = df_in.copy()\n    \n    # Replace all the Descriptions that correspond to duplicated StockCodes with None\n    df_out.loc[df_out.duplicated([col1]), col2] = None \n\n    # Fill in None Description values with their first appearance\n    df_out.loc[df_out[col2].isnull(), col2] = df_out.loc[df_out[col2].isnull(), col1].map(df_out.loc[df_out[col2].notnull()].set_index(col1)[col2])\n    return  df_out\n\n# Replace duplicate Description values that correspond to the same StockCode\ndf = repl_dup(df, 'StockCode', 'Description')\n# Replace duplicate StockCode values that correspond to the same Description\ndf = repl_dup(df, 'Description', 'StockCode')\n \n# See info about duplicated values \nprint_dupinfo(df)\ncompare_val_counts(df, 'Description', 'StockCode')","37cab7e8":"# Clean up Description column\ndef clean_descr(df_in):\n    # Removes ',', '.' and strips the Description values\n    df_in.Description = df_in.Description.str.replace(',', ' ')\n    df_in.Description = df_in.Description.str.replace('.', ' ')\n    df_in.Description = df_in.Description.str.strip()\n    print(\"Description column: Punctuation removed and values were stripped of spaces in the beginning and end.\")\n    return df_in\n\ndf = clean_descr(df)","2c6f07cf":"print(df.InvoiceDate.value_counts())","977325f9":"check_col_vals('InvoiceNo')","db9e5cf6":"# Create a copy so that we preserve the initial df\ndft = df.copy()\n# Check what kinds of chars are in InvoiceNo\ndft['nlso'] = dft.InvoiceNo.apply(check_str)\nprint(dft.loc[dft.nlso == (6,1,0,0), 'InvoiceNo'].value_counts().sort_index())","79c8f172":"# Check correlation\ncols_num = ['Quantity', 'UnitPrice']\ncorr = df[cols_num].corr()\ncorr.style.background_gradient(cmap='coolwarm')","8487a3a6":"# General info about the variable\nprint(df['Quantity'].describe())\nsns.distplot(df['Quantity'])\nplt.show()","dc4d89cc":"# Check if number of rows with 'C' in InvoiceNo is equal to the number of rows with negative values in Quantity\ncount_c = [1 for x in df['InvoiceNo'] if 'C' in x]\ncount_neg = [1 for x in df['Quantity'] if x < 0]\nprint('len(count_c) == len(count_neg) is {}'.format(len(count_c) == len(count_neg)))","a6dd4eb9":"df.loc[df.CustomerID == '17548']","bd5a74d7":"# Analysing outliers\ndef outlier_treatment(datacolumn):\n    # Returns lower and upper limit for outliers using IQR method\n    Q1, Q3 = np.percentile(datacolumn , [25,75])\n    IQR =  Q3 - Q1\n    lower_range = Q1 - (1.5 * IQR)\n    upper_range = Q3 + (1.5 * IQR)\n    return lower_range, upper_range\n\ndef outlier_analysis(datacolumn):\n    # Gives a summary of the outliers in the datacolumn\n    lowlim, highlim = outlier_treatment(datacolumn)\n    \n    # identify outliers\n    outliers = [x for x in datacolumn if x < lowlim or x > highlim]\n    print('Identified outliers: %d' % len(outliers))\n    print('Percentage of identified outliers: %f' % round((len(outliers)\/len(df)*100), 2) + '%')\n    # remove outliers\n    outliers_removed = [x for x in datacolumn if x >= lowlim and x <= highlim]\n    print('Non-outlier observations: %d' % len(outliers_removed))\n\noutlier_analysis(df['Quantity'])","9de0da8f":"# Analysis of Quantity column without outliers\n\n# Separate into sales and cancellations\nsales = df[df['Quantity'] > 0].reset_index()\ncancellations = df[df['Quantity'] < 0].reset_index()\ncancellations['Returns'] = cancellations['Quantity'].abs()\n\nquant_low, quant_high = outlier_treatment(df['Quantity'])\n\n# Sales and cancellations without the outliers\nsales_noout = sales[(sales['Quantity'] > quant_low) & (sales['Quantity'] < quant_high)] \ncanc_noout = cancellations[(cancellations['Quantity'] > quant_low) & (cancellations['Quantity'] < quant_high)] \n\n# Plot distributions of Quantity of sales and returns\nf,ax=plt.subplots(1,2,figsize=(15,5))\n\nsns.distplot(sales_noout['Quantity'], ax = ax[0])\nax[0].set_title('Sales Quantity Distribution', fontsize = 18)\nax[0].set_xlabel('Quantity', fontsize = 14)\nax[0].set_ylabel('Frequency', fontsize = 14)\n\nsns.distplot(canc_noout['Returns'], ax = ax[1])\nax[1].set_title('Returns Quantity Distribution', fontsize = 18)\nax[1].set_xlabel('Returns', fontsize = 14)\nax[1].set_ylabel('Frequency', fontsize = 14)\n\nplt.show()","b2f91aa0":"# General info about the variable\nprint(df['UnitPrice'].describe())\nsns.distplot(df['UnitPrice'])\nplt.show()","727fc93f":"# Outlier analysis\noutlier_analysis(df['UnitPrice'])","5571d8ef":"# Outlier boundaries for UnitPrice\nunit_low, unit_high = outlier_treatment(df['UnitPrice'])\n\n# Outlier free UnitPrice column\nunit_noout = df[(df['UnitPrice'] > unit_low) & (df['UnitPrice'] < unit_high)] \n\nprint(unit_noout['UnitPrice'].describe())\n\nsns.distplot(unit_noout['UnitPrice'])\nplt.title('UnitPrice distribution', fontsize=18)\nplt.xlabel('Cost', fontsize=14)\nplt.ylabel('Frequency', fontsize=14)\nplt.show()","0710f997":"def find_nas(df_in):\n    # Returns series with % of na for each column in df_in\n    cols = df_in.columns.values\n    na_per = []\n    for x in cols:\n        na_per.append((df_in[x].isna().sum()\/len(df_in.index))*100)\n\n    ser_out = pd.Series(na_per, index = cols)\n    ser_out = ser_out.sort_values(ascending = True)\n    return ser_out\n\ndef make_na_plot(na_ser):\n    # Creates a bar plot for the % of na values given from na_ser\n    blue = '#008fd5'\n    na_ser.plot(kind = 'barh', figsize = (10,5), color = blue)\n    \n    plt.title('% of na values', fontsize=24)\n    plt.xlabel('% na', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.ylabel('Columns', fontsize=18)\n    plt.yticks(fontsize=14)\n\n    plt.show()\n    \ndef drop_na(data):\n    data.dropna(inplace = True)\n    print('NAs droped. ', len(data.index), ' non-NA values left.\\n')\n    return data\n\ndef change_dt(data):\n    # Change data types to more appropriate ones\n    data['CustomerID'] = data['CustomerID'].astype(int).astype(str) # CustomerID is float, change to object\n    data['InvoiceDate'] =  pd.to_datetime(data['InvoiceDate']) # InvoiceDate should be datetime\n    print('Date types converted.\\n')\n    return data\n\ndef basic_clean(data):\n    # Drop Unnamed: 8 column, remove duplicate rows and find nas\n    \n    # Drop column since it has no info\n    data.drop(['Unnamed: 8'], axis = 1, inplace = True)\n    print('Unnamed: 8 column dropped.')\n    \n    # Number of duplicate rows\n    num_dup = len(data)-len(data.drop_duplicates())\n    print('Number of duplicate rows on the dataset = {}\/{} ({}%)'.format(num_dup, len(data), (num_dup\/len(data))*100))\n    \n    # Remove duplicate rows\n    data.drop_duplicates(inplace = True)\n    data.reset_index(inplace = True, drop = True)\n    print('Duplicates have been dropped. {} rows left.\\n'.format(len(data)))\n    \n    # Remove na values\n    data = drop_na(data)\n    \n    # Change data types for CustomerID and InvoiceDate\n    data = change_dt(data)\n    \n    return data\n\ndef repl_dup(df_in, col1, col2):\n    # Replace col2 values that correspond to the same col1 values by the col2 value that appears first for each col1 value\n    df_out = df_in.copy()\n    \n    # Replace all the Descriptions that correspond to duplicated StockCodes with None\n    df_out.loc[df_out.duplicated([col1]), col2] = None \n\n    # Fill in None Description values with their first appearance\n    df_out.loc[df_out[col2].isnull(), col2] = df_out.loc[df_out[col2].isnull(), col1].map(df_out.loc[df_out[col2].notnull()].set_index(col1)[col2])\n    return  df_out\n\ndef clean_descr(df_in):\n    # Removes ',', '.' and strips the Description values\n    df_in.Description = df_in.Description.str.replace(',', ' ')\n    df_in.Description = df_in.Description.str.replace('.', ' ')\n    df_in.Description = df_in.Description.str.strip()\n    print(\"Description column: Punctuation removed and values were stripped of spaces in the beginning and end.\")\n    return df_in\n\n\ndef clean_ecommerce(data):\n    '''\n    Performs the following cleaning tasks:\n    - Remove columns that contain no information.\n    - Remove duplicate rows.\n    - Find and handle NA values.\n    - Convert column values to the data types that are appropriate.\n    - Clean the Description column by correcting for misspelled or differently typed descriptions that correspond to the same StockCode.\n    - Clean the StockCode column by replacing different StockCodes for the same Description with the first one that appears in the data.\n    '''\n    # Remove Unnamed column, remove duplicate rows, remove nas, convert columns to appropriate datatypes\n    data = basic_clean(data)\n    \n    # Drop all rows with 'Unspecified' in Country\n    unspec_country_id = list(data[data['Country'] == 'Unspecified'].index)\n    data.drop(unspec_country_id, inplace = True)\n    data.reset_index(inplace = True, drop = True)\n    print('Deleted all rows that have \"Unspecified\" as their Country entry.\\n')\n    \n    # Replace duplicate Description values that correspond to the same StockCode\n    data = repl_dup(data, 'StockCode', 'Description')\n\n    # Replace duplicate StockCode values that correspond to the same Description\n    data = repl_dup(data, 'Description', 'StockCode')\n    print('StockCodes column has been cleaned.\\n')\n    \n    # Remove ',', '.' and strip the Description values\n    data = clean_descr(data)\n    print('Description column has been cleaned.\\n')\n    \n    return data\n","e3daaaef":"# Make sure that the function gives the same df as the one we work with in the notebook\ndf1 = pd.read_csv(file, encoding='ISO-8859-1')\ndf1 = clean_ecommerce(df1)\ndf1.equals(df)","eabf074b":"It seems like the series for the Description value counts and the StockCode value counts are not the same. Why could that be? Is that related to the fact that apparently some StockCodes are used for more than one product?\n\nTo explore that we take a look at the duplicated values of the StockCode and Description columns.","b91739c8":"# Analysis of Categorical Data\n\nCategorical columns: Country, CustomerID, StockCode, Description, InvoiceDate, InvoiceNo.\n\nThey can be further subcategorized as ordinal and nominal categorical variables.\n\n* Ordinal: InvoiceDate \n* Nominal: All the rest categorical variables.","ff395c53":"## Quantity\n\nThe quantities of each product (item) per transaction.","d035fb73":"So now we know what negative Quantity means, but what about the magnitude of that number? If it's an order that is cancelled is there a corresponding order where the customer ordered the product before cancelling it? We can check that by comparing the CustomerID and the StockCode between orders with oppposite Quantities. Let's try that with one customer.","861718f0":"In this case as well, the outliers are not something we want to dismiss since they just correspond to very expensive ordered or returned products. It will be usefull to see the distribution of the typical (outlier free) products that were ordered.","429927c1":"Here we note something interesting: the value counts for the Description column and the StockCode column are not the same. The reason this is interesting is because there is suppossed to be a 1-1 correspondence between StockCodes and Descriptions, but it seems like this is not really the case. Let's take a closer look with the first item in the value_counts series of the Description column.","a58339e4":"## CustomerID\n\nUnique ID assigned to each customer.","bb222d02":"## UnitPrice\n\nProduct price per unit.","957d314b":"We see that the lengths of the two value_count() series is the same which means that now there is a 1-1 corespondance between StockCodes values and Description values. We also see that the lengths of these series have changed and to be more accurate they have decreased. That's perfectly normal though: the extra values that both these series had were due to the misspellings of Descriptions or duplicate StockCodes used for the same Description value. These values have now been replaced so that there are no duplicate values of the StockCode column corresponding to the same Description column and vice versa.\n\nWe can now clean up the Description column a bit by removing punctuation and spaces from the beginning and end of the strings.","e1839375":"The rule of thumb is that an observation is an outlier if it lies beyond the upper or lower fence. These are defined as follows:\n\nUpper fence = Q3 + (1.5 IQR)\n\nLower fence = Q1 \u2013 (1.5 IQR)\n\nLet's take a look at the IQR analysis for this variable:","6f53827b":"## Country\n\nThe name of the country where each customer resides.","5c94d28a":"It seems that there's quite a lot going on here. We have some very large values for the maximum and minimum of this column. It also appears that the minimum value is negative. We also see that we have huge variability in our data which is probably due to the existence of these very large values (outliers).\n\nWe should try to understand what the negative values mean, and then perform an outlier analysis for this column to choose whether or not the outliers have any significance.\n\nBy inspecting the orders with negative quantity and their corresponding InvoiceNos we see that each one has a 'C' in front of it. A 'C' in the beginning of an InvoiceNo is common for an order that is cancelled. We can easily check that the number of orders with negative Quantity are the same as the number of orders with a 'C' in front of their InvoiceNo.","414739d5":"We see that our two numeric variables are not correalted. As a result they can be treated as independent for any potential models.","ecfcfc63":"**Dataset Description:**\nThis is a transnational dataset that contains all the transactions occurring between Nov-2016 to Dec-2017 for a UK-based online retail store.\n\nIn this notebook we clean the ecommerce dataset and provide options for further cleaning depending on how one would like to use this dataset. In more detail we do the following:\n\n* Remove columns that contain no information.\n* Remove duplicate rows.\n* Find and handle NA values.\n* Convert column values to the data types that are appropriate.\n\nFurthermore, we perform some more specialized cleaning of each column. In particular we:\n\n* Check the values of each column individually.\n* Clean the Description column by correcting for misspelled or differently typed descriptions that correspond to the same StockCode.\n* Clean the StockCode column by replacing different StockCodes for the same Description with the first one that appears in the data.\n* Provide an outlier analysis for the numeric columns (UnitPrice and Quantity) and look at their outlier free distributions.\n\n**Finally in the last cell we summarize all the cleaning that was done in one function so that the clean dataset can be used by anyone that wishes to use it.**\n\nWith the dataset clean we can then continue to perform some EDA and\/or segment the customers into different groups using a clustering methodology. This will be in separate notebooks that will be made public when ready.\n\nPlease let me know about any errors in my analysis and feel free to provide further recommendations for cleaning! Any kind of feedback is appreaciated!","e5d83f87":"# Initial Data Cleaning\n\nFor our initial data cleaning we are going to do the following:\n\n1. Get rid of columns that contain no information.\n2. Find na values.\n3. Handle na values.\n4. Get rid of duplicate rows.\n5. Change the data types of certain columns to make them more appropriate.","7576e5ee":"We see that the letter that those InvoiceNos contain is the letter 'C'. This could mean something about the nature of the invoice (it could mean that the invoice was cancelled) so we are not going to clean it.","3aa67209":"We see that there is no obvious pattern to the StockCode values, but generally they are stings that consist of letters and numbers of various lengths. We see that there are 12 entries that also have a \" \" character in them. Since this is not expected, we should take a look.","0f0bbf8b":"## InvoiceDate\n\nThe day when each transaction was generated.","b1e8e34d":"We see that the values that have a \" \" character in them correspond to a StockCode of Bank Charges. For now we have no reason to remove or alter them in any way.","26928230":"In our case the outliers are not something we should dismiss. They simply describe very large orders that were put. It should be interesting to check the distribution of orders and cancellations separetly and without outliers so we can see how the distributions of the more typical orders and cancellations look like.","d554e163":"We see that CustomerID values always consist of 5 digit numbers. It soes not seem like there is any cleaning needed for this column.","3a5bad41":"The interesting thing to note is that aparently the StockCode '85123A' is not unique to one product since as we see the 'White hanging heart T-light holder' has 2058 entries and not 2065 as we would expect from the analysis of the StockCode variable. Is this the case for other StockCodes too or is this just a mistake?\n\nTo answer that question we can compare the value counts for the two series we have: the one that contains the value counts for the StockCode column and the one that contains the value counts for the Description column.","830427bf":"### Outliers for Quantity column","b10d1cae":"We see that there are 241 entries that have \"Unspecified\" as country. Sice they are only a very small number, we could just dismiss them.","bcc74ec1":"The situation is similar to the Quantity column: very large maximum value and huge variability. The minimum seems to be 0 which would suggest that there are some items that are for free which also seems a bit odd. They could be part of special offers, or just items that the store gave for free to different customers (like manuals).\n\nLet's now run an outlier analysis.","fde2b40a":"# Summary","2bbcdc9b":"We see that the distribution for sales quantity is multi-modal with many different modes. The two most prominent ones are those with 2 and 12 items. It seems that these are the most typical quantities of products that the customers were ordering.\n\nThe distribution for the returns though is quite different. It's right skewed which suggests that usually a small number of products gets returned, with that number being 1 or 2.","7d830299":"We see that there are some InvoiceNo values that are of length 7 and that extra character is a letter. Let us take a closer look at those values.","c74831fc":"## StockCode\n\nProduct (item) code. It has an associated Description column that contains the name of the product.","5df2d523":"# All the cleaning in one function","bb8bdb3e":"We now can look at each column individually and see if there is any cleaning we can do.","ed4b13e2":"## InvoiceNo\n\nInvoice number (A 6-digit integral number uniquely assigned to each transaction).","4b8f87a0":"# Analysis of numeric variables\n\nThere are only two numeric variables in this dataset: Quantity and UnitPrices.","258103af":"# Loading the data","97f6c146":"We see that there distribution of the product costs is multimodal and relatively right skewed. This means that there are more cheaper products than expensive ones being bought. The multi-modality of the distribution could be indicative of many different \"price groups\" for the products that were ordered.","eaa3e73a":"We see that we have a large number of na values for the CustomerID column. From the data we have we cannot impute them in some way, so we will drop them. We also see that there is a small number of nas for the Description column. We could try to infer each item's Description value based on the StockCode it has, but unfortunately the rows that have na values for the Description column also have na values for the CustomerID as we can check with the code cell below. Therefore we drop them as well.\n\nWith the na values being dropped, we can also change the data types of certain columns to more appropriate ones. InvoiceDate should be a DateTime object and CustomerID should be a string.","5042e77c":"We see that the reason we don't have a '1-1' correspondance between the Description column and the StockCode column is because of the entries of the Description column. There are cases were the same product is written differently two or more times (eg instead of having \"+\" the second time we see a product, we have \"AND\", or the second time around we have commas in the description of a product that in another entry didn't and so forth). This doesn't necessarily mean that we have the same StockCode for two different products, but that the Description column is not very clean.\n\nTo clean the Description column up a bit we could replace all duplicate occurencies (for a given StockCode) with the first one so that at least the data is more uniform. By that I don't mean to replace the entire row, but only the Description value of a duplicate StockCode value.\n\nIn addition though, we see that there are also duplicate description values corresponding to different StockCode values (i.e the opposite). By a quick look at the data it doesn't seem like the products with two different StockCodes for the same Description changed StockCodes after some specific time, so we can assume that these entries are just an error. Since it doesn't make sense to not have a 1-1 correspondance between StockCodes and Descriptions we will apply the same method to the duplicate StockCodes for the same Description as we will do for the duplicate Descriptions for the same StockCode.","6dc86e60":"This column was successfully converted into DateTime in the 'Initial Data Cleaning' section with no issues. Some days may be missing but these are probably just because we removed more than 25% of our initial rows in the begging because they had no CustomerID.","728b6196":"We see that the last 10 rows correspond to an order that is placed on the 18th of April and then cancelled on the 4th of May. We don't have the order that was placed on the 29th of Novermber for this customer, but this order could be included in the rows we dismissed because they had missing CustomerIDs (remember, they were a lot of them, 25% of our initial data). It could also be that a cancellation for an order came right after the beggining of the data gathering so we have no history of a certain transaction.\n\nTherefore a negative Quantity is the number of products that were returned. We now continue with an outlier analysis.","15380149":"## Description\n\nProduct (item) name."}}