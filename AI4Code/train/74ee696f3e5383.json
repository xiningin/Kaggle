{"cell_type":{"9f535671":"code","7f9018e9":"code","d34be712":"code","df6fd3f9":"code","e19c7580":"code","abb567a2":"code","3770e91d":"code","70948224":"code","63bd723f":"code","0e1754cc":"code","af258075":"code","35525789":"code","3bf7494f":"code","ad2bcfb7":"code","89f4f140":"code","1d9a5dc7":"code","7f57cacd":"code","11180134":"code","a230b41f":"code","76e1f876":"code","0ab5b903":"markdown","52a328c9":"markdown","ff133bb8":"markdown","0d7fe95a":"markdown","8c913c71":"markdown","660d3432":"markdown","b738e789":"markdown","2d162a39":"markdown","0203b0e4":"markdown","e8175318":"markdown","9a613c4f":"markdown","67ead9fa":"markdown","35e5dbe4":"markdown","4048ded2":"markdown","8de7956b":"markdown","793c7b2b":"markdown","59b513c3":"markdown","c0b1c7b2":"markdown","20e2862e":"markdown","37fe1de1":"markdown","9cf18783":"markdown","d5875b5c":"markdown","44014a31":"markdown","c5580bd9":"markdown","f926d41f":"markdown"},"source":{"9f535671":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 14})\n\nimport re\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndistricts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")\nproducts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\")","7f9018e9":"districts_info = districts_info[districts_info.state.notna()].reset_index(drop=True)","d34be712":"temp_sectors = products_info['Sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_info = products_info.join(temp_sectors)\nproducts_info.drop(\"Sector(s)\", axis=1, inplace=True)\n\ndel temp_sectors","df6fd3f9":"products_info['primary_function_main'] = products_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_info['primary_function_sub'] = products_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_info['primary_function_sub'] = products_info['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\nproducts_info.drop(\"Primary Essential Function\", axis=1, inplace=True)","e19c7580":"PATH = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' \n\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    temp.append(df)\n    \n    \nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)","abb567a2":"fig, ax = plt.subplots(1, 1, figsize=(8,4))\n\nsns.histplot(engagement.groupby('district_id').time.nunique(), bins=30)\nax.set_title('Unique Days of Engagement Data per District')\nplt.show()","3770e91d":"# Delete previously created engagement dataframe and create a new one\ndel engagement\n\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    if df.time.nunique() == 366:\n        temp.append(df)\n\nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)\n\n# Only consider districts with full 2020 engagement data\ndistricts_info = districts_info[districts_info.district_id.isin(engagement.district_id.unique())].reset_index(drop=True)\nproducts_info = products_info[products_info['LP ID'].isin(engagement.lp_id.unique())].reset_index(drop=True)","70948224":"for district in districts_info.district_id.unique()[:10]:\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    print(f'District {district} uses {df.lp_id.nunique()} unique products.')\n    \nprint(f'\\nConcatenated engagement data contains {engagement.lp_id.nunique()} unique products.')\n","63bd723f":"print(len(engagement))\nengagement = engagement[engagement.lp_id.isin(products_info['LP ID'].unique())]\nprint(len(engagement))","0e1754cc":"engagement.time = engagement.time.astype('datetime64[ns]')","af258075":"us_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_info['state_abbrev'] = districts_info['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_info['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    geo_scope='usa',\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations=districts_info_by_state.state_abbrev,\n        zmax=1,\n        z = districts_info_by_state.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='white',\n        geo='geo',\n        colorscale=px.colors.sequential.Teal, \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()","35525789":"districts_info.pp_total_raw.unique()\ntemp = districts_info.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\nfig, ax = plt.subplots(1, 2, figsize=(24,4))\n\nsns.countplot(data=districts_info, x='locale', ax=ax[0], palette='GnBu')\n\nsns.heatmap(temp, annot=True,  cmap='GnBu', ax=ax[1])\nax[1].set_title('Heatmap of Districts According To locale and pp_total_raw')\nplt.show()","3bf7494f":"fig, ax = plt.subplots(2, 2, figsize=(16,8))\n\nsns.countplot(data=districts_info, x='pct_black\/hispanic', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,0])\nax[0,0].set_ylim([0,135])\nsns.countplot(data=districts_info, x='pct_free\/reduced', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,1])\nax[0,1].set_ylim([0,135])\n\nsns.countplot(data=districts_info, x='county_connections_ratio', palette='GnBu', ax=ax[1,0])\nax[1,0].set_ylim([0,135])\nsns.countplot(data=districts_info, x='pp_total_raw', order=['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ], palette='GnBu', ax=ax[1,1])\nax[1,1].set_ylim([0,135])\nax[1,1].set_xticklabels(ax[1,1].get_xticklabels(), rotation=90)\n\nplt.tight_layout()\nplt.show()","ad2bcfb7":"def replace_ranges_pct(range_str):\n    if range_str == '[0, 0.2[':\n        return 0.1\n    elif range_str == '[0.2, 0.4[':\n        return 0.3\n    elif range_str == '[0.4, 0.6[':\n        return 0.5\n    elif range_str == '[0.6, 0.8[':\n        return 0.7\n    elif range_str == '[0.8, 1[':\n        return 0.9\n    else:\n        return np.nan\n    \ndef replace_ranges_raw(range_str):\n    if range_str == '[4000, 6000[':\n        return 5000\n    elif range_str == '[6000, 8000[':\n        return 7000\n    elif range_str == '[8000, 10000[':\n        return 9000\n    elif range_str == '[10000, 12000[':\n        return 11000\n    elif range_str ==  '[12000, 14000[':\n        return 13000\n    elif range_str ==  '[14000, 16000[':\n        return 15000\n    elif range_str == '[16000, 18000[':\n        return 17000\n    elif range_str ==  '[18000, 20000[':\n        return 19000\n    elif range_str ==  '[20000, 22000[':\n        return 21000\n    elif range_str ==  '[22000, 24000[':\n        return 21000\n    else: \n        return np.nan\n    \ndistricts_info['pct_black_hispanic_num'] = districts_info['pct_black\/hispanic'].apply(lambda x: replace_ranges_pct(x))\ndistricts_info['pct_free_reduced_num'] = districts_info['pct_free\/reduced'].apply(lambda x: replace_ranges_pct(x))\ndistricts_info['pp_total_raw_num'] = districts_info['pp_total_raw'].apply(lambda x: replace_ranges_raw(x))\n\ndef plot_state_mean_for_var(col):\n    temp = districts_info.groupby('state_abbrev')[col].mean().to_frame().reset_index(drop=False)\n\n    fig = go.Figure()\n    layout = dict(\n        title_text = f\"Mean {col} per State\",\n        geo_scope='usa',\n    )\n\n    fig.add_trace(\n        go.Choropleth(\n            locations=temp.state_abbrev,\n            zmax=1,\n            z = temp[col],\n            locationmode = 'USA-states', # set of locations match entries in `locations`\n            marker_line_color='white',\n            geo='geo',\n            colorscale=px.colors.sequential.Teal, \n        )\n    )\n\n    fig.update_layout(layout)   \n    fig.show()\n\nplot_state_mean_for_var('pct_black_hispanic_num')\nplot_state_mean_for_var('pct_free_reduced_num')\nplot_state_mean_for_var('pp_total_raw_num')","89f4f140":"fig, ax = plt.subplots(1, 2, figsize=(16,4))\nsns.countplot(data=products_info, x='primary_function_main', palette ='GnBu', ax=ax[0])\nax[0].set_title('Main Categories in Primary Functions')\n\nsns.countplot(data=products_info[products_info.primary_function_main == 'LC'], x='primary_function_sub', palette ='GnBu', ax=ax[1])\nax[1].set_title('Sub-Categories in Primary Function LC')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nplt.show()","1d9a5dc7":"virtual_classroom_lp_id = products_info[products_info.primary_function_sub == 'Virtual Classroom']['LP ID'].unique()\n\n# Remove weekends from the dataframe\nengagement['weekday'] = pd.DatetimeIndex(engagement['time']).weekday\nengagement_without_weekends = engagement[engagement.weekday < 5]\n\n# Figure 1\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').pct_access.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.pct_access, label=products_info[products_info['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.legend()\nplt.show()\n\n# Figure 2\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').engagement_index.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.engagement_index, label=products_info[products_info['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.legend()\nplt.show()","7f57cacd":"products_info['lp_id'] = products_info['LP ID'].copy()\n\nf, ax = plt.subplots(nrows=3, ncols=3, figsize=(18, 8))\n\ni = 0\nj = 0\nfor subfunction in products_info[products_info.primary_function_main == 'LC'].primary_function_sub.unique():\n    lp_ids = products_info[products_info.primary_function_sub == subfunction]['LP ID'].unique()\n\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id.isin(lp_ids)]\n    temp = temp.groupby('lp_id').pct_access.mean().sort_values(ascending=False).to_frame().reset_index(drop=False)\n    temp = temp.merge(products_info[['lp_id', 'Product Name']], on='lp_id').head()\n    \n    sns.barplot(data=temp, x='pct_access', y='Product Name', palette='GnBu', ax=ax[i, j])\n    \n    ax[i, j].set_title(f'Top 5 in \\n{subfunction}', fontsize=12)\n    ax[i, j].set_xlim([0, 20])\n    j = j + 1\n    if j == 3:\n        i = i + 1\n        j = 0\n        \nf.delaxes(ax[2, 1])\nf.delaxes(ax[2, 2])\n\nplt.tight_layout()\nplt.show()\n","11180134":"products_info[products_info['Product Name'] == 'Among Us']","a230b41f":"engagement['quarter'] = pd.DatetimeIndex(engagement['time']).quarter.astype(str)\n\ntemp = engagement.merge(products_info[['lp_id', 'Product Name', 'primary_function_main', 'primary_function_sub']], on='lp_id')\ntemp = temp[temp.primary_function_sub.notna()]\ntemp = temp.groupby(['quarter', 'primary_function_sub'])['pct_access', 'engagement_index'].mean().reset_index(drop=False)\n\ntemp = temp.pivot(index='primary_function_sub', columns='quarter')[['pct_access', 'engagement_index']].fillna(0)\n\ntemp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n\ntemp['pct_access_delta'] = temp['pct_access_4'] - temp['pct_access_1']\ntemp['engagement_index_delta'] = temp['engagement_index_4'] - temp['engagement_index_1']\ntemp=temp.reset_index(drop=False)\n#temp = temp.merge(products_info[['lp_id', 'Product Name', 'primary_function_sub']], on='lp_id')\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\ndf = temp.sort_values(by='pct_access_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='pct_access_delta', y='primary_function_sub', palette='GnBu', ax=ax[0])\n\ndf = temp.sort_values(by='engagement_index_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='engagement_index_delta', y='primary_function_sub', palette='GnBu', ax=ax[1])\nplt.tight_layout()\nplt.show()","76e1f876":"\ntemp = engagement.fillna(0).groupby(['quarter', 'lp_id'])['pct_access', 'engagement_index'].mean().reset_index(drop=False)\n\ntemp = temp.pivot(index='lp_id', columns='quarter')[['pct_access', 'engagement_index']].fillna(0)\n\ntemp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n\ntemp['pct_access_delta'] = temp['pct_access_4'] - temp['pct_access_1']\ntemp['engagement_index_delta'] = temp['engagement_index_4'] - temp['engagement_index_1']\ntemp = temp.merge(products_info[['lp_id', 'Product Name', 'primary_function_sub']], on='lp_id')\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n\ndf = temp.sort_values(by='pct_access_delta', ascending=False).head(10)\n\nsns.barplot(data=df, x='pct_access_delta', y='Product Name', palette='GnBu', ax=ax[0])\n\ndf = temp.sort_values(by='engagement_index_delta', ascending=False).head(10)\n\nsns.barplot(data=df, x='engagement_index_delta', y='Product Name', palette='GnBu', ax=ax[1])\nplt.tight_layout()\nplt.show()","0ab5b903":"Note that at this stage, we have **removed quite a bit of data**. This obviously can lead to loss of information. However, on the other hand this makes the data easier to compare and therefore, it can help us in finding insights more quickly. To summarize, we have removed districts without any information on the location and we have removed districts with incomplete data in 2020.","52a328c9":"# Utilizing External Data\n\nYour entry will also be judged on whether you utilized external data:\n> Did the author utilize additional public data sources in their analysis?\n\nThankfully, your fellow Kagglers have got your covered here as well! So, again: Don't forget to show your appreciation of the dataset if you find it helpful :)\n\n* [Oxford COVID-19 Government Response Tracker](https:\/\/www.kaggle.com\/ruchi798\/oxford-covid19-government-response-tracker) by [Ruchi Bhatia](https:\/\/www.kaggle.com\/ruchi798)\n* [State Trend in Child Well Being - Data Book](https:\/\/www.kaggle.com\/surajitcba2021\/state-trend-in-child-well-being-data-book) by [Joy](https:\/\/www.kaggle.com\/surajitcba2021)\n* [COVID-19 Education Data](https:\/\/www.kaggle.com\/andradaolteanu\/covid19-education-data) by [Andrada Olteanu](https:\/\/www.kaggle.com\/andradaolteanu)\n* [COVID-19 US State Policy Database](https:\/\/www.kaggle.com\/cavfiumella\/covid19-us-state-policy-database) by [Andrea Serpolla](https:\/\/www.kaggle.com\/cavfiumella)\n\n\n\nIn our initial data analysis, we found that the column `county_connections_ratio` does not contain any valuable information. Maybe we can find an external dataset with information about broadband access that we can use.\n","ff133bb8":"**Summary**\n* Depending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.\n* When looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning","0d7fe95a":"<font color='turquoise'>One-Hot Encoding the Product Sectors<\/font>","8c913c71":"For the majority of school districts (133) there are 366 unique days available. However, for 43 school districts there are less than 366 unique days of data available. For example for district 3670 there is only data available from 2020-02-15 to 2020-03-02 or for district 2872 there is only data available for January 2020 and then two more single days in Feburary and March.","660d3432":"Finally, we will convert the `time` column to the type `datetime64[ns]` for easier handling.","b738e789":"# Preprocessing\nPreprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that **the way I preprocess the data may not be suited for your analysis purposes**. Therefore, before you begin preprocessing your data, think about which data you would like to keep and\/or modify and which data is not relevant for your analysis.\n\nIn the following subsections, I will proceed with the following basic preprocessing steps:\n- <font color='salmon'>dropping 57 school districts with NaN states (57\/233 ~ 25%)<\/font>\n- <font color='turquoise'>one-hot encoding the product sectors<\/font>\n- <font color='orange'>splitting up the primary essential function into main and sub category<\/font>\n![learnplatform.002.jpeg](attachment:f85cb16a-6095-4175-8f94-4b1c4f390289.jpeg)\n\n<font color='salmon'>Dropping Districts with NaN States<\/font>","2d162a39":"# Review of the Problem Statement\n\nNext, review the problem statement and brainstorm some ideas for questions you would like to answer with your analysis. For this, you could try something like a mindmap as follows.\n\n>**Problem Statement**\n> \n>The COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow.\nChallenge\n>\n>We challenge the Kaggle community to explore (1) the state of digital learning in 2020 and (2) how the engagement of digital learning relates to factors such as district demographics, broadband access, and state\/national level policies and events.\n>\n>We encourage you to guide the analysis with questions that are related to the themes that are described above (in bold font). Below are some examples of questions that relate to our problem statement:\n>\n> * What is the picture of digital connectivity and engagement in 2020?\n> * What is the effect of the COVID-19 pandemic on online and distance learning, and how might this also evolve in the future?\n> * How does student engagement with different types of education technology change over the course of the pandemic?\n> * How does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race\/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?\n> * Do certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?\n\nBased on the above problem statement from the challenge's description, you could start with a mindmap as shown below.\n![learnplatform.005.jpeg](attachment:26d90a09-778a-45b6-a48d-23dfbe405ec1.jpeg)\n\nFrom here you should continue your brainstorming and extend the above mindmap with further items.","0203b0e4":"Furthermore, if we look at a few sample districts, we can quickly see that most districts use more than the 369 unique products from `products_info`. In fact, the concatenated engagement data contains more than 8000 unique products. Since we don't have any additional information the majority of these products, we will remove engagement data for unknown products. This reduces the engagement data roughly by half.","e8175318":"Now we will try to understand that meaning behind `pct_access` and `engagement_index`.\n\n* `pct_access`: **% of students in the district have at least one page-load event** of a given product and on a given day\n* `engagement_index`: **Total page-load events per 1000 students** of a given product and on a given day\n\nIn the first figure below, you can see the overall mean `pct_access` of all products of the category 'Virtual Classroom'. For better understanding, weekends are removed from this visualization as students will not attend classes on weekends and this would add a disturbing visual effect and make the plot more difficult to understand. Let's summarize what we see:\n* the home schooling phase starts at the beginning of March\n* there is a bell-shape between March and July\n* during July and August there are summer holidays and therefore no classes to attend\n* after the summer holidays the `pct_access` increases to a higher level as observed at the beginning of the pandemic and it stays somewhat constant\n* there are a few drops in `pct_access` visible throughout the year - these might be national holidays or other holidays\n* Zoom and Meet seem to be the two most popular products for virtual classrooms\n\nOver the last quarter of 2020, we can see a `pct_access` of roughly 15. What does this mean? 15 % of students in the district have at least one page-load event of Zoom or Meet. To be honest, that seems a little bit low from the home schooling point of view given that every student needs to attend classes on a school day. However, this seems to hint at the fact that not all students had to attend classes virtually but were able to physically go to school. Judging from [this State-by-State Map of Where School Buildings Are Opened or Closed](https:\/\/www.edweek.org\/leadership\/map-where-are-schools-closed\/2020\/07) it seems like a lot of schools offered in-person education in 2020. **That means, when looking at digital learning, we should probably focus our analysis on districts where digital learning was actually applied to get some key insights**.\n\nWhile for `pct_access` Zoom and Meet seem to have roughly similiar values, we can see in the lower graph that Meet has more than 4 times the value of Zoom for `engagement_index` in the last quarter of 2020. What does this mean? If we have 1000 page-load events per 1000 students for Zoom on a given day that means that one student uses Zoom once a day. In contrast, Meet is used 4 or 5 times daily on average per student.\n\nTo make things a little bit clearer: If every students has two applications on their phone, the `pct_access` indicates **how many of the students** access this app on a daily basis but the `engagement_index` tells you **how much the students engage** with that application on a daily basis.","9a613c4f":"Below we can see the top 5 most accessed products for each LC sub-category sorted by the mean `pct_access` for 2020 over all districts. We can see that most of the products are on average accessed by less than 5 % of students on a daily basis. Exceptions are YouTube, Google Docs, and Canvas. YouTube in this case is a difficult one to evaluate since it can be used for leisure in addition to education, so we need to be careful here. Google Docs seems to make a lot of sense since students can use Google Docs. According to [Canvas](https:\/\/community.canvaslms.com\/t5\/Canvas-Basics-Guide\/What-is-Canvas\/ta-p\/45) it is \n> Canvas is a web-based learning management system, or LMS. It is used by learning institutions, educators, and students to access and manage online course learning materials and communicate about skill development and learning achievement.\n\nso it also makes sense that this one is quite often accessed.\n\nRegarding the sub-category 'Career Planning & Job Search' the average `pct_access` is very low. This is probably due top the fact that career planning is only relevant to older students. Therefore, we can probably exclude this subcategory when inspecting the digital learning aspect.","67ead9fa":"The most common category in the column Primary Essential Function is Learning & Curriculum (LC) as shown on the below left figure. For the categories Classroom Management (CM) and School & District Operations (SDO) there are far fewer tool options available. ","35e5dbe4":"Furthermore, we will concatenate the engagement data from all remaining districts in one dataframe by adding the key column `district_id` to each engagement file as shown below.\n![learnplatform4.001.jpeg](attachment:830ae306-55d8-45e8-817d-20d8bb758cda.jpeg)","4048ded2":"After preprocessing, we are left with a reduced `districts_info` dataframe with 176 districts and the `product_info` dataframe looks are follows:\n\n![learnplatform.003.jpeg](attachment:2cd2325d-8bc7-4a87-8a62-75ba9fd94069.jpeg)","8de7956b":"<font color='orange'>Splitting up the Primary Essential Function<\/font>","793c7b2b":"# Read, Read, Read!\n\nNow that you have brainstormed some ideas, it's time to do some reading. To be honest, whether you brainstorm ideas first or start doing your reading does not matter because this is an iterative process. Reading an article can give you a new idea in which direction, you want to further explore the dataset. \n\nAs a starting point, I would recommend you check out the challenge's discussion section. Often times one or more Kagglers will have kindly shared some resources to begin your reading. In this challenge, [Charlie Craine](https:\/\/www.kaggle.com\/crained) was kind enough to gather some interesting resources and share the collection with us [in the discussion \"Papers on Digital Learning\"](https:\/\/www.kaggle.com\/c\/learnplatform-covid19-impact-on-digital-learning\/discussion\/256883). PS: Don't forget to show your appreciation of the discussion thread if you find it helpful :)","59b513c3":"To make the data easier to compare, we will only consider distrcits with engagement data for everyday in 2020.","c0b1c7b2":"The same approach as above is used below for the product level instead of the subfunction level. ","20e2862e":"Just on a side note because I saw this on accident: **'Among Us'**, which is a multiplayer game that was quite popular during 2020, **is also listed as a product under the main category LC**. So, we need to take the list of products with a grain of salt when we are evaluating them as 'digital learning products'. ","37fe1de1":"<div class=\"alert alert-block alert-success\">You can find my entry for this challenge here: <a href=\"https:\/\/www.kaggle.com\/iamleonie\/maslow-before-bloom\">Maslow before Bloom<\/a><\/div>\n\n# How To Approach Analytics Challenges\n\n*Note: This notebook was called \"Gentle Introduction to The Dataset\" until version 8. In version 9, I decided to make this notebook into a mini tutorial on how I approach analytics challenges.*\n\nAnalytics challenges are my favorite kind of challenge here on Kaggle. Firstly, I think this is a great way for beginners to get comfortable with Python and exploring large datasets. Secondly, you don't need any GPU computing power as for example you might need for a coding competition with image data. And lastly, you can learn something new about a topic and be creative with writing and visualizing.\n\n\nFor further resources, you can check out my (unfinished) tutorial series on the [Data Journalism Workflow](https:\/\/www.kaggle.com\/iamleonie\/data-journalism-workflow).","9cf18783":"# Putting Everything Together\n\nAt the end your entry for this challenge will be evaluated based on the following criteria as mentioned in the [evaluation section](https:\/\/www.kaggle.com\/c\/learnplatform-covid19-impact-on-digital-learning\/overview\/evaluation).\n> Clarity (5 pts)\n>    * Did the author present a clear thread of questions or themes motivating their analysis?\n>    * Did the author document why\/what\/how a set of methods was chosen and used for their analysis?\n>    * Is the notebook documented in a way that is easily reproducible (e.g., code, additional data sources, citations)?\n>    * Does the notebook contain clear data visualizations that help effectively communicate the author\u2019s findings to both experts and non-experts?\n> \n> Accuracy (5 pts)\n>    * Did the author process the data (e.g., merging) and\/or additional data sources accurately?\n>    * Is the methodology used in the analysis appropriate and reasonable?\n>    * Are the interpretations based on the analysis and visualization reasonable and convincing?\n>\n> Creativity (5 pts)\n>    * Does the notebook help the reader learn something new or challenge the reader to think in a new way?\n>    * Does the notebook leverage novel methods and\/or visualizations that help reveal insights from data and\/or communicate findings?\n>    * Did the author utilize additional public data sources in their analysis?\n\nWhen creating an analytics report, make sure you spend a little bit of time on the formatting. Often I see entries for these challenges that are purely a collection of plots. For you these plots might make sense since you have put a lot of effort into creating them. However, for someone else your plot might only look pretty but not convey the information you want it to. So, I recommend to spend a few minutes to **add some explanation text to your visualization:**\n* What are we looking at? Based on which data was this plot created? What data was excluded (any filters)? What are the values (mean, sum, median, difference, etc)?\n* What can we see (increase, decrease, static, distribution)?\n* What can we learn from this plot? What are the main take aways?\n\nAlso, you might want to strategically hide or show code. **If you want to explicitly explain how you did something in the code, show us the code. But then also explain it to us with some text**. If your code is only used to create a plot then maybe think about hiding the code. If someone is interested in how you did something, they can still unhide your code and have a look. But for everyone else unneccessary code only disturbs the reading flow.\n![learnplatform6.001.jpeg](attachment:f1af97a6-df24-4ce2-860c-dde974ee8c7f.jpeg)\n\n# Good Luck\nHopefully, this notebook has given you a rough starting point for this challenge. I am looking forward to your entries. Good luck!","d5875b5c":"In above example to showcase the difference between `pct_access` and `engagement_index`, we intuitively chose products from the subfunction 'Virtual Classroom' based on our \"domain knowledge\" that in-person education does not need video conferencing tools but during the pandemic these tools gained a lot of popularity. Let's do a quick analysis and see whether the avaiable data reflects the assumption we had made before.\n\nFor this purpose, we will **average the `pct_access` and `engagement_index` data over each quarter in 2020 and then take the difference between the last quarter of 2020 and the first quarter of 2020.** Based on this approach, we should be able to see which subfunction category that gained the most engagement over the course of the pandemic in 2020.\n\nAs you can see in the below left figure, the data reflects our assumption that products of the subfunction 'Virtual Classroom' gained the most change in `pct_access`. At second place, we have Learning Management Systems (LMS). These top two categories are also the top two categories from the `engagement_index` point of view.","44014a31":"# Initial Exploratory Data Analysis (EDA)\n\nLet's begin with a simple EDA. First of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. (19\/50). The states with the most available school districts are CT (29) and UT (24) while there are also states with only one school district (FL, TN, NY, AZ).","c5580bd9":"If we look at the distributions of the remaining columns in `districts_info`, you can quickly see that `county_connections_ratio` only has one unique value which is `[0.18, 1[`. To be fair, before dropping all rows without any state information, this column had another value of `[1, 2[` but only for one data point. So, this column does not really contain any valuable information. Maybe we can get this sort of information from an external dataset.","f926d41f":"# Getting An Overview of the Dataset\n\nIn this analytics challenge, we are given multiple .csv files as shown in below figure. The main information contained in these files is **which tools are used with what engagement in which school district** in the United States of America in 2020.\n\nThe districts_info.csv file contains information about each school district and the products_info.csv file contains information about the top 370 tools used for digital learning. For each school district, there is an additional file that contains the engagement for each tool for everyday in 2020. The files can be joined by the key columns <font color='lightblue'>district_id<\/font> and <font color='orange'>lp_id<\/font>.\n\n![learnplatform.001.jpeg](attachment:10d597ce-2a66-4e0f-a16f-ea0a95f2cd18.jpeg)"}}