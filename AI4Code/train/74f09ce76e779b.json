{"cell_type":{"a66d7307":"code","3369d4a9":"code","c87cda13":"code","7bd28e6a":"code","57ef1ca3":"code","cf933056":"code","cd227090":"code","96a28b86":"code","2ac40f41":"code","7f8aad67":"code","88b89e58":"code","eece17d7":"code","02a7ce42":"code","8146eb64":"code","50fcc7ba":"code","243158ee":"code","5fd4feab":"code","bbb5d3cc":"code","72f8ab61":"code","98e27823":"code","43de9ee7":"code","953358b2":"code","84030c17":"code","49ae054e":"code","d1377c7c":"code","73569f9c":"code","62fccaac":"code","4631f5bf":"code","d36077ce":"code","ea2d07b9":"code","ffb2b39c":"code","85ac40f2":"code","f66dd856":"code","bbcae2b2":"code","ae42a3a5":"code","962c9973":"code","5f76b4bf":"code","cceb86ac":"code","594caebf":"code","89898384":"markdown","dd712232":"markdown","2b855d78":"markdown","1c0addc3":"markdown","9061b7ad":"markdown","10ce0c3e":"markdown","368898fc":"markdown","345e3225":"markdown","218ffc0f":"markdown","ec17d0f6":"markdown","4daa4693":"markdown","47b6a1f5":"markdown","99c197f1":"markdown","b3e6007d":"markdown","2d90c707":"markdown","f21a8798":"markdown"},"source":{"a66d7307":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n%matplotlib inline","3369d4a9":"col_names = ['Pregnancies', 'Glucose', 'BloodPressure','SkinThickness','Insulin','BMI', 'DiabetesPedigreeFunction','Age', 'Outcome']\ndata = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head()","c87cda13":"feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age']\nX = data[feature_cols]\ny = data['Outcome']","7bd28e6a":"#splitting data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0)","57ef1ca3":"#using logestic regression\nfrom sklearn.linear_model import LogisticRegression\nlogReg = LogisticRegression()\nlogReg.fit(X_train, y_train)","cf933056":"y_pred_class  = logReg.predict(X_test)\ny_pred_class","cd227090":"#calculate accuracy\nfrom sklearn import metrics\nprint(metrics.accuracy_score(y_test, y_pred_class))","96a28b86":"y_test.value_counts()","2ac40f41":"#calculate the % of ones (using mean coz there is only 0 and 1 present)\ny_test.mean()","7f8aad67":"#calculate the % of 0\n1 -  y_test.mean()","88b89e58":"#calculate null accuracy (for binary classification problem coded as 1 and 0)\nmax(y_test.mean(), 1 -  y_test.mean() )","eece17d7":"#calculate null accuracy (for multi class classification)\ny_test.value_counts().head(1)\/ len(y_test)","02a7ce42":"print( metrics.confusion_matrix(y_test, y_pred_class))","8146eb64":"confusion = metrics.confusion_matrix(y_test, y_pred_class)\nTP = confusion[1,1]\nTN = confusion[0,0]\nFP = confusion[0,1]\nFN = confusion[1,0]\n","50fcc7ba":"#both are equal\nprint( (TP + TN) \/ float(TP + TN + FP + FN))\nprint(metrics.accuracy_score(y_test, y_pred_class))","243158ee":"#Both are equal \nprint(( FP+ FN)\/ float(TP + TN + FP + FN))\nprint(1 - metrics.accuracy_score(y_test, y_pred_class))","5fd4feab":"#Both are same\nprint(TP\/ float( TP +FN))\nprint( metrics.recall_score(y_test, y_pred_class))","bbb5d3cc":"print(TN\/ float(TN+FP))","72f8ab61":"print(FP \/ float(TN+FP))","98e27823":"print( TP \/ float(TP+FP))\nprint(metrics.precision_score(y_test, y_pred_class))","43de9ee7":"# print the first 10 predicted responses\nlogReg.predict(X_test)[0:10]","953358b2":"# print the first 10 predicted probabilities of class membership\nlogReg.predict_proba(X_test)[0:10, :]","84030c17":"# print the first 10 predicted probabilities for class 1\nlogReg.predict_proba(X_test)[0:10, 1]","49ae054e":"# store the predicted probabilities for class 1\ny_pred_prob = logReg.predict_proba(X_test)[:, 1]","d1377c7c":"# allow plots to appear in the notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt","73569f9c":"# histogram of predicted probabilities\nplt.hist(y_pred_prob, bins=8)\nplt.xlim(0, 1)\nplt.title('Histogram of predicted probabilities')\nplt.xlabel('Predicted probability of diabetes')\nplt.ylabel('Frequency')","62fccaac":"# predict diabetes if the predicted probability is greater than 0.3\nfrom sklearn.preprocessing import binarize\ny_pred_class = binarize([y_pred_prob], 0.3)[0]","4631f5bf":"# print the first 10 predicted probabilities\ny_pred_prob[0:10]","d36077ce":"# print the first 10 predicted classes with the lower threshold\ny_pred_class[0:10]","ea2d07b9":"# previous confusion matrix (default threshold of 0.5)\nprint(confusion)","ffb2b39c":"# new confusion matrix (threshold of 0.3)\nprint(metrics.confusion_matrix(y_test, y_pred_class))","85ac40f2":"# sensitivity has increased (used to be 0.24)\nprint(46 \/ float(46 + 16))","f66dd856":"# specificity has decreased (used to be 0.91)\nprint(80 \/ float(80 + 50))","bbcae2b2":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","ae42a3a5":"# define a function that accepts a threshold and prints sensitivity and specificity\ndef evaluate_threshold(threshold):\n    print('Sensitivity:', tpr[thresholds > threshold][-1])\n    print('Specificity:', 1 - fpr[thresholds > threshold][-1])","962c9973":"evaluate_threshold(0.5)","5f76b4bf":"evaluate_threshold(0.3)","cceb86ac":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\nprint(metrics.roc_auc_score(y_test, y_pred_prob))","594caebf":"# calculate cross-validated AUC\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(logReg, X, y, cv=10, scoring='roc_auc').mean()","89898384":"### Specificity : when the actual value is Negative, how often is the prediction correct?","dd712232":"## ROC Curves and Area Under the Curve (AUC)\n\n**Question:** Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?\n\n**Answer:** Plot the ROC curve!","2b855d78":"### sentivity : when the actual value is positive, how often is the prediction correct?","1c0addc3":"# Null Accuracy: accuracy that could be achieved by always predicting the most frequent class","9061b7ad":"### classification accuracy: overall how often is the classifier correct?","10ce0c3e":"### False positive rate: when the actual value is Negative, how often is the prediction INcorrect?","368898fc":"**Conclusion:**\n\n- **Threshold of 0.5** is used by default (for binary problems) to convert predicted probabilities into class predictions\n- Threshold can be **adjusted** to increase sensitivity or specificity\n- Sensitivity and specificity have an **inverse relationship**","345e3225":"- ROC curve can help you to **choose a threshold** that balances sensitivity and specificity in a way that makes sense for your particular context\n- You can't actually **see the thresholds** used to generate the curve on the ROC curve itself","218ffc0f":"## Adjusting the classification threshold\n ","ec17d0f6":"- AUC is useful as a **single number summary** of classifier performance.\n- If you randomly chose one positive and one negative observation, AUC represents the likelihood that your classifier will assign a **higher predicted probability** to the positive observation.\n- AUC is useful even when there is **high class imbalance** (unlike classification accuracy).","4daa4693":"# Metrics computed from confusion matrix","47b6a1f5":"# CONFUSION MATRIX\nloosly defined as the table that descricibes the performance of a classification model","99c197f1":"### Precision: when the positive value is predicted, how often is the prediction correct ","b3e6007d":"### Classification error: overall how often is the classifier incorrect","2d90c707":"# Classification accuracy: percentage of correct predictions","f21a8798":"AUC is the **percentage** of the ROC plot that is **underneath the curve**:"}}