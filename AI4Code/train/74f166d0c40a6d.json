{"cell_type":{"7610f9d9":"code","50e93bf7":"code","12ee96be":"code","f1923e4e":"code","c980413b":"code","6fb56b99":"code","2bab15f3":"code","ec539aa7":"code","2f8cdd36":"code","8ddb3dfd":"code","443bdf30":"code","d1faf6b8":"code","9c61d13b":"code","635f56fe":"code","0be1396b":"code","fcdf77a7":"code","e4be19e8":"code","b7a58720":"code","710fe83e":"code","e8e22403":"code","6e749bae":"code","319df63b":"code","d29158af":"code","ddafc65e":"code","cb5a7eed":"code","6262942e":"code","ef5e4b71":"code","91289ec4":"code","8d52495d":"code","0a74d95b":"code","bbaaef8f":"code","a1246e38":"code","d1b23b48":"code","e29c86e4":"code","f55ece79":"code","4075276b":"code","5539a66f":"code","c81eaa26":"markdown","0b753a95":"markdown","b4816bab":"markdown"},"source":{"7610f9d9":"import pandas as pd\nimport numpy as np\nimport os\nimport joblib\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()","50e93bf7":"_input_path = os.path.join('..', 'input', '1056lab-football-players-value-prediction')\nos.listdir(_input_path)","12ee96be":"df_train = pd.read_csv(os.path.join(_input_path, 'train.csv'), index_col=0)\ndf_test = pd.read_csv(os.path.join(_input_path, 'test.csv'), index_col=0)","f1923e4e":"target_col = list(set(df_train.columns) - set(df_test.columns))[0]\nprint(target_col)\n\nlog_trans = False","c980413b":"from sklearn.metrics import mean_squared_log_error, mean_squared_error\ndef rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","6fb56b99":"fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\nsns.distplot(df_train[target_col], ax=axs[0], kde=False, axlabel=\"Origin\")\nsns.distplot(np.log1p(df_train[target_col]), kde=False, ax=axs[1], axlabel=\"Log Transed\")\nplt.show()","2bab15f3":"fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\nsns.boxplot(df_train[target_col], ax=axs[0])\nsns.boxplot(np.log1p(df_train[target_col]), ax=axs[1])\nplt.show()","ec539aa7":"df_train.head()","2f8cdd36":"df_test.head()","8ddb3dfd":"df_train.info()","443bdf30":"df_test.info()","d1faf6b8":"def extract_objects(train, test=None):\n    df = pd.concat([train, test], axis=0) if test is not None else train\n    \n    object_cols = []\n    for col in df.columns:\n        typ = df[col].dtype\n        if typ == object or (typ != int and typ != float):\n            object_cols.append(col)\n    return object_cols","9c61d13b":"object_cols = extract_objects(df_train, df_test)\nf\"Object Columns : {object_cols}\"","635f56fe":"df_train[target_col] = np.log1p(df_train[target_col])\nlog_trans = True","0be1396b":"from sklearn.base import BaseEstimator, TransformerMixin\nclass CountEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, return_df=False, return_full=False, ignore_cols=[], trans_cols=[]):\n        self.return_df = return_df\n        self.return_full = return_full\n        self.ignore_cols = ignore_cols\n        self.trans_cols = trans_cols\n    \n    def _check_in_cols(self, X):\n        if isinstance(X, pd.core.series.Series):\n            return not X.name in self.trans_cols\n        if isinstance(X, pd.core.frame.DataFrame):\n            pass\n    \n    def fit(self, X, y=None):\n        assert isinstance(X, pd.core.frame.DataFrame) or isinstance(X, pd.core.series.Series), \"X\u306fDataFrame\u304bSeries\u306b\u3057\u3066\u304f\u3060\u3055\u3044\"\n        \n        if len(self.trans_cols) <= 0 or self.trans_cols is None:\n            if isinstance(X, pd.core.series.Series):\n                cols = [X.name]\n            elif isinstance(X, pd.core.frame.DataFrame):\n#                 cols = X.columns\n                cols = extract_obj_cols(X)\n                cols = list(set(cols) - set(self.ignore_cols))\n            self.trans_cols = cols\n\n        count = {}\n        for col in self.trans_cols:\n            if isinstance(X, pd.core.series.Series):\n                count[col] = X.value_counts().to_dict()\n            else:\n                count[col] = X[col].value_counts().to_dict()\n        self.count = count\n\n        return self\n    \n    def transform(self, X, y=None):\n#         print(self.trans_cols)\n#         print(self.count)\n        assert self.trans_cols is not None, \"\u5909\u63db\u3059\u308b\u5217\u540d\u304c\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u307e\u305b\u3093\"\n#         assert not False in [i in X.columnsself.trans_cols]\n        \n        df_transed = X.copy()\n        \n        for col in self.trans_cols:\n            if isinstance(X, pd.core.series.Series):\n                diff = list([set(self.count[col].keys())][0] - set(X.unique()))\n#                 print(diff)\n                for i in diff: self.count[i] = 0\n                df_transed = df_transed.map(self.count[col])\n            else:\n                diff = list(set([self.count[col].keys()][0]) - set(X[col].unique()))\n                for i in diff: self.count[i] = 0\n                df_transed[\"CE_\"+col] = df_transed[col].map(self.count[col])\n        \n        if self.return_full or isinstance(X, pd.core.series.Series):\n            return df_transed\n        else:\n            return df_transed[[\"CE_\"+col for col in self.trans_cols]]\n\n\nclass FrequencyEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X: pd.Series, y=None):\n        self._dict = X.value_counts().to_dict()\n        return self\n    \n    def transform(self, X: pd.Series, y=None):\n        transed = X.map(self._dict) \/ X.count()\n        return transed\n\n\nclass CombinCountEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, trans_cols=[], prefix='', return_full=True):\n        self.trans_cols = trans_cols\n        self.prefix = prefix\n        self.return_full = return_full\n        \n    def fit(self, X: pd.DataFrame, y=None):\n        if len(self.trans_cols) <= 0:\n            self.trans_cols = extract_obj_cols(X.columns)\n        return self\n    \n    def transform(self, X: pd.DataFrame, y=None):\n        X_ = X.copy()\n        transed_cols = []\n        for cols in list(itertools.combinations(self.trans_cols, 2)):\n            col_name = '{}{}_{}'.format(self.prefix, cols[0], cols[1])\n            _tmp = X_[cols[0]].astype(str) + '_' + X_[cols[1]].astype(str)\n            cnt_map = _tmp.value_counts().to_dict()\n            X_['CCE_'+col_name] = _tmp.map(cnt_map)\n            transed_cols.append('CCE_'+col_name)\n        \n        return X_ if self.return_full else X_[transed_cols]\n\n\nclass AutoCalcEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, num_cols=[], return_full=True, target_col='symboling'):\n        self.num_cols = num_cols\n        self.return_full = return_full\n        self.target_col = target_col\n        \n    def fit(self, X: pd.DataFrame, y=None):\n        if len(self.num_cols) <= 0:\n            self.num_cols = list(set(X.columns) - set(extract_obj_cols(X)))\n        if self.target_col in self.num_cols:\n            self.num_cols.remove(self.target_col)\n        return self\n    \n    def transform(self, X: pd.DataFrame, y=None):\n        X_ = X.copy()\n        transed_cols = []\n        for cols in list(itertools.combinations(self.num_cols, 2)):\n            left, right = X_[cols[0]].astype(float), X_[cols[1]].astype(float)\n            X_['{}_plus_{}'.format(cols[0], cols[1])] = left + right\n            transed_cols.append('{}_plus_{}'.format(cols[0], cols[1]))\n            X_['{}_mul_{}'.format(cols[0], cols[1])] = left * right\n            transed_cols.append('{}_mul_{}'.format(cols[0], cols[1]))\n            try:\n                X_['{}_div_{}'.format(cols[0], cols[1])] = left \/ right\n                transed_cols.append('{}_div_{}'.format(cols[0], cols[1]))\n\n#                 X_['{}_div_{}'.format(cols[0], cols[1])] = right \/ left\n#                 transed_cols.append('{}_div_{}'.format(cols[1], cols[0]))\n                    \n            except:\n                print('{}_div_{}'.format(cols[0], cols[1]))\n            \n        return X_ if self.return_full else X_[transed_cols]\n\n\nclass NullCounter(BaseEstimator, TransformerMixin):\n    def __init__(self, count_cols=[], encoded_feateure_name='null_count'):\n        self.count_cols = count_cols\n        self.encoded_feateure_name = encoded_feateure_name\n        \n    def fit(self, X: pd.DataFrame, y=None):\n        return self\n    \n    def transform(self, X: pd.DataFrame, y=None):\n        X[self.encoded_feateure_name] = X.isnull().sum(axis=1)\n        return X","fcdf77a7":"def check_multi_values(df, search_cols=None):\n    if search_cols is None or len(search_cols) <= 0:\n        search_cols_ = df.columns\n    else:\n        search_cols_ = search_cols\n    \n    multi_cols = []\n    for col in search_cols_:\n        tmp = [val for val in df[col].unique() if ',' in str(val)]\n        multi_cols = multi_cols if len(tmp) == 0 else np.append(multi_cols, col)\n        \n    return multi_cols","e4be19e8":"print(\n    np.unique([list(check_multi_values(df_train)),  list(check_multi_values(df_test))])\n)","b7a58720":"np.max([\n    df_train['player_positions'].apply(lambda x: len(x.split(','))).max(),\n    df_test['player_positions'].apply(lambda x: len(x.split(','))).max()\n])","710fe83e":"df_train['player_positions'].str.split(',')","e8e22403":"def padding(df_tr, df_te, col, padding_val=np.nan):\n    max_values = np.max([\n        df_tr[col].apply(lambda x: len(str(x).split(','))).max(),\n        df_te[col].apply(lambda x: len(str(x).split(','))).max()\n    ])\n#     df = pd.concat([df_tr[col], df_te[col]])\n    return (\n        df_tr[col].str.split(',').apply(lambda x: x + [padding_val] * (max_values - len(x))),\n        df_te[col].str.split(',').apply(lambda x: x + [padding_val] * (max_values - len(x)))\n    )","6e749bae":"pad_posi_train, pad_posi_test = padding(df_train, df_test, 'player_positions')\n\ndf_train['position_first'] = pad_posi_train.apply(lambda x: x[0])\ndf_test['position_first'] = pad_posi_test.apply(lambda x: x[0])\n\ndf_train['position_second'] = pad_posi_train.apply(lambda x: x[1])\ndf_test['position_second'] = pad_posi_test.apply(lambda x: x[1])\n\ndf_train['position_third'] = pad_posi_train.apply(lambda x: x[2])\ndf_test['position_third'] = pad_posi_test.apply(lambda x: x[2])\n\ndf_train.drop('player_positions', axis=1, inplace=True)\ndf_test.drop('player_positions', axis=1, inplace=True)","319df63b":"def exp_to_num(df, col, sep='+'):\n    def multi(values):\n#         print(values)\n        result = None\n        values = [float(val) for val in values]\n        if values is None or np.isnan(values).any():\n            result = np.nan\n        elif len(values) == 1:\n            result = float(values[0])\n        elif len(values) == 2:\n            result = float(values[0]) * 10 ** int(values[1])\n        else:\n            result = values\n        return result\n    \n    return df[col].apply(lambda x: multi(str(x).split(sep)))","d29158af":"exp_cols = []\nfor col in df_test.columns:\n    if df_train[col].apply(lambda x: str(x).split('+')).dropna().apply(lambda x: len(x)).max() > 1:\n        exp_cols.append(col)\nexp_cols","ddafc65e":"for col in tqdm(exp_cols):\n    df_train[col] = exp_to_num(df_train, col)\n    df_test[col] = exp_to_num(df_test, col)","cb5a7eed":"df_train['work_rate_first'] = df_train['work_rate'].apply(lambda x: x.split('\/')[0])\ndf_test['work_rate_first'] = df_test['work_rate'].apply(lambda x: x.split('\/')[0])\n\ndf_train['work_rate_second'] = df_train['work_rate'].apply(lambda x: x.split('\/')[1])\ndf_test['work_rate_second'] = df_test['work_rate'].apply(lambda x: x.split('\/')[1])\n\ndf_train.drop('work_rate', axis=1, inplace=True)\ndf_test.drop('work_rate', axis=1, inplace=True)","6262942e":"df_train[\"joined\"] = pd.to_datetime(df_train[\"joined\"])\ndf_test[\"joined\"] = pd.to_datetime(df_test[\"joined\"])\n\ndf_train[\"joined_day\"] = df_train[\"joined\"].apply(lambda x: x.day)\ndf_test[\"joined_day\"] = df_test[\"joined\"].apply(lambda x: x.day)\n\ndf_train[\"joined_month\"] = df_train[\"joined\"].apply(lambda x: x.month)\ndf_test[\"joined_month\"] = df_test[\"joined\"].apply(lambda x: x.month)\n\ndf_train[\"joined_year\"] = df_train[\"joined\"].apply(lambda x: x.year)\ndf_test[\"joined_year\"] = df_test[\"joined\"].apply(lambda x: x.year)\n\ndf_train.drop([\"joined\"], axis=1, inplace=True)\ndf_test.drop([\"joined\"], axis=1, inplace=True)","ef5e4b71":"object_cols = extract_objects(df_train, df_test)\nobject_cols","91289ec4":"from sklearn.preprocessing import LabelEncoder\nimport category_encoders as ce\n\nnc = NullCounter()\ndf_train = nc.fit_transform(df_train)\ndf_test = nc.transform(df_test)\n\nfor col in tqdm(object_cols):\n#     le = LabelEncoder()\n    if col == 'joined': continue\n    le = ce.OrdinalEncoder()\n    df_train['LE_' + col] = le.fit_transform(df_train[col].values.reshape(-1, 1))\n    df_test['LE_' + col] = le.transform(df_test[col].values.reshape(-1, 1))\n    \n    te = ce.TargetEncoder()\n    df_train['TE_' + col] = te.fit_transform(df_train[col], df_train[target_col])\n    df_test['TE_' + col] = te.transform(df_test[col])\n    \n    cel = CountEncoder()\n    df_train[f\"CE_{col}\"] = cel.fit_transform(df_train[col])\n    df_test[f\"CE_{col}\"] = cel.transform(df_test[col])\n    \n    fe = FrequencyEncoder()\n    df_train[f\"FE_{col}\"] = fe.fit_transform(df_train[col])\n    df_test[f\"FE_{col}\"] = fe.transform(df_test[col])\n    \n    df_train.drop(col, axis=1, inplace=True)\n    df_test.drop(col, axis=1, inplace=True)","8d52495d":"# def reduce_mem_usage(df, verbose=True):\n#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n#     start_mem = df.memory_usage(deep=True).sum() \/ 1024**2    \n#     for col in df.columns:\n#         col_type = df[col].dtypes\n#         if col_type in numerics:\n#             c_min = df[col].min()\n#             c_max = df[col].max()\n#             if str(col_type)[:3] == 'int':\n#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n#                     df[col] = df[col].astype(np.int8)\n#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n#                     df[col] = df[col].astype(np.int16)\n#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n#                     df[col] = df[col].astype(np.int32)\n#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n#                     df[col] = df[col].astype(np.int64)  \n#             else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 else:\n#                     df[col] = df[col].astype(np.float64)    \n#     end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n#     if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n#     return df\n\n# df_train = reduce_mem_usage(df_train)\n# df_test = reduce_mem_usage(df_test)","0a74d95b":"df_train.head()","bbaaef8f":"import optuna\nimport optuna.integration.lightgbm as optuna_lgb\nfrom sklearn.model_selection import train_test_split\n\nX = df_train.drop(target_col, axis=1)\ny = df_train[target_col] if log_trans else np.log1p(df_train[target_col])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)\n\ndtrain = optuna_lgb.Dataset(X_train, label=y_train)\ndvalid = optuna_lgb.Dataset(X_valid, label=y_valid)\ndtest = optuna_lgb.Dataset(df_test)\n\nlgb_params = {\n    'objective': 'root_mean_squared_error',\n    'seed': 42,\n    'metric': \"rmse\"  \n}\n\nbest_params, history = {}, []\nmodel = optuna_lgb.train(lgb_params, dtrain, valid_sets=dvalid,\n                        verbose_eval=False, num_boost_round=10000,\n                        early_stopping_rounds=100,\n                        best_params=best_params,\n                        tuning_history=history)","a1246e38":"best_params","d1b23b48":"predict = model.predict(df_test)","e29c86e4":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(model=model, feature_perturbation='tree_path_dependent')\nshap_values = explainer.shap_values(X)\n\nshap.summary_plot(shap_values, X)\nshap.summary_plot(shap_values, X, plot_type='bar')","f55ece79":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\n\nkf = KFold(n_splits=10, shuffle=True, random_state=0)\n\noof_lgb = np.zeros(len(y))\noof_xgb = np.zeros(len(y))\noof_cat = np.zeros(len(y))\npreds_lgb = np.zeros(len(df_test))\npreds_xgb = np.zeros(len(df_test))\npreds_cat = np.zeros(len(df_test))\n\nfor i, ids in enumerate(kf.split(X)):\n    print(f\"{i+1} Fold\")\n    \n    X_train, y_train = X.iloc[ids[0]], y.iloc[ids[0]]\n    X_valid, y_valid = X.iloc[ids[1]], y.iloc[ids[1]]\n    \n    print('\\tFitting....')\n    print('\\t\\tLightGBM')\n    lgb_model = lgb.LGBMRegressor(**best_params).fit(X_train, y_train)\n    print('\\t\\tXGBoost')\n    xgb_model = xgb.XGBRegressor().fit(X_train, y_train)\n    print('\\t\\tCatBoost')\n    cat_model = cat.CatBoostRegressor(silent=True).fit(X_train, y_train)\n    \n    print('\\tPredicting....')\n    print('\\t\\tLightGBM')\n    cv_pred = np.expm1(lgb_model.predict(X_valid))\n    oof_lgb[ids[1]] += cv_pred\n    print('\\t\\tXGBoost')\n    cv_pred = np.expm1(xgb_model.predict(X_valid))\n    oof_xgb[ids[1]] += cv_pred\n    print('\\t\\tCatBoost')\n    cv_pred = np.expm1(cat_model.predict(X_valid))\n    oof_cat[ids[1]] += cv_pred\n#     print(\"CV : {}\".format(rmse(np.expm1(y_valid), cv_pred)))\n    \n    preds_lgb += lgb_model.predict(df_test)\n    preds_xgb += xgb_model.predict(df_test)\n    preds_cat += cat_model.predict(df_test)\n    print()\n    \npreds_lgb \/= kf.n_splits\npreds_xgb \/= kf.n_splits\npreds_cat \/= kf.n_splits","4075276b":"df_submit_lgb = pd.read_csv(os.path.join(_input_path, 'sampleSubmission.csv'))\n# df_submit[target_col] = np.expm1(predict) if log_trans else predict\ndf_submit_lgb[target_col] = np.expm1(preds_lgb) if log_trans else preds_lgb\ndf_submit_lgb.to_csv('submit_lgb.csv', index=False)\n\ndf_submit_xgb = pd.read_csv(os.path.join(_input_path, 'sampleSubmission.csv'))\n# df_submit[target_col] = np.expm1(predict) if log_trans else predict\ndf_submit_xgb[target_col] = np.expm1(preds_xgb) if log_trans else preds_xgb\ndf_submit_xgb.to_csv('submit_xgb.csv', index=False)\n\ndf_submit_cat = pd.read_csv(os.path.join(_input_path, 'sampleSubmission.csv'))\n# df_submit[target_col] = np.expm1(predict) if log_trans else predict\ndf_submit_cat[target_col] = np.expm1(preds_cat) if log_trans else preds_cat\ndf_submit_cat.to_csv('submit_cat.csv', index=False)","5539a66f":"df_train.to_csv(\"train_transed.csv\")\ndf_test.to_csv(\"test_transed.csv\")","c81eaa26":"# EDA","0b753a95":"# Preprocess","b4816bab":"# Predict"}}