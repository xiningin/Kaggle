{"cell_type":{"63a5b846":"code","9a573b02":"code","09b8d188":"code","dd8e4be6":"code","7e0ab11a":"code","8066bc6b":"code","fce04af1":"code","49076829":"code","da1ecca3":"code","4e64a447":"code","09913d3d":"code","5c085f02":"code","dfd9dfd4":"code","11e42920":"code","75e4375c":"code","4cc80c15":"code","047e4034":"code","f0c14363":"code","dba9c7a4":"code","bccbe3c8":"code","1a5ac822":"code","eb3b650b":"code","b29789c9":"code","23e2b6c5":"code","b65b508d":"code","257f985d":"code","7e1f473c":"code","9b9ece7d":"code","a6234e17":"code","e0132004":"code","81d217ef":"code","524bc99e":"code","62a03eb8":"code","3075b3c9":"code","cb61bfca":"code","efc1c75b":"code","f0f4cc2d":"code","abf99c35":"code","9f9b0cc8":"markdown","bb05672d":"markdown","df187df3":"markdown","6216e4b4":"markdown","5b64d960":"markdown","29a3bf12":"markdown","ba47bd65":"markdown","68704cca":"markdown","1c6beb8b":"markdown","b56166c3":"markdown","aacb5fd7":"markdown","c372cff2":"markdown","ddcc6cbf":"markdown","418cc4e8":"markdown","1af15bc2":"markdown","aaeac19b":"markdown","650d2c34":"markdown"},"source":{"63a5b846":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport sys\nimport time\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport torchvision.transforms as transforms\nloader=transforms.ToTensor()\n\nfrom PIL import Image","9a573b02":"import os\nimport shutil\ndata_dir=r'..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset'\nos.listdir(data_dir)","09b8d188":"root=r'..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset'\n\nIMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', 'webp']\ndef has_file_allowed_extension(filename, extensions):\n    \"\"\"Check if the file is supported scalable type\n\n    Args:\n        filename (string): file path \n                 Extensions: Scalable type list, an acceptable image file type\n\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    \"\"\"\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in  extensions) # Return to True or False list\n\n\ndef make_dataset(dir, class_to_idx, extensions):\n    \"\"\"\n                Return, such as [(image path, category index value corresponding to), (), ...]\n    \"\"\"\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in class_to_idx.keys():\n        d=os.path.join(dir,target,target)\n        print(d)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in  sorted(os.walk (d)):\n            for fname in fnames:\n                if  has_file_allowed_extension(fname, extensions): #Viewing if the file is supported, it is continued\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n    return images\n                \n    \nclass CustomDataFolder(Dataset):\n    \n    def __init__(self, root, loader, extensions, transform=None, target_transform=None):\n        \n        def find_classes( dir):\n            classes=[]\n            for class_ in os.listdir(dir):\n                if os.path.isdir(root+'\/'+class_):\n                    classes.append(class_)\n            class_to_idx = {classes[i]: i for i in  range(len(classes))}\n            return classes, class_to_idx\n        \n        classes, class_to_idx = find_classes (root) # get class name and class index, such as ['cat', 'dog']with{'cat': 0, 'dog': 1}\n                 #       [(image path, the category value of the image), (), ...], that is, tag each image\n        samples = make_dataset(root, class_to_idx, extensions) \n        if len(samples) == 0:\n            raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n                               \"Supported extensions are: \" + \",\".join(extensions)))\n\n        self.root = root\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n        self.targets = [s[1] for s in  samples] # Lists consisting of all images\n\n        self.transform = transform\n        self.target_transform = target_transform\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        \"\"\"\n        path, target = self.samples[index]\n        image = Image.open(path)\n        sample =  self.loader (image) # loading pictures\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n    \n\n    \n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n        fmt_str += '    Root Location: {}\\n'.format(self.root)\n        tmp = '    Transforms (if any): '\n        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        tmp = '    Target Transforms (if any): '\n        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        return fmt_str","dd8e4be6":"class CustomImageFolder(CustomDataFolder):\n    \"\"\"A generic data loader where the images are arranged in this way: ::\n\n        root\/dog\/xxx.png\n        root\/dog\/xxy.png\n        root\/dog\/xxz.png\n\n        root\/cat\/123.png\n        root\/cat\/nsdf3.png\n        root\/cat\/asd932_.png\"\"\"\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=loader):\n        super(CustomImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n                                          transform=transform,\n                                          target_transform=target_transform)\n        self.imgs = self.samples","7e0ab11a":"\nimport torchvision.transforms as tt\ntransform = tt.Compose([\n                         tt.Scale((400,400)), \n                         # tt.RandomRotate\n                         # tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                         ])\n\ndataset=CustomImageFolder(root,transform=transform)","8066bc6b":"img,label=dataset[5000]\nprint(img.shape,label)","fce04af1":"dataset.classes","49076829":"def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))\nshow_example(*dataset[10])","da1ecca3":"show_example(*dataset[5000])","4e64a447":"show_example(*dataset[7000])","09913d3d":"random_seed = 42\ntorch.manual_seed(random_seed);\n","5c085f02":"val_size=900\ntrain_size=len(dataset)-val_size\n\ntrain_ds,valid_ds= random_split(dataset, [train_size, val_size])\nprint(len(train_ds),len(valid_ds))\n","dfd9dfd4":"from torch.utils.data.dataloader import DataLoader\n\nbatch_size=16","11e42920":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_dl = DataLoader(valid_ds, batch_size, num_workers=2, pin_memory=True)","75e4375c":"for img,label in train_dl:\n    print(img.shape)\n    print(label)\n    break","4cc80c15":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=4).permute(1, 2, 0))\n        break\nshow_batch(train_dl)","047e4034":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","f0c14363":"device = get_default_device()\ndevice","dba9c7a4":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(val_dl, device)","bccbe3c8":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","1a5ac822":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","eb3b650b":"class SimpleResidualBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        return self.relu2(out) + x # ReLU can be applied before or after adding the input","b29789c9":"simple_resnet = to_device(SimpleResidualBlock(), device)\n\nfor images, labels in train_dl:\n    out = simple_resnet(images)\n    print(out.shape)\n    break\n    \ndel simple_resnet, images, labels\ntorch.cuda.empty_cache()","23e2b6c5":"def conv_block(in_channels,out_channels,pool=False):\n    layers=[nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1),\n           nn.BatchNorm2d(out_channels),\n           nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self,in_channels,num_classes):\n        super().__init__()\n        self.conv1=conv_block(in_channels,64)\n        self.conv2=conv_block(64,128,pool=True)\n        self.res1=nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        self.conv3=conv_block(128, 256, pool=True)\n        self.conv4=conv_block(256, 512, pool=True)\n        self.res2=nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n                                        nn.Flatten(), \n                                        nn.Dropout(0.2),\n                                        nn.Linear(73728, num_classes))\n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out    \n    \nmodel = to_device(ResNet9(3, 9), device)\nmodel\n# After res2 torch.Size([64, 512, 50, 50])\n# After maxpool torch.Size([64, 512, 12, 12])\n# torch.Size([64, 73728])\n# torch.Size([64, 73728])","b65b508d":"torch.cuda.empty_cache()","257f985d":"batch_size = 32\nn_iters = 30\nepochs  = 10#int( n_iters \/ (len(train_dl) \/ batch_size))\ninput_dim = 3*400*400\noutput_dim = 9\nlr_rate  = 0.001","7e1f473c":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","9b9ece7d":"for img,label in valid_dl:\n    print(img.shape)\n    print(label)\n    break","a6234e17":"history = [evaluate(model, valid_dl)]\nhistory","e0132004":"epochs = 8\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","81d217ef":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","524bc99e":"plot_accuracies(history)","62a03eb8":"plot_losses(history)","3075b3c9":"from torchvision.datasets import ImageFolder\npath='..\/input\/a-large-scale-fish-dataset\/NA_Fish_Dataset'\nimport torchvision.transforms as tt\ntransform = tt.Compose([\n                         tt.Scale((400,400)), \n                         tt.ToTensor()\n                         # tt.RandomRotate\n                         # tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                         ])\ntest_data=ImageFolder(path,transform=transform)\ntest_data.classes","cb61bfca":"for img,label in test_data:\n    print(img.shape)\n    print(label)\n    break","efc1c75b":"def predict_image(img,model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return test_data.classes[preds[0].item()]\n","f0f4cc2d":"img,label=test_data[34]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', test_data.classes[label], ', Predicted:', predict_image(img, model))","abf99c35":"torch.save(model.state_dict(), 'fishes_large_scale-resnet9.pth')","9f9b0cc8":"## Paramters\n","bb05672d":"## Single Block\n![](http:\/\/miro.medium.com\/max\/1140\/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)","df187df3":"## Test the residual block","6216e4b4":"## GPU settings","5b64d960":"## Load data to GPU","29a3bf12":"![](http:\/\/raw.githubusercontent.com\/lambdal\/cifar10-fast\/master\/net.svg)","ba47bd65":"## Show Multiple Images in Grid","68704cca":"## Training the Model","1c6beb8b":"## Test the images","b56166c3":"## Resnet Architecture","aacb5fd7":"## Base Class Module","c372cff2":"## Plotting Functions","ddcc6cbf":"## Save the Parameters","418cc4e8":"## Main Resnet Architecture","1af15bc2":"## Divide Data into train and test","aaeac19b":"`Load the data into batches`","650d2c34":"`Resnet9 Architecture`"}}