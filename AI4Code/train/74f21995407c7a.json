{"cell_type":{"8f650be0":"code","4782cc36":"code","af190a3a":"code","a8612fe0":"code","76773f09":"code","64541d39":"code","8a678c77":"code","5fde0a05":"code","a752eb1a":"code","04d9c0fb":"code","b9d311f9":"code","c70d92b3":"code","325cedd6":"code","6e835c1a":"code","4a0e1731":"code","9b11ac5c":"code","3525a3c9":"code","c2b2a167":"code","f81395b3":"code","f5ce7cf1":"code","c709c2f7":"code","f3cfb9e7":"code","a0cfc1d6":"code","dd15b1f8":"code","9771889e":"code","26f4721e":"code","28c31420":"code","4c832258":"code","5d36ed59":"code","dd3dd893":"code","b4595d50":"code","02c27470":"code","8e2aaa00":"code","82b313c9":"code","177442cb":"code","d9004274":"code","6633d1e6":"code","635d165d":"code","b5b01ecd":"code","327a4cde":"code","50c0e2b8":"code","ccc30650":"code","86e5ad01":"code","e4a02ec7":"code","c3145daa":"code","2c17ec75":"code","dea47219":"code","8cda7dcf":"code","57642673":"code","8e60199c":"code","bdd9f7ae":"code","aa1799a9":"code","6d269c8a":"markdown","2e1ea469":"markdown","288473b7":"markdown","8210d9db":"markdown","6327dfd9":"markdown","0be3e1e3":"markdown","4b4e5d89":"markdown","f10f33ae":"markdown","d5da91a4":"markdown","6a5dfc19":"markdown","516b2403":"markdown","4cc7b2dd":"markdown","aa1f5769":"markdown","44aa2924":"markdown","e2037168":"markdown","425451e6":"markdown","bb210b2f":"markdown","4bbb1c24":"markdown","f1296511":"markdown"},"source":{"8f650be0":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4782cc36":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, cross_validate\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, LabelEncoder, RobustScaler, Normalizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRFClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import multilabel_confusion_matrix, label_ranking_loss, log_loss, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.decomposition import PCA \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","af190a3a":"test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_targets = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","a8612fe0":"#quick look at our data types & null counts \ntrain_features.info()","76773f09":"# To better understand the numeric data, we want to use the .describe() method. \n# This gives us an understanding of the central tendencies of the data. \n\ntrain_features.describe()","64541d39":"# Reading the train dataset\ntrain_features.head(3)","8a678c77":"# Reading the test dataset \ntest_features.head(3)","5fde0a05":"# To calculate the rows and columns in the dataset\nprint('The Training dataset has {} rows and {} columns.'.format(len(train_features), len(train_features.columns)))","a752eb1a":"print('The Test dataset has {} rows and {} columns.'.format(len(test_features), len(test_features.columns)))","04d9c0fb":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\n\nsns.countplot(train_features['cp_type'],palette=(\"Blues\"), ax=ax[0])\nax[0].set_title('cp_type distribution')\n\nsns.countplot(train_features['cp_time'],palette=(\"Blues\"), ax=ax[1])\nax[1].set_title('cp_time distribution')\n\nsns.countplot(train_features['cp_dose'],palette=(\"Blues\"), ax=ax[2])\nax[2].set_title('cp_dose distribution')\n\nfig.suptitle('Distribution in Train dataset of Type, Time and Dose')","b9d311f9":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\n\nsns.countplot(test_features['cp_type'],palette=(\"BuGn_r\"), ax=ax[0])\nax[0].set_title('cp_type distribution')\n\nsns.countplot(test_features['cp_time'],palette=(\"BuGn_r\"), ax=ax[1])\nax[1].set_title('cp_time distribution')\n\nsns.countplot(test_features['cp_dose'],palette=(\"BuGn_r\"), ax=ax[2])\nax[2].set_title('cp_dose distribution')\n\nfig.suptitle('Distribution in Test dataset of Type, Time and Dose')","c70d92b3":"train_features['cp_type'] = train_features['cp_type'].astype('category')\ntrain_features['cp_type'].cat.categories = [0, 1]\ntrain_features['cp_type'] = train_features['cp_type'].astype(\"int\")","325cedd6":"train_features['cp_dose'] = train_features['cp_dose'].astype('category')\ntrain_features['cp_dose'].cat.categories = [0, 1]\ntrain_features['cp_dose'] = train_features['cp_dose'].astype(\"int\")","6e835c1a":"train_features['cp_time'] = train_features['cp_time'].astype('category')\ntrain_features['cp_time'].cat.categories = [0, 1, 2]\ntrain_features['cp_time'] = train_features['cp_time'].astype(\"int\")","4a0e1731":"train_features.head()","9b11ac5c":"print('Number of \"g-\" features are: ', len([i for i in train_features.columns if i.startswith('g-')]))\nprint('Number of \"c-\" features are: ', len([i for i in train_features.columns if i.startswith('c-')]))","3525a3c9":"fig, ax = plt.subplots(3, 3, figsize=(20, 5))\n\nsns.kdeplot(test_features['g-0'], shade = True, color = 'coral', ax=ax[0, 0])\nsns.kdeplot(test_features['g-20'], shade = True, color = 'coral', ax=ax[0, 1])\nsns.kdeplot(test_features['g-555'], shade = True, color = 'coral', ax=ax[0, 2])\nsns.kdeplot(test_features['g-105'], shade = True, color = 'coral', ax=ax[1, 0])\nsns.kdeplot(test_features['g-725'], shade = True, color = 'coral', ax=ax[1, 1])\nsns.kdeplot(test_features['g-598'], shade = True, color = 'coral', ax=ax[1, 2])\nsns.kdeplot(test_features['g-366'], shade = True, color = 'coral', ax=ax[2, 0])\nsns.kdeplot(test_features['g-450'], shade = True, color = 'coral', ax=ax[2, 1])\nsns.kdeplot(test_features['g-600'], shade = True, color = 'coral', ax=ax[2, 2])","c2b2a167":"fig, ax = plt.subplots(3, 3, figsize=(20, 5))\n\nsns.kdeplot(test_features['c-0'], shade = True, color = 'blue', ax=ax[0, 0])\nsns.kdeplot(test_features['c-20'], shade = True, color = 'blue', ax=ax[0, 1])\nsns.kdeplot(test_features['c-99'], shade = True, color = 'blue', ax=ax[0, 2])\nsns.kdeplot(test_features['g-66'], shade = True, color = 'blue', ax=ax[1, 0])\nsns.kdeplot(test_features['g-88'], shade = True, color = 'blue', ax=ax[1, 1])\nsns.kdeplot(test_features['g-73'], shade = True, color = 'blue', ax=ax[1, 2])\nsns.kdeplot(test_features['g-45'], shade = True, color = 'blue', ax=ax[2, 0])\nsns.kdeplot(test_features['g-59'], shade = True, color = 'blue', ax=ax[2, 1])\nsns.kdeplot(test_features['g-37'], shade = True, color = 'blue', ax=ax[2, 2])","f81395b3":"g_cols = [f'g-{i}' for i in range(772)]\nfig, ax = plt.subplots(1, 2, figsize=(20, 4))\n\nsns.distplot(train_features[g_cols].mean(), kde=False,color = 'green', bins = 75, ax = ax[0])\nsns.distplot(train_features[g_cols].std(), kde=False,color = 'green',  bins = 75, ax = ax[1])","f5ce7cf1":"c_cols = [f'c-{i}' for i in range(100)]\nfig, ax = plt.subplots(1, 2, figsize=(20, 4))\n\nsns.distplot(train_features[c_cols].mean(), kde=False,color = 'purple', bins = 15, ax = ax[0])\nsns.distplot(train_features[c_cols].std(), kde=False,color = 'purple',  bins = 15, ax = ax[1])","c709c2f7":"# Compute the correlation matrix\ncorr = train_features[g_cols[:40]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Pairwise correlations of gene features for first 40')\nplt.show()","f3cfb9e7":"f, ax = plt.subplots(figsize=(15, 4))\nsns.distplot(train_features[g_cols].corr(),color='r')\nplt.title(\"Distribution for gene feature\")\nplt.show()","a0cfc1d6":"plt.figure(figsize=(10,7))\nsns.heatmap(train_features[c_cols].corr(), cmap='coolwarm');","dd15b1f8":"f, ax = plt.subplots(figsize=(15, 4))\nsns.distplot(train_features[c_cols].corr(),color='lightsalmon')\nplt.title(\"Distribution for Gene Feature\")\nplt.show()","9771889e":"train_features_cor = train_features.corr()","26f4721e":"train_features_cor.head()","28c31420":"vifs = pd.DataFrame(np.linalg.inv(train_features_cor.values).diagonal(), index = train_features_cor.index, columns=['VIF'])","4c832258":"vifs.tail(6)","5d36ed59":"# we take a value where vif is greater than 15.\n\ngreater_vifs = vifs.where(vifs>15)\ngreater_vifs = greater_vifs.dropna()","dd3dd893":"cols_remove = greater_vifs.index","b4595d50":"cols_remove \n# This is a feature that have highly correlated with any number of the other variables.","02c27470":"new_train_features_data = train_features.drop(columns=cols_remove) # we drop these columns highly correlated","8e2aaa00":"new_train_features_data.head()","82b313c9":"test_features['cp_type'] = test_features['cp_type'].astype('category')\ntest_features['cp_type'].cat.categories = [0, 1]\ntest_features['cp_type'] = test_features['cp_type'].astype(\"int\")","177442cb":"test_features['cp_dose'] = test_features['cp_dose'].astype('category')\ntest_features['cp_dose'].cat.categories = [0, 1]\ntest_features['cp_dose'] = test_features['cp_dose'].astype(\"int\")","d9004274":"test_features['cp_time'] = test_features['cp_time'].astype('category')\ntest_features['cp_time'].cat.categories = [0, 1, 2]\ntest_features['cp_time'] = test_features['cp_time'].astype(\"int\")","6633d1e6":"test_features.head()","635d165d":"targets = train_targets_scored","b5b01ecd":"targets.head()","327a4cde":"print('The Tergets dataset has {} rows and {} columns.'.format(len(targets), len(targets.columns)))","50c0e2b8":"target_cols = targets.columns[1:]","ccc30650":"targets_fre = (targets[target_cols].mean() * 100).sort_values()[-20:].index","86e5ad01":"plt.figure(figsize=(15,4))\n(targets[targets_fre].mean() * 100).sort_values().plot.bar()\nplt.title('Most frequent targets')\nplt.ylabel('% of true labels')\nplt.show()","e4a02ec7":"plt.figure(figsize=(15,4))\nvc = targets[target_cols].sum(axis=1).value_counts()\nplt.title('True labels per row distribution')\nplt.ylabel('# of rows')\nplt.xlabel('# of true targets per row')\nplt.bar(vc.index, vc.values)\nplt.show()","c3145daa":"train_targets_scored.drop(['sig_id'], axis=1, inplace=True)","2c17ec75":"train_targets_scored.head()","dea47219":"test_ids = test_features['sig_id']","8cda7dcf":"for d in [new_train_features_data, test_features]:\n    d.drop(['sig_id','cp_type', 'cp_dose', 'cp_time'], axis=1, inplace=True)\n    \ntrain_features.head()","57642673":"x_train, x_cv, y_train, y_cv = train_test_split(new_train_features_data, train_targets_scored, test_size=0.2) ","8e60199c":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(512, input_dim=x_train.shape[1], activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(y_train.shape[1], activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()","bdd9f7ae":"factor= 0.8250037987063858\npatience=2\nmin_lr= 5.101088055532695e-05\nlr=6.353131263848553e-05\nbatch_size=353\nepochs=359\n\ndef callbacks(file_path):\n    reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n                                         factor=factor,\n                                         patience=patience,\n                                         cooldown=1,\n                                         min_lr=min_lr,\n                                         verbose=1)\n    checkpoint = ModelCheckpoint(filepath = file_path,monitor='val_loss',\n                                 mode='min',save_best_only=True,verbose=1)\n\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience= patience)\n\n    return [reduce_learning_rate,checkpoint,early]\n\nfile_path = model.name+'best_weights.hd5'\ncallbacks_list = callbacks(file_path = file_path)\n\noptimizer = tf.keras.optimizers.Adam(lr=lr, amsgrad=True)\n#compile the model\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=optimizer)\n\nhistory=model.fit(x_train,y_train,epochs= epochs, batch_size=batch_size, callbacks = callbacks_list)","aa1799a9":"(test_targets[pd.read_csv('..\/input\/lish-moa\/sample_submission.csv').columns]).to_csv('submission.csv',index=False)","6d269c8a":"### Gene Feature","2e1ea469":"### Mean and standard deviation for Cell viability feature","288473b7":"### Test Features","8210d9db":"* The most case the points are centred around 0 are close to normal distribution.\n* In some case we can find skeness in the data","6327dfd9":"### correlation matrix for gene feature ","0be3e1e3":"## Checking the Normal Distribution or Gaussian Distribution in the dataset.","4b4e5d89":"### Mean and standard deviation for gene feature","f10f33ae":"The lowest value is around 0.6, therefore the value are highly correlated","d5da91a4":"Top 20 most frequent targets.","6a5dfc19":"### correlation matrix for cell viability feature","516b2403":"# Model Building","4cc7b2dd":"Some feature are most correlated with other feature. To see the multicolinearity of all feature, we are using **Variance Inflation Factor(VIF)**.","aa1f5769":"In this we can see that some of the feature are strongly correlate with each other.","44aa2924":"### Cell viability features","e2037168":"### Numerical value in the dataset","425451e6":"## categorical data in the dataset","bb210b2f":"* The g- signify gene feature. It's from  g-0 : g-771\n* The c- signify cell viability feature, It's from  c-0 : c-99","4bbb1c24":"# Targets","f1296511":"The correlation of the gene is Normal Distributed or Gaussian Distribution and also we do have some outlayers."}}