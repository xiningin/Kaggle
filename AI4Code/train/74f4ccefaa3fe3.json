{"cell_type":{"e81b0a33":"code","da0c2378":"code","e2ac813c":"code","c37ad1f3":"code","776ceab8":"code","8fdd45e9":"code","85949b3f":"code","a334f1c7":"code","21f597a2":"code","2bff8cf9":"code","bcae93fb":"code","c9da9246":"code","fbeadd33":"code","ce996709":"code","aeb4d34c":"code","e73f4008":"code","15644029":"code","ecbbd7b0":"code","10d513ae":"code","9cf21b9a":"code","e9fc29b1":"code","acc884be":"code","fb8cf904":"code","e62ea6f6":"code","5b99530a":"code","4eaa96ef":"code","5a6d965d":"code","df4dc8d4":"code","e0c80cb0":"code","148d1643":"markdown","aa11b084":"markdown","e40cca52":"markdown","65b19f53":"markdown","1ade8edf":"markdown","5a6d5e32":"markdown","2f08603f":"markdown","bb9bf781":"markdown","63d5ef65":"markdown","fa4ad936":"markdown","34d0a028":"markdown","7d03eb7b":"markdown","0c6aa5ad":"markdown","8d9ab139":"markdown","a8b97688":"markdown","ee149379":"markdown","3c93974e":"markdown","3ae19a30":"markdown","6722ed20":"markdown","4b8c3e9e":"markdown","7c9a6720":"markdown"},"source":{"e81b0a33":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import RandomizedSearchCV, KFold\nfrom sklearn.metrics import accuracy_score, classification_report","da0c2378":"import os\npath = '\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/'\n#\u00a0select the main folders\nfolders = os.listdir(path)\n#\u00a0delete that file\nds = '.DS_Store'\nfolders.pop(folders.index(ds))\n#\u00a0add the path to the folders\nfolders = [(path + folder + '\/') for folder in folders]\nfolders","e2ac813c":"# returns two lists of image routes, normal, pneumonia\n# lists contained in a dictionary\ndef main_folder_data(main_folder):\n    res = {}\n    # iterate the subfolders located there\n    for sub_folder in os.listdir(main_folder):\n        #\u00a0not to take the ds\n        if sub_folder != ds:\n            #\u00a0list to set the image routes\n            data = []\n            #\u00a0actual route\n            actual_route = main_folder + sub_folder + '\/'\n            #\u00a0iterate the images\n            for image in os.listdir(actual_route):\n                #\u00a0not to take the ds\n                if image != ds:\n                    # the image path\n                    data.append(actual_route + image)\n            #\u00a0save the routes in the dict\n            res[sub_folder.lower()] = data\n    return res\n\n#\u00a0how the function looks like with val_dic\nmain_folder_data(folders[0])","c37ad1f3":"# get all the images route for each dataset\nval_dic = main_folder_data(folders[0])\ntest_dic = main_folder_data(folders[1])\ntrain_dic = main_folder_data(folders[2]) \n\ndics = [train_dic, val_dic, test_dic]\n\n# set the dicts in a dataframe for plot\n#\u00a0just to see the number of images in each\nnumbers = pd.DataFrame()\n#\u00a0how many images of each category\nnum_normal = np.array([len(dic['normal']) for dic in dics])\nnum_pneum = np.array([len(dic['pneumonia']) for dic in dics])\n#\u00a0set the dataframe\nnumbers['number'] = np.concatenate((num_normal, num_pneum))\nnumbers['state'] = ['normal','normal','normal','pneumonia','pneumonia','pneumonia']\nnumbers['set'] = ['validation','testing', 'training','validation','testing', 'training']\n\n# Draw a nested barplot by species and sex\ng = sns.catplot(\n    data=numbers, kind=\"bar\",\n    x=\"set\", y=\"number\", hue=\"state\",\n    ci=\"sd\", palette=\"dark\", alpha=.6, height=6\n)\ng.despine(left=True)\ng.set_axis_labels(\"Datasets\",\"Number of Images\")\ng.legend.set_title(\"\")\nprint(numbers['number'].values)","776ceab8":"X = []\n#\u00a0open an image with gray color, and with a new size of 128x128. val_dic['normal'][0], first validation image\nimage = keras.preprocessing.image.load_img(val_dic['normal'][0], color_mode='grayscale', target_size=(128,128))\n# then convert the image to an array and save it in x\nX.append(keras.preprocessing.image.img_to_array(image))\n\n# do the same for the second image\nimage = keras.preprocessing.image.load_img(val_dic['normal'][1], color_mode='grayscale', target_size=(128,128))\nX.append(keras.preprocessing.image.img_to_array(image))\n\n# and select another two images with pneumonia\nimage = keras.preprocessing.image.load_img(val_dic['pneumonia'][0], color_mode='grayscale', target_size=(128,128))\nX.append(keras.preprocessing.image.img_to_array(image))\nimage = keras.preprocessing.image.load_img(val_dic['pneumonia'][1], color_mode='grayscale', target_size=(128,128))\nX.append(keras.preprocessing.image.img_to_array(image))","8fdd45e9":"# set the plot size\nfig = plt.figure(figsize = (10,10))\n\n#\u00a0iterate the images by index in X\nfor i in range(4):\n    #\u00a0it just works that way\n    plt.subplot(220+i+1)\n    #\u00a0show the i image\n    plt.imshow(X[i])\n    if i < 2:\n        plt.title('Normal')\n    else:\n        plt.title('Pneumonia')","85949b3f":"def get_set(dic):\n    images = []\n    labels = []\n    for key in dic.keys():  \n        for path in dic[key]:\n            # open the images with the format\n            image = keras.preprocessing.image.load_img(path, color_mode='grayscale', target_size=(128,128))\n            #\u00a0add the image to the list\n            images.append(keras.preprocessing.image.img_to_array(image))\n            # add the label to the list\n            labels.append(1 if key == 'pneumonia' else 0)\n    #\u00a0convert the lists to np arrays\n    #\u00a0i think it must be type float\n    images = np.array(images, dtype = float)\n    #\u00a0apply the normalization for each image\n    images = images\/255.0\n    # and too the labels in a np array\n    labels = np.array(labels)\n    return images, labels\n\n# set every dataset\nx_val, y_val = get_set(val_dic)\nx_test, y_test = get_set(test_dic)\nx_train, y_train = get_set(train_dic)","a334f1c7":"# shapes of each set\nprint(f'Train: {y_train.shape[0]}, Test: {y_test.shape[0]}, Val: {y_val.shape[0]}')\n\n# shapes of each image path dic\nval_size = len(val_dic['normal']) + len(val_dic['pneumonia'])\ntest_size = len(test_dic['normal']) + len(test_dic['pneumonia'])\ntrain_size = len(train_dic['normal']) + len(train_dic['pneumonia'])\nprint(f'Train: {train_size}, Test: {test_size}, Val: {val_size}')    ","21f597a2":"#\u00a0define a function to set the neural network\ndef cnn(num_filters_1, num_filters_2, num_filters_3, hidden_nodes):\n    \n    # create the cnn  model\n    model = keras.models.Sequential()\n    # set the input shape as the previous images\n    model.add(keras.Input(shape = (128, 128, 1)))\n    \n    # 1st convolution layer, padding is valid by default\n    # kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n    model.add(keras.layers.Conv2D(num_filters_1, (3,3), padding = 'valid', kernel_regularizer = 'l2'))\n    #\u00a0makes nn faster through the input normalization\n    model.add(keras.layers.BatchNormalization())\n    # activation function as relu\n    model.add(keras.layers.Activation('relu'))\n    # Inputs not set to 0 are scaled up by 1\/(1 - rate)\n    model.add(keras.layers.Dropout(0.2))\n    # apply pooling function, same padding is valid by default\n    model.add(keras.layers.MaxPooling2D((2,2), padding = 'valid'))\n    \n    # 2nd convolution layer, same as the first one\n    model.add(keras.layers.Conv2D(num_filters_2, (3,3), padding = 'valid', kernel_regularizer = 'l2'))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation('relu'))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.MaxPooling2D((2,2), padding = 'valid'))\n    \n    # 3rd convolution layer same as last one\n    model.add(keras.layers.Conv2D(num_filters_3, (3,3), padding = 'valid', kernel_regularizer = 'l2'))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation('relu'))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.MaxPooling2D((2,2), padding = 'valid'))\n    \n    # flatten and hidden layer\n    # Flattens the input. Does not affect the batch size.\n    model.add(keras.layers.Flatten())\n    # add a 'regular' neural network layer\n    #\u00a0kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n    model.add(keras.layers.Dense(hidden_nodes, kernel_regularizer =  'l2'))\n    # apply the normalization\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Activation('relu'))\n    model.add(keras.layers.Dropout(0.2))\n    \n    # output layer\n    model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n    \n    # compile the model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","2bff8cf9":"fig = plt.figure(figsize=(7,5))\n# set the graphic\nplt.hist(y_train)\nplt.xlabel('Class')\nplt.ylabel('Frequency')","bcae93fb":"print(x_test.shape)\nprint(y_test.shape)","c9da9246":"from keras.preprocessing.image import ImageDataGenerator\n\n# instance the image gen with the params\ntrain_gen = ImageDataGenerator(\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1\n)\n\n# fit the image generator with the training images\ntrain_gen.fit(x_train)","fbeadd33":"# instance a model using the previous cnn function\nmodel = keras.wrappers.scikit_learn.KerasClassifier(build_fn=cnn, verbose=1)","ce996709":"# def cnn(num_filters_1, num_filters_2, num_filters_3, hidden_nodes):\n# these are the param ranges to optimize the model\nparam = {'num_filters_1' : [135,170], 'num_filters_2' : [60,100],\n         'num_filters_3' : [35 , 50], 'hidden_nodes'  : [35, 50]}\n\n#\u00a0\"optimize\" the model using RandomizedSearchCV\ngrd = RandomizedSearchCV(\n    estimator = model,\n    param_distributions = param,\n    cv = KFold(n_splits = 3, shuffle = True, random_state = 1),\n    verbose = 1\n)\n# start using the RandomizedSearchCV\nsearch = grd.fit(x_train, y_train, epochs = 10, batch_size = 32, verbose = 0)","aeb4d34c":"print(search.best_params_)","e73f4008":"# declare a model with the previous function and with the best params\nmodel = cnn(search.best_params_['num_filters_1'], search.best_params_['num_filters_2'], search.best_params_['num_filters_3'], search.best_params_['hidden_nodes'])\n# see the model summary\nprint(model.summary())","15644029":"# use that method as a callback for the model\nreduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10)\n# train the model with the image generator, train_gen\n# save the training phases in history \nhistory = model.fit(\n    # train with the image generator\n    train_gen.flow(\n        x_train,\n        y_train,\n        batch_size=32\n    ), epochs=100, validation_data=(x_val, y_val), callbacks=[reduce])","ecbbd7b0":"# plot the accuracy\nfig = plt.figure(figsize = (20,5))\nplt.plot(history.history['accuracy']) # accuracy for the training set\nplt.plot(history.history['val_accuracy']) # sccuracy for the validate set\nplt.legend(['training accuracy', 'validation accuracy'])\nplt.title('Accuracy on training set and validation set')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.show()","10d513ae":"# plot the loss\nfig = plt.figure(figsize = (20,5))\nplt.plot(history.history['loss']) # loss for the training set\nplt.plot(history.history['val_loss']) # loss for the validate set\nplt.legend(['training loss', 'validation loss'])\nplt.title('Loss on training set and validation set')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","9cf21b9a":"# predict with the test set\ny_pred = model.predict(x_test)\ny_pred.shape","e9fc29b1":"# as the dimention are two, we need 1 dim\ny_pred = np.array(y_pred.reshape(-1))","acc884be":"y_pred[:10]","fb8cf904":"# and we need int values to evaluate\ny_pred = np.around(y_pred)","e62ea6f6":"print('Predictions:', y_pred[:5])\nprint('Answers:', y_test[:5])","5b99530a":"print('Accuarcy:', accuracy_score(y_test, y_pred))","4eaa96ef":"print(classification_report(y_test, y_pred))","5a6d965d":"from sklearn.metrics import confusion_matrix\n# declare the matrix\ncm = np.array(confusion_matrix(y_test, y_pred), dtype = int)\n# plot the matrix\nfig = plt.figure(figsize = (10,10))\nheatmap = sns.heatmap(cm, annot=True, square=True)\nplt.ylabel('True Label')\nplt.xlabel('Predictions')\nplt.title('Confussion Matrix')\nplt.show()","df4dc8d4":"model.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","e0c80cb0":"from tensorflow.keras.models import model_from_json\n# save the model as json\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","148d1643":"# Frequency of each class\n> `Normal: 0, Pneumonia: 1`.","aa11b084":"# Plot the train result","e40cca52":"# Confussion matrix","65b19f53":"# Accuracy","1ade8edf":"# The Model","5a6d5e32":"# Read The Dataset - Get the image routes for each dataset\nSet all the image routes in dicts, and each dict will be for each set: **train, validation and test**","2f08603f":"# Processing the Images","bb9bf781":"# Trying parameters","63d5ef65":"## Example about how to process an image\nlets open some images to see how to use the library.","fa4ad936":"## Verify the each set shape","34d0a028":"# Use an image generator\nImage generator as its name says it generate images in base of the given but edited. For example `images with zoom` and some other things. `Is commonly used for training process`.","7d03eb7b":"> Perform **RandomizedSearch** for hyperparameter tuning. Select the best of parameters for training the model.","0c6aa5ad":"# Use the best parameters","8d9ab139":"# Classification report","a8b97688":"# Evaluate the model","ee149379":"> [RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html) RandomizedSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cscore_samples\u201d, \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used.\n\n> [KFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html) Provides train\/test indices to split data in train\/test sets. Split dataset into k consecutive folds (without shuffling by default).\n\nThe previous cell process is going to take many **time depending on the number of epcohs**. Basically what it is doing is `training testing the model with diferent params` for then look for the greatest params to train the model and it has a goot performance. These best params are:","3c93974e":"# Save the model","3ae19a30":"> I have based the model and training process in these [notebook](https:\/\/www.kaggle.com\/shiratorizawa\/pnuemonia-detection-using-cnn)","6722ed20":"# Conclusion\nThe results are good but we have `11 true negatives that means 11 people with pneumonia that were classified healty`, that's quite dangerous. And there are `54 healty people that were diagnosticated with pneumonia`, but that's not too serious as those pople are healty and eventually they will be diagnosticated to be healty. The results are very good but could be better.","4b8c3e9e":"> [ReduceLROnPlateau()](https:\/\/keras.io\/api\/callbacks\/reduce_lr_on_plateau\/) is used to reduce learning rate when a metric has stopped improving. It seems to be working as the val accuracy start changeing since the epoch 10","7c9a6720":"## Set the images in datasets\nSet the images in datasets for use a **cnn model**.\n> First open all images from **each dict** and set the categories: `Normal: 0, Pneumonia: 1`.\n\n> Convert the sets to **np arrays** and **float type** and then **normalize the data**, it means to do X\/255. 255 as the images are in a grey color, so the images are a **squared matrix** and `each value is between 0 and 255, black and white`.\n\n> Then do the same process for each dic.\n\nYes, it will take a moment"}}