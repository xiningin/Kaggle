{"cell_type":{"d6c5ac21":"code","9fdc54a4":"code","5e81741a":"code","dfdd27c6":"code","a2e99d94":"code","97561b14":"code","edb1c76c":"code","9ca1c4aa":"code","acfcb0bf":"code","a1ef8482":"code","49597bae":"markdown","fe8cc852":"markdown","c4049d8a":"markdown","8fe36e51":"markdown","8f78aac5":"markdown","2615c136":"markdown","10793e26":"markdown","b25d0cef":"markdown","c2961a81":"markdown","ce2f4de9":"markdown"},"source":{"d6c5ac21":"import matplotlib.pyplot as plt\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport os\nimport sys\nimport torch\nimport torchvision\nsys.path.append('..\/input\/caltech256')","9fdc54a4":"from caltech256 import Caltech256\nfrom torchvision.transforms import transforms\n\nroot = '..\/input\/caltech256\/256_objectcategories\/256_ObjectCategories\/'\ncsv_path = '..\/input\/caltech256\/caltech256.csv'\nbatch_size = 32\n\ncal_transform = transforms.Compose([\n    transforms.Resize((299,299)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\ntrainset = Caltech256(root=root, transform=cal_transform, splits=[1,2,3,4], csv_path=csv_path)\ntestset = Caltech256(root=root, transform=cal_transform, splits=[5], csv_path=csv_path)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=0)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=0)","5e81741a":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\nclasses = trainset.classNames\n\n# show images\ndef imshow(img):\n    img = img \/ 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join('%5s' % classes[labels[j]] for j in range(8)))","dfdd27c6":"cache_dir = os.path.expanduser(os.path.join('~', '.torch'))\nmodels_dir = os.path.join(cache_dir, 'models')\nos.makedirs(models_dir, exist_ok=True)\n!cp ..\/input\/pretrained-pytorch-models\/* ~\/.torch\/models\/","a2e99d94":"from torchvision import models\nfrom torch import nn\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pretrained Inceptionv3\nmodel = models.inception_v3(pretrained=True)\n\n# Freeze some layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace last linear layers\nnum_ftrs = model.AuxLogits.fc.in_features\nmodel.AuxLogits.fc = nn.Linear(num_ftrs, trainset.nclasses)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, trainset.nclasses)\nmodel = model.to(device)\n\n# Training criterion\nweights = trainset.weights.to(device)\ncriterion = nn.CrossEntropyLoss(weight=weights)","97561b14":"# Observe that all parameters are being optimized\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\n# Let the gradient flow\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Loop over the dataset multiple times\nfor epoch in range(1):\n    # train set\n    running_loss = 0.0\n    running_corrects = 0.0\n    model.train()\n    for i, (inputs, labels) in enumerate(trainloader, 0):\n        # get the inputs\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs, aux_outputs = model(inputs)\n        loss1 = criterion(outputs, labels)\n        loss2 = criterion(aux_outputs, labels)\n        loss = loss1 + 0.4*loss2\n        loss.backward()\n        optimizer.step()\n        _, preds = torch.max(outputs, 1)\n\n        # print statistics\n        running_loss += loss.item()\n        running_corrects += torch.sum(preds == labels.data)\n        if i % 100 == 0:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f acc: %.3f' %\n                  (epoch + 1, i, running_loss \/ 100, \n                   running_corrects.double() \/ 100 \/ 32 * 100))\n            running_loss = .0\n            running_corrects = .0\n\n    # test set\n    running_corrects = .0\n    model.eval()\n    for i, (inputs, labels) in enumerate(testloader, 0):\n        # get the inputs\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = model(inputs)\n\n        _, preds = torch.max(outputs, 1)\n        running_corrects += torch.sum(preds == labels.data)\n    epoch_acc = running_corrects.double() \/ len(testset) * 100  \n    print('val acc:' + str(epoch_acc))\n","edb1c76c":"import torch.nn.functional as F\n\ndef mModel(x):\n    if model.transform_input:\n        x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 \/ 0.5) + (0.485 - 0.5) \/ 0.5\n        x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 \/ 0.5) + (0.456 - 0.5) \/ 0.5\n        x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 \/ 0.5) + (0.406 - 0.5) \/ 0.5\n        x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n    # 299 x 299 x 3\n    x = model.Conv2d_1a_3x3(x)\n    # 149 x 149 x 32\n    x = model.Conv2d_2a_3x3(x)\n    # 147 x 147 x 32\n    x = model.Conv2d_2b_3x3(x)\n    x = F.max_pool2d(x, kernel_size=3, stride=2)\n    # 73 x 73 x 64\n    x = model.Conv2d_3b_1x1(x)\n    x = model.Conv2d_4a_3x3(x)\n    # 71 x 71 x 192\n    x = F.max_pool2d(x, kernel_size=3, stride=2)\n    # 35 x 35 x 192\n    x = model.Mixed_5b(x)\n    x = model.Mixed_5c(x)\n    x = model.Mixed_5d(x)\n    x = model.Mixed_6a(x)\n    # 17 x 17 x 768\n    x = model.Mixed_6b(x)\n    x = model.Mixed_6c(x)\n    x = model.Mixed_6d(x)\n    x = model.Mixed_6e(x)\n    if model.training and model.aux_logits:\n        aux = model.AuxLogits(x)\n    x = model.Mixed_7a(x)\n    # 8 x 8 x 1280\n    x = model.Mixed_7b(x)\n    x = cam = model.Mixed_7c(x)\n    x = F.avg_pool2d(x, kernel_size=8)\n    # 1 x 1 x 2048\n    x = F.dropout(x, training=model.training)\n    x = x.view(x.size(0), -1)\n    x = model.fc(x)\n    # 1000 (num_classes)\n    if model.training and model.aux_logits:\n        return x, aux, cam\n    return x, cam\n\n    \ndef get_preds_and_cams(imgs):\n    # check input\n    if len(imgs.shape) == 3:\n        imgs = imgs.reshape(1,3,299,299)\n    \n    # get maps\n    model.eval()\n    preds, maps = mModel(imgs.to(device))\n    \n    # get cams\n    W = model.fc.weight.cpu().detach().numpy()\n    maps = maps.cpu().detach().numpy()\n    maps = np.transpose(maps, (0,2,3,1))\n    cams = np.dot(maps, W.T)\n    \n    return preds, cams\n\ndef show_cams(pyImg, cams, preds):\n    # show main image\n    img = pyImg.detach().numpy()\n    img = np.transpose(img, (1,2,0))\n    img = img \/ 2 + 0.5\n    plt.imshow(img)\n    \n    # show cams\n    pred = preds.cpu().detach().numpy().argmax()\n    cam = cams[:, :, pred]\n    cam = resize(cam, (299,299), preserve_range=True)\n    plt.imshow(cam, alpha=0.7)\n    plt.show()\n\n    ","9ca1c4aa":"from skimage.transform import resize\n\nfor i in range(50):\n    pyImg, y = testset[i]\n    \n    preds, cams = get_preds_and_cams(pyImg)\n    show_cams(pyImg, cams[0], preds[0])\n\n","acfcb0bf":"torch.save(model.state_dict(), 'model')","a1ef8482":"model2 = models.Inception3()\n\n# Replace last linear layers\nnum_ftrs = model2.AuxLogits.fc.in_features\nmodel2.AuxLogits.fc = nn.Linear(num_ftrs, trainset.nclasses)\nnum_ftrs = model2.fc.in_features\nmodel2.fc = nn.Linear(num_ftrs, trainset.nclasses)\nmodel2 = model.to(device)\n\n# load weights\nmodel2.load_state_dict(torch.load('model'))\nmodel2.eval()\n","49597bae":"# 2. Prepare the training\n## Trick to load pretrained weights","fe8cc852":"## Define the training procedure","c4049d8a":"## b) Get train test splits","8fe36e51":"## Load model in the memory","8f78aac5":"## Load","2615c136":"# 1. Prepare the data\n## a) Import some libraries","10793e26":"# 4. Visualize\n## Visualization function","b25d0cef":"# Save and load model\n## Save","c2961a81":"## c) Print some images","ce2f4de9":"# 3. Train"}}