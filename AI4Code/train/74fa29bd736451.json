{"cell_type":{"84b19cb1":"code","72aec605":"code","71d34ca6":"code","fc5ae776":"code","ea13d731":"code","9ec7f986":"code","d7f468ac":"code","80ab0bfe":"code","4e013ec7":"code","0786ce49":"code","b1ce5a98":"code","e234e908":"code","94c717df":"code","7543fc4b":"code","6cabcee4":"code","ea4ac7fd":"code","54a9d4c2":"code","9e162294":"code","ef1fc09e":"code","ee12143f":"code","35392cc2":"code","9c037f79":"code","fff8e766":"code","7f105dc3":"code","9ace6662":"code","11759aab":"code","78dd3043":"code","49a1b151":"code","407c663a":"code","efb6de20":"code","6f109839":"code","d7a0db14":"code","d0159a2a":"code","c8935628":"code","370e1b08":"code","91869dfa":"code","2496d498":"code","98f7ad47":"code","e437486e":"code","adf12108":"code","8f8c0ee6":"markdown","d4b669bd":"markdown","e5364cad":"markdown","bc9b00c7":"markdown","c5275f8a":"markdown","52e05da4":"markdown","da5993c9":"markdown","0d34423a":"markdown","2245443e":"markdown","0ec36ff1":"markdown","9a638e11":"markdown","40a0d2fa":"markdown","ccacb45a":"markdown","9cf5be30":"markdown","f635f5f9":"markdown","8e426215":"markdown","f698f543":"markdown","2ec3b930":"markdown","ed13c2e5":"markdown","627e02f8":"markdown","8780aebc":"markdown"},"source":{"84b19cb1":"#!pip install --upgrade pip\n!pip install twint\n#!pip install optimuspyspark\n#!pip install aiohttp\n#!pip install aiodns\n#!pip install beautifulsoup4\n#!pip install cchardet\n#!pip install elasticsearch\n#!pip install pysocks\n#!pip install pandas>=0.23.0\n#!pip install aiohttp_socks\n#!pip install schedule\n#!pip install geopy\n!pip install nest_asyncio\n!pip install TextBlob\n#!pip install wordcloud\n!pip install vaderSentiment","72aec605":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport datetime\n\nimport twint\n\n# Set up TWINT config\nc = twint.Config()\nfrom collections import Counter\n\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nimport nest_asyncio\nnest_asyncio.apply()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71d34ca6":"users = [\n    'liberoliber',\n    'volvogroup',\n    'volvogroupse',\n    'volvogroupbe',\n    'volvogroupfr',\n    'volvogroupna',\n    'volvogrouppl',\n    'volvogroupeu',\n    'volvotrucks',\n    'jobsatvolvo',\n]","fc5ae776":"def get_followings(username):\n\n    c = twint.Config()\n    c.Username = username\n    c.Pandas = True\n    c.Store_object = True\n    c.Hide_output = False\n\n    twint.run.Following(c)\n    list_of_followings = twint.storage.panda.Follow_df\n\n    return list_of_followings['following'][username]","ea13d731":"followings = {}\nfollowing_list = []\nfor person in users:\n    #print('#####\\nStarting: ' + person + '\\n#####') # - I decided to disable this printing since the list can be very long.\n    try:\n        followings[person] = get_followings(person)\n        following_list = following_list + followings[person]\n    except KeyError:\n        print('IndexError while loading '+ person +\"  twitter account.\")","9ec7f986":"Counter(following_list).most_common(10)","d7f468ac":"follow_relations ={}\nfor following_user in followings.keys():\n    follow_relation_list = []\n    for followed_user in followings.keys():\n        if followed_user in followings[following_user]:\n            follow_relation_list.append(True)\n        else:\n            follow_relation_list.append(False)\n    follow_relations[following_user] = follow_relation_list","80ab0bfe":"following_df = pd.DataFrame.from_dict(follow_relations, orient='index',  columns=followings.keys())\nfollowing_df","4e013ec7":"def get_mention_count(user, mention_word):\n\n    c = twint.Config()\n    c.Username = user\n    c.Search = '@' + mention_word\n    c.Store_object = True\n\n    twint.run.Search(c)\n    tweets = twint.output.tweets_list\n    mention_count = len(tweets)\n    tweets.clear()\n\n    return mention_count","0786ce49":"mention_relations = {}\nfor mentioning_user in users:\n    #print('#####\\nStarting: ' + mentioning_user + '\\n#####') # - I decided to disable this printing since the list can be very long.\n    mention_count_list = []\n    for mentioned_user in users:\n        mention_count = get_mention_count(mentioning_user, mentioned_user)\n        mention_count_list.append(mention_count)\n    mention_relations[mentioning_user] = mention_count_list","b1ce5a98":"mention_df = pd.DataFrame.from_dict(mention_relations, orient='index', columns=mention_relations.keys())","e234e908":"mention_df","94c717df":"import datetime","7543fc4b":"count=0\nbase = \"2020-05-28 00:00:00\"\nbase1 = datetime.datetime.strptime(base,'%Y-%m-%d %H:%M:%S')\ndate_list = [base1 - datetime.timedelta(days=x) for x in range(360)]","6cabcee4":"# Set up TWINT config\nc = twint.Config()","ea4ac7fd":"# Configure the keywords list\nlist_keywords = [\"VolvoGroup\", \"WeareVolvoGroup\"]","54a9d4c2":"list_keywords","9e162294":"for i in list_keywords:\n    print(i)\n    c = twint.Config()\n    c.Search = i\n    c.Pandas = True\n    c.Since = \"2020-01-01\"\n    c.Until = \"2020-05-29\"\n    #c.Location = True\n    c.Limit = 99999\n    #c.Near = \"wroclaw \"\n    c.Custom_csv = [\"id\", \"user_id\", \"username\", \"date\", \"tweet\"]\n    c.Output = f\"hh_{count}.csv\"\n    twint.run.Search(c)","ef1fc09e":"c.Custom_csv = [\"id\", \"user_id\", \"username\", \"date\", \"tweet\"]\nc.Output = f\"hh_{count}.csv\"","ee12143f":"\n\n\n#c.Search = \"Volvo Group\"\n# Custom output format\n#c.Format = \"Username: {username} |  Tweet: {tweet}\"\n#c.Limit = 1\n#c.Pandas = True\n#twint.run.Search(c)","35392cc2":"def available_columns():\n    return twint.output.panda.Tweets_df.columns","9c037f79":"def twint_to_pandas(columns):\n    return twint.output.panda.Tweets_df[columns]","fff8e766":"available_columns()","7f105dc3":"tweets = twint_to_pandas([\"date\", \"username\", \"tweet\", \"hashtags\", \"nlikes\"])\n\ntweets","9ace6662":"tweets.count()","11759aab":"tweets=tweets.sort_values(by=\"date\")\n\n","78dd3043":"tweets","49a1b151":"pip install vaderSentiment","407c663a":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyzer= SentimentIntensityAnalyzer()\nsentence = \"This is a shit VADER Example\"\nanalyzer.polarity_scores(sentence)\n#{'neg': 0.0, 'neu': 0.494, 'pos': 0.506, 'compound': 0.6249}","efb6de20":"from textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer","6f109839":"df =tweets\ndf.head()","d7a0db14":"#load the tweets into textblob\ndesc_blob = [TextBlob(desc) for desc in df['tweet']]\n#add the sentiment metrics to the dataframe\ndf['tb_Pol'] = [b.sentiment.polarity for b in desc_blob]\ndf['tb_Subj'] = [b.sentiment.subjectivity for b in desc_blob]\n#show dataframe\ndf.head(3)","d0159a2a":"#load VADER\nanalyzer = SentimentIntensityAnalyzer()\n#Add VADER metrics to dataframe\ndf['compound'] = [analyzer.polarity_scores(v)['compound'] for v in df['tweet']]\ndf['negative'] = [analyzer.polarity_scores(v)['neg'] for v in df['tweet']]\ndf['neutral'] = [analyzer.polarity_scores(v)['neu'] for v in df['tweet']]\ndf['positive'] = [analyzer.polarity_scores(v)['pos'] for v in df['tweet']]\ndf","c8935628":"pip install googletrans","370e1b08":"from googletrans import Translator\ntranslator = Translator()","91869dfa":"df","2496d498":"translator.translate(df['tweet'][2]).text","98f7ad47":"text = translator.translate(df['tweet'][2]).text\nanalyzer.polarity_scores(text)","e437486e":"def sentiment_analyzer_scores(text, engl=True):\n    if engl:\n        trans = text\n    else:\n        trans = translator.translate(text).text\n    score = analyzer.polarity_scores(trans)\n    lb = score['compound']\n    if lb >= 0.05:\n        return 1\n    elif (lb > -0.05) and (lb < 0.05):\n        return 0\n    else:\n        return -1","adf12108":"text = df['tweet'][2]\nsentiment_analyzer_scores(text, False)","8f8c0ee6":"# Analyze sentiment of tweets","d4b669bd":"# Conclusion\nThis was a very simple and nice example of some basic social media analyses on the Volvo Group Twitter accounts just for fun.\n","e5364cad":"# Building the dataframes\nNoe is time to save the result dictionary into a pandas dataframe for a more user-friendly visualization. \nThe rows of the dataframe show the users who are following, whereas the columns indicate the users who are followed.","bc9b00c7":"Initializing the Python Code with twint and nest_asyncio\nWe need twint library for scraping data, pandas for creating dataframes, and collections to get the grouped value counts in a list.\n\nThe twint library use an event loop, and this can cause a problem with Jupyter\/Kaggle kerneld that also runs an event loop, and these loops can\u2019t be nested.\n\nThis thread (https:\/\/github.com\/jupyter\/notebook\/issues\/3397#issuecomment-376803076) explains that we have two event loops running - one in Jupyter and one in the twint library. Luckily Ewald de Wit created an awesome library called 'nest_asyncio' that we can use to workaround this error. So i will use it here too!","c5275f8a":"Defining the data format and base","52e05da4":"# TextBlob Sentiment Analysis\nUsing a few simple list comprehensions, it is easy to load the description column as a TextBlob, and then create two new columns to store the Polarity and Subjectivity.","da5993c9":"****Searching for Volvo Group tweets","0d34423a":"# Mention Counts Analysis\nMention counts are another strong relationship indicator between Twitter accounts. \n\nThe function **get_mention_count()** returns the mention counts between two accounts in one direction. \nThe mentioned username is used as input to the mention_word and in the function, an \u2018@\u2019 character is added to the beginning of it in order to separate mentions more precisely.","2245443e":"# Relationship Analysis: who follows who?\nAfter getting all the following lists, we can calculate the most common values in the **following_list** variable to get the most popular accounts among the selected Volvo Group twitter accounts. \nTo get the most followed 10 accounts, I am using Counter function from collections library.","0ec36ff1":"# Analyzing Twitter Relationships between my account and Volvo Group accounts with Python\n\nSocial media analysis is one of the hot topics for me, since I am very familiar with this environment. I've been very active on Twitter, Instagram, Facebook and mostly on [LinkedIn](https:\/\/www.kaggle.com\/liberoliber\/my-linkedin-network-analysis).\n\nIn this article, I will give an overview on how to scrape twitter data with the help of twint (A python library that enables you to scrap twitter data without API access.) and analyze some relationships based on followings and mentionings among my account and a list of corporate accounts of the Volvo Group (My company, by the way).","9a638e11":"# Using Vader in other languages\nThe Vader is really a great tool but unfortunately it is all build over the English language (Vader does not work directelly with other languages).\nBut if you live or work on countries that speak other languages, you can easily create a \u201cturnaround\u201d and translate your text from its original language to English before applying Vader.\nFor that, we will use Googletrans, a free and unlimited python library that implemented Google Translate API (for details, please refer to the API Documentation).","40a0d2fa":"# Following Relations among Volvo Group Accounts\nNow I want to see who is following who in the Volvo Group. \n\nTo found out it, I will use a for loop that checks if any of the twitter accounts in the list is in the following list of another account. \nAs a result, it creates a dictionary of lists showing the following statuses represented by Trues and Falses.","ccacb45a":"Finally, I will convert the **mention_relationships** to a pandas dataframe and it becomes an understandable and more easily interpretable table.","9cf5be30":"Sentiment analysis on Volvo Group Tweets","f635f5f9":"# Building the following list!\nUsing the **get_followings()** function, we can get different following lists for every person in our users list and store the results to a dictionary (**followings**) and a list (**following_list**). \n\nThe **following_list** is a joined version of all followings and we will use it in order to calculate the most followed Twitter accounts in the next section.\n\nThe for loop below creates these two variables. \n\n**Basic Error Handling**: Sometimes Twitter does not respond to our request and in this case, we get an Index Error. For such cases, I added an exception to the code to skip these users.","8e426215":"Then I will start with creating a user list that consists of Volvo Group twitter accounts. My analysis will include the relationships of these accounts and mine (@liberoliber). \n\n\nmy twitter account: @Liberoliber\n\nVolvo Group Twitter Accounts:\n\n@volvogroup\n \n@volvogroupSE - Volvo group Sweden\n\n@volvogroupBE - Volvo group Belgium\n\n@volvogroupFR - Volvo group France\n\n@volvogroupNA - Volvo group North America\n\n@volvogroupPL - Volvo group Poland - where I am living.\n \n@volvogroupeu\n \n@volvotrucks\n\n@jobsatvolvo\n\n","f698f543":"The Python libraries TextBlob and VADER Sentiment Analysis make it super easy to generate simple sentiment metrics without training a model. They offer out of the box solutions and are easy to interpret. In a few lines of code, anyone can generate metrics and look for outliers and trends in the sentiment of their text data.","2ec3b930":"# Saving results into Pandas","ed13c2e5":"In the analysis, we use two nested **for** loops to retrieve mention counts of every user to all others in our group. As a result, we will get the **mention_relationships** dictionary.","627e02f8":"**VADER Sentiment Analysis**\nCall the Sentiment Intensity Analyzer. Then use list comprehensions to create a new column in the dataframe for each polarity_scores metric. The dataframe is a little over 100k rows; this might take a few minutes to complete.","8780aebc":"# Following Relationships Analysis\nI will start using the function **get_followings()** that sends a request to twint library with a username. This function will return a list of accounts each entry on our account list follows."}}