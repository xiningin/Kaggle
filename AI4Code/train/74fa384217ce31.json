{"cell_type":{"f98ab208":"code","65ff66ae":"code","82041d43":"code","8e39b48e":"code","076068ec":"code","2dd0c9e0":"code","94567cd5":"code","56837a05":"code","1c6ea1ca":"code","901c29a6":"code","2f6666bd":"code","884886fc":"code","b8559b1c":"code","021dc663":"code","51f48d1e":"code","f4eec70f":"code","e529f42d":"code","5d969ceb":"code","f2452050":"code","321c0a72":"code","ee8f075a":"code","65490fbf":"code","62947ce5":"code","8e4e634a":"code","c29a51aa":"code","831d45a9":"code","e0742a99":"code","15d1ce36":"code","efdb5fd9":"code","f5037fc7":"code","7ff5251f":"code","ed2c4b4c":"code","43eec687":"code","f9a33cbc":"code","918cd4db":"code","71b62371":"code","4935aac2":"code","c8661dd0":"code","792e9a40":"code","aafc084a":"code","9ff17811":"code","3ae14913":"code","508c0c5a":"markdown","5d47ba38":"markdown","fd63fb17":"markdown","7e69e92d":"markdown","70d4a08e":"markdown","3b218ed8":"markdown","01758847":"markdown","32037fb5":"markdown","c82a64db":"markdown","1daa3260":"markdown","0322c8f3":"markdown"},"source":{"f98ab208":"import sys\nsys.path = [\n    '..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master',\n] + sys.path","65ff66ae":"# !pip install torchsummary","82041d43":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *\nfrom fastai.callbacks import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score,confusion_matrix\nimport matplotlib.image as image\nfrom tqdm.notebook import tqdm\nimport os\nimport gc\nimport zipfile\nimport openslide\nimport cv2\nfrom PIL import Image\nimport skimage.io as sk\nimport warnings\n# from torchsummary import summary\nfrom sys import getsizeof\nwarnings.filterwarnings(\"ignore\")","8e39b48e":"device = torch.device('cuda')","076068ec":"tile_size = 256\nimage_size = 256\nn_tiles = 36\nbatch_size = 8\nnum_workers = 4\nTRAIN = '\/kaggle\/input\/prostate-cancer-grade-assessment\/train_images\/'","2dd0c9e0":"path1 = Path('\/kaggle\/input\/panda-36-tiles')","94567cd5":"sld = os.listdir(TRAIN)\nsld = [x[:-5] for x in sld]","56837a05":"df_duplicates = pd.read_csv('..\/input\/duplicates-panda\/duplicates.csv')\n# df_duplicates.head()","1c6ea1ca":"duplicate_files = df_duplicates['file2'].tolist()","901c29a6":"df = pd.read_csv('\/kaggle\/input\/prostate-cancer-grade-assessment\/train.csv')\ndf = df[df['image_id'].isin(sld)]\ndf = df[~df['image_id'].isin(duplicate_files)]\ndf.columns = ['fn', 'data_provider', 'isup_grade', 'gleason_score']","2f6666bd":"df['kfold'] = -1\ndf = df.sample(frac=1.,random_state=2020).reset_index(drop=True)\nkf = StratifiedKFold(n_splits=5)\ny = df.isup_grade.values\nfor f,(t_,v_) in enumerate(kf.split(X=df,y=y)):\n    df.loc[v_,'kfold'] = f","884886fc":"# df.head()","b8559b1c":"import seaborn as sns\nsns.countplot(x=df[df.kfold==1].isup_grade);\nplt.title('Fold - 1: Images count');","021dc663":"def get_tiles(img, mode=0):\n        result = []\n        h, w, c = img.shape\n        pad_h = (tile_size - h % tile_size) % tile_size + ((tile_size * mode) \/\/ 2)\n        pad_w = (tile_size - w % tile_size) % tile_size + ((tile_size * mode) \/\/ 2)\n\n        img2 = np.pad(img,[[pad_h \/\/ 2, pad_h - pad_h \/\/ 2], [pad_w \/\/ 2,pad_w - pad_w\/\/2], [0,0]], constant_values=255)\n        img3 = img2.reshape(\n            img2.shape[0] \/\/ tile_size,\n            tile_size,\n            img2.shape[1] \/\/ tile_size,\n            tile_size,\n            3\n        )\n\n        img3 = img3.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n        n_tiles_with_info = (img3.reshape(img3.shape[0],-1).sum(1) < tile_size ** 2 * 3 * 255).sum()\n        if len(img) < n_tiles:\n            img3 = np.pad(img3,[[0,N-len(img3)],[0,0],[0,0],[0,0]], constant_values=255)\n        idxs = np.argsort(img3.reshape(img3.shape[0],-1).sum(-1))[:n_tiles]\n        img3 = img3[idxs]\n        for i in range(len(img3)):\n            result.append({'img':img3[i], 'idx':i})\n        return result","51f48d1e":"class TiffImageItemList(ImageList):\n    def open(self,fn):\n        path = '\/kaggle\/input\/prostate-cancer-grade-assessment\/train_images\/'\n        fl = path + str(fn)+'.tiff'\n        img = sk.MultiImage(fl)[1]\n        res = get_tiles(img)\n        imgs = []\n        for i in range(36):\n            im = res[i%len(res)]['img']\n            imgs.append(im)\n        imgs = np.array(imgs)\n        final_image = np.concatenate(np.array([np.concatenate(imgs[j:j+6],axis=1).astype(np.uint8) for j in range(0,36,6)]),axis=0)\n        final_image = cv2.resize(final_image, dsize=(512, 512), interpolation=cv2.INTER_CUBIC)\n        \n        return vision.Image(pil2tensor(final_image,np.float32).div_(255))","f4eec70f":"trn_idx,val_idx = list(df[df.kfold!=4].index),list(df[df.kfold==4].index)\nrandom.shuffle(trn_idx)\nrandom.shuffle(val_idx)","e529f42d":"data = (TiffImageItemList.from_df(df,path='',cols='fn')\n                          .split_by_idxs(trn_idx,val_idx)\n                          .label_from_df(cols='isup_grade')\n                          .transform(get_transforms())\n                          .databunch(num_workers=4,bs=batch_size)\n                          .normalize(imagenet_stats))\n","5d969ceb":"#data.show_batch(rows=3,figsize=(20,8))","f2452050":"# stats = ([0.785946], [0.45007266])\n# data_img = (ImageList.from_df(df,path1,folder='.',suffix='.png',cols='fn')\n#                 .split_by_idxs(trn_idx,val_idx)\n#                 .label_from_df(cols='isup_grade',)\n#                 .transform(get_transforms(do_flip=True), size=300)\n#                 .databunch(bs=batch_size).normalize(imagenet_stats))","321c0a72":"# data_img.show_batch(rows=3,figsize=(20,8),seed=2020)","ee8f075a":"data_img = data","65490fbf":"len(data_img.train_ds), len(data_img.valid_ds), data_img.classes, data_img.train_ds[0][0].data.shape,data_img.c","62947ce5":"from efficientnet_pytorch import model as enet","8e4e634a":"pretrained_model = {\n    'efficientnet-b3': '..\/input\/efficientnet-pytorch\/efficientnet-b3-c8376fa2.pth'\n}\n\nenet_type = 'efficientnet-b3'\nout_dim = 6","c29a51aa":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n\n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        x = self.extract(x)\n        x = self.myfc(x)\n        return x","831d45a9":"arch = enetv2(enet_type, out_dim=out_dim)","e0742a99":"kp = KappaScore()\nkp.weights = 'quadratic'","15d1ce36":"learn = Learner(data_img, arch , metrics = [kp] , model_dir = '\/kaggle\/working\/')","efdb5fd9":"learn.lr_find()\nlearn.recorder.plot()","f5037fc7":"gc.collect()","7ff5251f":"cb2 = SaveModelCallback(learn, monitor = 'kappa_score', every = 'improvement', mode='max', name = 'best_model_ft' )\ncb3 = ReduceLROnPlateauCallback(learn,  monitor = 'kappa_score', mode = 'max',factor = 0.2,patience = 4, min_delta = 0.01)","ed2c4b4c":"learn.split([arch.myfc])","43eec687":"epochs = 4\nlearn.fit_one_cycle(epochs ,max_lr = 1e-3, callbacks = [cb2,cb3])","f9a33cbc":"learn.unfreeze()\nlearn.fit_one_cycle(4 ,max_lr = 1e-3, callbacks = [cb2,cb3])","918cd4db":"learn.recorder.plot_losses()","71b62371":"learn.load('best_model_ft');","4935aac2":"learn.export('\/kaggle\/working\/panda.pkl')","c8661dd0":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()","792e9a40":"# learn = learn.to_fp32()","aafc084a":"# test_df = pd.read_csv('\/kaggle\/input\/prostate-cancer-grade-assessment\/test.csv')\n# df.drop('kfold', axis=1, inplace=True)\n# df.columns = ['image_id', 'data_provider', 'isup_grade', 'gleason_score']\n# data_dir = '..\/input\/prostate-cancer-grade-assessment'\n# image_folder = os.path.join(data_dir, 'test_images')\n# is_test = os.path.exists(image_folder)  # IF test_images is not exists, we will use some train images.\n# image_folder = image_folder if is_test else os.path.join(data_dir, 'train_images')\n\n# test = test_df if is_test else df.sample(n=100)","9ff17811":"# def image_test(fn,image_folder):     \n#     path2 = image_folder +'\/'\n#     fl = path2 + str(fn)+'.tiff'\n#     img = sk.MultiImage(fl)[1]\n#     res = get_tiles(img)\n#     imgs = []\n#     for i in range(36):\n#         im = res[i%len(res)]['img']\n#         imgs.append(im)\n#     imgs = np.array(imgs)\n#     final_image = np.concatenate(np.array([np.concatenate(imgs[j:j+6],axis=1).astype(np.uint8) for j in range(0,36,6)]),axis=0)\n#     final_image = cv2.resize(final_image, dsize=(300, 300), interpolation=cv2.INTER_CUBIC)\n#     return vision.Image(pil2tensor(final_image,np.float32).div_(255))","3ae14913":"# ts_name = test.image_id.values\n# pred = np.zeros(len(ts_name))\n    \n# for j in tqdm(range(len(ts_name))):\n#     ans = int(learn.predict(image_test(ts_name[j],image_folder))[0])\n#     pred[j] = ans\n        \n# out = pd.DataFrame({'image_id':ts_name,'isup_grade':pred.astype(int)})\n# out.to_csv('submission.csv',index=False)","508c0c5a":"## Databunch of Processed Images: Using fastai's own ImageList","5d47ba38":"## Inference Kernel can be found [**here**](https:\/\/www.kaggle.com\/ianmoone0617\/panda-effnet-b3-inference-fastai-custom-imagelist)","fd63fb17":"## Model Efficient-B3","7e69e92d":"* I have converted the tiff files they can be found [**here**](https:\/\/www.kaggle.com\/ianmoone0617\/panda-36-tiles-resize)\n* Lets start with Custum ImageItem List first","70d4a08e":"* Train and validation split","3b218ed8":"# Prostate cANcer graDe Assessment (PANDA) Challenge\n### Prostate cancer diagnosis using the Gleason grading system","01758847":"# Data Processing for fastai \n* We have 2 options either we write a custom Imagelist function or\n* We first convert all images first then use then As we like.\n\nLater will take time at first but will Speed up process later. As Fastai datablock will not have to process large **.tiff** files every time","32037fb5":"## Custom Fastai TiffImageList to Directly Process Slides","c82a64db":"### Metrics Kappa Score","1daa3260":"## DataBunch of Custom TiffImageItemList ","0322c8f3":"## Stratified Kfold"}}