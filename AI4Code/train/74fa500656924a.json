{"cell_type":{"a88f7e24":"code","2ab30f40":"code","2fe37ce2":"code","8b2eeb24":"code","ae8a27dc":"code","62e1ae94":"code","402bd42a":"code","7b9fb8bb":"code","12a64c9c":"code","bf7d6d63":"code","cf270a79":"code","bf390e35":"code","8b695ceb":"code","b28a03ab":"code","4fa9c24e":"code","abe0580b":"code","379b20fd":"code","d0c4a53f":"code","d72e39ec":"code","f3550fcd":"code","f47ca7df":"code","ae75e52a":"code","08bcf3e7":"code","e8599636":"code","80326c9f":"code","55609e88":"code","840df13d":"code","af622ea6":"code","e69ed9c6":"code","1aea7a7e":"code","9c515346":"code","0f98cb7a":"code","782e73e2":"code","4288fee3":"code","0b600c74":"code","4a159616":"code","e4574869":"code","eb6d2709":"code","96b78f3d":"code","cd69907f":"code","1ab88313":"code","0120ec8a":"code","fe5657d5":"code","17e8b292":"code","15f6c786":"code","ede6487e":"markdown","885bf21a":"markdown"},"source":{"a88f7e24":"!pip install emoji --quiet","2ab30f40":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.sparse import hstack\nfrom scipy import sparse\nfrom numpy import asarray\nfrom numpy import savetxt\n\n\nimport emoji\nimport os\nimport re\nimport itertools\nimport math\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport tensorflow_hub as hub\n\n\n","2fe37ce2":"# LOAD DATASETS\ndf_train = pd.read_csv('\/kaggle\/input\/Quora Question Pairs - Train.csv', index_col='id')\ndf_test = pd.read_csv('\/kaggle\/input\/Quora Question Pairs - Test.csv',  index_col='id')","8b2eeb24":"df_train.shape","ae8a27dc":"# A list of CONTRACTION_EN from http:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\nCONTRACTION_EN = {\n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'll\": \"i will\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'll\": \"it will\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"needn't\": \"need not\",\n    \"oughtn't\": \"ought not\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"she'd\": \"she would\",\n    \"she'll\": \"she will\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"that'd\": \"that would\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'll\": \"they will\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'll\": \"we will\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"who'll\": \"who will\",\n    \"who's\": \"who is\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"you'd\": \"you would\",\n    \"you'll\": \"you will\",\n    \"you're\": \"you are\",\n    \"thx\": \"thanks\",\n    \"lool\": \"lol\"\n}\n\n# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nABBREVATION_EN = {\n    \"$\": \" dollar \",\n    \"\u20ac\": \" euro \",\n    \"4ao\": \"for adults only\",\n    \"a.m\": \"before midday\",\n    \"a3\": \"anytime anywhere anyplace\",\n    \"aamof\": \"as a matter of fact\",\n    \"acct\": \"account\",\n    \"adih\": \"another day in hell\",\n    \"afaic\": \"as far as i am concerned\",\n    \"afaict\": \"as far as i can tell\",\n    \"afaik\": \"as far as i know\",\n    \"afair\": \"as far as i remember\",\n    \"afk\": \"away from keyboard\",\n    \"app\": \"application\",\n    \"approx\": \"approximately\",\n    \"apps\": \"applications\",\n    \"asap\": \"as soon as possible\",\n    \"asl\": \"age, sex, location\",\n    \"atk\": \"at the keyboard\",\n    \"ave.\": \"avenue\",\n    \"aymm\": \"are you my mother\",\n    \"ayor\": \"at your own risk\",\n    \"b&b\": \"bed and breakfast\",\n    \"b+b\": \"bed and breakfast\",\n    \"b.c\": \"before christ\",\n    \"b2b\": \"business to business\",\n    \"b2c\": \"business to customer\",\n    \"b4\": \"before\",\n    \"b4n\": \"bye for now\",\n    \"b@u\": \"back at you\",\n    \"bae\": \"before anyone else\",\n    \"bak\": \"back at keyboard\",\n    \"bbbg\": \"bye bye be good\",\n    \"bbc\": \"british broadcasting corporation\",\n    \"bbias\": \"be back in a second\",\n    \"bbl\": \"be back later\",\n    \"bbs\": \"be back soon\",\n    \"be4\": \"before\",\n    \"bfn\": \"bye for now\",\n    \"blvd\": \"boulevard\",\n    \"bout\": \"about\",\n    \"brb\": \"be right back\",\n    \"bros\": \"brothers\",\n    \"brt\": \"be right there\",\n    \"bsaaw\": \"big smile and a wink\",\n    \"btw\": \"by the way\",\n    \"bwl\": \"bursting with laughter\",\n    \"c\/o\": \"care of\",\n    \"cet\": \"central european time\",\n    \"cf\": \"compare\",\n    \"cia\": \"central intelligence agency\",\n    \"csl\": \"can not stop laughing\",\n    \"cu\": \"see you\",\n    \"cul8r\": \"see you later\",\n    \"cv\": \"curriculum vitae\",\n    \"cwot\": \"complete waste of time\",\n    \"cya\": \"see you\",\n    \"cyt\": \"see you tomorrow\",\n    \"dae\": \"does anyone else\",\n    \"dbmib\": \"do not bother me i am busy\",\n    \"diy\": \"do it yourself\",\n    \"dm\": \"direct message\",\n    \"dwh\": \"during work hours\",\n    \"e123\": \"easy as one two three\",\n    \"eet\": \"eastern european time\",\n    \"eg\": \"example\",\n    \"embm\": \"early morning business meeting\",\n    \"encl\": \"enclosed\",\n    \"encl.\": \"enclosed\",\n    \"etc\": \"and so on\",\n    \"faq\": \"frequently asked questions\",\n    \"fawc\": \"for anyone who cares\",\n    \"fb\": \"facebook\",\n    \"fc\": \"fingers crossed\",\n    \"fig\": \"figure\",\n    \"fimh\": \"forever in my heart\",\n    \"ft.\": \"feet\",\n    \"ft\": \"featuring\",\n    \"ftl\": \"for the loss\",\n    \"ftw\": \"for the win\",\n    \"fwiw\": \"for what it is worth\",\n    \"fyi\": \"for your information\",\n    \"g9\": \"genius\",\n    \"gahoy\": \"get a hold of yourself\",\n    \"gal\": \"get a life\",\n    \"gcse\": \"general certificate of secondary education\",\n    \"gfn\": \"gone for now\",\n    \"gg\": \"good game\",\n    \"gl\": \"good luck\",\n    \"glhf\": \"good luck have fun\",\n    \"gmt\": \"greenwich mean time\",\n    \"gmta\": \"great minds think alike\",\n    \"gn\": \"good night\",\n    \"g.o.a.t\": \"greatest of all time\",\n    \"goat\": \"greatest of all time\",\n    \"goi\": \"get over it\",\n    \"gps\": \"global positioning system\",\n    \"gr8\": \"great\",\n    \"gratz\": \"congratulations\",\n    \"gyal\": \"girl\",\n    \"h&c\": \"hot and cold\",\n    \"hp\": \"horsepower\",\n    \"hr\": \"hour\",\n    \"hrh\": \"his royal highness\",\n    \"ht\": \"height\",\n    \"ibrb\": \"i will be right back\",\n    \"ic\": \"i see\",\n    \"icq\": \"i seek you\",\n    \"icymi\": \"in case you missed it\",\n    \"idc\": \"i do not care\",\n    \"idgadf\": \"i do not give a damn fuck\",\n    \"idgaf\": \"i do not give a fuck\",\n    \"idk\": \"i do not know\",\n    \"ie\": \"that is\",\n    \"i.e\": \"that is\",\n    \"ifyp\": \"i feel your pain\",\n    \"IG\": \"instagram\",\n    \"iirc\": \"if i remember correctly\",\n    \"ilu\": \"i love you\",\n    \"ily\": \"i love you\",\n    \"imho\": \"in my humble opinion\",\n    \"imo\": \"in my opinion\",\n    \"imu\": \"i miss you\",\n    \"iow\": \"in other words\",\n    \"irl\": \"in real life\",\n    \"j4f\": \"just for fun\",\n    \"jic\": \"just in case\",\n    \"jk\": \"just kidding\",\n    \"jsyk\": \"just so you know\",\n    \"l8r\": \"later\",\n    \"lb\": \"pound\",\n    \"lbs\": \"pounds\",\n    \"ldr\": \"long distance relationship\",\n    \"lmao\": \"laugh my ass off\",\n    \"lmfao\": \"laugh my fucking ass off\",\n    \"lol\": \"laughing out loud\",\n    \"ltd\": \"limited\",\n    \"ltns\": \"long time no see\",\n    \"m8\": \"mate\",\n    \"mf\": \"motherfucker\",\n    \"mfs\": \"motherfuckers\",\n    \"mfw\": \"my face when\",\n    \"mofo\": \"motherfucker\",\n    \"mph\": \"miles per hour\",\n    \"mr\": \"mister\",\n    \"mrw\": \"my reaction when\",\n    \"ms\": \"miss\",\n    \"mte\": \"my thoughts exactly\",\n    \"nagi\": \"not a good idea\",\n    \"nbc\": \"national broadcasting company\",\n    \"nbd\": \"not big deal\",\n    \"nfs\": \"not for sale\",\n    \"ngl\": \"not going to lie\",\n    \"nhs\": \"national health service\",\n    \"nrn\": \"no reply necessary\",\n    \"nsfl\": \"not safe for life\",\n    \"nsfw\": \"not safe for work\",\n    \"nth\": \"nice to have\",\n    \"nvr\": \"never\",\n    \"nyc\": \"new york city\",\n    \"oc\": \"original content\",\n    \"og\": \"original\",\n    \"ohp\": \"overhead projector\",\n    \"oic\": \"oh i see\",\n    \"omdb\": \"over my dead body\",\n    \"omg\": \"oh my god\",\n    \"omw\": \"on my way\",\n    \"p.a\": \"per annum\",\n    \"p.m\": \"after midday\",\n    \"pm\": \"prime minister\",\n    \"poc\": \"people of color\",\n    \"pov\": \"point of view\",\n    \"pp\": \"pages\",\n    \"ppl\": \"people\",\n    \"prw\": \"parents are watching\",\n    \"ps\": \"postscript\",\n    \"pt\": \"point\",\n    \"ptb\": \"please text back\",\n    \"pto\": \"please turn over\",\n    \"qpsa\": \"what happens\",\n    \"ratchet\": \"rude\",\n    \"rbtl\": \"read between the lines\",\n    \"rlrt\": \"real life retweet\",\n    \"rofl\": \"rolling on the floor laughing\",\n    \"roflol\": \"rolling on the floor laughing out loud\",\n    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n    \"rt\": \"retweet\",\n    \"ruok\": \"are you ok\",\n    \"sfw\": \"safe for work\",\n    \"sk8\": \"skate\",\n    \"smh\": \"shake my head\",\n    \"sq\": \"square\",\n    \"srsly\": \"seriously\",\n    \"ssdd\": \"same stuff different day\",\n    \"tbh\": \"to be honest\",\n    \"tbs\": \"tablespooful\",\n    \"tbsp\": \"tablespooful\",\n    \"tfw\": \"that feeling when\",\n    \"thks\": \"thank you\",\n    \"tho\": \"though\",\n    \"thx\": \"thank you\",\n    \"tia\": \"thanks in advance\",\n    \"til\": \"today i learned\",\n    \"tl;dr\": \"too long i did not read\",\n    \"tldr\": \"too long i did not read\",\n    \"tmb\": \"tweet me back\",\n    \"tntl\": \"trying not to laugh\",\n    \"ttyl\": \"talk to you later\",\n    \"u\": \"you\",\n    \"u2\": \"you too\",\n    \"u4e\": \"yours for ever\",\n    \"utc\": \"coordinated universal time\",\n    \"w\/\": \"with\",\n    \"w\/o\": \"without\",\n    \"w8\": \"wait\",\n    \"wassup\": \"what is up\",\n    \"wb\": \"welcome back\",\n    \"wtf\": \"what the fuck\",\n    \"wtg\": \"way to go\",\n    \"wtpa\": \"where the party at\",\n    \"wuf\": \"where are you from\",\n    \"wuzup\": \"what is up\",\n    \"wywh\": \"wish you were here\",\n    \"yd\": \"yard\",\n    \"ygtr\": \"you got that right\",\n    \"ynk\": \"you never know\",\n    \"zzz\": \"sleeping bored and tired\"\n}\n\n\ndef remove_contractions(text):\n    return CONTRACTION_EN[text.lower()] if text.lower() in CONTRACTION_EN.keys() else text\n\ndef remove_abbrevation(text):\n    return ABBREVATION_EN[text.lower()] if text.lower() in ABBREVATION_EN.keys() else text\n\ndef clean_dataset(text):\n    # To lowercase\n    text = text.lower()\n    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n    text= ''.join(c for c in text if c <= '\\uFFFF') \n    text = text.strip()\n    # Remove misspelling words\n    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text))\n    # Remove punctuation\n    text = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\\/\\|\\'\\(\\']\", \" \", text).split())\n    # Remove emoji\n    text = emoji.demojize(text)\n    text = text.replace(\":\",\" \")\n    text = ' '.join(text.split()) \n    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n    # Remove Mojibake (also extra spaces)\n    text = ' '.join(re.sub(\"[^\\u4e00-\\u9fa5\\u0030-\\u0039\\u0041-\\u005a\\u0061-\\u007a]\", \" \", text).split())\n    return text\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'#', '', text) # Remove hashtag\n    text = re.sub(r'@\\w+', '', text) # Remove mentions\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\n\ndef find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n\ndef find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?:\/\/\\S+\", tweet)]) or 'no'\n\ndef process_text(df):\n    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    return df","62e1ae94":"df_train = df_train.fillna('None')\ndf_test = df_test.fillna('None')","402bd42a":"df_train['question1']","7b9fb8bb":"df_train['question1'] = df_train['question1'].apply(lambda x: clean_text(x))\ndf_train['question1'] = df_train['question1'].apply(lambda x : remove_contractions(x))\ndf_train['question1'] = df_train['question1'].apply(lambda x : clean_dataset(x))\ndf_train['question1'] = df_train['question1'].apply(lambda x : remove_abbrevation(x))","12a64c9c":"df_train['question2'] = df_train['question2'].apply(lambda x: clean_text(x))\ndf_train['question2'] = df_train['question2'].apply(lambda x : remove_contractions(x))\ndf_train['question2'] = df_train['question2'].apply(lambda x : clean_dataset(x))\ndf_train['question2'] = df_train['question2'].apply(lambda x : remove_abbrevation(x))","bf7d6d63":"df_train['question1']","cf270a79":"'''df_train['is_equal'] = (df_train['question1'] == df_train['question2'])\ndf_train['len_question1'] = df_train['question1'].apply(lambda x: len(x))\ndf_train['len_question2'] = df_train['question2'].apply(lambda x: len(x))\ndf_train['len_diff_question1'] = df_train['len_question1'] - df_train['len_question2'] '''","bf390e35":"df_train.head()","8b695ceb":"y = df_train['is_duplicate']","b28a03ab":"## Same thing for testing dataset\ndf_test['question1'] = df_test['question1'].apply(lambda x: clean_text(x))\ndf_test['question1'] = df_test['question1'].apply(lambda x : remove_contractions(x))\ndf_test['question1'] = df_test['question1'].apply(lambda x : clean_dataset(x))\ndf_test['question1'] = df_test['question1'].apply(lambda x : remove_abbrevation(x))\ndf_test['question2'] = df_test['question2'].apply(lambda x: clean_text(x))\ndf_test['question2'] = df_test['question2'].apply(lambda x : remove_contractions(x))\ndf_test['question2'] = df_test['question2'].apply(lambda x : clean_dataset(x))\ndf_test['question2'] = df_test['question2'].apply(lambda x : remove_abbrevation(x))\n'''df_test['is_equal'] = (df_test['question1'] == df_test['question2'])\ndf_test['len_question1'] = df_test['question1'].apply(lambda x: len(x))\ndf_test['len_question2'] = df_test['question2'].apply(lambda x: len(x))\ndf_test['len_diff_question1'] = df_test['len_question1'] - df_test['len_question2']'''","4fa9c24e":"def train_model(train_features,train_target,n_iterations):\n    \n    model = RandomizedSearchCV(estimator = XGBClassifier(), param_distributions =\n                      {\n                          'learning_rate': [0.1,0.2,0.3],\n                          'n_estimators': [100,150,300],\n                          'max_depth': [2,3,4,5,6]\n                      },\n                      n_iter = n_iterations,\n                      scoring = 'accuracy',\n                      n_jobs = 5\n                      )\n    \n    model.fit(train_features,train_target)\n    \n    return model","abe0580b":"train_text_q1 = df_train.question1\ntrain_text_q2 = df_train.question2\ntest_text_q1 = df_test.question1\ntest_text_q2 = df_test.question2\nall_text = pd.concat([train_text_q1, train_text_q2,test_text_q1, test_text_q2 ])","379b20fd":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    norm='l2',\n    min_df=0,\n    smooth_idf=False,\n    max_features=20000)\nword_vectorizer.fit(all_text)","d0c4a53f":"train_word_features1 = word_vectorizer.transform(train_text_q1)\ntrain_word_features2 = word_vectorizer.transform(train_text_q2)\ntest_word_features1 = word_vectorizer.transform(test_text_q1)\ntest_word_features2 = word_vectorizer.transform(test_text_q2)\n\ntrain_features = hstack([train_word_features1, train_word_features2]).tocsr()\ntest_features = hstack([test_word_features1, test_word_features2]).tocsr()","d72e39ec":"model_tdif = train_model(train_features, y,test_features,10)","f3550fcd":"y_pred = model_tdif.predict(test_features) ","f47ca7df":"y_pred= y_pred.round().astype(np.int)","ae75e52a":"# save to csv file\nsavetxt('submission_quora1.csv', y_pred, delimiter=',')","08bcf3e7":"embed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\")","e8599636":"X_train_embeddings1 = embed(df_train.question1)","80326c9f":"X_train_embeddings2 = embed(df_train.question2)","55609e88":"X_train_embeddings = np.concatenate((X_train_embeddings1.numpy(), X_train_embeddings2.numpy()), axis=1)","840df13d":"del X_train_embeddings1\ndel X_train_embeddings2\ngc.collect()","af622ea6":"X_feat=sparse.csr_matrix(X_train_embeddings)","e69ed9c6":"del X_train_embeddings\ngc.collect()","1aea7a7e":"y.shape, X_feat.shape","9c515346":"model_google_hub = XGBClassifier()","0f98cb7a":"model_google_hub.fit(X_feat,y)","782e73e2":"model_google_hub","4288fee3":"gc.collect()","0b600c74":"X_test_embeddings1 = embed(df_test.question1)","4a159616":"X_test_embeddings2 = embed(df_test.question2)","e4574869":"X_test_embeddings = np.concatenate((X_test_embeddings1.numpy(), X_test_embeddings2.numpy()), axis=1)","eb6d2709":"del X_test_embeddings1\ndel X_test_embeddings2\ngc.collect()","96b78f3d":"X_test_embeddings.shape","cd69907f":"X_test = sparse.csr_matrix(X_test_embeddings)","1ab88313":"del X_test_embeddings\ngc.collect()","0120ec8a":"y_pred_2 = model_google_hub.predict(X_test) ","fe5657d5":"y_pred2 = y_pred_2.round().astype(np.int)","17e8b292":"y_pred2","15f6c786":"# save to csv file\nsavetxt('submission_quora2.csv', y_pred2, delimiter=',')","ede6487e":"## Training Xgboost with google Sentence encoder","885bf21a":"## Training Xgboost with TfidfVectorizer"}}