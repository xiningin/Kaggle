{"cell_type":{"3b9ed103":"code","facc4240":"code","7bea1048":"code","2e61de4d":"code","8131c46f":"code","b4f566a0":"code","14bff573":"code","6d04974e":"code","c2d9d9cc":"code","4fefce5c":"code","cef808ad":"code","bc9aea5e":"code","6465f0e0":"code","e5d8bb0b":"code","839e32e7":"code","d8201858":"code","578f0f1b":"code","1f276133":"code","74837965":"code","7d80101c":"code","811c86da":"code","ee16c6c8":"code","ffc1ccb3":"code","d676e9b1":"code","5adb4906":"code","52b1d6da":"code","d9ad601d":"code","0085491e":"code","c148968a":"code","19b0177f":"code","4222c31c":"code","37df5277":"code","b6f2c6fa":"code","dad6142b":"code","754d579b":"code","719b24a7":"code","0a96f7d2":"code","c55aa59f":"code","c9ad24b8":"code","6290d8ad":"code","e28c5cec":"code","4a04306a":"code","10488c5f":"code","8e26f4b6":"code","8cedc778":"code","872e63eb":"code","2b71e0b8":"code","c42168ef":"code","efc74cc3":"markdown","e4b6e5aa":"markdown","51b0aafc":"markdown","827b704c":"markdown","0eba8330":"markdown","9cb88e6a":"markdown","d8d8bd1b":"markdown","1af79ed3":"markdown","867c9534":"markdown","cd0c7e0d":"markdown","b050be5b":"markdown","8c1abbe0":"markdown","02f5bdcb":"markdown","63946286":"markdown","c05761ba":"markdown","85cfaa73":"markdown","ad3a2c81":"markdown","fa87935a":"markdown","82df0cc1":"markdown","b1f87ce9":"markdown","0190c881":"markdown","223dc0ab":"markdown","537007ea":"markdown","b15ccaaa":"markdown","ac260dfe":"markdown","e9a23a8f":"markdown","16979021":"markdown","892bd544":"markdown","98d24dc1":"markdown","8d0028c9":"markdown"},"source":{"3b9ed103":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom xgboost import XGBClassifier","facc4240":"pd.pandas.set_option('display.max_columns',None)\ntrain=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","7bea1048":"train.head()","2e61de4d":"train.shape","8131c46f":"test.shape","b4f566a0":"train.info()","14bff573":"#number of passengers survived\nsum(list(train.Survived))","6d04974e":"train.describe()","c2d9d9cc":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4fefce5c":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","cef808ad":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","bc9aea5e":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6465f0e0":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","e5d8bb0b":"plt.figure(figsize=(14, 6))\nplt.subplot(1,2,1)\ncols = ['blue', 'lightcoral']\ntrain['Sex'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',shadow=True, colors=cols)\nplt.title('Total Male\/Female onboard')\nplt.subplot(1,2,2)\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train,palette='plasma')\nplt.title('Sex vs Survived')\nplt.ylabel(\"Survival Rate\")\nplt.show()\n","839e32e7":"train.groupby([\"Survived\"]).Fare.mean()\n","d8201858":"#Correlation:  Its the most basic way to find relation between any two quantities.\ncorr = train.corr()\n\nplt.figure(figsize=(15,15))\n\nsns.heatmap(corr,vmax=0.9,square=True)\nplt.show()","578f0f1b":"train.corr()[\"Survived\"]","1f276133":"train.isnull().mean()","74837965":"#lets drop cabin\ntrain.drop(\"Cabin\", axis = 1, inplace = True)\ntest.drop(\"Cabin\", axis = 1, inplace = True)","7d80101c":"#age can be filled by median\ntrain[\"Age\"].fillna(train[\"Age\"].median(), inplace = True)\ntest[\"Age\"].fillna(test[\"Age\"].median(), inplace = True) ","811c86da":"#embarked can be filled by median\/or mode\ntrain['Embarked'].value_counts(normalize=True)","ee16c6c8":"Embarked_mode=train['Embarked'].mode()[0]\nEmbarked_mode","ffc1ccb3":"\ntrain[\"Embarked\"].fillna(\"S\", inplace = True)","d676e9b1":"test.isnull().mean()","5adb4906":"#fare can be filled with median\ntest[\"Fare\"].fillna(test[\"Fare\"].median(), inplace = True)","52b1d6da":"test.isnull().mean()","d9ad601d":"train.isnull().mean()","0085491e":"train['Sex'] = train['Sex'].map({'female': 0, 'male': 1})\ntest['Sex']= test['Sex'].map({'female': 0, 'male': 1})\n\ntrain['Embarked'] = train['Embarked'].map({'S': 0, 'C': 1,'Q': 2})\ntest['Embarked']= test['Embarked'].map({'S': 0, 'C': 1,'Q': 2})","c148968a":"train.drop([\"Name\",\"Ticket\"], axis = 1, inplace = True)\ntest.drop([\"Name\",\"Ticket\"], axis = 1, inplace = True)","19b0177f":"train[\"Family\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"Family\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\ntrain=train.drop([\"SibSp\",\"Parch\"],axis=1)\ntest=test.drop([\"SibSp\",\"Parch\"],axis=1)\nprint(train.shape)\nprint(test.shape)","4222c31c":"scaler = StandardScaler()\n\ntrain[['Age','Fare']] = scaler.fit_transform(train[['Age','Fare']])\ntest[['Age','Fare']] = scaler.transform(test[['Age','Fare']])\n\ntrain.head()","37df5277":"test.head()\n","b6f2c6fa":"X_train = train.drop(['Survived','PassengerId'], axis=1)\ny_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1)\nX_train.shape, y_train.shape, X_test.shape","dad6142b":"# Logistic Regression\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\n\n# Making Predictions\ny_pred = LR.predict(X_test)","754d579b":"# Calculating the Accuracy of the model.\n\nprint(\"Accuracy:\",round(LR.score(X_train, y_train)*100,2))","719b24a7":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)","0a96f7d2":"#Checking accuracy\nrandom_forest.score(X_train, y_train)\n\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","c55aa59f":"Features=['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Family']\nfeature_importance = pd.Series(random_forest.feature_importances_,index=Features).sort_values(ascending=False)\nfeature_importance","c9ad24b8":"combine = [train, test]","6290d8ad":"\n\ntrain[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e28c5cec":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['Family'] == 1, 'IsAlone'] = 1\n\ntrain[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","4a04306a":"X_train = train.drop(['Survived','PassengerId'], axis=1)\ny_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1)\nX_train.shape, y_train.shape, X_test.shape","10488c5f":"random_forest_2 = RandomForestClassifier(n_estimators=100)\nrandom_forest_2.fit(X_train, y_train)\nY_pred = random_forest_2.predict(X_test)","8e26f4b6":"#Checking accuracy\nrandom_forest_2.score(X_train, y_train)\n\nacc_random_forest_2 = round(random_forest_2.score(X_train, y_train) * 100, 2)\nacc_random_forest_2","8cedc778":"Features=['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Family',\"IsAlone\"]\nfeature_importance = pd.Series(random_forest_2.feature_importances_,index=Features).sort_values(ascending=False)\nfeature_importance","872e63eb":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': Y_pred})\n\noutput.to_csv('my_submission_2.csv', index=False)\nprint(\"Your submission was successfully saved!\")","2b71e0b8":"# Instantiate our model\nxg = XGBClassifier()\nxg.fit(X_train, y_train)\nxg_predictions = xg.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': xg_predictions})\n\noutput.to_csv('my_submission_3.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","c42168ef":"xg.score(X_train,y_train)","efc74cc3":"Fare,number of siblings\/spouses,parents\/children,age,Pcclass seems important.\n(For newbies: Negative  sign in correlation just means that quantities are inversely proprtional, they can be negative and important)\npassenger ID is obviously irrelevant","e4b6e5aa":"Children were saved more","51b0aafc":"Sib\/Sp and Parch can be combined","827b704c":"This seems better than Family feature","0eba8330":"Believe me this is the most interesting thing i found.Dont want to draw any conclusion.But do think wisely.","9cb88e6a":"Only 38% people survived","d8d8bd1b":"**3.)XGBoost**","1af79ed3":"Lets try to make a new feature","867c9534":"**Feature selection**","cd0c7e0d":"1.)Basic logistic regression","b050be5b":"For anyone who has seen the movie this should not come as a surprise.(obviously there was space on wood plank for Leonardo DiCaprio)","8c1abbe0":"This is also poorer than random forest classifier.\nWithout using other models i can confidently say that Random forest classifier is indeed the best.\nFeature engineering could be better.\n\"Data is fuel to the model, Make sure it is good quality\"\nWell i clearly did this notebook in a stipulated time limit,thus this was the best i can offer.\nSuggestions to improve are welcome.\nHappy learning.","02f5bdcb":"2.)Random forest classifier","63946286":"We have 10 features( exclusing passenger id and survived)","c05761ba":"342 survived out of 891","85cfaa73":"**Handling missing values**","ad3a2c81":"Well Is Alone doesnt seems as important as we thought.\nNeither removing Embarked helped","fa87935a":"**Lets drop and combine some features now**","82df0cc1":"Feature Scaling","b1f87ce9":"**Converting categorical to numeric**","0190c881":"**MODELS**","223dc0ab":"Lets try to understand how survival varies with different features","537007ea":"But do note-gender seems an important feature for prediction","b15ccaaa":"Lets move ahead","ac260dfe":"Well family doesnt seems to be that important.(Only as a feature here)(Highly pun intended.)","e9a23a8f":"Higher class survived more","16979021":"I know what you want next.Here we go","892bd544":"**DATA EXPLORATION**","98d24dc1":"higher fare people survived more","8d0028c9":"**The target of this notebook is to quickly gain insights of a dataset and build a competitive model in a stipulated time to check basic knowledge\nI hope you will like it.This gives a competitive score and actually a pretty good one in first try within an hour.\nThis notebook is useful for people who quickly want to know how to approach a data science problem.Do upvote if u like it.**"}}