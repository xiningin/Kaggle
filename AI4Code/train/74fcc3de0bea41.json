{"cell_type":{"08a53fe3":"code","5e40829d":"code","89310f74":"code","7bdbcf24":"code","1197fc62":"code","771ff6d9":"code","c66d89d3":"code","c59014b0":"code","6528d1ab":"code","0896b4e2":"code","ae6e9d40":"code","1cdc8ad7":"code","c87d0181":"code","ffa09ea4":"code","62da15c2":"code","372a5462":"code","3c72207b":"code","749dd416":"code","689eea31":"code","eb861ca7":"code","7d702341":"code","19dcb301":"code","87acf5fa":"code","4836d00c":"code","32e33bf0":"code","6a326ed3":"code","45da494c":"code","eeb176c1":"code","eb9e9d5b":"code","68b8964d":"code","128f6be1":"code","a5c54559":"code","13f38310":"code","152097a0":"code","00e19ad6":"code","944fd95d":"code","8f886915":"code","afbb1f0c":"code","a7498fc2":"code","28e42554":"code","d51d136f":"code","3634964a":"code","1e6e2919":"code","46647ec9":"code","a5a45ed1":"code","5b61fe31":"code","f38400ae":"code","c7d64210":"code","b91e8d11":"code","a8ef4185":"code","bfd2229e":"code","86f02400":"code","7cd2a6cc":"code","d7b62f9f":"code","0ee7969c":"code","e4d4f7ed":"code","d6a3de0b":"code","6cddf34f":"code","dd9e5ae4":"code","fdcd3dd1":"code","a334708d":"code","6a0053bc":"code","dc8ac760":"code","f413de39":"code","6ce25731":"code","584e6b82":"code","739038ce":"code","71fb0f04":"code","58c79a67":"code","6e18b266":"code","0e1dd6a1":"code","5a7ba6ed":"code","88130b55":"code","3c7b3a5e":"code","d61fb5de":"code","e8637032":"code","0eb4ba79":"code","f33b1f62":"code","a7982b7c":"code","16b4c8d7":"markdown","6b89d3df":"markdown","24ece647":"markdown","7b2c758e":"markdown","b4f651a8":"markdown","00045ed2":"markdown","a7bbaebe":"markdown","24797c5a":"markdown","eb923a36":"markdown","f1af873d":"markdown","3ac40615":"markdown","489cd0e1":"markdown","ed0d42e0":"markdown","de42a7fe":"markdown","f0f68ea4":"markdown","58d0e38e":"markdown","82ec0f90":"markdown","94615938":"markdown","bd46a788":"markdown","e9b1d4ac":"markdown","ee56fed8":"markdown","0d552b50":"markdown","116b8f32":"markdown","51a88e9f":"markdown","26becf25":"markdown","0c631d58":"markdown","8d4012e8":"markdown","ddd4f03c":"markdown","ccdc2460":"markdown","33395b0b":"markdown","f82f5e96":"markdown","bcef83dc":"markdown","1d124b9c":"markdown","cc713e48":"markdown","3a46381e":"markdown","b4c20fd6":"markdown","9894e5e9":"markdown","8214ddbb":"markdown","093b9e7e":"markdown","82f9ca5a":"markdown","ed19014c":"markdown","f19724f8":"markdown","865e3d06":"markdown","236cc7b1":"markdown","2af5fead":"markdown","998b760f":"markdown","e9f949e2":"markdown","d8103b53":"markdown","e2616496":"markdown","2b4325e3":"markdown","f983b6a2":"markdown","aa676bcd":"markdown","9c3164bd":"markdown","12dd4d03":"markdown","50de4235":"markdown","4fe7fa11":"markdown"},"source":{"08a53fe3":"import numpy as np  # library to handle data in a vectorized manner\nimport pandas as pd  # library for data analsysis\n\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)\n\nfrom bs4 import BeautifulSoup\n\nimport json  # library to handle JSON files\n\n#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import (\n    Nominatim,\n)  # convert an address into latitude and longitude values\n\nimport requests  # library to handle requests\nfrom urllib import request\n\nfrom pandas.io.json import json_normalize  # tranform JSON file into a pandas dataframe\n\n# Matplotlib\/Seaborn and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# import sklearn functions\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.model_selection import (\n    train_test_split,\n    cross_val_score,\n    KFold,\n    GridSearchCV,\n)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import neighbors\n\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\n# create a Graphviz file\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n\n# import StringIO, pydot\n# import pylab, pydotplus\nfrom sklearn.externals.six import StringIO\n\n# import xgboost\nimport xgboost as xgb\n\n# import keras functions\nimport keras\nfrom keras import Sequential, regularizers\nfrom keras.optimizers import SGD, RMSprop, Adam\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.utils import np_utils\n\n#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium  # map rendering library\n\nimport time\n\nprint(\"Libraries imported.\")\n","5e40829d":"# The CSV files is exported from my Web Scraping of School informations.ipynb file\nschool = pd.read_csv(\"..\/input\/school_scores.csv\")\nschool = school.iloc[:, 1:]\n\n# Since the origin csv file includes all of the schools in Ontario, here we need to define the Cities belongs to GTA\n# Due to project objective, I didn't selected all of the cities in GTA\nGTA = [\n    \"Toronto\",\n    \"Mississauga\",\n    \"Ajax\",\n    \"Brampton\",\n    \"Unionville\",\n    \"Oakville\",\n    \"Richmond Hill\",\n    \"Markham\",\n    \"Oshawa\",\n    \"Whitby\",\n    \"Caledon\",\n    \"Pickering\",\n    \"Maple\",\n    \"Thornhill\",\n    \"Woodbridge\",\n    \"Aurora\",\n    \"Vaughan\",\n    \"Concord\",\n    \"Stouffville\",\n    \"Milton\",\n    \"Newmarket\",\n    \"King City\",\n    \"Durham\",\n]\nschool = school[school[\"school_city\"].isin(GTA)]\nschool.head()\n","89310f74":"# check the size of dataset\nschool.shape  # totally 1030 schools and five columns\n","7bdbcf24":"# The CSV files is exported from my Web Scraping of House informations.ipynb file\nhouse = pd.read_csv(\"..\/input\/house.csv\")\nhouse = house.iloc[:, 1:]\nhouse.head()\n","1197fc62":"# Check the size of the dataset and those doesn't have the size data\nprint(\"shape of table house\", house.shape)\nprint(\"shape of na values\", house[house[\"size\"].isna()].shape)\nprint(\"shape of not na values\", house[house[\"size\"].notna()].shape)\n","771ff6d9":"house[\"size\"].unique()[:5]\n","c66d89d3":"# Manipulate size column\nhouse2 = house[\"size\"].str.strip(\"sqft\").str.strip(\"<\").str.strip().str.split(\"-\")\n\nfor i in range(len(house2)):\n    try:\n        house2[i] = np.mean([float(i) for i in house2[i]])\n    except TypeError:\n        house2[i] = float(house2[i])\n    except ValueError:\n        house2[i] = float(house2[i][0].strip(\"+\"))\n\nhouse[\"size\"] = house2\n","c59014b0":"# drop outliers\nhouse.drop(house[house[\"size\"] < 300].index, inplace=True)\n","6528d1ab":"# Replace missing values of size column to the mean of the sizes with the same type and number of bedroom\nhousefillna = pd.Series()\nptypes = house.ptype.unique()\nnum_bedrooms = house.num_bedroom.unique()\nfor ptype in ptypes:\n    for num_bedroom in num_bedrooms:\n        try:\n            batch = (house.ptype == ptype) & (house.num_bedroom == num_bedroom)\n            h = house[batch][\"size\"].fillna(house[batch][\"size\"].mean())\n            housefillna = housefillna.append(h)\n        except:\n            housefillna = housefillna.append(house[batch][\"size\"])\n\nprint(len(housefillna))\n\nhouse[\"size\"] = housefillna\n","0896b4e2":"# Drop the rest 13 null rows\nhouse.drop(house[house[\"size\"].isna()].index, inplace=True)\n","ae6e9d40":"print(\"shape of table house\", house.shape)\nprint(\"shape of na values\", house[house[\"size\"].isna()].shape)\nprint(\"shape of not na values\", house[house[\"size\"].notna()].shape)\n","1cdc8ad7":"# Manipulate num_bedroom column\nhouse.num_bedroom = house.num_bedroom.str.strip(\" bd\")\nhouse.num_bedroom = house.num_bedroom.str.strip(\"-\")\nhouse = house[house.num_bedroom != \"\"]\nhouse.num_bedroom = house.num_bedroom.astype(int)\n","c87d0181":"# Manipulate num_bathroom column\nhouse.num_bathroom = house.num_bathroom.str.strip(\" ba\")\nhouse.num_bathroom = house.num_bathroom.str.strip(\"-\")\nhouse = house[house.num_bathroom != \"\"]\nhouse.num_bathroom = house.num_bathroom.astype(int)\n\n# drop outliers\nhouse = house[house.num_bathroom <= 9]\n","ffa09ea4":"# Manipulate price column\nhouse.price = house.price.str.replace(\",\", \"\")\nhouse.price = house.price.astype(float)\n","62da15c2":"house.info()\n","372a5462":"house.ptype.unique()\n","3c72207b":"# Map categories data into numeric\nhouse.ptype = house.ptype.map({\"condo\": 1, \"townhouse\": 2, \"house\": 3})\n# Create price_per_room column\nhouse[\"price_per_room\"] = house[\"price\"] \/ house[\"num_bedroom\"]\n# Drop address column\nhouse.drop(\"address\", axis=1, inplace=True)\n","749dd416":"# Group data by location parameters\nhouse = house.groupby([\"post_code\", \"neighborhood\"]).mean()\nhouse.reset_index(inplace=True)\nhouse.head()\n","689eea31":"school.columns = [\"name\", \"city\", \"school_score\", \"post_code\", \"address\"]\n","eb861ca7":"df = school.merge(house, on=\"post_code\")\n","7d702341":"df.columns\n","19dcb301":"df.head()\n","87acf5fa":"df = df.loc[\n    :,\n    [\n        \"city\",\n        \"neighborhood\",\n        \"post_code\",\n        \"school_score\",\n        \"num_bedroom\",\n        \"num_bathroom\",\n        \"price\",\n        \"price_per_room\",\n        \"latitude\",\n        \"longitude\",\n        \"ptype\",\n        \"size\",\n    ],\n]\ndf.head()\n","4836d00c":"df.shape\n","32e33bf0":"# Create new columns for analysis usage\ndf[\"type_muti_price_per_room\"] = df.ptype * df.price_per_room\ndf[\"tot_rooms\"] = df.num_bathroom + df.num_bedroom\ndf.head()\n","6a326ed3":"# Create Score Bins\ndf[\"score_binned\"] = pd.qcut(df[\"school_score\"], 5)\ndf.score_binned.value_counts()\n","45da494c":"# Plot Hist Map\naxes = df.tot_rooms.plot.hist(alpha=0.7)\naxes = df.school_score.plot.hist(alpha=0.7)\nplt.legend(labels=[\"tot_rooms\", \"school_score\"])\n","eeb176c1":"for feature in [\"price\", \"type_muti_price_per_room\"]:\n    df.loc[:, [\"score_binned\", feature]].groupby(\n        \"score_binned\"\n    ).mean().reset_index().plot.bar(x=\"score_binned\", y=feature)\n","eb9e9d5b":"# lat = []\n# lng = []\n# post_code_responsed = []\n# postcode_no_response = []\n# key = \"\" #please input your google api key\n# for postcode in df.post_code:\n#     URL = \"https:\/\/maps.googleapis.com\/maps\/api\/geocode\/json?address={},+Toronto,+CA&key={}\".format(\n#     postcode,key\n# )\n\n#     # Do the request and get the response data\n#     try:\n#         req = requests.get(URL)\n#         res = req.json()\n#         result = res['results'][0]\n#         lat.append(result['geometry']['location']['lat'])\n#         lng.append(result['geometry']['location']['lng'])\n#         post_code_responsed.append(postcode)\n#         print(postcode, \"finished\")\n#     except:\n#         print(postcode, \"google API response no result\")\n#         postcode_no_response.append(0)\n    ","68b8964d":"# lat_lng = pd.DataFrame([post_code_responsed, lat, lng]).T\n# lat_lng.columns = ['post_code', 'lat', 'lng']\n# lat_lng.to_csv(\"lat_lng.csv\", header=['post_code', 'lat', 'lng'])\n# lat_lng.head()\n","128f6be1":"# address = \"Toronto, Canada\"\n\n# geolocator = Nominatim(user_agent=\"tor_explorer\")\n# location = geolocator.geocode(address)\n# latitude = location.latitude\n# longitude = location.longitude\n# print(\"The geograpical coordinate of Toronto are {}, {}.\".format(latitude, longitude))\n\nlatitude = 43.653963\nlongitude = -79.387207\n","a5c54559":"CLIENT_ID = \"\"  # please input your Foursquare ID\nCLIENT_SECRET = \"\"  # please input your Foursquare Secret\nVERSION = \"20180605\"  # Foursquare API version\n","13f38310":"# function that extracts the category of the venue\ndef get_category_type(row):\n    try:\n        categories_list = row[\"categories\"]\n    except:\n        categories_list = row[\"venue.categories\"]\n\n    if len(categories_list) == 0:\n        return None\n    else:\n        return categories_list[0][\"name\"]\n","152097a0":"LIMIT = 300  # limit of number of venues returned by Foursquare API\n\ndef getNearbyVenues(names, latitudes, longitudes, radius=1000):\n\n    venues_list = []\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n\n        # create the API request URL\n        url = \"https:\/\/api.foursquare.com\/v2\/venues\/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n            CLIENT_ID, CLIENT_SECRET, VERSION, lat, lng, radius, LIMIT\n        )\n\n        # make the GET request\n        try:\n            results = requests.get(url).json()[\"response\"][\"groups\"][0][\"items\"]\n        except:\n            print(\"can't get request results from url\")\n\n        # return only relevant information for each nearby venue\n        venues_list.append(\n            [\n                (\n                    name,\n                    lat,\n                    lng,\n                    v[\"venue\"][\"name\"],\n                    v[\"venue\"][\"location\"][\"lat\"],\n                    v[\"venue\"][\"location\"][\"lng\"],\n                    v[\"venue\"][\"categories\"][0][\"name\"],\n                )\n                for v in results\n            ]\n        )\n\n    nearby_venues = pd.DataFrame(\n        [item for venue_list in venues_list for item in venue_list]\n    )\n    nearby_venues.columns = [\n        \"Neighborhood\",\n        \"Neighborhood Latitude\",\n        \"Neighborhood Longitude\",\n        \"Venue\",\n        \"Venue Latitude\",\n        \"Venue Longitude\",\n        \"Venue Category\",\n    ]\n\n    return nearby_venues\n","00e19ad6":"# # Run the above function on each post code and create a new dataframe called toronto_venues.\n# toronto_venues = getNearbyVenues(\n#     names=neighborhoods[\"post_code\"],\n#     latitudes=neighborhoods[\"latitude\"],\n#     longitudes=neighborhoods[\"longitude\"],\n#     radius=1000,\n# )\n\n# toronto_venues.to_csv(\"toronto_venues_1000.csv\")\n\n","944fd95d":"toronto_venues = pd.read_csv(\"..\/input\/toronto_venues_1000.csv\", encoding=\"ISO-8859-1\").iloc[\n    :, 1:\n]\n","8f886915":"toronto_venues[\"post_code\"] = toronto_venues.Neighborhood\nvenues_cnt = toronto_venues.groupby(\"post_code\").count().Venue.reset_index()\nvenues_cnt.columns = [\"post_code\", \"Venue_cnt\"]\ntoronto_venues = toronto_venues.merge(venues_cnt, on=\"post_code\")\n\nprint(\n    \"There are {} uniques categories.\".format(\n        len(toronto_venues[\"Venue Category\"].unique())\n    )\n)\n\ns = (\n    toronto_venues[\"Venue Category\"].value_counts() >= 3\n)  # Drop Categories with only 1-2 records\ni = s[s == True].index\ntoronto_venues = toronto_venues[toronto_venues[\"Venue Category\"].isin(i)]\n\nprint(\n    \"There are {} uniques categories after filter.\".format(\n        len(toronto_venues[\"Venue Category\"].unique())\n    )\n)\n","afbb1f0c":"toronto_venues.head()\n","a7498fc2":"# one hot encoding\ntoronto_onehot = pd.get_dummies(\n    toronto_venues[[\"Venue Category\"]], prefix=\"\", prefix_sep=\"\"\n)\n\n# add neighborhood column back to dataframe\ntoronto_onehot[\"Neighborhood\"] = toronto_venues[\"post_code\"]\n\n# move neighborhood column to the first column\nfixed_columns = [toronto_onehot.columns[-1]] + list(toronto_onehot.columns[:-1])\ntoronto_onehot = toronto_onehot[fixed_columns]\n\ntoronto_onehot.head()\n","28e42554":"toronto_onehot.shape\n","d51d136f":"toronto_grouped = toronto_onehot.groupby(\"Neighborhood\").mean().reset_index()\ntoronto_grouped.shape\n","3634964a":"toronto_grouped.rename({\"Neighborhood\": \"post_code\"}, axis=1, inplace=True)\ntoronto_grouped.head()\n","1e6e2919":"df_total = df.merge(toronto_grouped, on=\"post_code\")\ncnt = toronto_venues.loc[:, [\"post_code\", \"Venue_cnt\"]]\ncnt.drop_duplicates(inplace=True)\ndf_total = df_total.merge(cnt, on=\"post_code\")\nprint(df_total.shape)\ndf_total.head()\n","46647ec9":"cols = list(df_total.columns)\ncols = [\n    x\n    for x in cols\n    if x\n    not in [\n        \"city\",\n        \"neighborhood\",\n        \"post_code\",\n        \"latitude\",\n        \"longitude\",\n        \"score_binned\",\n    ]\n]\ncoef = np.corrcoef(df_total[cols].values.T)\ncoef_matrix = pd.DataFrame(coef, columns=cols, index=cols)\ncoef_matrix.head()\n","a5a45ed1":"print(len(coef_matrix))\nprint(len(coef_matrix[abs(coef_matrix.price) > 0.1]))\n","5b61fe31":"useful_features = coef_matrix[abs(coef_matrix.price) > 0.1].index\nuseful_features = list(useful_features)\n","f38400ae":"# drop outliers\nprint(len(df_total))\ndf_total = df_total[(df_total.price > 2 * 10 ** 5) & (df_total.price < 2.8 * 10 ** 6)]\ndf_total = df_total[df_total.post_code != \"L1K0S1\"]\nprint(len(df_total))\n","c7d64210":"df_clusters = df_total.loc[:, useful_features + [\"latitude\", \"longitude\"]]\n# df_clusters.drop('score_binned',axis=1,inplace=True)\ndf_clusters.head()\n\n# Standard processing\nsc = StandardScaler()\ndf_clusters_standard = sc.fit_transform(df_clusters)\ndf_clusters_standard = pd.DataFrame(df_clusters_standard)\ndf_clusters_standard.columns = df_clusters.columns\ndf_clusters_standard.head()\n","b91e8d11":"# set number of clusters\nkclusters = 6\n\n# run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(df_clusters_standard)\n\n# check cluster labels generated for each row in the dataframe\nkmeans.labels_[0:10]\n\n# add clustering labels\ndf_clusters_standard.insert(0, \"Cluster Labels\", kmeans.labels_)\ndf_clusters_standard.head()\n","a8ef4185":"# import matplotlib.cm as cm\n# create map\nmap_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i + x + (i * x) ** 2 for i in range(kclusters)]\ncolors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\nrainbow = [colors.rgb2hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(\n    df_clusters[\"latitude\"],\n    df_clusters[\"longitude\"],\n    df_clusters_standard[\"school_score\"],\n    df_clusters_standard[\"Cluster Labels\"],\n):\n    label = folium.Popup(str(poi) + \" Cluster \" + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=rainbow[cluster - 1],\n        fill=True,\n        fill_color=rainbow[cluster - 1],\n        fill_opacity=0.7,\n    ).add_to(map_clusters)\n\nmap_clusters\n","bfd2229e":"cols = coef_matrix.iloc[(-np.abs(coef_matrix[\"price\"].values)).argsort()].index[:12]\n\ncoef = np.corrcoef(df_total[cols].values.T)\nfig, ax = plt.subplots(figsize=(12, 12))  # Sample figsize in inches\nhm = sns.heatmap(\n    coef,\n    cbar=True,\n    annot=True,\n    square=True,\n    fmt=\".2f\",\n    annot_kws={\"size\": 15},\n    yticklabels=cols,\n    xticklabels=cols,\n    ax=ax,\n)\n","86f02400":"from pandas.plotting import scatter_matrix\n\ncols = coef_matrix.iloc[(-np.abs(coef_matrix[\"price\"].values)).argsort()].index[:11]\nscatter_matrix = scatter_matrix(\n    df_total[cols], alpha=0.2, figsize=(17, 17), diagonal=\"kde\"\n)\n","7cd2a6cc":"temp = df_total\ntemp[\"price_binned\"] = pd.qcut(\n    temp[\"price\"], 5, labels=[\"very low\", \"low\", \"medium\", \"high\", \"very high\"]\n)\ntemp.price_binned.value_counts()\n","d7b62f9f":"for feature in cols[3:]:\n    temp.loc[:, [\"price_binned\", feature]].groupby(\n        \"price_binned\"\n    ).mean().reset_index().plot.bar(x=\"price_binned\", y=feature, logy=True)\n","0ee7969c":"# Create features and label\nfeatures = [\n    \"school_score\",\n    \"num_bedroom\",\n    \"num_bathroom\",\n    \"ptype\",\n    \"size\",\n] + useful_features[9:]\nX = df_total.loc[:, features]\ny = df_total.loc[:, \"price\"]\n\n# Split data into Training and Testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Create a dataframe to store the performance of each models\nscores = pd.DataFrame()\n","e4d4f7ed":"# Create Predict and Plot function for ML methods\ndef try_different_method(method):\n    method.fit(X_train, y_train)\n    y_pred = method.predict(X_test)\n\n    y_test_temp = y_test.reset_index(drop=True)\n    order = y_pred.argsort(axis=0)\n    y_pred = y_pred[order]\n    y_test_temp = y_test_temp[order]\n\n    #     maer = np.mean(abs(y_pred - y_test_temp) \/ y_test_temp)\n    mse = metrics.mean_squared_error(y_test_temp, y_pred)\n    r2 = metrics.r2_score(y_test_temp, y_pred)\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(\n        np.arange(len(y_pred)),\n        y_test_temp,\n        \"ro\",\n        markersize=4,\n        label=\"list price\",\n        alpha=0.5,\n    )\n    plt.plot(\n        np.arange(len(y_pred)),\n        y_pred,\n        \"bo-\",\n        markersize=4,\n        label=\"predict price\",\n        alpha=0.9,\n    )\n\n    plt.grid()\n    plt.title(\"MSE: %f\" % mse)\n    print(\"mean_squared_error: %f\" % mse)\n    print(\"r2: %f\" % r2)\n    #     print('mean_abs_error_rate: %f' % maer)\n    plt.legend()\n    return (r2, mse)\n","d6a3de0b":"# parameters to search over with cross-validation\ngrid_params = [\n    {\n        \"n_estimators\": [10, 50, 100],\n        \"max_depth\": [3, 6, 8, 10, None],\n        \"min_samples_leaf\": [1, 2, 5],\n    }\n]\n\nclf = GridSearchCV(RandomForestRegressor(), grid_params, cv=5, scoring=\"r2\", n_jobs=2)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameter values: %r\\n\" % clf.best_params_)\n","6cddf34f":"# RandomForestRegressor\nrf = RandomForestRegressor(\n    n_estimators=clf.best_params_[\"n_estimators\"],\n    criterion=\"mse\",\n    max_depth=clf.best_params_[\"max_depth\"],\n    min_samples_leaf=clf.best_params_[\"min_samples_leaf\"],\n    n_jobs=2,\n    random_state=None,\n)\n\nperformance_rf = try_different_method(rf)\n\nscores.loc[0, \"Random Forest\"] = performance_rf[0]\nscores.loc[1, \"Random Forest\"] = performance_rf[1]\n","dd9e5ae4":"importance = pd.DataFrame({\"feature\": features, \"importance\": rf.feature_importances_})\nimportance.sort_values(by=\"importance\", axis=0, ascending=False, inplace=True)\nimportance[:18].plot(\n    x=\"feature\",\n    y=\"importance\",\n    kind=\"bar\",\n    figsize=(8, 4),\n    title=\"Feature Importance\",\n    logy=True,\n)\n","fdcd3dd1":"# parameters to search over with cross-validation\ngrid_params = [{\"max_depth\": [3, 4, 5, 6, 8, None], \"min_samples_leaf\": [1, 2, 5, 7]}]\n\ntree = GridSearchCV(DecisionTreeRegressor(), grid_params, cv=5, scoring=\"r2\", n_jobs=2)\ntree.fit(X_train, y_train)\n\nprint(\"Best parameter values: %r\\n\" % tree.best_params_)\n","a334708d":"# RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(\n    max_depth=tree.best_params_[\"max_depth\"],\n    min_samples_leaf=tree.best_params_[\"min_samples_leaf\"],\n    random_state=None,\n)\n\nperformance_tree = try_different_method(tree)\nscores.loc[0, \"Decistion Tree\"] = performance_tree[0]\nscores.loc[1, \"Decistion Tree\"] = performance_tree[1]\n","6a0053bc":"# dot_data = StringIO()\n# export_graphviz(\n#     tree,\n#     out_file=dot_data,\n#     feature_names=features,\n#     #                 class_names=['Churn'],\n#     filled=True,\n#     rounded=True,\n#     leaves_parallel=False,\n#     rotate=False,\n#     special_characters=True,\n# )\n# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n# # graph.write_pdf(\"tree_vehicles.pdf\")  # this line saves the diagram to a pdf file\n# Image(graph.create_png())\n","dc8ac760":"# parameters to search over with cross-validation\ngrid_params = [\n    {\n        \"max_depth\": [3, 4, 5],\n        \"learning_rate\": [0.01, 0.1, 1],\n        \"n_estimators\": [10, 50],\n        \"reg_lambda\": [10, 1, 0.1, 0.01],\n        \"objective\": [\"reg:linear\"],\n    }\n]\n\nxgbr = GridSearchCV(xgb.XGBRegressor(), grid_params, cv=5, scoring=\"r2\", n_jobs=2)\nxgbr.fit(X_train, y_train)\n\nprint(\"Best parameter values: %r\\n\" % xgbr.best_params_)\n","f413de39":"xgbr = xgb.XGBRegressor(\n    max_depth=xgbr.best_params_[\"max_depth\"],\n    learning_rate=xgbr.best_params_[\"learning_rate\"],\n    n_estimators=xgbr.best_params_[\"n_estimators\"],\n    reg_lambda=xgbr.best_params_[\"reg_lambda\"],\n    n_jobs=2,\n)\n\nperformance_XGB = try_different_method(xgbr)\nscores.loc[0, \"XGBoost\"] = performance_XGB[0]\nscores.loc[1, \"XGBoost\"] = performance_XGB[1]\n","6ce25731":"importance = pd.DataFrame(\n    {\"feature\": features, \"importance\": xgbr.feature_importances_}\n)\nimportance.sort_values(by=\"importance\", axis=0, ascending=False, inplace=True)\nimportance[:18].plot(\n    x=\"feature\",\n    y=\"importance\",\n    kind=\"bar\",\n    figsize=(8, 4),\n    title=\"Feature Importance\",\n    logy=True,\n)\n","584e6b82":"# Standard processing\nsc = StandardScaler()\nX_standard = sc.fit_transform(X)\nX_standard = pd.DataFrame(X_standard)\nX_standard.columns = X.columns\nX_standard.head()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)  # , random_state=42\n","739038ce":"# parameters to search over with cross-validation\ngrid_params = [{\"n_neighbors\": [i for i in range(1, 10)]}]\n\nknn = GridSearchCV(\n    neighbors.KNeighborsRegressor(), grid_params, cv=5, scoring=\"r2\", n_jobs=2\n)\nknn.fit(X_train, y_train)\n\nprint(\"Best parameter values: %r\\n\" % knn.best_params_)\n","71fb0f04":"knn = neighbors.KNeighborsRegressor(\n    n_neighbors=knn.best_params_[\"n_neighbors\"], n_jobs=2\n)\n\nperformance_KNN = try_different_method(knn)\nscores.loc[0, \"KNN\"] = performance_KNN[0]\nscores.loc[1, \"KNN\"] = performance_KNN[1]\n","58c79a67":"# Standard processing\nsc = StandardScaler()\nX_standard = sc.fit_transform(X)\nX_standard = pd.DataFrame(X_standard)\nX_standard.columns = X.columns\nX_standard.head()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)  # , random_state=42\n","6e18b266":"a = 100\ndrop = 0.1\nn = len(features)\n\nmodel = Sequential(\n    [\n        Dense(\n            int(n * 1.2), input_dim=n, kernel_initializer=\"normal\", activation=\"relu\"\n        ),\n        Dropout(drop),\n        Dense(int(n), kernel_initializer=\"normal\", activation=\"linear\"),\n        Dropout(drop),\n        Dense(\n            int(n * 0.5),\n            activation=\"linear\",\n            kernel_regularizer=regularizers.l1_l2(l1=a, l2=a),\n        ),\n        Dropout(drop),\n        Dense(1, kernel_initializer=\"normal\"),\n    ]\n)\n","0e1dd6a1":"model.summary()\n","5a7ba6ed":"model.compile(\n    loss=\"mean_squared_error\",\n    optimizer=\"adam\",\n    metrics=[\"mean_squared_error\", \"mae\", \"mape\"],\n)\nhistory = model.fit(\n    X_train, y_train, validation_split=0.2, epochs=50, verbose=0, shuffle=True\n)\n","88130b55":"# plot metrics\nax1 = plt.plot(history.history[\"mean_squared_error\"])\nax2 = plt.plot(history.history[\"val_mean_squared_error\"])\nplt.legend([\"mean_squared_error\", \"val_mean_squared_error\"])\n","3c7b3a5e":"y_pred = model.predict(X_test)\ny_pred_train = model.predict(X_train)\ny_test = np.array(y_test)\n","d61fb5de":"def my_r2_score(v_true, v_pred):\n    ssres = np.sum(np.square(v_true - v_pred))\n    sstot = np.sum(np.square(v_true - np.mean(v_true)))\n    return 1 - ssres \/ (sstot)\n","e8637032":"print(\"r2 on Test Set:\", r2_score(y_test, y_pred))\nprint(\"r2 on Train Set:\", r2_score(y_train, y_pred_train))\nprint(\"MSE on Test Set:\", mean_squared_error(y_test, y_pred))\nprint(\"MSE on Train Set:\", mean_squared_error(y_train, y_pred_train))\nprint(\"MAE on TestSet:\", mean_absolute_error(y_test, y_pred))\nprint(\"MAE on TrainSet:\", mean_absolute_error(y_train, y_pred_train))\n","0eb4ba79":"y_pred_temp = np.array([i[0] for i in y_pred])\ny_test_temp = y_test  # .reset_index(drop=True)\norder = y_pred_temp.argsort(axis=0)\ny_pred_temp = y_pred_temp[order]\ny_test_temp = y_test_temp[order]\n\n\nmse_NN = metrics.mean_squared_error(y_test_temp, y_pred_temp)\nr2_NN = metrics.r2_score(y_test_temp, y_pred_temp)\n\nplt.figure(figsize=(10, 6))\nplt.plot(\n    np.arange(len(y_pred_temp)), y_test_temp, \"ro-\", markersize=4, label=\"school score\", alpha=0.8\n)\nplt.plot(\n    np.arange(len(y_pred_temp)),\n    y_pred_temp,\n    \"bo-\",\n    markersize=4,\n    label=\"predict score\",\n    alpha=0.5,\n)\n\nplt.grid()\nplt.title(\"MSE: %f\" % mse_NN)\nprint(\"mean_squared_error: %f\" % mse_NN)\nprint(\"r2: %f\" % r2_NN)\n# print('mean_abs_error_rate: %f' % maer)\nplt.legend()\n\nscores.loc[0, \"Neural network\"] = r2_NN\nscores.loc[1, \"Neural network\"] = mse_NN\nscores\n","f33b1f62":"scores = scores.T\nscores.columns = [\"R2\", \"Mean Squared Error\"]\nscores\n","a7982b7c":"ax1 = scores.plot.bar(y=\"R2\")\nax2 = scores.plot.bar(y=\"Mean Squared Error\")\nplt.legend()\n","16b4c8d7":"Acquire the top 300 venues that are in this location within a radius of 1000 meters.","6b89d3df":"In this project, we will use regression methods to **predict the housing price of different zones**. Since there's only 700 examples but 329 features. Here's the methodologies going through:\n\nFirst, **dimensionality reduction** using the **correlation matrix** to filter features to avoid the sparse issue of the training set.\n\nSecond, **remove the outliers** due to the typo of the website, or the special case, which will have negative impacts on our model performance.\n\nThe third step, I'd like to use the K-Means clustering method and folium create a map of the GTA area to explore the basic **clusters distribution** of zones.\n\nThe fourth step, **Data Analysis & Visualization**, so we can picture the impacts of different features on the prices.\n\nThe fifth step, Machine Learning Modeling, I'll use different **regression models** and **grid search** methods to find the best hyper-parameters of each model and looking into their performance and confirm if that works well.\n\nAt last, I'll **compare the performances** among different methods and drop some ideas on **how to improve the accuracy** of our prediction in the future.","24ece647":"Based on the R2 and MSE peformace of different methods, we can tell that **Random Forest** and **XGBoost** are the best choices of our case and dataset, althrough the score is limited due to lack of exact size, built year, and other important features.\n\nFor the method of the Neural network method, in this project, I've just used some basic functions by Keras for practice and comparison usage. Since the dataset itself is very small, can't work its advantages, the performance only similar to what we have with Decision Tree algorithms. In the real industry, it's only required when we need huge dataset or special business cases.","7b2c758e":"#### Use Graphviz Create a Decision Tree Graph","b4f651a8":"### Optional: Acquire latitude\/longitude coordinates of each neighborhood through Google API","00045ed2":"#### Plot Feature Importance graph","a7bbaebe":"## 5. Modeling <a name=\"Modeling\"><\/a>","24797c5a":"#### Plot Feature Importance graph","eb923a36":"#### Use try_different_method function to plot prediction graph","f1af873d":"### 5.4 Decision Tree algorithm","3ac40615":"#### Use GridSearch method to figure the best hyper-parameters","489cd0e1":"# Housing \\ School \\ Venues Analysis and Prediction of GTA","ed0d42e0":"#### Manipulate Venues Data","de42a7fe":"### 2.1 Extract Data","f0f68ea4":"#### Use try_different_method function to plot prediction graph","58d0e38e":"Use geopy library to get the latitude and longitude values of Toronto.","82ec0f90":"## 2. Data <a name=\"data\"><\/a>","94615938":"## 6. Results and Discussion <a name=\"results\"><\/a>","bd46a788":"#### Use GridSearch method to figure the best hyper-parameters","e9b1d4ac":"#### Merge School Table and House Table","ee56fed8":"As a resident of the Greater Toronto Area (GTA) for more than 10 years, I'd like to choose GTA data as my Applied Data Science Capstone Project. The basic idea is using the **Venues Data** and **School Rankings** to predict **Housing Price**. Also, look into some **relationship** or **patterns** between different factors.\n\nToronto is the largest city in Canada, however, compare with those largest cities in the world, we have only 6 million people live in it, most of which lives and works in the south area. The population density and diversity are our unique features, that makes some of the parameters in our project needs to be customized.\n\nWhen we have all of the data and the models, we can create some visualization graphs to check those patterns and compare the performance among different models. We can also create a map to map each district is clustered according to the features like school ranking and venue density.","0d552b50":"### 5.5 XGBoost Regressor algorithm","116b8f32":"Define Foursquare Credentials and Version","51a88e9f":"#### Use try_different_method function to plot prediction graph","26becf25":"## 7. Conclusion <a name=\"conclusion\"><\/a>","0c631d58":"#### Manipulate House Dataset","8d4012e8":"### 5.2 Create Predict and Plot function for different Machine Learning Methods","ddd4f03c":"#### Change Column names of School Dataset","ccdc2460":"#### Load School Dataset from CSV file","33395b0b":"#### Read toronto_venues_1000 dataset from csv","f82f5e96":"Now, let's perform some basic explanatory data analysis and use Data Visualization method to determine some relationships between cleaned features and prices.","bcef83dc":"\n\n## 1. Introduction: Business Problem <a name=\"introduction\"><\/a>","1d124b9c":"#### Use try_different_method function to plot prediction graph","cc713e48":"group rows by neighborhood and by taking the mean of the frequency of occurrence of each category","3a46381e":"#### Use GridSearch method for Random Forest to figure the best hyper-parameters","b4c20fd6":"### 5.5 KNN algorithm","9894e5e9":"### 5.6 Use Keras deploy Nural networks","8214ddbb":"### 4.1 Use seaborn to plot heatmap","093b9e7e":"### 5.1 Create Training\/Testing Datasets and Performance Record Dataframe","82f9ca5a":"### 4.2 Plot scatter_matrix","ed19014c":"### 2.4 Group all data into one dataset","f19724f8":"#### Load House Dataset from CSV file","865e3d06":"The purpose of this project was using web scraping method and API to collect the related data of GTA including housing, school, Venus. Then use ETL method to get a clean version for analysis\/visualization, eventually deploy different machine learning methods both using Skitlearn and Keras.\n\nAs we can figure from the performance and analyst. Most of the features do prove there are some correlations between itself and price, especially put them together to get a decent prediction performance. However, as we mentioned, there are so many candidate features I couldn't get so far. Basic on my personal experience and relator business experience, I'm pretty sure we will have much better performance with more official data especially like housing size, land size, built year, management fee, security ranking, income, ages, rental rate, etc.\n","236cc7b1":"## Table of contents\n* [1. Introduction: Business Problem](#introduction)\n* [2. Data](#data)\n* [3. Methodology](#methodology)\n* [4. Data Analysis & Visualization](#Analysis&Visualization)\n* [5. Modeling](#Modeling)\n* [6. Results and Discussion](#results)\n* [7. Conclusion](#conclusion)","2af5fead":"#### Use GridSearch method to figure the best hyper-parameters","998b760f":"### 3.3 Explore the clusters distribution of different zones","e9f949e2":"### 3.1 Create correlation matrix","d8103b53":"### 5.3 Random Forest algorithm","e2616496":"### 2.2 Transform Data","2b4325e3":"### 2.3 Acquire Venues data through Foursquare","f983b6a2":"## 3. Methodology <a name=\"methodology\"><\/a>","aa676bcd":"### 4.3 Use binning method and matplotlib to plot some bar graphs","9c3164bd":"#### Import libraries","12dd4d03":"### 3.2 Remove the outliers ","50de4235":"## 4. Data Analysis & Visualization <a name=\"Analysis&Visualization\"><\/a>","4fe7fa11":"At first, I took me hours to build a quick draft model: with only the data I gathered from some realtor website using web scraping method, then use foursquare API to combine each of the house records with Venues Data. However, the accuracy of the performance does not look great. \n\nThe biggest issue is unlike in the US, here in Canada all of the sold price and most of the important detail information such as build year, sqrt, land size, property tax, are **not opened to the public**, so there's no good solution to get all of the data I required through open source.\n\nSo the next step is to try to find other related could affect the housing price, in order to do so, I went through other websites including official school ranking for 2017-2018 provided by Fraser Institute, since just as many of other parents, the ranking of the school is one of the biggest concerns when we purchased our own house. Also, I chose a listing website for an approximate size and type for each of the property.\n\nPlease note there's two way to get latitude and longitude information: I tried **Google API** and web scrapping, both worked.\n\nEventually, here the key datasets I used:\n* **School Ranking** Dataset - Obtained from the **Fraser Institute** website through **web scraping** methods\n    * The name of schools\n    * Cities of schools located\n    * 2017-18 Rating of Elementary Schools\n    * Postcodes of schools\n    * Addresses of schools\n\n* **Housing Info** Dataset - Obtained from **residences listing websites** through **web scraping** methods\n    * Addresses of houses\n    * Number of bedrooms for each house\n    * Number of bathrooms for each house\n    * Postcodes of houses\n    * Latitudes\n    * Longitudes\n    * The names of Neighborhood\n    * The type of the residences: House\/Townhouse\/Condo\n    * Listing prices\n    * Approximate sizes: Better than nothing, since I couldn't find an easy way to get the exact size of each residence\n\n* **Venues Data** Dataset - Obtained through **Foursquare API**\n    * Counts of Venues closed to the Neighborhood\n    * The frequency of each Venues Category such as Office, Bus Stop, Pizza Place, Coffee, Chinese Restaurant, Italian Restaurant, etc."}}