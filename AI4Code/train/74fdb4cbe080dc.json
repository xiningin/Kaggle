{"cell_type":{"c0ec7e2b":"code","ac6d42dd":"code","7e98a5c7":"code","3cc232c1":"code","2ec2141b":"code","56ca5272":"code","7eb63202":"code","30db630b":"code","df36edf0":"code","2cbdd9b3":"code","00bbad36":"code","0812dc1d":"code","61dad403":"code","c6481c9e":"code","58c9e440":"code","a26c7ad4":"code","6dbe9c78":"code","8b8dfd05":"code","c1c32bd2":"code","27cf685c":"code","a9182fbd":"code","cd459c5a":"code","7ecb7d2b":"code","b5065c1d":"code","83a76bef":"code","7a5d98a3":"code","324b5957":"code","f0b59a91":"code","196207c3":"code","aca56ce7":"code","1f514a34":"code","10f734c3":"code","de37d29d":"code","c409ce96":"code","af08281a":"code","5dff4892":"code","3c6cd207":"markdown","2c89f6d5":"markdown","1385b7ef":"markdown","cc6c152e":"markdown","739d23dd":"markdown","6c2ef1a3":"markdown"},"source":{"c0ec7e2b":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\nimport random as rd # generating random numbers\nimport datetime # manipulating date formats\n\nfrom tqdm import tqdm\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","ac6d42dd":"# Load sales\nsales=pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nsales.head()","7e98a5c7":"# you can use this tip to save some memory : #downcast data to save memory : from 134.4+ MB, we went to 61.6+ MB\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\nprint(\"-- mem before\", sales.info())\nsales = downcast_dtypes(sales)\nprint(\"-- mem after\", sales.info())\n","3cc232c1":"from dateutil.parser import parse\nsales['rdate'] = sales['date'].apply(lambda d: parse(d))","2ec2141b":"df = sales.groupby(by = 'item_id').agg({'rdate': [\"min\",\"max\"]}).reset_index()\ndf.head()","56ca5272":"df['days'] =  df.apply(lambda d : (d['rdate']['max'] - d['rdate']['min']).days   , axis = 1   )\n\ndf.head()","7eb63202":"def njours(item_id):\n    return len(sales[ (sales.item_id == item_id)   ].rdate.unique())\n\ndf['njours'] =  df.item_id.apply(lambda id : njours(id) )\ndf.head()","30db630b":"df.sort_values(by = ['njours', 'days'], ascending = [False, False],  inplace = True)\ndf.head()","df36edf0":"item_id = 5822\n# item_id = 12552\ndf.head(10)","2cbdd9b3":"item_ids = df.head(10).item_id.values\n\ndef build_ts(item_id):\n\n    df2 = sales[sales.item_id == item_id][['item_cnt_day', 'rdate']].groupby( by = [ 'rdate']  ).sum().reset_index()\n    ts = pd.Series(index = df2.rdate, data = df2.item_cnt_day.values)\n    return ts\n","00bbad36":"ts = build_ts(item_ids[2])\nts.plot(figsize= (18,6))","0812dc1d":"def exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n    \n\n    \ndef plotExponentialSmoothing(series, alphas):\n    \"\"\"\n        Plots exponential smoothing with different alphas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters\n        \n    \"\"\"\n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(18, 7))\n        plt.plot(series.values, \"c\", label = \"Actual\", alpha = 0.5)\n        for alpha in alphas:\n            plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Exponential Smoothing\")\n        plt.grid(True);\n        \n","61dad403":"ts = build_ts(item_ids[9])\nplotExponentialSmoothing(ts, [0.3, 0.05])\n","c6481c9e":"def double_exponential_smoothing(series, alpha, beta):\n    \"\"\"\n        series - dataset with timeseries\n        alpha - float [0.0, 1.0], smoothing parameter for level\n        beta - float [0.0, 1.0], smoothing parameter for trend\n    \"\"\"\n    # first value is same as series\n    result = [series[0]]\n    level_ = [series[0]]\n    trend_ = [series[0]]\n    for n in range(1, len(series)+1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series): # forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n        trend = beta*(level-last_level) + (1-beta)*trend\n        result.append(level+trend)\n        level_.append(level)\n        trend_.append(trend)\n    return result, level_, trend_\n\ndef plotDoubleExponentialSmoothing(series, alphas, betas):\n    \"\"\"\n        Plots double exponential smoothing with different alphas and betas\n        \n        series - dataset with timestamps\n        alphas - list of floats, smoothing parameters for level\n        betas - list of floats, smoothing parameters for trend\n    \"\"\"\n    \n    with plt.style.context('seaborn-white'):    \n        plt.figure(figsize=(20, 8))\n        for alpha in alphas:\n            for beta in betas:\n                r, l, t = double_exponential_smoothing(series, alpha, beta)\n                plt.plot(r, label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\", alpha = 0.5, linestyle = '-')\n                \n                \n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing\")\n        plt.grid(True)\n        \n        plt.figure(figsize=(20, 8))\n#         plt.plot(series.values, label = \"Actual\", alpha = 0.5, linestyle = '-')\n        for alpha in alphas:\n            for beta in betas:\n                r, l, t = double_exponential_smoothing(series, alpha, beta)\n                plt.plot(l, label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\", alpha = 0.5, linestyle = '-')\n                \n                \n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing - level\")\n        plt.grid(True)        \n\n        plt.figure(figsize=(20, 8))\n#         plt.plot(series.values, label = \"Actual\", alpha = 0.5, linestyle = '-')\n        for alpha in alphas:\n            for beta in betas:\n                r, l, t = double_exponential_smoothing(series, alpha, beta)\n                plt.plot(t, label=\"Alpha {}, beta {}\".format(alpha, beta))\n        plt.plot(series.values, label = \"Actual\", alpha = 0.5, linestyle = '-')\n                \n                \n        plt.legend(loc=\"best\")\n        plt.axis('tight')\n        plt.title(\"Double Exponential Smoothing - trend\")\n        plt.grid(True)        \n","58c9e440":"ts = build_ts(item_ids[0])\nplotDoubleExponentialSmoothing(ts, alphas=[0.9], betas=[0.9])\n","a26c7ad4":"r, l, t = double_exponential_smoothing(ts, 0.5, 0.5)\n","6dbe9c78":"l","8b8dfd05":"plotDoubleExponentialSmoothing(ts, alphas=[0.5, 0.02], betas=[0.9])","c1c32bd2":"ts = build_ts(item_ids[0])\n\nfrom statsmodels.tsa.api import SimpleExpSmoothing, ExponentialSmoothing,  Holt\n\nmdl = SimpleExpSmoothing(ts, initialization_method=\"estimated\")\nmdl.fit()\n\n\n","27cf685c":"plt.figure(figsize=(18, 8))\nplt.plot(ts)\nplt.plot(mdl.fittedvalues)\n# plt.plot(fcast)\n","a9182fbd":"mdl.params","cd459c5a":"fcast = mdl.forecast(12)\nfcast","7ecb7d2b":"\n# mdl = SimpleExpSmoothing.best_params()\n# print('Best Score: ', grid_result.best_score_)","b5065c1d":"ts = build_ts(item_ids[9])\n\nmdl = Holt(ts, initialization_method=\"estimated\", damped_trend=True).fit(optimized = True)\n\nprint(mdl.params)\n\n","83a76bef":"plt.figure(figsize=(18, 8))\nplt.plot(ts)\nplt.plot(mdl.fittedvalues)","7a5d98a3":"fcast = mdl.forecast(12)\nfcast","324b5957":"ts = build_ts(item_ids[0])\n","f0b59a91":"fit1 = ExponentialSmoothing(ts, seasonal_periods=7, trend='add', seasonal='add', use_boxcox=False, initialization_method=\"estimated\").fit()\nfit2 = ExponentialSmoothing(ts, seasonal_periods=7, trend='add', seasonal='mul', use_boxcox=False, initialization_method=\"estimated\").fit()\nfit3 = ExponentialSmoothing(ts, seasonal_periods=7, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit()\nfit4 = ExponentialSmoothing(ts, seasonal_periods=7, trend='add', seasonal='mul', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit()\nresults=pd.DataFrame(index=[r\"$\\alpha$\",r\"$\\beta$\",r\"$\\phi$\",r\"$\\gamma$\",r\"$l_0$\",\"$b_0$\",\"SSE\"])\nparams = ['smoothing_level', 'smoothing_trend', 'damping_trend', 'smoothing_seasonal', 'initial_level', 'initial_trend']\nresults[\"Additive\"]       = [fit1.params[p] for p in params] + [fit1.sse]\nresults[\"Multiplicative\"] = [fit2.params[p] for p in params] + [fit2.sse]\nresults[\"Additive Dam\"]   = [fit3.params[p] for p in params] + [fit3.sse]\nresults[\"Multiplica Dam\"] = [fit4.params[p] for p in params] + [fit4.sse]","196207c3":"results","aca56ce7":"fit1 = ExponentialSmoothing(ts, seasonal_periods=28, trend='add', seasonal='add', use_boxcox=False, initialization_method=\"estimated\").fit()\nfit2 = ExponentialSmoothing(ts, seasonal_periods=28, trend='add', seasonal='mul', use_boxcox=False, initialization_method=\"estimated\").fit()\nfit3 = ExponentialSmoothing(ts, seasonal_periods=28, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit()\nfit4 = ExponentialSmoothing(ts, seasonal_periods=28, trend='add', seasonal='mul', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit()\nresults=pd.DataFrame(index=[r\"$\\alpha$\",r\"$\\beta$\",r\"$\\phi$\",r\"$\\gamma$\",r\"$l_0$\",\"$b_0$\",\"SSE\"])\nparams = ['smoothing_level', 'smoothing_trend', 'damping_trend', 'smoothing_seasonal', 'initial_level', 'initial_trend']\nresults[\"Additive\"]       = [fit1.params[p] for p in params] + [fit1.sse]\nresults[\"Multiplicative\"] = [fit2.params[p] for p in params] + [fit2.sse]\nresults[\"Additive Dam\"]   = [fit3.params[p] for p in params] + [fit3.sse]\nresults[\"Multiplica Dam\"] = [fit4.params[p] for p in params] + [fit4.sse]\nresults","1f514a34":"fit1 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='add', use_boxcox=False, initialization_method=\"estimated\").fit()\nfit2 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='mul', use_boxcox=False, initialization_method=\"estimated\").fit()\nfit3 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit()\nfit4 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='mul', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit()\nresults=pd.DataFrame(index=[r\"$\\alpha$\",r\"$\\beta$\",r\"$\\phi$\",r\"$\\gamma$\",r\"$l_0$\",\"$b_0$\",\"SSE\"])\nparams = ['smoothing_level', 'smoothing_trend', 'damping_trend', 'smoothing_seasonal', 'initial_level', 'initial_trend']\nresults[\"Additive\"]       = [fit1.params[p] for p in params] + [fit1.sse]\nresults[\"Multiplicative\"] = [fit2.params[p] for p in params] + [fit2.sse]\nresults[\"Additive Dam\"]   = [fit3.params[p] for p in params] + [fit3.sse]\nresults[\"Multiplica Dam\"] = [fit4.params[p] for p in params] + [fit4.sse]\nresults","10f734c3":"fit1 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='add', use_boxcox=False, initialization_method=\"estimated\").fit(smoothing_trend = 0)\nfit2 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='mul', use_boxcox=False, initialization_method=\"estimated\").fit(smoothing_trend = 0)\nfit3 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit(smoothing_trend = 0)\nfit4 = ExponentialSmoothing(ts, seasonal_periods=365, trend='add', seasonal='mul', damped_trend=True, use_boxcox=False, initialization_method=\"estimated\").fit(smoothing_trend = 0)\nresults=pd.DataFrame(index=[r\"$\\alpha$\",r\"$\\beta$\",r\"$\\phi$\",r\"$\\gamma$\",r\"$l_0$\",\"$b_0$\",\"SSE\"])\nparams = ['smoothing_level', 'smoothing_trend', 'damping_trend', 'smoothing_seasonal', 'initial_level', 'initial_trend']\nresults[\"Additive\"]       = [fit1.params[p] for p in params] + [fit1.sse]\nresults[\"Multiplicative\"] = [fit2.params[p] for p in params] + [fit2.sse]\nresults[\"Additive Dam\"]   = [fit3.params[p] for p in params] + [fit3.sse]\nresults[\"Multiplica Dam\"] = [fit4.params[p] for p in params] + [fit4.sse]\nresults\n\n\n","de37d29d":"plt.figure(figsize=(18, 8))\nplt.plot(ts)\nplt.plot(fit1.fittedvalues)\n\n","c409ce96":"fig, ax = plt.subplots(1,1,figsize=(24,9))\n\nplt.plot(ts.values, color='black', alpha = 0.5)\nplt.plot(fit1.fittedvalues.values, marker='x', color='blue')\nfit1.forecast(24).rename('Holt-Winters (add-add-seasonal)').plot(ax=ax, style='--', marker='o', color='red', legend=True)\n\nplt.show()","af08281a":"fig, ax = plt.subplots(1,1,figsize=(24,9))\n\nplt.plot(ts.values, color='black', alpha = 0.5)\nplt.plot(fit3.fittedvalues.values, marker='x', color='blue')\nfit3.forecast(24).rename('Holt-Winters').plot(ax=ax, style='--', marker='o', color='red', legend=True)\n\nplt.show()","5dff4892":"fit1.fittedvalues","3c6cd207":"# Holt double exponential smoothing\n\n\n","2c89f6d5":"# TS des produits les plus r\u00e9pandus\n\npour chaque produit \n\n- voir les dates min \/ max \n- le nombre de dates avec des ventes\n- par jour\n- aggreger par semaine, par mois\n\n\n\n\n","1385b7ef":"# Double exponential smoothing\n","cc6c152e":"# Simple Exponential Smoothing\n\n\n$ \\hat{y}_{t+1} = \\alpha . y_t + (1 -\\alpha ) . \\hat{y}_{t} $\n","739d23dd":"# Holt Winter","6c2ef1a3":"# With statsmodels\n\n"}}