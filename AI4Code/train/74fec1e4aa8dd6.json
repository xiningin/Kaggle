{"cell_type":{"c029a45a":"code","4c807405":"code","bb14ecb3":"code","a5fb183f":"code","b24ef213":"code","464f5935":"code","b6e19dbe":"code","e7d2c6ad":"code","84b45e71":"code","8dacc607":"code","58c72ae7":"code","0191c26b":"code","4c8f95a4":"code","c8604753":"code","9392ac35":"code","b6b66c67":"code","66f9a3e3":"code","e7305e82":"code","f77689e8":"code","7907d599":"code","c9f91d6b":"code","52b3cc9d":"code","883831dc":"code","d51fbc0b":"code","d4b2562e":"code","1edb962b":"code","72ca4f35":"code","d47c8367":"code","b0d28759":"code","d30598d2":"markdown","54073cc6":"markdown","facf0bbe":"markdown","2ce2c619":"markdown","bff7109a":"markdown","545c330e":"markdown","3eaf1b2c":"markdown","5f0a6c69":"markdown","15f675d0":"markdown","43d44553":"markdown","11d77d56":"markdown","7cd6462f":"markdown","f22aa2c0":"markdown","c044c575":"markdown","bbdd336c":"markdown","13e53d60":"markdown","d2a1b4a9":"markdown","733b3aca":"markdown","1ce2c7db":"markdown","afdf9f0a":"markdown","2265df47":"markdown","89a2546e":"markdown","927f232a":"markdown","02dd47cd":"markdown","90d90586":"markdown","cf9e8ff7":"markdown"},"source":{"c029a45a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c807405":"# plottting lib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport xgboost\nfrom xgboost import XGBClassifier\n### pre-processing lib\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\nfrom sklearn. model_selection import train_test_split,GridSearchCV,KFold,cross_val_predict,RandomizedSearchCV\n### classification lib required\nfrom sklearn.ensemble import AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier,RandomForestClassifier,VotingClassifier,StackingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import KernelPCA,PCA\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier,RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.svm import SVC\n## different metrices\nfrom sklearn.metrics import accuracy_score,r2_score","bb14ecb3":"data=pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndata","a5fb183f":"data.describe()","b24ef213":"corelation=data.corr()\nplt.figure(figsize=(14,12))\nsns.heatmap(corelation,annot=True)","464f5935":"corelation['quality'].sort_values()","b6e19dbe":"corelation.columns","e7d2c6ad":"selected_features=['volatile acidity', 'citric acid','sulphates', 'alcohol','quality']","84b45e71":"feat_data=data[selected_features]\npd.plotting.scatter_matrix(feat_data,alpha=0.1,figsize=(10,10))\nplt.title('Scatter Matrix plot of selected features.')","8dacc607":"plt.figure(figsize=(10,10))\npd.plotting.radviz(feat_data,'quality')","58c72ae7":"#labels=feat_data.pop('quality')\n#X_train,X_test,y_train,y_test=train_test_split(feat_data,labels,test_size=0.2,random_state=42)","0191c26b":"labels=feat_data.pop('quality')\nX_train,X_test,y_train,y_test=train_test_split(data,labels,test_size=0.2,random_state=42)","4c8f95a4":"#X_train,y_train","c8604753":"svm=SVC(gamma='scale', probability=True)\nsvm.fit(X_train,y_train)\n\nsvm_pred=svm.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,svm_pred)\nprint(\"SVM accuracy is :{}\".format(score))","9392ac35":"random_f=RandomForestClassifier(n_estimators=250)\nrandom_f.fit(X_train,y_train)\nrandom_f_pred=random_f.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,random_f_pred)\nprint(\"random forest accuracy is :{}\".format(score))","b6b66c67":"log=LogisticRegression(solver='liblinear')\nlog.fit(X_train,y_train)\npred=log.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"LogisticRegression accuracy is :{}\".format(score))","66f9a3e3":"Decision=DecisionTreeClassifier()\nDecision.fit(X_train,y_train)\npred=Decision.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"DecisionTreeClassifier accuracy is :{}\".format(score))","e7305e82":"guassian=GaussianNB()\nguassian.fit(X_train,y_train)\npred=guassian.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"GaussianNB accuracy is :{}\".format(score))","f77689e8":"KNN=KNeighborsClassifier()\nKNN.fit(X_train,y_train)\npred=KNN.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"KNeighborsClassifier accuracy is :{}\".format(score))","7907d599":"Ada=AdaBoostClassifier()\nAda.fit(X_train,y_train)\npred=Ada.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"AdaBoostClassifier accuracy is :{}\".format(score))","c9f91d6b":"Bagging=BaggingClassifier(n_estimators=300)\nBagging.fit(X_train,y_train)\npred=Bagging.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"BaggingClassifier accuracy is :{}\".format(score))","52b3cc9d":"Ex_Tree=ExtraTreesClassifier(n_estimators=300)\nEx_Tree.fit(X_train,y_train)\npred=Ex_Tree.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"ExtraTreesClassifier accuracy is :{}\".format(score))","883831dc":"XGB=XGBClassifier()\nXGB.fit(X_train,y_train)\npred=XGB.predict(X_test)\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,pred)\nprint(\"XGBClassifier accuracy is :{}\".format(score))","d51fbc0b":"def get_models():\n\tmodels = list()\n\tmodels.append(LogisticRegression(solver='liblinear'))\n\tmodels.append(DecisionTreeClassifier())\n\tmodels.append(SVC(gamma='scale', probability=True))\n\tmodels.append(GaussianNB())\n\tmodels.append(KNeighborsClassifier())\n\tmodels.append(AdaBoostClassifier())\n\tmodels.append(BaggingClassifier(n_estimators=10))\n\tmodels.append(RandomForestClassifier(n_estimators=10))\n\tmodels.append(ExtraTreesClassifier(n_estimators=10))\n\tmodels.append(XGBClassifier())\n\treturn models","d4b2562e":"!pip install mlens","1edb962b":"import mlens\nfrom mlens.ensemble import SuperLearner\ndef get_super_learner(X):\n\tensemble = SuperLearner(scorer=accuracy_score, folds=10, shuffle=True, sample_size=len(X))\n\t# add base models\n\tmodels = get_models()\n\tensemble.add(models)\n\t# add the meta model\n\tensemble.add_meta(LogisticRegression(solver='lbfgs'))\n\treturn ensemble","72ca4f35":"ensemble = get_super_learner(X_train)","d47c8367":"# fit the super learner\nensemble.fit(X_train.values,y_train.values)\n# summarize base learners\nprint(ensemble.data)\n\n# make predictions on hold out set\n## here i face error with an pandas dataframe input hence i convert it into numpy array \n##may be mlens lib still not support direct pipeline of pandas dataframe\n## may be they will fix this issue further :)\n\nyhat = ensemble.predict(X_test.values)\n\nprint(\"*\"* 30)\nscore=accuracy_score(y_test,yhat)\nprint(\"Super Learner accuracy is :{}\".format(score))","b0d28759":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, yhat))","d30598d2":"* **Here i got an result of almost 99% hence we can consider this an significant result**\n* **here is no overfitting happening and we can clearly see that with using super learner techinqe we can obtain significant results.**\n\n### **Note: this notebook goal is not achieve maximum result.. it is for learning and understanding the new concept and its utilization**","54073cc6":"### Extreme gradient boosting classifier.","facf0bbe":"1. Define function in which all the model you want to use\n2. put them into and single list as shown below return it.","2ce2c619":"### Random Forest Classifier","bff7109a":"## Super-learner model ","545c330e":"import mlens and define super learners.\n\nhere base learner means this model learn something and based on this models output we are going to train our original model. basically its like public poll on the social topics or any topic this is an base idea behind this concept.","3eaf1b2c":"lets start with first our fav. classifier which is *svm*","5f0a6c69":"# Thank You for opening this notebook!!!\n\n## This notebook is for introduction to the MLens library\n In this notebook i have performed feature seleaction, Correlational plot, trained custom models like e.g. svm,Randomforest,etc.\n \n1. Support Vector Classifier(SVC)\n2. Random Forest Classifier\n3. LogisticRegression\n4. DecisionTreeClassifier\n5. GaussianNB\n6. KNeighborsClassifier\n7. AdaBoostClassifier\n8. BaggingClassifier  \n9. ExtraTreesClassifier\n10. Gradient boosting classifer\n    ","15f675d0":"## **please...if you have read this notebook upto this point  and if you like make sure to upvote this notebook so that it will give some boost to work hard****","43d44553":"### Simple feature Selection we are going to perform ","11d77d56":"upto this point we created simple models lets combine them so that they can act as an base model and from that we can train our meta model so that we can get significant amount of result.","7cd6462f":"### lets split out data ","f22aa2c0":"### support vector machine classifier","c044c575":"## lets see how much accuracy we get through some normal classifiers","bbdd336c":"### Logistic Regression","13e53d60":"### k-nearest neighbors Classifier","d2a1b4a9":"## importing data ","733b3aca":"### AdaBoost Classifier","1ce2c7db":"### co-relation plot for seeing how much data corelated to predicting label","afdf9f0a":"### ExtraTrees Classifier","2265df47":"### Decision Tree Classifier","89a2546e":"### BaggingClassifier","927f232a":"### i have commented below lines beacuase i am using feature selection but it eventually giving less accuracy and f1-score hence i consider taking full dataset with all columns and rows","02dd47cd":"### if you have face any error regarding mlens.\n### make sure you have install mlens lib using \n> !pip install mlens\n> ","90d90586":"### see variance, S.D. and mean so that we can think about normalization or not ","cf9e8ff7":"### Nearest Neighbour classifier "}}