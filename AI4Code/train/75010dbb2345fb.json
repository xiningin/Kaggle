{"cell_type":{"eabe9725":"code","f157676d":"code","7cad0a87":"code","36dcba8c":"markdown","eeca6888":"markdown","198f09ab":"markdown"},"source":{"eabe9725":"import numpy as np\nimport pandas as pd\nimport os\nimport scipy as sp\nfrom functools import partial\nfrom sklearn import metrics\nfrom collections import Counter\nimport json","f157676d":"# put numerical value to bins\ndef to_bins(x, tresholds):\n    if x <= tresholds[0]:\n        return 0\n    for i in range(1, len(tresholds)):\n        if x > tresholds[i - 1] and x <= tresholds[i]:\n            return i\n    return len(tresholds)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _loss(self, coef, X, y, idx):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        ll = -metrics.cohen_kappa_score(y, X_p, weights='quadratic')\n        return ll\n\n    def fit(self, X, y):\n        coefs = []\n        nsplits = 4\n        for split_i in range(nsplits):\n            coef = [1.5, 2.0, 2.5, 3.0]\n            golden1 = 0.618\n            golden2 = 1 - golden1\n            ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n            for it1 in range(10):\n                for idx in range(4):\n                    # golden section search\n                    a, b = ab_start[idx]\n                    # calc losses\n                    coef[idx] = a\n                    la = self._loss(coef, X[split_i::nsplits], y[split_i::nsplits], idx)\n                    coef[idx] = b\n                    lb = self._loss(coef, X[split_i::nsplits], y[split_i::nsplits], idx)\n                    for it in range(20):\n                        # choose value\n                        if la > lb:\n                            a = b - (b - a) * golden1\n                            coef[idx] = a\n                            la = self._loss(coef, X[split_i::nsplits], y[split_i::nsplits], idx)\n                        else:\n                            b = b - (b - a) * golden2\n                            coef[idx] = b\n                            lb = self._loss(coef, X[split_i::nsplits], y[split_i::nsplits], idx)\n            coefs.append(coef)\n        coef = list(np.array(coefs).mean(axis=0))\n        self.coef_ = {'x': coef}\n    \n    def predict(self, X, coef):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x'] ","7cad0a87":"optR = OptimizedRounder()\noptR.fit(valid_predictions, targets)\ncoefficients = optR.coefficients()\nvalid_predictions = optR.predict(valid_predictions, coefficients)\ntest_predictions = optR.predict(test_predictions, coefficients)","36dcba8c":"In case of any questions, feel free to ask! :)","eeca6888":"In this kernel I present my version of Abishek's optimizer https:\/\/www.kaggle.com\/abhishek\/optimizer-for-quadratic-weighted-kappa  \nIn my experiments this gives a bit better CV and LB scores.\n\nThe difference is that it is based on golden-section method https:\/\/en.wikipedia.org\/wiki\/Golden-section_search instead of Nelder-Mead.  \nI believe that Nelder-Mead stops converging too early because the optimized function is not continious (due to finite dataset).  \nFirst-order methods (such as GS) are usually slower, but they don't care about the smoothness of the function.  \nSo, GS doesn't get stuck on a plato.  ","198f09ab":"How to use? (this class is compatible with original one)"}}