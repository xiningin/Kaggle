{"cell_type":{"4790bf69":"code","4889702b":"code","49e46b19":"code","d19ceb97":"code","6d8a8013":"code","90ca573e":"code","4455db0f":"code","b33692cd":"code","b713b4db":"code","45bfd6fc":"code","a418fdc3":"code","72e8443f":"code","eac9c266":"code","4d728fa1":"code","ff881296":"code","766eb329":"code","f8ee1794":"code","e8b060b4":"code","80131ef2":"code","f13f4537":"code","9ade12fc":"code","88e26cfc":"code","6b1bbb0c":"code","4685f821":"code","1d6edf76":"code","02db6d14":"code","db91452e":"code","ce39e64b":"code","77d2682f":"code","b3b874fa":"code","e43ea3ce":"code","d5c53a81":"code","810cd705":"code","d97c938f":"code","1ba84a08":"code","44e00f57":"code","409915d4":"code","c7715d93":"code","8199586a":"code","4bdb9b71":"code","d6626df6":"code","4fef2fb0":"code","bb48e74e":"code","6b0c3386":"code","a8614bad":"code","d8392c5d":"code","1eab2b38":"code","406ef699":"markdown","91a3b389":"markdown","bd94958a":"markdown","2e48af41":"markdown","7de60afa":"markdown","5aa056e3":"markdown","311b365b":"markdown","fca42c30":"markdown","82321527":"markdown","48160ce1":"markdown","d9791410":"markdown","53e306f0":"markdown","ee51081e":"markdown","f68ac10f":"markdown","deeb6911":"markdown","6adbded9":"markdown","f4a24ec2":"markdown","1c627b17":"markdown","ab612cfe":"markdown","e93bb937":"markdown","5687d73d":"markdown","0d138af3":"markdown","1e9a6db8":"markdown","766f7367":"markdown","d78f4886":"markdown","4de6d40b":"markdown","2b088758":"markdown","1003186d":"markdown","ea801457":"markdown","80eab3c9":"markdown"},"source":{"4790bf69":"# Basic Operation\nimport pandas as pd\nimport numpy as np\n\n# Text Preprocessing & Cleaning\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport re\n\n\nfrom sklearn.model_selection import train_test_split # Split Data \nfrom imblearn.over_sampling import SMOTE # Handling Imbalanced\n\n# Model Building\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score # Performance Metrics  \n\n\n# Data Visualization \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom termcolor import cprint\nimport seaborn as sns\nimport warnings   \n\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4889702b":"df=pd.read_csv('\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv')","49e46b19":"df.head()","d19ceb97":"df.tail()","6d8a8013":"df.info()","90ca573e":"df.describe()","4455db0f":"df.shape","b33692cd":"df.isnull().sum()","b713b4db":"cprint(\"Total number of sentiments of tweets :\",'green')\nprint(df.airline_sentiment.value_counts())\nplt.figure(figsize = (10, 8))\nax = sns.countplot(x = 'airline_sentiment', data = df)\nax.set_title(label = 'Total number of sentiments of tweets', fontsize = 20)\nplt.show()","45bfd6fc":"ax.set_title(label = 'Total number of sentiments of tweets:')\ncolors=sns.color_palette('husl',10)\npd.Series(df['airline_sentiment']).value_counts().plot(kind='pie',colors=colors,labels=['Negative','Neutral','Postive'],explode=[0.05,0.02,0.04],shadow=True,autopct='%.2f',fontsize=12,figsize=(6,6),title=\"Total Tweets for Each Sentiment\")\n\nplt.show()","a418fdc3":"colors=sns.color_palette('husl',10)\npd.Series(df['airline']).value_counts().plot(kind=\"bar\",color=colors,figsize=(10,8),fontsize=10,rot=0,title='Total No. of Tweets of Airline')\nplt.xlabel('Airline',fontsize=10)\nplt.ylabel('No.of Tweets',fontsize=10)","72e8443f":"cprint(\"Total number of tweets for each airline :\",'green')\nprint(df.groupby('airline')['airline_sentiment'].count())\n\nplt.figure(figsize = (10, 8))\nax = sns.countplot(x = 'airline', data = df, palette = 'pastel')\nax.set_title(label = 'Total number of tweets for each airline', fontsize = 20)\nplt.show()\n\ncprint(\"Total number of sentiment tweets for each airline :\",'green')\nairlines= ['US Airways','United','American','Southwest','Delta','Virgin America']\nfor i in airlines :\n    print('{} : \\n'.format(i),df.loc[df.airline == i].airline_sentiment.value_counts())","eac9c266":"cprint('Reasons Of Negative Tweets :','green')\nprint(df.negativereason.value_counts())\n\nplt.figure(figsize = (24, 12))\nsns.countplot(x = 'negativereason', data = df, palette = 'hls')\nplt.title('Reasons Of Negative Tweets About Airlines', fontsize = 20)\nplt.show()","4d728fa1":"NR_Count=df['negativereason'].value_counts()\ndef NCount(Airline):\n    airlineName =df[df['airline']==Airline]\n    count= airlineName['negativereason'].value_counts()\n    Unique_reason= df['negativereason'].unique()\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame\n\ndef Plot_Reason(airline):\n    a= NCount(airline)\n    count=a['count']\n    Id = range(1,(len(a)+1))\n    plt.bar(Id,count, color=['darkviolet','yellow','blue','lime','pink','crimson','gold','cyan','orange','purple'])\n    plt.xticks(Id,a['Reasons'],rotation=90)\n    plt.title('Count of Reasons for '+ airline)\n    \nplt.figure(2,figsize=(16, 14))\nfor i in airlines:\n    indices= airlines.index(i)\n    plt.subplot(2,3,indices+1)\n    plt.subplots_adjust(hspace=0.9)\n    Plot_Reason(i)","ff881296":"positive=df[df['airline_sentiment']=='positive'].text\nneutral=df[df['airline_sentiment']=='neutral'].text\nnegative=df[df['airline_sentiment']=='negative'].text","766eb329":"plt.figure(figsize=(24,20))\nworld_cloud_postive=WordCloud(min_font_size=3,max_words=3200,width=1600,height=720).generate(\"\".join(positive))\nplt.imshow(world_cloud_postive,interpolation='bilinear')\nax.grid(False)","f8ee1794":"plt.figure(figsize=(24,12))\nworld_cloud_neutral=WordCloud(min_font_size=3,max_words=3200,width=1600,height=720).generate(\" \".join(neutral))\nplt.imshow(world_cloud_neutral,interpolation='bilinear')\nax.grid(False)","e8b060b4":"plt.figure(figsize = (24,12)) \nworldcould_neg = WordCloud(min_font_size = 3,  max_words = 3200 , width = 1600 , height = 720).generate(\" \".join(negative))\nplt.imshow(worldcould_neg,interpolation = 'bilinear')\nax.grid(False)","80131ef2":"# convert Sentiments to 0,1,2\ndef convert_Sentiment(sentiment):\n    if  sentiment == \"positive\":\n        return 2\n    elif sentiment == \"neutral\":\n        return 1\n    elif sentiment == \"negative\":\n        return 0","f13f4537":"# Apply convert_Sentiment function\ndf.airline_sentiment = df.airline_sentiment.apply(lambda x : convert_Sentiment(x))","9ade12fc":"df.airline_sentiment","88e26cfc":"# Remove stop words\ndef remove_stopwords(text):\n    text = ' '.join([word for word in text.split() if word not in (stopwords.words('english'))])\n    return text\n\n# Remove url  \ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# Remove punct\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Remove html \ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# Remove @username\ndef remove_username(text):\n    return re.sub('@[^\\s]+','',text)\n\n# Remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n# Decontraction text\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text  \n\n# Seperate alphanumeric\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ndef char(text):\n    substitute = re.sub(r'[^a-zA-Z]',' ',text)\n    return substitute\n\n# combaine negative reason with  tweet (if exsist)\ndf['final_text'] = df['negativereason'].fillna('') + ' ' + df['text'] \n\n\n# Apply functions on tweets\ndf['final_text'] = df['final_text'].apply(lambda x : remove_username(x))\ndf['final_text'] = df['final_text'].apply(lambda x : remove_url(x))\ndf['final_text'] = df['final_text'].apply(lambda x : remove_emoji(x))\ndf['final_text'] = df['final_text'].apply(lambda x : decontraction(x))\ndf['final_text'] = df['final_text'].apply(lambda x : seperate_alphanumeric(x))\ndf['final_text'] = df['final_text'].apply(lambda x : unique_char(cont_rep_char,x))\ndf['final_text'] = df['final_text'].apply(lambda x : char(x))\ndf['final_text'] = df['final_text'].apply(lambda x : x.lower())\ndf['final_text'] = df['final_text'].apply(lambda x : remove_stopwords(x))","6b1bbb0c":"# result\ndf['final_text']","4685f821":"X = df['final_text']\ny = df['airline_sentiment']","1d6edf76":"tfid = TfidfVectorizer()\nX_final =  tfid.fit_transform(X)","02db6d14":"# Handling imbalanced using SMOTE\nsmote = SMOTE()\nx_sm,y_sm = smote.fit_resample(X_final,y)","db91452e":"X_train , X_test , y_train , y_test = train_test_split(x_sm , y_sm , test_size=0.25,random_state=3)","ce39e64b":"random_forest_classifier = RandomForestClassifier()\nrandom_forest_classifier.fit(X_train,y_train)","77d2682f":"random_forest_classifier_prediction =  random_forest_classifier.predict(X_test)","b3b874fa":"accuracy_score(random_forest_classifier_prediction,y_test)","e43ea3ce":"xgb = XGBClassifier()\nxgb.fit(X_train,y_train)","d5c53a81":"xgb_prediction =  xgb.predict(X_test)","810cd705":"accuracy_score(xgb_prediction,y_test)","d97c938f":"gbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)","1ba84a08":"gbc_prediction =  gbc.predict(X_test)","44e00f57":"accuracy_score(gbc_prediction,y_test)","409915d4":"svm = SVC()\nsvm.fit(X_train,y_train)","c7715d93":"svm_prediction =  svm.predict(X_test)","8199586a":"accuracy_score(svm_prediction,y_test)","4bdb9b71":"nb = MultinomialNB()\nnb.fit(X_train,y_train)","d6626df6":"nb_prediction =  nb.predict(X_test)","4fef2fb0":"accuracy_score(nb_prediction,y_test)","bb48e74e":"des_tree_classifier = DecisionTreeClassifier()\ndes_tree_classifier.fit(X_train,y_train)","6b0c3386":"des_tree_classifier_prediction=des_tree_classifier.predict(X_test)","a8614bad":"accuracy_score(des_tree_classifier_prediction,y_test)","d8392c5d":"cr = classification_report(y_test, random_forest_classifier_prediction)","1eab2b38":"print(\"Classification Report:\\n----------------------\\n\", cr)\n\ncm = confusion_matrix(y_test,random_forest_classifier_prediction)\n\n\n# plot confusion matrix \nplt.figure(figsize=(10,6))\nsentiment_classes = ['Negative', 'Neutral', 'Positive']\nsns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n            xticklabels=sentiment_classes,\n            yticklabels=sentiment_classes)\nplt.title('Confusion matrix', fontsize=16)\nplt.xlabel('Actual label', fontsize=12)\nplt.ylabel('Predicted label', fontsize=12)\nplt.show()","406ef699":"# Now Check the Head and Tail of the dataset","91a3b389":"# HANDLING IMBALANCE","bd94958a":"# DATA VISUALIZATION\n\n","2e48af41":"# Now World Cloud of Postive Sentiments","7de60afa":"# Naive Bayes\n","5aa056e3":"# Now Support vector machine Accuracy Score","311b365b":"# Support vector machine","fca42c30":"# Now Apply TFIDF on cleaned tweets","82321527":"# Now XGBClassifier Accuracy Score","48160ce1":"# Gradient Boosting Classifier","d9791410":"# TEXT PREPROCESSING AND CLEANING","53e306f0":"# World could of Negative sentiments","ee51081e":"# Now Check Nan value in our dataset","f68ac10f":"# VISUALIZE MODEL PERFORMENCE","deeb6911":"# Now Reasons Of Negative Tweets","6adbded9":"# # Split Data into train & test ","f4a24ec2":"# Now XGBClassifier","1c627b17":"# Now Random Forest","ab612cfe":"# IMPORT LIBRARIES","e93bb937":"# Load The Data","5687d73d":"# Now Reasons Of Negative Tweets on Every AirLine Company","0d138af3":"# Now Randome Forest Accuracy Score","1e9a6db8":"# World could of Neutral sentiments","766f7367":"# Split Text Of Sentiments","d78f4886":"# Now Decision Tree Classifier Accuracy Score","4de6d40b":"# Decision Tree","2b088758":"# EDA Part","1003186d":"Now Gradient Boosting Classifier Accuracy Score","ea801457":"# Now Naive Bayes Accuracy Score","80eab3c9":"# Thanks You!!!"}}