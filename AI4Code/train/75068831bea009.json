{"cell_type":{"00163876":"code","7e1078b3":"code","ca6cd4f5":"code","125a690d":"code","3400a066":"code","793d6e90":"code","a7f0cd46":"code","c30cfc7b":"code","b7a8ac2c":"code","51e86518":"code","09848778":"code","e53cd455":"code","0d581f33":"code","3186a48d":"code","3268205a":"code","db14c220":"code","0f36c7ac":"code","2be8834b":"code","46c7af70":"code","de4950dc":"code","ca0e6c34":"code","7d54b1bc":"code","ecb8a42f":"code","0243fa08":"code","3dd6e63c":"code","89bfbd87":"code","92bf77a3":"code","f9b1634a":"code","d052eeac":"markdown","c1af424d":"markdown"},"source":{"00163876":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ninput_dir = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e1078b3":"train = pd.read_csv(f\"{input_dir}\/train.csv\")\ntest = pd.read_csv(f\"{input_dir}\/test.csv\")\n\ntest_ids = test.Id","ca6cd4f5":"train.head()\ntest.head()","125a690d":"all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))","3400a066":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\nprices.hist()","793d6e90":"#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","a7f0cd46":"all_data = pd.get_dummies(all_data)\nall_data.head()","c30cfc7b":"#filling NA's with the mean of the column:\nall_data = all_data.fillna(all_data.mean())","b7a8ac2c":"#creating matrices for sklearn:\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice","51e86518":"from sklearn.linear_model import Ridge, Lasso, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","09848778":"# Train and evaluate Ridge model\nmodel_ridge = Ridge(alpha=0.1)\nmodel_ridge.fit(X_train, y)\ny_pred = model_ridge.predict(X_test)\nprint(y_pred)\ny_pred = np.expm1(y_pred)\ny_pred\nX_test.index = X_test.reset_index().index+1461\nsub_df = pd.DataFrame({\"Id\":X_test.index, \"SalePrice\":y_pred})\nsub_df.SalePrice.hist()\nsub_df.to_csv(\"\/kaggle\/working\/sub1.csv\", index=False)\n##First submission RMSE = 0.13565","e53cd455":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Ridge Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.show()\nprint(f\"Best: a={cv_ridge.index[cv_ridge.argmin()]}, rmse={cv_ridge.min()}\")","0d581f33":"alphas = [0.0002,0.0005,0.001,0.005, 0.01,0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_lasso = [rmse_cv(Lasso(alpha = alpha)).mean() \n            for alpha in alphas]\ncv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_lasso.plot(title = \"lasso Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.show()\nprint(f\"Best: a={cv_lasso.index[cv_lasso.argmin()]}, rmse={cv_lasso.min()}\")","3186a48d":"## Part 4\ncoeffs = [Lasso(alpha = alpha).fit(X_train, y).coef_ for alpha in alphas]\nl0_norms = np.count_nonzero(coeffs, axis=1)\nprint(l0_norms)\nplt.plot(l0_norms, alphas)\nplt.title('l0 norm for varying alphas')\nplt.xlabel('alpha')\nplt.ylabel('l0 norm')\nplt.xlim(0, 75)\nplt.show()","3268205a":"## Part 5\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.pipeline import make_pipeline\nestimators = [\n    ('Ridge Regression', Ridge(alpha=0.1)),\n    ('Lasso', Lasso(alpha=0.0005))\n]\n\nregr = StackingRegressor(estimators=estimators, final_estimator=Ridge())\nregr.fit(X_train, y)\ny_pred_stack = regr.predict(X_test)\nprint(y_pred_stack)\ny_pred_stack = np.expm1(y_pred_stack)\ny_pred_stack\nX_test.index = X_test.reset_index().index+1461\nsub_df = pd.DataFrame({\"Id\":X_test.index, \"SalePrice\":y_pred_stack})\nsub_df.SalePrice.hist()\nsub_df.to_csv(\"\/kaggle\/working\/sub2.csv\", index=False)\n\n# second submission RMSE: 0.12486","db14c220":"## Part 6\nimport xgboost as xgb\n\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n\ndtrain = xgb.DMatrix(X_train, label = y)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1)\nmodel_xgb.fit(X_train, y)\n\nxgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\n\npreds = 0.8*lasso_preds + 0.2*xgb_preds\n\nsolution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"xgb_sol.csv\", index = False)\n\n## Score Submission: 0.12315","0f36c7ac":"## Part 7\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1)\nmodel_xgb.fit(X_train, y)\n\nxgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\n\npreds = 0.7*lasso_preds + 0.3*xgb_preds\n\nsolution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"optimized_sol.csv\", index = False)\n\n## Score Submission: 0.12299","2be8834b":"##Overfit model\nimport xgboost as xgb\n\nmodel_lasso = LassoCV(alphas = [0.000000000001]).fit(X_train, y)\n\ndtrain = xgb.DMatrix(X_train, label = y)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1)\nmodel_xgb.fit(X_train, y)\n\nxgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\n\npreds = 0.8*lasso_preds + 0.2*xgb_preds\n\nsolution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"xgb_overfit.csv\", index = False)\n\n## Score Submission: 0.13586","46c7af70":"##Underfit model\nimport xgboost as xgb\n\nmodel_lasso = LassoCV(alphas = [0.8]).fit(X_train, y)\n\ndtrain = xgb.DMatrix(X_train, label = y)\ndtest = xgb.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n\nmodel_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1)\nmodel_xgb.fit(X_train, y)\n\nxgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))\n\npreds = 0.8*lasso_preds + 0.2*xgb_preds\n\nsolution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"xgb_underfit.csv\", index = False)\n\n## Score Submission: 0.22830","de4950dc":"X = all_data.iloc[:train.shape[0], :]\ny = train.iloc[:, -1].tolist()\n\ntest = all_data.iloc[train.shape[0]:, :]","ca0e6c34":"# Setup cross validation folds\nfrom sklearn.model_selection import KFold, cross_val_score\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","7d54b1bc":"# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","ecb8a42f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","0243fa08":"from lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.metrics import r2_score\n\nMLA = [\n      # Light Gradient Boosting Regressor\n      LGBMRegressor(objective='regression'),\n\n      # XGBoost Regressor\n      XGBRegressor(objective='reg:squarederror'),\n\n      # Gradient Boosting Regressor\n      GradientBoostingRegressor(),\n\n      # Random Forest Regressor\n      RandomForestRegressor(),\n       \n      # CatBoost Regressor\n      CatBoostRegressor()\n]","3dd6e63c":"row_index = 0\nMLA_compare = pd.DataFrame()\n\nfor classifier in MLA:\n  classifier.fit(X_train, y_train)\n  y_pred = classifier.predict(X_test)\n\n  k_fold_accuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, scoring=\"r2\").mean()*100\n\n  MLA_name = classifier.__class__.__name__\n  MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n  MLA_compare.loc[row_index, 'Normal R2 Score'] = r2_score(y_test, y_pred)\n  MLA_compare.loc[row_index, 'K-Fold R2 Score'] = k_fold_accuracy\n\n  row_index+=1","89bfbd87":"MLA_compare = MLA_compare.sort_values(\"K-Fold R2 Score\", ascending=False).reset_index(drop=True)\nMLA_compare","92bf77a3":"cbr = CatBoostRegressor()\ncbr.fit(X_train, y_train)\n\ny_pred = cbr.predict(X_test)\nr2_score(y_test, y_pred)","f9b1634a":"test_data_pred = cbr.predict(test)\noutput = pd.DataFrame({'id': test_ids, 'SalePrice': test_data_pred})\noutput.to_csv('catboost&kfold.csv', index=False)\n\n## Score Submission: 0.12815","d052eeac":"Ridge Regression","c1af424d":"Data Preprocessing"}}