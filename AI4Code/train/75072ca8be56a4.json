{"cell_type":{"b087f78d":"code","3e427455":"code","669935ce":"code","b8652041":"code","37f653ce":"code","e9a3f955":"code","d569b3bc":"code","b103dc53":"code","de061503":"code","21163eb7":"code","6e45dfe2":"code","9743fc66":"code","44efb3eb":"code","ecfa3c98":"code","465b3a40":"code","4226410f":"code","66a5bbfc":"code","9d5131cf":"code","0f4c1ad6":"code","ca966667":"code","0c8109ca":"code","3261d0dc":"code","c43ad965":"code","a362d257":"code","4fc81e0d":"code","7aff989d":"code","f0345adc":"code","c706cfa5":"code","08687a97":"code","aebe9303":"code","dc64a4c6":"code","487ecc23":"code","c410df95":"code","899506fd":"markdown","79df4159":"markdown","d3225b6c":"markdown","8e394380":"markdown","76f5b2bd":"markdown","3726b302":"markdown","861a21b7":"markdown","09a13c49":"markdown","19db1429":"markdown","ac544e52":"markdown","c6888db2":"markdown","894c5823":"markdown","f0c768fe":"markdown","1148e728":"markdown","7e34ff5c":"markdown","4881ac4a":"markdown","94b302f1":"markdown","7cd22469":"markdown","fe243acc":"markdown","762519c1":"markdown","7da1ca4a":"markdown","cf155361":"markdown","95f33be4":"markdown","644a9917":"markdown","3880f1df":"markdown","99a67d43":"markdown","7501b998":"markdown","69b0669e":"markdown","d79430f5":"markdown","61009abe":"markdown","6e5a9dde":"markdown","aaa41a29":"markdown"},"source":{"b087f78d":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning\n\nimport numpy as np\nimport random\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport plotly.express as px\n\n!pip uninstall -y transformers\n!pip install transformers\n\nimport transformers\nimport tokenizers\n\n# Hugging Face new library for datasets (https:\/\/huggingface.co\/nlp\/)\n!pip install nlp\nimport nlp\n\nimport datetime\n\nstrategy = None\n\ndef seed_all(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nseed_all(2020)","3e427455":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","669935ce":"original_train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\n\noriginal_train = shuffle(original_train)\noriginal_valid = original_train[:len(original_train) \/\/ 5]\noriginal_train = original_train[len(original_train) \/\/ 5:]","b8652041":"print(f\"original - training: {len(original_train)} examples\")\noriginal_train.head(10)","37f653ce":"print(f\"original - validation: {len(original_valid)} examples\")\noriginal_valid.head(10)","e9a3f955":"original_test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\nprint(f\"original - test: {len(original_test)} examples\")\noriginal_test.head(10)","d569b3bc":"mnli = nlp.load_dataset(path='glue', name='mnli')","b103dc53":"print(mnli, '\\n')\n\nprint('The split names in MNLI dataset:')\nfor k in mnli:\n    print('   ', k)\n    \n# Get the datasets\nprint(\"\\nmnli['train'] is \", type(mnli['train']))\n\nmnli['train']","de061503":"print('The number of training examples in mnli dataset:', mnli['train'].num_rows)\nprint('The number of validation examples in mnli dataset - part 1:', mnli['validation_matched'].num_rows)\nprint('The number of validation examples in mnli dataset - part 2:', mnli['validation_mismatched'].num_rows, '\\n')\n\nprint('The class names in mnli dataset:', mnli['train'].features['label'].names)\nprint('The feature names in mnli dataset:', list(mnli['train'].features.keys()), '\\n')\n\nfor elt in mnli['train']:\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', mnli['train'].features['label'].names[elt['label']])\n    print('idx', elt['idx'])\n    print('-' * 80)\n    \n    if elt['idx'] >= 10:\n        break","21163eb7":"snli = nlp.load_dataset(path='snli')\n\nprint('The number of training examples in snli dataset:', snli['train'].num_rows)\nprint('The number of validation examples in snli dataset:', snli['validation'].num_rows, '\\n')\n\nprint('The class names in snli dataset:', snli['train'].features['label'].names)\nprint('The feature names in snli dataset:', list(snli['train'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(snli['train']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', snli['train'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 10:\n        break","6e45dfe2":"xnli = nlp.load_dataset(path='xnli')\n\nprint('The number of validation examples in xnli dataset:', xnli['validation'].num_rows, '\\n')\n\nprint('The class names in xnli dataset:', xnli['validation'].features['label'].names)\nprint('The feature names in xnli dataset:', list(xnli['validation'].features.keys()), '\\n')\n\nfor idx, elt in enumerate(xnli['validation']):\n    \n    print('premise:', elt['premise'])\n    print('hypothesis:', elt['hypothesis'])\n    print('label:', elt['label'])\n    print('label name:', xnli['validation'].features['label'].names[elt['label']])\n    print('-' * 80)\n    \n    if idx >= 3:\n        break","9743fc66":"def _get_features(elt):\n    '''\n    Args:\n        elt: elements of a `nlp.arrow_dataset.Dataset` that we have seen above\n    \n    Yields: tuples of 3 elements: (premise, hypothesis, language)\n    '''\n\n    if type(elt) == pd.core.series.Series:\n        yield (elt['premise'], elt['hypothesis'], elt['lang_abv'])    \n    \n    elif type(elt['premise']) == str:  \n        yield (elt['premise'], elt['hypothesis'], 'en')\n    \n    elif type(elt) == dict:\n        \n        # dict of strings\n        premises = elt['premise']\n        \n        # dict of lists\n        hypotheses_dict = elt['hypothesis']\n        \n        # lists\n        langs = hypotheses_dict['language']\n        translations = hypotheses_dict['translation']\n        \n        hypotheses = {k: v for k, v in zip(langs, translations)}\n                \n        for lang in elt['premise']:\n            if lang in hypotheses:\n                yield (elt['premise'][lang], hypotheses[lang], lang)\n        \ndef _get_raw_datasets_from_nlp(ds: nlp.arrow_dataset.Dataset):\n    \"\"\" From a `nlp.arrow_dataset.Dataset` that we have seen above to a generator of dictionaries with unified format.\n    \n    Yield a dictionary with keys: 'premise', 'hypothesis', 'label', 'lang'\n    \"\"\"\n    \n    for _, elt in enumerate(ds):\n        label = elt['label']\n        for features in _get_features(elt):\n            \n            label = -1\n            if 'label' in elt:\n                label= elt['label']\n            \n            yield {'premise': features[0], 'hypothesis': features[1], 'label': label, 'lang': features[2]}\n            \ndef _get_raw_datasets_from_dataframe(ds: pd.core.frame.DataFrame):\n    \n    result = []\n    \n    for idx, elt in ds.iterrows():\n        for features in _get_features(elt):\n            \n            label = -1\n            if 'label' in elt:\n                label= elt['label']\n            \n            yield {'premise': features[0], 'hypothesis': features[1], 'label': label, 'lang': features[2]}\n\nraw_ds_mapping = {\n    'original train': (_get_raw_datasets_from_dataframe, original_train, len(original_train)),\n    'original valid': (_get_raw_datasets_from_dataframe, original_valid, len(original_valid)),\n    'snli train': (_get_raw_datasets_from_nlp, snli['train'], snli['train'].num_rows),\n    'snli valid': (_get_raw_datasets_from_nlp, snli['validation'], snli['validation'].num_rows),\n    'mnli train': (_get_raw_datasets_from_nlp, mnli['train'], mnli['train'].num_rows),\n    'mnli valid 1': (_get_raw_datasets_from_nlp, mnli['validation_matched'], mnli['validation_matched'].num_rows),\n    'mnli valid 2': (_get_raw_datasets_from_nlp, mnli['validation_mismatched'], mnli['validation_mismatched'].num_rows),\n    'xnli valid': (_get_raw_datasets_from_nlp, xnli['validation'], xnli['validation'].num_rows * 15), # 15 languages\n    'original test': (_get_raw_datasets_from_dataframe, original_test, len(original_test)),\n}\ndef get_raw_dataset(ds_name):\n    \n    fn, ds, nb_examples = raw_ds_mapping[ds_name]\n    \n    for x in fn(ds):\n        yield x","44efb3eb":"for k in raw_ds_mapping:\n    for idx, x in enumerate(get_raw_dataset(k)):\n        print(x)\n        if idx >= 3:\n            break","ecfa3c98":"def get_unbatched_dataset(ds_names, model_name, max_len=64):\n    \"\"\"\n    Args:\n        ds_names: list[str] or dict[str:int], the names of dataset to use, and optionally, how many examples to use from each of them.\n        model_name: list[str], a list of valid Hugging Face transformers' model names.\n            For example: 'distilbert-base-uncased', 'bert-base-uncased', etc.\n    \n    Returns:\n        A `tf.data.Dataset`.\n    \"\"\"\n\n    if type(ds_names) == list:\n        ds_names = {k: None for k in ds_names}\n    ds_names = {k: v for k, v in ds_names.items() if k in raw_ds_mapping}    \n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    \n    # This is a list of generators\n    raw_datasets = [get_raw_dataset(x) for x in ds_names]\n    \n    nb_examples = 0\n\n    sentence_pairs = []\n    labels = []    \n    for name in ds_names:\n        \n        raw_ds = get_raw_dataset(name)\n        nb_examples_to_use = raw_ds_mapping[name][2]\n        if ds_names[name]:\n            nb_examples_to_use = min(ds_names[name], nb_examples_to_use)\n        nb_examples += nb_examples_to_use\n        \n        n = 0\n        for x in raw_ds:\n            sentence_pairs.append((x['premise'], x['hypothesis']))\n            labels.append(x['label'])\n            n += 1\n            if n >= nb_examples_to_use:\n                break\n\n    # `transformers.tokenization_utils_base.BatchEncoding` object -> `dict`\n    r = dict(tokenizer.batch_encode_plus(batch_text_or_text_pairs=sentence_pairs, max_length=max_len, padding='max_length', truncation=True))\n\n    # This is very slow\n    dataset = tf.data.Dataset.from_tensor_slices((r, labels))\n\n    return dataset, nb_examples\n\ndef get_batched_training_dataset(dataset, nb_examples, batch_size=16, shuffle_buffer_size=1, repeat=False):\n    \n    if repeat:\n        dataset = dataset.repeat()\n    \n    if not shuffle_buffer_size:\n        shuffle_buffer_size = nb_examples\n    dataset = dataset.shuffle(shuffle_buffer_size)\n    \n    dataset = dataset.batch(batch_size, drop_remainder=True)\n    \n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset\n\n\ndef get_prediction_dataset(dataset, batch_size=16):\n    \n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    \n    return dataset","465b3a40":"for k in raw_ds_mapping.keys():\n\n    ds, nb_examples = get_unbatched_dataset({k: 100}, model_name='distilbert-base-uncased')\n    ds_batched = get_batched_training_dataset(ds, nb_examples, batch_size=16, shuffle_buffer_size=1, repeat=False)\n    print('{} - select {} examples'.format(k, nb_examples))\n    \n    for x in ds_batched:\n        # print(x)\n        break","4226410f":"def keep_head_tail(lst, len_wanted, head_ratio=0.5):#0.5\n    len_head = int(len_wanted*head_ratio)\n    len_tail = len_wanted-len_head\n    if len_tail==0: return lst[:len_head]\n    return lst[:len_head]+lst[-len_tail:] ","66a5bbfc":"class Classifier(tf.keras.Model):\n    \n    def __init__(self, model_name):\n        \n        super(Classifier, self).__init__()\n        \n        self.transformer = transformers.TFAutoModel.from_pretrained(model_name)\n        self.dropout = tf.keras.layers.Dropout(rate=0.05)\n        self.global_pool = tf.keras.layers.GlobalAveragePooling1D()\n        self.classifier = tf.keras.layers.Dense(3)\n\n    def call(self, inputs, training=False):\n        \n        # Sequence outputs\n        x = self.transformer(inputs, training=training)[0]        \n        x = self.dropout(x, training=training)\n        x = self.global_pool(x)\n        \n        return self.classifier(x)\n\nclass Trainer:\n    \n    def __init__(\n        self, ds_names, model_name, max_len=64,\n        batch_size_per_replica=16, prediction_batch_size_per_replica=64,\n        shuffle_buffer_size=1\n    ):\n\n        global strategy\n        \n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        except ValueError:\n            strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\n        print('Number of replicas:', strategy.num_replicas_in_sync)             \n        \n        self.ds_names = ds_names\n        self.model_name = model_name\n        self.max_len = max_len\n    \n        self.batch_size_per_replica = batch_size_per_replica\n        self.prediction_batch_size_per_replica = prediction_batch_size_per_replica\n        \n        self.batch_size = batch_size_per_replica * strategy.num_replicas_in_sync\n        self.prediction_batch_size = prediction_batch_size_per_replica * strategy.num_replicas_in_sync\n\n        self.shuffle_buffer_size = shuffle_buffer_size\n\n        train_ds, self.nb_examples = get_unbatched_dataset(\n            ds_names=ds_names, model_name=model_name, max_len=max_len\n        )\n        self.train_ds = get_batched_training_dataset(\n            train_ds, self.nb_examples, batch_size=self.batch_size,\n            shuffle_buffer_size=self.shuffle_buffer_size, repeat=True\n        )\n        \n        valid_ds, self.nb_valid_examples = get_unbatched_dataset(\n            ds_names=['original valid'], model_name=model_name, max_len=max_len\n        )\n        self.valid_ds = get_prediction_dataset(valid_ds, self.prediction_batch_size)\n        self.valid_labels = next(iter(self.valid_ds.map(lambda inputs, label: label).unbatch().batch(len(original_valid))))\n        \n        test_ds, self.nb_test_examples = get_unbatched_dataset(\n            ds_names=['original test'], model_name=model_name, max_len=max_len\n        )\n        self.test_ds = get_prediction_dataset(test_ds, self.prediction_batch_size)\n        \n        self.steps_per_epoch = self.nb_examples \/\/ self.batch_size\n                   \n    def get_model(self, model_name, lr, verbose=False):\n\n        with strategy.scope():\n\n            model = Classifier(model_name)\n\n            # False = transfer learning, True = fine-tuning\n            model.trainable = True \n\n            # Just run a dummy batch, not necessary\n            dummy = model(tf.constant(1, shape=[1, 64]))\n\n            if verbose:\n                model.summary()\n\n            # Instiate an optimizer with a learning rate schedule\n            optimizer = tf.keras.optimizers.Adam(lr=lr)\n\n            # Only `NONE` and `SUM` are allowed, and it has to be explicitly specified.\n            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n            \n            # Instantiate metrics\n            metrics = {\n                'train loss': tf.keras.metrics.Sum(),\n                'train acc': tf.keras.metrics.SparseCategoricalAccuracy()\n            }\n\n            return model, loss_fn, optimizer, metrics\n        \n    def get_routines(self, model, loss_fn, optimizer, metrics):\n\n        def train_1_step(batch):\n            \n            inputs, labels = batch\n    \n            with tf.GradientTape() as tape:\n\n                logits = model(inputs, training=True)\n                # Remember that we use the `SUM` reduction when we define the loss object.\n                loss = loss_fn(labels, logits) \/ self.batch_size\n\n            grads = tape.gradient(loss, model.trainable_variables)\n\n            # Update the model's parameters.\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))            \n            \n            # update metrics\n            metrics['train loss'].update_state(loss)\n            metrics['train acc'].update_state(labels, logits)\n\n        @tf.function\n        def dist_train_1_epoch(data_iter):\n            \"\"\"\n            Iterating inside `tf.function` to optimized training time.\n            \"\"\"\n            for _ in tf.range(self.steps_per_epoch):\n                strategy.run(train_1_step, args=(next(data_iter),))        \n\n        @tf.function                \n        def predict_step(batch):\n\n            inputs, _ = batch\n            \n            logits = model(inputs, training=False)\n            return logits\n\n        def predict_fn(dist_test_ds):\n\n            all_logits = []\n            for batch in dist_test_ds:\n\n                # PerReplica object\n                logits = strategy.run(predict_step, args=(batch,))\n\n                # Tuple of tensors\n                logits = strategy.experimental_local_results(logits)\n\n                # tf.Tensor\n                logits = tf.concat(logits, axis=0)\n\n                all_logits.append(logits)\n\n            # tf.Tensor\n            logits = tf.concat(all_logits, axis=0)\n\n            return logits         \n                \n        return dist_train_1_epoch, predict_fn\n        \n    def train(self, train_name, model_name, epochs, verbose=False):\n\n        global strategy\n        \n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        except ValueError:\n            strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\n        print('Number of replicas:', strategy.num_replicas_in_sync)        \n        \n        model, loss_fn, optimizer, metrics = self.get_model(model_name, 1e-5, verbose=verbose)\n        dist_train_1_epoch, predict_fn = self.get_routines(model, loss_fn, optimizer, metrics)\n        \n        train_dist_ds = strategy.experimental_distribute_dataset(self.train_ds)\n        train_dist_iter = iter(train_dist_ds)\n        \n        dist_valid_ds = strategy.experimental_distribute_dataset(self.valid_ds)\n        dist_test_ds = strategy.experimental_distribute_dataset(self.test_ds)\n\n        history = {}\n        best_acc=0.5\n        for epoch in range(epochs):\n            \n            s = datetime.datetime.now()\n\n            dist_train_1_epoch(train_dist_iter)\n\n            # get metrics\n            train_loss = metrics['train loss'].result() \/ self.steps_per_epoch\n            train_acc = metrics['train acc'].result()\n\n            # reset metrics\n            metrics['train loss'].reset_states()\n            metrics['train acc'].reset_states()\n                   \n            print('epoch: {}\\n'.format(epoch + 1))\n            print('train loss: {}'.format(train_loss))\n            print('train acc: {}\\n'.format(train_acc)) \n                \n            e = datetime.datetime.now()\n            elapsed = (e - s).total_seconds()            \n            \n            logits = predict_fn(dist_valid_ds)\n\n            valid_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(self.valid_labels, logits, from_logits=True, axis=-1))\n            valid_acc = tf.reduce_mean(tf.keras.metrics.sparse_categorical_accuracy(self.valid_labels, logits))\n            if valid_acc>best_acc:\n                best_acc=valid_acc\n                #save the model\n                model.save_weights('best.h5')\n                \n            print('valid loss: {}'.format(valid_loss))\n            print('valid acc: {}\\n'.format(valid_acc))\n            \n            print('train timing: {}\\n'.format(elapsed))\n            \n            history[epoch] = {\n                'train loss': float(train_loss),\n                'train acc': float(train_acc),\n                'valid loss': float(valid_loss),\n                'valid acc': float(valid_acc),                \n                'train timing': elapsed\n            }\n\n            print('-' * 40)\n        \n        print('best acc:{}'.format(best_acc))\n        model.load_weights('best.h5')\n        logits = predict_fn(dist_test_ds)\n        preds = tf.math.argmax(logits, axis=-1)\n        \n        submission = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')\n        submission['prediction'] = preds.numpy()\n        submission.to_csv(f'submission-{train_name}.csv', index=False)\n        \n        return history, submission,logits","9d5131cf":"def print_config(trainer):\n\n    print('nb. of training examples used: {}'.format(trainer.nb_examples))\n    print('nb. of valid examples used: {}'.format(trainer.nb_valid_examples))\n    print('nb. of test examples used: {}'.format(trainer.nb_test_examples))\n    \n    print('per replica batch size for training: {}'.format(trainer.batch_size_per_replica))\n    print('batch size for training: {}'.format(trainer.batch_size))\n\n    print('per replica batch size for prediction: {}'.format(trainer.prediction_batch_size_per_replica))\n    print('batch size for prediction: {}'.format(trainer.prediction_batch_size))\n    \n    print('steps per epoch: {}'.format(trainer.steps_per_epoch))","0f4c1ad6":"epochs = 15#15\n# model_name = 'jplu\/tf-xlm-roberta-base'\nmodel_name = 'jplu\/tf-xlm-roberta-large'\n# model_name = 'distilbert-base-uncased'","ca966667":"# trainer = Trainer(\n#     ds_names={'original train': None}, model_name=model_name,\n#     max_len=64, batch_size_per_replica=16, prediction_batch_size_per_replica=64,\n#     shuffle_buffer_size=None\n# )\n\n# print_config(trainer)\n\n# train_name = f'{model_name} + original-dataset'.replace('\/', '-')\n# history_1, submission_1 = trainer.train(train_name=train_name, model_name=model_name, epochs=epochs, verbose=True)","0c8109ca":"def plot(history, metric):\n    \"\"\"\n    metric: 'loss' or 'acc'\n    \"\"\"\n    \n    h = {\n        f'train {metric}': [history[epoch][f'train {metric}'] for epoch in history],\n        f'valid {metric}': [history[epoch][f'valid {metric}'] for epoch in history]\n    }\n        \n    fig = px.line(\n        h, x=range(1, len(history) + 1), y=[f'train {metric}', f'valid {metric}'], \n        title=f'model {metric}', labels={'x': 'Epoch', 'value': metric}\n    )\n    fig.show()\n    \ndef plot_2(history1, history2, metric, desc1, desc2):\n    \n    h = {\n        f'train {metric} - {desc1}': [history1[epoch][f'train {metric}'] for epoch in history1],\n        f'valid {metric} - {desc1}': [history1[epoch][f'valid {metric}'] for epoch in history1],\n        f'train {metric} - {desc2}': [history2[epoch][f'train {metric}'] for epoch in history2],\n        f'valid {metric} - {desc2}': [history2[epoch][f'valid {metric}'] for epoch in history2]        \n    }\n        \n    fig = px.line(\n        h, x=range(1, len(history1) + 1), y=[f'train {metric} - {desc1}', f'valid {metric} - {desc1}', f'train {metric} - {desc2}', f'valid {metric} - {desc2}'], \n        title=f'model {metric}', labels={'x': 'Epoch', 'value': metric}\n    )\n    fig.show()    ","3261d0dc":"# plot(history_1, 'loss')\n# plot(history_1, 'acc')","c43ad965":"# pd.read_csv(f'submission-{train_name}.csv').head(20)","a362d257":"# trainer = Trainer(\n#     ds_names={'original train': None, 'xnli valid': None}, model_name=model_name,\n#     max_len=64, batch_size_per_replica=16, prediction_batch_size_per_replica=64,\n#     shuffle_buffer_size=None\n# )\n\n# print_config(trainer)\n\n# train_name = f'{model_name} + extra-xnli'.replace('\/', '-')\n# history_2, submission_2 = trainer.train(train_name=train_name, model_name=model_name, epochs=epochs, verbose=True)","4fc81e0d":"# plot(history_2, 'loss')\n# plot(history_2, 'acc')","7aff989d":"# plot_2(history_1, history_2, 'loss', desc1='only original dataset', desc2='+ xnli')\n# plot_2(history_1, history_2, 'acc', desc1='only original dataset', desc2='+ xnli')","f0345adc":"# pd.read_csv(f'submission-{train_name}.csv').head(20)","c706cfa5":"trainer = Trainer(\n    ds_names={'original train': None, 'xnli valid': None, 'mnli train': 60000, 'mnli valid 1': None, 'mnli valid 2': None}, model_name=model_name,\n    max_len=208, batch_size_per_replica=16, prediction_batch_size_per_replica=64,#16\n    shuffle_buffer_size=None\n)\n\nprint_config(trainer)\n\ntrain_name = f'{model_name} + extra-xnli-mnli'.replace('\/', '-')\nhistory_3, submission_3,preds = trainer.train(train_name=train_name, model_name=model_name, epochs=epochs, verbose=True)","08687a97":"np.savez_compressed('preds',a=preds)","aebe9303":"plot(history_3, 'loss')\nplot(history_3, 'acc')","dc64a4c6":"# plot_2(history_1, history_3, 'loss', desc1='only original dataset', desc2='+ mnli\/xnli')\n# plot_2(history_1, history_3, 'acc', desc1='only original dataset', desc2='+ mnli+xnli')","487ecc23":"# plot_2(history_2, history_3, 'loss', desc1='+ xnli', desc2='+ mnli\/xnli')\n# plot_2(history_2, history_3, 'acc', desc1='+ xnli', desc2='+ mnli\/xnli')","c410df95":"s = pd.read_csv(f'submission-{train_name}.csv')\ns.to_csv(f'submission.csv', index=False)\n\ns.head(20)","899506fd":"#### Import","79df4159":"### original dataset + xnli","d3225b6c":"## Extra datasets","8e394380":"Note that the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset, described in [this competition data page](https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\/data):\n\n> label: the classification of the relationship between the premise and hypothesis (0 for entailment, 1 for neutral, 2 for contradiction)","76f5b2bd":"The class names are still\n```\n    ['entailment', 'neutral', 'contradiction'],\n```\nhowever, the features `premise` and `hypothesis` are no longer `string` but `dictionary` which contain sentences in different language! ","3726b302":"Let's use what we learned to check some training examples","861a21b7":"### original dataset + xnli + mnli","09a13c49":"#### look inside 'nlp.arrow_dataset.Dataset'\n\nIn order to get the number of examples in a dataset, for example, `mnli['train']`, you can do\n```\n    mnli['train'].num_rows\n```\n\n\nYou can iterate a [nlp.arrow_dataset.Dataset](https:\/\/huggingface.co\/nlp\/master\/package_reference\/main_classes.html#nlp.Dataset) object like:\n```\n    for elt in mnli['train']:\n        ...\n```\nEach step, you get an example (which is a dictionary containing features - in a general sense).\n\nYou can also access the content of a [nlp.arrow_dataset.Dataset](https:\/\/huggingface.co\/nlp\/master\/package_reference\/main_classes.html#nlp.Dataset) object by specifying a feature name . For example, the training dataset in `mnli` has `premise`, `hypothesis`, `label` and `idx` as features.\n\nYou can either specify a feature name first (you get a list) followed by a slice, like\n```\n    # You get a `list` first, then slice it\n    mnli['train']['premise'][:3]\n```\nor use slice notation first to get a dictionary (which represents a sliced dataset) followed by a feature name.\n```\n    # You get a `dictionary` (of lists) first, then a list\n    mnli['train'][:3]['premise']\n```\n\nThe results will be the same.\n\nIn order to get the name of the classes, you can do\n\n```\nmnli['train'].features['label'].names\n```","19db1429":"#### sanity check","ac544e52":"#### Compare [only original dataset] vs. [+ xnli]","c6888db2":"## Competition dataset","894c5823":"### Load more extra datasets","f0c768fe":"#### The Cross-Lingual NLI Corpus (XNLI)\n\nThe [MNLI](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) and [SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/) contain only english sentences. Let's load the [Cross-lingual NLI Corpus (XNLI)](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/) dataset. It contains only validation and test dataset, not training examples.","1148e728":"Folded from https:\/\/www.kaggle.com\/yihdarshieh\/more-nli-datasets\nChanges:\n1. Using xmlr large model instead of base model;\n2. Set the random seeds\n3. Using more mnli datas(60000)","7e34ff5c":"# Datasets","4881ac4a":"### Make a unified format of raw datasets","94b302f1":"#### The Stanford Natural Language Inference Corpus (SNLI)\n\nFirst, let's load the [The Stanford Natural Language Inference Corpus (SNLI)](https:\/\/nlp.stanford.edu\/projects\/snli\/). It contains $570000$ sentence pairs annotated with textual entailment information.","7cd22469":"### train on the original dataset","fe243acc":"#### Compare [only original dataset] vs. [+ xnli\/mnli]","762519c1":"#### Plot history","7da1ca4a":"#### Compare [+ xnli] vs. [+ xnli\/mnli]","cf155361":"## Unified dataset format\n\nSince the 4 datasets have different formats, we are going to create an unified interface, which uses [tf.data.Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset), to make working with them easier.","95f33be4":"### Let's use Hugging Face new library [nlp](https:\/\/huggingface.co\/nlp\/), to get more NLI datasets.","644a9917":"<center><img src=\"https:\/\/raw.githubusercontent.com\/chiapas\/kaggle\/master\/competitions\/contradictory-my-dear-watson\/header.png\" width=\"1000\"><\/center>\n<br>\n<center><h1>Detecting contradiction and entailment in multilingual text using TPUs<\/h1><\/center>\n<br>\n\n#### Natural Language Inferencing (NLI) is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the _premise_ and the _hypothesis_ ), and deciding how they are related- if the premise entails the hypothesis, contradicts it, or neither.\n\n#### In this notebook, we will use more NLI datasets, including\n\n* [The Stanford Natural Language Inference Corpus (SNLI)](https:\/\/nlp.stanford.edu\/projects\/snli\/)\n* [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/)\n* [Cross-lingual NLI Corpus (XNLI)](https:\/\/cims.nyu.edu\/~sbowman\/xnli\/)\n\n#### We will also use Hugging Face recent library [nlp](https:\/\/huggingface.co\/nlp\/) to work with these datasets.","3880f1df":"## Tranier","99a67d43":"#### check the loaded dataset\n\nLet's look some information about the MNLI dataset. The (default) return value of [nlp.load_dataset](https:\/\/huggingface.co\/nlp\/package_reference\/loading_methods.html#nlp.load_dataset) is a dictionary with split names as keys, usually they are `train`, `validation` and `test`, but not always. The values are [nlp.arrow_dataset.Dataset](https:\/\/huggingface.co\/nlp\/master\/package_reference\/main_classes.html#nlp.Dataset).\n\n","7501b998":"Again, the class names are\n```\n    ['entailment', 'neutral', 'contradiction'] \n```\nwhich corresponds to the original competition dataset.\n\nIn [SNLI](https:\/\/nlp.stanford.edu\/projects\/snli\/), we have the same premise with different hypotheses\/labels. With a first try, I got `nan` as the training loss value. So I won't use this dataset in the current notebook.","69b0669e":"#### sanity check","d79430f5":"# Training","61009abe":"#### Load a dataset - The Multi-Genre NLI Corpus (MNLI)\nFirst, let's load the [The Multi-Genre NLI Corpus (MultiNLI, MNLI)](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/). It contains $433000$ sentence pairs annotated with textual entailment information.","6e5a9dde":"### Working with tf.data.Dataset","aaa41a29":"## Train"}}