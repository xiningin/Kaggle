{"cell_type":{"5cfe9be9":"code","2da23840":"code","87309b05":"code","54f09aeb":"code","1c25dc56":"code","999a3c0f":"code","cadfbdce":"code","df630fde":"code","4459e81e":"code","4fb4b29c":"code","edd08a79":"markdown","0e9e55cf":"markdown","18a24b5c":"markdown","5931c76d":"markdown","5ff94403":"markdown","74e94c5b":"markdown","b6e98526":"markdown","fc3dd3d7":"markdown","556d9a16":"markdown","fd7672af":"markdown","8b4870b9":"markdown"},"source":{"5cfe9be9":"example = \"example-1\tI saw Oratile slap Rorisang on her left shoulder.\ther\t31\tOratile\t6\tFalse\tRorisang\t19\tTrue\"\nline = example.split('\\t')","2da23840":"max_seq_length = 12 #For the cometition, I used 64 and 128, but keeping it very short for clarity.","87309b05":"text = line[1]\nP_offset = int(line[3])\nA_offset = int(line[5])\nB_offset = int(line[8])","54f09aeb":"char_off = sorted([\n  [P_offset, 0],\n  [A_offset, 1],\n  [B_offset, 2]\n], key=lambda x: x[0])\nchar_off","1c25dc56":"text_segments = [text[:char_off[0][0]], \ntext[char_off[0][0]:char_off[1][0]], \ntext[char_off[1][0]:char_off[2][0]], \ntext[char_off[2][0]:]]\ntext_segments","999a3c0f":"!cp ..\/input\/bertfiles\/tokenization.py .\nimport tokenization\ntokenizer = tokenization.FullTokenizer(vocab_file='..\/input\/bertfiles\/vocab.txt', do_lower_case=True)\ntoken_segments = []\nnum_tokens = []\nfor segment in text_segments:\n    token_segment = tokenizer.tokenize(segment)\n    token_segments.append(token_segment)\n    num_tokens.append(len(token_segment))\ntoken_segments","cadfbdce":"import numpy as np\nwhile np.sum(num_tokens) > (max_seq_length - 2):\n    index = np.argmax([num_tokens[0] * 2, num_tokens[1], num_tokens[2], num_tokens[3] * 2])\n    if index == 0:\n        token_segments[index] = token_segments[index][1:]\n    elif index == 3:\n        token_segments[index] = token_segments[index][:-1]\n    else: #middle segments\n        middle = num_tokens[index] \/\/ 2\n        token_segments[index] = token_segments[index][:middle] + token_segments[index][middle + 1:]\n    num_tokens[index] -= 1\ntoken_segments","df630fde":"tokens = []\ntokens.append(\"[CLS]\")\nfor segment in token_segments:\n    temp = ''\n    for token in segment:\n        tokens.append(token)\ntokens.append(\"[SEP]\")","4459e81e":"offset = 1 #to account for \"[CLS]\"\nfor i, row in enumerate(char_off):\n    offset += num_tokens[i]\n    row[0] = offset\n\ntoken_off = sorted(char_off, key=lambda x: x[1])\ntoken_off","4fb4b29c":"P_mask = [0] * max_seq_length\nA_mask = [0] * max_seq_length\nB_mask = [0] * max_seq_length\n\nP_mask[token_off[0][0]] = 1\nA_mask[token_off[1][0]] = 1\nB_mask[token_off[2][0]] = 1\n\nprint(P_mask)\nprint(A_mask)\nprint(B_mask)\nprint(tokens)","edd08a79":"Make an array of the char offsets, including an index column which will be used to remember positions of P, A, B after sorting by offset. Then sort by the offset column.","0e9e55cf":"# Preparing GAP for BERT: Input Layer","18a24b5c":"Use the offsets in the first column to split the text into 4 segments. This technique assists the accurate conversion of character offsets to token offsets without the need for any whitespace tokenization.","5931c76d":"Concatenate the segments back together.","5ff94403":"This kernel only shows how character offsets are converted to BERT token offsets for explanatory purposes. A full repository of the software to fine tune BERT and do predictions for this dataset is available [here.](https:\/\/github.com\/kenkrige\/BERT-Fine-tune-for-GAP)","74e94c5b":"Use BERT Wordpiece to tokenize each segment.","b6e98526":"Those are the inputs to the model. The masks are forwarded to the output layer and the sentence of tokens is processed by the BERT hidden layers.","fc3dd3d7":"For a short working example, I've chosen something I might hear from one of my school students:\n> \"I saw Oratile slap Rorisang on her left shoulder.\"","556d9a16":"Makle the position masks for P, A, B","fd7672af":"Replace the char offsets with token offsets, using the lengths of token segments cumulatively. Then sort offsets on column 2 back to the original order of P, A, B.","8b4870b9":"Truncate by removing one token at a time until the number of tokens is two less than the maximum sequence length. The extra two allow for BERT's start and end tokens. Each time remove the furthest token from any offset point."}}