{"cell_type":{"b12920c2":"code","eddfa4a3":"code","30c42bcd":"code","57f2cd19":"code","69bc9e5c":"code","18d528d7":"code","ba93ab7a":"code","2fb831f7":"code","941475cb":"code","b867903d":"code","31dd574a":"code","f71673dc":"markdown","8d4106cd":"markdown","d62be1c4":"markdown","076b4319":"markdown","9c68e0a5":"markdown","6027ef48":"markdown","0ae240dd":"markdown"},"source":{"b12920c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eddfa4a3":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n\nprint(\"Columns\", df.columns.tolist())","30c42bcd":"df.head()","57f2cd19":"survived = df[df['Survived'] == 1]\nnot_survived = df[df['Survived'] == 0]\n\nmale_survived = survived[survived['Sex']=='male']\nfemale_survived = survived[survived['Sex']=='female']\n\nmale_not_survived = not_survived[not_survived['Sex']=='male']\nfemale_not_survived = not_survived[not_survived['Sex']=='female']\n\nprint(\"Total no of passenger survived: \", survived['PassengerId'].count())\nprint(\"Total no of Male passenger survived: \", male_survived['PassengerId'].count())\nprint(\"Total no of Female passenger survived: \", female_survived['PassengerId'].count())\n\nprint(\"Total no of passenger not survived: \", not_survived['PassengerId'].count())\nprint(\"Total no of Male passenger not survived: \", male_not_survived['PassengerId'].count())\nprint(\"Total no of Female passenger not survived: \", female_not_survived['PassengerId'].count())\n\nprint(\"Total no of passenger onboarded: \", df['PassengerId'].count())","69bc9e5c":"# limit to categorical data using df.select_dtypes()\ncategorical_df = df.select_dtypes(include=[object])\ncategorical_df.head(3)","18d528d7":"# we can make use of pandas's get_dummies() method or scikitlearn's OneHotEncoder for encoding purpose\n# Step 1: call get_dummies method of pandas\n# Step 2: concatenate dummy columns withh original dataframe\n# Step 3: Drop one of the column from each dummy set\n\n# Step 1: get dummies columns from categorical column\nencoded_sex_col = pd.get_dummies(df['Sex'])\nencoded_embarked_col = pd.get_dummies(df['Embarked'])\n#pd.get_dummies(df['Ticket'])\n#pd.get_dummies(df['Cabin'])\n\n# Step 2: Concate\nencoded_df = pd.concat([df,encoded_sex_col, encoded_embarked_col], axis='columns')\nprint(encoded_df.head(3))\n\n\n# Dummy variable Trap and Multicollinearity\n# Now this is a problem we have to deal with when we genereate dummy column. \n# This can easily be understood by just googling so I'm not going in that direction and as a solution we \n# just have drop one column each dummy set that means, fr embarked dummy coolumns, we can drop one columns from \"C\",\"Q\" or \"S\".\n\n#Step 3: Drop one of the column from each dummy set \nfinal = encoded_df.drop(['Q', 'male'], axis='columns')\nprint(\"################## Dataframe after Step 3 ############\")\nprint(final.head(3))","ba93ab7a":"# Dropping unnecessary features\nfinal = final.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Sex', 'Embarked'], axis='columns')\nprint(final.head())\n\n\nfinal = final.dropna(axis = 0, how ='any')","2fb831f7":"# Now this seems pretty good go for training our model but before that we have do a \n# bit of work which is \"Train-Test Split\" which we would keep in 70:30 ratio.\nfeatures = [c for c in final.columns.tolist() if c not in ['Survived']]\ntarget = ['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(final[features], final[target], test_size=0.30, random_state=42)\n\nprint(X_train.head())","941475cb":"# Training model\nkf = KFold(n_splits=20)\nprint(kf)\n\nrfc = RandomForestClassifier(max_depth=50, random_state=0, n_jobs=10, n_estimators=150)\nscores = []\nfor train_index, test_index in kf.split(final):\n    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    train, test = final.iloc[train_index], final.iloc[test_index]    \n    \n    rfc.fit(train[features], train[target].values.ravel())\n    print(\"Score:\", rfc.score(test[features], test[target]))\n    scores.append(rfc.score(test[features], test[target]))\n    \n# Instead of looping over indexes returned by kf.split() mwthod we can also achieve in simpler way    \n# scores = cross_val_score(rfc, final[features], final[target], cv=20)","b867903d":"y_prediction = rfc.predict(X_test)\ny_prediction\n\nprint(\"################## Confusion Matrix ##################\")\nprint(confusion_matrix(y_test, y_prediction))","31dd574a":"print(\"################## Classification Report ##################\")\nprint(classification_report(y_test, y_prediction))\n\nprint(\"Score:\", rfc.score(X_test, y_test))","f71673dc":"#### 1. Male\/Female survival","8d4106cd":"# Understanding the features\n\nIt's important to know each feature and get a nice deep understanding of each of the avaialble feature. That would give us nice insights of data and we can take many decisions like approach to clean data, manipulate and selection of feature to train model. \n\nList of columns:\n\n['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\n\nNow we go thorough each of the columns and undserstand what each column means:\n\n1. PassengerId: It's nothing but a unique id of each passenger\n2. Survived(Target): Now this is our target variable that we would be predicting on test data and if we are satisfied if our model then we will do prediction on unknown data to know how generize our model is. Survived has 2 possible values that is either '1' or '0' where '1' being passenger has survived and '0' means passenger did not survived.\n3. Pclass: This feature indicates the which class each passenger belongs to. A passenger can belog to  either of class \"3\", \"2,\" or \"1\". Where \"3\" is the lowest class of ticket and \"1\" means the top class ticket which obiously means passenger belogs to class \"1\" has more luxury bedrooms and facilities.\n4. Name: Name of the passenger\n5. Sex: Sex of the passenger. This is the categorical data later on we will convert this into some meaningful data so that we can feed this to model\n6. Age: Age of the passegner. Now this is this is very crucial to know what age group has highest no of survival and which age group has hightest no of deaths. We will plot some charts to get a view on this.\n7. SibSp: Tells no of siblings\/spouses particular passenger has onboarded on the ship. Sibling = brother, sister, stepbrother, stepsister. Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n8. Parch: Tells no of parents \/ children particular passenger has onboarded on the ship. \n    Parent = mother, father. Child = daughter, son, stepdaughter, stepson. Some children travelled only with a nanny, therefore parch=0 for them\n9. Ticket: Ticket no. Will try to get some info from this feature, may be by applying some regex.\n10. Fare: We will plot a chart to know which fare range has highest no of survival\/deaths.\n11. Cabin: Cabin no where passenger staied.\n12. Embarked: From which port the passegner has on-boarded the ship.","d62be1c4":"### Exploratory data analysis","076b4319":"### Feature engineering","9c68e0a5":"### Model Evaluation","6027ef48":"#### The machine learning models are mathematical models and works with numbers. That means, if we have textual data, we can't use such data to train the model. But this never mean that we should drop such data. Dropping a single column is loss of information and this is not recomanded. In fact what we can do is, we can covert such categorical data into numbers without killing its actual meaning.\n\nA categorical variable is a variable whose values take on the value of labels. For example, the variable may be \u201ccolor\u201d and may take on the values \u201cred,\u201d \u201cgreen,\u201d and \u201cblue.\u201d\n\nThis article will help you understand about categorial variabes: https:\/\/machinelearningmastery.com\/how-to-prepare-categorical-data-for-deep-learning-in-python\/\n\nCategorical Vriables can be divided into below two types:\n\n1] Nominal: Male-Female, Red-Green-Blue, Jan-Feb-March-Apr.\n\n2] Ordinal: Where there is some sort of numerical order in between them. i.e. Degree(Garduate, Masters and PhD), Rating(not satisfied, satisfied, very happy)\n\n\nIf we apply categorical classfication on our titanic dataset , we get below:\n\n1] Nominal Features: Sex, Ticket, Cabin, Embarked\n\n2] Categoriccal Features: Pclass (already given into integer form so we don't need to apply any Encoding)","0ae240dd":"### Training the Model"}}