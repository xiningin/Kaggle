{"cell_type":{"765b49eb":"code","f0043af5":"code","5816447d":"code","48d76364":"code","c7e8f4a6":"code","36d0b55a":"code","9b1e0d61":"code","e3cb9fe9":"code","06e5e6ab":"code","869b4097":"code","f2e89239":"code","9c8d9949":"code","181ebfc8":"code","9d8ea177":"code","8b9df028":"code","608cec70":"code","12979345":"code","091ed105":"code","fbc4d2ba":"code","7a0c3037":"code","3e7530e0":"code","e3a29639":"code","1152c386":"code","c2c67962":"code","a4c8a677":"code","ce506a3d":"code","d951dcd7":"code","6282dd15":"code","c9b7c891":"code","3f18f3aa":"code","ddfae416":"code","bd45e4ab":"code","bda684b0":"code","49e795bd":"code","bbd80a93":"code","d244598c":"code","e762f497":"code","1d4b4349":"code","a2feaf7d":"code","4742d669":"code","e28fa5b7":"code","4eac65be":"code","9a4568dc":"code","028ef76f":"code","dec824ef":"code","ebaa52ab":"code","f2126d50":"code","e209f01e":"code","fa59ec31":"code","861819e8":"code","e37f9741":"code","7859a0c9":"markdown","106fd763":"markdown","624da086":"markdown","f4ee9729":"markdown","91479480":"markdown","7f573a1d":"markdown","2fdd01f9":"markdown","978c9d2a":"markdown","a6ae924f":"markdown","53cabea3":"markdown","ca5c6a40":"markdown","b267324a":"markdown","c2450e26":"markdown","df5394e0":"markdown","87647ec1":"markdown","6e5957ec":"markdown","e4010b9c":"markdown","0f60f0b4":"markdown","02b84333":"markdown","d81b0b08":"markdown","0e858692":"markdown","58550f4b":"markdown","8338ca8e":"markdown","7f5034a0":"markdown","97ce614d":"markdown","3cd3f5f7":"markdown","8f19b9dc":"markdown","c5f70717":"markdown","5be19b80":"markdown","198b2fe7":"markdown","d55f72be":"markdown","2e08c9f9":"markdown","c9a2f55b":"markdown","116211d9":"markdown","003f161f":"markdown","6c881c4a":"markdown","321e5c9e":"markdown","23af8599":"markdown","fa63d542":"markdown","2e47ef07":"markdown","38545822":"markdown","9374079c":"markdown","1881d023":"markdown","c4ba0d7c":"markdown","c304bf48":"markdown","8bf9656a":"markdown","5d558404":"markdown","28717eca":"markdown","7dcd9a34":"markdown","b66f463c":"markdown","e44bcc8b":"markdown","f8abb364":"markdown","f7cd8d3b":"markdown","be5b0b53":"markdown"},"source":{"765b49eb":"import numpy as np\nimport pandas as pd\n\nimport os\n\nimport matplotlib.pyplot as plt#visualization\nimport seaborn as sns#visualization\n%matplotlib inline\n\nfrom PIL import  Image\n\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport io\nimport plotly.offline as py #visualization\npy.init_notebook_mode(connected=True) #visualization\nimport plotly.graph_objs as go #visualization\nimport plotly.tools as tls #visualization\nimport plotly.figure_factory as ff #visualization\nfrom plotly.subplots import make_subplots #visualization\n\npd.set_option('display.max_columns', 100)","f0043af5":"df = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","5816447d":"df.head()","48d76364":"print(df.shape)\nsummary = pd.DataFrame(df.dtypes,columns=['dtypes'])\nsummary['Missing'] = df.isnull().sum().values    \nsummary['Uniques'] = df.nunique().values\nsummary","c7e8f4a6":"df.describe()","36d0b55a":"for column in df.columns:\n    if '' in df[column].values or ' ' in df[column].values:\n        print(column)","9b1e0d61":"df['TotalCharges'] = df['TotalCharges'].replace('',np.nan)\ndf['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)","e3cb9fe9":"totalinstances = df.shape[0]\ntotalnull = df['TotalCharges'].isnull().sum()\nprint(round((totalnull\/totalinstances*100),2),'%')","06e5e6ab":"df.dropna(axis=0, inplace = True)","869b4097":"df['TotalCharges'] = df['TotalCharges'].astype('float')","f2e89239":"df[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})","9c8d9949":"for column in df.columns:\n    print(column +':','\\n',np.unique(df[column])[:5],'\\n')","181ebfc8":"nointernet = []\nfor column in df.columns:\n    if \"No internet service\" in df[column].unique():\n        nointernet.append(column)\n\nfor col in nointernet : \n    df[col]  = df[col].replace('No internet service', 'No')","9d8ea177":"label = list(df['Churn'].unique())\nvalue = df['Churn'].value_counts()\nvalue_percent = list(round(value\/df.shape[0],2))","8b9df028":"t1 = go.Bar(\n    x=label,\n    y=value_percent,\n    width = [0.5, 0.5],\n    marker=dict(\n        color=['green', 'blue'])\n    )\n\n\nlayout = go.Layout(dict(\n    title='Overall Customer Churn Rate',\n    plot_bgcolor  = \"rgb(243,243,243)\",\n    paper_bgcolor = \"rgb(243,243,243)\",\n    xaxis = dict(\n        gridcolor = 'rgb(255, 255, 255)',\n        title = \"Churn\",\n        zerolinewidth=1,\n        ticklen=5,\n        gridwidth=2\n        ),\n    yaxis = dict(\n        gridcolor = 'rgb(255, 255, 255)',\n        title = \"Percent\",\n        zerolinewidth=1,\n        ticklen=5,\n        gridwidth=2\n        ),\n    )\n)\n\nfig = go.Figure(data=t1, layout=layout)\nfig.update_layout(title_x=0.5)\npy.iplot(fig)\n\n\n","608cec70":"# Separating the churn rates for comparison of various categorical features.\nchurn = df[df['Churn'] == 'Yes']\nretention = df[df['Churn'] == 'No']","12979345":"# Generally, we round up sample sizes when estimating population mean\/proportion.\n\nimport math\n\ndef round_decimals_up(og_list, decimals:int=2):\n    \"\"\"\n    Returns rounded up list to a specific number of decimal places.\n    \"\"\"\n    \n    rounded_list = []\n    if not isinstance(decimals, int):\n        raise TypeError(\"decimal places must be an integer\")\n    elif decimals < 0:\n        raise ValueError(\"decimal places has to be 0 or more\")\n    elif decimals == 0:\n        return math.ceil(number)\n\n    factor = 10 ** decimals\n    \n    for number in og_list:\n        \n        rounded_list.append((math.ceil(number * factor) \/ factor))\n    \n    return rounded_list","091ed105":"def barplot_rounded(col):\n    \n    rounded_churn = round_decimals_up((churn[col].value_counts()\/churn.shape[0]),3)\n    rounded_retention = round_decimals_up((retention[col].value_counts()\/retention.shape[0]),3)\n    \n    t1 = go.Bar(\n        x = list(churn[col].value_counts().keys()),\n        y = rounded_churn, \n        name = 'Churn',\n        marker_color = 'rgb(55, 83, 109)'\n    )\n    \n    t2 = go.Bar(\n        x = list(retention[col].value_counts().keys()),\n        y = rounded_retention,\n        name = 'Retention',\n        marker_color = 'rgb(26, 118, 255)'\n    )\n    \n    data = [t1,t2]\n    \n    layout = go.Layout(dict(\n        title = \"Churn Rate by \" + col,\n        plot_bgcolor  = \"rgb(243,243,243)\",\n        paper_bgcolor = \"rgb(243,243,243)\",\n        xaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = col,\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        yaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = \"Percent\",\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        )\n    )\n    \n    fig  = go.Figure(data=data,layout=layout)\n    fig.update_layout(title_x=0.5)\n    py.iplot(fig)","fbc4d2ba":"df.nunique()[df.nunique() <= 4]","7a0c3037":"categorical = df.nunique()[df.nunique() <= 4].index[:-1]","3e7530e0":"for col in categorical:\n    barplot_rounded(col)","e3a29639":"tenure = df.groupby('Churn')['tenure'].value_counts().unstack(0)\n\ndef tenure_scatter(df):\n    \n    t1 = go.Scatter(\n            x = list(df.index),\n            y = df.loc[:,'Yes'],\n            mode = 'markers',\n            name = 'Churn',\n            marker = dict(\n                line = dict(\n                    color = \"black\",\n                    width = .5),\n                color = 'blue',\n                opacity = 0.8\n               ),\n        )\n\n    t2 = go.Scatter(\n            x = list(df.index),\n            y = df.loc[:,'No'],\n            mode = 'markers',\n            name = 'Retention',\n            marker = dict(\n                line = dict(\n                    color = \"black\",\n                    width = .5),\n                color = 'green',\n                opacity = 0.8\n               ),\n        )\n\n    layout = go.Layout(dict(\n        title='Churn based on Tenure',\n        plot_bgcolor  = \"rgb(243,243,243)\",\n        paper_bgcolor = \"rgb(243,243,243)\",\n        xaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = \"Tenure\",\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        yaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = \"Churn\/Retention\",\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        )\n    )\n    \n    data = [t1,t2]\n\n    fig  = go.Figure(data=data,layout=layout)\n    fig.update_layout(title_x=0.5)\n    py.iplot(fig)\n\ntenure_scatter(tenure)","1152c386":"def bin_tenure(df) :\n    \n    if df[\"tenure\"] <= 12 :\n        return \"Tenure 0-12\"\n    elif (df[\"tenure\"] > 12) & (df[\"tenure\"] <= 24 ):\n        return \"Tenure 12-24\"\n    elif (df[\"tenure\"] > 24) & (df[\"tenure\"] <= 48) :\n        return \"Tenure 24-48\"\n    elif (df[\"tenure\"] > 48) & (df[\"tenure\"] <= 60) :\n        return \"Tenure 48-60\"\n    elif df[\"tenure\"] > 60 :\n        return \"Tenure 60+\"\n    \ndf[\"bin_tenure\"] = df.apply(lambda df:bin_tenure(df),\n                                      axis = 1)\n\ntenure_binned = df.groupby('Churn')['bin_tenure'].value_counts().unstack(0)\n\ntenure_scatter(tenure_binned)","c2c67962":"# Feature engineering for creating scatterplots on Monthly\/Total Charges.\n\ndf_copy = df.copy()\n\ndf_copy.loc[df_copy.Churn=='No','Churn'] = 0 \ndf_copy.loc[df_copy.Churn=='Yes','Churn'] = 1\n\ndf_copy.head()","a4c8a677":"print(df_copy['Churn'].dtype)","ce506a3d":"# Change dtype.\ndf_copy['Churn'] = df_copy['Churn'].astype(int)","d951dcd7":"df_mc = df_copy.groupby('MonthlyCharges').Churn.mean().reset_index()","6282dd15":"def charges_scatter(df,col,title,x_title):\n    \n    t1 = go.Scatter(\n                x = df[col],\n                y = df['Churn'],\n                mode = 'markers',\n                name = 'Churn',\n                marker = dict(\n                    line = dict(\n                        color = \"black\",\n                        width = .5),\n                    color = 'red',\n                    opacity = 0.8\n                   ),\n            )\n\n    layout = go.Layout(dict(\n        title = title,\n        plot_bgcolor  = \"rgb(243,243,243)\",\n        paper_bgcolor = \"rgb(243,243,243)\",\n        xaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = x_title,\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        yaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = \"Churn Rate\",\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        )\n    )\n\n    data = [t1]\n\n    fig = go.Figure(data=data,layout=layout)\n    fig.update_layout(title_x=0.5)\n    py.iplot(fig)\n\ncharges_scatter(\n    df_mc,\n    'MonthlyCharges',\n    'Churn Rate by Monthly Charges',\n    'Monthly Charges'\n)\n","c9b7c891":"print(df_copy['MonthlyCharges'].dtype)","3f18f3aa":"df_copy['MonthlyCharges'].value_counts().head()","ddfae416":"df_copy = df.copy()\n\ndf_copy.loc[df_copy.Churn=='No','Churn'] = 0 \ndf_copy.loc[df_copy.Churn=='Yes','Churn'] = 1\n\ndf_copy['Churn'] = df_copy['Churn'].astype(int)\ndf_copy.MonthlyCharges = df_copy.MonthlyCharges.round()\ndf_copy.TotalCharges = df_copy.TotalCharges.round()\n\ndf_mc = df_copy.groupby('MonthlyCharges').Churn.mean().reset_index()\ndf_tc = df_copy.groupby('TotalCharges').Churn.mean().reset_index()\n\ncharges_scatter(\n    df_mc,\n    'MonthlyCharges',\n    'Churn Rate by Monthly Charges',\n    'Monthly Charges'\n)","bd45e4ab":"charges_scatter(\n    df_tc,\n    'TotalCharges',\n    'Churn Rate by Total Charges',\n    'Total Charges'\n)","bda684b0":"# Average monthly charges for each tenure group.\ndf_num = df.groupby(['Churn','bin_tenure'])[['MonthlyCharges','TotalCharges']].mean().reset_index()\n\ndf_num_churn = df_num[df_num.Churn == 'Yes']\ndf_num_retention = df_num[df_num.Churn == 'No']","49e795bd":"t1 = go.Bar(\n    x = df_num_churn.bin_tenure,\n    y = df_num_churn.MonthlyCharges,\n    name = 'Churn',\n    marker = dict(\n        line = dict(\n            width = 1\n        )\n    ),\n    text = 'Churn'\n)\n\nt2 = go.Bar(\n    x = df_num_retention.bin_tenure,\n    y = df_num_retention.MonthlyCharges,\n    name = 'Retention',\n    marker = dict(\n        line = dict(\n            width =1\n        )\n    ),\n    text = 'Retention'\n)\n\nlayout = go.Layout(\n    dict(\n        title = 'Average Monthly Charge by Tenure',\n        plot_bgcolor  = \"rgb(243,243,243)\",\n        paper_bgcolor = \"rgb(243,243,243)\",\n        xaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = 'Tenure Group',\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        yaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = \"Average Monthly Charge\",\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        )\n    )\n\ndata = [t1,t2]\n\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(title_x=0.5)\npy.iplot(fig)","bbd80a93":"t1 = go.Bar(\n    x = df_num_churn.bin_tenure,\n    y = df_num_churn.TotalCharges,\n    name = 'Churn',\n    marker = dict(\n        line = dict(\n            width = 1\n        )\n    ),\n    text = 'Churn'\n)\n\nt2 = go.Bar(\n    x = df_num_retention.bin_tenure,\n    y = df_num_retention.TotalCharges,\n    name = 'Retention',\n    marker = dict(\n        line = dict(\n            width =1\n        )\n    ),\n    text = 'Retention'\n)\n\nlayout = go.Layout(\n    dict(\n        title = 'Average Total Charge by Tenure',\n        plot_bgcolor  = \"rgb(243,243,243)\",\n        paper_bgcolor = \"rgb(243,243,243)\",\n        xaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = 'Tenure Group',\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        yaxis = dict(\n            gridcolor = 'rgb(255, 255, 255)',\n            title = \"Average Total Charge\",\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        )\n    )\n\ndata = [t1,t2]\n\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(title_x=0.5)\npy.iplot(fig)","d244598c":"from sklearn.preprocessing import LabelEncoder, StandardScaler\n\n#Encoded dataframe\ndf_enc = df.copy()\ndf_enc.drop(columns=[\"bin_tenure\"], inplace=True)\ncustomerid = ['customerID']\ntarget = [\"Churn\"]\n\n#categorical columns\ncat_cols = df_enc.nunique()[df_enc.nunique() < 6].keys().tolist()\ncat_cols = [x for x in cat_cols if x not in target]\n#numerical columns\nnum_cols = [x for x in df_enc.columns if x not in cat_cols + target + customerid]\n#Binary columns with 2 values\nbin_cols = df_enc.nunique()[df_enc.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    df_enc[i] = le.fit_transform(df_enc[i])\n    \n#Duplicating columns for multi value columns\ndf_enc = pd.get_dummies(data = df_enc,columns = multi_cols)\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(df_enc[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_unscaled = df_enc.copy()\n\ndf_enc = df_enc.drop(columns = num_cols,axis = 1)\ndf_enc = df_enc.merge(scaled,left_index=True,right_index=True,how = \"left\").dropna()\ndf_enc.head(n=3)","e762f497":"#Check the values for each column for encoding and scaling.\nfor column in df_enc.columns:\n    print(column +':','\\n',np.unique(df_enc[column])[:5],'\\n')","1d4b4349":"df_enc.describe()[num_cols]","a2feaf7d":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nX = df_enc.drop(columns=['customerID','Churn'],axis=1)\ny = df_enc.Churn\n\nos = SMOTE(random_state=42)\n\nsm_X_train, sm_X_test, sm_y_train, sm_y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\nsm_X,sm_y = os.fit_sample(sm_X_train,sm_y_train)\n\n# Overwriting as all classification models will use smoted data.\nX = pd.DataFrame(data = sm_X,columns=sm_X_train.columns)\ny = pd.DataFrame(data = sm_y,columns=['Churn'])\n\ny.Churn.value_counts()","4742d669":"from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve,scorer\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom yellowbrick.classifier import ClassificationReport, DiscriminationThreshold\nfrom yellowbrick.model_selection import FeatureImportances","e28fa5b7":"# X = df_enc.drop(columns=['customerID','Churn'],axis=1)\n# y = df_enc.Churn\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3 ,random_state = 42)\n\nlog_reg = LogisticRegression(\n    C=1,\n    fit_intercept=True,\n    penalty='l2',\n    solver='liblinear'\n)\n\n\ndef performance(model, title_text, importance=False):\n    model.fit(X_train,y_train)\n\n    prediction = model.predict(X_test)\n    probability = model.predict_proba(X_test)\n\n    auc = roc_auc_score(y_test,prediction)\n\n    fpr,tpr,thresholds = roc_curve(y_test,probability[:,1])\n    \n    accuracy = accuracy_score(y_test,prediction)\n    print (\"Accuracy Score : \", accuracy,'\\n')\n    print (\"Area under curve : \", auc, '\\n')\n\n    report = ClassificationReport(model, classes=['Retention','Churn'])\n\n    report.score(X_test, y_test)\n    c = report.poof()\n\n    conf_matrix = confusion_matrix(y_test, prediction)\n\n    t1 = go.Heatmap(\n        z = conf_matrix ,\n        x = [\"Not churn\",\"Churn\"],\n        y = [\"Not churn\",\"Churn\"],\n        showscale  = True,\n        colorscale = \"Portland\",\n        name = \"Matrix\"\n    )\n\n    t2 = go.Scatter(\n        x=fpr, \n        y=tpr, \n        mode='lines', \n        line=dict(\n            color='darkorange',\n            width=2\n        ),\n        name= auc\n    )\n\n    t3 = go.Scatter(\n        x=[0, 1], \n        y=[0, 1], \n        mode='lines', \n        line=dict(\n            color='navy',\n            width=2,\n            dash='dash'\n        ),\n        showlegend=False\n    )\n\n    fig = make_subplots(\n        rows=2, \n        cols=1, \n        subplot_titles=(\n            'Confusion Matrix',\n            'Receiver Operating Characteristic'\n        )\n    )\n    fig.append_trace(t1,1,1)\n    fig.append_trace(t2,2,1)\n    fig.append_trace(t3,2,1)\n    fig.update_layout(\n        height=700, \n        width=600,\n        plot_bgcolor = 'lightgrey',\n        paper_bgcolor = 'lightgrey',\n        title_text=title_text,\n        title_x=0.5,\n        showlegend=False,\n    )\n\n    fig.update_xaxes(\n        range=[-0.05,1.1],\n        title=\"False Positive Rate\",\n        row=2, col=1\n    )\n    fig.update_yaxes(\n        range=[-0.05,1.1],\n        title=\"True Positive Rate\",\n        row=2, col=1\n    )\n\n    annot = list(fig.layout.annotations)\n    annot[0].y = 1.02\n    annot[1].y = 0.4\n    fig.layout.annotations = annot\n\n    py.iplot(fig)\n\n    # Fit and show the discrimination threshold\n    visualizer = DiscriminationThreshold(model)\n    visualizer.fit(X_train,y_train)\n    v = visualizer.poof()\n    \n    if importance == False:\n        pass\n    else:\n        feature_importance = FeatureImportances(model, relative=False)\n\n        # Fit and show the feature importances\n        feature_importance.fit(X_train,y_train)\n        f = feature_importance.poof()\n    \n    return accuracy\n\nlog = performance(\n    log_reg, \n    title_text=\"Logistic Regression Performance\",\n    importance=True)","4eac65be":"from sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\n\nrfe = RFE(log_reg)\nrfe = rfe.fit(X_train,y_train.values.ravel())\n\nprint(rfe.support_)\nprint()\nprint(rfe.ranking_)","9a4568dc":"df_rfe = pd.DataFrame({\n    \"rfe_support\":rfe.support_,\n    \"columns\":X.columns,\n    \"ranking\":rfe.ranking_\n})\n\nrfe_cols = df_rfe[df_rfe[\"rfe_support\"] == True][\"columns\"].tolist()\nrfe_cols","028ef76f":"X_train, X_test, y_train, y_test = train_test_split(X[rfe_cols], y, test_size = 0.3 ,random_state = 42)\n\nlog_rfe = performance(\n    log_reg,\n    title_text=\"Logistic Regression Performance\",\n    importance=True)","dec824ef":"from sklearn.feature_selection import SelectKBest, chi2\n\n#Have to use unstandardized dataframe since chi2 does not take negative values.\ncols = [i for i in df_unscaled.columns if i not in customerid + target ]\n\nchi2_select = SelectKBest(score_func = chi2,k = 3)\nchi2_fit = chi2_select.fit(df_unscaled[cols],df_unscaled.Churn)\n\nscore = pd.DataFrame({\n    \"features\":cols,\n    \"scores\":chi2_fit.scores_,\n    \"p_values\":chi2_fit.pvalues_ \n})\nscore = score.sort_values(by = \"scores\" ,ascending =False)\n\n#Label each columns as either numerical or categorical.\nscore[\"feature_type\"] = np.where(score[\"features\"].isin(num_cols),\"Numerical\",\"Categorical\")\ndisplay(score)\n\nt1 = go.Scatter(\n    x = score[score[\"feature_type\"] == \"Categorical\"][\"features\"],\n    y = score[score[\"feature_type\"] == \"Categorical\"][\"scores\"],\n    name = \"Categorial\",mode = \"lines+markers\",\n    marker = dict(\n        color = \"red\",\n        line = dict(width =1)\n    )\n)\n\nt2 = go.Bar(\n    x = score[score[\"feature_type\"] == \"Numerical\"][\"features\"],\n    y = score[score[\"feature_type\"] == \"Numerical\"][\"scores\"],name = \"Numerical\",\n    marker = dict(\n        color = \"blue\",\n        line = dict(width =1)\n    ),\n    xaxis = \"x2\",\n    yaxis = \"y2\"\n)\n\nlayout = go.Layout(dict(\n    title = \"Chi-Squared Scores\",\n    plot_bgcolor  = \"rgb(243,243,243)\",\n    paper_bgcolor = \"rgb(243,243,243)\",\n    xaxis = dict(\n        gridcolor = 'rgb(255, 255, 255)',\n        tickfont = dict(size =10),\n        domain=[0, 0.7],\n        tickangle = 90,zerolinewidth=1,\n        ticklen=5,gridwidth=2),\n    yaxis = dict(\n        gridcolor = 'rgb(255, 255, 255)',\n        title = \"scores\",\n        zerolinewidth=1,\n        ticklen=5,gridwidth=2),\n    margin = dict(b=200),\n    xaxis2=dict(\n        domain=[0.8, 1],tickangle = 90,\n        gridcolor = 'rgb(255, 255, 255)'),\n    yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n        )\n)\n\ndata=[t1,t2]\nfig = go.Figure(data=data,layout=layout)\nfig.update_layout(title_x=0.5)\npy.iplot(fig)","ebaa52ab":"score_10 = score.nlargest(10,'scores')['features']\nX_train, X_test, y_train, y_test = train_test_split(X[score_10], y, test_size = 0.3 ,random_state = 42)\n\nlog_chi = performance(\n    log_reg,\n    title_text=\"Logistic Regression Performance\",\n    importance=False)","f2126d50":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG,display\n\ncols = [i for i in df_enc.columns if i not in customerid + target ]    \n\nrf_x = X[cols]\nrf_y = y[target]\n\nX_train, X_test, y_train, y_test = train_test_split(rf_x, rf_y, test_size = 0.3 ,random_state = 42)\n\nrfc = RandomForestClassifier()\n\nforest = performance(\n    rfc,\n    title_text=\"Random Forest Performance\",\n    importance=True\n)\n","e209f01e":"from sklearn import svm\nfrom sklearn.model_selection import GridSearchCV ","fa59ec31":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3 ,random_state = 42)\n\nclf = svm.SVC()\n\n# Finding the optimal parameters.\nparam_grid = {'C': [0.1, 1, 10],  \n              'gamma': [1, 0.1, 0.01], \n              'kernel': ['rbf','linear']}  \n  \ngrid = GridSearchCV(clf, param_grid, refit = True) \n  \n# Fitting the model for grid search \ngrid.fit(X_train, y_train) \n\nprint(grid.best_params_) \nprint(grid.best_estimator_)\n","861819e8":"clf = svm.SVC(\n    C=1,\n    gamma=1,\n    kernel='rbf'\n)\n\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\n\nsvm_acc = accuracy_score(y_test,prediction)\nprint (\"Accuracy Score : \", svm_acc,'\\n')\n\nreport = ClassificationReport(clf, classes=['Retention','Churn'])\n\nreport.score(X_test, y_test)\nc = report.poof()\n\nconf_matrix = confusion_matrix(y_test, prediction)\n\nt1 = go.Heatmap(\n    z = conf_matrix ,\n    x = [\"Not churn\",\"Churn\"],\n    y = [\"Not churn\",\"Churn\"],\n    showscale  = True,\n    colorscale = \"Portland\",\n    name = \"Matrix\"\n)\n\nlayout = go.Layout(\n    dict(\n        title = 'Confusion Matrix SVM',\n        plot_bgcolor  = \"rgb(243,243,243)\",\n        paper_bgcolor = \"rgb(243,243,243)\",\n        xaxis = dict(\n            gridcolor = 'lightgrey',\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        yaxis = dict(\n            gridcolor = 'lightgrey',\n            zerolinewidth=1,\n            ticklen=5,\n            gridwidth=2\n            ),\n        )\n    )\n\ndata = [t1]\n\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(title_x=0.5)\npy.iplot(fig)","e37f9741":"models = pd.DataFrame({\n    'Models':[\n        'Logistic Regression',\n        'Logistic Regression + RFE',\n        'Logistic Regression + Chi^2',\n        'Random Forest Classifier',\n        'Support Vector Machine'\n    ],\n    'Scores':[\n        log, \n        log_rfe,  \n        log_chi, \n        forest, \n        svm_acc, \n    ]\n})\n\nprint(\"*** Accuracy Scores ***\")\nmodels.sort_values(by='Scores', ascending=False)","7859a0c9":"# 7. Conclusion <a class=\"anchor\" id=\"conclusion\"><\/a>","106fd763":"## 4.3 Churn Rate by Numerical Features","624da086":"# Can we predict the churn\/retention rate of Telco customers?","f4ee9729":"There are 2 outcomes for churn thus is considered binary classification. All the predictor variables dtypes have been converted to numerical (either int or float) and label encoded. ","91479480":"Recursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features.","7f573a1d":"There seems to be no relationship between **Total Charges** and **Churn Rate**.","2fdd01f9":"Generally, retention tends to increase as the the tenure increases while churn gradually decreases as the tenure increases. To get a better understanding of the effect of tenure on churn rates, tenure will be grouped into 6 bins (72 total tenure groups thus 12 months per bin).","978c9d2a":"## 5.3 Recursive Feature Elimination","a6ae924f":"## 5.1 Synthetic Minority Oversampling Technique (SMOTE)","53cabea3":"# 6. Model Accuracy Results <a class=\"anchor\" id=\"accuracy\"><\/a>","ca5c6a40":"# 2. Data Overview <a class=\"anchor\" id=\"data_overview\"><\/a>","b267324a":"## 3.2 Label Encoding","c2450e26":"Replace columns with value \"No internet service\" to \"No\" to group and reduce labels. It can be surmised that if there is no internet service, then the customer cannot have access to the features. ","df5394e0":"Random Forest Classifier is an ensemble method (based on the divide-and-conquer approach) of decision trees generated on a randomly split dataset. This collection of decision tree classifiers is also known as the forest. The individual decision trees are generated using an attribute selection indicator such as information gain, gain ratio, and Gini index for each attribute. Each tree depends on an independent random sample. In a classification problem, each tree votes and the most popular class is chosen as the final result. It is simpler and more powerful compared to the other non-linear classification algorithms.","87647ec1":"## 4.2 Customer Churn Rate By Categorical Features","6e5957ec":"# 3. Feature Engineering <a class=\"anchor\" id=\"feature_engineering\"><\/a>","e4010b9c":"## 5.6 Support Vector Machine","0f60f0b4":"## 5.4 Univariate Selection - Chi^2","02b84333":"For Chi-Squared feature selection, only the desired number of variable with the best chi-squared values are selected. However, ultimately, how many and which variables to include in the model are determined by not only the results from the feature selection but also business logic. \n\nAnother thing to note is the difference in the feature importance for the recursive feature elimination and the chi-squared. This is because each method gives a different perspective of how the variable can be useful depending on how the algorithms learn about the relationshup between y~X. \n\nTo see the difference in performance, 10 variables with the highest score will be selected.","d81b0b08":"Data Visualization libraries have been imported. Machine Learning algorithms from sklearn, yellowbrick, and etc. will be imported when creating the models.","0e858692":"# 1. Importing Library <a class=\"anchor\" id=\"importing_library\"><\/a>","58550f4b":"## 4.1 Overall Customer Churn Rate","8338ca8e":"On average, for all groups, people who churned had higher total charge than people who were retained.","7f5034a0":"Significant amount of 0% and 100% churn rate. This is due to the fact that numerous **Monthly Charges** values only have 1 instance. Therefore, the **Monthly Charges** will be rounded and grouped to the nearest whole number to further examine the relationship between **Monthly Charges** and **Churn Rate**. Same for **Total Charges**.","97ce614d":"## Content","3cd3f5f7":"The accuracy and AUC scores both decreased but, again, there are only slight differences. Now, which features to include is driven by business logic\/decision.","8f19b9dc":"Always change the datatype of variables if applicable to optimize speed and memory.","c5f70717":"The Random Forest model produced a much higher accuracy and AUC score compared to any of the logistic regression models with feature elimination. The Discrimination Threshold of 0.49 is very close to 50\/50 likelihood threshold. ","5be19b80":"After grouping the tenure into 12 month bins, it is evident that the churn rate decreases the longer the customer stays with the company. Retention rate tends to fluctuate over the 72 months period suggesting that there may be some other factors\/influences that are causing the fluctuation or just that there are less instances between 12-24 and 48-60 month periods. ","198b2fe7":"# 4. Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"><\/a>","d55f72be":"More customers are retained (0.73%) than churned (0.27%). Proportionally, there are much more customers retained than churned. However, whether the 27% churn rate is acceptable or not depends on the business goals. ","2e08c9f9":"Both the accuracy score and the area under curve for the logistic regression is approximately 0.81. This means that the model has 81% chance of classifying the churn rates correctly or capability of distinguishing between classes. \n\nNote on Precision, Recall, and F1 score:\n\nRecall = a metric which measures a models ability to find all the relevant cases in a dataset.\n\nPrecision = a metric which measures a models ability to correctly identify only relevant instances.\n\nF1 score = a weighted average of precision and recall where the metric will fall between 0 and 1, with a higher value being better.\n\nThe classification report shows an average precision value of 0.81 meaning when the model predicts the churn or retention of the customers, it is actually correct 81% of the time. Therefore, there is much less false positives for both Churn and Retention. Based on the average recall value of 0.81, the model predicts 81% of actual churn\/retention out of all churn\/retention. Therefore, there are much less amount of false negatives.\n\nThe Discrimination Threshold algorithm is tuned to look for the optimal F1 score, which is annotated as a threshold of 0.45. Therefore, the probability or score at which the positive class is chosen over the negative class is at 0.45.\n\nNote on Linear Models:\nGeneralized linear models compute a predicted independent variable via the linear combination of an array of coefficients with an array of dependent variables. GLMs are fit by modifying the coefficients so as to minimize error and regularization techniques specify how the model modifies coefficients in relation to each other. Therefore, larger coefficients are necessarily \u201cmore informative\u201d because they contribute a greater weight to the final prediction in most cases.\n\nHowever, how \"important\" a feature is only makes sense in the context of a specific model being used, and not the real world. For example, Logistic Regression and Random Forests are two completely different methods that make use of the features (in conjunction) differently to maximise predictive power. Therefore, a different set of features offer the most predictive power for each model no matter how counter intuitive the set of features may appear.\n\nBased on the Logistic Regression, Phone Service, Contract_Two year, and InternetService_No have a strong relationship with the Churn Rate.","c9a2f55b":"Each row\/instance\/observation represents a customer. \n\nEach column represents a feature\/customer\u2019s attribute that may or may not have an influence in the prediction of customer churn\/retention.","116211d9":"After SMOTE is performed, the number of churned and retained customers are equivalent.","003f161f":"Before creating various classification models, the data distribution should be considered, since, machine learning algorithms do not consider class distribution. More specifically, Decision Trees and Logistic Regression tend to have bias towards the majority class thus leading to potential misclassification of the minority class. \n\nSMOTE aims to balance class distribution by randomly increasing minority class examples by replicating them (NOT duplicating). The algorithm takes samples of the feature space for each target class and its nearest neighbors, and generates new examples that combine features of the target case with features of its neighbors. ","6c881c4a":"Performing the Recursive Feature Elimination slightly decreased the accuracy score and AUC score, but not much of a significant difference. However, since, the number of predictor variables have been decreased, the model is less complex.","321e5c9e":"The total null values constitute only ~0.16% of the total data. Therefore, dropping these instances will not have any significant effect on the accuracy of the subsequent statistical models. ","23af8599":"Univariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the response variable. ","fa63d542":"SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane that best divides the dataset into classes.","2e47ef07":"**This notebook goes through exploratory data analysis and predictive classification modeling of the Kaggle Telco Customer Churn dataset with Python.**\n\nThe primary purpose of this kernel is to showcase my skills in those two practices for a classification business problem. ","38545822":"There are various machine learning algorithms for classification problem which, in this case, is predicting the churn\/retention rate of customers using Telco's telecom network services. In this predictive analysis, 3 classification algorithms were performed:\n\n1. Logistic Regression (plus feature selection)\n2. Random Forest \n3. Support Vector Machine\n\nAs shown on the model accuracy results dataframe above, support vector machine had the highest score while logistic regression with chi-squared univariate feature selection had the lowest score. Regardless of the difference in accuracy score, each algorithm comes with their own set of pros and cons. \n\nFor example, although SVM and Random Forest provide the highest accuracy, the systematic functions of both algorithms are generally considered black boxes meaning that only the response\/result is accounted for and the function\/approach to how the result was produced is of little interest. After all, a forest consists of a large number of deep decision trees, where each tree is trained on bagged data using random selection of features, so gaining a full understanding of the decision process by examining each individual tree is infeasible. As for SVM, the data are transformed by kernel method to another space, which is not related to input space or, in other words, the data before and after SVM implementation are not in the same dimensionality thus cannot be compared. That is why for the SVM performance, feature importance has been excluded not by choice but due to impossibility.\n\nOn the other hand, the logistic regressions clearly had lower accuracy than the two counterpart algorithms. However, they compensate with interpretability as the model can measure how relevant a predictor is (based on the coefficient size) and its direction of association (positive or negative relationship). But then again, logistic regression assumes linearity of the data which, in the real world, is rarely the case.\n\nIs best accuracy the goal? Is determination of feature importance the goal even though the accuracy may suffer? In the end, which method to implement into the business model, again, is determined by business logic, intuition, and goals.","9374079c":"## 5.5 Random Forest Classifier","1881d023":"Although there are no null values, there may be empty strings or white spaces that serve as null values. Therefore, check to see if there are any features containing the two placeholders.","c4ba0d7c":"SVM produced a higher accuracy compared to the logistics regression with or without feature elimination.","c304bf48":"There seems to be no relationship between **Monthly Charges** and **Churn Rate**.","8bf9656a":"1. No null values.\n2. Multiple categorical features.","5d558404":"# 5. Models <a class=\"anchor\" id=\"models\"><\/a>","28717eca":"These are all the categorical features that may be can be plotted into the **barplot** function. Churn is excluded as that is our target variable. ","7dcd9a34":"Temporarily replacing the values for exploratory analysis. Will be reverted back to numerical datatype when performing various machine learning models.","b66f463c":"* [1. Importing Library](#importing_library)\n* [2. Data Overview](#data_overview)\n* [3. Feature Engineering](#feature_engineering)\n* [4. Exploratory Data Analysis](#eda)\n* [5. Models](#models)\n* [6. Model Accuracy Result](#accuracy)\n* [7. Conclusion](#conclusion)\n","e44bcc8b":"1. Gender, Multiple Lines, and Phone Service features does not have any signficant differences in Churn Rates between the categories. For example, the churn\/retention percentage for Female and Male (gender variable) were 0.5\/0.49 and 0.5\/0.5, respectively.\n\n2. The rest of the features seems to have noticeable Churn Rate differences.\n\nNote: While these barplots provide a general idea of how each categorical feature individually determine the customer churn rate, we cannot make any conclusion on the systematic function that provides the information on the target variable, since, there are more than one input variables (features). Some variables may be weak in predicting the Churn Rate on their own, but when combined with one or more variables, their influence may be much greater. Various machine learning algorithms are performed to make any predictions or inferences on the relationship between the features and target.","f8abb364":"## 5.2 Logistic Regression","f7cd8d3b":"## Motivation","be5b0b53":"On average, for all groups, people who churned had higher monthly charge than people who were retained."}}