{"cell_type":{"b77cdb58":"code","377c07b4":"code","c7b71755":"code","86b65e57":"code","249f2db9":"code","41dc08d1":"code","330a765e":"code","e404e942":"code","479c689c":"code","1c2e32df":"code","d47f1c27":"code","024e8d15":"code","e579ee8b":"code","99b1f290":"code","9970938e":"code","c50db228":"code","c7f2f3fb":"code","8fbba734":"code","41b9af64":"code","e6fce2c7":"code","e098ec76":"code","0d21cc5d":"code","c90674cd":"code","16114620":"code","fecc6e5c":"code","381a3547":"code","29f79ae2":"code","991cdb33":"code","80c9e9b9":"code","c6fe5c55":"code","63c060e6":"code","41d653db":"code","aadec8ad":"code","8741b90c":"code","66fe086a":"code","9f59b47c":"code","0cc4fe9d":"code","a5b077b9":"code","2b67b104":"code","81b4e080":"code","bdb903dc":"code","eb9b6164":"code","c67bf31a":"code","6e8de2ac":"code","6fa9b98c":"code","2a9c4504":"code","575d8b7e":"code","784e1dfd":"code","274d03b2":"code","05778dbf":"code","5f8c06c1":"code","d3498001":"code","f05e57c7":"code","28dfc7ca":"code","400de2bd":"code","957949d4":"code","da819858":"code","232dc29d":"code","0a0eb74b":"code","f10a2bf9":"code","7d2bf7d7":"code","0102f8df":"code","31c4a4c5":"code","15a2d504":"code","98cb729f":"code","8aa9290c":"code","0c6e9256":"markdown","7982fb0a":"markdown","f686a95a":"markdown","8666f7e6":"markdown","415796bb":"markdown","099f5f3b":"markdown","5a198695":"markdown","21c726ec":"markdown","993f1211":"markdown","e3a7eeff":"markdown","ace03533":"markdown","0845ef23":"markdown","7cd96c35":"markdown","435b793c":"markdown","b06342ec":"markdown","fed92098":"markdown","2d15ccb8":"markdown","6fdc71a0":"markdown","0115ae86":"markdown","b0134c88":"markdown","f30e8319":"markdown","a5eee1f3":"markdown","394cdcb1":"markdown"},"source":{"b77cdb58":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #visualization tool\nimport seaborn as sns #visualization tool","377c07b4":"df=pd.read_csv(\"..\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv\")#to import covid_19_data\ndf.info()#to see basic information about the data frame","c7b71755":"df.head(10)#to see the first 10 rows of the data frame","86b65e57":"df.columns#to see the columns","249f2db9":"#Correlation Map\ndf.corr()# to see correlation between rows without mapping","41dc08d1":"sns.heatmap(df.corr(),annot=True,linewidths=5,fmt=\".3f\")#mapped version of the correlation\nplt.show()","330a765e":"#Line Plot\ndf.Deaths.plot(kind=\"line\",color=\"red\",label=\"Deaths\",grid=True,alpha=1)#alpha= to set opacity. rest defines itself\ndf.Confirmed.plot(color=\"blue\",label=\"Confirmed\",alpha=0.5)\nplt.legend(loc=\"upper left\")#to put label on the plot. loc=location\nplt.xlabel(\"Time\")#to label x axis\nplt.ylabel(\"Number of Confirmed Cases and Deaths\")\nplt.title(\"Line Plot of Confirmed Cases and Deaths\")\nplt.show()","e404e942":"#Scatter Plot\ndf.plot(kind=\"scatter\",x=\"Deaths\",y=\"Recovered\",alpha=0.1,color=\"blue\")#It is not a good correlation example.But it shows us some errors of the dataset\nplt.xlabel(\"Number of Deaths\")\nplt.ylabel(\"Recovered Cases\")\nplt.title(\"Correlation Between Number of Deaths and Recovered Cases\")\nplt.show()\n","479c689c":"#Histogram\ndf.Deaths.plot(kind=\"hist\",bins=50,range=(0,250),density=True)#bins to set number of bars\nplt.xlabel(\"Deaths\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Number of Death Distribution\")\nplt.show()","1c2e32df":"#Subplots\ndf.plot(subplots=True)\nplt.show","d47f1c27":"#Histogram Subplots\nfig,axes=plt.subplots(nrows=2,ncols=1)\ndf.plot(kind=\"hist\",y=\"Confirmed\",bins=50,range=(0,250),ax=axes[0])\ndf.plot(kind=\"hist\",y=\"Confirmed\",bins=50,range=(0,250),ax=axes[1],cumulative=True)\nplt.savefig(\"graph.png\")\nplt","024e8d15":"#Filterin Data Frame with Pandas\nx=(df[\"Confirmed\"]<50000)&(df[\"Deaths\"]>10000)#too see the datas where confirmed cases are less than 50000 and number of deaths are more than 10000\ndf[x]","e579ee8b":"df.head()#to see first 5 rows","99b1f290":"df.tail()#to see last 5 rows","9970938e":"df.shape#to see the number of the rows and columns","c50db228":"df.info()#to see the data types in data set.","c7f2f3fb":"print(df[\"Country\/Region\"].value_counts(dropna=False))#dropna=False:to also count NaN values","8fbba734":"df.describe()#ignores NaN entries","41b9af64":"df1=df.head(13)#since the data set is huge, lets take first 13 entries to see boxplot clearly\ndf1.boxplot(column=\"Confirmed\",by=\"Country\/Region\")\n#from top the bottom, the horizontal lines:max,75%,median,25%,min\n#the circles are outlier datas\nplt.show()","e6fce2c7":"#lets use df1 to see melt()\ndf1=df.head()\ndf1","e098ec76":"melted=pd.melt(frame=df1,id_vars=\"SNo\",value_vars=[\"Confirmed\",\"Deaths\"])\n#id_vars =what we don't want to melt\n#value_vars=what we want to melt\nmelted","0d21cc5d":"melted.pivot(index=\"SNo\",columns=\"variable\",values=\"value\")","c90674cd":"#vertical concatenation\ndata1=df.head()\ndata2=df.tail()\n#now we can concate two data frame\nconc_data_row=pd.concat([data1,data2],axis=0,ignore_index=True)\n#axis=0: to concat vertically\nconc_data_row","16114620":"#horizontal concatenation\ndata1=df[\"Confirmed\"].head()\ndata2=df[\"Deaths\"].head()\nconc_data_col=pd.concat([data1,data2],axis=1)\n#axis=1: to concat horizontally\nconc_data_col","fecc6e5c":"df.dtypes#to see the data types in our data frame","381a3547":"#to convert data\ndf[\"Province\/State\"]=df[\"Province\/State\"].astype(\"category\")#from object to categorical\ndf.dtypes","29f79ae2":"df[\"Province\/State\"].value_counts(dropna=False)#dropna=False:to see NaN values","991cdb33":"df[\"Province\/State\"].dropna(inplace=True)#inplace=True: to not assign it to new variables","80c9e9b9":"#assert df[\"Province\/State\"].notnull().all()\n#i dont know why assert gives error since i drop Nan values with dropna()","c6fe5c55":"#Creating data frames from dictionary\ncountry=[\"Turkey\",\"Germany\"]\npopulation=[\"80\",\"55\"]\nlist_label=[\"country\",\"population\"]\nlist_col=[country,population]\nzipped=list(zip(list_label,list_col))\ndata_dict=dict(zipped)\ndf1=pd.DataFrame(data_dict)\ndf1","63c060e6":"#Adding new columns\ndf1[\"Capital\"]=[\"Ankara\",\"Berlin\"]\ndf1","41d653db":"#Broadcasting\ndf1[\"income\"]=0\ndf1","aadec8ad":"#Let's convert ObservationDate to datetime object\ndf[\"ObservationDate\"]=pd.to_datetime(df[\"ObservationDate\"])\ndf.dtypes","8741b90c":"df=df.set_index(\"ObservationDate\")#To make Observation Date as index","66fe086a":"df","9f59b47c":"print(df.loc[\"2020-01-22\"])#to select data due to date index","0cc4fe9d":"df.resample(\"M\").mean()#Resampling with months","a5b077b9":"df.resample(\"M\").mean().interpolate(\"linear\")#to interpolate due to means","2b67b104":"#to not get effected from the codes we wrote, let's start over\ndata=pd.read_csv(\"..\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv\")\ndata=data.set_index(\"SNo\")#convert SNo to index\ndata.head()","81b4e080":"#indexing using square brackets\ndata[\"Province\/State\"][3]","bdb903dc":"#indexing using column attribute and row label\ndata.Confirmed[2]","eb9b6164":"#indexing using loc accesor\ndata.loc[3,[\"Province\/State\"]]","c67bf31a":"#selecting only some columns\ndata[[\"Confirmed\",\"Province\/State\"]]","6e8de2ac":"#difference between selecting columns\nprint(type(data[\"Confirmed\"]))#series\nprint(type(data[[\"Confirmed\"]]))#data frame","6fa9b98c":"#slicing and indexing\ndata.loc[1:10,\"Province\/State\":\"Confirmed\"]","2a9c4504":"#reverse slicing\ndata.loc[10:1:-1,\"Province\/State\":\"Confirmed\"]","575d8b7e":"#from sth to end\ndata.loc[1:10,\"Confirmed\":]","784e1dfd":"#creating boolean series\nboolean=data.Recovered<500\ndata[boolean]","274d03b2":"#combining filters\nfirst_filter=data.Recovered<500\nsecond_filter=data.Confirmed<500\ndata[first_filter&second_filter]","05778dbf":"#filtering column based others\ndata.Recovered[data.Confirmed>10000]#Recovered values when confirmed>10000","5f8c06c1":"#plain python functions\ndef div(n):\n    return n\/2\ndata.Confirmed.apply(div)","d3498001":"#lambda function\ndata.Confirmed.apply(lambda n:n\/2)","f05e57c7":"#defining columns using other columns\ndata[\"Recovered+Death\"]=data.Deaths+data.Recovered\ndata.head()","28dfc7ca":"print(data.index.name)#to see the name of index","400de2bd":"#Overwriting index\ndata2=data.copy()\ndata2.index=range(100,172580,1)#to change index range\ndata2.head()#now the index starts from 100","957949d4":"data1=data.set_index([\"ObservationDate\",\"Deaths\"])#ObservationDate is outer,Deaths is inner index\ndata1.head(50)","da819858":"data1.loc[\"01\/23\/2020\",1]#to select by indexes","232dc29d":"dic={\"treatment\":[\"A\",\"A\",\"B\",\"B\"],\"gender\":[\"F\",\"M\",\"F\",\"M\"],\"response\":[10,45,5,9],\"age\":[15,4,72,65]}\ndf1=pd.DataFrame(dic)\ndf1","0a0eb74b":"#pivoting\ndf1.pivot(index=\"treatment\",columns=\"gender\",values=\"response\")","f10a2bf9":"df2=df1.set_index([\"treatment\",\"gender\"])#to make treatment and gender our indexes\ndf2","7d2bf7d7":"df2.unstack(level=0)#level determines the index(treatment)","0102f8df":"df2.unstack(level=1)#gender","31c4a4c5":"#swap inner and outer index\ndf3=df2.swaplevel(0,1)\ndf3","15a2d504":"pd.melt(df1,id_vars=\"treatment\",value_vars=[\"age\",\"response\"])","98cb729f":"df1","8aa9290c":"df1.groupby(\"treatment\").age.mean()#calculate the average age according to treatment","0c6e9256":"**FILTERING DATA FRAMES**\n\n* Creating boolean series\n* Combining filters\n* Filtering column based others","7982fb0a":"**CATEGORICALS AND GROUPY**","f686a95a":"**HIERARCHICAL INDEXING**","8666f7e6":"**2.PANDAS**\n* Fast and efficient for data frames\n* Operations with files are easy (csv, text etc.)\n* Handling missing and messy datas\n* Reshaping data for more efficient usage\n* Slicing and indexing data\n* Analysing time series data","415796bb":"# MANIPULATING DATA FRAMES WITH PANDAS\n\n**INDEXING DATA FRAMES**\n\n* Indexing using square brackets\n* Using column attribute and row label\n* Using loc accesor\n* Selecting only some columns","099f5f3b":"# INDEXING PANDAS TIME SERIES\n\ndatetime=object parse_dates(boolean):Transforms date to ISO8601(yyyy-mm-dd hh:mm:ss) format","5a198695":"# VISUAL EXPOLORATORY DATA ANALYSIS\n\nBox plots: visualize basic statistics like outliers, min\/max or quantiles","21c726ec":"# MISSING DATA AND TESTING WITH AN ASSERT\n\nWhat to do when there is missing data\n* leave as it is\n* dropna() to drop them\n* fillna() to fill missing values with NaN\n* fill missing values with test statistics like mean","993f1211":"**MELTING DATA FRAMES**","e3a7eeff":"**SLICING DATA FRAME**\n\n* Difference between selecting columns\n    * Series and data frames\n* Slicing and indexing series\n* Reverse slicing\n* From something to end","ace03533":"# RESAMPLING PANDAS TIME SERIES\n* Resampling:statistical method over different time intervals(\"M\"=month,\"A\"=year)\n* Downsapmling:reduce date time rows to slower requency like from daily to weekly\n* Upsampling:increase date time rows to faster frequency like from daily to hourly\n* Interpolate:interpolate values according to different methods line \"linear\",\"time\" or \"index\"\n\nInterpolation is a statistical method by which related known values are used to estimate an unknown price or potential yield of a security. Interpolation is achieved by using other established values that are located in sequence with the unknown value.","0845ef23":"# TRANSFORMING DATA\n\n* Plain python functions\n* Lambda function\n* Defining column using other columns","7cd96c35":"**STACKING AND UNSTACKING DATA FRAME**\n\n* Deal with multi label index\n* level:position of unstacked index\n* swaplevel:change inner and outer level index position","435b793c":"**EXPLORATORY DATA ANALYSIS(EDA)**\nvalue_counts:Frequency counts\n\noutliers:the value that is considerably higher or lower from the rest of the data\n\ndescribe() method includes:\n* count:number of entries\n* mean:average of entries\n* std:standart deviation\n* min:minimum entry\n* max:maximum entry","b06342ec":"**TIDY DATA**\n\nTo tidy data we use melt():\n* melt() to unpivot given data frame from wide format to long format,optionally leaving identifier variables set","fed92098":"**PIVOTING DATA**\n\npivot() is simply reverse of melt","2d15ccb8":"**DATA TYPES**\n\nobject(string),boolean,integer,float,categorical.","6fdc71a0":"# BUILDING DATA FRAMES FROM SCRATCH\n\n* We can build dataframe from dictionaries.\n    * zip():This function returns a list of tuples,where the i'th tuple contains the i'th element from each of the argument sequences or iterables.\n    * Broadcasting:Creating new column and assign a value to entire column","0115ae86":"# CLEANING DATA\n**DIAGNOSE DATA FOR CLEANING**\nBefor exploring data, we need to diagnose and clean it.\n\nUnclean Data:\n* Column name incostistency like upper-lower case letter,space usage etc.\n* Missing data\n* Different language\n* head(),tail(),columns(),shape(),info()","b0134c88":"# **DATA SCIENCE CHEATSHEET WITH COVID19 DATASET**\nNeeded Skills for Data Sc\u0131ence\n1. Basic Tools: Python, R, SQL etc.\n2. Basic Statistics: Such as mean, median, standart deviation etc.\n3. Data Munging: Correcting messy and difficult data.\n4. Data Visualization: Visualizing data with matplotlib, seaborn etc.\n5. Machine Learning: What is the math behind and how to implement it.\n","f30e8319":"# **PYTHON**\n **1.MATPLOTLIB**\n\nBasic python library to plot lines, scatters and histograms.Allows us to customize the plots.(colors,labels,opacity,grid,figsize,tickets etc.)\n1. Line plots are better when x is time.\n2. Scatter is better when there is correlations between variables.\n3. Histogram is better to see distribution of numerical data.\n","a5eee1f3":"**CONCATENATING DATA**","394cdcb1":"# INDEX OBJECTS AND LABELED DATA"}}