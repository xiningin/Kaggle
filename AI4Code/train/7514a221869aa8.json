{"cell_type":{"c8d7e073":"code","edbe5f3b":"code","c51cb78b":"code","e0e702b4":"code","069df41c":"code","327858c4":"code","ffd76bbd":"code","6204b2ef":"code","74852b96":"code","e490aa89":"code","6758e6bd":"code","46f2dfc5":"code","1133a0b7":"code","bb4fc704":"code","aee92eea":"code","a9659d7b":"code","11b56f8e":"code","1f9c51ae":"code","ce54b1ce":"code","77a0d737":"code","2d00aedb":"code","53f40565":"code","c3f32a17":"code","ae85e0c5":"code","d8fa113d":"code","3c37ed3d":"code","a3709e1e":"code","3f026b15":"code","1490fa5d":"code","7d113b77":"code","cfc5935d":"markdown","c81e07d3":"markdown","4ba393e2":"markdown","c4c1b200":"markdown","81ee13bc":"markdown","ac7e8e21":"markdown","9752b56b":"markdown","c354f68c":"markdown","4fffb823":"markdown","6b153c76":"markdown","cbdc5a49":"markdown","35882f75":"markdown","f9cb8452":"markdown","e0ddb46f":"markdown"},"source":{"c8d7e073":"import os\ncsv_files = []\njson_files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        print(file_path)\n        if file_path.endswith('.csv'):\n            csv_files.append(file_path)\n        elif file_path.endswith('.json'):\n            json_files.append(file_path)","edbe5f3b":"# sort filenames (alphabetically) to maintain right file order of both file formats\nsorted_csv = sorted(csv_files)\nsorted_json = sorted(json_files)","c51cb78b":"import pandas as pd\n\n# read data and drop unused columns to save space\ndef read_data(csv_names, json_names):\n    assert len(csv_names) == len(json_names)\n    csvs, jsons = [], []\n    for idx, _ in enumerate(csv_names):\n        df = pd.read_csv(csv_names[idx], encoding='utf-8', encoding_errors='ignore', infer_datetime_format=True)\n        df.drop(columns=['tags', 'thumbnail_link', 'description'], inplace=True)\n        csvs.append(df)\n        jsn = pd.read_json(json_names[idx])\n        jsn.drop(columns=['kind','etag'], inplace=True)\n        jsons.append(jsn)\n    return csvs, jsons","e0e702b4":"from IPython.display import display\ncsvs, jsons = read_data(sorted_csv, sorted_json)\ndisplay(csvs[0].head())\ncsvs[0].dtypes # check data types to confirm if pandas inferred them correctly","069df41c":"# convert columns to their correct type\ndef convert_data(csv_data):\n    for idx, _ in enumerate(csv_data):\n        csv_data[idx]['trending_date'] = pd.to_datetime(csv_data[idx]['trending_date'], format='%y.%d.%m', utc=True)\n        csv_data[idx]['title'] = csv_data[idx]['title'].astype(str)\n        csv_data[idx]['channel_title'] = csv_data[idx]['channel_title'].astype(str)\n        csv_data[idx]['category_id'] = csv_data[idx]['category_id'].astype('int64')\n        csv_data[idx]['publish_time'] = pd.to_datetime(csv_data[idx]['publish_time'])\n        csv_data[idx]['views'] = csv_data[idx]['views'].astype('int64')\n        csv_data[idx]['likes'] = csv_data[idx]['likes'].astype('int64')\n        csv_data[idx]['dislikes'] = csv_data[idx]['dislikes'].astype('int64')\n        csv_data[idx]['comment_count'] = csv_data[idx]['comment_count'].astype('int64')\n        csv_data[idx]['comments_disabled'] = csv_data[idx]['comments_disabled'].astype(bool)\n        csv_data[idx]['ratings_disabled'] = csv_data[idx]['ratings_disabled'].astype(bool)\n        csv_data[idx]['video_error_or_removed'] = csv_data[idx]['video_error_or_removed'].astype(bool)\n    return csv_data","327858c4":"converted_csvs = convert_data(csvs)\ndisplay(converted_csvs[0].head())\nconverted_csvs[0].dtypes","ffd76bbd":"display(jsons[0].head())\ndisplay(jsons[0].dtypes)\nprint('\\n{}'.format(jsons[0].values[0])) # prints first row of first dataframe constructed from json","6204b2ef":"# extract id, category name and convert them to their expected type\ndef extract_json(json_data):\n    cleaned = []\n    for idx, _ in enumerate(json_data):\n        df = pd.DataFrame([dict(item[0]) for item in json_data[idx].values])\n        df['category_name'] = df['snippet'].apply(lambda snp: dict(snp).get('title', 'Unknown'))\n        df = df[['id', 'category_name']]\n        df.drop_duplicates(inplace=True)\n        df['id'] = df['id'].astype('int64')\n        df['category_name'] = df['category_name'].astype(str)\n        cleaned.append(df.set_index('id'))\n    return cleaned","74852b96":"extracted_jsons = extract_json(jsons)\nextracted_jsons[0].head()","e490aa89":"# join csvs and their corresponding json by category_id\ndef join_by_category(csv_data, json_data):\n    joined = []\n    for idx, _ in enumerate(csv_data):\n        df = csv_data[idx].join(json_data[idx], on='category_id') # left join\n        df['category_name'] = df['category_name'].fillna('NA') # fill category names for categories that are not found in json\n        joined.append(df)\n    return joined","6758e6bd":"joined_data = join_by_category(converted_csvs, extracted_jsons)\nfirst_five_cat = extracted_jsons[0].head(5)['category_name'].values\n\n# to verify if joins are performed correctly\njoined_data[0][joined_data[0]['category_name'].isin(first_five_cat)].head()","46f2dfc5":"# combine dataframes of different countries\ndef combine(joined, filenames):\n    assert len(joined) == len(filenames)\n    df = pd.DataFrame()\n    for idx, _ in enumerate(filenames):\n        country = filenames[idx].split('\/')[-1].split('_')[0]\n        joined[idx]['country'] = country\n        df = pd.concat([df,joined[idx]], axis=0)\n    return df","1133a0b7":"combined = combine(joined_data, sorted_json)\ndisplay(combined.head())\ncombined.dtypes","bb4fc704":"combined.isna().sum() # confirm that we have no empty values","aee92eea":"row_dup_count = combined.groupby(list(combined.columns), as_index=False).size()\nrow_dup_count = row_dup_count[row_dup_count['size'] > 1].sort_values('size')\nrow_dup_count.tail()","a9659d7b":"dropped = combined.drop_duplicates()\ndropped[dropped.duplicated(keep=False)] # to verify that there are no duplicates","11b56f8e":"g_col = ['country', 'trending_date', 'video_id']\nagg_col = ['title']\n\nvid_title_cnt = dropped.groupby(g_col)[agg_col].count()\nerror_count = vid_title_cnt[vid_title_cnt.title > 1].sort_values(agg_col)\ndisplay(error_count.tail())\ndisplay(dropped[(dropped.video_id == '#NAME?') & (dropped.trending_date == '2018-03-08') & (dropped.country == 'MX')].head())","1f9c51ae":"g_col = ['channel_title', 'title', 'publish_time', 'trending_date', 'country']\ndup_cnt = dropped.groupby(g_col, as_index=False).size().sort_values('size')\ndup_cnt = dup_cnt[dup_cnt['size'] > 1]\ndisplay(dup_cnt.tail())\n\nsample = dup_cnt.loc[dup_cnt.index[0]]\ndropped[(dropped['channel_title'] == sample[0]) & (dropped['title'] == sample[1]) & (dropped['publish_time'] == sample[2]) & (dropped['trending_date'] == sample[3]) & (dropped['country'] == sample[4])]","ce54b1ce":"always_inc = ['views']\nsort = dropped.sort_values(g_col + always_inc)\n\ndisplay(sort[sort.duplicated(subset=g_col, keep=False)].head()) # show first 5 duplicated videos\n\ndisplay(sort[(sort['video_id'] == 'F3_j5bdh57E') & (sort['trending_date'] =='2018-01-19')]) # before\ndropped_view = sort.drop_duplicates(subset=g_col, keep='last').reset_index(drop=True) # try to keep video with most updated info\n# after\ndisplay(dropped_view[(dropped_view['video_id'] == 'F3_j5bdh57E') & (dropped_view['trending_date'] =='2018-01-19')])\n\n\ndup_count = dropped_view.groupby(g_col, as_index=False).size()\ndup_count[dup_count['size'] > 1].sort_values('size').tail() # verify that there are no duplicate videos","77a0d737":"cat_like = dropped_view.groupby(['category_name'])[['likes', 'dislikes']].agg('sum').reset_index()\ncat_like['like_ratio'] = cat_like['likes'] \/ cat_like['dislikes']\ncat_like.head()","2d00aedb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')","53f40565":"plt.figure(figsize=(10,6))\nplt.title('Ratio of likes-dislikes in different categories')\nsns.barplot(y='category_name', x='like_ratio', data=cat_like.sort_values('like_ratio', ascending=False))\nplt.show()","c3f32a17":"cat_cnt = dropped_view.groupby(['country', 'category_name'])[['likes', 'views']].agg('sum').reset_index()\n\nfor cnt in cat_cnt['country'].unique():\n    plt.figure(figsize=(10,6))\n    plt.title('Total likes in different categories for country {}'.format(cnt))\n    sns.barplot(y='category_name', x='likes', data=cat_cnt[cat_cnt['country'] == cnt].sort_values('likes', ascending=False))\n    plt.show()","ae85e0c5":"g_col = ['country','title', 'channel_title', 'publish_time']\ntrending_date_count = dropped_view.groupby(g_col, as_index=False)[['trending_date']].nunique()\ntop5_trending = trending_date_count.groupby(['country'], as_index=False).apply(lambda group: group.sort_values(['trending_date'], ascending=False).head(5))\ntop5_trending.columns = g_col + ['trending_date_count']\nfor cnt in top5_trending['country'].unique():\n    display(top5_trending[top5_trending['country'] == cnt])","d8fa113d":"g_col = ['country','title', 'channel_title', 'publish_time']\nmost_liked = dropped_view.groupby(g_col, as_index=False)[['likes']].last()\ntop5_liked = most_liked.groupby(['country'], as_index=False).apply(lambda group: group.sort_values(['likes'], ascending=False).head(5))\nfor cnt in top5_liked['country'].unique():\n    display(top5_liked[top5_liked['country'] == cnt])","3c37ed3d":"import numpy as np\ng_col = ['country','title', 'channel_title', 'publish_time']\ntrending_col = ['trending_date']\ntrend_date = dropped_view[g_col + trending_col]\ntrend_date['days_to_trend'] = (trend_date['trending_date'].dt.date - trend_date['publish_time'].dt.date).dt.days\nvid_time_trend = trend_date.groupby(g_col, as_index=False)[['days_to_trend']].max()\nsumm_df = pd.DataFrame()\nfor cnt in vid_time_trend.country.unique():\n    data = vid_time_trend[vid_time_trend['country'] == cnt]\n    summ = data.describe().T\n    summ['country'] = cnt\n    summ_df = pd.concat([summ_df,  summ])\n\nsumm_df.set_index('country', inplace=True)\ndisplay(summ_df)","a3709e1e":"vid_trend_count = trend_date.groupby(g_col, as_index=False)[['trending_date']].nunique()\nvid_trend_count.columns = g_col + ['days_trending']\ndisplay(vid_trend_count.sort_values('days_trending').tail())\n\nfor cnt in vid_trend_count.country.unique():\n    data = vid_trend_count[vid_trend_count['country'] == cnt]\n    plt.figure(figsize=(10,5))\n    plt.title('Count of video trending days in country {}'.format(cnt))\n    plt.hist(x='days_trending', data=data, density=True, bins=range(0, 40, 2))\n    plt.xlabel('trending_day_count')\n    plt.ylabel('density')\n    plt.show()","3f026b15":"g_col = ['country','title', 'channel_title', 'publish_time']\nagg_col = ['trending_date']\nsort = dropped_view.sort_values(g_col + agg_col)\nshift_down = sort.groupby(g_col)[agg_col].shift()\nsort['trending_diff'] = ((sort['trending_date'] - shift_down['trending_date']).dt.days).fillna(0)\nmax_diff = sort.groupby(g_col, as_index=False)[['trending_diff']].max()\nmax_diff = max_diff[max_diff['trending_diff'] > 1].sort_values('trending_diff')\n\ndisplay(max_diff.tail())\n\nfor cnt in max_diff.country.unique():\n    data = max_diff[max_diff['country'] == cnt]\n    plt.figure(figsize=(10,5))\n    plt.title('Maximum gap in video trending dates for country {}'.format(cnt))\n    plt.hist(x='trending_diff', data=data, density=True)\n    plt.xlabel('trending_date_gap')\n    plt.ylabel('density')\n    plt.show()","1490fa5d":"g_col = ['country','channel_title','title', 'publish_time', 'trending_date']\nagg_col = ['views']\n\npop_channel = dropped_view.sort_values(g_col).groupby(g_col[:-1], as_index=False)[agg_col].last()\nchannel_total = pop_channel.groupby(g_col[0:2], as_index=False)[agg_col].sum()\ncountry_top = channel_total.groupby('country', as_index=False).apply(lambda x: x.sort_values(['views', 'channel_title'], ascending=[False, True]).head(5))\n\n\nfor cnt in country_top.country.unique():\n    data = country_top[country_top['country'] == cnt]\n    plt.figure(figsize=(10,5))\n    plt.title('Top 5 channels with most views in country {}'.format(cnt))\n    sns.barplot(y='channel_title', x='views', data=data, palette='mako')\n    plt.show()","7d113b77":"g_col = ['country','channel_title','title', 'publish_time', 'trending_date']\nagg_col = ['views', 'likes', 'dislikes', 'comment_count']\n\nnum_data = dropped_view.sort_values(g_col).groupby(g_col[:-1], as_index=False)[agg_col].last()\nfor cnt in num_data.country.unique():\n    data = num_data[num_data['country'] == cnt]\n    plt.figure(figsize=(10,6))\n    plt.title('Correlation heatmap of country {}'.format(cnt))\n    sns.heatmap(data.iloc[:,-4:].corr(), cmap='mako', annot=True, fmt='.2f')\n    plt.show()","cfc5935d":"observation: there are some rows which have up to 4 copies, so we need to remove the duplicates for correct analysis results","c81e07d3":"observation: videos that belong to music, entertainment and comedy categories are most prefered by more than half of the countries. whereas people in Japan, Korea and Russia prefer videos of category people and blogs more than comedy","4ba393e2":"observation: trending videos likes and dislikes are (positively) correlated in all countries considered. in overall for a video, getting high likes did not mean obtaining low dislikes and this could probably be due to the fact that some groups of audiences who have watched the video prefer different content. for UK, no. of comments have stronger tendency to increase with dislikes than likes as opposed to other countries.","c4c1b200":"observation: in majority of the countries considered, most of videos has taken at maximum 3 days to get trending after being published whilst in England and US half of the videos has taken one week or more to be trending","81ee13bc":"observation: for some videos, there are both old and updated data thus we need to clean them before analysis","ac7e8e21":"observation: ibighit, PewDiePie and Dude Perfect are 3 high view channels found across more than half of the countries. surprisingly, Great Britain exhibit different pattern from other nations in terms of top five channels with most views","9752b56b":"observation: #NAME? is id of multiple videos, we might need more information that does #NAME? means unavailable video id as it seems unlikely for different videos to have same id","c354f68c":"observation: for majority of countries, a video has taken less than a week to become popular again. nevertheless in USA and England, a video's trending dates can be as far as 3 weeks or more","4fffb823":"NOTE: here we assumed that for same video, record that come later after sorting ascendingly by views is the most updated one. to be more sure we might need data telling when the record was created\/updated","6b153c76":"observation: dataframes read from json files are in unusable form and contain many unused data so we need to extract useful data fields from them","cbdc5a49":"observation: in all countries except UK and US, total days a video intrended hardly exceed seven. on the other hand, America and Britain have some videos become trending thirty to forty times in total","35882f75":"observation: pandas has incorrectly inferred type of some columns e.g. date & date time type etc. so we might need to convert column values to our expected type","f9cb8452":"observation: for Canada, some of most trending videos are also most liked videos. for all countries in this dataset not including India, some videos by ibighit have been favored by audiences a lot","e0ddb46f":"observation: according to the data, audience more prefer videos from categories that give them positive emotions, new knowledge or entertainment than unprefering. in contrast, more dislikes are given to videos related to society and politics compared to likes"}}