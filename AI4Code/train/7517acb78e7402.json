{"cell_type":{"99e70bb2":"code","a72d151a":"code","a9cd9f77":"code","c48e28bc":"code","c11d6758":"code","5fda81f3":"code","e2cd6c09":"code","6d81f28b":"code","dbad3d6f":"code","cb9e38c4":"code","936ef623":"code","d9c286a7":"code","28997add":"code","10eb27c2":"code","6d40ea47":"code","702ad4e9":"code","0d778077":"code","2823c621":"code","3890f2c1":"code","71ad4700":"code","be30a2fb":"code","d144a990":"code","68ec94f7":"code","a4f6ece2":"code","4edac5c8":"code","0164c30e":"code","0fa86e99":"code","d68f81fb":"code","b03ed8b7":"code","5a571989":"code","9e529003":"code","35c30a30":"code","8ae1acdd":"code","12b93e57":"markdown","befa26e7":"markdown","29feb74b":"markdown","5939498a":"markdown","10461362":"markdown","939b00be":"markdown","3731a3b4":"markdown","6708a1e7":"markdown","5a28b4ad":"markdown","3fb5b749":"markdown","6ac8f1db":"markdown","92a4bfb1":"markdown","64b87850":"markdown","7d69bef7":"markdown"},"source":{"99e70bb2":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import the numpy and pandas package\n\nimport numpy as np\nimport pandas as pd\n\n# Data Visualisation\n\nimport matplotlib.pyplot as plt \n ","a72d151a":"# Importing all datasets\nchurn_data = pd.read_csv('..\/input\/churn_data.csv')\ncustomer_data = pd.read_csv('..\/input\/customer_data.csv')\ninternet_data = pd.read_csv('..\/input\/internet_data.csv')","a9cd9f77":"# Merging on 'customerID'\ndf_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')\n\n# Final dataframe with all predictor variables\ndataset = pd.merge(df_1, internet_data, how='inner', on='customerID')\n\n# Let's see the head of our master dataset\ndataset.head()\n\n# let's look at the statistical aspects of the dataframe\ndataset.describe()\n\n# Let's see the type of each column\ndataset.info()","c48e28bc":"# Checking Null values\ndataset.isnull().sum()*100\/dataset.shape[0]\n\n","c11d6758":"#Replacing NAN values in totalcharges\ndataset['TotalCharges'].describe()\ndataset['TotalCharges'] = dataset['TotalCharges'].replace(' ', np.nan)\ndataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'])\n\nvalue = (dataset['TotalCharges']\/dataset['MonthlyCharges']).median()*dataset['MonthlyCharges']\ndataset['TotalCharges'] = value.where(dataset['TotalCharges'] == np.nan, other =dataset['TotalCharges'])\ndataset['TotalCharges'].describe()\n\n","5fda81f3":"\n#Model Building\n#Data Preparation\n#Converting some binary variables (Yes\/No) to 0\/1\n# List of variables to map\n\nvarlist =  ['PhoneService', 'PaperlessBilling', 'Churn', 'Partner', 'Dependents']","e2cd6c09":"# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})","6d81f28b":"# Applying the function to the var list\ndataset[varlist] = dataset[varlist].apply(binary_map)\ndataset.head()","dbad3d6f":"#For categorical variables with multiple levels, create dummy features (one-hot encoded)\n# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(dataset[['Contract', 'PaymentMethod', 'gender', 'InternetService']], drop_first=True)\n\n# Adding the results to the master dataframe\ndataset = pd.concat([dataset, dummy1], axis=1)\ndataset.head()","cb9e38c4":"# Creating dummy variables for the remaining categorical variables and dropping the level with big names.\n\n# Creating dummy variables for the variable 'MultipleLines'\nml = pd.get_dummies(dataset['MultipleLines'], prefix='MultipleLines')\n# Dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'], 1)\n#Adding the results to the master dataframe\ndataset = pd.concat([dataset,ml1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineSecurity'.\nos = pd.get_dummies(dataset['OnlineSecurity'], prefix='OnlineSecurity')\nos1 = os.drop(['OnlineSecurity_No internet service'], 1)\n# Adding the results to the master dataframe\ndataset = pd.concat([dataset,os1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineBackup'.\nob = pd.get_dummies(dataset['OnlineBackup'], prefix='OnlineBackup')\nob1 = ob.drop(['OnlineBackup_No internet service'], 1)\n# Adding the results to the master dataframe\ndataset = pd.concat([dataset,ob1], axis=1)\n\n# Creating dummy variables for the variable 'DeviceProtection'. \ndp = pd.get_dummies(dataset['DeviceProtection'], prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'], 1)\n# Adding the results to the master dataframe\ndataset = pd.concat([dataset,dp1], axis=1)\n\n# Creating dummy variables for the variable 'TechSupport'. \nts = pd.get_dummies(dataset['TechSupport'], prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'], 1)\n# Adding the results to the master dataframe\ndataset = pd.concat([dataset,ts1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingTV'.\nst =pd.get_dummies(dataset['StreamingTV'], prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'], 1)\n# Adding the results to the master dataframe\ndataset = pd.concat([dataset,st1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingMovies'. \nsm = pd.get_dummies(dataset['StreamingMovies'], prefix='StreamingMovies')\nsm1 = sm.drop(['StreamingMovies_No internet service'], 1)\n# Adding the results to the master dataframe\ndataset = pd.concat([dataset,sm1], axis=1)\ndataset.head()\n","936ef623":"# We have created dummies for the below variables, so we can drop them\ndataset = dataset.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)\ndataset.info()","d9c286a7":"# Checking for outliers in the continuous variables\nnum_telecom = dataset[['tenure','MonthlyCharges','SeniorCitizen','TotalCharges']]\n# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_telecom.describe(percentiles=[.25, .5, .75, .90, .95, .99])\n","28997add":"# Checking up the missing values (column-wise)\ndataset.isnull().sum()\n","10eb27c2":"# Removing NaN TotalCharges rows\ndataset = dataset[~np.isnan(dataset['TotalCharges'])]","6d40ea47":"# Checking percentage of missing values after removing the missing values\nround(100*(dataset.isnull().sum()\/len(dataset.index)), 2)","702ad4e9":"\n# Putting feature variable to X\nfrom sklearn.model_selection import train_test_split #use 'cross_validation' instead of\n                                                     #'model_selection' Executing in jupyter or spyder \nX = dataset.drop(['Churn','customerID'], axis=1)\nX.head()\n\n# Putting response variable to y\ny = dataset['Churn']\n\ny.head()","0d778077":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)\n","2823c621":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\n\nX_train.head()","3890f2c1":"#Model Building\n# Logistic regression model\nimport statsmodels.api as sm\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()\n\n","71ad4700":"#Feature Selection Using RFE\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)\nrfe.support_\n\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))\n\n\ncol = X_train.columns[rfe.support_]\nX_train.columns[~rfe.support_]\n","be30a2fb":"#Adding a constant\n\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()\n","d144a990":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n\n\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","68ec94f7":"#Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","a4f6ece2":"# Confusion matrix\nfrom sklearn import metrics\nconfusion_matrix = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion_matrix)\n","4edac5c8":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))\n","0164c30e":"#Making predictions on the test set\nX_test[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_test[['tenure','MonthlyCharges','TotalCharges']])\nX_test = X_test[col]\nX_test.head()\n\nX_test_sm = sm.add_constant(X_test)\ny_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","0fa86e99":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","d68f81fb":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","b03ed8b7":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index\n","5a571989":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","9e529003":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\ny_pred_final.head()","35c30a30":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex_axis(['CustID','Churn','Churn_Prob'], axis=1)\n# Let's see the head of y_pred_final\ny_pred_final.head()\ny_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.42 else 0)\ny_pred_final.head()\n","8ae1acdd":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.final_predicted)","12b93e57":"**\tMerging all datasets based on condition (\"customer_id \")**","befa26e7":"** \tOne hot encoding**","29feb74b":"** \t Check the overall accuracy**","5939498a":"**# There are no NULL values in the dataset, hence it is clean**","10461362":" \tBinary encoding**","939b00be":"** \tData Cleaning - checking the null values**","3731a3b4":"**LOGISTIC REGRESSION**\n\nProblem Statement :\n\n \"You have a telecom firm which has collected data of all its customers\"\nThe main types of attributes are :\n\n\t1.Demographics (age, gender etc.)\n    \n\t2.Services availed (internet packs purchased, special offers etc)\n    \n\t3.Expenses (amount of recharge done per month etc.)\n    \nBased on all this past information, you want to build a model which will predict whether a particular customer will churn or not. \nSo the variable of interest, i.e. the target variable here is \u2018Churn\u2019 which will tell us whether or not a particular customer has churned. It is a binary variable  1 means that the customer has churned and 0 means the customer has not churned.\nWith 21 predictor variables we need to predict whether a particular customer will switch to another telecom provider or not.\n","6708a1e7":"** \tCreate a confusion matrix on train set and test**","5a28b4ad":"** \tCreating dummy variables and removing the extra columns**","3fb5b749":"**Import necessary libraries**","6ac8f1db":"**\tImporting all datasets**","92a4bfb1":"**\tCreating a new column predicted with 1  if churn  > 0.5  else 0**","64b87850":"** \tGetting the predicted values on train set**","7d69bef7":"** Model building******"}}