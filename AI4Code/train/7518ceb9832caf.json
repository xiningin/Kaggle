{"cell_type":{"be7e77a3":"code","5433e2f8":"code","6bb926db":"code","8269b857":"code","715fc2a3":"code","423ec7cd":"code","bfc229ff":"code","733c5254":"code","6426a0bd":"code","e561596f":"code","f1862bcd":"code","621a93ae":"code","86fe8c4d":"code","76773644":"code","61485cba":"code","1b563f2c":"code","eba00ecd":"code","c6b4cb8f":"code","614e6021":"code","e8485f04":"code","0911b25f":"code","5e12f731":"code","333afae5":"code","bf7b7781":"code","68bba06d":"code","254cfa33":"code","37349cb0":"code","a05dd83a":"code","c54b5357":"code","4b381b82":"code","0d46cbe3":"code","b6f16484":"markdown","31c0b62f":"markdown","25e40c21":"markdown","63a8fd17":"markdown","ca60760c":"markdown","90ec8c45":"markdown"},"source":{"be7e77a3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5433e2f8":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.decomposition import PCA\n\nfrom bayes_opt import BayesianOptimization\nimport warnings\nwarnings.filterwarnings('ignore')\n\nRANDOM_SEED = 123","6bb926db":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")","8269b857":"train['magic1'] = train['cont10']\/train['cont11']\ntrain['magic2'] = train['cont11']\/train['cont10']\ntrain['magic3'] = train['cont1']\/train['cont7']\ntrain['magic4'] = train['cont7']\/train['cont1']\ntrain['magic5'] = train['cont4']\/train['cont6']\n\ntest['magic1'] = test['cont10']\/test['cont11']\ntest['magic2'] = test['cont11']\/test['cont10']\ntest['magic3'] = test['cont1']\/test['cont7']\ntest['magic4'] = test['cont7']\/test['cont1']\ntest['magic5'] = test['cont4']\/test['cont6']","715fc2a3":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\nX = train.drop('target', axis=1)\ny = train.target","423ec7cd":"from catboost import CatBoostRegressor\ncat = CatBoostRegressor(iterations=1000)","bfc229ff":"model = [cat]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"CAT RMSE Mean Score: \", np.mean(score))","733c5254":"model = [cat]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=10, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"CAT RMSE Mean Score: \", np.mean(score))","6426a0bd":"import lightgbm\nlgbm = lightgbm.LGBMRegressor(random_state=RANDOM_SEED, n_jobs=-1, metric= 'rmse')","e561596f":"model = [lgbm]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"LGBM RMSE Mean Score: \", np.mean(score))","f1862bcd":"model = [lgbm]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=10, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"LGBM RMSE Mean Score: \", np.mean(score))","621a93ae":"from xgboost import XGBRegressor\nxgbr = XGBRegressor(random_state=RANDOM_SEED)","86fe8c4d":"model = [xgbr]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"XGB RMSE Mean Score: \", np.mean(score))","76773644":"dtrain = lightgbm.Dataset(data=X, label=y)\n\ndef hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight, learning_rate):\n      \n        params = {'application':'regression','num_iterations': 5000,\n                  'early_stopping_round':100, 'metric':'rmse'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['learning_rate'] = learning_rate\n        cv_result = lightgbm.cv(params, dtrain, nfold=3, \n                                seed=RANDOM_SEED, stratified=False, \n                                verbose_eval =None, metrics=['rmse'])\n        \n        return -np.min(cv_result['rmse-mean'])","61485cba":"pds = {\n    'num_leaves': (5, 50),\n    'feature_fraction': (0.2, 1),\n    'bagging_fraction': (0.2, 1),\n    'max_depth': (2, 20),\n    'min_split_gain': (0.001, 0.1),\n    'min_child_weight': (10, 50),\n    'learning_rate': (0.01, 0.5),\n      }","1b563f2c":"# optimizer = BayesianOptimization(hyp_lgbm,pds,random_state=RANDOM_SEED)\n# optimizer.maximize(init_points=10, n_iter=50)","eba00ecd":"# optimizer.max['params']","c6b4cb8f":"import catboost as cgb\n\ndef cat_hyp(depth, bagging_temperature, l2_leaf_reg, learning_rate):\n  params = {\"iterations\": 100,\n            \"loss_function\": \"RMSE\",\n            \"verbose\": False} \n  params[\"depth\"] = int(round(depth)) \n  params[\"bagging_temperature\"] = bagging_temperature\n  params[\"learning_rate\"] = learning_rate\n  params[\"l2_leaf_reg\"] = l2_leaf_reg\n  \n  cat_feat = [] # Categorical features list, we have nothing in this dataset\n  cv_dataset = cgb.Pool(data=X,\n                        label=y,\n                        cat_features=cat_feat)\n\n  scores = cgb.cv(cv_dataset,\n              params,\n              fold_count=3)\n  return -np.min(scores['test-RMSE-mean']) ","614e6021":"# Search space\npds = {'depth': (4, 10),\n       'bagging_temperature': (0.1,10),\n       'l2_leaf_reg': (0.1, 10),\n       'learning_rate': (0.1, 0.2)\n        }","e8485f04":"# optimizer = BayesianOptimization(cat_hyp, pds, random_state=RANDOM_SEED)\n# optimizer.maximize(init_points=10, n_iter=50)","0911b25f":"# optimizer.max['params']","5e12f731":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X, y, feature_names=X.columns.values)\n\ndef hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma, learning_rate):\n    params = {\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'nthread':-1\n     }\n    \n    params['max_depth'] = int(round(max_depth))\n    params['subsample'] = max(min(subsample, 1), 0)\n    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n    params['min_child_weight'] = int(min_child_weight)\n    params['gamma'] = max(gamma, 0)\n    params['learning_rate'] = learning_rate\n    scores = xgb.cv(params, dtrain, num_boost_round=500,verbose_eval=False, \n                    early_stopping_rounds=10, nfold=3)\n    return -scores['test-rmse-mean'].iloc[-1]","333afae5":"pds ={\n  'min_child_weight':(3, 20),\n  'gamma':(0, 5),\n  'subsample':(0.7, 1),\n  'colsample_bytree':(0.1, 1),\n  'max_depth': (3, 10),\n  'learning_rate': (0.01, 0.5)\n}","bf7b7781":"# optimizer = BayesianOptimization(hyp_xgb, pds, random_state=RANDOM_SEED)\n# optimizer.maximize(init_points=4, n_iter=15)","68bba06d":"# optimizer.max['params']","254cfa33":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression","37349cb0":"param_lgbm = {\n     'bagging_fraction': 0.973905385549851,\n     'feature_fraction': 0.2945585590881137,\n     'learning_rate': 0.03750332268701348,\n     'max_depth': int(7.66),\n     'min_child_weight': int(41.36),\n     'min_split_gain': 0.04033836353603582,\n     'num_leaves': int(46.42),\n     'application':'regression',\n     'num_iterations': 5000,\n     'metric': 'rmse'\n}\n\nparam_cat = {\n     'bagging_temperature': 0.31768713094131684,\n     'depth': int(8.03),\n     'l2_leaf_reg': 1.3525686450404295,\n     'learning_rate': 0.2,\n     'iterations': 100,\n     'loss_function': 'RMSE',\n     'verbose': False\n}\n\nparam_xgb = {\n     'colsample_bytree': 0.8119098377889549,\n     'gamma': 2.244423418642122,\n     'learning_rate': 0.015800631696721114,\n     'max_depth': int(9.846),\n     'min_child_weight': int(15.664),\n     'subsample': 0.82345,\n     'objective': 'reg:squarederror',\n     'eval_metric':'rmse',\n     'num_boost_roun' : 500\n}","a05dd83a":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import svm\nimport lightgbm","c54b5357":"estimators = [\n        ('lgbm', lightgbm.LGBMRegressor(**param_lgbm, random_state=RANDOM_SEED, n_jobs=-1)),\n        ('xgbr', XGBRegressor(**param_xgb, random_state=RANDOM_SEED, nthread=-1)),\n        ('cat', CatBoostRegressor(**param_cat)),\n        ('mlp', MLPRegressor()) # without tuning\n]","4b381b82":"reg = StackingRegressor(\n    estimators=estimators,\n    final_estimator=LinearRegression(),\n    n_jobs=-1,\n    cv=5\n)\n\nreg.fit(X, y)\n\ny_pred = reg.predict(test)","0d46cbe3":"sample['target'] = y_pred\nsample.to_csv(\"submission.csv\", index=False)","b6f16484":"Now we will use Bayesian Optimization to tune the hyperparameter. Our goal is to minimize RMSE, but Bayesian Optimization here only support maximizing, so that's why we add a minus sign in the RMSE, so maximizing the minus RMSE is equal to minimizing the RMSE. Just a matter of sign.\n\nYou can also adjust what parameter you want to tune and the range of hyperparameter. You can also how many point or how many try during the optimization. ","31c0b62f":"## LGBM Tuning","25e40c21":"## CAT Tuning","63a8fd17":"## XGB Tuning","ca60760c":"## Stacking\n\nWe will use the best parameter as a learner and use Linear Regression as the meta-learner. You can also tune the meta-learner parameter. Also, make sure to convert some parameters into an integer.","90ec8c45":"Hello everyone!\n\n**This notebook presents a straightforward code to tune hyperparameter of LGBM, CAT, and XGB with Bayesian Optimization. It is like GridSearchCV and RandomizedSearchCV.**\n\nGridSearchCV searches for all combinations of parameters, and it could take a very long time. Not very efficient. RandomizedSearchCV searches the combination randomly. Somehow the algorithm can skip the optimal parameter, especially if the search grid is enormous. Bayesian Optimization is a smarter method to tune the hyperparameter. I won't discuss the theory behind it in this notebook as it is straightforward.\n\nIf you have any questions regarding the code, please comment below. I will update the notebook accordingly.\n\n**Please do upvote the notebook if this notebook helps you as it will be a benchmark for me to do more work in the future. Thank you :)**"}}