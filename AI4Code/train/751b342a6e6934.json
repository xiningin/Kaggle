{"cell_type":{"183bd370":"code","4ae390bf":"code","2c9e354a":"code","00eacb20":"code","568d8215":"code","10968fb8":"code","93922e79":"code","c3ff43c6":"code","bbf50ba3":"code","0b05bb6d":"code","b0d74de0":"code","89b5d062":"code","4b5bdc80":"code","f948a560":"code","923cdaa4":"code","21e33e5d":"code","d3c1e301":"code","874225f4":"code","4ae2b2e1":"code","d09dec18":"code","8c6da56a":"code","c234ef48":"code","0b6ae778":"code","a36a5753":"code","2b5946b0":"code","544bea9f":"code","f47204b5":"code","71ab95c2":"code","5be4f3df":"code","2e2e330d":"code","e34177a6":"code","21670c60":"code","34971da6":"markdown","6e040b59":"markdown","ac31eeb8":"markdown","422b1884":"markdown","3ac2d20e":"markdown","486673b4":"markdown","1da31157":"markdown","875a2c75":"markdown","7e0dcf4c":"markdown","0f85058f":"markdown","563f2ce6":"markdown","af5c9ab3":"markdown","d2a04553":"markdown","4638ab0a":"markdown","50d3a4e5":"markdown","da2f7bad":"markdown","bc152b9b":"markdown","2b2a8f52":"markdown","2e712bd3":"markdown","4db74aca":"markdown","a3b6f790":"markdown","9621495f":"markdown","21390a5d":"markdown","742e07ff":"markdown"},"source":{"183bd370":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ae390bf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make NumPy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)\n# Turn off the warning altogether\npd.set_option('mode.chained_assignment',None)","2c9e354a":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)","00eacb20":"def plot_loss(history):\n  plt.plot(history.history['loss'], label='loss')\n  plt.plot(history.history['val_loss'], label='val_loss')\n  plt.ylim([0, 10])\n  plt.xlabel('Epoch')\n  plt.ylabel('Error [MPG]')\n  plt.legend()\n  plt.grid(True)","568d8215":"import numpy as np\n\nurl = \"http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/auto-mpg\/auto-mpg.data\"\nnames = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\"acceleration\", \"model year\", \"origin\", \"car name\"]\nwidths = [7, 4, 10, 10, 11, 7, 4, 4, 30]\n\n# Get the data\nX_full = pd.read_fwf(url, names=names, widths=widths, na_values=['?'])\nX = X_full.copy()\nX.tail()","10968fb8":"def analyze_target(target_label, df):\n    from scipy import stats\n    from scipy.stats import norm, skew, kurtosis, boxcox\n\n    y = df[target_label]\n    sns.distplot(y, fit=norm)\n\n    _skew = skew(y)\n    _kurtosis = kurtosis(y)\n\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(y)\n    print(\"\\n mu = {:.2f} and sigma = {:.2f}, skew = {:.2f} kurtosis = {:.2f}\\n\".format(mu, sigma, _skew, _kurtosis))\n\n    # #Now plot the distribution\n    plt.legend([\"Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )\".format(mu, sigma)], loc=\"best\")\n    plt.ylabel(\"Frequency\")\n    plt.title(target_label + \" distribution\")\n\n    #Get also the QQ-plot\n    fig = plt.figure()\n    res = stats.probplot(y, plot=plt)\n    plt.show()\n    \nanalyze_target(\"mpg\", X)","93922e79":"from scipy.special import boxcox1p\n\n\nlam_l = 0.4 # optimized value\nX[\"mpg\"] = boxcox1p(X_full[\"mpg\"], lam_l) \n\nanalyze_target(\"mpg\", X)","c3ff43c6":"# A dictionary of companies getting from a feature \"car name\"\nbrands_dict = {\n    \"amc\": \"AMC\",\n    \"audi\": \"Audi\",\n    \"bmw\": \"Bmw\",\n    \"buick\": \"Buick\",\n    \"cadillac\": \"Cadillac\",\n    \"capri\": \"Capri\",\n    \"chevroelt\": \"Chevrolet\",\n    \"chevrolet\": \"Chevrolet\",\n    \"chevy\": \"Chevrolet\",\n    \"chrysler\": \"Chrysler\",\n    \"datsun\": \"Datsun\",\n    \"dodge\": \"Dodge\",\n    \"fiat\": \"Fiat\",\n    \"ford\": \"Ford\",\n    \"hi\": \"IH\",\n    \"honda\": \"Honda\",\n    \"maxda\": \"Mazda\",\n    \"mazda\": \"Mazda\",\n    \"mercedes\": \"Mercedes-Benz\",\n    \"mercedes-benz\": \"Mercedes-Benz\",\n    \"mercury\": \"Mercury\",\n    \"nissan\": \"Nissan\",\n    \"oldsmobile\": \"Oldsmobile\",\n    \"opel\": \"Opel\",\n    \"peugeot\": \"Peugeot\",\n    \"plymouth\": \"Plymouth\",\n    \"pontiac\": \"Pontiac\",\n    \"renault\": \"Renault\",\n    \"saab\": \"Saab\",\n    \"subaru\": \"Subaru\",\n    \"toyota\": \"Toyota\",\n    \"toyouta\": \"Toyota\",\n    \"triumph\": \"Triumph\",\n    \"vokswagen\": \"Volkswagen\",\n    \"volkswagen\": \"Volkswagen\",\n    \"volvo\": \"Volvo\",\n    \"vw\": \"Volkswagen\"\n}\n\n\n# Create a new feature named Company\nX[\"company\"] = [brands_dict[X_full[\"car name\"][i].replace('\"', '').split()[0]] for i in range(len(X[\"car name\"]))]\nX.tail()","bbf50ba3":"corr_matrix = X.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corr_matrix, vmax=0.9, square=True)","0b05bb6d":"print(corr_matrix[\"mpg\"].sort_values(ascending=False)[:5], '\\n')\nprint(corr_matrix[\"mpg\"].sort_values(ascending=False)[-5:])","b0d74de0":"# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=[\"mpg\"], inplace=True)\n\ny = X[\"mpg\"]\nX.drop([\"mpg\"], axis=1, inplace=True)","89b5d062":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)","4b5bdc80":"print(X_train.isna().sum())\nprint(X_valid.isna().sum())","f948a560":"from sklearn.impute import SimpleImputer\n\n# Imputation\nhp_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\nimputed_hp_train = hp_imputer.fit(X_train[[\"horsepower\"]])\nimputed_hp_valid = hp_imputer.fit(X_valid[[\"horsepower\"]])\n\n# Put them back to X_train and X_valid dataframe\nX_train[\"hp\"] = imputed_hp_train.transform(X_train[[\"horsepower\"]]).ravel()\nX_valid[\"hp\"] = imputed_hp_valid.transform(X_valid[[\"horsepower\"]]).ravel()","923cdaa4":"print(X_train.isna().sum())\nprint(X_valid.isna().sum())","21e33e5d":"X_train.tail()","d3c1e301":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[[\"origin\"]]))\nOH_cols_valid = pd.DataFrame(OH_encoder.fit_transform(X_valid[[\"origin\"]]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_valid.index = X_valid.index\n\n# Add one-hot encoded columns to numerical features\nX_train = pd.concat([X_train, OH_cols_train], axis=1)\nX_valid = pd.concat([X_valid, OH_cols_valid], axis=1)","874225f4":"X_train.tail()","4ae2b2e1":"X_valid.tail()","d09dec18":"from category_encoders import TargetEncoder\n\nencoder = TargetEncoder()\nencoded_comp_train = encoder.fit(X_train[[\"company\"]], y_train)\nencoded_comp_valid = encoder.fit(X_valid[[\"company\"]], y_valid)\n\n# Put them back to X_train and X_valid dataframe\nX_train[\"company_encode\"] = encoded_comp_train.transform(X_train[[\"company\"]])\nX_valid[\"company_encode\"] = encoded_comp_valid.transform(X_valid[[\"company\"]])","8c6da56a":"X_train.tail()","c234ef48":"X_valid.tail()","0b6ae778":"features = [\"cylinders\", \"displacement\", \"weight\", \"acceleration\", \"model year\", \"hp\", 0, 1, 2, \"company_encode\"]\nX_train[features].tail()","a36a5753":"%%time\ndef linear_regression_single(column):\n    # Create a numpy array made of the feature with column name \n    feature = np.array(X_train[column])\n    # Init the tf.keras.layers.Normalization\n    feature_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n    # Fit the state of the preprocessing layer to the horsepower data\n    feature_normalizer.adapt(feature)\n    \n    # Build the Keras Sequential model\n    model = tf.keras.Sequential([\n        feature_normalizer,\n        layers.Dense(units=1)\n    ])\n    \n    # Configure the training procedure using the Keras Model.compile method\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n        loss=\"mean_absolute_error\"\n    )\n\n    # Execute the training for 100 epochs\n    history = model.fit(\n        X_train[column], y_train, epochs=100, verbose=0, validation_split=0.2\n    )\n    print(\"Finish \", column)\n\n    return model, history\n\ntest_results = {}\nfor f in [\"cylinders\", \"displacement\", \"weight\", \"acceleration\", \"model year\", \"hp\", \"company_encode\"]:\n    feature_model, history = linear_regression_single(f)\n    test_results[f+\"_model\"] = feature_model.evaluate(\n        X_valid[f],\n        y_valid, verbose=0)","2b5946b0":"test_results","544bea9f":"%%time\ndef linear_regression_multi(columns):\n    # Create a numpy array made of the feature with column name \n    features = np.array(X_train[columns])\n    # Create the tf.keras.layers.Normalization\n    features_normalizer = layers.Normalization(axis=-1)\n    # Fit the state of the preprocessing layer to the horsepower data\n    features_normalizer.adapt(features)\n    \n    # Build the Keras Sequential model\n    model = tf.keras.Sequential([\n        features_normalizer,\n        layers.Dense(units=1)\n    ])\n    \n    # Configure the training procedure using the Keras Model.compile method\n    model.compile(\n        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n        loss=\"mean_absolute_error\"\n    )\n\n    # Execute the training for 100 epochs\n    history = model.fit(\n        X_train[columns], y_train, epochs=100, verbose=0, validation_split=0.2\n    )\n\n    return model, history\n\n\nmulti_model, history = linear_regression_multi(features)                                                 \ntest_results[\"multi_model\"] = multi_model.evaluate(\n    X_valid[features],\n    y_valid, verbose=0)","f47204b5":"%%time\ndef dnn_regression_multi(columns):\n    # Create a numpy array made of the feature with column name \n    features = np.array(X_train[columns])\n    # Create the tf.keras.layers.Normalization\n    features_normalizer = layers.Normalization(axis=-1)\n    # Fit the state of the preprocessing layer to the horsepower data\n    features_normalizer.adapt(features)\n    \n    # Build the Keras Sequential model\n    model = tf.keras.Sequential([\n        features_normalizer,\n        layers.Dense(64, activation='relu'),\n        layers.Dense(64, activation='relu'),        \n        layers.Dense(units=1)\n    ])\n    \n    # Configure the training procedure using the Keras Model.compile method\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n        loss=\"mean_absolute_error\"\n    )\n\n    # Execute the training for 100 epochs\n    history = model.fit(\n        X_train[columns], y_train, epochs=100, verbose=0, validation_split=0.2\n    )\n\n    return model, history\n\n\ndnn_model, history = dnn_regression_multi(features)                                                 \ntest_results[\"dnn_model\"] = dnn_model.evaluate(\n    X_valid[features],\n    y_valid, verbose=0)","71ab95c2":"pd.DataFrame(test_results, index=['Mean absolute error [mpg]']).T","5be4f3df":"predictions = dnn_model.predict(X_valid[features]).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(y_valid, predictions)\nplt.xlabel('True Values [MPG]')\nplt.ylabel('Predictions [MPG]')\nlims = [0, 50]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","2e2e330d":"error = predictions - y_valid\nplt.hist(error, bins=25)\nplt.xlabel('Prediction Error [MPG]')\n_ = plt.ylabel('Count')","e34177a6":"dnn_model.save('dnn_model')","21670c60":"reloaded = tf.keras.models.load_model('dnn_model')\n\ntest_results['reloaded'] = reloaded.evaluate(\n    X_train[features], y_train, verbose=0)\n\npd.DataFrame(test_results, index=['Mean absolute error [mpg]']).T","34971da6":"### Reload the model","6e040b59":"# Performance\n\nSince all models have been trained, you can review their test set performance.\n\nThe \"dnn_model\" has the highest score which indicates it the best model amongs the stars.","ac31eeb8":"Back to our first assumption, the \"displacement\" and \"weight\" features have a better score than others.","422b1884":"The \"company\" is a categorical variable with high cardinality.\n\nIt is not wise to use one-hot encoding since it can create very high dimensionality, instead we will use Target-based Encoding.","3ac2d20e":"# The Auto MPG dataset\nThe dataset is available from the UCI [Machine Learning Repository](http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/auto-mpg\/auto-mpg.data).\n\n## Get the data\nFirst download and import the dataset using pandas:","486673b4":"## Transformation of the target variable","1da31157":"Visualize the model's training progress","875a2c75":"## Analysis on the target variable","7e0dcf4c":"## Linear regression with multiple inputs","0f85058f":"# Normalization\n\nIt is good practice to normalize features that use different scales and ranges.\n\nThe ```tf.keras.layers.Normalization``` is a clean and simple way to add feature normalization into your model.","563f2ce6":"As we can see, the columns \"displacement\" and \"weight\" are strongly negatively correlated.\n\nWe can guess if we predict the \"mpg\" value from given columns, the \"displacement\" and \"weight\" will predict better than others.","af5c9ab3":"# Split features from labels\n\nSeparate the target value\u2014the \"label\"\u2014from the features. This label is the value that you will train the model to predict.","d2a04553":"## Select features we will use to train the model","4638ab0a":"## Split the data into training and test sets\n\nNow, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.","50d3a4e5":"The \"origin\" column is categorical, not numeric. \n\nSo the next step is to one-hot encode the values in the column with OneHotEncoder class from scikit-learn.","da2f7bad":"The column \"horsepower\" contains missing value.\n\nUse SimpleImputer to replace missing values with the mean value and create a new column name \"hp\".","bc152b9b":"## Data Correlation\n\nCorrelation map to see how features are correlated with each other and with \"mpg\".","2b2a8f52":"## Feature Engineering\n\nCreate a new feature named \"company\" basing on \"car name\"","2e712bd3":"# Linear Regression\n\n## Linear regression with one variable\n\nBegin with a single-variable linear regression to predict \"mpg\" \nfrom \"cylinders\", \"displacement\", \"weight\", \"acceleration\", \"model year\", \"hp\", one by one.","4db74aca":"Look at some correlation values in a list format.","a3b6f790":"# Regression with a deep neural network (DNN)\nThese models will contain a few more layers than the linear model:\n\n- The normalization layer, as before (with horsepower_normalizer for a single-input model and normalizer for a multiple-input model).\n\n- Two hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity.\n\n- A linear Dense single-output layer.","9621495f":"## Clean tha data\n\nIdentify columns with missing values","21390a5d":"### Save it for later use","742e07ff":"The target variable is right skewed distribution.\n\nAs (linear) models prefer normally distributed data, we need to transform this variable and make it more normally distributed."}}