{"cell_type":{"fd013fa0":"code","b31298bc":"code","2df3955b":"code","e644ca66":"code","c6c954e2":"code","48885491":"code","17b32cc7":"code","08b1fdbf":"code","c94006cf":"code","416296c0":"code","80441878":"code","dad0f060":"code","3ba45344":"code","5d649eb2":"code","35672ab9":"code","5b859370":"code","1eea6831":"code","899e9105":"code","f8a15fd0":"code","1e044aea":"code","b7bea606":"code","73498a4c":"code","bb909bce":"code","f493b765":"code","61793705":"code","3e22db00":"code","ca3de4a4":"code","5eead36c":"code","d779f7c8":"code","7ce0bcb2":"code","82f0ea9e":"markdown","6a366434":"markdown","cf201785":"markdown","d5287a76":"markdown","dc0abaa3":"markdown","053da4ed":"markdown","298d352f":"markdown","7982ddb8":"markdown","c1013233":"markdown","bfdc0263":"markdown","b15a2393":"markdown","0110805a":"markdown","370d3349":"markdown","2bc20464":"markdown","4a44d873":"markdown","3fc55029":"markdown","003d02af":"markdown","8a0b0f41":"markdown","44b8ad62":"markdown","09a032ea":"markdown","05b37295":"markdown","4339ff1f":"markdown","bc642104":"markdown","953c40c4":"markdown","32b8edc9":"markdown","aa8c25df":"markdown","a0551738":"markdown","a5b4b5f4":"markdown","82605ecf":"markdown","6026365b":"markdown","1319cfc3":"markdown","45fc2819":"markdown"},"source":{"fd013fa0":"! pip install pyspark","b31298bc":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import isnull, when, count, col\nfrom pyspark.ml.feature import VectorAssembler,StringIndexer\nfrom pandas.plotting import scatter_matrix\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nimport pandas as pd\nimport numpy as np","2df3955b":"spark = SparkSession.builder.appName('tps-2021-ml-with-pyspark').getOrCreate()\ndf = spark.read.csv('..\/input\/tabular-playground-series-apr-2021\/train.csv', header = True, inferSchema = True)\ndf.printSchema()","e644ca66":"df.show(5)","c6c954e2":"df.groupby('Survived').count().show()","48885491":"numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\ndf.select(numeric_features).describe().show()","17b32cc7":"numeric_data = df.select(numeric_features).toPandas()\n\naxs = scatter_matrix(numeric_data, figsize=(8, 8));\n\n# Rotate axis labels and remove axis ticks\nn = len(numeric_data.columns)\nfor i in range(n):\n    v = axs[i, 0]\n    v.yaxis.label.set_rotation(0)\n    v.yaxis.label.set_ha('right')\n    v.set_yticks(())\n    h = axs[n-1, i]\n    h.xaxis.label.set_rotation(90)\n    h.set_xticks(())","08b1fdbf":"df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()","c94006cf":"dataset = df.replace('null', None)\\\n        .dropna(how='any')","416296c0":"dataset.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()","80441878":"dataset = dataset.drop('PassengerId')\ndataset = dataset.drop('Ticket')\ndataset = dataset.drop('Fare')\ndataset = dataset.drop('Cabin')\ndataset = dataset.drop('Embarked')\ndataset = dataset.drop('Name')\ndataset.show()","dad0f060":"sex_in = StringIndexer(inputCol=\"Sex\", outputCol=\"Sex_encoding\")\ndf1 = sex_in.fit(dataset).transform(dataset)\ndf1.show()","3ba45344":"required_features = ['Survived',\n                    'Pclass',\n                    'SibSp',\n                    'Parch',\n                     'Age',\n                     'Sex_encoding'\n                   ]\n\nassembler = VectorAssembler(inputCols=required_features, outputCol='features')\n\ntransformed_data = assembler.transform(df1)","5d649eb2":"transformed_data.show()","35672ab9":"(training_data, test_data) = transformed_data.randomSplit([0.8,0.2])","5b859370":"print(\"Training Dataset Count: \" + str(training_data.count()))\nprint(\"Test Dataset Count: \" + str(test_data.count()))","1eea6831":"rf = RandomForestClassifier(labelCol='Survived', \n                            featuresCol='features',\n                            maxDepth=5)\nmodel = rf.fit(training_data)\nrf_predictions = model.transform(test_data)","899e9105":"multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Survived', metricName = 'accuracy')\nprint('Random Forest classifier Accuracy:', multi_evaluator.evaluate(rf_predictions))","f8a15fd0":"dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'Survived', maxDepth = 3)\ndtModel = dt.fit(training_data)\ndt_predictions = dtModel.transform(test_data)\ndt_predictions.select('Sex_encoding', 'Pclass','SibSp', 'Parch', 'Age', 'Survived').show(10)","1e044aea":"multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Survived', metricName = 'accuracy')\nprint('Decision Tree Accuracy:', multi_evaluator.evaluate(dt_predictions))","b7bea606":"gb = GBTClassifier(labelCol = 'Survived', featuresCol = 'features')\ngbModel = gb.fit(training_data)\ngb_predictions = gbModel.transform(test_data)","73498a4c":"multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'Survived', metricName = 'accuracy')\nprint('Gradient-boosted Trees Accuracy:', multi_evaluator.evaluate(gb_predictions))","bb909bce":"df_pd = gb_predictions.toPandas()","f493b765":"df_pd.head()","61793705":"df_pd.to_csv(\"gbModel.csv\",index = False)","3e22db00":"gb_df = pd.read_csv(\".\/gbModel.csv\")","ca3de4a4":"gb_df.head()","5eead36c":"submission  = pd.read_csv(\"..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv\")\nsubmission = submission.drop(\"Survived\",axis=1)\nsubmission.head()","d779f7c8":"results = pd.concat([submission,gb_df.Survived],axis = 1)","7ce0bcb2":"results.to_csv(\"submission.csv\", index = False)","82f0ea9e":"Now, Data grouping by Survived for checking the classes are perfectly balanced!!","6a366434":"## 3. Features Convert into Vector","cf201785":"## 2. Unnecessary columns dropping\n","d5287a76":"**Evaluate our Random Forest Classifier model.**","dc0abaa3":"## Exploring The Data:\n\n### Data Dictionary\n![Imgur](https:\/\/i.imgur.com\/bkNeXxE.png)\n\n## Variable Notes\n**pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp**: The dataset defines family relations in this way...\n\n**Sibling** = brother, sister, stepbrother, stepsister\n\n**Spouse** = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch**: The dataset defines family relations in this way...\n\n**Parent** = mother, father\n\n**Child** = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","053da4ed":"## Correlations\n\nChecking Correlations between independent variables\n","298d352f":"## About the Problem:\nUsing the machine learning tools, we need to analyze the information about the passensgers of Titanic and predict which passenger has survived. This problem has been published by Kaggle and is widely used for learning basic concepts of Machine Learning","7982ddb8":"Wow!! \ud83d\udc4c That\u2019s great now datasets haven\u2019t any missing values.\ud83d\ude00","c1013233":"## Show Dataset\nHave a peek of the first five observations. In PySpark you can show the data using **show()**","bfdc0263":"**Again check missing data**","b15a2393":"### 1. Missing Data Handling:\n","0110805a":"## Data preparation and feature engineering\nIn this part, we will remove unnecessary columns and fill the missing values. Finally, we will select features for ml models. These features will be divided into two parts: train and test.\n\nLet\u2019s starting the mission \ud83d\udc68\u200d\ud83d\ude80 ","370d3349":"**Evaluate our Decision Tree model.**","2bc20464":"## Summary statistics for numeric variables","4a44d873":"# References:\n1. [Building a Machine Learning (ML) Model with PySpark](https:\/\/towardsdatascience.com\/first-time-machine-learning-model-with-pyspark-3684cf406f54)\n2. [Apache Spark 3.1.1](http:\/\/spark.apache.org\/docs\/latest\/ml-guide.html)\n3. [A Must-Read Guide on How to Work with PySpark on Google Colab for Data Scientists!](https:\/\/www.analyticsvidhya.com\/blog\/2020\/11\/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists\/)\n4. [Learning basic ML by Titanic Survival Prediction](https:\/\/www.kaggle.com\/harunshimanto\/learning-basic-ml-by-titanic-survival-prediction)","3fc55029":"**oh no!!Survived class not balanced!!**","003d02af":"# Intro Of This Notebook:\n\nI will go through in this notebook the whole process of **EDA** & Creating a **Machine Learning Model** on the famous **Titanic** dataset, which is used by many people all over the world.\n\nThe **goal** of this notebook is to show **EDA** and **how to build an ml model** using **PySpark**.","8a0b0f41":"## Load Data","44b8ad62":"# Machine learning Model Building\n","09a032ea":"## Load Libraries","05b37295":"> Machine learning models sparking when **PySpark** gave the accelerator gear like the **Need for Speed** gaming cars.","4339ff1f":"# Make Sumbission file","bc642104":"## 2. Decision Tree Classifier\n\nDecision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, while not requiring feature scaling and are able to capture non-linearities and feature interactions.","953c40c4":"**Evaluate our Gradient-Boosted Tree Classifier.**","32b8edc9":"## 3. Gradient-boosted Tree classifier Model\n\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.","aa8c25df":"## How to install PySpark? \n\n**PySpark** installing process is very easy as like other python\u2019s packages. (eg. Pandas, Numpy,scikit-learn). Want to know more then you can read [my article on **PySpark**](https:\/\/towardsdatascience.com\/first-time-machine-learning-model-with-pyspark-3684cf406f54)","a0551738":"## What is PySpark?\n\n**Spark** is the name of the engine, that realizes cluster computing while **PySpark** is the Python\u2019s library to use Spark.\n\n**PySpark** is a great language for performing exploratory data analysis at scale, building machine learning pipelines, and creating ETLs for a data platform. If you\u2019re already familiar with Python and libraries such as Pandas, then PySpark is a great language to learn in order to create more scalable analyses and pipelines.","a5b4b5f4":"Oh Shit!! We got lots of missing data!! \n\n* In Age columns we got 3292 missing data\n* In Ticket columns we got 4623 missing data\n* In Fare columns we got 134 missing data\n* In Cabin columns we got 67866 missing data\n* In Embarked columns we got 250 missing data\n\nNow our mission for handling the missing data.","82605ecf":"## 1. Random Forest Classifier\n\nRandom forest is a supervised learning algorithm which is used for both classification and regression cases, as well. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees mean more robust forests, in a similar way, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which is better than a single decision tree because it reduces the over-fitting by averaging the result.","6026365b":"Done!!\u270c\ufe0f Now features are converted into a vector. \ud83e\uddee","1319cfc3":"## 4. Train and Test Split\nRandomly split data into train and test sets, and set seed for reproducibility.","45fc2819":"# Conclusion\n\nPySpark is a great language for data scientists to learn because it enables scalable analysis and ML pipelines. If you\u2019re already familiar with Python and Pandas, then much of your knowledge can be applied to Spark. To sum it up, we have learned **EDA & how to build a machine learning application using PySpark**. We tried two algorithms and **gradient boosting performed best on our data set.**"}}