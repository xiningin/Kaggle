{"cell_type":{"fa74109c":"code","9a2a0d22":"code","0e692eac":"code","373e793f":"code","3f78b691":"code","9123c684":"code","2cbd1f2b":"code","664ba2db":"code","2d0f5dae":"code","515aae59":"code","c7628b3e":"code","79cc3c5b":"code","98691217":"code","7dc99e8d":"code","1cf48ca7":"code","0e20bd96":"code","634bf16b":"code","5460c4f0":"code","94c0d7a1":"code","088d4fe7":"code","6db56154":"code","a20190f3":"code","4c6d0211":"code","b173c2c3":"code","1f228659":"code","e2d3a53e":"code","06f80b5b":"code","68614480":"code","137d09a5":"markdown","da71269e":"markdown","a099831f":"markdown","3fd87513":"markdown"},"source":{"fa74109c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9a2a0d22":"import pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics","0e692eac":"train_df=pd.read_csv('..\/input\/train.csv',index_col=0)\ntest_df=pd.read_csv('..\/input\/test.csv',index_col=0)","373e793f":"train_df.describe()","3f78b691":"test_df.head()","9123c684":"y_train=train_df.pop('Survived')\nall_df=pd.concat((train_df,test_df),axis=0)","2cbd1f2b":"y_train.head()","664ba2db":"all_df.drop(['Name'],axis=1,inplace=True)","2d0f5dae":"all_df.drop(['Ticket'],axis=1,inplace=True)\nall_df.drop(['Cabin'],axis=1,inplace=True)","515aae59":"all_df.head()","c7628b3e":"all_df['Pclass'].dtype","79cc3c5b":"all_df['Pclass']=all_df['Pclass'].astype(str)","98691217":"all_df['Pclass'].value_counts()","7dc99e8d":"pd.get_dummies(all_df['Pclass'],prefix='Pclass').head()","1cf48ca7":"all_dummy_df=pd.get_dummies(all_df)\nall_dummy_df.head()","0e20bd96":"all_dummy_df['SibSp'].value_counts()\n#all_dummy_df['Parch'].value_counts()","634bf16b":"all_dummy_df.isnull().sum().sort_values(ascending=False).head(10)","5460c4f0":"mean_cols= all_dummy_df.mean()\nmean_cols.head(10)","94c0d7a1":"all_dummy_df=all_dummy_df.fillna(mean_cols)","088d4fe7":"all_dummy_df['Fare']=np.log1p(all_dummy_df['Fare'])\n","6db56154":"all_dummy_df.head()","a20190f3":"dummy_train_df=all_dummy_df.loc[train_df.index]\ndummy_test_df=all_dummy_df.loc[test_df.index]","4c6d0211":"X_train = dummy_train_df.values\nX_test = dummy_test_df.values","b173c2c3":"dummy_train_df.shape","1f228659":"def tanh(x):  \n    return np.tanh(x)\n\ndef tanh_deriv(x):  \n    return 1.0 - np.tanh(x)*np.tanh(x)\n\ndef logistic(x):  \n    return 1\/(1 + np.exp(-x))\n\ndef logistic_derivative(x):  \n    return logistic(x)*(1-logistic(x))\n","e2d3a53e":"class NeuralNetwork:   \n    def __init__(self, layers, learning_rate,activation='tanh'):  \n        \"\"\"  \n        :param layers: A list containing the number of units in each layer.\n        Should be at least two values  \n        :param activation: The activation function to be used. Can be\n        \"logistic\" or \"tanh\"  \n        \"\"\"  \n        if activation == 'logistic':  \n            self.activation = logistic  \n            self.activation_deriv = logistic_derivative  \n        elif activation == 'tanh':  \n            self.activation = tanh  \n            self.activation_deriv = tanh_deriv\n        self.learning_rate=learning_rate\n        \n        self.weights = []  \n        for i in range(1, len(layers) - 1):  \n            self.weights.append((2*np.random.random((layers[i - 1] + 1, layers[i] + 1))-1)*0.25)  \n            self.weights.append((2*np.random.random((layers[i] + 1, layers[i + 1]))-1)*0.25)\n    \n    \n    def fit(self, X, y,epochs=10000):         \n        X = np.atleast_2d(X)         \n        temp = np.ones([X.shape[0], X.shape[1]+1])         \n        temp[:, 0:-1] = X  # adding the bias unit to the input layer         \n        X = temp         \n        y = np.array(y)\n    \n        for k in range(epochs):  \n            i = np.random.randint(X.shape[0])  \n            a = [X[i]]\n    \n            for l in range(len(self.weights)):  #going forward network, for each layer\n                a.append(self.activation(np.dot(a[l], self.weights[l])))  #Computer the node value for each layer (O_i) using activation function\n            error = y[i] - a[-1]  #Computer the error at the top layer\n            deltas = [error * self.activation_deriv(a[-1])] #For output layer, Err calculation (delta is updated error)\n            \n            #Staring backprobagation\n            for l in range(len(a) - 2, 0, -1): # we need to begin at the second to last layer \n                #Compute the updated error (i,e, deltas) for each node going from top layer to input layer \n                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_deriv(a[l]))  \n            deltas.reverse()  \n            for i in range(len(self.weights)):  \n                layer = np.atleast_2d(a[i])  \n                delta = np.atleast_2d(deltas[i])  \n                self.weights[i] += self.learning_rate * layer.T.dot(delta)\n                \n                \n    def predict(self, x):         \n        x = np.array(x)         \n        temp = np.ones(x.shape[0]+1)         \n        temp[0:-1] = x         \n        a = temp         \n        for l in range(0, len(self.weights)):             \n            a = self.activation(np.dot(a, self.weights[l]))         \n        return a\n    def predicts(self,X):\n        predictions=[]\n        for i in X:\n            predictions.append(self.predict(i))\n        return predictions\n","06f80b5b":"import math\nparams=[0.001,0.01,0.03,0.05,0.07,0.1,0.2,0.3,0.4,0.5,0.6]\nRMSEs=[]\nfor param in params:\n    nn = NeuralNetwork([12,2,1],param,'tanh')\n    nn.fit(X_train,y_train)\n    y_predict=nn.predicts(X_train)\n    training_root_mean_squared_error=math.sqrt(metrics.mean_squared_error(y_predict,y_train))\n    RMSEs.append(training_root_mean_squared_error)","68614480":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(params, RMSEs)\nplt.title(\"learning_rate vs RMSE\");","137d09a5":"# Model","da71269e":"# Combine data","a099831f":"# Acquire data\n","3fd87513":"# Feature Engineering"}}