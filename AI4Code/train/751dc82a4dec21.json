{"cell_type":{"c308bce5":"code","192703d2":"code","4e4ad061":"code","21a6897c":"code","9599ab02":"code","23b13cda":"code","bfb9f0c7":"code","46262109":"code","101dd729":"code","e04f03cd":"code","9447db51":"code","c3ccb127":"code","99857442":"code","97a9c92f":"code","3520681f":"code","cde8baaa":"code","418cb490":"code","3c5c25fd":"code","59898f2f":"code","ca64f7ea":"code","c67af892":"code","db2ca7bd":"code","980b22fb":"code","e38aeb31":"code","60caf12d":"code","9728daf6":"markdown","d92b8719":"markdown","81e54606":"markdown","ead0a340":"markdown","fbdfa461":"markdown","39823eaa":"markdown","856913b5":"markdown","20199413":"markdown","6d7ce924":"markdown","9dd1224e":"markdown"},"source":{"c308bce5":"# load all the needed packages\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import LabelEncoder\nfrom shapely.geometry import Point, Polygon, shape\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","192703d2":"import warnings\nwarnings.filterwarnings(\"ignore\")","4e4ad061":"sns.set(style = 'darkgrid')\nsns.set_palette('PuBuGn_d')","21a6897c":"# load dataset then parsing the dates column into datetime\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","9599ab02":"# show the train data\ntrain.head(3)","23b13cda":"# show the tolal number of recoreds\ntrain.shape","bfb9f0c7":"# check for NAN values\ntrain.isnull().sum()","46262109":"# check for doublications\ntrain.duplicated().any()","101dd729":"# show the number of doublications\ntrain.duplicated().sum()","e04f03cd":"# drop duplicate rows and keep only the unique values\ntrain = train.drop_duplicates()\ntrain.shape","9447db51":"# explore crime categories\ntrain['Category'].unique()","c3ccb127":"# plot the total number of incidents for each category\nx = sns.catplot('Category', data = train, kind = 'count', aspect = 3, height = 4.5)\nx.set_xticklabels(rotation = 85)","99857442":"# encode crime categories\nle = preprocessing.LabelEncoder()\ncategory = le.fit_transform(train['Category'])","97a9c92f":"# encode weekdays, districts and hours\ndistrict = pd.get_dummies(train['PdDistrict'])\ndays = pd.get_dummies(train['DayOfWeek'])\ntrain['Dates'] = pd.to_datetime(train['Dates'], format = '%Y\/%m\/%d %H:%M:%S')\nhour = train['Dates'].dt.hour\nhour = pd.get_dummies(hour)","3520681f":"# pass encoded values to a new dataframe\nenc_train = pd.concat([hour, days, district], axis = 1)\nenc_train['Category'] = category","cde8baaa":"# add gps coordinates\nenc_train['X'] = train['X']\nenc_train['Y'] = train['Y']","418cb490":"# repeat data handling for test data by encoding weekdays, districts and hours\ndistrict = pd.get_dummies(test['PdDistrict'])\ndays = pd.get_dummies(test['DayOfWeek'])\ntest['Dates'] = pd.to_datetime(test['Dates'], format = '%Y\/%m\/%d %H:%M:%S')\nhour = test['Dates'].dt.hour\nhour = pd.get_dummies(hour)","3c5c25fd":"# create a new dataframe for encoded test values\nenc_test = pd.concat([hour, days, district], axis = 1)","59898f2f":"# add gps coordinates\nenc_test['X'] = test['X']\nenc_test['Y'] = test['Y']","ca64f7ea":"training, validation = train_test_split(enc_train, train_size = 0.60)","c67af892":"features = ['Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN', 'X', 'Y']\n# add the time\nfeatures.extend(x for x in range(0,24))","db2ca7bd":"start = time.time()\nmodel = BernoulliNB()\nmodel.fit(training[features], training['Category'])\npredicted = np.array(model.predict_proba(validation[features]))\nend = time.time()\nsecs = (end - start)\nloss = log_loss(validation['Category'], predicted)\nprint(\"Total seconds: {} and loss {}\".format(secs, loss))","980b22fb":"# now let's see what log_loss score we get if we apply LogisticRegression\nstart = time.time()\nmodel = LogisticRegression(C = 0.01)\nmodel.fit(training[features], training['Category'])\npredicted = np.array(model.predict_proba(validation[features]))\nend = time.time()\nsecs = (end - start)\nloss = log_loss(validation['Category'], predicted)\nprint(\"Total seconds: {} and loss {}\".format(secs, loss))","e38aeb31":"model = BernoulliNB()\nmodel.fit(enc_train[features], enc_train['Category'])\npredicted = model.predict_proba(enc_test[features])","60caf12d":"# extract results\nresult = pd.DataFrame(predicted, columns = le.classes_)\nresult.to_csv('results.csv', index = True, index_label = 'Id')","9728daf6":"Now 2323 is a small number compared to the total number of rows which means we can drop them!","d92b8719":"However Naive Bayes is a fairly simple model, it can give great results!","81e54606":"log_loss or logarithmic loss is a classification metric based on probabilities where it quantifies the accuracy of a classifier by penalising false classifications, in other words minimising the log_loss means maximising the accuracy and lower log-loss value makes better predictions. Also BernoulliNB took only 2.6xx secs to run while LogisticRegression took much longer.","ead0a340":"So there are no missing values in the dataset.","fbdfa461":"Next step is to split up the enc_train into a training and validation set so that we there's a way to access the model performance without touching the test data.","39823eaa":"# Predicting and Analysing San Francisco Crimes","856913b5":"For more visualisation you need to check out this [notebook](https:\/\/nbviewer.jupyter.org\/github\/just4data\/San-Francisco-Crime-Analysis-And-Prediction\/blob\/master\/sf_crime_classification.ipynb) as .shp files preview is not supported here!","20199413":"# Machine Learning Model","6d7ce924":"This notebook attempts to analysis and predict the class of crimes committed within the city of San Francisco. The code first exploring and visualising crime patterns across the neighbourhoods and police districts.\n\nThen applying a machine learning algorithm in order to guess the category of the crimes based on their time and location of occurrence by using the Naive Bayes classifier as it's one of the simplest classification algorithms.","9dd1224e":"### Predicting the category of San Francisco crimes"}}