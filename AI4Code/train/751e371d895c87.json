{"cell_type":{"4b69974e":"code","a2b52ccc":"code","78709c77":"code","2f2e5246":"code","ddf637eb":"code","33199794":"code","2b314f5b":"code","6ef77b2c":"code","6f3eaeac":"code","125508f3":"code","221cf740":"code","7a6d7048":"code","f8520509":"code","23274e06":"code","26a9a69b":"code","6d278e22":"code","5a20d112":"code","5cacae04":"code","d5a0bde1":"code","ddbd6eb0":"code","0ae18959":"code","763563c9":"code","6ffee8ea":"code","cf5b8d76":"code","46c7b9ee":"code","46803753":"code","96864e05":"code","91a25fc5":"markdown","6fc88549":"markdown","deed48dc":"markdown","80d62156":"markdown","a6d1154c":"markdown","8374c3f3":"markdown","dc3a6836":"markdown","1be0aea2":"markdown","8a02920c":"markdown","7fbf7f79":"markdown","e5e433d0":"markdown","ff0740e0":"markdown","4cfeeeff":"markdown"},"source":{"4b69974e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2b52ccc":"#reading the file\ndf = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","78709c77":"#Lets take a glimpse of the dataset\ndf.head()","2f2e5246":"df.describe()","ddf637eb":"#checking data types\ndf.info()","33199794":"#Checking null values in each column\ndf.isnull().sum()","2b314f5b":"#Checking length of dataset\nprint(df.shape)","6ef77b2c":"#Target variable\ndf['quality'].value_counts()","6f3eaeac":"import plotly.express as px\ndf_new = df['quality'].value_counts().rename_axis('Winequality').reset_index(name='counts')\ndf_new\nfig = px.pie(df_new, values='counts', names='Winequality')\nfig.show()","125508f3":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16,10))\nsns.boxplot(y='fixed acidity',x='quality',data=df)\nplt.yticks(rotation =90,fontsize=12)\nplt.xticks(fontsize=12)\nplt.title(\"Impact of fixed acidity on Quality\",fontsize=30)\nplt.xlabel('Quality',fontsize=26)\nplt.ylabel('Fixed Acidity',fontsize=26)\nplt.show()","221cf740":"df.columns","7a6d7048":"df['Good'] =df['quality'].apply(lambda x : 1 if(x>5) else 0)\ndf.drop(['quality'],inplace=True,axis=1)","f8520509":"#Lets see if we have a balanced distribution \ndf['Good'].value_counts()","23274e06":"plt.figure(figsize=(16,10))\nsns.scatterplot(x='alcohol',y='residual sugar',data=df,hue='Good')\nplt.yticks(rotation =90,fontsize=12)\nplt.xticks(fontsize=12)\nplt.title(\"PLOT- A \",fontsize=30)\nplt.xlabel('Alchohol',fontsize=26)\nplt.ylabel('Residual Sugar',fontsize=26)\nplt.show()\n","26a9a69b":"plt.figure(figsize=(16,10))\nsns.boxplot(y='residual sugar',x='Good',data=df)\nplt.yticks(rotation =90,fontsize=12)\nplt.xticks(fontsize=12)\nplt.title(\"PLOT- B Checking outliers\",fontsize=30)\nplt.xlabel('Good',fontsize=26)\nplt.ylabel('Residual Sugar',fontsize=26)\nplt.show()\n","6d278e22":"plt.figure(figsize=(16,10))\nsns.boxplot(y='free sulfur dioxide',x='Good',data=df)\nplt.yticks(rotation =90,fontsize=12)\nplt.xticks(fontsize=12)\nplt.title(\"PLOT- C- Checking outliers\",fontsize=30)\nplt.xlabel('Good',fontsize=26)\nplt.ylabel('Residual Sugar',fontsize=26)\nplt.show()","5a20d112":"plt.figure(figsize=(16,10))\nsns.boxplot(y='total sulfur dioxide',x='Good',data=df)\nplt.yticks(rotation =90,fontsize=12)\nplt.xticks(fontsize=12)\nplt.title(\"PLOT- D Checking outliers\",fontsize=30)\nplt.xlabel('Good',fontsize=26)\nplt.ylabel('Total Sulphur Dioxide',fontsize=26)\nplt.show()","5cacae04":"#Data preparation\nX = df.drop(['Good'],axis=1)\nY = df['Good']\ndel df","d5a0bde1":"#train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.30,random_state = 90)","ddbd6eb0":"#model building - Random Forest \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score","0ae18959":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n#Criterion\ncriterion = ['gini','entropy']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'criterion': criterion,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n","763563c9":"rf_tune = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf_tune,\n                               param_distributions = random_grid,\n                               n_iter = 100,\n                               cv = 3,\n                               verbose=2,\n                               random_state=42,\n                               n_jobs = -1)\nrf_random.fit(X_train,Y_train)\n","6ffee8ea":"rf_random.best_params_","cf5b8d76":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(rf_random,X_test,Y_test)","46c7b9ee":"Y_pred = rf_random.predict(X_test)","46803753":"accuracy_score(Y_test,Y_pred)","96864e05":"f1_score(Y_test,Y_pred)","91a25fc5":"There are certain outliers in our data here","6fc88549":"# DATA VISUALIZATION","deed48dc":" Finally we got an accuracy of 80.41 % with an f1 score of 0.82.","80d62156":"We can see some outliers for Free Sulphur Dioxide.","a6d1154c":"As per above distribution, lets create a binary feature for wine quality having two values Good(1) and Bad(0)","8374c3f3":"We can see some outliers for Total Sulphur Dioxide , for Good Quality Wine. ","dc3a6836":"# DATA MODELLING AND TUNING","1be0aea2":"To classify wine as good or not we will use Random Forest Algorithm.It is an ensemble model, basically it considers outputs from multiple trees trained on data and reports the output given by the majority of trees.  ","8a02920c":"![istockphoto-1301017778-170667a.jpg](attachment:2a9d24d7-e772-44c9-a51d-512715d10fe2.jpg)","7fbf7f79":"# DATA PROPERTIES","e5e433d0":"Welcome to my notebook, please give and upvote if you like it and feel free to share any feedback in the comments.In this notebook we are going to deep dive into the red wine quality dataset, study the dataset and apply a classification model to predict the quality.","ff0740e0":"# DATA PREPARATION","4cfeeeff":"***Using Hyperparameter Tuning***<br>\nSince there are multiple model parameters, we need to find the best parameters. This is where Hyperparameter tuning is used.It basically uses a defined grid of parameters and checks the model performance on different combinations of parameters and reports the set of parameters on which we have the best model performance.\n\nThere are 2 techniques for Hyperparameter tuning :\n1. RandomizedSearchCV - Checks on random combinations.\n2. GridSearchCV - Checks on all combinantions\n\nWe will use Randomized search CV as it is efficient and less time consuming."}}