{"cell_type":{"cf2bc381":"code","0cc63e99":"code","70bb4e73":"code","ef718b64":"code","fcbd77c3":"code","e483f155":"code","932b27e9":"code","27e17da0":"code","3b5edfaa":"code","4fe66250":"code","7db0daeb":"code","4e44e12a":"code","9acc6c68":"markdown","969f1e7c":"markdown","0c2b0512":"markdown"},"source":{"cf2bc381":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cc63e99":"dataset=pd.read_csv('..\/input\/clustering\/musteriler.csv')","70bb4e73":"dataset.head()","ef718b64":"X=dataset.iloc[:,3:].values\nX","fcbd77c3":"import matplotlib.pyplot as plt \nplt.scatter(X[:,0],X[:,1])","e483f155":"from sklearn.cluster import KMeans\nneg=[]\nfor i in range(1,11):\n    kmeans=KMeans(n_clusters = i, init='k-means++', random_state= 123)\n    kmeans.fit(X)\n    neg.append(kmeans.inertia_)\nplt.plot(range(1,11),clusters)","932b27e9":"kmeans=KMeans(n_clusters=4,init='k-means++')\nkmeans.fit(X)","27e17da0":"clusters = kmeans.fit_predict(X)\ndataset[\"label\"] = clusters","3b5edfaa":"dataset","4fe66250":"plt.scatter(dataset.Hacim[dataset.label == 0 ],dataset.Maas[dataset.label == 0],color = \"red\")\nplt.scatter(dataset.Hacim[dataset.label == 1 ],dataset.Maas[dataset.label == 1],color = \"green\")\nplt.scatter(dataset.Hacim[dataset.label == 2 ], dataset.Maas[dataset.label == 2],color = \"blue\")\nplt.scatter(dataset.Hacim[dataset.label == 3 ], dataset.Maas[dataset.label == 3],color = \"violet\")\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],color = \"yellow\")\nplt.show()","7db0daeb":"from scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(X,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","4e44e12a":"from sklearn.cluster import AgglomerativeClustering\nac = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\nY_tahmin = ac.fit_predict(X)\n#print(Y_tahmin)\n\nplt.scatter(X[Y_tahmin==0,0],X[Y_tahmin==0,1],s=100, c='red')\nplt.scatter(X[Y_tahmin==1,0],X[Y_tahmin==1,1],s=100, c='blue')\nplt.scatter(X[Y_tahmin==2,0],X[Y_tahmin==2,1],s=100, c='green')\nplt.scatter(X[Y_tahmin==3,0],X[Y_tahmin==3,1],s=100, c='yellow')\nplt.title('HC')\nplt.show()","9acc6c68":"determining number of clusters ","969f1e7c":"hence optimal number of clusters are 3","0c2b0512":"heriechical clustering"}}