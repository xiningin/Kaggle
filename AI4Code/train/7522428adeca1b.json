{"cell_type":{"1bf0eaae":"code","3df09c87":"code","64a7074c":"code","ca9d6c3f":"code","4ec856bc":"code","d623fca2":"code","6bb95290":"code","9d28e56d":"code","8d6f4aae":"code","fc3c061c":"code","8bb16bfb":"code","fd5f14c8":"code","d7ceef78":"code","e3234203":"code","83d5804a":"code","f35816c9":"code","8226137a":"code","c6f216d4":"code","d60436e2":"code","374b2493":"code","2ee9773c":"code","3b3c6713":"code","c205a016":"code","0a1345c5":"code","5e05b22e":"code","4be68b71":"code","d55c1d83":"code","fa760387":"code","1c364bd3":"code","e33deba0":"code","5acf7f1d":"code","c1f27cbd":"markdown","ccaeb8b3":"markdown","6da7ea4b":"markdown","3f6c5634":"markdown","827adc33":"markdown","819b2ddc":"markdown","c4722f23":"markdown","a716d86e":"markdown","fac7d577":"markdown","a56e48c4":"markdown","25b7bc48":"markdown","6c3225d1":"markdown","0c8bfa7b":"markdown","214bde5b":"markdown","97c171f0":"markdown","f2888217":"markdown","4a525f36":"markdown","c8ef50fa":"markdown","3c8a8064":"markdown","1806dd00":"markdown","4526b50c":"markdown","1ee54ec8":"markdown","1cc81350":"markdown","33cb565d":"markdown","be5ec9c0":"markdown","3c56977a":"markdown","e4c2388a":"markdown","6f8413dc":"markdown"},"source":{"1bf0eaae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3df09c87":"import warnings\nwarnings.filterwarnings(\"ignore\")","64a7074c":"import pandas as pd\ntrain= pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\nprint(\"data shape :\", train.shape)\ntrain.head()","ca9d6c3f":"test= pd.read_csv(\"\/kaggle\/input\/fake-news\/test.csv\")\nprint(\"data shape :\", test.shape)\ntest.head()","4ec856bc":"# creating an additional column to distinguish between the datasets after concating\ntrain[\"train\/test\"]= \"train\"\ntest[\"train\/test\"]= \"test\"\n\n# concating\ncombined_data= pd.concat([train, test], axis=0)\nprint(\"combined data shape :\", combined_data.shape)\ncombined_data.head()","d623fca2":"combined_data= combined_data.drop(columns=[\"id\",\"author\" , \"title\", \"label\"], axis=1)\ncombined_data.head()","6bb95290":"combined_data.isnull().sum()","9d28e56d":"# replace missing values\ncombined_data= combined_data.fillna(\" \")","8d6f4aae":"import re\ndef clean(text):\n    # removing all the characters other than alphabets\n    cleaned_text= re.sub(\"[^a-zA-Z]\", \" \", text)\n    # converting text to lower case\n    cleaned_text= cleaned_text.lower()\n    return cleaned_text\n\n# Now creating a separate column which contains the above function applied to \"text\" column\ncombined_data[\"cleaned(only alphabets)\"]= combined_data[\"text\"].apply(lambda x : clean(x) )\ncombined_data.head()","fc3c061c":"def text_to_words(text):\n    words= text.split()\n    return words\n\n# column containing text converted to list of words\ncombined_data[\"text_to_words\"]= combined_data[\"cleaned(only alphabets)\"].apply(lambda x : text_to_words(x))\ncombined_data.head()","8bb16bfb":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\ndef clean_words(word_list):\n    # applying stop words\n    word= [stemmer.stem(word) for word in word_list if not word in stopwords.words(\"english\")]\n    # joining them again\n    word= \" \".join(word)\n    return word  \n\ncombined_data[\"cleaned_words\"]= combined_data[\"text_to_words\"].apply(lambda x: clean_words(x))","fd5f14c8":"combined_data.head(7)","d7ceef78":"def word_count(text):\n    text = str(text)\n    return len(text.split(' '))\n\ncombined_data['word_count'] = combined_data['cleaned_words'].apply(word_count)\ncombined_data.head()","e3234203":"print(\"Text Before Preprocessing :\\n\\n\", combined_data[\"text\"][0])\nprint(\"\\n\\nText After Preprocessing :\\n\\n\",combined_data[\"cleaned_words\"][0])","83d5804a":"train_data= combined_data[combined_data[\"train\/test\"] == \"train\"]\ntest_data= combined_data[combined_data[\"train\/test\"] == \"test\"]","f35816c9":"from keras.preprocessing.text import one_hot\nvocab_size= 10000\n\n# one-hot encoding train_data\ntrainWords_to_list= []\nfor i in range(0, len(train_data)):\n    trainWords_to_list.append(train_data[\"cleaned_words\"][i])\n    \nencoded_train= [one_hot(word, vocab_size) for word in trainWords_to_list]\n\n\n# one-hot encoding test_data\ntestWords_to_list= []\nfor i in range(0, len(test_data)):\n    testWords_to_list.append(test_data[\"cleaned_words\"][i])\n    \nencoded_test= [one_hot(word, vocab_size) for word in testWords_to_list]","8226137a":"encoded_train[0]","c6f216d4":"from keras.preprocessing.sequence import pad_sequences\ntext_length= 500\n\n# applying padding to one-hot encoded train_data\npadded_train= pad_sequences(encoded_train, padding= \"pre\", maxlen=text_length)\n\n# applying padding to one-hot encoded test_data\npadded_test= pad_sequences(encoded_test, padding= \"pre\", maxlen=text_length)","d60436e2":"padded_train[0]","374b2493":"padded_test[0]","2ee9773c":"import numpy as np\nfeature= np.array(padded_train)\ntarget= np.array(train[\"label\"])","3b3c6713":"print(\"shape of feature\", feature.shape)\nfeature","c205a016":"print(\"shape of target\", target.shape)\ntarget","0a1345c5":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(feature, target, test_size=0.2, random_state=10)","5e05b22e":"from keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Dropout, LSTM\n\n# number of features is required to be mentioned in order to convert word into it's vector form \nembedding_features= 30\n\nmodel= Sequential()\n# this layer converts padded data into vectors\nmodel.add(Embedding(vocab_size, embedding_features, input_length= text_length))\nmodel.add(Dropout(0.3))\n\n# LSTM layer with 100 neurons\nmodel.add(LSTM(units= 50))\nmodel.add(Dropout(0.3))\n\n# output layer\nmodel.add(Dense(units=1, activation=\"sigmoid\"))\n\nmodel.summary()","4be68b71":"model.compile(loss= \"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nhistory= model.fit(x_train, y_train,\n          validation_data= (x_val, y_val),\n          batch_size= 100, epochs=10)","d55c1d83":"history.history.keys()","fa760387":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nepochs= range(len(history.history[\"accuracy\"]))\n# accuracy plot\nplt.plot(epochs, history.history[\"accuracy\"])\nplt.plot(epochs, history.history[\"val_accuracy\"])\nplt.xlabel(\"epoch\")\nplt.ylabel(\"accuracy\")\nplt.title(\"Accuracy\")\nplt.legend([\"train_data\", \"validation_data\"])\nplt.show()\n\n# loss plot\nplt.plot(epochs, history.history[\"loss\"])\nplt.plot(epochs, history.history[\"val_loss\"])\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.title(\"Loss\")\nplt.legend([\"train_data\", \"validation_data\"])\nplt.show()","1c364bd3":"pred_val= model.predict_classes(x_val)\n\nfrom sklearn.metrics import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\ncm= confusion_matrix(y_val, pred_val)\nplot_confusion_matrix(cm, figsize=(5,5))","e33deba0":"test_array= np.array(padded_test)\nprediction_test= model.predict_classes(test_array)","5acf7f1d":"submission= pd.DataFrame()\nsubmission[\"id\"]= test[\"id\"]\nsubmission[\"label\"]= prediction_test\nsubmission.to_csv(\"LSTM_model.csv\", index=False)\nsubmission.head()","c1f27cbd":"<span style=\"color:Olive; font-size:15px;\">\nLet's have a look at what we have done till now. Let's compare the text before and after preprocessing \n<\/span>","ccaeb8b3":"<center>\n<span style=\"font-family:luxury; color:MediumVioletRed; font-size:45px;\"> FAKE NEWS CLASSIFIER <\/span>\n<\/center>   \n<center>\n<span style=\"font-family:luxury; color:Plum\t; font-size:30px;\"> LSTM Model <\/span>\n<\/center> ","6da7ea4b":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:20px;\">Checking model performance through validation data <\/span>","3f6c5634":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:25px;\">Splitting train_data into train and validation set<\/span>","827adc33":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\"> Reading train data<\/span>","819b2ddc":"<span style=\"color:Olive; font-size:15px;\">\nThis is how one-hot encoded text looks like. Each word is represented by it's index number present in the vocabulary size.\n<\/span>","c4722f23":"<span style=\"color:Olive; font-size:15px;\">\n    The \"text\" column is the column which contains the news, so we only need to preprocess this, we'll also take \"train\/test\"\ncolumn so that both the datasets can be distinguished later\n<\/span>","a716d86e":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:25px;\"> One-hot Encoding<\/span><br>\n<span style=\"color:Olive; font-size:15px;\">\nBefore applying word embedding, words must be one-hot encoded\n<\/span>","fac7d577":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\"> Data Preprocessing<\/span><br>","a56e48c4":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:25px;\"> Text Padding<\/span><br>\n<span style=\"color:Olive; font-size:15px;\">\nModel require to have inputs with the same shape and size. And not all the sentences have the same length. So we need to do padding.<br>\n    Padding is done by adding zeros for short sentences(pre or post the sentence) and truncating the sentences which exceed the max number of words which is declared by \"maxlen\".\n<\/span>","25b7bc48":"<span style=\"color:Olive; font-size:15px;\">\nRemoving records with missing values.\n<\/span>","6c3225d1":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\">Submission<\/span>","0c8bfa7b":"<span style=\"color:Olive; font-size:15px;\">\nBut in test_data, we cannot see any zeros. This is because the length of this text is already greater than mentioned text_length= 500. So instead it's length have been truncated to make it equal 500.\n<\/span>","214bde5b":"<span style=\"color:Olive; font-size:15px;\">\nIt can be seen that in the first text of train_data, zeros have been added to make it equal to text_length=500\n<\/span>","97c171f0":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:25px;\"> Vector Representation of words<\/span><br>\n<span style=\"color:Olive; font-size:15px;\">\nNow we have to convert meaningful text into vector representation such that a machine can understand the pattern associated in any text and can make out the context of sentences.<br>\nAnd for this we will be using word embedding technique.    \n<\/span>","f2888217":"<span style=\"color:Olive; font-size:15px;\">\n Second step is to convert the whole text into a list of words, so that the stop words can be applied to them later\n<\/span>","4a525f36":"<span style=\"color:Olive; font-size:15px;\">\nSo we got our cleaned_words, create a function to count number of words it contains\n<\/span>","c8ef50fa":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\">Model Training<\/span>","3c8a8064":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\">Model Evaluation<\/span>","1806dd00":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:25px;\"> Check Missing Values<\/span><br>","4526b50c":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\"> Reading test data<\/span>","1ee54ec8":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\"> Separating train and test dataset<\/span><br>","1cc81350":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\">Creating Feature and Target for train_data<\/span>","33cb565d":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\">Model Building<\/span>","be5ec9c0":"<span style=\"color:Olive; font-size:15px;\">\nThe first step in data preprocessing is to consider only alphabets and removing the numbers, special characters and converting all the words to lower case\n<\/span>","3c56977a":"<span style=\"font-family:luxury; color:MediumVioletRed; font-size:30px;\"> Combined data<\/span><br>\n<span style=\"color:Olive; font-size:15px;\">\n    Concating both train and test dataset so that text preprocessing before giving it to model can be done simultaneously on both of them.\n<\/span>","e4c2388a":"<span style=\"color:Olive; font-size:15px;\">\n Words like a, the , is , then etc are redundant and doesn't play any role in nlp, so it's better to remove these type of words. And stop words is the way to do that.\n<\/span>","6f8413dc":"<span style=\"color:Olive; font-size:15px;\">\nSince the test data doesn't have \"label\" column in it, so we need to create a validation data from train_data for training purpose.\n<\/span>"}}