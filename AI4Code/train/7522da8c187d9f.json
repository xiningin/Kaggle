{"cell_type":{"2f35d967":"code","47ae678a":"code","69f0bcf7":"code","091c402a":"code","de3f3429":"code","7295cb95":"code","5445b46b":"code","9ad22f2e":"code","ca19ac74":"code","5f0c35f8":"code","b9c23b27":"code","c19e8c93":"code","f375c36a":"code","8e8ab31d":"markdown","d41a7791":"markdown"},"source":{"2f35d967":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport os\n \nimport pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","47ae678a":"# Load all data required \npath=\"..\/input\/CORD-19-research-challenge\/2020-03-13\/\"\ncord_all_sources=pd.read_csv(path+\"all_sources_metadata_2020-03-13.csv\")\ncord_all_sources.shape","69f0bcf7":"cord_all_sources.head(5)","091c402a":"# Remove nulls from abstract \ncord_all_sources['abstract'].dropna()","de3f3429":"cord_all_sources.keys()","7295cb95":"biorxiv_dir = '\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/biorxiv_medrxiv\/biorxiv_medrxiv\/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))\n\nall_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)\n\n\nfile = all_files[0]\nprint(\"Dictionary keys:\", file.keys())\n\n","5445b46b":"cord_all_sources['abstract'].head(10)\ncord_all_sources['sha'].head(10)","9ad22f2e":"# TF  lib not working so starting with simple search and will add TF once its working\n# I am going to combine two data files with metadata for better search results and without much efforts to Original  file system.\n# Idea is to make all the data as it is, without much efforts to clean or face any performance issues. \nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\n\n\ndef search_covid_data(querystring,key):\n    abstract= []\n    abstract = (cord_all_sources['abstract'].dropna())\n    for i in range(len(abstract)):\n        if key == 'any':\n            if any(word in str(abstract[1:11]) for word in querystring):\n                paper_id = i\n                \n        elif key=='all':\n             if all(word in str(abstract[1:11]) for word in querystring):\n                paper_id = i\n                \n    paper_id_a = cord_all_sources['sha'][paper_id]\n     \n    \n    if file['paper_id'] == paper_id_a:\n        print(file['body_text'][0]['text'])\n    else:\n        print(\"No Paper ID found or NULL ID. But here is actual text\")\n        print(cord_all_sources['title'][0])\n    \n\n\n","ca19ac74":"# simple search without any heavy lib but accuarcy may not be good\nsearch_covid_data(\"The geographic spread of 2019 novel coronaviru\",'all')\n ","5f0c35f8":"import nltk\nfrom nltk.stem.lancaster import LancasterStemmer\nimport numpy as np\nimport tflearn\nimport tensorflow as tf\nimport random\n \nimport string\nimport unicodedata\nimport sys\nfrom sklearn.metrics import precision_score\n\ndef NLPSearch(sSearchString):\n    # TF not working - need to look \n     with open(filenames) as json_data:\n     data = json.load(json_data)\n\n    categories = list(data.keys())\n    words = []\n    docs = []\n\n    for each_category in data.keys():\n        for each_sentence in data[each_category]:\n\n            each_sentence = remove_punctuation(each_sentence)\n            print(each_sentence)\n\n            w = nltk.word_tokenize(each_sentence)\n            print(\"tokenized words: \", w)\n            words.extend(w)\n            docs.append((w, each_category))\n\n            words = [stemmer.stem(w.lower()) for w in words]\n            words = sorted(list(set(words)))\n\n#print(words)\n#print(docs)\n\n\ntraining = []\noutput = []\n\noutput_empty = [0] * len(categories)\n\n\nfor doc in docs:\n\n    bow = []\n\n    token_words = doc[0]\n\n    token_words = [stemmer.stem(word.lower()) for word in token_words]\n\n    for w in words:\n        bow.append(1) if w in token_words else bow.append(0)\n\n    output_row = list(output_empty)\n    output_row[categories.index(doc[1])] = 1\n\n    training.append([bow, output_row])\n\nrandom.shuffle(training)\ntraining = np.array(training)\n\ntrain_x = list(training[:, 0])\ntrain_y = list(training[:, 1])\n\n\ntf.reset_default_graph()\n\nnet = tflearn.input_data(shape=[None, len(train_x[0])])\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, 8)\nnet = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\nnet = tflearn.regression(net)\n\n\nmodel = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n# Start training (apply gradient descent algorithm)\nmodel.fit(train_x, train_y, n_epoch=2000, batch_size=10, show_metric=True)\nmodel.save('model.tflearn')\n\nmodel.load('model.tflearn')\nsent_1 = \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\",\n\"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\"\n\n\n\n\ndef get_tf_record(sentence):\n    global words\n    # tokenize the pattern\n    sentence_words = nltk.word_tokenize(sentence)\n    # stem each word\n    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n    # bag of words\n    bow = [0]*len(words)\n    for s in sentence_words:\n        for i, w in enumerate(words):\n            if w == s:\n                bow[i] = 1\n\n    return(np.array(bow))\n\n\n# we can start to predict the results for each of the 4 paper\nprint(categories[np.argmax(model.predict([get_tf_record(sent_1)]))])","b9c23b27":"# pip install Whoosh Search","c19e8c93":"!pip install Whoosh","f375c36a":"from whoosh.fields import Schema, TEXT, ID\nfrom whoosh import index\n","8e8ab31d":"**I will use tensor flow to make search more relevent with abstract part of the given json data. I have divided this problem in two parts.**\n1. Use NLP based search for better accuracy -  TF not working in this notebook, need to try again . if it fails\n2. Go to standard search using keywords \/ REG expression.\n3. Using Whoosh search for keywords based but it needs internet access.\n4. Hybrid search using NLP + REG expression ","d41a7791":"Actual articles files - json"}}