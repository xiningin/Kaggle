{"cell_type":{"f04c2c1f":"code","437d02dd":"code","7dfbbd16":"code","10846fd4":"code","45555c1c":"code","e341c580":"code","ada0dc18":"code","cf3ce4ed":"code","cda407b1":"code","ff013bd9":"code","8036ec73":"code","08d2b252":"code","4ece7c35":"code","d2b24862":"code","cc4d446c":"code","edb13321":"code","771763c1":"code","c83102ac":"code","9ccde59c":"code","ff5b86ec":"code","b06b5cfe":"code","a5aa675e":"code","6128bf3c":"markdown","a9d5c725":"markdown","a51baae9":"markdown","b91fa97f":"markdown","f869cc27":"markdown","849b6d26":"markdown","dc0bab5b":"markdown","1fa8e29c":"markdown"},"source":{"f04c2c1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","437d02dd":"df = pd.read_csv('..\/input\/heart.csv')","7dfbbd16":"df.head()","10846fd4":"# stats\ndf.describe()","45555c1c":"df.info()","e341c580":"plt.rcParams['figure.figsize'] = (20,8)\ndf.hist()","ada0dc18":"y = df['target']\ndf.drop('target', axis=1,inplace=True)","cf3ce4ed":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score","cda407b1":"labelEncoder = LabelEncoder()\ndf['oldpeak'] = labelEncoder.fit_transform(y=df['oldpeak'])","ff013bd9":"X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.3, random_state=17)","8036ec73":"tree = DecisionTreeClassifier(random_state=17, max_depth=3, min_samples_leaf=2)\ntree.fit(X=X_train, y=y_train)","08d2b252":"tree.score(X_test, y_test)","4ece7c35":"preds = tree.predict(X_test)\naccuracy_score(y_true=y_test, y_pred=preds)","d2b24862":"export_graphviz(tree, 'tree1.dot', filled=True, feature_names=X_train.columns, rounded=True)\n!dot -Tpng 'tree1.dot' -o  'tree1.png'","cc4d446c":"!ls","edb13321":"from sklearn.model_selection import GridSearchCV, StratifiedKFold","771763c1":"# Let's vary hyperparameters from 2 - 10\nbest_parameters = {'max_depth': np.arange(2,11), 'min_samples_leaf': np.arange(2,11)}\ndecision_tree = DecisionTreeClassifier(criterion='entropy') # for information gain and entropy\nmodel = GridSearchCV(estimator=decision_tree, param_grid=best_parameters, n_jobs=-1, verbose=1, cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=17))\nmodel.fit(X_train, y_train)\nmodel.best_params_","c83102ac":"model.best_score_","9ccde59c":"preds_2 = model.predict(X_test)","ff5b86ec":"accuracy_score(y_test, preds_2)","b06b5cfe":"export_graphviz(model.best_estimator_, out_file='tree2.dot', filled=True, feature_names=X_train.columns, rounded=True)","a5aa675e":"!dot -Tpng 'tree2.dot' -o 'tree2.png'","6128bf3c":"Attribute Information:\n> 1. age\n> 2. sex\n> 3. chest pain type (4 values)\n> 4. resting blood pressure\n> 5. serum cholestoral in mg\/dl\n> 6. fasting blood sugar > 120 mg\/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved\n> 9. exercise induced angina\n> 10. oldpeak = ST depression induced by exercise relative to rest\n> 11. the slope of the peak exercise ST segment\n> 12. number of major vessels (0-3) colored by flourosopy\n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\n","a9d5c725":"Okay, so now, we have an accuracy of approximately 83%. Let's see if Logistic Regression can help.","a51baae9":"Let's use GridSearch and Stratified K-fold Cross-validation","b91fa97f":"## Observations:\n1. 303 rows of data\n2. population age: 29 -> 77 (avg 54)\n3. Sex (=1) is 68.32 % of the population\n4. Target (=1) is 54.46 % of the population\n5. All features are encoded to class numbers\n6. Most variables have a finite number of classes","f869cc27":"Training a very basic DT classifier, we can see that it is possible to achieve a 75% accuracy. This is not a good performance, let's see if we can improve on this performance by using cross-validation.","849b6d26":"<img src='tree1.png'>","dc0bab5b":"<img src='tree2.png'>","1fa8e29c":"We can notice that we got a 8% improvement in the results when we use a grid search to find optimal hyperparameters and then use the derived tree to classify. So, let's visualize the tree again."}}