{"cell_type":{"06d86ee4":"code","882fdde7":"code","c2ce94ad":"code","fcd72655":"code","71bcc998":"code","c95732e6":"code","ee7a7632":"code","4c94997a":"code","1a95359b":"code","3ef8e7c1":"code","bac3618f":"code","ed003e4e":"code","f61f6e93":"code","2d4c7d20":"code","6960e18c":"markdown","dd6f31d5":"markdown","98b7acad":"markdown","898a788c":"markdown","e54ee85b":"markdown","6a1f957d":"markdown","3f6e6978":"markdown","5a4db938":"markdown","edae6383":"markdown","809a7f07":"markdown","4dbf3410":"markdown"},"source":{"06d86ee4":"import numpy as np\nfrom numpy import tanh\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport math\nimport timeit","882fdde7":"pd.set_option('display.max_columns', 50)\nafr = pd.read_csv('..\/input\/africa-economic-banking-and-systemic-crisis-data\/african_crises.csv').drop(['case','cc3','country','year'], axis=1)\nafr.dropna(inplace=True)\nafr.banking_crisis = afr.banking_crisis.map({'crisis':1,'no_crisis':-1})\nafr","c2ce94ad":"x = afr.drop('banking_crisis', axis=1).values\ny = afr.banking_crisis.values\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)","fcd72655":"labels = ['batch', 'stochastic']\nlearnings = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n# funzione per eseguire i calcoli\n# passati un array di percettroni, il tipo di perceptron desiderato e l'errore da raggiungere, addestra i percettroni con i learning_rates\n# sopra definiti e restituisce 2 array con i calcoli della precisione e degli errori commessi sul test set\ndef calc(p, t, eps = 0.2):\n    start = timeit.default_timer()\n    ris = []; er = []\n    for l in learnings:\n        ris.append([]); er.append([]); ind = learnings.index(l)\n        for i in range(len(p)):    \n            p[i].train(x=X_train, y=y_train, x_test=X_test, y_test=y_test, learning_rate=l, no_batch=[i], t=t, eps=eps)\n            ris[ind].append(p[i].acc); er[ind].append(p[i].errors)\n    stop = timeit.default_timer()\n    print('Time: ', stop - start)  \n    return ris, er\n# funzioni per disegnare\nplt.rcParams.update({'font.size': 22})\n\n# disegna le funzioni segno, sigmoid, relu, tanh\ndef dis_func(t=0, names=[]):\n    plt.rcParams[\"figure.figsize\"] = (20, 5)\n    if t == 0:\n        fun = lambda x:x\n    elif t == 1:\n        fun = sigmoid\n    elif t == 2:\n        fun = tanh\n    else:\n        fun = relu\n    deriv = derivative(t)\n    l = np.linspace(-5, 5, 100)\n    f, axx = plt.subplots(1,2)\n    yd = []; yy = []\n    for u in l:\n        yy.append(fun(u))\n        yd.append(deriv(fun(u)))\n    axx[0].plot(l, yy)\n    axx[0].grid(True)\n    axx[0].set_title(names[0])\n    axx[1].plot(l, yd); axx[1].grid(True); axx[1].set_title(names[1])\n    plt.rcParams[\"figure.figsize\"] = (25, 20)\n# disegna accuratezza ed errori commessi\ndef dis(p, r, e):\n    plt.rcParams[\"figure.figsize\"] = (25, 20)\n    fig, ax = plt.subplots(len(learnings), 2) \n    for l in range(len(learnings)):\n        ax[l][0].set_title('accuratezza con learning_rate = {} '.format(learnings[l]))\n        ax[l][1].set_title('errore quadratico medio con learning_rate = {}'.format(learnings[l]))\n        for i in range(len(p)):\n            ax[l][0].plot(range(1,len(r[l][i]) + 1), r[l][i], label=labels[i])\n            ax[l][0].legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n            ax[l][1].plot(range(1,len(e[l][i]) + 1), e[l][i], label=labels[i])\n            ax[l][1].legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n    plt.legend()\n    plt.tight_layout()","71bcc998":"# Funzione per il calcolo della sigmoide\ndef sigmoid(gamma):\n    if gamma < 0:\n        return 1 - 1\/(1 + math.exp(gamma))\n    else:\n        return 1\/(1 + math.exp(-gamma))\n    \n# funzione per il calcolo della relu\ndef relu(x):\n    return max(0.001*x, x)\n\n# Serve per fornire alla classe perceptron la giusta funzione segno in base alla funzione di attivazione \ndef sign(t=0):\n    if t == 1:\n        return lambda x: np.where(x >= 0.5, 1, -1)\n    return lambda x: np.where(x >= 0.0, 1, -1)\n    \n# Fornisce alla classe perceptron la derivata della funzione di attivazione in base alla funzione di attivazione\ndef derivative(t=0):\n    if t == 0:\n        return lambda x: 1\n    if t == 1:\n        return lambda x: x * ( 1 - x )   \n    if t == 2:\n        return lambda x: 1.0 - x**2\n    if t == 3:\n        return lambda x: 1 if x >= 0 else 0.001","c95732e6":"\n# classe perceptron\nclass perceptron:\n    \n    # In base al parametro definsice la funzione di attivazione\n    def act(self, t=0):\n        if t == 0:\n            return lambda x: np.dot(x, self.w[1:]) + self.w[0]  \n        if t == 1:\n            return lambda x: sigmoid(np.dot(x, self.w[1:]) + self.w[0])\n        if t == 2:\n            return lambda x: tanh(np.dot(x, self.w[1:]) + self.w[0])\n        if t == 3:\n            return lambda x: relu(np.dot(x, self.w[1:]) + self.w[0])\n    \n    # predice applicando la funzione segno all'attivazione\n    def predict(self,x): \n        return self.sign(self.activation(x)) \n    \n    # calcolo dell'errore quadratico medio\n    def err_calc(self, x, y):\n        ris = []; m = 0\n        for xi, yi in zip(x, y):\n            p = self.sign(self.activation(xi))\n            m += ( yi - p ) ** 2\n        return m \/ ( 2 * len(x) )\n    \n    # calcolo dell' accuratezza\n    def accuracy(self, x, y):\n        ris = []\n        for e in x:\n            ris.append(self.predict(e.tolist()))\n        return (sum(np.array(ris) == np.array(y)) \/ len(y))\n    \n    # inizializza i valori\n    # x \u00e8 il train set, learning_rate = eta, no_batch = 0 crea un perceptron di tipo batch, t il tipo di attivazione\n    def initialize(self, x, learning_rate, no_batch, t=0):\n        # assegno le funzioni in base al tipo desiderato\n        self.sign = sign(t); self.activation = self.act(t); self.derivative = derivative(t)  \n        self.errors = []; self.no_batch = no_batch; self.acc = []; self.inputs_dim = len(x[0])\n        self.w = np.random.uniform(low=-0.05, high=0.05, size=(self.inputs_dim + 1,))\n        self.learning_rate = learning_rate; self.w[0] = 1 # bias iniziale\n        \n    # aggiorna pesi\n    def update(self, xi, yi):\n        s = self.activation(xi)\n        p = self.sign(s)\n        # aggiorno solo se ha sbagliato la predizione\n        if p - yi != 0: \n            err = yi - s \n            # se tipo batch aggiorna i delta\n            if self.no_batch == 0: \n                self.dw[1:] += self.learning_rate * xi * err * self.derivative(s)\n                self.dw[0] += self.learning_rate * err * self.derivative(s)\n            else: \n                # se non batch aggiorna i pesi\n                self.w[1:] += self.learning_rate * xi * err * self.derivative(s)\n                self.w[0] += self.learning_rate * err * self.derivative(s)\n                \n    # addestra x,y sono il train , x_test, y_test il test, epochs il numero massimo di epoche se non raggiunge l'errore desiderato,\n    # eps l'errore da raggiungere, no_batch=0 crea un perceptron di tipo batch, t = tipo di attivazione\n    def train(self, x, y, x_test, y_test, learning_rate = 0.001, epochs=10000, eps=0.2, no_batch=1, t=0):\n        # inizializza i dati\n        self.initialize(x=x, learning_rate=learning_rate, no_batch=no_batch, t=t) \n        # itera sule epoche\n        for e in range(epochs): \n            # se batch inizializza i delta\n            if self.no_batch == 0:\n                self.dw = np.zeros(self.inputs_dim + 1)\n            # chiama la funzione di update per ogni esempio    \n            for xi, yi in zip(x, y): \n                self.update(xi, yi)\n            # se batch somma delta ai pesi\n            if self.no_batch == 0: \n                self.w -= self.dw \/ len(x)\n            # crea gli array di accuratezza ed errore    \n            self.acc.append(self.accuracy(x=x_test, y=y_test)) \n            er = self.err_calc(x=x, y=y)\n            self.errors.append(er) \n            #se raggiunto l'errore interrompe\n            if er < eps and e > 10: \n                break","ee7a7632":"dis_func(t=0,names=['sign(x)', 'sign(x) derivata'])","4c94997a":"perc_n = [perceptron(), perceptron()]\nris_n,er_n = calc(perc_n, t=0)\ndis(perc_n, ris_n, er_n)","1a95359b":"dis_func(t=1,names=['sigmoid(x)', 'sigmoid(x) derivata']) # disegna la funzione","3ef8e7c1":"perc_s = [perceptron(), perceptron()]\nris_s,er_s = calc(perc_s, t=1)\ndis(perc_s, ris_s, er_s)  ","bac3618f":"dis_func(t=2,names=['tanh(x)', 'tanh(x) derivata'])","ed003e4e":"perc_t = [perceptron(), perceptron()]\nris_t,er_t = calc(perc_t, t=2)\ndis(perc_t, ris_t, er_t)","f61f6e93":"dis_func(t=3,names=['relu(x)', 'relu(x) derivata'])","2d4c7d20":"perc_r = [perceptron(), perceptron()]\nris_r,er_r = calc(perc_r, t=3)\ndis(perc_r, ris_r, er_r)","6960e18c":"Import dei valori, rimozione dei valori nulli e mapping {-1, 1} degli output","dd6f31d5":"Definizione delle funzioni di supporto per la classe perceptron","98b7acad":"Applicazione del perceptron con funzione di attivazione tanh(x).\n\n    def tanh(x):\n        \n        return sinh(x) \/ cosh(x)\n        \nVantaggi:\n* Range [-1,1].\n* Gradiente pi\u00f9 incisivo della sigmoide.\n\nProblemi:\n* Vanishing gradient.\n\nDimostrazione della derivata della tanh : \n\n(d\/dx) (tanh(x)) =\n\n(d\/dx) (sinh(x) \/ cosh(x)) =\n\n((sinh(x))' * cosh(x) - sinh(x) * (cosh(x))') \/ (cosh(x)) ^ 2 = \n\n(cosh(x) * cosh(x) - sinh(x) * sinh(x)) \/ (cosh(x)) ^ 2 =\n\n(cosh(x) ^ 2 - sinh(x) ^ 2) \/ (cosh(x)) ^ 2 =\n\n(cosh(x) ^ 2 \/ cosh(x) ^ 2) - ( sinh(x) ^ 2 \/ cosh(x) ^ 2) =\n\n1 - ( sinh(x) ^ 2 \/ cosh(x) ^ 2) =\n\n1 - ( sinh(x) \/ cosh(x) ) ^ 2 = \n\n1 - tanh(x) ^ 2\n","898a788c":"Applicazione del perceptron con funzione di attivazione relu(x).\n\n    def relu(x):\n\n        return max(0.001*x, x)\n        \nDella funzione relu, ne esistono diverse varianti, in particolare, quella normale differisce da quella utilizzata in quanto definita come max(0, x), ma rischia di far morire il perceptron, ovvero una volta che il arriva a predire 0 non si riesce a recuperarlo, per questo ho usato una variante detta leaky relu, che per valori negativi restituisce max(0.001 * x, x). Altre varianti sono ad esempio la parametric relu definita come return max(a * x, x) con a > 0 , di cui la leaky relu ne \u00e8 a sua volta una variante o la exponential relu definita come max(a * (e ^x - 1) * x, x).\n\nIn generale, la relu :\n\n* E' pi\u00f9 veloce da calcolare rispetto alle altre funzioni.\n* Converge pi\u00f9 velocemente.\n* No vanishing gradient.\n\nProblemi:\n* Dying ReLU : neuroni che muoiono arrivando a predire sempre 0.\n* Non usata in certe applicazioni in quanto pu\u00f2 restituire valori molto elevati.\n* Valori troppo elevati del learning rate possono portare a non convergere.\n* Di solito viene utilizzata negli strati interni delle reti neurali in quanto ha un risultato lienare se positivo.\n\n\nDimostrazione della derivata della relu : \n\n(d\/dx) (relu(x)) =\n\ncaso 1 : x >= 0\n\n(d\/dx) x = 1\n\ncaso 2 : x < 0\n\n(d\/dx) 0.001 * x = 0.001\n","e54ee85b":"Import delle librerie necessarie","6a1f957d":"Implementazione del perceptron","3f6e6978":"Come si pu\u00f2 notare, ReLu e identit\u00e0, tendono a convergere pi\u00f9 in fretta ad una soluzione, il problema \u00e8 che a learning rate pi\u00f9 alti, non arrivano a convergere in tempo ragionevole. Le altre 2 funzioni, risultano invece essere pi\u00f9 stabili, in quanto convergono al risultato anche se con pi\u00f9 lentezza. Inoltre, sigmoide e tanh risultano avere un apprendimento molto instabile all'aumentare del learning rate, a differenza di ReLu e identit\u00e0.","5a4db938":"Definizione delle funzione per il successivo utilizzo della classe perceptron, in particolare, le funzioni per eseguire i calcoli di accuratezza del perceptron e per disegnare riusltati e funzioni di attivazione","edae6383":"Applicazione del perceptron con funzione di attivazione sigmoide(x).\n\n    def sigmoid(gamma):\n\n        if gamma < 0:\n    \n            return 1 - 1\/(1 + math.exp(gamma))\n       \n        else:\n    \n            return 1\/(1 + math.exp(-gamma))\n\nVantaggi:\n* Molto comoda per rappresentare probabilit\u00e0 in quanto ha range di output [0,1].\n* Buona per predire relazione binarie.\n\nProblemi :\n* Vanishing gradient : il gradiente diventa talmente piccolo che causa un cambiamento dei pesi talemente piccolo da bloccare quasi l'apprendimento nei primi strati delle reti neurali.\n* Computazionalmente espensiva per via dell'exp.\n\nDimostrazione della derivata della sigmoide : \n\n(d\/dx)\u03c3(x)=\n\n(d\/dx)( 1 \/ (1+e^(-x)) )=\n\n(d\/dx)(1+e^(\u2212x))^(\u22121)=\n\n\u2212(1+e^(\u2212x))^(\u22122) * (\u2212e^(\u2212x))=\n\ne^(\u2212x) \/ (1+e^(\u2212x))^2=\n\n1 \/ (1+e^(\u2212x)) * (e^(\u2212x) \/ (1+e^(\u2212x))=\n\n1 \/ (1+e^(\u2212x)) * ((1+e^(\u2212x)\u22121) \/ (1+e^(\u2212x)))=\n\n1 \/ (1+e^(\u2212x)) * (((1+e^(\u2212x)) \/ (1+e^(\u2212x))) \u2212 (1 \/ (1+e^(\u2212x))))=\n\n1 \/ (1+e^(\u2212x)) * (1 \u2212 (1 \/ (1+e^(\u2212x))=\n\n\u03c3(x)\u22c5(1\u2212\u03c3(x))","809a7f07":"Split del dataset in train e test","4dbf3410":"Applicazione del perceptron con funzione di attivazione segno(x)"}}