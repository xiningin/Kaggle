{"cell_type":{"d129c36b":"code","97ca7d6d":"code","c48a4dad":"code","aa00a010":"code","88083643":"code","5dcc30d2":"code","b2d99fd5":"code","b8e1d89b":"code","87f872aa":"code","62404b80":"code","509a8350":"code","ff00b6d6":"code","63bc2bc3":"code","806b0c70":"code","be7d6289":"code","9027cb9d":"code","8dcd301e":"code","ff5eea84":"code","ef6369fb":"code","5d9b1fb5":"code","b9ee12bf":"code","9f00b315":"code","013c48ba":"code","ae455ce2":"code","7093809b":"code","7a0f6787":"code","0830b18a":"code","a2e7ceb3":"code","48102738":"code","31acbe06":"code","4aeb269e":"code","6d77e2ff":"code","07d740b2":"code","69a01712":"code","af05bd2f":"code","a5c9d8f3":"code","4bc08ceb":"code","6fdd1b07":"code","97b9ed04":"code","9501230f":"code","c1c0f974":"code","80244333":"code","611abf32":"code","e5583362":"code","61133532":"code","752b29c8":"code","4f416b87":"code","aa840f2a":"code","a3837dd5":"code","5194da8d":"code","4a9589aa":"code","b1c83ad1":"code","1b3ad104":"code","66446334":"code","b51723e1":"code","b53636f7":"code","9be6fab6":"code","ac3468a0":"code","c7fd9f4d":"code","bbae997f":"code","36834e68":"code","b5076eb2":"code","42d89e62":"code","078c2b3e":"code","6d934b3d":"code","e8e70557":"code","cd79c946":"code","43b774ab":"code","8e39267a":"code","2d37af9f":"code","fe267707":"code","b8a939a0":"code","9831b8c6":"code","ce90a54c":"code","1e8b1f44":"code","edc78e6d":"code","d87fe580":"code","8364a84e":"code","642ffb79":"code","c6c5bd58":"markdown","2f37db64":"markdown","ce458b25":"markdown","09f2f10e":"markdown","5dd7eb27":"markdown","f6d8792f":"markdown","275b66a7":"markdown","de5f3edd":"markdown","eba6b45f":"markdown","8d45652f":"markdown","566f0561":"markdown","6e6a2e46":"markdown","93399e1e":"markdown"},"source":{"d129c36b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97ca7d6d":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nimport re\nimport string","c48a4dad":"train_data = pd.read_csv('\/kaggle\/input\/nlpcsv\/train.csv')\ntrain_data.head()","aa00a010":"train_data = train_data.drop('id', axis='columns')\ntrain_data.head()","88083643":"def clean_train_data(x):\n    text = x\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text) \n    text = re.sub(r'[^\\w\\s]','',text) \n    text = re.sub('\\w*\\d\\w*', '', text) \n    text = re.sub('\\n', '', text)\n    return text","5dcc30d2":"clean_data = train_data.copy()","b2d99fd5":"clean_data['text'] = train_data.text.apply(lambda x : clean_train_data(x))\nclean_data.head()","b8e1d89b":"eng_stopwords = nltk.corpus.stopwords.words(\"english\")","87f872aa":"def remove_eng_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in eng_stopwords]\n    join_text = ' '.join(remove_stop)\n    return join_text","62404b80":"remove_stop_data = clean_data.copy()","509a8350":"remove_stop_data['text'] = clean_data.text.apply(lambda x : remove_eng_stopwords(x))\nremove_stop_data.head()","ff00b6d6":"print(\"Before remove stopwords\", len(clean_data['text'][0]))\nprint(\"After remove stopwords\", len(remove_stop_data['text'][0]))","63bc2bc3":"from itertools import chain\nfrom collections import Counter","806b0c70":"list_words = remove_stop_data['text'].str.split()\nlist_words_merge = list(chain(*list_words))\n\nd = Counter(list_words_merge)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_common_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_common_words.head()","be7d6289":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=top_common_words)\nplt.xticks(rotation=90)","9027cb9d":"common_words_value = top_common_words['index'].values\nremove_words = ['man', 'life', 'night', 'house', 'heart']\nnew_stop_words = [x for x in common_words_value if x not in remove_words]\nnew_stop_words","8dcd301e":"def remove_new_stopwords(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [word for word in token_text if word not in new_stop_words]\n    join_text = ' '.join(remove_stop)\n    return join_text","ff5eea84":"new_stop_data = remove_stop_data.copy()","ef6369fb":"new_stop_data['text'] = remove_stop_data.text.apply(lambda x : remove_new_stopwords(x))\nnew_stop_data.head()","5d9b1fb5":"print(\"Before remove stopwords\", len(remove_stop_data['text'][4]))\nprint(\"After remove stopwords\", len(new_stop_data['text'][4]))","b9ee12bf":"plt.figure(figsize=(10,7))\nsns.set(style=\"darkgrid\")\nsns.countplot(x=\"author\", data=train_data)\nplt.title('Author text distribution')","9f00b315":"all_words_after = train_data['text'].str.split()\nmerged = list(chain(*all_words_after))\nd = Counter(merged)\ndf = pd.DataFrame(data=d, index=['count'])\ntop_count_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\ntop_count_words.head()","013c48ba":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=top_count_words)\nplt.xticks(rotation=90)\n\nplt.title(\"Most common words before removing stop words\")","ae455ce2":"all_words_before = new_stop_data['text'].str.split()\nmerged = list(chain(*all_words_before))\nd = Counter(merged)\ndf = pd.DataFrame(data=d, index=['count'])\nbefore_top_words = df.T.sort_values(by=['count'], ascending=False).reset_index().head(50)\nbefore_top_words.head()","7093809b":"plt.figure(figsize=(15,7))\nsns.set(style=\"darkgrid\")\nsns.barplot(x=\"index\", y='count', data=before_top_words)\nplt.xticks(rotation=90)\n\nplt.title(\"Most common words after removing stop words\")","7a0f6787":"eap_cloud = train_data[train_data.author == 'EAP'].text.values\nhpl_cloud = train_data[train_data.author == 'HPL'].text.values\nmws_cloud = train_data[train_data.author == 'MWS'].text.values","0830b18a":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwc = WordCloud(stopwords=STOPWORDS, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","a2e7ceb3":"plt.figure(figsize=(20, 15))\n\nplt.subplot(1, 3, 1)\nasd = \" \".join(eap_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Edgar Allen Poe')\n\nplt.subplot(1, 3, 2)\nasd = \" \".join(hpl_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('HP Lovecraft')\n\nplt.subplot(1, 3, 3)\nasd = \" \".join(mws_cloud)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Mary Shelley')\n\nplt.figtext(.5,.63,'All writers, word clouds before stop word removal', color='b', fontsize=25, ha='center')","48102738":"eap_cloud_before = new_stop_data[new_stop_data.author == 'EAP'].text.values\nhpl_cloud_before = new_stop_data[new_stop_data.author == 'HPL'].text.values\nmws_cloud_before = new_stop_data[new_stop_data.author == 'MWS'].text.values","31acbe06":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwc = WordCloud(stopwords=STOPWORDS, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","4aeb269e":"plt.figure(figsize=(20, 15))\n\nplt.subplot(1, 3, 1)\nasd = \" \".join(eap_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Edgar Allen Poe')\n\nplt.subplot(1, 3, 2)\nasd = \" \".join(hpl_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('HP Lovecraft')\n\nplt.subplot(1, 3, 3)\nasd = \" \".join(mws_cloud_before)\nwc.generate(asd)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Mary Shelley')\n\nplt.figtext(.5,.63,'All writers, word clouds After stop word removal', color='b', fontsize=25, ha='center')","6d77e2ff":"full_name = {'EAP': 'Edgar Allen Poe', 'MWS': 'Mary Shelley', 'HPL': 'HP Lovecraft'}\nwriter_name = ['EAP', 'MWS', 'HPL']\nwriter_count_obj = {'writer_full_name': [], 'total_words': [], 'unique_words': []}\nfor name in writer_name:\n    name_all_words = new_stop_data[new_stop_data.author == name].text.str.split()\n    name_merged = list(chain(*name_all_words))\n    name_total_len = len(name_merged)\n    myset = set(name_merged)\n    \n    writer_count_obj['writer_full_name'].append(full_name[name])\n    writer_count_obj['total_words'].append(name_total_len)\n    writer_count_obj['unique_words'].append(len(myset))","07d740b2":"words_df = pd.DataFrame(writer_count_obj)\nwords_df","69a01712":"tidy = words_df.melt(id_vars='writer_full_name').rename(columns=str.title)\ntidy","af05bd2f":"fig, ax1 = plt.subplots(figsize=(15, 10))\ntidy = words_df.melt(id_vars='writer_full_name').rename(columns=str.title)\nsns.barplot(x='Writer_Full_Name', y='Value', hue='Variable', data=tidy, ax=ax1)\nsns.despine(fig)","a5c9d8f3":"from nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","4bc08ceb":"lemm = WordNetLemmatizer()","6fdd1b07":"def word_lemmatizer(text):\n    token_text = nltk.word_tokenize(text)\n    remove_stop = [lemm.lemmatize(w) for w in token_text]\n    join_text = ' '.join(remove_stop)\n    return join_text","97b9ed04":"lemmatize_data = new_stop_data.copy()\nlemmatize_data['text'] = new_stop_data.text.apply(lambda x : word_lemmatizer(x))\nlemmatize_data.head()","9501230f":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vec = CountVectorizer(stop_words='english')\ndata_count_vec = count_vec.fit_transform(lemmatize_data.text)\ndata_count_vec","c1c0f974":"data_count_df = pd.DataFrame(data_count_vec.toarray(), columns=count_vec.get_feature_names())\ndata_count_df.index = lemmatize_data.author\ndata_count_df","80244333":"from sklearn.decomposition import NMF, LatentDirichletAllocation","611abf32":"lda_model = LatentDirichletAllocation(n_components=5, max_iter=5, learning_method = 'online', learning_offset = 50.,random_state = 0)","e5583362":"lda_model.fit(data_count_vec)","61133532":"lda_model.components_","752b29c8":"print_words = 20\nget_feature_names = count_vec.get_feature_names()\nfor index, topic in enumerate(lda_model.components_):\n    words = \" \".join([get_feature_names[i] for i in topic.argsort()[:-print_words - 1 :-1]])\n    print(f\"Topic - {index}:\")\n    print(words)\n    print(\"-\"*100)\n    print('\\n')","4f416b87":"from gensim import matutils, models\nimport scipy.sparse","aa840f2a":"data_count_df.index.name = None\nnew_dtm_t_data = data_count_df.T","a3837dd5":"spare_counts = scipy.sparse.csr_matrix(new_dtm_t_data)\nnew_corpus = matutils.Sparse2Corpus(spare_counts)\nnew_corpus","5194da8d":"cv = count_vec\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())","4a9589aa":"gensim_lda_topic = models.LdaModel(corpus=new_corpus, id2word=id2word, num_topics=5, passes=10)\ngensim_lda_topic.print_topics()","b1c83ad1":"train2 = pd.read_csv('\/kaggle\/input\/nlpcsv\/train.csv')\ntrain2.head()","1b3ad104":"print(nltk.word_tokenize(train2.text[0]))","66446334":"text = list(train2.text.values)\ntext","b51723e1":"tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                stop_words='english')\ntf = tf_vectorizer.fit_transform(text)","b53636f7":"print(tf_vectorizer.get_feature_names()[0:100])","9be6fab6":"from sklearn.model_selection import train_test_split\n\ntrain1 ,test1 = train_test_split(train2,test_size=0.2) \nnp.random.seed(0)\ntrain1.head()","ac3468a0":"X_train = train1['text'].values\nX_test = test1['text'].values\ny_train = train1['author'].values\ny_test = test1['author'].values","c7fd9f4d":"X_train[1],y_train[0]","bbae997f":"X_test[0],y_test[0]","36834e68":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn import svm","b5076eb2":"text_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', svm.LinearSVC())\n                    ])\ntext_clf = text_clf.fit(X_train,y_train)\ny_test_predicted = text_clf.predict(X_test)\nnp.mean(y_test_predicted == y_test)","42d89e62":"text_clf = Pipeline([('vect', TfidfVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', svm.LinearSVC())\n                    ])\ntext_clf = text_clf.fit(X_train,y_train)\ny_test_predicted = text_clf.predict(X_test)\nnp.mean(y_test_predicted == y_test)","078c2b3e":"from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\nimport xgboost as xgb\nimport lightgbm as lgbm\nfrom sklearn.naive_bayes import MultinomialNB","6d934b3d":"rfc = RandomForestClassifier()\netrc = ExtraTreesClassifier()\nxgbc = xgb.XGBClassifier()\nlgbmc = lgbm.LGBMClassifier()\nmnb = MultinomialNB()","e8e70557":"text_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', rfc)\n                    ])\ntext_clf = text_clf.fit(X_train,y_train)\ny_test_predicted = text_clf.predict(X_test)\nnp.mean(y_test_predicted == y_test)","cd79c946":"text_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', xgbc)\n                    ])\ntext_clf = text_clf.fit(X_train,y_train)\ny_test_predicted = text_clf.predict(X_test)\nnp.mean(y_test_predicted == y_test)\n","43b774ab":"text_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', lgbmc)\n                    ])\ntext_clf = text_clf.fit(X_train,y_train)\ny_test_predicted = text_clf.predict(X_test)\nnp.mean(y_test_predicted == y_test)","8e39267a":"text_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', mnb)\n                    ])\ntext_clf = text_clf.fit(X_train,y_train)\ny_test_predicted = text_clf.predict(X_test)\nnp.mean(y_test_predicted == y_test)","2d37af9f":"text = list(train2[train2['author']=='EAP'].text.values)","fe267707":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(lowercase=True,stop_words='english')","b8a939a0":"X =vectorizer.fit_transform(text)","9831b8c6":"from sklearn.decomposition import TruncatedSVD\nlsa = TruncatedSVD(n_components=3,n_iter=500)","ce90a54c":"lsa.fit(X)","1e8b1f44":"terms = vectorizer.get_feature_names()","edc78e6d":"for i,comp in enumerate(lsa.components_):\n    termsInComp = zip(terms,comp)\n    sortedterms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:10]\n    print(\"Topic %d:\" % i)\n    for term in sortedterms:\n        print(term[0])\n    print(\" \")","d87fe580":"from sklearn.decomposition import NMF","8364a84e":"vectorizer = TfidfVectorizer(max_features=1500, min_df=10, stop_words='english')\nX = vectorizer.fit_transform(text)\nwords = np.array(vectorizer.get_feature_names())\n\nprint(X)\nprint(\"X = \", words)","642ffb79":"nmf = NMF(n_components=10, solver=\"mu\")\nW = nmf.fit_transform(X)\nH = nmf.components_\n\nfor i, topic in enumerate(H):\n     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))","c6c5bd58":"# NMF Model","2f37db64":"## Topic List\n1. Topic 1: sea, water\n2. Topic 2: house, room\n3. Topic 3: dream, star\n4. Topic 4: time, table\n5. Topic 5: friend, heart","ce458b25":"# LDA with gensim ","09f2f10e":"# LSA with gensim ","5dd7eb27":"## Topic List\n1. Topic 1: friend, kind\n2. Topic 2: bod\n3. Topic 3: house\n4. Topic 4: man, moon,sea\n5. Topic 5: life, love","f6d8792f":"# EDA","275b66a7":"# Lemmatization","de5f3edd":"# -----------------------------------------","eba6b45f":"# ----Topic modelling----","8d45652f":"# Data Cleaning","566f0561":"# Vectorizing","6e6a2e46":"## Find out common words","93399e1e":"# Stopword Removal"}}