{"cell_type":{"091f5158":"code","d59a73f6":"code","ba49fda8":"code","4702a369":"code","0bccd58d":"code","a8d8082d":"code","9b7dd4e6":"code","98b0bd58":"code","bcfc5e4b":"code","160db0b3":"code","e2e4237b":"code","646bac5f":"code","ab11f25b":"code","f6219d31":"code","5fc4a353":"code","90246652":"code","aff341ab":"code","71fe7da0":"code","773296de":"code","b60e0146":"code","1849adc0":"code","a14fc8fc":"code","da658400":"code","601058e8":"markdown","1f3b0d02":"markdown","9768caef":"markdown","2db6f4ac":"markdown","37094734":"markdown","7c12ae6e":"markdown","802b1e7f":"markdown","3e86ef78":"markdown","2af23296":"markdown","15718112":"markdown","18e5096e":"markdown","9ce98e25":"markdown"},"source":{"091f5158":"import numpy as np \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport os\nimport scipy.stats as stats\nfrom copy import copy\nimport seaborn as sns\nfrom IPython.display import display\nimport missingno as msno\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d59a73f6":"train_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=0)\ntest_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=0)\nprint()\nmsno.matrix(train_df)\nprint(train_df.isnull().sum().sort_values(ascending=False)[:20])\n","ba49fda8":"sales_skew = train_df[\"SalePrice\"].skew()\nsales_kurt = train_df[\"SalePrice\"].kurt()\nprint(sales_skew,sales_kurt)\nsns.displot(train_df[\"SalePrice\"])","4702a369":"import matplotlib.pyplot as plt\nstats.probplot(train_df[\"SalePrice\"], dist=\"norm\", fit=True, rvalue=True, plot=plt)","0bccd58d":"quantitative = [f for f in train_df.columns if train_df.dtypes[f] != 'object']\nqualitative = [f for f in train_df.columns if train_df.dtypes[f] == 'object']\ndef encode(df, feature, target_feature):\n    \"\"\"\n    #     This function takes a dataframe, a feature(a categorical feature) and a target_feature(the feature that should be used for encoding)\n    #     and returns a new feature with the original feature name + postfix(_E). \n    #     This new feature consists of encoded value of unique original value but the values are weighted(incremented) based on the \n    #     mean of target_feature and grouped by the feature itself.\n    #     \"\"\"\n    ordering = pd.DataFrame()\n    ordering['val'] = df[feature].unique()\n    ordering.index = ordering.val\n    ordering['spmean'] = df[[feature, target_feature]].groupby(feature).mean()[target_feature]\n    ordering = ordering.sort_values('spmean')\n    ordering['ordering'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['ordering'].to_dict()\n\n    for cat, o in ordering.items():\n        df.loc[df[feature] == cat, feature+'_E'] = o\n\nqual_encoded = []\nfor q in qualitative:  \n    encode(train_df, q, 'SalePrice')\n    qual_encoded.append(q+'_E')\n    print(qual_encoded)\n","a8d8082d":"all_data = pd.concat((train_df, test_df)).reset_index(drop = True)\n## Dropping the target variable. \nall_data.drop(['SalePrice'], axis = 1, inplace = True)","9b7dd4e6":"# Correlation to each other\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrelation_mat = all_data.corr()\nmask = np.zeros_like(correlation_mat)\nmask[np.triu_indices_from(mask)] = True\nval_min = 0.5\nval_max = 0.99\nmy_cmap = copy(plt.cm.YlGnBu)\nmy_cmap.set_over(\"white\")\nmy_cmap.set_under(\"white\")\nf, ax = plt.subplots(figsize=(10, 8))\nax = sns.heatmap(correlation_mat,mask=mask, \n                 annot = False,\n                 cmap=my_cmap,\n                 linewidths=.001,\n                 linecolor='black',\n                vmin=val_min,\n                vmax=val_max,\n                )\nplt.show()","98b0bd58":"print(train_df.corr()['SalePrice'][:].sort_values(ascending=False))","bcfc5e4b":"from statsmodels.stats.diagnostic import het_white\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom pandas.api.types import is_numeric_dtype\ny = train_df[\"SalePrice\"]\nfor col in all_data.columns:\n    if is_numeric_dtype(all_data[col]):\n        try:\n            x = train_df[[col]].fillna(0)\n            x = sm.add_constant(x)\n            model = sm.OLS(y, x).fit()\n            white_test = het_white(model.resid,  model.model.exog)\n            labels = ['Test Statistic', 'Test Statistic p-value', 'F-Statistic', 'F-Test p-value']\n            \n            if float(white_test[1]) > 0.05:\n                print(f\"UNEQUAL VARIANCE for {col}\")\n                [print(labels[i],white_test[i]) for i in range(len(white_test))]\n                print('\\n\\n')\n        except Exception as e:\n            print(str(e))","160db0b3":"train_df.isna().sum().sort_values(ascending=False)[:50]","e2e4237b":"to_str = [\"OverallCond\", \"OverallQual\",\"MSSubClass\", \"YrSold\", \"MoSold\"]\nfor col in to_str:\n    #all_data[col] = all_data[col].astype(str)\n    pass\nfill_nan = [\"Functional\", \"Utilities\", \"Exterior1st\", \"Exterior2nd\", \"KitchenQual\", \"SaleType\", \"Electrical\"  ]\n\n\n    \n","646bac5f":"all_data['Functional'] = all_data['Functional'].fillna('Typ') \nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub') \nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(\"TA\") \nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(\"SBrkr\") ","ab11f25b":"all_data.isna().sum()","f6219d31":"from sklearn.preprocessing import RobustScaler\n\ntransformer = RobustScaler().fit(all_data)\ntransformer.transform(X)","5fc4a353":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df, train_df[\"SalePrice\"],test_size = .33, random_state = 0)\n","90246652":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","aff341ab":"#print(all_data)\ndfnew = all_data.select_dtypes(include=np.number)\nprint(dfnew.shape)\nprint(dfnew.isna().sum())","71fe7da0":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nlin_reg = LinearRegression()\ncv = KFold(shuffle=True, random_state=2, n_splits=10)\nscores = cross_val_score(lin_reg, train_df,train_df[\"SalePrice\"],cv = cv, scoring = 'neg_mean_absolute_error')","773296de":"# Anything above Z-score of 4 is removed\nfrom scipy import stats\ntrain_df = train_df.fillna(0)\nnumeric_df = train_df[NUMERIC_COLS]\n#print(numeric_df)\nz_scores = stats.zscore(numeric_df)\n#print(z_scores)\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 4).all(axis=1)\nrefined_df = numeric_df[filtered_entries]\n\nno_outliers = str(len(numeric_df)-len(refined_df))\nprint(f\"{no_outliers} outliers removed from data, with abs(Z-score) > 4\")\n#print(refined_df)","b60e0146":"non_num = train_df[[col for col in train_df.columns.tolist() if col not in NUMERIC_COLS]]\nprint(non_num.head())\n\ntrain_df = train_df[NUMERIC_COLS]\nfrom sklearn.preprocessing import LabelEncoder\n\nfor col in non_num.columns:\n    enc = LabelEncoder()\n    enc.fit(non_num[col].astype(str))\n    #train_df[col] = enc.transform(non_num[col].astype(str))\n    \n","1849adc0":"train_features = train_df.copy()\ntest_features = test_df.copy()\n\ntrain_labels = train_features.pop('SalePrice')\n#test_labels = test_features.pop('SalePrice')","a14fc8fc":"\n# Regression Example With Boston Dataset: Standardized\nfrom pandas import read_csv\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nX= train_features[[]\"OverallQual\"]\ny= train_labels\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state = 0)\nprint(X_train.shape)\n## importing necessary models.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n## Call in the LinearRegression object\nlin_reg = LinearRegression(normalize=True, n_jobs=-1)\n## fit train and test data. \nlin_reg.fit(X_train, y_train)\n## Predict test data. \ny_pred = lin_reg.predict(X_test)\nprint ('%.2f'%mean_squared_error(y_test, y_pred))","da658400":"\"\"\"def baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(1, input_dim=1, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n# evaluate model with standardized dataset\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=1)))\npipeline = Pipeline(estimators)\nkfold = KFold(n_splits=10)\nresults = cross_val_score(pipeline, X, Y, cv=kfold)\nprint(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\"\"\"","601058e8":"Strong multicollinearity:\n   - GarageCars to GarageArea\n   - GarageYrBlt to YearBuilt\n   - 1stFlrSF to TotalBsmtSF\n   ","1f3b0d02":"## Encoding categorical features","9768caef":"## Plotting feautres to SalesPrice","2db6f4ac":"## Imputing missing values","37094734":"## Training a simple regression model\n\nSource: https:\/\/www.tensorflow.org\/tutorials\/keras\/regression","7c12ae6e":"## Outliers","802b1e7f":"## Coding Non-Numeric variables","3e86ef78":"## Multicollinearity","2af23296":"## Correlation to SalePrice","15718112":"## Regression Assumptions","18e5096e":"## White's test of heteroscedasticity","9ce98e25":"## Handling Outliers"}}