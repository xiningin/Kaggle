{"cell_type":{"fb0b308e":"code","02d9b915":"code","ef782701":"code","1513a547":"code","55d2017b":"code","d2578c0f":"code","9f56e9b7":"code","b9d22d1c":"code","0f0fe09c":"code","0e69a9c6":"code","3e61388c":"code","9b201e08":"code","7e32a8c5":"code","2706a842":"code","f1115445":"code","40d1097e":"code","385161c8":"code","85a906fc":"code","87506044":"code","2a48fbfd":"code","3200914b":"code","05d9ad7f":"markdown","a4dd10e2":"markdown","3428c8de":"markdown","35e2be11":"markdown","756701fb":"markdown","a149b451":"markdown","00996160":"markdown","f235281e":"markdown","7c4b2000":"markdown","9af96a5f":"markdown","1004f517":"markdown","1a13d6e1":"markdown","c87963cf":"markdown","1431ac3e":"markdown","e7ff8c6c":"markdown","1f872f69":"markdown","5d7ab118":"markdown","4967a029":"markdown","34d1f345":"markdown"},"source":{"fb0b308e":"# Import the dependencies\nimport numpy as np\nfrom scipy.linalg import toeplitz, cholesky, sqrtm\nfrom scipy.linalg import inv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")\nprint(\"Import completed\")","02d9b915":"# Setting up the time data:\ndt = 0.05\nT = 5 + dt\nN = int(round(T\/dt))\nt = np.arange(0,T,dt)\nprint(N, \"data elements\")\nprint ('Starting with', t[0:5])\nprint ('Ending with', t[N-5:N])\nprint ('Data elements', np.size(t))","ef782701":"variance = 4 #input\n\n# randn generates an array of shape (in this case N), filled with random floats \n# sampled from a univariate \u201cnormal\u201d (Gaussian) distribution of mean 0 and variance 1 \n# so 68.3% of the samples are between -1 and + 1; \n# 95.5% of the samples between -2 and +2; \n# 98.7% of the samples between -3 and +3\n# Multiple with the standard deviation to get the right variance\nw = np.sqrt(variance) * np.random.randn(N)\n\n# or simply use the python numpy.random.normal function to draw random samples from a normal (Gaussian) distribution\n# w = np.random.normal(0,np.sqrt(variance),N)\n\n#Show the first 80 values for visual inspection\nprint(w[0:80])\n\n# Plot Gaussian white noise sequence to get the rough ragged noise due to the Identically distributed and statistically independent:\nplt.plot(t,w,label ='White noise')\nplt.title('White noise')\nplt.show","1513a547":"# Print the distribution plot to showcase the bell-curve \n# (The more values the better the curve becomes visible)\nMin_graph = min(w)\nMax_graph = max(w)\nx = np.linspace(Min_graph, Max_graph, N) # to plot the data\nsns.distplot(w, bins=20, kde=False)\nprint (\"The mean is: \", np.mean(w))","55d2017b":"#Show with the same random seed we get the same random sequence\nrandom_seed=1234\n\nnp.random.seed(random_seed)\nw1 = np.random.randn(N)\nplt.plot(t[0:80],w1[0:80],label ='White noise 1')\n\nnp.random.seed(random_seed)\nw2 = np.random.randn(N)\nplt.plot(t[0:80],w2[0:80],linestyle='dashed',label ='White noise 2')\n\nplt.title('White noise comparison')\nplt.show\n\n","d2578c0f":"#Let's compare both methods with a low and high correlation covariance matrix\nSw = np.matrix('9 2;2 16') # input covariance matrix\nSw_high = np.matrix('9 9;9 16') # input, high correlation\n# note: this matrix must be symmetric since it is a covariance matrix\n# note: on the diagonal you will find the variances\n\n# Generate white Gaussian noise sequences:\nn = Sw.shape[1] # dimension of noise\nnp.random.seed(random_seed)\n\nL =sqrtm(Sw)  #Sqrtm method\nL_cho =cholesky(Sw, lower=True)  #Cholesky method\n\nx=np.random.randn(n,N)\nw = np.dot(L,x)\nw_cho=np.dot(L_cho,x)\n\nnp.random.seed(random_seed) #same random seed to generate the same sequences\nw_high = np.dot(cholesky(Sw_high).T,np.random.randn(n,N))\nw_cho_high = np.dot(sqrtm(Sw_high).T,np.random.randn(n,N))\n\n# Plot the white noise sequence:\nfig, axes = plt.subplots(2,2)\nplt.subplots_adjust(wspace=0.35, hspace=0.35)\nfig.suptitle('Two dimensional correlated white noise')\naxes[0,0].plot(t,w[0,:],label='Gaussian white noise 1 low correlation')\naxes[0,0].set_xlabel('time [s]')\naxes[0,0].set_ylabel('Noise 1 L')\naxes[0,0].grid(1)\naxes[0,1].plot(t,w[1,:],label='Gaussian white noise 2 low correlation')\naxes[0,1].set_xlabel('time [s]')\naxes[0,1].set_ylabel('Noise 2 L')\naxes[0,1].grid(1)\naxes[1,0].plot(t,w_high[0,:],label='Gaussian white noise 1 high correlation')\naxes[1,0].set_xlabel('time [s]')\naxes[1,0].set_ylabel('Noise 1 H')\naxes[1,0].grid(1)\naxes[1,1].plot(t,w_high[1,:],label='Gaussian white noise 2 high correlation')\naxes[1,1].set_xlabel('time [s]')\naxes[1,1].set_ylabel('Noise 2 H')\naxes[1,1].grid(1)\n\n# Calculate the variance\/covariance of the generated data sets\nprint (\"Covariance matrix with low covariance as input for first sequence\")\nprint(Sw)\nprint (\"Covariance matrix with high covariance as input for second sequence\")\nprint(Sw_high)\nprint (\"Estimated variance\/covariance of first sequence (sqrtm method)\")\nprint(np.cov(w))\nprint (\"Estimated variance\/covariance of second sequence (sqrtm method)\")\nprint(np.cov(w_high))\nprint (\"Estimated variance\/covariance of first sequence (cholesky method)\")\nprint(np.cov(w_cho))\nprint (\"Estimated variance\/covariance of second sequence (cholesky method)\")\nprint(np.cov(w_cho_high))\nprint (\"The mean of the first sequence (sqrtm method): \", np.mean(w[0,:]))\nprint (\"The mean of the second sequence (sqrtm method): \", np.mean(w_high[0,:]))\nprint (\"The mean of the first sequence (cholesky method): \", np.mean(w_cho[0,:]))\nprint (\"The mean of the second sequence (cholesky method): \", np.mean(w_cho_high[0,:]))","9f56e9b7":"print('Square root matrix:')\nprint(L)\nprint('Cholesky decomposition:')\nprint(L_cho)\nprint('quick check both cholesky and sqrtm have the property L * L^T = Sw')\nprint(np.dot(L,L.T))\nprint(np.dot(L_cho,L_cho.T))","b9d22d1c":"sigma = 0.158 # in FEP sigma is usually 0.158 and << 1\n\n#Showcase the effect of sigma\nm=np.arange(-T,T,dt)\n\nsigma_1508=np.exp(-m**2\/(2*15.8**2))    #gamma=0.008\nsigma_5=np.exp(-m**2\/(2*5**2))          #gamma=0.08\nsigma_1058=np.exp(-m**2\/(2*1.58**2))    #gamma=0.8\nsigma_05=np.exp(-m**2\/(2*0.5**2))       #gamma=8\nsigma_0158=np.exp(-m**2\/(2*0.158**2))   #gamma=80\nsigma_005=np.exp(-m**2\/(2*0.05**2))    #gamma=800\n\n#rho_normalized=1.\/(np.sqrt(2*np.pi\/gamma))*np.exp(-gamma\/4*tau**2)\n\nplt.plot(m,sigma_1508,label ='$\\sigma=$ 15.8')\nplt.plot(m,sigma_5,label ='$\\sigma=$ 5')\nplt.plot(m,sigma_1058,label ='$\\sigma=$ 1.58')\nplt.plot(m,sigma_05,label ='$\\sigma=$ 0.5')\nplt.plot(m,sigma_0158,label ='$\\sigma=$ 0.158')\nplt.plot(m,sigma_005,label ='$\\sigma=$ 0.05')\n\nplt.legend(loc='upper left')\n","0f0fe09c":"# Let's calculate the convoluted noise in an old-fashioned for-next embedded loop \n# for visual inspection\/understanding what is happening\n# intuition, the top of the filter ho (h(0)) in the inner loop is centered on position p\nwn=w[1,:] #1 dimensional white noise example\nwc_trad = np.zeros(N) #initialize convoluted noise on zeros\nfor p in range(0, N):\n    sum=0\n    average=0\n    for h in range(-p, N-p):\n        delta = wn[p+h]*np.exp(-(h*dt)**2\/(2*sigma**2)) \n        sum = sum + delta\n        average = average + wn[p+h]\n        # Below some code for visual inspection of what is happening in the first 5 iterations\n        # Show the significant deltas\n        if abs(delta) > 0.05 and p<5:\n            print ('top=',p,'h=',h, 'delta=','{0:.5f}'.format(delta) ,\\\n                   '(','{0:.5f}'.format(wn[p+h]), '*' , \\\n                   '{0:.5f}'.format(np.exp(-(h*dt)**2\/(2*sigma**2))), ')', \\\n                   'sum=','{0:.6f}'.format(sum), 'plain sum=', '{0:.5f}'.format(average) )\n    next\n    wc_trad[p] = sum\n    if p<5:\n            print ('top=',p, 'total sum =', sum ) #visual inspection\n            print ('top=',p, 'plain sum =', average, 'Average=', average\/N ) #visual inspection \nnext\nplt.plot(t,wn,label='white noise')\nplt.plot(t,wc_trad,label='Convoluted noise')\nplt.axhline(y=0.0, color='red')\nplt.legend(loc='upper left')\nplt.title('$\\sigma=$ ' + str(sigma))\nplt.show","0e69a9c6":"# Scypy has a toeplitz function that we can use to calculate the convolution very handy\n# Below example shows what toeplitz calculates\n# See the kernel in the the kolomns shifting, by observing the top of the Gaussian (in this case 8)\n\nprint(toeplitz([8,4,2,1,1,0]))","3e61388c":"# Now we can calculate the convoluted noise with 2 lines of code (for all dimensions of w)\n# The noise graphs are identical\nP = toeplitz(np.exp(-t**2\/(2*sigma**2)))\nwc=np.dot(w,P) #Calculate the convoluted noise\n\n# Plot results\nplt.plot(t,w[1,:],label='white noise') \nplt.plot(t,wc[1,:],label='Convoluted noise') #Show one dimension\nplt.axhline(y=0.0, color='red')\nplt.title('$\\sigma=$ ' + str(sigma))\nplt.legend(loc='upper left')\nplt.show","9b201e08":"wc_1508=np.dot(w,toeplitz(np.exp(-t**2\/(2*15.8**2)))) #gamma=0.008\nwc_5=np.dot(w,toeplitz(np.exp(-t**2\/(2*5**2)))) #gamma=0.08\nwc_1058=np.dot(w,toeplitz(np.exp(-t**2\/(2*1.58**2)))) #gamma=0.8            \nwc_05=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.5**2)))) #gamma=8\nwc_0158=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.158**2)))) #gamma=80\nwc_005=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.05**2)))) #gamma=800\nwc_00158=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.0158**2)))) #gamma=8000              \n               \n#Example with a forced exact zero mean\nwn=w[1,:].copy() #1 dimensional white noise example\nprint ('White noise mean',np.mean(wn))\nprint ('White noise sum',np.sum(wn))\nwn-=np.mean(wn) # normalize to zero mean by substracting the mean\nprint ('White noise with forced 0 mean',np.mean(wn))\nprint ('White noise with forced 0 sum',np.sum(wn))\nwc_1508_zm=np.dot(wn,toeplitz(np.exp(-t**2\/(2*15.8**2))))\n\n# Plot results\nplt.plot(t,w[1,:],label='white') \nplt.plot(t,wc_1508[1,:],label='$\\sigma=$ 15.8') \nplt.plot(t,wc_5[1,:],label='$\\sigma=$ 5') \nplt.plot(t,wc_1058[1,:],label='$\\sigma=$ 1.58') \nplt.plot(t,wc_05[1,:],label='$\\sigma=$ 0.5')\nplt.plot(t,wc_0158[1,:],label='$\\sigma=$ 0.158')\n#plt.plot(t,wc_005[1,:],label='$\\sigma=$ 0.05') \nplt.plot(t,wc_00158[1,:],label='$\\sigma=$ 0.0158',linestyle='dashed') #Dashed to show it overlaps with white noise\nplt.plot(t,wc_1508_zm,label='$\\sigma=$ 15.8 forced') #Show random numbers with forced zero mean\n\nplt.title('gamma pallette')\nplt.legend(loc='best')\nplt.show","7e32a8c5":"#Show how the cumulative sum of all random numbers develops\n\nw_sum=w.copy()\nprint('Sum', np.sum(w[1,:]))\nfor j in range(1,N):\n    w_sum[:,j]=w_sum[:,j-1]+w[:,j]\nplt.plot(t,w[1,:],label='white') \nplt.plot(t,w_sum[1,:],label='sum') ","2706a842":"#And simular example some a larger series of 5M random numbers\nnp.random.seed(random_seed)\nw_lots =  np.random.randn(5000000)\nprint(np.mean(w_lots))\nprint(np.sum(w_lots))\nw_sum=w_lots.copy()\nfor j in range(1,5000000):\n    w_sum[j]=w_sum[j-1]+w_lots[j]\nplt.plot(w_lots,label='white') \nplt.plot(w_sum,label='sum') \n    ","f1115445":"# Calculate the variance\/covariance of the generated data sets\nprint (\"Covariance matrix as input for the white noise\")\nprint(Sw)\nprint (\"Estimated variance\/covariance of the white noise\")\nprint(np.cov(w))\nprint (\"Estimated variance\/covariance of the convoluted noise\")\nprint(np.cov(wc))\nprint (\"The mean of the white first sequence: \", np.mean(w[0,:]))\nprint (\"The mean of the convoluted first sequence: \", np.mean(wc[0,:]))\nprint (\"The mean of the white second sequence: \", np.mean(w[1,:]))\nprint (\"The mean of the convoloted second sequence: \", np.mean(wc[1,:]))","40d1097e":"#Show example\np=toeplitz([1,0.7,0.3,0.2,0.1,0])\n#p=toeplitz([9,4,2,1,0])\nprint('toeplitz P: ')\nprint(p)\np2=np.dot(p,p.T)\nprint('squared: ')\nprint(p2)\np3=np.diag(p2)\nprint('take diagonal: ')\nprint(p3)\np4=np.sqrt(p3)\nprint('take square root: ')\nprint(p4)\np5=1.\/p4\nprint('inverse ')\nprint(p5)\nf=np.diag(p5) #F\nprint('create matrix F: ')\nprint(f)\nprint(f.T)\nprint('Multiplying matrix P with F ')\np7=np.dot(p, f)\nprint(p7)\nprint('Verify that F.T dot P.T dot P dot F = I for the diagonal (identity matrix ) ')\np8=np.dot(f.T,p.T)\np9=np.dot(p8,p7)\nprint(p9)\n","385161c8":"# Make the smoothened noise:\nF = np.diag(1.\/np.sqrt(np.diag(np.dot(P.T,P))))\nws= np.dot(wc,F)\nplt.plot(t,w[1,:],label='white noise') \nplt.plot(t,wc[1,:],label='Convoluted noise')\nplt.plot(t,ws[1,:],label='noise with temporal smoothness')\nplt.title('$\\sigma=$ ' + str(sigma))\nplt.legend(loc='upper left')\nplt.show\n\n# Calculate the variance\/covariance of the generated data sets\nprint (\"Covariance matrix as input for the white noise\")\nprint(Sw)\nprint (\"Estimated variance\/covariance of the white noise\")\nprint(np.cov(w))\nprint (\"Estimated variance\/covariance of the normalized convoluted noise\")\nprint(np.cov(ws))\nprint (\"The mean of the white first sequence: \", np.mean(w[0,:]))\nprint (\"The mean of the normalized convoluted first sequence: \", np.mean(ws[0,:]))\nprint (\"The mean of the white second sequence: \", np.mean(w[1,:]))\nprint (\"The mean of the normalized convoloted second sequence: \", np.mean(ws[1,:]))\n","85a906fc":"print (\"Covariance matrix as input for the white noise\")\nprint(Sw)\nprint (\"Estimated covariance matrix of the white noise\")\nprint(np.cov(w))\n\n#wc_1508=np.dot(w,toeplitz(np.exp(-t**2\/(2*15.8**2)))) #gamma=0.008\n#wc_5=np.dot(w,toeplitz(np.exp(-t**2\/(2*5**2)))) #gamma=0.08\n#wc_1058=np.dot(w,toeplitz(np.exp(-t**2\/(2*1.58**2)))) #gamma=0.8            \n#wc_05=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.5**2)))) #gamma=8\n#wc_0158=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.158**2)))) #gamma=80\n#wc_005=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.05**2)))) #gamma=800\n#wc_00158=np.dot(w,toeplitz(np.exp(-t**2\/(2*0.0158**2)))) #gamma=8000              \n               \n\nP_1508 = toeplitz(np.exp(-t**2\/(2*15.8**2)))\nK_1508=np.dot(P_1508,np.diag(1.\/np.sqrt(np.diag(np.dot(P_1508.T,P_1508)))))\nws_1508= np.dot(w,K_1508)\nprint ('Sigma 15.8')\nprint('Mean:' ,np.mean(ws_1508[1,:]))\nprint(np.cov(ws_1508))\n\nP_5 = toeplitz(np.exp(-t**2\/(2*5**2)))\nK_5=np.dot(P_5,np.diag(1.\/np.sqrt(np.diag(np.dot(P_5.T,P_5)))))\nws_5= np.dot(w,K_5)\nprint ('Sigma 5')\nprint('Mean:' ,np.mean(ws_5[1,:]))\nprint(np.cov(ws_5))\n\nP_1058 = toeplitz(np.exp(-t**2\/(2*1.58**2)))\nK_1058=np.dot(P_1058,np.diag(1.\/np.sqrt(np.diag(np.dot(P_1058.T,P_1058)))))\nws_1058= np.dot(w,K_1058)\nprint ('Sigma 1.58')\nprint('Mean:' ,np.mean(ws_1058[1,:]))\nprint(np.cov(ws_1058))\n\nP_05 = toeplitz(np.exp(-t**2\/(2*0.5**2)))\nK_05=np.dot(P_05,np.diag(1.\/np.sqrt(np.diag(np.dot(P_05.T,P_05)))))\nws_05= np.dot(w,K_05)\nprint ('Sigma 0.5')\nprint('Mean:' ,np.mean(ws_05[1,:]))\nprint(np.cov(ws_05))\n\nP_0158 = toeplitz(np.exp(-t**2\/(2*0.158**2)))\nK_0158=np.dot(P_0158,np.diag(1.\/np.sqrt(np.diag(np.dot(P_0158.T,P_0158)))))\nws_0158= np.dot(w,K_0158)\nprint ('Sigma 0.158')\nprint('Mean:' ,np.mean(ws_0158[1,:]))\nprint(np.cov(ws_0158))\n\nP_005 = toeplitz(np.exp(-t**2\/(2*0.05**2)))\nK_005=np.dot(P_005,np.diag(1.\/np.sqrt(np.diag(np.dot(P_005.T,P_005)))))\nws_005= np.dot(w,K_005)\nprint ('Sigma 0.05')\nprint('Mean:' ,np.mean(ws_005[1,:]))\nprint(np.cov(ws_005))\n\nP_00158 = toeplitz(np.exp(-t**2\/(2*0.0158**2)))\nK_00158=np.dot(P_00158,np.diag(1.\/np.sqrt(np.diag(np.dot(P_00158.T,P_00158)))))\nws_00158= np.dot(w,K_00158)\nprint ('Sigma 0.0158')\nprint('Mean:' ,np.mean(ws_00158[1,:]))\nprint(np.cov(ws_00158))\n\n# Plot results\nplt.plot(t,w[1,:],label='white') \nplt.plot(t,ws_1508[1,:],label='S15.8') \nplt.plot(t,ws_5[1,:],label='S5') \nplt.plot(t,ws_1058[1,:],label='S1.58') \nplt.plot(t,ws_05[1,:],label='S0.5')\nplt.plot(t,ws_0158[1,:],label='S0.158')\nplt.plot(t,ws_005[1,:],label='S0.05') \nplt.plot(t,ws_00158[1,:],label='S0.0158',linestyle='dashed') #Dashed to show it overlaps with white noise\n\nplt.title('gamma pallette')\nplt.legend(loc='best')\nplt.show","87506044":"plt.acorr(ws[1,:], maxlags=50, label='smooth', color='red')\nplt.acorr(w[1,:], maxlags=50, label='white', color='blue')\nplt.title('autocorrelate')\nplt.legend(loc='best')\nplt.show","2a48fbfd":"np.random.seed(random_seed)\n# Setting up the time data:\ndt = 0.05\nT = 5+dt\nN = int(round(T\/dt))\nt = np.arange(0,T,dt)\n# Desired covariance matrix (noise in R\u02c62):\n# note: this matrix must be symmetric\nSw = np.matrix('9 2;2 16')\n# Generate white Gaussian noise sequences:\nn = Sw.shape[1] # dimension of noise\nL =sqrtm(Sw)  #Sqrtm method\n#L=cholesky(Sw, lower=True)  #Cholesky method\nw = np.dot(L,np.random.randn(n,N))\n# Plot the first white noise sequence:\nplt.plot(t,w.T[:,1],label='test')\n# Set up convolution matrix:\nsigma = 0.158\nP = toeplitz(np.exp(-t**2\/(2*sigma**2)))\nF = np.diag(1.\/np.sqrt(np.diag(np.dot(P.T,P))))\n# Make the smoothened noise:\nK = np.dot(P,F)\nws = np.dot(w,K)\nplt.plot(t,ws.T[:,1]) # some plot versions plot expect data in same dimension, hence the ws.T to align with w\nplt.title('$\\sigma=$ ' + str(sigma))\nplt.show","3200914b":"#Try traditional normalisation as a way to get correct zero mean and correct variance \n# Setting up the data:\ndt = 0.05\nT = 5 + dt\nN = int(round(T\/dt))\nt = np.arange(0,T,dt)\nnp.random.seed(random_seed)\nSw = np.matrix('9 2;2 16') \n#Sw = np.matrix('9 2 4;2 8 5; 4 5 24')\nn = Sw.shape[1] # dimension of noise\n#white noise\nL =sqrtm(Sw)  #Sqrtm method\nw = np.dot(L,np.random.randn(n,N))\n#convoluted noise\nsigma = 0.158\nP = toeplitz(np.exp(-t**2\/(2*sigma**2)))\nwc=np.dot(w,P) \n#Smoothened noise\nF = np.diag(1.\/np.sqrt(np.diag(np.dot(P.T,P))))\nws = np.dot(wc,F)\n\n#Alternative smoothened noise\nws_alt = wc.copy()\nfor j in range(0,n):\n    ws_alt[j,:] -= np.mean(ws_alt[j,:]) # normalize mean to 0\n    ws_alt[j,:] \/= np.std(ws_alt[j,:]) \/ np.sqrt(Sw[j,j]) #normalize variance to original covariance matrix\n\n#Plot the result\nplt.plot(t,w[1,:],label='white noise') \n#plt.plot(t,wc[1,:],label='Convoluted noise')\nplt.plot(t,ws[1,:],label='noise with temporal smoothness')\nplt.plot(t,ws_alt[1,:],label='noise with alternative temporal smoothness')\nplt.title('$\\sigma=$ ' + str(sigma))\nplt.legend(loc='upper left')\nplt.show\n\n# Calculate the variance\/covariance of the generated data sets\nprint (\"Covariance matrix as input for the white noise\")\nprint(Sw)\nprint (\"Estimated variance\/covariance of the white noise\")\nprint(np.cov(w))\nprint (\"Estimated variance\/covariance of the normalized convoluted noise\")\nprint(np.cov(ws))\nprint (\"Estimated variance\/covariance of the alternative normalized convoluted noise\")\nprint(np.cov(ws_alt))\nprint (\"The mean of the white first sequence: \", np.mean(w[0,:]))\nprint (\"The mean of the normalized convoluted first sequence: \", np.mean(ws[0,:]))\nprint (\"The mean of the alternative normalized convoluted first sequence: \", np.mean(ws_alt[0,:]))\nprint (\"The mean of the white second sequence: \", np.sum(w[1,:]))\nprint (\"The mean of the normalized convoloted second sequence: \", np.sum(ws[1,:]))\nprint (\"The mean of the alternative convoloted second sequence: \", np.sum(ws_alt[1,:]))\n","05d9ad7f":"## 1. Generate Gaussian White Noise\n\nSo, what is gaussian white noise?\n\n[Source WIKI](https:\/\/en.wikipedia.org\/wiki\/White_noise) \n\n### White noise\n \n+ White noise is a random signal having equal intensity at different frequencies, giving it a constant power spectral density.\n+ In discrete time, white noise is a discrete signal whose samples are regarded as a sequence of serially uncorrelated random variables with zero mean and finite variance; \n+ Identically distributed and statistically independent random variables are the simplest representation of white noise\n+ White noise can be produced by randomly choosing each sample independently\n\nEasy example is throwing a six sided dice 100 times which will give you a random sequence with a uniform distribution (1\/6 th each number), although it is not zero mean, but you get the point\n\n### Gaussian white noise\n+ Gaussian white noise is a random signal with a Gaussian intensity (normal) distribution. \n+ Gaussian refers to the probability distribution with respect to the value, in this context the probability of the signal falling within any particular range of amplitudes. While the term 'white' refers to the way the signal power is distributed (i.e., independently) over time or among frequencies.\n+ We can therefore find Gaussian white noise, but also Poisson, Cauchy, etc. white noises. Thus, the two words \"Gaussian\" and \"white\" are often both specified in mathematical models of systems. \n\nEasy example is throwing 2 six sided dice 100 times which will give you a random sequence but not with a uniform distribution, (e.g. 5\/36 number 6 and 1\/36 number 12)\n","a4dd10e2":"** Properties of a one-dimensional random walk with Gaussian steps **  \nThe expectation of a random walk is 0, thus the mean of all random numbers approaches zero as the number of steps increases. But the variance of a random walk does not converge with \\\\(Number of steps \\to \\infty\\\\),  the probabilities (proportional to the possible numbers) approaches a normal distribution.\n\nThus the sum of N random zero mean Gaussian numbers is not exactly zero. It is a probability distribution with mean of zero and variance in the order of N. Which makes sense: a random generator that is forced to \u00a8the sum of all random drawn numbers is always zero\u00a8 actually can only pick 0 every time and that is not random anymore.  \n\nIn other words, if we take a wide filter (high \\\\(\\sigma)\\\\) the y-position of the horizontal line is a probability distribution with mean of zero and variance increasing with N increasing (and not always close to 0)\n","3428c8de":"## 3. Normalize to the original variances\nVisible inspection of the convoluted noise shows immediately that the variance of the convoluted noise is not the same anymore as variance of the random noise as defined in the covariance matrix \\\\(\\Sigma _{w}\\\\).  Also the mean starts to shift away from 0.\n\nThe covariances will inevitably change because we introduced temporal correlation. We can however achieve normalization back to the original variances.\n\nLet's print the estimated covariance matrix and mean of the convoluted noise.\n","35e2be11":"How to normalize the sequence can be best seen by regarding the convolution as a matrix calculation:  \n\\\\(\\bar{w}=wP\\\\).  We are going to normalize the sequence by multiplying P by an by normalisation matrix F ( \\\\(\\bar{w}=wPF\\\\)) such that the variances of w will be equal to \\\\(\\bar{w}\\\\). \n\nIn the FEP it is achied by normalizing P with F where \n\n\\\\( F=diagonal\\left( \\frac{1}{\\sqrt{diag(P^TP)}} \\right ) \\\\)\n\nBelow example shows what it calculates so you can build the intuition for it.\n\nIn essence every i'th element of the diagonal of P is divided by the root of the i'th element of \\\\(P^TP\\\\), which is in essence the \u00a8integral\u00a8 of the filter in each column, such that the i'th element of the diagonal \\\\(F^TP^TPF\\\\) equals 1  . \n\nSince we want for \\\\(\\bar{w}\\\\) the same variance as w:  \nTODO: \n\n(Note that \\\\(F^T=F\\\\) since F only has the diagonal filled)\n\n","756701fb":"# Noise with temporal smoothness\nWelcome to my notebook on Kaggle. I did record my notes so it might help others in their journey to understand Active Inference minimizing the underlying [Free Energy](https:\/\/en.wikipedia.org\/wiki\/Free_energy_principle) by examples (in this case the generation of noise with temporal smoothness, which is some form of \u00a8colored noise\u00a8 in signal processing or what I would call \"Natural noise\"). This notebook is based on the many research papers of the Free Energy Principle (FEP) by Karl Friston and investigation done by Sherin Grimbergen, TU Delft. \n\n<img src=\"https:\/\/krisp.ai\/blog\/wp-content\/uploads\/2019\/04\/jason-rosewell-60014-unsplash-1080x720.jpg\" width=400>\nPhoto by [Jason Rosewell](https:\/\/unsplash.com\/@jasonrosewell) on [Unsplash](https:\/\/unsplash.com\/photos\/3VzJwKx6hGc)  \n\nArtificial Intelligence is not yet really intelligent, it is a (linear) regression technique and with a lot of data and a lot of compute power you can still deliver amazing results. Research into true biological inspired artificial intelligence continues and neuroscience has produced a candidate which suggests that several global brain theories might be unified within a free-energy framework: Free Energy Principle (**FEP**) by Karl Friston ([The genius neuroscientist who might hold the key to true AI](https:\/\/www.wired.com\/story\/karl-friston-free-energy-principle-artificial-intelligence\/))\n\nUnderstanding Active Inference and the FEP is not easy and has many intricate details (if you needed to google the word intricate it proves the point). This notebook is to help catalyze knowledge and research on Active Inference in an engineering\/robotics\/data sciences\/machine learning context.\n\nOne of these intricate details is the creation of noise with temporal smoothness, noise samples at consecutive times that are correlated. This notebook is to understand step-by-step the generation of this type of noise as done in the FEP. \n\n\n","a149b451":"### multi-dimensional noise example\n\nLet's generate an n-dimensional zero mean white gaussian noise \\\\(N(0,\\Sigma _{w} )\\\\) , with a certain covariance matrix \\\\(\\Sigma _{w}\\\\).\n\nInput parameter is the covariance matrix \\\\(\\Sigma _{w}\\\\) which defines the variances and covariances of the n dimensions. The covariance matrix is a generalization of the covariance of two variables and captures the way in which all variables in the dataset may change together.\n+ The diagonal of the covariance matrix are the variances of each of the random variables.\n+ The covariance matrix is a square and symmetric matrix  \n+ For a quick refresher on covariance matrixes: https:\/\/en.wikipedia.org\/wiki\/Covariance_matrix , https:\/\/machinelearningmastery.com\/introduction-to-expected-value-variance-and-covariance\/ or https:\/\/www.youtube.com\/watch?v=9B5vEVjH2Pk  \n\nThe Cholesky decomposition is commonly used in simulating systems like Monte Carlo to generate random sequences with multiple correlated variables (https:\/\/en.wikipedia.org\/wiki\/Cholesky_decomposition ). It is nicely explained in video: https:\/\/www.youtube.com\/watch?time_continue=852&v=QCqsJVS8p5A\n\nThis link gives a good explanation: https:\/\/math.stackexchange.com\/questions\/163470\/generating-correlated-random-numbers-why-does-cholesky-decomposition-work why the chlesky decomposition does the job, in essence:\n\\\\( covariance matrix =\\mathbb{E} \\left(ww^T\\right) = \\mathbb{E} \\left((Lx)(Lx)^T \\right) = \\underbrace{\\mathbb{E} \\left(Lx x^T L^T\\right) = L \\mathbb{E} \\left(xx^T \\right) L^T}_{\\text{ Since expectation is a linear operator}} = LIL^T = LL^T = covariance matrix\\\\)  \n\n\nExample for 2-dimensions:  \nThe covariance matrix is: \\\\(\\begin{pmatrix}\n1 &\\rho \\\\ \n\\rho &1 \n\\end{pmatrix}\\\\)  \nThe Cholesky decomposition: \\\\(\\begin{pmatrix}\n1 &0 \\\\ \n\\rho &\\sqrt{1-\\rho^2} \n\\end{pmatrix}.\\begin{pmatrix}\n1 &\\rho \\\\ \n0 &\\sqrt{1-\\rho^2} \n\\end{pmatrix}=\\begin{pmatrix}\n1 &\\rho \\\\ \n\\rho &1 \n\\end{pmatrix}\\\\)\n\nSo if we have 2 two uncorrelated Gaussian random variables \\\\(\\binom{x_{1}}{x_{2}}\\\\) of mean 0 and variance 1 then  \n\\\\(\\begin{pmatrix}\n1 &0 \\\\ \n\\rho &\\sqrt{1-\\rho^2} \n\\end{pmatrix}.\n\\begin{pmatrix}\nx_{1} \\\\ \nx_{2} \n\\end{pmatrix}=\n\\begin{pmatrix}\nx_{1} \\\\ \n\\rho x_{1}+\\sqrt{1-\\rho^2}x_{2} \n\\end{pmatrix}\n\\\\)   \ngenerates two correlated normal variables \\\\(\\binom{w_{1}}{w_{2}}\\\\) with given correlation coefficient  \\\\(\\rho \\\\)\n\n\nIn the active_inference library spm_DEM_z.m from the Wellcome Trust Centre for Neuroimaging, the generation of sequences makes use of the function spm_sqrtm which extends matlab sqrtm functionality by using a computationally expedient approximation. The matlab sqrtm function calculates the Matrix square root, where X = sqrtm(A) returns the principal square root of the matrix A, that is, X*X = A.  \nSo like the Cholesky decomposition, the Matrix square root function is used to generate sequences with multiple correlated variables.\n\n\n","00996160":"## 2. Convolution\nThe Free energy principle assumes that noise has some form of temporal smoothness (because random fluctuations originate from dynamical systems themselves). The noise samples are identically distributed, but not independent. This temporal smoothness is created by convoluting the random noise by a filter h(t). The Free Energy assumes a Gaussian filter.\n\nThe white gaussian noise will be convoluted with a filter: \\\\(h(t)=e^{-\\frac{t^2}{2\\sigma^2 }}\\\\) to generate convoluted noise. \n\nSo, what is convolution? For a quick refresher see [Convolution on wiki](https:\/\/en.wikipedia.org\/wiki\/Convolution) or watch this [video](https:\/\/www.youtube.com\/watch?v=N-zd-T17uiE). Below illustration gives you the intuition of convolution. Trying to describe it in words: For each point t on the f(t) curve the new value is calculated by overlaying f(t) with g(x) centered around t and calculate for each point g(x) times f(x). In other words g(t) shifts over f(t) and for each point it calculates the result by weighing the neighboring points on f(x) by g(x). \n![source WIKI](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6a\/Convolution_of_box_signal_with_itself2.gif)\n\nThe mathematical expression of convoluting our Gaussian white noise w with filter h:  \n\\\\(\\bar{w}(t)=\\int_{-\\infty}^{\\infty}w(t-\\tau )h(\\tau) \\delta \\tau\\\\)  \nwhere \\\\(\\bar{w}(t)\\\\) is the convoluted version of the noise w(t)\n\nIn discrete time used in simulations it becomes:  \n\\\\(\\bar{w}(t)=\\sum_{n=-\\infty}^{\\infty}w(t-n )h(n)\\\\)\n\n","f235281e":"### Why noise?  \n\n[Noise](https:\/\/www.merriam-webster.com\/dictionary\/noise) refers to any random fluctuations of data. Irrelevant or meaningless data or output occurring along with desired information. \nIn nature, noise and other inaccuracies are all around us and biological life, as well engineering algorithms (e.g. [Kalman filtering](https:\/\/en.wikipedia.org\/wiki\/Kalman_filter) ), need to function despite these fluctuations.  \nHaving a robotic system working reliable under uncertainty is a significant challenge. Image you want a robotic soccer player intercept a rolling ball, one of the hardest things to do. You have fluctuations in the omni-vision (camera system) to detect the ball, the wheels to drive for intercept, the ball itself is not rolling straight, and so on and so forth. And somehow as humans we can receive a ball without a second thought.\n\nNoise is modelled in the Free Energy Principle (similar as in [optimal control theory](https:\/\/en.wikipedia.org\/wiki\/Control_theory)) as fluctuations in the actual states and fluctuations in the measurement. Typically noise is expressed like in a  [Linear dynamical system](https:\/\/en.wikipedia.org\/wiki\/Linear_dynamical_system) where the state space is described as (basic form):\n\n\\\\(\\dot{x}= f(x,u) + w\\\\)  \n\\\\(y= g(x) + z\\\\)\n\nThe noise appears in the equations:  \n- z is noise in the measurement (random fluctuations of sensory states)\n- w noise in the actual environment (random fluctuations of the motion of hidden states)\n- x is the hidden state being estimated, y is the observation, u is the control signal, f and g are functions to describe the dynamic system.\n\n### Why noise with temporal smoothness? \nIn conventional control theory it is assumed that fluctuations are independent, a sequence of serially uncorrelated random variables with zero mean (infinitely rough). (e.g. Gaussian white noise). Random fluctuations are sufficiently fast that we do not observe their serial or temporal correlations.\n\nThis is less plausible for biological reality, random fluctuations originate from dynamical systems themselves (e.g. sound, waves). Therefore, these signals are continuous and not infinitely rough as is white noise for example. It is noise with some form of temporal smoothness.\n\nThis means higher order derivatives of the noise do contain information, as also the noise is differentiable with finite variance (in contrast to white noise that has infinite variance so there is no information in higher order derivatives). An agent can estimate these higher order motions internally (position, speed, acceleration, jerk, etc) using generalized motions, for example in the linear dynamical system example (under local linearity assumptions) :\n\n\\\\(\\dot{x}= f(x,u) + w\\\\)  \n\\\\(\\dot{x}'= f'(x,u)\\cdot{x}' + {w}'\\\\)  \n\\\\(\\dot{x}''= f'(x,u)\\cdot{x}'' + {w}''\\\\)  \n\\\\(\\dot{x}'''= f'(x,u)\\cdot{x}''' + {w}'''\\\\)  \netc\n\nThe promise is, because of being able to estimate these higher motions internally by the agent, that active inference could outperform eg kalman filtering. Something interesting to test in some subsequent notebooks. For now let us understand how FEP generates natural noise.\n\n\n","7c4b2000":"### Understanding the results\nThe results were somehow different as I expected, so needed to reconcile for myself to understand what is happening. Did some experiments and printed the intermediate results in the old-fashioned for-next embedded loop some code blocks above. Please play around for yourself to get the intuition what is happening. \nBelow my notes:\n\n** \\\\(\\sigma\\\\) = 0.158, why is the convoluted noise above the white noise spikes?  **  \nI was somehow expecting to see the line to be closer around zero, not having spikes even above the white noise, random numbers are drawn from a zero mean Gaussian, some + some - do they not balance out? Looking at the printouts of sample examples it became apparent why. See example below, by chance the first 5 numbers are positive and all contribute most to the convoluted result. Drawing random numbers from a zero mean gaussian does not mean you get nicely alternating positive and negative values.\n\ntop= 0 h= 0 delta= 2.395466 ( 2.395466 x 1.000000 ) sum= 2.395466 plain sum= 2.395466  \ntop= 0 h= 1 delta= 1.586399 ( 1.667736 x 0.951229 ) sum= 3.981865 plain sum= 4.063202  \ntop= 0 h= 2 delta= 1.268216 ( 1.549002 x 0.818731 ) sum= 5.250081 plain sum= 5.612204  \ntop= 0 h= 3 delta= 1.174851 ( 1.842534 x 0.637628 ) sum= 6.424932 plain sum= 7.454737  \ntop= 0 h= 4 delta= 2.351477 ( 5.233308 x 0.449329 ) sum= 8.776409 plain sum= 12.688045  \ntop= 0 h= 5 delta= -0.819987 ( -2.862038 x 0.286505 ) sum= 7.956421 plain sum= 9.826007 \n \n** \\\\(\\sigma\\\\) = 15,8, why is the convoluted noise not close to 0? **  \nMaybe if I take a wide filter (high gamma) the line will come close to zero because the average is almost zero? But it is not. Looking at the printouts it became apparent why. With \\\\(\\sigma \\to \\infty\\\\) the convoluted noise will be a straight line equal to the sum of all random values. And the sum of all the random numbers is not exactly zero. In example above the mean is -0.23594336400718183 , N=101 so the sum is -23.8 resulting in a straight line on -23.8 in the graph (mean = sum \/ N ). By zero mean normalizing the random numbers (and thus ensuring the sum is 0) the line is as exactly on zero as expected, see test graph above.\n\nNote that we are still looking at intermediate results because next step is the normalization of the data which will also impact the position of the line.\n\n** Random walk with Gaussian steps **  \nThe sum of the random numbers develops as a [random walk](https:\/\/en.wikipedia.org\/wiki\/Random_walk) with Gaussian steps. Let's see some examples.\n\n\n","9af96a5f":"### Filter h(t)\n\nIn the FEP, the white gaussian noise will be convoluted with an unnormalized Gaussian filter with standard deviation  \\\\(\\sigma\\\\) and zero mean.   \nSo the noise will be smoothened with filter \\\\(h(t)=e^{-\\frac{t^2}{2\\sigma^2 }}\\\\) where \\\\(\\sigma\\\\) defines the amount of roughness\/smoothness introduced in the noise:  \n* as \\\\(\\sigma\\\\) gets bigger the filter gets broader and increasingly more datapoints around the current sample will add weight to smoothen the signal. With \\\\(\\sigma \\to \\infty\\\\) the convoluted noise will be a straight line equal to the sum of all random values.\n* as \\\\(\\sigma\\\\) gets smaller the filter gets smaller and increasingly less datapoints around the current sample will add weight to smoothen the signal so the signals keeps its original roughness. With \\\\(\\sigma \\to 0\\\\) the convoluted noise will be the same as the white noise.\n\n\nNote: Friston is also sometimes referring to a roughness parameter \\\\(\\gamma\\\\) where \\\\(\\sigma^2 = \\frac{2}{\\gamma }\\\\), so the filter becomes \\\\(h(t)=e^{-\\frac{t^2}{2\\sigma^2 }}=e^{-\\frac{\\gamma}{4}t^2}\\\\)\n\n\n \n","1004f517":"### Understanding the results\n\nAs expected, With \\\\(\\sigma \\to 0\\\\) the convoluted noise will be the same as the white noise and therefor matching the covariances matrix. The other way around is also true, with \\\\(\\sigma \\to \\infty\\\\) the convoluted noise will be less matching the covariance matrix of random white noise (a straight line is not particular random).\nThe smaller the sigma the closer the match to the covariance matrix of the white noise but also less temporal smoothness. Interesting to see is that the normalization also pulls the graph closer to zero even with a wide filter.\n\nHence the recommendation in the FEP is for sigma to be << 1 (but don't make it too small else you dont have enough temporal smoothness). A sigma of 0.158 (corresponding to gamma 80) is often used.\n\n\n### Auto correlation\n\nBecause we introduced temporal correlations in the noise we should be able to see this in the autocorrelation of the normalized convoluted noise. (Auto correlation is the correlation of one time series data to the same time series data whith a time lag \\\\(\\tau\\\\) expressed on the horizontal axis in the graph, see this [video on auto correlation](https:\/\/www.youtube.com\/watch?v=ZjaBn93YPWo) for an introduction if needed). As expected white noise gives a low autocorrelation, while the normalized convoluted noise shows higher correlations.  \nNote that the filter \\\\(h(t)\\\\) is not equal to the auto correlation funtion  \\\\(\\rho(\\tau)\\\\). ","1a13d6e1":"### 1-dimensional example\nLet's generate a 1-dimensional white gaussian noise \\\\(N(0,\u03c3^2 )\\\\), a zero mean Gaussian with a certain variance.\nPython has standard functions to generate these random numbers, e.g. use the python numpy.random.normal function to draw random samples from a normal (Gaussian) distribution ","c87963cf":"And let's see the effect  of the various roughness parameters on the normalized convolution in the graph below.","1431ac3e":"Armed with this knowledge you should be in a position to better understand the following lines of Matlab code in the active_inference library spm_DEM_z.m from the Wellcome Trust Centre for Neuroimaging ([SPM](https:\/\/www.fil.ion.ucl.ac.uk\/spm\/))\n~~~\nK  = toeplitz(exp(-t.^2\/(2*s^2)));  \nK  = K*diag(1.\/sqrt(diag(K*K')));  \nz{i}  = spm_sqrtm(inv(P))*randn(M(i).l,N)*K; # note P = the precision Matrix, N length of data sequence \n~~~\nAnd the below Python code from Sherin Grimbergen TU Delft for noise with temporal smoothness generation\n","e7ff8c6c":"## How noise with temporal smoothness is generated in Active Inference\n\nThe following lines of Matlab code in the active_inference library spm_DEM_z.m from the Wellcome Trust Centre for Neuroimaging ([SPM](https:\/\/www.fil.ion.ucl.ac.uk\/spm\/)) generate the noise with temporal smoothness\n\n~~~\nK  = toeplitz(exp(-t.^2\/(2*s^2)));  \nK  = K*diag(1.\/sqrt(diag(K*K')));  \nz{i}  = spm_sqrtm(inv(P))*randn(M(i).l,N)*K; # note P = the precision Matrix, N length of data sequence  \n~~~\n\nFor me these code lines were not easy to understand, so have been digging in to understand how the noise is generated. In the notebook below my investigation. The noise with temporal smoothness is generated in main 3 steps:\n1. Generate Gaussian white Noise\n1. Convolution to create smoothness in the noises, i.e. noise samples at consecutive times that are correlated\n1. Normalize to the original variances  \n\nPlease copy this kernel and try our for yourself as well. If it helped you please upvote top right so other might find this kernel as well to help catalyze knowledge and research on Active Inference in an engineering\/data sciences\/machine learning context.","1f872f69":"Out of scope for the notebook but by understanding the FEP code some simplifications come to mind.  \nCould we avoid the last normalization step by taking a normalized Gaussian Filter?  \nCould we take an alternative approach in the last normalization step and apply a traditional normalization of the dataset, this will move\/stretch the convoluted noise line vertically to the values of the original covariance matrix and will keep a temporal smoothness?\nSomething that could be further investigated.","5d7ab118":"## Discussion\n","4967a029":"Let's see the effects of the various roughness parameters on the convolution in the graph below.\nNote that the first and last samples of the convoluted noise are not correct, there are no samples before t=0 and after t=T for the correct weighing. This could be solved with some extra padding with random numbers, if needed.","34d1f345":"### Repeatable random sequences\nIn experiments we need random data but we must also have the possibility to have the same random data to compare results. For example compare the efficiency of different algorithms under same circumstances.\n\nWe can make use of the fact that the Python\u2019s random module is not truly random, it is pseudo-random with a deterministic algorithm. It produces the numbers from an initial seed value. By reuse of the same seed value we can generate the same random sequences."}}