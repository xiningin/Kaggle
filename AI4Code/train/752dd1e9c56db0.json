{"cell_type":{"99f4d0c2":"code","7b4902fd":"code","70394da4":"code","02b47cb0":"code","7438c47a":"code","7c2eaec2":"code","3ff912e2":"code","14c13bf8":"code","24e36ba5":"code","418c69e0":"code","9083779c":"code","7c2bf80a":"code","0a2e0a77":"code","e4c6589a":"code","d7d1b82b":"code","5420e40e":"code","b7ae2d69":"code","fd41955e":"code","d7fde59b":"code","94862fb9":"code","bcef59df":"code","2314dab4":"code","e042e0eb":"markdown","683a4ab7":"markdown","2c250d8e":"markdown","e700c8f1":"markdown","1883e6bb":"markdown","dac2eaa1":"markdown","11046604":"markdown","6e6a6010":"markdown","d287265e":"markdown","f7ff3d3a":"markdown","d05babb4":"markdown","fa7d6298":"markdown","07d3a5c5":"markdown","cd321e27":"markdown"},"source":{"99f4d0c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b4902fd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom scipy.stats.stats import pearsonr\nfrom scipy.stats import norm\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve,train_test_split\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,MinMaxScaler, Normalizer, RobustScaler,LabelEncoder\nfrom collections import Counter\nwarnings.filterwarnings('ignore')\nsns.set(style='white', context='notebook', palette='deep')","70394da4":"#importing data\ntrain = pd.read_csv(\"\/kaggle\/input\/new_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/new_test.csv\")\ntest.head()","02b47cb0":"train.head()","7438c47a":"# creating new variable id as we needed in final output file\nid = test[\"id\"].values\ny_train_1 = train[\"sentiment_class\"]\n#train.drop([\"sentiment_class\"], axis=1,inplace=True)#droping target data from file\ntrain.head()","7c2eaec2":"#now combining train and test\ndataset = pd.concat([train,test],sort=False,ignore_index=True)\ntemp_df = pd.concat([train,test],sort=False,ignore_index=True)\ndataset.drop(\"id\",axis=1,inplace=True)#dropping id from final data\ndataset.info()","3ff912e2":"print(dataset[\"lang\"].isnull().value_counts())","14c13bf8":"print(y_train_1)","24e36ba5":"from wordcloud import WordCloud,STOPWORDS\ndf=train[train['sentiment_class']==0]# do analy for 1 and -1 too\nwords = ' '.join(df['original_text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","418c69e0":"#we are here cleaning the tweets like removing stopwords,and other stuffs so we get only letters\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\ndef tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) \ndef clean_tweet_length(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return(len(meaningful_words)) \ndataset.drop(\"sentiment_class\",axis=1,inplace=True)\ndataset['clean_tweet']=dataset['original_text'].apply(lambda x: tweet_to_words(x))\ndataset['Tweet_length']=dataset['original_text'].apply(lambda x: clean_tweet_length(x))\ndataset.drop(\"original_text\",axis=1,inplace=True)\ndataset[\"lang\"] = dataset[\"lang\"].fillna(\"en\")#filling some values\ndataset.head()","9083779c":"#for transforming clean_tweet column as it contain sentances or words or string\nfrom sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(dataset['clean_tweet'])\ndataset.drop(\"clean_tweet\",axis=1,inplace=True)\nfeatures = train_features.toarray()\nfeatures = pd.DataFrame(features)\ndataset = dataset.join(features)","7c2bf80a":"#label encoding on lang and author columns\nle = LabelEncoder()\ndataset['lang']=le.fit_transform(dataset[\"lang\"].values)\ndataset['original_author']=le.fit_transform(dataset[\"original_author\"].values)\ndataset.head()","0a2e0a77":"# to encode categorical data eg. lang and author columns\nenc = OneHotEncoder(handle_unknown='ignore')\nenc_df = pd.DataFrame(enc.fit_transform(dataset[['lang']]).toarray())\nenc_df_1 = pd.DataFrame(enc.fit_transform(dataset[['original_author']]).toarray())","e4c6589a":"#now concating both encoded data to main data\ndataset = pd.concat([dataset,enc_df],axis=1,sort=False,ignore_index=True)\ndataset = pd.concat([dataset,enc_df_1],axis=1,sort=False,ignore_index=True)","d7d1b82b":"from sklearn.model_selection import cross_val_score,GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier,StackingClassifier,RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import f1_score","5420e40e":"print(dataset.shape)\ndataset.head()","b7ae2d69":"# we have so many columns(22272) so reduce them in 10 columns\npca  = PCA(n_components=10, random_state=1)\ndf = pca.fit_transform(dataset)\ndf = pd.DataFrame(data=df, columns=[\"compenent_1\",\"compenent_1\",\"compenent_1\",\"compenent_1\",\n                                    \"compenent_1\",\"compenent_1\",\"compenent_1\",\"compenent_1\",\n                                     \"compenent_1\",\"compenent_1\"])","fd41955e":"#splitting train and test data\nX_train = dataset[:len(train)]\ntest = dataset[len(train):]","d7fde59b":"#apply train test split to test model performence only donot do in final prediction\nT_train,T_test,y_train,y_test = train_test_split(X_train,y_train_1, random_state=1)","94862fb9":"lr = LogisticRegression(C=1)\nxgb = XGBClassifier( n_estimators = 120 )              \ngboost = GradientBoostingClassifier(learning_rate = 0.01, max_depth = 4, n_estimators = 100)\nbayes = GaussianNB()\nrfc = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=12, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)","bcef59df":"#u can use grid search to find parameters i already used that\n\nxgb.fit(T_train,y_train)\ny_pred = xgb.predict(T_test)\nprint(xgb.score(T_train,y_train))\nprint(100*f1_score(y_test, y_pred,average='weighted'))","2314dab4":"output = pd.DataFrame({\"id\":id,\"sentiment_class\":y_pred})\noutput.to_csv(\"submission_1.csv\",index=False)","e042e0eb":"# applying CountVectorizer","683a4ab7":"# one hot encoder","2c250d8e":"# label encoder","e700c8f1":"# pretty good score.... by this score u can be in top 25 on hackerearth","1883e6bb":"no null values so now we can perform our next step","dac2eaa1":"**i have done some data analysing process that i didnt mention here\ni introduces few new datasets from original data**\n1)introduces new language nl\n2) new author as naren\n3) new tweet count as -1\ni have done all that after analysing the data and removing unnecessary items\nthis is final dataset lets begin","11046604":"> both values are numerical now","6e6a6010":"# PCA - dimentionality Reduction","d287265e":"# models","f7ff3d3a":"# preparing Model","d05babb4":"# DATA CLEANING(original_text)","fa7d6298":"# output","07d3a5c5":"**so target is removed**","cd321e27":"# Data Analysing"}}