{"cell_type":{"ab8a81f2":"code","8e05c34b":"code","bbdf4a8e":"code","30c269c3":"code","12c9dfe8":"code","890c4f77":"code","5cb33172":"code","d0e612c2":"code","4c6cb9b6":"code","09c8e635":"code","a60ea562":"code","ad5f449b":"code","24a7f66e":"code","b06530cd":"code","e42a9bc1":"code","3d6366c4":"code","d7d11d6b":"code","b0eba8e0":"code","8c8c8f04":"code","501b46ad":"code","3da29da8":"code","ccc4c47c":"markdown"},"source":{"ab8a81f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e05c34b":"!pip install git+https:\/\/github.com\/qubvel\/efficientnet","bbdf4a8e":"# LOAD LIBRARIES\nimport time\nstartNB = time.time()\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, AvgPool2D, MaxPool2D , Flatten , Dropout, BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport efficientnet.tfkeras as efn\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import recall_score, accuracy_score, precision_recall_fscore_support, classification_report,confusion_matrix, f1_score\nfrom sklearn.utils import class_weight\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport cv2\nimport os\n\nprint(tf.__version__)\n\nprint('TensorFlow version =',tf.__version__)","30c269c3":"# VERSION MAJOR and MINOR for logging\nmm = 1; rr = 1\n\n# Default batch size can be changed later\nSEED = 42\nBATCH_SIZE = 32\nDIM = 128\nimg_size = DIM\nLR = 1e-3\nDECAY = 0.75\n\n# BEGIN LOG FILE\nf = open(f'log-{mm}-{rr}.txt','a')\nprint('Logging to \"log-%i-%i.txt\"'%(mm,rr))\nf.write('TensorFlow version ={tf.__version__}')\nf.write('#############################\\n')\nf.write(f'Trial mm={mm}, rr={rr}\\n')\nf.write('efNetB5, batch_size='+str(BATCH_SIZE)+', seed='+str(SEED)+', '+str(DIM)+'x'+str(DIM)+', fold=0, LR '+str(1e-3)+' with '+str(0.75)+' decay\\n')\nf.write('#############################\\n')\nf.close()","12c9dfe8":"# Let's create a function that will import and label the image set\nlabels = [\"jute\", \"maize\", \"sugarcane\", \"wheat\", \"rice\"]\n\ndef get_data(data_dir):\n    data = [] \n    path = os.path.join('\/kaggle\/input\/', data_dir)\n    for label in labels:\n        path_label = os.path.join(path, label)\n        for img in os.listdir(path_label):\n            try:\n                img_arr = cv2.imread(os.path.join(path_label, img), cv2.IMREAD_COLOR)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n                # Images are charged as BGR we switch channels to RGB\n                RGB_arr = resized_arr[:,:,[2,1,0]]\n                data.append([RGB_arr, labels.index(label)])\n            except Exception as e:\n                print(img, e)                    \n    return np.array(data)\n\ndef get_extra_data(data_dir):\n    \"\"\"\n    Serves for the extra data set of 8 images\n    \"\"\"\n    data = [] \n    path_label = os.path.join('\/kaggle\/input\/', data_dir)\n    for img in os.listdir(path_label):\n        try:\n            img_arr = cv2.imread(os.path.join(path_label, img), cv2.IMREAD_COLOR)\n            resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n            # Images are charged as BGR we switch channels to RGB\n            RGB_arr = resized_arr[:,:,[2,1,0]]\n            for label in labels:\n                if label in img:\n                    # print(img, label)\n                    data.append([RGB_arr, labels.index(label)])\n        except Exception as e:\n            print(img, e)                    \n    return np.array(data)","890c4f77":"train_data = get_data('agriculture-crop-images\/kag2')\ntest_extra_data =  get_extra_data('testssss\/test_crop_image')","5cb33172":"x_data = []\ny_data = []\n\nx_test_extra = []\ny_test_extra = []\n\nfor feature, label in train_data:\n    x_data.append(feature)\n    y_data.append(label)\n    \nfor feature, label in test_extra_data: \n    x_test_extra.append(feature)\n    y_test_extra.append(label)","d0e612c2":"sns.set_style('darkgrid')\nsns.countplot(y_data).set_title('Train data')","4c6cb9b6":"sns.set_style('darkgrid')\nsns.countplot(y_test_extra).set_title('Test data')","09c8e635":"X_train = np.array(x_data).reshape(-1, img_size, img_size, 3)\nX_test_extra = np.array(x_test_extra).reshape(-1, img_size, img_size, 3)\n\n# We convert numerical to one hot encoding\ny_train = np.array(tf.keras.utils.to_categorical(y_data, num_classes=5))\ny_test_extra = np.array(tf.keras.utils.to_categorical(y_test_extra, num_classes=5))\n\nprint(X_train.shape, y_train.shape, X_test_extra.shape, y_test_extra.shape)","a60ea562":"def build_model():\n    \n    # We input the images we have reshaped in 3 channels (RGB)\n    inp = tf.keras.Input(shape=(DIM,DIM,3))\n    # We use the pretrained weights but not the top\n    base_model = efn.EfficientNetB5(weights='imagenet',include_top=False, input_shape=(DIM,DIM,3))\n\n    x = base_model(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    # use a strong droptout because we have few input images\n    x = tf.keras.layers.Dropout(0.5)(x)\n    # predict for 5 classes\n    x = tf.keras.layers.Dense(5, activation='softmax',name='x1',dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=inp, outputs=x)\n    opt = tf.keras.optimizers.Adam(lr=0.001)\n    model.compile(loss='categorical_crossentropy', optimizer = opt,\\\n              metrics=['categorical_accuracy'])\n        \n    return model","ad5f449b":"# CUSTOM LEARNING SCHEUDLE\nLR_START = 1e-5\nLR_MAX = 1e-3\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_STEP_DECAY = 0.75\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)\/\/10)\n    return lr\n    \nlr2 = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nrng = [i for i in range(100)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y); \nplt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\nplt.title('Training Schedule',size=16); plt.show()","24a7f66e":"train_datagen = ImageDataGenerator(\n    rotation_range=30,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.1,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)\n\n# Test gen is useless in this case\n# test_datagen = ImageDataGenerator()","b06530cd":"class CustomCallback(tf.keras.callbacks.Callback):\n    def __init__(self, valid_data, target, fold, mm=0, rr=0, patience=10):\n        self.valid_inputs = valid_data\n        self.valid_outputs = target\n        self.fold = fold\n        self.patience = patience\n        self.mm = mm\n        self.rr = rr\n        \n    def on_train_begin(self, logs={}):\n        self.valid_f1 = [0]\n        \n    def on_epoch_end(self, epoch, logs={}):\n        # At the end of the epoch we predict the classes on validation\n        preds = self.model.predict(self.valid_inputs)\n        # we transform vector prediction into numerical class\n        preds = np.argmax(preds,axis=1)\n\n        # Calculate metrics\n        p, r, f1score, support = precision_recall_fscore_support(self.valid_outputs,preds,average='macro')\n        a = accuracy_score(self.valid_outputs,preds)\n\n        # LOG TO FILE\n        f = open('log-%i-%i.txt'%(self.mm,self.rr),'a')\n        f.write('#'*25); f.write('\\n')\n        f.write('#### FOLD %i EPOCH %i\\n'%(self.fold+1,epoch+1))\n        f.write('#### PRECISION: p=%.5f' % (p) )\n        f.write('#### RECALL: r=%.5f' % (r) )\n        f.write('#### F1SCORE: f1=%.5f' % (f1score) )\n        f.write('#### ACCURACY: a1=%.5f\\n' % (a) )\n\n\n        print('\\n'); print('#'*25)\n        print('#### FOLD %i EPOCH %i'%(self.fold+1,epoch+1))\n        print('#### PRECISION: p=%.5f' % (p) )\n        print('#### RECALL: r=%.5f' % (r) )\n        print('#### F1SCORE: f1=%.5f' % (f1score) )\n        print('#### ACCURACY: a1=%.5f' % (a) )\n        print('#'*25)\n        \n        # Stop training after multiple epochs if validation f1 score is not improving\n        self.valid_f1.append(f1score)\n        x = np.asarray(self.valid_f1)\n        if np.argsort(-x)[0]==(len(x)-self.patience-1):\n            print('#### F1 no increase for %i epochs: EARLY STOPPING' % self.patience)\n            f.write('#### F1 no increase for %i epochs: EARLY STOPPING\\n' % self.patience)\n            self.model.stop_training = True\n            \n        if (f1score>0.000)&(f1score>np.max(self.valid_f1[:-1])):\n            print('#### Saving new best...')\n            f.write('#### Saving new best...\\n')\n            self.model.save_weights('fold%i-m%i-%i.h5' % (self.fold,self.mm,self.rr))\n            \n        f.close()","e42a9bc1":"# Predictions will be saved for future uses\noo = np.zeros((X_train.shape[0],5))\n\n# We will get 5 split of our data which could give 5 attempts at training\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n\n# We retransform y with argmax because split method can't handle onehotencoding\nfor fold,(idxT,idxV) in enumerate(skf.split(X_train,np.argmax(y_train, axis=1))):\n         \n    print('#'*25)\n    print('### FOLD %i' % (fold+1))\n    print('### train on %i images. validate on %i images'%(len(idxT),len(idxV)))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n    \n    # We identify the training flow wwith the IDs given by split method\n    # Shuffle is very important \n    train_flow = train_datagen.flow(\n        x=X_train[idxT],\n        y=y_train[idxT],\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        sample_weight=None,\n        seed=SEED\n    )\n    \n    # CustomCallback will allow us to save best model weights\n    cc = CustomCallback(valid_data=X_train[idxV], target=np.argmax(y_train[idxV], axis=1), fold=fold, mm=mm, rr=rr, patience=15)\n    \n    h = model.fit(train_flow, epochs = 20, verbose=1, callbacks=[cc, lr2])\n\n    print('#### Loading best weights...')\n    model.load_weights('fold%i-m%i-%i.h5' % (fold,mm,rr))\n    \n    oo = model.predict(X_train)\n\n    # SAVE OOF and IDXV\n    np.save('oo-%i-%i'%(mm,rr),oo)\n    np.save('idxV-%i-%i'%(mm,rr),idxV)\n    np.save('Y_train-%i-%i'%(mm,rr),y_train)\n    # we will limit ourself to one fold\n    break","3d6366c4":"def display_stats_and_confusion_matrix(y_pred, y, names=labels):\n    \"\"\"\n    Print stats and display confustion matrix\n    y must be provided as numerical values not one hot\n    \"\"\"\n    cm = confusion_matrix(y,y_pred)\n    cm = pd.DataFrame(cm , index = names , columns = names)\n    cm.index.name = 'Label'\n    cm.columns.name = 'Predicted'\n\n    precision, recall, fscore, support = precision_recall_fscore_support(y_pred, y, average=None)\n    print(\"#########################\")\n    for p,l in zip(precision, names):\n        print(\"#### Precision for %s %.2f\" % (l, p))\n    print(\"#########################\")        \n    for r,l in zip(recall, names):\n        print(\"#### Recall for %s %.2f\" % (l, r))\n    print(\"#########################\")        \n    for f,l in zip(fscore, names):\n        print(\"#### F1Score for %s %.2f\" % (l, f))\n    print(\"#########################\")\n\n    group_counts = [\"{0:0.0f}\".format(value) for value in cm.to_numpy().flatten()]\n    # Percentage are normalized so as to interpret read values\n    group_percentages = [\"{0:.2%}\".format(value) for value in cm.to_numpy().flatten()\/np.sum(cm.to_numpy())]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(5,5)\n\n    plt.figure(figsize = (10,10))\n    sns.heatmap(cm,\n                annot=labels,\n                cmap= \"coolwarm\",\n                linecolor = 'black',\n                linewidth = 1,\n                fmt='')   ","d7d11d6b":"y_pred_num = np.argmax(model.predict(X_train[idxV]),axis=1)\ny_num = np.argmax(y_train[idxV],axis=1)\ndisplay_stats_and_confusion_matrix(y_pred_num,y_num)","b0eba8e0":"y_pred_extra_num = np.argmax(model.predict(X_test_extra), axis=1)\ny_extra_num = np.argmax(y_test_extra,axis=1)\ndisplay_stats_and_confusion_matrix(y_pred_extra_num,y_extra_num)","8c8c8f04":"df = pd.DataFrame({'label':y_extra_num, 'prediction':y_pred_extra_num, 'img': X_test_extra.reshape(len(y_extra_num),-1).tolist()})","501b46ad":"print(\"### Correctly classifed images\")\nplt.figure(figsize=(20,20))\ni_ = 0\n\ncorrectImages = df[df['label'] == df['prediction']]\nfor index, row in correctImages.iterrows():\n    im = np.array(row['img']).reshape(DIM,DIM,3)\n    actual_label = labels[row['label']]\n    predicted = labels[row['prediction']]\n    plt.subplot(5, 5, i_+1).title.set_text(\"Label: %s \" % (predicted))\n    plt.imshow(im)\n    plt.axis('off')\n    i_ += 1\n    if index >= 25:\n        break","3da29da8":"print(\"### Misclassified images\")\nplt.figure(figsize=(20,20))\ni_ = 0\n\nmultipleImages = df[df['label'] != df['prediction']]\nfor index, row in multipleImages.iterrows():\n    im = np.array(row['img']).reshape(DIM,DIM,3)\n    actual_label = labels[row['label']]\n    predicted = labels[row['prediction']]\n    plt.subplot(5, 5, i_+1).title.set_text(\"Label: %s Predicted: %s\"%(actual_label, predicted))\n    plt.imshow(im)\n    plt.axis('off')\n    i_ += 1\n    if index >= 25:\n        break","ccc4c47c":"****Data Loading****[](http:\/\/)"}}