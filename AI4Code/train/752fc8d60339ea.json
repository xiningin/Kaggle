{"cell_type":{"23bb346b":"code","e721708a":"code","c9691742":"code","9677299b":"code","6ce5178b":"code","06d3a333":"code","e386bac9":"code","e3896231":"code","e18ad3b4":"code","46de6ecc":"code","9a4769cb":"code","7c4d62e9":"code","dd578c07":"code","81b609fa":"code","73e376c5":"code","01803fda":"code","c13956ef":"code","5295a719":"code","03f641b4":"code","63afc121":"markdown","91ecd07c":"markdown","a90f1bb8":"markdown","c9151153":"markdown","4fc452b8":"markdown","cce91481":"markdown","6f4a2a39":"markdown","8c4d32e8":"markdown","f8382f22":"markdown","192b817a":"markdown","d79b1ef9":"markdown","80d7a49b":"markdown","2129eaf2":"markdown","8039151e":"markdown","8d260a9e":"markdown","38c1dafa":"markdown","169ee18c":"markdown"},"source":{"23bb346b":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud\nfrom mlxtend.preprocessing import TransactionEncoder\nfrom mlxtend.frequent_patterns import apriori, association_rules, fpgrowth \nfrom PIL import Image\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","e721708a":"dataset = pd.read_csv('..\/input\/market-basket-optimization\/Market_Basket_Optimisation.csv', header = None)\ndataset.head()","c9691742":"dataset.shape","9677299b":"dataset.describe()","6ce5178b":"mask = np.array(Image.open('..\/input\/hamb1png\/Hamb1.png'))","06d3a333":"plt.subplots(figsize=(12,8))\nwordcloud = WordCloud(\n                          background_color='White',max_words = 60,\n                          mask = mask, contour_color='orange', contour_width=4, \n                          width=1500, margin=10,\n                          height=1080\n                         ).generate(\" \".join(dataset[0]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","e386bac9":"Products = pd.DataFrame(dataset[0].value_counts())\nTwenty_Products = pd.DataFrame(dataset[0].value_counts()).head(20)\n\nsns.barplot(x = Twenty_Products.index, y = Twenty_Products[0])\n\nlabels =Twenty_Products.index.tolist()\nplt.gcf().set_size_inches(15, 7)\n\nplt.title('20 most popular products vs. amount', fontsize = 20)\nplt.xlabel('Most popular products', fontsize = 15)\nplt.ylabel('Amount', fontsize = 15)\n\nplt.xticks(ticks = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19] , labels = labels, rotation = '45')\nplt.show()","e3896231":"Twenty_Products.columns = ['Amount']\nTwenty_Products.head(10).style.background_gradient(cmap='plasma')","e18ad3b4":"Twenty_Products[\"Item\"] = Twenty_Products.index\nTwenty_Products[\"20 most popular items\"] = \"20 most popular items\"\nTwenty_Products['index'] = list(range(len(Twenty_Products)))\nTwenty_Products.set_index('index')\nTwenty_Products = Twenty_Products[['Item','Amount',\"20 most popular items\"]]","46de6ecc":"fig = px.treemap(Twenty_Products, path=[\"20 most popular items\", 'Item'], values='Amount',\n                  color=Twenty_Products[\"Amount\"], hover_data=['Item'],\n                  color_continuous_scale='plasma',\n                  )\nfig.show()","9a4769cb":"tickets = []\nfor i in range(dataset.shape[0]):\n    tickets.append([str(dataset.values[i,j]) for j in range(dataset.shape[1])])\n    \ntickets = np.array(tickets)","7c4d62e9":"TE = TransactionEncoder()\ndataset2 = TE.fit_transform(tickets)\ndataset2 = pd.DataFrame(dataset2, columns = TE.columns_)","dd578c07":"dataset2.head(3)","81b609fa":"dataset_20 = dataset2.copy()\ndataset_20 = dataset_20[Twenty_Products[\"Item\"]] # Using the previous DF.","73e376c5":"def encode_units(x):\n    if x == False:\n        return 0 \n    if x == True:\n        return 1","01803fda":"dataset_20 = dataset_20.applymap(encode_units) # Element wise function in DF.\ndataset_20.head(3)","c13956ef":"frequent_itemsets = apriori(dataset_20, min_support = 0.01, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","5295a719":"frequent_itemsets[ (frequent_itemsets['length'] >= 2) &\n                   (frequent_itemsets['support'] >= 0.04) ]","03f641b4":"frequent_itemsets[ (frequent_itemsets['length'] == 3)].head(3)","63afc121":"**Loading and getting to know the dataset**","91ecd07c":"**Visualizations**","a90f1bb8":"The dataset has 7501 rows, that represent each customer in a store. I'll do some visualizations to have a better idea of what are the most popular products.\n\nWe have to take into account that we don't know the time of the year or the hour of this transactions. It is expected that in summer people tend to buy more water and fruit for example.","c9151153":"I'll define as 'tickets' each row of the dataset, so I'll generate an array with every ticket.","4fc452b8":"What about a set of 3 items?","cce91481":"## Association Rules","6f4a2a39":"We will keep these 5 itemsets.","8c4d32e8":"## Apriori","f8382f22":"The objective of this kernel is to do an association rule algorithms Apriori to find correlations between different items. With associacion rule learning, recommendation systems can be made as well as strategies of product placement in stores.\n\nThe algorithm is bases on a simple statement: 'If someone does\/buys\/watches this, that person might do\/buy\/watch this other thing'","192b817a":"Apriori uses three principal concepts:\n- Support: probability of some action\/item in the dataset.\n- Confidence: How sure am I that two things are happening consequently?\n- Lift: Confidence \/ Support. It's a measure to illustrate how likely is the second action to happen given the first action. \n\nIn this case I'll work using only the support score.","d79b1ef9":"As I'm interested in itemset with 2 or more items, I'll filter this.\n\nAlso, I'd like highest support scores.\n\n**We could increase the support score to filter more and more itemsets, but if I want to come up with a comercial strategy I'll try to offer some discounts on itemsets**","80d7a49b":"**Importing packages and modules**","2129eaf2":"I'll change the False to 0 , True to 1","8039151e":"**Model**","8d260a9e":"Now I have a dataframe with every single product and True or False values wheter they were or not bought in that ticket.\n\nI'll use only the 20 most popular products.","38c1dafa":"The analysis will be made on the first column, as it has the most number of rows.","169ee18c":"Thanks for reaching the end!! Upvote if you liked it!"}}