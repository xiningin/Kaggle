{"cell_type":{"df869021":"code","57113b3b":"code","5cefa88f":"code","46ae4ade":"code","4549828c":"code","b57680cc":"code","343297cd":"code","8c698f4d":"code","b0569e24":"code","f4fae725":"code","b8a41ae0":"code","f5f63fa2":"code","29ba130a":"code","77f14793":"code","7c21a1c6":"code","8abbb6da":"code","42adbdc3":"code","33dedb3d":"code","82a3e524":"code","70b1637d":"code","16c7eaa9":"code","373614d4":"code","b2c05074":"code","2c23b621":"code","75eed452":"code","e510ae14":"code","aa49e021":"code","55abd9af":"code","3ed5e28c":"code","ab6ba415":"code","2f5df9f8":"code","35918f1e":"code","627bc7a3":"code","2187f674":"code","f68fdd77":"code","80fe5a2a":"code","8633be8c":"code","615a1d2e":"code","dc452e21":"code","c4bf9507":"code","dc755e8e":"code","79f00cbe":"code","ff6adb66":"code","e6931a21":"code","105bf664":"code","2cf1f6cf":"code","ddcfca00":"code","ffbe9cb3":"code","be3f71d3":"code","0a37be51":"code","58a76f97":"code","0457ef6b":"code","f386997d":"code","d892d41d":"code","cacff415":"code","c3a768ef":"code","21a6ae63":"code","a21b7a9f":"code","629b8c14":"code","b49f7392":"code","5d60b415":"code","ced629e4":"code","b7221f4f":"code","342faf54":"code","9abef7df":"code","5afc6dba":"code","6f2898ce":"code","1cdb3b3f":"code","2086e65e":"code","6113f919":"code","0fa8cfe0":"code","4372ef6c":"code","b8d5d9f4":"code","2f73b309":"code","f64be59a":"markdown","77199513":"markdown","7d541663":"markdown","634d0165":"markdown","7033642c":"markdown","cbf568dc":"markdown","9809ec7d":"markdown","945c79de":"markdown","bf3f55cd":"markdown","baa2fa69":"markdown","eb462539":"markdown","a8589f25":"markdown","2def1419":"markdown","0e9c382d":"markdown","c28da93a":"markdown","6ba24477":"markdown","b1793f7d":"markdown","d7ed9d3d":"markdown","282259ee":"markdown"},"source":{"df869021":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.set()","57113b3b":"pd.set_option('display.max_columns', None)\ntrain = pd.read_excel(r\"Train.xlsx\",parse_dates=['Date_of_Journey','Arrival_Time'])\ntrain.head()","5cefa88f":"train.info()","46ae4ade":"train.isna().sum()","4549828c":"train[train.Route.isna()]","b57680cc":"train[(train.Source=='Delhi') & (train.Destination=='Cochin') &(train.Price==7480) & (train.Dep_Time=='09:45')]","343297cd":"train.Route.fillna('DEL \u2192 MAA \u2192 COK',inplace=True)\ntrain.Total_Stops.fillna('1 stop',inplace=True)","8c698f4d":"train[train.Total_Stops.isna()]","b0569e24":"train.Date_of_Journey.unique()","f4fae725":"(train[\"Date_of_Journey\"]).dt.day.unique()","b8a41ae0":"train.head()","f5f63fa2":"## Convert month & date from date of journey\ntrain['Journey_day']=train['Date_of_Journey'].dt.day\ntrain['Journey_month']=train['Date_of_Journey'].dt.month\n\n# Departure time is when a plane leaves the gate. \n# Similar to Date_of_Journey we can extract values from Dep_Time\n\n# Extracting Hours\ntrain[\"Dep_hour\"] = pd.to_datetime(train[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ntrain[\"Dep_min\"] = pd.to_datetime(train[\"Dep_Time\"]).dt.minute\n\n# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n# Extracting Hours\ntrain[\"Arrival_hour\"] = train.Arrival_Time.dt.hour\n\n# Extracting Minutes\ntrain[\"Arrival_min\"] = train.Arrival_Time.dt.minute","29ba130a":"train.head()","77f14793":"# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n\ntrain.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n\n# Now we can drop Dep_Time as it is of no use\ntrain.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n\n\n# Now we can drop Arrival_Time as it is of no use\ntrain.drop([\"Arrival_Time\"], axis = 1, inplace = True)","7c21a1c6":"train.head()","8abbb6da":"# Time taken by plane to reach destination is called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","42adbdc3":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain[\"Duration_hours\"] = duration_hours\ntrain[\"Duration_mins\"] = duration_mins","33dedb3d":"train.drop([\"Duration\"], axis = 1, inplace = True)","82a3e524":"train.head()","70b1637d":"train[\"Airline\"].value_counts()","16c7eaa9":"# From graph we can see that Jet Airways Business have the highest Price.\n# Apart from the first Airline almost all are having similar median\n\n# Airline vs Price\nsns.catplot(y = \"Price\", x = \"Airline\", data = train.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 6, aspect = 3)\nplt.show()","373614d4":"# As Airline is Nominal Categorical data we will perform OneHotEncoding\n\nAirline = train[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first= True)\n\nAirline.head()","b2c05074":"train[\"Source\"].value_counts()","2c23b621":"# Source vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\nplt.show()","75eed452":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\nSource = train[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first= True)\n\nSource.head()","e510ae14":"train[\"Destination\"].value_counts()","aa49e021":"# As Destination is Nominal Categorical data we will perform OneHotEncoding\n\nDestination = train[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first = True)\n\nDestination.head()","55abd9af":"train[\"Route\"]","3ed5e28c":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntrain.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","ab6ba415":"train[\"Total_Stops\"].value_counts()","2f5df9f8":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding keys\n\ntrain.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","35918f1e":"train.head()","627bc7a3":"# Concatenate dataframe --> train_data + Airline + Source + Destination\ntrain = pd.concat([train, Airline, Source, Destination], axis = 1)","2187f674":"train.head()","f68fdd77":"train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","80fe5a2a":"train.head()","8633be8c":"train.shape","615a1d2e":"test = pd.read_excel(\"Test.xlsx\")","dc452e21":"test.head()","c4bf9507":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest.dropna(inplace = True)\nprint(test.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest[\"Journey_day\"] = pd.to_datetime(test.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest[\"Journey_month\"] = pd.to_datetime(test[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest[\"Dep_hour\"] = pd.to_datetime(test[\"Dep_Time\"]).dt.hour\ntest[\"Dep_min\"] = pd.to_datetime(test[\"Dep_Time\"]).dt.minute\ntest.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest[\"Arrival_hour\"] = pd.to_datetime(test.Arrival_Time).dt.hour\ntest[\"Arrival_min\"] = pd.to_datetime(test.Arrival_Time).dt.minute\ntest.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest[\"Duration_hours\"] = duration_hours\ntest[\"Duration_mins\"] = duration_mins\ntest.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test[\"Source\"].value_counts())\nSource = pd.get_dummies(test[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test + Airline + Source + Destination\ntest = pd.concat([test, Airline, Source, Destination], axis = 1)\n\ntest.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)\n\n","dc755e8e":"test.head()","79f00cbe":"test.shape","ff6adb66":"X= train.drop('Price',axis=1)\ny=train.Price","e6931a21":"X.head()","105bf664":"y.head()","2cf1f6cf":"# Finds correlation between Independent and dependent attributes\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","ddcfca00":"# Important feature using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","ffbe9cb3":"print(selection.feature_importances_)","be3f71d3":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()\n","0a37be51":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","58a76f97":"from sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","0457ef6b":"y_pred = reg_rf.predict(X_test)","f386997d":"reg_rf.score(X_train, y_train)","d892d41d":"reg_rf.score(X_test, y_test)","cacff415":"sns.distplot(y_test-y_pred)\nplt.show()","c3a768ef":"\nplt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","21a6ae63":"from sklearn import metrics","a21b7a9f":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","629b8c14":"# RMSE\/(max(DV)-min(DV))\n\n2090.5509\/(max(y)-min(y))","b49f7392":"metrics.r2_score(y_test, y_pred)","5d60b415":"from sklearn.model_selection import RandomizedSearchCV","ced629e4":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","b7221f4f":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","342faf54":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","9abef7df":"rf_random.fit(X_train,y_train)","5afc6dba":"rf_random.best_params_","6f2898ce":"prediction = rf_random.predict(X_test)","1cdb3b3f":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","2086e65e":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","6113f919":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","0fa8cfe0":"import pickle\n# open a file, where you ant to store the data\nfile = open('flight_rf.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","4372ef6c":"model = open('flight_rf.pkl','rb')\nforest = pickle.load(model)","b8d5d9f4":"y_prediction = forest.predict(X_test)","2f73b309":"metrics.r2_score(y_test, y_prediction)","f64be59a":"## Fitting model using Random Forest\n\n1. Split dataset into train and test set in order to prediction w.r.t X_test\n2. If needed do scaling of data\n    * Scaling is not done in Random forest\n3. Import model\n4. Fit the data\n5. Predict w.r.t X_test\n6. In regression check **RSME** Score\n7. Plot graph","77199513":"---","7d541663":"---","634d0165":"---","7033642c":"## Treating missing values","cbf568dc":"---","9809ec7d":"## Hyperparameter Tuning\n\n\n* Choose following method for hyperparameter tuning\n    1. **RandomizedSearchCV** --> Fast\n    2. **GridSearchCV**\n* Assign hyperparameters in form of dictionery\n* Fit the model\n* Check best paramters and best score","945c79de":"## Importing dataset\n\n1. Since data is in form of excel file we have to use pandas read_excel to load the data\n2. After loading it is important to check the complete information of data as it can indication many of the hidden infomation such as null values in a column or a row\n3. Check whether any null values are there or not. if it is present then following can be done,\n    1. Imputing data using Imputation method in sklearn\n    2. Filling NaN values with mean, median and mode using fillna() method\n4. Describe data --> which can give statistical analysis","bf3f55cd":"---","baa2fa69":"## Extracting new date time features for more robustness\n\nFor this we require pandas **to_datetime** to convert object data type to datetime dtype.\n\n<span style=\"color: purple;\">**.dt.day method will extract  day of that date**<\/span>\\\n<span style=\"color: purple;\">**.dt.month method will extract  month of that date**<\/span>","eb462539":"## Test set","a8589f25":"---","2def1419":"## Feature Selection\n\nFinding out the best feature which will contribute and have good relation with target variable.\nFollowing are some of the feature selection methods,\n\n\n1. <span style=\"color: purple;\">**heatmap**<\/span>\n2. <span style=\"color: purple;\">**feature_importance_**<\/span>\n3. <span style=\"color: purple;\">**SelectKBest**<\/span>","0e9c382d":"## Handling Categorical Data\n\n","c28da93a":"## Save the model to reuse it again","6ba24477":"## EDA","b1793f7d":"---","d7ed9d3d":"# Flight Price Prediction\n---","282259ee":"---"}}