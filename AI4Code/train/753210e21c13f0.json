{"cell_type":{"69f101a1":"code","9290bf62":"code","6b52efde":"code","38d700c5":"code","d3b2ceb7":"code","1aef1c60":"code","905ca952":"code","1946527c":"markdown","f491b67a":"markdown"},"source":{"69f101a1":"import numpy as np \nimport pandas as pd \n\nimport numpy as np \nimport pandas as pd\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\nimport time\nfrom tqdm import tqdm, tqdm_notebook; tqdm.pandas() # Progress bar\nimport math\n\nfrom tensorflow.keras.utils import to_categorical\nseed = 1234\nnp.random.seed(seed)\n\nt_start = time.time()\n\nimport warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()\nwarnings.filterwarnings(\"ignore\")\n","9290bf62":"# Preprocessing parameters (it can be changed according to convenience)\nsr = 44100 # Sampling rate\nduration = 5\nhop_length = 347 # to make time steps 128\nfmin = 20\nfmax = sr \/\/ 2\nn_mels = 128\nn_fft = n_mels * 20\nsamples = sr * duration\nsmpl = 15 # size of samples from each bird code folder","6b52efde":"# Read train\ndf = pd.read_csv('..\/input\/birdsong-recognition\/train.csv')\n\n# Sample train\nconcat = []\nfor label in df['species'].unique() :\n    concat.append(df[df.species == label].sample(smpl, replace = True))\ndf= pd.concat(concat)\n\n# encode labels\ni = 0\ndict_map = {}\nfor label in df['ebird_code'].unique() :\n    dict_map[label] = i\n    i+=1\ndf['num_labels'] = df['ebird_code'].map(dict_map)","38d700c5":"# Preprocessing functions\n\ndef read_audio(path):\n    '''\n    Reads in the audio file and returns\n    an array that we can turn into a melspectogram\n    '''\n    y, _ = librosa.core.load(path, sr=44100)\n    # trim silence\n    if 0 < len(y): # workaround: 0 length causes error\n        y, _ = librosa.effects.trim(y)\n    if len(y) > samples: # long enough\n        y = y[0:0+samples]\n    else: # pad blank\n        padding = samples - len(y)\n        offset = padding \/\/ 2\n        y = np.pad(y, (offset, samples - len(y) - offset), 'constant')\n    return y\n\ndef audio_to_melspectrogram(audio):\n    '''\n    Convert to melspectrogram after audio is read in\n    '''\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=sr,\n                                                 n_mels=n_mels,\n                                                 hop_length=hop_length,\n                                                 n_fft=n_fft,\n                                                 fmin=fmin,\n                                                 fmax=fmax)\n    return librosa.power_to_db(spectrogram).astype(np.float32)\n\ndef read_as_melspectrogram(path):\n    '''\n    Convert audio into a melspectrogram \n    so we can use machine learning\n    '''\n    mels = audio_to_melspectrogram(read_audio(path))\n    return mels\n\ndef convert_wav_to_image(df, path):\n    X = []\n    for _,row in tqdm_notebook(df.iterrows(), total = df['ebird_code'].unique().shape[0] * smpl):\n        if row['filename'] != 'XC195038.mp3' :\n            x = read_as_melspectrogram('{}\/{}\/{}'.format(path[0],str(row['ebird_code']) ,str(row['filename'])))\n            X.append(x.transpose())\n    return X\n\ndef normalize(img):\n    '''\n    Normalizes an array \n    (subtract mean and divide by standard deviation)\n    '''\n    eps = 0.001\n    if np.std(img) != 0:\n        img = (img - np.mean(img)) \/ np.std(img)\n    else:\n        img = (img - np.mean(img)) \/ eps\n    return img\n\ndef normalize_dataset(X):\n    '''\n    Normalizes list of arrays\n    (subtract mean and divide by standard deviation)\n    '''\n    normalized_dataset = []\n    for img in X:\n        normalized = normalize(img)\n        normalized_dataset.append(normalized)\n    return normalized_dataset","d3b2ceb7":"# Preprocess dataset and create validation sets\nX = np.array(convert_wav_to_image(df, ['..\/input\/birdsong-recognition\/train_audio']))\nX = normalize_dataset(X)\ny = df['num_labels'].values","1aef1c60":"# Visualize an melspectogram example\nplt.figure(figsize=(15,10))\nplt.title('Visualization of audio file', weight='bold')\nplt.imshow(X[0]);","905ca952":"# Save spectograms as PKL format\n\nimport pickle\n\nwith open('X_train.pkl', 'wb') as f:\n    pickle.dump(X, f)\n    \nwith open('y_train.pkl', 'wb') as x:\n    pickle.dump(y, x)","1946527c":"Here the data is ready to be consumed by an RNN, LSTM, CNN or other.\n\n**Do not hesitate to upvote if it was useful to you**","f491b67a":"In this notebook ill show haw i do to import mp3 files, preprocess and save them for ML.\n\nthis process can be done in 5 steps :\n\n* Read train csv and smaple it.\n* Apply padding to make mp3 files same length or duration.\n* trasnform this files to spectogram.\n* normalise them.\n* export as pkl format.\n\nI was inspired by Carlo Lepelaars notebook: <a href=\"https:\/\/www.kaggle.com\/carlolepelaars\/bidirectional-lstm-for-audio-labeling-with-keras\">referance<\/a>"}}