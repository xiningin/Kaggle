{"cell_type":{"b1057166":"code","5fa84fba":"code","a9bdfcf3":"code","f64fef85":"code","7f50c1bc":"code","8901638e":"code","66c6308f":"code","9fb2976d":"code","256dec68":"code","ab831f76":"code","208121f0":"code","2e241636":"code","a2748162":"code","8e6a8596":"code","f8e53d68":"code","505fb02a":"code","824698d1":"code","aad88337":"code","32e82650":"code","8f55964f":"code","34667ebf":"code","da549b1c":"code","fa42446a":"code","7058e51d":"code","f2860133":"code","8da6e3da":"code","c77ecc0f":"code","1819e002":"code","58ab6cac":"code","845a908a":"code","80275808":"code","ac3b37b1":"code","14fefb6d":"code","0aafec68":"code","6061cd31":"code","8fd9980e":"code","a1f7fbc1":"code","f5f5340c":"code","d3348a6b":"code","1d124234":"code","813fc369":"code","e64e9b11":"code","1f5ffb81":"markdown","b5779277":"markdown","ce737ded":"markdown","75ec45ed":"markdown","589c4085":"markdown","5664e3cd":"markdown","677fd008":"markdown","0215f867":"markdown","77367f57":"markdown","a7dd695f":"markdown","64f7cdf9":"markdown","8de5e26a":"markdown","b263c3d0":"markdown","f2fbc55c":"markdown","74129d41":"markdown","4af7ec70":"markdown","b5920dd9":"markdown","ea87a17b":"markdown","f04c97ff":"markdown","0a1a3ac4":"markdown","de63883e":"markdown"},"source":{"b1057166":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve","5fa84fba":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","a9bdfcf3":"data.head()","f64fef85":"data.shape","7f50c1bc":"data.diagnosis.unique()","8901638e":"data.columns","66c6308f":"data.isna().sum()","9fb2976d":"data['Unnamed: 32'].unique()","256dec68":"data.drop('Unnamed: 32', axis = 'columns', inplace = True)","ab831f76":"data_copy = data.copy(deep=True)","208121f0":"y = data.diagnosis\ndata.drop(['diagnosis', 'id'], axis = 'columns', inplace = True)","2e241636":"data.head()","a2748162":"data_copy.drop(['id'], axis = 'columns' , inplace = True)\ndata_copy.head()","8e6a8596":"plt.figure(figsize = (20,10))\nsns.set_theme(style=\"darkgrid\")\n\nradius = data_copy[['radius_mean','radius_se','radius_worst','diagnosis']]\nsns.pairplot(radius, hue='diagnosis', markers=[\"o\", \"s\"])","f8e53d68":"area = data_copy[['area_mean','area_se','area_worst','diagnosis']]\nsns.pairplot(area, hue='diagnosis', markers=[\"o\", \"s\"])","505fb02a":"perimeter = data_copy[['perimeter_mean','perimeter_se','perimeter_worst','diagnosis']]\nsns.pairplot(perimeter, hue='diagnosis', markers=[\"o\", \"s\"])","824698d1":"texture = data_copy[['texture_mean','texture_se','texture_worst','diagnosis']]\nsns.pairplot(texture, hue='diagnosis', markers=[\"o\", \"s\"])","aad88337":"compactness = data_copy[['compactness_mean','compactness_se','compactness_worst','diagnosis']]\nsns.pairplot(compactness, hue='diagnosis', markers=[\"o\", \"s\"])","32e82650":"concavity = data_copy[['concavity_mean','concavity_se','concavity_worst','diagnosis']]\nsns.pairplot(concavity, hue='diagnosis', markers=[\"o\", \"s\"])","8f55964f":"symmetry = data_copy[['symmetry_mean','symmetry_se','symmetry_worst','diagnosis']]\nsns.pairplot(symmetry, hue='diagnosis', markers=[\"o\", \"s\"])","34667ebf":"fractal_dimension = data_copy[['fractal_dimension_mean','fractal_dimension_se','fractal_dimension_worst','diagnosis']]\nsns.pairplot(fractal_dimension, hue='diagnosis', markers=[\"o\", \"s\"])","da549b1c":"smoothness = data_copy[['smoothness_mean','smoothness_se','smoothness_worst','diagnosis']]\nsns.pairplot(smoothness, hue='diagnosis', markers=[\"o\", \"s\"])","fa42446a":"ax = sns.countplot(y)\nBenign, Malignant = y.value_counts(normalize = True)\nprint(f'The percentage of Benign case is : {Benign*100}\\n\\n')\nprint(f'The percentage of Malignant case is : {Malignant*100}\\n\\n')","7058e51d":"y=pd.DataFrame(y)\ny","f2860133":"plt.figure(figsize=(20,20))\nmask = np.triu(np.ones_like(data.corr()))\nsns.heatmap(data.corr(), cmap=\"Blues\", annot=True, mask=mask)","8da6e3da":"corr = data.corr()\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n                columns[j] = False\nselected_columns = data.columns[columns]\ndata = data[selected_columns]","c77ecc0f":"columns","1819e002":"data.shape","58ab6cac":"fig = plt.figure(figsize = (20, 25))\nj = 0\nfor i in data.columns:\n    plt.subplot(6, 4, j+1)\n    j += 1\n    sns.distplot(data[i][y['diagnosis']=='B'], color='#86994d', label = 'Benign')\n    sns.distplot(data[i][y['diagnosis']=='M'], color='#ed6a4a', label = 'Malignant')\n    plt.legend(loc='best')\nfig.suptitle('Breast Cancer Data Analysis')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","845a908a":"data.head()","80275808":"le = LabelEncoder()\ny.diagnosis = le.fit_transform(y.diagnosis)\ny.head()","ac3b37b1":"min_max_scaler = MinMaxScaler()\ndata[[\"radius_mean\", \"texture_mean\"]] = min_max_scaler.fit_transform(\n    data[[\"radius_mean\", \"texture_mean\"]])\ndata.head()","14fefb6d":"X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)","0aafec68":"def model_build(model, X_train, y_train, X_test):\n    model.fit(X_train, y_train)\n    y_predicted = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)\n    print(\"CLASSIFICATION REPORT CHART: \",\"\\n\\n\",classification_report(y_test, y_predicted),\"\\n\")\n    print(\"CONFUSION MATRIX\",\"\\n\\n\",confusion_matrix(y_test,y_predicted),\"\\n\")\n    print('ROC-AUC: ',roc_auc_score(y_test,y_prob[:,1]),\"\\n\")\n    print(\"TOTAL ACCURACY IN TRAINING: \",\"\\n\",model.score(X_train,y_train),\"\\n\")\n    print(\"TOTAL ACCURACY IN TESTING: \",\"\\n\",model.score(X_test,y_test),\"\\n\")\n    \n    plt.figure(figsize = (15,10))\n    fpr, tpr, thresholds = roc_curve(y_test, y_prob[:,1])\n    plt.subplot(2, 2, 1)\n    plt.plot(fpr, tpr, label='ROC curve')\n    plt.plot([0, 1], [0, 1], 'g--', label='Random guess')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC (Receiver operating characteristic) Curve')\n    plt.legend()\n    \n    precision, recall, thresholds = precision_recall_curve(y_test, y_prob[:,1])\n    plt.subplot(2, 2, 2)\n    plt.title(\"Precision-Recall-F1 vs Threshold Chart\")\n    plt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\n    plt.plot(thresholds, (2*precision[:-1]*recall[:-1])\/(precision[:-1]+recall[:-1]), \"g--\", label=\"F1_score\")\n    plt.ylabel(\"Precision, Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"lower left\")\n    plt.show()\n    \n    return y_predicted, y_prob","6061cd31":"model_lr = LogisticRegression()\ny_predicted_lr,y_prob_lr = model_build(model_lr, X_train, y_train, X_test)","8fd9980e":"model_rfc = RandomForestClassifier(criterion = 'gini')\ny_predicted_rfc,y_prob_rfc = model_build(model_rfc, X_train, y_train, X_test)","a1f7fbc1":"model_dtc = DecisionTreeClassifier()\ny_predicted_dtc,y_prob_dtc = model_build(model_dtc, X_train, y_train, X_test)","f5f5340c":"model_svc = SVC(probability=True)\ny_predicted_svc,y_prob_svc = model_build(model_svc, X_train, y_train, X_test)","d3348a6b":"score_lr = cross_val_score(model_lr, data, y, cv = 10)\nprint(f'The average score for Logistic Regression classifier is: {np.average(score_lr)}')","1d124234":"score_rfc = cross_val_score(model_rfc, data, y, cv = 10)\nprint(f'The average score for Random Forest classifier is: {np.average(score_rfc)}')","813fc369":"score_dtc = cross_val_score(model_dtc, data, y, cv = 10)\nprint(f'The average score for Decision Tree classifier is: {np.average(score_dtc)}')","e64e9b11":"score_svc = cross_val_score(model_svc, data, y, cv = 10)\nprint(f'The average score for Support Vector classifier is: {np.average(score_svc)}')","1f5ffb81":"## Logistic Regression Model","b5779277":"## Support Vector Classifier","ce737ded":"<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\"><b>\n    Feature Description<\/b><\/span><\/div>\n\n<div style=\"text-align: justify\"><span style=\"color:#000000; font-family:Georgia; font-size:1.2em;\">\n   The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.  <\/span><\/div>\n&nbsp;\n\n\n<div style=\"text-align: justify\"><table style=\"width:80%\">\n  <tr>\n    <th align= \"left\">Features<\/th>\n    <th align= \"left\">Description<\/th>\n  <\/tr>\n\n  <tr>\n    <td>ID number <\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>Diagnosis<\/td>\n    <td> (M = malignant, B = benign)<\/td>\n  <\/tr>\n  <tr>\n    <td>radius<\/td>\n    <td>mean of distances from center to points on the perimeter<\/td>\n  <\/tr>\n  <tr>\n    <td>texture<\/td>\n    <td>standard deviation of gray-scale values<\/td>\n  <\/tr>\n  <tr>\n    <td>perimeter<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>area<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>smoothness<\/td>\n    <td>local variation in radius lengths<\/td>\n  <\/tr>\n  <tr>\n    <td>compactness<\/td>\n    <td>(perimeter^2 \/ area - 1.0)<\/td>\n  <\/tr>\n  <tr>\n    <td>concavity<\/td>\n    <td>severity of concave portions of the contour<\/td>\n  <\/tr>\n  <tr>\n    <td>concave points<\/td>\n    <td>number of concave portions of the contour<\/td>\n  <\/tr>\n  <tr>\n    <td>symmetry<\/td>\n    <td><\/td>\n  <\/tr>\n  <tr>\n    <td>fractal dimension<\/td>\n    <td> (\"coastline approximation\" - 1)<\/td>\n  <\/tr>\n<\/table><\/div>","75ec45ed":"## Feature Selection","589c4085":"Since, unnamed column has all 569 entries as nan so we do not need this column.","5664e3cd":"## Decision Tree Classifier","677fd008":"We see that, out of all the models, Random forest classifier performs the best.","0215f867":"## Splitting our dataset into training and testing samples.","77367f57":"### We will try logistic regression, Random forest classifier, Decision tree classifier and support vector classifier.\nWe will define a function named model_build to train the models and print the classification report.","a7dd695f":"## Random Forest Classifier","64f7cdf9":"Making a copy of the dataset to perform Exploratory Data Analysis","8de5e26a":"### Now, we will scale the columns so that they can be used to build our model.","b263c3d0":"Eliminating those columns that have correlation of more than 0.9","f2fbc55c":"## Importing Libraries","74129d41":"### We can see that there are some features that have to be normalized before we can use them for building our model.\n\nFirst, we will encode the diagnosis column of the y dataset.","4af7ec70":"## Performing Exploratory Data Analysis","b5920dd9":"## Performing K-fold Cross Validation to find which model has higher accuracy","ea87a17b":"The diagnosis column has only two values to it. M - for malignant and B - for Benign. We will use this column as our target column\/ output column to build our machine learning model.","f04c97ff":"We can see that the column number have reduced from 30 to 20.","0a1a3ac4":"We see that Malignant has been encoded as 1, while Benign has been encoded as 0.","de63883e":"# <div style=\"text-align: Left\"><span style=\"color:#67636b; font-family:Georgia;\">Exploratory Data Analysis and Machine Learning Models on Breast Cancer Dataset<\/span><\/div>"}}