{"cell_type":{"aeae36ed":"code","48ceabc4":"code","8c2d7af8":"code","70d8cf91":"code","11521703":"code","fddff1b0":"code","e0a61057":"code","e0ce735d":"code","987c2de3":"code","fddaf299":"code","87d5d37b":"code","33e1dc0d":"code","e77e475e":"code","9ea2887b":"code","784f062f":"code","ef0ce825":"code","b2d5f8ed":"code","f7084227":"code","892830b2":"code","10232e96":"markdown","950251f7":"markdown","16c5b8cb":"markdown","3554e37f":"markdown","0c193f57":"markdown","66dce1f9":"markdown","533a8901":"markdown","e540a097":"markdown","3e515451":"markdown","6c8b479d":"markdown","124ac946":"markdown","cf8e17d1":"markdown","61e10e25":"markdown","cd3655ea":"markdown","973ec7d2":"markdown","2ea63732":"markdown","1d68cefb":"markdown","94f957c6":"markdown","8d7f9d52":"markdown","466321fe":"markdown","10f99e7c":"markdown","9b273398":"markdown","d7805242":"markdown","167d21e3":"markdown","a75b41e9":"markdown","297b4166":"markdown"},"source":{"aeae36ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# FOR EDA --------------- # \nimport matplotlib.pyplot as plt \n%matplotlib inline \nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\n# ----------------------- #\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48ceabc4":"data = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')","8c2d7af8":"display(data.info(), data.head())","70d8cf91":"def target_count(data,column):\n    trace = go.Bar( x = data[column].value_counts().values.tolist(),\n    y = data[column].unique(),\n    orientation = 'h',\n    text = data[column].value_counts().values.tolist(),\n    textfont=dict(size=20),\n    textposition = 'auto',\n    opacity = 0.5,marker=dict(colorsrc='tealrose',\n            line=dict(color='#000000',width=1.5))\n    )\n    layout = (dict(title= \"EDA of {} column\".format(column),\n                  autosize=True,height=800,))\n    fig = dict(data = [trace], layout=layout)\n    \n    py.iplot(fig)\n\n# --------------- donut chart to show there percentage -------------------- # \n\ndef target_pie(data,column):\n    trace = go.Pie(labels=data[column].unique(),values=data[column].value_counts(),\n                  textfont=dict(size=15),\n                   opacity = 0.5,marker=dict(\n                   colorssrc='tealrose',line=dict(color='#000000', width=1.5)),\n                   hole=0.6)\n                  \n    layout = dict(title=\"Dounat chart to see %age of individual elements\")\n    fig = dict(data=[trace],layout=layout)\n    py.iplot(fig)","11521703":"# Model \n\ntarget_count(data,'model')\ntarget_pie(data,'model')\n","fddff1b0":"# Transmition\n\ntarget_count(data,'transmission')\ntarget_pie(data,'transmission')","e0a61057":"# fuelType\n\ntarget_count(data,'fuelType')\ntarget_pie(data,'fuelType')","e0ce735d":"#df['model','transmission','fuelType'] = data['model','transmission','fuelType']\n#for feat in ['model','transmission','fuelType']:\n\nfrom sklearn.preprocessing import LabelEncoder\nlb_make = LabelEncoder()\n## model_LE\ndata[\"LE_model\"] = lb_make.fit_transform(data[\"model\"])\n\n## model_LE\ndata[\"LE_transmission\"] = lb_make.fit_transform(data[\"transmission\"])\n\n## model_LE\ndata[\"LE_fuelType\"] = lb_make.fit_transform(data[\"fuelType\"])\n\n## results\ndata[[\"model\",\"LE_model\",\"transmission\",\"LE_transmission\",\"fuelType\",\"LE_fuelType\"]].head(11)","987c2de3":"print(sum(data['engineSize'] == 0))\nprint(sum(data['tax'] == 0))","fddaf299":"# We have simply replaced 0 values with null.\ndata[[\"engineSize\",\"tax\"]] = data[[\"engineSize\",\"tax\"]].replace(0,np.NaN)\ndata.isnull().sum()","87d5d37b":"def find_median(var):\n    temp = data[data[var].notnull()]\n    temp = data[[var,'model']].groupby('model')[[var]].median().reset_index()\n    return temp","33e1dc0d":"# model filling\n\nfind_median('tax')","e77e475e":"data = data.dropna()\ndata = data.reset_index(drop=True)","9ea2887b":"display(data.info)","784f062f":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #label \n    matrix_cols = correlation.columns.tolist()\n    #convert to array as it can't take values directly. \n    corr_array = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                      x=matrix_cols,\n                      y=matrix_cols,\n                      colorscale='Viridis',\n                      colorbar = dict()\n                      )\n    layout = go.Layout(dict(title='Correlation Matrix for variables provided.',\n                          margin = dict(r=0,l=100,\n                                       t = 0, b =100,),\n                          yaxis = dict(tickfont = dict(size = 9)),\n                          xaxis = dict(tickfont = dict(size = 9)),\n                          )\n                      )\n    fig = go.Figure(data = [trace], layout = layout)\n    py.iplot(fig)","ef0ce825":"## So let's start  by finding the correlation between the columns. \n\ncorrelation_plot()\n","b2d5f8ed":"from sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nreg = linear_model.LinearRegression()\nX = data[['LE_model', 'LE_transmission', 'LE_fuelType', 'engineSize','year', 'tax','mileage','mpg']]\nY = data['price']\ntrain_X,test_X,train_Y,test_Y = train_test_split(X,Y,test_size=0.2)\nreg.fit(train_X,train_Y)\nprint('Performance Score(GB): %.1f ' %(reg.score(test_X,test_Y)*100))","f7084227":"from sklearn.ensemble import GradientBoostingRegressor\nGB=GradientBoostingRegressor(random_state=0)\nGB.fit(train_X,train_Y)\nprint('Performance Score(GB): %.1f ' %(GB.score(test_X,test_Y)*100))","892830b2":"from xgboost import XGBRegressor\nXGB=XGBRegressor(random_state=0)\nXGB.fit(train_X,train_Y)\nprint('Performance score(XGB): %.1f ' %(XGB.score(test_X,test_Y)*100))","10232e96":"# Heatmap(correlation)","950251f7":"# Task 1: Encoding Data","16c5b8cb":"# Let's Begin.","3554e37f":"## Currently I think its better to drop these rows as out of 10,000+ dataset few 100 rows won't matter. \n## Note: I am making an expection of this dataset only...","0c193f57":"### Here we can easyly see that our data has been Label-Encoded we will be droping the previous columns later after the EDA. ","66dce1f9":" # Prediction model","533a8901":"### Now lets find what values can be filled in place of null.<br>\n### Well we can assume that both tax and engineSize depends on the model of the car so we are going to group them and replace the null values with the median of tax and size of that model.","e540a097":"### There are 3 type of Transmitions with most of the cars are automatic.","3e515451":"### Here we can see that all 'Series'(5,6,1,7,2,4) type model were sold more than other models followed by X type models.","6c8b479d":"## Now that we are finished with preparing data now we can finally start finding correlation.","124ac946":"# Feature Engineering","cf8e17d1":"# Finally we have reached the end!!! \n# So I conclude that there are many more ways to predict the price of the BMW car price, meanwhile I have found that XGBoost was the most accurate predictor with 95.6% validation accuracy. ","61e10e25":"# 3. *fuelType*","cd3655ea":"## Though no null value is present however some data discrepancies can be seen \n## engineSize and tax cannot be 0. Thus we are going to consider them null values and find suitable values to fill in","973ec7d2":"## We will be encoding columns -- model,transmission and fuelType \n\n### But lets see what kind of data is there in these columns so as to decide which type of encoding will be better for the data. ","2ea63732":"# 2.*Transmition*","1d68cefb":"# Task : Predict the price of the car(bmw). \n\n# Given: A csv file with all the require data so that a prediction can be made. \n\n# Hope you like the notebook. Please comment and upvote it if you like it.","94f957c6":"From the above correlation map the following information can be derived. \n\n1. Price & Year have a direct connection -- maybe the older car the less price --> \n2. Price & EngineSize are also correlated. -- maybe bigger the engine more the price -->\n3. Price & LE_model are also correlated. -- better model more the price -->\n4. Price & tax are also have some correlation -- less price thus lower tax has to be paid.-->\n\n5. engineSize & tax are also correlated and indirectly affect the price.\n\nIn the following process we will try to find more about it..","8d7f9d52":"Seeing the type of data I am going to use label encoding for encoding the data ","466321fe":"### Most of the cars are of Diesel fuelType followed by Petrol fuelType rest make a very small portion. ","10f99e7c":"# Re-Checking few columns for data-discrepancies","9b273398":"# Thank You for going through the notebook. Hope you found it interesting and helpful. \n# Please comment and upvote this notebook. Thank you again..","d7805242":"## Label-Encoding","167d21e3":"### Here we have 9 columns and 10780 rows of data. and no null values currently. \n\n### But before making any prediction we have to do the following things --\n\n### 1. Convert all the data into some numerical format for a proper evaluation. \n\n### 2. Take look on the data provided and categorise them as valuable or not for the prediction.\n","a75b41e9":"Let's start by reading the data and finding more about the data in the csv file. ","297b4166":"# 1. *Model* "}}