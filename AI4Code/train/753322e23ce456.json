{"cell_type":{"1d38c9b8":"code","50cf6085":"code","5dcb514c":"code","eab4cda8":"code","9b617f1a":"code","a9064040":"code","081edbb4":"code","a30f72aa":"code","ec2216f3":"code","c24263b6":"code","48d66aaf":"code","4ae8d2fb":"code","f6bde059":"code","fb9b2de6":"code","ce2791b0":"code","bb1bbcca":"code","cc8f41ae":"code","9fa3c648":"code","097cbd27":"code","2419743a":"code","824268d8":"code","afaa6784":"code","0d84b1f9":"code","2c78ca1c":"code","8cc4bc88":"code","12347db5":"code","6a55cb0e":"code","598a085c":"code","5f07eb4e":"code","45cbf15e":"code","de70776f":"code","5c7cc036":"code","96367884":"code","bccb8f16":"code","973bcf9b":"code","a74283e4":"code","5a7d9cb1":"code","23a6a747":"code","f3140f78":"code","a1c1035a":"code","730d3f3a":"code","b63614e9":"code","81b27aa5":"code","97c889b2":"code","c60f4e56":"code","b740f13c":"code","26b06f3c":"code","4811870a":"code","b236f49b":"code","d0a6d631":"markdown","e6be91aa":"markdown","b98aafef":"markdown","750000a0":"markdown","32a76258":"markdown","c5ddb65f":"markdown","ae84a14b":"markdown","50f898b9":"markdown","ae8a7602":"markdown","3ccaf63f":"markdown","31b769da":"markdown","fdff8ce5":"markdown","0333357e":"markdown","34ac6c23":"markdown","afc43d73":"markdown","5e19ffa9":"markdown","99ff7a7c":"markdown","9a133734":"markdown","cf8ad749":"markdown","f0162b8d":"markdown","40be1659":"markdown","c8d316ab":"markdown","7ee25b9f":"markdown","e7bc9b07":"markdown","e67b2700":"markdown","36cd49c9":"markdown","2d62def7":"markdown","e60edd57":"markdown","fd718245":"markdown","801d0556":"markdown"},"source":{"1d38c9b8":"!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_sm-0.2.4.tar.gz\n!pip install scispacy\n!git clone https:\/\/github.com\/epfml\/sent2vec.git","50cf6085":"cd sent2vec","5dcb514c":"!pip install .","eab4cda8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport spacy\nimport en_core_sci_sm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\nimport spacy\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\n\npd.options.mode.chained_assignment = None  # default='warn'","9b617f1a":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nall_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nprint(len(all_json), 'JSON files detected')\nmetadata_path = f'{root_path}metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str,\n    'doi': str\n})\nprint(len(meta_df), 'files in metadata.csv')\nprint(meta_df.columns)\nmeta_df.head(3)","a9064040":"meta_df['publish_date'] = pd.to_datetime(meta_df['publish_time'], infer_datetime_format=True)\nmeta_df['publish_date'].head()","081edbb4":"# 'source_x', 'license', 'journal', 'has_pdf_parse', 'has_pmc_xml_parse', 'full_text_file'\nprint(\"Source:\")\nprint(meta_df['source_x'].value_counts(), '\\n')\n\nprint(\"license:\")\nprint(meta_df['license'].value_counts(), '\\n')\n\nprint(\"Journal:\")\nprint(meta_df['journal'].value_counts(), '\\n')\n\nprint(\"has_pdf_parse:\")\nprint(meta_df['has_pdf_parse'].value_counts(), '\\n')\n\nprint(\"has_pmc_xml_parse:\")\nprint(meta_df['has_pmc_xml_parse'].value_counts(), '\\n')\n\nprint(\"full_text_file:\")\nprint(meta_df['full_text_file'].value_counts(), '\\n')","a30f72aa":"##### Keyword patterns to search for\nkeywords = [r\"2019[\\-\\s]?n[\\-\\s]?cov\", \"2019 novel coronavirus\", \"coronavirus 2019\", r\"coronavirus disease (?:20)?19\",\n            r\"covid(?:[\\-\\s]?19)?\", r\"n\\s?cov[\\-\\s]?2019\", r\"sars-cov-?2\", r\"wuhan (?:coronavirus|cov|pneumonia)\",\n            r\"rna (?:coronavirus|cov|pneumonia)\", r\"mers (?:coronavirus|cov|pneumonia)\", r\"influenza (?:coronavirus|cov|pneumonia)\",\n            r\"sars (?:coronavirus|cov|pneumonia)\", r\"sars\", r\"mers\", r\"pandemic\", r\"pandemics\"]\n\ncovid_keywords = [r\"2019[\\-\\s]?n[\\-\\s]?cov\", \"2019 novel coronavirus\", \"coronavirus 2019\", r\"coronavirus disease (?:20)?19\",\n            r\"covid(?:[\\-\\s]?19)?\", r\"n\\s?cov[\\-\\s]?2019\", r\"sars-cov-?2\", r\"wuhan (?:coronavirus|cov|pneumonia)\"]\n\n# Build regular expression for each keyword. Wrap term in word boundaries\nregex = \"|\".join([\"\\\\b%s\\\\b\" % keyword.lower() for keyword in keywords])\ncovid_regex = \"|\".join([\"\\\\b%s\\\\b\" % keyword.lower() for keyword in covid_keywords])\n\ndef tags(text):\n    if re.findall(regex, str(text).lower()):\n        tags = \"COVID-19\"\n    else:\n        tags=\"NON COVID\"\n    return tags\n\ndef covid_tags(text):\n    if re.findall(covid_regex, str(text).lower()):\n        tags = \"COVID-19\"\n    else:\n        tags=\"NON COVID\"\n    return tags\n\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n        self.paper_id = content['paper_id']\n        self.content = content\n        self.abstracts = []\n        self.body_texts = []\n        self.countries = []\n        self.title = ''\n        \n        if \"abstract\" in self.content:\n            for idx, entry in enumerate(self.content['abstract']):\n                if not isinstance(entry['text'], str):\n                    continue\n                d={}\n                d[\"idx\"] = idx\n                d[\"section\"] = entry['section']\n                d[\"para\"] = entry['text']\n                self.abstracts.append(d)\n    \n        for idx,entry in enumerate(self.content['body_text']):\n            if not isinstance(entry['text'], str):\n                continue\n            d = {}\n            d[\"idx\"] = idx\n            d[\"section\"] = entry['section']\n            d[\"para\"] = entry['text']\n            self.body_texts.append(d)\n    \n        if \"metadata\" in self.content and \"authors\" in self.content[\"metadata\"]:\n            for idx, author in enumerate(self.content['metadata']['authors']):\n                if 'affiliation' in author and 'location' in author['affiliation']:\n                    location = author['affiliation']['location']\n                    if 'region' in location:\n                        self.countries.append(location['region'])\n                    elif 'country' in location:\n                        self.countries.append(location['country'])\n        self.countries = list(set(self.countries))\n    \n        if \"metadata\" in self.content and \"title\" in self.content[\"metadata\"]:\n            self.title = self.content[\"metadata\"][\"title\"]\n    \n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstracts[:2]}... {self.body_texts[:2]}... {self.countries[:]}...'\n\nfirstRow = FileReader(all_json[0])\nprint(firstRow.paper_id)\nprint(firstRow.title)\nprint(firstRow.countries)","ec2216f3":"no_of_files_not_found = 0\ndict_ = {\n    'cord_uid':[], 'paper_id': [], 'section': [], 'sub_section': [], 'paragraph': [], 'authors': [], 'title': [],\n    'journal': [], 'publish_date': [], 'tags':[], 'countries': [], 'source_x': [], 'full_text_file': []\n}\n\nfor index, meta in tqdm(meta_df.iterrows(), total=meta_df.shape[0]): # \n    has_pdf_parse = meta.has_pdf_parse\n    has_pmc_xml_parse = meta.has_pmc_xml_parse\n    \n    if has_pmc_xml_parse == True:\n        full_text_file_path = root_path + meta.full_text_file + '\/' + meta.full_text_file + '\/pmc_json\/' + meta.pmcid + '.xml.json'\n    elif has_pdf_parse == True:\n        full_text_file_path = root_path + meta.full_text_file + '\/' + meta.full_text_file + '\/pdf_json\/' + meta.sha + '.json'\n    \n    if os.path.isfile(full_text_file_path) == False:\n        no_of_files_not_found += 1\n        continue\n    \n    content = FileReader(full_text_file_path)\n    \n    cord_uid = meta.cord_uid\n    authors = meta.authors\n    title = meta.title\n    journal = meta.journal\n    publish_date = meta.publish_date\n    source_x = meta.source_x\n    full_text_file = meta.full_text_file\n        \n#     print('Authors : ', authors)\n#     print('Title : ', title)\n#     print('Journal : ', journal)\n#     print('Publish Date : ', publish_date)\n#     print('Journal : ', journal)\n#     if len(content.countries)>0:\n#         print('Countries : ', ','.join(content.countries))\n        \n    metadata_abstract = meta.abstract\n    if isinstance(metadata_abstract, str):\n        dict_['cord_uid'].append(cord_uid)\n        dict_['paper_id'].append(content.paper_id)\n        dict_['section'].append('Metadata Abstract')\n        dict_['sub_section'].append('Metadata Abstract')\n        dict_['tags'].append(tags(metadata_abstract))\n        dict_['paragraph'].append(metadata_abstract)\n        dict_['publish_date'].append(publish_date)\n        dict_['authors'].append(authors)\n        dict_['title'].append(title)\n        dict_['journal'].append(journal)\n        dict_['countries'].append(','.join(content.countries))\n        dict_['source_x'].append(source_x)\n        dict_['full_text_file'].append(full_text_file)\n    \n    for items in content.abstracts:\n        dict_['cord_uid'].append(cord_uid)\n        dict_['paper_id'].append(content.paper_id)\n        dict_['section'].append('JSON Abstract')\n        dict_['sub_section'].append(items['section'])\n        dict_['tags'].append(tags(str(items['para'])))\n        dict_['paragraph'].append(items['para'])\n        dict_['publish_date'].append(publish_date)\n        dict_['authors'].append(authors)\n        dict_['title'].append(title)\n        dict_['journal'].append(journal)\n        dict_['countries'].append(','.join(content.countries))\n        dict_['source_x'].append(source_x)\n        dict_['full_text_file'].append(full_text_file)\n        \n    for items in content.body_texts:\n        dict_['cord_uid'].append(cord_uid)\n        dict_['paper_id'].append(content.paper_id)\n        dict_['section'].append('Body')\n        dict_['sub_section'].append(items['section'])\n        dict_['tags'].append(tags(str(items['para'])))\n        dict_['paragraph'].append(items['para'])\n        dict_['publish_date'].append(publish_date)\n        dict_['authors'].append(authors)\n        dict_['title'].append(title)\n        dict_['journal'].append(journal)\n        dict_['countries'].append(','.join(content.countries))\n        dict_['source_x'].append(source_x)\n        dict_['full_text_file'].append(full_text_file)\n        \nprint(no_of_files_not_found, 'files not found.')","c24263b6":"import os.path\ndef extract_covid_abstracts_body(dict_):\n    df_all = pd.DataFrame(dict_,\n                        columns=['cord_uid', 'paper_id', 'section', 'sub_section', 'paragraph', 'authors', 'title',\n                                 'journal', 'publish_date', 'tags', 'countries', 'source_x', 'full_text_file' ])\n    print(\"# of All publications : \", df_all['cord_uid'].nunique())\n    \n    df_titles = df_all[df_all['title'].notnull() & df_all['title'] != '']\n    df_na_titles = df_all[df_all['title'].isna() | df_all['title'] == '']\n    \n    print(df_titles['cord_uid'].nunique(), 'publications have titles')\n    print(df_na_titles['cord_uid'].nunique(), 'publications have titles missing')\n    \n    # TODO remove duplicates\n    display(df_all.head(3))\n    \n    df_titles = df_titles[['cord_uid', 'title', 'publish_date', 'countries', 'journal', 'source_x', 'full_text_file']].drop_duplicates()\n    df_titles = df_titles.drop_duplicates(subset='title', keep='last')\n    print(len(df_titles), 'unique titles selected.')\n    display(df_titles.head(5))\n#     # Extract only Covid articles\n#     df_covid_content = df_all.loc[df_all['tags'] == 'COVID-19']\n#     print(\"Covid19 papers : \", df_covid_content['paper_id'].nunique())\n    \n    df_metadata_abstracts = df_all.loc[df_all['section']==\"Metadata Abstract\"]\n    df_json_abstracts = df_all.loc[df_all['section']==\"JSON Abstract\"]\n    df_body = df_all.loc[df_all['section']==\"Body\"]\n#     df_body_meta = df_body.loc[df_body.paper_id.isin(df_metadata_abstracts.paper_id.values)]\n#     df_body_non_meta = df_body.loc[~df_body.paper_id.isin(df_metadata_abstracts.paper_id.values)]\n    \n    return df_titles, df_metadata_abstracts, df_body\n\ndf_titles, df_abstracts, df_body = extract_covid_abstracts_body(dict_)\npaper_bodies = df_body.groupby(['cord_uid'])\nprint(\"Metadata unique abstracts : \", df_abstracts['paper_id'].nunique())\nprint(\"# of unique meta papers : \", df_body['paper_id'].nunique())\n\n# df_abstracts.head()\ndf_abstracts['tags'].value_counts()","48d66aaf":"import seaborn as sns\nimport matplotlib.pylab as plt\n%matplotlib inline\n\ndf_titles.columns","4ae8d2fb":"df_titles['month'] = df_titles[\"publish_date\"].dt.month\ndf_titles['year'] = df_titles[\"publish_date\"].dt.year\n\ngrouped_titles_by_time_source = df_titles.groupby(['year', 'month', 'full_text_file'])['title'].count()\ngrouped_titles_by_time_source = grouped_titles_by_time_source.reset_index()\ngrouped_titles_by_time_source.head()","f6bde059":"# plt.figure(figsize=(8, 5))\ng = sns.catplot(x=\"full_text_file\", kind=\"count\", data=grouped_titles_by_time_source);\ng.fig.set_figheight(4)\ng.fig.set_figwidth(6)\nplt.xticks(rotation=45)\nplt.title('# of titles published by full_text_file')","fb9b2de6":"# # Summarize the number of publications\n# groups_titles = df_titles.groupby(['year', 'month'])['title'].count()\n# groups_titles = groups_titles.reset_index()\n# groups_titles.tail()\n\nplt.figure(figsize=(20, 5))\ng = sns.relplot(x=\"year\", y=\"title\", kind=\"line\", hue=\"full_text_file\", dashes=False, markers=True, data=grouped_titles_by_time_source)\ng.fig.set_figheight(5)\ng.fig.set_figwidth(20)\nplt.title('# of titles published over time')","ce2791b0":"!pip install us","bb1bbcca":"# COUNTRIES_LOOKUP\n[c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Italy').alpha_2)]","cc8f41ae":"import pycountry     # package for all countries\nimport us # package for us states\nfrom fuzzywuzzy import fuzz\nfrom fuzzywuzzy import process\n\n# countries lookup\nCOUNTRIES_LOOKUP = {c.name.lower():c.name for c in pycountry.countries}\nCANADA_SUBDIVISIONS = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Canada').alpha_2)]\nINDIAN_STATES = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='India').alpha_2)]\nJAPAN_SUBS = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Japan').alpha_2)]\nAUS_SUBS = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Australia').alpha_2)]\nCHINA_SUBS = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='China').alpha_2)]\nENG_SUBS = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='United Kingdom').alpha_2)]\nCOLOMBIA_SUBS = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Colombia').alpha_2)]\nITALY_SUBS = [c.name for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Italy').alpha_2)]\n\n# countries abbreviations lookup\nCAN_ABR = [ c.code.replace(c.country_code+'-', '') for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Canada').alpha_2)]\nAUS_ABR = [c.code.replace(c.country_code+'-', '') for c in pycountry.subdivisions.get(country_code=pycountry.countries.get(name='Australia').alpha_2)]\n\ndf_titles['countries'].replace('', \"NA\", inplace=True)","9fa3c648":"# import time\n\n# country_counts = []\n# for auth_coun in tqdm(df_titles[\"countries\"].values):\n#     country_counts.append(len([c for c in auth_coun.split(\",\")]))\n# df_titles['country_counts'] = country_counts\n# # print(df_titles['country_counts'].value_counts())\n\n# start_time = time.time()\n# author_coun_list = df_titles.loc[df_titles['country_counts'] >= 15]['countries'].values\n# not_found_list = []\n# for author_couns in tqdm(author_coun_list):\n#     coun_list = author_couns.split(\",\")\n#     for coun_raw in coun_list:\n        \n#         coun = re.sub('[^A-Za-z\\s]+', '', coun_raw).strip()\n#         us_state = us.states.lookup(coun)\n\n#         if coun == \"UK\":\n#             coun = \"United Kingdom\"\n#         if coun == \"Ont\":\n#             coun = \"Ontario\"\n        \n#         fuzzy_country = process.extractOne(coun.lower(), COUNTRIES_LOOKUP.keys(), scorer=fuzz.token_set_ratio) \n#         fuzzy_canada = process.extractOne(coun.lower(), CANADA_SUBDIVISIONS, scorer=fuzz.token_set_ratio) \n#         fuzzy_india = process.extractOne(coun.lower(), INDIAN_STATES, scorer=fuzz.token_set_ratio) \n#         fuzzy_japan = process.extractOne(coun.lower(), JAPAN_SUBS, scorer=fuzz.token_set_ratio) \n#         fuzzy_australia = process.extractOne(coun.lower(), AUS_SUBS, scorer=fuzz.token_set_ratio) \n#         fuzzy_china = process.extractOne(coun.lower(), CHINA_SUBS, scorer=fuzz.token_set_ratio) \n#         fuzzy_britain = process.extractOne(coun.lower(), ENG_SUBS, scorer=fuzz.token_set_ratio) \n#         fuzzy_colombia = process.extractOne(coun.lower(), COLOMBIA_SUBS, scorer=fuzz.token_set_ratio) \n#         fuzzy_espana_score = fuzz.token_sort_ratio(coun.lower(), \"espa\u00f1a\")\n        \n#         normalized_country = 'lookup_match_failed'\n#         if us_state or coun.lower() == \"usa\" or coun.lower() == \"united states\":\n#             normalized_country = \"usa\"\n        \n#         elif coun.lower() in COUNTRIES_LOOKUP:\n#             normalized_country = coun.lower()\n        \n#         elif coun.lower() == 'russia':\n#             normalized_country = 'russian federation'\n        \n#         elif coun.lower() == 'vietnam':\n#             normalized_country = 'vietnam'\n        \n#         elif coun.lower() == 'south korea'\n#             normalized_country = 'south korea'\n        \n#         elif coun.lower() == 'roc' or coun.lower() == 'pr china' or coun.lower() == 'prchina':\n#             normalized_country = 'china'\n        \n#         elif fuzzy_espana_score >= 85 or coun.lower() == \"sp\":\n#             normalized_country = 'spain'\n        \n#         elif coun in CAN_ABR or fuzzy_canada[1] >= 85:\n#             normalized_country = 'canada'\n# #             print(coun, \"================= fuzzy_canada ===================>\", normalized_country)\n        \n#         elif fuzzy_country[1] >= 85:\n# #             print(coun, \"================= fuzzy_country ===================>\", fuzzy_country[0].lower())\n#             normalized_country = fuzzy_country[0].lower()\n        \n#         elif fuzzy_india[1] >= 85:\n# #             print(coun, \"================= fuzzy_india ===================>\", fuzzy_india[0].lower())\n#             normalized_country = 'india'\n        \n#         elif fuzzy_japan[1] >= 85:\n# #             print(coun, \"================= fuzzy_japan ===================>\", fuzzy_japan[0].lower())\n#             normalized_country = 'japan'\n        \n#         elif coun in AUS_ABR or fuzzy_australia[1] >= 85:\n#             normalized_country = 'australia'\n# #             print(coun, \"================= fuzzy_australia ===================>\", normalized_country)\n        \n#         elif fuzzy_china[1] >= 85:\n# #             print(coun, \"================= fuzzy_china ===================>\", fuzzy_china[0].lower())\n#             normalized_country = 'china'\n        \n#         elif fuzzy_britain[1] >= 85:\n# #             print(coun, \"================= fuzzy_britain ===================>\", fuzzy_britain[0].lower())\n#             normalized_country = 'united kingdom'\n        \n#         elif fuzzy_colombia[1] >= 85:\n# #             print(coun, \"================= fuzzy_colombia ===================>\", fuzzy_colombia[0].lower())\n#             normalized_country = 'colombia'\n    \n#         elif fuzzy_italy[1] >= 85:\n# #             print(coun, \"================= fuzzy_italy ===================>\", fuzzy_italy[0].lower())\n#             normalized_country = 'italy'\n    \n# #         elif len(coun) > 3:\n# #             try:\n# #                 matches = pycountry.countries.search_fuzzy(coun)\n# #                 if len(matches) > 0:\n# #                     normalized_country = matches[0].name.lower()\n# #             except:\n# # #                 print(coun, \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n# #                 not_found_list.append(coun)\n    \n#         else:\n#             not_found_list.append(coun_raw)\n\n#         normalized_country = \"_\".join(normalized_country.split(\" \"))\n# #         print(coun, '=>', normalized_country)\n# print('\\n', len(set(not_found_list)), \"countries not found...\")\n# print(\"time elapsed = \", time.time() - start_time, \"for\", len(author_coun_list), \"rows\")","097cbd27":"not_found_list = []\ntitles_normalized_countries = []\ncount_countries = []\n\nfor auth_coun in tqdm(df_titles[\"countries\"].values):\n    coun_list = [c.strip() for c in auth_coun.split(\",\")]\n    \n    pub_normalized_countries = []\n    for coun_raw in coun_list:    \n        coun = re.sub('[^A-Za-z\\s]+', '', coun_raw).strip()\n        us_state = us.states.lookup(coun)\n\n        if coun == \"UK\":\n            coun = \"United Kingdom\"\n        if coun == \"Ont\":\n            coun = \"Ontario\"\n        \n        fuzzy_country = process.extractOne(coun.lower(), COUNTRIES_LOOKUP.keys(), scorer=fuzz.token_set_ratio) \n        fuzzy_canada = process.extractOne(coun.lower(), CANADA_SUBDIVISIONS, scorer=fuzz.token_set_ratio) \n        fuzzy_india = process.extractOne(coun.lower(), INDIAN_STATES, scorer=fuzz.token_set_ratio) \n        fuzzy_japan = process.extractOne(coun.lower(), JAPAN_SUBS, scorer=fuzz.token_set_ratio) \n        fuzzy_australia = process.extractOne(coun.lower(), AUS_SUBS, scorer=fuzz.token_set_ratio) \n        fuzzy_china = process.extractOne(coun.lower(), CHINA_SUBS, scorer=fuzz.token_set_ratio) \n        fuzzy_britain = process.extractOne(coun.lower(), ENG_SUBS, scorer=fuzz.token_set_ratio) \n        fuzzy_colombia = process.extractOne(coun.lower(), COLOMBIA_SUBS, scorer=fuzz.token_set_ratio) \n        fuzzy_italy = process.extractOne(coun.lower(), ITALY_SUBS, scorer=fuzz.token_set_ratio) \n        fuzzy_espana_score = fuzz.token_sort_ratio(coun.lower(), \"espa\u00f1a\")\n        \n        normalized_country = 'lookup_match_failed'\n        if us_state or coun.lower() == \"usa\" or coun.lower() == \"united states\":\n            normalized_country = \"usa\"\n        \n        elif coun.lower() in COUNTRIES_LOOKUP:\n            normalized_country = coun.lower()\n        \n        elif coun.lower() == 'russia':\n            normalized_country = 'russian federation'\n        \n        elif coun.lower() == 'vietnam':\n            normalized_country = 'vietnam'\n        \n        elif coun.lower() == 'south korea':\n            normalized_country = 'south korea'\n        \n        elif coun.lower() == 'roc' or coun.lower() == 'pr china' or coun.lower() == 'prchina':\n            normalized_country = 'china'\n        \n        elif fuzzy_espana_score >= 85 or coun.lower() == \"sp\":\n            normalized_country = 'spain'\n        \n        elif coun in CAN_ABR or fuzzy_canada[1] >= 85:\n            normalized_country = 'canada'\n#             print(coun, \"================= fuzzy_canada ===================>\", normalized_country)\n        \n        elif fuzzy_country[1] >= 85:\n#             print(coun, \"================= fuzzy_country ===================>\", fuzzy_country[0].lower())\n            normalized_country = fuzzy_country[0].lower()\n        \n        elif fuzzy_india[1] >= 85:\n#             print(coun, \"================= fuzzy_india ===================>\", fuzzy_india[0].lower())\n            normalized_country = 'india'\n        \n        elif fuzzy_japan[1] >= 85:\n#             print(coun, \"================= fuzzy_japan ===================>\", fuzzy_japan[0].lower())\n            normalized_country = 'japan'\n        \n        elif coun in AUS_ABR or fuzzy_australia[1] >= 85:\n            normalized_country = 'australia'\n#             print(coun, \"================= fuzzy_australia ===================>\", normalized_country)\n        \n        elif fuzzy_china[1] >= 85:\n#             print(coun, \"================= fuzzy_china ===================>\", fuzzy_china[0].lower())\n            normalized_country = 'china'\n        \n        elif fuzzy_britain[1] >= 85:\n#             print(coun, \"================= fuzzy_britain ===================>\", fuzzy_britain[0].lower())\n            normalized_country = 'united kingdom'\n        \n        elif fuzzy_colombia[1] >= 85:\n#             print(coun, \"================= fuzzy_colombia ===================>\", fuzzy_colombia[0].lower())\n            normalized_country = 'colombia'\n        \n        elif fuzzy_italy[1] >= 85:\n#             print(coun, \"================= fuzzy_italy ===================>\", fuzzy_italy[0].lower())\n            normalized_country = 'italy'\n        \n#         elif len(coun) > 3:\n#             try:\n#                 matches = pycountry.countries.search_fuzzy(coun)\n#                 if len(matches) > 0:\n#                     normalized_country = matches[0].name.lower()\n#             except:\n# #                 print(coun, \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n#                 not_found_list.append(coun)\n    \n        else:\n            not_found_list.append(coun_raw)\n\n        normalized_country = \"_\".join(normalized_country.split(\" \"))\n        pub_normalized_countries.append(normalized_country)\n    \n    titles_normalized_countries.append(pub_normalized_countries)\n    count_countries.append(len(pub_normalized_countries))\n\nprint('\\n', len(set(not_found_list)), \"countries not found...\")","2419743a":"# Tally occurrences of words in a list\nfrom collections import Counter\n\n# print(len(set(not_found_list)))\n\ncnt = Counter()\nfor word in not_found_list:\n    cnt[word] += 1\n# cnt\nsorted(cnt.items(), key=lambda item: item[1], reverse=True)","824268d8":"df_titles['normalized_countries_count'] = count_countries\ndf_titles['normalized_countries'] = [' '.join(list(set(c))) for c in titles_normalized_countries]\ndf_titles['normalized_countries'].value_counts()","afaa6784":"df_titles['cord_uid'].head(5)","0d84b1f9":"from sklearn.feature_extraction.text import CountVectorizer\n\n# create a dataframe from a word matrix\ndef wm2df(wm, feat_names, cord_uids):\n    \n    # create an index for each row\n#     doc_names = ['Pub{:d}'.format(idx) for idx, _ in enumerate(wm)]\n    df = pd.DataFrame(data=wm.toarray(), index=cord_uids,\n                      columns=feat_names)\n    return(df)\n  \n# set of documents\ncorpora = [' '.join(list(set(c))) for c in titles_normalized_countries]\n\n# instantiate the vectorizer object\ncvec = CountVectorizer(lowercase=False, min_df=50)\n\n# convert the documents into a document-term matrix\nwm = cvec.fit_transform(corpora)\n\n# retrieve the terms found in the corpora\ntokens = cvec.get_feature_names()\n\n# create a dataframe from the matrix\ndf_country_vectors = wm2df(wm, tokens, df_titles['cord_uid'].values)\ndf_country_vectors.head()","2c78ca1c":"df_country_vectors.ix['i0zym7iq']","8cc4bc88":"import seaborn as sns\nimport matplotlib.pylab as plt\n%matplotlib inline\n\nno_of_pubs_by_country = df_country_vectors.sum()\nno_of_pubs_by_country.sort_values(ascending=False, inplace=True)\nno_of_pubs_by_country = no_of_pubs_by_country.drop(labels=['lookup_match_failed'])\nno_of_pubs_by_country = no_of_pubs_by_country.head(17)","12347db5":"ax = no_of_pubs_by_country.plot.bar(figsize=(12, 6), rot=45, title='# of titles published by top 15 countries')","6a55cb0e":"df_country_vectors[\"publish_date\"] = df_titles[\"publish_date\"].values\ndf_country_vectors['month'] = df_country_vectors[\"publish_date\"].dt.month\ndf_country_vectors['year'] = df_country_vectors[\"publish_date\"].dt.year\n\ntop_author_countries = list(no_of_pubs_by_country.index.values)\n# top_author_countries.remove('year')\n# top_author_countries.remove('month')\n\nyear_wise = df_country_vectors.groupby(['year']).sum()\nyear_wise = year_wise[top_author_countries]\n\nyear_wise = year_wise.ix[[2000.0, 2001.0, 2002.0, 2003.0, 2004.0,\n              2005.0, 2006.0, 2007.0, 2008.0, 2009.0, 2010.0, 2011.0, 2012.0,\n              2013.0, 2014.0, 2015.0, 2016.0, 2017.0, 2018.0, 2019.0\n]]\nyear_wise\nyear_wise.plot(figsize=(20, 6), rot=45, title='# of titles published by top 15 countries accross years')","598a085c":"df_country_vectors[\"publish_date\"] = df_titles[\"publish_date\"].values\ndf_country_vectors['month'] = df_country_vectors[\"publish_date\"].dt.month\ndf_country_vectors['year'] = df_country_vectors[\"publish_date\"].dt.year\n\nyear_wise = df_country_vectors.groupby(['year']).sum()\ncoun_wise = year_wise.T\ncoun_wise = coun_wise[[2000.0, 2001.0, 2002.0, 2003.0, 2004.0,\n              2005.0, 2006.0, 2007.0, 2008.0, 2009.0, 2010.0, 2011.0, 2012.0,\n              2013.0, 2014.0, 2015.0, 2016.0, 2017.0, 2018.0, 2019.0\n]]\ntop_author_countries = list(no_of_pubs_by_country.index.values)\n# top_author_countries.remove('year')\n# top_author_countries.remove('month')\n# top_author_countries.remove('usa')\n\ncoun_wise = coun_wise.ix[top_author_countries]\ncoun_wise.plot(figsize=(20, 8), rot=90, title='# of titles published by top 15 countries in different years')\nplt.xticks(range(0,len(coun_wise.index)), top_author_countries, rotation=90)","5f07eb4e":"# df_country_vectors.index = df_titles[\"publish_date\"].values\n# df_country_vectors = df_country_vectors.cumsum()\n# # df_country_vectors.head()\n# # df_country_vectors[['united_kingdom', 'italy']].plot(subplots=True, figsize=(6, 6)); plt.legend(loc='best')\n\n# fig, axes = plt.subplots(nrows=2, ncols=2)\n\n# df_country_vectors['united_kingdom'].plot(ax=axes[0,0]); axes[0,0].set_title('united_kingdom')\n# df_country_vectors['italy'].plot(ax=axes[0,1]); axes[0,1].set_title('italy')\n# df_country_vectors['usa'].plot(ax=axes[1,0]); axes[1,0].set_title('usa')\n# df_country_vectors['china'].plot(ax=axes[1,1]); axes[1,1].set_title('china')\n\n\n# # df_country_vectors[['united_kingdom', 'italy']].plot.box()","45cbf15e":"# plt.figure(figsize=(20, 5))\n# # g = sns.relplot(x=\"year\", y=\"china\", kind=\"line\", data=coun_grps_titles)\n# g = sns.lineplot(data=coun_grps_titles)\n# g.fig.set_figheight(5)\n# g.fig.set_figwidth(20)\n# plt.title('# of titles published over time')\n\n# g = coun_grps_titles[['year', 'united_kingdom', 'italy']].plot(x=\"year\", figsize=(20, 5), title='# of titles published over time for countries')","de70776f":"import gc\ngc.collect()","5c7cc036":"import sent2vec\n#model_path = \"\/kaggle\/input\/biosentvec\/BioSentVec_CORD19-bigram_d700.bin\"\nmodel_path = \"\/kaggle\/input\/covid-sent2vec-ver2\/BioSentVec_CORD19-bigram_d700_v2.bin\"\nmodel = sent2vec.Sent2vecModel()\ntry:\n    model.load_model(model_path)\nexcept Exception as e:\n    print(e)\nprint(\"model successfully loaded\")","96367884":"# Vaccines\nsub_tasks = [\n    \"What do we know about vaccines and therapeutics for COVID 19 or the novel coronavirus? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics for COVID 19 or the novel coronavirus?\",\n    \"What is the effectiveness of drugs being developed and tried (like chloroquine, arbidol, remdesivir, and favipiravir) to treat COVID-19 patients.\",\n    \"Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\",\n    \"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients for COVID 19 or the novel coronavirus.\",\n    \"Exploration of use of best animal models and their predictive value for a human vaccine.\",\n    \"Capabilities to discover a therapeutic for COVID 19 or the novel coronavirus, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\",\n    \"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up for COVID 19 or the novel coronavirus. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\",\n    \"Efforts targeted at a universal coronavirus vaccine.\",\n    \"Efforts to develop animal models and standardize challenge studies for COVID 19 or the novel coronavirus\",\n    \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers for COVID 19 or the novel coronavirus\",\n    \"Approaches to evaluate risk for enhanced disease after vaccination for COVID 19 or the novel coronavirus\",\n    \"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models in conjunction with therapeutics for COVID 19 or the novel coronavirus\"\n]\nsub_tasks_names = [\n    \"summary\",\n    \"drugs_efficacy\",\n    \"bench_trials\",\n    \"complication_ade\",\n    \"exploration_animal_models\",\n    \"therapeutics_efficacy\",\n    \"alternative_models\",\n    \"universal_coronavirus_vaccine.\",\n    \"develop_animal_models\",\n    \"prophylaxis_clinical_studies\",\n    \"risk_post_vaccination\",\n    \"vaccine_immune_response\"\n]","bccb8f16":"from scipy.spatial import distance\n\ndef create_abstract_task_embeddings():\n    abstract_vectors = []\n    paper_cord_uids = []\n    for cord_uid, abstract in tqdm(df_abstracts[[\"cord_uid\",\"paragraph\"]].values):\n        if not isinstance(abstract, str):\n            continue\n\n        sub_tasks_vectors = []\n        for sub_task in sub_tasks:\n            # prepare the individual vectors of paragraphs\n            embs = model.embed_sentences([abstract, sub_task])\n            cosine_sim = 1 - distance.cosine(embs[0], embs[1])\n            sub_tasks_vectors.append(cosine_sim)\n\n        # compute the max of vectors to get paper vectors\n        abstract_vectors.append(sub_tasks_vectors)\n        paper_cord_uids.append(cord_uid)\n\n    # store the paper vector into df\n    print(len(abstract_vectors), len(df_abstracts))\n    df_abstracts_vectors = pd.DataFrame(abstract_vectors, columns=sub_tasks_names)\n    \n    return df_abstracts_vectors\n\ndf_abstracts_vectors = create_abstract_task_embeddings()\n","973bcf9b":"df_abstracts_vectors.describe()","a74283e4":"plt.figure(figsize=(20, 5))\nsns.boxplot(data=df_abstracts_vectors)\nplt.xticks(rotation=45)","5a7d9cb1":"df_rounded = df_abstracts_vectors.round(0).astype(pd.Int32Dtype())\ndf_rounded[\"date\"] = df_abstracts.publish_date.values\ndf_rounded['month'] = df_rounded[\"date\"].dt.month\ndf_rounded['year'] = df_rounded[\"date\"].dt.year\n\n# Summarize the number of publications\ngroups_abs_vects = df_rounded.groupby(['year', 'month']).sum()\ngroups_abs_vects = groups_abs_vects.reset_index()","23a6a747":"plt.figure(figsize=(20, 5))\ng = sns.relplot(x=\"year\", y=\"universal_coronavirus_vaccine.\", kind=\"line\", data=groups_abs_vects)\ng.fig.set_figheight(5)\ng.fig.set_figwidth(20)\nplt.title('# of publications on \"Efforts targeted at a universal coronavirus vaccine\" over time')","f3140f78":"g = sns.relplot(x=\"year\", y=\"alternative_models\", kind=\"line\", data=groups_abs_vects)\ng.fig.set_figheight(5)\ng.fig.set_figwidth(20)\nplt.title('# of publications on \"Alternative models to prioritize and distribute scarce, newly proven therapeutics\" over time')","a1c1035a":"def display_time_plots(sub_task_name):\n    plt.figure(figsize=(20, 5))\n    g = sns.relplot(x=\"year\", y=sub_task_name, kind=\"line\", data=groups_abs_vects);\n    g.fig.set_figheight(5)\n    g.fig.set_figwidth(20)\n    plt.title('# of publications over time')","730d3f3a":"# # df_abstracts_indexed = df_abstracts.set_index(df_abstracts.publish_date)\n# df_tsplot = df_abstracts_vectors.copy()\n# # df_tsplot[\"date\"] = df_abstracts.publish_date\n\n# plt.figure(figsize=(20,8))\n\n# # This plot shows the confidence level of the match between each sub_task and related content\n# g = sns.boxplot(data=df_tsplot)\n# plt.xticks(rotation=45)\n# # g = sns.pairplot(df_tsplot)\n\n# # g = sns.relplot(x=\"date\", y=\"summary\", kind=\"line\", data=df_tsplot)\n\n# # def display_correlation_plots_abstracts():\n    \n# #     corr = df_abstracts_vectors.corr()\n# #     ax = sns.heatmap(\n# #         corr, \n# #         vmin=-1, vmax=1, center=0,\n# #         cmap=sns.diverging_palette(20, 220, n=200),\n# #         square=True\n# #     )\n# #     ax.set_xticklabels(\n# #         ax.get_xticklabels(),\n# #         rotation=45,\n# #         horizontalalignment='right'\n# #     );\n\n# #     g = sns.pairplot(corr)","b63614e9":"def generate_title_embeddings():\n    title_dict = {}\n    titles_list = []\n    title_covid_tags = []\n    for cord_uid, title in tqdm(df_titles[[\"cord_uid\",\"title\"]].values):\n        if isinstance(title, str):\n            title_dict[cord_uid] = model.embed_sentence(title)\n            titles_list.append(title)\n            title_covid_tags.append(covid_tags(title))\n\n    paper_ids = list(title_dict.keys())\n    t_vectors = np.array(list(title_dict.values()))\n    nsamples, x, y = t_vectors.shape\n    titles_array = t_vectors.reshape((nsamples,x*y))\n    print(titles_array.shape)\n    \n#     print(\"Computing cosine similarity matrix.....\")\n#     cosine_sim_matrix = cosine_similarity(titles_array, titles_array)\n    \n#     n_sim_articles = 3\n#     input_idx = 1\n\n#     sim_indexes = np.argsort(cosine_sim_matrix[input_idx])[::-1][1:n_sim_articles+1]\n#     print(\"sim_indexes\", sim_indexes)\n\n#     print(\"-------QUERY TITLE-----\")\n#     print(titles_list[input_idx])\n#     print()\n#     print(f\"----TOP {n_sim_articles} SIMILAR TITLES-----\")\n#     for sim_idx in sim_indexes:\n#         print(titles_list[sim_idx])\n#         print()\n#         print(\"-\"*50)\n    \n    \n    return paper_ids, titles_array, titles_list, title_covid_tags\n\npaper_ids, titles_array, titles_list, title_covid_tags = generate_title_embeddings()","81b27aa5":"def summarize_paragraph(para_text):\n    doc = medical(para_text)\n    \n    entities_list = []\n    sent_dict = {}\n    sent_list = []\n    for sent in doc.sents:\n        sent_dict[sent.text] = model.embed_sentence(sent.text)\n        sent_list.append(sent.text)\n        entities_list.extend([ent.text for ent in doc.ents])\n\n    s_vectors = np.array(list(sent_dict.values()))\n    nsamples, x, y = s_vectors.shape\n    sent_vectors = s_vectors.reshape((nsamples,x*y))\n    cosine_sim_matrix_sents = cosine_similarity(sent_vectors, query_vector.reshape(1,-1))\n\n    if len(sent_list) > 30:\n        no_of_sents_summ = int(len(sent_list) * 0.1)\n    elif len(sent_list) > 20:\n        no_of_sents_summ = int(len(sent_list) * 0.2)\n    elif len(sent_list) > 10:\n        no_of_sents_summ = int(len(sent_list) * 0.3)\n    elif len(sent_list) > 5:\n        no_of_sents_summ = int(len(sent_list) * 0.4)\n    else:\n        no_of_sents_summ = len(sent_list)\n\n    sent_sim_indexes = np.argsort(cosine_sim_matrix_sents.reshape(1,-1)[0])[::-1][:no_of_sents_summ]\n    sents_summ = [sent_list[j] for j in sent_sim_indexes]\n    return \" \".join(sents_summ), entities_list","97c889b2":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1)\n\ndef get_documents_clusters_by_titles(query_statement, no_of_docs=30):\n    is_COVID = covid_tags(query_statement)\n    \n    display(Markdown(\"# Query : \"+query_statement))\n    \n    # extract the cosine similarity of query against titles\n    title_sim_matrix_query = cosine_similarity(titles_array, query_vector.reshape(1,-1))\n    titles_similarities = title_sim_matrix_query.reshape(1,-1)[0]*100\n    \n    df_publications = pd.DataFrame(list(zip(paper_ids, titles_list, titles_similarities, title_covid_tags)), \n               columns =['cord_uid', 'title', 'title_similarity', 'tag'])\n    df_publications = df_publications.sort_values(by='title_similarity', ascending=False)\n    \n    if is_COVID == 'COVID-19':\n        df_publications = df_publications.loc[df_publications['tag'] == 'COVID-19']\n    \n    if df_publications['title_similarity'].max() > 80:\n        df_shortlisted = df_publications.loc[df_publications['title_similarity'] >= 65]\n    elif df_publications['title_similarity'].max() > 75:\n        df_shortlisted = df_publications.loc[df_publications['title_similarity'] >= 60]\n    elif df_publications['title_similarity'].max() > 70:\n        df_shortlisted = df_publications.loc[df_publications['title_similarity'] >= 55]\n    else:\n        df_shortlisted = df_publications.loc[df_publications['title_similarity'] >= 50]\n    \n    display(Markdown(\"Selected top \"+ str(len(df_shortlisted)) + \" publications\"))\n#     display(df_shortlisted[['paper_id', 'title', 'title_similarity']])\n    \n    return df_shortlisted['cord_uid'].values","c60f4e56":"def get_documents_clusters_by_titles_n_abstracts(query_statement, no_of_docs=30):\n    is_COVID = covid_tags(query_statement)\n    \n    display(Markdown(\"**Query : \"+query_statement+\"**\"))\n    \n    # extract the cosine similarity of query against abstracts\n    abstract_sim_matrix_query = cosine_similarity(values_array, query_vector.reshape(1,-1))\n    abstracts_similarities = abstract_sim_matrix_query.reshape(1,-1)[0]*100\n    \n    # extract the cosine similarity of query against titles\n    title_sim_matrix_query = cosine_similarity(titles_array, query_vector.reshape(1,-1))\n    titles_similarities = title_sim_matrix_query.reshape(1,-1)[0]*100\n    \n    df_shortlisted = pd.DataFrame(list(zip(paper_ids, titles_list, titles_similarities, abstracts_list, \n                                           abstracts_similarities, title_covid_tags)), \n               columns =['paper_id', 'title', 'title_similarity', 'abstract', 'abstract_similarity', 'tag'])\n    df_shortlisted = df_shortlisted.sort_values(['title_similarity', 'abstract_similarity'], ascending=(False, False))\n    \n    if is_COVID == 'COVID-19':\n        df_shortlisted = df_shortlisted.loc[df_shortlisted['tag'] == 'COVID-19']\n    \n    df_shortlisted[\"Relevance\"] = (df_shortlisted['title_similarity']+df_shortlisted['abstract_similarity'])\/2\n    df_shortlisted = df_shortlisted.sort_values(by='Relevance', ascending=False)\n    df_shortlisted = df_shortlisted.loc[df_shortlisted['Relevance'] >= 45]\n    display(Markdown(str(len(df_shortlisted)) + \" publications shortlisted. Selecting top 30:\"))\n    df_shortlisted = df_shortlisted.head(no_of_docs)\n    \n    shortlisted_abstracts = df_shortlisted[\"abstract\"].values\n    abs_summ = []\n    for para in shortlisted_abstracts:\n        abs_summ.append(summarize_paragraph(para))\n    \n    df_shortlisted[\"abs_summary\"] = abs_summ\n    \n    display(df_shortlisted[['Relevance','title', 'title_similarity', 'abs_summary', 'abstract_similarity']])\n    return df_shortlisted['paper_id'].values","b740f13c":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nstopwords = set(STOPWORDS)\nstopwords.update([\"SARS\", \"COVID\", \"et al\", \"patient\", \"will\", 'may'])\n\ndef depict_word_cloud(paragraphs, query_name):\n    text = \" \".join(paragraphs)\n    \n    wordcloud = WordCloud(stopwords=stopwords, width = 3600, height = 2400, min_font_size = 10, \n                          background_color=\"white\").generate(text)\n    \n    # Display the generated image:\n    plt.figure(figsize=(15,10))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n    \n    wordcloud.to_file(\"\/kaggle\/working\/\"+ query_name +\"_wordcloud.png\")","26b06f3c":"def get_paragraphs_cluster(sub_task, paper_ids, no_of_paras=10):\n    is_COVID = covid_tags(sub_task)\n\n    df_paragraphs = pd.DataFrame()\n    body_para_list = []\n    for paper_id in paper_ids:\n        try:\n            body = paper_bodies.get_group(paper_id)\n        except:\n            continue\n        body[\"section_id\"] = (pd.RangeIndex(stop=body.shape[0])+1)\/len(body)*100\n        df_paragraphs = df_paragraphs.append(body, ignore_index = True) \n        body_para_list.extend(body[\"paragraph\"].values)\n    \n    para_dict = {}\n    para_list = []\n    para_covid_tags = []\n    p = 0\n    for para in body_para_list:\n        para_dict[p] = model.embed_sentence(para)\n        para_list.append(para)\n        para_covid_tags.append(covid_tags(para))\n        p += 1\n\n    # Para level vectors\n    p_vectors = np.array(list(para_dict.values()))\n    nsamples, x, y = p_vectors.shape\n    para_vectors = p_vectors.reshape((nsamples,x*y))\n\n    para_matrix_query = cosine_similarity(para_vectors, query_vector.reshape(1,-1))\n    para_similarities_array = para_matrix_query.reshape(1,-1)[0]\n    \n    df_paragraphs['tag'] = para_covid_tags\n    df_paragraphs[\"para_similarity\"] = para_similarities_array*100\n    \n    if is_COVID == 'COVID-19':\n        df_paragraphs = df_paragraphs.loc[df_paragraphs['tag'] == 'COVID-19']\n    \n#     df_paragraphs = df_paragraphs.loc[df_paragraphs['para_similarity'] >= 50]\n#     df_paragraphs[\"Relevance\"] = (2*df_paragraphs['para_similarity']+df_paragraphs['section_id'])\/3\n    \n    df_paragraphs = df_paragraphs.sort_values(by=['para_similarity', 'publish_date'], ascending=(False, False))\n    \n    if len(df_paragraphs) > 15:\n        df_paragraphs = df_paragraphs.head(15)\n    \n    display(Markdown(str(len(df_paragraphs)) + \" paragraphs shortlisted.\"))\n#     display(df_paragraphs[['publish_date', 'title', 'paragraph', 'para_similarity']])\n    return df_paragraphs","4811870a":"def comprehend_paragraph(para_text):\n    doc = medical(para_text)\n        \n    sent_dict = {}\n    sent_list = []\n    for sent in doc.sents:\n        sent_dict[sent.text] = model.embed_sentence(sent.text)\n        sent_list.append(sent.text)\n\n    s_vectors = np.array(list(sent_dict.values()))\n    nsamples, x, y = s_vectors.shape\n    sent_vectors = s_vectors.reshape((nsamples,x*y))\n    sent_sim_matrix_sents = cosine_similarity(sent_vectors, query_vector.reshape(1,-1))\n    sents_similarities = sent_sim_matrix_sents.reshape(1,-1)[0]*100\n    \n    df_sents = pd.DataFrame(list(zip(sent_list, sents_similarities)), columns =['sentence', 'similarity'])\n    df_sents = df_sents.sort_values(by='similarity', ascending=False)\n    df_sents = df_sents.loc[df_sents['similarity']>50]\n    \n    return ' '.join(df_sents['sentence'].values), df_sents['similarity'].max()","b236f49b":"from sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial import distance\nfrom IPython.display import display, Markdown\nno_of_docs = 30\nno_of_paras = 10\n\n# load the medical model for NER\nmedical = en_core_sci_sm.load()\n\n# iterate the sub-task queries\nfor sub_task_id, sub_task in enumerate(sub_tasks):  \n    # compute embedding of sub-task\n    query_vector = model.embed_sentence(sub_task)\n    selected_paper_ids = get_documents_clusters_by_titles(sub_task, no_of_docs)\n    \n#     display_time_plots(sub_tasks_names[sub_task_id])\n    \n    df_paragraphs = get_paragraphs_cluster(sub_task, selected_paper_ids, no_of_paras)\n\n    shortlisted_body_paras = df_paragraphs[\"paragraph\"].values\n    para_summ = []\n    entities_list = []\n    for para in shortlisted_body_paras:\n        summ, para_entities = summarize_paragraph(para)\n        para_summ.append(summ)\n        entities_list.append(\" \".join(para_entities))\n    \n    depict_word_cloud(entities_list, sub_tasks_names[sub_task_id])\n    \n    df_paragraphs[\"para_summary\"] = para_summ\n    df_paragraphs.to_csv(\"\/kaggle\/working\/subtask_\"+sub_tasks_names[sub_task_id]+\"_answers.csv\", index=False)\n    display(Markdown(\"**Query : \"+sub_task+\"**\"))\n\n    if len(df_paragraphs) == 0:\n        display(Markdown(\"No answer\"))\n    else:\n        \n        t= pd.to_datetime(str(df_paragraphs['publish_date'].values[0])) \n        timestring = t.strftime('%Y.%m.%d')\n        display(Markdown(\"***\"+ df_paragraphs['title'].values[0] + \" (\" + timestring + \"):***\"))\n        display(Markdown(\"> \"+ df_paragraphs['para_summary'].values[0] +\"**\"))\n        display(df_paragraphs[['publish_date', 'title', 'para_summary', 'para_similarity']])","d0a6d631":"# Title embeddings & Plots:","e6be91aa":"# **Approach:**\n\nWe have trained the sent2vec model on the most recent dump of CORD-19 corpus to generate sentence embeddings. This model is trained on 14658255 sentences and 864320 words. **Fasttext embeddings are huge in size so we could not generate this in the working directory of the kernel.** We trained our model on AWS and uploaded the same as additional dataset.","b98aafef":"Iterate JSON directories to load all the paragraphs from abstracts and body of different papers. Abstracts if available in metadata.csv is also loaded separately.","750000a0":"**Load FastText sent2vec model trained on CORD-19 corpus**","32a76258":"# Install and set-up dependencies","c5ddb65f":"**We can note that there has been special interest after the occurence of the first outbreak of SARS in early 2000 and thereafter number of published titles have increased considerably**","ae84a14b":"**Let's analyze the available abstracts with respect to the queries for this Task:**","50f898b9":"# Benefits of this approach:\n\n* Very simple and straightforward approach without complex indexing and other dependencies which need to be set up\n* Based on FastText Sent2Vec embeddings trained on CORD-19 corpus - embeddings are contextually relevant to the queries\n* Filtering helps remove lots of non-covid content\n* Text summarization helps to summarize longer paragraphs\n\n# Further Improvements possible:\n* Embeddings are currently computed on entire paragraphs rather than at sentence level. Aggregating sentence level embeddings at paragraph level would be more relevant\n* Currently only looks for answers in papers which have abstracts available in the metadata.csv. Approach needs to be further scaled to entire literature\n* Disease names can be further extracted using spacy medical NER models that can add further structure to the response","ae8a7602":"**Add the extracted normalized author countries back to titles data frame:**","3ccaf63f":"# **Goal for Round 2: ** \n\nThis notebook tries to discover better & meaningful answers and visualizations around the following questions:\n\n**What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?**\n\n**Effectiveness of drugs being developed and tried to treat COVID-19 patients.**\n\n**Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.**\n\n**Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.**\n\n**Exploration of use of best animal models and their predictive value for a human vaccine.**\n\n**Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.**\n\n**Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.**\n\n**Efforts targeted at a universal coronavirus vaccine.**\n\n**Efforts to develop animal models and standardize challenge studies**\n\n**Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers**\n\n**Approaches to evaluate risk for enhanced disease after vaccination**\n\n**Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models in conjunction with therapeutics**","31b769da":"**The sentence embeddings extracted for available abstracts in metadata.csv are compared with the several smaller questions around this particular tasks. The following box plot shows the match of the literature around these questions, kind off measuring the confidence levels of the answers: **\n\nSome insights from the plot:\n* **universal_coronavirus_vaccine** has the lowest mean confidence. \n    This means that for the question *\"Efforts targeted at a universal coronavirus vaccine.\"* the answers may not be too relevant\n* **alternative_models** has the highest mean confidence.\n    This means that for the question *\"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up for COVID 19 or the novel coronavirus. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\"* the answers will hopefully be more relevant","fdff8ce5":"**In the above plot we can see USA easily leads the race in research. It has particularly picked up around 2003, which was the first SARs outbreak.**","0333357e":"# Abstract embeddings & Plots:","34ac6c23":"Load metadata abstracts & body paragraphs into Dataframes","afc43d73":"# Number of Titles published over time","5e19ffa9":"**Let's look at the number of published titles on \"Efforts targeted at a universal coronavirus vaccine\" with time:**","99ff7a7c":"**The above plot again tries to show the RELATIVE research done by different countries in the twentieth century. We have dropped the USA since it becomes difficult to visualize the other countries!**","9a133734":"**We can note that there have been spurts in the research with time.**\n\n**Now let's look at the number of published titles on \"Alternative models to prioritize and distribute scarce, newly proven therapeutics\" with time:**","cf8ad749":"# Extract Abstract & Body paragraphs from JSON","f0162b8d":"**Compute the Bag-Of_words for the extracted countries using CountVectorizer:**","40be1659":"# Sub-task wise paragraph clusters, pre-processing & medical NER extraction, histograms:","c8d316ab":"# Sub-task Queries","7ee25b9f":"# **About Us:**\nWe are a group of AI and NLP scientists with experience across NLP, image processing and computer vision. Covid-19 Kaggle challenge has provided us with a unique opportunity to help humanity fight the corona virus pandemic collectively by utilizing benefits of AI and NLP. We have focused on creating NLP solution to enable users to ask questions and get the most accurate results from the vast corpus of medical journals.\n","e7bc9b07":"**Countries that were not found from look up:**","e67b2700":"# Extraction of author countries:","36cd49c9":"![sent2vec.PNG](attachment:sent2vec.PNG)","2d62def7":"# Visualizing research in different countries on epidemics and pandemics:","e60edd57":"![image.png](attachment:image.png)","fd718245":"# Load the Input JSON files & metadata.csv","801d0556":"# Sub-task wise document clusters & statistics (by license, author location, date, Title word cloud):"}}