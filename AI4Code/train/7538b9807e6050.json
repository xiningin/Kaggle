{"cell_type":{"64a8b278":"code","4f3d26c5":"code","9b315c5d":"code","85959114":"code","4dfd26b2":"code","1cddd359":"code","21e658f6":"code","cbb04c90":"code","698f316e":"code","724505c8":"code","966fde47":"code","27c7f0a0":"code","2f24890f":"code","2c47b2bd":"code","f770d6f8":"code","caecfbcc":"code","4a9ac596":"code","e766a847":"code","5682559b":"code","d4873b0e":"code","0e9b9b72":"code","ac288d0e":"markdown","d564e0bc":"markdown","ea95de82":"markdown","470cd2ce":"markdown","11032933":"markdown","27d9eab9":"markdown","5939bb79":"markdown","a759e6c3":"markdown","ada5d9f7":"markdown","fb23dea8":"markdown","759634e8":"markdown","25cee997":"markdown","f4a9657e":"markdown","bd857236":"markdown","039d05ee":"markdown"},"source":{"64a8b278":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f3d26c5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9b315c5d":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',usecols=['Age','Fare','Survived'])","85959114":"data.head()","4dfd26b2":"plt.figure(figsize=(10,7))\nsns.distplot(data['Age'])\nplt.show()","1cddd359":"# with the help of histogram\nfigure = data['Age'].hist(bins=50)\nfigure.set_title('dist of Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('Age')\nplt.show()","21e658f6":"# with the help of boxplot\nfigure = data.boxplot(column='Age')\nplt.show()","cbb04c90":"data[data['Age']>70]","698f316e":"df = data.copy()\n\nIQR = df['Age'].quantile(0.75) - df['Age'].quantile(0.25)\n\nlower = df['Age'].quantile(0.25) - 1.5* IQR\nupper = df['Age'].quantile(0.75) + 1.5* IQR\n\noutliers = np.where(df['Age']>upper,True, np.where(df['Age']<lower,True,False))\n\ndf = df.loc[~(outliers)]","724505c8":"# or you can write in this way too..\ndf = df[~((df['Age']<lower) & (df['Age']>upper))]\n#df","966fde47":"## assuming Age is normally distributed\n# calculate the lower and upper boundary using IQR\nupper_boundary = data['Age'].mean() + 3*data['Age'].std()\nlower_boundary = data['Age'].mean() - 3*data['Age'].std()\nprint(upper_boundary)\nprint(lower_boundary)","27c7f0a0":"# here age cannot be negative so it's simple that we will impute the outliers with the upper boundary.\ndf = data.copy()\ndf['Age'] = np.where(df['Age']>upper_boundary, upper_boundary,data['Age'])","2f24890f":"figure = df['Age'].hist(bins=50)\nfigure.set_title('dist of Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('Age')\nplt.show()","2c47b2bd":"# thsi is also similar as above, here we use 1.5\nIQR = data['Age'].quantile(0.75) - data['Age'].quantile(0.25)\nIQR","f770d6f8":"lower = data['Age'].quantile(0.25) - IQR * 1.5\nupper = data['Age'].quantile(0.75) + IQR * 1.5","caecfbcc":"df = data.copy()\ndf['Age'] = np.where(df['Age']>upper,upper, np.where(df['Age']<lower,lower,data['Age']))\n","4a9ac596":"figure = df['Age'].hist(bins=50)\nfigure.set_title('dist of Age')\nfigure.set_xlabel('Age')\nfigure.set_ylabel('Age')\nplt.show()","e766a847":"sns.distplot(df['Age'])","5682559b":"# for Extreme Outliers use this boundaries.\nlower_bridge = data['Age'].quantile(0.25) -  3 * IQR\nupper_bridge = data['Age'].quantile(0.75) +  3 * IQR\nprint(lower_bridge)\nprint(upper_bridge)","d4873b0e":"lower = 1\nupper = 71\ndf = data.copy()\ndf['Age'] = np.where(df['Age']>upper,upper, np.where(df['Age']<lower,lower,data['Age'])) ","0e9b9b72":"df = data.copy()\nlower = df['Age'].quantile(0.10)\nupper = df['Age'].quantile(0.90)\nprint(lower)\nprint(upper)","ac288d0e":"**Thank you guys so much following this notebook upto here, if you find something interesting and useful then please upvote this notebook and help others. It will be much more motivation for me to be moving ahead with my journey. If you find any corrections or something wrong in my interpretation then I am open to all the corrections. please use the comment section**","d564e0bc":"## *B) CENSORING*\n**Censoring (or capping) means setting the maximum and\/or the minimum of the distribution at an arbitrary value.**\n\n**Key-Points**\n1. it does not remove any data\n2. It distorts the shape of distribution\n\n**The number by which we can Cap the distribution can be decided by the help of various methods as discussed**","ea95de82":"* the difference in the outliers can be easily be seen from upper graph and this.","470cd2ce":"## Outliers\n**Outliers are the data point which are significantly different from the other remaining data. outliers are in very less amount but had a severe effect on the performance of the Machine learning model. Thus, it is necessary to detect and handle the outliers correctly.**","11032933":"**OBSERVATION**\n* We can clearly see that outliers are there from both histogram as well boxplot is clearly visible\n* mean age is near 29, 25th quantile lying at 20, 50 at 29 and 75th at 38. and above 68 it's giving us a outliers\n* let's us expand the techniques to handle the outliers when variable is normally distributed as well when it's skewed","27d9eab9":"### 4) Using Quantiles\n**This method is less used and quite similar to Arbitrarily handling the outliers. In this method you choose the quantile range what you want at upper and lower boundary and use this**","5939bb79":"### Detecting Outliers\n**There are several techniques to detect the outliers. In which most important and generally used technique to detect outliers is**\n* Using Visualization plots such as boxplot and scatter plot\n* using the normal Distribution (mean and std dev).","a759e6c3":"### 1)Gaussian Approximation**\n**If data is normally distributed then we can use this**","ada5d9f7":"### 2) Inter-quantal range proximity rule\n**In this the boundaries are determined with the help of IQR**","fb23dea8":"### 3) Arbitrarily\n**It is based on your choice of requirements. you can choose the values of lower and upper as per your need and impute it**","759634e8":"## Methods to Handle the outliers.\n1. Trimming :- trimming simply means removing the outliers and get rid of it. but this technique is not much useful because we can loose some useful information by doing so.\n\n2. Censoring(Capping) :- It means setting the variable distribution at maximum and minimum values. it means values which are biigger means far from range are replaced from some maximum value within range and value in negative axis replaces by minimum value.\n* there are various ways of performing Capping by which we will talking is this forward section.\n\n3. Imputation :- In this Technique the Outliers are treated as Missing values and we apply Missing value Imputation Technique on them to bring in a particular range.","25cee997":"* here age has approximat normal distribution but then also we can see some of the outliers above 80. let's check","f4a9657e":"**For More Please Visit:**\n\n**Techniques for missing data Imputation and categorical encoding**\n https:\/\/www.kaggle.com\/rxsraghavagrawal\/all-technique-for-missing-imputation-cat-encoding\/\n \n**Feature Transformation And Feature Scaling Techniques:**\nhttps:\/\/www.kaggle.com\/rxsraghavagrawal\/feature-transformation-feature-scaling\/\n\n**Thank You**","bd857236":"## *A) TRIMMING*\n**Trimming merely means removing the outliers from our dataset. Only we need here is to decide the metric from which we have to remove the outliers**\n\n**Key-Points**\n* It is straight forward method to impliment\n* fast and easy\n* whenever we have some large dataset and less outliers then we can easily use this(eg-> more then 1000 rows and only 5-10 outliers then you can remove).","039d05ee":"## *C) IMPUTATION*\n**Another technique to deal with outliers to assume them as Missing data and use Missing Imputation technique on them. If you did like to explore this technique then please check my previous notebook.\nhttps:\/\/www.kaggle.com\/rxsraghavagrawal\/all-technique-for-missing-imputation-cat-encoding\/"}}