{"cell_type":{"1c73d7f6":"code","6971f4e7":"code","987c037f":"code","c5c1d20d":"markdown","13f623cd":"markdown"},"source":{"1c73d7f6":"from gensim.models import Word2Vec\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot","6971f4e7":"# define training data\nsentences = [\n    ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n    ['this', 'is', 'the', 'second', 'sentence'],\n    ['yet', 'another', 'sentence'],\n    ['one', 'more', 'sentence'],\n    ['and', 'the', 'final', 'sentence']\n]\n\n# train model\nmodel = Word2Vec(sentences, size=100, window=5, min_count=1, workers=3, sg=0)\n# summarize the model\nwords = list(model.wv.vocab)\nprint(words)\n\n# access vector for one word\nprint(model['sentence'].shape)\n\n# save the model\nmodel.save('tutorial_embedding.bin')\n\n# load the model\nnew_model = Word2Vec.load('tutorial_embedding.bin')\nprint(new_model)","987c037f":"# retrieve all of the vector from the trained model\nX = model[model.wv.vocab]\n\n# create a 2-dimensional PCA model of the word vectors\npca = PCA(n_components=2)\nresult = pca.fit_transform(X)\n\n# set figure size of the graph\npyplot.figure(figsize=(10,6))\n# pulling out the two dimensions as x and y coordinates\npyplot.scatter(result[:, 0], result[:, 1])\n\n# annotate the points on the graph with the words themselves\nwords = list(model.wv.vocab)\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i,0], result[i,1]))\n    \npyplot.show()","c5c1d20d":"### Word Embedding\nWord Embedding is the numerical representation of text where words with similar semantics have similar representation.\n\n### Develop Word2Vec Embedding\n\nParameters\n- **size:** (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token(word)\n- **window:** (default 5) The maximum distance between a target word and words around the target words.\n- **min_count:** (default 5) The minimum count of words to consider when training the model; words with an occurence less than this count will be ignored\n- **workers:** (default 3) The number of threads to use while training.\n- **sg:** (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)\n\nThe defaults are often good enough when just getting started. If you have lot of cores, as most modern computers do, I strongly encourage you to increase workers to match the number of cores.","13f623cd":"### Visualize Word Embedding"}}