{"cell_type":{"6305be4d":"code","0e8db01c":"code","898efb81":"code","ca58600e":"code","8cf5c269":"code","050cdebd":"code","1ffcb09b":"code","548dbef0":"code","e0fa0798":"code","0f0ba05d":"code","cc054229":"markdown","76943aec":"markdown","945761e2":"markdown","ab80a62d":"markdown","55a4559e":"markdown","87bceb4c":"markdown","49ba35c5":"markdown","e0b7a076":"markdown"},"source":{"6305be4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e8db01c":"# Import libraries and modules modules.\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport pandas as pd","898efb81":"file = pd.read_csv(\"..\/input\/nips-2015-papers\/Papers.csv\")\ncommon_texts = file[\"Abstract\"]\ncommon_texts","ca58600e":"common_texts = [word_tokenize(sw_removed.lower()) for sw_removed in common_texts if not sw_removed in stopwords.words()]\ncommon_texts[0][:5]","8cf5c269":"# Tagged documents are input for doc2vec model. \ntagged_data = []\nfor i, doc in enumerate(common_texts):\n    tagged = TaggedDocument(doc, [i])\n    tagged_data.append(tagged)\n\ntagged_data[0]","050cdebd":"max_epochs = 100\nvec_size = 20\nalpha = 0.025\n\nmodel = Doc2Vec(vector_size=vec_size,\n               alpha=alpha, \n               min_alpha=0.00025,\n               min_count=1,\n               dm=1)\n\nmodel.build_vocab(tagged_data)\n\nfor epoch in range(max_epochs):\n    print('iteration{0}'.format(epoch))\n    model.train(tagged_data,\n                total_examples=model.corpus_count,\n                epochs=model.epochs)\n    \n    # Decrease the learning rate\n    model.alpha -= 0.0002\n    \n    # fix the learning rate, no decay\n    model.min_alpha = model.alpha\n\nmodel.save(\"articles.model\")\nprint(\"Model Saved\")","1ffcb09b":"from gensim.models.doc2vec import Doc2Vec\nfrom nltk.tokenize import word_tokenize\n\nmodel = Doc2Vec.load(\".\/articles.model\")","548dbef0":"# Create new sentence and vectorize it. \nnew_sentence = \"this is a new sentence\".split(\" \")\nnew_sentence_vectorized = model.infer_vector(new_sentence)\n\n# Calculate cosine similarity. \nsimilar_sentences = model.docvecs.most_similar(positive=[new_sentence_vectorized])","e0fa0798":"similar_sentences","0f0ba05d":"# Output\noutput = []\nfor i, v in enumerate(similar_sentences):\n    index = v[0]\n    output.append([common_texts[index], v[1]])\n\npd.DataFrame(output, columns=[\"common_texts\", \"cosine_similarity\"])","cc054229":"## Training the model\nOnce we have data ready for our model, we can start training it. We will create an object which is in this case 'model' and we will provide hyperparameters to it. You can tweak with these hyperparameters to enhance the efficiency of your model. Lastly, we will save our model because it can be cumbersome to train the model and yes you can deploy your model using that saved file.","76943aec":"Lastly, we will tag our data. Tagging data means each sentence or document (you can say) is mapped with a unique index. This tagged data will be the input for our model. ","945761e2":"## The Results\nTo visualize our results we will use pandas to convert the output to a data frame so it will make our output pretty much meaningful.","ab80a62d":"Now let's add a sentence and infer it to a vector in order to feed it to our model. ","55a4559e":"## Evaluating our model\nSo now we have trained our model it's time to see our model in action. We will be using cosine similarity measure for evaluating our model. Let's import our libraries and modules again and load our model that we saved. ","87bceb4c":"## Data Pre-processing\nTo use this data we need to pre-process it first. What do I mean with pre-processing? Well in this case we will convert all the words to lowercase, remove stop words and tokenize it. To remove stopwords we will be using NLTK and you might have guessed while importing libraries and modules.","49ba35c5":"## Getting Started\n\nWe will build an article recommender by using Gensim Doc2Vec so without further ado let's get started. First of let us do some housekeeping for our Article Recommender by importing python libraries","e0b7a076":"## Conclusion\nThat's pretty much it for this tutorial. Now you have your model that you can deploy and make more useful projects. You are welcome to share your thoughts!!!"}}