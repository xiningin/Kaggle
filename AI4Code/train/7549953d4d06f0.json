{"cell_type":{"f3e8b3c6":"code","f0ff0380":"code","36575a6e":"code","ea2f3150":"code","6ed9d823":"code","57aa8c1c":"code","0b3351df":"code","634301a1":"code","b3314669":"code","de36a21a":"code","c000a17e":"code","e3dac250":"code","77a3ee9b":"code","fc132160":"code","8b2557c6":"code","d32a7dae":"code","796f8f42":"code","6159469a":"code","00b03db5":"code","b8f8500c":"code","4da317ce":"code","fc4717ed":"code","78b9e6b8":"code","00c61374":"code","14f99264":"code","6c319a68":"code","824ba720":"code","eda1502b":"code","3396a5b9":"code","e3f2b38f":"code","80974c9a":"code","ff976be5":"code","4fcabc3f":"markdown","83d4d4ee":"markdown","fa7ed7bc":"markdown","af55f23a":"markdown","0dd3a178":"markdown","98bfe334":"markdown","16a24f69":"markdown","e2733875":"markdown","988a37c4":"markdown","176f07ce":"markdown","c49fd46f":"markdown","f3554c1f":"markdown","75c7fae9":"markdown","ed1b1bdc":"markdown","cb4a434d":"markdown","7a2e0019":"markdown","4052c06f":"markdown","d84b0e31":"markdown","b66f06b2":"markdown","b931d2c6":"markdown","cd43370a":"markdown","75350729":"markdown","ceeb5034":"markdown","1d914d20":"markdown","5fde4858":"markdown","5c87e7d8":"markdown","b8cafe5b":"markdown","1325b473":"markdown","07a1c663":"markdown","ecd46550":"markdown"},"source":{"f3e8b3c6":"import csv\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import  LogisticRegression, lars_path\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,power_transform\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectPercentile \nimport warnings\nwarnings.filterwarnings(\"ignore\",category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning) \nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.plotting import gmap\nfrom bokeh.models import WMTSTileSource, LinearColorMapper, LogColorMapper,ColumnDataSource, HoverTool, CustomJS, Slider, ColorBar, FixedTicker\nfrom bokeh.transform import linear_cmap, factor_cmap\nfrom bokeh.layouts import row, column\nfrom matplotlib import colors as mcolors","f0ff0380":"data = pd.read_csv(\"..\/input\/sonar.all-data.csv\",header = None,prefix='V')\ndata.head()","36575a6e":"print (\"Data size is: {}\".format(data.shape))\nprint (\"Variable types: \\n{}\".format(data.dtypes))","ea2f3150":"data.describe()  # does not 'describe' the last qualitative column","6ed9d823":"data[data.isnull().any(axis=1)]","57aa8c1c":"data['V60'].value_counts()","0b3351df":"def conv(x):\n    if x == 'M':\n        return 1\n    if x == 'R':\n        return 0\n    \ndata['V60'] = data['V60'].apply(lambda x : conv(x))","634301a1":"num_cols = data.shape[1]\ndata_corr = data.corr(method = 'pearson') \nfig,axes = plt.subplots(figsize=(15,5))\nfor i in range (0,20):\n    plt.plot(data_corr.iloc[i:num_cols,i], label=str(i))\nplt.xticks(rotation='vertical')\nplt.legend(bbox_to_anchor=(1.1, 1.05))\nplt.axhline(y=0,color='k')\nplt.ylabel(\"Pearson correlation coefficient\")\nplt.show()","b3314669":"fig = plt.figure(figsize=(12,10))\nplt.pcolor(data_corr)\nplt.colorbar()\nplt.show()","de36a21a":"data.hist(xlabelsize = 0, figsize=(20,12))\nplt.show()","c000a17e":"skewness = []\nskewness_sqrt = []\nskewness_yj = []\n\ndata_sqrt = data.apply(np.sqrt) \n\ntemp = power_transform(data.iloc[:,:-1],method = \"yeo-johnson\")\ndata_yj = pd.DataFrame(temp,columns=data.iloc[:,:-1].columns.tolist())\n\nfor var in data.iloc[:,:-1].columns:\n        skewness.append(data[var].skew())\nfor var in data_sqrt.iloc[:,:-1].columns:\n        skewness_sqrt.append(data_sqrt[var].skew())\nfor var in data_yj.iloc[:,:-1].columns:\n        skewness_yj.append(data_yj[var].skew())\n        \nfig,ax = plt.subplots(3,1, figsize=(30,6),sharex=True)\nbins = 208\nfontsize = 15\nax[0].hist(skewness,bins=bins)\nax[1].hist(skewness_sqrt,bins=bins)\nax[2].hist(skewness_yj,bins=bins)\nax[0].text(3,2.5,\"no transform\",fontsize = fontsize)\nax[1].text(3,2.5,\"sqrt\",fontsize = fontsize)\nax[2].text(3,2.5,\"yeo-johnson\",fontsize = fontsize)\n\nfig.subplots_adjust(hspace=0)\nfor ax in ax:\n    ax.label_outer()\n    ax.axvline(x=0,color='r')\n    \nplt.show()","e3dac250":"fig,ax = plt.subplots(figsize=(10,6))\nplt.plot(skewness, label = \"No transformation\")\nplt.plot(skewness_sqrt, color='r', label = \"Sqrt transformation\")\nplt.plot(skewness_yj, color='g', label = \"Yeo-Johnson transformation\")\nplt.axhline(y=0,color='k')\nplt.xlabel(\"Variable\/Column\")\nplt.ylabel(\"Skewness\")\nplt.legend(loc=\"best\")\nplt.show()","77a3ee9b":"data_yj.hist(xlabelsize = 0, figsize=(20,12))\nplt.show()","fc132160":"data.plot(kind = 'box',figsize=(20,10))\nplt.xticks(rotation='vertical')\nplt.show()","8b2557c6":"outlier_idx = []\nfor col in data.columns.tolist():\n    Q1 = np.percentile(data[col],25)\n    Q3 = np.percentile(data[col],75)\n    outlier = 1.5*(Q3-Q1)\n    outlier_list = data[(data[col]<Q1-outlier) | (data[col]>Q3+outlier)].index\n    outlier_idx.extend(outlier_list)\n\namount_of_rows_with_outliers=[]\nfor i in range(1,data.shape[1]-1):\n    idx_mult_outliers = set([x for x in outlier_idx if outlier_idx.count(x)>i])\n    amount_of_rows_with_outliers.append(len(idx_mult_outliers))\n    \noutput_notebook()\nTOOLS = \"pan,wheel_zoom,reset,hover,save\"\nmy_dict = dict(amount_of_rows_with_outliers = amount_of_rows_with_outliers, number_of_outliers = list(range(1,data.shape[1]-1)))\nsource2 = ColumnDataSource(my_dict)\np = figure(plot_width=600, plot_height=400, tools = TOOLS, tooltips=[(\"Number of outliers\",\"@number_of_outliers\"),(\"Amount of rows with such amount of outliers\",\"@amount_of_rows_with_outliers\")])\np.line(x=\"number_of_outliers\",y=\"amount_of_rows_with_outliers\", source = source2, line_width  = 2)\np.xaxis.axis_label = 'Number of outliers'\np.yaxis.axis_label = 'Amount of rows with outliers'\nshow(p)","d32a7dae":"rows_to_remove = set(list([x for x in outlier_idx if outlier_idx.count(x)>7]))\ndata_new = data.drop(rows_to_remove,axis=0)\nprint(\"The expected number of rows in the new dataset is 208 - 13 = {}\".format(data_new.shape[0]))","796f8f42":"skewness_new = []\n\nfor var in data_new.iloc[:,:-1].columns:\n        skewness_new.append(data_new[var].skew())\n\nfig,ax = plt.subplots(2,1, figsize=(20,6),sharex=True)\nbins = 208\nfontsize = 15\nax[0].hist(skewness_yj,bins=bins)\nax[1].hist(skewness_new,bins=bins)\nax[0].text(3,2.5,\"Yeo-Johnson\",fontsize = fontsize)\nax[1].text(3,2.5,\"Original with removal\",fontsize = fontsize)\n\nfig.subplots_adjust(hspace=0)\nfor ax in ax:\n    ax.label_outer()\n    ax.axvline(x=0,color='r')\n\nplt.show()","6159469a":"fig,ax = plt.subplots(figsize=(10,6))\nplt.plot(skewness_yj, label = \"Yeo-Johnson removal\")\nplt.plot(skewness_new,color='r', label = \"Original with removal\")\nplt.xlabel(\"Column\")\nplt.ylabel(\"Skewness\")\nplt.legend(loc=\"best\")\nax.axhline(y=0,color='k')\nplt.show()","00b03db5":"transformer_data = StandardScaler().fit(data)\ndata_norm = pd.DataFrame(transformer_data.transform(data),columns = data.columns.tolist())\ndata_norm.head()","b8f8500c":"data_norm.plot(kind = 'box',figsize=(20,10))\nplt.xticks(rotation='vertical')\nplt.show()","4da317ce":"X = data_norm.drop('V60',axis=1).values\ny = data_norm['V60'].values\n\nalphas, _, coefs = lars_path(X, y, verbose=True)\ncoefs_df = pd.DataFrame(coefs.T, columns =  data_norm.drop('V60',axis=1).columns.tolist())\n\ncolors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\nlisty = []\nfor item in colors.keys():\n    listy.append(item)\nrandom_color = listy[0:data.shape[1]-1]\n\nlist_of_alphas = [alphas]*(data.shape[1]-1)\nlist_of_data = coefs_df.T.values.tolist()\nmy_dictionary = dict(alpha = list_of_alphas,dat = list_of_data,var_names=data.iloc[:,:-1].columns.tolist(),c=random_color)\nsource3=ColumnDataSource(my_dictionary)\n\np = figure(plot_width=800, plot_height=600, tools = TOOLS, tooltips=[(\"Variable\",\"@var_names\")])\np.multi_line(xs = 'alpha',ys ='dat',source=source3, line_width  = 2,line_color='c')\n\np.xaxis.axis_label=\"alpha\"\nshow(p)","fc4717ed":"select  = SelectPercentile(percentile=10)\nselect.fit(X,y)\nmask = select.get_support()\nprint (pd.DataFrame(mask))","78b9e6b8":"# The orignal dataset\nX = data.drop('V60',axis=1)\ny = data['V60']\n\nnumber_of_models = 6\n\nresult_matrix_grid = [[] for _ in range(number_of_models)] \nresult_matrix_test = [[] for _ in range(number_of_models)]\nnames=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    grid_search_pipelines=[]\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    \n    linsvc_param = {'linsvc__C': [0.001, 0.01, 0.1, 1, 10, 100], 'linsvc__penalty': [\"l1\",\"l2\"]}\n    grid_search_pipelines.append((\"LINSVC\",linsvc_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"linsvc\",LinearSVC(dual=False,max_iter=3000))])))\n    \n    knn_param = {'knn__n_neighbors':[2,5,10]}\n    grid_search_pipelines.append((\"KNN\",knn_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"knn\",KNeighborsClassifier())])))\n    \n    lr_param = {'lr__C':[0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"LR\",lr_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"lr\",LogisticRegression())])))\n    \n    rfc_param = {'rfc__max_features':['auto',None]}\n    grid_search_pipelines.append((\"RFC\",rfc_param,Pipeline([(\"rfc\",RandomForestClassifier(n_estimators=2000))])))\n    \n    gbc_param = {'gbc__loss':[\"deviance\",\"exponential\"],'gbc__max_depth':[1,3],'gbc__learning_rate':[0.01,0.1,0.2,0.3]}\n    grid_search_pipelines.append((\"GBC\",gbc_param,Pipeline([(\"gbc\",GradientBoostingClassifier(n_estimators=2000))])))\n\n    \n    i = 0\n    for name,param,model in grid_search_pipelines:\n        names.append(name)\n        grid = GridSearchCV(estimator=model,param_grid=param,cv=10,n_jobs=-1)\n        grid.fit(X_train,y_train)\n        result_matrix_grid[i].append(grid.best_score_)\n        result_matrix_test[i].append(grid.score(X_test,y_test))\n        i +=1\n\nprint (\"Elapsed time is: \", time.time()-start, \"seconds\")\n\ngrid_results = pd.DataFrame(np.array(result_matrix_grid).T,columns=names[0:number_of_models])\nprint (\"\\nGrid results:\")\nfor i in grid_results.columns:\n    print (\"{}: {:.2f} \u00b1 {:.2f}\".format(i,grid_results[i].mean(),grid_results[i].std()))\n    \ntest_results = pd.DataFrame(np.array(result_matrix_test).T,columns=names[0:number_of_models])\nprint(\"\\nTest results:\")\nfor i in test_results.columns:\n    print (\"{}: {:.2f} \u00b1 {:.2f}\".format(i,test_results[i].mean(),test_results[i].std()))","00c61374":"# The yj dataset\nX = data_yj#.drop('V60',axis=1)\ny = data['V60']\n\nnumber_of_models = 6\n\nresult_matrix_grid = [[] for _ in range(number_of_models)] \nresult_matrix_test = [[] for _ in range(number_of_models)]\nnames=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    grid_search_pipelines=[]\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    \n    linsvc_param = {'linsvc__C': [0.001, 0.01, 0.1, 1, 10, 100], 'linsvc__penalty': [\"l1\",\"l2\"]}\n    grid_search_pipelines.append((\"LINSVC\",linsvc_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"linsvc\",LinearSVC(dual=False))])))\n    \n    knn_param = {'knn__n_neighbors':[5,10],'knn__algorithm':['auto','brute']}\n    grid_search_pipelines.append((\"KNN\",knn_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"knn\",KNeighborsClassifier())])))\n    \n    lr_param = {'lr__C':[0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"LR\",lr_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"lr\",LogisticRegression())])))\n    \n    rfc_param = {'rfc__max_features':['auto',None]}\n    grid_search_pipelines.append((\"RFC\",rfc_param,Pipeline([(\"rfc\",RandomForestClassifier(n_estimators=2000))])))\n    \n    gbc_param = {'gbc__loss':[\"deviance\",\"exponential\"],'gbc__max_depth':[1,3],'gbc__learning_rate':[0.01,0.1,0.2,0.3]}\n    grid_search_pipelines.append((\"GBC\",gbc_param,Pipeline([(\"gbc\",GradientBoostingClassifier(n_estimators=2000))])))\n\n    \n    i = 0\n    for name,param,model in grid_search_pipelines:\n        names.append(name)\n        grid = GridSearchCV(estimator=model,param_grid=param,cv=10,n_jobs=-1)\n        grid.fit(X_train,y_train)\n        result_matrix_grid[i].append(grid.best_score_)\n        #print(name, grid.best_score_)\n        result_matrix_test[i].append(grid.score(X_test,y_test))\n        i +=1\n\nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\ngrid_results = pd.DataFrame(np.array(result_matrix_grid).T,columns=names[0:number_of_models])\nprint (\"\\nGrid results:\")\nfor i in grid_results.columns:\n    print (\"{}: {:.2f} \u00b1 {:.2f}\".format(i,grid_results[i].mean(),grid_results[i].std()))\n    \ntest_results = pd.DataFrame(np.array(result_matrix_test).T,columns=names[0:number_of_models])\nprint(\"\\nTest results:\")\nfor i in test_results.columns:\n    print (\"{}: {:.2f} \u00b1 {:.2f}\".format(i,test_results[i].mean(),test_results[i].std()))","14f99264":"# Reduced dataset\ndata_red = data[[\"V9\",\"V10\",\"V11\",\"V35\",\"V44\",\"V47\",\"V48\",\"V51\",\"V60\"]]\n\nX = data_red.drop('V60',axis=1)\ny = data_red['V60']\n\nnumber_of_models = 6\n\nresult_matrix_grid = [[] for _ in range(number_of_models)] \nresult_matrix_test = [[] for _ in range(number_of_models)]\nnames=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    grid_search_pipelines=[]\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    \n    linsvc_param = {'linsvc__C': [0.001, 0.01, 0.1, 1, 10, 100], 'linsvc__penalty': [\"l1\",\"l2\"]}\n    grid_search_pipelines.append((\"LINSVC\",linsvc_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"linsvc\",LinearSVC(dual=False))])))\n    \n    knn_param = {'knn__n_neighbors':[5,10],'knn__algorithm':['auto','brute']}\n    grid_search_pipelines.append((\"KNN\",knn_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"knn\",KNeighborsClassifier())])))\n    \n    lr_param = {'lr__C':[0.001, 0.01, 0.1, 1, 10, 100]}\n    grid_search_pipelines.append((\"LR\",lr_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"lr\",LogisticRegression())])))\n    \n    rfc_param = {'rfc__max_features':['auto',None]}\n    grid_search_pipelines.append((\"RFC\",rfc_param,Pipeline([(\"rfc\",RandomForestClassifier(n_estimators=2000))])))\n    \n    gbc_param = {'gbc__loss':[\"deviance\",\"exponential\"],'gbc__max_depth':[1,3],'gbc__learning_rate':[0.01,0.1,0.2,0.3]}\n    grid_search_pipelines.append((\"GBC\",gbc_param,Pipeline([(\"gbc\",GradientBoostingClassifier(n_estimators=2000))])))\n    \n\n        \n    i = 0\n    for name,param,model in grid_search_pipelines:\n        names.append(name)\n        grid = GridSearchCV(estimator=model,param_grid=param,cv=10,n_jobs=-1)\n        grid.fit(X_train,y_train)\n        result_matrix_grid[i].append(grid.best_score_)\n        #print(name, grid.best_score_)\n        result_matrix_test[i].append(grid.score(X_test,y_test))\n        i +=1\n\nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\ngrid_results = pd.DataFrame(np.array(result_matrix_grid).T,columns=names[0:number_of_models])\nprint (\"\\nGrid results:\")\nfor i in grid_results.columns:\n    print (\"{}: {:.2f} \u00b1 {:.2f}\".format(i,grid_results[i].mean(),grid_results[i].std()))\n    \ntest_results = pd.DataFrame(np.array(result_matrix_test).T,columns=names[0:number_of_models])\nprint(\"\\nTest results:\")\nfor i in test_results.columns:\n    print (\"{}: {:.2f} \u00b1 {:.2f}\".format(i,test_results[i].mean(),test_results[i].std()))","6c319a68":"X = data_yj\ny = data['V60']\n\nresult_matrix_grid = [] \nresult_matrix_test = []\nbest_param=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    \n    svm_param = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100],'svm__kernel':['rbf','poly','sigmoid'],'svm__degree':[3,4,5]}\n    #grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    pipe = Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])\n    grid = GridSearchCV(estimator=pipe,param_grid=svm_param,cv=10,n_jobs=-1)\n    grid.fit(X_train,y_train)\n    result_matrix_grid.append(grid.best_score_)\n    result_matrix_test.append(grid.score(X_test,y_test))\n    best_param.append(grid.best_params_)\n\n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\nprint (\"\\nGrid results: {:.2f} \u00b1 {:.2f}\".format(np.array(result_matrix_grid).mean(),np.array(result_matrix_grid).std()))\nprint (\"\\nTest results: {:.2f} \u00b1 {:.2f}\".format(np.array(result_matrix_test).mean(),np.array(result_matrix_test).std()))\nprint(\"\\nBest parameters: {}\".format(best_param))","824ba720":"pd.DataFrame.from_dict(best_param)","eda1502b":"X = data_yj\ny = data['V60']\n\nresult_matrix_grid = [] \nresult_matrix_test = []\nbest_param=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    \n    svm_param = {'svm__C': [3,5,10,20,30,40,50,60,70], 'svm__gamma': [0.3,0.5,1,2,3,4,5,6,7]}\n    #grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    pipe = Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])\n    grid = GridSearchCV(estimator=pipe,param_grid=svm_param,cv=10,n_jobs=-1)\n    grid.fit(X_train,y_train)\n    result_matrix_grid.append(grid.best_score_)\n    result_matrix_test.append(grid.score(X_test,y_test))\n    best_param.append(grid.best_params_)\n\n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\nprint (\"Grid results: {:.2f} \u00b1 {:.2f}\".format(np.array(result_matrix_grid).mean(),np.array(result_matrix_grid).std()))\nprint (\"Test results: {:.2f} \u00b1 {:.2f}\".format(np.array(result_matrix_test).mean(),np.array(result_matrix_test).std()))\nprint(\"Best parameters: {}\".format(best_param))","3396a5b9":"pd.DataFrame.from_dict(best_param)","e3f2b38f":"X = data_yj\ny = data['V60']\n\nresult_matrix_grid = [] \nresult_matrix_test = []\nbest_param=[]\n\nstart = time.time()\n\nfor _ in range(0,10):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    \n    svm_param = {'svm__C': [2,3,4,5,6,7], 'svm__gamma': [0,2,0.3,0.4,0.5,0.6,0.7]}\n    #grid_search_pipelines.append((\"SVC\",svm_param,Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])))\n    pipe = Pipeline([(\"scaler\",MinMaxScaler()),(\"svm\",SVC())])\n    grid = GridSearchCV(estimator=pipe,param_grid=svm_param,cv=10,n_jobs=-1)\n    grid.fit(X_train,y_train)\n    result_matrix_grid.append(grid.best_score_)\n    result_matrix_test.append(grid.score(X_test,y_test))\n    best_param.append(grid.best_params_)\n\n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\n\nprint (\"Grid results: {:.2f} \u00b1 {:.2f}\".format(np.array(result_matrix_grid).mean(),np.array(result_matrix_grid).std()))\nprint (\"Test results: {:.2f} \u00b1 {:.2f}\".format(np.array(result_matrix_test).mean(),np.array(result_matrix_test).std()))\nprint(\"Best parameters: {}\".format(best_param))","80974c9a":"pd.DataFrame.from_dict(best_param)","ff976be5":"X = data.drop('V60',axis=1)\ny = data['V60']\n\nfinal_test = []\n\nstart = time.time()\n\nfor _ in range(0,100):\n    X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size=0.2,random_state=np.random.randint(low=0,high=1000))\n    svm = SVC(C=3,gamma=0.5)\n    svm.fit(X_train,y_train)\n    final_test.append(svm.score(X_test,y_test))\n    \n    \nprint (\"Elapsed time is: \", time.time()-start, \"sec\")\nprint (\"\\nTest results: {:.2f} \u00b1 {:.2f}\".format(np.array(final_test).mean(),np.array(final_test).std())) ","4fcabc3f":"### Contents \n\n1. Looking at the data\n - Loading the data and checking some basic statistics \n - Checking correlation between the variables\n - Checking the skewness of the data\n - Checking the outliers\n2. Feature Selection\n - Using Lars\n - Using sklearn's FeatureSelection\n3. Analysing the preformance \n - Analysing six different models\n - Tuning the best model\n4. Results","83d4d4ee":"Lets narrow down the parameters a bit more given the results above ","fa7ed7bc":"### 2.2. Using FeatureSelection (no model required) ","af55f23a":"So now it is time to tune the best two model: SVC. First, lets expand the search by adding also different kernels:","0dd3a178":"## 2. Feature selection ","98bfe334":"## Result\nThe best accuracy was achived using SVC and the following parameters:\nC: 2-5\ngamma: 0.3-0.5\nand the rest are default. ","16a24f69":"Next, I will plot a graph that given a particular number of outliers shows how many observations (rows) have this amount of outliers. It is an interactive graph. ","e2733875":"The data is chirped signal (i.e. signal of increasing frequency) taken in 60 different times (Variables V0-V59) and the target variable (V60) is either a Rock (R) or a mine (M)","988a37c4":"### 2.1. LARS","176f07ce":"### Checking the skewness of the data","c49fd46f":"By zooming in it shows that the 6 most important variables are V10, V11, V35, V44, V48 and V51","f3554c1f":"Six models are used:\n- Support Vector Classifier\n- Linear Support Vector Classifier\n- K-Neighbors Classifier\n- Linear Regression\n- Random Forest \n- Gradient Boosting Classifier\n\nInstead of splitting the data into train-test set only once, it was decided to average it on 10 random splittings for more robust perfromance. \nThere are three datasets to assess:\n- the original dataset (data)\n- the Yeo-Johnson-transformed dataset (data_yj)\n- the reduced dataset with only the chosen variables according to the previous feature selection","75c7fae9":"and one last time...","ed1b1bdc":"# Sonar Mines vs Rocks with explanations","cb4a434d":"It seems that the Yeo-Johnson-transformed dataset gives the best results, but the difference from the original dataset is within statistical error. Nevertheless, I will continue with the Yeo-Johnson-transformed dataset. ","7a2e0019":"To put somewhere a threshold, I will remove all the rows with more than 7 outliers. There are 13 of them as can be checked above. It is sort of a compormise, because out dataset is small and removing more rows may affect the results.","4052c06f":"Lets convert the M and R into 1 and 0 ","d84b0e31":"It seems that the Yeo-Johnson transformation is the best.","b66f06b2":"## 3. Performance evaluation","b931d2c6":"### Checking and removing the outliers","cd43370a":"Checking if there is some missing data ","75350729":"## 1. Looking at the data \n### Loading the data and checking some basic statistics ","ceeb5034":"First, the data need to be normalised","1d914d20":"### Checking correlation ","5fde4858":"Bescause the way the data is collected I expect higher correlation between adjacent variables. For example, for the first 20 variables, as shown further, the correlation first decreases and then stabilises around very low values. ","5c87e7d8":"The data do not look too bad, but most variables are not noramlly distributed. I will try squre root tranformation and the Yeo-Johnson transformation","b8cafe5b":"Here the variables are a bit different: V9, V10, V11, V44, V47, V48","1325b473":"Let check the skewness against the best transformation again after the removal of the 13 rows","07a1c663":"The Yeo-Johnson is still much better.","ecd46550":"Checking the distribution of the target variable"}}