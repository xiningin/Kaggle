{"cell_type":{"95ea1011":"code","40c04bea":"code","b9bfbd04":"code","1434e17a":"code","288c61db":"code","4b7fa3f3":"code","b0cc09e9":"code","4d7ce673":"code","dae18e3d":"code","de10499d":"code","e36f3020":"code","5a46f88e":"code","831c5160":"code","318381e4":"code","c6442f62":"code","02a3aaa1":"code","765e292c":"code","1f7d171f":"code","d8d422bd":"code","34b9dd0f":"code","824d67a2":"code","a939701b":"code","4390f418":"code","91db37a3":"code","11e82972":"code","b8b631ed":"code","69343e68":"code","78ace0dc":"code","30148747":"code","e180922d":"code","a1c62505":"code","98c3511a":"code","71f2218e":"code","40a244ab":"code","b7f43493":"code","8510792d":"code","fc3044f4":"code","c0501f12":"code","90edef01":"code","43be1d09":"code","075aa455":"code","fce5cdb7":"code","1818b8c5":"code","c5488829":"code","7db9ee48":"code","23f37c4a":"code","7d11886b":"code","26de21a0":"code","c08fb8e2":"code","72767246":"code","cb3cf501":"code","a1975de7":"code","5aa0e77c":"code","f847f64b":"code","205774a2":"code","a0029fa1":"code","7c82bc5b":"code","9f36c716":"code","7895d0a4":"code","06c491fb":"code","1e367f67":"code","ea054c56":"code","9e6a3bb2":"code","b2891497":"code","3d4cd176":"code","7cfa2336":"code","822759ec":"code","1a3e9de6":"code","38fa983f":"code","f29c2532":"code","2e529f34":"code","4ecce9e6":"code","c4af071f":"code","2529fe40":"code","494ae97f":"code","4873e41d":"code","c73c5746":"code","2c6aa7cc":"code","f91136d9":"code","7c79e03f":"code","f18b2785":"code","5498742a":"code","b7e227f8":"code","71484107":"code","3ae9ba9e":"code","6c20c64d":"code","51752d2b":"code","fa071039":"code","3f177a9f":"code","4ea76d88":"code","4e94ff50":"code","5faec70f":"code","d57ccf19":"code","3555b31a":"code","d2127dd7":"code","6e0dffd3":"code","a9b861da":"code","c3ca7326":"code","f6ec0ddd":"code","f516fb0b":"code","80698921":"code","62486846":"code","f85725f9":"code","eecf61d5":"code","6a82d7ba":"code","a4bdeab4":"code","a9d16947":"code","e3c8aedb":"code","cce1c5a1":"code","6a5a4cd1":"code","0ffc33e0":"code","a1d37ddc":"code","d3961faa":"code","c848c1dd":"code","b2595103":"code","2d907a39":"code","90460bde":"code","ab3564e1":"code","277c31ce":"code","ec70ae13":"code","ec9b6168":"code","f0ed63c5":"code","7b881814":"code","f634e3fe":"code","ca980e17":"code","f35e18a6":"code","afd5ba5c":"code","de1a76e1":"code","683e2c85":"code","4a14c46e":"code","9a9dec88":"code","b5735d00":"code","62e13d89":"code","e7e15b2e":"code","cca89e39":"code","fed32df1":"code","d5a2029f":"code","9ddc8b7b":"code","3edc2aa7":"code","00bc2681":"markdown","1d15a20b":"markdown","91f57be3":"markdown","50d87b70":"markdown","39e0e661":"markdown","ff01147f":"markdown","c98fc984":"markdown","0928cea9":"markdown","c291059b":"markdown","f6dbaf72":"markdown","e0f5d879":"markdown","16a41658":"markdown","ee5ff042":"markdown","ca64b1f8":"markdown","e20ef090":"markdown","c77dde05":"markdown","f718572f":"markdown","c1cacaf7":"markdown","1fd95c29":"markdown","552a1774":"markdown","dbed3976":"markdown","f935fffd":"markdown","68e44dbb":"markdown"},"source":{"95ea1011":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40c04bea":"df=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")","b9bfbd04":"df.head()","1434e17a":"df.describe()","288c61db":"df.skew()","4b7fa3f3":"df.skew().index[df.skew().values>1]","b0cc09e9":"outlier_cols=df.skew().index[df.skew().values>1]","4d7ce673":"df.isnull().sum().index[df.isnull().sum().values>0]","dae18e3d":"df.dtypes.unique()","de10499d":"df_copy=df.copy()","e36f3020":"df_copy.shape","5a46f88e":"df_copy.select_dtypes(include=['O','object']).columns","831c5160":"cat_columns=df_copy.select_dtypes(include=['O','object']).columns\n\nfor cols in cat_columns:\n    df_copy[cols].fillna(df_copy[cols].mode()[0],inplace=True)","318381e4":"num_columns=df_copy.select_dtypes(exclude=['O','object']).columns\n\nfor cols in num_columns:\n    if cols in outlier_cols:\n        df_copy[cols].fillna(df_copy[cols].median(),inplace=True)\n    else:\n        df_copy[cols].fillna(df_copy[cols].mean(),inplace=True)","c6442f62":"#check for null values again\n\ndf_copy.isnull().sum().index[df_copy.isnull().sum().values>0]","02a3aaa1":"#Checking for outliers\n\ndf_copy.skew()","765e292c":"import plotly.express as px\n\nfig=px.box(df_copy['LotArea'])\nfig.show(\"notebook\")","1f7d171f":"fig=px.histogram(df_copy['LotArea'])\nfig.show(\"notebook\")","d8d422bd":"from scipy.stats.mstats import winsorize\npx.histogram(winsorize(df_copy['LotArea'], limits=[0.05, 0.05]))","34b9dd0f":"df_copy.corr()['SalePrice'].sort_values() #checking correlation of other columns with target","824d67a2":"X=df_copy.copy()\nX.drop([\"SalePrice\",\"Id\"],axis=1,inplace=True)\ny=df_copy['SalePrice']","a939701b":"px.histogram(y)","4390f418":"y = np.log1p(y) #evaluation criteria is rmse of log of y and predicted","91db37a3":"px.histogram(y)","11e82972":"#from sklearn.model_selection import train_test_split\n    \n#X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=123)","b8b631ed":"num_train_columns=['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold']","69343e68":"\n### Scaling the data\nfrom sklearn.preprocessing import QuantileTransformer\n\nqt=QuantileTransformer(output_distribution='normal',random_state=0)\n\nX_num_transformed=qt.fit_transform(X[num_train_columns])\n\n#transform validation set\n#X_val_transformed=qt.transform(X_val)\n","78ace0dc":"X[num_train_columns].shape","30148747":"df_num_transformed=pd.DataFrame(X_num_transformed.reshape(-1,36),columns=X[num_train_columns].columns)","e180922d":"df_num_transformed.head(1)","a1c62505":"df_num_transformed.skew()","98c3511a":"\noutlier_cols=df_num_transformed.skew().index[df_num_transformed.skew().values>1]\n\nfor cols in outlier_cols:\n    df_num_transformed[cols]=winsorize(df_num_transformed[cols], limits=[0.05, 0.05])","71f2218e":"df_num_transformed.skew()","40a244ab":"#since there are outliers we will use robust scaler to scale the data\nfrom sklearn.preprocessing import RobustScaler\n\nrs=RobustScaler()\n\nX_num_transformed=rs.fit_transform(df_num_transformed)","b7f43493":"df_num_transformed=pd.DataFrame(X_num_transformed.reshape(-1,36),columns=X[num_train_columns].columns)","8510792d":"X_transformed= pd.concat([df_num_transformed,X[cat_columns]],axis=1)","fc3044f4":"X_transformed.head(1)","c0501f12":"\"\"\"\n### Label Encoding using JamesSteinEncoder\n\nThe correct way to implement encoders it to to do within cross-validation:\nhttp:\/\/kiwidamien.github.io\/james-stein-encoder.html\n\nimport category_encoders as ce\n\n# Build the encoder\nencoder = ce.JamesSteinEncoder(cols=cat_columns)\n\n# Encode the frame and view it\nX_train_transformed_encoded = encoder.fit_transform(X_train_transformed, y_train)\n\n#transform the validation set\nX_val_transformed_encoded = encoder.transform(X_val_transformed, y_val)\n\n# Look at the first few rows\nX_train_transformed_encoded.head()\n\"\"\"","90edef01":"import warnings\nwarnings.filterwarnings(\"once\")","43be1d09":"\"\"\"\n!pip install xgboost\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nimport category_encoders as ce\nfrom sklearn.preprocessing import QuantileTransformer\n\n#we will use a pipeline to do the following \nqt=QuantileTransformer(output_distribution='normal', random_state=0)\nencoder = ce.JamesSteinEncoder(cols=cat_columns)\nxgb=XGBRegressor(random_state=123)\n\n# Build the model, including the encoder\nmodel = Pipeline([\n    ('james_stein', encoder), #categorical encoding\n    ('transformer', qt), #transformation to normal distribution\n    ('xgboost', xgb) #xgboost regressor\n])\n\nxgb_scores=cross_validate(model,X_train,y_train,cv=3,return_train_score=True,scoring=['neg_mean_squared_error','r2'])\n\"\"\"","075aa455":"#xgb_scores","fce5cdb7":"#np.sqrt(-xgb_scores['test_neg_mean_squared_error'].mean())","1818b8c5":"\"\"\"\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(model, X_train, y_train, cv=3,\n                                                                      return_times=True,random_state=123,\n                                                                     scoring='neg_mean_squared_error')\n    \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n\n# Plot learning curve\nimport plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean,\n                    mode='lines+markers',\n                    name='Training score'))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_scores_mean,\n                    mode='lines+markers',\n                    name='Cross Validation score'))\n\nfig.show()\n\"\"\"","c5488829":"\"\"\"\n!pip install xgboost\n!pip install scikit-optimize\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nimport category_encoders as ce\nfrom sklearn.preprocessing import QuantileTransformer\n#Bayesian optimization over hyper parameters.\n\nfrom skopt import BayesSearchCV\n\n#we will use a pipeline to do the following \nqt=QuantileTransformer(output_distribution='normal', random_state=0)\nencoder = ce.JamesSteinEncoder(cols=cat_columns)\nxgb=XGBRegressor(random_state=123)\n\nparam_grid = {\n    'xgboost__max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n    'xgboost__min_child_weight': np.arange(0.0001, 0.5, 0.001),\n    'xgboost__gamma': np.arange(0.0,40.0,0.005),\n    'xgboost__learning_rate': np.arange(0.0005,0.3,0.0005),\n    'xgboost__subsample': np.arange(0.01,1.0,0.01),\n    'xgboost__n_estimators':[100,300,500,700]}\n\n#tuned_XGB=BayesSearchCV(xgb,param_grid,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\n#tuned_XGB.fit(X_train_encoded_transformed,y_train)\n\n# Build the model, including the encoder\nmodel = Pipeline([\n    ('james_stein', encoder), #categorical encoding\n    ('transformer', qt), #transformation to normal distribution\n    ('xgboost', xgb) #xgboost regressor\n])\n\ntuned_XGB=BayesSearchCV(model,param_grid,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\ntuned_XGB.fit(X_train,y_train)\n\"\"\"","7db9ee48":"#np.sqrt(-tuned_XGB.best_score_)","23f37c4a":"#tuned_XGB.best_estimator_['xgboost']","7d11886b":"#tuned_XGB.best_estimator_","26de21a0":"'''\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(tuned_XGB.best_estimator_, X_train, y_train, cv=3,\n                                                                      return_times=True,random_state=123,\n                                                                     scoring='neg_mean_squared_error')\n    \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n\n# Plot learning curve\nimport plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean,\n                    mode='lines+markers',\n                    name='Training score'))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_scores_mean,\n                    mode='lines+markers',\n                    name='Cross Validation score'))\n\nfig.show()\n'''","c08fb8e2":"#tuned_XGB.best_params_","72767246":"\"\"\"\nfrom xgboost import plot_importance\n\nplot_importance(tuned_XGB.best_estimator_['xgboost'])\n\"\"\"","cb3cf501":"#X_train.columns[45]","a1975de7":"#stroing the feature imp and the column name as a list\n\n#feature_imp = pd.DataFrame(sorted(zip(tuned_XGB.best_estimator_['xgboost'].feature_importances_,X_train.columns)), columns=['Value','Feature'])","5aa0e77c":"\"\"\"\nfeature_imp.sort_values(by='Value',ascending=False,inplace=True)\npx.bar(feature_imp,x='Feature',y='Value')\n\"\"\"","f847f64b":"#feature_imp[feature_imp['Value']>0].shape[0] #to get number of \"important\" columns ","205774a2":"#list(feature_imp['Feature'][:19])","a0029fa1":"#X_train.head(2)","7c82bc5b":"\"\"\"\nchosen_cols=list(feature_imp['Feature'][:19])\n#new_cols=[\"Total_Area\"]\n\n#final_cols=chosen_cols+new_cols\n\nX_train_reduced=X_train[chosen_cols]\nX_val_reduced=X_val[chosen_cols]\n\"\"\"","9f36c716":"#X_train_reduced.head()","7895d0a4":"#reduced_cat_columns=X_train_reduced.select_dtypes(include=['O','object']).columns","06c491fb":"#we will use a pipeline to do the following with the reduced dataset\n\n\"\"\"\nqt=QuantileTransformer(output_distribution='normal', random_state=0)\nencoder = ce.JamesSteinEncoder(cols=reduced_cat_columns)\nxgb=XGBRegressor(random_state=123)\n\nparam_grid = {\n    'xgboost__max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n    'xgboost__min_child_weight': np.arange(0.0001, 0.5, 0.001),\n    'xgboost__gamma': np.arange(0.0,40.0,0.005),\n    'xgboost__learning_rate': np.arange(0.0005,0.3,0.0005),\n    'xgboost__subsample': np.arange(0.01,1.0,0.01),\n    'xgboost__n_estimators':np.arange(100,2000,200)}\n\n#tuned_XGB=BayesSearchCV(xgb,param_grid,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\n#tuned_XGB.fit(X_train_encoded_transformed,y_train)\n\n# Build the model, including the encoder\nmodel = Pipeline([\n    ('james_stein', encoder), #categorical encoding\n    ('transformer', qt), #transformation to normal distribution\n    ('xgboost', xgb) #xgboost regressor\n])\n\ntuned_XGB=BayesSearchCV(model,param_grid,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\ntuned_XGB.fit(X_train_reduced,y_train)\n\"\"\"","1e367f67":"\"\"\"\nGot 0.12559413105106665 by keeping 19 most imp features\n\n\nnp.sqrt(-tuned_XGB.best_score_) #score is better than score with all features\n\"\"\"","ea054c56":"#tuned_XGB.best_estimator_['xgboost']","9e6a3bb2":"\"\"\"\n#Plotting learning to check if it is overfitting or underfitting\n\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(tuned_XGB.best_estimator_, X_train_reduced, y_train, cv=3,\n                                                                      return_times=True,random_state=123,\n                                                                     scoring='neg_mean_squared_error')\n    \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n\n# Plot learning curve\nimport plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean,\n                    mode='lines+markers',\n                    name='Training score'))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_scores_mean,\n                    mode='lines+markers',\n                    name='Cross Validation score'))\n\nfig.show()\n\"\"\"","b2891497":"!pip install catboost","3d4cd176":"from catboost import CatBoostRegressor\n\ncat=CatBoostRegressor(random_state=123,cat_features= cat_columns)\n","7cfa2336":"cat_columns #just checking ","822759ec":"from skopt.space import Real, Categorical, Integer\nfrom skopt import BayesSearchCV","1a3e9de6":"# Defining your params and training catboost on entire training set\nsearch_spaces = {'iterations': Integer(100, 2000),\n                 'depth': Integer(1, 8),\n                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n                 'bagging_temperature': Real(0.0, 1.0),\n                 'border_count': Integer(1, 255),\n                 'l2_leaf_reg': Integer(2, 40)}\n\ntuned_cat=BayesSearchCV(cat,search_spaces,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\n\ntuned_cat.fit(X_transformed,y)","38fa983f":"np.sqrt(-tuned_cat.best_score_) #0.10051084885385934 with winsorizing and gaussian quantile transformer","f29c2532":"tuned_cat.best_params_","2e529f34":"X_transformed.shape","4ecce9e6":"#Plotting learning to check if it is overfitting or underfitting\n\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(tuned_cat.best_estimator_, X_transformed, y, cv=3,\n                                                                      return_times=True,random_state=123,\n                                                                     scoring='neg_mean_squared_error')\n\n    \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n\n# Plot learning curve\nimport plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean,\n                    mode='lines+markers',\n                    name='Training score'))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_scores_mean,\n                    mode='lines+markers',\n                    name='Cross Validation score'))\n\nfig.show()","c4af071f":"test_scores_mean.mean()","2529fe40":"from sklearn.feature_selection import SelectFromModel #we will use this embedded method\n\nembeded_cat_selector = SelectFromModel(tuned_cat.best_estimator_, max_features=X_transformed.shape[1])\nembeded_cat_selector.fit(X_transformed, y)","494ae97f":"embeded_cat_support = embeded_cat_selector.get_support()\nembeded_cat_feature = X_transformed.loc[:,embeded_cat_support].columns.tolist()\nprint(str(len(embeded_cat_feature)), 'selected features')","4873e41d":"reduced_features_catboost=embeded_cat_feature","c73c5746":"cat_importance_features=pd.DataFrame(zip(tuned_cat.best_estimator_.feature_importances_,X_transformed.columns), columns=['Value','Feature'])\ncat_importance_features[cat_importance_features['Value']>0].shape[0] #to get number of \"important\" columns ","2c6aa7cc":"cat_importance_features=cat_importance_features['Feature'].values[:60]","f91136d9":"X_catboost_reduced=X_transformed[embeded_cat_feature] #selecting features chosen by 'selectfrommodel'","7c79e03f":"catboost_cat_columns=X_catboost_reduced.select_dtypes(include=['O','object']).columns","f18b2785":"cat=CatBoostRegressor(random_state=123,cat_features= catboost_cat_columns)\n\n# Defining your params\nsearch_spaces = {'iterations': Integer(100, 2000),\n                 'depth': Integer(1, 8),\n                 'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n                 'random_strength': Real(1e-9, 10, 'log-uniform'),\n                 'bagging_temperature': Real(0.0, 1.0),\n                 'border_count': Integer(1, 255),\n                 'l2_leaf_reg': Integer(2, 30)}\n\ntuned_cat=BayesSearchCV(cat,search_spaces,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\ntuned_cat.fit(X_catboost_reduced,y)","5498742a":"tuned_cat.best_score_   #-0.010524306153207024 with winszorize and gaussian quantile transformation","b7e227f8":"tuned_cat.best_params_ ","71484107":"#Plotting learning to check if it is overfitting or underfitting\n\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(tuned_cat.best_estimator_, X_catboost_reduced, y, cv=3,\n                                                                      return_times=True,random_state=123,\n                                                                     scoring='neg_mean_squared_error')\n\n    \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n\n# Plot learning curve\nimport plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean,\n                    mode='lines+markers',\n                    name='Training score'))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_scores_mean,\n                    mode='lines+markers',\n                    name='Cross Validation score'))\n\nfig.show()","3ae9ba9e":"!pip install lightgbm\nimport lightgbm as lgb","6c20c64d":"gbm = lgb.LGBMRegressor(random_state=123)","51752d2b":"#reduced_cat_columns","fa071039":"#X_train_reduced.dtypes","3f177a9f":"cat_columns","4ea76d88":"\nX_transformed_copy=X_transformed.copy()\n\nfor col in cat_columns:\n    X_transformed_copy[col] = X_transformed_copy[col].astype('category')\n","4e94ff50":"X_transformed_copy.dtypes","5faec70f":"\nparam_test ={'num_leaves': Integer(6, 50), \n             'min_child_samples': Integer(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': Real(0.2,0.8), \n             'colsample_bytree': Real(0.4, 0.6),\n             'n_estimators':Integer(100, 2000),\n             'depth': Integer(1, 8),\n            'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n             'bagging_temperature': Real(0.0, 1.0),\n             'border_count': Integer(1, 255),\n             'l2_leaf_reg': Integer(2, 30)}\n\ntuned_gbm=BayesSearchCV(gbm,param_test,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\ntuned_gbm.fit(X_transformed_copy,y)\n","d57ccf19":"np.sqrt(-tuned_gbm.best_score_)","3555b31a":"\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(tuned_gbm.best_estimator_, X_transformed_copy, y, cv=3,\n                                                                      return_times=True,random_state=123,\n                                                                     scoring='neg_mean_squared_error')\n    \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n\n# Plot learning curve\nimport plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean,\n                    mode='lines+markers',\n                    name='Training score'))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_scores_mean,\n                    mode='lines+markers',\n                    name='Cross Validation score'))\n\nfig.show()\n","d2127dd7":"from sklearn.feature_selection import SelectFromModel #we will use this embedded method\n\nembeded_gbm_selector = SelectFromModel(tuned_gbm.best_estimator_, max_features=X_transformed_copy.shape[1])\nembeded_gbm_selector.fit(X_transformed_copy, y)","6e0dffd3":"embeded_gbm_support = embeded_gbm_selector.get_support()\nembeded_gbm_feature = X_transformed_copy.loc[:,embeded_gbm_support].columns.tolist()\nprint(str(len(embeded_gbm_feature)), 'selected features')","a9b861da":"X_gbm_reduced=X_transformed_copy[embeded_gbm_feature] #selecting features chosen by 'selectfrommodel'","c3ca7326":"\nparam_test ={'num_leaves': Integer(6, 50), \n             'min_child_samples': Integer(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': Real(0.2,0.8), \n             'colsample_bytree': Real(0.4, 0.6),\n             'n_estimators':Integer(100, 2000),\n             'depth': Integer(1, 8),\n            'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n             'bagging_temperature': Real(0.0, 1.0),\n             'border_count': Integer(1, 255),\n             'l2_leaf_reg': Integer(2, 30)}\n\ntuned_gbm=BayesSearchCV(gbm,param_test,cv=3,scoring='neg_mean_squared_error',random_state=123,verbose=1,return_train_score=True,n_jobs=-1)\ntuned_gbm.fit(X_gbm_reduced,y)\n","f6ec0ddd":"tuned_gbm.best_score_","f516fb0b":"\nfrom sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(tuned_gbm.best_estimator_, X_gbm_reduced, y, cv=3,\n                                                                      return_times=True,random_state=123,\n                                                                     scoring='neg_mean_squared_error')\n    \ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\n\n# Plot learning curve\nimport plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=train_sizes, y=train_scores_mean,\n                    mode='lines+markers',\n                    name='Training score'))\nfig.add_trace(go.Scatter(x=train_sizes, y=test_scores_mean,\n                    mode='lines+markers',\n                    name='Cross Validation score'))\n\nfig.show()\n","80698921":"!pip install keras-tuner\n","62486846":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom kerastuner.tuners import BayesianOptimization","f85725f9":"cat_predictions=tuned_cat.predict(X_catboost_reduced)\nlightgbm_predictions=tuned_gbm.predict(X_gbm_reduced)","eecf61d5":"X_train_NN=pd.DataFrame()\n\nX_train_NN['CatBoost']=np.expm1(cat_predictions)\nX_train_NN['LightGBM']=np.expm1(lightgbm_predictions)","6a82d7ba":"def build_model(hp):\n    model = keras.Sequential()\n    for i in range(hp.Int('num_layers', 2, 20)):\n        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n                                            min_value=32,\n                                            max_value=512,\n                                            step=32),\n                               activation='relu'))\n    model.add(layers.Dense(1, activation='linear'))\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n        loss='mean_squared_error',\n        metrics=['mean_squared_error'])\n    return model\n","a4bdeab4":"tuner = BayesianOptimization(\n    build_model,\n    objective='mean_squared_error',\n    max_trials=5,\n    executions_per_trial=3,\n    directory='project',\n    project_name='House Prediction',\noverwrite=True)","a9d16947":"tuner.search(X_train_NN, y,epochs=150)","e3c8aedb":"tuner.results_summary()","cce1c5a1":"final_NN = tuner.get_best_models(num_models=1)[0]","6a5a4cd1":"test_df_copy=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","0ffc33e0":"test_df=test_df_copy.copy()","a1d37ddc":"test_df.head(2)","d3961faa":"cat_columns=test_df.select_dtypes(include=['O','object']).columns\n\nfor cols in cat_columns:\n    test_df[cols].fillna(test_df[cols].mode()[0],inplace=True)","c848c1dd":"outlier_cols=test_df.skew().index[test_df.skew().values>1]","b2595103":"num_columns=test_df.select_dtypes(exclude=['O','object']).columns\n\nfor cols in num_columns:\n    if cols in outlier_cols:\n        test_df[cols].fillna(test_df[cols].median(),inplace=True)\n    else:\n        test_df[cols].fillna(test_df[cols].mean(),inplace=True)","2d907a39":"#check for null values again\n\ntest_df.isnull().sum().index[test_df.isnull().sum().values>0]","90460bde":"#Checking for outliers\n\ntest_df.skew()","ab3564e1":"\"\"\"\noutlier_cols=test_df.skew().index[test_df.skew().values>1]\n\nfor cols in outlier_cols:\n    test_df[cols]=winsorize(test_df[cols], limits=[0.05, 0.05])\n\"\"\"","277c31ce":"test_df.drop([\"Id\"],axis=1,inplace=True)","ec70ae13":"test_num_transformed=qt.transform(test_df[num_train_columns])","ec9b6168":"test_num_transformed.shape","f0ed63c5":"df_test_num_transformed=pd.DataFrame(test_num_transformed.reshape(-1,36),columns=test_df[num_train_columns].columns)","7b881814":"df_test_num_transformed.head(1)","f634e3fe":"df_test_num_transformed.skew()","ca980e17":"\noutlier_cols=df_test_num_transformed.skew().index[df_test_num_transformed.skew().values>1]\n\nfor cols in outlier_cols:\n    df_test_num_transformed[cols]=winsorize(df_test_num_transformed[cols], limits=[0.05, 0.05])","f35e18a6":"df_test_num_transformed.skew()","afd5ba5c":"X_test_num_transformed=rs.transform(df_test_num_transformed)","de1a76e1":"X_test_num_transformed=pd.DataFrame(X_test_num_transformed.reshape(-1,36),columns=test_df[num_train_columns].columns)","683e2c85":"test_transformed= pd.concat([X_test_num_transformed,test_df[cat_columns]],axis=1)","4a14c46e":"test_transformed.head(2)","9a9dec88":"#X_test_catboost_reduced=test_transformed[cat_importance_features] #selecting features chosen by 'selectfrommodel'","b5735d00":"embeded_cat_feature","62e13d89":"X_test_catboost_reduced=test_transformed[embeded_cat_feature] #selecting features chosen by 'selectfrommodel'","e7e15b2e":"test_cat_predictions=tuned_cat.predict(X_test_catboost_reduced)","cca89e39":"\n#creating copy of X_test_reduced and changing categorical data to category type for lightgbm\ntest_transformed_copy=test_transformed.copy()\n\nfor col in cat_columns:\n    test_transformed_copy[col] = test_transformed_copy[col].astype('category')\n\n\nX_test_gbm_reduced=test_transformed_copy[embeded_gbm_feature] #selecting features chosen by 'selectfrommodel'\ntest_gbm_predictions=tuned_gbm.predict(X_test_gbm_reduced)\n","fed32df1":"X_test_NN=pd.DataFrame()\n\nX_test_NN['CatBoost']=np.expm1(test_cat_predictions)\nX_test_NN['LightGBM']=np.expm1(test_gbm_predictions)\n\nfinal_predictions=final_NN.predict(X_test_NN)","d5a2029f":"#gives 0.12960 \n\n#final_predictions=(0.7*np.expm1(test_cat_predictions))+(0.3*np.expm1(test_gbm_predictions))","9ddc8b7b":"sub = pd.DataFrame()\nsub['Id'] = test_df_copy['Id']\nsub['SalePrice'] = np.expm1(final_predictions)\nsub.head()","3edc2aa7":"sub.to_csv('submission.csv',index=False)","00bc2681":"We will apply the same formula to the other skewed columns as well later on.","1d15a20b":"Seems to be overfitting a little bit. Let's try reducing the features.","91f57be3":"### Catboost with reduced features","50d87b70":"## Submission","39e0e661":"### Modeling with reduced features","ff01147f":"Lightgbm can handle only features, that are of category type, not object. So we will create a copy of the X_train_reduced dataframe and change the catgeorical features to category type. [Link for reference](https:\/\/stackoverflow.com\/questions\/56070396\/why-does-categorical-feature-of-lightgbm-not-work)","c98fc984":"## Stacking and Ensembling of models using NN","0928cea9":"There are still some columns with outliers like PoolArea","c291059b":"## Test Set","f6dbaf72":"There are still some columns with outliers like PoolArea","e0f5d879":"We will use the columns selected by the \"selectFromModel\". We had saved the column names in \"embeded_cat_feature\" list.","16a41658":"## Modeling\n\n\nI tried using XGBoost , Catboost and LightGBM. Catboost performed the best and hence I have commented out the remaining.\n\n### 1. XGBoost\n\nWe will first model with all features and perform error analysis to understand which features to add\/remove.","ee5ff042":"### 3. LightGBM","ca64b1f8":"From the learning curve, we can see that the model is overfitting the data. The error on the training set it too low(very close to 0) while the error on cv set is higher. Let's fine tune the hyperparameters and plot the learning curve with the tuned XGBoost model. ","e20ef090":"We will perform the following:\n\n1. Transform the data using Quantile Transformer with gaussian output as there are outliers.\n2. Winsorize the remaining outliers\n3. Scale the data using Robust Scaler","c77dde05":"### Train Test Split","f718572f":"### LightGBM with reduced features","c1cacaf7":"### 2. CatBoost\n\nWe will train CatBoost on the reduced training set","1fd95c29":"You can see from the above graph that it is not overfitting anymore.","552a1774":"**Adding additional features?**\n\nArea and quality seems to have high importance. We will create a column called \"Area_assoc\" which will depicted the overall area associated with a house (i.e. Lot area + Garage area + Basement Area etc.) and \"Avg_Quality\" which will be the average of all the quality scores.\n\n**Update on above:**\n\nTried adding the above columns about the performance worsened so deleted it.","dbed3976":"Looks like the model is overfitting. Let's plot the learning curve to check.","f935fffd":"There seems to be some null values and a lot of categorical columns. ","68e44dbb":"Below columns contain data points that skew the distribution of data"}}