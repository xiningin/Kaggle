{"cell_type":{"6854401f":"code","fc6cc49e":"code","7077f9bc":"code","81660862":"code","d1da618c":"code","f8cb123d":"code","30ee33e8":"code","4a264cbb":"code","140af10d":"code","5f1b3235":"code","da2a4697":"code","1c9d4ed2":"code","d6d77b15":"code","968787ac":"code","84f4838e":"code","19812101":"code","c51157f3":"code","3f5489b2":"markdown","a2091b68":"markdown","233d255f":"markdown","71d54294":"markdown","6a0c0b36":"markdown","f8abcda5":"markdown","86d3bb61":"markdown"},"source":{"6854401f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fc6cc49e":"x = np.arange(-5.0, 5.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 2*(x) + 3\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\n#plt.figure(figsize=(8,6))\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()","7077f9bc":"#cubic model\nx = np.arange(-5.0, 5.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\ny = 1*(x**3) + 1*(x**2) + 1*x + 3\ny_noise = 20 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()","81660862":"#quadratic model\n\nx = np.arange(-5.0, 5.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\ny = np.power(x,2)\ny_noise = 2 * np.random.normal(size=x.size)\nydata = y + y_noise\nplt.plot(x, ydata,  'bo')\nplt.plot(x,y, 'r') \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()","d1da618c":"#exponentical model\nX = np.arange(-5.0, 5.0, 0.1)\n\n##You can adjust the slope and intercept to verify the changes in the graph\n\nY= np.exp(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()","f8cb123d":"#Logarithmic model\nX = np.arange(-5.0, 5.0, 0.1)\n\nY = np.log(X)\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()","30ee33e8":"#Sigmoidal\/Logistic\nX = np.arange(-5.0, 5.0, 0.1)\n\n\nY = 1-4\/(1+np.power(3, X-2))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()","4a264cbb":"df = pd.read_csv(\"..\/input\/chinagdp\/china_gdp.csv\")","140af10d":"df.head()","5f1b3235":"plt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()","da2a4697":"# simple logistic model\nX = np.arange(-5.0, 5.0, 0.1)\nY = 1.0 \/ (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()","1c9d4ed2":"def sigmoid(x, Beta_1, Beta_2):\n     y = 1 \/ (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y","d6d77b15":"beta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')","968787ac":"# Lets normalize our data\nxdata =x_data\/max(x_data)\nydata =y_data\/max(y_data)","84f4838e":"from scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))","19812101":"x = np.linspace(1960, 2015, 55)\nx = x\/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()","c51157f3":"# split data into train\/test\nmsk = np.random.rand(len(df)) < 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )","3f5489b2":"From the graph we understand that logistic model is a good approximation","a2091b68":"\nWe will use this formula for the logistic function is the following:\n\n$$ \\hat{Y} = \\frac1{1+e^{\\beta_1(X-\\beta_2)}}$$\n\n$\\beta_1$: Controls the curve's steepness,\n\n$\\beta_2$: Slides the curve on the x-axis.","233d255f":"## Plotting the dataset","71d54294":"If the data shows a curvy trend, then linear regression will not produce very accurate results when compared to a non-linear regression because, as the name implies, linear regression presumes that the data is linear. \nLet's learn about non linear regressions and apply an example on python. In this notebook, we fit a non-linear model to the datapoints corrensponding to China's GDP from 1960 to 2014.","6a0c0b36":"we can use __curve_fit__ which uses non-linear least squares to fit our sigmoid function, to data. Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.\n\npopt are our optimized parameters.","f8abcda5":"### Validate","86d3bb61":"## A quick look into different models"}}