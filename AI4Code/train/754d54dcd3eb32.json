{"cell_type":{"e4a2a025":"code","e6010ff5":"code","1d68ff6c":"code","b4658020":"code","95d906b3":"code","55248982":"code","de80a86f":"code","ea4f5668":"code","e10c0742":"code","23179294":"code","5ff923c1":"code","1a723f31":"code","00e878c0":"code","123f2475":"code","bb272d1d":"code","af109855":"code","cf68faa9":"code","a2a2260d":"code","f521fcfa":"code","d9c98291":"code","78999413":"code","073dd248":"code","a7157d7f":"code","56a0df6c":"code","85e08bbd":"code","5f36a2e9":"code","d8978574":"code","65185a0f":"code","96fee28f":"markdown","e8d0db8b":"markdown","cf33f1a2":"markdown","f3b1eac6":"markdown","04ea5ceb":"markdown"},"source":{"e4a2a025":"import os\nimport re\nimport time\nimport string\nimport pickle\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport torch\nimport torch.nn as nn\n\nimport seaborn as sns\n\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n\nfrom collections import defaultdict","e6010ff5":"train_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain_df.head()","1d68ff6c":"def get_cv_ids():\n    global train_df\n    df=train_df.copy()\n    cv_ids=[]\n    ranges=[(-4, -3.0), (-3.0, -2.0),(-2.0, -1.0),(-1.0, 0.0), (0.0, 1.0), (1.0, 2)]\n    for r in ranges:\n        l=r[0]\n        h=r[1]\n        \n        cur_cvids=list(df[(df.target>=l) & (df.target<h)].id.values)\n        np.random.choice(cur_cvids)\n        cv_ids+=cur_cvids[:int(len(cur_cvids)*0.1)]\n    return cv_ids\ncv_ids=get_cv_ids()\n\n\nval_df=train_df[train_df.id.isin(cv_ids)].copy()\ntrain_df=train_df[train_df.id.isin(cv_ids)==False].copy()\n\nprint('Number Of Validation Records:', len(val_df))\nprint('Number Of Train Records:', len(train_df))\n\nval_df.target.hist(bins=100)\nplt.show()","b4658020":"train_df.target.hist(bins=100)\nplt.show()","95d906b3":"glove_path='..\/input\/glove6b100dtxt\/glove.6B.100d.txt'\nglove_embeddings={}\nwith open(glove_path) as file:\n    for line in file:\n        line=line.split()\n        word=line[0]\n        v=np.array(line[1:]).astype(np.float)\n        glove_embeddings[word]=v\nprint(len(glove_embeddings))","55248982":"def seed_all():\n    torch.manual_seed(0)\n    random.seed(0)\n    np.random.seed(0)\n\nseed_all()","de80a86f":"class Tokenizer:\n    def __init__(self):\n        self.lemmatizer=WordNetLemmatizer()\n        self.nlp=English()\n    def __call__(self, doc):\n        tokens=[]\n        for token in self.nlp(doc):\n            if token.like_num or token.text=='':\n                continue\n            token=token.lower_.strip()\n            for p in string.punctuation:\n                token=token.replace(p, ' ')\n            token=token.split(' ')\n            token=[w for w in token if w!='']\n            tokens+=token\n        return tokens","ea4f5668":"tokenizer=Tokenizer()\ntrain_df['doc']=train_df.excerpt.apply(tokenizer)\ntrain_df.head()","e10c0742":"MAX_SEQ_LEN=150\nBATCH_SIZE=128","23179294":"target_mean=train_df.target.mean()\ntarget_std=train_df.target.std()\n\nprint(\"Taget Mean:\", target_mean)\nprint(\"Taget Std:\", target_std)","5ff923c1":"train_df['normalized_target']=(train_df.target - target_mean)\/target_std\nsns.histplot(data=train_df, x='normalized_target')","1a723f31":"train_df['normalized_target'].describe()","00e878c0":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, phase):\n        self.df=df\n        self.phase=phase\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        doc=row.doc\n        \n        X=torch.zeros((MAX_SEQ_LEN, 100), dtype=torch.float32)\n        for i, word in enumerate(doc):\n            if i >= MAX_SEQ_LEN:\n                break\n            if word in glove_embeddings:\n                X[i]=torch.tensor(glove_embeddings[word])\n        \n        if self.phase=='train':\n            y=torch.tensor(row.normalized_target, dtype=torch.float32)\n            return (X, y)\n        return X\n    def __len__(self):\n        return len(self.df)","123f2475":"class ProjectionHead(nn.Module):\n    def __init__(self, in_features,out_feat):\n        super().__init__()\n        self.linear1=nn.Linear(in_features, 512)\n        self.bn=nn.BatchNorm1d(512)\n        self.dropout=nn.Dropout(0.2)\n        self.relu=nn.ReLU()\n        self.linear2=nn.Linear(512, out_feat)\n    def forward(self, x):\n        x=self.linear1(x)\n        x=self.bn(x)\n        x=self.dropout(x)\n        x=self.relu(x)\n        x=self.linear2(x)\n        return x\n\nclass Model(nn.Module):\n    def __init__(self, embedd_size, hidden_size):\n        super().__init__()\n        self.hidden_size=hidden_size\n        self.gru=nn.GRU(embedd_size, hidden_size, num_layers=2, \n                        dropout=0.2, bidirectional=True,batch_first=True)\n        self.bn=nn.BatchNorm1d(2*hidden_size)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.2)\n        self.proj_head=ProjectionHead(2*hidden_size, 1)\n    def forward(self, x):\n        batch_size=x.shape[0]\n        (_, h_n)=self.gru(x)\n        h_n=h_n.view(2, 2, batch_size, self.hidden_size)\n        h_n=h_n[1, :, :, :].permute(1, 0, 2)\n        h_n1=h_n[:, 0, :]\n        h_n2=h_n[:, 1, :]\n        h=torch.cat([h_n1, h_n2], dim=1)\n        \n        h=self.bn(h)\n        h=self.relu(h)\n        h=self.dropout(h)\n        \n        y=self.proj_head(h)\n        return y","bb272d1d":"def lr_test(min_lr, max_lr, train_dataloader):\n    model=Model(100, 128)\n    model.train()\n    optimizer=torch.optim.AdamW(model.parameters(), lr=min_lr)\n    mse_loss=nn.MSELoss(reduction='mean')\n    schedular=torch.optim.lr_scheduler.StepLR(optimizer, 1, 1.05)\n\n    lrs=[]\n    losses=[]\n\n    for i in range(10):\n        print('Epoch:', i+1)\n        for j, (X, y) in enumerate(train_dataloader):\n            y_hat=model(X).view(-1)\n            loss=mse_loss(y_hat, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            schedular.step()\n\n            lrs+=schedular.get_last_lr()\n            losses.append(loss.item())\n\n            if j%10==0:\n                print(lrs[-1])\n\n            if lrs[-1]>max_lr:\n                break\n        if lrs[-1]>max_lr:\n            break\n    return lrs, losses","af109855":"def train_epoch(model, optimizer, criterion, train_dataloader, scheduler):\n    epoch_loss=0.0\n    model.train()\n    for (X, y) in train_dataloader:\n        batch_size=X.shape[0]\n        y_hat=model(X).view(-1)\n        loss=criterion(y_hat, y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        epoch_loss+=loss.item()\n    epoch_loss\/=len(train_dataloader)\n    return epoch_loss\n\n\ndef get_val_loss(model, val_dataloader):\n    y_diff=[]\n    model.eval()\n    for (X, y) in val_dataloader:\n        with torch.no_grad():\n            y_hat=model(X).view(-1)\n            y_diff=list(torch.abs(y_hat-y).numpy())\n    \n    y_diff=np.sqrt( np.mean(np.square(y_diff) ))\n    return y_diff","cf68faa9":"train_dataset=Dataset(train_df, 'train')\nval_dataset=Dataset(train_df, 'train')\n\n\ntrain_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)","a2a2260d":"train_losses=[]\nval_losses=[]\nmodels=[]\nfor i in range(5):\n    best_loss=None\n    max_patience=5\n    patience=7\n    epochs=30\n\n    model=Model(100, 128)\n    optimizer=torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-6)\n    scheduler=torch.optim.lr_scheduler.OneCycleLR(optimizer, \n                                                  max_lr=2e-4,\n                                                  epochs=epochs,\n                                                  steps_per_epoch=len(train_dataloader))\n    mse_loss=nn.MSELoss(reduction='mean')\n\n    for e in range(epochs):\n        if patience==0:\n            print(\"Stoping at the epoch:{}\".format(e+1))\n            break\n        t1=time.time()\n        epoch_loss=train_epoch(model, optimizer, mse_loss, train_dataloader, scheduler)\n        val_loss=get_val_loss(model, val_dataloader)\n        t2=time.time()\n\n        if (best_loss is None) or (val_loss<=best_loss):\n            best_loss=val_loss\n            patience=max_patience\n            torch.save(model, 'model_{}.pt'.format(i+1))\n        if val_loss > best_loss:\n            patience-=1\n        print('Epoch:{} | Time Taken: {:.2f} | Train LOSS:{:.4f} | ValLoss:{:.4f}'.format(e+1, (t2-t1)\/60, epoch_loss, val_loss))\n    model=torch.load('model_{}.pt'.format(i+1))\n    models.append(model)","f521fcfa":"def infer(models, dataloader):\n    preds=[]\n    for X in dataloader:\n        y_hat=torch.zeros(X.shape[0])\n        for model in models:\n            model.eval()\n            with torch.no_grad():\n                y=model(X).view(-1)\n                y_hat+=(target_std*y) + target_mean\n            \n        preds+=list(y_hat.numpy()\/len(models))\n    return preds","d9c98291":"infer_train_dataset=Dataset(train_df, 'test')\ninfer_train_dataloader=torch.utils.data.DataLoader(infer_train_dataset, batch_size=200, shuffle=False)\ntrain_df['preds'] = infer(models, infer_train_dataloader)\ntrain_df[['id', 'target', 'normalized_target', 'preds']].head()","78999413":"(np.sqrt((train_df.preds-train_df.target)**2)).mean()","073dd248":"_, ax=plt.subplots(2, 1)\nsns.boxplot(data=train_df, x='target', ax=ax[0])\nsns.boxplot(data=train_df, x='preds', ax=ax[1])\n","a7157d7f":"sns.histplot(train_df, x='target', bins=100, color='red')\nsns.histplot(train_df, x='preds', bins=100,)\n","56a0df6c":"train_df.to_csv('train_with_preds.csv', index=False)","85e08bbd":"test_df=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df['doc']=test_df.excerpt.apply(tokenizer)\ntest_df.head()","5f36a2e9":"infer_test_dataset=Dataset(test_df, 'test')\ninfer_test_dataloader=torch.utils.data.DataLoader(infer_test_dataset, batch_size=200, shuffle=False)\ntest_df['target'] = infer(models, infer_test_dataloader)\n","d8978574":"submission_df=test_df[['id', 'target']].copy()\nsubmission_df.head()","65185a0f":"submission_df.to_csv('submission.csv', index=False)","96fee28f":"# Submission","e8d0db8b":"# Model","cf33f1a2":"# Load Glove 100-d vectors","f3b1eac6":"lets us consider all the words that appear atleast in 5 documents","04ea5ceb":"# Configuration"}}