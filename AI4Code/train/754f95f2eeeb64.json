{"cell_type":{"3269aa83":"code","5e947cda":"code","082d4c9a":"code","13983395":"code","f9cc0172":"code","ca31686b":"code","64bd446d":"code","02afbe62":"code","ea330d35":"code","d052beb0":"code","970d3a65":"code","0717505b":"code","ecdf7c04":"code","4f1323f8":"code","a0d6056f":"code","fc1a5ad0":"code","82f5b4d0":"code","81258f2d":"code","d9562820":"code","a3051e8a":"markdown","06fe0bb6":"markdown","9d55dafe":"markdown","cda5e541":"markdown","78c47929":"markdown","a924c87d":"markdown","d2d3508e":"markdown","a0717515":"markdown","8b260ad4":"markdown","f82c0694":"markdown","e5c53361":"markdown"},"source":{"3269aa83":"!pip install tensorflow_hub\n!pip install bert-for-tf2\n!pip install tensorflow\n!pip install sentencepiece\n!pip install transformers","5e947cda":"try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\nimport bert\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tokenizers import BertWordPieceTokenizer\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\nfrom transformers import DistilBertModel,DistilBertTokenizer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.keras.layers import Dense, Input\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras import backend as K\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","082d4c9a":"BertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","13983395":"import re\nTAG_RE = re.compile(r'<[^>]+>')\n\ndef remove_tags(text):\n    return TAG_RE.sub('', text)\n\ndef preprocess_text(sen):\n    # Removing html tags\n    sentence = remove_tags(sen)\n\n    # Remove punctuations and numbers\n    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n\n    # Single character removal\n    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n\n    # Removing multiple spaces\n    sentence = re.sub(r'\\s+', ' ', sentence)\n\n    return sentence","f9cc0172":"def tokenize_bert(data):\n    tokenized = data.apply((lambda x: tokenizer.convert_tokens_to_ids(['[CLS]']) + tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x))))\n    return tokenized\ndef pad_mask(data_tokenized,max_len):\n    padded = tf.keras.preprocessing.sequence.pad_sequences(data_tokenized, maxlen=max_len, dtype='int32', padding='post',value=0.0)\n    masked = np.where(padded!=0,1,0)\n    return padded, masked\ndef get_max_len(data):\n    max_len = 0\n    for val in data:\n        tmp = len(tokenizer.tokenize(val))\n        if tmp > max_len:\n            max_len = tmp\n    return max_len","ca31686b":"def summarize_model(history):\n    pyplot.subplot(211)\n    pyplot.title('Loss')\n    pyplot.plot(history.history['loss'], label='train')\n    pyplot.plot(history.history['val_loss'], label='test')\n    pyplot.legend()\n    # plot accuracy during training\n    pyplot.subplot(212)\n    pyplot.title('Accuracy')\n    pyplot.plot(history.history['accuracy'], label='train')\n    pyplot.plot(history.history['val_accuracy'], label='test')\n    pyplot.legend()\n    pyplot.show()","64bd446d":"def encode(df):\n    tweet = tf.ragged.constant([tokenizer.convert_tokens_to_ids(tokenizer.tokenize(s)) for s in df])\n    cls1 = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*tweet.shape[0]\n    input_word_ids = tf.concat([cls1, tweet], axis=-1)\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    type_cls = tf.zeros_like(cls1)\n    type_tweets = tf.zeros_like(tweet)\n    input_type_ids = tf.concat([type_cls, type_tweets], axis=-1).to_tensor()\n    \n    inputs = {\n      'input_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n    return inputs","02afbe62":"train = pd.read_csv(\"\/kaggle\/input\/multi-label-classification-tweets-preprocessing\/cleaned_training_tweets.csv\")\nvalidate = pd.read_csv(\"\/kaggle\/input\/multi-label-classification-tweets-preprocessing\/cleaned_develop_tweets.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/multi-label-classification-tweets-preprocessing\/cleaned_testing_tweets.csv\")\ntrain.info()","ea330d35":"train[\"clean\"]  = train[\"clean\"].apply(lambda x: preprocess_text(x) )\nvalidate[\"clean\"]  = validate[\"clean\"].apply(lambda x: preprocess_text(x) )\ntest[\"clean\"]  = test[\"clean\"].apply(lambda x: preprocess_text(x) )\n\nprint(\"length of train set:\",len(train))\nprint(\"length of test set:\",len(test))\nprint(\"length of validation set:\",len(validate))","d052beb0":"#load labels\nfrom sklearn.preprocessing import MultiLabelBinarizer\nem=['anger', 'anticipation', 'disgust', 'fear','joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise','trust']\n#reformate emotion classes\ndef labelling_classes(df):\n    arr=[]\n    for i,val in enumerate(df.iterrows()):\n        lbl=[]\n        for e in em:\n            if df.iloc[i][e]==1:\n                lbl.append(e)\n        arr.append(lbl)\n    return arr\n   \n#define classes \ntrain[\"classes\"]=np.array(labelling_classes(train))\nvalidate[\"classes\"]=np.array(labelling_classes(validate))\n\nmlb = MultiLabelBinarizer(classes=(\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"))\ny_enc = mlb.fit_transform(train[\"classes\"])\n\nmlb = MultiLabelBinarizer(classes=(\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"))\nydev_enc = mlb.fit_transform(validate[\"classes\"])","970d3a65":"all_df = pd.concat([train,validate,test])","0717505b":"#get max length of input data\nmax_len = get_max_len(train[\"clean\"]) + 1\n#encode and prepare data for Bert input\nencode_ds_all = encode(all_df[\"clean\"])","ecdf7c04":"encode_ds_tr = {'input_ids':encode_ds_all[\"input_ids\"][0:6838,:],\n                'input_mask':encode_ds_all[\"input_mask\"][0:6838,:],\n                'input_type_ids':encode_ds_all[\"input_type_ids\"][0:6838,:]}\nencode_ds_tr","4f1323f8":"encode_ds_val = {'input_ids':encode_ds_all[\"input_ids\"][6838:7724,:],\n                'input_mask':encode_ds_all[\"input_mask\"][6838:7724,:],\n                'input_type_ids':encode_ds_all[\"input_type_ids\"][6838:7724,:]}\nencode_ds_val","a0d6056f":"def build_bert(max_len):\n    input_ids = keras.layers.Input(shape=(max_len,), name=\"input_ids\", dtype=tf.int32)\n    input_typ = keras.layers.Input(shape=(max_len,), name=\"input_type_ids\", dtype=tf.int32)\n    input_mask = keras.layers.Input(shape=(max_len,), name=\"input_mask\", dtype=tf.int32)\n    bert_inputs = {\"input_ids\": input_ids, \"input_mask\": input_mask,'input_type_ids':input_typ}\n    ## BERT encoder\n    bert_model = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", trainable=True,name='keraslayer')\n    pooled_output, _ = bert_model([input_ids, input_mask,input_typ])\n    out =  keras.layers.Dense(128, activation='relu',name='last_layer')(pooled_output)\n    out =  keras.layers.Dense(11, activation='sigmoid')(out)\n    model = keras.Model(inputs=bert_inputs,outputs=out)\n    \n    return model\n\nmodel = build_bert(max_len)\nmodel.summary()","fc1a5ad0":"tf.keras.utils.plot_model(model, show_shapes=True, dpi=48)","82f5b4d0":"loss = tf.keras.losses.BinaryCrossentropy (from_logits=False)\noptimizer = keras.optimizers.Adam(lr=1e-6,decay=1e-6\/32)\nmodel.compile(optimizer=optimizer, loss=[loss, loss],metrics=[\"accuracy\"])\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True)\nearlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy')\nfine_history = model.fit(encode_ds_tr, y_enc, validation_split=0.34,shuffle=True,\n                          epochs=5,batch_size=32,verbose=1)","81258f2d":"encode_ds_ts = {'input_ids':encode_ds_all[\"input_ids\"][7724:,:],\n                'input_mask':encode_ds_all[\"input_mask\"][7724:,:],\n                'input_type_ids':encode_ds_all[\"input_type_ids\"][7724:,:]}\nencode_ds_ts","d9562820":"y_pred=model.predict(encode_ds_val)\ny_pred = y_pred.round()\nprint(classification_report(ydev_enc,y_pred))","a3051e8a":"# Import Needed Libraries","06fe0bb6":"# Install Needed libraries","9d55dafe":"Transfer learning is when a model developed for one task is reused to work on a second task. \nFine tuning is one approach to transfer learning.\n\nBERT (Bidirectional Encoder Representations from Transformers) is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. So, training a BERT model from scratch on a small dataset would result in overfitting.\n\nSo, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning.\n\u201cBERT stands for Bidirectional Encoder Representations from Transformers. It is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of NLP tasks.\u201d\nDifferent Fine-Tuning Techniques\nTrain the entire architecture \u2013 We can further train the entire pre-trained model on our dataset and feed the output to a softmax layer. In this case, the error is back-propagated through the entire architecture and the pre-trained weights of the model are updated based on the new dataset.\nTrain some layers while freezing others \u2013 Another way to use a pre-trained model is to train it partially. What we can do is keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained.\nFreeze the entire architecture \u2013 We can even freeze all the layers of the model and attach a few neural network layers of our own and train this new model. Note that the weights of only the attached layers will be updated during model training.\n","cda5e541":"# Prepare Data for Bert ","78c47929":"# load Bert tokenizer","a924c87d":"# Load Data","d2d3508e":"# Adjust Fine tune bert model settings","a0717515":"# Helper Functions","8b260ad4":"# Build Model","f82c0694":"# Evaluation","e5c53361":"# Data Statistics"}}