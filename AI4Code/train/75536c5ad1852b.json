{"cell_type":{"58cb9648":"code","4c9c686d":"code","66a46d67":"code","deaa7228":"code","abc1a5d6":"code","8a60cef1":"code","d08b7c2b":"code","478b6198":"code","fb5ba96b":"code","75b123bf":"code","5f02c825":"code","3b6885de":"code","e2a81363":"code","a3eefee1":"code","8c1d8688":"code","0092d12c":"code","3ecf02c5":"code","9c958f76":"code","4df57d36":"code","419e114d":"code","db9b800f":"code","fe2be744":"code","ff4fb8e3":"code","55464457":"code","2487fed6":"code","2c21fc6b":"code","3d79b3ee":"code","b6623d0c":"code","53b95ab2":"code","cc25035e":"code","d9670c16":"code","0adb6d16":"code","2a60e34b":"code","927129df":"code","c209ac4b":"code","2ec8f041":"code","11834c9c":"code","4b273cac":"code","865e9507":"code","fd8e8f4a":"code","6d397816":"code","11c9f81d":"code","709fdfdd":"code","5bce1453":"code","82aa1670":"code","51ddaf7d":"code","c8104702":"code","cdc52bb6":"code","0df89216":"code","1d26f407":"code","68abd51a":"code","c7b5b04f":"code","b24dedab":"code","24b6a4aa":"code","0122231f":"code","f10f39f7":"code","92cd2685":"code","7d9f5065":"code","e0cf1b3b":"code","5cb7872f":"code","6d185c87":"code","c38c5886":"code","087045ac":"code","315cc5a9":"code","18d1ce94":"code","1e41b9e9":"code","1c10e921":"code","8bf94d7c":"code","68aba144":"code","deeb50cd":"code","bef1b8da":"code","04d8273b":"code","b33325c8":"code","8335b841":"code","2020c779":"code","1ad6dedc":"code","727be79d":"code","8cb66579":"code","d5244865":"code","2393edf3":"code","d8e64c84":"code","f13bd430":"code","7f34eb01":"code","b6022b9b":"code","0b22ca92":"code","81e317bf":"code","42846a66":"code","3c1ba864":"code","7cdcc408":"code","f9bfeaf1":"code","ba9450ee":"code","2099c843":"code","dfddbf73":"code","257faadf":"code","163f1fbd":"code","ca20f67e":"code","3f4e74d7":"code","1ec09264":"code","7f90bd44":"code","3b07da55":"code","ba9403b0":"code","2c53ef14":"code","bb3a8146":"code","e482fbef":"code","93bbdcfe":"code","f7bb9bbb":"code","fa5baf4a":"code","66c83ef0":"code","a3270d99":"code","819c5b75":"code","af5196f0":"code","15b6f2ab":"code","b679d76d":"code","e90a5851":"code","9d4fd41d":"code","026a5c50":"code","5301dd80":"code","29707705":"code","e4be8777":"code","c3cd1713":"code","1a6f2d1f":"code","0f96b28f":"code","841a5f52":"code","4d8381db":"code","cccb2dd1":"code","de541519":"code","96132e00":"code","9630f88b":"code","c1c891fa":"code","ae6e8f6d":"code","b2c2a562":"code","06c7a6cb":"code","d9a0f788":"code","36d86b0a":"code","d2a1a839":"code","9444d704":"code","3d589098":"code","bb8c0148":"code","53e030b8":"code","c600125a":"code","9f7996b0":"code","fb601e97":"code","41e0146f":"code","752c5dfb":"markdown","5baaaf85":"markdown","2888247c":"markdown","2870bd46":"markdown","3dde3b6e":"markdown","beed36b4":"markdown","b3f4ab36":"markdown","6e40f69a":"markdown","92f547b5":"markdown","dfa2386b":"markdown","705f5cf4":"markdown","5049bd41":"markdown","5a89aee8":"markdown","02f87f3a":"markdown","c8dda725":"markdown","90c33ec3":"markdown","8ca23a15":"markdown","afe77189":"markdown","e0f95370":"markdown","e1e283f1":"markdown","a2c70995":"markdown","8b339da4":"markdown","6c18258e":"markdown","7e5f355a":"markdown","b5751dce":"markdown","ac48692f":"markdown","caefc524":"markdown","b95bd603":"markdown","ca15887b":"markdown","0a9af77e":"markdown","e2fefe46":"markdown","07681b4a":"markdown","da5a1318":"markdown","6a8dd508":"markdown","6fe4fbb3":"markdown","9443d332":"markdown","69a67d00":"markdown","4326ac3d":"markdown","4661af99":"markdown","6746067f":"markdown","581d0a80":"markdown","fdc46ef6":"markdown","d1baca59":"markdown","471e0f4f":"markdown","fe54d51b":"markdown","1b602ea3":"markdown","804c6813":"markdown","3edeea3d":"markdown","74252b3c":"markdown","39b7134d":"markdown","1580cc1e":"markdown","52d5b819":"markdown","4ccd806f":"markdown","6fea407e":"markdown","e29851ac":"markdown","3bf908d2":"markdown"},"source":{"58cb9648":"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4c9c686d":"df=pd.read_csv('\/kaggle\/input\/seedsdata\/seeds.csv')\ndf.head()","66a46d67":"df.info()","deaa7228":"df.describe()","abc1a5d6":"sns.pairplot(df)","8a60cef1":"sns.pairplot(df,hue='seedType')","d08b7c2b":"plt.figure(figsize=(8,8))\ncor=df.corr()\nsns.heatmap(cor,annot=True,cmap='coolwarm')\nplt.ylim(8,0)","478b6198":"X=df.drop('ID',axis=1,inplace=True)","fb5ba96b":"a=df.groupby('seedType').count()\na","75b123bf":"feature_cols=['area','perimeter','compactness','lengthOfKernel','widthOfKernel','asymmetryCoefficient','lengthOfKernelGroove']","5f02c825":"X=feature_cols\ny=df['seedType']","3b6885de":"df.area.astype(float)","e2a81363":"df.info()","a3eefee1":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\ndf_new=ss.fit_transform(df)","8c1d8688":"X=df.drop('seedType',axis=1)","0092d12c":"y=df['seedType']","3ecf02c5":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=3,test_size=0.30)","9c958f76":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)","4df57d36":"print(f'Coefficients: {lin_reg.coef_}')\nprint(f'Intercept: {lin_reg.intercept_}')\nprint(f'R^2 score: {lin_reg.score(X, y)}')","419e114d":"lin_reg = LinearRegression()\nmodel = lin_reg.fit(X_train,y_train)\nprint(f'R^2 score for train: {lin_reg.score(X_train, y_train)}')\nprint(f'R^2 score for test: {lin_reg.score(X_test, y_test)}')","db9b800f":"import statsmodels.api as sm\n%matplotlib inline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\n\nX_constant = sm.add_constant(X)\nmodel = sm.OLS(y, X_constant).fit()\n\npredictions = model.predict(X_constant)\nmodel.summary()","fe2be744":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(fit_intercept=True,solver='liblinear',multi_class='ovr')\nmodel.fit(X_train,y_train)\ny_test_pred=model.predict(X_test)\ny_test_prob=model.predict_proba(X_test)","ff4fb8e3":"model","55464457":"predict=model.predict(X_train)","2487fed6":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve, classification_report","2c21fc6b":"print(classification_report(y_train,predict))","3d79b3ee":"print('AUC Value of the model:',roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","b6623d0c":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()","53b95ab2":"dt.fit(X,y)","cc25035e":"dt=DecisionTreeClassifier()\n\ndt.fit(X_train,y_train)","d9670c16":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","0adb6d16":"from sklearn.model_selection import GridSearchCV\n\ndtc=DecisionTreeClassifier()\n\n\n#from sklearn import preprocessing\n#y = preprocessing.label_binarize(y, classes=[0, 1, 2])\n\nparams={'max_depth':[2,3,4,5,6],\n        'min_samples_leaf':[1,2,3,4,5,6,7],\n        'min_samples_split':[2,3,4,5,6,7,8,9,10],\n        'criterion':['gini','entrophy']}\ngsearch=GridSearchCV(dtc,param_grid=params,cv=3,scoring='accuracy')","2a60e34b":"gsearch.fit(X,y)","927129df":"gsearch.best_params_","c209ac4b":"gs=pd.DataFrame(gsearch.cv_results_)\ngs.head()","2ec8f041":"dt=DecisionTreeClassifier(**gsearch.best_params_)\n\ndt.fit(X_train,y_train)","11834c9c":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","4b273cac":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n\ndtc=DecisionTreeClassifier()\nparams={'max_depth':sp_randint(2,20),\n        'min_samples_leaf':sp_randint(1,20),\n        'min_samples_split':sp_randint(2,40),\n        'criterion':['gini','entrophy']}\n\n\nrsearch=RandomizedSearchCV(dtc,param_distributions=params,\n                           cv=3,n_iter=200,scoring='accuracy')","865e9507":"rsearch.fit(X,y)","fd8e8f4a":"rsearch.best_params_","6d397816":"rs=pd.DataFrame(rsearch.cv_results_)\nrs.head()","11c9f81d":"dt=DecisionTreeClassifier(**rsearch.best_params_)\n\ndt.fit(X_train,y_train)","709fdfdd":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test) \n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","5bce1453":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=3,test_size=0.30)","82aa1670":"from sklearn.ensemble import RandomForestClassifier\n\nrfc=RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train,y_train)","51ddaf7d":"y_test_pred=rfc.predict(X_test)\ny_test_prob=rfc.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","c8104702":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nrfc=RandomForestClassifier()\n\nparams={'n_estimators':sp_randint(100,200),\n        'max_features':sp_randint(1,24),\n        'max_depth':sp_randint(2,10),\n        'min_samples_split':sp_randint(2,20),\n        'min_samples_leaf':sp_randint(1,20),\n        'criterion':['gini','entropy']}\n\nrsearch=RandomizedSearchCV(rfc,param_distributions=params,n_iter=50,cv=3,scoring='accuracy',\n                           random_state=3,return_train_score=True)\nrsearch.fit(X,y)","cdc52bb6":"rsearch.best_params_","0df89216":"pd.DataFrame(rsearch.cv_results_).head(5)","1d26f407":"rfc=RandomForestClassifier(**rsearch.best_params_,random_state=3)\nrfc.fit(X_train,y_train)","68abd51a":"y_test_pred=rfc.predict(X_test)\ny_test_prob=rfc.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","c7b5b04f":"from sklearn.model_selection import GridSearchCV\nrfc=RandomForestClassifier()\n\nparams={'max_depth':[2,3,4,5,6],\n        'min_samples_leaf':[1,2,3,4,5,6,7],\n        'min_samples_split':[2,3,4,5,6,7,8,9,10],\n        'criterion':['gini','entrophy']}\ngsearch=GridSearchCV(dtc,param_grid=params,cv=3,scoring='accuracy')","b24dedab":"gsearch.fit(X,y)","24b6a4aa":"gsearch.best_params_","0122231f":"gs=pd.DataFrame(gsearch.cv_results_)\ngs.head()","f10f39f7":"rfc=RandomForestClassifier(**gsearch.best_params_)\n\nrfc.fit(X_train,y_train)","92cd2685":"y_test_pred=dt.predict(X_test)\ny_test_prob=dt.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","7d9f5065":"from sklearn.naive_bayes import MultinomialNB\nmnb=MultinomialNB()","e0cf1b3b":"mnb.fit(X_train,y_train)\n\ny_test_pred=mnb.predict(X_test)\ny_test_prob=mnb.predict_proba(X_test)\n\nprint ('Confusion Matrix -Test :','\\n',confusion_matrix(y_test,y_test_pred))\n\nprint ('Overall accuracy -Test :',accuracy_score(y_test,y_test_pred))\n\nprint ('AUC -Test :', roc_auc_score(y_test,y_test_prob, multi_class='ovr'))","5cb7872f":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(random_state=3)\n\nada.fit(X_train,y_train)\n\n\ny_test_pred=ada.predict(X_test)\ny_test_prob = ada.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","6d185c87":"import lightgbm as lgb\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform","c38c5886":"lgbm = lgb.LGBMClassifier()","087045ac":"params = { 'n_estimators' : sp_randint(50,200),\n        'max_depth' : sp_randint(2,15),\n         'learning_rate' : sp_uniform(0.201,0.5),\n         'num_leaves' : sp_randint(20,50)}","315cc5a9":"rsearch = RandomizedSearchCV(lgbm, param_distributions=params, cv=3, n_iter=200, n_jobs=-1, random_state=3)","18d1ce94":"rsearch.fit(X,y)","1e41b9e9":"rsearch.best_params_","1c10e921":"lgbm = lgb.LGBMClassifier(**rsearch.best_params_)\nlgbm.fit(X_train,y_train)\n\ny_test_pred=lgbm.predict(X_test)\ny_test_prob = lgbm.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","8bf94d7c":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver='liblinear')\n\nlr.fit(X_train,y_train)\n\ny_test_pred=lr.predict(X_test)\ny_test_prob = lr.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","68aba144":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\nknn = KNeighborsClassifier()\n\nparams={'n_neighbors' : sp_randint(1,15),'p' : sp_randint(1,5)}\n\nrsearch_knn = RandomizedSearchCV(knn, param_distributions=params, cv =3,n_iter=50,n_jobs=-1,return_train_score=True, random_state=3)\n\nrsearch_knn.fit(X,y)","deeb50cd":"rsearch_knn.best_params_","bef1b8da":"knn = KNeighborsClassifier(**rsearch_knn.best_params_)","04d8273b":"knn.fit(X_train,y_train)\ny_train_pred=knn.predict(X_train)\n\ny_test_pred=knn.predict(X_test)\ny_test_prob = knn.predict_proba(X_test)\n\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","b33325c8":"from sklearn.ensemble import VotingClassifier\n\nlr=LogisticRegression(solver='liblinear')\nknn=KNeighborsClassifier(**rsearch_knn.best_params_)\ndt=DecisionTreeClassifier(**gsearch.best_params_)","8335b841":"## Hard Voting\n\nclf = VotingClassifier(estimators=[('lr',lr),('knn',knn),('dt',dt)], voting='hard')\n\nclf.fit(X_train, y_train)\n\ny_train_pred = clf.predict(X_train)\ny_test_pred = clf.predict(X_test)\n\nprint(\"Accuracy score Train : \",accuracy_score(y_train,y_train_pred))\nprint(\"Accuracy score Test : \",accuracy_score(y_test,y_test_pred))\nprint(\"\\n\")\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","2020c779":"#Soft Voting -- equal weightages\n\nclf =VotingClassifier(estimators=[('lr',lr),('knn',knn),('dt',dt)],voting='soft')\n\nclf.fit(X_train,y_train)\n\ny_test_pred=clf.predict(X_test)\ny_test_prob = clf.predict_proba(X_test)\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","1ad6dedc":"#Soft Voting -- Different weightages\n\nclf =VotingClassifier(estimators=[('lr',lr),('knn',knn),('dt',dt)],weights=[1,2,3],voting='soft')\nclf.fit(X_train,y_train)\n\ny_test_pred=clf.predict(X_test)\ny_test_prob = clf.predict_proba(X_test)\n\nprint('Confusion Matrix - Test : ','\\n' , confusion_matrix(y_test,y_test_pred))\nprint('Classification Report - Test : ','\\n' , classification_report(y_test,y_test_pred))\nprint('Overall Accuracy - Test : ' , accuracy_score(y_test,y_test_pred))\nprint('AUC - Test : ' , roc_auc_score(y_test,y_test_prob,multi_class='ovr'))","727be79d":"df2=df.copy()","8cb66579":"from scipy.stats import zscore\ndf_scaled = df2.apply(zscore)","d5244865":"df_scaled.head()","2393edf3":"from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters = 3)","d8e64c84":"model","f13bd430":"cluster_range = range( 1, 15 )\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 10 )\n  clusters.fit(df_scaled)\n # labels = clusters.labels_\n # centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","7f34eb01":"# Elbow plot\n\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","b6022b9b":"kmeans = KMeans(n_clusters=3, n_init = 15, random_state=2345)","0b22ca92":"kmeans.fit(df_scaled)","81e317bf":"centroids = kmeans.cluster_centers_","42846a66":"centroids","3c1ba864":"centroid_df = pd.DataFrame(centroids, columns = list(df_scaled) )","7cdcc408":"centroid_df","f9bfeaf1":"df_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))\n\ndf_labels['labels'] = df_labels['labels'].astype('category')","ba9450ee":"snail_df_labeled = df.join(df_labels)","2099c843":"df_analysis = (snail_df_labeled.groupby(['labels'] , axis=0)).head() \ndf_analysis","dfddbf73":"snail_df_labeled['labels'].value_counts() ","257faadf":"from mpl_toolkits.mplot3d import Axes3D","163f1fbd":"fig = plt.figure(figsize=(8, 6))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=20, azim=100)\nkmeans.fit(df_scaled)\nlabels = kmeans.labels_\nax.scatter(df_scaled.iloc[:, 0], df_scaled.iloc[:, 1], df_scaled.iloc[:, 3],c=labels.astype(np.float), edgecolor='k')\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Length')\nax.set_ylabel('Height')\nax.set_zlabel('Weight')\nax.set_title('3D plot of KMeans Clustering')","ca20f67e":"# Now we know our best k value is 3, I am creating a new kmeans model:\nkmeans2 = KMeans(n_clusters=3)\n\n# Training the model:\nclusters = kmeans2.fit_predict(df)\n\n# Adding a label feature with the predicted class values:\ndf_k = df.copy(deep=True)\ndf_k['label'] = clusters","3f4e74d7":"plt.figure(figsize=(7,5))\nax1 = plt.subplot(1,2,1)\nplt.title('Original Classes')\nsns.scatterplot(x='area', y='perimeter', hue='seedType', style='seedType', palette='plasma',data=df, ax=ax1)\n\nax2 = plt.subplot(1,2,2)\nplt.title('Predicted Classes')\nsns.scatterplot(x='area', y='perimeter', hue='label', style='label', palette='plasma',data=df_k, ax=ax2)\nplt.show()","1ec09264":"print('Original Data Classes:')\nprint(df.seedType.value_counts())\nprint('-' * 30)\nprint('Predicted Data Classes:')\nprint(df_k.label.value_counts())","7f90bd44":"from scipy.cluster.hierarchy import linkage, dendrogram\nplt.figure(figsize=[10,10])\nmerg = linkage(df, method='ward')\ndendrogram(merg, leaf_rotation=90)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Euclidean Distances')\nplt.show()","3b07da55":"from sklearn.cluster import AgglomerativeClustering\n\nhie_clus = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\ncluster2 = hie_clus.fit_predict(df)\n\ndf_h = df.copy(deep=True)\ndf_h['label'] = cluster2","ba9403b0":"plt.title('Original Classes')\nsns.scatterplot(x='area', y='perimeter', hue='seedType', style='seedType', data=df,palette='viridis')\nplt.show()\nplt.title('K-Means Classes')\nsns.scatterplot(x='area', y='perimeter', hue='label', style='label', data=df_k,palette='viridis')\nplt.show()\nplt.title('Hierarchical Classes')\nsns.scatterplot(x='area', y='perimeter', hue='label', style='label', data=df_h,palette='viridis')\nplt.show()","2c53ef14":"print('Original Data Classes:')\nprint(df.seedType.value_counts())\nprint('-' * 30)\nprint('K-Means Predicted Data Classes:')\nprint(df_k.label.value_counts())\nprint('-' * 30)\nprint('Hierarchical Predicted Data Classes:')\nprint(df_h.label.value_counts())","bb3a8146":"from __future__ import print_function\n%matplotlib inline\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX, y = make_blobs(n_samples=500,\n                  n_features=2,\n                  centers=4,\n                  cluster_std=1,\n                  center_box=(-10.0, 10.0),\n                  shuffle=True,\n                  random_state=1)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.Spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.Spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c=\"white\", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()","e482fbef":"df_k.sample(5)","93bbdcfe":"x= df_k.drop('label',axis=1)\ny= df_k['label']","f7bb9bbb":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=test_size, random_state=seed)","fa5baf4a":"from sklearn.preprocessing import StandardScaler\nindependent_scalar = StandardScaler()\nx_train = independent_scalar.fit_transform (x_train) #fit and transform\nx_validate = independent_scalar.transform (x_validate) # only transform","66c83ef0":"from sklearn.tree import DecisionTreeClassifier \n#DecisionTreeClassifier is the corresponding Classifier\nDtree = DecisionTreeClassifier(max_depth=3)\nDtree.fit (x_train, y_train)","a3270d99":"predictValues_train = Dtree.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\npredictValues_validate = Dtree.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","819c5b75":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","af5196f0":"\nRFclassifier = RandomForestClassifier(n_estimators = 100, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","15b6f2ab":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","b679d76d":"RFclassifier = RandomForestClassifier(n_estimators = 11, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","e90a5851":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","9d4fd41d":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","026a5c50":"from sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore","5301dd80":"x= df_k.drop('label',axis=1)\ny= df_k['label']","29707705":"x_standardize = x.apply(zscore)","e4be8777":"#KNN only takes array as input hence it is importanct to convert dataframe to array\nx1 = np.array(x_standardize)\ny1 = np.array(y)","c3cd1713":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x1, y1, test_size=test_size, random_state=seed)","1a6f2d1f":"KNN = KNeighborsClassifier(n_neighbors= 8 , weights = 'uniform', metric='euclidean')\nKNN.fit(x_train, y_train)","0f96b28f":"predictValues_train = KNN.predict(x_train)\nprint(predictValues_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\nprint(\"Train Accuracy  :: \",accuracy_train)","841a5f52":"predictValues_validate = KNN.predict(x_validate)\nprint(predictValues_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","4d8381db":"df_h.sample(5)","cccb2dd1":"x= df_k.drop('label',axis=1)\ny= df_k['label']","de541519":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=test_size, random_state=seed)","96132e00":"from sklearn.preprocessing import StandardScaler\nindependent_scalar = StandardScaler()\nx_train = independent_scalar.fit_transform (x_train) #fit and transform\nx_validate = independent_scalar.transform (x_validate) # only transform","9630f88b":"from sklearn.tree import DecisionTreeClassifier \n#DecisionTreeClassifier is the corresponding Classifier\nDtree = DecisionTreeClassifier(max_depth=3)\nDtree.fit (x_train, y_train)","c1c891fa":"predictValues_train = Dtree.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\npredictValues_validate = Dtree.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","ae6e8f6d":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","b2c2a562":"RFclassifier = RandomForestClassifier(n_estimators = 100, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","06c7a6cb":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","d9a0f788":"RFclassifier = RandomForestClassifier(n_estimators = 11, random_state = 0,min_samples_split=5,criterion='gini',max_depth=5)\nRFclassifier.fit(x_train, y_train)","36d86b0a":"predictValues_validate = RFclassifier.predict(x_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\n\npredictValues_train = RFclassifier.predict(x_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\n\n\nprint(\"Train Accuracy  :: \",accuracy_train)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","d2a1a839":"print('Classification Report')\nprint(classification_report(y_validate, predictValues_validate))","9444d704":"from sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore","3d589098":"x= df_k.drop('label',axis=1)\ny= df_k['label']","bb8c0148":"x_standardize = x.apply(zscore)","53e030b8":"#KNN only takes array as input hence it is importanct to convert dataframe to array\nx1 = np.array(x_standardize)\ny1 = np.array(y)","c600125a":"test_size = 0.30 # taking 70:30 training and test set\nseed = 7  # Random numbmer seeding for reapeatability of the code\nx_train, x_validate, y_train, y_validate = train_test_split(x1, y1, test_size=test_size, random_state=seed)","9f7996b0":"KNN = KNeighborsClassifier(n_neighbors= 8 , weights = 'uniform', metric='euclidean')\nKNN.fit(x_train, y_train)","fb601e97":"predictValues_train = KNN.predict(x_train)\nprint(predictValues_train)\naccuracy_train=accuracy_score(y_train, predictValues_train)\nprint(\"Train Accuracy  :: \",accuracy_train)","41e0146f":"predictValues_validate = KNN.predict(x_validate)\nprint(predictValues_validate)\naccuracy_validate=accuracy_score(y_validate, predictValues_validate)\nprint(\"Validation Accuracy  :: \",accuracy_validate)","752c5dfb":"# Agglomerative Clustering:","5baaaf85":"# CLUSTERING:","2888247c":"# LOGISTIC REGRESSION:","2870bd46":"**Every attribute median and max values of all features:**","3dde3b6e":"# LINEAR REGRESSION:","beed36b4":"# MULTINOMINAL NAIVE BAYES:","b3f4ab36":"# STACKING the results of 3 learners (Decision Tree, K-NN , Logistic Regression)","6e40f69a":"**Splitting independent and dependent variables:**","92f547b5":"# ADA BOOST CLASSIFIER:","dfa2386b":"# K-NN","705f5cf4":"# END","5049bd41":"# Heat Map:","5a89aee8":"# LINEAR REGRESSION - OLS:","02f87f3a":"# K-Means: ","c8dda725":"# DECISION TREE CLASSIFIER:","90c33ec3":"# Decision Tree - HYPER PARAMETER TUNING USING RANDOMIZED SEARCH:","8ca23a15":"Measurements of geometrical properties of kernels belonging to three different varieties of wheat-  Kama,Rosa and Canadian. \n\nA soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes.","afe77189":"**Standardization the data:**","e0f95370":"**Train - Test Split:**","e1e283f1":"out of the all the boosting techniques used, LightGBM works the best.","a2c70995":"___________________________________________________________________________________________________________________________","8b339da4":"**Having dealt the problem wrt classification, lets check on how the data performs when the problem is processed with clustering algorithms and techniques.**","6c18258e":"___________________________________________________________________________________________________________________________","7e5f355a":"In agglomerative clustering, Random forest is the best generalized model with accuracy of 0.98%.","b5751dce":"**NOTE:**","ac48692f":"# RANDOM FOREST:","caefc524":"**Checking the data type of each attribute and if any missing value in each feature:**","b95bd603":"# K - MEANS:","ca15887b":"# **Decision Tree - HYPER PARAMETER TUNING USING GRID SEARCH:**","0a9af77e":"**Hierarchical Clustering Algorithm:**","e2fefe46":"# RANDOM FOREST CLASSIFIER:","07681b4a":"# DECISION TREE CLASSIFIER:","da5a1318":"**Comparing Original, K-Means and Hierarchical Clustered Classes:**","6a8dd508":"# K-NN :","6fe4fbb3":"Building a model using K-means algorithms of clustering, Random forest classifier has the highest accuracy in f1 score without underfitting or overfitting values.","9443d332":"**Reading the seeds dataset:**","69a67d00":"With the analysis, we can see the highest accuracy is for the clusters with  n=4 and the silhouette score is about 0.65.","4326ac3d":"# K-NN ALGORITHM:","4661af99":"Random Forest classifier and the hyper parameters have been tuned using grid search and randomised search. Out of which rsearch has the highest accuracy in test data.","6746067f":"# SEEDS DATA SET:","581d0a80":"# Decision Tree Classifier","fdc46ef6":"The analysis for the given dataset is done in both the perspectives of classification and clustering algorithms.\n\nThe maximum accuracy and the best models are evaluated using different techniques and test scores. \n\nAs per the necessity and requirement the obtained results can be modified further to proceed.","d1baca59":"Stacking 3 learners for better results, with the scaled data - we can find the overall accuracy and Auc results of the test data has been better in terms of Soft voting either using equal weightages or different weightages than Hard voting technique.","471e0f4f":"**Importing necessary Libraries:**","fe54d51b":"**HIERARCHICAL CLUSTERING ALGORITHM:**\n\n**Creating the Dendrogram:**\n\nWe use dendrogram to find how many classes we have in our data set.","1b602ea3":"# Build An Classification model with Hierarchical clustering:","804c6813":"# Random Forest - HYPER PARAMETER TUNING USING GRID SEARCH:","3edeea3d":"Let's deal the problem with Classification first.","74252b3c":"# Silhouette analysis for K-Means clustering:","39b7134d":"Desicion Tree classifier and hyperparameter tuning using grid search and randomised search all 3 give an accuracy of 0.95 and AUC is about 0.96  for the test data.","1580cc1e":"# Random Forest -  HYPER PARAMETER TUNING USING RANDOMIZED SEARCH:","52d5b819":"# RANDOM FOREST:","4ccd806f":"**Comparing Original Classes and K-Means Algorithm Classes:**\n\nFor visualization I will use only two features (area and perimeter) for the original and predicted datasets. Different classes will have seperate color and styles.","6fea407e":"# LIGHTGBM CLASSIFIER:","e29851ac":"# Build An Classification model with Non - Hierarchical clustering:","3bf908d2":"**From the dendrogram we can read there are 3 classes in our data set.**"}}