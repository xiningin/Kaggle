{"cell_type":{"21ce92f9":"code","61c451ee":"code","78817a70":"code","c08c2d65":"code","52f1f57b":"code","100608bf":"code","dbf5fa1a":"code","868ac544":"code","416fd4d1":"code","ff83b31f":"code","9fa8690f":"code","0da48460":"code","3ee7c8db":"code","152dba76":"code","50a630ab":"code","1b438cae":"code","0af111fd":"code","929dda59":"code","3157acfc":"code","dbb50b52":"code","07cabcf4":"code","aaba0595":"code","24f15a9c":"code","a78a3389":"code","f15b5682":"code","551aac0e":"code","4a0bf4d3":"code","32b9c1a5":"code","b95fdf00":"code","ee2a7966":"code","f3ff3d74":"code","a47481de":"markdown","4828744a":"markdown","48541cb1":"markdown","7124b7b6":"markdown","ed237dec":"markdown","f29fdad7":"markdown","6248e3a8":"markdown","348b926a":"markdown","21366ca1":"markdown","b2503f12":"markdown","689a3671":"markdown","d7520ffe":"markdown"},"source":{"21ce92f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61c451ee":"# Add environment Packages paths to conda\nimport os, sys\n# env_name = \"food_review\"\n# sys.path.append(f\"C:\\\\Environments\\\\{env_name}\\\\lib\\\\site-packages\\\\\")\n\nimport pandas as pd\nimport numpy as np\n\n# Text preprocessing packages\nimport nltk # Text libarary\n# nltk.download('stopwords')\nimport string # Removing special characters {#, @, ...}\nimport re # Regex Package\nfrom nltk.corpus import stopwords # Stopwords\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer # Stemmer & Lemmatizer\nfrom gensim.utils import simple_preprocess  # Text ==> List of Tokens\n\n# Text Embedding\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Saving Model\nimport pickle\n\n# Visualization Packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(font_scale=1.3)\n%matplotlib inline","78817a70":"%%time\ndf = pd.read_csv('\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv')\ndf = df[:10000]","c08c2d65":"f\"{df.shape[0]:,} Review\"","52f1f57b":"df.head()","100608bf":"cols = ['Text', 'Score']\ndf_text = df[cols].copy()\ndf_text.head()","dbf5fa1a":"df_text = df_text.drop_duplicates()","868ac544":"df_text.reset_index(drop=True, inplace=True)","416fd4d1":"df_text['target'] = df_text['Score'].apply(lambda x : 0 if x<3 else 1)\ndf_text.head()","ff83b31f":"sns.countplot(x='target', data= df_text)","9fa8690f":"# Sample from positive reviews Same number of negative reviews\nNEG_N = df_text.target.value_counts()[0]\ndf_pos = df_text[df_text['target'] == 1]['Text'].sample(NEG_N, replace=False)\ndf_text_balanced = pd.concat([df_text.iloc[df_pos.index], df_text[df_text.target == 0]])","0da48460":"## PLot the target again after balancing\n## Write your code here\nsns.countplot(x='target', data= df_text_balanced)","3ee7c8db":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")\nlemmatizer= WordNetLemmatizer()","152dba76":"## Clean your reviews using stemmer, lemmatizer & stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nporter_stemmer = PorterStemmer()\ndef stem_sentences(sentence):\n    tokens = sentence.split()\n    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n    return ' '.join(stemmed_tokens)\n\ndf_text_balanced['Text'] = df_text_balanced['Text'].apply(stem_sentences)","50a630ab":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n\n#df = pd.DataFrame(['this was cheesy', 'she likes these books', 'wow this is great'], columns=['text'])\ndf_text_balanced['Text'] = df_text_balanced.Text.apply(lemmatize_text)","1b438cae":"df_text_balanced['Text'] = df_text_balanced['Text'].apply(lambda x: [item for item in x if item not in stop_words])","0af111fd":"df_text_balanced.head()","929dda59":"X = df_text_balanced['Text']\ny = df_text_balanced['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","3157acfc":"## TFIDF embedding for the Description\n#from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n# fit on training (such vectorizer will be saved for deployment)\nvectorizer_tfidf =  vectorizer.fit(X.apply(lambda X: ' '.join(X)))\n# transform on training data\nX_train = vectorizer_tfidf.transform(X_train.apply(lambda X_train: ' '.join(X_train)))\n# transform on testing data\nX_test = vectorizer.transform(X_test.apply(lambda X_test: ' '.join(X_test)))","dbb50b52":"# See the dimensions of your data embeddings before entering to the model\nX_train.shape, X_test.shape","07cabcf4":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","aaba0595":"## initialize your Model\nclf = RandomForestClassifier(n_jobs=3) \n# Fit your Model on the Training Dataset\nclf.fit(X_train,y_train)\n# Predict on Test data\npreds = clf.predict(X_test)\n# Calculate Model Accuracy\nacc = accuracy_score(preds, y_test)\nprint(f\"Model Accuracy = {round(acc*100,2)}%\")","24f15a9c":"def raw_test(review, model, vectorizer):\n    # Clean Review\n    review_c = review\n    # Embed review using tf-idf vectorizer\n    embedding = vectorizer.transform([review_c])\n    # Predict using your model\n    prediction = model.predict(embedding)\n    # Return the Sentiment Prediction\n    return \"Positive\" if prediction == 1 else \"Negative\"","a78a3389":"review_1 = \"That's a good Dish, Good Job\"\nreview_2 = \"That's the worst Dish ever tasted\"","f15b5682":"raw_test(review_1, clf, vectorizer_tfidf)","551aac0e":"raw_test(review_2, clf, vectorizer_tfidf)","4a0bf4d3":"import pickle","32b9c1a5":"model_name = 'rf_model.pk'\nvectorizer_name = 'tfidf_vectorizer.pk'\nmodel_path = os.path.join('\/', model_name)\nvect_path = os.path.join('\/', vectorizer_name)\n\npickle.dump(clf, open(model_path,'wb')) ## Save model\npickle.dump(vectorizer_tfidf, open(vect_path, 'wb')) ## Save tfidf-vectorizer","b95fdf00":"loaded_model = pickle.load(open(model_path, 'rb'))\nloaded_vect = pickle.load(open(vect_path, 'rb'))","ee2a7966":"raw_test(review_1, loaded_model, loaded_vect)","f3ff3d74":"raw_test(review_2, loaded_model, loaded_vect)","a47481de":"### **Load model Again and test them**","4828744a":"### **Target Variable Pre-Processing**\n\ntarget will be\n\n* 0 if score < 3\n\n* 1 otherwise","48541cb1":"### **Text Pre-Processing**","7124b7b6":"### **Drop Duplicates**","ed237dec":"### **Split Test & Training Data**","f29fdad7":"### **Saving Models for Deployment**","6248e3a8":"### **Raw Instance Prediction**","348b926a":"#### **Sklearn framework steps**\n* init\n* fit\n* predict","21366ca1":"### **Modelling**","b2503f12":"### **Text Embedding**\n - Use `TfidfVectorizer`\n - `fit` on the training data only\n - `transform` on training and test","689a3671":"### **Plot Countplot for target Variable**","d7520ffe":"### **Balance Data Target**"}}