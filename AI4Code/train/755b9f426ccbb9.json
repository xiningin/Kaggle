{"cell_type":{"71562d9e":"code","e912d5b3":"code","c0e97d7f":"code","dad555b1":"code","096b1521":"code","3cd00790":"code","bac83a1c":"code","bdf216e3":"code","462dcec0":"code","98ea0bcb":"code","e4bf8394":"code","37770060":"code","6dfed1cb":"code","9d2cd655":"code","03a0fe3f":"code","3be549e8":"code","8755749b":"code","899a9e0b":"code","12bff346":"code","04ed3d40":"code","3e3f02c2":"code","969eb90d":"code","226e4217":"code","0ddca9ce":"code","fb6c616a":"code","eb1bb002":"code","5e4b98e2":"code","d69115c2":"code","cbb429a4":"code","0b45427e":"code","27642425":"markdown","42e1af67":"markdown","2dfc38d4":"markdown","4e4b7243":"markdown","0ee01422":"markdown","13e8e6e6":"markdown","0d1d0199":"markdown","6af5ff0d":"markdown","5448bd28":"markdown","cec9865c":"markdown","8dbbf269":"markdown","c332ecc7":"markdown","27d926b3":"markdown","0bdd8b6d":"markdown","a860acba":"markdown","b130fbae":"markdown","72e58054":"markdown","79c43426":"markdown","339c4830":"markdown","a51da5fd":"markdown","4bf669e5":"markdown","679fb338":"markdown","a6afe4ad":"markdown","190b8b82":"markdown","976bba33":"markdown","67e2d3cf":"markdown"},"source":{"71562d9e":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras as kr\n%matplotlib inline","e912d5b3":"# MNIST Dataset parameters.\nnum_features = 784 # data features (img shape: 28*28).\n\n# Training parameters.\nbatch_size = 128\nepochs = 50\n\n# Network Parameters\nhidden_1 = 128 # 1st layer num features.\nhidden_2 = 64 # 2nd layer num features (the latent dim).","c0e97d7f":"from tensorflow.keras.datasets import mnist, fashion_mnist\n\ndef load_data(choice='mnist', labels=False):\n    if choice not in ['mnist', 'fashion_mnist']:\n        raise ('Choices are mnist and fashion_mnist')\n    \n    if choice is 'mnist':\n        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n    else:\n        (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n    \n    X_train, X_test = X_train \/ 255., X_test \/ 255.\n    X_train, X_test = X_train.reshape([-1, 784]), X_test.reshape([-1, 784])\n    X_train = X_train.astype(np.float32, copy=False)\n    X_test = X_test.astype(np.float32, copy=False)\n    \n    if labels:\n        return (X_train, y_train), (X_test, y_test)\n    \n    return X_train, X_test\n\n\ndef plot_predictions(y_true, y_pred):    \n    f, ax = plt.subplots(2, 10, figsize=(15, 4))\n    for i in range(10):\n        ax[0][i].imshow(np.reshape(y_true[i], (28, 28)), aspect='auto')\n        ax[1][i].imshow(np.reshape(y_pred[i], (28, 28)), aspect='auto')\n    plt.tight_layout()","dad555b1":"def plot_digits(X, y, encoder, batch_size=128):\n    \"\"\"Plots labels and MNIST digits as function of 2D latent vector\n\n    Parameters:\n    ----------\n    encoder: Model\n        A Keras Model instance\n    X: np.ndarray\n        Test data\n    y: np.ndarray\n        Test data labels\n    batch_size: int\n        Prediction batch size\n    \"\"\"\n    # display a 2D plot of the digit classes in the latent space\n    z_mean, _, _ = encoder.predict(X, batch_size=batch_size)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y)\n    plt.colorbar()\n    plt.xlabel(\"z[0] Latent Dimension\")\n    plt.ylabel(\"z[1] Latent Dimension\")\n    plt.show()\n    \n    \ndef generate_manifold(decoder):\n    \"\"\"Generates a manifold of MNIST digits from a random noisy data.\n\n    Parameters:\n    ----------\n    decoder: Model\n        A Keras Model instance\n    \"\"\"\n    \n    # display a 30x30 2D manifold of digits\n    n = 30\n    digit_size = 28\n    figure = np.zeros((digit_size * n, digit_size * n))\n    \n    # linearly spaced coordinates corresponding to the 2D plot\n    # of digit classes in the latent space\n    grid_x = np.linspace(-4, 4, n)\n    grid_y = np.linspace(-4, 4, n)[::-1]\n    \n    for i, yi in enumerate(grid_y):\n        for j, xi in enumerate(grid_x):\n            z_sample = np.array([[xi, yi]])\n            x_decoded = decoder.predict(z_sample)\n            digit = x_decoded[0].reshape(digit_size, digit_size)\n            figure[i * digit_size: (i + 1) * digit_size,\n                   j * digit_size: (j + 1) * digit_size] = digit        \n    \n    plt.figure(figsize=(10, 10))\n    start_range = digit_size \/\/ 2\n    end_range = n * digit_size + start_range + 1\n    pixel_range = np.arange(start_range, end_range, digit_size)\n    sample_range_x = np.round(grid_x, 1)\n    sample_range_y = np.round(grid_y, 1)\n    \n    plt.xticks(pixel_range, sample_range_x)\n    plt.yticks(pixel_range, sample_range_y)\n    plt.xlabel(\"z[0] Latent Dimension\")\n    plt.ylabel(\"z[1] Latent Dimension\")\n    plt.imshow(figure, cmap='Greys_r')\n    plt.show()","096b1521":"inputs = kr.Input(shape=(num_features, ))\nencoder = kr.layers.Dense(hidden_1, activation='sigmoid')(inputs)\nencoder = kr.layers.Dense(hidden_2, activation='sigmoid')(encoder)\nencoder_model = kr.Model(inputs, encoder, name='encoder')\nencoder_model.summary()","3cd00790":"latent_dim = kr.Input(shape=(hidden_2, ))\ndecoder = kr.layers.Dense(hidden_1, activation='sigmoid')(latent_dim)\ndecoder = kr.layers.Dense(num_features, activation='sigmoid')(decoder)\ndecoder_model = kr.Model(latent_dim, decoder, name='decoder')\ndecoder_model.summary()","bac83a1c":"outputs = decoder_model(encoder_model(inputs))\nmnist_model = kr.Model(inputs, outputs )\nmnist_model.compile(optimizer='adam', loss='mse')\nmnist_model.summary()","bdf216e3":"X_train, X_test = load_data('mnist')\nmnist_model.fit(x=X_train, y=X_train, batch_size=batch_size, shuffle=False, epochs=epochs)","462dcec0":"y_true = X_test[:10]\ny_pred = mnist_model.predict(y_true)\nplot_predictions(y_true, y_pred)","98ea0bcb":"# Encoder\ninputs = kr.Input(shape=(num_features, ))\nencoder = kr.layers.Dense(hidden_1, activation='sigmoid')(inputs)\nencoder = kr.layers.Dense(hidden_2, activation='sigmoid')(encoder)\nencoder_model = kr.Model(inputs, encoder, name='encoder')\nencoder_model.summary()\n\n# Decoder\nlatent_dim = kr.Input(shape=(hidden_2, ))\ndecoder = kr.layers.Dense(hidden_1, activation='sigmoid')(latent_dim)\ndecoder = kr.layers.Dense(num_features, activation='sigmoid')(decoder)\ndecoder_model = kr.Model(latent_dim, decoder, name='decoder')\ndecoder_model.summary()\n\n# AE\noutputs = decoder_model(encoder_model(inputs))\nfmnist_model = kr.Model(inputs, outputs )\nfmnist_model.compile(optimizer='adam', loss='mse')\nfmnist_model.summary()","e4bf8394":"X_train, X_test = load_data('fashion_mnist')\nfmnist_model.fit(x=X_train, y=X_train, batch_size=batch_size, shuffle=False, epochs=epochs)","37770060":"y_true = X_test[:10]\ny_pred = fmnist_model.predict(y_true)\nplot_predictions(y_true, y_pred)","6dfed1cb":"def sampling(args):\n    \"\"\"Reparameterization trick. Instead of sampling from Q(z|X), \n    sample eps = N(0,I) z = z_mean + sqrt(var)*eps.\n\n    Parameters:\n    -----------\n    args: list of Tensors\n        Mean and log of variance of Q(z|X)\n\n    Returns\n    -------\n    z: Tensor\n        Sampled latent vector\n    \"\"\"\n\n    z_mean, z_log_var = args\n    eps = tf.random_normal(tf.shape(z_log_var), dtype=tf.float32, mean=0., stddev=1.0, name='epsilon')\n    z = z_mean + tf.exp(z_log_var \/ 2) * eps\n    return z","9d2cd655":"hidden_dim = 512\nlatent_dim = 2  # The bigger this is, more accurate the network is but 2 is for illustration purposes.","03a0fe3f":"inputs = kr.layers.Input(shape=(num_features, ), name='input')\nx = kr.layers.Dense(hidden_dim, activation='relu')(inputs)\nz_mean = kr.layers.Dense(latent_dim, name='z_mean')(x)\nz_log_var = kr.layers.Dense(latent_dim, name='z_log_var')(x)","3be549e8":"z = kr.layers.Lambda(sampling, name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = kr.Model(inputs, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()","8755749b":"latent_inputs = kr.layers.Input(shape=(latent_dim,), name='z_sampling')\nx = kr.layers.Dense(hidden_dim, activation='relu')(latent_inputs)\noutputs = kr.layers.Dense(num_features, activation='sigmoid')(x)\n\n# instantiate decoder model\ndecoder = kr.Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()","899a9e0b":"# # VAE model = encoder + decoder\noutputs = decoder(encoder(inputs)[2])  # Select the Z value from outputs of the encoder\nvae = kr.Model(inputs, outputs, name='vae')","12bff346":"# Reconstruction loss\nreconstruction_loss = tf.losses.mean_squared_error(inputs, outputs)\nreconstruction_loss = reconstruction_loss * num_features\n\n# KL Divergence loss\nkl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\nkl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\nvae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n\nvae.add_loss(vae_loss)\nvae.compile(optimizer='adam')\nvae.summary()","04ed3d40":"(X_train, _),  (X_test, y) = load_data('mnist', labels=True)\nvae.fit(X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, None))","3e3f02c2":"generate_manifold(decoder)","969eb90d":"plot_digits(X_test, y, encoder)  # y for label coloring","226e4217":"# new network parameters\ninput_shape = (28, 28, 1)\nbatch_size = 128\nfilters = 32\nlatent_dim = 2\nepochs = 30","0ddca9ce":"inputs = kr.Input(shape=input_shape, name='input')\n\nx = kr.layers.Conv2D(filters, (3, 3), activation='relu', strides=2, padding='same')(inputs)\nx = kr.layers.Conv2D(filters*2, (3, 3), activation='relu', strides=2, padding='same')(x)\n\n# shape info needed to build decoder model\nshape = x.get_shape().as_list()\n\n# generate latent vector Q(z|X)\nx = kr.layers.Flatten()(x)\nx = kr.layers.Dense(16, activation='relu')(x)\nz_mean = kr.layers.Dense(latent_dim, name='z_mean')(x)\nz_log_var = kr.layers.Dense(latent_dim, name='z_log_var')(x)\n\nz = kr.layers.Lambda(sampling, name='z')([z_mean, z_log_var])\n\n# instantiate encoder model\nencoder = kr.Model(inputs, [z_mean, z_log_var, z], name='encoder')\nencoder.summary()","fb6c616a":"# build decoder model\nlatent_inputs = kr.Input(shape=(latent_dim,), name='z_sampling')\nx = kr.layers.Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\nx = kr.layers.Reshape((shape[1], shape[2], shape[3]))(x)\nx = kr.layers.Conv2DTranspose(filters*2, (3, 3), activation='relu', strides=2, padding='same')(x)\nx = kr.layers.Conv2DTranspose(filters, (3, 3), activation='relu', strides=2, padding='same')(x)\noutputs = kr.layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same', name='decoder_output')(x)\n\n# instantiate decoder model\ndecoder = kr.Model(latent_inputs, outputs, name='decoder')\ndecoder.summary()","eb1bb002":"# instantiate VAE model\noutputs = decoder(encoder(inputs)[2])\nvae = kr.Model(inputs, outputs, name='vae')","5e4b98e2":"# Reconstruction loss\nreconstruction_loss = tf.losses.mean_squared_error(inputs, outputs)\nreconstruction_loss = reconstruction_loss * num_features\n\n# KL Divergence loss\nkl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\nkl_loss = -0.5 * tf.reduce_sum(kl_loss, axis=-1)\nvae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n\nvae.add_loss(vae_loss)\nvae.compile(optimizer='adam')\nvae.summary()","d69115c2":"X_train, X_test = X_train.reshape([-1, 28, 28, 1]), X_test.reshape([-1, 28, 28, 1])\nvae.fit(X_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, None))","cbb429a4":"generate_manifold(decoder)","0b45427e":"plot_digits(X_test, y, encoder)","27642425":"**Decoder**","42e1af67":"## Variational Autoencoder\n\n<img src=\"https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1560249262\/vae-gaussian.png\">","2dfc38d4":"As can be seen construction was okay not perfect due to some noise in the images. Increasing layer sizes may help.","4e4b7243":"**Encoder**","0ee01422":"<h2>Training: Loss function<\/h2>\n\nAn autoencoder uses the Loss function to properly train the network. The Loss function will calculate the differences between our output and the expected results. After that, we can minimize this error with gradient descent. There are more than one type of Loss function, it depends on the type of data.\n\n**Binary Values:**\n$$l(f(x)) = - \\sum_{k} (x_k log(\\hat{x}_k) + (1 - x_k) \\log (1 - \\hat{x}_k) \\ )$$\n\nFor binary values, we can use an equation based on the sum of Bernoulli's cross-entropy.  $x_k$ is one of our inputs and $\\hat{x}_k$ is the respective output. We use this function so that if $x_k$ equals to one, we want to push $\\hat{x}_k$ as close as possible to one. The same if $x_k$ equals to zero. If the value is one, we just need to calculate the first part of the formula, that is, $- x_k log(\\hat{x}_k)$. Which, turns out to just calculate $- log(\\hat{x}_k)$. And if the value is zero, we need to calculate just the second part, $(1 - x_k) \\log (1 - \\hat{x}_k) \\ )$ - which turns out to be $log (1 - \\hat{x}_k) $.\n\n**Real values:**\n$$l(f(x)) = - \\frac{1}{2}\\sum_{k} (\\hat{x}_k- x_k \\ )^2$$\n\nAs the above function would behave badly with inputs that are not 0 or 1, we can use the sum of squared differences for our Loss function.\n\nAs it was with the above example, $x_k$ is one of our inputs and $\\hat{x}_k$ is the respective output, and we want to make our output as similar as possible to our input.","13e8e6e6":"Let's simply visualize our graphs!","0d1d0199":"Now we need to create our encoder\/decoder. For this, we are going to use sigmoidal functions. Sigmoidal functions delivers great results with this type of network. This is due to having a good derivative that is well-suited to backpropagation.","6af5ff0d":"Looks like by using the decoder on noisy data we can generate a completely new data! Now how amazing is that? <3.\nBut this is an image data so the question we have in our mind that would it possible to use with convolutions so we can get better results? Of course! Now let's try convolution version of VAE.","5448bd28":"## Fashion MNIST\n\nLet's try our network with fashion mnist to see how well it performs.","cec9865c":"<img src=\"https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1560252253\/1-s2.0-S0009250918300460-gr3.jpg\" width=\"900\" height=\"600\">","8dbbf269":"**Autoencoder Model**","c332ecc7":"## Autoencoder Structure \n\n<img src=\"https:\/\/res.cloudinary.com\/dpyleb8ih\/image\/upload\/v1560249044\/deep_autoencoder.png\">\n\nAn autoencoder can be divided in two parts, the <b>encoder<\/b> and the <b>decoder<\/b>.\n\nThe encoder needs to compress the representation of an input. In this case we are going to reduce the dimension. The decoder works like encoder network in reverse. It works to recreate the input, as closely as possible. This plays an important role during training, because it forces the autoencoder to select the most important features in the compressed representation.","27d926b3":"**Decoder**","0bdd8b6d":"## Introduction\n\nAn autoencoder, also known as autoassociator or Diabolo networks, is an artificial neural network employed to recreate the given input. It takes a set of <b>unlabeled<\/b> inputs, encodes them and then tries to extract the most valuable information from them. They are used for feature extraction, learning generative models of data, dimensionality reduction and can be used for compression. \n\nAutoencoders are based on Restricted Boltzmann Machines, are employed in some of the largest deep learning applications. They are the building blocks of Deep Belief Networks (DBN).\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/xlkv9v7xzxhjww681dq3h1pydxcm4ktp.png\" width=400>","a860acba":"Now, let's give the parameters that are going to be used by our NN.","b130fbae":"## Conclusions\nBy changing the size of filters or layers, it's possible to get much more better results. Give it a try if you want to. And also if you like my kernel, please upvote it :) ","72e58054":"## Convolutional VAE","79c43426":"**Encoder**","339c4830":"We're trying to build a generative model here, not just a fuzzy data structure that can \"memorize\" images. We can't generate anything yet, since we don't know how to create latent vectors other than encoding them from images.\n\nThere's a simple solution here. We add a constraint on the encoding network, that forces it to generate latent vectors that roughly follow a unit gaussian distribution. It is this constraint that separates a variational autoencoder from a standard one.\n\nGenerating new images is now easy: all we need to do is sample a latent vector from the unit gaussian and pass it into the decoder.\n\nIn practice, there's a tradeoff between how accurate our network can be and how close its latent variables can match the unit gaussian distribution.\n\nWe let the network decide this itself. For our loss term, we sum up two separate losses: the generative loss, which is a mean squared error that measures how accurately the network reconstructed the images, and a latent loss, which is the KL divergence that measures how closely the latent variables match a unit gaussian. [ref](http:\/\/kvfrans.com\/variational-autoencoders-explained\/)","a51da5fd":"Although, we don't use maxpooling in our model, feel free to use it when you want to test and see if it helps(both for decreasing amount of trainable parameters and faster training.)","4bf669e5":"# AUTOENCODERS","679fb338":"**Encoder**","a6afe4ad":"**Define VAE Loss**","190b8b82":"**Sampling Layer**  \nUse reparameterization trick to push the sampling out as input","976bba33":"## MNIST","67e2d3cf":"**Decoder**"}}