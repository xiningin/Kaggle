{"cell_type":{"f650071a":"code","6c65cf92":"code","d30b3703":"code","316eee9d":"code","cc528fa8":"code","143c4cea":"code","460b5a25":"code","0c56cc73":"code","63053bc8":"code","96f8c525":"code","e1c56eaa":"code","9b9c27d1":"code","a0fadb79":"code","8f9e2863":"code","869795b1":"code","beff2993":"code","1481055a":"markdown","4ed57b40":"markdown","b6f6737e":"markdown","46a38d18":"markdown","5d812611":"markdown","0a2464d8":"markdown","804c2c33":"markdown"},"source":{"f650071a":"import numpy as np\nimport pandas as pd\nimport itertools\nimport glob\nimport os\nimport cv2\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.auto import tqdm\nfrom multiprocessing import Pool\nfrom matplotlib import pyplot as plt\nfrom sklearn.cluster import KMeans\nimport random","6c65cf92":"debug = False\nCONF_THRE = 0.3\nBASE_DIR = '..\/input\/nfl-health-and-safety-helmet-assignment'\n\nlabels = pd.read_csv(f'{BASE_DIR}\/train_labels.csv')\nif debug:\n    tracking = pd.read_csv(f'{BASE_DIR}\/train_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}\/train_baseline_helmets.csv')\nelse:\n    tracking = pd.read_csv(f'{BASE_DIR}\/test_player_tracking.csv')\n    helmets = pd.read_csv(f'{BASE_DIR}\/test_baseline_helmets.csv')","d30b3703":"# copied from https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\"timedelta64[ms]\") \/ 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\"))\n    return tracks\ntracking = add_track_features(tracking)","316eee9d":"if debug:\n    sample_keys = random.sample(list(tracking['gameKey'].unique()), 3)\n    helmets['gameKey'] = helmets['video_frame'].str.split('_').str[0]\n    tracking = tracking[tracking['gameKey'].isin(sample_keys)]\n    helmets = helmets[helmets['gameKey'].astype(int).isin(sample_keys)]\n    labels = labels[labels['gameKey'].astype(int).isin(sample_keys)]\ntracking.shape, helmets.shape, labels.shape","cc528fa8":"def find_nearest(array, value):\n    value = int(value)\n    array = np.asarray(array).astype(int)\n    idx = (np.abs(array - value)).argmin()\n    return array[idx]\n\ndef norm_arr(a):\n    a = a-a.min()\n    a = a\/a.max()\n    return a\n    \ndef dist(a1, a2):\n    return np.linalg.norm(a1-a2)\n\nmax_iter = 2000\ndef dist_for_different_len(a1, a2):\n    \"\"\"\n    Compute the L2 distance bewteen a1 and a2, \n    ================================================================================================================\n    Inputs\n        a1: x-axis values of players tracking sensor reading, after rotation.\n        a2: x-axis values of helmet boxes on image.\n    ---------------------------------------------------------------------------------------------------------------\n    Return\n        A tuple of two elements. \n        The 1st element is a scalar of the L2 distance between a1 and a2 (after both being normalised)\n        The 2nd element is indices of a1 where no good matches are found in a2.\n            \n        ###########################################################################\n        What does he do when len(a1) > len(a2)?\n            - He wants to remove some items from a1, so that it has the same size as a2.\n            - he needs to choose which items to be removed.\n            - So try all possible combinations of removables! (except when too many items)\n            - Find the case where L2 distance is minimised, with the corresponding removable indices in a1.\n        ###########################################################################\n    \n    ================================================================================================================\n    \"\"\"\n    assert len(a1) >= len(a2), f'{len(a1)}, {len(a2)}'   # make sure tracking sensor has more men than in the image.\n    len_diff = len(a1) - len(a2)\n    a2 = norm_arr(a2)\n    if len_diff == 0:\n        a1 = norm_arr(a1)\n        return dist(a1,a2), ()\n    else:\n        min_dist = 10000\n        min_detete_idx = None\n        cnt = 0        \n        \n        del_list = list(itertools.combinations(range(len(a1)),len_diff))   \n        \n        if len(del_list) > max_iter:\n            del_list = random.sample(del_list, max_iter)\n        for detete_idx in del_list:\n            this_a1 = np.delete(a1, detete_idx)\n            this_a1 = norm_arr(this_a1)\n            this_dist = dist(this_a1, a2)\n            if min_dist > this_dist:\n                min_dist = this_dist\n                min_detete_idx = detete_idx\n        return min_dist, min_detete_idx\n        \ndef rotate_arr(u, t, deg=True):\n    if deg == True:\n        t = np.deg2rad(t)\n    R = np.array([[np.cos(t), -np.sin(t)],\n                  [np.sin(t),  np.cos(t)]])\n    return  np.dot(R, u)\n\ndef dist_rot(tracking_df, a2):\n    \"\"\"\n    Find the players that have the best matches. The best matches are obtained when the L2 distance is minimised \n    between the helmet boxes and the concurrent tracking vectors.\n    ================================================================================================================\n    Inputs\n        tracking_df: tracking sensor data. if the corresponding image is taken from endzone, then the tracking\n                     sensor's (x,y) has already been swapped.\n        a2: centers of the helmet boxs on the x-axis.\n    ---------------------------------------------------------------------------------------------------------------\n    Return\n        A tuple of two elements. \n        The 1st element is a scalar of the L2 distance between a1 and a2 (after both being normalised)\n        The 2nd element is a list of the palyers that constitute the good matches.\n    ================================================================================================================\n    \"\"\"\n    tracking_df = tracking_df.sort_values('x')\n    x = tracking_df['x']\n    y = tracking_df['y']\n    min_dist = 10000\n    min_idx = None\n    min_x = None\n    dig_step = 3\n    dig_max = dig_step*10\n    for dig in range(-dig_max,dig_max+1,dig_step):   # -30 to +30 degrees \n        arr = rotate_arr(np.array((x,y)), dig)\n#         print(arr,'\\n')\n        this_dist, this_idx = dist_for_different_len(np.sort(arr[0]), a2)   # only keep the x-axis values for all players, after rotation?\n        if min_dist > this_dist:\n            min_dist = this_dist\n            min_idx = this_idx\n            min_x = arr[0]\n    tracking_df['x_rot'] = min_x\n    player_arr = tracking_df.sort_values('x_rot')['player'].values\n    players = np.delete(player_arr,min_idx)\n    return min_dist, players\n\n\ndef mapping_df(args):\n    \"\"\"\n    Map helmet boxes on the image with tracking sensor data (players). These are some important properties:\n        - Try different rotations of the image data.\n        - Condense both the image data and the tracking data to 1D\n            1) Always use the x-axis of the rotated images (sideline\/endzone), this is referred to as \"a1\"\n            2) For the tracking sensor, use x-axis when image is from sideline, and y-axis when image \n                is from endzone. Denoted by \"a2\".\n        - Matches are based on L2 distances between the vectors a1 and a2.\n        - For each time instance, the best match is the correspondence between each helmet box location in \n           image and the player associated with an index in a2.\n        - Since the orientation of the image x-axis can be flipped (maybe from different sides of the field),\n            so both directions are tried, denoted by a2_p and a2_m.\n    \"\"\"\n    video_frame, df = args\n    gameKey,playID,view,frame = video_frame.split('_')\n    gameKey = int(gameKey)\n    playID = int(playID)\n    frame = int(frame)\n    this_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\n    est_frame = find_nearest(this_tracking.est_frame.values, frame)\n    this_tracking = this_tracking[this_tracking['est_frame']==est_frame]\n    len_this_tracking = len(this_tracking)\n    df['center_h_p'] = (df['left']+df['width']\/2).astype(int)\n    df['center_h_m'] = (df['left']+df['width']\/2).astype(int)*-1\n    df = df[df['conf']>CONF_THRE].copy()\n    if len(df) > len_this_tracking:\n        df = df.tail(len_this_tracking)\n    df_p = df.sort_values('center_h_p').copy()\n    df_m = df.sort_values('center_h_m').copy()\n    \n    if view == 'Endzone':\n        this_tracking['x'], this_tracking['y'] = this_tracking['y'].copy(), this_tracking['x'].copy()\n    a2_p = df_p['center_h_p'].values\n    a2_m = df_m['center_h_m'].values\n\n    ############################################################################################################\n    # \"min_detete_idx\" is a misued of variable name here; it should be \"min_included_idx\"\n    ############################################################################################################\n    \n    min_dist_p, min_detete_idx_p = dist_rot(this_tracking ,a2_p)   \n    min_dist_m, min_detete_idx_m = dist_rot(this_tracking ,a2_m)   \n    if min_dist_p < min_dist_m:\n        min_dist = min_dist_p\n        min_detete_idx = min_detete_idx_p\n        tgt_df = df_p\n    else:\n        min_dist = min_dist_m\n        min_detete_idx = min_detete_idx_m\n        tgt_df = df_m\n        \n    tgt_df['label'] = min_detete_idx\n\n    return tgt_df[['video_frame','left','width','top','height','label']]","143c4cea":"# import glob\n# glob.glob('\/kaggle\/input\/nfl-health-and-safety-helmet-assignment\/train\/57906*')\n\nimport cv2\nvidcap = cv2.VideoCapture('\/kaggle\/input\/nfl-health-and-safety-helmet-assignment\/train\/57906_000718_Sideline.mp4')\nvid = np.array([vidcap.read()[1] for i in range(160)])\nprint(vid.shape)","460b5a25":"plt.figure(figsize=(25,9))\nplt.imshow(vid[159,:,:,:])\nplt.title('The 160th frame', fontsize=30, color='blue')","0c56cc73":"n = 500\ndf_list = list(helmets.groupby('video_frame'))\nvideo_frame = df_list[n][0]\nraw_df = df_list[n][1]\ndf = mapping_df(df_list[n])\nprint(video_frame)\ngameKey,playID,view,frame = video_frame.split('_')\ngameKey = int(gameKey)\nplayID = int(playID)\nframe = int(frame)\nthis_tracking = tracking[(tracking['gameKey']==gameKey) & (tracking['playID']==playID)]\nest_frame = find_nearest(this_tracking.est_frame.values, frame)\nthis_tracking = this_tracking[this_tracking['est_frame']==est_frame]\nthis_tracking.head()","63053bc8":"# raw_df","96f8c525":"# for demonstration purpose, ignore rotation.\n\na1 = norm_arr(this_tracking['x']).sort_values().values\na2 = norm_arr(raw_df['center_h_p'])\nprint('Original sizes')\nprint(a1.shape, a2.shape)\nprint('Random sample - make a1 and a2 the same size')\nscore_dict = {}\nfor i in range(10):\n    temp2 = a2.sample(len(a1)).sort_values()\n    print(i,np.sum((a1-temp2.values)**2)**0.5)\n    score_dict[i] = temp2.index","e1c56eaa":"# df.loc[score_dict[6],:]","9b9c27d1":"temp_x = df['left'] + df['width']\/2\ntemp_y = df['top'] + df['height']\/2\n\nfig1, ax1 = plt.subplots(ncols=2,nrows=1,figsize=(20,8))\nax1[0].scatter(this_tracking['x'],this_tracking['y'])\nax1[0].set_title('tracking sensor')\nax1[0].set_ylim([53.3,0])\nax1[0].set_xlim([40, 0])\nax1[1].scatter(temp_x,temp_y)\nax1[1].set_ylim([720, 0])\nax1[1].set_title('image frame')\nax1[0].grid(True)\nax1[1].grid(True)","a0fadb79":"# p = Pool(processes=4)\n# submission_df_list = []\n# df_list = list(helmets.groupby('video_frame'))\n# with tqdm(total=len(df_list)) as pbar:\n#     for this_df in p.imap(mapping_df, df_list):   # mapping_df is the function defined above\n#         submission_df_list.append(this_df)\n#         pbar.update(1)\n# p.close()","8f9e2863":"# submission_df = pd.concat(submission_df_list)\n# submission_df.to_csv('submission.csv', index=False)","869795b1":"# import time\n# i = 0\n# while True:\n#     time.sleep(1)\n#     print(i, end='\\r')\n#     i += 1","beff2993":"# # copied from https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n# class NFLAssignmentScorer:\n#     def __init__(\n#         self,\n#         labels_df: pd.DataFrame = None,\n#         labels_csv=\"train_labels.csv\",\n#         check_constraints=True,\n#         weight_col=\"isDefinitiveImpact\",\n#         impact_weight=1000,\n#         iou_threshold=0.35,\n#         remove_sideline=True,\n#     ):\n#         \"\"\"\n#         Helper class for grading submissions in the\n#         2021 Kaggle Competition for helmet assignment.\n#         Version 1.0\n#         https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n\n#         Use:\n#         ```\n#         scorer = NFLAssignmentScorer(labels)\n#         scorer.score(submission_df)\n\n#         or\n\n#         scorer = NFLAssignmentScorer(labels_csv='labels.csv')\n#         scorer.score(submission_df)\n#         ```\n\n#         Args:\n#             labels_df (pd.DataFrame, optional):\n#                 Dataframe containing theground truth label boxes.\n#             labels_csv (str, optional): CSV of the ground truth label.\n#             check_constraints (bool, optional): Tell the scorer if it\n#                 should check the submission file to meet the competition\n#                 constraints. Defaults to True.\n#             weight_col (str, optional):\n#                 Column in the labels DataFrame used to applying the scoring\n#                 weight.\n#             impact_weight (int, optional):\n#                 The weight applied to impacts in the scoring metrics.\n#                 Defaults to 1000.\n#             iou_threshold (float, optional):\n#                 The minimum IoU allowed to correctly pair a ground truth box\n#                 with a label. Defaults to 0.35.\n#             remove_sideline (bool, optional):\n#                 Remove slideline players from the labels DataFrame\n#                 before scoring.\n#         \"\"\"\n#         if labels_df is None:\n#             # Read label from CSV\n#             if labels_csv is None:\n#                 raise Exception(\"labels_df or labels_csv must be provided\")\n#             else:\n#                 self.labels_df = pd.read_csv(labels_csv)\n#         else:\n#             self.labels_df = labels_df.copy()\n#         if remove_sideline:\n#             self.labels_df = (\n#                 self.labels_df.query(\"isSidelinePlayer == False\")\n#                 .reset_index(drop=True)\n#                 .copy()\n#             )\n#         self.impact_weight = impact_weight\n#         self.check_constraints = check_constraints\n#         self.weight_col = weight_col\n#         self.iou_threshold = iou_threshold\n\n#     def check_submission(self, sub):\n#         \"\"\"\n#         Checks that the submission meets all the requirements.\n\n#         1. No more than 22 Boxes per frame.\n#         2. Only one label prediction per video\/frame\n#         3. No duplicate boxes per frame.\n\n#         Args:\n#             sub : submission dataframe.\n\n#         Returns:\n#             True -> Passed the tests\n#             False -> Failed the test\n#         \"\"\"\n#         # Maximum of 22 boxes per frame.\n#         max_box_per_frame = sub.groupby([\"video_frame\"])[\"label\"].count().max()\n#         if max_box_per_frame > 22:\n#             print(\"Has more than 22 boxes in a single frame\")\n#             return False\n#         # Only one label allowed per frame.\n#         has_duplicate_labels = sub[[\"video_frame\", \"label\"]].duplicated().any()\n#         if has_duplicate_labels:\n#             print(\"Has duplicate labels\")\n#             return False\n#         # Check for unique boxes\n#         has_duplicate_boxes = (\n#             sub[[\"video_frame\", \"left\", \"width\", \"top\", \"height\"]].duplicated().any()\n#         )\n#         if has_duplicate_boxes:\n#             print(\"Has duplicate boxes\")\n#             return False\n#         return True\n\n#     def add_xy(self, df):\n#         \"\"\"\n#         Adds `x1`, `x2`, `y1`, and `y2` columns necessary for computing IoU.\n\n#         Note - for pixel math, 0,0 is the top-left corner so box orientation\n#         defined as right and down (height)\n#         \"\"\"\n\n#         df[\"x1\"] = df[\"left\"]\n#         df[\"x2\"] = df[\"left\"] + df[\"width\"]\n#         df[\"y1\"] = df[\"top\"]\n#         df[\"y2\"] = df[\"top\"] + df[\"height\"]\n#         return df\n\n#     def merge_sub_labels(self, sub, labels, weight_col=\"isDefinitiveImpact\"):\n#         \"\"\"\n#         Perform an outer join between submission and label.\n#         Creates a `sub_label` dataframe which stores the matched label for each submission box.\n#         Ground truth values are given the `_gt` suffix, submission values are given `_sub` suffix.\n#         \"\"\"\n#         sub = sub.copy()\n#         labels = labels.copy()\n\n#         sub = self.add_xy(sub)\n#         labels = self.add_xy(labels)\n\n#         base_columns = [\n#             \"label\",\n#             \"video_frame\",\n#             \"x1\",\n#             \"x2\",\n#             \"y1\",\n#             \"y2\",\n#             \"left\",\n#             \"width\",\n#             \"top\",\n#             \"height\",\n#         ]\n\n#         sub_labels = sub[base_columns].merge(\n#             labels[base_columns + [weight_col]],\n#             on=[\"video_frame\"],\n#             how=\"right\",\n#             suffixes=(\"_sub\", \"_gt\"),\n#         )\n#         return sub_labels\n\n#     def get_iou_df(self, df):\n#         \"\"\"\n#         This function computes the IOU of submission (sub)\n#         bounding boxes against the ground truth boxes (gt).\n#         \"\"\"\n#         df = df.copy()\n\n#         # 1. get the coordinate of inters\n#         df[\"ixmin\"] = df[[\"x1_sub\", \"x1_gt\"]].max(axis=1)\n#         df[\"ixmax\"] = df[[\"x2_sub\", \"x2_gt\"]].min(axis=1)\n#         df[\"iymin\"] = df[[\"y1_sub\", \"y1_gt\"]].max(axis=1)\n#         df[\"iymax\"] = df[[\"y2_sub\", \"y2_gt\"]].min(axis=1)\n\n#         df[\"iw\"] = np.maximum(df[\"ixmax\"] - df[\"ixmin\"] + 1, 0.0)\n#         df[\"ih\"] = np.maximum(df[\"iymax\"] - df[\"iymin\"] + 1, 0.0)\n\n#         # 2. calculate the area of inters\n#         df[\"inters\"] = df[\"iw\"] * df[\"ih\"]\n\n#         # 3. calculate the area of union\n#         df[\"uni\"] = (\n#             (df[\"x2_sub\"] - df[\"x1_sub\"] + 1) * (df[\"y2_sub\"] - df[\"y1_sub\"] + 1)\n#             + (df[\"x2_gt\"] - df[\"x1_gt\"] + 1) * (df[\"y2_gt\"] - df[\"y1_gt\"] + 1)\n#             - df[\"inters\"]\n#         )\n#         # print(uni)\n#         # 4. calculate the overlaps between pred_box and gt_box\n#         df[\"iou\"] = df[\"inters\"] \/ df[\"uni\"]\n\n#         return df.drop(\n#             [\"ixmin\", \"ixmax\", \"iymin\", \"iymax\", \"iw\", \"ih\", \"inters\", \"uni\"], axis=1\n#         )\n\n#     def filter_to_top_label_match(self, sub_labels):\n#         \"\"\"\n#         Ensures ground truth boxes are only linked to the box\n#         in the submission file with the highest IoU.\n#         \"\"\"\n#         return (\n#             sub_labels.sort_values(\"iou\", ascending=False)\n#             .groupby([\"video_frame\", \"label_gt\"])\n#             .first()\n#             .reset_index()\n#         )\n\n#     def add_isCorrect_col(self, sub_labels):\n#         \"\"\"\n#         Adds True\/False column if the ground truth label\n#         and submission label are identical\n#         \"\"\"\n#         sub_labels[\"isCorrect\"] = (\n#             sub_labels[\"label_gt\"] == sub_labels[\"label_sub\"]\n#         ) & (sub_labels[\"iou\"] >= self.iou_threshold)\n#         return sub_labels\n\n#     def calculate_metric_weighted(\n#         self, sub_labels, weight_col=\"isDefinitiveImpact\", weight=1000\n#     ):\n#         \"\"\"\n#         Calculates weighted accuracy score metric.\n#         \"\"\"\n#         sub_labels[\"weight\"] = sub_labels.apply(\n#             lambda x: weight if x[weight_col] else 1, axis=1\n#         )\n#         y_pred = sub_labels[\"isCorrect\"].values\n#         y_true = np.ones_like(y_pred)\n#         weight = sub_labels[\"weight\"]\n#         return accuracy_score(y_true, y_pred, sample_weight=weight)\n\n#     def score(self, sub, labels_df=None, drop_extra_cols=True):\n#         \"\"\"\n#         Scores the submission file against the labels.\n\n#         Returns the evaluation metric score for the helmet\n#         assignment kaggle competition.\n\n#         If `check_constraints` is set to True, will return -999 if the\n#             submission fails one of the submission constraints.\n#         \"\"\"\n#         if labels_df is None:\n#             labels_df = self.labels_df.copy()\n\n#         if self.check_constraints:\n#             if not self.check_submission(sub):\n#                 return -999\n#         sub_labels = self.merge_sub_labels(sub, labels_df, self.weight_col)\n#         sub_labels = self.get_iou_df(sub_labels).copy()\n#         sub_labels = self.filter_to_top_label_match(sub_labels).copy()\n#         sub_labels = self.add_isCorrect_col(sub_labels)\n#         score = self.calculate_metric_weighted(\n#             sub_labels, self.weight_col, self.impact_weight\n#         )\n#         # Keep `sub_labels for review`\n#         if drop_extra_cols:\n#             drop_cols = [\n#                 \"x1_sub\",\n#                 \"x2_sub\",\n#                 \"y1_sub\",\n#                 \"y2_sub\",\n#                 \"x1_gt\",\n#                 \"x2_gt\",\n#                 \"y1_gt\",\n#                 \"y2_gt\",\n#             ]\n#             sub_labels = sub_labels.drop(drop_cols, axis=1)\n#         self.sub_labels = sub_labels\n#         return score\n\n# if debug:\n#     scorer = NFLAssignmentScorer(labels)\n#     baseline_score = scorer.score(submission_df)\n#     print(f\"validation score {baseline_score:0.4f}\") # this would be 0.33","1481055a":"## The original author's work below: iterate all data","4ed57b40":"## Jacky's demo\n\nBased on instance `57906_000718_Sideline_159`","b6f6737e":"In this notebook I will show you how to map helmet bbox and NGS tracking data.\n\n- I used [train|test]_baseline_helmets.csv for helmet bbox.\n- Compare the NGS tracking data with the lateral coordinates of the boxes.\n- Map the players to have the smallest difference between each boxes and coordinates.\n- The reason why I did not use vertical coordinates is because simplicity and the lateral direction is more accurate, but we can get more accurate mapping if we use depth as well.\n- The direction of the axis changes depending on the camera position, so I choose the one that gives smaller distance for both directions.\n- Since the camera and the XY axis will be rotated if it is out of the center of the court. So the angle at which the distance is the smallest is selected by grid-search.\n\nPlease check the code for details.","46a38d18":"## adding Estimated video frame to tracking\n\nI used @robikscube's great code here.","5d812611":"## validation\nI used @robikscube's great code here.","0a2464d8":"## mapping helmet box and NGS tracking data","804c2c33":"## setting and loading data"}}