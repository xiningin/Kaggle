{"cell_type":{"8b093830":"code","637ac024":"code","05cb2e81":"code","6ddddcd3":"code","82a25661":"code","13da36a6":"code","0bbc937d":"code","afad8934":"code","950fd82e":"markdown","aee67424":"markdown","1eb0ba4d":"markdown","2422e3e5":"markdown","d3d708f2":"markdown","99d82168":"markdown","b316175e":"markdown","3a784ab0":"markdown","261e2e17":"markdown"},"source":{"8b093830":"import tensorflow.keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport numpy as np\n","637ac024":"(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# just to make sure the data type is float32 not float64 which is slightly memory saving\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n","05cb2e81":" \n#z-score\nmean = np.mean(x_train,axis=(0,1,2,3))\nstd = np.std(x_train,axis=(0,1,2,3))\n\n#standardizing the data to keep it consistent\nx_train = (x_train-mean)\/(std+1e-7)\nx_test = (x_test-mean)\/(std+1e-7)\n\n# encoding the classes label\nnum_classes = 10\ny_train = utils.to_categorical(y_train,num_classes)\ny_test = utils.to_categorical(y_test,num_classes)\n \n#data augmentation to increase the dataset size\n\ndatagen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    )\ndatagen.fit(x_train)","6ddddcd3":"def lr_schedule(epoch):\n    lrate = 0.001\n    if epoch > 75:\n        lrate = 0.0005\n    return lrate\n \n\n","82a25661":"weight_decay = 1e-4 # coefficent affecting how high the weights are so we consider the model is overfitting \nmodel = Sequential()\n# padding = 'same' to make the input dims same as output dims\nmodel.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n# exponential lin unit (makes process faster)\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n \nmodel.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.3))\n \nmodel.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Activation('elu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n \nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation='softmax'))\n \nmodel.summary()\n \n","13da36a6":"batch_size = 64\n \nopt_rms = tensorflow.keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n","0bbc937d":"model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n                    steps_per_epoch=x_train.shape[0] \/\/ batch_size,epochs=100,\\\n                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n","afad8934":"#save to disk\nmodel_json = model.to_json()\nwith open('model.json', 'w') as json_file:\n    json_file.write(model_json)\nmodel.save_weights('model.h5') \n \n#testing\nscores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\nprint('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))","950fd82e":"#### this function will be used to make the training proccess faster","aee67424":"## In this notebook I will be going through the process of creating a CNN for object detection using the CIFAR-10 dataset\n\n### so Lets start by importing some useful libraries","1eb0ba4d":"### I have saved the model so I can use it later for future predictions\n### That's it and thank you","2422e3e5":"### Training process","d3d708f2":"### Data preproccessing","99d82168":"### Lets define the Model architecture","b316175e":"### now Lets determine some hyperparameters","3a784ab0":"### Lets load the dataset","261e2e17":"#### Note this section is the most challenging part of the whole process as it is only figured out by trial and error\n#### either manually or automatically \n#### so to save time (why to reinvent the wheel from scratch)\n#### I used the architecture defined here :https:\/\/appliedmachinelearning.blog\/2018\/03\/24\/achieving-90-accuracy-in-object-recognition-task-on-cifar-10-dataset-with-keras-convolutional-neural-networks\/"}}