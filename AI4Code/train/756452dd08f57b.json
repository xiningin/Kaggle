{"cell_type":{"72bc6806":"code","ddca0a36":"code","0c128bd4":"code","4d76c2ab":"code","a586386a":"code","3248e2e6":"code","fbf7b1d4":"code","acd015ea":"code","145aadc1":"code","2efd40d0":"code","44b120b2":"code","8cdcecc2":"code","fb298196":"code","168d41c9":"code","466df710":"code","676f9b75":"code","17a97373":"code","5dfcc648":"code","9a7c392c":"code","24f1c191":"code","8c6313cf":"code","b2c968b0":"code","6202ec20":"code","b8a902a7":"code","36243ca3":"code","6da9bf6b":"code","9ea5b9db":"code","f75563f4":"code","d8dd4b4b":"code","0f89bd0e":"code","ad7e292c":"code","69da15a9":"code","4a616dac":"code","77b7f65d":"code","28dd3957":"code","f696c0c0":"code","c540dfdf":"code","d87a3fbd":"code","e2fc230a":"code","08bfb974":"code","003a2937":"code","b4c74e0d":"code","973fedf4":"code","99313d88":"code","f61e8e15":"code","1df39f97":"code","a196ed63":"code","75d1360d":"code","f697f6a6":"code","4d4ada3c":"code","8ccb5623":"code","bb3e6547":"code","20f47ae9":"code","817bbab3":"code","62232a19":"code","64782392":"code","70147113":"code","b3ece43a":"code","2b20990a":"code","27ed94a1":"code","b606a065":"code","02544a38":"code","8038ede2":"code","cd9bd659":"code","93de9d17":"code","f8498e5c":"code","af6288bb":"code","bc055320":"code","7971eaaa":"code","6bdbe411":"code","46024c17":"code","a55ec775":"code","89c82729":"code","eeb24508":"code","fdb96579":"code","8edc6897":"code","d3a78761":"code","f49398b7":"code","3258a0a5":"code","098b6f51":"code","b554a037":"code","b1a99e7d":"markdown","6eeaa9bc":"markdown","735516ba":"markdown","ed1fb929":"markdown","90952f07":"markdown","1bbbcd92":"markdown","b0b8ebb0":"markdown","b5a67cf0":"markdown","868d8085":"markdown","12e24b66":"markdown","f2796ab0":"markdown","c121bc78":"markdown","c469f6df":"markdown","de4bc442":"markdown","0cd756fe":"markdown","ceccf7e9":"markdown","0181c707":"markdown","9bc7f2c5":"markdown","303992f8":"markdown","90c6aa9b":"markdown","7ab0b53d":"markdown","63f73026":"markdown","31184069":"markdown","206cb939":"markdown","b2c310d4":"markdown","d95e45ed":"markdown","358e9372":"markdown"},"source":{"72bc6806":"#Importing libraries\nimport numpy as np\nimport pandas as pd\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')\n#matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns',150)\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n \nimport os,random, math,psutil, pickle","ddca0a36":"print(os.listdir(\"..\/input\/ashrae-energy-prediction\"))","0c128bd4":"%%time\nroot=\"..\/input\/ashrae-energy-prediction\/\"\ntrain_df=pd.read_csv(root+\"train.csv\")\nwtrain_df=pd.read_csv(root+\"weather_train.csv\")\ntest_df=pd.read_csv(root+\"test.csv\")\nwtest_df=pd.read_csv(root+\"weather_test.csv\")\nbuilding_df=pd.read_csv(root+\"building_metadata.csv\")\nsample_df=pd.read_csv(root+\"sample_submission.csv\")","4d76c2ab":"print(f\"Shape of train_df: {train_df.shape}\")\nprint(f\"Shape of test_df: {test_df.shape}\")","a586386a":"#changing the datatype of timestamp column of training set\ntrain_df[\"timestamp\"]=pd.to_datetime(train_df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")","3248e2e6":"print(\"Size of train_df data is\", train_df.shape)\nprint(\"Size of wtrain_df data is\",wtrain_df.shape)\nprint(\"Size of wtest_df data is\",wtest_df.shape)\nprint(\"Size of test_df data is\",test_df.shape )\nprint(\"Size of building_df data is\", building_df.shape)\nprint(\"Size of sample_df data is\", sample_df.shape)\n","fbf7b1d4":"#Calculating Memory utilization in MB's\nprint(\"Memory utilization train_df data is\", train_df.memory_usage().sum()\/1024**2,\"MB\")\nprint(\"Memory utilization of wtrain_train_df data is\",wtrain_df.memory_usage().sum()\/1024**2,\"MB\")\nprint(\"Memory utilization of wtest_train_df data is\",wtest_df.memory_usage().sum()\/1024**2,\"MB\")\nprint(\"Memory utilization of test_df data is\",test_df.memory_usage().sum()\/1024**2,\"MB\")\nprint(\"Memory utilization of building_metadata_df data is\",building_df.memory_usage().sum()\/1024**2,\"MB\")\nprint(\"Memory utilization of sample_submission_df data is\",sample_df.memory_usage().sum()\/1024**2,\"MB\")","acd015ea":"# Function to reduce the dataframe size\n\ndef reduce_mem(df, verbose=True):\n    numerics=['int16','int32','int64','float16','float32','float64']\n    start_mem=df.memory_usage().sum()\/1024**2\n    for col in df.columns:\n        col_type=df[col].dtypes\n        if col_type in numerics:\n            c_min=df[col].min()\n            c_max=df[col].max()\n            if str(col_type)[:3]==\"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col]=df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col]=df[col].astype(np.int16)\n                elif c_min >np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col]=df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col]=df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col]=df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col]=df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float64).min and c_max < np.finfo(np.float64).max:\n                    df[col]=df[col].astype(np.float64)\n    end_mem=df.memory_usage().sum()\/1024**2\n    if verbose: print(\"Memory usage decreased to {:5.2f} MB ({:.1f}% reduction)\".format(end_mem,100*(start_mem-end_mem)\/start_mem))\n    return df","145aadc1":"train_df=reduce_mem(train_df)\nwtrain_df=reduce_mem(wtrain_df)\nwtest_df=reduce_mem(wtest_df)\ntest_df=reduce_mem(test_df)\nbuilding_df=reduce_mem(building_df)\nsample_df=reduce_mem(sample_df)","2efd40d0":"#Column Name of Dataframes\ndef colname(df):\n    name=df.columns\n    return name","44b120b2":"train_df.head(3)","8cdcecc2":"train_df.columns.values","fb298196":"wtrain_df.head(3)","168d41c9":"wtrain_df.columns.values","466df710":"wtest_df.head(3)","676f9b75":"wtest_df.columns.values","17a97373":"building_df.head(3)","5dfcc648":"building_df.columns.values","9a7c392c":"# target is meter_reading\n#individual subplots for {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}\ngrouped=train_df.groupby(['meter'])[\"meter_reading\"]\nncols=2\nnrows=int(np.ceil(grouped.ngroups\/ncols))\nfig,axes=plt.subplots(nrows=nrows,ncols=ncols,figsize=(30,15))\nfor (key,ax) in zip(grouped.groups.keys(),axes.flatten()):\n    if key==0:\n        label=\"Electricity\"\n    elif key==1:\n        label=\"Chilled Water\"\n    elif key==2:\n        label=\"Steam\"\n    elif key==3:\n        label=\"Hot Water\"\n    grouped.get_group(key).plot(ax=ax, label=label)\n    ax.legend(loc=\"upper left\")\n    ax.set_xlabel(\"Meter Reading\")\n    ax.set_ylabel(\"Frequency\")\nplt.show();\n    ","24f1c191":"#checking distribution of meterreading as a whole\nplt.figure(figsize=(15,8))\nplt.plot(train_df[\"meter_reading\"])\nplt.title(\"Distribution of Meter Reading as Whole\")\nplt.xlabel(\"Meter Reading\")\nplt.ylabel(\"Frequency\");","8c6313cf":"train_df[\"meter_reading\"].plot(kind=\"hist\",\n                              bins=30,\n                              figsize=(30,8),\n                              title=\"Distribution of Target Variable (meter_reading)\")\nplt.show()","b2c968b0":" #Goal: for each building and each meter type,visualise where target is missing and where target is zero vs time\n#Load Data\ntrain=train_df.set_index([\"timestamp\"])\n#Plot missing values per building per meter type\n\nf,a=plt.subplots(1,4,figsize=(20,30))\nfor meter in np.arange(4):\n    df=train[train.meter==meter].copy().reset_index()\n    df['timestamp']=pd.to_timedelta(df.timestamp).dt.total_seconds()\/3600\n    df.timestamp=df.timestamp.astype(int)\n    #print(df.timestamp.min())\n    df.timestamp-=df.timestamp.min()\n    #print(df.timestamp.max())\n    missmap=np.empty((1449,df.timestamp.max()+1))\n    #print(missmap.shape)\n    missmap.fill(np.nan)\n    for l in df.values:\n        if l[2]!=meter:continue\n        missmap[int(l[1]),int(l[0])]=0 if l[3]==0 else 1\n    a[meter].set_title(f'meter{meter}')\n    sns.heatmap(missmap,cmap='Paired',ax=a[meter],cbar=False)","6202ec20":"#checking missing values for train_df\ntotal=train_df.isnull().sum().sort_values(ascending=False)\npercent=(train_df.isnull().sum()\/train_df.isnull().count()*100).sort_values(ascending=False)\nmissing_train=pd.concat([total,percent], axis=1, keys=[\"Total\",\"Percentage\"])\nmissing_train.head()","b8a902a7":"#checking missing values for wtrain_df\ntotal=wtrain_df.isnull().sum().sort_values(ascending=False)\npercent=(wtrain_df.isnull().sum()\/wtrain_df.isnull().count()*100).sort_values(ascending=False)\nmissing_wtrain=pd.concat([total,percent],axis=1,keys=[\"Total\",\"Percent\"])\nmissing_wtrain","36243ca3":"#checking missing values for wtest_df\ntotal=wtest_df.isnull().sum().sort_values(ascending=False)\npercent=(wtest_df.isnull().sum()\/wtest_df.isnull().count()*100).sort_values(ascending=False)\nmissing_wtest=pd.concat([total,percent],axis=1,keys=[\"Total\",\"Percent\"])\nmissing_wtest","6da9bf6b":"#checking missing values in building_df\ntotal=building_df.isnull().sum().sort_values(ascending=False)\npercent=(building_df.isnull().sum()\/building_df.isnull().count()*100).sort_values(ascending=False)\nmissing_building=pd.concat([total,percent],axis=1,keys=[\"Total\",\"Percent\"])\nmissing_building","9ea5b9db":"#Columns in train_df\nprint(f'Column types in train_df::\\n{train_df.dtypes.value_counts()}')\n\n#Column in wtrain_df\nprint(f'\\nColumn types in wtrain_df::\\n{wtrain_df.dtypes.value_counts()}')\n\n#Column in wtest_df\nprint(f'\\nColumn types in wtest_df::\\n{wtest_df.dtypes.value_counts()}')\n\n#Column in wtrain_df and building_df\nprint(f'\\nColumn types in building_df::\\n{building_df.dtypes.value_counts()}')","f75563f4":"#Number of unique classes in each categorical\/object column in train_df\nprint(f'No. of unique classes in column of object type in train_df:\\n{train_df.select_dtypes(\"object\").apply(pd.Series.nunique,axis=0)}')\n\n#Number of unique classes in each categorical\/object column in wtrain_df\nprint(f'\\nNo. of unique classes in column of object type in wtrain_df:\\n{wtrain_df.select_dtypes(\"object\").apply(pd.Series.nunique,axis=0)}')\n\n#Number of unique classes in each categorical\/object column in wtest_df\nprint(f'\\nNo. of unique classes in column of object type in wtest_df:\\n{wtest_df.select_dtypes(\"object\").apply(pd.Series.nunique,axis=0)}')\n\n#Number of unique classes in each categorical\/object column in building_df\nprint(f'\\nNo. of unique classes in column of object type in building_df:\\n{building_df.select_dtypes(\"object\").apply(pd.Series.nunique,axis=0)}')","d8dd4b4b":"#finding the correleations in train_df\n\ncor=train_df.corr()\nprint(cor)","0f89bd0e":"#Plotting the coorelations in seaborn\n\nplt.figure(figsize=(20,8))\nsns.heatmap(cor, annot=True,cmap='RdYlBu', vmax=0.6)\nplt.title(\"Correlation Heatmap\");","ad7e292c":"#Finding unique buildings in train_df\nprint(f'No of unique buildings in the data:{train_df.building_id.nunique()}')","69da15a9":"#Plotting the distribution of meter_reading using hisogram\ntrain_df.meter_reading.hist(figsize=(7,5))\nplt.xlabel(\"Meter Reading\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Histogram Meter Reading\");","4a616dac":"#Plotting on wtrain_df and wtest_df\n#wtrain_df.head()\n#Plotting air_temperature distribution of wtrain_df and wtest_df\n\ndef plot_dist(col, kde=True):\n    ''' plot dist curve on wtrain_df ad wtest_df for a given column name'''\n    \n    fig,ax=plt.subplots(figsize=(20,5))\n    sns.distplot(wtrain_df[col].dropna(), color=\"green\", ax=ax).set_title(f'Distribution of {col}',fontsize=16)\n    sns.distplot(wtest_df[col].dropna(),ax=ax,color=\"purple\").set_title(f'Distribution of {col}',fontsize=16)\n    plt.xlabel(col, fontsize=16)\n    plt.legend(['train','test'])\n    plt.show()","77b7f65d":"name=list(colname(wtrain_df))\n#print(name)\n#Plottting distribution of air_temperature\nplot_dist(\"air_temperature\")","28dd3957":"#Plotting Disribution of cloud_coverage\nplot_dist(\"cloud_coverage\")","f696c0c0":"#Plotting dew_temperature\nplot_dist('dew_temperature')","c540dfdf":"#Plotting sea_level_pressure\nplot_dist('sea_level_pressure')","d87a3fbd":"#Plotting distribution of wind_direction\nplot_dist('wind_direction')","e2fc230a":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller","08bfb974":"#Check the stationarity of the timeseries\n\ndef check_stn(ts):\n    #Determining rolling statistics\n    rolmean=ts.rolling(window=12, center=False).mean()\n    rolstd=ts.rolling(window=12,center=False).std()\n    \n    #Plot rolling statistics\n    plt.figure(figsize=(20,8))\n    orig=plt.plot(ts, label=\"Original\")\n    mean=plt.plot(rolmean, label=\"Rolling Mean\")\n    std=plt.plot(rolstd,label=\"Rolling Std\")\n    plt.xlabel(\"timestamp\")\n    plt.ylabel(\"meter_reading\")\n    plt.legend(loc=\"best\")\n    plt.title(\"Plotting Rolling Mean and Rolling Std\")\n    plt.show()\n    \n    #Perform Dickey-Fuller Test\n    print(\"Result of Dickey-Fuller Test\")\n    ts=ts.iloc[:,0].values\n    dftest=adfuller(ts, autolag=\"AIC\")\n    dfoutput=pd.Series(dftest[0:4],index=[\"Test Statistics\",\"p-value\",\"No.Of Lags Used\",\"No. Of Observations Used\"])\n    for key, value in dftest[4].items():\n        dfoutput[f'Critical Value of {key}']=value\n    print(dfoutput)\n    ","003a2937":"ts=pd.DataFrame(train_df.groupby(['timestamp']).meter_reading.sum())\ncheck_stn(ts)\n#Note: For TimeSeries to be Stationary value of Test Statistics has to be less then the Critical Value.\n#Conclusion: This is a stationary time series","b4c74e0d":"#Decomposition of TimeSeries\nimport statsmodels.api as sm\n#multiplicative \nres=sm.tsa.seasonal_decompose(ts.values,period=12, model=\"multiplicative\")\nfig=res.plot()","973fedf4":"#Additive\nres=sm.tsa.seasonal_decompose(ts.values,period=12, model=\"additive\")\nfig=res.plot()","99313d88":"y_mean=train_df.groupby(['timestamp'])[\"meter_reading\"].mean()\nplt.figure(figsize=(10,5))\nplt.plot(y_mean)\nplt.xlabel(\"timestamp\")\nplt.ylabel(\"Mean Of meter_reading\")\nplt.title(\"Plotting Mean of meter_reading and timestamp\")\nplt.show()","f61e8e15":"plt.figure(figsize=(10,5))\nplt.plot(y_mean.rolling(window=10).std())\nax=plt.axhline(y=0.009,color=\"red\")\nplt.show()","1df39f97":"plt.figure(figsize=(10,5))\nplt.plot(y_mean.rolling(window=10).std())\nplt.axhline(y=0.009,color=\"red\")\nplt.axhspan(0, 905, color='green', alpha=0.1)\nplt.axhspan(906,1505, color='red', alpha=0.1);","a196ed63":"train_df[\"meter\"]=pd.Categorical(train_df.meter).rename_categories({0:\"Electricity\",1:\"Chilled_Water\",2:\"Steam\",3:\"Hot_Water\"})\ndaily_train=train_df.copy()\ndaily_train['date']=daily_train.timestamp.dt.date\ndaily_train=daily_train.groupby([\"date\",\"building_id\",\"meter\"]).sum()\ndaily_train.head(2)","75d1360d":"daily_train_agg=daily_train.groupby([\"date\",\"meter\"]).agg([\"sum\",\"mean\",\"idxmax\",\"max\"])\ndaily_train_agg=daily_train_agg.reset_index()\nlevel_0=daily_train_agg.columns.droplevel(0)\nlevel_1=daily_train_agg.columns.droplevel(1)\nlevel_0=['' if x=='' else '_'+ x for x in level_0]\ndaily_train_agg.columns=level_1 + level_0\ndaily_train_agg.rename_axis(None,axis=1)\ndaily_train_agg.head(3)","f697f6a6":"plt.figure(figsize=(20,8))\nax=sns.lineplot(x=\"date\",y=\"meter_reading_sum\", data=daily_train_agg, hue=\"meter\")\nax.set_title(\"Total Energy per day\")\nax.set_xlabel(\"Date\")\nplt.show()","4d4ada3c":"plt.figure(figsize=(20,8))\nax=sns.lineplot(x=\"date\",y=\"meter_reading_max\", data=daily_train_agg, hue=\"meter\")\nax.set_title(\"Maximum Energy per day\")\nax.set_xlabel(\"Date\")\nplt.show()","8ccb5623":"daily_train_agg[\"building_id_max\"]=[x[1] for x in daily_train_agg['meter_reading_idxmax'] ]\ndaily_train_agg.head()","bb3e6547":"# No of days a building has the maximum electricity consumption\n\nout_elec=daily_train_agg[daily_train_agg[\"meter\"]==\"Electricity\"][\"building_id_max\"].value_counts()\nprint(f'Number of time buildings having maximum consumption of Electricity:\\n{out_elec}')","20f47ae9":"# Maximum electricity is consumed by only 6 buildings\nplt.figure(figsize=(20,8))\ndaily_train_elec=daily_train_agg[daily_train_agg['meter']==\"Electricity\"]\n#print(f'Data type of building_id_max is :: {daily_train_elec.building_id_max.dtype}') #Note: It has to be categorical\ndaily_train_elec[\"building_id_max\"]=pd.Categorical(daily_train_elec[\"building_id_max\"])\n\nax=sns.scatterplot(x=\"date\",y=\"meter_reading_max\",hue=\"building_id_max\", data=daily_train_elec,alpha=0.8)\nax.set_xlim((daily_train_elec[\"date\"].min()+pd.to_timedelta(-1,unit=\"M\"),daily_train_elec[\"date\"].max()+pd.to_timedelta(2,unit=\"M\")))\nplt.xlabel(\"Date\", fontsize=16)\nplt.ylabel(\"meter_reading_max\",fontsize=16)\nplt.title(\"Building Having High Consumption of Electricity\", fontsize=16)\nplt.show()","817bbab3":"#Number of days the buidling having higher consumption for ChilledWater\ndaily_train_chilled=daily_train_agg[daily_train_agg[\"meter\"]==\"Chilled_Water\"]\ndaily_train_chilled[\"building_id_max\"]=pd.Categorical(daily_train_chilled[\"building_id_max\"])\nbld_count=daily_train_chilled[\"building_id_max\"].value_counts()\nprint(f'No of days building has high demand for Chilled_Water:\\n{bld_count}')","62232a19":"#Plotting the buildings meter_reading for building having demand of Chilled_Water\nplt.figure(figsize=(20,8))\nax=sns.scatterplot(x=\"date\",y=\"meter_reading_max\",hue=\"building_id_max\", data=daily_train_chilled)\nplt.xlim((daily_train_chilled[\"date\"].min()+pd.to_timedelta(-2,unit=\"M\"), daily_train_chilled[\"date\"].max()+pd.to_timedelta(2,unit=\"M\")))\nplt.xlabel(\"Date\",fontsize=16)\nplt.ylabel(\"meter_reading\",fontsize=16)\nplt.show()","64782392":"#No of days building having high demand for Steam\ndaily_train_steam=daily_train_agg[daily_train_agg[\"meter\"]==\"Steam\"]\ndaily_train_steam[\"building_id_max\"]=pd.Categorical(daily_train_steam[\"building_id_max\"])\nsteam_count=daily_train_steam[\"building_id_max\"].value_counts()\nprint(f'Number of days building having high demands for Steam among all buildings:\\n{steam_count}')","70147113":"#Plotting the meter_reading for buildings having high demand for Steam for each date.\nplt.figure(figsize=(20,8))\nax=sns.scatterplot(x=\"date\",y=\"meter_reading_max\", data=daily_train_steam, hue=\"building_id_max\")\nplt.xlim((daily_train_steam[\"date\"].min()+pd.to_timedelta(-2,unit=\"M\"),daily_train_steam[\"date\"].max()+pd.to_timedelta(2,unit=\"M\")))\nplt.xlabel(\"Date\", fontsize=16)\nplt.ylabel(\"meter_reading_max\",fontsize=16)\nplt.show()","b3ece43a":"#No of days each building have high demand for Hot_Water\n\ndaily_train_hw=daily_train_agg[daily_train_agg[\"meter\"]=='Hot_Water']\ndaily_train_hw[\"building_id_max\"]=pd.Categorical(daily_train_hw[\"building_id_max\"])\nhw_count=daily_train_hw[\"building_id_max\"].value_counts()\nprint(f'No of days each building has high demand for Hot_Water:\\n{hw_count}')","2b20990a":"#Plotting meter_reading for each building datewise having high demand for Hot_Water\nplt.figure(figsize=(20,8))\nax=sns.scatterplot(x=\"date\",y='meter_reading_max',data=daily_train_hw,hue=\"building_id_max\")\nplt.xlabel(\"Date\", fontsize=16)\nplt.ylabel(\"meter_reading_max\",fontsize=16)\nplt.title(\"Plotting meter_reading_max of each builiding having high demand for Hot_water\")\nplt.xlim((daily_train_hw[\"date\"].min()+pd.to_timedelta(-2,unit=\"M\"),daily_train_hw[\"date\"].max()+pd.to_timedelta(2,unit=\"M\")))\nplt.show()","27ed94a1":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split","b606a065":"wtrain_df[\"timestamp\"]=pd.to_datetime(wtrain_df[\"timestamp\"])\nwtest_df[\"timestamp\"]=pd.to_datetime(wtest_df[\"timestamp\"])\ntrain_df[\"timestamp\"]=pd.to_datetime(train_df[\"timestamp\"])\ntest_df[\"timestamp\"]=pd.to_datetime(test_df[\"timestamp\"])","02544a38":"temp_df=train_df[[\"building_id\"]] ##Note this syntax creates dataframe from a single column of DataFrame instead of Series\ntemp_df=temp_df.merge(building_df,how=\"left\",on=\"building_id\")\ndel temp_df[\"building_id\"]\ntrain_df=pd.concat([train_df,temp_df],axis=1)\ntrain_df.columns","8038ede2":"temp_df=train_df[['building_id']]\ntemp_df=temp_df.merge(building_df,on=[\"building_id\"],how=\"left\")\ndel temp_df[\"building_id\"]\ntest_df=pd.concat([test_df,temp_df],axis=1)\ntest_df.columns","cd9bd659":"# Merging wtest_df to test_df\ntemp_df=train_df[[\"site_id\",\"timestamp\"]]\ntemp_df=temp_df.merge(wtrain_df,on=[\"site_id\",\"timestamp\"],how=\"left\")\ndel temp_df[\"site_id\"], temp_df[\"timestamp\"]\ntrain_df=pd.concat([train_df,temp_df],axis=1)\ntrain_df.columns","93de9d17":"# Merging wtest_df to test_df\ntemp_df=test_df[[\"site_id\",\"timestamp\"]]\ntemp_df=temp_df.merge(wtest_df,on=[\"site_id\",\"timestamp\"],how=\"left\")\ndel temp_df[\"site_id\"], temp_df[\"timestamp\"]\ntest_df=pd.concat([test_df,temp_df],axis=1)\ntest_df.columns","f8498e5c":"#Saving the train_df and test_df as their pickle file\ntrain_df.to_pickle(\"train_df.pickle\")\ntest_df.to_pickle(\"test_df.pickle\")","af6288bb":"import gc\ndel train_df, test_df\ngc.collect()","bc055320":"#reading train and test data from pickle file\ntrain_df=pd.read_pickle(\"train_df.pickle\")\ntest_df=pd.read_pickle(\"test_df.pickle\")","7971eaaa":"print(train_df.columns)","6bdbe411":"train_df[\"age\"]=train_df[\"year_built\"].max() - train_df[\"year_built\"] + 1\ntest_df[\"age\"]=test_df[\"year_built\"].max() - test_df[\"year_built\"] + 1","46024c17":"train_df[\"month_datetime\"]=train_df[\"timestamp\"].dt.month.astype(np.int8)\ntrain_df[\"weekofyear_datetime\"]=train_df['timestamp'].dt.weekofyear.astype(np.int8)\ntrain_df[\"dayofyear_datetime\"]=train_df['timestamp'].dt.dayofyear.astype(np.int8)\ntrain_df[\"hour_datetime\"]=train_df['timestamp'].dt.hour.astype(np.int8)\ntrain_df[\"day_week\"]=train_df[\"timestamp\"].dt.dayofweek.astype(np.int8)\ntrain_df[\"day_month\"]=train_df[\"timestamp\"].dt.day.astype(np.int8)\ntrain_df[\"week_of_month\"]=train_df[\"timestamp\"].dt.day\/7\ntrain_df['week_of_month']=train_df[\"week_of_month\"].apply(lambda x: math.ceil(x)).astype(np.int8)\n\ntrain_df['year_built']=train_df[\"year_built\"] - 1900\ntrain_df[\"square_feet\"]=np.log(train_df['square_feet'])","a55ec775":"test_df[\"month_datetime\"]=test_df[\"timestamp\"].dt.month.astype(np.int8)\ntest_df[\"weekofyeat_datetime\"]=test_df[\"timestamp\"].dt.weekofyear.astype(np.int8)\ntest_df['dayofyear_dattetime']=test_df[\"timestamp\"].dt.dayofyear.astype(np.int8)\ntest_df[\"hour_datetime\"]=test_df[\"timestamp\"].dt.hour.astype(np.int8)\ntest_df[\"day_week\"]=test_df[\"timestamp\"].dt.dayofweek.astype(np.int8)\ntest_df['day_month']=test_df['timestamp'].dt.day.astype(np.int8)\ntest_df[\"week_of_month\"]=test_df[\"timestamp\"].dt.day\/7\ntest_df[\"week_of_month\"]=test_df[\"week_of_month\"].apply(lambda x: math.ceil(x)).astype(np.int8)\n\ntest_df['year_built']=test_df[\"year_built\"]-1900\ntest_df[\"square_feet\"]=np.log(test_df['square_feet'])","89c82729":"#Converting wind_direction into 16-wind compass rose from degrees\ndef degtocomp(colval):\n    val=int(colval\/22.5)\n    arr=[i for i in range(0,16)]\n    a=arr[(val%16)]\n    return a","eeb24508":"#train_df[\"wind_direction\"]=train_df[\"wind_direction\"].apply(degtocomp)","fdb96579":"le=LabelEncoder()\ntrain_df[\"primary_use\"]=le.fit_transform(train_df[\"primary_use\"]).astype('int8')\ntest_df[\"primary_use\"]=le.fit_transform(test_df[\"primary_use\"]).astype('int8')","8edc6897":"print(f'Number of unique values in primary_use for train_df:\\n {pd.unique(train_df[\"primary_use\"]).ravel()}\\n')\nprint(f'Number of unique values in primary_use for test_df:\\n {pd.unique(test_df[\"primary_use\"]).ravel()}')","d3a78761":"#Method to impute missing values\ndef avg_imputation(df, colname):\n    imputation=train_df.groupby(['timestamp'])[colname].mean()\n    df.loc[df[colname].isnull(),colname]=df[df[colname].isnull()][[colname]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n    del imputation\n    return df","f49398b7":"train_df=avg_imputation(train_df,\"wind_direction\")\ntrain_df=avg_imputation(train_df,\"wind_speed\")","3258a0a5":"#Missing values in train data\ncount=train_df.isnull().sum().sort_values(ascending=False)\nperc=(train_df.isnull().sum()\/train_df.isnull().count()*100).sort_values(ascending=False)\nmising=pd.concat([count,perc], axis=1, keys=[\"Mis_Count\",\"Mis_Percentage\"])\nmising=mising[mising[\"Mis_Count\"]>0]\nprint(mising)","098b6f51":"#Missing valuesin test data\ncount=test_df.isnull().sum().sort_values(ascending=False)\nperc=(test_df.isnull().sum()\/test_df.isnull().count()*100).sort_values(ascending=False)\nmising=pd.concat([count,perc], axis=1, keys=[\"Test_Mis_Count\",\"Test_Mis_Percentage\"])\nmising=mising[mising[\"Test_Mis_Count\"]>0]\nprint(mising)","b554a037":"train_df[\"floor_count\"]=train_df[\"floor_count\"].fillna(-999).astype(np,int16)\ntest_df[\"floor_count\"]=test_df[\"floor_count\"].fillna(-999).astype(np,int16)\n\ntrain_df[\"year_built\"]=train_df[\"year_built\"].fillna(-999).astype(np.int16)\ntest_df[\"year_built\"]=test_df[\"year_built\"].fillna(-999).astype(np.int16)\n\ntrain_df[\"age\"]=train_df[\"age\"].fillna(-999).astype(np.int16)\ntest_df[\"age\"]=test_df[\"age\"].fillna(-999).astype(np.int16)\n\ntrain_df[\"cloud_coverage\"]=train_df[\"cloud_coverage\"].fillna(-999).astype(np.int16)\ntest_df[\"cloud_coverage\"]=test_df[\"cloud_coverage\"].fillna(-999).astype(np.int16)","b1a99e7d":"# <a id=\"12\">12. Variables Encoding<\/a>\n#### <a href=\"#0\">Top<\/a>","6eeaa9bc":"# <a id=\"4\"> 4. Reducing Memory Size <\/a>\n###### <a href=\"#0\">Top<\/a>","735516ba":"# <a id=\"7\">7. Simple Single Series Analysis<\/a>\n#### <a href=\"#0\">Top<\/a>","ed1fb929":"## <a id=\"5-2\">5.2 Examine the Distribution of Target Variable<\/a>\n##### <a href=\"#0\">Top<\/a>","90952f07":"## <a id=\"5-3\">5.3 Missing Data and Zeroes Visualised<\/a>\n###### <a href=\"#0\">Top<\/a>\n","1bbbcd92":"### 16-Wind Compass Rose\n![image.png](attachment:image.png)","b0b8ebb0":"## <a id=\"5-1\">5.1 Data Description<\/a>\n###### <a href=\"#0\">Top<\/a>","b5a67cf0":"# <a id=\"13\">13. Handling Missing Values<\/a>\n#### <a href=\"#0\">Top<\/a>","868d8085":"## <a id=\"8-1\">8.1 Group Data on Daily Basis<\/a>\n#### <a href=\"#0\">Top<\/a>","12e24b66":"# <a id=\"0\">Table Of Contents<\/a>\n- <a href='#1'>1. Imports<\/a>\n- <a href=\"#2\">2 Read In Data<\/a>\n- <a href=\"#3\">3. Glipmse of Data<\/a>\n- <a href=\"#4\">4. Reducing Memory Size<\/a>\n- <a href=\"#5\">5. Exploratory Data Analysis<\/a>\n    - <a href=\"#5-1\">5.1 Data Description<\/a>\n    - <a href=\"#5-2\">5.2 Examine the Distribution of Target Variable<\/a>\n    - <a href=\"#5-3\" >5.3 Missing Data and Zeroes Visualised<\/a>\n    - <a href=\"#5-4\">5.4 Examining Missing Values<\/a>\n    - <a href=\"#5-5\">5.5 Column Types<\/a>\n    - <a href=\"#5-6\">5.6 Correlations<\/a>\n- <a href=\"#6\">6. Plotting<\/a>\n- <a href=\"#7\">7. Simple Single Series Analysis<\/a>\n- <a href=\"#8\">8. Outlier Distribution<\/a>\n    - <a href=\"#8-1\">8.1 Group Data on Daily Basis<\/a>\n    - <a href=\"#8-2\">8.2 Aggregating Data For Buildings<\/a>\n    - <a href=\"#8-3\">8.3 Identifying Outliers<\/a>\n- <a href=\"#9\">9. Feature Engineering and Modelling<\/a>\n- <a href=\"#10\">10. Pickling Train and Test Data<\/a>\n- <a href=\"#11\">11. Creating Features<\/a>\n- <a href=\"#12\">12. Variables Encoding<\/a>\n- <a href=\"#13\">13. Handling Missing Values<\/a>","f2796ab0":"# <a id=\"8\">8. Outlier Distribution<\/a>\n#### <a href=\"#0\">Top<\/a>","c121bc78":"  # <a id=\"1\">1. Imports <\/a>\n  #### <a href=\"#0\">Top<\/a>","c469f6df":"## <a id=\"8-3\">8.3 Identifying Outliers<\/a>\n#### <a href=\"#0\">Top<\/a>","de4bc442":"# <a id=\"9\">9. Feature Engineering and Modelling<\/a>\n#### <a href=\"#0\">Top<\/a>","0cd756fe":"## <a id=\"5-4\">5.4 Examining Missing Values<\/a>\n##### <a href=\"#0\">Top<\/a>","ceccf7e9":"# <a id=\"10\">10. Pickling Of Train and Test Data<\/a>\n#### <a href=\"#0\">Top<\/a>","0181c707":"## <a id=\"5-6\">5.6 Correlations<\/a>\n#### <a href=\"#0\">Top<\/a>","9bc7f2c5":"### Note: Feature Selection\n- Find the optimal feature subset using an evaluation measure. The choice of evaluation metric distinguish the three main strategies of feature selection   algorithms: the wrapper strategy, the filter strategy, and the embedded strategy.\n- Filter methods:\n    - information gain\n    - chi-square test\n    - correlation coefficient\n    - variance threshold\n- Wrapper methods:\n    - recursive feature elimination\n    - sequential feature selection algorithms\n- Embedded methods:\n    - L1 (LASSO) regularization\n    - decision tree","303992f8":"## <a id=\"9-1\">9.1 Merging buidling_df and train_df<\/a>\n#### <a href=\"#0\">Top<\/a>","90c6aa9b":"# <a id=\"5\">5. Exploratory Data Analysis<\/a>\n###### <a href=\"#0\">Top<\/a>","7ab0b53d":"## <a id=\"9-2\">9.2 Merging wtrain_df and train_df<\/a>\n#### <a href=\"#0\">Top<\/a>","63f73026":"## <a id=\"5-5\">5.5 Column Types<\/a>\n#### <a href=\"#0\">Top<\/a>","31184069":"# <a id=\"3\">3 Glimpse of Data<\/a>  \n###### <a href=\"#0\">Top<\/a>","206cb939":"# <a id=\"2\">2. Read in Data <\/a>    \n###### <a href=\"#0\">Top<\/a>","b2c310d4":"## <a id=\"8-2\">8.2 Aggregating Data For Buildings<\/a>\n#### <a href=\"#0\">Top<\/a>","d95e45ed":"# <a id=\"6\">6. Plotting<\/a>\n#### <a href=\"#0\">Top<\/a>","358e9372":"# <a id=\"11\">11. Features<\/a>\n#### <a href=\"#0\">Top<\/a>"}}