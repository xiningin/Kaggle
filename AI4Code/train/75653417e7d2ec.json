{"cell_type":{"8cdaa767":"code","fac4d517":"code","62a896d5":"code","4a38a5ae":"code","46addefb":"code","6f418873":"code","cb8748a2":"code","00dda0e8":"code","48e58687":"code","d568b216":"code","39c1cf8b":"code","76533c38":"code","fe092c49":"code","e0bad03a":"code","d1123241":"code","1007aee2":"code","cdbdba35":"code","c9be4a75":"markdown","583bc74f":"markdown","d50d9cbb":"markdown","f79301e3":"markdown","c7852b3c":"markdown","25f8800a":"markdown","210121ea":"markdown","08a2f996":"markdown","3da2eded":"markdown"},"source":{"8cdaa767":"# Import dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport os\nimport pathlib\nimport gc\nimport sys\nimport math \nimport time \nimport tqdm \nfrom tqdm import tqdm \nimport random\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import StratifiedKFold \n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport transformers \nimport datasets ","fac4d517":"# global config\nconfig = {\n    'nfolds': 10,\n    'batch_size': 32,\n    'learning_rate': 1e-4,\n    'num_epochs': 3,\n    'batch_size': 8,\n}\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# For reproducible results    \ndef seed_all(s):\n    random.seed(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['PYTHONHASHSEED'] = str(s) \nglobal_seed = 42\nseed_all(global_seed)","62a896d5":"df = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.head(5)","4a38a5ae":"df['y'].value_counts(normalize=True)","46addefb":"min_len = (df['y'] == 1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=global_seed)\ntrain_df = pd.concat([df[df['y'] == 1], df_y0_undersample]).reset_index(drop=True)\ntrain_df['y'].value_counts()","6f418873":"train_df.head()","cb8748a2":"n_folds = 10\n\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=global_seed)\nfor nfold, (train_index, val_index) in enumerate(skf.split(X=train_df.index,\n                                                           y=train_df.y)):\n    train_df.loc[val_index, 'fold'] = nfold\nprint(train_df.groupby(['fold', train_df.y]).size())","00dda0e8":"p_fold = 0\np_train = train_df.query(f'fold != {p_fold}').reset_index(drop=True)\np_valid = train_df.query(f'fold == {p_fold}').reset_index(drop=True)\n\nprint(len(p_train))\nprint(len(p_valid))\n\np_train.head()","48e58687":"checkpoint = \"bert-base-uncased\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(checkpoint)","d568b216":"train_ds = datasets.Dataset.from_pandas(p_train)\nvalid_ds = datasets.Dataset.from_pandas(p_valid)\n\nprint(train_ds)\nprint(valid_ds)","39c1cf8b":"def tokenize_function(example):\n    return tokenizer(example[\"text\"], truncation=True)\n\ntokenized_train_ds = train_ds.map(tokenize_function, batched=True)\ntokenized_valid_ds = valid_ds.map(tokenize_function, batched=True)\n\nprint(tokenized_train_ds)\nprint(tokenized_valid_ds)","76533c38":"data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n\ntf_train_ds = tokenized_train_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"y\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=config['batch_size'],\n)\n\ntf_valid_ds = tokenized_valid_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    label_cols=[\"y\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=config['batch_size'],\n)\n\nprint(len(tf_train_ds))\nprint(len(tf_valid_ds))","fe092c49":"from transformers import TFAutoModelForSequenceClassification\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","e0bad03a":"num_epochs = 2\nnum_train_steps = len(tf_train_ds) * num_epochs\n\nlr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n)\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_scheduler),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.summary()","d1123241":"fit_history = model.fit(tf_train_ds,\n                        epochs=num_epochs,\n                        validation_data=tf_valid_ds,\n                        verbose=1)","1007aee2":"test_df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ntest_ds = datasets.Dataset.from_pandas(test_df)\ntokenized_test_ds = test_ds.map(tokenize_function, batched=True)\ntf_test_ds = tokenized_test_ds.to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=config['batch_size'],\n)","cdbdba35":"raw_result = model.predict(tf_test_ds)\nresult = tf.sigmoid(raw_result.logits)\n\ntest_df['score'] = result.numpy()[:, 0]\nsubmission_df = test_df[['comment_id', 'score']]\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df","c9be4a75":"# 3. Model Training","583bc74f":"# 0. Settings","d50d9cbb":"### 1.3 k-fold","f79301e3":"### 1.2 Undersampling\n\nThe dataset is very unbalanced. Here we undersample the majority class. Other strategies might work better.","c7852b3c":"### 1. Create train data\n\nFor training data, I used [Toxic Comment Classification Challenge][1] dataset.\n\n[1]: https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data\n\nI turn it into a binary toxic\/ no-toxic classification","25f8800a":"---\n## [Jigsaw Rate Severity of Toxic Comments][1]\n---\n**Comments1**: 'Internet' is required for this Notebook.\n\n**Comments2**: Thanks to previous great Notebooks.\n\n1. [\u2623\ufe0f Jigsaw - Incredibly Simple Naive Bayes [0.768]][2]\n2. [AutoNLP for toxic ratings ;)][3]\n\n\n[1]: https:\/\/www.kaggle.com\/c\/jigsaw-toxic-severity-rating\/overview\n[2]: https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n[3]: https:\/\/www.kaggle.com\/abhishek\/autonlp-for-toxic-ratings","210121ea":"# 2. DataSet","08a2f996":"# 4. Prediction & Submit","3da2eded":"# 1. Data Preprocessing"}}