{"cell_type":{"e0384051":"code","7aabe54f":"code","69162bd6":"code","56df5c0a":"code","9fd3165e":"code","289a2c60":"code","5c8e1a8b":"code","0de80279":"code","d68fd47c":"code","8e6cd750":"code","e8b377b5":"code","5aecaae7":"code","f8780d02":"code","26f34fad":"code","75575b13":"code","a3c95ac3":"code","4ed16753":"code","1d57820b":"code","51623ca0":"markdown","f936933a":"markdown","8e417d51":"markdown","54d5d1a0":"markdown","d16c946c":"markdown","93c16869":"markdown","64656f3f":"markdown"},"source":{"e0384051":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly import tools\nfrom datetime import date\nimport pandas as pd\nimport numpy as np \nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random \nimport warnings\nimport operator\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\nimport pandas as pd\nfrom sklearn import ensemble\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import normalize\nimport math","7aabe54f":"train = pd.read_csv(\"..\/input\/forest-cover-type-kernels-only\/train.csv\")\ntest = pd.read_csv(\"..\/input\/forest-cover-type-kernels-only\/test.csv\")\n\nprint (\"Train Dataset: Rows, Columns: \", train.shape)\nprint (\"Test Dataset: Rows, Columns: \", test.shape)","69162bd6":"print (\"Glimpse of Train Dataset:... \")\ntrain.head()","56df5c0a":"print (\"Summary of Train Dataset: \")\ntrain.describe()","9fd3165e":"print (\"Top Columns having missing values\")\nmissmap = train.isnull().sum().to_frame().sort_values(0, ascending = False)\nmissmap.head()","289a2c60":"target = train['Cover_Type'].value_counts().to_frame()\nlevels = [\"Spruce\/Fir\", \"Lodgepole Pine\", \"Ponderosa Pine\", \"Cottonwood\/Willow\",\"Aspen\", \"Douglas-fir\", \"Krummholz\"]\ntrace = go.Bar(y=target.Cover_Type, x=levels, marker=dict(color='orange', opacity=0.6))\nlayout = dict(title=\"Cover_Type Tree Levels\", margin=dict(l=10), width=1200, height=400)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","5c8e1a8b":"def largest_index(inlist):\n    largest = -1\n    largest_index = 0\n    for i in range(len(inlist)):\n        item = inlist[i]\n        if item > largest:\n            largest = item\n            largest_index = i\n    return largest_index","0de80279":"def load_data():\n    loc_train = \"..\/input\/forest-cover-type-kernels-only\/train.csv\"\n    loc_test = \"..\/input\/forest-cover-type-kernels-only\/test.csv\"\n    loc_submission = \"kaggle.rf200.entropy.submission.csv\"\n    df_train = pd.read_csv(loc_train)\n    df_test = pd.read_csv(loc_test)\n    return (loc_train, loc_test, loc_submission, df_train,df_test)\n\nloc_train, loc_test, loc_submission, df_train,df_test = load_data()","d68fd47c":"cols_to_normalize = ['Aspect','Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Horizontal_Distance_To_Fire_Points']\ndf_train[cols_to_normalize] = normalize(df_train[cols_to_normalize])\ndf_test[cols_to_normalize] = normalize(df_test[cols_to_normalize])","8e6cd750":"feature_cols = [col for col in df_train.columns if col not in ['Cover_Type','Id']]\nfeature_cols.append('binned_elevation')\nfeature_cols.append('Horizontal_Distance_To_Roadways_Log')\nfeature_cols.append('Soil_Type12_32')\nfeature_cols.append('Soil_Type23_22_32_33')\nfeature_cols.append('Horizontal_Distance_To_Hydrology')","e8b377b5":"df_train['binned_elevation'] = [math.floor(v\/50.0) for v in df_train['Elevation']]\ndf_test['binned_elevation'] = [math.floor(v\/50.0) for v in df_test['Elevation']]\n\ndf_train['Horizontal_Distance_To_Roadways_Log'] = [math.log(v+1) for v in df_train['Horizontal_Distance_To_Roadways']]\ndf_test['Horizontal_Distance_To_Roadways_Log'] = [math.log(v+1) for v in df_test['Horizontal_Distance_To_Roadways']]\n\ndf_train['Soil_Type12_32'] = df_train['Soil_Type32'] + df_train['Soil_Type12']\ndf_test['Soil_Type12_32'] = df_test['Soil_Type32'] + df_test['Soil_Type12']\ndf_train['Soil_Type23_22_32_33'] = df_train['Soil_Type23'] + df_train['Soil_Type22'] + df_train['Soil_Type32'] + df_train['Soil_Type33']\ndf_test['Soil_Type23_22_32_33'] = df_test['Soil_Type23'] + df_test['Soil_Type22'] + df_test['Soil_Type32'] + df_test['Soil_Type33']\n\ndf_train['Horizontal_Distance_To_Hydrology_Log'] = [math.log(v+1) for v in df_train['Horizontal_Distance_To_Hydrology']]\ndf_test['Horizontal_Distance_To_Hydrology_Log'] = [math.log(v+1) for v in df_test['Horizontal_Distance_To_Hydrology']]","5aecaae7":"X_train = df_train[feature_cols]\nX_test = df_test[feature_cols]\ny = df_train['Cover_Type']","f8780d02":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=0.10, random_state=42, stratify=y)\nX_train.shape,y_train.shape,X_val.shape","26f34fad":"%%time\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\n\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_roun=25, n_folds=7, random_seed=42, n_estimators=10000, learning_rate=0.02, output_process=False,colsample_bytree=0.93,min_child_samples=56,subsample=0.84):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y)\n    # parameters\n    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight, colsample_bytree,min_child_samples,subsample):\n        params = {'application':'multiclass','num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':300, 'metric':'macroF1'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"num_class\"] = 8\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['colsample_bytree'] = 0.93\n        params['min_child_samples'] = 56,\n        params['subsample'] = 0.84\n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n    # range \n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (19, 45),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.8, 1),\n                                            'max_depth': (5, 8.99),\n                                            'lambda_l1': (0, 5),\n                                            'lambda_l2': (0, 3),\n                                            'min_split_gain': (0.001, 0.1),\n                                            'min_child_weight': (5, 50),\n                                            'colsample_bytree' : (0.7,1.0),\n                                            'min_child_samples' : (40,65),\n                                            'subsample' : (0.7,1.0)\n                                           }, random_state=0)\n    # optimize\n    lgbBO.maximize(init_points=init_round, n_iter=opt_roun)\n    \n    # output optimization process\n    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n    # return best parameters\n    return lgbBO.res['max']['max_params']\n\nopt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=10, opt_roun=10, n_folds=6, random_seed=42, n_estimators=500, learning_rate=0.02,colsample_bytree=0.93)","75575b13":"opt_params = {'bagging_fraction': 0.9957236684465528,\n 'colsample_bytree': 0.7953949538181928,\n 'feature_fraction': 0.7333800304661316,\n 'lambda_l1': 1.79753950286893,\n 'lambda_l2': 1.710590311253639,\n 'max_depth': 6,\n 'min_child_samples': 48,\n 'min_child_weight': 49,\n 'min_split_gain': 0.016737988780906453,\n 'num_leaves': 33,\n 'subsample': 0.9033449610388691}","a3c95ac3":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=0.20, random_state=42, stratify=y)\n# X_train.shape,y_train.shape,X_val.shape\nlgb = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.1, objective='multiclass',\n                             random_state=314, silent=True, metric='None', \n                             n_jobs=4, n_estimators=5000, class_weight='balanced')\nlgb.set_params(**opt_params)\n#         n_estimators=132,#132\n#         learning_rate=0.1,\n#         num_leaves=64,\n#         max_depth=7,\n#         min_data_in_leaf=40,\n#         max_bin =15,\n#         reg_alpha=0.8, reg_lambda=0.6,\n#         colsample_bytree=1.0,\n#         min_split_gain=0.001, objective = \"softmax\",random_state=42,\n#         stratified=True)\nlgb.fit(X_train,y_train)\nprint(lgb.score(X_train,y_train))\ny_pred = lgb.predict(X_test)","4ed16753":"sub = pd.read_csv(\"..\/input\/forest-cover-type-kernels-only\/sample_submission.csv\")\nsub.head()","1d57820b":"sub[\"Cover_Type\"] = y_pred\nsub.to_csv(\"submission_works.csv\", index = False)","51623ca0":"# Feature Engineering","f936933a":"<hr>\n## 2.Different Types of Cover Types\n<hr>","8e417d51":"# For Basic Model","54d5d1a0":"# 1.Load Data","d16c946c":"# Exploration of Forest Cover\n\n![Image](https:\/\/www.fs.fed.us\/foresthealth\/images\/FS_regions.gif)\n\nThe **National Forest System,** the **Rocky Mountain Region(Region-2)** enjoys a proud heritage in the Forest Service. The Shoshone National Forest in Wyoming and the White River National Forest in Colorado are among the first National Forests Congress created from the original Forest Reserves. The Region, headquartered in Golden, Colorado, comprises 17 national forests and 7 national grasslands.\n\nThe US Forest Service Rocky Mountain Region has formally identified four overarching themes as emphasis areas on which to focus strategic long-term efforts to preserve their special values: Forest and Grassland Health, Recreation, Water and Public Service. Forests and Grasslands continue to hold in trust America's resources- timber, wildlife, water, range, recreation - to ensure their availability today and tomorrow.","93c16869":"# 2.Normalize so Column","64656f3f":"<hr>\n# Part-1 Exploration Analysis\n<hr>\n## 1. Dataset Preparation\nLets view the snapshot of the dataset which is given for training and testing purposes."}}