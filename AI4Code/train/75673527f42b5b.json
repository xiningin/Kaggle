{"cell_type":{"31f93e76":"code","76cfea04":"code","c946afc8":"code","fc0f536a":"code","e625de5a":"code","86badd8e":"code","0fce82ae":"code","4f3743c0":"code","57bf5d2a":"code","03998bf9":"code","d1e59b63":"code","9a5bcb58":"code","2b3b15af":"markdown","446af7ed":"markdown","0048f22e":"markdown","0186d410":"markdown","bd638d65":"markdown"},"source":{"31f93e76":"# install some packages\n\n# ! pip install xgboost\n# ! pip install catboost","76cfea04":"# from looking at Kaggle, this dataset is quite clean, no missing values etc. so can skip straight to the model building phase\n# (but some pre-processing is still needed)\n\n# import dataset\nimport pandas as pd\ndataset = pd.read_csv('heart.csv')\n\n# encode categorical data labels as numerical\nfrom sklearn.preprocessing import LabelEncoder \nle = LabelEncoder()\ndataset[\"RestingECG\"] = le.fit_transform(dataset[\"RestingECG\"])\ndataset[\"ST_Slope\"] = le.fit_transform(dataset[\"ST_Slope\"])\ndataset[\"ExerciseAngina\"] = le.fit_transform(dataset[\"ExerciseAngina\"])\ndataset[\"ChestPainType\"] = le.fit_transform(dataset[\"ChestPainType\"])\ndataset[\"Sex\"] = le.fit_transform(dataset[\"Sex\"])\n\n# split into X and y\ny = dataset[\"HeartDisease\"]\nX = dataset.drop(columns=[\"HeartDisease\"])\n\n# scale variables to lie in same range\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# split into train\/test set\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n\n","c946afc8":"# Do hyperparameter optimization by bayes search cross validation\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Integer, Categorical\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import accuracy_score, make_scorer, confusion_matrix, precision_score, recall_score, accuracy_score\n\nhp_opt = True\n\n#     search_space = {\"n_estimators\": Integer(600, 900),\n#             \"max_depth\": Integer(8, 15),\n#             \"learning_rate\": Real(0.05, 0.3), \n#             \"reg_alpha\": Real(0, 1),\n#             \"reg_lambda\": Real(0, 1),\n#             \"gamma\": Real(0, 1),\n\nif hp_opt:\n    search_space = {\"n_estimators\": Integer(400, 1200),\n            \"max_depth\": Integer(2, 15),\n            \"learning_rate\": Real(0.05, 0.3), \n            \"reg_alpha\": Real(0, 2),\n            \"reg_lambda\": Real(0, 2),\n            \"gamma\": Real(0, 2),\n    #         \"min_child_weight\": Real(0, 50),\n    #         \"use_label_encoder\":Categorical([False]),\n    #         'binary:logistic':Categorical([\"logloss\"]),\n    #         'max_delta_step': Real(0, 20),\n    #         'subsample': Real(0, 1.0, 'uniform')\n        }\n\n    XGB_bayes_search = BayesSearchCV(\n        estimator = XGBClassifier(\n            n_jobs = 1,\n            objective = 'binary:logistic',\n            iid=\"False\"\n        ),\n        search_spaces = search_space,\n        scoring = 'accuracy',\n        cv = StratifiedKFold(n_splits=5),\n        n_jobs = 3,\n        n_iter = 64\n        )\n\n    def callback(res):\n        global i\n        print(\"Run \",i)\n        i += 1\n\n    i = 0\n    XGB_bayes_search.fit(X_train, y_train, callback=callback)\n\n    best_params = XGB_bayes_search.best_params_\n    best_estimator = XGB_bayes_search.best_estimator_\n    best_score = XGB_bayes_search.best_score_","fc0f536a":"if hp_opt:\n    print(best_params)\n    print(\"Best CV score:\")\n    print(best_score)\n    print(\"refit model test set score:\")\n    print(XGB_bayes_search.score(X_test, y_test))\n    print(\"refit model test set score: (other method) - it should be the same, but sometimes isn't - why?\")\n    print(XGB_bayes_search.best_estimator_.score(X_test, y_test))","e625de5a":"# Model Building\n\nclf = best_estimator\n\n# run the fitted model on the test set and obtain the probabilities that each data point lies in each class\ny_prob = clf.predict_proba(X_test)\n\n# set the probabilty threshold to determine precision\/recall tradeoff\nthreshold = 0.35\n\n# obtain 1\/0 list based on if probabiltiy is above\/below threshold\ncertainty = y_prob[:, 1] > threshold\ny_pred = certainty.astype(int)  \n\n# calculate metrics\nfpr, tpr, threshold = roc_curve(y_test, y_pred)\nroc_auc = auc(fpr, tpr)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\n\n\nprint(\"AUC: \", roc_auc)\nprint(\"Precision: \" , prec)\nprint(\"Recall: \" , rec)\nprint(\"Accuracy: \", acc)\n\nprint(\"\")\nprint(\"Confusion matrix: \")\nprint(pd.DataFrame(confusion_matrix(y_test, y_pred),\n             columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","86badd8e":"# Now try a Neural Network\nimport tensorflow as tf\nimport keras_tuner as kt\n\nhp_opt_nn = True\n\nif hp_opt_nn:\n    # split train set into cv train set and validation set\n    X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n\n    # model building function with custom hyperparameters\n    def build_model(hp):\n        inputs = tf.keras.Input(shape=(11,))\n        x = inputs\n\n        x = tf.keras.layers.Dense(hp.Int('layer_size_1', 3, 11, step=1), activation='relu')(x)\n        x = tf.keras.layers.Dense(hp.Int('layer_size_2', 3, 11, step=1), activation='relu')(x)\n        x = tf.keras.layers.Dropout(hp.Float('dropout_rate', 0.0, 0.5, step=0.05, default=0.2))(x)\n        x = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n        outputs = x\n\n        model = tf.keras.Model(inputs, outputs)\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(\n                hp.Float('learning_rate', 1e-3, 1e-1, sampling='log')),\n            loss='binary_crossentropy', \n            metrics=['accuracy', 'Recall', 'Precision', 'AUC'])\n        return model\n\n    tuner = kt.Hyperband(build_model, objective=\"val_accuracy\", max_epochs=30, overwrite=True)\n\n    # search for optimal hyperparameters\n    tuner.search(X_train_cv, y_train_cv,\n                 validation_data=(X_val, y_val),\n                 epochs=30,\n                 callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)])\n\n    best_model = tuner.get_best_models(1)[0]","0fce82ae":"# now run best model on test set (wasn't used during cross-validation)\n\n# loss_nn, acc_nn, rec_nn, prec_nn, auc_nn = best_model.evaluate(X_test, y_test)\n\n# predict class probabilities using NN model\ny_prob_nn = best_model.predict(X_test)\n\n# obtain 1\/0 list based on if probabiltiy is above\/below threshold\n# set threshold so that both models have around 98% recall, and can be compared\nthreshold = 0.15\ncertainty = y_prob_nn > threshold\ny_pred_nn = certainty.astype(int)  \n\n# calculate metrics\nfpr, tpr, threshold = roc_curve(y_test, y_pred_nn)\nroc_auc_nn = auc(fpr, tpr)\nprec_nn = precision_score(y_test, y_pred_nn)\nrec_nn = recall_score(y_test, y_pred_nn)\nacc_nn = accuracy_score(y_test, y_pred_nn)\n\n# print model summary\nbest_model.summary()\n\nprint(\"accuracy: \", acc_nn)\nprint(\"Recall: \", rec_nn)\nprint(\"Precision: \", prec_nn)\nprint(\"AUC: \", roc_auc_nn)","4f3743c0":"# visualise metrics for XGBoost and NN\n\nimport matplotlib.pyplot as plt\n\ndef plot_metric(metrics, metric_name, model_names, colours):\n    plt.bar(x=(model_names[0], model_names[1]), height=(metrics[0], metrics[1]), color=(colours[0], colours[1]))\n    plt.ylabel(metric_name)\n    plt.xlabel(\"Model\")\n    plt.show()    \n    print(model_names[0], metric_name, \": \",  round(metrics[0], 4))\n    print(model_names[1], metric_name, \": \",  round(metrics[1], 4))\n    \nplot_metric((acc, acc_nn), \"Accuracy\", [\"XGB\", \"NN\"], colours=[\"green\", \"blue\"])\nplot_metric((rec, rec_nn), \"Recall\", [\"XGB\", \"NN\"], colours=[\"green\", \"blue\"])\nplot_metric((prec, prec_nn), \"Precision\", [\"XGB\", \"NN\"], colours=[\"green\", \"blue\"])\nplot_metric((roc_auc, roc_auc_nn), \"AUC\", [\"XGB\", \"NN\"], colours=[\"green\", \"blue\"])","57bf5d2a":"# could we make a quick ensemble of the XGB and NN and see if it outperforms the individual models?\n\n# simply take average of the two probability vectors \ny_prob_ens = (y_prob[:, 1].reshape(184, 1) + y_prob_nn) \/ 2\n\n# as before, set recall to 98%\nthreshold = 0.35\ncertainty = y_prob_ens > threshold\ny_pred_ens = certainty.astype(int)  \n\n# calculate metrics\nfpr, tpr, threshold = roc_curve(y_test, y_pred_ens)\nroc_auc_ens = auc(fpr, tpr)\nprec_ens = precision_score(y_test, y_pred_ens)\nrec_ens = recall_score(y_test, y_pred_ens)\nacc_ens = accuracy_score(y_test, y_pred_ens)\n\nprint(\"accuracy: \", acc_ens)\nprint(\"Recall: \", rec_ens)\nprint(\"Precision: \", prec_ens)\nprint(\"AUC: \", roc_auc_ens)\n\nplot_metric((acc_ens, acc), \"Accuracy\", [\"Ensemble\", \"XGB\"], colours=[\"teal\", \"green\"])\nplot_metric((rec_ens, rec), \"Recall\", [\"Ensemble\", \"XGB\"], colours=[\"teal\", \"green\"])\nplot_metric((prec_ens, prec), \"Precision\", [\"Ensemble\", \"XGB\"], colours=[\"teal\", \"green\"])\nplot_metric((roc_auc_ens, roc_auc), \"AUC\", [\"Ensemble\", \"XGB\"], colours=[\"teal\", \"green\"])","03998bf9":"# catBoost algorithm (takes a long time)\n\nhp_opt_cat = True\n\nfrom catboost import CatBoostClassifier\n\nif hp_opt_cat:\n    search_space = {\"iterations\": Integer(4, 11),\n            \"depth\": Integer(2, 8),\n            \"learning_rate\": Real(0.05, 0.3), \n            \"l2_leaf_reg\": Real(0, 2),\n            \"random_strength\": Real(0, 2),\n            \"bagging_temperature\": Real(0, 2),\n        }\n\n    CB_bayes_search = BayesSearchCV(\n        estimator = CatBoostClassifier(),\n        search_spaces = search_space,\n        scoring = 'accuracy',\n        cv = StratifiedKFold(n_splits=5),\n        n_jobs = 3,\n        n_iter = 32\n        )\n\n    def callback(res):\n        global i\n        print(\"Run \",i)\n        i += 1\n\n    i = 0\n    CB_bayes_search.fit(X_train, y_train, callback=callback)\n\n    best_params_CB = CB_bayes_search.best_params_\n    best_estimator_CB = CB_bayes_search.best_estimator_\n    best_score_CB = CB_bayes_search.best_score_","d1e59b63":"clf = best_estimator_CB\nbest_params = CB_bayes_search.best_params_\nprint(best_params)\n\n# run the fitted model on the test set and obtain the probabilities that each data point lies in each class\ny_prob_CB = clf.predict_proba(X_test)\n\n# set the probabilty threshold to determine precision\/recall tradeoff\nthreshold = 0.4\n\n# obtain 1\/0 list based on if probabiltiy is above\/below threshold\ncertainty = y_prob_CB[:, 1] > threshold\ny_pred_CB = certainty.astype(int)  \n\n# calculate metrics\nfpr, tpr, threshold = roc_curve(y_test, y_pred_CB)\nroc_auc_CB = auc(fpr, tpr)\nprec_CB = precision_score(y_test, y_pred_CB)\nrec_CB = recall_score(y_test, y_pred_CB)\nacc_CB = accuracy_score(y_test, y_pred_CB)\n\n\nprint(\"AUC: \", roc_auc_CB)\nprint(\"Precision: \" , prec_CB)\nprint(\"Recall: \" , rec_CB)\nprint(\"Accuracy: \", acc_CB)\n\nplot_metric((acc_CB, acc), \"Accuracy\", [\"CatBoost\", \"XGB\"], colours=[\"orange\", \"green\"])\nplot_metric((rec_CB, rec), \"Recall\", [\"CatBoost\", \"XGB\"], colours=[\"orange\", \"green\"])\nplot_metric((prec_CB, prec), \"Precision\", [\"CatBoost\", \"XGB\"], colours=[\"orange\", \"green\"])\nplot_metric((roc_auc_CB, roc_auc), \"AUC\", [\"CatBoost\", \"XGB\"], colours=[\"orange\", \"green\"])","9a5bcb58":"# do 3-ensemble\n\ny_prob_3ens = (y_prob[:, 1].reshape(184, 1) + y_prob_nn + y_prob_CB[:, 1].reshape(184, 1)) \/ 3\n\n# as before, set recall to 98%\nthreshold = 0.42\ncertainty = y_prob_3ens > threshold\ny_pred_3ens = certainty.astype(int)  \n\n# calculate metrics\nfpr, tpr, threshold = roc_curve(y_test, y_pred_3ens)\nroc_auc_3ens = auc(fpr, tpr)\nprec_3ens = precision_score(y_test, y_pred_3ens)\nrec_3ens = recall_score(y_test, y_pred_3ens)\nacc_3ens = accuracy_score(y_test, y_pred_3ens)\n\nprint(\"accuracy: \", acc_3ens)\nprint(\"Recall: \", rec_3ens)\nprint(\"Precision: \", prec_3ens)\nprint(\"AUC: \", roc_auc_3ens)\n\nplot_metric((acc_3ens, acc_ens), \"Accuracy\", [\"3-Ensemble\", \"2-Ensemble\"], colours=[\"purple\", \"teal\"])\nplot_metric((rec_3ens, rec_ens), \"Recall\", [\"3-Ensemble\", \"2-Ensemble\"], colours=[\"purple\", \"teal\"])\nplot_metric((prec_3ens, prec_ens), \"Precision\", [\"3-Ensemble\", \"2-Ensemble\"], colours=[\"purple\", \"teal\"])\nplot_metric((roc_auc_3ens, roc_auc_ens), \"AUC\", [\"3-Ensemble\", \"2-Ensemble\"], colours=[\"purple\", \"teal\"])","2b3b15af":"Both the XGB and NN models have had their decision boundaries tuned to give a recall score of 98% (recall is very important in a heart disease dataset). Doing so, The XGB model slightly outperformed the NN in all used metrics; accuracy, precision and AUC. Running this notebook several times however, it was found that both models are quite unstable and lead to varying final test set metrics. Therefore, to give a better representation, the whole notebook should be run several times. From this, a mean and uncertainty in the metrics could be obtained.\n","446af7ed":"As can be seen, the basic ensemble of the XGB and NN model outperforms the individual XGB model in all categories. This is a significant improvement and begs the question of what other models could be added to an ensemble to achieve even better results? A potential candidate could be catBoost, which is a decision tree method specifically made for categorical data - which makes up 5\/11 attributes in this dataset.\n\nAdditionally, a more advanced ensemble method could be chosen, e.g. putting more emphasis on the better models. Or even a stacking ensemble where an entirely new model is made that learns how to most efefctively weight the probabilities generated by the sub-models.","0048f22e":"## Comparing XGBoost, CatBoost and NN on a tabular dataset\n\nThis notebook aims to compare the performance of two widely-used, but very different, models on a Kaggle dataset (https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction\/tasks?taskId=6055) which aims to predict heart disease onset using a range of patient information. The notebook will use scikit-learn's implementation of XGBoost and CatBoost and tensorflow's neural networks. Hyperparameter optimization will be run for all models and the performance will be compared. Various ensembles of the models will also be tested.","0186d410":"CatBoost slightly outperforms XGB in all metrics - let's add it as a third sub-model to the ensemble.","bd638d65":"As expected, the 3-ensemble model outperforms the 2-ensemble model. It was found that running this notebook several times produced varying results for the optimizers for the sub-models. More data exploration and subsequent transformation should be done to provide a hopefully less unstable training process. Also, the best models should be saved to a file so that the hyperparameter optimization steps don't have to be re-run each time."}}