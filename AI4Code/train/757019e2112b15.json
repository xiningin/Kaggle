{"cell_type":{"815e81a7":"code","d2653212":"code","cdbd1a59":"code","4a61b24e":"code","63b905a2":"code","10514a89":"code","22743d60":"code","3fce6db7":"code","c1cb1dc5":"code","5a724b2c":"code","213a4763":"code","f7d1610e":"code","0b151072":"code","6ce42817":"code","6e8bf8a5":"code","0384d77f":"code","8ddd7ad0":"code","a9b92ce0":"code","c1100e18":"code","4049607f":"code","d6720196":"code","e809ea17":"code","aefbd7ab":"code","11ec69e5":"code","53e7aa80":"code","0204abc6":"code","02947048":"code","7cb3a1d9":"code","e24f1896":"code","88d91351":"code","a0c797f4":"code","94d86957":"markdown","aa6b9f69":"markdown","3be428e4":"markdown","c0637cfd":"markdown","a6b40f98":"markdown","7468f2e5":"markdown","c7047e29":"markdown","967b57ee":"markdown","107a720d":"markdown","5966445e":"markdown","7370a958":"markdown","cc30ff46":"markdown","ae440144":"markdown","3ce3fc88":"markdown","3d163bad":"markdown"},"source":{"815e81a7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for feature importance study\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nimport shap\n\n# ML\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nimport os\nimport gc\n\n# Reproducability\ndef set_seed(seed = 0):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    print('*** --- Set seed \"%i\" --- ***' %seed)\n\n# Custom theme\nplt.style.use('fivethirtyeight')\n\nfigure = {'dpi': '100'}\nfont = {'family': 'serif'}\ngrid = {'linestyle': ':', 'alpha': .9}\naxes = {'titlecolor': 'black', 'titlesize': 20, 'titleweight': 'bold',\n        'labelsize': 12, 'labelweight': 'bold'}\n\nplt.rc('font', **font)\nplt.rc('figure', **figure)\nplt.rc('grid', **grid)\nplt.rc('axes', **axes)\n\nmy_colors = ['#DC143C', '#FF1493', '#FF7F50', '#FFD700', '#32CD32', \n             '#4ddbff', '#1E90FF', '#663399', '#708090']\n\ncaption = \"\u00a9 maksymshkliarevskyi\"\n\n# Show our custom palette\nsns.palplot(sns.color_palette(my_colors))\nplt.title('Custom palette')\nplt.text(6.9, 0.75, caption, size = 8)\nplt.show()","d2653212":"def model_imp_viz(model, columns, bias = 0.01):\n    imp = pd.DataFrame({'importance': model.feature_importances_,\n                        'features': columns}).sort_values('importance', \n                                                          ascending = False)\n    fig, ax = plt.subplots(figsize = (10, 40))\n    plt.title('Feature importances', size = 15, fontweight = 'bold', fontfamily = 'serif')\n\n    sns.barplot(x = imp.importance, y = imp.features, edgecolor = 'black',\n                palette = reversed(sns.color_palette(\"viridis\", len(imp.features))))\n\n    for i in ['top', 'right']:\n            ax.spines[i].set_visible(None)\n\n    rects = ax.patches\n    labels = imp.importance\n    for rect, label in zip(rects, labels):\n        x_value = rect.get_width() + bias\n        y_value = rect.get_y() + rect.get_height() \/ 2\n\n        ax.text(x_value, y_value, round(label, 4), fontsize = 9, color = 'black',\n                 ha = 'center', va = 'center')\n    ax.set_xlabel('Importance', fontweight = 'bold', fontfamily = 'serif')\n    ax.set_ylabel('Features', fontweight = 'bold', fontfamily = 'serif')\n    plt.show()","cdbd1a59":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv', \n                    index_col = 0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv', \n                   index_col = 0)\nss = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","4a61b24e":"train.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","63b905a2":"test.describe().T.style.background_gradient(subset = ['count'], cmap = 'viridis') \\\n    .bar(subset = ['mean', '50%'], color = my_colors[6]) \\\n    .bar(subset = ['std'], color = my_colors[0])","10514a89":"dtypes = train.dtypes.value_counts().reset_index()\n\nplt.figure(figsize = (12, 1))\nplt.title('Data types\\n')\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[0, 1],\n         label = str(dtypes.iloc[0, 0]), color = my_colors[4])\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[1, 1], \n         left = dtypes.iloc[0, 1], \n         label = str(dtypes.iloc[1, 0]), color = my_colors[5])\nplt.legend(loc = 'upper center', ncol = 3, fontsize = 13,\n           bbox_to_anchor = (0.5, 1.45), frameon = False)\nplt.yticks('')\nplt.text(110, -0.9, caption, size = 8)\nplt.show()","22743d60":"claim = train.claim\n\ntrain.drop(['claim'], axis = 1, inplace = True)","3fce6db7":"# Concatenate train and test datasets\nall_data = pd.concat([train, test], axis = 0)\n\n# columns with missing values\ncols_with_na = all_data.isna().sum()[all_data.isna().sum() > 0].sort_values(ascending = False)\ncols_with_na","c1cb1dc5":"train_na = train.isnull().sum(1)\ntest_na = test.isnull().sum(1)","5a724b2c":"missing = pd.DataFrame({'na': train_na, \n                        'claim': claim}).groupby('na').agg(['mean', 'sum'])\nmissing['claim']['mean'] = missing['claim'] * 100\n\nmissing['claim']['mean'].plot.bar(figsize=(15, 5), \n                                  color = my_colors[0], \n                                  legend = None,\n                                  width = 0.7)\nplt.title('Claim rate by missing features')\nplt.xticks(rotation=0)\nplt.xlabel('Sum of missing features')\nplt.ylabel('Claim rate, %')\nplt.show()\n\nmissing['claim']['sum'].plot.bar(figsize=(15, 5), \n                                 color = my_colors[2], \n                                 legend = None,\n                                 width = 0.7)\nplt.title('Sum of observations by missing features')\nplt.xticks(rotation=0)\nplt.xlabel('Sum of missing features')\nplt.ylabel('Sum of observations')\nplt.show()","213a4763":"print('Train data')\nfig = plt.figure(figsize = (20, 140))\nfor idx, i in enumerate(train.columns):\n    fig.add_subplot(np.ceil(len(train.columns)\/4), 4, idx+1)\n    train.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(9, -20000, caption, size = 12)\nplt.show()","f7d1610e":"print('Test data')\nfig = plt.figure(figsize = (20, 140))\nfor idx, i in enumerate(test.columns):\n    fig.add_subplot(np.ceil(len(test.columns)\/4), 4, idx+1)\n    test.iloc[:, idx].hist(bins = 20)\n    plt.title(i)\nplt.text(9, -15000, caption, size = 12)\nplt.show()","0b151072":"corr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\nplt.text(124, 124, caption, size = 8)\nplt.show()","6ce42817":"corr = test.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nplt.figure(figsize = (15, 15))\nplt.title('Corelation matrix')\nsns.heatmap(corr, mask = mask, cmap = 'Spectral_r', linewidths = .5)\nplt.text(124, 124, caption, size = 8)\nplt.show()","6e8bf8a5":"train['na'] = train_na\ntest['na'] = test_na\n\ncolumn_name = train.columns\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ntrain = imputer.fit_transform(train)\ntest = imputer.transform(test)","0384d77f":"# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(train, claim, \n                                                      test_size = 0.2,\n                                                      random_state = 0)","8ddd7ad0":"# The basic model\nparams = {'random_state': 0,\n          'predictor': 'gpu_predictor',\n          'tree_method': 'gpu_hist',\n          'eval_metric': 'auc'}\n\nmodel = XGBClassifier(**params)\n\nmodel.fit(X_train, y_train, verbose = False)\n\npreds = model.predict_proba(X_valid)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, preds)\nprint('Valid AUC: ', metrics.auc(fpr, tpr))","a9b92ce0":"metrics.plot_confusion_matrix(model, X_valid, y_valid,\n                              cmap = 'inferno')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","c1100e18":"model_imp_viz(model, column_name.values, bias = 0.04)","4049607f":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nshap.summary_plot(shap_values, X_valid)","d6720196":"# The basic model\nXGB_params = {'n_estimators': 10000,\n              'max_depth': 2,\n              'colsample_bytree': 0.30,\n              'learning_rate': 0.09,\n              'reg_lambda': 18,\n              'reg_alpha': 18,\n              'random_state': 0,\n              'predictor': 'gpu_predictor',\n              'tree_method': 'gpu_hist',\n              'eval_metric': 'auc'}\n\nmodel = XGBClassifier(**XGB_params)\n\nmodel.fit(X_train, y_train,\n          eval_set = [(X_valid, y_valid)],\n          early_stopping_rounds = 300,\n          verbose = 1000)\n\npreds = model.predict_proba(X_valid)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, preds)\nprint('Valid AUC: ', metrics.auc(fpr, tpr))","e809ea17":"metrics.plot_confusion_matrix(model, X_valid, y_valid,\n                              cmap = 'inferno')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","aefbd7ab":"model_imp_viz(model, column_name.values, bias = 0.04)","11ec69e5":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nshap.summary_plot(shap_values, X_valid)","53e7aa80":"CATB_params = {'n_estimators': 10000,\n               'max_depth': 2,\n               'learning_rate': 0.09,\n               'grow_policy': 'SymmetricTree',\n               'task_type': 'GPU',\n               'eval_metric': 'AUC',\n               'random_state': 0}\n    \nmodel = CatBoostClassifier(**CATB_params)\nmodel.fit(X_train, y_train,  \n          eval_set=[(X_valid, y_valid)],\n          early_stopping_rounds = 400,\n          verbose = 1000)\n\npreds = model.predict_proba(X_valid)[:, 1]\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, preds)\nprint('Valid AUC: ', metrics.auc(fpr, tpr))","0204abc6":"metrics.plot_confusion_matrix(model, X_valid, y_valid,\n                              cmap = 'inferno')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","02947048":"model_imp_viz(model, column_name.values, bias = 3)","7cb3a1d9":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_valid)\n\nshap.summary_plot(shap_values, X_valid)","e24f1896":"FOLDS = 5\nss.claim = np.zeros(len(ss.claim))\nmetric = []\nkfold = KFold(n_splits = FOLDS, random_state = 0, shuffle = True)\ni = 1\nfor train_idx, test_idx in kfold.split(train):\n    X_train, y_train = train[train_idx, :], claim[train_idx]\n    X_test, y_test = train[test_idx, :], claim[test_idx]\n\n    model = XGBClassifier(**XGB_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_test, y_test)],\n              early_stopping_rounds = 300,\n              verbose = 0)\n    \n    preds = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds)\n    AUC = metrics.auc(fpr, tpr)    \n    print('[FOLD #{}] Validation AUC: {:.5f}'.format(i, AUC))\n\n    ss.claim += model.predict_proba(test)[:, 1] \/ FOLDS * 0.5\n    metric.append(AUC)\n    i += 1\nprint('*'*50)\nprint('[ALL FOLDS] Mean Validation AUC: {:.5f}'.format(np.mean(metric)))","88d91351":"metric = []\nkfold = KFold(n_splits = FOLDS, random_state = 0, shuffle = True)\ni = 1\nfor train_idx, test_idx in kfold.split(train):\n    X_train, y_train = train[train_idx, :], claim[train_idx]\n    X_test, y_test = train[test_idx, :], claim[test_idx]\n\n    model = CatBoostClassifier(**CATB_params)\n    model.fit(X_train, y_train,\n              eval_set = [(X_test, y_test)],\n              early_stopping_rounds = 400,\n              verbose = 0)\n    \n    preds = model.predict_proba(X_test)[:, 1]\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds)\n    AUC = metrics.auc(fpr, tpr)    \n    print('[FOLD #{}] Validation AUC: {:.5f}'.format(i, AUC))\n\n    ss.claim += model.predict_proba(test)[:, 1] \/ FOLDS * 0.5\n    metric.append(AUC)\n    i += 1\nprint('*'*50)\nprint('[ALL FOLDS] Mean Validation AUC: {:.5f}'.format(np.mean(metric)))","a0c797f4":"ss.to_csv('submission.csv', index = False)\nss","94d86957":"We have  training and  test observations. All our data is in `float32` format. Target feature has `int` format.\n\nBefore we continue, let's pull the target feature into the separate variable.","aa6b9f69":"Also, let's try a CatBoostClassifier.","3be428e4":"<h2 style='color:white; background:#1E90FF; border:0'><center>EDA<\/center><\/h2>\n\n[**Back to the start**](#section-start)","c0637cfd":"<h2 style='color:white; background:#1E90FF; border:0'><center>WORK IN PROGRESS...<\/center><\/h2>\n\n[**Back to the start**](#section-start)","a6b40f98":"We should also look at the correlations between features.","7468f2e5":"<h1 style='color:white; background:#1E90FF; border:0'><center>TPS-Sep: all for start (EDA,XGB+CatBoost Baseline)<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)\n\n<a id=\"section-start\"><\/a>\n\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n\nFor this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from `0.0` to `1.0`, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\n### See also my previous TPS works:\n- [TPS-Jun: starting point (EDA, Baseline, CV)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-jun-starting-point-eda-baseline-cv)\n- [TPS-July: EDA, Baseline Analysis (XGBRegressor)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-july-eda-baseline-analysis-xgbregressor)\n- [TPS-Aug: EDA, Baselines (XGB, Keras NN)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-aug-eda-baselines-xgb-keras-nn)","c7047e29":"It's important to see if our data has missing values.","967b57ee":"<h2 style='color:white; background:#1E90FF; border:0'><center>Test prediction<\/center><\/h2>\n\n[**Back to the table of contents**](#section-start)\n\nLet's make 5-folds cv prediction for XGBClassifier and  CatBoostClassifier.","107a720d":"First, let's load the data and take a look at basic statistics.","5966445e":"Wow, some groups have a low claim rate. Looks like this new feature can be used for prediction.\n\nNow, let's look at the feature distributions.","7370a958":"The feature we created is really very important!","cc30ff46":"<h2 style='color:white; background:#1E90FF; border:0'><center>Required functions<\/center><\/h2>\n\n[**Back to the start**](#section-start)","ae440144":"All features are weakly correlated.\n\n<h2 style='color:white; background:#1E90FF; border:0'><center>XGB Baseline<\/center><\/h2>\n\n[**Back to the start**](#section-start)\n\nIn this step, we'll train our simple baseline XGBClassifier model. There are missing values in our data. Let's fill them with an average.","3ce3fc88":"Let's create a variable with the sum of missing features, and then look at the claim rate for each group of this new feature.","3d163bad":"Let's try a slightly more complex model."}}