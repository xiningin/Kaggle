{"cell_type":{"51233c5c":"code","82c0cc98":"code","64a6b64e":"code","d6a1aada":"code","76ca78a4":"code","652f6d23":"code","150fd8f3":"code","bd43b99c":"code","e186c82c":"code","cfe33c41":"code","d38f0bd5":"code","2c94b47d":"code","9097afe3":"code","17fb3675":"code","6ed710cf":"code","8b183b04":"code","0e3bebd2":"code","a237ae04":"code","5d809e2f":"code","b313df61":"code","e6fdda91":"code","52f240c2":"code","ad1d8068":"code","38e47d46":"code","d7614a10":"markdown","58209a85":"markdown","f34a8371":"markdown","53f77391":"markdown","4b22f28b":"markdown","c1630661":"markdown","4aac377e":"markdown","e640bc5d":"markdown","bc737506":"markdown"},"source":{"51233c5c":"import os\nfrom pathlib import Path\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm import tqdm","82c0cc98":"ls ..\/input\/petfinder-pawpularity-score\/","64a6b64e":"train = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\n\nINPUT = Path('..\/input\/petfinder-pawpularity-score\/')\nTRAIN_IMG_DIR = INPUT \/ 'train'            \nTEST_IMG_DIR = INPUT \/'test'\n\ntrain.shape, test.shape","d6a1aada":"train.head()","76ca78a4":"test.head()","652f6d23":"train['img_path'] = train['Id'].apply(lambda x: f'..\/input\/petfinder-pawpularity-score\/train\/{str(x)}.jpg')\ntest['img_path'] = test['Id'].apply(lambda x: f'..\/input\/petfinder-pawpularity-score\/test\/{str(x)}.jpg')\ntarget_col = 'Pawpularity'\nmetadata_cols = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n                 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\ntrain[metadata_cols + [target_col]].mean()","150fd8f3":"test[metadata_cols].mean()","bd43b99c":"plt.hist(train[target_col], bins=50);","e186c82c":"def create_shape_feature(df):\n    width_height_list = []\n    file_size_list = []\n    for path_ in tqdm(df['img_path']):\n        width_height_list.append(Image.open(path_).size)\n        file_size_list.append(os.path.getsize(path_))\n    df['width_height'] = width_height_list\n    df['file_size'] = file_size_list\n    df['width'] = df['width_height'].apply(lambda x: x[0])\n    df['height'] = df['width_height'].apply(lambda x: x[1])\n    return df","cfe33c41":"train = create_shape_feature(train)\ntest = create_shape_feature(test)","d38f0bd5":"train['width_height'].value_counts()[:20]","2c94b47d":"test['width_height'].value_counts()","9097afe3":"im = Image.open(test['img_path'].values[0])\nplt.imshow(im);","17fb3675":"# !pip install ipyplot\n!python -m pip install --no-index --find-links=..\/input\/ipyplot ipyplot\nimport ipyplot","6ed710cf":"image_paths = []\nlabels = []\ncustom_texts = []\n\nfor col in metadata_cols:\n    tmp_df = train[train[col] == 1]\n    for i in range(4):\n        image_paths.append(tmp_df.iloc[i, :]['img_path'])\n        labels.append(col)\n        target = str(tmp_df.iloc[i, :][target_col])\n        meta = tmp_df.iloc[i, :][metadata_cols + ['width', 'height']].values\n        meta = ''.join([f'{col}:{m}, ' for m, col in zip(meta, metadata_cols + ['width', 'height'])])\n        custom_texts.append(f'target: {target}\\n{meta}')","8b183b04":"ipyplot.plot_class_tabs(image_paths, labels, custom_texts=custom_texts, force_b64=True, img_width=350)","0e3bebd2":"ipyplot.plot_images(test['img_path'].values, force_b64=True, img_width=100)","a237ae04":"train['area'] = train['width'] * train['height']\ntrain['size_per_ pixel'] = train['file_size'] \/ train['area']\n\ntest['area'] = test['width'] * test['height']\ntest['size_per_ pixel'] = test['file_size'] \/ test['area']","5d809e2f":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    # ref https:\/\/www.kaggle.com\/corochann\/permutation-importance-for-feature-selection-part1\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n\n\ndef calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    return mean_df\n\n\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(4, 6)):\n    importance_df = importance_df.iloc[-50:, :]\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()\n\n\ndef do_train(all_feature, params):\n\n    models = []\n    scores = []\n\n    gain_importance_list = []\n    split_importance_list = []\n\n    y = all_feature['Pawpularity'].values\n    X = all_feature.drop(['Id', 'img_path', 'width_height', 'Pawpularity'], axis=1)\n    print(f'features: {X.columns.values}')\n    print(f'num features: {len(X.columns)}')\n\n    oof = np.zeros(len(X))\n    \n    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n\n    for fold, (trn_idx, val_idx) in enumerate(kf.split(X)):\n\n        print(f\"Fold :{fold+1}\")\n\n        # create dataset\n        X_train, y_train = X.iloc[trn_idx], y[trn_idx]\n        X_valid, y_valid = X.iloc[val_idx], y[val_idx]\n\n        # weight\n        weights = None\n        lgbm_train = lgb.Dataset(X_train, y_train, weight=weights)\n        lgbm_valid = lgb.Dataset(X_valid, y_valid, reference=lgbm_train, weight=weights)\n\n        # model\n        model = lgb.train(params=params,\n                          train_set=lgbm_train,\n                          valid_sets=[lgbm_train, lgbm_valid],\n                          num_boost_round=5000,\n                          verbose_eval=100,\n                          categorical_feature=metadata_cols,\n                          early_stopping_rounds=30\n                          )\n\n        # validation\n        y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n        oof[val_idx] = y_pred\n\n        score = round(np.sqrt(mean_squared_error(y_true=y_valid, y_pred=y_pred)), 3)\n        print(f'RMSE: {score}')\n\n        # keep scores and models\n        scores.append(score)\n        models.append(model)\n        print(\"*\" * 5)\n\n        # --- calc model feature importance ---\n        feature_names = X_train.columns.values.tolist()\n        gain_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='gain')\n        gain_importance_list.append(gain_importance_df)\n\n        split_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='split')\n        split_importance_list.append(split_importance_df)\n\n    print(scores)\n    score = round(np.sqrt(mean_squared_error(y_true=y, y_pred=oof)), 3)\n    print('score: ', score)\n\n    gain_importance_df = calc_mean_importance(gain_importance_list)\n    split_importance_df = calc_mean_importance(split_importance_list)\n\n    return models, gain_importance_df, split_importance_df, oof, score","b313df61":"lgb_params = {\n    'objective': 'regression',\n    'max_depth': 3,\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.1,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 2,\n    'verbosity': -1,\n}\n\nmodels, gain_importance_df, split_importance_df, oof, score = do_train(train, lgb_params)","e6fdda91":"plot_importance(gain_importance_df, 'importance_gain')\nplot_importance(split_importance_df, 'importance_split')","52f240c2":"plt.scatter(train[target_col], oof, s=2)\nplt.xlabel('target')\nplt.ylabel('oof')\nplt.title(f'cv: {score}');","ad1d8068":"mean_of_target = train[target_col].mean()\nprint(f'Target average: {mean_of_target}')\n\nscore = np.sqrt(mean_squared_error(y_true=train[target_col], y_pred = np.ones(len(train)) * mean_of_target))\nprint(f'RMSE when predicting the average value of the target: {score}')","38e47d46":"sample = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\ntest_feature = test.drop(['Id', 'img_path', 'width_height'], axis=1)\n\npreds = []\nfor model in models:\n    preds.append(model.predict(test_feature, num_iteration=model.best_iteration))\n\nsample[target_col] = np.mean(preds, axis=0)\nsample.to_csv('submission.csv', index=False)","d7614a10":"Metafeature and image size features seem to contribute little to the score improvement.","58209a85":">test\/ - Folder containing randomly generated images in a format similar to the training set photos. The actual test data comprises about 6800 pet photos similar to the training set photos.\n\nThe test data images in the training phase are randomly generated images.","f34a8371":"# target: Pawpularity","53f77391":"# Image shape","4b22f28b":"# LightGBM","c1630661":"## test predict","4aac377e":"# Image","e640bc5d":"## test","bc737506":"## train"}}