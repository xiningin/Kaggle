{"cell_type":{"11cb586d":"code","e1b307a6":"code","5e4bced0":"code","c9869c38":"code","68a0ed18":"code","7ea6eb53":"code","7d746e3d":"code","981b5d4c":"code","479a6716":"code","6894facd":"code","8510f713":"code","b2d86d05":"code","909e84a1":"code","6e4bdb4f":"code","01678289":"code","2adf3e29":"code","177ad9cf":"code","d6764e64":"code","c9a4ce63":"code","0b43d952":"code","46cabe44":"code","266b6d69":"code","20214fc6":"code","23dafd82":"code","f20029fe":"code","5a9ab768":"code","93589ac2":"code","875fa45d":"code","cb793069":"code","470b99f3":"code","9a13f877":"code","7d8b0388":"code","58ecaa36":"code","7df77847":"code","d4a1a3d7":"code","05abfa22":"code","d27104a7":"code","2a8e0b08":"code","885cf54b":"code","d3d375d4":"markdown","7c521180":"markdown","eabe9dea":"markdown","5f5fa084":"markdown","66eadadb":"markdown","e6b34827":"markdown","55eafc84":"markdown","cd42bae6":"markdown","450ebb8b":"markdown","5470c47f":"markdown","6d2b7729":"markdown","3d950bc8":"markdown","83bf4417":"markdown","7d5ce106":"markdown","08bfe9e1":"markdown","08910477":"markdown","2cb2d31b":"markdown","4f878ad1":"markdown","7dd75101":"markdown","c60b8842":"markdown","971623bc":"markdown","97ca8c08":"markdown","82dd5222":"markdown","aeee048f":"markdown"},"source":{"11cb586d":"import os\nimport time\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# data manipulation\nimport json\nfrom pandas.io.json import json_normalize\nimport numpy as np\nimport pandas as pd\n# plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n# model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb","e1b307a6":"%%time\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\ntrain = load_df('..\/input\/train.csv')\ntest = load_df('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')\ngc.collect()","5e4bced0":"train.head()","c9869c38":"set(train.columns).difference(set(test.columns))","68a0ed18":"cons_col = [i for i in train.columns if train[i].nunique(dropna=False)==1]\ncons_col","7ea6eb53":"train = train.drop(cons_col + ['trafficSource.campaignCode'], axis=1)\ntest = test.drop(cons_col, axis=1)\ngc.collect()","7d746e3d":"print(train.shape)\nprint(test.shape)","981b5d4c":"def find_missing(data):\n    # number of missing values\n    count_missing = data.isnull().sum().values\n    # total records\n    total = data.shape[0]\n    # percentage of missing\n    ratio_missing = count_missing\/total\n    # return a dataframe to show: feature name, # of missing and % of missing\n    return pd.DataFrame(data={'missing_count':count_missing, 'missing_ratio':ratio_missing}, index=data.columns.values)\ntrain_missing = find_missing(train)\ntest_missing = find_missing(test)","479a6716":"train_missing.reset_index()[['index', 'missing_ratio']]\\\n    .merge(test_missing.reset_index()[['index', 'missing_ratio']], on='index', how='left')\\\n    .rename(columns={'index':'columns', 'missing_ratio_x':'train_missing_ratio', 'missing_ratio_y':'test_missing_ratio'})\\\n    .sort_values(['train_missing_ratio', 'test_missing_ratio'], ascending=False)\\\n    .query('train_missing_ratio>0')","6894facd":"if test.fullVisitorId.nunique() == len(sub):\n    print('Till now, the number of fullVisitorId is equal to the rows in submission. Everything goes well!')\nelse:\n    print('Check it again')","8510f713":"y = np.nan_to_num(np.array([float(i) for i in train['totals.transactionRevenue']]))\nprint('The ratio of customers with transaction revenue is', str((y != 0).mean()))   ","b2d86d05":"plt.figure(figsize=[12, 6])\nsns.distplot(y[y!=0])\nplt.xlabel('transactionRevenue')\nplt.show()","909e84a1":"train[\"totals.transactionRevenue\"] = train[\"totals.transactionRevenue\"].astype('float')\ntarget = np.log1p(train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum())\nprint('The ratio of customers with transaction revenue is', str((target != 0).mean()))","6e4bdb4f":"plt.figure(figsize=[12, 6])\nsns.distplot(target[target!=0])\nplt.xlabel('Target')\nplt.show()","01678289":"def plot_categorical(data, col, size=[8 ,4], xlabel_angle=0, title='', max_cat = None):\n    '''use this for ploting the count of categorical features'''\n    plotdata = data[col].value_counts() \/ len(data)\n    if max_cat != None:\n        plotdata = plotdata[max_cat[0]:max_cat[1]]\n    plt.figure(figsize = size)\n    sns.barplot(x = plotdata.index, y=plotdata.values)\n    plt.title(title)\n    if xlabel_angle!=0: \n        plt.xticks(rotation=xlabel_angle)\n    plt.show()\nplot_categorical(data=train, col='device.browser', size=[8 ,4], xlabel_angle=20, title='Device - Browser', max_cat=[0, 6])","2adf3e29":"plot_categorical(data=train, col='device.deviceCategory', size=[8 ,4], xlabel_angle=0, title='Device - Category')","177ad9cf":"plot_categorical(data=train, col='device.operatingSystem', size=[8 ,4], xlabel_angle=30, \n                 title='Device - Operating System', max_cat = [0, 7])","d6764e64":"plot_categorical(data=train, col='geoNetwork.city', size=[12 ,4], xlabel_angle=30, \n                 title='GeoNetwork - City', max_cat = [1, 20])","c9a4ce63":"plot_categorical(data=train, col='geoNetwork.country', size=[12 ,4], xlabel_angle=30, \n                 title='GeoNetwork - Country', max_cat = [0, 20])","0b43d952":"plot_categorical(data=train, col='geoNetwork.region', size=[12 ,4], xlabel_angle=30, \n                 title='GeoNetwork - Region', max_cat = [1, 20])","46cabe44":"plot_categorical(data=train, col='geoNetwork.metro', size=[12 ,4], xlabel_angle=90, \n                 title='GeoNetwork - metro', max_cat = [2, 20])","266b6d69":"plot_categorical(data=train, col='geoNetwork.subContinent', size=[8 ,4], xlabel_angle=30, \n                 title='GeoNetwork - SubContinent', max_cat = [0, 10])","20214fc6":"plot_categorical(data=train, col='geoNetwork.continent', size=[8 ,4], xlabel_angle=30, \n                 title='GeoNetwork - Continent')","23dafd82":"train['totals.bounces'] = train['totals.bounces'].fillna('0')\nplot_categorical(data=train, col='totals.bounces', size=[8 ,4], xlabel_angle=0, title='Totals - Bounces')","f20029fe":"train['totals.newVisits'] = train['totals.newVisits'].fillna('0')\nplot_categorical(data=train, col='totals.newVisits', size=[8 ,4], xlabel_angle=0, title='Totals - NewVisits')","5a9ab768":"plt.figure(figsize=[12, 6])\nsns.distplot(train['totals.hits'].astype('float'), kde=True,bins=30)\nplt.xlabel('totals.hits')\nplt.title('Total - Hits')\nplt.show()","93589ac2":"plt.figure(figsize=[12, 6])\nsns.distplot(train['totals.pageviews'].astype('float').fillna(0))\nplt.xlabel('totals.pageviews')\nplt.title('Total - Pageviews')\nplt.show()","875fa45d":"plot_categorical(data=train, col='trafficSource.adContent', size=[10 ,4], xlabel_angle=30, \n                 title='TrafficSource - AdContent', max_cat = [0, 10])","cb793069":"plot_categorical(data=train, col='trafficSource.medium', size=[10 ,4], xlabel_angle=30, \n                 title='TrafficSource - medium')","470b99f3":"plot_categorical(data=train, col='channelGrouping', size=[10 ,4], xlabel_angle=30, \n                 title='Channel Grouping')","9a13f877":"a = train.groupby(\"fullVisitorId\")[\"visitNumber\"].max()\nplt.figure(figsize=[12, 6])\nsns.distplot(a)\nplt.xlabel('VisitNumber')\nplt.title('Visit Number')\nplt.show()","7d8b0388":"plt.figure(figsize=[12, 6])\nsns.distplot(train['date'])\nplt.xlabel('Date')\nplt.title('Date')\nplt.show()","58ecaa36":"train_idx = train.fullVisitorId\ntest_idx = test.fullVisitorId\ntrain[\"totals.transactionRevenue\"] = train[\"totals.transactionRevenue\"].astype('float').fillna(0)\ntrain_y = train[\"totals.transactionRevenue\"]\ntrain_target = np.log1p(train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum())","7df77847":"train.drop(['fullVisitorId', 'sessionId', 'visitId'], axis = 1, inplace = True)\ntest.drop(['fullVisitorId', 'sessionId', 'visitId'], axis = 1, inplace = True)\nnum_col = [\"totals.hits\", \"totals.pageviews\", \"visitNumber\", \"visitStartTime\", 'totals.bounces',  'totals.newVisits']\nfor i in num_col:\n    train[i] = train[i].astype('float').fillna(0)\n    test[i] = test[i].astype('float').fillna(0)\ncat_col = [e for e in train.columns.tolist() if e not in num_col]\ncat_col.remove('date')\ncat_col.remove('totals.transactionRevenue')\nfor i in cat_col:\n    lab_en = LabelEncoder()\n    train[i] = train[i].fillna('not known')\n    test[i] = test[i].fillna('not known')\n    lab_en.fit(list(train[i].astype('str')) + list(test[i].astype('str')))\n    train[i] = lab_en.transform(list(train[i].astype('str')))\n    test[i] = lab_en.transform(test[i].astype('str'))\n    print('finish', i)","d4a1a3d7":"y_train = np.log1p(train[\"totals.transactionRevenue\"])\nx_train = train.drop([\"totals.transactionRevenue\"], axis=1)\nx_test = test.copy()\nprint(x_train.shape)\nprint(x_test.shape)","05abfa22":"folds = KFold(n_splits=5,random_state=6)\noof_preds = np.zeros(x_train.shape[0])\nsub_preds = np.zeros(x_test.shape[0])\n\nstart = time.time()\nvalid_score = 0\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(x_train, y_train)):\n    trn_x, trn_y = x_train.iloc[trn_idx], y_train[trn_idx]\n    val_x, val_y = x_train.iloc[val_idx], y_train[val_idx]    \n    \n    train_data = lgb.Dataset(data=trn_x, label=trn_y)\n    valid_data = lgb.Dataset(data=val_x, label=val_y)\n    \n    params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", 'n_estimators':10000, 'early_stopping_rounds':100,\n              \"num_leaves\" : 30, \"learning_rate\" : 0.01, \"bagging_fraction\" : 0.9,\n              \"feature_fraction\" : 0.3, \"bagging_seed\" : 0}\n    \n    lgb_model = lgb.train(params, train_data, valid_sets=[train_data, valid_data], verbose_eval=1000) \n    \n    oof_preds[val_idx] = lgb_model.predict(val_x, num_iteration=lgb_model.best_iteration)\n    oof_preds[oof_preds<0] = 0\n    sub_pred = lgb_model.predict(x_test, num_iteration=lgb_model.best_iteration) \/ folds.n_splits\n    sub_pred[sub_pred<0] = 0 # should be greater or equal to 0\n    sub_preds += sub_pred\n    print('Fold %2d RMSE : %.6f' % (n_fold + 1, np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))))\n    valid_score += np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))","d27104a7":"print('Session-level CV-score:', str(round(valid_score\/folds.n_splits,4)))\nprint(' ')\ntrain_pred = pd.DataFrame({\"fullVisitorId\":train_idx})\ntrain_pred[\"PredictedLogRevenue\"] = np.expm1(oof_preds)\ntrain_pred = train_pred.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\ntrain_pred.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\ntrain_pred[\"PredictedLogRevenue\"] = np.log1p(train_pred[\"PredictedLogRevenue\"])\ntrain_rmse = np.sqrt(mean_squared_error(train_target, train_pred['PredictedLogRevenue']))\nprint('User-level score:', str(round(train_rmse, 4)))\nprint(' ')\nend = time.time()\nprint('training time:', str(round((end - start)\/60)), 'mins')","2a8e0b08":"test_pred = pd.DataFrame({\"fullVisitorId\":test_idx})\ntest_pred[\"PredictedLogRevenue\"] = np.expm1(sub_preds)\ntest_pred = test_pred.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\ntest_pred.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\ntest_pred[\"PredictedLogRevenue\"] = np.log1p(test_pred[\"PredictedLogRevenue\"])\ntest_pred.to_csv(\"lgb_base_model.csv\", index=False) # submission","885cf54b":"lgb.plot_importance(lgb_model, height=0.5, max_num_features=20, ignore_zero = False, figsize = (12,6), importance_type ='gain')\nplt.show()","d3d375d4":"## <a id='2'>2. Check the Data<\/a>","7c521180":"**Be careful! In a typical time series application, we can't use latter records to predict previous one.**\n\nHowever till now, I am not sure we should treat this problem as a time-series one or not. Feel free to comment and discuss. \n\nBTW, if you want to conduct time-series split, see [sklearn document](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.TimeSeriesSplit.html) for more information.","eabe9dea":"###  <a id='3-3'>3.3 GeoNetwork group<\/a>","5f5fa084":"###  <a id='3-4'>3.4 Totals group<\/a>","66eadadb":"Thanks [this kernel](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\/notebook) by [Julian](https:\/\/www.kaggle.com\/julian3833) for handling the columns with JSON data. ","e6b34827":"## <a id='4'>4. Baseline Model<\/a>\n###  <a id='4-1'>4.1 Pre-processing<\/a>\n#### Index and Target","55eafc84":"###  <a id='4-2'>4.2 Modeling<\/a>\n#### train valid split","cd42bae6":"## <a id='3'>3. Explore the Data<\/a>","450ebb8b":"- <a href='#0'>0. Introduction<\/a>  \n- <a href='#1'>1. Get the Data<\/a>\n- <a href='#2'>2. Check the Data<\/a>\n    - <a href='#2-1'>2.1 Remove useless columns<\/a>\n    - <a href='#2-2'>2.2 Check missing features<\/a>\n- <a href='#3'> 3. Explore the data<\/a>\n    - <a href='#3-1'>3.1 Y label<\/a>\n    - <a href='#3-2'>3.2 Device group<\/a>\n    - <a href='#3-3'>3.3 GeoNetwork group<\/a>\n    - <a href='#3-4'>3.4 Totals group<\/a>\n    - <a href='#3-5'>3.5 TrafficSource group<\/a>\n    - <a href='#3-6'>3.6 Others<\/a>\n- <a href='#4'> 4. Baseline model<\/a>\n    - <a href='#4-1'>4.1 Pre-processing<\/a>\n    - <a href='#4-2'>4.2 Modeling<\/a>\n    - <a href='#4-3'>4.3 Feature importance<\/a>","5470c47f":"### <a id='2-1'>2.1 Remove useless columns<\/a>\n**The fields in train set but not test set**","6d2b7729":"###  <a id='3-2'>3.2 Device group<\/a>","3d950bc8":"###  <a id='3-6'>3.6  Others<\/a>\n**Channel Grouping**","83bf4417":"#### Pre-processing: label encoder","7d5ce106":"###  <a id='3-5'>3.5 TrafficSource group<\/a>","08bfe9e1":"## <a id='0'>0. Introduction<\/a>\nThe 80\/20 rule has proven true for many businesses\u2013only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.\n\nRStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have.\n\nIn this competition, you\u2019re challenged to analyze a Google Merchandise Store customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.\n\nSubmissions are scored on the root mean squared error(RMSE). For each *fullVisitorId* in the test set, you must predict the natural log of their total revenue in PredictedLogRevenue. We are predicting the natural log of the sum of all transactions per user. For every user in the test set, the target is:\n\n$$y_{user} = \\sum_{i=1}^ntransaction_{useri}$$\n\n$$target_{user} = ln(y_{user}+1)$$","08910477":"## <a id='1'>1. Get the Data<\/a>","2cb2d31b":"**Visit Number **","4f878ad1":"### <a id='2-2'>2.2 Check missing values<\/a>","7dd75101":"- **fullVisitorId**: A unique identifier for each user of the Google Merchandise Store.\n- **visitId**: An identifier for this session. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n- **sessionId**:  *fullVisitorId_visitId*; A unique identifier for this visit to the store.\n- **date** : The date on which the user visited the Store.\n- **visitNumber**: The session number for this user. If this is the first session, then this is set to 1.\n- **visitStartTime**: The timestamp (expressed as POSIX time).\n- **socialEngagementType**: Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n- **channelGrouping** : The channel via which the user came to the Store. 'Organic Search', 'Referral', 'Paid Search', 'Affiliates', 'Direct', 'Display', 'Social' or 'Other'.\n- **device** : The specifications for the device used to access the Store. **It includes 16 variables, 4 of which are useful. **\n- **geoNetwork**: This section contains information about the geography of the user. **It includes 11 variables, 7 of which are useful. **\n- **totals**: This section contains aggregate values across the session. **It includes 6 variables, 5 of which are useful**. **Specially, 'totals.transactionRevenue' is the target for modeling**\n- **trafficSource**: This section contains information about the Traffic Source from which the session originated. **It includes 14 variables, 12 of which are useful. **","c60b8842":"**Date**","971623bc":"###  <a id='4-3'>4.3 Feature importance<\/a>","97ca8c08":"#### The distribution of 'Target'","82dd5222":"###  <a id='3-1'>3.1 Y label<\/a>\n#### The distribution of 'transactionRevenue'","aeee048f":"**The fields with constant value**"}}