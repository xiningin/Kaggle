{"cell_type":{"3b3bb6a7":"code","61efe2e8":"code","0b4df35f":"code","db09b1be":"code","c22c06e6":"code","f5be92a3":"code","91255119":"code","26cd6833":"code","bd1dc5ef":"code","d3d112ea":"code","4739662d":"code","1a76d0ca":"code","5c22c634":"code","9d4e98dd":"code","6f907705":"code","dad044bd":"code","2b0ac8e1":"code","24607592":"markdown","a97b22b4":"markdown","984e40b5":"markdown"},"source":{"3b3bb6a7":"!pip install mnist \nimport numpy as np\nimport mnist\nfrom tensorflow import keras","61efe2e8":"# The first time you run this might be a bit slow, since the\n# mnist package has to download and cache the data.\ntrain_images = mnist.train_images()\ntrain_labels = mnist.train_labels()\n\nprint(train_images.shape) # (60000, 28, 28)\nprint(train_labels.shape) # (60000,)","0b4df35f":"import numpy as np\nimport mnist\n\ntrain_images = mnist.train_images()\ntrain_labels = mnist.train_labels()\ntest_images = mnist.test_images()\ntest_labels = mnist.test_labels()\n\n# Normalize the images.\ntrain_images = (train_images \/ 255) - 0.5\ntest_images = (test_images \/ 255) - 0.5\n\n# Reshape the images.\ntrain_images = np.expand_dims(train_images, axis=3)\ntest_images = np.expand_dims(test_images, axis=3)\n\nprint(train_images.shape) # (60000, 28, 28, 1)\nprint(test_images.shape)  # (10000, 28, 28, 1)","db09b1be":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\nfrom tensorflow.keras.models import Sequential\nnum_filters = 8\nfilter_size = 3\npool_size = 2\n\nmodel = Sequential([\n  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n  MaxPooling2D(pool_size=pool_size),\n  Flatten(),\n  Dense(10, activation='softmax'),\n])","c22c06e6":"model.compile(\n  'adam',\n  loss='categorical_crossentropy',\n  metrics=['accuracy'],\n)","f5be92a3":"import mnist\n\ntrain_labels = mnist.train_labels()\nprint(train_labels[0]) # 5","91255119":"from tensorflow.keras.utils import to_categorical\n\nmodel.fit(\n  train_images,\n  to_categorical(train_labels),\n  epochs=3,\n  validation_data=(test_images, to_categorical(test_labels)),\n)","26cd6833":"model.save_weights('cnn.h5')","bd1dc5ef":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n\nnum_filters = 8\nfilter_size = 3\npool_size = 2\n\n# Build the model.\nmodel = Sequential([\n  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n  MaxPooling2D(pool_size=pool_size),\n  Flatten(),\n  Dense(10, activation='softmax'),\n])\n\n# Load the model's saved weights.\nmodel.load_weights('cnn.h5')","d3d112ea":"# Predict on the first 5 test images.\npredictions = model.predict(test_images[:5])\n\n# Print our model's predictions.\nprint(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n\n# Check our predictions against the ground truths.\nprint(test_labels[:5]) # [7, 2, 1, 0, 4]","4739662d":"import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom keras import backend as K\n# from keras.models import Sequential\nfrom keras.layers import Input, Dense, Dropout, Activation, ZeroPadding2D\nfrom keras.layers import Flatten, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.models import Model, load_model, model_from_json, model_from_yaml\nfrom keras.utils import to_categorical\n\nbatch_sz = 128\nn_classes = 10\nn_epoch = 12\nshapeX = (28,28,1)\nclasses = np.asarray([0,1,2,3,4,5,6,7,8,9],dtype = np.float32)","1a76d0ca":"def MNIST_Model(input_shape = (28,28,1),classes = 10):\n\tX_input = Input(input_shape)\n\n\t# zero padding probably not required since the main digit is in the centre only\n\t# X = zeroPadding2D((1,1))(X_input)\n\n\tX = Conv2D(32,(3,3),strides = (1,1), name = 'conv0')(X_input)\n\tX = BatchNormalization(axis=3,name='bn0')(X)\n\tX = Activation('relu')(X)\n\tX = Conv2D(32,(3,3),strides = (1,1), name = 'conv1')(X)\n\tX = BatchNormalization(axis=3,name='bn1')(X)\n\tX = Activation('relu')(X)\n\tX = MaxPooling2D((2,2),strides = (2,2),name = 'MP1')(X)\n\n\tX = Conv2D(64,(3,3),strides = (1,1), name = 'conv2')(X)\n\tX = BatchNormalization(axis=3,name='bn2')(X)\n\tX = Activation('relu')(X)\n\tX = Conv2D(64,(3,3),strides = (1,1), name = 'conv3')(X)\n\tX = BatchNormalization(axis=3,name='bn3')(X)\n\tX = Activation('relu')(X)\n\tX = MaxPooling2D((2,2),strides = (2,2),name = 'MP2')(X)\n\t\n\tX = Dropout(0.2)(X)\n\tX = Flatten()(X)\n\tX = Dense(256,activation = 'relu',name= 'fc1')(X)\n\tX = Dropout(0.4)(X)\n\tX = Dense(n_classes,activation = 'softmax',name = 'fco')(X)\n\n\tmodel = Model(inputs = X_input,outputs = X, name = 'MNIST_Model')\n\treturn model\n","5c22c634":"\nmodelMNIST = MNIST_Model(shapeX,n_classes)\nprint (modelMNIST.summary())\n\nmodelMNIST.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n\nmodelMNIST.fit(train_images, to_categorical(train_labels), epochs = n_epoch, batch_size = batch_sz)\n\npred = modelMNIST.evaluate(test_images,to_categorical(test_labels))\n\n\n","9d4e98dd":"print (\"Loss = \" + str(pred[0]))\nprint (\"Test Accuracy = \" + str(pred[1]))","6f907705":"\nmodelMNIST.save_weights('cnn_1.h5')\n","dad044bd":"# Predict on the first 5 test images.\npredictions = modelMNIST.predict(test_images[:5])\n\n# Print our model's predictions.\nprint(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n\n# Check our predictions against the ground truths.\nprint(test_labels[:5]) # [7, 2, 1, 0, 4]","2b0ac8e1":"# Predict on the first 5 test images.\npredictions = modelMNIST.predict(test_images[:7])\n\n# Print our model's predictions.\nprint(np.argmax(predictions, axis=1)) # [7 2 1 0 4 1 4]\n\n# Check our predictions against the ground truths.\nprint(test_labels[:7]) # [7 2 1 0 4 1 4]","24607592":"1- normalize the image pixel values from [0, 255] to [-0.5, 0.5] \nto make our network easier to train (using smaller, centered values usually leads to better results) \n2-reshape each image from (28, 28) to (28, 28, 1) because Keras requires the third dimension.","a97b22b4":"The Sequential constructor takes an array of Keras Layers. We\u2019ll use 3 types of layers for our CNN: Convolutional, Max Pooling, and Softmax.\n\n* num_filters, filter_size, and pool_size are self-explanatory variables that set the hyperparameters for our CNN.\n* The first layer in any Sequential model must specify the input_shape, so we do so on Conv2D. Once this input shape is specified, Keras will automatically infer the shapes of inputs for later layers.\n* The output Softmax layer has 10 nodes, one for each class.","984e40b5":"Another code "}}