{"cell_type":{"d4630775":"code","1ba63b3d":"code","2a19e412":"code","c5af4faf":"code","584f1e5e":"code","fa00fc6a":"code","2f90ff68":"code","101bdb05":"code","fa2b7d56":"code","ec777f24":"code","e09cf564":"code","2089414d":"code","e442376d":"code","047a26a4":"code","fd38bee3":"code","70e5f677":"code","e6669a34":"code","04f1082d":"code","b937a2ff":"code","5250adab":"code","6680b9aa":"code","bc2194aa":"code","703d6e8f":"code","168f3716":"code","56b3e0fa":"code","0b20c7f3":"code","78d232ae":"code","b200be81":"code","42509b88":"code","e0972299":"code","3a524c5a":"code","dc7c3e93":"code","53346c97":"code","8661d459":"code","687b0260":"code","4c28cf65":"code","01356d7a":"code","a8b193a8":"code","d2d7baf2":"code","334dd9d0":"code","f5477b2d":"code","4698ac0d":"code","04d7855b":"code","2c1e8dad":"code","36120af5":"code","3f7138c5":"code","09b75fd6":"code","51023986":"code","9e7c2036":"code","f5f154af":"code","fd4209df":"code","5e32ff5a":"code","824ca72d":"code","64aed3f9":"code","3d10050b":"code","943d42c1":"code","0af83d49":"code","0426b348":"code","4f1e52f0":"code","6c7ac500":"code","3c176607":"code","462775ae":"code","4e9cd9ec":"code","4d23cfb3":"code","406412c5":"code","25155713":"code","44005adc":"code","d0464f9f":"code","d1d91024":"code","beea536a":"code","6374f12c":"code","b6bd080c":"code","00a9d66c":"code","7057cd6e":"code","117c0849":"markdown","e63c79f3":"markdown","69adada8":"markdown","f640d7d7":"markdown","ee11b0d4":"markdown","a5d083f7":"markdown","94caa03b":"markdown","6a4beb58":"markdown","cfe7872b":"markdown","7532ca80":"markdown","13259f70":"markdown","817804d1":"markdown","8fae16a7":"markdown","6519e656":"markdown","7dbee9f1":"markdown","2cca10d6":"markdown","f21e138c":"markdown","3df1baf7":"markdown","8e92357e":"markdown","cbef4a5b":"markdown","c354bb23":"markdown","3fd91c10":"markdown","6989dd86":"markdown","0dd8e11c":"markdown","66b9376b":"markdown","e107cb57":"markdown","bbacd836":"markdown","3aa70162":"markdown","070404f2":"markdown","393f646d":"markdown","83725fba":"markdown","fef765c7":"markdown","9ed54de2":"markdown"},"source":{"d4630775":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ba63b3d":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\nfrom wordcloud import WordCloud, STOPWORDS","2a19e412":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import f1_score, log_loss\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\nimport nltk","c5af4faf":"path = '\/kaggle\/input\/nlp-getting-started'","584f1e5e":"train_data = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_data = pd.read_csv(os.path.join(path,'test.csv'))\nsubmission_data = pd.read_csv(os.path.join(path, 'sample_submission.csv'))","fa00fc6a":"train_data.head()","2f90ff68":"test_data.head()","101bdb05":"submission_data.head()","fa2b7d56":"train_data.isnull().sum()","ec777f24":"train_data.shape","e09cf564":"test_data.isnull().sum()","2089414d":"test_data.shape","e442376d":"del train_data['location']\ndel test_data['location']","047a26a4":"train_data['target'].value_counts()","fd38bee3":"sns.distplot(train_data['target'].value_counts().values, color='y')","70e5f677":"train_data.head()","e6669a34":"train_data.shape","04f1082d":"train_text_lens = [len(text) for text in train_data['text'].values]\ntest_text_lens = [len(text) for text in test_data['text'].values]\ntrain_keyword_lens = [len(text) for text in train_data[~train_data.keyword.isnull()]['keyword'].values]\ntest_keyword_lens = [len(text) for text in test_data[~test_data.keyword.isnull()]['keyword'].values]\n\ntrain_text_no_word_lens = [len(text.split(' ')) for text in train_data['text'].values]\ntest_text_no_word_lens = [len(text.split(' ')) for text in test_data['text'].values]\ntrain_keyword_no_word_lens = [len(text.split(' ')) for text in train_data[~train_data.keyword.isnull()]['keyword'].values]\ntest_keyword_no_word_lens = [len(text.split(' ')) for text in test_data[~test_data.keyword.isnull()]['keyword'].values]","b937a2ff":"sns.distplot(train_text_lens)\nplt.show()\nsns.boxplot(train_text_lens)","5250adab":"sns.distplot(train_keyword_lens)\nplt.show()\nsns.boxplot(train_keyword_lens)","6680b9aa":"sns.distplot(train_text_no_word_lens)\nplt.show()\nsns.boxplot(train_text_no_word_lens)","bc2194aa":"sns.distplot(train_keyword_no_word_lens)\nplt.show()\nsns.boxplot(train_keyword_no_word_lens)","703d6e8f":"sns.distplot(test_text_lens,color='y')\nplt.show()\nsns.boxplot(test_text_lens, color='y')","168f3716":"sns.distplot(test_keyword_lens)\nplt.show()\nsns.boxplot(test_keyword_lens)","56b3e0fa":"sns.distplot(test_text_no_word_lens)\nplt.show()\nsns.boxplot(test_text_no_word_lens)","0b20c7f3":"sns.distplot(test_keyword_no_word_lens)\nplt.show()\nsns.boxplot(test_keyword_no_word_lens)","78d232ae":"contains_keyword = [len(set(a.split(' ')).intersection(b.split(' '))) for a, b in zip(train_data[~train_data.keyword.isnull()].keyword, train_data[~train_data.keyword.isnull()].text)]","b200be81":"collections.Counter(contains_keyword)","42509b88":"target = train_data[~train_data.keyword.isnull()].target.values","e0972299":"collections.Counter(target)","3a524c5a":"collections.Counter(target & contains_keyword)","dc7c3e93":"corr = np.corrcoef(target, contains_keyword)","53346c97":"sns.heatmap(corr)","8661d459":"del train_data['keyword']\ndel test_data['keyword']","687b0260":"train_text_no_word_unique_lens = [len(set(text.split(' '))) for text in train_data['text'].values]\ntest_text_no_word_unique_lens = [len(set(text.split(' '))) for text in test_data['text'].values]","4c28cf65":"sns.distplot(train_text_no_word_unique_lens)\nplt.show()\nsns.boxplot(test_text_no_word_unique_lens)","01356d7a":"all_text_train = ' '.join(train_data['text'].values)\nall_text_test = ' '.join(test_data['text'].values)","a8b193a8":"stopwords = set(STOPWORDS)","d2d7baf2":"wordcloud_train = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(all_text_train)\n\nwordcloud_test = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(all_text_test)","334dd9d0":"plt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud_train)\nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('Train Word Cloud')\nplt.show()\nplt.imshow(wordcloud_test)\nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('Test Word Cloud')\nplt.show()","f5477b2d":"stop_words = nltk.corpus.stopwords.words('english')\nlemmitizer = nltk.stem.WordNetLemmatizer()","4698ac0d":"def initial_preprocessing(text):\n    word_list = []\n    for w in text.split(' '):\n        wl = lemmitizer.lemmatize(w)\n        if w not in stop_words:\n            word_list.append(wl)\n    text = ' '.join(word_list)\n    \n    return text","04d7855b":"train_data.text = train_data.text.apply(initial_preprocessing)\ntest_data.text = test_data.text.apply(initial_preprocessing)","2c1e8dad":"xtrain, xvalid, ytrain, yvalid = train_test_split(train_data.text.values, train_data.target,\n                                                 stratify=train_data.target, \n                                                 random_state=42, test_size=0.2, shuffle=True)","36120af5":"xcvalid, xtest, ycvalid, ytest = train_test_split(xvalid, yvalid,\n                                                 stratify=yvalid, \n                                                 random_state=42, test_size=0.2, shuffle=True)","3f7138c5":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\nctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n","09b75fd6":"tfv.fit(list(xtrain)+list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxcvalid_tfv = tfv.transform(xcvalid)\nxtest_tfv = tfv.transform(xtest)","51023986":"ctv.fit(list(xtrain)+list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxcvalid_ctv = ctv.transform(xcvalid)\nxtest_ctv = ctv.transform(xtest)","9e7c2036":"xtrain_tfv.shape, xtrain_ctv.shape","f5f154af":"svd = TruncatedSVD(n_components=2500)\nsvd.fit(xtrain_tfv)","fd4209df":"svd.explained_variance_ratio_.sum()","5e32ff5a":"xtrain_tfv_svd = svd.transform(xtrain_tfv)\nxcvalid_tfv_svd = svd.transform(xcvalid_tfv)\nxtest_tfv_svd = svd.transform(xtest_tfv)","824ca72d":"stdscl = StandardScaler()\nstdscl_nmean = StandardScaler(with_mean=False)\nmmscl = MaxAbsScaler()","64aed3f9":"features = {}\nscalers = {}\nfeatures['tfv'] = [xtrain_tfv, xcvalid_tfv, xtest_tfv]\nfeatures['ctv'] = [xtrain_ctv, xcvalid_ctv, xtest_ctv]\nfeatures['tfv_svd'] = [xtrain_tfv_svd, xcvalid_tfv_svd, xtest_tfv_svd]\n\n# scalers['std'] = stdscl\nscalers['std_nmean'] = stdscl_nmean\nscalers['mm'] = mmscl","3d10050b":"models = {}\n\n# Logistic Regressions\nmodels['lr'] = {\"clf\" : LogisticRegression(C=1.0), \"scale\": False}\n\n# Logistic Regression with regularization\nmodels['lr_reg'] = {\"clf\" : LogisticRegression(C=0.7), \"scale\": True}\n\n# SVM with RBF\nmodels['svm_rbf'] = {\"clf\" : SVC(C=0.7, kernel='rbf'), \"scale\": True}\n\n# SVM with Polynomial\nmodels['svm_poly'] = {\"clf\" : SVC(C=0.7, kernel='poly'), \"scale\": True}\n\n# SVM with Sigmoid\nmodels['svm_sigmoid'] = {\"clf\" : SVC(C=0.7, kernel='sigmoid'), \"scale\": True}\n\n# SVM with Sigmoid\nmodels['multi_nomial'] = {\"clf\" : MultinomialNB(), \"scale\": False, \"non_negative\": True}\n\n# RandomForest with gini\nmodels['rforest_gini'] = {\"clf\" : RandomForestClassifier(criterion='gini', n_estimators=200), \"scale\": False}\n\n# RandomForest with entropy\nmodels['rforest_entropy'] = {\"clf\" : RandomForestClassifier(criterion='entropy', n_estimators=200), \"scale\": False}\n\n# Gradient boosting\nmodels['gradient_boost'] = {\"clf\" : GradientBoostingClassifier(n_estimators=200), \"scale\": False}","943d42c1":"class Trainer:\n    def __init__(self, models, features, **params):\n        self.models = models\n        self.features = features\n        self.scalers = params.get('scalers')\n        self.pipelines = {}\n        self.training_results = {}\n    \n    def createPipelines(self):\n        for model_name, model in self.models.items():\n            model_component = (model_name, model['clf'])\n            if model.get('non_negative', False) == False:\n                for scaler_name, scaler in self.scalers.items():\n                    scaler_component = (scaler_name, scaler)\n                    self.pipelines[model_name+' '+scaler_name] = [scaler_component, model_component]\n            if model['scale'] == False:\n                self.pipelines[model_name] = [(model_name, model['clf'])]\n        \n        return None\n                \n    def train(self):\n        self.createPipelines()\n        for pipeline_name, pipeline in self.pipelines.items():\n            print(\"Started training for Pipeline: \", pipeline_name)\n            for feature_name, feature in self.features.items():\n                if 'nomial' in pipeline_name and 'svd' in feature_name:\n                    continue\n                print(\"With %s features\"%(feature_name))\n                pipeline_clf = Pipeline(pipeline)\n                pipeline_clf.fit(feature[0], ytrain)\n                prediction_cvalid = pipeline_clf.predict(feature[1])\n                prediction_test = pipeline_clf.predict(feature[2])\n                f1score_cvalid = f1_score(prediction_cvalid, ycvalid)\n                f1score_test = f1_score(prediction_test, ytest)\n                print('F1score on cvalid and test set are: %f, %f'%(f1score_cvalid,f1score_test))\n                self.training_results[pipeline_name+'\/'+feature_name] = {'cvalid':f1score_cvalid,\n                                                                         'test':f1score_test\n                                                                        }\n        \n        return None\n        ","0af83d49":"trainer_obj = Trainer(models = models, features=features, scalers=scalers)","0426b348":"trainer_obj.train()","4f1e52f0":"training_results = trainer_obj.training_results","6c7ac500":"result_df = pd.DataFrame(training_results).T","3c176607":"result_df['Models'] = result_df.index","462775ae":"result_df","4e9cd9ec":"result_df = result_df.set_index('Models')","4d23cfb3":"stacked_df = result_df.stack().reset_index()","406412c5":"stacked_df.columns = ['Models', 'Set', 'F1Score']","25155713":"stacked_df","44005adc":"sns.barplot(y='Models', x='F1Score', hue='Set', data=stacked_df)","d0464f9f":"clf = GradientBoostingClassifier(n_estimators=200)\nclf.fit(xtrain_tfv, ytrain)","d1d91024":"prediction_cvalid = clf.predict(xcvalid_tfv)\nprediction_test = clf.predict(xtest_tfv)\nf1score_cvalid = f1_score(prediction_cvalid, ycvalid)\nf1score_test = f1_score(prediction_test, ytest)\nprint('F1score on cvalid and test set are: %f, %f'%(f1score_cvalid,f1score_test))","beea536a":"test_tfv = tfv.transform(test_data.text)","6374f12c":"predictions = clf.predict(test_tfv)","b6bd080c":"submission_data['id'] = test_data['id']\nsubmission_data['target'] = predictions","00a9d66c":"submission_data.head()","7057cd6e":"submission_data.to_csv('output_submissions.csv', index=False)","117c0849":"## Splitting the training dataset into train, cross-validation and test set","e63c79f3":"### Train the best classifier","69adada8":"### Lets start with Tf-Idf vectors","f640d7d7":"### Lets create all our models","ee11b0d4":"### Lets understand the distribution of the length of the posts","a5d083f7":"There are numerous no of algo in the ML literature those can be used for dimensionality reduction. \nBut we also need to consider the internal algorithm in them while chosing one because if the some internal step \n\nmight lead to unnecessary distortion in the core features important for the domain then that is an issue.\n\nAs our problem is text based so the preferred algorithm is TruncatedSVD.\n\nWhen truncated SVD is applied to term-document matrices (as returned by CountVectorizer or TfidfVectorizer), this transformation is known as latent semantic analysis (LSA). Refer to this link for better info: [Link](https:\/\/scikit-learn.org\/stable\/modules\/decomposition.html#truncated-singular-value-decomposition-and-latent-semantic-analysis)\n\n","94caa03b":"**Lemmitization and stopwords removal**","6a4beb58":"## We will start with some Exploratory Analytics","cfe7872b":"### Create the predictions","7532ca80":"### Findings\nThe train set is little more skewed than the test set. But overall they both follow the same distribution for the length of the text and no of words in the text\n\nFor the keywords both have the same distribution with some outliers. Also we found out that the length of the keywords is always 1. It is always a single word\n\n","13259f70":"### Findings\nHere we can observe that the number of features are huge.\n\nThe more the number of features the tougher is for the models to learn\n\nSo lets also consider some featres reduction algorthtim","817804d1":"### Lets check for Null Values","8fae16a7":"### Lets check the distribution of the labels","6519e656":"### Additional Preprocessing to remove words those dont not contribute much to the context","7dbee9f1":"### Lets visualize the accuracies","2cca10d6":"### Findings\nThe data looks to be balanced","f21e138c":"## Lets load the data","3df1baf7":"The word cloud shows that the occurence of different words is same in train and test set.\n\nAlso there seems to be some words those occur commonly in the text","8e92357e":"## Feature Engineering\nNow as we have explored the data in all terms we can proceed with the creation of the of the features","cbef4a5b":"## Import all the necessary Libraries","c354bb23":"### How to decide the no of components for SVD.\n\nThere is actually no fixed method to find an optimal value. \n\nOne way is to pick any value randomly and check how much variable these features are able to explain.\n\nGenerally if the features are able to explain more than 90% of the variance then they are considered ok.","3fd91c10":"### LSA","6989dd86":"### Fitting TF-IDF","0dd8e11c":"### Findings \nAs we can see that the distribution of the target and no of rows with text containing the keyword is the same\n\nBut when we check their dependency on each other it is negligible\n\nSo we can remove it as well","66b9376b":"### **Finding:** \nAs 1\/3 rd of the rows have the **location** column empty. So we can remove it","e107cb57":"### Lets define 2 types of scalers","bbacd836":"### Feature Scaling","3aa70162":"#### As Count vectors and Hash vectors have humongus dimensions, so reducing their dimension to the appropriate 90% explained variance will require more RAM.","070404f2":"### Lets check how many of the text actually contains the keyword","393f646d":"So here the data is text which needs to be converted to vectors.\n\nThere are multiple types of methods which create vectors based on the words present in the text\n\nBefore doing this we might need to know are there any particular words those occur in each text\n\nThis will also help us in judging the dependency of the target in some words","83725fba":"### Fitting Count vector","fef765c7":"Another very important step of feature engineering is feature scaling.\n\nIn a dataset consisting of multiple features, each feature might follow a different distribution with different extremes.\nFor a model all features are important and it will use all of them for its prediction.\n\nIn machine learning if the algorithm uses any distance metric to classifiy the given training example, \nthen any feature with higher range of variation will contribute more to the final prediction. Which is incorrect \n\nbecause range in itself if not an attribute which should dominate.\n\nSo its is always safe to bring all the features in the range of 0-1.\n\nFor some model it is even mandatory to bring the range in between 0-1 like SVM, Logistic regression(with C != 1), etc \n\nwhile there are some which dont require feature scaling like DecisionTree, RandomForest, etx","9ed54de2":"#### Lets see the correlation between them"}}