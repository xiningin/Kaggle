{"cell_type":{"0d006d84":"code","6f3956d7":"code","c91c3b60":"code","0e912018":"code","c5062d29":"code","de059155":"code","8c89701c":"code","05b7e981":"code","31bc8e67":"code","82a68172":"code","d0af2a7b":"code","8d226c95":"code","61ea6191":"code","200827f6":"code","f352b8a6":"code","7e3e1185":"code","783ae840":"code","98d51ed5":"code","6d1c146d":"code","3b80424b":"code","2b3e488f":"code","d1019cbc":"code","1792fed1":"code","bad7b340":"code","9a527629":"code","d5255df2":"code","2a551d2d":"code","a4d88f50":"code","4fdaa190":"code","a230f965":"code","c424f415":"code","75a6529d":"code","9faca321":"code","4c1e806a":"code","ac1aa0a4":"code","71892ba2":"code","dfc23057":"code","2de90b4c":"code","8b117042":"code","11cbc67c":"code","a61ed6ff":"code","41b29ad1":"code","8be309f8":"code","f7a2bb1d":"code","e3040e37":"code","f138901b":"code","01389d48":"markdown","bd35d4c6":"markdown","a1bc0c6d":"markdown","08785cb8":"markdown","566d58b3":"markdown","8020da55":"markdown","0be90758":"markdown","d18daf4e":"markdown"},"source":{"0d006d84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f3956d7":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport random\nimport tqdm\nimport seaborn as sns\nfrom keras.utils.vis_utils import plot_model\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import LSTM\nfrom keras.layers import TimeDistributed\nfrom fbprophet import Prophet ","c91c3b60":"chicago_df_1 = pd.read_csv('\/kaggle\/input\/crimes-in-chicago\/Chicago_Crimes_2001_to_2004.csv', error_bad_lines=False)\nchicago_df_2 = pd.read_csv('\/kaggle\/input\/crimes-in-chicago\/Chicago_Crimes_2005_to_2007.csv', error_bad_lines=False)\nchicago_df_3 = pd.read_csv('\/kaggle\/input\/crimes-in-chicago\/Chicago_Crimes_2008_to_2011.csv', error_bad_lines=False)\nchicago_df_4 = pd.read_csv('\/kaggle\/input\/crimes-in-chicago\/Chicago_Crimes_2012_to_2017.csv', error_bad_lines=False)","0e912018":"df = pd.concat([chicago_df_1, chicago_df_2, chicago_df_3, chicago_df_4], ignore_index=False, axis=0)\ndf.drop(['Unnamed: 0', 'Case Number', 'Case Number', 'IUCR', 'X Coordinate', 'Y Coordinate','Updated On','Year', 'FBI Code', 'Beat','Ward','Community Area', 'Location', 'District', 'Latitude' , 'Longitude'], inplace=True, axis=1)","c5062d29":"df.index = pd.DatetimeIndex(df.Date)\ndata = df.resample('M').size().reset_index()\ndata","de059155":"data[\"Date\"] = data[\"Date\"].dt.strftime(\"%m\/%d\/%Y\")","8c89701c":"data.columns = ['Date', 'Crime Count']\ndata.dtypes","05b7e981":"data","31bc8e67":"ip = np.asarray(data['Crime Count'].values)\nip = np.asarray([[i] for i in ip])\nip","82a68172":"def train_test_splitting(data, n_test):\n    return data[:-n_test], data[-n_test:]\n\n# transform list into supervised learning format\ndef series_to_supervised(data, n_in=1, n_out=1):\n    df = pd.DataFrame(data)\n    cols = list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n    # concatenate together\n    dframe = pd.concat(cols, axis=1)\n    # drop rows with NaN values\n    dframe.dropna(inplace=True)\n    return dframe.values\n \n# root mean squared error or rmse\ndef measure_error(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted)), r2_score(actual, predicted)\n\n# walk-forward validation for univariate data\ndef walk_forward_validation(data, n_test, cfg):\n    predictions = list()\n    # split dataset\n    train, test = train_test_splitting(data, n_test)\n    # fit model\n    model = model_fit(train, cfg)\n    \n    # seed history with training dataset\n    history = [x for x in train]\n    # step over each time-step in the test set\n    for i in range(len(test)):\n        # fit model and make forecast for history\n        yhat = model_predict(model, history, cfg)\n        # store forecast in list of predictions\n        predictions.append(yhat)\n        # add actual observation to history for the next loop\n        history.append(test[i])\n    # estimate prediction error\n    rmse, r2_score = measure_error(test, predictions)\n    print(' RMSE: %.3f \\t R2 score: %.3f' % (rmse, r2_score))\n    return rmse, r2_score , model\n \n# repeat evaluation of a config\ndef repeat_evaluate(data, config, n_test, n_repeats=30):\n    # fit and evaluate the model n times\n    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n    return scores\n \n# summarize model performance\ndef summarize_scores(name, scores):\n    # print a summary\n    rmse_scores = [i[0] for i in scores]\n    r2_scores = [i[1] for i in scores]\n    rmse_mean, rmse_std = np.mean(rmse_scores), np.std(rmse_scores)\n    r2_mean, r2_std = np.mean(r2_scores), np.std(r2_scores)\n    print('%s' % name)\n    print('RMSE: %.3f (+\/- %.3f)' % (rmse_mean, rmse_std))\n    print('R2: %.3f (+\/- %.3f)' % (r2_mean, r2_std))\n    # box and whisker plot\n    plt.boxplot(rmse_scores)\n    plt.show()\n    plt.boxplot(r2_scores)\n    plt.show()","d0af2a7b":"model = Sequential()\n# fit a model\ndef model_fit(train, config):\n    # unpack config\n    n_input, n_nodes, n_epochs, n_batch = config\n    # prepare data\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    # define model\n    mlp_model = Sequential()\n    mlp_model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n    mlp_model.add(Dense(3))\n    mlp_model.add(Dense(1))\n    mlp_model.compile(loss='mse', optimizer='adam')\n        \n    # fit\n    mlp_model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return mlp_model\n \n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_input, _, _, _ = config\n    # prepare data\n    x_input = np.array(history[-n_input:]).reshape(1, n_input)\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]\n","8d226c95":"# define config\nconfig = [24, 250, 100, 100]\nn_test = 12\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\nmlp_model = scores[0][2]\n# summarize scores\nsummarize_scores('mlp', scores)","61ea6191":"plot_model(mlp_model, to_file='mlp_model.png', show_shapes=True, show_layer_names=True)","200827f6":"def model_fit(train, config):\n    # unpack config\n    n_input, n_filters, n_kernel, n_epochs, n_batch = config\n    # prepare data\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_input, 1)))\n    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model\n\n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_input, _, _, _, _ = config\n    # prepare data\n    x_input = np.array(history[-n_input:]).reshape((1, n_input, 1))\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]","f352b8a6":"n_test = 12\n# define config\nconfig = [24, 256, 3, 100, 100]\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\ncnn_model = scores[0][2]\n# summarize scores\nsummarize_scores('cnn', scores)","7e3e1185":"plot_model(cnn_model, to_file='cnn_model.png', show_shapes=True, show_layer_names=True)","783ae840":"# difference dataset\ndef difference(data, interval):\n    return [data[i] - data[i - interval] for i in range(interval, len(data))]\n \n# fit a model\ndef model_fit(train, config):\n    # unpack config\n    n_input, n_nodes, n_epochs, n_batch, n_diff = config\n    # prepare data\n    if n_diff > 0:\n        train = difference(train, n_diff)\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(n_nodes, activation='relu', input_shape=(n_input, 1)))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    \n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model\n \n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_input, _, _, _, n_diff = config\n    # prepare data\n    correction = 0.0\n    if n_diff > 0:\n        correction = history[-n_diff]\n        history = difference(history, n_diff)\n    x_input = np.array(history[-n_input:]).reshape((1, n_input, 1))\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return correction + yhat[0]","98d51ed5":"n_test = 12\n# define config\nconfig = [24, 50, 100, 100, 12]\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\nrnn_model = scores[0][2]\n# summarize scores\nsummarize_scores('lstm', scores)","6d1c146d":"plot_model(rnn_model, to_file='rnn_model.png', show_shapes=True, show_layer_names=True)","3b80424b":"# fit a model\ndef model_fit(train, config):\n    # unpack config\n    n_seq, n_steps, n_filters, n_kernel, n_nodes, n_epochs, n_batch = config\n    n_input = n_seq * n_steps\n    # prepare data\n    data = series_to_supervised(train, n_in=n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], n_seq, n_steps, 1))\n    # define model\n    model = Sequential()\n    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(None,n_steps,1))))\n    model.add(TimeDistributed(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu')))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(n_nodes, activation='relu'))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(n_nodes, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    \n    # fit\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model\n \n# forecast with a pre-fit model\ndef model_predict(model, history, config):\n    # unpack config\n    n_seq, n_steps, _, _, _, _, _ = config\n    n_input = n_seq * n_steps\n    # prepare data\n    x_input = np.array(history[-n_input:]).reshape((1, n_seq, n_steps, 1))\n    # forecast\n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]","2b3e488f":"n_test = 12\n# define config\nconfig = [3, 12, 64, 3, 100, 200, 100]\n# grid search\nscores = repeat_evaluate(ip, config, n_test)\ncnn_lstm_model = scores[0][2]\n# summarize scores\nsummarize_scores('cnn-lstm', scores)","d1019cbc":"plot_model(cnn_lstm_model, to_file='cnn_lstm_model.png', show_shapes=True, show_layer_names=True)","1792fed1":"df = pd.concat([chicago_df_1, chicago_df_2, chicago_df_3], ignore_index=False, axis=0)\ntest_df = chicago_df_4","bad7b340":"df.head()","9a527629":"test_df.head()","d5255df2":"df.shape, test_df.shape","2a551d2d":"df.isna().sum()","a4d88f50":"# Dropping the following columns: ID Case Number Date Block IUCR Primary Type Description Location Description Arrest Domestic Beat District Ward Community Area FBI Code X Coordinate Y Coordinate Year Updated On Latitude Longitude Location\ndf.drop(['Unnamed: 0', 'Case Number', 'Case Number', 'IUCR', 'X Coordinate', 'Y Coordinate','Updated On','Year', 'FBI Code', 'Beat','Ward','Community Area', 'Location', 'District', 'Latitude' , 'Longitude'], inplace=True, axis=1)\ntest_df.drop(['Unnamed: 0', 'Case Number', 'Case Number', 'IUCR', 'X Coordinate', 'Y Coordinate','Updated On','Year', 'FBI Code', 'Beat','Ward','Community Area', 'Location', 'District', 'Latitude' , 'Longitude'], inplace=True, axis=1)","4fdaa190":"df.Date = pd.to_datetime(df.Date, format='%m\/%d\/%Y %I:%M:%S %p')\ntest_df.Date = pd.to_datetime(test_df.Date, format='%m\/%d\/%Y %I:%M:%S %p')","a230f965":"df.index = pd.DatetimeIndex(df.Date)\ntest_df.index = pd.DatetimeIndex(test_df.Date)","c424f415":"df['Primary Type'].value_counts()","75a6529d":"df.resample('M').size()\ntest_df.resample('M').size()","9faca321":"plt.plot(df.resample('M').size())\nplt.title('Crimes Count Per Month')\nplt.xlabel('Months')\nplt.ylabel('Number of Crimes')","4c1e806a":"prophet = df.resample('M').size().reset_index()\nprophet.columns = ['Date', 'Crime Count']\nprophet","ac1aa0a4":"prophet_df = pd.DataFrame(prophet)\nprophet_df","71892ba2":"prophet_df_final = prophet_df.rename(columns={'Date':'ds', 'Crime Count':'y'})","dfc23057":"prop = Prophet()\nprop.fit(prophet_df_final)","2de90b4c":"future = prop.make_future_dataframe(periods=1858)  #periods = no. of days for prediction\nforecast = prop.predict(future)","8b117042":"forecast","11cbc67c":"preds_df = forecast[132:]\npreds_df","a61ed6ff":"preds_df.Date = pd.to_datetime(preds_df.ds, format='%m\/%d\/%Y %I:%M:%S %p')\npreds_df.index = pd.DatetimeIndex(preds_df.Date)","41b29ad1":"preds = preds_df.yhat.resample('M').sum()\/100","8be309f8":"targets = test_df.resample('M').size()\nreqd = np.asarray(targets.values)\nans = np.asarray(preds.values)","f7a2bb1d":"print('RMSE: %.3f' % sqrt(mean_squared_error(reqd, ans)))\nprint('R2 score: %.3f' % r2_score(reqd, ans))","e3040e37":"figure = prop.plot(forecast, xlabel='Date', ylabel='Crime Rate')","f138901b":"figure = prop.plot_components(forecast)","01389d48":"## Multi Layer Perceptron model","bd35d4c6":"## Recurrent Neural Network Model with LSTM","a1bc0c6d":"## Model","08785cb8":"# Prophet","566d58b3":"## CNN with LSTM","8020da55":"# Neural Network Models","0be90758":"## Convolutional Neural Network Model","d18daf4e":"# **Reading Data**"}}