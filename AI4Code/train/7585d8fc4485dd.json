{"cell_type":{"31bd4ce1":"code","0f766cca":"code","8de4114d":"code","e3e24219":"code","d6a91646":"code","9622c938":"code","3b46f08d":"code","11943b0f":"code","4d7ba824":"code","c409d698":"code","0890095d":"code","4afbd7d1":"code","72ea735b":"code","3668863c":"code","5b2c45fb":"code","fcb075e3":"code","c4a9b40b":"code","ae961464":"code","759cf1d3":"code","f265e68d":"code","3d373678":"code","d9da9c9b":"code","ababbf56":"code","10272965":"code","37fd23a2":"code","a78074d7":"code","0d7e0682":"code","a1ac9965":"code","b340073f":"code","c1b3e9d7":"code","71358b5a":"code","fb694c54":"code","c326e5ff":"code","4bd63b8e":"code","8c491596":"code","f9a5ce3f":"code","eedad121":"code","5344e6f1":"code","176295b0":"code","969bef79":"code","5c68fa56":"code","2c213175":"code","ed648ab0":"code","0dc77d0a":"code","69e385a6":"code","bf52ef9c":"code","c2453895":"code","d421122e":"code","06d30c59":"code","161b8109":"code","65e2df96":"code","3eeb47f5":"code","7297b1bf":"code","2b632541":"code","ff3ea624":"markdown","f686eb29":"markdown","75aeb14e":"markdown","756fffa3":"markdown","3b9ede09":"markdown","85111f62":"markdown","ed2d91d2":"markdown","e9f01934":"markdown","923ee908":"markdown","5f34d63c":"markdown","103ded0e":"markdown","e592c254":"markdown"},"source":{"31bd4ce1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE,ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import Counter\nimport itertools\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","0f766cca":"###Carregando a base e verificando a forma da base. \ndf = pd.read_csv ('\/kaggle\/input\/hmeq-data\/hmeq.csv')\n##Verificando as dimens\u00f5es da base\ndf.shape\n###Observa-se que a base possui 5960 observa\u00e7\u00f5es e 13 vari\u00e1veis.","8de4114d":"###Verificando os nomes das colunas\ndf.columns","e3e24219":"###Verificando as vari\u00e1veis na forma transposta\ndf.head().T\n###\u00c9 possivel observar as 13 vari\u00e1veis e as 4 primeiras observa\u00e7\u00f5es. \n##E j\u00e1 se observa a presen\u00e7a de NAN","d6a91646":"###Verificando de forma aleat\u00f3ria para verificar outros detalhes\ndf.sample(5).T","9622c938":"###Coletando informa\u00e7\u00f5es dos dados\ndf.info()","3b46f08d":"# columns of dataset\ndf.columns","11943b0f":"####Nesses c\u00f3digos abaixo \u00e9 poss\u00edvel observar os valores de alguns casos \ndef rstr(df): return df.shape, df.apply(lambda x: [x.unique()])\nprint(rstr(df))","4d7ba824":"###VERIFICANDO SE AS VARI\u00c1VEIS DUMMIES FORAM CRIADAS\ndf.shape\n# Observa-se um aumento no m\u00famero de vari\u00e1veis de 13 para 19.","c409d698":"##Agora verifiacando as vari\u00e1veis dummies na base\ndf.sample(5).T","0890095d":"####AGORA, VAMOS VERIFICAR A ESTAT\u00cdSTICA DESCRITIVA DAS VARI\u00c1VEIS \nprint(df.describe().T)\n## vale ressaltar que as vari\u00e1veis qualitativas est\u00e3o por padr\u00e3o nessa an\u00e1lise.\n#ent\u00e3o a m\u00e9dia e desvios-padr\u00e3o n\u00e3o s\u00e3o adequados.","4afbd7d1":"###############talvez retirar\n###Verificando a estat\u00edstica descritiva das vari\u00e1veis qualitativas\n# categorical features\ncategorical_cols = [cname for cname in df.columns if\n                    df[cname].dtype in ['object']]\ncat = df[categorical_cols]\ncat.columns","72ea735b":"##Verificando se existem MISSING VALUES. Foi tamb\u00e9m calculado o percentual de missing cases\n\nfeat_missing = []\n\nfor f in df.columns:\n    missings = df[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings\/df.shape[0]\n        \n###Verificando o percentual de missing\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n    \n###Verificando quantas vari\u00e1veis apresentam os casos faltosos\n\nprint()\nprint('In total, there are {} variables with missing values'.format(len(feat_missing)))","3668863c":"#dropping rows that have missing data\ndf.dropna(axis=0, how='any', inplace=True)\ndf.info()","5b2c45fb":"##Verificando se existem MISSING VALUES. Foi tamb\u00e9m calculado o percentual de missing cases\n###Confirmando se restou valor faltoso\nfeat_missing = []\n\nfor f in df.columns:\n    missings = df[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings\/df.shape[0]\n        \n###Verificando o percentual de missing\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n    \n###Verificando quantas vari\u00e1veis apresentam os casos faltosos\n\nprint()\nprint('In total, there are {} variables with missing values'.format(len(feat_missing)))","fcb075e3":"###Criando uma nova coluna para iniciar a previs\u00e3o de forma mais acurada\ndf['VALUE_MORTDUE'] = df['VALUE'] - df['MORTDUE']","c4a9b40b":"df.info()","ae961464":"###Agora verificando novamente a estat\u00edstica descritiva das vari\u00e1veis quantitativas\nprint(df.describe().T)","759cf1d3":"######DEFININDO A \"TARGET\" \"BAD\"\ny=df.BAD","f265e68d":"####\nimport matplotlib.pyplot as plt\nax = sns.countplot(y='BAD', data=df).set_title(\"Clientes inadimplentes ou n\u00e3o\")","3d373678":"####Verificando a estat\u00edstica das vari\u00e1veis qualitativas\ny = y.astype(object)\ncount = pd.crosstab(index = y, columns=\"count\")\npercentage = pd.crosstab(index = y, columns=\"frequency\")\/pd.crosstab(index = y, columns=\"frequency\").sum()\npd.concat([count, percentage], axis=1)","d9da9c9b":"###Verificando as outras vari\u00e1veis categ\u00f3ricas\ncategorical_cols = [cname for cname in df.columns if\n                    df[cname].dtype in ['object']]\ncat = df[categorical_cols]\ncat.columns\n\n","ababbf56":"###Iportando bibliotecas para o modelo\nimport pandas\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot","10272965":"# GR\u00e1ficos de contagem de cada categoria\nsns.set( rc = {'figure.figsize': (5, 5)})\nfcat = ['REASON','JOB']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(x=cat[col], data=cat, palette=\"Set3\")\n    plt.show()","37fd23a2":"###Criando vari\u00e1veis dummies\ndf_dum = pd.get_dummies(df)","a78074d7":"###Verificando as novas vari\u00e1veis dummies na base transposta\ndf_dum.sample(5).T","0d7e0682":"###Verificando as caracter\u00edsticas das vari\u00e1veis num\u00e9ricas\nnumerical_cols = [cname for cname in df.columns if\n                 df[cname].dtype in ['float']]\nnum = df[numerical_cols]\nnum.columns\n\n","a1ac9965":"###Analisando os histogramas das vari\u00e1veis quantitativas\n##Os histogramas abaixo demonstram distribui\u00e7\u00f5es assim\u00e9tricas.\nf, axes = plt.subplots(3,3, figsize=(20,20))\nsns.distplot( df_dum[\"LOAN\"] , color=\"skyblue\", bins=15, kde=False, ax=axes[0, 0])\nsns.distplot( df_dum[\"DEBTINC\"] , color=\"olive\", bins=15, kde=False, ax=axes[0, 1])\nsns.distplot( df_dum[\"MORTDUE\"] , color=\"orange\", bins=15, kde=False, ax=axes[0, 2])\nsns.distplot( df_dum[\"YOJ\"] , color=\"yellow\", bins=15, kde=False, ax=axes[1, 0])\nsns.distplot( df_dum[\"VALUE\"] , color=\"pink\", bins=15, kde=False, ax=axes[1, 1])\nsns.distplot( df_dum[\"CLAGE\"] , color=\"gold\", bins=15, kde=False, ax=axes[1, 2])\nsns.distplot( df_dum[\"CLNO\"] , color=\"teal\", bins=15, kde=False, ax=axes[2, 1])\nsns.distplot( df_dum['DEROG'], color=\"blue\", bins=15, kde=False, ax=axes[2, 2])\nsns.distplot( df_dum['DELINQ'], color=\"green\", bins=15, kde=False, ax=axes[2, 0])","b340073f":"#VALIDA\u00c7\u00c3O CRUZADA ESTRATIFICADA + REAMOSTRAGEM\n###Uma abordagem para lidar com conjuntos de dados desequilibrados \u00e9 sobreamostrar a classe minorit\u00e1ria.\n###A abordagem mais simples envolve a duplica\u00e7\u00e3o de exemplos na classe minorit\u00e1ria, embora esses exemplos n\u00e3o adicionem novas informa\u00e7\u00f5es ao modelo. \n###Em vez disso, novos exemplos podem ser sintetizados a partir dos exemplos existentes.\n###Esse \u00e9 um tipo de aumento de dados para a classe minorit\u00e1ria e \u00e9 chamado de T\u00e9cnica de superamostragem por minoria sint\u00e9tica, ou SMOTE, para abreviar\n##Synthetic Minority Oversampling Technique = SMOT\ny = y.astype('int') \nsmo = SMOTE(random_state=0)\nX_resampled, y_resampled = smo.fit_resample(df_dum, y)\nprint(sorted(Counter(y_resampled).items()))","c1b3e9d7":"# Dividindo o DataFrame\nfrom sklearn.model_selection import train_test_split","71358b5a":"##### Treino e teste\ntrain, test = train_test_split(df_dum, test_size=0.15, random_state=42)\n\n# Veificando o tanho dos DataFrames\ntrain.shape, test.shape","fb694c54":"#######\ndf_dum.info()","c326e5ff":"#####features\nfeats = [c for c in df_dum.columns if c not in ['BAD']]","4bd63b8e":"df_dum.head()","8c491596":"###Cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","f9a5ce3f":"####Dividindo o dataset\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_resampled, y_resampled, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","eedad121":"#####features\nfeats = [c for c in df_dum.columns if c not in ['BAD']]","5344e6f1":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","176295b0":"###Gerando modelos da linha de base\n##oP\u00c7\u00d5ES DE TESTE E M\u00c9TRICA DA AVALIA\u00c7\u00c3O\n##Algoritmos de verifica\u00e7\u00e3o\n\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression(random_state=0)))\nmodels.append(('Bagging', BaggingClassifier(random_state=0)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state=0)))\nmodels.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=0)))\nmodels.append(('XGB', XGBClassifier(random_state=0)))\nresults_t = []\nresults_v = []\nnames = []\nscore = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n   #computando a matrix de confus\u00e3o para o algoritmo acima\n    cnf_matrix = confusion_matrix(y_valid, predictions_v)\n    np.set_printoptions(precision=2)\n\n#Gr\u00e1fico da matriz de confus\u00e3o n\u00e3o-normalizado\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score.append(f_dict)\nplt.show()    \nscore = pd.DataFrame(score, columns = ['model','accuracy_train', 'accuracy_valid'])","969bef79":"###Gerando as pontua\u00e7\u00f5es\nprint(score)","5c68fa56":"###Modelos de base escalonadas\n# Algoritmos de verifica\u00e7\u00e3o pontual com conjunto de dados padronizado\npipelines = []\npipelines.append(('Scaled_LogisticRegression', Pipeline([('Scaler', StandardScaler()),('LogisticRegression', LogisticRegression(random_state=0))])))\npipelines.append(('Scaled_Bagging', Pipeline([('Scaler', StandardScaler()),('Bagging', BaggingClassifier(random_state=0))])))\npipelines.append(('Scaled_RandomForest', Pipeline([('Scaler', StandardScaler()),('RandomForest', RandomForestClassifier(random_state=0))])))\npipelines.append(('Scaled_AdaBoost', Pipeline([('Scaler', StandardScaler()),('AdaBoost', AdaBoostClassifier(random_state=0))])))\npipelines.append(('Scaled_GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier(random_state=0))])))\npipelines.append(('Scaled_XGB', Pipeline([('Scaler', StandardScaler()),('XGB', XGBClassifier(random_state=0))])))\npipelines.append(('Scaled_NeuralNetwork', Pipeline([('Scaler', StandardScaler()),('NeuralNetwork', MLPClassifier(random_state=0))])))\nresults_t = []\nresults_v = []\nnames = []\nscore_sd = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in pipelines:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    # Computing Confusion matrix for the above algorithm\n    cnf_matrix = confusion_matrix(y_valid, predictions_v)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score_sd.append(f_dict)\nplt.show()   \nscore_sd = pd.DataFrame(score_sd, columns = ['model','accuracy_train', 'accuracy_valid'])\n","2c213175":"###Verificando os scores dos modelos\nprint(score_sd)","ed648ab0":"####Aplicando do m\u00e9todo ADASYN\ny = y.astype('int') \nada = ADASYN(random_state=0)\nX_resampled_, y_resampled_ = ada.fit_resample(df_dum, y)\nprint(sorted(Counter(y_resampled_).items()))\n","0dc77d0a":"# Separando o treino e valida\u00e7\u00e3o dos dados de treino\n\nX_train_, X_valid_, y_train_, y_valid_ = train_test_split(X_resampled_, y_resampled_, train_size=0.8, test_size=0.2,\n                                                                random_state=0)","69e385a6":"###MOdelos de linha de base\n# Op\u00e7\u00f5es de teste e m\u00e9trica da avalia\u00e7\u00e3o\n#Algoritmos de verifica\u00e7\u00e3o pontual\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression(random_state=0)))\nmodels.append(('Bagging', BaggingClassifier(random_state=0)))\nmodels.append(('RandomForest', RandomForestClassifier(random_state=0)))\nmodels.append(('AdaBoost', AdaBoostClassifier(random_state=0)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=0)))\nmodels.append(('XGB', XGBClassifier(random_state=0)))\nresults_t = []\nresults_v = []\nnames = []\nscore = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train_, y_train_)\n    predictions_t = my_model.predict(X_train_) \n    predictions_v = my_model.predict(X_valid_)\n    accuracy_train = accuracy_score(y_train_, predictions_t) \n    accuracy_valid = accuracy_score(y_valid_, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    #Matriz de confus\u00e3o do algoritmo acima \n    cnf_matrix = confusion_matrix(y_valid_, predictions_v)\n    np.set_printoptions(precision=2)\n\n  #Gr\u00e1fico n\u00e3o normalizado da matriz \n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score.append(f_dict)\nplt.show()    \nscore = pd.DataFrame(score, columns = ['model','accuracy_train', 'accuracy_valid'])","bf52ef9c":"##Verificando os scores\nprint(score)","c2453895":"####MODELOS DE LINHA DE BASE ESCALONADOS\n# Algoritmos de verifica\u00e7\u00e3o pontual com conjunto de dados padronizado\npipelines = []\npipelines.append(('Scaled_LogisticRegression', Pipeline([('Scaler', StandardScaler()),('LogisticRegression', LogisticRegression(random_state=0))])))\npipelines.append(('Scaled_Bagging', Pipeline([('Scaler', StandardScaler()),('Bagging', BaggingClassifier(random_state=0))])))\npipelines.append(('Scaled_RandomForest', Pipeline([('Scaler', StandardScaler()),('RandomForest', RandomForestClassifier(random_state=0))])))\npipelines.append(('Scaled_AdaBoost', Pipeline([('Scaler', StandardScaler()),('AdaBoost', AdaBoostClassifier(random_state=0))])))\npipelines.append(('Scaled_GBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier(random_state=0))])))\npipelines.append(('Scaled_XGB', Pipeline([('Scaler', StandardScaler()),('XGB', XGBClassifier(random_state=0))])))\npipelines.append(('Scaled_NeuralNetwork', Pipeline([('Scaler', StandardScaler()),('NeuralNetwork', MLPClassifier(random_state=0))])))\nresults_t = []\nresults_v = []\nnames = []\nscore_sd = []\nskf = StratifiedKFold(n_splits=5)\nfor (name, model) in pipelines:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=skf)\n    my_model.fit(X_train_, y_train_)\n    predictions_t = my_model.predict(X_train_) \n    predictions_v = my_model.predict(X_valid_)\n    accuracy_train = accuracy_score(y_train_, predictions_t) \n    accuracy_valid = accuracy_score(y_valid_, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    #Matriz de confus\u00e3o do algoritmo acima \n    cnf_matrix = confusion_matrix(y_valid_, predictions_v)\n    np.set_printoptions(precision=2)\n\n    #Gr\u00e1fico n\u00e3o normalizado da matriz\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[\"BAD\"], title=\"Confusion Matrix - \"+str(name))\n    score_sd.append(f_dict)\nplt.show()   \nscore_sd = pd.DataFrame(score_sd, columns = ['model','accuracy_train', 'accuracy_valid'])","d421122e":"#Verificando os scores\nprint(score)","06d30c59":"# Importando o k-means\n# Determinando a quantidade de clusters\n\n# Importando o k-means\nfrom sklearn.cluster import KMeans\n\n# Selecionando as variaveis para utilizar no modelo.\nX= df_dum[['MORTDUE','LOAN', 'YOJ']]\n\n# C\u00e1lculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range (1, 12):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n    sse.append(kmeans.inertia_)\nprint(sse)","161b8109":"###Agora verificando a sugest\u00e3o de quantos cluster deve ser formados \nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 12), sse, 'bx-')\nplt.title('Elbow Method')\nplt.xlabel('Quantidade de Clusters')\nplt.ylabel('SSE')\nplt.show()","65e2df96":"###Formando os cluster\nkmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\ncluster_id = kmeans.fit_predict(X)\ncluster_id","3eeb47f5":"##Salvando os resultados do dataset e  conferindo o tamanho\n\nX['cluster_id'] = cluster_id\n\n\nX.sample(10)","7297b1bf":"X.head()","2b632541":"##Gr\u00e1fico dos cluster e os centr\u00f3ides\nfig = plt.figure(figsize=(14,10))\n\nplt.scatter(X.values[:,0], X.values[:,1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='blue', marker=\"x\", s=200)\nplt.show()","ff3ea624":"**####Aplicando do m\u00e9todo ADASYN\n##ADASYN: M\u00e9todo de amostragem sint\u00e9tica adapt\u00e1vel para dados desequilibrados\n## Trata-se de uma t\u00e9cnica de sobreamostragem para classes minorit\u00e1rias\n#que \u00e9 usado principalmente para resolver o problema de classifica\u00e7\u00e3o desequilibrada nos casos de uso do Machine Learning\n#Tem como objetivo ajustar um modelo que aprende um limite de decis\u00e3o muito complexo, \n#resultando em uma classifica\u00e7\u00e3o bem-sucedida das classes, mas o resultado \u00e9 um modelo que sofre de alta vari\u00e2ncia \n#e baixa condi\u00e7\u00e3o de vi\u00e9s, no caso OVERFITING***","f686eb29":"MODELOS DA LINHA DE BASE","75aeb14e":"Ap\u00f3s a constru\u00e7\u00e3o do gr\u00e1fico de cluster, observa-se os centr\u00f3ides marcados de cor azul. A dist\u00e2ncia entre esses centr\u00f3ides pode ser calculada pelo m\u00e9todo de Euclides para gerar uma m\u00e9dia.","756fffa3":"Data Mining e Machine Learning II\nInstitui\u00e7\u00e3o: Centro Universit\u00e1rio IESB\n\nCurso: P\u00f3s Gradua\u00e7\u00e3o Ci\u00eancia de Dados - Campus BSB\/Asa Sul\n\nDisciplina: Data Mining e Machine Learning II\n\nOrientador: Marcos Vinicius Guimar\u00e3es\n\nAluna:L\u00cdDIA MARA AGUIAR BEZERRA","3b9ede09":"Sobre o gr\u00e1fico de cotovelo, \u00e9 sugerido formar 4 grupos (clusters)","85111f62":"PARTINDO PARA AN\u00c1LISE DE CLUSTER\n","ed2d91d2":"Esse banco \"HMEQ\" \u00e9 composto de informa\u00e7\u00f5es sobre o cr\u00e9dito do consumidor de um banco para verificar se a prova ou reprova empr\u00e9stimos. Assim, o objetivo desse trabalho foi realizar uma an\u00e1lise explorat\u00f3ria dos dados, bem como, desenvolver modelos preditivos para se descobrir a chance de um cliente ser um inadimplente ou n\u00e3o.","e9f01934":"DIVIDINDO O DATA SET\n","923ee908":"MODELOS DE BASE ESCALONADOS","5f34d63c":"MODELOS DE LINHA DE BASE","103ded0e":"> MODELOS DE BASES ESCALONADAS","e592c254":"GERANDO A MATRIZ DE CONFUS\u00c3O\n"}}