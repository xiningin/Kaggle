{"cell_type":{"0af62ee7":"code","58f10ed7":"code","2e956000":"code","a7ca84a5":"code","b71c04c2":"code","90e9a601":"code","fccba006":"code","4a55db02":"code","f825a0a4":"code","e088b6d2":"code","657eac0c":"code","ef0d4f46":"code","78e71173":"code","f8d13ab3":"code","1ec12081":"code","d5aa3ca5":"code","69db70d4":"code","e8060e47":"code","58746969":"markdown","a3e86276":"markdown"},"source":{"0af62ee7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","58f10ed7":"df = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","2e956000":"df.head()","a7ca84a5":"df.shape\n","b71c04c2":"X=df[df.columns[2:32]]\ny=df[df.columns[1]]","90e9a601":"\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=0, stratify=y)\n\n","fccba006":"X_train.shape","4a55db02":"X_test.shape","f825a0a4":"knr = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\nsvc1 = LinearSVC(random_state=0).fit(X_train,y_train)\nsvc2 = LinearSVC(C=3, random_state=0).fit(X_train,y_train)\n#lr2 and svm2 is with incresed regularization parameter ( C values)\n\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(X_test,y_test)\ndec_tree = DecisionTreeClassifier()\ndec_tree.fit(X_train,y_train)\nrand_forest = RandomForestClassifier(n_estimators=100, random_state=42)\nrand_forest.fit(X_train,y_train)\n#lr1 = LogisticRegression(random_state=0).fit(X_train, y_train)\n#lr2 = LogisticRegression(C=6, random_state=0).fit(X_train, y_train)\n","e088b6d2":"print(\"train score knr- \" + str(knr.score(X_train, y_train)))\nprint(\"test score knr- \" + str(knr.score(X_test, y_test)))\nprint(\"train score svc1- \" + str(svc1.score(X_train, y_train)))\nprint(\"test score svc1- \" + str(svc1.score(X_test, y_test)))\nprint(\"train score svc2- \" + str(svc2.score(X_train, y_train)))\nprint(\"test score svc2- \" + str(svc2.score(X_test, y_test)))\nprint(\"train score naive_bayes - \" + str(naive_bayes.score(X_train, y_train)))\nprint(\"test score naive_bayes - \" + str(naive_bayes.score(X_test, y_test)))\nprint(\"train score dec_tree- \" + str(dec_tree.score(X_train, y_train)))\nprint(\"test score dec_tree- \" + str(dec_tree.score(X_test, y_test)))\nprint(\"train score rand_forest- \" + str(rand_forest.score(X_train, y_train)))\nprint(\"test score rand_forest- \" + str(rand_forest.score(X_test, y_test)))\n","657eac0c":"from sklearn.metrics import confusion_matrix\n\nimport seaborn as sns\n#Confusion Matrix\ny_pred = knr.predict(X_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()\n\n","ef0d4f46":"#Confusion Matrix\ny_pred = svc1.predict(X_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","78e71173":"#Confusion Matrix\ny_pred = svc2.predict(X_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","f8d13ab3":"#Confusion Matrix\ny_pred = naive_bayes.predict(X_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","1ec12081":"#Confusion Matrix\ny_pred = dec_tree.predict(X_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","d5aa3ca5":"#Confusion Matrix\ny_pred = rand_forest.predict(X_test)\nconf_mat = confusion_matrix(y_test,y_pred)\n#Visualization Confusion Matrix\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(conf_mat,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.show()","69db70d4":"method_names = []\nmethod_scores = []\nmethod_names.append(\"Decision Tree\")\nmethod_scores.append(dec_tree.score(X_test,y_test))\nmethod_names.append(\"KNN\")\nmethod_scores.append(knr.score(X_test,y_test))\nmethod_names.append(\"SVC1\")\nmethod_scores.append(svc1.score(X_test,y_test))\nmethod_names.append(\"SVC2\")\nmethod_scores.append(svc2.score(X_test,y_test))\nmethod_names.append(\"Naive Bayes\")\nmethod_scores.append(naive_bayes.score(X_test,y_test))\nmethod_names.append(\"Random Forest\")\nmethod_scores.append(rand_forest.score(X_test,y_test))\n\n","e8060e47":"# Comparison between different classifiers\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(15,10))\n#plt.ylim([0.85,1])\nplt.bar(method_names,method_scores,width=0.5)\nplt.xlabel('Method Name')\nplt.ylabel('Method Score')","58746969":"# Now going to execute different types of classifiers one by one","a3e86276":"# Bravo!"}}