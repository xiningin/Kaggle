{"cell_type":{"8612d98c":"code","08350e8a":"code","3a5ca1dd":"code","a0fc961a":"code","26b9b934":"code","f948d9df":"code","9c358fc0":"code","1c6d5682":"code","dcd8c774":"code","f0092165":"code","aecde010":"code","e3caacc3":"code","2f1bd5a3":"code","8f12dcac":"code","6455e108":"code","2f07112f":"code","128e4cb7":"code","4bb69ced":"code","c2f9c185":"code","37246656":"code","56b14bb1":"code","5d07496b":"code","c6203537":"code","5dff9cba":"code","da3e85bc":"code","d8a5188d":"code","c15f13ed":"code","8349cb1a":"code","ade73e54":"code","3f3331d7":"code","9eb44b68":"code","3af77bd2":"code","a4a6c1d2":"code","e2e2fd71":"code","97923af3":"code","98062bb2":"code","e97fd8ee":"code","5157072a":"code","94ba884c":"code","8bbcc594":"code","cb392ca0":"code","0d768988":"code","927bc87c":"code","291d22e0":"code","89dece86":"code","437b82d6":"code","dca15aad":"code","2cdefdc9":"code","0b090eaa":"markdown","183c32f3":"markdown","f7d555d4":"markdown","e5faa727":"markdown","e0ea9518":"markdown","7189d9d5":"markdown","497fd238":"markdown","23a39105":"markdown","68def617":"markdown","db18b453":"markdown","10c19385":"markdown","93994fd1":"markdown","f8885c32":"markdown","d0fac4c0":"markdown","83bfb2ac":"markdown","567e0a33":"markdown","5d70c023":"markdown","0a06f696":"markdown","f37ff9b2":"markdown","c2594af0":"markdown","c7035a49":"markdown"},"source":{"8612d98c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","08350e8a":"!pip install openpyxl\n!pip install plotly\n!pip install xlrd\n!pip install seaborn\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3a5ca1dd":"df = pd.read_csv(\"..\/input\/openintro-possum\/possum.csv\")","a0fc961a":"df.shape","26b9b934":"df.ndim","f948d9df":"df.describe()","9c358fc0":"df.isnull().sum()","1c6d5682":"df.duplicated().sum()","dcd8c774":"# Dropping null rows \r\ndf.dropna(axis=0,inplace=True)","f0092165":"print(\"Data shape after dropping null values: {}\".format(df.shape))\r\nprint(\"Data dimension: {}\\n\".format(df.ndim))\r\nprint(\"Checking for null values: \\n{}\".format(df.isnull().sum()))","aecde010":"df.keys()","e3caacc3":"# Categorical columns\r\nunique = [\"sex\", 'Pop', 'site']\r\n\r\n# Accessing the key and values\r\nfor x in unique:\r\n    # Accessing the unique labels in each  column\r\n    print(f\"In column {x}: {df[x].unique()}\")\r\n    # Total number of unique labels in each column\r\n    print(f\"Total number of unique values: {df[x].nunique()}\\n\")","2f1bd5a3":"df.head(10)","8f12dcac":"df.sample(10)","6455e108":"df['site'] = df['site'].apply(lambda x:str(x))\r\ndf.info() #change site into string","2f07112f":"# Categorical columns\r\nunique = [\"sex\", 'Pop', 'site']\r\n\r\n\r\nfig, ax = plt.subplots(3, figsize=(10,10))\r\nax = ax.ravel()\r\n\r\nfor index, value in enumerate(unique):\r\n    sns.boxplot(x='age', y=value, data=df, ax=ax[index])","128e4cb7":"df.info()","4bb69ced":"# Looking into the correlation\r\nplt.subplots(figsize=(15,15))\r\nsns.heatmap(df.corr(),annot=True)\r\nplt.show()","c2f9c185":"# # Looking into the correlation\r\n# plt.subplots(figsize=(15,15))\r\n# sns.heatmap(df.corr(),annot=True, cmap=\"YlGnBu\", cbar_kws= {\"orientation\": \"horizontal\"})\r\n# plt.show()","37246656":"body = [\r\n'hdlngth',\r\n'skullw',\r\n'totlngth',\r\n'taill',\r\n'footlgth',\r\n'earconch',\r\n'eye',\r\n'chest',\r\n'belly']\r\n\r\nfig, ax = plt.subplots(3,3, figsize=(10,10), constrained_layout=True)\r\nax = ax.ravel()\r\n\r\n\r\nfor index, value in enumerate(body):\r\n    sns.histplot(x=value, data=df, ax=ax[index], kde=True)\r\n    ax[index].set_title(f\"Skewness:{np.around(df[value].skew(axis=0),2)}\")\r\n","56b14bb1":"sns.histplot(x='age', data=df, kde=True)\r\nplt.title(f\"Skewness:{np.around(df['age'].skew(axis=0),2)}\")","5d07496b":"df.head()","c6203537":"df.info()","5dff9cba":"sns.histplot(data=df,x=\"site\", kde=False, hue=\"site\")","da3e85bc":"fig = px.pie(df,names=\"site\")\r\nfig.update_layout(\r\ntitle={\"x\": 0.5, \"xanchor\":\"center\", \r\n\"font_family\":\"Times New Roman\", \r\n\"text\":\"Site percentages\"})\r\nfig.show()","d8a5188d":"sns.histplot(data=df,x=\"Pop\", kde=False, hue=\"Pop\")","c15f13ed":"sns.histplot(data=df,x=\"sex\", kde=False, hue=\"sex\",multiple=\"dodge\", shrink=.8)","8349cb1a":"fig = px.pie(df, names=\"sex\")\r\nfig.update_layout(\r\n     title={\r\n         \"font_family\": \"New Times Roman\",\r\n         \"font_size\": 20,\r\n         \"text\": \"Sex pie chart\",\r\n         \"x\": 0.5,\r\n         \"xanchor\": \"center\",\r\n\r\n    },\r\n    showlegend=True,\r\n    legend={\r\n        \"orientation\": \"h\",\r\n        \"bgcolor\": \"darkblue\",\r\n        \"font_color\":\"orange\"\r\n    }\r\n)\r\n\r\nfig.show()","ade73e54":"print(\"{}\\n\".format(df.sex.value_counts()))\r\nprint(\"{}\\n\".format(df.Pop.value_counts()))\r\nprint(\"{}\".format(df.site.value_counts()))","3f3331d7":"df['belly'].nunique()","9eb44b68":"df.info()","3af77bd2":"# Converting strings to integers, binary form \r\n# For sex, m is 1, while f is 0\r\n# For Pop, Vic is 1, other is 0\r\ndf['Pop']=list(map(int, df['Pop']=='Vic'))\r\ndf['sex']=list(map(int, df['sex']=='m'))","a4a6c1d2":"df.head(10)","e2e2fd71":"from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\r\nfrom sklearn.pipeline import make_pipeline, Pipeline\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\r\nfrom sklearn.svm import LinearSVC\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\r\nimport  xgboost as xgb\r\n\r\n# Thr case was dropped due to it being redundant \r\nX = df.drop(['sex', 'case'], axis=1)\r\ny = df['sex']\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)","97923af3":"# Creating the pipleline\r\n\r\npipe  = Pipeline([(\"preprocessing\", StandardScaler()),(\"classifier\",KNeighborsClassifier())])\r\n\r\n# Creating the Parameter grid\r\n\r\nparams_grid = [\r\n    {   \"preprocessing\": [StandardScaler(),RobustScaler(),None], \r\n        \"classifier\":[KNeighborsClassifier()],\r\n        \"classifier__n_neighbors\":[3,5,7]     \r\n    },\r\n    {\r\n        \"classifier\":[DecisionTreeClassifier(),RandomForestClassifier()],\r\n        # \"classifier__n_estimators\":[100,None],\r\n        \"classifier__max_depth\":[3,5,None],\r\n        \"preprocessing\": [None]\r\n    },\r\n\r\n    {\r\n        \"classifier\":[GradientBoostingClassifier(), xgb.XGBClassifier()],\r\n        # \"classifier__n_estimators\":[100,None],\r\n        \"classifier__max_depth\":[3,5,None],\r\n        \"classifier__learning_rate\":[0.001,0.01, 0.1],\r\n        \"preprocessing\": [None]\r\n    },\r\n\r\n    {\r\n        \"classifier\":[LinearSVC()],\r\n        \"classifier__C\":[0.01,0.1,1,10,100,None],\r\n        \"preprocessing\": [StandardScaler(),RobustScaler(),None]\r\n    }\r\n    \r\n\r\n    ]\r\n\r\n\r\nStratified_Shuffle_Split = StratifiedShuffleSplit(random_state=42, test_size=.40,n_splits=5)","98062bb2":"grid = GridSearchCV(pipe,params_grid,cv=Stratified_Shuffle_Split)\r\ngrid.fit(X_train,y_train)","e97fd8ee":"## Printing the values\r\nprint(\"Best params:\\n{}\\n\".format(grid.best_params_))\r\nprint(\"Best estimator:\\n{}\\n\".format(grid.best_params_))\r\nprint(\"Best score: \\n{}\\n\".format(grid.best_score_))\r\n\r\n# Displaying using the pandas format\r\nresults = pd.DataFrame(grid.cv_results_)\r\ndisplay(results.T)","5157072a":"grid.score(X_test,y_test)","94ba884c":"knn = KNeighborsClassifier(n_neighbors=3)\r\npipe2 = make_pipeline(StandardScaler(),knn)\r\npipe2.fit(X_train,y_train)","8bbcc594":"y_pred = pipe2.predict(X_test)\r\ny_pred","cb392ca0":"pipe2.classes_","0d768988":"# Classification report\r\nfrom sklearn.metrics import  classification_report,confusion_matrix,ConfusionMatrixDisplay\r\ny_test = y_test\r\n\r\ndef class_report(model,y_test,pred):\r\n    print(classification_report(y_test,pred))\r\n    cm = confusion_matrix(y_test,pred,labels=model.classes_)\r\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\r\n    disp.plot()\r\n    plt.show()\r\n\r\n\r\nclass_report(pipe2,y_test,y_pred)","927bc87c":"tn,fp,fn,tp = confusion_matrix(y_test, y_pred).ravel()\r\nprint(\"The tp,tn,fp,fn respectively: \\n{} {} {} {}\\n\".format(tp,tn,fp,fn))\r\nprint(\"TNR (Specificity) is: \\n{}\\n\".format({tn\/(tn+fp)}))\r\nprint(\"TPR (Sensitivity) is: \\n{}\\n\".format({tp\/(tp+fn)}))","291d22e0":"from sklearn.metrics import average_precision_score,roc_auc_score, plot_precision_recall_curve,plot_roc_curve\r\n\r\ny_predict_proba = pipe2.predict_proba(X_test)\r\nprint(\"Checking Uncertainties: \\n{}\\n\".format(y_predict_proba[:5]))\r\n\r\naverage_precision = average_precision_score(y_test,pipe2.predict_proba(X_test)[:,1])\r\nprint(\"Average Precision: {:.2f}\".format(average_precision))\r\ndisp = plot_precision_recall_curve(pipe2,X_test,y_test)\r\nplt.title(\"2-class Precision-Recall curve:\")","89dece86":"roc_scoring = roc_auc_score(y_test,pipe2.predict_proba(X_test)[:,1])\r\nprint(\"AUC score: {:.2f}\".format(roc_scoring))\r\ndisp = plot_roc_curve(pipe2,X_test,y_test)\r\nplt.title(\"2-class ROC curve:\")\r\nplt.show()","437b82d6":"from sklearn.metrics import roc_curve,roc_auc_score, precision_recall_curve\r\nknn = KNeighborsClassifier(n_neighbors=3)\r\npipe2 = make_pipeline(StandardScaler(),knn)\r\npipe2.fit(X_train,y_train)","dca15aad":"y_predict_proba = pipe2.predict_proba(X_test)[:,1]\r\nfpr,tpr, thresholds = roc_curve(y_test,y_predict_proba)\r\nauc_score = roc_auc_score(y_test,y_predict_proba)\r\n\r\nfig = px.area(\r\n    x=fpr, y=tpr,\r\n    title=f'ROC Curve (AUC= {auc_score:.2f})',\r\n    labels=dict(x='False Positive Rate', y='True Positive Rate'),\r\n    width=700, height=500\r\n)\r\n\r\nfig.add_shape(\r\n    type ='line',line=dict(dash='dash'),\r\n    x0=0,x1=1,y0=0,y1=1\r\n)\r\n\r\nfig.update_layout(\r\n     title={\r\n         \"font_family\": \"New Times Roman\",\r\n         \"font_size\": 20,\r\n         \"x\": 0.5,\r\n         \"xanchor\": \"center\",\r\n\r\n    },\r\n    showlegend=True,\r\n    legend={\r\n        \"orientation\": \"h\",\r\n        \"bgcolor\": \"darkblue\",\r\n        \"font_color\":\"orange\"\r\n    }\r\n)\r\n\r\nfig.update_yaxes(scaleanchor='x', scaleratio=1)\r\nfig.update_xaxes(constrain='domain')\r\nfig.show()","2cdefdc9":"y_predict_proba = pipe2.predict_proba(X_test)[:,1]\r\nprecision, recall, thresholds = precision_recall_curve(y_test,y_predict_proba)\r\naverage_precision = average_precision_score(y_test,pipe2.predict_proba(X_test)[:,1])\r\n\r\nfig = px.area(\r\n    x=recall, y=precision,\r\n    title=f'Precision-Recall Curve (AP= {average_precision:.2f})',\r\n    labels=dict(x='Recall', y='Precision'),\r\n    width=700, height=500\r\n)\r\nfig.update_layout(\r\n     title={\r\n         \"font_family\": \"New Times Roman\",\r\n         \"font_size\": 20,\r\n         \"x\": 0.5,\r\n         \"xanchor\": \"center\",\r\n\r\n    },\r\n    showlegend=True,\r\n    legend={\r\n        \"orientation\": \"h\",\r\n        \"bgcolor\": \"darkblue\",\r\n        \"font_color\":\"orange\"\r\n    }\r\n)\r\n\r\nfig.update_yaxes(scaleanchor='x', scaleratio=1)\r\nfig.update_xaxes(constrain='domain')\r\nfig.show()","0b090eaa":"## Building the algorithm\r\nFor this classification dataset, a pipeline will be used with the use of several algorithms;","183c32f3":"### Checking Skewness\r\nLevels of skewness\r\n1. (-0.5,0.5) = lowly skewed\r\n2. (-1,0-0.5) U (0.5,1) = Moderately skewed\r\n3. (-1 & beyond ) U (1 & beyond) = Highly skewed","f7d555d4":"### Observations\r\n1. Based on sex, females have one outlier and its median is more than that of males.\r\n2. Vic has more variations in age.\r\n3. Sites 2,4,6,7 all have outliers, furthermore site 6 has two outliers. Moreover, sites 1 and 5 have no outliers.\r\n4. Site 5 has the highest median ","e5faa727":"## Using plotly","e0ea9518":"## Performing EDA","7189d9d5":"From the grid search, with the use of a standard scaler, KNN neighbours 3 is the best on the training dataset.","497fd238":"### ROC curve","23a39105":"### Precision-recall curve","68def617":"age is the target variable and it has a reasonable good correlation with hdlngth, chest and belly, with values 0.33, 0.34 and 0.36 respectively","db18b453":"## Conclusion\r\nIn this analysis, different methods of evaluation were used with on this dataset with the use of a pipeline and grid search.\r\n- The KNN classifier is the best out of all with the use of a Standard scaler to aid it.\r\n- The AP and AUC are both 0.75, thereby explaining the dataset.\r\n- When the model can perfectly separate the two outcomes, the ROC curve forms a right angle and the AUC becomes 1.\r\n- Similarly to the ROC curve, when the two outcomes separate, precision-recall curves will approach the top-right corner. Typically, a model that produces a precision-recall curve that is closer to the top-right corner is better than a model that produces a precision-recall curve that is skewed towards the bottom of the plot. \r\n- class 1 (male) has high precision, having a value of 0.82. \r\n- The recall of class 0 (female) is high, having a value of 0.82\r\n- Specificity is 0.82, sensitivity is 0.6. This mean that the algorithm is highly confident in identifying females which is the negative class 0","10c19385":"No duplicates detected.","93994fd1":"The population of other is greater than that of Vic","f8885c32":"### Specificity or True Negative Rate (TNR)\r\nTNR (ranges from 0 to 1, higher is better) measures the proportion of negatives that are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition).\r\nTNR = TN\/(TN+FP)\r\n\r\n### Precision, Positive Predictive Value (PPV)\r\nPPV (ranges from 0 to 1, higher is better) is the ratio of true positives over all true and false positives:\r\nPrecision = TP\/(TP+FP)\r\nHigh precision means that an algorithm returned substantially more relevant results than irrelevant ones, or in other words the more likely everything it returns is right, but it does not mean it may get all the right results that are out there.\r\n\r\n\r\n### Recall, Sensitivity, Hit Rate or True Positive Rate (TPR)\r\nTPR (ranges from 0 to 1, higher is better) is the ratio of true positives over the sum of true positives and false negatives:\r\nRecall = TP \/ (TP+FN)\r\nHigh recall means that an algorithm returned most of the relevant results, \r\nRecall is used as performance metric when we need to identify all positive samples; that is, when it is important to avoid false negatives.","d0fac4c0":"## The Model","83bfb2ac":"There are two null values in age and one in footlgth","567e0a33":"## The Possum Regression Dataset for Classification.","5d70c023":"### ROC Curve","0a06f696":"### Precision-Recall curve","f37ff9b2":"1. hdlngth, totlngth and eye has negative values for skewness with values -0.05,0.25 and 0.06 respectively.\r\n2. skullw has the highest level of skewness","c2594af0":"The number of males is greater than that of females by 16.8%","c7035a49":"Site 1 has the highest number of members from the above plots."}}