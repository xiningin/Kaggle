{"cell_type":{"82a4f2d2":"code","f9c2c80f":"code","3795a5f0":"code","30159165":"code","cd9d31b5":"code","f65d5f8d":"code","283cb06d":"code","25df60cc":"code","6bac0207":"code","a5b432ff":"code","4f35f358":"code","2a9b3db8":"code","75f032c9":"code","a4a7d5d7":"code","66dafbdd":"code","195eab17":"code","56d9fe87":"code","16fc4a92":"code","8cdb8251":"markdown","2948f587":"markdown","ed6fc8ff":"markdown","adfdf7f4":"markdown","1e10904b":"markdown","e2759191":"markdown","564af84d":"markdown"},"source":{"82a4f2d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9c2c80f":"!pip install transformers","3795a5f0":"import torch","30159165":"from transformers import BertForQuestionAnswering\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","cd9d31b5":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')","f65d5f8d":"question = \"How many parameters does BERT-large have?\"\nanswer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"","283cb06d":"# Apply the tokenizer to the input text, treating them as a text-pair.\ninput_ids = tokenizer.encode(question, answer_text)\n\nprint('The input has a total of {:} tokens.'.format(len(input_ids)))","25df60cc":"# BERT only needs the token IDs, but for the purpose of inspecting the \n# tokenizer's behavior, let's also get the token strings and display them.\ntokens = tokenizer.convert_ids_to_tokens(input_ids)\n\n# For each token and its id...\nfor token, id in zip(tokens, input_ids):\n    \n    # If this is the [SEP] token, add some space around it to make it stand out.\n    if id == tokenizer.sep_token_id:\n        print('')\n    \n    # Print the token string and its ID in two columns.\n    print('{:<12} {:>6,}'.format(token, id))\n\n    if id == tokenizer.sep_token_id:\n        print('')","6bac0207":"# Search the input_ids for the first instance of the `[SEP]` token.\nsep_index = input_ids.index(tokenizer.sep_token_id)\n\n# The number of segment A tokens includes the [SEP] token istelf.\nnum_seg_a = sep_index + 1\n\n# The remainder are segment B.\nnum_seg_b = len(input_ids) - num_seg_a\n\n# Construct the list of 0s and 1s.\nsegment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n# There should be a segment_id for every input token.\nassert len(segment_ids) == len(input_ids)","a5b432ff":"# Run our example through the model.\nstart_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text","4f35f358":"# Find the tokens with the highest `start` and `end` scores.\nanswer_start = torch.argmax(start_scores)\nanswer_end = torch.argmax(end_scores)\n\n# Combine the tokens in the answer and print it out.\nanswer = ' '.join(tokens[answer_start:answer_end+1])\n\nprint('Answer: \"' + answer + '\"')","2a9b3db8":"# Start with the first token.\nanswer = tokens[answer_start]\n\n# Select the remaining answer tokens and join them with whitespace.\nfor i in range(answer_start + 1, answer_end + 1):\n    \n    # If it's a subword token, then recombine it with the previous token.\n    if tokens[i][0:2] == '##':\n        answer += tokens[i][2:]\n    \n    # Otherwise, add a space then the token.\n    else:\n        answer += ' ' + tokens[i]\n\nprint('Answer: \"' + answer + '\"')","75f032c9":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\n#sns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (16,8)","a4a7d5d7":"# Pull the scores out of PyTorch Tensors and convert them to 1D numpy arrays.\ns_scores = start_scores.detach().numpy().flatten()\ne_scores = end_scores.detach().numpy().flatten()\n\n# We'll use the tokens as the x-axis labels. In order to do that, they all need\n# to be unique, so we'll add the token index to the end of each one.\ntoken_labels = []\nfor (i, token) in enumerate(tokens):\n    token_labels.append('{:} - {:>2}'.format(token, i))","66dafbdd":"# Create a barplot showing the start word score for all of the tokens.\nax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n\n# Turn the xlabels vertical.\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n\n# Turn on the vertical grid to help align words to scores.\nax.grid(True)\n\nplt.title('Start Word Scores')\n\nplt.show()","195eab17":"# Create a barplot showing the end word score for all of the tokens.\nax = sns.barplot(x=token_labels, y=e_scores, ci=None)\n\n# Turn the xlabels vertical.\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n\n# Turn on the vertical grid to help align words to scores.\nax.grid(True)\n\nplt.title('End Word Scores')\n\nplt.show()","56d9fe87":"import pandas as pd\n\n# Store the tokens and scores in a DataFrame. \n# Each token will have two rows, one for its start score and one for its end\n# score. The \"marker\" column will differentiate them. A little wacky, I know.\nscores = []\nfor (i, token_label) in enumerate(token_labels):\n\n    # Add the token's start score as one row.\n    scores.append({'token_label': token_label, \n                   'score': s_scores[i],\n                   'marker': 'start'})\n    \n    # Add  the token's end score as another row.\n    scores.append({'token_label': token_label, \n                   'score': e_scores[i],\n                   'marker': 'end'})\n    \ndf = pd.DataFrame(scores)\n","16fc4a92":"# Draw a grouped barplot to show start and end scores for each word.\n# The \"hue\" parameter is where we tell it which datapoints belong to which\n# of the two series.\ng = sns.catplot(x=\"token_label\", y=\"score\", hue=\"marker\", data=df,\n                kind=\"bar\", height=6, aspect=4)\n\n# Turn the xlabels vertical.\ng.set_xticklabels(g.ax.get_xticklabels(), rotation=90, ha=\"center\")\n\n# Turn on the vertical grid to help align words to scores.\ng.ax.grid(True)","8cdb8251":"visualizing the end score","2948f587":"visualizing start score","ed6fc8ff":"clean the answer format.","adfdf7f4":"load bert model","1e10904b":"load bert tokenizer","e2759191":"visualizing start and end score together","564af84d":"Install transformer library"}}