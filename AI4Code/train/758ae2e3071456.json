{"cell_type":{"25e42c3e":"code","7d10eb7f":"code","da1d564d":"code","5937018a":"code","03267dc4":"code","721252f7":"code","27f70992":"markdown","b7182fa4":"markdown","42e551ea":"markdown","aa597a33":"markdown","228e2f4d":"markdown","8aeb6a8e":"markdown"},"source":{"25e42c3e":"import os\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Dense, Dropout, Reshape, Conv1D, BatchNormalization, Activation, AveragePooling1D, GlobalAveragePooling1D, Lambda, Input, Concatenate, Add, UpSampling1D, Multiply\nfrom keras.models import Model\nfrom keras.objectives import mean_squared_error\nfrom keras import backend as K\nfrom keras.losses import binary_crossentropy, categorical_crossentropy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\nfrom keras.initializers import random_normal\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, train_test_split","7d10eb7f":"df_train = pd.read_csv(\"..\/input\/liverpool-ion-switching\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/liverpool-ion-switching\/test.csv\")\n\n# I don't use \"time\" feature\ntrain_input = df_train[\"signal\"].values.reshape(-1,4000,1)#number_of_data:1250 x time_step:4000\ntrain_input_mean = train_input.mean()\ntrain_input_sigma = train_input.std()\ntrain_input = (train_input-train_input_mean)\/train_input_sigma\ntest_input = df_test[\"signal\"].values.reshape(-1,4000,1)\ntest_input = (test_input-train_input_mean)\/train_input_sigma\n\ntrain_target = df_train[\"open_channels\"].values.reshape(-1,4000,1)\n\nidx = np.arange(train_input.shape[0])\ntrain_idx, val_idx = train_test_split(idx, random_state = 111,test_size = 0.2)\n\nval_input = train_input[val_idx]\ntrain_input = train_input[train_idx] \nval_target = train_target[val_idx]\ntrain_target = train_target[train_idx] \n\nprint(\"train_input:{}, val_input:{}, train_target:{}, val_target:{}\".format(train_input.shape, val_input.shape, train_target.shape, val_target.shape))","da1d564d":"def cbr(x, out_layer, kernel, stride, dilation):\n    x = Conv1D(out_layer, kernel_size=kernel, dilation_rate=dilation, strides=stride, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef se_block(x_in, layer_n):\n    x = GlobalAveragePooling1D()(x_in)\n    x = Dense(layer_n\/\/8, activation=\"relu\")(x)\n    x = Dense(layer_n, activation=\"sigmoid\")(x)\n    x_out=Multiply()([x_in, x])\n    return x_out\n\ndef resblock(x_in, layer_n, kernel, dilation, use_se=True):\n    x = cbr(x_in, layer_n, kernel, 1, dilation)\n    x = cbr(x, layer_n, kernel, 1, dilation)\n    if use_se:\n        x = se_block(x, layer_n)\n    x = Add()([x_in, x])\n    return x  \n\ndef Unet(input_shape=(None,1)):\n#     layer_n = 64\n    layer_n = 96\n#     kernel_size = 7\n#     depth = 2\n    kernel_size = 8\n    depth = 3\n\n    input_layer = Input(input_shape)    \n    input_layer_1 = AveragePooling1D(5)(input_layer)\n    input_layer_2 = AveragePooling1D(25)(input_layer)\n#     input_layer_2 = AveragePooling1D(21)(input_layer)\n    \n    ########## Encoder\n    x = cbr(input_layer, layer_n, kernel_size, 1, 1)#1000\n    for i in range(depth):\n        x = resblock(x, layer_n, kernel_size, 1)\n    out_0 = x\n\n    x = cbr(x, layer_n*2, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*2, kernel_size, 1)\n    out_1 = x\n\n    x = Concatenate()([x, input_layer_1])    \n    x = cbr(x, layer_n*3, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*3, kernel_size, 1)\n    out_2 = x\n\n    x = Concatenate()([x, input_layer_2])    \n    x = cbr(x, layer_n*4, kernel_size, 5, 1)\n    for i in range(depth):\n        x = resblock(x, layer_n*4, kernel_size, 1)\n    \n    ########### Decoder\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_2])\n    x = cbr(x, layer_n*3, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_1])\n    x = cbr(x, layer_n*2, kernel_size, 1, 1)\n\n    x = UpSampling1D(5)(x)\n    x = Concatenate()([x, out_0])\n    x = cbr(x, layer_n, kernel_size, 1, 1)    \n\n    x = Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\")(x)\n    out = Activation(\"sigmoid\")(x)\n    out = Lambda(lambda x: 12*x)(out)\n    \n    model = Model(input_layer, out)\n    \n    return model\n\n\ndef augmentations(input_data, target_data):\n    #flip\n    if np.random.rand()<0.5:    \n        input_data = input_data[::-1]\n        target_data = target_data[::-1]\n\n    return input_data, target_data\n\n\ndef Datagen(input_dataset, target_dataset, batch_size, is_train=False):\n    x=[]\n    y=[]\n  \n    count=0\n    idx_1 = np.arange(len(input_dataset))\n    #idx_2 = np.arange(len(input_dataset))\n    np.random.shuffle(idx_1)\n    #np.random.shuffle(idx_2)\n\n    while True:\n        for i in range(len(input_dataset)):\n            input_data = input_dataset[idx_1[i]]\n            target_data = target_dataset[idx_1[i]]\n            #input_data_mix = input_dataset[idx_2[i]]\n            #target_data_mix = target_dataset[idx_2[i]]\n\n            if is_train:\n                input_data, target_data = augmentations(input_data, target_data)\n                #input_data_mix, target_data_mix = augmentations(input_data_mix, target_data_mix)\n                \n            x.append(input_data)\n            y.append(target_data)\n            count+=1\n            if count==batch_size:\n                x=np.array(x, dtype=np.float32)\n                y=np.array(y, dtype=np.float32)\n                inputs = x\n                targets = y       \n                x = []\n                y = []\n                count=0\n                yield inputs, targets\n\n                \ndef model_fit(model, train_inputs, train_targets, val_inputs, val_targets, n_epoch, batch_size=32):\n    hist = model.fit_generator(\n        Datagen(train_inputs, train_targets, batch_size, is_train=True),\n        steps_per_epoch = len(train_inputs) \/\/ batch_size,\n        epochs = n_epoch,\n        validation_data=Datagen(val_inputs, val_targets, batch_size),\n        validation_steps = len(val_inputs) \/\/ batch_size,\n        callbacks = [lr_schedule],\n        shuffle = False,\n        verbose = 1\n        )\n    return hist\n\n\ndef lrs(epoch):\n    if epoch<35:\n        lr = learning_rate\n    elif epoch<50:\n        lr = learning_rate\/10\n    else:\n        lr = learning_rate\/100\n    return lr\n","5937018a":"K.clear_session()\nmodel = Unet()\n# print(model.summary())\n\nlearning_rate=0.0005\nn_epoch=100\n# batch_size=32\nbatch_size=72\n\n\nlr_schedule = LearningRateScheduler(lrs)\nmodel.compile(loss=\"mean_squared_error\", \n              optimizer=Adam(lr=learning_rate),\n              metrics=[\"mean_absolute_error\"])\n\nhist = model_fit(model, train_input, train_target, val_input, val_target, n_epoch, batch_size)","03267dc4":"pred = ((model.predict(val_input)+model.predict(val_input[:,::-1,:])[:,::-1,:])\/2).reshape(-1)\nprint(\"VALIDATION_SCORE: \", cohen_kappa_score(val_target.reshape(-1), np.round(pred, 0), weights=\"quadratic\"))","721252f7":"pred = ((model.predict(test_input)+model.predict(test_input[:,::-1,:])[:,::-1,:])\/2).reshape(-1)\n\ndf_sub = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\", dtype={'time':str})\ndf_sub.open_channels = np.array(np.round(pred,0), np.int)\ndf_sub.to_csv(\"submission.csv\",index=False)","27f70992":"## Training","b7182fa4":"## Import Library","42e551ea":"## Introduction\nIn this kernel, I'd like to share the approach using 1D Convolutional Neural Networks(1D CNN). 1D CNN is sometimes effective to analyze time series data. This kernel introduces U-Net architecture with 1D CNN.","aa597a33":"## Define Model\nThis section defines U-Net(se-resnet base).\nInput and output of the U-Net are follows:\n* Input: 4000 time steps of \"signal\"\n* Output: 4000 time steps of \"open_channels\"","228e2f4d":"## Predict and Submit\nThis is not the main topic of this kernel, so I just round predicted values.","8aeb6a8e":"## Load and Split Dataset\nSimply split the input data into certain length."}}