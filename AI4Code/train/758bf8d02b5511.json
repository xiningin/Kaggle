{"cell_type":{"60dfc752":"code","66dc1766":"code","048e42df":"code","05834f7e":"code","695f7167":"code","71a33bb5":"code","529e5625":"code","13a7621d":"code","86afdcdf":"code","b844713e":"code","2223760d":"code","5d9343ee":"code","4694e424":"code","dc4b0172":"code","5c55f8ef":"code","fa5c8029":"code","8b58e35b":"code","d8411d91":"code","cdb61af2":"code","c9f32527":"code","9af51ae8":"markdown","02cc0a2f":"markdown","7fdb8b8b":"markdown","e812809b":"markdown"},"source":{"60dfc752":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","66dc1766":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub","048e42df":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","05834f7e":"import tokenization","695f7167":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","71a33bb5":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","529e5625":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","13a7621d":"train_org = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ntrain_org.shape","86afdcdf":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_org[train_org['id'].isin(ids_with_target_error)]","b844713e":"#Lets fix them\ntrain_org.at[train_org['id'].isin(ids_with_target_error),'target'] = 0\ntrain_org[train_org['id'].isin(ids_with_target_error)]","2223760d":"train_add = pd.read_csv(\"\/kaggle\/input\/train-addd\/train_add.csv\")\ntrain_add","5d9343ee":"#Lets append it to the originsl dataset\ntrain = train_org.append(train_add)\ntrain.shape","4694e424":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","dc4b0172":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","5c55f8ef":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","fa5c8029":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","8b58e35b":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')","d8411d91":"test_pred = model.predict(test_input)","cdb61af2":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","c9f32527":"submission.head()","9af51ae8":"I have done above thing only for a slighter high score\n","02cc0a2f":"**Please consider Upvoting if you like this kernel.******\n\nThank you for reading this kernel.\nHappy learning and sharing \ud83d\ude0a","7fdb8b8b":"While looking this model and my own models, I found out that these kind of predictions are so sensitive to the training data. Next, I read tweets in training data and figure out, that some of them have errors","e812809b":"I have seen a lot of people pooling the output of BERT, then add some dense layers. So, i thought i would give a try on using BERT. I have also seen various learning rates for fine tuning. In this kernel i want to try some ideas like No pooling, directly using CLS embedding, adding sigmoid output directly to the last layer of BERT, fixed learning rate. epochs and optimizer."}}