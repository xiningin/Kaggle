{"cell_type":{"a7677da0":"code","dd837302":"code","5659ac52":"code","eb267108":"code","c8ef6e3e":"code","28ecf62b":"code","b9829474":"code","533d60e7":"code","bde68a2f":"code","a59b0d12":"code","bc64b8d1":"code","24596997":"code","a2469687":"code","9539434c":"code","8c97c077":"code","5adcfaac":"code","31bbb400":"code","6b072a5f":"code","c3a37998":"code","9d058aff":"code","22b98dd5":"code","828f46d8":"code","53447520":"code","0daef551":"code","d4613130":"code","5b1ed2dd":"code","e1b208ad":"code","0fd0faa9":"code","7a6ee009":"code","093b00b0":"code","c15d2d00":"code","d2598d6e":"code","57aeefcd":"code","ccd243e2":"code","7046b694":"code","19a69df4":"code","7e36fed6":"code","5f074dd8":"markdown","8a9d1c0e":"markdown","d90f4df9":"markdown","97b74fb8":"markdown","5d8bb99f":"markdown","b366b7f7":"markdown","f8d8f563":"markdown","7b22483d":"markdown","d7beddf9":"markdown"},"source":{"a7677da0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","dd837302":"heart_df = pd.read_csv(\"..\/input\/heart-clean\/heart_clean.csv\")","5659ac52":"heart_df.shape","eb267108":"heart_df.info()","c8ef6e3e":"heart_df.head()","28ecf62b":"heart_df.tail()","b9829474":"heart_df.describe(include='all')","533d60e7":"heart_df.sample(10)","bde68a2f":"heart_df.isnull().sum()","a59b0d12":"heart_df.dtypes","bc64b8d1":"heart_df.hist(figsize=(10,15))","24596997":"sns.pairplot(heart_df, diag_kind='kde', hue='HeartDisease')","a2469687":"corr = heart_df.corr()\ncorr","9539434c":"sns.heatmap(corr, annot=False)","8c97c077":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","5adcfaac":"x=heart_df.drop(\"HeartDisease\",axis=1)\ny=heart_df[\"HeartDisease\"]","31bbb400":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)","6b072a5f":"model=LogisticRegression(solver='liblinear')\nmodel.fit(x_train,y_train)","c3a37998":"for idx, col_name in enumerate(x_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, model.coef_[0][idx]))","9d058aff":"intercept = model.intercept_[0]\nprint(\"The intercept for our model is {}\".format(intercept))","22b98dd5":"predictions = model.predict(x_test)","828f46d8":"model_score = model.score(x_train, y_train)\nprint(model_score)","53447520":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, predictions)","0daef551":"from sklearn import metrics\nprint(metrics.classification_report(y_test, predictions))","d4613130":"cm=metrics.confusion_matrix(y_test, predictions, labels=[0, 1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\",\"Yes\"]],\n                  columns = [i for i in [\"No\",\"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True,fmt='g')","5b1ed2dd":"from sklearn.tree import DecisionTreeClassifier\ndTree = DecisionTreeClassifier(criterion = 'gini', random_state = 1)\ndTree.fit(x_train, y_train)","e1b208ad":"print(dTree.score(x_train, y_train))\nprint(dTree.score(x_test, y_test))","0fd0faa9":"dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, random_state = 1)\ndTreeR.fit(x_train, y_train)\nprint(dTreeR.score(x_train, y_train))\nprint(dTreeR.score(x_test, y_test))","7a6ee009":"print(dTreeR.score(x_test, y_test))\ny_predict = dTreeR.predict(x_test)\n\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","093b00b0":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(n_estimators=50, bootstrap=True, oob_score=True, random_state=1)\nbgcl = bgcl.fit(x_train, y_train)","c15d2d00":"y_predict = bgcl.predict (x_test)\nprint(bgcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","d2598d6e":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier (n_estimators = 10, random_state = 1)\n#abcl = AdaBoostClassisifier (n_estimators = 50, random_state = 1)\nabcl = abcl.fit(x_train, y_train)","57aeefcd":"y_predict = abcl.predict (x_test)\nprint(abcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","ccd243e2":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, random_state=1)\ngbcl = gbcl.fit(x_train, y_train)","7046b694":"y_predict = gbcl.predict (x_test)\nprint(gbcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","19a69df4":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 50, random_state = 1, max_features = 8)\nrfcl = rfcl.fit(x_train, y_train)","7e36fed6":"y_predict = rfcl.predict (x_test)\nprint(rfcl.score(x_test, y_test))\ncm = metrics.confusion_matrix(y_test, y_predict, labels = [0,1])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"No\", \"Yes\"]],\n                    columns = [i for i in [\"No\", \"Yes\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot = True, fmt = 'g')","5f074dd8":"### Ensemble RandomForest Classifier","8a9d1c0e":"### Ensebmle Learning - AdaBoosting","d90f4df9":"### Split the Data","97b74fb8":"### Fit the Model - Logistic Regression","5d8bb99f":"### Reducing over fitting (Regularization) - Decision Tree","b366b7f7":"### Build Decision Tree Model","f8d8f563":"#### The confusion matrix\n\n#### No, No = True Positives (TP): We correctly predicted that they do not have Heart Disease 100.\n\n#### Yes, Yes = True Negatives (TN): We correctly predicted that they have Heart Disease 138.\n\n#### No, Yes = False Positives (FP): We incorrectly predicted that they don't have Heart Disease (a \"Type I error\") 26 Falsely predict positive Type I error.\n\n#### Yes, No = False Negatives (FN): We incorrectly predicted that they have Heart Disease (a \"Type II error\") 12 Falsely predict negative Type II error.","7b22483d":"### Ensebmle Learning - Bagging","d7beddf9":"### Ensebmle Learning - GradientBoostingClassifier"}}