{"cell_type":{"707463ff":"code","0f2a1d71":"code","f6dd3f82":"code","3b61141e":"code","369c4629":"code","f8d270ad":"code","d4aef6e3":"code","c57e76b3":"code","998ec36c":"code","6bfdd115":"code","6d43499f":"code","e613b49f":"code","e908b785":"code","cd40b950":"code","292a749e":"code","0d1da31d":"code","3a74c411":"code","e39cc45f":"code","dd8f5837":"code","5bd9227e":"code","c1e279c7":"code","9d21bfde":"code","91bde6b1":"code","7484c285":"code","50e9abac":"code","71f336fc":"code","8f151547":"code","4b63735e":"code","8f22de72":"code","80135861":"code","74a1a272":"code","58e3284d":"code","d7aa2ad6":"code","063d435c":"code","9d649c33":"code","006a4036":"code","c1b0527a":"code","0ed7a852":"code","21b3a80f":"code","3068b2ff":"code","a635b637":"code","c2fead4d":"code","5ef5832d":"code","9f728c88":"code","d90f651b":"code","1d2b38c9":"code","c9b5d347":"code","3b5d820a":"code","7d1fb0d3":"code","4eedc846":"code","aede712b":"code","f0538bda":"code","18aca2b2":"code","57a59bf0":"code","cbfe4b18":"markdown"},"source":{"707463ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f2a1d71":"data=pd.read_csv('\/kaggle\/input\/insurance-prediction\/insurance.csv')","f6dd3f82":"data","3b61141e":"data.info","369c4629":"data.describe(include='all')","f8d270ad":"data['sex'].replace(to_replace=['male','female'],value=[0,1],inplace=True)","d4aef6e3":"data","c57e76b3":"data['smoker'].replace(to_replace=['no','yes'],value=[0,1],inplace=True)","998ec36c":"data","6bfdd115":"data['region'].replace(to_replace=data['region'].unique(),value=list(range(len(data['region'].unique()))),\n                      inplace=True)","6d43499f":"data","e613b49f":"region_variable_ohe = np.eye(len(data['region'].unique()),len(data['region'].unique()))[data['region']]","e908b785":"region_variable_ohe","cd40b950":"processed_data = data.drop(['region'],axis=1)","292a749e":"processed_data","0d1da31d":"region_variable_df = pd.DataFrame(data=region_variable_ohe,columns=['Region 0', 'Region 1','Region 2','Region 3'])\n","3a74c411":"region_variable_df","e39cc45f":"processed_data = pd.concat([processed_data,region_variable_df],axis=1)","dd8f5837":"processed_data","5bd9227e":"gt_labels = processed_data['charges']\n\nprocessed_data.drop(['charges'],axis=1,inplace=True)","c1e279c7":"processed_data['charges'] = gt_labels","9d21bfde":"processed_data","91bde6b1":"Y = np.array(processed_data['charges']).reshape(processed_data.shape[0],1)\nX_transpose = np.array(processed_data.drop(['charges'],axis=1))","7484c285":"Y.shape","50e9abac":"X_transpose.shape","71f336fc":"X_transpose = (X_transpose - np.mean(X_transpose,axis=0))\/np.std(X_transpose,axis=0)","8f151547":"X_transpose_df = pd.DataFrame(data=X_transpose)","4b63735e":"X_transpose_df","8f22de72":"X_transpose_train = X_transpose[0:int(0.7*X_transpose.shape[0])]\nX_transpose_cv = X_transpose[int(0.7*X_transpose.shape[0]):int(0.9*X_transpose.shape[0])]\nX_transpose_test = X_transpose[int(0.9*X_transpose.shape[0]):]","80135861":"Y_train = (Y[0:int(0.7*X_transpose.shape[0])] - np.mean(Y[0:int(0.7*X_transpose.shape[0])]))\/np.std(Y[0:int(0.7*X_transpose.shape[0])])\nY_cv = Y[int(0.7*X_transpose.shape[0]):int(0.9*X_transpose.shape[0])]\nY_test = Y[int(0.9*X_transpose.shape[0]):]","74a1a272":"N_train = X_transpose_train.shape[0]\nm = X_transpose_train.shape[1]","58e3284d":"def predicted_answers(theta0,theta,X_transpose):\n    return theta0 + np.matmul(X_transpose,theta)","d7aa2ad6":"def neg_log_loss(labels,predicted_labels,theta,reg_strength):\n    E = labels - predicted_labels\n    return (1\/labels.shape[0])*np.matmul(E.T,E) + (reg_strength*(np.linalg.norm(theta))**2\/2)","063d435c":"def fit(step_size,reg_strength):\n    theta0_initial = 0\n    theta_initial = np.zeros((m,1))\n    epsilon = step_size\n    tol = 10**(-6)\n    iterations = list()\n    neg_log_loss_history = list()\n    iteration_number = 0\n    mini_batch_size = 72\n    time_steps = N_train\/\/mini_batch_size\n    epoch_counter = 0 \n    invalid_value_flag = 0 \n    \n    while(True):\n        \n        for i in range(0,time_steps):\n            rand_indices = np.random.choice(a=np.arange(0,N_train),size=mini_batch_size,replace=False)\n            X_transpose_mini_batch = X_transpose_train[rand_indices]\n            train_labels_mini_batch = Y_train[rand_indices]\n            \n            Predicted_answers_initial = predicted_answers(theta0_initial,theta_initial,\n                                                          X_transpose_mini_batch)\n            theta0_final = theta0_initial - epsilon*np.mean(Predicted_answers_initial-train_labels_mini_batch)\n            theta_final = theta_initial - epsilon*((1\/mini_batch_size)*np.matmul(X_transpose_mini_batch.T,\n                                                                                (Predicted_answers_initial-train_labels_mini_batch))+(reg_strength*theta_initial))\n            \n            neg_log_loss_initial = neg_log_loss(train_labels_mini_batch,Predicted_answers_initial,\n                                                theta_initial,reg_strength)\n            \n            Predicted_answers_final = predicted_answers(theta0_final,theta_final,X_transpose_mini_batch)\n            neg_log_loss_final = neg_log_loss(train_labels_mini_batch,Predicted_answers_final,\n                                             theta_final,reg_strength)\n            \n            if np.isnan(neg_log_loss_final[0][0]) == True:\n                invalid_value_flag = 1\n                break\n                \n            theta0_initial = theta0_final\n            theta_initial = theta_final\n            \n            iterations.append(iteration_number)\n            neg_log_loss_history.append(neg_log_loss_initial[0][0])\n            \n            iteration_number = iteration_number + 1\n            \n            #print(\"Iterations =\",iteration_number,\"Mean Squared Error Loss =\",neg_log_loss_initial)\n            \n        epoch_counter = epoch_counter + 1\n        \n        #print(\"\\n\\nEpochs =\",epoch_counter,\"Mean Squared Error Loss =\",neg_log_loss_initial,\"\\n\\n\")\n\n        \n        if abs(neg_log_loss_initial - neg_log_loss_final) < tol or invalid_value_flag == 1:\n            print(\"\\n\\nEpochs =\",epoch_counter,\"Mean Squared Error Loss =\",neg_log_loss_initial,\"\\n\\n\")\n            break\n            \n    return {(step_size,reg_strength):[theta0_final,theta_final]}","9d649c33":"def compute_mse(labels,predicted_labels):\n    E = (labels - predicted_labels)\n    return (1\/labels.shape[0])*np.matmul(E.T,E)","006a4036":"def rmse(labels,trained_theta0,trained_theta,X_transpose):\n    predicted_labels = predicted_answers(trained_theta0,trained_theta,X_transpose)\n    return np.sqrt(compute_mse(labels,predicted_labels))","c1b0527a":"grid_search_results = list()\nfor step_size in [0.0000001,0.000001,0.00001]:\n    for reg_hyp_param in [0.0001,0.001,0.01,0.1,1,10,100]:\n        grid_search_results.append(fit(step_size,reg_hyp_param))","0ed7a852":"Y_cv = (Y_cv - np.mean(Y_cv))\/np.std(Y_cv)","21b3a80f":"Y_cv","3068b2ff":"grid_search_cv_results = dict()\nfor configs in grid_search_results:\n    trained_params = list(configs.values())\n    grid_search_cv_results[tuple(configs.keys())] = rmse(Y_cv,trained_params[0][0],\n                                                        trained_params[0][1],X_transpose_cv)","a635b637":"grid_search_cv_results","c2fead4d":"grid_search_train_results = dict()\nfor configs in grid_search_results:\n    trained_params = list(configs.values())\n    grid_search_train_results[tuple(configs.keys())] = rmse(Y_train,trained_params[0][0],\n                                                        trained_params[0][1],X_transpose_train)","5ef5832d":"grid_search_train_results","9f728c88":"random_search_results = list()\nfor i in range(200):\n    step_size = np.random.uniform(low=10**(-7),high=10**(-4))\n    reg_hyp_param = np.random.uniform(low=10**(-4),high=10**(3))\n    random_search_results.append(fit(step_size,reg_hyp_param))","d90f651b":"random_search_results","1d2b38c9":"random_search_train_results = dict()\nfor configs in random_search_results:\n    trained_params = list(configs.values())\n    random_search_train_results[tuple(configs.keys())] = rmse(Y_train,trained_params[0][0],\n                                                        trained_params[0][1],X_transpose_train)","c9b5d347":"random_search_train_results","3b5d820a":"random_search_cv_results = dict()\nfor configs in random_search_results:\n    trained_params = list(configs.values())\n    random_search_cv_results[tuple(configs.keys())] = rmse(Y_cv,trained_params[0][0],\n                                                        trained_params[0][1],X_transpose_cv)","7d1fb0d3":"random_search_cv_results","4eedc846":"random_search_results = list()\nfor i in range(10):\n    step_size = np.random.uniform(low=10**(-5),high=9*(10**(-5)))\n    reg_hyp_param = np.random.uniform(low=10**(-2),high=9*(10**(-2)))\n    random_search_results.append(fit(step_size,reg_hyp_param))","aede712b":"random_search_cv_results = dict()\nfor configs in random_search_results:\n    trained_params = list(configs.values())\n    random_search_cv_results[tuple(configs.keys())] = rmse(Y_cv,trained_params[0][0],\n                                                        trained_params[0][1],X_transpose_cv)","f0538bda":"random_search_train_results = dict()\nfor configs in random_search_results:\n    trained_params = list(configs.values())\n    random_search_train_results[tuple(configs.keys())] = rmse(Y_train,trained_params[0][0],\n                                                        trained_params[0][1],X_transpose_train)","18aca2b2":"random_search_train_results","57a59bf0":"random_search_cv_results","cbfe4b18":"# Train Linear Regression in Mini Batch "}}