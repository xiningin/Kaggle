{"cell_type":{"675c80a9":"code","bd7ddaf1":"code","86507faf":"code","c2198a04":"code","95fb9285":"code","8791d7f5":"code","ecd24eda":"code","2c1b3208":"code","f31da75e":"code","b551f15c":"code","7854b5b6":"code","cb043f40":"code","09185298":"code","108122c0":"code","d68a5dc9":"code","bda0a386":"code","7c172af5":"code","7a7fde8e":"code","8e89b7bd":"code","9b3019c3":"code","077a3a31":"code","326acd81":"code","5c830049":"code","86a729c8":"code","6c71da62":"code","17f85611":"code","d4dc78bd":"code","a90673b7":"code","84bb093c":"code","59b37980":"code","b6c20928":"code","bd8514d3":"code","f442dbc1":"code","23ff29dc":"code","9ee2c1f9":"code","e2e07e33":"code","ab232c15":"code","538a82f9":"code","cef43efd":"code","3963ba77":"code","c4a7667a":"code","c2d5edd9":"code","e7b7eaac":"code","882486a8":"code","46a7df0c":"markdown","0078e931":"markdown","c6c3f486":"markdown","8e298f01":"markdown","ad0201cd":"markdown"},"source":{"675c80a9":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns","bd7ddaf1":"# first we will read the data : \n\ndf = pd.read_csv('..\/input\/employee-future-prediction\/Employee.csv')\ndf.head().style.background_gradient(axis = 0)","86507faf":"# There is no NAN values on the data we have , also the below code will show that there is no NAN . \nplt.figure(figsize=(12,8))\nsns.heatmap(df.isnull() , cbar = False)","c2198a04":"df.isnull().sum()","95fb9285":"# Checking the Dtype :\ndf.info()","8791d7f5":"# Transform data to catgorical variable :\n\ndf = pd.DataFrame(df)\n\ndf['Education'] = df['Education'].astype('category')\ndf['City'] = df['City'].astype('category')\ndf['Gender'] = df['Gender'].astype('category')\ndf['EverBenched'] = df['EverBenched'].astype('category')","ecd24eda":"# Now we are going to handle the object data : \ndf['Education'] = df['Education'].cat.codes\ndf['City'] = df['City'].cat.codes\ndf['Gender'] = df['Gender'].cat.codes\ndf['EverBenched'] = df['EverBenched'].cat.codes","2c1b3208":"# Now data has been transformed , and now there is no object data : \ndf.head().style.background_gradient(axis=0)","f31da75e":"sns.pairplot(data = df , hue = 'LeaveOrNot')","b551f15c":"sns.boxplot(data = df , x = 'LeaveOrNot' , y ='Age')","7854b5b6":"sns.boxplot(data = df , x = 'LeaveOrNot' , y ='ExperienceInCurrentDomain')","cb043f40":"plt.figure(figsize=(6,3),dpi = 150)\nsns.scatterplot(data = df , x = 'ExperienceInCurrentDomain' , y ='Age' , hue ='LeaveOrNot')","09185298":"plt.figure(figsize=(6,3),dpi = 150)\nsns.scatterplot(data = df , x = 'JoiningYear' , y ='ExperienceInCurrentDomain' , hue ='LeaveOrNot')","108122c0":"plt.figure(figsize=(6,3),dpi = 150)\nsns.scatterplot(data = df , x = 'JoiningYear' , y ='Age' , hue ='LeaveOrNot')","d68a5dc9":"plt.figure(figsize=(6,3),dpi = 150)\nsns.heatmap(df.corr(),annot=True)","bda0a386":"# Now as we see there is imbalnce data issue , so we have to solve it before fitting data : \nsns.countplot(data = df , x = 'LeaveOrNot')","7c172af5":"# Now we are going to divid the data to X and y : \nX = df.drop('LeaveOrNot' , axis = 1)\ny = df['LeaveOrNot']","7a7fde8e":"from imblearn import over_sampling , under_sampling\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_resambled , y_resampled = ros.fit_resample(X,y)","8e89b7bd":"# Split the data to train and test data : \nfrom sklearn.model_selection import train_test_split","9b3019c3":"X_train, X_test, y_train, y_test = train_test_split(X_resambled, y_resampled, test_size=0.3, random_state=101)","077a3a31":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","326acd81":"scaled_x_train = scaler.fit_transform(X_train)\nscaled_x_test  = scaler.transform(X_test)","5c830049":"from sklearn.linear_model import LogisticRegression","86a729c8":"log_model = LogisticRegression()\nlog_model.fit(scaled_x_train,y_train)","6c71da62":"# Now we will import metrics that through it we will know if the model is working good or not : \n\nfrom sklearn.metrics import accuracy_score , classification_report , plot_confusion_matrix , plot_precision_recall_curve , plot_roc_curve","17f85611":"def Needed_Metrics (model):\n    y_pred = model.predict(scaled_x_test)\n    print('\/n')\n    print(classification_report(y_test,y_pred))\n    print('\/n')\n    plot_confusion_matrix(model,scaled_x_test,y_test)\n    plot_roc_curve(model,scaled_x_test,y_test)\n    plot_precision_recall_curve(model,scaled_x_test,y_test)","d4dc78bd":"Needed_Metrics(log_model)","a90673b7":"from sklearn.neighbors import KNeighborsClassifier","84bb093c":"# now we are going to investigate what is the best K value , so we are going to make fore loop to get the best K value : \n\ntest_error_rate = []\n\nfor k in range(1,30):\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    knn_model.fit(scaled_x_train , y_train)\n    \n    y_pred = knn_model.predict(scaled_x_test)\n    \n    error_rate = 1 - accuracy_score(y_test,y_pred)\n    \n    test_error_rate.append(error_rate)","59b37980":"plt.figure(figsize=(6,3),dpi = 150)\nplt.plot(range(1,30), test_error_rate)\nplt.xlabel('K Value')\nplt.ylabel('Test Error Rate')\nplt.title('Choose the best K Value')","b6c20928":"knn_model = KNeighborsClassifier(n_neighbors=1)\nknn_model.fit(scaled_x_train,y_train)","bd8514d3":"Needed_Metrics(knn_model)","f442dbc1":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV","23ff29dc":"param_grid = {\"C\":[0.001,0.01,1]}\nsvc = SVC()","9ee2c1f9":"grid_model = GridSearchCV(svc,param_grid)","e2e07e33":"grid_model.fit(scaled_x_train,y_train)","ab232c15":"grid_model.best_params_","538a82f9":"Needed_Metrics(grid_model)","cef43efd":"from sklearn.tree import DecisionTreeClassifier","3963ba77":"tree_model = DecisionTreeClassifier()\ntree_model.fit(scaled_x_train,y_train)","c4a7667a":"Needed_Metrics(tree_model)","c2d5edd9":"tree_model.feature_importances_","e7b7eaac":"X.columns","882486a8":"pd.DataFrame(index = X.columns , data = tree_model.feature_importances_ , columns = ['feature importance']).sort_values('feature importance').style.background_gradient(axis=0)","46a7df0c":"**Decision_tree Algorithm******","0078e931":"**Logistic_Regression Algorithm ******","c6c3f486":"Apply Needed_Metrics () Function To Get Metrics :","8e298f01":"**KNN_Algorithm **","ad0201cd":"**SVM Algorithm : **"}}