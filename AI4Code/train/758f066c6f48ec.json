{"cell_type":{"e03c1d2e":"code","843814ca":"code","4301c566":"code","bf23b97d":"code","04dc0154":"code","f8d78481":"code","40b857c8":"code","5e63ee1c":"code","e772caa1":"code","5d0bc7d1":"code","abfc2574":"code","c0c5a7c4":"code","91f162fe":"code","c80baa4c":"code","8e19e9ce":"code","bf8133ce":"code","3d98fcae":"code","6089f893":"code","ff2a86c8":"code","a63b4110":"markdown","c6a5e6e9":"markdown","0e2157ad":"markdown","e8d1017e":"markdown"},"source":{"e03c1d2e":"%matplotlib inline\n\nimport xgboost as xgb\nxgb.__version__","843814ca":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nimport shap\n\n# load JS visualization code to notebook\nshap.initjs()","4301c566":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bf23b97d":"train = pd.read_csv('..\/input\/ncaaw-2021-features-only\/tourney.csv')\ntest = pd.read_csv('..\/input\/ncaaw-2021-features-only\/test.csv')","04dc0154":"y = train[\"result\"]\ns = train[\"Season\"]\nX = train.drop(['Season','TeamID1','TeamID2','result'], axis=1)\n\n\nX_test = test.drop(['ID', 'Season','TeamID1','TeamID2'], axis=1)","f8d78481":"train_oof = np.zeros((X.shape[0],))\ntest_preds = 0\ntrain_oof.shape","40b857c8":"xgb_params= {\n        \"objective\": \"binary:logistic\",\n        \"max_depth\": 2,\n        \"learning_rate\": 0.01,\n        \"colsample_bytree\": 0.8,\n        \"subsample\": 0.9,\n        #\"reg_alpha\" : 0,\n        \"min_child_weight\": 30,\n        \"n_jobs\": 2,\n        \"seed\": 2001,\n        'tree_method': \"gpu_hist\",\n        \"gpu_id\": 0,\n        'predictor': 'gpu_predictor'\n    }","5e63ee1c":"test = xgb.DMatrix(X_test)","e772caa1":"train_oof = np.zeros((X.shape[0],))\ntest_preds = 0\ntrain_oof.shape","5d0bc7d1":"NUM_FOLDS = 10\nkf = GroupKFold(n_splits=NUM_FOLDS)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(X, y, s))):\n        #print(f'Fold {f}')\n        train_df, val_df = X.iloc[train_ind], X.iloc[val_ind]\n        train_target, val_target = y.iloc[train_ind], y.iloc[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        model =  xgb.train(xgb_params, train_df, 400)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(log_loss(val_target, temp_oof))","abfc2574":"log_loss(y, train_oof)","c0c5a7c4":"np.save('train_oof', train_oof)\nnp.save('test_preds', test_preds)","91f162fe":"%%time\nshap_preds = model.predict(test, pred_contribs=True)","c80baa4c":"# summarize the effects of all the features\nshap.summary_plot(shap_preds[:,:-1], X_test)","8e19e9ce":"shap.summary_plot(shap_preds[:,:-1], X_test, plot_type=\"bar\")","bf8133ce":"%%time\nshap_interactions = model.predict(test, pred_interactions=True)","3d98fcae":"def plot_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (feature_names[i] + \"-\" + feature_names[j], aggregate_interactions[i][j] * 2))\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n    plt.bar(interaction_features[:k], interaction_values[:k])\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    \nplot_top_k_interactions(X_test.columns, shap_interactions, 10)","6089f893":"test = pd.read_csv('..\/input\/ncaaw-2021-features-only\/test.csv')\nWSampleSubmission = pd.read_csv('..\/input\/ncaaw-march-mania-2021\/WSampleSubmissionStage1.csv')","ff2a86c8":"idx = test_preds.shape[0] \/\/2\ntest_preds[idx:] = 1 - test_preds[idx:]\n\npred = pd.concat([test.ID, pd.Series(test_preds)], axis=1).groupby('ID')[0]\\\n        .mean().reset_index().rename(columns={0:'Pred'})\nsub = WSampleSubmission.drop(['Pred'],axis=1).merge(pred, on='ID')\nsub.to_csv('submission.csv', index=False)\nsub.head()","a63b4110":"Now we look at feature interactions.","c6a5e6e9":"Now let's make a submission.","0e2157ad":"In this notebook we'll explore feature importance using SHAP values. SHAP values are the most mathematically consistent way for getting feature importances, and they work particulalry nicely with the tree-based models. Unfortunately, calculating SHAP values is an extremely resource intensive process. However, starting with XGBoost 1.3 it is possible to calcualte these values on GPUs, whcih speeds up the process by a factor of 20X - 50X compared to calculating the same on a CPU. Furthermore, it is also possible to calculate SHAP values for feature interactions. The GPU speedup for those is even more dramatic - it takes a few minutes, as opposed to days or even longer on a CPU.\n","e8d1017e":"Next, we calculate the SHAP values for the test set.\n"}}