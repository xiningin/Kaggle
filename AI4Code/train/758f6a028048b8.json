{"cell_type":{"5339941f":"code","3c3819ad":"code","bc383c85":"code","1cbe30eb":"code","635878f9":"code","fda438a3":"code","150cb0ed":"code","b5a55784":"code","decc7a88":"code","9a70660f":"code","13cc2031":"code","01d143e4":"code","cc98a4f0":"code","def83fda":"code","32db29c0":"code","48ac490b":"code","7cc5cdbb":"code","bf6f4992":"code","9114a539":"code","d9b8d253":"code","b0b3299f":"code","49830c39":"code","f613106a":"code","beef7b87":"code","7cc28f83":"code","e38b5604":"code","a2b76f51":"code","1d028f9c":"code","cbf89cf0":"code","45a1f031":"code","94582dd1":"code","1520ca47":"code","efe68840":"markdown","802e9b12":"markdown","24c7842d":"markdown","a26036e0":"markdown","32dbbc8d":"markdown","63a3ee7b":"markdown","b3e36763":"markdown","0b9710ef":"markdown","f495659b":"markdown","f270593a":"markdown","2d9f8edb":"markdown","d749210c":"markdown"},"source":{"5339941f":"!pip install tweet-preprocessor 2>\/dev\/null 1>\/dev\/null","3c3819ad":"!pip install pyyaml h5py  2>\/dev\/null 1>\/dev\/null","bc383c85":"import preprocessor as p\nimport numpy as np \nimport pandas as pd \nimport emoji\nimport keras\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\nfrom tqdm import tqdm","1cbe30eb":"misspell_data = pd.read_csv(\"\/kaggle\/input\/spelling\/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\nmisspell_data.misspell = misspell_data.misspell.str.strip()\nmisspell_data.misspell = misspell_data.misspell.str.split(\" \")\nmisspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\nmisspell_data.drop_duplicates(\"misspell\",inplace=True)\nmiss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))\n\n#Sample of the dict\n{v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]}","635878f9":"def misspelled_correction(val):\n    for x in val.split(): \n        if x in miss_corr.keys(): \n            val = val.replace(x, miss_corr[x]) \n    return val\n\n","fda438a3":"print(misspelled_correction('abouy => about'))","150cb0ed":"contractions = pd.read_csv(\"\/kaggle\/input\/contractions\/contractions.csv\")\ncont_dic = dict(zip(contractions.Contraction, contractions.Meaning))","b5a55784":"print(contractions.head())","decc7a88":"def cont_to_meaning(val): \n  \n    for x in val.split(): \n        if x in cont_dic.keys(): \n            val = val.replace(x, cont_dic[x]) \n    return val\n","9a70660f":"print(\"I'm => I am\")","13cc2031":"p.set_options(p.OPT.MENTION, p.OPT.URL)\np.clean(\"hello guys @alx #sport\ud83d\udd25 1245 https:\/\/github.com\/s\/preprocessor\")","01d143e4":"def punctuation(val): \n  \n    punctuations = '''()-[]{};:'\"\\,<>.\/@#$%^&_~'''\n  \n    for x in val.lower(): \n        if x in punctuations: \n            val = val.replace(x, \" \") \n    return val\n","cc98a4f0":"punctuation(\"test @ #ldfldlf??? !! \")","def83fda":"def clean_text(val):\n    val = misspelled_correction(val)\n    val = cont_to_meaning(val)\n    val = p.clean(val)\n    val = ' '.join(punctuation(emoji.demojize(val)).split())\n    \n    return val","32db29c0":"clean_text(\"isn't \ud83d\udca1 adultry @ttt good bad ... ! ? \")","48ac490b":"test_list = [1,7,10,7]\n\nprint(\"original list\", test_list)\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(test_list)\n\nprint(\"after label encoder fit_transform\", integer_encoded)\n\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nY = onehot_encoder.fit_transform(integer_encoded)\n\nprint(\"After onehot encoder fit_transform\\n\", Y)\n","7cc5cdbb":"def get_Y(data):\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(data)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    Y = onehot_encoder.fit_transform(integer_encoded)\n    return Y","bf6f4992":"train_text = pd.read_csv('\/kaggle\/input\/tweet-emotion\/emotion\/train_text.txt', sep='\\n', names=['text'])\ntrain_label = pd.read_csv('\/kaggle\/input\/tweet-emotion\/emotion\/train_labels.txt', sep='\\n', names=['label'])\ntrain = pd.DataFrame()\ntrain['text'] = train_text.text\ntrain['label'] = train_label.label\n\ntest_text = pd.read_csv('\/kaggle\/input\/tweet-emotion\/emotion\/test_text.txt', sep='\\n', names=['text'])\ntest_label = pd.read_csv('\/kaggle\/input\/tweet-emotion\/emotion\/test_labels.txt', sep='\\n', names=['label'])\ntest = pd.DataFrame()\ntest['text'] = test_text.text\ntest['label'] = test_label.label\n\n# clean data\ntrain.text = train.text.apply(clean_text)\ntest.text = test.text.apply(clean_text)\n\n# drop the data which is optimism\ntrain = train[train.label != 2]\ntest = test[test.label != 2]\n\n\nX_train = train.text\nX_test = test.text\ny_train = get_Y(train.label)\ny_test = get_Y(test.label)\n\n\n\nmax_len = 252\nEpoch = 15\nbatch_size = 32","9114a539":"print(X_train, X_test, y_train, y_test)","d9b8d253":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])\n\ndef build_model(transformer, max_len=160):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(3, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","b0b3299f":"AUTO = tf.data.experimental.AUTOTUNE\nMODEL = 'roberta-base'","49830c39":"tokenizer = AutoTokenizer.from_pretrained(MODEL)","f613106a":"X_train_t = regular_encode(X_train, tokenizer, maxlen=max_len)\nX_test_t = regular_encode(X_test, tokenizer, maxlen=max_len)","beef7b87":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_t, y_train))\n    .repeat()\n    .shuffle(1995)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_test_t, y_test))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\n","7cc28f83":"transformer_layer = TFAutoModel.from_pretrained(MODEL)\nmodel_roberta_base = build_model(transformer_layer, max_len=max_len)\nhistory = model_roberta_base.summary()","e38b5604":"import os\nif 'training' not in os.listdir('.'):\n    os.mkdir('training')\ncheckpoint_path = \"training\/cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\nos.listdir('.')\nos.listdir(checkpoint_dir)","a2b76f51":"n_steps = X_train.shape[0] \/\/ batch_size\nhistory = model_roberta_base.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=Epoch, callbacks=[cp_callback])","1d028f9c":"print(history.history)","cbf89cf0":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.legend()\nplt.title('Loss')\nplt.xlabel('Epoce')\nplt.ylabel('loss')","45a1f031":"plt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='validation')\nplt.legend()\nplt.title('Accuracy')\nplt.xlabel('Epoce')\nplt.ylabel('acc')","94582dd1":"sent_to_id = {'anger': 0, 'joy':1, 'sadness': 2}\ndef get_sentiment2(model,text):\n    text = clean_text(text)\n    #tokenize\n    x_test1 = regular_encode([text], tokenizer, maxlen=max_len)\n    test1 = (tf.data.Dataset.from_tensor_slices(x_test1).batch(1))\n    #test1\n    sentiment = model.predict(test1,verbose = 0)\n    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]\n    result = pd.DataFrame([sent_to_id.keys(),sent]).T\n    result.columns = [\"sentiment\",\"percentage\"]\n    result=result[result.percentage !=0]\n    return result\n\ndef plot_result(result):\n    print(result)","1520ca47":"result =get_sentiment2(model_roberta_base,\"Had an absolutely brilliant day \u00f0\u0178\u02dc\u0081 loved seeing an old friend and reminiscing\")\nplot_result(result)\nresult =get_sentiment2(model_roberta_base,\"The pain my heart feels is just too much for it to bear. Nothing eases this pain. I can\u2019t hold myself back. I really miss you\")\nplot_result(result)\nresult =get_sentiment2(model_roberta_base,\"I hate this game so much,It make me angry all the time \")\nplot_result(result)","efe68840":"### Misspelled data <a class=\"anchor\" id=\"dp-md\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","802e9b12":"<a href=\"https:\/\/www.linkedin.com\/in\/ouassim-adnane\/\">Ouassim Adnane<\/a> 08 June 2020","24c7842d":"### Encoding the data and train test split <a class=\"anchor\" id=\"m-ed\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","a26036e0":"# Tweet Emotions Analysis (12 emotions) <a class=\"anchor\" id=\"tea\"><\/a>","32dbbc8d":"### Punctuations and emojis <a class=\"anchor\" id=\"dp-p\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","63a3ee7b":"<img src=\"https:\/\/www.feelingfacescards.com\/images\/feeling_faces_chart_poster.jpg\" \/>","b3e36763":"### Contractions <a class=\"anchor\" id=\"dp-c\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","0b9710ef":"# Modeling  <a class=\"anchor\" id=\"m\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","f495659b":"### Test Roberta Model Results <a class=\"anchor\" id=\"m-rbr\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","f270593a":"### Roberta Base Model <a class=\"anchor\" id=\"m-rb\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","2d9f8edb":"# Data preparation  <a class=\"anchor\" id=\"dp\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>","d749210c":"### Remove URLS and mentions <a class=\"anchor\" id=\"dp-r\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >Back to the table of contents<\/a>"}}