{"cell_type":{"9048aee7":"code","5ac4ee1a":"code","a98a985b":"code","51967463":"code","d23caeda":"code","9cc78d41":"code","af8dd14a":"code","487e4a4c":"code","ea9f33bb":"code","7bead5d1":"code","a58953ab":"markdown","b360ae29":"markdown"},"source":{"9048aee7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.datasets import make_moons,make_circles,make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ac4ee1a":"random_state=42\nn_samples=10000\nn_features=20\nn_classes=2\nnoise_class=0.1\nn_estimators=100\nnoise_moon=0.1\nnoise_circle=0.1\nmax_depth=2\n\n#You can see the differences in the results by changing the parameters\n\nX,y=make_classification(n_samples=n_samples,\n                   n_features=n_features,\n                   n_classes=n_classes,\n                   n_repeated=0,\n                   n_redundant=0,\n                   n_informative=n_features-1,\n                   random_state=random_state,\n                   flip_y=noise_class,\n                   n_clusters_per_class=1)","a98a985b":"data=pd.DataFrame(X)\ndata['target']=y\nplt.figure()\nsns.scatterplot(x=data.iloc[:,0],y=data.iloc[:,1],hue='target',data=data)\nplt.show()","51967463":"data_classification=(X,y)","d23caeda":"\nmoon=make_moons(n_samples=n_samples,noise=noise_moon,random_state=random_state)\ndata=pd.DataFrame(moon[0])\ndata['target']=moon[1]\nsns.scatterplot(x=data.iloc[:,0],y=data.iloc[:,1],hue='target',data=data)\nplt.show()\n","9cc78d41":"\ncircle=make_circles(n_samples=n_samples,noise=noise_circle,random_state=random_state,factor=0.1)\ndata=pd.DataFrame(circle[0])\ndata['target']=circle[1]\nsns.scatterplot(x=data.iloc[:,0],y=data.iloc[:,1],hue='target',data=data)\nplt.show()\n","af8dd14a":"datasets=[moon,circle]","487e4a4c":"svc=SVC()\nknn=KNeighborsClassifier(n_neighbors=15)\ndt=DecisionTreeClassifier(random_state=random_state,max_depth=max_depth)\nrf=RandomForestClassifier(n_estimators=n_estimators,random_state=random_state)\nada=AdaBoostClassifier(base_estimator=dt,n_estimators=n_estimators,random_state=random_state)\nv1=VotingClassifier(estimators=[('svc',svc),('knn',knn),('dt',dt),('rf',rf),('ada',ada)])\nnames=['SVC','KNN','Decision Tree','Random Forest','Ada Boost','v1']\nclassifiers=[svc,knn,dt,rf,ada,v1]","ea9f33bb":"h=0.2\ni=1\nfigure=plt.figure(figsize=(18,6))\nfor ds_cnt,ds in enumerate(datasets):\n    X,y=ds\n    X=RobustScaler().fit_transform(X)\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.4,random_state=random_state)\n    x_min,x_max=X[:,0].min()-.5,X[:,0].max()+.5\n    y_min,y_max=X[:,1].min()-.5,X[:,1].max()+.5\n    xx,yy=np.meshgrid(np.arange(x_min,x_max,h),\n                     np.arange(y_min,y_max,h))\n    \n    cm=plt.cm.RdBu\n    cm_bright=ListedColormap(['#FF0000','#0000FF'])\n    ax=plt.subplot(len(datasets),len(classifiers)+1,i)\n    \n    if ds_cnt==0:\n        ax.set_title('input data')\n    ax.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap=cm_bright,edgecolors='k')\n    ax.scatter(X_test[:,0],X_test[:,1],c=y_test,cmap=cm_bright,alpha=0.5,marker='^',edgecolors='k')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i+=1\n    print('Dataset: {}'.format(ds_cnt))\n    \n    for name, clf in zip(names,classifiers):\n        ax=plt.subplot(len(datasets),len(classifiers)+1,i)\n        clf.fit(X_train,y_train)\n        score=clf.score(X_test,y_test)\n        print('{} test score : {}'.format(name,score))\n        score_train=clf.score(X_train,y_train)\n        print('{} train score : {}'.format(name,score_train))\n        print()\n        \n        if hasattr(clf,'decision_function'):\n            Z=clf.decision_function(np.c_[xx.ravel(),yy.ravel()])\n        else:\n            Z=clf.predict(np.c_[xx.ravel(),yy.ravel()])\n        \n        Z=Z.reshape(xx.shape)\n        ax.contourf(xx,yy,Z,cmap=cm,alpha=.8)\n        \n        ax.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap=cm_bright,edgecolors='k')\n        ax.scatter(X_test[:,0],X_test[:,1],c=y_test,cmap=cm_bright,alpha=0.6,marker='^',edgecolors='white')\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt==0:\n            ax.set_title(name)\n        score=score*100\n        ax.text(xx.max()-.3,yy.min()+.3,('%.1f'%score),size=15,horizontalalignment='right')\n        i+=1\n    print('----------------------------------------------')\nplt.tight_layout()\nplt.show()\n        \n            \n    ","7bead5d1":"def make_classify(dc,clf,name):\n    x,y=dc\n    x=RobustScaler().fit_transform(x)\n    X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=.4,random_state=random_state)\n    \n    for name, clf in zip(names,classifiers):\n        clf.fit(X_train,y_train)\n        score=clf.score(X_test,y_test)\n        print('{} test score: {}'.format(name,score))\n        score_train=clf.score(X_train,y_train)\n        print('{} train score: {}'.format(name,score_train))\n        print()\nprint('Dataset #2')\nmake_classify(data_classification,classifiers,names)","a58953ab":"## Create Data","b360ae29":"##  Basic Classifiers: KNN, SVM, DT"}}