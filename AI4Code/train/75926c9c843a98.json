{"cell_type":{"c6b5b061":"code","55e4f5aa":"code","02d1fe96":"code","888c3804":"code","75def6b2":"code","fb73a8f4":"code","df010ef2":"code","0499d45b":"code","45788b7f":"code","21a237ef":"code","6a5d9890":"code","c4794a77":"code","eb4501d6":"code","1045b70a":"code","091049a1":"code","e34ca07d":"code","91a5b3e6":"code","4e71ac0c":"code","4cd4f00c":"code","8a22d77c":"code","0dcf3a1a":"code","3fa59272":"code","0f45545f":"code","31e56c01":"code","fe1f40f7":"code","100f3856":"code","a2f8b37d":"code","92ea0842":"code","3b8afc0e":"code","77e2010b":"code","b0c07506":"code","c8bc0d2a":"code","d1560dc7":"code","35ad0e8f":"code","4c0c036a":"code","dfd096aa":"code","920fa0e3":"code","1efbb096":"code","a6eaa27e":"code","2dca3d3b":"code","efeae6e0":"code","cf94d0ea":"code","39056490":"code","76a66b4e":"code","a2d0a020":"code","64a5dea5":"code","824bf4fa":"code","1e0be273":"code","bafc271f":"code","59ac38c6":"code","6ce58fe9":"markdown","58709c6e":"markdown","1785954c":"markdown","9a79cb1c":"markdown","34609858":"markdown","06d79505":"markdown","b0184588":"markdown","17d54b3e":"markdown","ba75d81f":"markdown","0f6b9365":"markdown","a2ebd41a":"markdown","004ef6a1":"markdown"},"source":{"c6b5b061":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport plotly\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport matplotlib.pyplot as plt\nimport pylab as pl\nsns.set(font_scale = 0.7)\nimport os\nprint(os.listdir(\"..\/input\"))\n","55e4f5aa":"#Cargamos el data set mediante Python y Pandas mediante read_csv\n#Tener en cuenta indicadar el separador como recomendacion con delimiter se recomienda\n#En caso tengamos un campo donde se guarden los ID unico podemos caolocar elnumero de columna en index_col para usarlo de indexacion\ndata = pd.read_csv(\"..\/input\/avocado.csv\",delimiter=\",\",index_col=0)\n\n#Visualizamos el volumen de filas y columnas de nuestros datos\ndata.shape","02d1fe96":"#Podemos mediante el comando info identificar los tipos de campos 123\ndata.info()","888c3804":"#Vamos a ver un peque\u00f1o resumen de los datos\ndata.head(5)","75def6b2":"#Vamos a validar que se cumple ambos formulas\n#Todo aquel registro que no cumpla con la regla pasara a ser una inconsistencia la cual descartaremos.\ndataIncorrecta = data[((data['4046'] + data['4225'] + data['4770'] + data['Total Bags']) != data['Total Volume'] ) | ((data['Small Bags'] + data['Large Bags'] + data['XLarge Bags']) != data['Total Bags'] )]\ndataCorrecta =  data[((data['4046'] + data['4225'] + data['4770'] + data['Total Bags']) == data['Total Volume'] ) & ((data['Small Bags'] + data['Large Bags'] + data['XLarge Bags']) == data['Total Bags'] )]\n\ndataIncorrecta.reset_index(inplace = True)\ndataCorrecta.reset_index(inplace = True)\n\nprint(\"Data Total\",data.shape)\nprint(\"Data Incorrecta\",dataIncorrecta.shape)\nprint(\"Data Correcta\",dataCorrecta.shape)","fb73a8f4":"#Analizamos valores perdidos en caso de que existan\ndataCorrecta.isnull().sum()","df010ef2":"import itertools\nplt.subplots(figsize=(28,20))\ntime_spent=['Small Bags','Large Bags','XLarge Bags','XLarge Bags']\nlength=len(time_spent)\nfor i,j in itertools.zip_longest(time_spent,range(length)):\n    plt.subplot((length\/2),2,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    dataCorrecta[i].hist(bins=18,edgecolor='black')\n    plt.axvline(dataCorrecta[i].mean(),linestyle='dashed',color='r')\n    plt.title(i,size=20)\n    plt.xlabel('Tama\u00f1o de la bolsa')\n    plt.ylabel('cantidad de bolsas')\nplt.show()","0499d45b":"dataCorrecta =dataCorrecta[(dataCorrecta['Small Bags']<300000.0) & (dataCorrecta['Large Bags']<150000.0) & (dataCorrecta['XLarge Bags']<10000.0)]\ndataCorrecta.shape","45788b7f":"import itertools\nplt.subplots(figsize=(25,16))\ntime_spent=['Small Bags','Large Bags','XLarge Bags','XLarge Bags']\nlength=len(time_spent)\nfor i,j in itertools.zip_longest(time_spent,range(length)):\n    plt.subplot((length\/2),2,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    dataCorrecta[i].hist(bins=18,edgecolor='black')\n    plt.axvline(dataCorrecta[i].mean(),linestyle='dashed',color='r')\n    plt.title(i,size=20)\n    plt.xlabel('Tama\u00f1o de la bolsa')\n    plt.ylabel('cantidad de bolsas')\nplt.show()","21a237ef":"pl.figure(figsize=(12,5))\npl.title(\"Distribution Price\")\nax = sns.distplot(dataCorrecta[\"AveragePrice\"], color = 'g')","6a5d9890":"sns.boxplot(y=\"type\", x=\"AveragePrice\", data=dataCorrecta, palette = 'Set3')","c4794a77":"conventional = dataCorrecta[dataCorrecta.type==\"conventional\"]\norganic = dataCorrecta[dataCorrecta.type==\"organic\"]\n\ngroupBy1_price = conventional.groupby('Date').mean()\nscatter1 = go.Scatter(x=groupBy1_price.AveragePrice.index, y=groupBy1_price.AveragePrice, name=\"Conventional\")\n\ngroupBy2_price = organic.groupby('Date').mean()\nscatter2 = go.Scatter(x=groupBy2_price.AveragePrice.index, y=groupBy2_price.AveragePrice, name=\"Organic\")\n\ndata = [scatter1, scatter2]\nlayout=go.Layout(title=\"Time Series Plot for Mean Daily Price of Conventional and Organic Avocados\", xaxis={'title':'Date'}, yaxis={'title':'Prices'})\nfigure=go.Figure(data=data,layout=layout)\niplot(figure)","eb4501d6":"#dataCorrecta['Date2']=pd.to_datetime(dataCorrecta['Date'], format=\"%Y\/%m\/%d\")\ndataCorrecta['Date'] =dataCorrecta['Date'].astype('datetime64[ns]')\n\n","1045b70a":"#Tenemos que analizar los datos categoricos no numericos, para transformarlos en numericos, o ver si son utiles\ndataCorrecta['type'].value_counts()","091049a1":"dataCorrecta.head()","e34ca07d":"dataCorrecta['year'].value_counts()","91a5b3e6":"dataCorrecta['region'].value_counts()","4e71ac0c":"fig, ax = plt.subplots(1, 1, figsize=(10,6))\nsns.boxplot(x='year',y='AveragePrice',data=dataCorrecta,color='red')","4cd4f00c":"#Analizamos el comportamiento de los procesio por region de las paltas organicos\nmask = dataCorrecta['type']=='organic'\ng = sns.factorplot('AveragePrice','region',data=dataCorrecta[mask],\n                   hue='year',\n                   height=13,\n                   aspect=0.8,\n                   palette='magma',\n                   join=False,\n              )","8a22d77c":"#Analizamos el comportamiento de los procesio por region de las paltas convencionales\nmask = dataCorrecta['type']=='conventional'\ng = sns.factorplot('AveragePrice','region',data=dataCorrecta[mask],\n                   hue='year',\n                   height=13,\n                   aspect=0.8,\n                   palette='magma',\n                   join=False,\n              )","0dcf3a1a":"label = LabelEncoder()\ndicts = {}\n\nlabel.fit(dataCorrecta.type.drop_duplicates()) \ndicts['type'] = list(label.classes_)\ndataCorrecta.type = label.transform(dataCorrecta.type) ","3fa59272":"dataCorrecta['type'].value_counts()","0f45545f":"#Realizamos una tabla de correlacion, para conocer el nivel de relacion entre los campos y entre nuestro target\ncols = ['AveragePrice','Total Volume','4046','4225','4770','Total Bags','Small Bags','Large Bags','XLarge Bags','year','type']\nsns.set(font_scale = 1.5)\ncorr = dataCorrecta[cols].corr('spearman') \nplt.figure(figsize = ( 14 , 14 )) \nsns.heatmap(corr,annot=True,fmt='.2f',cmap=\"YlGnBu\");","31e56c01":"#Creamos las variables ficticias para region\nregion_dummi =pd.get_dummies(dataCorrecta['region'], prefix='reg')\nregion_dummi.head()","fe1f40f7":"#Agregamos los datos fictios a la data original\ndata_nueva = pd.concat([dataCorrecta, region_dummi], axis=1)\ndata_nueva.head() ","100f3856":"#eliminamos las columna region\ndata_nueva = data_nueva.drop('region', 1)\ndata_nueva = data_nueva.drop('index', 1)","a2f8b37d":"#corr = data_nueva.corr('spearman') \n#plt.figure(figsize = ( 35 , 20 )) \n#sns.heatmap(corr,annot=True,fmt='.2f',cmap=\"YlGnBu\");","92ea0842":"data_train = pd.DataFrame(index=data_nueva.index)\ntarger_train = pd.DataFrame(index=data_nueva.index)\ndata_train = data_nueva\ntarger_train = data_nueva['AveragePrice']","3b8afc0e":"data_train['monthy'] = data_train['Date'].astype('datetime64[ns]').apply(lambda ts: ts.month)","77e2010b":"dummi_month =pd.get_dummies(data_train['monthy'], prefix='month')\ndata_train = pd.concat([data_train, dummi_month], axis=1)","b0c07506":"data_train = data_train.drop(['monthy'],axis=1)\n#data_train = data_train.drop(['Date'],axis=1)","c8bc0d2a":"data_train.head(5)","d1560dc7":"data_train['year'].value_counts()","35ad0e8f":"region_dummi_fecha =pd.get_dummies(data_train['year'], prefix='year')","4c0c036a":"data_train = pd.concat([data_train, region_dummi_fecha], axis=1)","dfd096aa":"data_train.head(5)","920fa0e3":"data_train = data_train.drop(['year'],axis=1)\ndata_train = data_train.drop(['Date'], axis=1)\ndata_train = data_train.drop(['AveragePrice'],axis=1)","1efbb096":"\ndata_train.head(5)","a6eaa27e":"targer_train.mean()","2dca3d3b":"%config InlineBackend.figure_format = 'svg'\nsns.set(font_scale = 1)\nsns.distplot(targer_train);","efeae6e0":"sns.set(font_scale = 1)\nsns.distplot(np.log1p(targer_train));","cf94d0ea":"data_train.head()","39056490":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(data_train,targer_train,test_size=0.2)\nprint(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","76a66b4e":"#Probamos con un modelo simple Ridge\nfrom sklearn.linear_model import Ridge\nridge = Ridge(random_state=17)\nridge.fit(X_train, y_train);\nridge_pred = ridge.predict(X_test)","a2d0a020":"sns.set(font_scale = 1)\nplt.hist(y_test, bins=50, alpha=.5, color='red', label='true', range=(0,4));\nplt.hist(ridge_pred, bins=50, alpha=.5, color='green', label='pred', range=(0,4));\nplt.legend();","64a5dea5":"mean_absolute_error(y_test, ridge_pred)","824bf4fa":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Lasso,Ridge,BayesianRidge,ElasticNet,HuberRegressor,LinearRegression,LogisticRegression,SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# print X_train.shape, X_test.shape\n\nclassifiers = [['DecisionTree :',DecisionTreeRegressor()],\n               ['RandomForest :',RandomForestRegressor()],\n               ['KNeighbours :', KNeighborsRegressor(n_neighbors = 2)],\n               ['SVM :', SVR()],\n               ['AdaBoostClassifier :', AdaBoostRegressor()],\n               ['GradientBoostingClassifier: ', GradientBoostingRegressor()],\n               ['Xgboost: ', XGBRegressor()],\n               ['CatBoost: ', CatBoostRegressor(logging_level='Silent')],\n               ['Lasso: ', Lasso()],\n               ['Ridge: ', Ridge(random_state=17)],\n               ['BayesianRidge: ', BayesianRidge()],\n               ['ElasticNet: ', ElasticNet()],\n               ['HuberRegressor: ', HuberRegressor()]]\n\nprint(\"Accuracy Results...\")\n\n\nfor name,classifier in classifiers:\n    classifier = classifier\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    print(name, (np.sqrt(mean_squared_error(y_test, predictions))))","1e0be273":"classifier = RandomForestRegressor()\nclassifier.fit(X_train, y_train)\npredictions = classifier.predict(X_test)\n","bafc271f":"mean_absolute_error(y_test, predictions)","59ac38c6":"sns.set(font_scale = 1)\ndata = pd.DataFrame({'Y Test':y_test , 'Pred':predictions},columns=['Y Test','Pred'])\nsns.lmplot(x='Y Test',y='Pred',data=data,palette='rainbow')\ndata.head()","6ce58fe9":"## Luego de este punto podemos trabajarlo con un modelo de prediccion\nDe acuerdo con el modelo de prediccion podemos trabajar los datos para predecir el precio promedio individual, empezamos con el RIDGE","58709c6e":"En este caso vamos a tomar el RandomForest y lo probamos de manera individual con el MAE","1785954c":"### Visualizacion de informacion","9a79cb1c":"Como bien indique tenemos que entender los datos y el significado de cada columna correctamente, solo de esa manera podemos avanzar cualquier paso siguiente.\n\nEn este caso que nos dice cada columna:\n\n| Nombre | Descripcion  |\n|---------|--------------|\n| Date | Fecha de observacion  |\n| AveragePrice | El precio promedio de cada palta  |\n| type | El tipo (conventional or organic)  | \n| year | El a\u00f1o  | \n| Region | La ciudad o region observada  |  \n| Total Volume | El volumen total comprado  |  \n| 4046 | El numero total de paltas con codigo PLU 4046  |  \n| 4225 | El numero total de paltas con codigo PLU 4225 sold  |  \n| 4770 | El numero total de paltas con codigo PLU 4770 sold  |  \n| Total Bags | El numero total de bolsas vendidas  |  \n| Small Bags\t| El numero total de bolsas peque\u00f1as  |  \n| Large Bags | El numero total de bolsas grandes  |  \n| XLarge Bags | El numero total de bolsas extra grandes  |  \n\nCabe resaltar que\n\nTotal Volume = 4046 + 4225 + 4770 + Total Bags\n\nTotal Bags = Small Bags + Large Bags + XLarge Bags\n\nMas informacion de la data en [data](http:\/\/www.hassavocadoboard.com\/retail\/volume-and-price-data)","34609858":"Creamos la separacion entre la data de test y entrenamiento","06d79505":"### Analisis de Dataset de precios de paltas\n\nVamos a trabajar este kernel analizando un conjunto de datos y aplicando nuestro conocimiento de EDA asi como reconocimiento de funciones utiles de pandas y numpy","b0184588":"## **Enginner Features**\nVamos a realizar un grafico para extraer las anomalias o valores extremos para poder estandarizar lo mas posible nuestros datos","17d54b3e":"## **Consistencia de la informacion**\n\nLuego de conocer la informacion vamos a analizar la consistencia de la informacion","ba75d81f":"Generamos un grafico de correlacion","0f6b9365":"Probamos con varios modelos para determinar el que nos da mejores resultados con la configuracion estandar","a2ebd41a":"![Paltas para todos](https:\/\/exoticfruitbox.com\/wp-content\/uploads\/2015\/10\/aguacate.jpg)","004ef6a1":"## **Analisis de datos**\n\nPara poder trabajar los datos primeros debemos conocer los datos y tipos de datos con los cuales vamos a trabajar, en este caso hemos tomado un Dataset de precios de paltas."}}