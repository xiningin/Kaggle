{"cell_type":{"19dca348":"code","60949578":"code","35193df0":"code","78e669ec":"code","80ff9056":"code","3ca42a43":"markdown","089e921d":"markdown","baec88e6":"markdown","5b44f564":"markdown","f0144f47":"markdown","2a249605":"markdown","832cf8c7":"markdown","360e9c45":"markdown","bdb0c028":"markdown"},"source":{"19dca348":"# ! pip install -U imbalanced-learn\n# ! pip install mlxtend","60949578":"import itertools\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.datasets import make_classification\n\nfrom mlxtend.plotting import plot_decision_regions\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib import pyplot as plt\n\n%matplotlib inline","35193df0":"svm = LinearSVC(random_state=2019)\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(20,20))\n\nclass_weights = [[.3,  .3,   .3], \n                 [.7,  .2,   .1],\n                 [.93, .05,  .02],\n                 [.97, .017, .13]]\n\nfor _weights, grd in zip(class_weights, itertools.product([0, 1], repeat=2)):\n    \n    X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1,\n                            weights=_weights,\n                            class_sep=0.8, random_state=42)\n    svm.fit(X, y)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X, y=y, clf=svm, legend=2)\n    plt.title(f'SVM with {_weights} class proportion');","78e669ec":"from imblearn.over_sampling import  RandomOverSampler, SMOTE, ADASYN\n\nsvm = LinearSVC(random_state=2019)\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(20,20))\n\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1, weights=[0.03, 0.07, 0.9],\n                           class_sep=0.8, random_state=1)\n\n\nresamplers = [ RandomOverSampler(random_state=42), \n              SMOTE(random_state=42), \n              ADASYN(random_state=42) ]\n\ntitles = ['RandomOverSampler', 'SMOTE', 'ADASYN']\n\nfor resampler, grd, title in zip(resamplers, itertools.product([0, 1], repeat=2), titles):\n    X_resampled, y_resampled = resampler.fit_resample(X, y)\n    svm.fit(X_resampled, y_resampled)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_resampled, y=y_resampled, clf=svm, legend=2)\n    plt.title(f'SVM anbd {title} method');\n\nsvm.fit(X, y)\nax = plt.subplot(gs[1, 1])\nfig = plot_decision_regions(X=X, y=y, clf=svm, legend=2)\nplt.title(f'SVM and NO method');\n","80ff9056":"from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids, NearMiss\n\nsvm = LinearSVC(random_state=2019)\ngs = gridspec.GridSpec(2, 2)\nfig = plt.figure(figsize=(20,20))\n\nX, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n                           n_redundant=0, n_repeated=0, n_classes=3,\n                           n_clusters_per_class=1, weights=[0.03, 0.07, 0.9],\n                           class_sep=0.8, random_state=1)\n\n\nresamplers = [ RandomUnderSampler(random_state=42), \n              ClusterCentroids(random_state=42), \n              NearMiss(random_state=42) ]\n\ntitles = ['RandomUnderSampler', 'ClusterCentroids', 'NearMiss']\n\nfor resampler, grd, title in zip(resamplers, itertools.product([0, 1], repeat=2), titles):\n    X_resampled, y_resampled = resampler.fit_resample(X, y)\n    svm.fit(X_resampled, y_resampled)\n    ax = plt.subplot(gs[grd[0], grd[1]])\n    fig = plot_decision_regions(X=X_resampled, y=y_resampled, clf=svm, legend=2)\n    plt.title(f'SVM anbd {title} method');\n\nsvm.fit(X, y)\nax = plt.subplot(gs[1, 1])\nfig = plot_decision_regions(X=X, y=y, clf=svm, legend=2)\nplt.title(f'SVM and NO method');","3ca42a43":"With a greater imbalanced ratio, the decision function favor the class with the larger number of samples, usually referred as the majority class. Roughly speaking, weight of class began depends on count samples. Also, we can`t use some metrics, like accuracy, if we have disproportion of samples. Consider, how we can solve this problem using http:\/\/imbalanced-learn.org\/en\/stable\/index.html package.","089e921d":"### 4. Do not forget about parameter `class_weight`.\nIt is supported by many classifiers from `sklearn`, you just need set this parameter to `'balanced'`","baec88e6":"Suggest, we have some dataset. \"Imbalanced\" means that the count of examples from one class is much more, than from others. Let`s see how SVM work with different level of class balancing","5b44f564":"### 2. Under-sampling\nGiven an original data set $S$, prototype generation algorithms will generate a new set $S'$ where $|S'| < |S|$ and $S' \\not\\in\nS.$ In other words, prototype generation technique will reduce the number of samples in the targeted classes but the remaining samples are generated \u2014 and not selected \u2014 from the original set.\n\n+ **ClusterCentroids** makes use of K-means to reduce the number of samples. Therefore, each class will be synthesized with the centroids of the K-means method instead of the original samples.\n\n+ **RandomUnderSampler** is a fast and easy way to balance the data by randomly selecting a subset of data for the targeted classes.\n\n+ **NearMiss** is equal to  RandomUnderSampler with adds some heuristic rules to select samples.","f0144f47":"### 3. Try to combine over-sampling and under-sampling\nIt can be done by using class `SMOTEENN` from `imblearn.combine`. ","2a249605":"#                                 What we can do with umbalanced data?","832cf8c7":"As we can see, all methods help to improve decision surface.","360e9c45":"# Now, lets summarize.\nThe disproportion of class samples may influence on work of ML algorithms. To avoid that, we can add samples or drop samples by some procedures. The kind of procedure is another parameter to tune.","bdb0c028":"### 1. Over-sampling\nIn this approach, we will try generate new samples in the classes which are under-represented.\n+ **RandomOverSampler.**\nIt is most naive strategy, when we generate new samples by randomly sampling with replacement the current available samples. Like a bootstrap.\n+ **Synthetic Minority Oversampling Technique (SMOTE)**\nGenerating new samples by interpolation.\n+ **Adaptive Synthetic (ADASYN)**\nGenerating new samples by interpolation, but focus on the samples, that are wrong classified by  k-Nearest Neighbors algorithm."}}