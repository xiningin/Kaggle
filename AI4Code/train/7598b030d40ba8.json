{"cell_type":{"1f18bcdd":"code","34b346e5":"code","d6ab6416":"code","ac585918":"code","9b29c193":"code","68463117":"code","4e2ec366":"code","0739b732":"code","b435c737":"code","4fa1d3ac":"code","0709803c":"code","2759372e":"code","423315c0":"code","a0d7a9ea":"code","2ddfced8":"code","2d6b016a":"code","3e29a6f6":"code","af0071e4":"code","9017a103":"code","ca45aaa5":"code","6d7f1a13":"code","6095cf84":"code","af141486":"code","8372d982":"code","e32ff06a":"code","737f0e85":"code","ec0cec43":"code","9a09b3cb":"code","81dba845":"code","61ab950f":"code","92258c0a":"code","a2e79c16":"code","ad73f9a8":"code","37f47dd0":"code","eecfa1be":"code","2cf3d860":"code","31faee9a":"code","d65a0b20":"code","f6da8195":"code","9ee9d680":"code","8ea55154":"code","9c2e71e7":"code","7967fbff":"code","9aca8504":"code","37c67196":"code","81ed9abb":"code","2ed12efa":"code","e2f0aa37":"code","9ac1031a":"code","7b92ed8b":"code","0f421d83":"code","6ab52e24":"code","f7fd8aa6":"code","938c79c8":"code","263f5d67":"code","9cbe7ce3":"code","1c9be352":"markdown","0fb53d5c":"markdown","3c788232":"markdown","8db6031e":"markdown","36ca6a39":"markdown","397bc34f":"markdown","3cac7eed":"markdown","bb5ceb15":"markdown","d1b8bb26":"markdown","5453d9db":"markdown","21996800":"markdown","5d1dccf1":"markdown","70ab8419":"markdown"},"source":{"1f18bcdd":"# Put these at the top of every notebook, to get automatic reloading and inline plotting\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","34b346e5":"# This file contains all the main external libs we'll use\nfrom fastai.imports import *\nfrom fastai.transforms import *\nfrom fastai.conv_learner import *\nfrom fastai.model import *\nfrom fastai.dataset import *\nfrom fastai.sgdr import *\nfrom fastai.plots import *\nfrom glob import glob\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","d6ab6416":"PATH = \"..\/input\/\"\nTMP_PATH = \"\/tmp\/tmp\"\nMODEL_PATH = \"\/tmp\/model\/\"\nsz=64","ac585918":"torch.cuda.is_available(), torch.backends.cudnn.enabled","9b29c193":"os.listdir(PATH)","68463117":"# Import training dataset\ntraining_fruit_img = []\ntraining_label = []\nfor dir_path in tqdm(glob(\"..\/input\/*\/fruits-360\/Training\/*\")):\n    img_label = dir_path.split(\"\/\")[-1]\n    for img_path in glob(os.path.join(dir_path, \"*.jpg\")):\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, (64, 64))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        training_fruit_img.append(img)\n        training_label.append(img_label)\ntraining_fruit_img = np.array(training_fruit_img)\ntraining_label = np.array(training_label)\nlen(np.unique(training_label))","4e2ec366":"# Import test dataset\ntest_fruit_img = []\ntest_label = []\nfor dir_path in tqdm(glob(\"..\/input\/*\/fruits-360\/Test\/*\")):\n    img_label = dir_path.split(\"\/\")[-1]\n    for img_path in glob(os.path.join(dir_path, \"*.jpg\")):\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, (64, 64))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        test_fruit_img.append(img)\n        test_label.append(img_label)\ntest_fruit_img = np.array(test_fruit_img)\ntest_label = np.array(test_label)\nlen(np.unique(test_label))","0739b732":"label_to_id = {v : k for k, v in enumerate(np.unique(training_label))}\nid_to_label = {v : k for k, v in label_to_id.items()}","b435c737":"training_label_id = np.array([label_to_id[i] for i in training_label])\ntest_label_id = np.array([label_to_id[i] for i in test_label])","4fa1d3ac":"img = training_fruit_img[0]\nplt.imshow(img);","0709803c":"training_label_id[0], training_label[0]","2759372e":"img.shape","423315c0":"img[:4,:4]","a0d7a9ea":"def get_1st(path, pattern): return glob(f'{path}\/{pattern}\/*.jpg')[0]","2ddfced8":"dc_path = \"..\/input\/fruits-360_dataset\/fruits-360\/\"\nlist_paths = [get_1st(f\"{dc_path}Training\", \"Avocado\"), get_1st(f\"{dc_path}Training\", \"Carambula\")]\nplots_from_files(list_paths, titles=[\"Avocado\", \"Carambula\"], maintitle=\"Single-label classification\")","2d6b016a":"fnames = np.array([f'{f}'.split('..\/input\/fruits-360_dataset\/fruits-360\/')[1] for f in sorted(glob(f'{dc_path}Training\/*\/*.jpg'))])\nfnames[:5]","3e29a6f6":"img_label = np.array([filename.split(\"\/\")[-2] for filename in fnames])","af0071e4":"training_label_id = np.array([label_to_id[i] for i in img_label])\ntraining_label_id","9017a103":"dc_path","ca45aaa5":"arch=resnet34\ndata = ImageClassifierData.from_names_and_array(\n    path=dc_path, \n    fnames=fnames, \n    y=training_label_id, \n    classes=set(img_label),\n    test_name='Test\/*\/',\n    tfms=tfms_from_model(arch, sz)\n)","6d7f1a13":"learn = ConvLearner.pretrained(arch, data, precompute=True, tmp_name=TMP_PATH, models_name=MODEL_PATH)","6095cf84":"learn.fit(0.01, 2)","af141486":"# This is the label for a val data\ndata.val_y","8372d982":"data.classes","e32ff06a":"log_preds = learn.predict()\nlog_preds.shape","737f0e85":"preds = np.argmax(log_preds, axis=1)  # from log probabilities to 0 or 1\nprobs = np.exp(log_preds[:,0])        # pr(Apple Braeburn)","ec0cec43":"def rand_by_mask(mask): return np.random.choice(np.where(mask)[0], 4, replace=False)\ndef rand_by_correct(is_correct): return rand_by_mask((preds == data.val_y)==is_correct)","9a09b3cb":"def plots(ims, figsize=(12,6), rows=1, titles=None):\n    f = plt.figure(figsize=figsize)\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, len(ims)\/\/rows, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i])","81dba845":"def load_img_id(ds, idx): return np.array(PIL.Image.open(dc_path+ds.fnames[idx]))\n\ndef plot_val_with_title(idxs, title):\n    imgs = [load_img_id(data.val_ds,x) for x in idxs]\n    title_probs = [probs[x] for x in idxs]\n    print(title)\n    return plots(imgs, rows=1, titles=title_probs, figsize=(16,8))","61ab950f":"# 1. A few correct labels at random\nplot_val_with_title(rand_by_correct(True), \"Correctly classified\")","92258c0a":"# 2. A few incorrect labels at random\nplot_val_with_title(rand_by_correct(False), \"Incorrectly classified\")","a2e79c16":"def most_by_mask(mask, mult):\n    idxs = np.where(mask)[0]\n    return idxs[np.argsort(mult * probs[idxs])[:4]]\n\ndef most_by_correct(y, is_correct): \n    mult = -1 if (y==1)==is_correct else 1\n    return most_by_mask(((preds == data.val_y)==is_correct) & (data.val_y == y), mult)","ad73f9a8":"plot_val_with_title(most_by_correct(label_to_id['Nectarine'], True), \"Most correct Nectarines\")","37f47dd0":"plot_val_with_title(most_by_correct(label_to_id['Avocado'], True), \"Most correct Avocados\")","eecfa1be":"most_uncertain = np.argsort(np.abs(probs -0.5))[:4]\nplot_val_with_title(most_uncertain, \"Most uncertain predictions\")","2cf3d860":"learn = ConvLearner.pretrained(arch, data, precompute=True, tmp_name=TMP_PATH, models_name=MODEL_PATH)","31faee9a":"lrf=learn.lr_find()","d65a0b20":"learn.sched.plot()","f6da8195":"tfms = tfms_from_model(resnet34, sz, aug_tfms=transforms_side_on, max_zoom=1.1)","9ee9d680":"def get_augs():\n    data = ImageClassifierData.from_names_and_array(\n        path=dc_path, \n        fnames=fnames, \n        y=training_label_id, \n        classes=set(img_label),\n        tfms=tfms_from_model(arch, sz),\n        test_name='Test\/*\/',\n        num_workers=1,\n        bs=2\n    )\n    x,_ = next(iter(data.aug_dl))\n    return data.trn_ds.denorm(x)[1]","8ea55154":"ims = np.stack([get_augs() for i in range(6)])","9c2e71e7":"plots(ims, rows=2)","7967fbff":"data = ImageClassifierData.from_names_and_array(\n    path=dc_path, \n    fnames=fnames, \n    y=training_label_id, \n    classes=set(img_label),\n    test_name='Test\/*\/',\n    tfms=tfms\n)\nlearn = ConvLearner.pretrained(arch, data, precompute=True, tmp_name=TMP_PATH, models_name=MODEL_PATH)","9aca8504":"learn.fit(1e-2*5, 1)","37c67196":"learn.precompute=False","81ed9abb":"learn.fit(1e-2*5, 3, cycle_len=1)","2ed12efa":"learn.sched.plot_lr()","e2f0aa37":"learn.unfreeze()","9ac1031a":"lr=np.array([1e-4*5,1e-3*5,1e-2*5])","7b92ed8b":"learn.fit(lr, 3, cycle_len=1, cycle_mult=2)","0f421d83":"learn.sched.plot_lr()","6ab52e24":"log_preds,y = learn.TTA()\nprobs = np.mean(np.exp(log_preds),0)","f7fd8aa6":"accuracy_np(probs, y)","938c79c8":"preds = np.argmax(probs, axis=1)\nprobs = probs[:,1]","263f5d67":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, preds)","9cbe7ce3":"plot_confusion_matrix(cm, data.classes)","1c9be352":"N\u00f3s podemos ver o plot da loss versus o learning rate at\u00e9 onde ela deixa de cair\n","0fb53d5c":"Por padr\u00e3o, quando criamos um learner, apenas a ultima camada est\u00e1 `unfrozen` e, ent\u00e3o, apenas ela pode ter seus pesos alterados. Queremos que todas as camadas sofram altera\u00e7\u00f5es.","3c788232":"O par\u00e2metro cycle_mult basicamente diz para multiplicarmos o tamanho do ciclo por uma constante. No caso aqui, ent\u00e3o, teremos um total de 3 ciclos: um de tamanho 1, o segundo de tamanho 2 e o terceiro de tamanho 2*2=4. No total, ent\u00e3o teremos, 1+2+4=7 \u00e9pocas !","8db6031e":"![](https:\/\/cdn-images-1.medium.com\/max\/2000\/1*TgAz1qaKu_SzuRmsO-6WGQ.png)","36ca6a39":"# Escolhendo o Learning Rate\n\nEscolher o Learning Rate ideal era uma tarefa muito complicada at\u00e9 ent\u00e3o\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*vsXrd010HEYLVfoe2F-ZiQ.png)\n\nExclusiva da biblioteca fastai, \u00e9 uma ideia bem simples: n\u00f3s aumentamos o learning rate at\u00e9 um valor pequeno, at\u00e9 a loss parar de reduzir. N\u00f3s plotamos o learning rate em rela\u00e7\u00e3o a batches das imagens para podermos ver como ele se parece.\n\nPrimeiro n\u00f3s criamos um learner, uma vez que queremos saber como setar o learning rate para um modelo n\u00e3o treinado.\n","397bc34f":"\nO modelo foi treinado para reconhecer imagens do imagenet (enquanto apenas os ultimos pesos foram inicializados de maneira aleat\u00f3ria). N\u00e3o queremos desaprender o que o modelo j\u00e1 aprender para os casos mais basicos. Ent\u00e3o, n\u00f3s precisamos ser menos agressivos com as primeiras camadas e, logo, devemos set\u00e1-las com um learning rate menor. As primeiras ter\u00e3o um learning rate de 1e-4*5 (para mantermos a constante); os do meio (1e-3*5) e os ultimos, (1e-2*5), como antes.","3cac7eed":"O que queremos, na verdade, \u00e9 que a cada restart, n\u00f3s fa\u00e7amos uma busca mais \"suave no espa\u00e7o\"","bb5ceb15":"Precisamos setar o precomputing para False para podermos utilizar data augmentation","d1b8bb26":"## Transfer Learning\n\n![](https:\/\/image.slidesharecdn.com\/practicaldeeplearning-160329181459\/95\/practical-deep-learning-16-638.jpg)","5453d9db":"Essa fun\u00e7\u00e3o serve para fazermos data augmentation tamb\u00e9m nos dados de Teste (ou Valida\u00e7\u00e3o).","21996800":"O que \u00e9 o par\u00e2metro cycle_len? Basicamente n\u00f3s \"alteramos\" o conjunto dos nossos pesos, meio que para \"incentivar\" o nosso modelo a encontrar uma configura\u00e7\u00e3o de pesos melhor. Em outra palavra, n\u00f3s aumentamos o learning_rate, for\u00e7ando o modelo a pular no \"espa\u00e7o de pesos\" ","5d1dccf1":"## Data Augmentation\n\nEm caso de poucos dados, ou outros problemas como overfitting,uma estrat\u00e9gia muito cl\u00e1ssica \u00e9 o que a gente chama de **Data Augmentation** em que, basicamente, n\u00f3s geramos dados novos com base nos antigos. Para fotos, \u00e9 comum utilizarmos um flip horizontal das fotos (transforms_side_on), como tamb\u00e9m podemos definir um zoom","70ab8419":"# Classificando Frutas\n\nN\u00f3s vimos que o modelo inicial n\u00e3o conseguiu classificar uma imagem de avocado muito bem, isso se deve porqu\u00ea esses modelos pr\u00e9 treinados conseguem generalizar at\u00e9 certo ponto porqu\u00ea eles s\u00e3o especializados para classificar aquilo que eles foram treinados. No caso do ImageNet, temos as mais de 1000 categorias diferentes, mas e se sa\u00edmos disso? Os modelos come\u00e7am a n\u00e3o se dar t\u00e3o bem.\n\n![](https:\/\/imgur.com\/0SxDibp.jpg)\n\n![](https:\/\/imgur.com\/bUgbvcK.jpg)\n\n![](https:\/\/imgur.com\/TZfkZtF.jpg)\n\nAs primeiras camas da rede conseguem reconhecer pontos e linhas e a medida que n\u00f3s vamos nos aprofundando, ela consegue aprender cada vez estruturas mais e mais complexas. S\u00f3 que essa \"complexidade\" diz respeito \u00e0s classes que a rede foi treinada inicialmente. Ser\u00e1 que conseguimos mudar isso? Sim ! E isso \u00e9 poss\u00edvel por meio de uma t\u00e9cnica chamada de Transfer Learning"}}