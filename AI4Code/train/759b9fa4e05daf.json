{"cell_type":{"2f6eb03d":"code","6ab86144":"code","64c4e2ee":"code","389525c9":"code","fcef3334":"code","6c505cf8":"code","361b6890":"code","8be3291d":"code","e98eb3d0":"code","5b214a89":"code","0db805a9":"code","d53535e4":"code","bc3c39e4":"code","7483f96b":"code","e881e3a8":"code","e0de89df":"code","1b16eef5":"code","1c3e559e":"code","63201b85":"code","5ec2ca7c":"code","36c21d5e":"code","2b569304":"code","00feb521":"code","9da53e01":"code","215c0cd1":"code","fc8a2c6e":"code","909fbc78":"code","1a31a832":"code","705fd242":"code","1b09ce83":"code","471456e4":"code","29137fe7":"code","c2109d36":"code","be467100":"code","62bdcb12":"code","9e0084c2":"code","6c91d71e":"code","5138d1d4":"code","f93cae00":"markdown","e92a90e4":"markdown","94f1ec8d":"markdown","37b6735c":"markdown","2feb81d2":"markdown","44be6837":"markdown","ace6b9e1":"markdown","c3a150d6":"markdown","b3c517b6":"markdown","42df77f0":"markdown","bc1e3fa7":"markdown","a72905a1":"markdown","c3f3976a":"markdown","07d7803d":"markdown","8b544a85":"markdown","f736036f":"markdown","e7a13e61":"markdown","26de8308":"markdown","94a6f134":"markdown","8c46a5d3":"markdown","68497163":"markdown","4e69695d":"markdown","f2c4ca3f":"markdown","b1da4dd3":"markdown"},"source":{"2f6eb03d":"#Let me know if you need anymore details or if you have suggestions\n#Linkedin : @justsuyash","6ab86144":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","64c4e2ee":"data  = pd.read_csv('..\/input\/fish-market\/Fish.csv')","389525c9":"df = data.copy() #Copying it just in case\ndf.head(10)","fcef3334":"df['Species'].unique()","6c505cf8":"df.info()","361b6890":"#Lets Check if there are any Null Values in the data set:\ndf.isnull().values.any()","8be3291d":"df.describe()","e98eb3d0":"first_quarlitle_weight = df['Weight'].quantile(0.25)\nthird_quarlitle_weight = df['Weight'].quantile(0.75)\n\ninter_quartile_weight = third_quarlitle_weight - first_quarlitle_weight\n\nlower_range_weight = first_quarlitle_weight - 1.5*inter_quartile_weight\nupper_range_weight = third_quarlitle_weight + 1.5*inter_quartile_weight","5b214a89":"#This way we get a resonable estimate of an outlier\ndf[ (df['Weight'] < lower_range_weight) | (df['Weight']>upper_range_weight)]","0db805a9":"first_quarlitle_length1 = df['Length1'].quantile(0.25)\nthird_quarlitle_length1 = df['Length1'].quantile(0.75)\n\ninter_quartile_length1 = third_quarlitle_length1 - first_quarlitle_length1\n\nlower_range_length1 = first_quarlitle_length1 - 1.5*inter_quartile_length1\nupper_range_length1 = third_quarlitle_length1 + 1.5*inter_quartile_length1\n","d53535e4":"df[ (df['Length1'] < lower_range_length1) | (df['Length1']>upper_range_length1)]","bc3c39e4":"excess_weight  = df[ (df['Weight'] < lower_range_weight) | (df['Weight']>upper_range_weight)]\ndf.drop(excess_weight.index,inplace=True)","7483f96b":"df.describe()","e881e3a8":"zero_weights  = df [ data['Weight'] == 0]\ndf.drop(zero_weights.index,inplace=True)","e0de89df":"df.describe()","1b16eef5":"sns.heatmap(df.corr(),annot=True, cmap='YlGnBu')","1c3e559e":"#Lets do a pairplot to see if we can find something.\nsns.pairplot(df, hue='Species')","63201b85":"df['Species'] = df['Species'].astype('category')\ndf['species_cat'] = df['Species'].cat.codes","5ec2ca7c":"c = df['Species'].astype('category')\n\nd = dict(enumerate(c.cat.categories))\nprint (d)","36c21d5e":"sns.pairplot(df,hue='species_cat')","2b569304":"data_with_dummies = df.drop(['species_cat'],axis=1)\n","00feb521":"data_with_dummies = pd.get_dummies(df, drop_first=True)","9da53e01":"data_with_dummies","215c0cd1":"data_with_dummies.columns","fc8a2c6e":"x = data_with_dummies.drop(['Weight'],axis=1)\ny = data_with_dummies['Weight']","909fbc78":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state = 42)","1a31a832":"lin = LinearRegression()","705fd242":"lin.fit(x_train,y_train)","1b09ce83":"y_train_predicted = lin.predict(x_train)\nr2_score(y_train, y_train_predicted)","471456e4":"from sklearn.model_selection import cross_val_score\ncross_val_score_train = cross_val_score(lin, x_train, y_train, cv=10, scoring='r2')\nprint(cross_val_score_train)","29137fe7":"cross_val_score_train.mean()","c2109d36":"predict = lin.predict(x_test)","be467100":"# calculate these metrics by hand!\nfrom sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, predict))\nprint('MSE:', metrics.mean_squared_error(y_test, predict))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predict)))","62bdcb12":"print(r2_score(y_test, predict))","9e0084c2":"plt.scatter(y_test,predict)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')","6c91d71e":"sns.pairplot(df,hue='species_cat')","5138d1d4":"c = df['Species'].astype('category')\n\nd = dict(enumerate(c.cat.categories))\nprint (d)","f93cae00":"### Hmm, so no null values in dataset,but how about wrong values?","e92a90e4":"### Lets what are the species, maybe it will help us to include that in out features later on","94f1ec8d":"**Thats a huge correlation, now that we have discovered this, there are two approach towards this one is \"One Hot Encoding\"\nThe other is as you can see above Categorical Numbering,\nI will prefer to create dummy variable as it assigns the value '1' to each category and hence it works better in algorithms as it doesnt add any extra weight to any particular category**\n\n\n\n**So lets drop 'species_cat'**","37b6735c":"**It is safe to assume that whats throwing us off is lesser data, as fish 5:'Smelt' is centered around a certain weight, \nalso there are only 6 white fishes with a wide distribution. But thats just my 2 cent, Let me know if you have any suggestions. Lets Learn together**","2feb81d2":"**We can see right away that minimum weight is 0 which is not possible also since the median weight is 398 and std is 357, the maximum weight 1650 seems like and outlier, this could throw off our predictions in the Linear Regression Model. But lets confrim out assumption**","44be6837":"# EXTRAS :","ace6b9e1":"### Lets Check if there are any Null Values in the data set:","c3a150d6":"### Linear Regression","b3c517b6":"### Crearting Data frame with dummies","42df77f0":"# ****Trying to see an overview of Data that we have****","bc1e3fa7":"### For Starters we can look at the pairplot again","a72905a1":"**Now we have come across a probelm that we could have had a hunch about,the variables realted to the dimensions of the fish are all correlated\nBut is that really a problem? \nI mean we have 3 features that are correlated to each other and also the target\nThis makes the probablity of a the most useful feature being selected the highest if we give it all three, moreover we are not concerned\nwith speed as the data set is quite small. \n**\n\n### Verdict : We keep all three\n\n","c3f3976a":"### Looks like there is a realtion between species of fish and the weights, makes sense, lets check further how much of that is true","07d7803d":"**Now that we have removed outliers and errorenous values, lets check out what are features important for our prediction**","8b544a85":"## Data Columns\n* Species - species name of fish\n* Weight - weight of fish in Gram g\n* Length1 - vertical length in cm\n* Length2 - diagonal length in cm\n* Length3 - cross length in cm\n* Height - height in cm\n* Widthdiagonal - width in cm\n\nWe have been given the task to predict weight.","f736036f":"## Now lets think about what can be done to improve the model?","e7a13e61":"## Now lets train out model using Linear Regression","26de8308":"### I will repeat Not at all bad","94a6f134":"## Not at all bad","8c46a5d3":"**So thats two features telling us that these are outliers, we should from them to keep them throwing us off of out predictions**","68497163":"### Classic Train Test Split","4e69695d":"**Since we have very less data to work on every set counts, Lets make is even more solid before we drop these,\nLets estimate the outliers for Length**","f2c4ca3f":"**So now our outliers are dropped, there is just one problem, weight cannot be 0 as shown in the minimum below, so we need to drop that too:** ","b1da4dd3":"### Determining Succes with Train DataSet:"}}