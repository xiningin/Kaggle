{"cell_type":{"3dcfaccb":"code","f33ab529":"code","14efa5ff":"code","8de79578":"code","5e7d2d02":"code","16ddf431":"code","9642399b":"code","d76d0ab6":"code","902d6929":"code","586515cd":"code","c0743174":"code","08b9985b":"code","f473d47a":"code","d7a79ea3":"code","b36b7eaf":"code","70984f12":"code","bce4f711":"code","d0691d2c":"code","265e69f6":"code","bfd4a578":"code","56e341eb":"code","a2aee7b0":"code","b808c483":"code","5d769cc7":"code","5fc2fce7":"code","a462b6f6":"code","266c84b0":"code","0235d31a":"code","15af1d11":"code","0a0b1ab3":"code","84310cd8":"code","33def89d":"code","1f65c73b":"code","73d46788":"code","8e433124":"code","73c892e5":"code","f4e3cb50":"code","83f7c56d":"code","a20fe147":"code","78313821":"code","68bd4115":"code","d7d53279":"code","2d3fb8d7":"code","a2c65d51":"code","8d590bd0":"code","c5c566d6":"code","5e8a9afd":"code","e4717057":"code","1c443fe8":"code","cfd2baf9":"code","5b781c95":"code","dfe42aaf":"code","a9d06613":"code","9a4e7eca":"code","18408614":"code","27056b85":"code","67d10249":"code","60a972a7":"code","a862bd51":"code","4a98743e":"code","6cc3db34":"code","c29b0495":"code","e8f89692":"code","00195596":"code","d6c9c9f5":"code","9321861a":"code","429d7c4d":"code","09b69146":"code","8fed762c":"code","9daadeae":"code","96c01f24":"code","20e2932a":"code","af6ed0cf":"markdown","98296edb":"markdown","3e16877d":"markdown","a0874ba4":"markdown","76ac1216":"markdown","595b8e7f":"markdown","1929d0ee":"markdown","22316125":"markdown","63b44762":"markdown","3c7a9258":"markdown","ec41e1eb":"markdown","bde7383a":"markdown","7cef320c":"markdown","581c4716":"markdown","8321f957":"markdown","9d789f28":"markdown","549983d0":"markdown","080c39fa":"markdown","ae6a834d":"markdown","61f538a9":"markdown","72d0b80c":"markdown","35f2b158":"markdown","42707ff5":"markdown","935baeaf":"markdown","0a6cd0aa":"markdown","357892de":"markdown","1c9110b4":"markdown","69cbd5bb":"markdown","7f6ad146":"markdown","9fe10c8d":"markdown","9edaf23b":"markdown","ed34f55f":"markdown","5d24c6f9":"markdown","52b65495":"markdown","c3218e5d":"markdown","f4032896":"markdown","2b533e74":"markdown","081a0e4a":"markdown","4a328278":"markdown","9f88c7a9":"markdown","07b1ec6c":"markdown","ae2b4bcf":"markdown","bf839e2c":"markdown","64da35c5":"markdown","3e4cf9eb":"markdown","828461eb":"markdown","d0b3b06b":"markdown","f5f3ea9a":"markdown","1e644425":"markdown","818804b9":"markdown","70725ece":"markdown","1616c8e1":"markdown","eee0fb02":"markdown","77ff3040":"markdown","3bd92b0d":"markdown","9aeff77b":"markdown","572358f9":"markdown","f39944b3":"markdown","709ab001":"markdown","d5fbb53c":"markdown","a5d3d0df":"markdown"},"source":{"3dcfaccb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f33ab529":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","14efa5ff":"bike_sharing = pd.read_csv(\"\/kaggle\/input\/boombikes-bike-sharing-service\/day.csv\")","8de79578":"# Check the head of the dataset\nbike_sharing.head()","5e7d2d02":"bike_sharing.shape","16ddf431":"bike_sharing.info()","9642399b":"bike_sharing.describe()","d76d0ab6":"# percentage of missing values in each column\nround(100*(bike_sharing.isnull().sum()\/len(bike_sharing)), 2).sort_values(ascending=False)","902d6929":"# percentage of missing values in each row\nround((bike_sharing.isnull().sum(axis=1)\/len(bike_sharing))*100,2).sort_values(ascending=False)","586515cd":"#### Checking for duplicate values\n\nbike_dup = bike_sharing.copy()\n\n# Checking for duplicates and dropping the entire duplicate row if any\nbike_dup.drop_duplicates(subset=None, inplace=True)\nbike_dup.shape","c0743174":"bike_sharing.shape","08b9985b":"bike_dummy=bike_sharing.iloc[:,1:16]","f473d47a":"for col in bike_dummy:\n    print(bike_dummy[col].value_counts(ascending=False), '\\n\\n\\n')","d7a79ea3":"bike_sharing.columns","b36b7eaf":"bike_new=bike_sharing[['season', 'yr', 'mnth', 'holiday', 'weekday',\n       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n       'cnt']]","70984f12":"bike_new.info()","bce4f711":"sns.pairplot(data = bike_new, vars=['cnt', 'temp', 'atemp', 'hum','windspeed'])\nplt.show()","d0691d2c":"plt.figure(figsize=(20, 15))\nplt.subplot(2,4,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bike_new)\nplt.subplot(2,4,2)\nsns.boxplot(x = 'yr', y = 'cnt', data = bike_new)\nplt.subplot(2,4,3)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bike_new)\nplt.subplot(2,4,4)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bike_new)\nplt.subplot(2,4,5)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bike_new)\nplt.subplot(2,4,6)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bike_new)\nplt.subplot(2,4,7)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bike_new)\nplt.show()","265e69f6":"import calendar\nbike_new['mnth'] = bike_new['mnth'].apply(lambda x: calendar.month_abbr[x])","bfd4a578":"# Maping seasons\nbike_new.season = bike_new.season.map({1: 'Spring',2:'Summer',3:'Fall',4:'Winter'})","56e341eb":"# Mapping weathersit\nbike_new.weathersit = bike_new.weathersit.map({1:'Clear',2:'Mist & Cloudy', \n                                             3:'Light Snow & Rain',4:'Heavy Snow & Rain'})","a2aee7b0":"#Mapping Weekday\nbike_new.weekday = bike_new.weekday.map({0:\"Sunday\",1:\"Monday\",2:\"Tuesday\",3:\"Wednesday\",4:\"Thrusday\",5:\"Friday\",6:\"Saturday\"})","b808c483":"# Check the dataframe now\n\nbike_new.head()","5d769cc7":"# Get the dummy variables for the features ''season','mnth','weekday','weathersit'' and store it in a new variable - 'dummy'\ndummy = bike_new[['season','mnth','weekday','weathersit']]","5fc2fce7":"dummy = pd.get_dummies(dummy,drop_first=True )","a462b6f6":"# Adding the dummy variables to the original dataset\nbike_new = pd.concat([dummy,bike_new],axis = 1)","266c84b0":"# Checking the dataframe\n\nbike_new.head()","0235d31a":"#Deleting the orginal columns season.weathersit,weekday,mnth\nbike_new.drop(['season'],axis=1,inplace=True)\nbike_new.drop(['weathersit'],axis=1,inplace=True)\n\nbike_new.drop(['weekday'],axis=1,inplace=True)\n\nbike_new.drop(['mnth'],axis=1,inplace=True)\n\n\nbike_new.head()","15af1d11":"bike_new.shape","0a0b1ab3":"bike_new.info()","84310cd8":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(bike_new, train_size = 0.7, test_size = 0.3, random_state = 100)","33def89d":"test.shape","1f65c73b":"train.shape","73d46788":"train.info()","8e433124":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","73c892e5":"# Apply scaler() to all the columns except the 'dummy' variables.\nnum_vars = ['cnt','hum','windspeed','temp','atemp']\n\ntrain[num_vars] = scaler.fit_transform(train[num_vars])","f4e3cb50":"train.head()","83f7c56d":"train.describe()","a20fe147":"plt.figure(figsize = (25, 25))\nsns.heatmap(train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","78313821":"y_train = train.pop('cnt')\nX_train = train","68bd4115":"y_train.shape","d7d53279":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","2d3fb8d7":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)            \nrfe = rfe.fit(X_train, y_train)","a2c65d51":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","8d590bd0":"col = X_train.columns[rfe.support_]\ncol","c5c566d6":"X_train.columns[~rfe.support_]","5e8a9afd":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","e4717057":"import statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)","1c443fe8":"X_train_rfe.head()","cfd2baf9":"lm_1 = sm.OLS(y_train,X_train_rfe).fit()","5b781c95":"# Check the parameters obtained\nlm_1.params","dfe42aaf":"print(lm_1.summary())","a9d06613":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9a4e7eca":"# dropping `const` column as the vif is > 5\nX_train_rfe = X_train_rfe.drop(['const'], axis=1)","18408614":"# Calculate the VIFs for the new model again\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","27056b85":"# dropping `hum` column as the vif is > 5\nX_train_rfe = X_train_rfe.drop(['hum'], axis=1)","67d10249":"vif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","60a972a7":"# Adding a constant variable \nX_train_lm = sm.add_constant(X_train_rfe)\n\n# Create a first fitted model\nlm_2 = sm.OLS(y_train,X_train_lm).fit()","a862bd51":"# Check the summary\nprint(lm_2.summary())","4a98743e":"# Calculate the VIFs for the new model again\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6cc3db34":"y_train_cnt = lm_2.predict(X_train_lm)","c29b0495":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","e8f89692":"# Plot the qq-plot of the error terms\nsm.qqplot((y_train - y_train_cnt), fit=True, line='45')\nplt.show()","00195596":"num_vars = ['cnt','hum','windspeed','temp','atemp']\n\n\ntest[num_vars] = scaler.transform(test[num_vars])","d6c9c9f5":"test.describe()","9321861a":"y_test = test.pop('cnt')\nX_test = test","429d7c4d":"# Adding constant variable to test dataframe\nX_test = sm.add_constant(X_test)","09b69146":"test_col = X_train_lm.columns\nX_test=X_test[test_col[1:]]\n# Adding constant variable to test dataframe\nX_test = sm.add_constant(X_test)\n\nX_test.info()","8fed762c":"# Making predictions using the final model\n\ny_pred = lm_2.predict(X_test)","9daadeae":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","96c01f24":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)","20e2932a":"param = pd.DataFrame(lm_2.params)\nparam.insert(0,'Variables',param.index)\nparam.rename(columns = {0:'Coefficient value'},inplace = True)\nparam['index'] = list(range(0,15))\nparam.set_index('index',inplace = True)\nparam.sort_values(by = 'Coefficient value',ascending = False,inplace = True)\nparam","af6ed0cf":"Now that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final model that we got.","98296edb":"Although scaling doesnot impact the linear model in the case of simple linear regression, however while performing multiplwe linear regression it might impact the model. As we can see that the value of the feature cnt has much higher values as compared to the other features like temp, atemp etc.So it is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. There are two common ways of rescaling:\n\n- Min-Max scaling\n- Standardisation (mean-0, sigma-1)\n\nThis time, we will use MinMax scaling.","3e16877d":"### Building model using statsmodel, for the detailed statistics","a0874ba4":"### <font color = 'red'> Conclusion: <\/font> \n\nHere we see that except for temp(that has a vif value slightly more than 5, that can be ignored) rest all the columns have a vif value less than 5.\n\nHence, we finalise `lm_2` as the final model to proceed with the future prdeictions.","76ac1216":"## Data Cleaning\n\nChecking value_counts() for entire dataframe.\n\nThis will help to identify any Unknown\/Junk values present in the dataset.\n\nCreate a copy of the dataframe, without the 'instant' column,as this will have unique values, and donot make sense to do a value count on it.","595b8e7f":"## Step 7: Residual Analysis of the train data","1929d0ee":"## Step 2: Visualising the Data","22316125":"### Reading the Bike sharing dataset","63b44762":"### Dividing into X and Y sets for the model building","3c7a9258":"- From R-Sqaured and adj R-Sqaured value of both train and test dataset we could conclude that the above variables can well explain more than 81% of bike demand.\n- Coeffiencients of the variables explains the factors effecting the bike demand\n\n- Based on final model top three features contributing significantly towards explaining the demand are:\n\n - Temperature (0.437655)\n - weathersit : Light Snow, Light Rain + Mist & Cloudy (-0.292892)\n - year (0.234287)\n\nHence, it can be clearly concluded that the variables `temperature` , `season`\/ `weather situation` and `month`  are significant in predicting the demand for shared bikes .","ec41e1eb":"So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","bde7383a":"### <font color = 'red'> Conclusion: <\/font>\nAs can be seen from the map, `atemp` and `temp` seems to be correlated to the target variable `cnt`. Since, not much can be stated about the other independent variables , hence we'll build a model using all the columns.","7cef320c":"## Step 3: Data Preparation","581c4716":"### <font color = 'red'> Conclusion: <\/font>\n\nWe can clearly see that the error terms are centred around 0 and follows a normal distribution, this is in accordance with the stated assumptions of linear regression.","8321f957":"#### Visualising Categorical Variables : Using a Boxplot","9d789f28":"#### Mapping the categorical values to their respective categorical string values (reference data dictionary)","549983d0":"#### Checking the coefficients to see which variables are highly correlated","080c39fa":"#### Visualising Numeric Variables : Using a pairplot","ae6a834d":"## Step 6: Building a linear model\n\nAPPROACH USED :\n\nWe will use a mixed approach to build the model.\n\nHere we are using `RFE` approach for feature selection and then we will use the `statsmodel` approach for building the model ","61f538a9":"### <font color = 'red'> Conclusion : <\/font>\nHere we see that the p-value for all the variables is < 0.05 . Hence, we keep all the columns and proceed with the model. ","72d0b80c":"#### Inferences:\n\nThere seems to be no Junk\/Unknown values in the entire dataset.","35f2b158":"### <font color = 'red'> Conclusion: <\/font>\n\nWe can see that the `r_squared on the test set is 0.813` and the `r-squared on the trained set 0.840` which is quiet reasonable and nearly equal, which means that whatever data the model was trained with, it has been almost able to apply those learnings in the test data.","42707ff5":"### <font color = 'red'> Conslusions: <\/font>\n- The graph clearly shows the qualitative distributions of the data, now if the model suggests the important predictors, using these graphs we can be more confident about the predictions of the model.\n- For the variable season, we can clearly see that the category 3 : Fall, has the highest median, which shows that the demand was high during this season. It is least for 1: spring .\n- The year 2019 had a higher count of users as compared to the year 2018\n- The bike demand is almost constant throughout the week.\n- The count of total users is in between 4000 to 6000 (~5500) during clear weather\n- The count is highest in the month of August\n- The count of users is less during the holidays","935baeaf":"#### Inferences: \n\nDuplicate values are not present","0a6cd0aa":"#### Dividing into X_test and y_test","357892de":"Based on the high level look at the data and the data dictionary, the following variables can be removed from further analysis:\n\ninstant : Its only an index value , we have a default index for the same purpose\n\ndteday : This has the date, Since we already have seperate columns for 'year' & 'month',hence, we can carry out our analysis without this column .\n\ncasual & registered : Both these columns contains the count of bike booked by different categories of customers. Since our objective is to find the total count of bikes and not by specific category, we will ignore these two columns.\n\nWe will save the new dataframe as bike_new, so that the original dataset is preserved for any future analysis\/validation","1c9110b4":"### Conclusion:\nHere we see that most of the data points lie on the straight line , which indicates that the error terms are normally distributed .","69cbd5bb":"### Data Quality Check","7f6ad146":"#### Applying the scaling on the test sets","9fe10c8d":"## Step 8: Making Predictions Using the Final Model","9edaf23b":"The variables `mnth` `weekday` `season` `weathersit` have various levels, for ex, `weathersit` has 3 levels , similarly variable `mnth` has 12 levels.   \nWe will create DUMMY variables for these 4 categorical variables namely - `mnth`, `weekday`, `season` & `weathersit`.","ed34f55f":"##### All the positive coefficients like temp,season_Summer indicate that an increase in these values will lead to an increase in the value of cnt.\n##### All the negative coefficients indicate that an increase in these values will lead to a decrease in the value of cnt.","5d24c6f9":"#### Calculating the r-squared\n\nR-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 \u2013 100% scale.","52b65495":"#### Getting the variable names and the coefficient values for the final equation of the best fitted line","c3218e5d":"### Feature Selection\n\nWe start with 15 variables.\n\nWe need to use the LinearRegression function from SciKit Learn for its compatibility with RFE (which is a utility from sklearn)","f4032896":"### Preparing the final model","2b533e74":"#### Summary of the linear model","081a0e4a":"## Step 1: Reading and Understanding the Data","4a328278":"#### Note:\nThe VIF value of temp is nearly equal to 5. Hence, we are not dropping this feature.","9f88c7a9":"#### We generally want a VIF that is less than 5. So there are clearly some variables we need to drop.","07b1ec6c":"### Creating Dummy Variables","ae2b4bcf":"### <font color = 'red'> Final Conclusions : <\/font>\nBy using the above scatter plot and the table , We can see that the equation of our best fitted line is:\n\n$ cnt = 0.2466 + 0.437 \\times  temp + 0.2342  \\times  yr + 0.8865 \\times season Winter + 0.0682 \\times mnth Sept + 0.0033 \\times season Summer - 0.0418 \\times mnth Nov - 0.04452 \\times mnth Dec - 0.0050 \\times mnth Jan - 0.0503 \\times mnth Jul - 0.0716 \\times season Spring - 0.0814 \\times weathersit Mist Cloudy - 0.0919 \\times holiday - 0.1585 \\times windspeed - 0.2928 \\times weathersit Light Snow Rain $","bf839e2c":"### Inspecting the Dataset","64da35c5":"## Step 4: Splitting the Data into Training and Testing Sets\nThe first basic step for regression is performing a train-test split.","3e4cf9eb":"### Conclusion: \nWe can colude that the final model fit isn't by chance, and has descent predictive power.","828461eb":"## Step 5 :Rescaling the Features","d0b3b06b":"### Removing redundant & unwanted columns","f5f3ea9a":"## Step 9: Model Evaluation","1e644425":"### Checking VIF for multicollinearity\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","818804b9":"#### Cross-verifying the above conclusion using a qq-plot as well:","70725ece":"#### Inferences :\n\nNo missing\/NULL values found","1616c8e1":"## Final Recommendations for the Company:\n- The months - Jan , Jul , Sep , Nov , Dec should be considered by the company as they have a higher demand as compared to other months.\n- With an increase in temperature the demand also increases, hence it should keep track of the weather conditions.\n- During the Winter season the demand rises, hence it should be well prepared to meet the high demand","eee0fb02":"#### Adding a constant variable \nFor statsmodels, we need to explicitly fit a constant using sm.add_constant(X) because if we don't perform this step, statsmodels fits a regression line passing through the origin, by default.","77ff3040":"#### Here we'll do the following tasks:\n- We would be able to check if all the variables are linearly related or not (important if we want to proceed with a linear model)\n- Checking if there are any multicollinearity that exist\n- Here's where we can also identify if some predictors directly have a strong association(correlation) with the outcome variable\n\nWe'll visualise our data using `matplotlib` and `seaborn`.","3bd92b0d":"#### Running the linear model","9aeff77b":"### Problem Statement:\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\nEssentially the company wants :\n- To understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19, by creating a linear model.\n- To identify the variables affecting their revenues i.e. Which variables are significant in predicting the demand for shared bikes.\n- To know the accuracy of the model, i.e. How well those variables describe the bike demands\n\nThey have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.","572358f9":"#### Predicting using values used by the final model","f39944b3":"Plotting the graph for actual versus predicted values.","709ab001":"#### Checking for NULL\/MISSING values","d5fbb53c":"# Bike Sharing Assignment","a5d3d0df":"### <font color = 'red'> Conclusion : <\/font>\nBy visualising the numeric variables, we can conclude that a linear model can be considered in this case because there are atleast some independent variables like atemp , temp etc. that show a positive correlation with the target variable cnt .  "}}