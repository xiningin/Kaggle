{"cell_type":{"f4816e96":"code","2238d953":"code","73557a06":"code","15b29239":"code","4dcecb68":"code","45beefd4":"code","6f7a4750":"code","6934f010":"code","bef9a4d0":"code","8a0d2274":"code","bbd5d39b":"code","c77830d9":"code","d67a4847":"code","0c7e6961":"code","ce0257a2":"code","27af5789":"code","b203c263":"code","5217862f":"code","7787f5d3":"code","704b235f":"code","b31f57c3":"code","2f954063":"code","c7c49e7a":"code","b1535ecd":"code","46f8f7bb":"code","2cdeb0bc":"code","e3cbbf0d":"code","8b860078":"code","08f4a797":"code","9ccd58e7":"code","e535171a":"code","1805a3ee":"code","dcada8d2":"code","c77533fa":"code","75f0e207":"code","6e868459":"code","b045f794":"code","f2a01546":"code","aba14739":"code","3e07d423":"code","4bc60edf":"code","245ddcc9":"code","da5b69a9":"code","e6c1368a":"code","56b4dbb9":"code","6020ae2c":"code","2a4bb97d":"code","26b0088c":"code","937badf4":"code","2ece7c8b":"code","885b655e":"code","691386e7":"code","72078563":"code","fce5b1aa":"code","813cebe2":"code","346985e4":"code","cf7db2ee":"code","e5f0eabb":"code","52eae114":"code","e24267a0":"code","b64021b5":"code","50d41784":"code","153ae069":"code","b6e608b1":"code","6b50bab5":"code","a67a4dc6":"code","7bbc7de5":"code","be62ba4c":"code","fe39d779":"code","20327628":"code","ee169cfb":"code","6bd9c85f":"code","2f00fe39":"code","80d045ad":"code","1ef1a115":"code","a29a72c6":"code","3bc82593":"code","e040350e":"code","d3ac3a64":"code","44821df8":"code","efc6a597":"code","9159c981":"code","cc470b17":"code","c4b90f81":"code","2c01d6fe":"code","cee50e4c":"code","40a0f896":"code","41c537c0":"code","8f593f5b":"code","25db003b":"code","3c093a2c":"markdown","841b8231":"markdown","05f33a95":"markdown","d7c3d468":"markdown","8a5df088":"markdown","367d7e6c":"markdown","ebd7eb50":"markdown","33befc86":"markdown","a9b90649":"markdown","7980d69f":"markdown","e8e95593":"markdown","bbfb4b49":"markdown","691f8e98":"markdown","12c0c1c5":"markdown","0027a02e":"markdown","ddded07b":"markdown","b59b85ee":"markdown","a4b483a0":"markdown","fc56d030":"markdown","8a92ae51":"markdown","361b3e80":"markdown","9210b356":"markdown","b7f8b109":"markdown","f14e0874":"markdown","570a7891":"markdown","8bfd850a":"markdown","9d5aa849":"markdown","6d0eab84":"markdown","6f92aaf6":"markdown","9d4bd4a7":"markdown","5cbcb2b0":"markdown","11b6ca1e":"markdown","489731c1":"markdown","49a37715":"markdown","da1171c9":"markdown","d74e993d":"markdown","73435c4d":"markdown","127d3c10":"markdown","4f28b4f7":"markdown","078c7dd5":"markdown","a08c47e4":"markdown","0ee0c129":"markdown"},"source":{"f4816e96":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.svm import SVR","2238d953":"# Display all columns of a dataframe\npd.pandas.set_option(\"display.max_columns\", None)","73557a06":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","15b29239":"test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","4dcecb68":"print(\"Train Dataset: \", train.shape)\nprint(\"Test Dataset: \", test.shape)","45beefd4":"train.describe()","6f7a4750":"train.info()","6934f010":"# Sales Price Distribution\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(6,5))\nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Sales Price\")\nax.set(title=\"Home Sales Price Distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","bef9a4d0":"# Skew and kurt\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","8a0d2274":"plt.figure(figsize=(20,10))\nsns.heatmap(data=train.corr(), cmap=\"Blues\", square=True)\nplt.show()","bbd5d39b":"# Get features with missing values\nfeatures_with_na = [feature for feature in train.columns if train[feature].isnull().sum() > 0]","c77830d9":"# Print missing features and its percentage in train dataset\nfor feature in features_with_na:\n    print(feature, np.round(train[feature].isnull().mean(), 4), \"% missing values\")","d67a4847":"for feature in features_with_na:\n    data = train.copy()\n    #Create a variable that indicates 1 if the values is missing and 0 otherwise.\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # Plot bar graph of median SalesPrice for values missing or present in train dataset\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel(\"SalePrice\")\n    plt.title(feature)\n    plt.show()","0c7e6961":"numerical_features = [feature for feature in train.columns if train[feature].dtype != 'O']\nprint(\"Number of numerical features: \", len(numerical_features))\ntrain[numerical_features].head()","ce0257a2":"temporal_features = [feature for feature in numerical_features if 'Year' in feature or 'Yr' in feature]\nprint(\"Number of temporal features: \", len(temporal_features))\ntrain[temporal_features].head()","27af5789":"for feature in temporal_features:\n    data = train.copy()\n    \n    data.groupby(feature)['SalePrice'].median().plot()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","b203c263":"for feature in temporal_features:\n    data = train.copy()\n    \n    if feature != 'YrSold':\n        data[feature] = data['YrSold'] - data[feature]\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()","5217862f":"discrete_features = [feature for feature in numerical_features if len(train[feature].unique()) <=25 \n                     and feature not in temporal_features + ['Id']]\nprint(\"Length of discrete features: \", len(discrete_features))\ntrain[discrete_features].head()","7787f5d3":"for feature in discrete_features:\n    data = train.copy()\n    \n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","704b235f":"continuous_features = [feature for feature in numerical_features if feature not in discrete_features + temporal_features + ['Id']]\nprint(\"Length of continuous features: \", len(continuous_features))\ntrain[continuous_features].head()","b31f57c3":"for feature in continuous_features:\n    data = train.copy()\n    \n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","2f954063":"for feature in continuous_features:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('Sale Price')\n        plt.title(feature)\n        plt.show()","c7c49e7a":"for feature in continuous_features:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.title(feature)\n        plt.show()","b1535ecd":"categorial_features = [feature for feature in train.columns if train[feature].dtypes == 'O']\nprint(categorial_features)","46f8f7bb":"train[categorial_features].head()","2cdeb0bc":"for feature in categorial_features:\n    print(\"Feature {} has {} unique values\".format(feature, len(train[feature].unique())))","e3cbbf0d":"for feature in categorial_features:\n    data = train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Sale Price')\n    plt.title(feature)\n    plt.show()","8b860078":"train[categorial_features].head()","08f4a797":"categorial_with_nan = [feature for feature in categorial_features if train[feature].isnull().sum() > 0]\nprint(categorial_with_nan)\nfor feature in categorial_with_nan:\n    print(\"Feature {}, has {}% missing values in train dataset\", (feature, np.round(train[feature].isnull().mean(), 4)))","9ccd58e7":"for feature in categorial_with_nan:\n    train[feature].fillna('Missing', inplace=True)","e535171a":"categorial_with_nan = [feature for feature in categorial_features if test[feature].isnull().sum() > 0]\nprint(categorial_with_nan)\nfor feature in categorial_with_nan:\n    print(\"Feature {}, has {}% missing values in test dataset\", (feature, np.round(test[feature].isnull().mean(), 4)))","1805a3ee":"for feature in categorial_with_nan:\n    test[feature].fillna('Missing', inplace=True)","dcada8d2":"train[categorial_features].head()","c77533fa":"test[categorial_features].head()","75f0e207":"print(\"Train Dataset Categorial Features:\",train[categorial_features].shape)\nprint(\"Test Dataset Categorial Features:\",test[categorial_features].shape)","6e868459":"print(numerical_features)","b045f794":"numerical_with_nan = [feature for feature in numerical_features if train[feature].isnull().sum() > 0]\nprint(numerical_with_nan)\nfor feature in numerical_with_nan:\n    print(\"Feature {} has {}% missing values in train datset\", (feature,np.round(train[feature].isnull().mean(), 4)))","f2a01546":"for feature in numerical_with_nan:\n    train[feature].fillna(train[feature].median(), inplace=True)","aba14739":"train.head()","3e07d423":"numerical_with_nan = [feature for feature in numerical_features if feature not in ['SalePrice'] and test[feature].isnull().sum() > 0]\nprint(numerical_with_nan)\nfor feature in numerical_with_nan:\n    print(\"Feature {} has {}% missing values in test datset\", (feature,np.round(test[feature].isnull().mean(), 4)))","4bc60edf":"for feature in numerical_with_nan:\n    test[feature].fillna(test[feature].median(), inplace=True)","245ddcc9":"test.head()","da5b69a9":"print(\"Train Dataset\", train.shape)\nprint(\"Test Dataset\", test.shape)","e6c1368a":"train[temporal_features].head()","56b4dbb9":"for feature in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    train[feature] = train['YrSold'] - train[feature]\n    test[feature] = test['YrSold'] - test[feature]","6020ae2c":"train[temporal_features].head()","2a4bb97d":"test[temporal_features].head()","26b0088c":"train.head()","937badf4":"num_non_zero_skewed_features_train_set = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ntrain[num_non_zero_skewed_features_train_set].head()","2ece7c8b":"for feature in num_non_zero_skewed_features_train_set:\n    train[feature] = np.log(train[feature])","885b655e":"num_non_zero_skewed_features_test_set = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\ntest[num_non_zero_skewed_features_test_set].head()","691386e7":"for feature in num_non_zero_skewed_features_test_set:\n    test[feature] = np.log(test[feature])","72078563":"train[num_non_zero_skewed_features_train_set].head()","fce5b1aa":"test[num_non_zero_skewed_features_test_set].head()","813cebe2":"train[categorial_features].head()","346985e4":"print(train.shape)\nprint(test.shape)","cf7db2ee":"print(len(categorial_features))\nprint(len(numerical_features))","e5f0eabb":"remaining_features = [feature for feature in train.columns if feature not in categorial_features + numerical_features]\nprint(remaining_features)","52eae114":"train1 = train.copy()\ntest1 = test.copy()","e24267a0":"data = pd.concat([train1,test1], axis=0)\ntrain_rows = train1.shape[0]\n\nfor feature in categorial_features:\n    dummy = pd.get_dummies(data[feature])\n    for col_name in dummy.columns:\n        dummy.rename(columns={col_name: feature+\"_\"+col_name}, inplace=True)\n    data = pd.concat([data, dummy], axis = 1)\n    data.drop([feature], axis = 1, inplace=True)\n\ntrain1 = data.iloc[:train_rows, :]\ntest1 = data.iloc[train_rows:, :] ","b64021b5":"train1.head()","50d41784":"test1.head()","153ae069":"print(\"Train\",train1.shape)\nprint(\"Test\",test1.shape)","b6e608b1":"from sklearn.preprocessing import MinMaxScaler, RobustScaler\n\nscaling_features = [feature for feature in train1.columns if feature not in ['Id', 'SalePrice']]\nscaling_features","6b50bab5":"print(len(scaling_features))","a67a4dc6":"train1[scaling_features].head()","7bbc7de5":"scaler = RobustScaler()\nscaler.fit(train1[scaling_features])","be62ba4c":"X_train = scaler.transform(train1[scaling_features])\nX_test = scaler.transform(test1[scaling_features])","fe39d779":"print(\"Train\", X_train.shape)\nprint(\"Test\", X_test.shape)","20327628":"y_train = train1['SalePrice']","ee169cfb":"X = pd.concat([train1[['Id','SalePrice']].reset_index(drop=True), pd.DataFrame(X_train, columns = scaling_features)], axis =1)\nprint(X.shape)\nX.head()","6bd9c85f":"n_folds = 12\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y_train, y_pred):\n    return np.sqrt(mean_squared_error(y_train, y_pred))","2f00fe39":"lasso = Lasso(alpha =0.0005, random_state=0)\nelasticNet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=0)\nkernelRidge = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nsvr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)\ngradientBoosting = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =0)\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =0, nthread = -1)\nlgbm = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, random_state=0)\nrandomForest = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=0)","80d045ad":"scores ={}","1ef1a115":"score = rmsle_cv(lasso)\nprint(\"Lasso:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['lasso'] = (score.mean(), score.std())\nlasso_model = lasso.fit(X_train, y_train)\ny_pred_lasso = lasso_model.predict(X_train)\nrmsle(y_train,y_pred_lasso)","a29a72c6":"score = rmsle_cv(elasticNet)\nprint(\"ElasticNet:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['elasticNet'] = (score.mean(), score.std())\nelasticNet_model = elasticNet.fit(X_train, y_train)\ny_pred_elasticNet = elasticNet_model.predict(X_train)\nrmsle(y_train,y_pred_elasticNet)","3bc82593":"score = rmsle_cv(kernelRidge)\nprint(\"KernelRidge:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['kernelRidge'] = (score.mean(), score.std())\nkernelRidge_model = kernelRidge.fit(X_train, y_train)\ny_pred_kernelRidge = kernelRidge_model.predict(X_train)\nrmsle(y_train,y_pred_kernelRidge)","e040350e":"score = rmsle_cv(svr)\nprint(\"SVR:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['svr'] = (score.mean(), score.std())\nsvr_model = svr.fit(X_train, y_train)\ny_pred_svr = svr_model.predict(X_train)\nrmsle(y_train,y_pred_svr)","d3ac3a64":"score = rmsle_cv(gradientBoosting)\nprint(\"GradientBoostingRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['gradientBoosting'] = (score.mean(), score.std())\ngradientBoosting_model = gradientBoosting.fit(X_train, y_train)\ny_pred_gradientBoosting = gradientBoosting_model.predict(X_train)\nrmsle(y_train,y_pred_gradientBoosting)","44821df8":"score = rmsle_cv(xgb)\nprint(\"XGBRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['xgb'] = (score.mean(), score.std())\nxgb_model = xgb.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_train)\nrmsle(y_train,y_pred_xgb)","efc6a597":"score = rmsle_cv(lgbm)\nprint(\"LGBMRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['lgbm'] = (score.mean(), score.std())\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred_lgbm = lgbm_model.predict(X_train)\nrmsle(y_train,y_pred_lgbm)","9159c981":"score = rmsle_cv(randomForest)\nprint(\"RandomForestRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['randomForest'] = (score.mean(), score.std())\nrandomForest_model = randomForest.fit(X_train, y_train)\ny_pred_randomForest = randomForest_model.predict(X_train)\nrmsle(y_train,y_pred_randomForest)","cc470b17":"def ensemble_models(X):\n    return ((0.1 * lasso_model.predict(X)) +\n            (0.1 * elasticNet_model.predict(X)) +\n            (0.1 * kernelRidge_model.predict(X)) +\n            (0.1 * svr_model.predict(X)) +\n            (0.2 * gradientBoosting_model.predict(X)) + \n            (0.1 * xgb_model.predict(X)) +\n            (0.2 * lgbm_model.predict(X)) +\n            (0.1 * randomForest_model.predict(X)))","c4b90f81":"averaged_score = rmsle(y_train, ensemble_models(X_train))\nscores['averaged'] = (averaged_score, 0)\nprint('RMSLE averaged score on train data:', averaged_score)","2c01d6fe":"def stack_models(X):\n    return ((0.7 * ensemble_models(X)) +\n            (0.15 * lasso_model.predict(X)) +\n#             (0.1 * elasticNet_model.predict(X)) +\n#             (0.1 * gradientBoosting_model.predict(X)) + \n            (0.15 * xgb_model.predict(X))\n#             (0.15 * lgbm_model.predict(X))\n           )","cee50e4c":"stacked_score = rmsle(y_train, stack_models(X_train))\nscores['stacked'] = (stacked_score, 0)\nprint('RMSLE stacked score on train data:', stacked_score)","40a0f896":"sns.set_style(\"white\")\nfig = plt.figure(figsize=(20, 10))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.4f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score', size=20, labelpad=12.5)\nplt.xlabel('Regression Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\nplt.title('Regression Model Scores', size=20)\nplt.show()","41c537c0":"test_predict = np.exp(stack_models(X_test))\nprint(test_predict[:5])","8f593f5b":"sub = pd.DataFrame()\nsub['Id'] = test['Id']\nsub['SalePrice'] = test_predict\nsub.to_csv('submission.csv',index=False)","25db003b":"sub1 = pd.read_csv('submission.csv')\nsub1.head()","3c093a2c":"# Create Dummies","841b8231":"# Train Model","05f33a95":"### Continuous Variables","d7c3d468":"<p> As the continuous variables are all skewed, we will use logarithmic transformation to visualize.","8a5df088":"As we see here for most houses where features are missing the Sales Price is comparatevily low.\nWhy? We will know in a while.","367d7e6c":"# Explore Data","ebd7eb50":"**Don't forget to upvote if you like the kernel.**","33befc86":"<p>While converting variables using logarithmic transformation we see there are only 5 skewed variables - LotFrontage, LotArea, \n1stFlrSF, GrLivArea and SalePrice which has non-zero values. <\/p>","a9b90649":"# Exploratiory Data Analysis","7980d69f":"# Visualize model scores","e8e95593":"Replace the missing values in test set with median since there are outliers","bbfb4b49":"#### Numeric Variables","691f8e98":"We may assume the same numeric features will be skewed in test set as well.","12c0c1c5":"### Setup cross validation strategy","0027a02e":"<p>The first 3 plots here look fine as the recent the year house is built\/remodling done\/garage build, the higher the SalesPrice.\nBut in the 4th plot Sales Price is decreasing as the Year is increasing. Ideally SalesPrice should increase with every passing year.<\/p>","ddded07b":"## Goal\nThe goal is to predict the Sales Price for the test dataset with the given features.","b59b85ee":"##### Since the numeric variables are skewed, we will perform log normal distribution.","a4b483a0":"<p> Lets see the relation between temporal variables and SalesPrice.","fc56d030":"# Read Data","8a92ae51":"### Categorial Variables","361b3e80":"#### Temporal Variables","9210b356":"As we see here that the SalePrice is skewed towards right. \nAnd it is a problem because most of the ML models don't perform well with skewed\/un-normally distributed data. \nSo we have ton apply a log(x) tranform to fix the skew.","b7f8b109":"### Temporal Variables (Date-time variables)\n<p> In the above train dataset we have 4 temporal variables<\/p>","f14e0874":"### Check Missing Values","570a7891":"Replace the missing values in train set with median since there are outliers","8bfd850a":"#### Categorical Variables","9d5aa849":"# Stack Models","6d0eab84":"So now we also know that \"Houses where faeture values are missing have comparatively low price\", because no remodelling or feature enhancements are done recently.","6f92aaf6":"### Discrete Variables","9d4bd4a7":"### Outliers","5cbcb2b0":"#### Dataset for House Price Prediction is from below URL:\n#### https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data    ","11b6ca1e":"##### Handle rare categorical features - which are present in less than 1% of the observations.","489731c1":"<p> So lets see the relation between the first 3 year variables and the Year Sold","49a37715":"<p> So above scatter plot indicates: \n1. The lesser the difference between house YrSold and house year built\/remodling done\/garagebuilt, the higher the Sales Price.\n2. When Sales Price is less then it means the house is old with no\/not recent alterations done.","da1171c9":"# Import packages","d74e993d":"### Numerical Variables","73435c4d":"## Steps-\n    1. Introduction\n    2. Read Data\n    3. Explore Data\n        a. Data size\n        b. Strtucture of Data\n    4. Exploratory Data Analysis & Visualization\n        a. Check Missing Values\n        b. Correlation between missing values and Sales Price\n        c. Check numerical varibale\n        d. Temporal variables and correlation with Sales Price\n        e. Discrete variables and correlation with Sales Price\n        f. Continuous variables, skeweness and outliers\n        g. Categorial variables and correlation with Sales Price\n    5. Missing value Imputation\n    6. Handle Rare Categorial Features\n    7. Label Encoding\n    8. Scaling\n    9. Train models\n        a. Lasso Regression\n        b. Elastic Net Regression\n        c. Kernel Ridge Regression\n        d. Support Vector Regression\n        e. Gradient Boosting Regression\n        f. XGBoost Regression\n        g. Light GBM Regression\n        h. Random Forest Regression\n    10. Stack models\n    11. Visualize model scores","127d3c10":"#### As we see there are many missing values in the train dataset, so lets check the relationship between missing values and the sales price.","4f28b4f7":"# Scaling","078c7dd5":"## Missing value imputation","a08c47e4":"## Description\n\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","0ee0c129":"<p>Plot categorial features with target variable"}}