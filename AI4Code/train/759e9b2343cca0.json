{"cell_type":{"4bc94000":"code","9eb2b8c4":"code","f289640c":"code","4bf31247":"code","5baca59b":"code","9e2bf81c":"code","131d3981":"code","37ab32fa":"code","51278220":"code","d40bb72a":"code","b0fdddaa":"code","5998a3e0":"code","5bc8dc47":"code","cb5ceae9":"code","98cdbb40":"code","5433d1c2":"code","3efed575":"code","5288cffc":"code","3d7508bb":"code","e9f7b2ee":"code","361f5244":"code","770e2e92":"code","3ac71b44":"code","072e869d":"code","3b51d29d":"code","9ea5e334":"code","f744e35d":"code","f6636d1e":"code","f840b76a":"code","cc36b27c":"code","9301c2e1":"code","3f503919":"code","777b3955":"code","62838c40":"code","7d37035b":"code","c76fa529":"code","9339e01c":"code","caef2887":"code","9982f385":"code","5a7b57a1":"code","788fbd7a":"code","7683fdca":"code","257d6439":"code","7b4fe644":"code","21f9179b":"code","15d6afae":"code","a8dd406a":"code","5a35b8ad":"code","3334868d":"code","c3cb9a0a":"code","9f2e25ca":"code","37f754ef":"code","59c7a356":"code","d3ebab73":"code","c7cd913e":"code","770c2c2b":"code","8575c5e5":"code","4546a084":"code","65289ff0":"code","dbdbfe5d":"code","1539a499":"code","4fd6cfe4":"code","58d04776":"code","5848b975":"code","9ed94938":"code","3de0110f":"code","bd0bb793":"code","08f30dd9":"code","e697d4b5":"code","c9698248":"code","48dc3bac":"code","1193be2c":"code","e005353f":"code","44fe724a":"code","6c46ba21":"code","6d8f1bb9":"code","2d59dc4c":"code","a7579d66":"code","9c66048d":"code","333dbd30":"code","514d289f":"code","6d16bbc3":"code","0ced02be":"code","fe5a9e53":"code","67938e67":"markdown","2362f5f1":"markdown","ae5ef136":"markdown","57afce2e":"markdown","604b64a4":"markdown","8262c9d8":"markdown","45056484":"markdown","7d67af2d":"markdown","78bf7573":"markdown","2db15b1d":"markdown","58eb87a1":"markdown","1d71a933":"markdown","4d70993f":"markdown","34708874":"markdown","e16bc338":"markdown","ed353f97":"markdown","635a3dcf":"markdown","ca5d678a":"markdown","9304db50":"markdown","ebed7954":"markdown","1f5c00b5":"markdown","3cf19926":"markdown","de408861":"markdown","7680ea7e":"markdown","d0ad66bc":"markdown","b9941ff8":"markdown","3ff376b1":"markdown","7b6773ea":"markdown","0aa14a47":"markdown","49519b86":"markdown","982c03c3":"markdown","53bda518":"markdown","6218d0ba":"markdown","b74a4dc5":"markdown","0e38723f":"markdown"},"source":{"4bc94000":"#Importing the libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statistics as stat\n\npd.set_option('display.max_columns', 150)\npd.set_option('display.max_rows', 200)\n\n\n\nfrom sklearn import linear_model, metrics\nfrom sklearn.linear_model import Ridge,Lasso\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n\n","9eb2b8c4":"#hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","f289640c":"#lets load the dataframe\nhousing=pd.read_csv('..\/input\/house-price-prediction\/train.csv')\nhousing.head()","4bf31247":"housing.shape","5baca59b":"housing.info()#checking overall values and type of variable ","9e2bf81c":"#checking some more statistical measures of numeric columns and overall it is evident that there are outliers in some variable \n#we will take care of that in EDA \nhousing.describe(percentiles=[0.05,.25, .5, .75, .90, .95, .99]).T\n","131d3981":"''' Checking Null Values '''\n#finding out the total null values and  null percentage in each column \n\ntotal = pd.DataFrame(housing.isnull().sum().sort_values(ascending=False), columns=['Total_null'])\npercentage = pd.DataFrame(round(100*(housing.isnull().sum()\/housing.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Null_Percentage'])\npd.concat([total, percentage], axis = 1)","37ab32fa":"#visualising the percentage null in each columns distributions \nnull_data=round(housing.isnull().sum()\/len(housing)*100,2).sort_values(ascending=False)\nplt.figure(figsize=[12,5],dpi=150)\nplt.rc('xtick', labelsize=8)\nplt.rc('ytick', labelsize=7)\nnull_data = null_data[null_data>0]\nnull_data.plot.bar()\nplt.show()\n","51278220":"#making all numeric columns( integer and float ) in one dataset \nhousing_num = housing.select_dtypes(include=['float64', 'int64'])\nprint(housing_num.shape)\nhousing_num.head()\n\n","d40bb72a":"# making all Categoric variables(object type ) in one  dataset\nhousing_obj = housing.select_dtypes(exclude=['float64', 'int64'])\nprint(housing_obj.shape)\nhousing_obj.head()\n","b0fdddaa":"#checking the null values before imputation present in the numerical dataframe \n\nfor column in housing_num.columns.values:\n    if housing_num[column].isnull().values.sum() != 0:\n        missing_percentage=housing_num[column].isnull().values.sum()\/len(housing_num)\n        print(column, missing_percentage)","5998a3e0":"housing_num[['LotFrontage','MasVnrArea','GarageYrBlt']].describe()","5bc8dc47":"#Imputing `LotFrontage`,`MasVnrArea`,`GarageYrBlt` with  mean values \n\nhousing_num['LotFrontage'].fillna(housing_num['LotFrontage'].mean(),inplace=True) \nhousing_num['MasVnrArea'].fillna(housing_num['MasVnrArea'].mean(),inplace=True)   \nhousing_num['GarageYrBlt'].fillna(housing_num['GarageYrBlt'].mean(),inplace=True) ","cb5ceae9":"#Rechecking the Null values present int he dataset after imputation \n\nhousing_num.isnull().sum()\n","98cdbb40":"housing_num","5433d1c2":"for i in housing_num:\n    print(i,':\\n',housing_num[i].value_counts(dropna=False),sep='',end='\\n--------------------------\\n\\n')","3efed575":"housing_num.info()","5288cffc":"housing_num.drop(columns=['Id'],inplace=True)\n","3d7508bb":"plt.figure(figsize=(6,3.2),dpi=150)\nplt.rc('xtick', labelsize=6)\nplt.rc('ytick', labelsize=6)\n\nsns.distplot(housing_num['SalePrice'])\nplt.show()\n\n","e9f7b2ee":"#log transforming the  predictor variable\nhousing_num['SalePrice'] = np.log1p(housing_num['SalePrice'])","361f5244":"#checking post -trasnform \nplt.figure(figsize=(6,3.2),dpi=150)\nplt.rc('xtick', labelsize=6)\nplt.rc('ytick', labelsize=6)\nsns.distplot(housing_num['SalePrice'])\nplt.show()\n","770e2e92":"for values in housing_num:  \n    plt.figure(figsize=(8,6))\n    plt.rc('xtick', labelsize=11)\n    plt.rc('ytick', labelsize=11)\n    sns.boxplot(x=values,data=housing_num)\nplt.show()","3ac71b44":"for features in housing_num.columns.values:\n    plt.figure(figsize=(5,4))\n    plt.scatter(housing_num[features],housing_num['SalePrice'], alpha = 0.3)\n    plt.title(\"SalePrice vs \"+str(features))\n    plt.xlabel(str(features))\n    plt.ylabel('SalePrice')\nplt.show()","072e869d":"# Converting years to age\nhousing_num['YearBuilt_Age'] = housing_num['YearBuilt'].max() - housing_num['YearBuilt']\nhousing_num['YearRemodAdd_Age'] = housing_num['YearRemodAdd'].max() - housing_num['YearRemodAdd']\nhousing_num['YrSold_Age'] = housing_num['YrSold'].max() - housing_num['YrSold']\nhousing_num['GarageYrBlt_Age'] = housing_num['GarageYrBlt'].max() - housing_num['GarageYrBlt']\n\n# Dropping columns\nhousing_num.drop(['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt'], axis=1, inplace = True)","3b51d29d":"housing_num[['YearBuilt_Age','YearRemodAdd_Age','YrSold_Age','GarageYrBlt_Age']].head(10)","9ea5e334":"fig = plt.figure(figsize=(25,23),dpi=150)\nplt.rc('xtick', labelsize=16)\nplt.rc('ytick', labelsize=16)\nsns.heatmap(housing_num.corr(), annot = True, cmap=\"Greens\",fmt='.1f')\nplt.show()","f744e35d":"#removing the features \nhousing_num.drop(columns=['GarageArea','GarageYrBlt_Age','YearRemodAdd_Age','1stFlrSF','TotRmsAbvGrd','YearBuilt_Age','YrSold_Age'],inplace=True)\n     ","f6636d1e":"#dropping those non coreelated varibles\nhousing_num.drop(columns=['PoolArea','LowQualFinSF','MSSubClass','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal','BsmtFinSF2','BsmtHalfBath'],inplace=True)\n     \n","f840b76a":"#after doing some manual feature elimination checking heatmap again \nfig = plt.figure(figsize=(15,12),dpi=150)\nplt.rc('xtick', labelsize=12)\nplt.rc('ytick', labelsize=12)\nsns.heatmap(housing_num.corr(), annot = True, cmap=\"Greens\",fmt='.1f')\nplt.show()","cc36b27c":"#checking the null values before imputation present in the numerical dataframe \n\nfor column in housing_obj.columns.values:\n    if housing_obj[column].isnull().values.sum() != 0:\n        missing_percentage=(housing_obj[column].isnull().values.sum()\/len(housing_num))\n        print(column, missing_percentage)","9301c2e1":"#chceking the value counts present in each categorical columns \nfor i in housing_obj:\n    print(i,':\\n',housing_obj[i].value_counts(dropna=False),sep='',end='\\n--------------------------\\n\\n')","3f503919":"# Remove the Columns with Majority NaN Values in it which Include\nhousing_obj.drop(columns=['Alley','PoolQC','Fence','MiscFeature'],inplace=True)\n\n\nhousing_obj['FireplaceQu'].fillna('No Fireplace',inplace=True)                  \nhousing_obj['MasVnrType'].fillna(stat.mode(housing_obj['MasVnrType']),inplace=True)   \nhousing_obj['Electrical'].fillna(stat.mode(housing_obj['Electrical']),inplace=True)   \nhousing_obj['BsmtQual'].fillna('No Basement',inplace=True)                          \nhousing_obj['BsmtCond'].fillna('No Basement',inplace=True)                      \nhousing_obj['BsmtExposure'].fillna('No Basement',inplace=True)               \nhousing_obj['BsmtFinType1'].fillna('No Basement',inplace=True)           \nhousing_obj['BsmtFinType2'].fillna('No Basement',inplace=True)                  \nhousing_obj['GarageType'].fillna('No Garage',inplace=True)                      \nhousing_obj['GarageFinish'].fillna('No Garage',inplace=True)         \nhousing_obj['GarageQual'].fillna('No Garage',inplace=True)                      \nhousing_obj['GarageCond'].fillna('No Garage',inplace=True)                   ","777b3955":"#so dropping dominant  variables \nhousing_obj.drop(columns=['Street','Utilities','Condition2','RoofMatl','Heating','Functional','PavedDrive','GarageCond','Electrical','LandSlope','BsmtFinType2'],inplace=True)","62838c40":"#lets check the Null values again after imputation \nhousing_obj.isnull().sum()","7d37035b":"# creating dummy variables for categorical variables\n# convert into dummies - one hot encoding\nhousing_dummies = pd.get_dummies(housing_obj, drop_first=True)\nprint(housing_dummies.shape)\nhousing_dummies.head()","c76fa529":"#making the final clean dataset to build the model \ndf=pd.concat([housing_num,housing_dummies],axis=1)","9339e01c":"X= df.drop('SalePrice',axis=1)\ny= df['SalePrice']","caef2887":"X.shape","9982f385":"X.head()","5a7b57a1":"y.shape","788fbd7a":"y.head()","7683fdca":"#splitting train and test columns using sklearn library \nnp.random.seed(0)\nX_train, X_test,y_train,y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)","257d6439":"#cheking shape of train test data\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7b4fe644":"scaler = StandardScaler()\n\ncols=list(housing_num.columns.values)\ncols.remove('SalePrice')\n\nX_train[cols] = scaler.fit_transform(X_train[cols])\nX_train.head()","21f9179b":"y_train.head()","15d6afae":"X_test[cols] = scaler.fit_transform(X_test[cols])\nX_test.head()","a8dd406a":"from sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(linreg, 40)\nrfe = rfe.fit(X_train, y_train)\nprint(rfe.support_)","5a35b8ad":"col=X_train.columns[rfe.support_]\ncol","3334868d":"import statsmodels.api as sm\nX_train_rfe = sm.add_constant(X_train[list(col)])","c3cb9a0a":"lm = sm.OLS(y_train, X_train_rfe).fit()\nprint(lm.summary())","9f2e25ca":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nridge = Ridge()\n\n# cross validation :\n\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","37f754ef":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results = cv_results[cv_results['param_alpha']<=200]\ncv_results[['param_alpha','params','mean_test_score','mean_train_score']]","59c7a356":"model_cv.best_params_","d3ebab73":"model_cv.best_score_","c7cd913e":"# plotting mean test and train scores with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n# plotting\nplt.figure(figsize=(10,8))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","770c2c2b":"alpha = 10.0\nridge = Ridge(alpha=alpha)\nridge.fit(X_train, y_train)\ny_pred_ridge_train=ridge.predict(X_train)\nprint('Train R2 Square : ',round(r2_score(y_train,y_pred_ridge_train),2))\ny_pred_ridge_test=ridge.predict(X_test)\nprint('Test R2 Square : ',round(r2_score(y_test,y_pred_ridge_test),2))","8575c5e5":"sns.distplot((y_train-y_pred_ridge_train))","4546a084":"#Ridge model parameters\nmodel_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 2) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nvar_coef = list(zip(cols, model_parameters))\nvar_coef = [x for x in var_coef if abs(x[-1] != 0)]\nvar_coef","65289ff0":"df1 = {'Feature':list(list(zip(*var_coef))[0]),'Coeff':list(list(zip(*var_coef))[1])}\nridge_params = pd.DataFrame(data = df1)\n","dbdbfe5d":"ridge_params.reindex(ridge_params.Coeff.abs().sort_values(ascending = False).index)","1539a499":"lasso = Lasso()\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","4fd6cfe4":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results[['param_alpha','params','mean_test_score','mean_train_score']]","58d04776":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(10,8))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","5848b975":"model_cv.best_params_","9ed94938":"model_cv.best_score_","3de0110f":"alpha = 0.0001\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train)\ny_pred_lasso_train=lasso.predict(X_train)\nprint('Train R2 Square : ',round(r2_score(y_train,y_pred_lasso_train),2))\ny_pred_lasso_test=lasso.predict(X_test)\nprint('Test R2 Square : ',round(r2_score(y_test,y_pred_lasso_test),2))\n#lasso.coef_","bd0bb793":"sns.distplot((y_train-y_pred_lasso_train))","08f30dd9":"#lasso model parameters\nmodel_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nvar_coeff = list(zip(cols, model_parameters))\nvar_coeff = [x for x in var_coeff if abs(x[-1] != 0)]\nvar_coeff","e697d4b5":"df2 = {'Feature':list(list(zip(*var_coef))[0]),'Coeff':list(list(zip(*var_coef))[1])}\nlasso_params = pd.DataFrame(data = df2)\n","c9698248":"lasso_params.reindex(lasso_params.Coeff.abs().sort_values(ascending = False).index)","48dc3bac":"print('r2_score in train dataset:')\nprint('r2_score for ridge:', round(r2_score(y_train, y_pred_ridge_train), 2))\nprint('r2_score for lasso:', round(r2_score(y_train, y_pred_lasso_train), 2))\n\nprint('r2_score in test dataset:')\nprint('r2_score for ridge:', round(r2_score(y_test, y_pred_ridge_test), 2))\nprint('r2_score for lasso:', round(r2_score(y_test, y_pred_lasso_test), 2))","1193be2c":"#lets make the alpha double and compare the outcome for Ridge\nalpha = 20\nridge = Ridge(alpha=alpha)\nridge.fit(X_train, y_train)\ny_pred_ridge_train=ridge.predict(X_train)\nprint(round(r2_score(y_train,y_pred_ridge_train),2))\ny_pred_ridge_test=ridge.predict(X_test)\nprint(round(r2_score(y_test,y_pred_ridge_test),2))","e005353f":"model_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nvar_coeff = list(zip(cols, model_parameters))\nvar_coeff = [x for x in var_coeff if abs(x[-1] != 0)]\ndf3 = pd.DataFrame.from_records(var_coeff, columns =['Features', 'B-Coeff'])\ndf3['B-Coeff']=df3['B-Coeff'].abs()\ndf3=df3.sort_values(by=['B-Coeff'],ascending=False)\ndf3\n","44fe724a":"print('Top 5 Predictor Variables using Ridge after Doubling the Alpha :',df3[1:6].values)","6c46ba21":"#for Lasso doubling the value of alpha\nalpha = 0.002\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train) \ny_pred_lasso_train=lasso.predict(X_train)\nprint(round(r2_score(y_train,y_pred_lasso_train),2))\ny_pred_lasso_test=lasso.predict(X_test)\nprint(round(r2_score(y_test,y_pred_lasso_test),2))","6d8f1bb9":"model_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nvar_coeff = list(zip(cols, model_parameters))\nvar_coeff = [x for x in var_coeff if abs(x[-1] != 0)]\ndf = pd.DataFrame.from_records(var_coeff, columns =['Features', 'B-Coeff'])\ndf['B-Coeff']=df['B-Coeff'].abs()\ndf=df.sort_values(by=['B-Coeff'],ascending=False)\nprint('Top 5 Predictor Variables using Lasso after Doubling the Alpha :',df[1:6].values)\n","2d59dc4c":"X_train=X_train.drop(['Neighborhood_Crawfor','GrLivArea' , 'Neighborhood_NridgHt', 'OverallQual', 'Neighborhood_Somerst'],axis=1)\nX_test=X_test.drop(['Neighborhood_Crawfor','GrLivArea' , 'Neighborhood_NridgHt', 'OverallQual', 'Neighborhood_Somerst'],axis=1)","a7579d66":"lasso = Lasso()\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) \n","9c66048d":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results[['param_alpha','params','mean_test_score','mean_train_score']]","333dbd30":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(10,8))\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","514d289f":"model_cv.best_params_","6d16bbc3":"alpha = 0.0001\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train) \ny_pred_lasso_train=lasso.predict(X_train)\nprint('Train R2 Square : ',round(r2_score(y_train,y_pred_lasso_train),2))\ny_pred_lasso_test=lasso.predict(X_test)\nprint('Test R2 Square : ',round(r2_score(y_test,y_pred_lasso_test),2))","0ced02be":"model_parameters = list(lasso.coef_)\nmodel_parameters.insert(0, lasso.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nvar_coeff = list(zip(cols, model_parameters))\nvar_coeff = [x for x in var_coeff if abs(x[-1] != 0)]\ndf = pd.DataFrame.from_records(var_coeff, columns =['Features', 'B-Coeff'])\ndf['B-Coeff']=df['B-Coeff'].abs()\ndf=df.sort_values(by=['B-Coeff'],ascending=False)","fe5a9e53":"print('Top 5 Predictor Variables using Lasso after Droping 5 Important Predictor Variables :',df[1:6].values)","67938e67":"### Descriptive analysis of target variable \" SalePrice\"","2362f5f1":"## Data scaing ","ae5ef136":"### Insights from Null Values \n\n- From both the table and visualization it is clear that there are few columns which contains high Null values i. more than 80 %,they are as follows \n\n\n                   `PoolQC`\n                   `MiscFeature\n                   `Alley`\n                   `Fence`\n- we will drop these columns \n- The remaining columns which have Null values under 20 percent we are going to examine them and do the imputation further .\n    \n### Now delve into the data set and understand it separately for `Numerical Columns ` and `Categorical Columns`.\n\n","57afce2e":" `1.From the above observation we can conclude that ``ID`` is contains unique values and not essential for the modeling so we can drop it .`\n\n-`` 2.there are few columns which have only some discrete values so it can be categorised ,so we need to change its data type from `int` to `obj``","604b64a4":"`Question 3`\n\n`After building the model, you realised that the five most important predictor variables in the lasso model are not available in the incoming data. You will now have to create another model excluding the five most important predictor variables. Which are the five most important predictor variables now?`","8262c9d8":"# EDA - Explorartory Data Analysis\n#### Let's understand and examine the data in order to get a clean dataset to work on further \n\n- we will deal with the Null values and impute it \n- we are going to look for the outlier and treat them to make the data clean \n- we will make make some **Visualization** to have a clear picture how the variables are behaving before proceeding to **Model building **.\n\nFor visualisation we are using `Matplotlib ` and `seaborn` \n","45056484":"# Assignment Questions and answers ","7d67af2d":"# Inferences\n","78bf7573":"### Insights \n","2db15b1d":"## 3. Data Preparation \n\n\n#### Data Preparation\n\nLet's now prepare the data and build the model.","58eb87a1":"- from the above correlation heatmap we have analysed that there are some multi colinearity is going on on a high scale with the target variable \"saleprice\"\n- they are as follows \n\n                   1.OverallQual-0.8\n                   2.TotalBsmtSF-0.6       \n                   3.1stFlrSF   -0.6\n                   4.GrLivArea  -0.6       \n                   5.FullBath   -0.7 \n                   6.GarageCars -0.7\n                   7.GarageArea -0.7\n                   \n\n        \n- All the above variables have colinnearity threshhold more than 0.5 so we need to eliminate these features\n        \n- there are few other variables who shows the multicolinearity among themselves \n-  they are as follows\n`\n                     1.TotalBsmtSF & 1stFlrSF\n                     2.GarageCars & GarageArea\n                     3.YearBuilt_Age & YearRemodAdd_age\n                     4.YearBuilt_Age   & GarageYrBlt_Age  \n\n   `\n  - we need to drop these variables as well\n                   \n                     \n                                \n","1d71a933":"## Model Building\n\n### RFE\n\n`First we will go with RFE for feature selaction of top 40 features and build a basic Regression Model `\n","4d70993f":"`we have done both Ridge and Lasso Regression in the dataset \nwe have concluded top 10 variables which manupulates the price of houses and can be used to the best for our business purpose `\n\n`so the predictor variables we got from Ridge Regularization ` :\n\n                            1.MSZoning(RH,RM,FV,RL)\n                            2.SaleType_ConLD\n                            3.Neighborhood_(Crawfor,MeadowV,StoneBr,Somerst)\n                            4.GrLivArea\n                            6.LandContour_Low\n                            7.Exterior1st_BrkFace\n                            8.KitchenQual_Fa\n                            9.OverallQual`\n                           \n\n                            \n  `The predictor variables from Lasso Regularization :`\n  \n                           \n                            1.MSZoning(RH,RM,FV,RL)\n                            2.SaleType_ConLD\n                            3.Neighborhood_(Crawfor,MeadowV,StoneBr,Somerst)\n                            4.GrLivArea\n                            5.BldgType_Twnhs\n                            6.LandContour_Low\n                            7.GarageQual_Gd\n                            8.KitchenQual_Fa\n                            9.OverallQual\n                            10.Exterior1st_BrkFace\n                           `\n         ","34708874":"- Now the target variable is rightly skewed ","e16bc338":"### lets first check  if there is any relatioship going on with target variable visually by scatter plot ","ed353f97":"` 2 Question 2`\n\n `You have determined the optimal value of lambda for ridge and lasso regression during the assignment. Now, which o   one will you choose to apply and why? `","635a3dcf":"### Splitting the Data into Training and Testing Sets","ca5d678a":"### Let's check the value counts in each column ","9304db50":"### Imputation of Numerical Columns ","ebed7954":"`What is the optimal value of alpha for ridge and lasso regression? What will be the changes in the model if you choose double the value of alpha for both ridge and lasso? What will be the most important predictor variables after the change is implemented?`","1f5c00b5":"`What is the optimal value of alpha for ridge and lasso regression?`\n","3cf19926":"## Ridge Regression - L2","de408861":"###  There are some derived Metric present in the dataset lets treat them \n","7680ea7e":"## Lasso Regularization - L1","d0ad66bc":"- There are 38 numerical columns and 43 non-numeric or object type columns present in the dataset ","b9941ff8":"`We have performed both Lasso and Ridge and it is very much evnident that we should use Lasso over Ridge as `\n      - Ridge dont dont do feature elimination and takes all teh varible into consideration where as Lasso do feature elimination by means of applying hard penality making the coeeficient  to shink to absolute zero but Ridge minimises  the coeeficient tend to zero .\n      \n      -So lasso makes the model more simple and deduct the unnecessary columns for large datasets \n      \n      -So here we are gonna use Lasso over Ridge \n      \n      ","3ff376b1":"## understanding categorical variable \n\n### checking Null values and impute them \n","7b6773ea":"## Understanding Numerical Columns ","0aa14a47":"##### There are few columns who are highly un correlated withe the target variable concluded both from the scatter plot and heatmap \nthey are \n`'YrSold','MoSold','PoolArea','LowQualFinSF','MSSubClass','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal''","49519b86":"- `SalePrice` is our target varible and we need to check certain things about it before proceeding further .\n-  we need to check the distibution of the dependednt variable must be following `normal distribution` in order tto satisfy the assumptions of `Linear Regression` .\n- we can check this by visualizing it in `dist plot`.","982c03c3":"### Now let's check outliers in the dataframe \n- we will visualize with boxplot \n","53bda518":"- there are no more null values in the numeric dataset ","6218d0ba":"   - we can see that `SalePrice` is rightly skewed.\n   - we need to transform it with `log` to make it rightly distributed normally and perform `linear regression`","b74a4dc5":"`The optimal values of lambda i.e alpha for Ridge =10\nThe optimal values of lambda i.e alpha for Lasso=0.001`","0e38723f":"### we observe that \n\n\n1.Alley,PoolQC,Fence,MiscFeature - these variables have very high Null value i.e more than 95 percent so we can drop these from the data set .\n\n2. columns have largely one value present so we can drop these columns as well.they are \n                                `Street,Utilities,'Condition2','RoofMatl','Heating','Functional','PavedDrive','GarageCond','Electrical','LandSlope'\n\n3.There are some Misinterpretation in the dataset so we have done the necessary imputations in the dataset ."}}