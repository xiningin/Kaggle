{"cell_type":{"4bd3dc59":"code","61253be5":"code","1d4e5edd":"code","c4706071":"code","0c7afa1d":"code","cefc4cc7":"code","b7caa8f9":"code","69eb135b":"code","e299532b":"code","253c8e0c":"code","a9491877":"code","aed7f074":"code","a00f33ad":"code","34adee3d":"code","60d5056e":"code","254632b2":"code","592537d3":"markdown","d30a42fe":"markdown","3d2bec2f":"markdown","a48db3e3":"markdown","5d74868b":"markdown","1f898f7d":"markdown","f1fb0cd3":"markdown","fcabe6ee":"markdown","bfa81805":"markdown","99bf05db":"markdown","a23d7114":"markdown","0c739952":"markdown","e7d58c12":"markdown","31d2a805":"markdown","d8cef466":"markdown","6c81a77b":"markdown","518b12c7":"markdown"},"source":{"4bd3dc59":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","61253be5":"def sigmoid(x):\n    s = 1\/(1+np.exp(-x))\n    ds = s*(1-s)\n    return(s,ds)\n","1d4e5edd":"x = np.arange(-6,6,0.01)","c4706071":"sigmoid(x)[1].shape","0c7afa1d":"#Setup centeres axes\nfig, ax = plt.subplots(figsize=(9,5))\n\nax.spines['left'].set_position('center')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n#Create and show plot\nax.plot(x,sigmoid(x)[0], color=\"#307EC7\", linewidth=3, label=\"sigmoid\")\nax.plot(x,sigmoid(x)[1], color=\"#9621E2\", linewidth=3, label=\"derivative\")\n\nax.legend(loc=\"upper left\", frameon=False)\nfig.show()","cefc4cc7":"def tanh_activation(z):\n    s = (np.exp(z)-np.exp(-z))\/(np.exp(z)+np.exp(-z))\n    ds = 1-s**2\n    return(s,ds)","b7caa8f9":"z = np.arange(-4,4,0.01)","69eb135b":"#Setup centeres axes\nfig, ax = plt.subplots(figsize=(9,5))\n\nax.spines['left'].set_position('center')\nax.spines['bottom'].set_position('center')\n\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n#Create and show plot\nax.plot(z,tanh_activation(z)[0], color=\"#307EC7\", linewidth=3, label=\"Tanh\")\nax.plot(z,tanh_activation(z)[1], color=\"#9621E2\", linewidth=3, label=\"derivative\")\n\nax.legend(loc=\"upper left\", frameon=False)\nfig.show()","e299532b":"def Relu_activation(r):\n    return(np.maximum(r,0))","253c8e0c":"r = np.arange(-4,4,1)\nRelu_activation(r)","a9491877":"#Setup centeres axes\nfig, ax = plt.subplots(figsize=(9,5))\n\nax.spines['left'].set_position('center')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n#Create and show plot\nax.plot(r,Relu_activation(r), color=\"#307EC7\", linewidth=3, label=\"Relu\")\n\nax.legend(loc=\"upper left\", frameon=False)\nfig.show()","aed7f074":"def leakRelu_activation_1(r):\n    return(np.where(r>0,r,r*0.1))\n\ndef leakRelu_activation_2(r):\n    return(np.where(r>0,r,r*0.2))","a00f33ad":"#Setup centeres axes\nfig, ax = plt.subplots(figsize=(9,5))\n\nax.spines['left'].set_position('center')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n#Create and show plot\nax.plot(r,leakRelu_activation_1(r), color=\"#307EC7\", linewidth=3, label=\"Relu_0.1\")\nax.plot(r,leakRelu_activation_2(r), color=\"#9621E2\", linewidth=3, label=\"Relu_0.2\")\n\nax.legend(loc=\"upper left\", frameon=False)\nfig.show()","34adee3d":"def soft_max(r):\n    s = np.exp(r)\/sum(np.exp(r))\n    return(s)","60d5056e":"rs = np.arange(0,8,0.1)\nrs_soft = soft_max(rs)\nprob_sum = np.sum(rs_soft)\nprint(prob_sum)","254632b2":"#Setup centeres axes\nfig, ax = plt.subplots(figsize=(9,5))\n\nax.spines['left'].set_position('center')\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\n\nax.xaxis.set_ticks_position('bottom')\nax.yaxis.set_ticks_position('left')\n\n#Create and show plot\nax.plot(rs,soft_max(rs), color=\"#307EC7\", linewidth=3, label=\"Softmax\")\n\nax.legend(loc=\"upper left\", frameon=False)\nfig.show()","592537d3":"ReLu Activation Function\n\nEquation :- A(x) = max(0,x). It gives an output x if x is positive and 0 otherwise.","d30a42fe":"Observations:\n\n(i) If you see the sigmoid function has values between 0 to 1\n\n(ii) The output is not Zero-Centered\n\n(iii) Sigmoids saturate and kill gradients.\n\n(iv) see at top and bottom level of sigmoid functions the curve changes slowly,if you calculate slope(gradients) it is zero,that is shown in derivative curve above.\n\nProblem with sigmoid:\n\ndue to this when the x value is small or big the slope is zero\u2014 ->then there is no learning \u2014 ->then there is no learning.\n\nWhen we will use Sigmoid:\n\n(i) if you want output value between 0 to 1 use sigmoid at output layer neuron only\n\n(ii) when you are doing binary classification problem use sigmoid\n\notherwise sigmoid is not preferred","3d2bec2f":"----------------------------------------------------------------------","a48db3e3":"Value Range :- [0, inf)\n\nNature :- non-linear, which means we can easily backpropagate the errors and have multiple layers of neurons being activated by the ReLU function.\n\nUses :- ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations. At a time only a few neurons are activated making the network sparse making it efficient and easy for computation.\n\nit avoids and rectifies vanishing gradient problem . Almost all deep learning Models use ReLu nowadays.\n\nBut its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n\nAnother problem with ReLu is that some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.\n\nTo fix this problem another modification was introduced called Leaky ReLu to fix the problem of dying neurons. It introduces a small slope to keep the updates alive.\n\nWe then have another variant made form both ReLu and Leaky ReLu called Maxout function .","5d74868b":"----------------------------------------------------------------------","1f898f7d":"Softmax Function","f1fb0cd3":"Sigmoid and Derivative","fcabe6ee":"The tanh function is just another possible functions that can be used as a nonlinear activation function between layers of a neural network. It actually shares a few things in common with the sigmoid activation function. They both look very similar. But while a sigmoid function will map input values to be between 0 and 1, Tanh will map values to be between -1 and 1.","bfa81805":"----------------------------------------------------------------------","99bf05db":"LeakyRelu Activation Function","a23d7114":"Tanh=(e^z-e^(-z))\/(e^z+e^(-z)\n\nDerivative of tanh(z):\n\na=(e^z-e^(-z))\/(e^z+e^(-z)\n\nuse same u\/v rule\n\nda=[(e^z+e^(-z))*d(e^z-e^(-z))]-[(e^z-e^(-z))*d((e^z+e^(-z))]\/[(e^z+e^(-z)]\u00b2\n\nda=[(e^z+e^(-z))*(e^z+e^(-z))]-[(e^z-e^(-z))*(e^z-e^(-z))]\/[(e^z+e^(-z)]\u00b2\n\nda=[(e^z+e^(-z)]\u00b2-[(e^z-e^(-z)]\u00b2\/[(e^z+e^(-z)]\u00b2\n\nda=1-[(e^z-e^(-z))\/(e^z+e^(-z)]\u00b2\n\nda=1-a\u00b2\n\n","0c739952":"Tanh and Derivative","e7d58c12":"1)Sigmoid:\n\nIt is also called as logistic activation function.\n\nf(x)=1\/(1+exp(-x) the function range between (0,1)\n\nDerivative of sigmoid:\n\njust simple u\/v rule i.e (vdu-udv)\/v\u00b2\n\ndf(x)=[(1+exp(-x)(d(1))-d(1+exp(-x)*1]\/(1+exp(-x))\u00b2\n\nd(1)=0,\n\nd(1+exp(-x))=d(1)+d(exp(-x))=-exp(-x) so\n\ndf(x)=exp(-x)\/(1+exp(-x))\u00b2\n\ndf(x)=[1\/(1+exp(-x))]*[1-(1\/(1+exp(-x))]\n\ndf(x)=f(x)*(1-f(x))","31d2a805":"Softmax turns arbitrary real values into probabilities, which are often useful in Machine Learning. The math behind it is pretty simple: given some numbers,\n\nThe outputs of the Softmax transform are always in the range [0,1] and add up to 1. Hence, they form a probability distribution.\n\nf(xs) = np.exp(xs) \/ sum(np.exp(xs))","d8cef466":"Observations:\n\n(i)Now it\u2019s output is zero centered because its range in between -1 to 1 i.e -1 < output < 1 .\n\n(ii) Hence optimization is easier in this method hence in practice it is always preferred over Sigmoid function .\n\nBut still it suffers from Vanishing gradient problem.\n\nWhen will use:\n\nUsually used in hidden layers of a neural network as it\u2019s values lies between-1 to 1 hence the mean for the hidden layer comes out be 0 or very close to it, hence helps in centering the data by bringing mean close to 0. This makes learning for the next layer much easier.","6c81a77b":"----------------------------------------------------------------------","518b12c7":"Neural Netwok: Activation Functions"}}