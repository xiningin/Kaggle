{"cell_type":{"4ae8eb7b":"code","9d5f7c9e":"code","2618ac4c":"code","f8eb851b":"code","8c4c7139":"code","36aa4e0d":"code","c5437f4c":"code","d1f13b70":"code","1ac733df":"code","9508e68f":"code","fdf0bb86":"code","0be8d6b8":"code","bed6041e":"code","0418dc39":"code","735c2f49":"code","4b46b464":"code","b5022bbd":"code","66946b3f":"code","55b2e974":"code","cbb1ac06":"code","5209a66e":"code","326c43fa":"code","a2d0d3e2":"code","ca98d2a0":"code","a68053c6":"code","f3bafe80":"code","a64fa818":"code","d1536191":"code","785173f3":"code","748b1a06":"code","c1502e8f":"code","b5d84c8c":"code","2b31e2fd":"code","9c044c89":"code","580fe271":"code","393303bc":"code","8512d7ed":"code","64cea4b7":"code","2ec978bb":"code","9a5e5712":"code","a5591eb7":"code","74caf13b":"code","abde8c51":"code","8f9a8464":"code","828082c2":"code","4c0bb16e":"code","ae23ce13":"code","2913eca9":"code","94b94ce4":"code","01c67ed5":"code","3156aacf":"code","00520765":"code","f7a28f0d":"code","471bbfb4":"code","f7d01adb":"code","11520ef5":"code","a1563fe5":"code","74019d7b":"code","6dc9422c":"code","ed7887ce":"code","2795a47f":"code","2f29a841":"code","b0f10cf6":"code","f2c543c1":"code","422f1d75":"code","cec67021":"code","1b858ed3":"code","f99f2e9f":"code","5b8f61a6":"code","fdd33341":"markdown","901e4d4c":"markdown","66a9fc98":"markdown","99998b98":"markdown","33c9e16d":"markdown","869b5047":"markdown","b34501dc":"markdown","fffe5267":"markdown","2cceacf6":"markdown","1bce284d":"markdown","d61fda56":"markdown","5c9a8551":"markdown","ae6bfc77":"markdown","3211c3df":"markdown","a820fc4f":"markdown","3ff3f635":"markdown","4fba5a42":"markdown","b260c954":"markdown","5faac0dd":"markdown","03c2d491":"markdown","7ed24c58":"markdown","a6c94962":"markdown","ee5d54f4":"markdown","49d3f2a9":"markdown"},"source":{"4ae8eb7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d5f7c9e":"df_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')","2618ac4c":"df_train.columns","f8eb851b":"df_train = df_train.drop('id', axis = 1)","8c4c7139":"df_train.head()","36aa4e0d":"df_train.describe()","c5437f4c":"df_train.info()","d1f13b70":"import matplotlib.pyplot as plt\nimport seaborn as sns","1ac733df":"plt.figure(figsize = (15,15))\nplt.boxplot(df_train.iloc[:, :-1], vert = False, showfliers=True)\nplt.show()","9508e68f":"df_ot3 = df_train.quantile(0.95)\ndf_ot4 = df_train > (df_ot3*1.5)\ndf_ot4.shape","fdf0bb86":"for i in range(0,50):\n    print(\"Feature \\n\",i)\n    print(df_ot4.iloc[:,i].value_counts())","0be8d6b8":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder","bed6041e":"X = df_train.iloc[:, :-1]\nX.head()","0418dc39":"y = df_train.iloc[:, -1]\ny","735c2f49":"Lenc = LabelEncoder()\nLenc.fit(y)\ny = Lenc.transform(y)","4b46b464":"y","b5022bbd":"mnmx_scl = MinMaxScaler()\nmnmx_scl.fit(X)\nX = mnmx_scl.transform(X)","66946b3f":"X_df = pd.DataFrame(X)\nX_df.describe()","55b2e974":"from sklearn.model_selection import train_test_split","cbb1ac06":"X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size = 0.4, stratify = y)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","5209a66e":"dpi = 200\nplt.figure(figsize = (10,10))\nplt.hist(y_train, label = 'y train data class count')\nplt.hist(y_test, label = 'y test data class count')\nplt.legend()","326c43fa":"from xgboost import XGBClassifier\nimport xgboost\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, make_scorer","a2d0d3e2":"xgb_clf = XGBClassifier(learning_rate = 0.1, n_estimators = 200,use_label_encoder = False, verbose= None, objective = 'multi:softmax', eval_metric = 'mlogloss',eval_set = [X_test, y_test])\n\nxgb_clf.fit(X_train, y_train)","ca98d2a0":"y_pred_xgb_pr = xgb_clf.predict_proba(X_test)\n\ny_pred_xgb = xgb_clf.predict(X_test)","a68053c6":"acc_scr_xgb = accuracy_score(y_test, y_pred_xgb)\nacc_scr_xgb","f3bafe80":"auc_score_xgb = roc_auc_score(y_test, y_pred_xgb_pr, multi_class = 'ovr')\nauc_score_xgb","a64fa818":"clf_xgb = classification_report(y_test, y_pred_xgb)\nprint(clf_xgb)","d1536191":"cfm_xgb = confusion_matrix(y_test, y_pred_xgb)","785173f3":"plt.figure(figsize = (8, 6))\nsns.heatmap(cfm_xgb, annot = True)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","748b1a06":"from sklearn.decomposition import PCA","c1502e8f":"n_comp = np.array([25,30,35,40])\n\nacc_scr_xgb2 = np.linspace(0,0,5)\nauc_score_xgb2 = np.linspace(0,0,5)","b5d84c8c":"xgb_clf2 = XGBClassifier(learning_rate = 0.1, n_estimators = 200,use_label_encoder = False, verbose= None, objective = 'multi:softmax', eval_metric = 'mlogloss')","2b31e2fd":"for i,j in enumerate(n_comp):\n    pca = PCA(n_components = j)\n    pca.fit(X)\n    X_pca = (pca.transform(X))\n    X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size = 0.4, stratify = y)\n    xgb_clf2.fit(X_pca_train, y_train)\n    y_pred_xgb_pr2 = xgb_clf2.predict_proba(X_pca_test)\n    y_pred_xgb2 = xgb_clf2.predict(X_pca_test)\n    acc_scr_xgb2[i] = accuracy_score(y_test, y_pred_xgb2)\n    print(\"Iteration\", i , \"accuracy_score =,\",  acc_scr_xgb2[i])\n    auc_score_xgb2[i] = roc_auc_score(y_test, y_pred_xgb_pr2, multi_class = 'ovr')\n    print(\"Iteration\", i , \"roc_auc_score =,\",  auc_score_xgb2[i])","9c044c89":"auc_scr_list = np.linspace(0,0,4)\nacc_scr_list = np.linspace(0,0,4)\n\nfor i in range(len(auc_score_xgb2)):\n    if(i<4):\n        auc_scr_list[i] = auc_score_xgb2[i]\n        acc_scr_list[i] = acc_scr_xgb2[i]","580fe271":"dpi = 200\nfig, ax = plt.subplots()\nax.plot(n_comp, np.array(acc_scr_list), label='Accuracy ')\nax.plot(n_comp, np.array(auc_scr_list), label='ROC auc score') \nplt.figure(figsize = (10, 10))\nax.set_xlabel('PCA reduced Features') \nax.set_ylabel('score') \nax.set_title(\"PCA num of Features vs scores\")\nax.legend() \nplt.show()","393303bc":"feat_importances = xgb_clf.feature_importances_","8512d7ed":"feat_list = X_train.columns","64cea4b7":"plt.figure(figsize = (15,15))\nplt.barh(list(feat_list), feat_importances)\nplt.yticks(list(range(0,50)))\nplt.xlabel(\"Feature score\")\nplt.ylabel(\"Feature list\")","2ec978bb":"sample_wt_dict = { 0 : 1.5 , 1: 1, 2: 1, 3: 1.5}","9a5e5712":"plt.hist(y_train)","a5591eb7":"tot = len(y_train)\nprint(\"Class 0 ratio\", len(y_train[y_train==0])\/tot)\nprint(\"Class 1 ratio\", len(y_train[y_train==1])\/tot)\nprint(\"Class 2 ratio\", len(y_train[y_train==2])\/tot)\nprint(\"Class 3 ratio\", len(y_train[y_train==3])\/tot)","74caf13b":"weight_list = [sample_wt_dict[i] for i in y_train]\nlen(weight_list)","abde8c51":"xgb_clf3 = XGBClassifier(learning_rate = 0.1, n_estimators = 200,use_label_encoder = False, verbose= None, objective = 'multi:softmax')\n\nxgb_clf3.fit(X_train, y_train, sample_weight = weight_list)","8f9a8464":"y_pred_xgb_pr3 = xgb_clf3.predict_proba(X_test)\n\ny_pred_xgb3 = xgb_clf3.predict(X_test)","828082c2":"acc_scr_xgb3 = accuracy_score(y_test, y_pred_xgb3)\nacc_scr_xgb3","4c0bb16e":"auc_score_xgb3 = roc_auc_score(y_test, y_pred_xgb_pr3, multi_class = 'ovr')\nauc_score_xgb3","ae23ce13":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","2913eca9":"random_forest_clf = RandomForestClassifier()\n\nparams = {'max_depth': [3, 10], 'n_estimators': [100, 200]}\n\nrand_for_cv = RandomizedSearchCV(random_forest_clf, params, cv=5)\n\nrand_for_cv.fit(X_train, y_train)\n\nrand_for_cv.best_params_","94b94ce4":"random_forest_clf_iter = RandomForestClassifier(max_depth = 3, n_estimators = 100)\n\nrandom_forest_clf_iter.fit(X_train, y_train)","01c67ed5":"y_pred_rf = random_forest_clf_iter.predict(X_test)\n\ny_pred_rf_pr = random_forest_clf_iter.predict_proba(X_test)","3156aacf":"acc_scr_rf = accuracy_score(y_test, y_pred_rf)\nacc_scr_rf","00520765":"roc_scor_rf = roc_auc_score(y_test, y_pred_rf_pr, multi_class='ovr')\nroc_scor_rf","f7a28f0d":"clf_rep_rf = classification_report(y_test, y_pred_rf)\nprint(clf_rep_rf)","471bbfb4":"import shap","f7d01adb":"%%time\nexplainer = shap.TreeExplainer(xgb_clf)\n\nshap_values = explainer.shap_values(X_test)","11520ef5":"shap.summary_plot(shap_values, X_test)","a1563fe5":"shap.summary_plot(shap_values[0], X_test)","74019d7b":"shap.dependence_plot(25, shap_values[0], X_test, interaction_index='auto')","6dc9422c":"shap.summary_plot(shap_values[2], X_test)","ed7887ce":"shap.dependence_plot(5, shap_values[2], X_test, interaction_index='auto')","2795a47f":"df_test_data = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","2f29a841":"df_test_data.head()","b0f10cf6":"df_test_data = df_test_data.drop('id', axis = 1)","f2c543c1":"mnmx_scl = MinMaxScaler()\nmnmx_scl.fit(df_test_data)\nX_test1 = mnmx_scl.transform(df_test_data)","422f1d75":"X_test_df = pd.DataFrame(X_test1)","cec67021":"X_test_df.describe()","1b858ed3":"y_pred_test_prob = xgb_clf.predict_proba(X_test_df)\ny_pred_test_prob","f99f2e9f":"sample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","5b8f61a6":"# create submission file\n\nclass_labels = [\"Class_1\",\"Class_2\",\"Class_3\",\"Class_4\"]\n\nsample_submission.drop(columns=class_labels, inplace=True)\n\nsubmission = (sample_submission.join(pd.DataFrame(data=y_pred_test_prob, columns=class_labels)))\nsubmission.to_csv(\"my_submission.csv\", index=False)\nsubmission","fdd33341":"#### With Stratified split we are able to extract proportional test data","901e4d4c":"### Model explainability with SHAP","66a9fc98":"### XGBClassifier iter3 - with sample weight distribution impact and performance measures","99998b98":"#### *From Dataset information method - we can find that all feature values are ***discrete variables*** (indicating it could be ordinal value of some categorical data).*","33c9e16d":"* - *From above interaction plot we can infer that **lower feature value of 25 has higher impact** on outcome variable which gradually reduces.*\n* - *Impact of feature 4 on feature 25 is uniformly distributed with some of the highvalue points near -0.2 shap value*","869b5047":"#### *There are significant quantity of outliers in the dataset for each feature.*\n#### *We will check the outlier count above 95% IQR to decide on the Feature Scaler to apply*\n#### ( *MinMaxScaler* - significant outlier impact, *Robust scaler* - no impact of outliers)","b34501dc":"#### XGBClassifier feature importances are as follows:\n#### ***Feature 2, Feature 13, Feature 32, Feature 44, Feature 29***","fffe5267":"#### *Actual test data file load, predict and file submission*","2cceacf6":"### XGBClassifier iter2 - with PCA feature reduction technique - Performance measures","1bce284d":"##### **Boolean Dataframe with values > 1.5 times 0.95 IQR value for each feature**","d61fda56":"##### *With sample weights **increase of class0 and class3 upto two times of class1 and class2** - the accuracy remains at the same level as iteration 1 and roc_auc_score have reduced.*","5c9a8551":"#### ***Class 0*** target variable outcome SHAP summary plot:","ae6bfc77":"* From above summary plot for Class 0 outcome, it is inferred that:\n*   - ***features 25** has wide impact based on shap values (range -0.65 to 0.6) followed by ***features 38, 6, 17***\n*   - ***features 37** has positive impact as some of the instances extend until 0.7 followed ***features 9, 30, 0, 45, 35***\n*   - ***feature 14** has negative impact *(SHAP value: -0.6)*","3211c3df":"##### - As we have checked PCA with various iterations, *reduced num of features = 35* have given the *highest auc_score (55.94%) and accuracy (57.5%)*\n##### - However the value is less than the performance score achieved with all the set of features in dataset (without PCA)","a820fc4f":"#### We can see that the True value count for *IQR(0.95)X1.5* are more than ***1.5% of the data***. So we will go with ***MinMaxscaler*** that includes outliers in the process.","3ff3f635":"##### Applying best params from Random search CV","4fba5a42":"### Random Forest Classifier - Model train and performance measures","b260c954":"#### ***Class 2*** target variable outcome SHAP summary plot:","5faac0dd":"* *From above interaction plot we can infer that **lower feature value of 5 has wide impact** on outcome variable which gradually has positive impact as it increases*\n* *Impact of feature 34 on feature 5 is uniformly distributed*","03c2d491":"### XGBClassifier iter1 - Model train and performance measures","7ed24c58":"### Model performance metrics\n#### - XGBClassifier iteration 1 ***without class imbalance corrections and PCA*** have higher score compared to *RandomForest Classifier* \n#### - Accuracy are almost at the same level but the roc_auc_score is higher.","a6c94962":"#### Above is the SHAP library interpretation of XGBClassifier iteration 1\n#####    **We can infer the impact of each feature on target variable outcome:**\n#####    **feature 14 has high impact based followed by *features 37, 6, 15, 31***","ee5d54f4":"* From above summary plot for Class 2 outcome, it is inferred that:\n*   - ***features 5** has wide impact based on shap values (range -1 to 0.6) followed by ***features 34, 32, 42, 16***\n*   - ***features 12** has positive impact as some of the instances extend until 1.1 followed ***features 44, 2, 47, 22***\n*   - ***feature 15 and 38** have negative impact on this class outcome","49d3f2a9":"XGBClassifier() --> GridSearchCV done in previous notebook version and below are the best params estimate: n_estimators = 200, learning_rate = 0.1"}}