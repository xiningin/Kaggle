{"cell_type":{"0242ba2c":"code","ad66ea0b":"code","08b8faac":"code","fd90d8e0":"code","90d0f3eb":"code","c575ad85":"code","cd7349dd":"code","df084961":"code","61481c7b":"markdown","52efbec2":"markdown","57334ac9":"markdown","b10905fe":"markdown","d80fb241":"markdown","1725e0f3":"markdown","fa1e821a":"markdown","5b53c4f3":"markdown","6026dade":"markdown","6395478d":"markdown","a0d354a1":"markdown"},"source":{"0242ba2c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom yellowbrick.model_selection import LearningCurve\nfrom sklearn.model_selection import StratifiedShuffleSplit","ad66ea0b":"power = pd.read_csv('..\/input\/experimental-power-and-thrust-coefficients\/power.csv')\nthrust = pd.read_csv('..\/input\/experimental-power-and-thrust-coefficients\/thrust.csv') ","08b8faac":"sns.scatterplot(data=power, x='TSR', y='Cp', hue='Type');\nplt.grid(False)","fd90d8e0":"sns.scatterplot(data=thrust, x='TSR', y='CT', hue='Type');\nplt.grid(False)","90d0f3eb":"sns.scatterplot(data=thrust, x='TSR', y='CT', hue='Gamma');\nplt.grid(False)","c575ad85":"X = thrust[['TSR', 'CT']].to_numpy()\ny = LabelEncoder().fit_transform(thrust['Type'])\n\nlrc = LogisticRegression()\nlrc.fit(X, y)\n\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=lrc, legend=2)\n\n# Adding axes annotations\nplt.xlabel('TSR')\nplt.ylabel('CT')\nplt.title('LogisticRegression in Component')\nplt.ylim(0, 1.7)\nplt.xlim(0, 8)\nplt.grid(False)\nplt.show()\n","cd7349dd":"X = thrust[['TSR', 'CT']].to_numpy()\ny = LabelEncoder().fit_transform(thrust['Type'])\n\n\nlrc = make_pipeline(PolynomialFeatures(2), LogisticRegression(solver='liblinear', C=1))\nlrc.fit(X, y)\n\n\n# Plotting decision regions\nplot_decision_regions(X, y, clf=lrc, legend=2)\n\n# Adding axes annotations\nplt.xlabel('TSR')\nplt.ylabel('CT')\nplt.title('LogisticRegression in Component')\nplt.ylim(0, 1.7)\nplt.xlim(0, 8)\nplt.grid(False)\nplt.show()","df084961":"lrc = make_pipeline(PolynomialFeatures(2), LogisticRegression(solver='liblinear', C=1, max_iter=1e4))\nsizes = np.linspace(0.1, 1.0, 10)\n\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.3)\n\nvisualizer = LearningCurve(\n    lrc, cv=cv, scoring='accuracy', train_sizes=sizes)\n\nvisualizer.fit(X, y)\nplt.xlim(50, 600)\nvisualizer.show();  ","61481c7b":"In this notebook, we will take a look at a simple techinique that can lead to good results without relying on higher complexity. It's the Polynomial Features, which creates polynomial combinations of the features with degree less than or equal to the specified degree.","52efbec2":"## 1. Introduction","57334ac9":"The objetive of the study, as far as I understand, Is to study how the performance change according to the TSR and yaw condition. Let's take a look at how they behave.","b10905fe":"The model seems good, but the regions are missclassifying some instance, mostly, Shroud and Diffuser. We can vizualize why, since Logistic Regression is a linear model and separate Decision Region linearly. To overcome this, we can apply polynomial features to induce non-linearity.","d80fb241":"No overfitting. The model seems fine. Thank you for reading it, please upvote and leave your thoughts down in the comments.","1725e0f3":"## 2. Simple EDA","fa1e821a":"The components are well cluster together for this two features (CT and TSR). For this reason, we are not going to consider other parameters. To finish, we will just take a look at the yaw condition.","5b53c4f3":"As can be observed, one can model their classification just knowing the extremes of yaw condition (0\u00ba and 25\u00ba), but we won't test it in this notebook, just sugest it.","6026dade":"Seems great, actually, is perfect, with not much effort. At last, let's see the learning curve to deduce if we are overfitting the model.","6395478d":"For this notebook, we will use Logistic Regression, due to its simplicity.","a0d354a1":"## 3. Training Models "}}