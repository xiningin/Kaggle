{"cell_type":{"3ad394a9":"code","33867be3":"markdown"},"source":{"3ad394a9":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, TimeDistributed, Dense, Activation, Dropout, Flatten\nimport matplotlib.pyplot as plt\n\n# To generate the text\ndef generate_text(model, length):\n    ix = [np.random.randint(VOCAB_SIZE)]\n    y_char = [ix_to_char[ix[-1]]]\n    X = np.zeros((1, length, VOCAB_SIZE))\n    for i in range(length):\n        X[0, i, :][ix[-1]] = 1\n        print(ix_to_char[ix[-1]], end=\"\")\n        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n        y_char.append(ix_to_char[ix[-1]])\n    return ('').join(y_char)\n\ndata = open('..\/input\/tarantino_scripts.txt', 'r').read()\nchars = sorted(list(set(data)))\nVOCAB_SIZE = len(chars)\nSEQ_LENGTH = 50\n\nix_to_char = {ix: char for ix, char in enumerate(chars)}\nchar_to_ix = {char: ix for ix, char in enumerate(chars)}\n\nX = np.zeros((int(len(data) \/ SEQ_LENGTH), SEQ_LENGTH, VOCAB_SIZE))\ny = np.zeros((int(len(data) \/ SEQ_LENGTH), SEQ_LENGTH, VOCAB_SIZE))\n\nfor i in range(0, int(len(data) \/ SEQ_LENGTH)):\n    X_sequence = data[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n    X_sequence_ix = [char_to_ix[value] for value in X_sequence]\n    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n    for j in range(SEQ_LENGTH):\n        input_sequence[j][X_sequence_ix[j]] = 1.\n    X[i] = input_sequence\n    y_sequence = data[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n    y_sequence_ix = [char_to_ix[value] for value in y_sequence]\n    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n    for j in range(SEQ_LENGTH):\n        target_sequence[j][y_sequence_ix[j]] = 1.\n    y[i] = target_sequence\n\nprint(X.shape, y.shape)\n\nmodel = Sequential()\nmodel.add(LSTM(1024, input_shape=(None, VOCAB_SIZE), return_sequences=True))\nmodel.add(LSTM(1024, return_sequences=True))\nmodel.add(TimeDistributed(Dense(VOCAB_SIZE)))\nmodel.add(Activation('softmax'))\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n\nlosses = []\n\nfor nbepoch in range(1, 151):\n    print('Epoch ', nbepoch)\n    history = model.fit(X, y, batch_size=64, verbose=1, epochs=1)\n    if nbepoch % 10 == 0:\n        model.model.save('checkpoint_{}_epoch_{}.h5'.format(512, nbepoch))\n    generate_text(model, 50)\n    print(\"\\nLoss is \", history.history['loss'])\n    losses.append(history.history['loss'][-1])\n    print('\\n\\n\\n')\n\nmodel.save('final_model.h5')\n\nwith open(\"losses.txt\", \"w\") as text_file:\n    s = \"\"\n    index = 1\n    for loss in losses:\n        s = s + \"Epoch \" + str(index) + \"- Loss is \" + str(loss) + \"\\n\"\n        index = index + 1\n    text_file.write(s)\n\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.savefig('loss_graph.png')\nplt.show()\n","33867be3":"Code taken from this tutorial- [Creating A Text Generator Using Recurrent Neural Network](http:\/\/https:\/\/chunml.github.io\/ChunML.github.io\/project\/Creating-Text-Generator-Using-Recurrent-Neural-Network\/)\n"}}