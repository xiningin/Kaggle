{"cell_type":{"f9c95e7c":"code","4cf13ee8":"code","36937fc1":"code","c67ea23a":"code","4c8ee0e7":"code","4ab9fa08":"code","2f891e6e":"code","433dbecf":"code","779a8c5d":"code","503ad6a2":"code","67290661":"code","52b3adec":"code","0b50da68":"code","d411bf70":"code","cd12c9c0":"code","93811643":"code","a0e74cae":"code","80c41db5":"code","5c91f800":"code","e7ad368d":"code","337932e2":"code","609cb1e5":"code","4cb2b1ac":"code","7c6facb7":"markdown","4977a664":"markdown","a1a7fff5":"markdown","7f2b3f54":"markdown","e4432839":"markdown","794de6ca":"markdown","5484a218":"markdown","a87f60eb":"markdown","ee81e707":"markdown"},"source":{"f9c95e7c":"import sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n#matplotlib.rcParams['font.size'] = 14\n#matplotlib.rcParams['figure.figsize'] = (9, 5)\n#matplotlib.rcParams['figure.facecolor'] = '#00000000'","4cf13ee8":"#load IRIS Dataset\n\niris_df = pd.read_csv(\"..\/input\/iriscsv\/iris.csv\")\niris_df = iris_df.rename(columns={\"variety\" : \"species\"})\niris_df.head()\nX = iris_df.iloc[:,0:4].values\ny = iris_df.iloc[:,4].values","36937fc1":"#encode target labels with value between 0 and n_classes-1\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle =  LabelEncoder()\ny_le = le.fit_transform(y) \ny_le","c67ea23a":"#target into one hot encoding\ny_ohe = pd.get_dummies(y_le).values","4c8ee0e7":"#Finally we have X and y_ohe\nX","4ab9fa08":"y_ohe","2f891e6e":"#Finding the optimum number of clusters for k-means classification\nfrom sklearn.cluster import KMeans\nwcss = []\n\n\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(X)\n    \n    #Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided.\n    wcss.append(kmeans.inertia_) ","433dbecf":"plt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","779a8c5d":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","503ad6a2":"plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')\n\nplt.legend()","67290661":"fig = plt.figure(figsize = (15,15))\nax = fig.add_subplot(111, projection='3d')\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')\nplt.legend()\nplt.show()\n","52b3adec":"# Define a variable to establish three classes\/species.\n#class_count = 3\n# Define standard RGB color scheme for visualizing ternary classification in order to match the color map used later.\n#plot_colors = 'ryb'\n# Define marker options for plotting class assignments of training data.\n#markers = 'ovs'\n# We also need to establish a resolution for plotting.  I favor clean powers of ten, but this is not by any means a hard and fast rule.\n#plot_res = 0.01","0b50da68":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n\ntrain_x, test_x, train_y, test_y = train_test_split(X[:,0:3], y_ohe, train_size=0.8,shuffle=True)\n\n# Apply the decision tree classifier model to the data using all four parameters at once.\n\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(train_x, train_y)","d411bf70":"print('Accuracy score is:',cross_val_score(clf, test_x, test_y, cv=3, scoring='accuracy').mean())","cd12c9c0":"# Prepare a plot figure with set size.\nplt.figure(figsize = (12,9))\n# Plot the decision tree, showing the decisive values and the improvements in Gini impurity along the way.\ntree.plot_tree(clf,filled=True)\n# Display the tree plot figure.\nplt.show()","93811643":"# Test on unseen data\nfrom sklearn.metrics import  accuracy_score\n\n#Checking our model performance on actual unseen test data.. \npred_y= clf.predict(test_y)\n\nprint('Model Accuracy Score on totally unseen data(Xtest) is:',accuracy_score(test_y, pred_y)*100,'%')\n\nactual = np.argmax(test_y,axis=1)\npredicted = np.argmax(pred_y,axis=1)\nprint(f\"Actual: {actual}\")\nprint(f\"Predicted: {predicted}\")","a0e74cae":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","80c41db5":"# Split the data for training and testing\ntrain_x, test_x, train_y, test_y = train_test_split(X[:,0:3], y_ohe, train_size=0.8)\n\ntrain_x.shape, train_y.shape, test_x.shape, test_y.shape","5c91f800":"#Define model\n#Giving input will lead model be able to build \n\nmodel = Sequential()\n\nmodel.add(layers.Dense(10, activation=\"relu\", name=\"layer1\",input_shape=(3,)))\nmodel.add(layers.Dense(10, activation=\"relu\", name=\"layer2\"))\nmodel.add(layers.Dense(3, activation=\"softmax\", name=\"outputs\"))\n\n#model.summary()","e7ad368d":"# Adam optimizer with learning rate of 0.001\noptimizer = Adam(lr=0.001)\nmodel.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_x, train_y, verbose=1, batch_size=5, epochs=200)","337932e2":"# Test on unseen data\n\nresults = model.evaluate(test_x, test_y)\n\nprint('Final test set loss: {:4f}'.format(results[0]))\nprint('Final test set accuracy: {:4f}'.format(results[1]))","609cb1e5":"#Predict test data\npred_y = model.predict(test_x)\npred_y","4cb2b1ac":"# Actual and predicted value\nactual = np.argmax(test_y,axis=1)\npredicted = np.argmax(pred_y,axis=1)\nprint(f\"Actual: {actual}\")\nprint(f\"Predicted: {predicted}\")","7c6facb7":"#### Using the elbow method to determine the optimal number of clusters for k-means clustering","4977a664":"## Data preprocessing","a1a7fff5":"## K-Means ##\n\nK-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. In K-Means, each cluster is associated with a centroid.","7f2b3f54":" \n ## Decision Tree Classifier","e4432839":"#### 3d scatterplot using matplotlib\n","794de6ca":"#### Visualising the clusters","5484a218":"## Deep Learning","a87f60eb":"Even if we already know the classes for the 150 instances of irises, it could be interesting to create a model that predicts the species from the petal and sepal width and length. One model that is easy to create and understand is a decision tree, which can be created with sklearn","ee81e707":"#### Implementing K-means\nAs we can see from above plot, we can say that elbow method suggest 3 as number of clusters"}}