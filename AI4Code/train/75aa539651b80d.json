{"cell_type":{"f42860ec":"code","57d4b94a":"code","ac52a3d6":"code","cf66376d":"code","25cfd76c":"code","65ecd148":"code","8cf66da5":"code","5bd26116":"code","280839c8":"code","6a51e698":"code","c4342822":"code","6e6b85a1":"code","826e40d4":"code","5211c666":"code","9f7f41af":"code","665cadcc":"code","bcd1738f":"code","e047fe8d":"code","58ade04f":"code","2e613f47":"code","8bc9a688":"code","0dde2a26":"code","b3322648":"code","b88cdf57":"code","7f513233":"code","021ae5c7":"markdown","c1767446":"markdown","4e4ffdfd":"markdown","66ee26e7":"markdown","69c4a659":"markdown","209aae74":"markdown","18d536c5":"markdown","affeb982":"markdown"},"source":{"f42860ec":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns","57d4b94a":"data=pd.read_json('..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json',lines=True)","ac52a3d6":"data","cf66376d":"data.is_sarcastic.value_counts()","25cfd76c":"word_len=[]\nfor i in range(len(data)):\n    word_len.append(len(data.headline[i].split(' ')))\nprint(\"Max word length of the headlines:\",max(word_len))\nprint(\"average word length of the headlines:\",round(np.mean(word_len)))","65ecd148":"#splitting into 4:1 ratio\n\nX_train, X_test, Y_train, Y_test= train_test_split(data.headline,data.is_sarcastic,test_size=0.2,\n                                                  random_state=1729)","8cf66da5":"#import BERT preprocessor and encoder from Tensorflow Hub\n\n#using a 4 layered small-bert with H=768\n\nbert_preprocess = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\nbert_encoder = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-768_A-12\/2\")","5bd26116":"#we use pooled outputs to get the tokenized keys\n#example\n\ndef get_sentence_embeding(sentences):\n    preprocessed_text = bert_preprocess(sentences)\n    return bert_encoder(preprocessed_text)['pooled_output']\n\nexample=[data.headline.values[100],\n         data.headline.values[200],\n        data.headline.values[300],\n        data.headline.values[400],\n        data.headline.values[500]]\n\nprint(\"Sentence embedding's shape for 1 sentence:\",get_sentence_embeding(example)[0].shape)\nprint(\"Sentence embeddings of the string list 'example':\",get_sentence_embeding(example))","280839c8":"# BERT model layers\n\n\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\nX=outputs['pooled_output']\n\n# Custom layers and fine-tuning\n\nnn= tf.keras.layers.Dense(128,activation='relu')(X)\nnn = tf.keras.layers.Dropout(0.3, name=\"dropout\")(nn)\nnn = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(nn)\n\n#Create a model using the input text layer and output layer(includes layers inbetween)\n\nmodel = tf.keras.Model(inputs=[text_input], outputs = [nn])","6a51e698":"model.summary()","c4342822":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","6e6b85a1":"#saving the model which shows best validation accuracy","826e40d4":"checkpoint_filepath = '.\/model_checkpoints\/'\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_accuracy',   ##    monitoring validation accuracy \n    mode='max',               ##\n    save_best_only=True)","5211c666":"hist=model.fit(X_train,Y_train,validation_data=(X_test, Y_test),validation_batch_size=16,\n                batch_size=16, epochs=10, \n                callbacks=[model_checkpoint_callback])","9f7f41af":"#stopping early because of GPU time constraints :), train for more epochs to get better results.","665cadcc":"#load saved model weights\nmodel.load_weights('model_checkpoints\/')","bcd1738f":"Y_pred=model.predict(X_test)","e047fe8d":"#since we are using sigmoid at the output layer\n\nY_pred=(Y_pred>0.5)*1.0","58ade04f":"Y_pred","2e613f47":"from sklearn.metrics import confusion_matrix,classification_report\n\ncm=confusion_matrix(Y_pred,Y_test)","8bc9a688":"plt.figure(figsize=(6,4))\nsns.heatmap(cm,annot=True,fmt='d',cmap='Blues')\nplt.title(\"0- Not sarcastic, 1- Sarcastic\")\nplt.show()","0dde2a26":"#for better results train for more epochs or use bigger BERT models(L=12 or 24)\n\n\nprint(classification_report(Y_pred,Y_test))","b3322648":"def predict_on_texts(text):\n    pred=model.predict(text)[0][0]\n    if(pred>0.5):\n        result=\"Sarcastic\"\n    else:\n        result=\"Not sarcastic\"\n    return result","b88cdf57":"text=[\"a man killed his neighbour because of some minor dispute\"]\npredict_on_texts(text)","7f513233":"text=[\"Dog becomes president of a country\"]\npredict_on_texts(text)","021ae5c7":"# Step 3: Prepare dataset and preprocessing","c1767446":"# Step 6: Predictions on Test Set","4e4ffdfd":"# Step 1: Import required modules ","66ee26e7":"# Step 7: Random Predictions on unseen data","69c4a659":"# Step 5: Train the model","209aae74":"# Step 4: Create a fine-tuned BERT model","18d536c5":"# I'd say it's performing pretty well even on unseen data!\n\n# Upvote if you found this useful!","affeb982":"# Step 2: Read data"}}