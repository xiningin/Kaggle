{"cell_type":{"0496ce5b":"code","b2a88d9f":"code","a78429f9":"code","e45c7c84":"code","22874cbb":"code","bf653712":"code","06eb7f8e":"code","52b8c377":"code","790a2fbd":"code","40860753":"code","3fe26a22":"code","bab64ee9":"code","64f3c3de":"code","890279d2":"code","ff51c3dd":"code","a77fcef8":"code","372cd1b7":"code","2fb3fff5":"code","506abd45":"code","169a044b":"code","70ffdb0c":"code","27ef0f0d":"code","5cffc159":"code","6cf3f70d":"code","41ba109e":"code","669f6dee":"code","777841d6":"code","22f08d6b":"code","c90b8763":"code","b81da22c":"code","b378e4a5":"markdown","65bf21c1":"markdown","3a10d2df":"markdown","0f5968c0":"markdown","feaf016e":"markdown","0eacd49e":"markdown","a35f9071":"markdown","f92fa395":"markdown","0b621b54":"markdown","40d76221":"markdown","ea245f58":"markdown","ed3b3d24":"markdown","33f2e63a":"markdown","19f64ff8":"markdown","2d7de73a":"markdown","afce7a11":"markdown","db1c717a":"markdown","dc2d1c88":"markdown"},"source":{"0496ce5b":"import pandas as pd\nimport pyarrow.parquet as pq # Used to read the data\nimport os \nimport numpy as np\nfrom keras.layers import *\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split \nfrom keras import backend as K \nfrom keras import optimizers\nimport tensorflow as tf\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom keras.callbacks import *\n%matplotlib inline\nimport matplotlib.pyplot as plt","b2a88d9f":"# select how many folds will be created\nN_SPLITS = 5\n# it is just a constant with the measurements data size\nsample_size = 800000","a78429f9":"def matthews_correlation_coeff(y_true, y_pred):\n    '''Calculates the Matthews correlation coefficient measure for quality\n    of binary classification problems.\n    '''\n    y_pred = tf.convert_to_tensor(y_pred, np.float32)\n    y_true = tf.convert_to_tensor(y_true, np.float32)\n\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n\n    tp = K.sum(y_pos * y_pred_pos)\n    tn = K.sum(y_neg * y_pred_neg)\n\n    fp = K.sum(y_neg * y_pred_pos)\n    fn = K.sum(y_pos * y_pred_neg)\n\n    numerator = (tp * tn - fp * fn)\n    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    return numerator \/ (denominator + K.epsilon())","e45c7c84":"# load the training set metadata, defines which signals are in which order in the data\ntrain_meta = pd.read_csv('..\/input\/vsb-power-line-fault-detection\/metadata_train.csv')\n# set index, it makes the data access much faster\ntrain_meta = train_meta.set_index(['id_measurement', 'phase'])\ntrain_meta.head()","22874cbb":"# load the test set metadata, defines which signals are in which order in the data\ntest_meta = pd.read_csv('..\/input\/vsb-power-line-fault-detection\/metadata_train.csv')\n# set index, it makes the data access much faster\ntest_meta = test_meta.set_index(['id_measurement', 'phase'])\ntest_meta.head()","bf653712":"!ls ..\/input\/preprocessing-with-python-multiprocessing","06eb7f8e":"df_test_pre = pd.read_csv(\"..\/input\/preprocessing-with-python-multiprocessing\/my_test_combined_scaled.csv.gz\", compression=\"gzip\")","52b8c377":"df_test_pre.shape","790a2fbd":"1084640\/160","40860753":"df_train_pre = pd.read_csv(\"..\/input\/preprocessing-with-python-multiprocessing\/my_train_combined_scaled.csv.gz\", compression=\"gzip\")\ndf_train_pre.shape","3fe26a22":"df_train_pre.columns","bab64ee9":"df_train_pre.drop(\"Unnamed: 0\", axis=1, inplace=True)\ndf_test_pre.drop(\"Unnamed: 0\", axis=1, inplace=True)","64f3c3de":"#number of \"observations\" in test dataset\ndf_test_pre.shape[0]\/160","890279d2":"df_train_pre.shape","ff51c3dd":"#number of \"observations\" in training dataset\n464640\/160","a77fcef8":"train_meta.index.get_level_values('id_measurement').unique()","372cd1b7":"pd.set_option('display.max_rows', 5)\ndf_train_pre.iloc[0:160,:22]","2fb3fff5":"df_train_pre.iloc[0:160,22:44]","506abd45":"df_train_pre.iloc[0:160,44:66]","169a044b":"df_train_pre.iloc[160:320]","70ffdb0c":"pd.reset_option('display.max_rows')","27ef0f0d":"from sklearn.metrics import matthews_corrcoef\n\n# The output of this kernel must be binary (0 or 1), but the output of the NN Model is float (0 to 1).\n# So, find the best threshold to convert float to binary is crucial to the result\n# this piece of code is a function that evaluates all the possible thresholds from 0 to 1 by 0.01\ndef threshold_search(y_true, y_proba):\n    best_threshold = 0\n    best_score = 0\n    scores = []\n    for threshold in [i * 0.01 for i in range(100)]:\n        yp_np = np.array(y_proba)\n        yp_bool = yp_np >= threshold\n        score = matthews_corrcoef(y_true, yp_bool)\n        #score = K.eval(matthews_correlation(y_true.astype(np.float64), (y_proba > threshold).astype(np.float64)))\n        scores.append(score)\n        if score > best_score:\n            print(\"found better score:\"+str(score)+\", th=\"+str(threshold))\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'matthews_correlation': best_score}\n    scores_df = pd.DataFrame({\"score\": scores})\n    print(\"scores plot:\")\n    scores_df.plot()\n    plt.show()\n    return search_result","5cffc159":"def create_model(input_data):\n    input_shape = input_data.shape\n    inp = Input(shape=(input_shape[1], input_shape[2],), name=\"input_signal\")\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True, name=\"lstm1\"), name=\"bi1\")(inp)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=False, name=\"lstm2\"), name=\"bi2\")(x)\n    #other kernels have used also a custom Attention layer but I leave it out for simplicity here\n#    x = Attention(input_shape[1])(x)\n    x = Dense(128, activation=\"relu\", name=\"dense1\")(x)\n    x = Dense(64, activation=\"relu\", name=\"dense2\")(x)\n    x = Dense(1, activation='sigmoid', name=\"output\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[matthews_correlation_coeff])\n    return model","6cf3f70d":"#if any of the 3 signals for a measurement id is labeled as faulty, this labels the whole set of 3 as faulty\ny = (train_meta.groupby(\"id_measurement\").sum()\/3 > 0)[\"target\"]","41ba109e":"#to see the number of targets matches the number of rows in training dataset\ny.shape","669f6dee":"#if using all signal values separately, the number of rows would be 8712, or 2904*3.\n#X = df_train_pre.values.reshape(8712, 160, 22)\n#but with the current data format I show above, it is 2904 rows, or \"observations\"\nX = df_train_pre.values.reshape(2904, 160, 66)\nX.shape","777841d6":"X_test = df_test_pre.values.reshape(6779, 160, 66)","22f08d6b":"eval_preds = np.zeros(X.shape[0])\nlabel_predictions = []\n\nsplits = list(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=123).split(X, y))\nfor idx, (train_idx, val_idx) in enumerate(splits):\n    K.clear_session()\n    print(\"Beginning fold {}\".format(idx+1))\n    train_X, train_y, val_X, val_y = X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n\n    model = create_model(X)\n    #checkpoint to save model with best validation score. keras seems to add val_xxxxx as name for metric to use here\n    ckpt = ModelCheckpoint('weights.h5', save_best_only=True, save_weights_only=True, monitor='val_matthews_correlation_coeff', verbose=1, mode='max')\n    earlystopper = EarlyStopping(patience=25, verbose=1) \n    model.fit(train_X, train_y, batch_size=128, epochs=50, validation_data=[val_X, val_y], callbacks=[ckpt, earlystopper])\n    # loads the best weights saved by the checkpoint\n    model.load_weights('weights.h5')\n\n    print(\"finding threshold\")\n    predictions = model.predict(val_X, batch_size=512)\n    best_threshold = threshold_search(val_y, predictions)['threshold']\n    \n    print(\"predicting test set\")\n    pred = model.predict(X_test, batch_size=300, verbose=1)\n    pred_bool = pred > best_threshold\n    labels = pred_bool.astype(\"int32\")\n    label_predictions.append(labels)\n    \n","c90b8763":"label_predictions = [pred.flatten() for pred in label_predictions]\n\nimport scipy\n\n# Ensemble with voting\nlabels = np.array(label_predictions)\n#convert list of predictions into set of columns\nlabels = np.transpose(labels, (1, 0))\n#take most common value (0 or 1) or each row\nlabels = scipy.stats.mode(labels, axis=-1)[0]\nlabels = np.squeeze(labels)\n\nsubmission = pd.read_csv('..\/input\/vsb-power-line-fault-detection\/sample_submission.csv')\nlabels3 = np.repeat(labels, 3)\nsubmission['target'] = labels3\nsubmission.to_csv('_voted_submission.csv', index=False)\nsubmission.head()","b81da22c":"sum(labels3)","b378e4a5":"The preprocessed data has 160 timesteps. Number of rows should match the number of measurements times the number of timesteps:","65bf21c1":"Finally, train and run a simple LSTM classifier for all this:","3a10d2df":"And the 3 signals for the second measurement id:","0f5968c0":"To drop the excess column:","feaf016e":"The test dataframe loaded above has 22 features calculated for each of the 3 signals per measurement id. So 66 columns. It becomes 67 when loaded, because dumping the values to disk with pandas.to_csv seems to have generated one extra column (maybe the index?).  Number of actual rows should match the number of measurement id's in the corresponding dataset:","0eacd49e":"LSTM requires 3-dimensional input, so this reshapes the dataframe values from 2D dataframe to a 3D numpy matrix in the required format. 2904 observations, 160 timesteps, each with 66 features. \n\nFeatures for each timestep are on a single row in the dataframe (66 on a row) as is. The dataframe here being \"df_train_pre\". The dataframe has 160 rows per measurement id as shown above (df_train_pre.iloc[0:160] for measurement id 1 and df_train_pre.iloc[160:320] for measurement id 2, and so on). Each of these measurement id sets should be its own \"observation\" in the numpy matrix used as input for the LSTM. \n\nThe following reshape creates the required input format, setting the overall input shape as (2904, 160, 66). This is 2904 observations, 160 timesteps for each of those 2904 observations, and 66 features for each of those 160 timesteps. This is the 3D format format LSTM expects as input. A timestep has been formed by splitting the sequence of signal values over time to 160 separate values on after the other, and collecting the 66 features for that timeslot.","a35f9071":"For me, this VSB power line competition was a good chance to learn how to use LSTM or RNN in general (I expect GRU should not be much different to apply with Keras..). I need a place to write things down so I remember another day and not just today. I wrote myself a [blog post](https:\/\/swenotes.wordpress.com\/2019\/02\/22\/learning-to-lstm\/) to remind myself. This kernel is an attempt to put some working code somewhere.\n\nIf I got any part wrong about here, or missing something, do let me know :).\n\nI started with the [kernel](https:\/\/www.kaggle.com\/braquino\/5-fold-lstm-attention-fully-commented-0-694) by Bruno Marek. Then played with the data and classifiers myself, built a separate [preprocessing kernel](https:\/\/www.kaggle.com\/donkeys\/preprocessing-with-python-multiprocessing) as well. This kernel uses data produced by that preprocessing kernel.\n\nThere are other public kernels in the competition with better scores but I wanted to keep this simple to help myself more clearly understand the core concepts.\n","f92fa395":"LSTM is about timesteps, and in this case the 800k measurements per signal were summarized to 160 timesteps per signal in the pre-processing. So things like average values of 5000 measurementes (800k\/5000=160). These are now in rows 0-159 for the first measurements id, where the columns 0-21 are for the first signal, columns 22-43 for second signal, and 44-65 for the third signal.\n\nThis continues for the following measurements with the 3 signals per measurement id in the columns. So the signals for the second measurement id are in rows 160-319.\n\nA look at first signal for the first measurement id:","0b621b54":"Matthews correlation coefficient is simply the measure given in this Kaggle competition as a way to measure the score. It [seems](https:\/\/en.wikipedia.org\/wiki\/Matthews_correlation_coefficient) that a value of 1 would mean perfect prediction, and 0 equal to random values. So maybe 0.6-0.7 that many kernels get is not all that bad? ","40d76221":"Create the actual LSTM model. For more explanations, see my [blog post](https:\/\/swenotes.wordpress.com\/2019\/02\/22\/learning-to-lstm\/).","ea245f58":"Convert the above predictions into suitable submission format for the competition:","ed3b3d24":"Now to do the same for the test-dataset, but remembering it has more rows, so the first dimension is higher. Maybe because the people at Kaggle want to make life difficult for the competitors and so the test set is much bigger :).","33f2e63a":"Second signal:","19f64ff8":"Just a quick look at how many positive (faulty power line) predictions did we get:","2d7de73a":"The training data-set should look about the same but with only the 2904 measurements, so fewer rows:","afce7a11":"Third signal:","db1c717a":"The data files produced by the preprocessing kernel is shown above. I had to compress them using gzip to fit them into the kernel 5GB output size limit. Hence the decompression and the filename suffix here.","dc2d1c88":"Since the dataset has been combined to have all 3 phase signals per measurement id on a single row (22\\*3=66 features\/columns), as combined features, I need to combine the prediction targets for all 3 signals also into one. "}}