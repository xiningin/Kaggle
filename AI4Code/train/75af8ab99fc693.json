{"cell_type":{"6e17130d":"code","a84cb53d":"code","2169428e":"code","9fd11689":"code","66f75b75":"code","234729c9":"code","945dd051":"code","6c764ebc":"code","db611dda":"code","796fd9c1":"code","b03f9f22":"code","48b94532":"code","26cde019":"code","a17031c8":"code","25539ae7":"code","6d7d38c0":"code","4b3f98e5":"code","9a46cc87":"code","8b2b1eb3":"code","169bd0ba":"code","eb2a6ead":"code","478dc953":"code","bc4487ca":"code","0c2a14e3":"code","42d541b6":"code","63ff5971":"code","84bd84cd":"code","016d878c":"code","2eaff129":"code","deb31a26":"code","079d3536":"code","7a131cea":"code","8915df35":"code","a053124e":"code","cc6f79eb":"code","d39040be":"code","bd4825ad":"code","f947f0c1":"code","70c7a70f":"code","b8afbbf0":"code","ace47505":"code","549bc459":"code","f1d81ce2":"code","13a0afeb":"code","da7d76fd":"code","44c4dbac":"code","92aea317":"code","e7519f24":"code","c71d8fa8":"code","85fbf70a":"code","ea99751a":"code","c4a53c99":"code","1ad6f208":"code","1b797235":"code","1f5aa1ab":"code","1d847d29":"code","a7761ac6":"code","2b87ae5d":"code","2e0f2677":"code","0abbc8c6":"code","2ba02367":"code","4229e5d2":"code","71eb0bf6":"code","a6cc60a7":"code","cf01cce8":"code","5faf5fa6":"code","11222dec":"code","444c14c7":"code","172fd703":"markdown","dfb186d6":"markdown","7ad19511":"markdown","e5b22479":"markdown","ffe64fbc":"markdown","419d1678":"markdown","be208827":"markdown","55bef01f":"markdown","ca78dffc":"markdown","9df17120":"markdown","187c726e":"markdown","5f0c9691":"markdown","e49c97df":"markdown","e61d966c":"markdown","7867d8a0":"markdown","8e88591d":"markdown","5dc97693":"markdown","c639fe25":"markdown","e8602845":"markdown","f8e41470":"markdown","8af1c8fc":"markdown","4c7c8e11":"markdown","35aae0aa":"markdown","e8a22b67":"markdown","a9fd2e0b":"markdown","338b228b":"markdown","74a2c51c":"markdown","86f38b27":"markdown","9d3f0391":"markdown","2a3d11ac":"markdown","8a971d7b":"markdown","d9a65066":"markdown","3f2135a4":"markdown","e61f1cd1":"markdown","e314bb35":"markdown","baa56168":"markdown","78a4df84":"markdown","59782ba6":"markdown","59b3a9d2":"markdown","c96cd432":"markdown","7cee055f":"markdown","c130dc12":"markdown","8e5db64b":"markdown"},"source":{"6e17130d":"from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils import class_weight\nfrom tensorflow.keras import models, layers, activations, callbacks\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff\nsns.set()\n\n%matplotlib inline","a84cb53d":"df = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ndf.head()","2169428e":"# Set id as index\ndf.set_index('id', inplace=True)\ndf.head()","9fd11689":"# Check for missing values\ndf.isna().sum()","66f75b75":"df.describe()","234729c9":"df.corr()","945dd051":"# Check gender distribution\ndf['Gender'].value_counts()","6c764ebc":"# Check distribution of customer's age\n# by using histogram\ndf['Age'].hist(bins=np.arange(20, 90, 5))  # adjust bin width to 5\nplt.title(\"Customers' Age Distribution\", size=16)\nplt.xlabel('Age (years)')\nplt.ylabel('Total');","db611dda":"# Check the proportion of customers interested based on driving license ownership\ndf.groupby('Driving_License')['Response'].mean()","796fd9c1":"df['Region_Code'].nunique()  # There are 53 unique values of region_code","b03f9f22":"# Group Response by Region_Code, aggregated by mean\ndf.groupby('Region_Code')['Response'].mean().sort_values(ascending=False)","48b94532":"df['Previously_Insured'].value_counts()","26cde019":"# Most customers' vehicle age is less than 2 years\ndf['Vehicle_Age'].value_counts()","a17031c8":"df['Vehicle_Damage'].value_counts()  ","25539ae7":"vehicle_age_damage = df[['Vehicle_Age',\n                         'Vehicle_Damage']].value_counts().unstack()  # Count values\nvehicle_age_damage","6d7d38c0":"vehicle_age_damage = vehicle_age_damage.iloc[[1, 0, 2],\n                                             [1, 0]]  # Reorder rows and columns\nvehicle_age_damage","4b3f98e5":"vehicle_age_damage.eval('yes_prop = Yes \/ (Yes+No)',\n                        inplace=True)  # Calculate vehicle damage proportion\nvehicle_age_damage","9a46cc87":"vehicle_age_damage['yes_prop'].plot(kind='bar')\nplt.xticks(rotation=30)\nplt.xlabel('Vehicle Age')\nplt.title('Proportion of Customers That Have Experienced Vehicle Damage', size=16);","8b2b1eb3":"# Calculate the proportion of customer interested grouped by Vehicle_Damage\ndf.groupby('Vehicle_Damage')['Response'].mean()","169bd0ba":"# Calculate the proportion of customer interested grouped by Previously_Insured\ndf.groupby('Previously_Insured')['Response'].mean()","eb2a6ead":"# Create Pivot table with Previously_Insured and Vehicle_Damage as grouping variables\ninsured__vehicle_damage = df.pivot_table('Response',\n                                         index='Previously_Insured',\n                                         columns='Vehicle_Damage')\ninsured__vehicle_damage.rename(index={0: 'No', 1: 'Yes'}, inplace=True)\nsns.heatmap(insured__vehicle_damage, annot=True, fmt='.4f')\nplt.title('Proportion of Customers Interested', size=16);","478dc953":"df['Policy_Sales_Channel'].nunique()  # Count unique values in the column","bc4487ca":"sales_channel_count = df['Policy_Sales_Channel'].value_counts()\nsales_channel_count","0c2a14e3":"(sales_channel_count <= 100).sum()","42d541b6":"# Check the distribution of vintage (number of days customer has been associated with the company)\ndf['Vintage'].hist()\nplt.title('Distribution of Vintage', size=16)\nplt.xlabel('Vintage (days)');","63ff5971":"def plot_vintage_mean_response(df=df, bins=10):\n    '''\n    Plot the proportion of customer interested for every bin defined by vintage\n\n    INPUT:\n    df - pandas dataframe\n    bins - number of bins\n\n    OUTPUT:\n    histograms\n    '''\n\n    xmin, xmax = df['Vintage'].min(), df['Vintage'].max()\n\n    total_counts, bins = np.histogram(df['Vintage'],\n                                      bins=bins, range=(xmin, xmax))\n    yes_counts, _ = np.histogram(df['Vintage']*df['Response'],\n                                 bins=bins, range=(xmin, xmax))\n\n    plt.hist(bins[:-1], bins[1:], weights=yes_counts\/total_counts)\n    plt.title('Proportion of Customers Interested', size=16)\n    plt.xlabel('Vintage (days)')\n\n    return None\n\n\nplot_vintage_mean_response()","84bd84cd":"plot_vintage_mean_response(bins=30)","016d878c":"# The distribution of Annual Premium is right-skewed\ndf['Annual_Premium'].hist(bins = 60)\nplt.title('Distribution of Annual Premium', size=16)\nplt.xlabel('Annual Premium');","2eaff129":"df['Annual_Premium'].plot(kind = 'hist', range = (0, 150000), bins = 40)\nplt.title('Distribution of Annual Premium', size=16)\nplt.xlabel('Annual Premium');","deb31a26":"df.loc[df['Annual_Premium'] <= 10000, 'Annual_Premium'].value_counts()","079d3536":"# Check the data where annual premium = 2630\noutliers_df = df[df['Annual_Premium'] == 2630]\noutliers_df.head()","7a131cea":"outliers_df.describe()","8915df35":"# Split the dataset into predictor and response variable\nX = df.drop('Response', axis=1)  # Predictor\ny = df['Response']  # Response","a053124e":"# Initialize OneHot Encoder to handle categorical variables\nonehot_enc = OneHotEncoder(handle_unknown='ignore')","cc6f79eb":"def preprocess_data(df=X, onehot_enc=onehot_enc, fit=True):\n    '''\n    Preprocess data according to analysis from previous section and encode the categorical variables into dummies\n\n    INPUT:\n    df - dataframe to be processed\n\n    OUTPUT:\n    Numpy array containing processed data\n    '''\n\n    df = df.drop('Vintage', axis=1)  # Drop Vintage column\n    \n    # Change Annual Premium with value 2630 to NaN\n    df.loc[df['Annual_Premium'] == 2630, 'Annual_Premium'] = np.nan\n    \n    # Group Policy Sales Channels that appear less than equal 100 times\n    psc_count = df['Policy_Sales_Channel'].value_counts()\n    low_psc_index = psc_count[psc_count <= 100].index\n    df['Policy_Sales_Channel'] = df['Policy_Sales_Channel'].apply(\n        lambda x: 'Others' if x in low_psc_index else str(x))\n    \n    df['Region_Code'] = df['Region_Code'].astype('int')  # Change dtype to int\n    \n    # Isolate categorical features except Driving_License and Previously_Insured\n    # (already encoded as dummies)\n    df_need_dummies = df[['Gender', 'Region_Code', 'Vehicle_Age',\n                          'Vehicle_Damage', 'Policy_Sales_Channel']].astype('category')\n    if fit:\n        onehot_enc.fit(df_need_dummies)\n    \n    array_dummies = onehot_enc.transform(df_need_dummies).toarray()\n        \n    df_no_dummies = df[['Age', 'Driving_License',\n                        'Previously_Insured', 'Annual_Premium']]\n    # fill missing values (NaN) with column mean\n    array_no_dummies = df_no_dummies.apply(lambda col: col.fillna(col.mean())).values\n\n    # Concat df_no_dummies and df_dummies along the columns\n    processed = np.concatenate([array_no_dummies, array_dummies], axis=1)\n    \n    return processed\n\n\nX_processed = preprocess_data(X)\nX_processed","d39040be":"# Create processed data features\nfeatures = ['Age', 'Driving_License',\n            'Previously_Insured', 'Annual_Premium',\n            *onehot_enc.get_feature_names(['Gender', 'Region_Code', 'Vehicle_Age',\n                                           'Vehicle_Damage', 'Policy_Sales_Channel'])]\nfeatures","bd4825ad":"# Split the data into train data and validation data\n# Use stratify argument to split the data proportionately based on response variable\nX_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.3,\n                                                  random_state=40, stratify=y)","f947f0c1":"# Initiate and fit the model to the train data\nlog_reg = Pipeline([('scaler', StandardScaler()),  # Normalize the feature \n                    ('logistic_reg', LogisticRegression(n_jobs=-1,\n                                                        class_weight='balanced'))])\n# Use class weight 'balanced' because there are signifcantly more customers that are not interested\nlog_reg.fit(X_train, y_train)","70c7a70f":"# Predict the probability customer in validation data is interested\ny_pred_proba = log_reg.predict_proba(X_val)[:, 1]\ny_pred_proba","b8afbbf0":"# Calculate the predicted response and show the classification report\ny_pred = log_reg.predict(X_val)\nprint(classification_report(y_val, y_pred))","ace47505":"# Create the confusion matrix\nconf_matrix = confusion_matrix(y_val, y_pred)\n(tn, fp, fn, tp) = conf_matrix.ravel()\nlog_reg_precision = tp \/ (tp+fp)\nlog_reg_recall = tp \/ (tp+fn)\nlog_reg_auc_roc = roc_auc_score(y_val, y_pred)\nprint('Precision: ', log_reg_precision)\nprint('Recall: ', log_reg_recall)\nprint('AUC-ROC score: ', log_reg_auc_roc)","549bc459":"#Plot the confusion matrix\nlabels = ('Not Interested', 'Interested')\n\nplt.figure(figsize = (8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d',\n            xticklabels=labels, yticklabels=labels, \n            annot_kws={'size': 14})\nplt.yticks(rotation=0)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix', size=16);","f1d81ce2":"# Create Regression Coefficient Data Frame\nreg_coef = log_reg['logistic_reg'].coef_[0]\nreg_coef_df = pd.DataFrame({'abs_coef': np.abs(reg_coef),\n                            'sign_coef': np.sign(reg_coef)},\n                           index=features)\n\n# Sort Data Frame based on absolute value of coefficient\n# We can compare the coefficient between variables because the data have been normalized\nfeature_importance = reg_coef_df.sort_values('abs_coef',\n                                             ascending=False).head(20)\nfeature_importance","13a0afeb":"# Initiate and fit the model to the train data\nrf_classifier = Pipeline([('scaler', StandardScaler()),\n                          ('random_forest',\n                           RandomForestClassifier(n_estimators=300,\n                                                  n_jobs=-1,\n                                                  class_weight='balanced'))])\nrf_classifier.fit(X_train, y_train)","da7d76fd":"# Predict the probability customer in validation data is interested\ny_pred_proba = rf_classifier.predict_proba(X_val)[:, 1]\ny_pred_proba","44c4dbac":"# Calculate the predicted response and show the classification report\ny_pred = rf_classifier.predict(X_val)\nprint(classification_report(y_val, y_pred))","92aea317":"# Create the confusion matrix\nconf_matrix = confusion_matrix(y_val, y_pred)\n(tn, fp, fn, tp) = conf_matrix.ravel()\nrf_precision = tp \/ (tp+fp)\nrf_recall = tp \/ (tp+fn)\nrf_auc_roc = roc_auc_score(y_val, y_pred)\nprint('Precision: ', rf_precision)\nprint('Recall: ', rf_recall)\nprint('AUC-ROC score: ', rf_auc_roc)","e7519f24":"#Plot the confusion matrix\nsns.heatmap(conf_matrix, annot=True, fmt='d',\n            xticklabels=labels, yticklabels=labels,\n            annot_kws={'size': 14})\nplt.yticks(rotation=0)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix', size=16);","c71d8fa8":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nnum_features = X_processed.shape[1]\n\nmodel_nn = models.Sequential([layers.InputLayer((num_features,)),\n                              layers.Dense(128, activation='relu'),\n                              layers.Dropout(0.5),\n                              layers.Dense(64, activation='relu'),\n                              layers.Dropout(0.5),\n                              layers.Dense(32, activation='relu'),\n                              layers.Dropout(0.5),\n                              layers.Dense(16, activation='relu'),\n                              layers.Dropout(0.5),\n                              layers.Dense(4, activation='relu'),\n                              layers.Dropout(0.5),\n                              layers.Dense(1, activation='sigmoid')])\n\nmodel_nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel_nn.summary()","85fbf70a":"class_weights = class_weight.compute_class_weight('balanced',\n                                                  classes=np.unique(y_train),\n                                                  y=y_train)\nclass_weight_dict = dict(enumerate(class_weights))\n\nmodel_nn.fit(X_train_scaled, y_train, batch_size=2048, epochs=30,\n             validation_data = (X_val_scaled, y_val), class_weight=class_weight_dict)","ea99751a":"y_pred_proba = model_nn.predict(X_val_scaled)\ny_pred_proba","c4a53c99":"y_pred = (y_pred_proba >= .5).astype('int')\nprint(classification_report(y_val, y_pred))","1ad6f208":"# Create the confusion matrix\nconf_matrix = confusion_matrix(y_val, y_pred)\n(tn, fp, fn, tp) = conf_matrix.ravel()\nnn_precision = tp \/ (tp+fp)\nnn_recall = tp \/ (tp+fn)\nnn_auc_roc = roc_auc_score(y_val, y_pred)\nprint('Precision: ', nn_precision)\nprint('Recall: ', nn_recall)\nprint('AUC-ROC score: ', nn_auc_roc)","1b797235":"#Plot the confusion matrix\nsns.heatmap(conf_matrix, annot=True, fmt='d',\n            xticklabels=labels, yticklabels=labels,\n            annot_kws={'size': 14})\nplt.yticks(rotation=0)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix', size=16);","1f5aa1ab":"# Plot heatmap of pivot table with Previously_Insured and Vehicle_Damage as grouping variables\nx_labels = ['No', 'Yes']\ny_labels = ['No', 'Yes']\nz = insured__vehicle_damage.values.round(4)  # Round to 4 decimal places\n\nfig = ff.create_annotated_heatmap(z=z,\n                                  x=x_labels, y=y_labels, showscale=True,\n                                  hovertemplate='Vehicle_Damage: %{x}' +\n                                  '<br>Previously_Insured: %{y}<br>' +\n                                  'Proportion: %{z:.4f}<extra><\/extra>')\nfig.update_xaxes(title='Vehicle_Damage', side='bottom')\nfig.update_yaxes(title='Previously_Insured')\nfig.update_layout(title='Proportion of Customers Interested')\nfig.show('notebook')","1d847d29":"feature_importance.head(5)  # Previously_Insured and Vehicle_Damage are the top features for prediction","a7761ac6":"# Unpivot table to plot as bar chart\ninsured_damage_df = pd.melt(insured__vehicle_damage.reset_index(),\n                            id_vars='Previously_Insured',\n                            value_vars=['Yes', 'No'],\n                            value_name='Prop_Response')\ninsured_damage_df","2b87ae5d":"# Plot proportion of customers interested using bar chart\nfig = px.bar(insured_damage_df, x='Vehicle_Damage', y='Prop_Response',\n             color='Previously_Insured', barmode='group',\n             title=(\"Proportion of Customers Interested Based On\" \n                    \"'Previously_Insured' and 'Vehicle_Damage' Indicators\"))\nfig.update_yaxes(title='')\nfig.show('notebook')","2e0f2677":"# Plot the mean response for every bin determined by Vintage data\nfig = px.histogram(df, x='Vintage', y='Response', histfunc='avg', nbins=50,\n                   title='Proportion of Customers Interested Based On Vintage')\nfig.update_traces(marker_line_width=.5, marker_line_color='white')\nfig.update_yaxes(title='')\nfig.update_xaxes(title='Vintage (days)')\nfig.show('notebook')","0abbc8c6":"# Show metrics of the best model\nprint('Precision: ', log_reg_precision)\nprint('Recall: ', log_reg_recall)\nprint('AUC-ROC score: ', log_reg_auc_roc)","2ba02367":"# Plot metrics of models tested in the previous section\n\nmetrics_df = pd.DataFrame({'Metrics': ['Precision', 'Recall', 'AUC-ROC Score'],\n                           'Logistic Regression': [log_reg_precision, log_reg_recall,\n                                                   log_reg_auc_roc],\n                           'Random Forest': [rf_precision, rf_recall, rf_auc_roc],\n                           'Neural Network': [nn_precision, nn_recall, nn_auc_roc]})\nmetrics_df = pd.melt(metrics_df, id_vars='Metrics', value_vars=['Logistic Regression',\n                                                                'Random Forest',\n                                                                'Neural Network'],\n                     var_name='Models', value_name='Score')\n\n\nfig = px.bar(metrics_df, x='Models', y='Score',\n             color='Metrics', barmode='group',\n             title='Model Evaluation Metrics ')\nfig.update_yaxes(title='')\nfig.show('notebook')","4229e5d2":"test_df = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\ntest_df.set_index('id', inplace=True)\ntest_df.head()","71eb0bf6":"X_test = preprocess_data(test_df, fit=False)\nX_test","a6cc60a7":"# Fit the model once again to the full train data\nlog_reg.fit(X_processed, y)","cf01cce8":"y_test_pred_proba = log_reg.predict_proba(X_test)[:, 1]\ny_test_pred_proba","5faf5fa6":"# Count number of predicted interested customers\ny_test_pred = (y_test_pred_proba >= .5).astype(int)\npd.Series(y_test_pred).value_counts()","11222dec":"submission = pd.DataFrame({'Response': y_test_pred_proba},\n                          index=test_df.index)\nsubmission.head()","444c14c7":"submission.to_csv('vehicle_insurance.csv')","172fd703":"With 0.8 AUC-ROC score, the logistic regression model is good enough. This model is also exceptional for predicting interested customers as shown by the 94% recall (target market will be wide enough to cover most of the interested customers). Although the precision is only 28% (28% of advertised customers is interested), it is acceptable because advertising to random customers will only get 12% precision (the proportion of interested customers in the dataset is approximately 12%).","dfb186d6":"Distribution of vintage is approximately uniform","7ad19511":"There are positive correlations between vehicle age and vehicle damage experience. The older the vehicle, the more likely it has got damaged.","e5b22479":"As analyzed in the previous section, `Previously_Insured` and `Vehicle_Damage` indicators are indeed the best predictor for customers' interests.","ffe64fbc":"Even though the precision in this model is higher than the logistic regression model, the recall and AUC-ROC score is considerably lower, meaning that the target market is not large enough, as shown by the confusion matrix. Only 9500 customers will receive ads compared to around 38000 by using logistic regression. Therefore, this model is not good enough to predict whether a customer is interested","419d1678":"# <center>Health Insurance Cross Sell Prediction<\/center>","be208827":"There are 155 unique policy sales channels listed, with 93 channels appear less than equal 100 times. In the next section we will group these channels to 1 group called `Others`","55bef01f":"## Section 2: Data Understanding","ca78dffc":"### Question 3: How well we can predict customers' interests based on customer data?","9df17120":"## Section 4: Data Modelling","187c726e":"## Section 3: Data Preparation","5f0c9691":"## Section 1: Business Understanding","e49c97df":"- 'Previously Insured' -> whether the customer already has Vehicle Insurance from another company\n- 'Vehicle Damage' -> wheter customer got his\/her vehicle damaged in the past","e61d966c":"It seems there are outliers, let's check it out","7867d8a0":"The number of males is slightly higher than females","8e88591d":"### Question 3: How well we can predict customers' interests based on customer data?","5dc97693":"There are several categorical variables included such as `Region_Code` and `Policy_Sales_Channel` because they are encoded as integer (although they have float dtype, we must fix it later)","c639fe25":"Neural network model's performance is similar to logistic regression. However because logistic regression model is more interpretable, we will use it to evaluate the result and make predictions for test data.","e8602845":"As shown by the chart above, there is no distinguishable differences between older and newer customers in terms of interest in the new vehicle insurance. This means customers are not considering how long they have been associated with the company to determine whether they are interested in this product. Hence, the `Vintage` variable is not a good predictor for customers' interests. The company's data engineers might consider not to collect this information for this use case because it doesn't have any predictive value.","f8e41470":"### Logistic Regression","8af1c8fc":"### Question 2: Are older customers more interested in vehicle insurance than newer customers?","4c7c8e11":"### Question 2: Are older customers more interested in vehicle insurance than newer customers?","35aae0aa":"There is no identifiable pattern to determine whether 2630 is an encoding for missing value, outlier, or the true value. However, we will treat it as missing value because 2630 rupees ~ $36, quite unlikely to be the true annual premium. Missing values will be imputed using column mean.","e8a22b67":"An insurance company that has provided Health Insurance Products to its customer wants to expand its business to Vehicle Insurance. It needs to build a model to predict whether customers from the past year are interested in its new Vehicle Insurance Products to plan for marketing strategies. First, we will analyze how some variables are related to customers' interests. Then, we will build a machine learning model to classify whether a customer is interested. Customers forecasted to be interested in this product will become the target market and receive ads promoting this product.","a9fd2e0b":"## Section 6: Make Prediction on Test Data","338b228b":"### Explore","74a2c51c":"### Question 1: How do 'Previously Insured' and 'Vehicle Damage' indicators correlate with customers' interest in this new vehicle insurance?","86f38b27":"Dominated by young (20-30 years) and middle-aged (40-50 years) customers (bimodal distribution)","9d3f0391":"### Gather","2a3d11ac":"No missing values in the dataset.","8a971d7b":"Using the result from visualization and modeling, `Previously_Insured` and `Vehicle_Damage` indicators are correlated with customers' interests in the new vehicle insurance. `Previously_Insured` indicator has a negative correlation with customers' interests, since people that already have vehicle insurance most likely don't need another vehicle insurance. On the other hand, `Vehicle_Damage` indicator has a positive correlation. The reason is people who have experienced vehicle damage before have realized the importance of vehicle insurance to cover the loss. Therefore, they might be more interested, especially if they don't have any vehicle insurance before.","d9a65066":"No significant differences between the number of customers that have experienced vehicle damage.","3f2135a4":"Only 0.3% of customers don't have a driving license. However, customers with no driving license tend to be not interested in vehicle insurance so, we will keep this column for prediction.","e61f1cd1":"### Random Forest Classifier","e314bb35":"We will try 2 different machine learning models to classify whether customer is interested in the new vehicle insurance. The metrics to score the model are AUC-ROC (Area under ROC curve) and the precision and recall of the model. ","baa56168":"The proportion of customer interested in every bin is approximately equal. Furthermore, the absolute value of correlation between response and vintage is the lowest compared to other varibales. Hence, vintage is not a good predictor variable for response and we should consider to drop it.","78a4df84":"### Neural Network","59782ba6":"The Logistic Regression and Neural Network models are quite good at predicting customers' interests with approximately 0.8 AUC-ROC score while the Random Forest model performs poorly in this dataset. The results from the former models are similar with a slight difference in the precision-recall tradeoff. However, because the Logistic Regression model is more interpretable than the neural network, which acts as a black box, it is considered better than the Neural Network model. Consequently, we will use the Logistic Regression model to predict the test data in the next section.","59b3a9d2":"From 3 cells above, customer that does not have vehicle insurance but have experienced vehicle damage are most likely to be interested in vehicle insurance product.","c96cd432":"## Section 5: Evaluate the Results","7cee055f":"Number of customers not having vehicle insurance is higher, presenting an opportunity for the company.","c130dc12":"There are noticeable differences in customers interested proportion based on `Region_Code`. Because this variable is categorical, we will use dummies to encode this variable in the next section.","8e5db64b":"### Question 1: How do 'Previously Insured' and 'Vehicle Damage' indicators correlate with customers' interest in this new vehicle insurance?"}}