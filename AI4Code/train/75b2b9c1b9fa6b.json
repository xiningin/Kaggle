{"cell_type":{"dcfc43d4":"code","acbff27f":"code","c3eb4c09":"code","15fc85c8":"code","eed33a42":"code","c911d1f4":"code","f5df9482":"code","06a8bb38":"code","499c223f":"code","197aaf6c":"code","a97fced6":"code","48717879":"code","0a8c28ab":"code","abe6d432":"code","41a4e3d3":"code","ea0a4dce":"code","9d995cf3":"code","81e3ee5d":"code","a069fc70":"code","698ad2ba":"code","1106b95f":"code","f0ad9250":"code","65fc86ee":"code","38514e7f":"code","40283834":"code","18f29f89":"code","1399308c":"code","57cb3f38":"code","cb4c7d12":"code","f83d88fd":"code","6163318e":"code","e1eff26b":"code","3ec18b3a":"code","b9b1ebba":"code","9adeb963":"code","cf60b8d0":"code","7965fc6b":"code","e81ee9c9":"code","7d155994":"code","36436e61":"code","01567e18":"code","d6a29edd":"code","8a0dcdd5":"code","71c696d4":"code","192612a8":"code","bbcba2d3":"code","5e5bbf4c":"code","32b7582b":"code","ec33d3a1":"code","4a0b4843":"code","be07f060":"code","3e266a88":"code","b784fb94":"markdown","9515d992":"markdown","52913546":"markdown","6130dac2":"markdown","68bd5e49":"markdown","56fcafc1":"markdown","36b21c1b":"markdown","8500be21":"markdown","c5c3d309":"markdown","1e56d414":"markdown","344a7079":"markdown","6a53d3b9":"markdown","18f67223":"markdown","d3ac4bfc":"markdown","3005f4b6":"markdown","f57f507f":"markdown","f18cd684":"markdown","adb01512":"markdown","bde38740":"markdown","939168c7":"markdown","717f6d48":"markdown","7e590c73":"markdown","45e157f3":"markdown","54f98279":"markdown","da7d2343":"markdown","bff8c560":"markdown","6d2167d3":"markdown","57adc5bd":"markdown","5be522e9":"markdown","d1ba8904":"markdown","86f87979":"markdown","e10c86c5":"markdown","23788a6c":"markdown","e95bde7e":"markdown","4b87a06a":"markdown","534e4de7":"markdown","9c5c1aee":"markdown","8d724359":"markdown","7e20a522":"markdown"},"source":{"dcfc43d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acbff27f":"import glob\nimport os.path as osp\nimport copy\n\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pickle\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torchvision import datasets, models\nfrom torchvision.utils import make_grid\n\nimport os\nimport time\nfrom PIL import Image\nfrom IPython.display import display\n\nimport warnings\nwarnings.filterwarnings('ignore')","c3eb4c09":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n## Image Augmentation \n\n# skimage\nfrom skimage.io import imshow, imread, imsave\nfrom skimage.transform import rotate, AffineTransform, warp,rescale, resize, downscale_local_mean\nfrom skimage import color,data\nfrom skimage.exposure import adjust_gamma\nfrom skimage.util import random_noise\n\n\n# 3D scatter plot\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib import colors\n\n\n#OpenCV-Python\nimport cv2\n\n# imgaug\nimport imageio\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\nSAMPLE_LEN=100","15fc85c8":"class Config:\n    num_classes = 12\n    img_size = 224\n    batch_size = 64\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    min_lr = 10**-12\n    max_lr = 10\n    pretrained = False\n    criterion = nn.CrossEntropyLoss()\n    epochs = 1","eed33a42":"train_image_path = '..\/input\/plant-pathology-2021-fgvc8\/train_images'\ntest_image_path = '..\/input\/plant-pathology-2021-fgvc8\/test_images'\ntrain_df_path = '..\/input\/plant-pathology-2021-fgvc8\/train.csv'\ntest_df_path = '..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv'","c911d1f4":"df_train = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2021-fgvc8\/train.csv\")\ndf_sub = pd.read_csv(\"\/kaggle\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv\")","f5df9482":"#The number of labels\nlen(df_train.labels.unique())","06a8bb38":"#The no.values per label\ndf_train.labels.value_counts()","499c223f":"plt.figure(figsize=(15,12))\nlabels = sns.barplot(df_train.labels.value_counts().index,df_train.labels.value_counts())\nfor item in labels.get_xticklabels():\n    item.set_rotation(45)","197aaf6c":"source = df_train['labels'].value_counts()","a97fced6":"fig = go.Figure(data=[go.Pie(labels=source.index,values=source.values)])\nfig.update_layout(title='Label distribution')\nfig.show()","48717879":"# img_shapes = {}\n# for image_name in tqdm(os.listdir(train_image_path)[:300]):\n#     image = cv2.imread(os.path.join(train_image_path, image_name))\n#     img_shapes[image.shape] = img_shapes.get(image.shape, 0) + 1\n\n# print(img_shapes)","0a8c28ab":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndef encode_label(df):\n    df['encoded_label'] = le.fit_transform(df.labels.values)\n    return df\n\nencode_label(df_train)\n    \n# L\u01b0u t\u1eeb \u0111i\u1ec3n m\u00e3 h\u00f3a\ndf_labels_idx = df_train.loc[df_train.duplicated([\"labels\", \"encoded_label\"])==False]\\\n                [[\"encoded_label\", \"labels\"]].set_index(\"encoded_label\").sort_index()\ndisplay(df_labels_idx)","abe6d432":"# def load_image(image_id):\n#     file_path = image_id\n#     image = cv2.imread(train_image_path+'\/'+ file_path)\n#     return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# # L\u1ea5y 100 \u1ea3nh l\u00e0m sample v\u1edbi SAMPLE_LEN=100 cho RBG Channel Analysis\n\n# train_images = df_train[\"image\"][:SAMPLE_LEN].apply(load_image)","41a4e3d3":"# red_values = [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))]\n# green_values = [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))]\n# blue_values = [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]\n# values = [np.mean(train_images[idx]) for idx in range(len(train_images))]","ea0a4dce":"# fig = ff.create_distplot([red_values], group_labels=[\"R\"], colors=[\"red\"])\n# fig.update_layout(showlegend=False, template=\"simple_white\")\n# fig.update_layout(title_text=\"Ph\u00e2n ph\u1ed1i K\u00eanh \u0110\u1ecf\")\n# # fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig","9d995cf3":"# fig = ff.create_distplot([green_values], group_labels=[\"G\"], colors=[\"green\"])\n# fig.update_layout(showlegend=False, template=\"simple_white\")\n# fig.update_layout(title_text=\"Ph\u00e2n ph\u1ed1i K\u00eanh Xanh L\u00e1\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig","81e3ee5d":"# fig = ff.create_distplot([blue_values], group_labels=[\"B\"], colors=[\"blue\"])\n# fig.update_layout(showlegend=False, template=\"simple_white\")\n# fig.update_layout(title_text=\"Ph\u00e2n ph\u1ed1i K\u00eanh Xanh Lam\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig","a069fc70":"# fig = go.Figure()\n\n# for idx, values in enumerate([red_values, green_values, blue_values]):\n#     if idx == 0:\n#         color = \"Red\"\n#     if idx == 1:\n#         color = \"Green\"\n#     if idx == 2:\n#         color = \"Blue\"\n#     fig.add_trace(go.Box(x=[color]*len(values), y=values, name=color, marker=dict(color=color.lower())))\n    \n# fig.update_layout(yaxis_title=\"Mean value\", xaxis_title=\"Color channel\",\n#                   title=\"Mean value vs. Color channel\", template=\"plotly_white\")","698ad2ba":"# fig = ff.create_distplot([red_values, green_values, blue_values],\n#                          group_labels=[\"R\", \"G\", \"B\"],\n#                          colors=[\"red\", \"green\", \"blue\"])\n# fig.update_layout(title_text=\"Distribution of red channel values\", template=\"simple_white\")\n# fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[0].marker.line.width = 0.5\n# fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[1].marker.line.width = 0.5\n# fig.data[2].marker.line.color = 'rgb(0, 0, 0)'\n# fig.data[2].marker.line.width = 0.5\n# fig","1106b95f":"df_train['label_list'] = df_train['labels'].str.split(' ')","f0ad9250":"lbls = ['healthy','complex','rust','frog_eye_leaf_spot','powdery_mildew','scab']\nfor x in lbls:\n    df_train[x]=0","65fc86ee":"def lbl_lgc(col,lbl_list):\n    if col in lbl_list:\n        res = 1 \n    else:\n        res = 0\n    return res","38514e7f":"for x in lbls:\n    df_train[x] = np.vectorize(lbl_lgc)(x,df_train['label_list'])","40283834":"df_train","18f29f89":"df_train_lbl_onehot = pd.get_dummies(df_train, columns=[\"labels\"], prefix=[\"LBL\"])","1399308c":"df_train_lbl_onehot.columns","57cb3f38":"# plt.figure(figsize=(35,20))\n# fig = px.parallel_categories(df_train[['healthy','complex','rust','frog_eye_leaf_spot','powdery_mildew','scab']], color=\"healthy\", color_continuous_scale=\"sunset\",\\\n#                              title=\"Parallel categories plot of targets\")\n# fig","cb4c7d12":"from sklearn.model_selection import train_test_split","f83d88fd":"def make_datapath_list(phase='train', val_size=0.25):\n    if phase in [\"train\", \"val\"]:\n        phase_path = \"train_images\"\n    elif phase in [\"test\"]:\n        phase_path = \"test_images\"\n    else:\n        print(f\"{phase} not in path\")    \n        \n    \"\"\"\n    Use resized training dataset for betting training speed\n    Resized datase from: https:\/\/www.kaggle.com\/ankursingh12\/resized-plant2021\n    \"\"\"\n    if phase == 'train' or phase == 'val': \n        rootpath = \"\/kaggle\/input\/resized-plant2021\/img_sz_256\/\"\n    else:\n        rootpath = \"\/kaggle\/input\/plant-pathology-2021-fgvc8\/test_images\/\"\n    \n    target_path = osp.join(rootpath+\"\/*.jpg\")\n    path_list = []\n    \n    for path in glob.glob(target_path):\n        path_list.append(path)\n        \n    if phase in [\"train\", \"val\"]:\n        train, val = train_test_split(path_list, test_size=val_size, random_state=0, shuffle=True)\n        if phase == \"train\":\n            path_list = train\n        else:\n            path_list = val\n    \n    return path_list","6163318e":"train_list = make_datapath_list(phase='train')\nprint(f'The length of training set: {len(train_list)}')\nval_list = make_datapath_list(phase='val')\nprint(f'The length of valuation set: {len(val_list)}')\ntest_list = make_datapath_list(phase='test')\nprint(f'The length of testing set: {len(test_list)}')","e1eff26b":"import albumentations as A\nfrom albumentations import Compose\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\n\ntransform = {\n    'train': Compose([\n        A.Rotate(p=0.1, limit=(-85, 80)),\n        A.RandomShadow(\n            num_shadows_lower=2, \n            num_shadows_upper=3, \n            shadow_dimension=3, \n            shadow_roi=(0, 0.7, 0.4, 0.8), \n            p=0.4\n        ),\n        A.ShiftScaleRotate(\n            shift_limit=0.055, \n            scale_limit=0.065, \n            rotate_limit=35, \n            p=0.6\n        ),\n        A.RandomFog(\n            fog_coef_lower=0.2, \n            fog_coef_upper=0.2, \n            alpha_coef=0.2, \n            p=0.3\n        ),\n        A.RGBShift(\n            r_shift_limit=25, \n            g_shift_limit=15, \n            b_shift_limit=15, \n            p=0.3\n        ),\n        A.RandomBrightnessContrast(p=0.3),\n        A.GaussNoise(\n            var_limit=(50, 70),  \n            always_apply=False, \n            p=0.3\n        ),\n        A.Resize(height=Config.img_size, width=Config.img_size),\n        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n        ToTensorV2()\n    ]),\n    'val': Compose([\n        A.Resize(Config.img_size, Config.img_size),\n        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),            \n        ToTensorV2()\n    ]),\n    'test': Compose([\n        A.Resize(Config.img_size, Config.img_size),\n        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n}","3ec18b3a":"class PlantDataset(Dataset):\n    \"\"\"\n    Class to create a Dataset\n    \n    Attributes\n    ----------\n    df_train : DataFrame\n        DataFrame containing the image labels.\n    file_list : list\n        A list containing the paths to the images\n    transform : object\n        Instance of the preprocessing class (ImageTransform)\n    phase : 'train' or 'val' or 'test'\n        Specify whether to use train, validation, or test\n    \"\"\"\n    def __init__(self, df_train, file_list, transform=None, phase='train'):\n        self.df_train = df_train\n        self.df_labels_idx = df_labels_idx\n        self.file_list = file_list\n        self.transform = transform[phase]\n        self.phase = phase\n        \n    def __len__(self):\n        \"\"\"\n        Returns the number of images.\n        \"\"\"\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Get data in Tensor format and labels of preprocessed images.\n        \"\"\"\n        \n        # Load the index number image.\n        img_path = self.file_list[index]\n        img = Image.open(img_path)\n        \n        # Preprocessing images\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_transformed = self.transform(image=img)\n        \n        # image name\n        image_name = img_path[-20:]\n        \n        # Extract the labels\n        if self.phase in [\"train\", \"val\"]:\n            label = df_train.loc[df_train[\"image\"]==image_name][\"encoded_label\"].values[0]\n        elif self.phase in [\"test\"]:\n            label = -1\n        \n        return img_transformed, label, image_name","b9b1ebba":"train_dataset = PlantDataset(df_train, train_list, transform=transform, phase='train')\nval_dataset = PlantDataset(df_train, val_list, transform=transform, phase='val')\ntest_dataset = PlantDataset(df_train, test_list, transform=transform, phase='test')\n\nindex = 0\n\nprint(\"\u3010train dataset\u3011\")\nprint(f\"img num : {train_dataset.__len__()}\")\n# print(f\"img : {train_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {train_dataset.__getitem__(index)[1]}\")\nprint(f\"image name : {train_dataset.__getitem__(index)[2]}\")\n\nprint(\"\\n\u3010validation dataset\u3011\")\nprint(f\"img num : {val_dataset.__len__()}\")\n# print(f\"img : {val_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {val_dataset.__getitem__(index)[1]}\")\nprint(f\"image name : {val_dataset.__getitem__(index)[2]}\")\n\nprint(\"\\n\u3010test dataset\u3011\")\nprint(f\"img num : {test_dataset.__len__()}\")\n# print(f\"img : {test_dataset.__getitem__(index)[0].size()}\")\nprint(f\"label : {test_dataset.__getitem__(index)[1]}\")\nprint(f\"image name : {test_dataset.__getitem__(index)[2]}\")","9adeb963":"train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, num_workers=2,shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=Config.batch_size, num_workers=2, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=Config.batch_size, num_workers=2, shuffle=False)\n\n# to Dictionary\ndataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader, \"test\": test_dataloader}","cf60b8d0":"for i, image_data in enumerate(train_dataloader):\n    break","7965fc6b":"plt.figure(figsize=(20, 20))\n\nim = make_grid(image_data[0]['image'], nrow=8)\nplt.imshow(np.transpose(im.numpy(), (1, 2, 0)))","e81ee9c9":"from sklearn.metrics import f1_score, accuracy_score","7d155994":"model_config = {\n    \"name\": \"2 FCs, 0.0001 Lr, 30 Epochs\",\n    \"classifier\": torch.nn.Sequential(\n                  torch.nn.Linear(2048, 512),\n                  torch.nn.Linear(512, 12)),\n    \"lr\": 0.0001,\n    \"epoch\": 30\n}","36436e61":"use_pretrained = True\npretrained_model = models.resnext50_32x4d(pretrained=use_pretrained)","01567e18":"name, classifier, lr, epoch = model_config.values()\n\npretrained_model.fc = classifier\n\nprint(f'Model name: {name}')\nprint(pretrained_model)","d6a29edd":"# def lr_finder(model, min_lr, max_lr, dataset_lenght=train_dataset.__len__(), \\\n#               batch_size=Config.batch_size, criterion=Config.criterion):\n#     iter_lrs = [min_lr]\n#     iter_losses = []\n    \n#     factor = np.exp(np.log(max_lr \/ min_lr) \/ (dataset_lenght \/ batch_size))\n    \n#     # Train model with 1 epoch\n#     model.to(Config.device)\n#     for i, data in tqdm(enumerate(dataloaders_dict['train']), total=len(dataloaders_dict['train'])):\n        \n#         optimizer = Adam(model.parameters(), lr=min_lr)\n        \n#         # set inputs, labels based on dataloader's batch data\n#         inputs = data[0]['image']\n#         labels = data[1]\n#         inputs = inputs.to(Config.device)\n#         labels = labels.to(Config.device)\n\n#         #zero the parameter gradients\n#         optimizer.zero_grad()\n\n#         # forward\n#         outputs = model(inputs)\n#         loss = criterion(outputs, labels)\n        \n#         # backward + optimize only if in training phase\n#         loss.backward()\n#         optimizer.step()\n                \n#         # Update and append next iteration learning rate\n#         iter_lrs.append(min_lr)\n#         min_lr = min_lr * factor\n        \n#         # Append this iteration loss\n#         iter_losses.append(np.log(loss.cpu().data.numpy().tolist()))\n        \n#     iter_lrs.pop()\n    \n#     # Plot loss vs log-scaled learning rate\n#     plt.figure(figsize=(10, 7))\n#     plot = sns.lineplot(iter_lrs, iter_losses)\n#     plot.set(xscale=\"log\", \n#              xlabel=\"Learning Rate (log-scale)\", \n#              ylabel=\"Training Loss\",\n#              title=\"Optimal learning rate is slightly below minimum\")","8a0dcdd5":"# test_model = copy.deepcopy(pretrained_model)\n# lr_finder(test_model, Config.min_lr, Config.max_lr)","71c696d4":"Config.lr = 10**-3\noptimizer = Adam(pretrained_model.parameters(), lr=Config.lr)","192612a8":"def plot_result(train_losses, train_accuracy, train_f1, val_losses, val_accuracy, val_f1):\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 7))\n    ax1.plot(train_losses, label='Train')\n    ax1.plot(val_losses, label='Validation')\n    ax1.set_title('Loss')\n    ax1.legend()\n\n    ax2.plot(train_accuracy, label='Train')\n    ax2.plot(val_accuracy, label='Validation')\n    ax2.set_title('Accuracy')\n    ax2.legend()\n\n    ax3.plot(train_f1, label='Train')\n    ax3.plot(val_f1, label='Validation')\n    ax3.set_title('F1 Score')\n    ax3.legend()","bbcba2d3":"def append_list(list, appended):\n    for el in appended:\n        list.append(el)\n    return list","5e5bbf4c":"def train_model(model, criterion, optimizer, num_epochs=Config.epochs):\n    \n    train_losses = []\n    train_accuracy = []\n    train_f1 = []\n\n    val_losses = []\n    val_accuracy = []\n    val_f1 = []\n    \n    print(f\"Devices to be used : {Config.device}\")\n    model.to(Config.device)\n    torch.backends.cudnn.benchmark = True\n    \n    start_time = time.time()\n        \n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n            \n            epoch_targets = []\n            epoch_predictions = []\n\n            # Iterate over data.\n            for i, data in tqdm(enumerate(dataloaders_dict[phase]), total=len(dataloaders_dict[phase])):\n#                 inputs = np.transpose(data[0]['image'], (0, 3, 1, 2))\n                inputs = data[0]['image']\n                labels = data[1]\n                inputs = inputs.to(Config.device)\n                labels = labels.to(Config.device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                np_preds = preds.cpu().data.numpy()\n                np_labels = labels.cpu().data.numpy()\n                append_list(epoch_predictions, np_preds)\n                append_list(epoch_targets, np_labels)\n                \n                batch_f1 = f1_score(preds.cpu().data.numpy(), labels.cpu().data.numpy(), average='weighted')\n                \n#                 if i % 50 == 0 and i != 0:\n#                 print(f'Batch: {i}  |  Loss: {loss.item():.4f}   |   F1-score: {batch_f1:.4f}%')         \n\n            epoch_loss = running_loss \/ len(dataloaders_dict[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders_dict[phase].dataset)\n            \n            epoch_f1 = f1_score(epoch_predictions, epoch_targets, average='weighted')\n            \n            if phase == 'train':\n                train_losses.append(epoch_loss)\n                train_accuracy.append(epoch_acc)\n                train_f1.append(epoch_f1)\n            else:\n                val_losses.append(epoch_loss)\n                val_accuracy.append(epoch_acc)\n                val_f1.append(epoch_f1)\n    \n            print('{} Loss: {:.4f} Acc: {:.4f} F1_score: {:.4f}'.format('----> ' + phase.capitalize(), epoch_loss, epoch_acc, epoch_f1))\n            \n    print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed\n    \n    plot_result(train_losses, train_accuracy, train_f1, val_losses, val_accuracy, val_f1)\n    \n    return model","32b7582b":"trained_model = train_model(pretrained_model, Config.criterion, optimizer, num_epochs=1)","ec33d3a1":"def save_model(model, filename):\n    Pkl_Filename = name + \".pkl\"\n\n    with open(Pkl_Filename, 'wb') as file:\n        pickle.dump(model, file)\n        \nsave_model(trained_model, name)","4a0b4843":"class PlantPredictor():\n    \"\"\"\n    Class for predicting labels from output results\n    \n    Attributes\n    ----------\n    df_labels_idx: DataFrame\n        DataFrame that associates INDEX with a label name\n    \"\"\"\n    \n    def __init__(self, model, df_labels_idx, dataloaders_dict):\n        self.model = model\n        self.df_labels_idx = df_labels_idx\n        self.dataloaders_dict = dataloaders_dict\n        self.df_submit = pd.DataFrame()\n        \n    \n    def __predict_max(self, out):\n        \"\"\"\n        Get the label name with the highest probability.\n        \n        Parameters\n        ----------\n        predicted_label_name: str\n            Name of the label with the highest prediction probability\n        \"\"\"\n        maxid = np.argmax(out.detach().numpy(), axis=1)\n        df_predicted_label_name = self.df_labels_idx.iloc[maxid]\n        \n        return df_predicted_label_name\n    \n    def inference(self):\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        df_pred_list = []\n        for i, data in enumerate(self.dataloaders_dict['test']):\n            image_name = data[2]\n            self.model.to(device)\n            inputs = data[0]['image']\n            inputs = inputs.to(device)\n            out = self.model(inputs)\n            device = torch.device(\"cpu\")\n            out = out.to(device)\n            df_pred = self.__predict_max(out).reset_index(drop=True)\n            df_pred[\"image\"] = image_name\n            df_pred_list.append(df_pred)\n            \n        self.df_submit = pd.concat(df_pred_list, axis=0)\n        self.df_submit = self.df_submit[[\"image\", \"labels\"]].reset_index(drop=True)","be07f060":"predictor = PlantPredictor(trained_model, df_labels_idx, dataloaders_dict)\npredictor.inference()\ndf_submit = predictor.df_submit.copy()","3e266a88":"df_submit.to_csv('submission.csv', index=False)\ndf_submit","b784fb94":"## Ph\u00e2n ph\u1ed1i K\u00eanh \u0110\u1ecf","9515d992":"# 5. Hu\u1ea5n luy\u1ec7n v\u00e0 \u0111\u00e1nh gi\u00e1 model","52913546":"## 3.2 Augumentation\nNh\u1eadn th\u1ea5y l\u01b0\u1ee3ng d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cung c\u1ea5p kh\u00e1 h\u1ea1n ch\u1ebf, \u0111\u1ec3 tr\u00e1nh vi\u1ec7c m\u00f4 h\u00ecnh g\u1eb7p t\u00ecnh tr\u1ea1ng overfit, ch\u00fang ta n\u00ean s\u1eed d\u1ee5ng k\u1ef9 thu\u1eadt *Augmentation* \u0111\u1ec3 t\u0103ng l\u01b0\u1ee3ng d\u1eef li\u1ec7u c\u00f3 s\u1eb5n.","6130dac2":"# RGB Analysis\nHistogram l\u00e0 m\u1ed9t bi\u1ec3u di\u1ec5n \u0111\u1ed3 h\u1ecda cho bi\u1ebft t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb m\u00e0u kh\u00e1c nhau trong h\u00ecnh \u1ea3nh. Trong kh\u00f4ng gian m\u00e0u RGB, c\u00e1c gi\u00e1 tr\u1ecb pixel n\u1eb1m trong kho\u1ea3ng t\u1eeb 0 \u0111\u1ebfn 255 trong \u0111\u00f3 0 l\u00e0 m\u00e0u \u0111en v\u00e0 255 l\u00e0 m\u00e0u tr\u1eafng. Ph\u00e2n t\u00edch bi\u1ec3u \u0111\u1ed3 c\u00f3 th\u1ec3 gi\u00fap ch\u00fang ta hi\u1ec3u \u0111\u01b0\u1ee3c ph\u00e2n b\u1ed1 \u0111\u1ed9 s\u00e1ng, \u0111\u1ed9 t\u01b0\u01a1ng ph\u1ea3n v\u00e0 c\u01b0\u1eddng \u0111\u1ed9 c\u1ee7a h\u00ecnh \u1ea3nh. B\u00e2y gi\u1edd ch\u00fang ta h\u00e3y xem bi\u1ec3u \u0111\u1ed3 c\u1ee7a m\u1ed9t m\u1eabu \u0111\u01b0\u1ee3c ch\u1ecdn ng\u1eabu nhi\u00ean t\u1eeb m\u1ed7i danh m\u1ee5c.","68bd5e49":"# 6. D\u1ef1 \u0111o\u00e1n d\u1eef li\u1ec7u c\u1ee7a t\u1eadp test","56fcafc1":"### T\u1ea1o predictor ","36b21c1b":"# 3. X\u1eed l\u00fd d\u1eef li\u1ec7u","8500be21":"## <a name=\"Wheat Detection\">Plant Pathology 2021 FGVC8 <\/a>\n\n#### <a name=\"About_Competition\"> Gi\u1edbi thi\u1ec7u <\/a>\n\nT\u00e1o l\u00e0 m\u1ed9t trong nh\u1eefng lo\u1ea1i c\u00e2y \u0103n qu\u1ea3 \u00f4n \u0111\u1edbi quan tr\u1ecdng nh\u1ea5t tr\u00ean th\u1ebf gi\u1edbi. B\u1ec7nh ch\u00e1y l\u00e1  l\u00e0 m\u1ed1i \u0111e d\u1ecda l\u1edbn \u0111\u1ed1i v\u1edbi n\u0103ng su\u1ea5t v\u00e0 ch\u1ea5t l\u01b0\u1ee3ng chung c\u1ee7a v\u01b0\u1eddn t\u00e1o. Quy tr\u00ecnh ch\u1ea9n \u0111o\u00e1n b\u1ec7nh tr\u00ean v\u01b0\u1eddn t\u00e1o hi\u1ec7n nay d\u1ef1a tr\u00ean vi\u1ec7c d\u00f2 t\u00ecm th\u1ee7 c\u00f4ng c\u1ee7a con ng\u01b0\u1eddi, t\u1ed1n nhi\u1ec1u th\u1eddi gian v\u00e0 chi ph\u00ed.\n\nM\u1eb7c d\u00f9 c\u00e1c m\u00f4 h\u00ecnh d\u1ef1a tr\u00ean th\u1ecb gi\u00e1c m\u00e1y t\u00ednh \u0111\u00e3 cho th\u1ea5y nhi\u1ec1u h\u1ee9a h\u1eb9n trong vi\u1ec7c x\u00e1c \u0111\u1ecbnh b\u1ec7nh th\u1ef1c v\u1eadt, nh\u01b0ng v\u1eabn c\u00f2n m\u1ed9t s\u1ed1 h\u1ea1n ch\u1ebf c\u1ea7n \u0111\u01b0\u1ee3c gi\u1ea3i quy\u1ebft. S\u1ef1 kh\u00e1c bi\u1ec7t l\u1edbn v\u1ec1 c\u00e1c tri\u1ec7u ch\u1ee9ng h\u00ecnh \u1ea3nh c\u1ee7a m\u1ed9t b\u1ec7nh \u0111\u01a1n l\u1ebb tr\u00ean c\u00e1c gi\u1ed1ng t\u00e1o kh\u00e1c nhau, ho\u1eb7c c\u00e1c gi\u1ed1ng m\u1edbi c\u00f3 ngu\u1ed3n g\u1ed1c \u0111\u01b0\u1ee3c tr\u1ed3ng tr\u1ecdt, l\u00e0 nh\u1eefng th\u00e1ch th\u1ee9c l\u1edbn \u0111\u1ed1i v\u1edbi vi\u1ec7c x\u00e1c \u0111\u1ecbnh b\u1ec7nh d\u1ef1a tr\u00ean th\u1ecb gi\u00e1c m\u00e1y t\u00ednh. Nh\u1eefng bi\u1ebfn th\u1ec3 n\u00e0y ph\u00e1t sinh do s\u1ef1 kh\u00e1c bi\u1ec7t trong m\u00f4i tr\u01b0\u1eddng ch\u1ee5p \u1ea3nh v\u00e0 t\u1ef1 nhi\u00ean, v\u00ed d\u1ee5, m\u00e0u s\u1eafc l\u00e1 v\u00e0 h\u00ecnh th\u00e1i l\u00e1, tu\u1ed5i c\u1ee7a c\u00e1c m\u00f4 b\u1ecb nhi\u1ec5m b\u1ec7nh, n\u1ec1n \u1ea3nh kh\u00f4ng \u0111\u1ed3ng nh\u1ea5t v\u00e0 \u0111\u1ed9 chi\u1ebfu s\u00e1ng kh\u00e1c nhau trong qu\u00e1 tr\u00ecnh ch\u1ee5p \u1ea3nh, v.v.\n\nPlant Pathology 2021-FGVC8 c\u00f3 t\u1eadp d\u1eef li\u1ec7u th\u00ed \u0111i\u1ec3m g\u1ed3m 3.651 h\u00ecnh \u1ea3nh RGB v\u1ec1 b\u1ec7nh l\u00e1 tr\u00ean qu\u1ea3 t\u00e1o. T\u1eadp d\u1eef li\u1ec7u ch\u1ee9a kho\u1ea3ng 23.000 h\u00ecnh \u1ea3nh RGB ch\u1ea5t l\u01b0\u1ee3ng cao v\u1ec1 c\u00e1c b\u1ec7nh tr\u00ean l\u00e1 t\u00e1o, bao g\u1ed3m m\u1ed9t t\u1eadp d\u1eef li\u1ec7u l\u1edbn v\u1ec1 b\u1ec7nh \u0111\u01b0\u1ee3c chuy\u00ean gia ch\u00fa th\u00edch. B\u1ed9 d\u1eef li\u1ec7u n\u00e0y ph\u1ea3n \u00e1nh c\u00e1c t\u00ecnh hu\u1ed1ng th\u1ef1c t\u1ebf b\u1eb1ng c\u00e1ch th\u1ec3 hi\u1ec7n c\u00e1c n\u1ec1n kh\u00f4ng \u0111\u1ed3ng nh\u1ea5t c\u1ee7a h\u00ecnh \u1ea3nh chi\u1ebfc l\u00e1 \u0111\u01b0\u1ee3c ch\u1ee5p \u1edf c\u00e1c giai \u0111o\u1ea1n tr\u01b0\u1edfng th\u00e0nh kh\u00e1c nhau v\u00e0 v\u00e0o c\u00e1c th\u1eddi \u0111i\u1ec3m kh\u00e1c nhau trong ng\u00e0y trong c\u00e1c c\u00e0i \u0111\u1eb7t m\u00e1y \u1ea3nh ti\u00eau c\u1ef1 kh\u00e1c nhau.\n                           \n\n#### <a name=\"Specific Objectives\">X\u00e1c \u0111\u1ecbnh m\u1ee5c ti\u00eau<\/a>           \n\nM\u1ee5c ti\u00eau ch\u00ednh c\u1ee7a cu\u1ed9c thi l\u00e0 ph\u00e1t tri\u1ec3n c\u00e1c m\u00f4 h\u00ecnh d\u1ef1a tr\u00ean m\u00e1y h\u1ecdc \u0111\u1ec3 ph\u00e2n lo\u1ea1i ch\u00ednh x\u00e1c m\u1ed9t h\u00ecnh \u1ea3nh l\u00e1 nh\u1ea5t \u0111\u1ecbnh t\u1eeb b\u1ed9 d\u1eef li\u1ec7u th\u1eed nghi\u1ec7m cho m\u1ed9t lo\u1ea1i b\u1ec7nh c\u1ee5 th\u1ec3 v\u00e0 x\u00e1c \u0111\u1ecbnh m\u1ed9t b\u1ec7nh ri\u00eang l\u1ebb t\u1eeb nhi\u1ec1u tri\u1ec7u ch\u1ee9ng b\u1ec7nh tr\u00ean m\u1ed9t h\u00ecnh \u1ea3nh l\u00e1 \u0111\u01a1n l\u1ebb.\n\n\n#### <a name=\"Y\u00eau c\u1ea7u\">Y\u00eau c\u1ea7u<\/a>           \n\nC\u00e1c b\u00e0i n\u1ed9p cho cu\u1ed9c thi n\u00e0y ph\u1ea3i \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n th\u00f4ng qua Notebooks. \u0110\u1ec3 submit th\u00e0nh c\u00f4ng, c\u00e1c \u0111i\u1ec1u ki\u1ec7n sau ph\u1ea3i \u0111\u01b0\u1ee3c \u0111\u00e1p \u1ee9ng:\n\n* CPU Notebook <= 9 gi\u1edd th\u1eddi gian ch\u1ea1y\n* GPU Notebook <= 2 gi\u1edd th\u1eddi gian ch\u1ea1y\n* Truy c\u1eadp Internet b\u1ecb v\u00f4 hi\u1ec7u h\u00f3a\n*Pre-trained models \u0111\u01b0\u1ee3c cho ph\u00e9p\nT\u1ec7p g\u1eedi ph\u1ea3i c\u00f3 t\u00ean l\u00e0 submit.csv\n\n\n#### <a name=\"dataset_description\">M\u00f4 t\u1ea3 d\u1eef li\u1ec7u<\/a>: \n\nD\u1eef li\u1ec7u l\u01b0u gi\u1eef h\u00ecnh \u1ea3nh c\u1ee7a c\u00e2y t\u00e1o. L\u00e1 c\u00e2y kh\u1ecfe m\u1ea1nh v\u00e0 b\u1ecb nhi\u1ec5m b\u1ec7nh.\n\nFiles train.csv - d\u1eef li\u1ec7u t\u1eadp hu\u1ea5n luy\u1ec7n.\n\nImage - ID c\u1ee7a h\u00ecnh \u1ea3nh\n\nLabel - c\u00e1c l\u1edbp m\u1ee5c ti\u00eau th\u1ec3 hi\u1ec7n t\u1ea5t c\u1ea3 c\u00e1c b\u1ec7nh \u0111\u01b0\u1ee3c t\u00ecm th\u1ea5y trong h\u00ecnh \u1ea3nh. Nh\u1eefng l\u00e1 kh\u00f4ng t\u1ed1t c\u00f3 qu\u00e1 nhi\u1ec1u b\u1ec7nh \u0111\u1ec3 ph\u00e2n lo\u1ea1i b\u1eb1ng m\u1eaft th\u01b0\u1eddng s\u1ebd c\u00f3 l\u1edbp ph\u1ee9c t\u1ea1p, v\u00e0 c\u0169ng c\u00f3 th\u1ec3 c\u00f3 m\u1ed9t t\u1eadp h\u1ee3p con c\u1ee7a c\u00e1c b\u1ec7nh \u0111\u01b0\u1ee3c x\u00e1c \u0111\u1ecbnh.\n\n\nsample_submission.csv - T\u1ec7p g\u1eedi m\u1eabu \u1edf \u0111\u1ecbnh d\u1ea1ng:\n\n    1. image\n    2. labels\n\ntrain_images - t\u1eadp \u1ea3nh train.\n\ntest_images - t\u1eadp \u1ea3nh test. \n\nPh\u00e2n lo\u1ea1i Labels:\n*     healthy\n*     complex\n*     frog_eye_leaf_spot\n*     frog_eye_leaf_spot complex\n*     powdery_mildew\n*     powdery_mildew complex\n*     rust\n*     rust complex\n*     rust frog_eye_leaf_spot\n*     scab\n*     scab frog_eye_leaf_spot\n*     scab frog_eye_leaf_spot complex\n","c5c3d309":"## K\u1ebft Lu\u1eadn\nCh\u00fang t\u00f4i th\u1ea5y r\u1eb1ng \u1ea3nh c\u00f3 \u0111\u1ed9 ph\u00e2n gi\u1ea3i r\u1ea5t cao n\u00ean ch\u00fang t\u00f4i sau \u0111\u00f3 \u0111\u00e3 \u00e1p d\u1ee5ng resize l\u1ea1i \u1ea3nh \u0111\u1ec3 c\u1ea3i thi\u1ec7n th\u1eddi gian ch\u1ea1y.","1e56d414":"# N\u1ed9i dung\n\n* [<font size=4>EDA<\/font>](#1)\n    * [Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u, l\u1eadp bi\u1ec3u \u0111\u1ed3](#1.1)\n    * [M\u1ed9t s\u1ed1 \u1ea3nh v\u00ed d\u1ee5 t\u1eeb t\u1eadp d\u1eef li\u1ec7u](#1.2)\n    * [RGB Analysis](#1.3)\n    * [Parallel categories plot](#1.3)\n","344a7079":"# Parallel categories plot","6a53d3b9":"## T\u1ed5ng h\u1ee3p c\u00e1c k\u00eanh","18f67223":"## Ph\u00e2n ph\u1ed1i K\u00eanh Xanh L\u00e1","d3ac4bfc":"### Quan s\u00e1t:\nK\u00eanh m\u00e0u xanh lam c\u00f3 s\u1ef1 ph\u00e2n b\u1ed1 \u0111\u1ed3ng \u0111\u1ec1u nh\u1ea5t trong s\u1ed1 ba k\u00eanh m\u00e0u, v\u1edbi \u0111\u1ed9 l\u1ec7ch t\u1ed1i thi\u1ec3u (l\u1ec7ch m\u1ed9t ch\u00fat sang tr\u00e1i). K\u00eanh m\u00e0u xanh lam cho th\u1ea5y s\u1ef1 thay \u0111\u1ed5i l\u1edbn gi\u1eefa c\u00e1c h\u00ecnh \u1ea3nh trong t\u1eadp d\u1eef li\u1ec7u.","3005f4b6":"### Th\u1ef1c hi\u1ec7n d\u1ef1 \u0111o\u00e1n v\u00e0 t\u1ea1o submission","f57f507f":"# 2. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u","f18cd684":"### Bi\u1ec3u \u0111\u1ed3 so s\u00e1nh s\u1ed1 l\u01b0\u1ee3ng \u1ea3nh c\u1ee7a c\u00e1c nh\u00e3n ","adb01512":"## K\u1ebft Lu\u1eadn\n\n- T\u1eadp d\u1eef li\u1ec7u kh\u00e1 kh\u00f4ng c\u00e2n b\u1eb1ng theo bi\u1ec3u \u0111\u1ed3 h\u00ecnh tr\u00f2n \u1edf tr\u00ean\n- Ch\u00fang t\u00f4i s\u1ebd ch\u1ecdn chi\u1ebfn l\u01b0\u1ee3c l\u1ea5y m\u1eabu th\u00edch h\u1ee3p \u0111\u1ec3 gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y. Data augmentation \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 th\u00eam c\u00e1c m\u1eabu b\u1ed5 sung t\u1eeb c\u00e1c l\u1edbp thi\u1ec3u s\u1ed1. Trong  h\u00ecnh \u1ea3nh c\u1ee7a ch\u00fang t\u00f4i, \u0111i\u1ec1u n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c x\u1eed l\u00fd \u0111\u01b0\u1ee3c b\u1eb1ng c\u00e1ch th\u00eam \u0111\u1ed9 m\u00e9o v\u00e0o d\u1eef li\u1ec7u b\u1eb1ng c\u00e1ch th\u1ef1c hi\u1ec7n d\u1ecbch, xoay, thay \u0111\u1ed5i t\u1ef7 l\u1ec7 c\u0169ng nh\u01b0 b\u1eb1ng c\u00e1ch th\u00eam c\u00e1c lo\u1ea1i nhi\u1ec5u (\u00e1p d\u1ee5ng albumentation)","bde38740":"### Quan s\u00e1t: \n\nTrong s\u01a1 \u0111\u1ed3 tr\u00ean, ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y m\u1ed1i quan h\u1ec7 gi\u1eefa 6 lo\u1ea1i. \u0110\u00fang nh\u01b0 d\u1ef1 \u0111o\u00e1n, kh\u00f4ng th\u1ec3 n\u00e0o m\u1ed9t chi\u1ebfc l\u00e1 kh\u1ecfe m\u1ea1nh l\u1ea1i c\u00f3 th\u1ec3 b\u1ecb v\u1ea3y, g\u1ec9 s\u1eaft, hay nhi\u1ec1u b\u1ec7nh kh\u00e1c. Ngo\u00e0i ra, m\u1ed7i chi\u1ebfc l\u00e1 kh\u00f4ng kh\u1ecfe m\u1ea1nh \u0111\u1ec1u c\u00f3 m\u1ed9t trong c\u00e1c b\u1ec7nh v\u1ea3y, g\u1ec9 s\u1eaft ho\u1eb7c nhi\u1ec1u b\u1ec7nh. T\u1ea7n su\u1ea5t c\u1ee7a m\u1ed7i k\u1ebft h\u1ee3p c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c nh\u00ecn th\u1ea5y b\u1eb1ng c\u00e1ch di chu\u1ed9t qua plot.","939168c7":"## T\u00ecm learing rate ph\u00f9 h\u1ee3p cho m\u00f4 h\u00ecnh\n","717f6d48":"# 4. \u0110\u1ecbnh ngh\u0129a m\u00f4 h\u00ecnh","7e590c73":"D\u1ef1a v\u00e0o bi\u1ec3u \u0111\u1ed3, ch\u00fang ta c\u00f3 th\u1ec3 ch\u1ecdn learning rate c\u1ee7a Adam = 10^-3. ","45e157f3":"## 3.3 T\u1ea1o class PlantDataset\n\u0110\u00e2y l\u00e0 m\u1ed9t class \u0111\u1eb7c bi\u1ec7t \u0111\u01b0\u1ee3c k\u1ebf th\u1eeba t\u1eeb l\u1edbp Dataset cur Pytorch \u0111\u1ec3 load \u1ea3nh v\u00e0 th\u1ef1c hi\u1ec7n transform \u1ea3nh.","54f98279":"### 6. L\u01b0u m\u00f4 h\u00ecnh","da7d2343":"## 3.5 Create Dataloader","bff8c560":"Distinct List of labels \n\n\n\n*     healthy\n*     complex\n*     rust\n*     frog_eye_leaf_spot\n*     powdery_mildew\n*     scab","6d2167d3":"# 1.  Thi\u1ebft l\u1eadp gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c tham s\u1ed1 c\u1ed1 \u0111\u1ecbnh\nThi\u1ebft l\u1eadp gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c tham s\u1ed1 c\u1ed1 \u0111\u1ecbnh c\u00f3 trong b\u00e0i:\n1. *num_classes*: t\u1ed5ng s\u1ed1 l\u01b0\u1ee3ng nh\u00e3n.\n2. *img_size*: k\u00edch th\u01b0\u1edbc c\u1ee7a \u1ea3nh sau qu\u00e1 tr\u00ecnh resized b\u1edfi DataLoader.\n3. *batch_size*: k\u00edch th\u01b0\u1edbc m\u1ed7i batch.\n4. *device*: accelerator \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng.\n5. *criterion*: h\u00e0m m\u1ea5t m\u00e1t (loss function) \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng.","57adc5bd":"Ch\u00fang t\u00f4i \u0111\u00e3 plot m\u1ed9t v\u00e0i h\u00ecnh \u1ea3nh trong training data \u1edf tr\u00ean (c\u00e1c gi\u00e1 tr\u1ecb RGB c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c nh\u00ecn th\u1ea5y b\u1eb1ng c\u00e1ch di chu\u1ed9t qua h\u00ecnh \u1ea3nh). C\u00e1c ph\u1ea7n m\u00e0u xanh l\u00e1 c\u00e2y c\u1ee7a h\u00ecnh \u1ea3nh c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam r\u1ea5t th\u1ea5p, nh\u01b0ng ng\u01b0\u1ee3c l\u1ea1i, c\u00e1c ph\u1ea7n m\u00e0u n\u00e2u c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam cao. \u0110i\u1ec1u n\u00e0y cho th\u1ea5y r\u1eb1ng c\u00e1c ph\u1ea7n m\u00e0u xanh l\u00e1 c\u00e2y (healthy) c\u1ee7a h\u00ecnh \u1ea3nh c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam th\u1ea5p, trong khi c\u00e1c ph\u1ea7n unhealthy c\u00f3 nhi\u1ec1u kh\u1ea3 n\u0103ng c\u00f3 gi\u00e1 tr\u1ecb m\u00e0u xanh lam cao. **\u0110i\u1ec1u n\u00e0y c\u00f3 th\u1ec3 cho th\u1ea5y r\u1eb1ng k\u00eanh m\u00e0u xanh lam c\u00f3 th\u1ec3 l\u00e0 ch\u00eca kh\u00f3a \u0111\u1ec3 ph\u00e1t hi\u1ec7n b\u1ec7nh tr\u00ean c\u00e2y tr\u1ed3ng**","5be522e9":"### Quan s\u00e1t:\nGi\u00e1 tr\u1ecb k\u00eanh m\u00e0u xanh l\u00e1 c\u00e2y c\u00f3 ph\u00e2n ph\u1ed1i \u0111\u1ed3ng \u0111\u1ec1u h\u01a1n gi\u00e1 tr\u1ecb k\u00eanh m\u00e0u \u0111\u1ecf nh\u01b0ng l\u1ec7ch ph\u1ea3i, v\u1edbi \u0111\u1ec9nh nh\u1ecf h\u01a1n. S\u1ef1 ph\u00e2n b\u1ed1 c\u0169ng c\u00f3 \u0111\u1ed9 l\u1ec7ch b\u00ean ph\u1ea3i (tr\u00e1i ng\u01b0\u1ee3c v\u1edbi m\u00e0u \u0111\u1ecf) v\u00e0 ch\u1ebf \u0111\u1ed9 l\u1edbn h\u01a1n kho\u1ea3ng 160. \u0110i\u1ec1u n\u00e0y cho th\u1ea5y r\u1eb1ng m\u00e0u xanh l\u00e1 c\u00e2y r\u00f5 n\u00e9t h\u01a1n trong nh\u1eefng h\u00ecnh \u1ea3nh n\u00e0y so v\u1edbi m\u00e0u \u0111\u1ecf, \u0111i\u1ec1u n\u00e0y c\u00f3 \u00fd ngh\u0129a, b\u1edfi v\u00ec \u0111\u00e2y l\u00e0 h\u00ecnh \u1ea3nh c\u1ee7a nh\u1eefng chi\u1ebfc l\u00e1!","d1ba8904":"## 3.1 T\u1ea1o \u0111\u01b0\u1eddng d\u1eabn cho \u1ea3nh v\u00e0 ph\u00e2n chia t\u1eadp hu\u1ea5n luy\u1ec7n, t\u1eadp th\u1ea9m \u0111\u1ecbnh","86f87979":"# M\u1ed9t s\u1ed1 \u1ea3nh v\u00ed d\u1ee5 t\u1eeb t\u1eadp d\u1eef li\u1ec7u\nCh\u00fang t\u00f4i s\u1ebd ki\u1ec3m tra k\u00edch th\u01b0\u1edbc c\u1ee7a 300 h\u00ecnh \u1ea3nh \u0111\u1ea7u ti\u00ean\n\nNh\u01b0 c\u00f3 th\u1ec3 th\u1ea5y b\u00ean d\u01b0\u1edbi th\u00ec t\u1ea5t c\u1ea3 c\u00e1c h\u00ecnh \u1ea3nh c\u00f3 k\u00edch th\u01b0\u1edbc kh\u00e1c nhau.","e10c86c5":"## Ph\u00e2n ph\u1ed1i K\u00eanh Xanh Lam","23788a6c":"### 5.2 Hu\u1ea5n luy\u1ec7n v\u00e0 gi\u00e1m s\u00e1t m\u00f4 h\u00ecnh","e95bde7e":"## M\u00e3 h\u00f3a nh\u00e3n\nM\u00e3 h\u00f3a c\u00e1c nh\u00e3n c\u00f3 trong t\u1eadp d\u1eef li\u1ec7u v\u1ec1 d\u1ea1ng *integer* \u0111\u1ec3 m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c.","4b87a06a":"### Quan s\u00e1t :\nC\u00e1c gi\u00e1 tr\u1ecb k\u00eanh m\u00e0u \u0111\u1ecf c\u00f3 v\u1ebb g\u1ea7n nh\u01b0 ph\u00e2n ph\u1ed1i chu\u1ea9n, nh\u01b0ng h\u01a1i l\u1ec7ch v\u1ec1 b\u00ean tr\u00e1i (\u0110\u1ed9 l\u1ec7ch \u00e2m). \u0110i\u1ec1u n\u00e0y cho th\u1ea5y r\u1eb1ng k\u00eanh m\u00e0u \u0111\u1ecf c\u00f3 xu h\u01b0\u1edbng t\u1eadp trung nhi\u1ec1u h\u01a1n \u1edf c\u00e1c gi\u00e1 tr\u1ecb cao h\u01a1n, v\u00e0o kho\u1ea3ng 100. C\u00f3 s\u1ef1 thay \u0111\u1ed5i l\u1edbn v\u1ec1 gi\u00e1 tr\u1ecb m\u00e0u \u0111\u1ecf trung b\u00ecnh tr\u00ean c\u00e1c h\u00ecnh \u1ea3nh.","534e4de7":"### K\u1ebft lu\u1eadn\nPh\u00e2n t\u00edch k\u00eanh m\u00e0u th\u00ec th\u1ea5y k\u00eanh m\u00e0u xanh ch\u1ee7 y\u1ebfu c\u00f3 ph\u00e2n b\u1ed1 \u1edf v\u00f9ng c\u00f3 intensity m\u1ea1nh ( v\u00ec n\u00f3 t\u1eadp trung \u1edf ph\u1ea7n cu\u1ed1i \u0111\u1ed3 th\u1ecb) ==> D\u1ec5 hi\u1ec3u v\u00ec \u1ea3nh to\u00e0n l\u00e1. V\u00ed d\u1ee5 nh\u01b0 \u1ea3nh to\u00e0n l\u00e1 m\u00e0 th\u1ea5y k\u00eanh m\u00e0u \u0111\u1ecf l\u1ea1i m\u1ea1nh h\u01a1n th\u00ec s\u1ebd ph\u00e1t hi\u1ec7n b\u1ea5t th\u01b0\u1eddng. \u1ede \u0111\u00e2y k\u00eanh m\u00e0u \u0111\u1ecf l\u1ea1i ph\u00e2n b\u1ed1 \u1edf v\u00f9ng gi\u1eefa m\u1ea1nh h\u01a1n k\u00eanh m\u00e0u xanh d\u01b0\u01a1ng, ch\u1ee9ng t\u1ecf m\u00e0u \u0111\u1ecf n\u00f3 c\u0169ng c\u00f3 xu\u1ea5t hi\u1ec7n nhi\u1ec1u l\u00e0 do m\u1ed9t s\u1ed1 l\u00e1 c\u00e2y b\u1ecb s\u00e2u b\u00eanh hay c\u00f3 m\u00e0u \u0111\u1ecf","9c5c1aee":"### 5.1 C\u00e1c bi\u1ec3u \u0111\u1ed3 c\u1ee7a m\u00f4 h\u00ecnh\n> 1. Loss\n> 2. Accuracy\n> 3. F1","8d724359":"C\u00f3 t\u1ed5ng c\u1ed9ng 12 nh\u00e3n kh\u00e1c nhau.","7e20a522":"### \u0110\u1ebfm s\u1ed1 l\u01b0\u1ee3ng nh\u00e3n v\u00e0 s\u1ed1 l\u01b0\u1ee3ng \u1ea3nh c\u1ee7a c\u00e1c nh\u00e3n"}}