{"cell_type":{"9c11f478":"code","9126222d":"code","0ebed4b7":"code","b73394b2":"code","4ebf824f":"code","3b0ed724":"code","232ae0c1":"code","47f9efea":"markdown","0cac0a62":"markdown","b67334cc":"markdown","38905d67":"markdown","2baff6e7":"markdown","edb29515":"markdown","30003b87":"markdown","6061bb87":"markdown","92b00d8a":"markdown"},"source":{"9c11f478":"# Import our libraries\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport os\nimport IPython.display as ipd  # To play sound in the notebook","9126222d":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","0ebed4b7":"# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","b73394b2":"# Source - RAVDESS; Gender - Female; Emotion - Happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_12\/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","4ebf824f":"# Source - RAVDESS; Gender - Male; Emotion - Happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_11\/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","3b0ed724":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","232ae0c1":"# Source - RAVDESS; Gender - Female; Emotion - happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_12\/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - happy \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_11\/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","47f9efea":"<a id=\"stats\"><\/a>\n## 3. Statistical features \nNow we've seen the shape of an MFCC output for each file, and it's a 2D matrix format with MFCC bands on the y-axis and time on the x-axis, representing the MFCC bands over time. To simplify things, what we're going to do is take the mean across each band over time. In other words, row means. But how does it present as a distinctive feature? \n\nSo if you look at the above MFCC plot, the first band at the bottom is the most distinctive band over the other bands. Since the time window is a short one, the changes observed overtime does not vary greatly. The key feature is capturing the information contained in the various bands. Lets plot the mean of each of the band and display it as a time series plot to illustrate the point. \n\nWe'll compare the Angry female and Angry male for the same sentence uttered. ","0cac0a62":"Since I've already outlined the various types of features [here](https:\/\/www.kaggle.com\/ejlok1\/part-2-extracting-audio-features\/notebook#Part-2---Extracting-Audio-Features), I'll just simplify things here and just use MFCC, because its the best feature for this particular problem and we're trying to get to a quick working baseline. Later on, during the accuracy improvement phase, we may expand our feature set to include Mel-Spectogram, Chroma, HPSS and etc... and not just a simple mean \n\n1. [MFCC quick intro](#mfcc)\n2. [Deepdive](#deep)\n3. [Statistical features](#stats)\n4. [Final thoughts](#final)\n\nUpvote this notebook if you like, and be sure to check out the other parts which are now available:\n* [Part 3 | Baseline model](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-3-baseline-model)\n* [Part 4 | Apply to new audio data](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-4-apply-to-new-audio-data)\n* [Part 5 | Data augmentation](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-5-data-augmentation)\n\nMost importantly, I want to thank the 4 authors for their excellent dataset, without it, writing this notebook could not have been possible. The original source of the dataset links are below:\n\n- [TESS](https:\/\/tspace.library.utoronto.ca\/handle\/1807\/24487)\n- [CREMA-D](https:\/\/github.com\/CheyneyComputerScience\/CREMA-D)\n- [SAVEE](http:\/\/kahlan.eps.surrey.ac.uk\/savee\/Database.html)\n- [RAVDESS](https:\/\/zenodo.org\/record\/1188976#.XYP8CSgzaUk)\n- [RAVDESS_Kaggle](https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio)","b67334cc":"# <center>Audio Emotion Recognition<\/center>\n## <center>Part 2 - Feature Extraction <\/center>\n#### <center> 21st August 2019 <\/center> \n#####  <center> Eu Jin Lok <\/center> \n\n## Introduction \nFollowing on from [Part 1](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-recognition-part-1-explore-data), we are now going to check out the various techniques for extracting useful features from audio for our classifier. I've done this before in another different kernel over [here](https:\/\/www.kaggle.com\/ejlok1\/part-2-extracting-audio-features\/notebook#Part-2---Extracting-Audio-Features). I haven't covered the entireity of the various audio features, just ones which I'm familiar with. Any suggestions or advice please drop me a note. For a more complete coverage of the various features, I suggest checking the [pyaudio journal](https:\/\/journals.plos.org\/plosone\/article\/file?id=10.1371\/journal.pone.0144610&type=printable)\n\nBroadly speaking there are two category of features:\n- Time domain features<br\/> \nThese are simpler to extract and understand, like the energy of signal, zero crossing rate, maximum amplitude, minimum energy, etc.\n- Frequency based features<br\/>\nare obtained by converting the time based signal into the frequency domain. Whilst they are harder to comprehend, it provides extra information that can be really handy such as pitch, rhythms, melody etc. Check this infographic below:","38905d67":"<a id=\"final\"><\/a>\n## 4. Final thoughts \nUsing MFCC is a good feature to differentiate the gender and emotions as demonstrated above. Even thou we've ommited alot of good information by just taking the mean, it seems we still capture enough to be able to see some difference. Whether this difference is significant for distinguishing the variou emotions, we'll find out in the next part where we will create a baseline emotion classifier\n\nUpvote this notebook if you like, and be sure to check out the other parts which are now available:\n* [Part 3 | Baseline model](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-3-baseline-model)\n* [Part 4 | Apply to new audio data](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-4-apply-to-new-audio-data)","2baff6e7":"![Audio_wave](https:\/\/www.nti-audio.com\/portals\/0\/pic\/news\/FFT-Time-Frequency-View-540.png)\nThe time vs frequency domain image sourced from __[here](https:\/\/www.nti-audio.com\/en\/support\/know-how\/fast-fourier-transform-fft)__ \n\n","edb29515":"<a id=\"deep\"><\/a>\n## 2. Deepdive\nWe can select a few examples and visualise the MFCC. lets take 2 different emotions and 2 different genders, and play it just to get a feel for what we are dealing with. Ie. whether the data (audio) quality is good. It gives us an early insight as to how likely our classifier is going to be successful.   ","30003b87":"Very placid response from the male counter part...","6061bb87":"So for the same sentence being uttered, there is a clear distint difference between male and female in that females tends to have a higher pitch. Lets look at a few others. Lets compare a Happy Female and a Happy Male","92b00d8a":"<a id=\"mfcc\"><\/a>\n## 1. MFCC quick intro \nMFCC is well known to be a good feature. And there's many ways you can slice and dice this one feature. But what is MFCC? It stands for Mel-frequency cepstral coefficient, and it is a good \"representation\" of the vocal tract that produces the sound. Think of it like an x-ray of your mouth\n\nThis post has a good deep dive into the [MFCC](https:\/\/medium.com\/prathena\/the-dummys-guide-to-mfcc-aceab2450fd) should you wish to. The most common machine learning application treats the MFCC itself as an 'image' and becomes a feature. The benefit of treating it as an image is that it provides more information, and gives one the ability to draw on transfer learning. This is certainly legit and yields good accuracy. However, research has also shown that statistics relating to MFCCs (or any other time or frequency domain) can carry good amount of information as well. We'll be investigating both of this methods"}}