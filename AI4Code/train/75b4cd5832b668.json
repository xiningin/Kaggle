{"cell_type":{"97f036c6":"code","677f2ca2":"code","197bac52":"code","5fece86d":"code","1ab50ced":"code","a98360fc":"code","294efc9e":"code","ce674fe1":"code","6d383395":"code","2e3f7f7c":"code","9f8d3a7c":"code","6df7c071":"code","20d7f44d":"code","f9a180ab":"code","daa4b63e":"code","2643e6e7":"code","bc6bde8b":"code","e9d03f2d":"code","42619c44":"code","e4d055dd":"code","ab2e2596":"code","c82bcba2":"code","33793381":"code","2229077b":"code","1de489c1":"code","d24004ae":"markdown","d10604ba":"markdown","8ad834a3":"markdown","ed415850":"markdown","7eb377f6":"markdown","b65f65b1":"markdown","8b8015d7":"markdown","aedf7b76":"markdown","59dee1e9":"markdown","73dec538":"markdown","adfd01db":"markdown","d01c8c15":"markdown","bdb4a314":"markdown","dad07655":"markdown"},"source":{"97f036c6":"import datetime as dt\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nsns.set_style('whitegrid')\n\n\nimport os\nfrom keras.applications import xception\nfrom keras.preprocessing import image\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nimport cv2\nfrom scipy.stats import uniform\n\nfrom tqdm import tqdm\nfrom glob import glob\n\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Masking\nfrom keras.utils import np_utils, to_categorical\n\n\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","677f2ca2":"## import Keras and its module for image processing and model building\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization","197bac52":"#copying the pretrained models to the cache directory\ncache_dir = os.path.expanduser(os.path.join('~', '.keras'))\nif not os.path.exists(cache_dir):\n    os.makedirs(cache_dir)\nmodels_dir = os.path.join(cache_dir, 'models')\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n\n#copy the Xception models\n!cp ..\/input\/keras-pretrained-models\/xception* ~\/.keras\/models\/\n#show\n!ls ~\/.keras\/models","5fece86d":"img = load_img('..\/input\/face-mask-detection\/dataset\/with_mask\/image_0.png')  # this is a PIL image\nx = img_to_array(img)  # this is a Numpy array \nprint('image shape: ', x.shape)\n\nprint('Wear Mask')\nplt.imshow(img)\nplt.show()\n\n\nimg = load_img('..\/input\/face-mask-detection\/dataset\/without_mask\/image_121.png')  # this is a PIL image\nx = img_to_array(img)  # this is a Numpy array \nprint('Without Mask')\nplt.imshow(img)\nplt.show()\n\n","1ab50ced":"\ndir_kaggle ='..\/input\/face-mask-detection'\ndata_kaggle ='..\/input\/face-mask-detection\/dataset'\nwith_mask ='....\/input\/face-mask-detection\/dataset\/with_mask'\nwithout_mask='..\/input\/face-mask-detection\/dataset\/without_mask'\n\n\nclass_data= ['with_mask','without_mask']\nlen_class_data = len(class_data)","a98360fc":"image_count = {}\ntrain_data = []\n\nfor i , class_data in tqdm(enumerate(class_data)):\n    class_folder = os.path.join(data_kaggle,class_data)\n    label = class_data\n    image_count[class_data] = []\n    \n    for path in os.listdir(os.path.join(class_folder)):\n        image_count[class_data].append(class_data)\n        train_data.append(['{}\/{}'.format(class_data, path), i, class_data])","294efc9e":"#show image count\nfor key, value in image_count.items():\n    print('{0} -> {1}'.format(key, len(value)))","ce674fe1":"#create a dataframe\ndf = pd.DataFrame(train_data, columns=['file', 'id', 'label'])\ndf.shape\ndf.head()","6d383395":"cnt_pro = df['label'].value_counts()\nplt.figure(figsize=(6,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('label', fontsize=12)\nplt.xticks(rotation=80)\nplt.show();","2e3f7f7c":"#masking function\ndef create_mask_for_plant(image):\n    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    lower_hsv = np.array([0,0,250])\n    upper_hsv = np.array([250,255,255])\n    \n    mask = cv2.inRange(image_hsv, lower_hsv, upper_hsv)\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n    \n    return mask\n\n#image remove_noise function\ndef remove_noise(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255 \n\n#image segmentation function\ndef segment_image(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n#image dilate function\ndef dilate_image(image):\n    mask = create_mask_for_plant(image)\n    output = cv2.bitwise_and(image, image, mask = mask)\n    return output\/255\n\n#sharpen the image\ndef sharpen_image(image):\n    image_blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    image_sharp = cv2.addWeighted(image, 1.5, image_blurred, -0.5, 0)\n    return image_sharp\n\n# function to get an image\ndef read_img(filepath, size):\n    img = image.load_img(os.path.join(data_kaggle, filepath), target_size=size)\n    #convert image to array\n    img = image.img_to_array(img)\n    return img","9f8d3a7c":"nb_rows = 4\nnb_cols = 5\nfig, axs = plt.subplots(nb_rows, nb_cols, figsize=(10, 5));\nplt.suptitle('SAMPLE IMAGES');\nfor i in range(0, nb_rows):\n    for j in range(0, nb_cols):\n        axs[i, j].xaxis.set_ticklabels([]);\n        axs[i, j].yaxis.set_ticklabels([]);\n        axs[i, j].imshow((read_img(df['file'][np.random.randint(440)], (255,255)))\/255.);\nplt.show();","6df7c071":"#get an image\nimg = read_img(df['file'][60],(255,255))\n\n#remove_noise\nremove_noise = remove_noise(img)\n#mask\nimage_mask = create_mask_for_plant(img)\n#dilate\nimage_dilate = dilate_image(img)\n#segmentation\nimage_segmented = segment_image(img)\n#sharpen the image\nimage_sharpen = sharpen_image(image_segmented)\n\nfig, ax = plt.subplots(1, 6, figsize=(10, 6));\nplt.suptitle('SAMPLE PROCESSED IMAGE', x=0.5, y=0.8)\nplt.tight_layout(1)\n\nax[0].set_title('ORIGINAL', fontsize=12)\nax[1].set_title('REMOVE NOISE', fontsize=12)\nax[2].set_title('MASK', fontsize=12)\nax[3].set_title('DILATE', fontsize=12)\nax[4].set_title('SEGMENTED', fontsize=12)\nax[5].set_title('SHARPEN', fontsize=12)\n\n\nax[0].imshow(img\/255);\nax[1].imshow(remove_noise);\nax[2].imshow(image_mask);\nax[3].imshow(image_dilate);\nax[4].imshow(image_segmented);\nax[5].imshow(image_sharpen);\n\n","20d7f44d":"INPUT_SIZE=255\n\n##preprocess the input\nX_train = np.zeros((len(df), INPUT_SIZE, INPUT_SIZE, df.shape[1]), dtype='float')\nfor i, file in tqdm(enumerate(df['file'])):\n    #read image\n    img = read_img(file,(INPUT_SIZE,INPUT_SIZE))\n    #masking and segmentation\n    image_segmented = segment_image(img)\n    #sharpen\n    image_sharpen = sharpen_image(image_segmented)\n    x = xception.preprocess_input(np.expand_dims(image_sharpen.copy(), axis=0))\n    X_train[i] = x","f9a180ab":"print('Train Image Shape: ', X_train.shape)\nprint('Train Image Size: ', X_train.size)","daa4b63e":"y = df['id']\ntrain_x, train_val, y_train, y_val = train_test_split(X_train, y, test_size=0.10, random_state=9)","2643e6e7":"print('WEAR MASK IMAGES ON TRAINING DATA: ',y_train[y_train==0].shape[0])\nprint('WITHOUT MASK IMAGES ON TRAINING DATA: ',y_train[y_train==1].shape[0])","bc6bde8b":"##get the features\nxception_bf = xception.Xception(weights='imagenet', include_top=False, pooling='avg')\nbf_train_x = xception_bf.predict(train_x, batch_size=32, verbose=2)\nbf_train_val = xception_bf.predict(train_val, batch_size=32, verbose=2)","e9d03f2d":"#print shape of feature and size\nprint('Train Shape: ', bf_train_x.shape)\nprint('Train Size: ', bf_train_x.size)\n\nprint('Validation Shape: ', bf_train_val.shape)\nprint('Validation Size: ', bf_train_val.size)","42619c44":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, Input\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils import np_utils\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n#keras Sequential model\nmodel = Sequential()\nmodel.add(Dense(units = 64 , activation = 'relu', input_dim=bf_train_x.shape[1]))\n\nmodel.add(Dense(units = 2, activation = 'softmax'))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.15))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()\n","e4d055dd":"#train the model \nhistory = model.fit(bf_train_x, y_train, epochs=50, batch_size=32);","ab2e2596":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","c82bcba2":"plt.plot(history.history['accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_accuracy.png')\n# summarize history for loss\nplt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_loss.png')","33793381":"#predict the validation data\npredictions = model.predict_classes(bf_train_val)","2229077b":"confusion_mat = confusion_matrix(y_val, predictions)\n\nplt.figure(figsize=(4,4))\nsns.heatmap(confusion_mat, square=True, annot=True,\n            yticklabels=['wear mask', 'without mask'],\n            xticklabels=['wear mask', 'without mask']);\nplt.title('CONFUSION MATRIX');\nplt.xlabel('Y_TRUE');\nplt.ylabel(\"PREDICTIONS\");","1de489c1":"print(classification_report(y_val, predictions))","d24004ae":"# SPLIT THE DATA","d10604ba":"# CONFUSION MATRIX\nConfusion Matrix is commonly used for a summarization of prediction results on a classification problem.The number of correct and incorrect predictions is summarized with counting values and each value broken down for each class. Each of them is the key to the confusion matrix. It shows the classification model is confused when it makes predictions, at this point in here it gives us insight not only into the errors being made by a classifier but also show the types of errors that are being made [3].","8ad834a3":"The plot_model() function in Keras will create a plot of your network[[2](https:\/\/machinelearningmastery.com\/visualize-deep-learning-neural-network-model-keras\/)] . \nThis function takes a few useful arguments:\n\n* model: (required) The model that you wish to plot.\n* to_file: (required) The name of the file to which to save the plot.\n* show_shapes: (optional, defaults to False) Whether or not to show the output shapes of each layer.\n* show_layer_names: (optional, defaults to True) Whether or not to show the name for each layer.","ed415850":"Data for training and testing\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 10%, assumed it ideal ratio between training and testing","7eb377f6":"#### CLASSIFICATION REPORT","b65f65b1":"### SHOW SAMPLE IMAGES","8b8015d7":"###  FEATURES EXTRACTION","aedf7b76":"# Visualize Model\n","59dee1e9":"LOSS is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. While Accuracy refers to how close a measurement is to the true value of what is being measured.","73dec538":"### SHOW SAMPLE PROCESSED IMAGE\n","adfd01db":"#### LOSS AND ACCURACY","d01c8c15":"### IMAGE PREPROCESSING","bdb4a314":"# Measurement\nWhen the classification process was already done. This work evaluated the results using the Confusion Matrix.","dad07655":"### DEEP LEARNING MODEL"}}