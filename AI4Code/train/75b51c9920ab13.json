{"cell_type":{"8b3e8aa5":"code","c7f392f5":"code","9e01f9ee":"code","654f929a":"code","eb436bc1":"code","7e729eb4":"code","35f90b4a":"markdown","ea49d23c":"markdown","20df8fcf":"markdown","6626bbbc":"markdown","f2733ff2":"markdown","047bc9d0":"markdown","bc5a8319":"markdown","e3a9e61a":"markdown","e817cd5c":"markdown","d41a1a1e":"markdown","360c7830":"markdown"},"source":{"8b3e8aa5":"# Install:\n# Kaggle environments.\n!git clone -q https:\/\/github.com\/Kaggle\/kaggle-environments.git\n!cd kaggle-environments && pip install .\n\n# GFootball environment.\n!apt-get update -y -qq > \/dev\/null\n!apt-get install -y -qq libsdl2-gfx-dev libsdl2-ttf-dev\n\n# Make sure that the Branch in git clone and in wget call matches !!\n!git clone -q -b v2.8 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n\n!wget -q https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.8.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip install .","c7f392f5":"!pip install rl-replicas","9e01f9ee":"import os\n\nimport gfootball\nimport gym\nimport torch\nimport torch.nn as nn\n\nfrom rl_replicas.algorithms import TRPO\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.value_function import ValueFunction\nfrom rl_replicas.common.optimizers import ConjugateGradientOptimizer\nfrom rl_replicas.common.torch_net import mlp\n\nalgorithm_name = 'trpo'\nenvironment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'\nepochs = 5\nsteps_per_epoch = 4000\npolicy_network_architecture = [64, 64]\nvalue_function_network_architecture = [64, 64]\nvalue_function_learning_rate = 1e-3\noutput_dir = '.\/trpo'\n\nenv: gym.Env = gym.make(environment_name)\n\npolicy_network = mlp(\n    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]\n)\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())\n)\n\nvalue_function_network = mlp(\n    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]\n)\nvalue_function: ValueFunction = ValueFunction(\n    network = value_function_network,\n    optimizer = torch.optim.Adam(value_function_network.parameters(), lr=value_function_learning_rate)\n)\n\nmodel: TRPO = TRPO(policy, value_function, env, seed=0)\n\nprint('an experiment to: {}'.format(output_dir))\n\nprint('algorithm:           {}'.format(algorithm_name))\nprint('epochs:              {}'.format(epochs))\nprint('steps_per_epoch:     {}'.format(steps_per_epoch))\nprint('environment:         {}'.format(environment_name))\n\nprint('value_function_learning_rate: {}'.format(value_function_learning_rate))\nprint('policy network:')\nprint(policy.network)","654f929a":"model.learn(\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    output_dir=output_dir,\n    model_saving=True\n)","eb436bc1":"import os\n\nimport gfootball\nimport gym\nimport torch\nimport torch.nn as nn\n\nfrom rl_replicas.algorithms import TRPO\nfrom rl_replicas.common.policy import Policy\nfrom rl_replicas.common.value_function import ValueFunction\nfrom rl_replicas.common.optimizers import ConjugateGradientOptimizer\nfrom rl_replicas.common.torch_net import mlp\n\nalgorithm_name = 'trpo'\nenvironment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'\nepochs = 5\nsteps_per_epoch = 4000\npolicy_network_architecture = [64, 64]\nvalue_function_network_architecture = [64, 64]\noutput_dir = '.\/trpo'\n\nprevious_model_location = os.path.join(output_dir, 'model.pt')\n\nenv: gym.Env = gym.make(environment_name)\n\npolicy_network = mlp(\n    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]\n)\n\npolicy: Policy = Policy(\n    network = policy_network,\n    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())\n)\n\nvalue_function_network = mlp(\n    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]\n)\nvalue_function: ValueFunction = ValueFunction(\n    network = value_function_network,\n    optimizer = torch.optim.Adam(value_function_network.parameters())\n)\n    \nif os.path.isfile(previous_model_location):\n    print('Load previous model: {}'.format(previous_model_location))\n    previous_model = torch.load(previous_model_location)\n\n    policy.network.load_state_dict(previous_model['policy_state_dict'])\n    policy.optimizer.load_state_dict(previous_model['policy_optimizer_state_dict'])\n    value_function.network.load_state_dict(previous_model['value_fn_state_dict'])\n    value_function.optimizer.load_state_dict(previous_model['value_fn_optimizer_state_dict'])\n\nmodel: TRPO = TRPO(policy, value_function, env, seed=0)\n\nprint('an experiment to: {}'.format(output_dir))\n\nprint('algorithm:           {}'.format(algorithm_name))\nprint('epochs:              {}'.format(epochs))\nprint('steps_per_epoch:     {}'.format(steps_per_epoch))\nprint('environment:         {}'.format(environment_name))\n\nprint('policy network:')\nprint(policy.network)","7e729eb4":"print('Re-start your training from {} epochs'.format(previous_model['epoch']))\nmodel.learn(\n    epochs=epochs,\n    steps_per_epoch=steps_per_epoch,\n    output_dir=output_dir,\n    model_saving=True\n)","35f90b4a":"install gfootball required tools","ea49d23c":"##### Note\nit's the same code with [my first post](https:\/\/www.kaggle.com\/yamatokataoka\/train-agent-with-rl-replicas#Train-with-rl-replicas)","20df8fcf":"You can check [my first post about training TRPO agent with the Reinforcement Learning Replications (rl-replicas)](https:\/\/www.kaggle.com\/yamatokataoka\/train-agent-with-rl-replicas).\n\nThe rl-replicas is an on-going project for implementaing key RL algorithms, VPG, TRPO, PPO. You can find more detail here, [https:\/\/github.com\/yamatokataoka\/reinforcement-learning-replications](https:\/\/github.com\/yamatokataoka\/reinforcement-learning-replications)\n\nIn this notebook, you'll learn how to continue your training with pre-trained model with rl-replicas.","6626bbbc":"##### Note\nimplementation for saving `model.pt`is \n\nhttps:\/\/github.com\/yamatokataoka\/reinforcement-learning-replications\/blob\/master\/rl_replicas\/common\/base_algorithms\/on_policy_algorithm.py#L106-L114","f2733ff2":"### Train with rl-replicas","047bc9d0":"Continue your training","bc5a8319":"You'll initialize all neccessary components for training again.\n\nThen load pre-trained model, `.\/trpo\/model.pt`.\n\nTo continue your work, you need to load state_dict for policy and value function.\n```\nprevious_model = torch.load(previous_model_location)\n\npolicy.network.load_state_dict(previous_model['policy_state_dict'])\npolicy.optimizer.load_state_dict(previous_model['policy_optimizer_state_dict'])\nvalue_function.network.load_state_dict(previous_model['value_fn_state_dict'])\nvalue_function.optimizer.load_state_dict(previous_model['value_fn_optimizer_state_dict'])\n```","e3a9e61a":"install rl-replicas","e817cd5c":"you'll set up the TRPO algorithm with `ConjugateGradientOptimizer`.\n\nThe trained model is saved in `.\/trpo\/model.pt`.\n\nYou'll use this to continue your training later.","d41a1a1e":"### Continue your training with rl-replicas\n\n","360c7830":"Start learning"}}