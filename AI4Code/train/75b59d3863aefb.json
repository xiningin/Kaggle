{"cell_type":{"40a89b37":"code","5618764a":"code","dd792603":"code","790cce10":"code","6b494a6d":"code","035976cb":"code","d94d9eb7":"code","e63b7e0a":"code","9fdff6a6":"code","d5d6aa52":"code","785f43b1":"code","31154d93":"code","5d0a1fb1":"code","e4169173":"code","b5fea33a":"code","b9e85e2f":"code","3d3a409d":"code","cf04646f":"code","fb6d0617":"code","aca0e52c":"code","ad2c98f0":"code","2b92f06b":"code","e9ae136d":"code","c4feabb2":"code","e5f86b52":"code","be7c1ee9":"code","21c0e165":"code","8cc98fa2":"code","2b410ab0":"code","487e1458":"code","31ae1bbc":"code","399dfb9e":"code","1a2606c7":"code","509b7b11":"code","7fcbbef7":"code","8ed26c3d":"code","8d6b021b":"code","32c74d23":"code","4f99967b":"code","1667af63":"code","52f298b0":"code","427ebbd0":"code","af227973":"code","0968d620":"code","d2b7da72":"code","693b67ff":"code","b5a2475d":"code","d7dc8303":"markdown","92988bba":"markdown","5563dac0":"markdown","4a127efa":"markdown","7a97aba1":"markdown","116657e6":"markdown","4555737b":"markdown","11a3de33":"markdown","cfd2b8f7":"markdown","1c4ff90c":"markdown","df6f8922":"markdown","12c3c3d2":"markdown","85194aaf":"markdown","9c5a0a78":"markdown","bb320f33":"markdown","04582d6b":"markdown","6483a563":"markdown","7f806211":"markdown","0e970cab":"markdown","d3db73fd":"markdown","acdd9c49":"markdown","b2d1a5cd":"markdown","fd9af7bc":"markdown","1fbc52a9":"markdown","0611d0d1":"markdown","3d29c8f6":"markdown","be0990f5":"markdown","4cc8158a":"markdown","8a2d6235":"markdown","b1e0eb2a":"markdown","3bbe7218":"markdown","42d1b187":"markdown","de87808a":"markdown"},"source":{"40a89b37":"import warnings\nimport random\nimport os\nimport gc\nimport shap","5618764a":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt \nimport seaborn           as sns\nimport joblib            as jb\nimport matplotlib.pyplot as plt","dd792603":"from sklearn.model_selection   import train_test_split\nfrom sklearn.preprocessing     import QuantileTransformer\nfrom sklearn.cluster           import KMeans\nfrom yellowbrick.cluster       import KElbowVisualizer, SilhouetteVisualizer\nfrom scipy.stats.mstats        import winsorize\nfrom sklearn.decomposition     import PCA\nfrom sklearn.impute            import SimpleImputer\nfrom sklearn.metrics           import silhouette_samples, silhouette_score\nfrom sklearn.model_selection   import train_test_split, KFold, RepeatedStratifiedKFold, StratifiedKFold  \nfrom sklearn                   import metrics\nfrom prettytable               import PrettyTable","790cce10":"import xgboost                 as xgb","6b494a6d":"def jupyter_setting():\n    \n    %matplotlib inline\n      \n    #os.environ[\"WANDB_SILENT\"] = \"true\" \n    #plt.style.use('bmh') \n    #plt.rcParams['figure.figsize'] = [20,15]\n    #plt.rcParams['font.size']      = 13\n     \n    pd.options.display.max_columns = None\n    #pd.set_option('display.expand_frame_repr', False)\n\n    warnings.filterwarnings(action='ignore')\n    warnings.simplefilter('ignore')\n    warnings.filterwarnings('ignore')\n    #warnings.filterwarnings(category=UserWarning)\n    \n    #pd.set_option('display.max_rows', 5)\n    #pd.set_option('display.max_columns', 500)\n    #pd.set_option('display.max_colwidth', None)\n\n    icecream = [\"#00008b\", \"#960018\",\"#008b00\", \"#00468b\", \"#8b4500\", \"#582c00\"]\n    #sns.palplot(sns.color_palette(icecream))\n    \n    return icecream\n\nicecream = jupyter_setting()\n\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n#warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)","035976cb":"def missing_zero_values_table(df):\n        mis_val         = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table        = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table        = mz_table.rename(columns = {df.index.name:'col_name', \n                                                     0 : 'Valores ausentes', \n                                                     1 : '% de valores totais'})\n        \n        mz_table['Tipo de dados'] = df.dtypes\n        mz_table                  = mz_table[mz_table.iloc[:,1] != 0 ]. \\\n                                     sort_values('% de valores totais', ascending=False)\n        \n        msg = \"Seu dataframe selecionado tem {} colunas e {} \" + \\\n              \"linhas. \\nExistem {} colunas com valores ausentes.\"\n            \n        print (msg.format(df.shape[1], df.shape[0], mz_table.shape[0]))\n        \n        return mz_table.reset_index()","d94d9eb7":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n        \n    return df","e63b7e0a":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    \n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.rcParams['font.size'] = 12\n    plt.title('Precision Recall vs threshold')\n    plt.xlabel('Threshold')\n    plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","9fdff6a6":"def plot_precision_vs_recall(precisions, recalls):\n    plt.plot(recalls[:-1], precisions[:-1], \"b-\", label=\"Precision\")\n    \n    plt.rcParams['font.size'] = 12\n    plt.title('Precision vs recall')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # plt.legend(loc=\"lower left\")\n    \n    plt.grid(True)","d5d6aa52":"def plot_roc_curve(fpr, tpr, label=None):\n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr, \"r-\", label=label)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('XGBR ROC curve for TPS 09')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.legend(loc=\"lower right\")\n    plt.grid(True)","785f43b1":"path = '..\/input\/tabular-playground-series-sep-2021\/'\n#path = '..\/input\/tabular-playground-series-sep-2021\/'\n#path = 'Data\/'","31154d93":"df1_train     = pd.read_csv(path + 'train.csv', index_col='id')\ndf1_test      = pd.read_csv(path + 'test.csv',index_col='id')\ndf_submission = pd.read_csv(path + 'sample_solution.csv')\n\ndf1_train.shape, df1_test.shape, df_submission.shape","5d0a1fb1":"df1_test.head()","e4169173":"df1_train = reduce_memory_usage(df1_train)\ndf1_test  = reduce_memory_usage(df1_test)\n\ndf1_train.shape, df1_test.shape","b5fea33a":"features_df = df1_train.columns.to_list()\nfeatures_df.remove('claim')","b9e85e2f":"df1_train['fe_n_missing'] = df1_train[features_df].isna().sum(axis=1)\ndf1_test['fe_n_missing']  = df1_test[features_df].isna().sum(axis=1)\n\ndf1_train.shape, df1_test.shape","3d3a409d":"features_df.append('fe_n_missing')","cf04646f":"imputer   = SimpleImputer(missing_values=np.nan, strategy='median')\ndf1_train = pd.DataFrame(imputer.fit_transform(df1_train), columns=df1_train.columns)\ndf1_test  = pd.DataFrame(imputer.fit_transform(df1_test),  columns=df1_test.columns)\ngc.collect()","fb6d0617":"missing_zero_values_table(df1_train)","aca0e52c":"missing_zero_values_table(df1_test)","ad2c98f0":"df1_train['fe_f5_zero']  = (df1_train['f5'] < 0.02).astype(int)\ndf1_test['fe_f5_zero']   = (df1_test['f5'] < 0.02).astype(int)\ndf1_train['fe_f50_zero'] = (df1_train['f50'] < 0.02).astype(int)\ndf1_test['fe_f50_zero']  = (df1_test['f50'] < 0.02).astype(int)","2b92f06b":"df1_train['fe_mean']    = df1_train.mean(axis=1)\ndf1_test['fe_mean']     = df1_test.mean(axis=1)\n\ndf1_train['fe_median']  = df1_train.median(axis=1)\ndf1_test['fe_median']   = df1_test.median(axis=1)\n\ndf1_train['fe_std']     = df1_train.std(axis=1)\ndf1_test['fe_std']      = df1_test.std(axis=1)\n\ndf1_train['fe_min']     = df1_train.min(axis=1)\ndf1_test['fe_min']      = df1_test.min(axis=1)\n\ndf1_train['fe_max']     = df1_train.max(axis=1)\ndf1_test['fe_max']      = df1_test.max(axis=1)\n\ndf1_train['fe_skew']    = df1_train.skew(axis=1)\ndf1_test['fe_skew']     = df1_test.skew(axis=1)\n\ndf1_train.shape, df1_test.shape","e9ae136d":"%%time\nscaler        = QuantileTransformer(output_distribution='normal', random_state=0)\n\ndf1_train_qt  = df1_train.copy().drop('claim', axis=1) \ncols_quantile = df1_train_qt.columns\n\ndf1_train_qt  = pd.DataFrame(scaler.fit_transform(df1_train_qt), columns=cols_quantile)\ndf1_test_qt   = pd.DataFrame(scaler.fit_transform(df1_test), columns=cols_quantile )","c4feabb2":"%%time\npca           = PCA(random_state=123)\ndf1_train_pca = pd.DataFrame(pca.fit_transform(df1_train_qt))","e5f86b52":"fig = plt.figure(figsize=(12,8))\nfig.subplots_adjust(wspace=.4, hspace=.4)\n\nax = fig.add_subplot(2, 1, 1)\n\nax.bar(range(1, 1+pca.n_components_),\n       pca.explained_variance_ratio_,\n       color='#FFB13F')\n\nax.set(xticks=[1, 2, 3, 4])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.title('Vari\u00e2ncia explicada', fontsize=15)\nplt.xlabel('Componentes principais', fontsize=13)\nplt.ylabel('% de vari\u00e2ncia explicada', fontsize=13)\n\nax = fig.add_subplot(2, 1, 2)\nax.bar(range(1, 1+pca.n_components_),\n       np.cumsum(pca.explained_variance_ratio_),\n       color='#FFB13F')\n\nax.set(xticks=[1, 2, 3, 4])\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.title('Vari\u00e2ncia explicada cumulativa', fontsize=15)\nplt.xlabel('Componentes principais', fontsize=13)\nplt.ylabel('% de vari\u00e2ncia explicada', fontsize=13)\nplt.show()\n\nt = PrettyTable(['Componente',\n                 'Vari\u00e2ncia explicada',\n                 'Vari\u00e2ncia explicada cumulativa'])\n\nprincipal_component = 1\ncum_explained_var   = 0\n\nfor explained_var in pca.explained_variance_ratio_:\n    cum_explained_var += explained_var\n    t.add_row([principal_component, explained_var, cum_explained_var])\n    principal_component += 1\n\nprint(t)","be7c1ee9":"n_components  = 20\npca           = PCA(n_components=n_components, random_state=123)\npca_feats     = [f'feature_pca_{i}' for i in range(n_components)]\ndf1_train_pca = pd.DataFrame(pca.fit_transform(df1_train_qt), columns=pca_feats)\ndf1_test_pca  = pd.DataFrame(pca.fit_transform(df1_test_qt) , columns=pca_feats)\n\ndf1_train_pca.head()","21c0e165":"df1_train  = df1_train.copy()\ndf1_test   = df1_test.copy()\ndf1_train.shape, df1_test.shape","8cc98fa2":"%%time \ngc.collect()\n\nplt.figure(figsize=(12, 7))\nvisualizer_1 = KElbowVisualizer(KMeans(random_state=59), k=(2,12))\nvisualizer_1.fit(df1_train_pca);\nvisualizer_1.poof();","2b410ab0":"gc.collect()\n\nmodel_kmeans = KMeans(n_clusters=4, random_state=59)\nmodel_kmeans.fit(df1_train_pca);\n\nclusters_train = model_kmeans.predict(df1_train_pca)\nclusters_test  = model_kmeans.predict(df1_test_pca)\n\ndf1_train['cluster'] = clusters_train\ndf1_test['cluster']  = clusters_test\n\ndf1_train.shape, df1_test.shape","487e1458":"!mkdir Data\n!mkdir Data\/pkl\n!mkdir Data\/sumbmission\n!mkdir model\n!mkdir model\/preds\n!mkdir model\/optuna","31ae1bbc":"jb.dump(df1_train,  \"Data\/pkl\/df2_train.pkl.z\")\njb.dump(df1_test,  \"Data\/pkl\/df2_test.pkl.z\")","399dfb9e":"gc.collect()","1a2606c7":"df2_train = df1_train.copy()\ndf2_test  = df1_test.copy() \n\n# df2_train     = jb.load('.\/Data\/pkl\/df2_train.pkl.z')\n# df2_test      = jb.load('.\/Data\/pkl\/df2_test.pkl.z')\n# df_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n\ndf2_train.shape, df2_test.shape","509b7b11":"X      = df2_train.drop(['claim'], axis=1)\ny      = df2_train['claim']\ncols   = X.columns\nX_test = df2_test\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, \n                                                      test_size    = 0.2,\n                                                      shuffle      = True, \n                                                      stratify     = y,\n                                                      random_state = 0)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape ","7fcbbef7":"params = {'random_state': 0,          \n          'predictor'   : 'gpu_predictor',\n          'tree_method' : 'gpu_hist',\n          'eval_metric' : 'auc'}","8ed26c3d":"model_xgb = xgb.XGBClassifier(**params)\nmodel_xgb.fit(X_train, y_train)\n\ny_hat_prob = model_xgb.predict_proba(X_valid)[:, 1]\ny_hat      = (y_hat_prob >.5).astype(int)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, y_hat_prob)\nlog_loss_     = metrics.log_loss(y_valid, y_hat_prob)                \nf1_score_     = metrics.f1_score(y_valid, y_hat)        \nauc_          = metrics.auc(fpr, tpr)    \n\nprint('AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f}'.format(auc_, f1_score_, log_loss_))\nprint('')","8d6b021b":"print(metrics.classification_report(y_valid, (y_hat_prob >.35).astype(int)))","32c74d23":"metrics.plot_confusion_matrix(model_xgb, X_valid, y_valid, cmap='inferno')\nplt.title('Confusion matrix')\nplt.grid(False)\nplt.show()","4f99967b":"precisions, recalls, thresholds = metrics.precision_recall_curve(y_valid, (y_hat_prob >.45).astype(int))\n\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","1667af63":"plot_precision_vs_recall(precisions, recalls)\nplt.show()","52f298b0":"threshold = .37\ny_hat_threshold = (y_hat_prob>threshold).astype(int)\nfpr, tpr, thresholds = metrics. roc_curve(y_valid, y_hat_threshold)\n\nplot_roc_curve(fpr, tpr, label=\"XGB\")\nplt.show()\n\nprint('F1-score: {:2.5f}'.format(metrics.f1_score(y_valid, y_hat)))\nprint('F1-score: {:2.5f} threshold({:2.2f})'.format(metrics.f1_score(y_valid, y_hat_threshold), threshold))","427ebbd0":"%%time\nX_train     = df2_train.drop(['claim'], axis=1)\ny_train     = df2_train['claim']\n\nparams = {'random_state': 0,          \n          'predictor'   : 'gpu_predictor',\n          'tree_method' : 'gpu_hist',\n          'eval_metric' : 'auc'}\n\ngc.collect()\n\nX_train_oof_shap = np.zeros((X_train.shape[0],X_train.shape[1]+1))\nfeature          = X_test.columns.to_list()\nscalers          = [#QuantileTransformer(output_distribution='uniform', random_state=0),\n                    QuantileTransformer(output_distribution='normal' , random_state=0)]\n\nfeature.append('claim')\n\nX_ts = pd.DataFrame(scaler.fit_transform(X_test.copy()), columns=X_test.columns)\n\nSEED = 59 \n\ngc.collect()\n\nX_train_oof_shap = np.zeros((X_train.shape[0],X_train.shape[1]+2))\nfeature          = X_test.columns.to_list()\nscalers          = [#QuantileTransformer(output_distribution='uniform', random_state=0),\n                    QuantileTransformer(output_distribution='normal' , random_state=0)]\n\nfeature.append('claim')","af227973":"%%time\n\nfor scaler in scalers: \n\n    FOLDS               = 5\n    df_submission.claim = 0\n    auc                 = []\n    lloss               = []\n    f1                  = []\n    model_feature_imp   = 0 \n    scale_distribution  = str.upper(scaler.get_params(True)['output_distribution']) \n    \n    kfold               = KFold(n_splits = FOLDS, random_state = 0, shuffle = True)\n \n    print('='*80)\n    print('Scaler: {} => {}'.format(scaler, scale_distribution))\n    print('='*80)\n\n    cols_model_base = ['fe_f50_zero', 'fe_f5_zero', 'fe_n_missing', 'cluster', \n                       'fe_std', 'f105', 'f102', 'f22', 'f79', 'f106', 'f71', \n                       'f77', 'f69', #'f38','f57','f40', 'f1', 'f26', 'f82'\n                      ]\n    \n    for i, (train_idx, test_idx) in enumerate(kfold.split(X_train)):\n\n        i+=1\n        \n        X_tr, y_tr = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        X_vl, y_vl = X_train.iloc[test_idx], y_train.iloc[test_idx]\n        \n        X_tr = pd.DataFrame(scaler.fit_transform(X_tr), columns=cols) \n        X_vl = pd.DataFrame(scaler.fit_transform(X_vl), columns=cols)                \n        \n        # # Treinar o modelo baseline\n        model_base = xgb.XGBClassifier(**params)\n        model_base.fit(X_tr[cols_model_base], y_tr)\n        \n        threshold = .5\n        \n        y_hat_prob_tr_mb = (model_base.predict_proba(X_tr[cols_model_base])[:, 1]>threshold).astype(int) \n        y_hat_prob_vl_mb = (model_base.predict_proba(X_vl[cols_model_base])[:, 1]>threshold).astype(int) \n        y_hat_prob_ts_mb = (model_base.predict_proba(X_ts[cols_model_base])[:, 1]>threshold).astype(int) \n        \n        X_tr['fe_md_baseline'] = y_hat_prob_tr_mb \n        X_vl['fe_md_baseline'] = y_hat_prob_vl_mb\n        X_ts['fe_md_baseline'] = y_hat_prob_ts_mb\n        \n        params_xgb = {#'n_estimators'    : 500,\n                      'max_depth'        : 7,\n                      'min_child_weight' : 40,\n                      'subsample'        : 0.662776837991846,\n                      'colsample_bynode' : 0.7392362752914446,\n                      'learning_rate'    : 0.18833941612449953,\n                      'colsample_bytree' : 0.6094875461582554,\n                      'reg_lambda'       : 10,\n                      'reg_alpha'        : 42,\n                      'eta'              : 0.07162137592923906,\n                      'alpha'            : 0.5197749697495789}\n\n        # Treinar XGB com a vari\u00e1vel fe_md_baseline \n        model = xgb.XGBClassifier(**params_xgb, \n                                  objective         = 'binary:logistic',                  \n                                  predictor         = 'gpu_predictor',\n                                  tree_method       = 'gpu_hist',\n                                  eval_metric       = 'auc', \n                                  random_state      = 59 \n                                 ) \n\n        model.fit(X_tr, y_tr)\n\n        y_hat_prob = model.predict_proba(X_vl)[:, 1]\n        y_hat      = (y_hat_prob >.5).astype(int) \n        \n        fpr, tpr, thresholds = metrics.roc_curve(y_vl, y_hat_prob)\n                 \n        log_loss_     = metrics.log_loss(y_vl, y_hat_prob)                \n        f1_score_     = metrics.f1_score(y_vl, y_hat)        \n        auc_          = metrics.auc(fpr, tpr)    \n                \n        msg = '[Fold {}] AUC: {:.5f} - F1: {:.5f} - L. LOSS: {:.5f}'\n        print(msg.format(i, auc_, f1_score_, log_loss_))\n\n        df_submission.claim += model.predict_proba(X_ts)[:, 1]\/FOLDS\n        model_feature_imp   += model.feature_importances_ \/ FOLDS\n        \n        X_train_oof_shap[test_idx, :] = np.hstack([X_vl , y_hat.reshape(-1, 1)])\n                \n        f1.append(f1_score_)\n        lloss.append(log_loss_)\n        auc.append(auc_)\n        \n    auc_mean   = np.mean(auc)\n    auc_std    = np.std(auc)\n    lloss_mean = np.mean(lloss)\n    f1_mean    = np.mean(f1)\n    \n    print('-'*80)\n    msg = '[Mean Fold] AUC: {:.5f}(Std:{:.5f}) - F1: {:.5f} - L. LOSS: {:.5f}'\n    print(msg.format(auc_mean, auc_std, f1_mean, lloss_mean))\n    print('='*80)\n    print('')\n    \n    # Gerar o arquivo de submiss\u00e3o \n    name_file_subm = str(scaler).lower()[:4] + '_' + scale_distribution \n    name_file_subm = 'Data\/sumbmission\/001_xgb_feature_engineering_' + name_file_subm + '.csv'\n    \n    df_submission.to_csv(name_file_subm, index=False)\n    \n    gc.collect()","0968d620":"# ================================================================================\n# Scaler: QuantileTransformer(output_distribution='normal', random_state=0) => NORMAL\n# ================================================================================\n# [Fold 1] AUC: 0.81068 - F1: 0.79229 - L. LOSS: 0.51150\n# [Fold 2] AUC: 0.76741 - F1: 0.73193 - L. LOSS: 0.59884\n# [Fold 3] AUC: 0.76947 - F1: 0.74404 - L. LOSS: 0.58815\n# [Fold 4] AUC: 0.76998 - F1: 0.73370 - L. LOSS: 0.59284\n# [Fold 5] AUC: 0.81071 - F1: 0.79160 - L. LOSS: 0.51071\n# --------------------------------------------------------------------------------\n# [Mean Fold] AUC: 0.78565(Std:0.02047) - F1: 0.75871 - L. LOSS: 0.56041\n# ================================================================================\n# \n# CPU times: user 3min 55s, sys: 10.8 s, total: 4min 5s\n# Wall time: 3min 59s","d2b7da72":"cols_feature    = X.columns.to_list()\ncols_feature.append('fe_md_baseline')\n\ndf               = pd.DataFrame()\ndf[\"Feature\"]    = cols_feature\ndf[\"Importance\"] = model_feature_imp \/ model_feature_imp.sum()\n\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize=(10, 30))\nbars = ax.barh(df[\"Feature\"], df[\"Importance\"], \n               height    = 0.8,\n               color     = \"blue\",  # mediumorchid\n               edgecolor = \"black\")\n\nax.set_title(\"Feature importances \\n\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Importance\", fontsize=15, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n\n# Adding labels on top\nax2 = ax.secondary_xaxis('top')\n#ax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\n\n# Inverting y axis direction so the values are decreasing\nplt.gca().invert_yaxis()","693b67ff":"shap.summary_plot(X_train_oof_shap[:,:-1], cols_feature,)","b5a2475d":"jb.dump(df2_train,  \"Data\/pkl\/df3_train.pkl.z\")\njb.dump(df2_test,  \"Data\/pkl\/df3_test.pkl.z\")\n\ngc.collect()","d7dc8303":"Pronto agora vamos para a pr\u00f3xima etapa que \u00e9 gerar a vari\u00e1vel de clusteriza\u00e7\u00e3o. ","92988bba":"Obtivemos uma `AUC` de `0.81061`, isso que dizer que o acrescimo de novas vari\u00e1veis nos dataset ajudou, por\u00e9m precisams fazer uma valida\u00e7\u00e3o mais robusta (valida\u00e7\u00e3o cruzada) em vez de uma separa\u00e7\u00e3o simples (Holdout), na valida\u00e7\u00e3o cruzada vamos ter uma estimativa mais consistente.  ","5563dac0":"Como podemos observar no gr\u00e1fico acima, precisamos criar um modelo para clusteriza\u00e7\u00e3o de 4 clusters, ent\u00e3o vamos fazer isso agora nos dados de treino e replicar para os dados de teste. ","4a127efa":"### 2.4.1. Scaler ","7a97aba1":"<h1 div class='alert alert-success'><center> TPS-Set: Feature Engineering<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26480\/logos\/header.png?t=2021-04-09-00-57-05)","116657e6":"## 1.1. Step 01","4555737b":"### 4.2.1. Feature_importances_ ","11a3de33":"## Shap\n\nSHAP (SHapley Additive exPlanations) \u00e9 uma abordagem te\u00f3rica de jogos para explicar a sa\u00edda de qualquer modelo de aprendizado de m\u00e1quina, ver o  [artigos](https:\/\/github.com\/slundberg\/shap#citations) para detalhes e cita\u00e7\u00f5es.","cfd2b8f7":"## 2.3. Step 03\nAqui vamos criar duas vari\u00e1veis que indica os valores est\u00e3o pr\u00f3ximo de zero ou n\u00e3o. ","1c4ff90c":"A ideia de adicionar o recurso \"n_missing\" abaixo foi tirada deste [bloco notas](https:\/\/www.kaggle.com\/maximkazantsev\/tps-09-21-eda-lightgbm-with-folds) de por [BIZEN](https:\/\/www.kaggle.com\/hiro5299834) .","df6f8922":"Vamos fazer o scaler dos datasets com o `QuatileTransforme` que teve melhor resultado na valida\u00e7\u00e3o cruzada e no kaggle com AUC de 0.74479 com XGB, que se encontra no [notebook anterior](https:\/\/www.kaggle.com\/rogeriodelfim\/01-tps-set-ponto-de-partida-eda-e-linha-de-base?scriptVersionId=73995146).  ","12c3c3d2":"\n## 4.1. Valida\u00e7\u00e3o Cruzada","85194aaf":"## 0.3. Carregar Dados","9c5a0a78":"- ","bb320f33":"## 4.2. Feature importances","04582d6b":"Vamos salvar os dataset, caso seja necess\u00e1rio refazer o processo podemos partir desse ponto. ","6483a563":"Acima podemos observar que algumas das vari\u00e1veis que criamos est\u00e3o entre as 25 vari\u00e1veis mais importantes para o modelo, por\u00e9m n\u00e3o gosto muito de jeito de analisar a import\u00e2ncia das vari\u00e1veis, vamos utilizar o `Shap` que utilza v\u00e1rios m\u00e9todos diferentes para encontrar as vari\u00e1veis importantes. ","7f806211":"**`NOTA:`**\n\nNo primeiro gr\u00e1fico observamos que os quatros primeiro compontes s\u00e3o pouco relevantes, com pouca explicabilidade, como podemos observar no segundo gr\u00e1fico e na tabela de vari\u00e2ncia acumulada, sendo assim, vou selecionar 20 componeste para a clusteriza\u00e7\u00e3o, pois a explicabilidade chega \u00e0 35.45%.","0e970cab":"<div class=\"alert alert-info\" role=\"alert\">\n    \n**`NOTA:`**\n\nComo podemos obsevar acima, a m\u00e9dia de AUC na valida\u00e7\u00e3o cruzada foi de 0.78552 e na submiss\u00e3o do kaggle foi uma AUC de 0.80771, sem ajustes nos parametros do XGB.  ","d3db73fd":"# <div class=\"alert alert-success\"> 1. Feature Engineering <\/div>","acdd9c49":"## 2.4. Step 04 \nNeste step vamos criar uma nova vari\u00e1vel com o algoritmo `kmeans`, para quem conhence o algoritmo vamos fazer uma clusteriza\u00e7\u00e3o, mas em primeiro lugar vamos criar algumas vari\u00e1veis com uma PCD com as 40 principais componentes para a clusteriza\u00e7\u00e3o.","b2d1a5cd":"Parametros do algoritmo","fd9af7bc":"## 0.2. Fun\u00e7\u00f5es","1fbc52a9":"# <div class=\"alert alert-success\">  0. IMPORTA\u00c7\u00d5ES <\/div>","0611d0d1":"Vamos salvar o dataset para o pr\u00f3ximo notebook, que ser\u00e1 a tunagem de parametros. ","3d29c8f6":"Como podemo observar a ordem das vari\u00e1veis importantes para o modelo mudar\u00e3o, agora temos as 4 primeira vari\u00e1veis que foram criadas como principais, no notebook de feature select vou utilizar o `Shap` para analise da sele\u00e7\u00e3o das vari\u00e1veis. ","be0990f5":"Nesta etapa vamos utilizar duas formas de verificar a import\u00e2ncia das vari\u00e1veis para o modelo: \n- As feature importance (feature_importances_) do pr\u00f3prio modelo \n- Shap","4cc8158a":"### 2.1.2. Clusteriza\u00e7\u00e3o\n\nNesta etapa vamos cria uma nova vari\u00e1vel com utiliza\u00e7\u00e3o do algoritmo `kmeams`, que tem a finalidade de criar clusters, vamos utilizar as vari\u00e1veis criadas no processo anterior `2.4.2. Gerar PCA`.","8a2d6235":"Agora vamos criar algumas vari\u00e1veis de estat\u00edstica descritiva. ","b1e0eb2a":"### 2.4.2. Gerar PCA\nNesta etapa vamos criar 20 componente principal que ser\u00e3o ser\u00e3o utilizadas na clusteriza\u00e7\u00e3o, normalmente as primeiras componentes de uma PCA resumem a maior parte da vari\u00e2ncia dos dados, no caso desse conjunto de dados vamos precisamos de 20 \u00e0 40 compontes para termos uma melhor explicabilidade da vari\u00e2ncia dos dados.  ","3bbe7218":"# <div class=\"alert alert-success\"> 3. Modelagem <\/div>","42d1b187":"# <div class=\"alert alert-success\"> 2. Split Train\/Test <\/div>","de87808a":"## 2.1. Step 02 \nTratamento de valores faltantes, para os valores faltantes vamos imputar a mediana. "}}