{"cell_type":{"3262d1af":"code","1371dc6e":"code","6f6ef250":"code","599743e5":"code","470379f9":"code","72bcafd7":"code","eaa5525c":"code","ca21df55":"code","dd7d7ea9":"code","87accf17":"code","d2393421":"code","0688dcd1":"code","fc92535b":"code","cc64f90c":"code","6211a144":"code","287361fb":"code","450da5fc":"code","e1c44f0a":"code","56e4b2c2":"code","53caca34":"code","07b6204d":"code","4df94586":"code","17805f3d":"code","bf9125ce":"code","fbd83737":"code","54922895":"code","e9e21c0a":"code","52f88511":"code","f6c13669":"code","09231141":"code","332832d0":"code","e6722e95":"code","17639be9":"code","fdb0f2b9":"code","e6696dac":"code","9cf40a0a":"code","8dc72c81":"code","dea58627":"code","18e440d5":"code","881e105f":"code","64ff48ab":"code","98f90e44":"code","48a3fe03":"code","1978387d":"code","0d93a580":"code","a30d98c1":"code","e422b3b2":"code","5a15f5f1":"code","c0120a7c":"code","3d009221":"code","6a8b76d7":"code","5d999662":"code","6c8bbf5c":"code","5919e30c":"code","05f18dd6":"code","2855dd67":"code","5d8a12b9":"code","6f6be7dd":"code","c9aaecf8":"code","a679691a":"code","e9e25d06":"code","7b76b12d":"code","580f13a6":"markdown","4a7784ed":"markdown","6aff1da7":"markdown","abb3a63e":"markdown","e2dcdba8":"markdown","63c00e51":"markdown","ef0074bd":"markdown","f4fd9345":"markdown","523f3d9b":"markdown","941a9300":"markdown","8aaa69c4":"markdown","8fcaa074":"markdown","129ac051":"markdown"},"source":{"3262d1af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom sklearn import preprocessing\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport plotly as py\nfrom plotly import tools\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nsns.set()\nfrom keras.layers import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers import TimeDistributed\nfrom keras.optimizers import RMSprop, SGD, Adam\nfrom keras.initializers import he_normal, he_uniform\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt\nimport statsmodels.api as sm\nimport time\nimport warnings\nimport itertools\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1371dc6e":"# Installing pmdarima for auto arima\n!pip install pmdarima","6f6ef250":"import pmdarima as pm","599743e5":"data=pd.read_csv(r'..\/input\/pakistans-largest-ecommerce-dataset\/Pakistan Largest Ecommerce Dataset.csv',parse_dates=['created_at'])\nprint(\"Data Dimensions are: \", data.shape)\ndata.columns=data.columns.str.replace(\" \", \"_\").str.lower()\nprint(\"Columns: \", data.columns)","470379f9":"data.head()","72bcafd7":"#Data exploration\ndata.info()","eaa5525c":"data.dtypes","ca21df55":"# Quantifying null values\nprint(data.isnull().sum())","dd7d7ea9":"# Visualizing the percentage of null values\ndata.isnull().mean().plot.bar(figsize=(12,6))\nplt.ylabel('Percentage of missing values')\nplt.xlabel('Variables')\nplt.title('Quantifying missing data')","87accf17":"data.drop([\"unnamed:_21\", \"unnamed:_22\", \"unnamed:_23\", \"unnamed:_24\", \"unnamed:_25\"], axis = 1, inplace=True)\ndata.dropna(how='all', axis=0, inplace=True)\ndata.rename(columns={\"_mv_\": \"mv\", \"category_name_1\": \"category_name\"}, inplace = True)","d2393421":"# Quantifying null values\nprint(data.isnull().sum())","0688dcd1":"# inspect unique values - categorical variable\ndata['status'].unique()","fc92535b":"data.groupby('bi_status')['status'].value_counts()","cc64f90c":"data['status'] = data['status'].replace(r'\\\\N', 'Cancelled', regex=True)","6211a144":"# For simplicity we can merge all statuses into Completed, Cancelled and Refund\ndict_status = {'Completed':['complete','closed','received','cod','paid','exchange','payment_review','pending','processing','holded','pending_paypal'],'Refund':['order_refunded','refund'], 'Cancelled':['canceled','fraud',np.nan]}\nfor n in range(len(dict_status)):\n    key,value=list(dict_status.items())[n]\n    data['status'].replace(value, key,inplace=True)\n    n+=1","287361fb":"data['status'].value_counts()","450da5fc":"# bar plots for status categorical variables\n\ndata['status'].value_counts().plot.bar()\nplt.xticks(rotation=0)\nplt.ylabel('Count')\nplt.title('Status  - Distinct Counts')","e1c44f0a":"# Check for relevance of different features of dataset\ndata[['created_at','working_date','sku','qty_ordered','price','grand_total','mv','discount_amount','sales_commission_code','customer_id']].head()","56e4b2c2":"data.drop(['working_date', 'mv', 'increment_id','bi_status','sales_commission_code'],axis=1,inplace=True)","53caca34":"data.info()","07b6204d":"data.isnull().sum()","4df94586":"print(\"Count Different Categories: \")\nprint(data['category_name'].value_counts(dropna=False)) ","17805f3d":"# Extracting all unique categories for category value'\\N'\nskunique=data[data['category_name']==r'\\N']['sku'].unique().tolist()","bf9125ce":"#Now we check for sku's found against '\\N' category in other categories \nsku_nil=data[data['sku'].isin(skunique)]\nsku_nil['category_name'].value_counts()","fbd83737":"# We found sku with '\\N' category also in categories as mentioned above\n# Updating the sku category '\\N' where same sku found in above categories and all remaining values of sku with Other's  \ndict_sku={}\ncat=[\"Men's Fashion\",'Others','Superstore','Mobiles & Tablets',\"Women's Fashion\",'Entertainment','Appliances']\nfor n in cat:\n    dict_sku[n]= n\nfor n in range(len(dict_sku)):\n    key,value=list(dict_sku.items())[n]\n    dict_sku[key]=sku_nil[sku_nil['category_name']== key]['sku'].unique().tolist()\n    data.loc[((data['sku'].isin(dict_sku[key])) & (data['category_name']==r'\\N')),'category_name']= key\n    n+=1\n","54922895":"data.loc[(data['category_name']==r'\\N'),'category_name']= 'Others'\ndata['category_name'] = data['category_name'].replace(np.nan, 'Others', regex=True)","e9e21c0a":"# Categories after updating all '\\N' and null categories\ndata['category_name'].value_counts(dropna=False).plot.bar(figsize=(12,6))\n#plt.xticks(rotation=0)\nplt.ylabel('Sales')\nplt.title('Sales Category Wise')","52f88511":"data[data['sku'].isnull()]","f6c13669":"# As out of 20 null sku values,  most of the order statuses are either cancelled or refund with grand_total 0.\n# Replacing nan values with sku_nan\ndata['sku'].fillna(\"sku_nan\",inplace=True)","09231141":"#Checking null values in columns customer_id and customer_since\ndata[data['customer_id'].isnull()]","332832d0":"#For customer_id null most of the orders are with order status as cancelled or refund\n#We can replace the null customer_id with value 0 and customer_since with 2018\ndata['customer_id'].fillna(\"0\",inplace=True)\ndata['customer_since'].fillna(\"1-2018\",inplace=True)","e6722e95":"#Replacing values < 0 with 0\ndata.loc[(data['grand_total']< 0), 'grand_total']=0","17639be9":"# Filter all competed orders \ndataordercomp= data[data['status']=='Completed']","fdb0f2b9":"#Get total of daily sales\ndf=dataordercomp.groupby('created_at').size().reset_index(name='orders')\ndf.columns=['date','orders']\ndf.set_index('date',inplace=True)","e6696dac":"plt.rcParams[\"figure.figsize\"] = [16,9]","9cf40a0a":"#Check general order trend\nplt.plot(df.orders)\n","8dc72c81":"df1=df\ndf1['year'] = df1.index.year\ndf1['month'] = df1.index.month\nfig = px.box(df1, x=\"year\", y=\"orders\")\nfig.update_layout(\n    title_text='Yearly Trend'\n    )\nfig.show()","dea58627":"fig = px.box(df1, x=\"month\", y=\"orders\")\nfig.update_layout(\n    title_text='Monthly Seasonality'\n    )\nfig.show()","18e440d5":"#For checking stationarity of series we use adfuller test\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\ndef adf_test(timeseries):\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag=\"AIC\")\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","881e105f":"adf_test(df.orders)","64ff48ab":"#Results shows that our data is not stationary, now we apply different techniques to make data stationary\n#\n#df['orders']=df.orders.rolling(window=5).mean()\ndf['orders']=np.log(df.orders)\n#df['orders']=df.orders.diff(periods=5)#.dropna(inplace=True)#fillna(0)\ndf['orders']=df['orders'].ewm(alpha=0.5).mean()","98f90e44":"adf_test(df.orders)","48a3fe03":"df.index = pd.date_range(start='2016-07-01',periods=789,freq='D') ","1978387d":"sm.graphics.tsa.plot_acf(df.orders);","0d93a580":"sm.graphics.tsa.plot_pacf(df.orders);","a30d98c1":"# To filter best p,q and d parameters for ARIMA model we use autorima function\nmodel = pm.auto_arima(df.orders, start_p=1, start_q=1,\n                      test='adf',       # use adftest to find optimal 'd'\n                      max_p=3, max_q=3, # maximum p and q\n                      m=1,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\n\nprint(model.summary())","e422b3b2":"from statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(df.orders, order=(3,0,3))\nmodel_fit = model.fit()\nprint(model_fit.summary())","5a15f5f1":"from math import sqrt\nfrom sklearn.metrics import mean_squared_error\nplt.plot(df.orders,color='r')\nplt.plot(model_fit.predict(dynamic=False),color='g')\nplt.legend(['Actual', 'Predicted'])","c0120a7c":"# Checking model results\nmodel_fit.plot_diagnostics(figsize=(15, 12))\nplt.show()","3d009221":"pred = model_fit.get_prediction(start=pd.to_datetime('2018-08-1'), dynamic=True)\npred_ci = pred.conf_int()","6a8b76d7":"ax = df.loc['2018':]['orders'].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecasted', alpha=.7)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Date')\nax.set_ylabel('Orders')\nplt.legend()\n\nplt.show()","5d999662":"# Extract the predicted and true values of our time series\ny_forecasted = pred.predicted_mean\ny_truth = df.loc['2018-08-1':]['orders']\n\n# Compute the mean square error\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n\nrmse =sqrt(mse)\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 2)))\n\nmape = np.mean(np.abs(np.exp(y_forecasted) - np.exp(y_truth))\/np.abs(np.exp(y_truth))) # MAPE\nprint('The Mean Absolute Percentage Error of our forecasts is {}'.format(round(mape, 2)))","6c8bbf5c":"# Get forecast 60 steps ahead in future\npred_uc = model_fit.get_forecast(steps=60)\n\n# Get confidence intervals of forecasts\npred_ci = pred_uc.conf_int()","5919e30c":"ax = df.orders.plot(label='observed', figsize=(20, 15))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.09)\nax.set_xlabel('Date')\nax.set_ylabel('No of Orders')\n\nplt.legend()\nplt.title('ARIMA Model')\nplt.show()","05f18dd6":"# Seasonal - fit stepwise auto-ARIMA\nsmodel = pm.auto_arima(df.orders, start_p=1, start_q=1,\n                         test='adf',\n                         max_p=3, max_q=3, m=12,\n                         start_P=0, seasonal=True,\n                         d=None, D=1, trace=True,\n                         error_action='ignore',  \n                         suppress_warnings=True, \n                         stepwise=True)\n\nsmodel.summary()","2855dd67":"# Now applying sesonal arima \nmod = sm.tsa.statespace.SARIMAX(df['orders'],\n                                            order=(2,0,0),\n                                            seasonal_order=(2,1,0,12),\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n#mod = sm.tsa.statespace.SARIMAX(df['orders'], trend='c', order=(21,0,(1,0,0,1)))\nres = mod.fit()\nprint(res.summary())","5d8a12b9":"res.plot_diagnostics();","6f6be7dd":"pred = res.get_prediction(start=pd.to_datetime('2018-08-1'), dynamic=True)\npred_ci = pred.conf_int()","c9aaecf8":"ax = df.loc['2018':]['orders'].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecasts', alpha=.7)\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Date')\nax.set_ylabel('Orders')\nplt.legend()\n\nplt.show()","a679691a":"# Extract the predicted and true values of our time series\ny_forecasted = pred.predicted_mean\ny_truth = df.loc['2018-08-1':]['orders']\n\n# Compute the mean square error\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n\nrmse =sqrt(mse)\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(rmse, 2)))\n\nmape = np.mean(np.abs(np.exp(y_forecasted) - np.exp(y_truth))\/np.abs(np.exp(y_truth))) # MAPE\nprint('The Mean Absolute Percentage Error of our forecasts is {}'.format(round(mape, 2)))","e9e25d06":"# Get forecast 60 steps ahead in future\npred_uc = res.get_forecast(steps=60)\n\n# Get confidence intervals of forecasts\npred_ci = pred_uc.conf_int()","7b76b12d":"ax = df.orders.plot(label='observed', figsize=(20, 15))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=0.05)\nax.set_xlabel('Date')\nax.set_ylabel('Orders')\n\nplt.legend()\nplt.title('Seasonal ARIMA Model')\nplt.show()","580f13a6":"# **Let's check columns for duplicate entries, non relevant data**","4a7784ed":"**Checking null\/not defined values in categorical variable category_name**","6aff1da7":"Overall, the model fit seems good. Residual errors overall shows uniform variance, histogram shows distribituion is likerly normal and most of the theoratical quantities are perfectly in line with the red line.So, let's use it to forecast.","abb3a63e":"**Checking for missing values**","e2dcdba8":"In this notebook classical time series forecasting models are used. In the next version I will try to explore LSTM.   ","63c00e51":"****Above visualization shows percentage of missing value for each column in dataset ****\n1. Columns Unnamed 21 to Unnamed 25 are 100% null\n2. Columns sales_commission_code has above 50% null values\n3. All the remaining columns have about 40% null values","ef0074bd":"****We can drop the columns working date, mv, sales_commission_code & other irrelevant cols because we have all the required relevant information in columns created_at, grand_total and discount_amount ****","f4fd9345":"# **Time Series analysis and Order predictions**","523f3d9b":"**Checking categorical variable sku for null entries**","941a9300":"Let's check data types of columns","8aaa69c4":"**From above observations it is concluded that all statuses falls under group Gross can be marked as Canceled, Net and Valid group of orders can be considered under complete category**","8fcaa074":"Log transform makes the series stationary, whereas differencing is not effective for this dataset. Similarly used Exponential Weighted Moving Average for smoothing.","129ac051":"Overall, the model fit seems good. Residual errors overall shows uniform variance, histogram shows distribituion is likerly normal and most of the theoratical quantities are perfectly in line with the red line.So, let's use it to forecast."}}