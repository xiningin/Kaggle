{"cell_type":{"b51d83a1":"code","b259dfb2":"code","4485e8c2":"code","60235779":"code","84c9c9e2":"code","6e3e4ea1":"code","8fa3f2f6":"code","a5cd8764":"code","4cb51a83":"code","7cb49002":"code","70b178e8":"code","744233b9":"code","1218b687":"code","4330e8d8":"code","2621612a":"code","f884b9a1":"code","f86c0fca":"code","47d35b9e":"code","f0a06a10":"code","a7fbf076":"code","561c22e1":"code","dcbee660":"code","f9151b47":"code","2b1a9c4d":"code","3afa1620":"code","dd228086":"code","7cf7ebdd":"code","f6dc143b":"code","b62fb61c":"code","732d35d2":"code","ff718bc0":"code","3d9a5bc6":"code","180d6970":"code","74eaa035":"code","4963c395":"code","0e7eca9e":"code","c8323e33":"markdown","0fa3177d":"markdown","59d32f75":"markdown","9f743045":"markdown","f4b7abb7":"markdown","3b163b3c":"markdown","43634228":"markdown","43fcf64f":"markdown","75f2bb95":"markdown","90569b6f":"markdown","97f43348":"markdown"},"source":{"b51d83a1":"# import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","b259dfb2":"titanic_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","4485e8c2":"titanic_df.head()","60235779":"# shape of dataset\ntitanic_df.shape","84c9c9e2":"# statistical analysis of train data\ntitanic_df.describe()","6e3e4ea1":"titanic_df.columns","8fa3f2f6":"titanic_df.info()","a5cd8764":"# check for null values\ntitanic_df.isnull().sum()","4cb51a83":"# fill null values of 'Age' column be median of that column\ntitanic_df['Age']=titanic_df['Age'].fillna(titanic_df['Age'].median())\n# fill null values of 'Embarked' column be mode of that column\ntitanic_df['Embarked']=titanic_df['Embarked'].fillna(titanic_df['Embarked'].mode()[0])","7cb49002":"# drop unnecessary columns\ntitanic_df.drop(columns=[\"Cabin\",\"Name\",\"Ticket\",\"PassengerId\"],axis=1,inplace=True )","70b178e8":"# create a new column by adding 'Parch' & 'SibSp' columns\ntitanic_df[\"Family_Member\"]=titanic_df['Parch']+titanic_df['SibSp']","744233b9":"# drop 'SibSp' & 'Parch' columns\ntitanic_df.drop(columns=[\"SibSp\",\"Parch\"],axis=1,inplace=True )","1218b687":"# check for null values\ntitanic_df.isnull().sum()","4330e8d8":"# separate columns having categorical values\nobject_df=titanic_df.select_dtypes(include=['object']).copy()\nobject_df.head()","2621612a":"object_df.nunique()","f884b9a1":"# unique values in object_df dataframe\nprint(\"Sex :\",object_df.Sex.unique())\nprint(\"Embarked :\",object_df.Embarked.unique())","f86c0fca":"# visualize frequency distribution using barplot\ndef plot_bar_graph(column_name):\n    ed_count = column_name.value_counts()\n    sns.set(style=\"darkgrid\")\n    sns.barplot(ed_count.index, ed_count.values, alpha=0.9)\n    plt.title('Frequency Distribution of {} Levels using Bar Plot'.format(column_name.name))\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel('{}'.format(column_name.name), fontsize=12)\n    plt.show() \n    ","47d35b9e":"# visualize frequency distribution using pie chart\ndef plot_pie_graph(column_name):\n    labels = column_name.astype('category').cat.categories.tolist()\n    counts = column_name.value_counts()\n    sizes = [counts[var_cat] for var_cat in labels]\n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\n    ax1.axis('equal')\n    plt.title('Frequency Distribution of {} Levels using Pie Chart'.format(column_name.name))\n    plt.show()","f0a06a10":"for col in object_df.columns:\n    plot_bar_graph(object_df[col])\n    plot_pie_graph(object_df[col])","a7fbf076":"# separate columns having numeric values\nint_df=titanic_df.select_dtypes(include=['integer']).copy()\nint_df.drop(axis=1,columns=['Survived'],inplace=True)\nint_df.head()","561c22e1":"# distribution of 'Pclass' column\nsns.distplot(int_df['Pclass'])","dcbee660":"# distribution of 'Family_Member' column\nsns.distplot(int_df['Family_Member'])","f9151b47":"# encode the categorical values into numeric values\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfor col in object_df.columns:\n    titanic_df[col]=le.fit_transform(titanic_df[col])","2b1a9c4d":"titanic_df.head()","3afa1620":"# co-relation of columns with eachother\nplt.figure(figsize=(15,10))\ncorr=titanic_df.corr()\nsns.heatmap(corr,annot=True,fmt=\"0.2f\",cmap=\"coolwarm\")","dd228086":"titanic_df.head()","7cf7ebdd":"# separate features & target variables\nx=titanic_df.drop(axis=1,columns=['Survived'])\ny=titanic_df['Survived']","f6dc143b":"# import classification libraries\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb","b62fb61c":"# this code is to show how much time required to train the model using different algorithms\nfrom datetime import datetime\ndef timer(start_time= None):\n  if not start_time:\n    start_time=datetime.now()\n    return start_time\n  elif start_time:\n    thour,temp_sec=divmod((datetime.now()-start_time).total_seconds(),3600)\n    tmin,tsec=divmod(temp_sec,60)\n    print('\\n Time taken: %i hours %i minutes and %s seconds. '% (thour,tmin,round(tsec,2)))","732d35d2":"# parameters of all classification algorithms respectively\nmodel_param={\n    'random_forest':{\n        'model':RandomForestClassifier(),\n        'params':{\n            'n_estimators':[1,5,10]\n        }\n    },\n    'logistic_regression':{\n        'model':LogisticRegression(multi_class='auto'),\n        'params':{\n            'C':[1,5,10]\n        }\n    },\n    \n    'decission_tree':{\n        'model':DecisionTreeClassifier(),\n        'params':{\n            'criterion' : [\"gini\", \"entropy\"]\n        }\n    },\n    'svm':{\n        'model':SVC(gamma='auto'),\n        'params':{\n            'C':[1,10,20],\n            'kernel':['rbf','linear']\n        }\n    },\n    'xgboost':{\n        'model':xgb.XGBClassifier(),\n        'params':{\n        'learning_rate':[0.20,0.30,0.35,0.37,0.40],\n        'max_depth':[6,7,8,9,10],\n        'min_child_weight':[5,7,8,9],\n        'gamma':[0.0,0.1,0.2,0.3],\n        'colsample_bytree':[0.5,0.7,0.8,0.9,1.0]\n        }\n    }\n    \n}","ff718bc0":"start_time=timer(None)\nscores=[]\nfor model_name,mp in model_param.items():\n    # Apply GridSearchCV\n    rs=GridSearchCV(mp['model'],mp['params'],cv=5,return_train_score=False)\n    rs.fit(x,y)\n    scores.append({\n        'model':model_name, # it'll retrive the best model name\n        \"best_score\":rs.best_score_, # it'll retrive the best accuracy score\n        'best_params':rs.best_params_ # it'll retrive the best parameter\n    })\ntimer(start_time)","3d9a5bc6":"df=pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf","180d6970":"rs.best_params_","74eaa035":"rs.best_estimator_","4963c395":"# Apply KFold\nfrom sklearn.model_selection import KFold\nskf=KFold(n_splits=8)\nxg3= xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.2,\n              learning_rate=0.37, max_delta_step=0, max_depth=7,\n              min_child_weight=7, missing=None, n_estimators=100, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)\nfor train_index,test_index in skf.split(x,y):\n    x_train,x_test,y_train,y_test=x.loc[train_index],x.loc[test_index],y[train_index],y[test_index]\n    xg3.fit(x_train, y_train)\nxg3.score(x_train, y_train)","0e7eca9e":"#Saving Scikitlearn models\nimport joblib\njoblib.dump(xg3, \"xgboost_titanic.pkl\")\n","c8323e33":"# **1. Overview**","0fa3177d":"**The Challenge**\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\nThe data has been split into two groups:\n\n  1. training set (train.csv)\n  2. test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n**Variable Notes**\n\n\n**pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp**: The dataset defines family relations in this way...\n\n**Sibling** = brother, sister, stepbrother, stepsister\n\n**Spouse** = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch**: The dataset defines family relations in this way...\n\n**Parent** = mother, father\n\n**Child** = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","59d32f75":"# **2. Import Libraries:**","9f743045":"# **Do the same data preprocessing steps for test dataset. I've uploaded this on my github accound.check** [here](https:\/\/github.com\/sidharth178\/kaggle-titanic-competition\/blob\/master\/test_titanic_model.ipynb) **.If you find this notebook useful give a \"upvote\"**","f4b7abb7":"# **6.Data Preprocessing**","3b163b3c":"# **9.Model Building**\n\n","43634228":"# **3. Load Dataset:**","43fcf64f":"# **8. Hyper Parameter tuning**","75f2bb95":"**NOTE:** Use google colab IDE to run this notebook","90569b6f":"# **5.Data Visualization:**","97f43348":"# **4.Exploratory Data Analysis:**"}}