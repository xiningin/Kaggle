{"cell_type":{"6689d2f7":"code","986db5fd":"code","66ac9b27":"code","b1867852":"code","395d7422":"code","4d45b746":"code","a9776c6d":"code","dcbb93ea":"code","a3d4681d":"code","20779894":"code","2af3b4b0":"code","1ab29faf":"code","5854e668":"code","d67f4d02":"code","0aba4b42":"code","10b7e89c":"code","9883201f":"code","a7c0ce68":"code","0c596a51":"code","08215969":"code","f2bbf22d":"code","84a8b2a1":"code","f547005f":"markdown","d8cfeee1":"markdown","af24d7ee":"markdown","aaba843c":"markdown","5efa97e6":"markdown","df3f5943":"markdown","476a4a0b":"markdown","687763e9":"markdown","e0d5d517":"markdown","61d1c78a":"markdown"},"source":{"6689d2f7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom catboost import Pool, cv, CatBoostClassifier\nfrom sklearn.model_selection import TimeSeriesSplit, train_test_split\nfrom sklearn.metrics import  classification_report, log_loss, roc_auc_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","986db5fd":"GAME_COLS  =['GAME_DATE_EST', 'GAME_ID', 'HOME_TEAM_ID',\n       'VISITOR_TEAM_ID', 'SEASON', 'HOME_TEAM_WINS']\ndf = pd.read_csv(\"\/kaggle\/input\/nba-games\/games.csv\",usecols = GAME_COLS,parse_dates=[\"GAME_DATE_EST\"],infer_datetime_format=True)\ndf = df.drop_duplicates().sort_values(\"GAME_DATE_EST\").set_index([\"GAME_DATE_EST\"])\nprint(df.shape)\ndf.head()","66ac9b27":"df.tail(3)","b1867852":"df_players = pd.read_csv(\"\/kaggle\/input\/nba-games\/players.csv\")\nprint(df_players.shape)\ndf_players.head()\n\n### joining the players means a many to 1 join - of all players per team. Kludgy code, skip for now, especially without further features at the player level","395d7422":"df_teams = pd.read_csv(\"\/kaggle\/input\/nba-games\/teams.csv\")\nprint(df_teams.shape)\ndf_teams.head()","4d45b746":"df.HOME_TEAM_ID.nunique() ## no obvious mismatch in # teams","a9776c6d":"df_ranking = pd.read_csv(\"\/kaggle\/input\/nba-games\/ranking.csv\",parse_dates=[\"STANDINGSDATE\"])\ndf_ranking.sort_values(\"STANDINGSDATE\",inplace=True)\nprint(df_ranking.shape)\n\n## drop the less interesting or amenably columns . We could get ratio features from the record cols, but that'd require splitting first : \ndf_ranking.drop([\"CONFERENCE\",\"LEAGUE_ID\",\"HOME_RECORD\",\"ROAD_RECORD\"],axis=1,inplace=True) \n\ndf_ranking.head()","dcbb93ea":"df_ranking[\"SEASON_ID\"].unique()","a3d4681d":"print(df.shape)\n\ndf_ranking.set_index(\"STANDINGSDATE\",inplace=True)\n\ndf = pd.merge_asof(df, df_ranking.add_suffix(\"_homeTeam\"),\n              left_index=True,\n                       right_index=True,\n              left_by=\"HOME_TEAM_ID\",\n                       right_by='TEAM_ID'+\"_homeTeam\",\n#                         suffixes=\"_homeTeam\",  ## for some reason this gives error, so we workaround it by adding suffixes\n                       allow_exact_matches=False)\n\ndf = pd.merge_asof(df, df_ranking.add_suffix(\"_awayTeam\"),\n              left_index=True,\n              right_index=True,\n              left_by=\"VISITOR_TEAM_ID\",\n                       right_by='TEAM_ID'+\"_awayTeam\",\n                       allow_exact_matches=False)\n\ndf.drop([\"SEASON_ID_awayTeam\",\"TEAM_ID_awayTeam\",\"TEAM_ID_homeTeam\"],axis=1,inplace=True) ## redundant\ndf.rename(columns={\"SEASON_ID_homeTeam\":\"SEASON_ID\"},inplace=True)\nprint(df.shape)\ndf.head()","20779894":"df.loc[df.G_homeTeam==0]","2af3b4b0":"def missing_game_rankings(row,suffix=\"_homeTeam\"):\n    if ((row[\"G\"+suffix]==0) & (row[\"W\"+suffix]==0)): \n        row[\"G\"+suffix]=np.nan\n        row[\"W\"+suffix]=np.nan\n        row[\"L\"+suffix]=np.nan\n        row[\"W_PCT\"+suffix]=np.nan\n    return row\n  \ndf = df.apply(lambda x: missing_game_rankings(x,suffix=\"_awayTeam\"),axis=1)\ndf = df.apply(lambda x: missing_game_rankings(x,suffix=\"_homeTeam\"),axis=1)\n\nprint(df.isna().sum())\n\ndf = df.dropna()\nprint(\"df without nans size:\", df.shape[0])","1ab29faf":"## sklearn temporal split is for CV - we don't need. data is sorted, so we'll just take the last 20% of rows\n## get only numeric columns - we don't need the strings here\nCUTOFF_ROW = int(df.shape[0]*0.8)\n# X = df.reset_index().drop([\"SEASON\"],axis=1)._get_numeric_data().copy() \nX = df.drop([\"SEASON\"],axis=1)._get_numeric_data().copy() \nX_train = X[:CUTOFF_ROW].drop([\"HOME_TEAM_WINS\"],axis=1)\nprint(\"X_train\",X_train.shape)\nX_test = X[CUTOFF_ROW:].drop([\"HOME_TEAM_WINS\"],axis=1)\nprint(\"X_test\",X_test.shape)\ny_train = X[:CUTOFF_ROW][\"HOME_TEAM_WINS\"]\nprint(\"y_train\",len(y_train))\ny_test = X[CUTOFF_ROW:][\"HOME_TEAM_WINS\"]","5854e668":"X_test","d67f4d02":"print([c for c in X_train.columns if 5<X_train[c].nunique()<8000])\n\ncategorical_cols = ['HOME_TEAM_ID', 'VISITOR_TEAM_ID']","0aba4b42":"## catBoost Pool object\ntrain_pool = Pool(data=X_train,label = y_train,cat_features=categorical_cols,\n#                   baseline= X_train[\"W_PCT_homeTeam\"], ## not as relevant as a baseline, since we subtracted by it (rather than dividing)\n#                   group_id = X_train['SEASON_ID']\n                 )\n\ntest_pool = Pool(data=X_test,label = y_test,cat_features=categorical_cols,\n#                   baseline= X_train[\"W_PCT_homeTeam\"], ## not as relevant as a baseline, since we subtracted by it (rather than dividing)\n#                   group_id = X_test['SEASON_ID']\n                 )","10b7e89c":"model = CatBoostClassifier(verbose=False) # ,task_type=\"GPU\") # use GPU acceleration - requires kernel to have GPU activated and limits availability\n\nmodel.fit(train_pool, plot=True,silent=True)\nprint(model.get_best_score())","9883201f":"## get results on test set \ntest_preds = model.predict(test_pool,prediction_type='Class')\nprint(classification_report(y_true=y_test,y_pred=test_preds))","a7c0ce68":"test_preds_proba = model.predict(test_pool,prediction_type='Probability')[:,1]\n\nprint(\"Test AUC:\")\nprint(\"%.4f\" % roc_auc_score(y_true=y_test, y_score = test_preds_proba))\n\nprint(\"Test Log Loss:\")\nprint(\"%.4f\" % log_loss(y_true=y_test, y_pred = test_preds_proba))","0c596a51":"params = {\n          \"loss_function\": \"Logloss\",\n          \"verbose\": False,\n          \"use_best_model\":True, ## requires a validation dataset to be provided.\n          \"custom_metric\":['Logloss', 'AUC',\"Precision\"],\n         }\n\ndf_cv = cv(pool=train_pool,params=params,plot=True,type=\"TimeSeries\",fold_count=6,metric_period=3)\n\ndisplay(df_cv.sample(5))\n\ntest_eval_cols = [c for c in df_cv.columns if (\"test\" in c) & (\"mean\" in c)]\ndisplay(df_cv[test_eval_cols].median())\ndisplay(df_cv.tail(1)[test_eval_cols])","08215969":"feature_importances = model.get_feature_importance(train_pool)\nfeature_names = X_train.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    print('{}: {}'.format(name, score))","f2bbf22d":"pd.concat([X_train,y_train],axis=1).to_csv(\"NBA_teams_train.csv\")\npd.concat([X_test,y_test],axis=1).to_csv(\"NBA_teams_test.csv\")","84a8b2a1":"pd.concat([X_train,y_train],axis=1)","f547005f":"If there are only 30 teams, aggregate features per team may help (there are \"only\" 20k games), but not by much.   (30 is not an extremely high cardinality).\n    * What might help are features per team that change over time, where there's a lot more variation!","d8cfeee1":"### if using catboost or lgbm, we'll define categorical variables & Pool\n\n* catboost hyperparam tuning : https:\/\/colab.research.google.com\/github\/catboost\/tutorials\/blob\/master\/python_tutorial.ipynb#scrollTo=nSteluuu_mif\n","af24d7ee":"* Load games data and remove columns that would be a leak for predicting the described game\n* A proper model would incorporate these features from previous games, e.g. teams which win by a large margin.\n    * I leave this as an exercise \"to the reader\" ;)\n    \n* As a lazy historical feature, we can take the rankings from the previous season! \n    * Requires us to \"shift\" and order by season - I may be doing this wrong (I have no idea how seasons work), so beware leaks!","aaba843c":"##### Temporally cross validated model : \n* Just on the train set. ","5efa97e6":"###### Feature importances\n* Can Look also at Shapley values : https:\/\/github.com\/slundberg\/shap\n    * catboost + shap has issues. .. \n        * e.g. https:\/\/github.com\/slundberg\/shap\/issues\/750\n        * tutoiral doesn't help - https:\/\/github.com\/slundberg\/shap\/blob\/master\/notebooks\/tree_explainer\/Catboost%20tutorial.ipynb\n        \n            * Likely caused due to categorical features + catboost and null splits : https:\/\/github.com\/slundberg\/shap\/issues\/757","df3f5943":"*  it looks like seasons aren't simply \"decade+number\" - making it trickier to add histoircal ones\n* Instead of merging by season_id , we'll merge by dates \"Before\", using pandas's asoiaf function","476a4a0b":"# ML Model\n* Split data by time\n* Build a machine learning model\n\n* Ideally we'd evaluate by probabilities (logloss) , to build a betting model. ","687763e9":"## Naive prediction of game wins, given only team data\n* A proper model would HAVE to have player data (accurate to the time of each game). \n* Even given more granular and exogenous data, we would expect this to be a very difficult task to predict on - there's a lot of luck involved. ","e0d5d517":"###### Export data for comparisons\/benchmarking","61d1c78a":"* we see rows with all 0 = missing values. We'll set their values to -1 , as a proxy for missingness.   (We can expect to theoretically see teams with 0 wins\/losses after all, although it's very unlikely)\n* We set 0s to nans for away or home \n\n* Since there are just 571 such games (likely the firs tones of each season, we could also simply drop them "}}