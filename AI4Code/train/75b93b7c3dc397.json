{"cell_type":{"22c1ef8b":"code","120f6c2f":"code","d19bb158":"code","ffff6981":"code","9b403e76":"code","f61bbb74":"code","01410370":"code","3a6a8836":"code","02d1eeb3":"code","e16a6dd6":"code","83f88a36":"code","3b6eca3a":"code","c37f76d9":"code","94165b07":"code","b2e7abc1":"code","7c5571f5":"code","30bf0d48":"code","9fbb74b1":"code","2f6e6b73":"code","b5c312cd":"code","9a2233f5":"code","c5646b84":"code","3373b218":"code","63ad01db":"code","66ca5081":"code","887770bb":"code","a21cef09":"code","6c201203":"code","7702b127":"markdown","bbac84a1":"markdown","a19fa805":"markdown","7e53477a":"markdown","be09dcb4":"markdown","aef5c638":"markdown","a37a1471":"markdown","5340fa5f":"markdown","80802ec2":"markdown"},"source":{"22c1ef8b":"# Lets start\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","120f6c2f":"# Install the Library (Refer: https:\/\/pypi.org\/project\/kesh-utils\/ )\n!pip install kesh-utils","d19bb158":"!pip install statsmodels==0.10.0rc2 --pre  # Statsmodel has sme problem with factorial in latest lib","ffff6981":"# Ignore the warnings if any\nimport warnings  \nwarnings.filterwarnings('ignore')","9b403e76":"# Load the dataset \nconsolidated_proba_df = pd.read_csv('..\/input\/consolidated_proba.csv')","f61bbb74":"# Quick check the consolidate dataset\nconsolidated_proba_df.head(10)","01410370":"# Load the util\nfrom KUtils.common import blend_util","3a6a8836":"# Let run and find best weighted avg coefficients for columns 'gnb_proba', 'dt_le_proba', 'rf_proba', 'xgb_proba'\n\nbest_blend_df = blend_util.find_best_stacking_blend(\n    consolidated_proba_df, \n    actual_target_column_name='Actual', \n    columns_to_blend=['gnb_proba', 'dt_le_proba', 'rf_proba', 'xgb_proba'],\n    starting_weight=1,\n    max_weight=10,\n    step_weight=1,\n    minimize_loss='rmse', # other option mae\n    verbose=False\n)","02d1eeb3":"# The best blend df gets appended as and when new best options are found. So all best blend options are at the end of the dataframe (Sort it if you want)\nbest_blend_df.tail()","e16a6dd6":"# Create new probablity based new model blend coefficients. Total weight 18\nconsolidated_proba_df['final_blended_proba'] = (consolidated_proba_df['gnb_proba']*1 + \n                                                consolidated_proba_df['dt_le_proba']*1 + consolidated_proba_df['rf_proba']*10 + consolidated_proba_df['xgb_proba']*6)\/18","83f88a36":"consolidated_proba_df.head(10)","3b6eca3a":"import matplotlib.pyplot as plt\nfrom KUtils.logistic_regression import auto_logistic_regression as autoglm\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer, recall_score, precision_score","c37f76d9":"# First(0) column contains prob for 0-class and second(1) contains prob for 1-class\ngnb_pred_df = pd.DataFrame({'Actual':consolidated_proba_df['Actual'], 'Probability':consolidated_proba_df['gnb_proba']}) \nreturn_dictionary = autoglm.calculateGLMKpis(gnb_pred_df, cutoff_by='Sensitivity-Specificity', include_cutoff_df_in_return=True)\ncutoff_df = return_dictionary['cutoff_df']","94165b07":"cutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificity'])\nplt.show()","b2e7abc1":"cutoff_df.plot.line(x='Probability', y=['Precision','Recall'])\nplt.show()","7c5571f5":"prob_column='gnb_proba'\nprob_cutoff = 0.04\nconsolidated_proba_df['predicted'] = consolidated_proba_df[prob_column].map(lambda x: 1 if x > prob_cutoff else 0)\n\nlocal_confusion_matrix = metrics.confusion_matrix(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'] )\n        \naccuracy = metrics.accuracy_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nprecision = metrics.precision_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nrecall = metrics.recall_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nf1_score = metrics.f1_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nroc_auc = metrics.roc_auc_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\n\nprint(\" Accuracy {0:.3f}, \\n Precision {1:.3f}, \\n Recall {2:.3f}, \\n f1_score {3:.3f}, \\n roc_auc {4:.3f}\".format(\n    accuracy, precision,recall,f1_score,roc_auc))","30bf0d48":"# First(0) column contains prob for 0-class and second(1) contains prob for 1-class\nrf_pred_df = pd.DataFrame({'Actual':consolidated_proba_df['Actual'], 'Probability':consolidated_proba_df['rf_proba']}) \nreturn_dictionary = autoglm.calculateGLMKpis(rf_pred_df, cutoff_by='Sensitivity-Specificity', include_cutoff_df_in_return=True)\ncutoff_df = return_dictionary['cutoff_df']","9fbb74b1":"cutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificity'])\nplt.show()","2f6e6b73":"cutoff_df.plot.line(x='Probability', y=['Precision','Recall'])\nplt.show()","b5c312cd":"prob_column='rf_proba'\nprob_cutoff = 0.3\nconsolidated_proba_df['predicted'] = consolidated_proba_df[prob_column].map(lambda x: 1 if x > prob_cutoff else 0)\n\nlocal_confusion_matrix = metrics.confusion_matrix(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'] )\n        \naccuracy = metrics.accuracy_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nprecision = metrics.precision_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nrecall = metrics.recall_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nf1_score = metrics.f1_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nroc_auc = metrics.roc_auc_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\n\nprint(\" Accuracy {0:.3f}, \\n Precision {1:.3f}, \\n Recall {2:.3f}, \\n f1_score {3:.3f}, \\n roc_auc {4:.3f}\".format(\n    accuracy, precision,recall,f1_score,roc_auc))","9a2233f5":"# First(0) column contains prob for 0-class and second(1) contains prob for 1-class\nxgb_pred_df = pd.DataFrame({'Actual':consolidated_proba_df['Actual'], 'Probability':consolidated_proba_df['xgb_proba']}) \nreturn_dictionary = autoglm.calculateGLMKpis(xgb_pred_df, cutoff_by='Sensitivity-Specificity', include_cutoff_df_in_return=True)\ncutoff_df = return_dictionary['cutoff_df']","c5646b84":"cutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificity'])\nplt.show()","3373b218":"cutoff_df.plot.line(x='Probability', y=['Precision','Recall'])\nplt.show()","63ad01db":"prob_column='xgb_proba'\nprob_cutoff = 0.2\nconsolidated_proba_df['predicted'] = consolidated_proba_df[prob_column].map(lambda x: 1 if x > prob_cutoff else 0)\n\nlocal_confusion_matrix = metrics.confusion_matrix(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'] )\n        \naccuracy = metrics.accuracy_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nprecision = metrics.precision_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nrecall = metrics.recall_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nf1_score = metrics.f1_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nroc_auc = metrics.roc_auc_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\n\nprint(\" Accuracy {0:.3f}, \\n Precision {1:.3f}, \\n Recall {2:.3f}, \\n f1_score {3:.3f}, \\n roc_auc {4:.3f}\".format(\n    accuracy, precision,recall,f1_score,roc_auc))","66ca5081":"# First(0) column contains prob for 0-class and second(1) contains prob for 1-class\nfinal_blend_pred_df = pd.DataFrame({'Actual':consolidated_proba_df['Actual'], 'Probability':consolidated_proba_df['final_blended_proba']}) \nreturn_dictionary = autoglm.calculateGLMKpis(final_blend_pred_df, cutoff_by='Sensitivity-Specificity', include_cutoff_df_in_return=True)\ncutoff_df = return_dictionary['cutoff_df']","887770bb":"cutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificity'])\nplt.show()","a21cef09":"cutoff_df.plot.line(x='Probability', y=['Precision','Recall'])\nplt.show()","6c201203":"prob_column='final_blended_proba'\nprob_cutoff = 0.28\nconsolidated_proba_df['predicted'] = consolidated_proba_df[prob_column].map(lambda x: 1 if x > prob_cutoff else 0)\n\nlocal_confusion_matrix = metrics.confusion_matrix(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'] )\n        \naccuracy = metrics.accuracy_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nprecision = metrics.precision_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nrecall = metrics.recall_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nf1_score = metrics.f1_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\nroc_auc = metrics.roc_auc_score(consolidated_proba_df['Actual'], consolidated_proba_df['predicted'])\n\nprint(\" Accuracy {0:.3f}, \\n Precision {1:.3f}, \\n Recall {2:.3f}, \\n f1_score {3:.3f}, \\n roc_auc {4:.3f}\".format(\n    accuracy, precision,recall,f1_score,roc_auc))","7702b127":"# An experiment to find Best Weighted avg ensemble model weights coefficient using Brute Force (Generic way)\n\n#### During multi model we come across a situation to use mix of model to offset the weakness of one model over other using stacking. For this we need right blend of coefficients for each model.\n#### It is a tedious job to find those blending coefficients. \n### One way to generate them using Brute force technic (Try all combination).\n\n#### For this I developed a generic utility whihc takes consolidated columns and few other parameter to go thru all possible combination and list best weighted coefficients.\n\nSource code available [here](https:\/\/github.com\/KeshavShetty\/kesh-utils\/tree\/master\/KUtils\/common) and PyPi package [here](https:\/\/pypi.org\/project\/kesh-utils\/)\n\nThe generic method is KUtils.common.blend_util() with arguments\n- df # The consilidated dataframe\n- actual_target_column_name, \n- columns_to_blend - # List of columns to use while blend, - Not fixed you can use any combination like 2, 3,... n columns which exist in consolidated data\n- starting_weight=1,\n- max_weight=10,\n- step_weight=1,\n- minimize_loss='rmse', # other option 'mae'\n- verbose=False \n\nThe parameter <b><u>starting_weight=1, max_weight=10, step_weight=1 acts something like range(1,10,10)<\/u><\/b> including upper limit.\nUtility will go thru all possible combinations and calculate MAE or RMSE between new blend probabability vs actual target\n\n> For this demo I used a consolidated probability dataset which I generated for some other binary classification which contains below columns\n\n- row_id # Unique rowid or original dataset\n- Actual # Actual target column from original datset (0,1)\n- gnb_proba # Probability generated from Gaussian Naive Bayes \n- dt_le_proba # Probability generated from Decision Tree\n- rf_proba \t# Probability generated thru Random Forest\n- xgb_proba # Probability generated thru XGBoost","bbac84a1":"### Plots of Gaussian Naive Bayes","a19fa805":"# Looking at RMSE you can say 16th row will have lowest error and coeeficients found are\n- for gnb_proba 1\n- for dt_le_proba 1\n- for rf_proba 10\n- for xgb_proba 6","7e53477a":"### Plots of Random Forest","be09dcb4":"### Plots of XGB","aef5c638":"### Plots of new blend","a37a1471":"### With blending we achived better Accuracy as well as other KPis like F1 and ROC improved\n\n#### You can experiement with different combinations for parameter columns_to_blend\n\n### Upvote if you liked the Kernel. Leave comments if any. ","5340fa5f":"# Extra\n### Lets check the efficiency of blending over individual model using Sensitivity-Specificity Cutoff","80802ec2":"# In action"}}