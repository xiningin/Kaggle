{"cell_type":{"9b207a83":"code","72b05f4f":"code","fc186c86":"code","25ed77bf":"code","f7e0695e":"code","cc184960":"code","94d5075e":"code","a7a55a32":"code","de25f21e":"code","f616281a":"code","5eaecba6":"code","ecf23e2e":"code","3db6a255":"code","1bb5b502":"code","e1effc4f":"code","5f41c415":"code","9a829bee":"code","df939b0e":"code","752f5f43":"code","9b47b2dc":"code","cbcd507f":"code","a5c78a9d":"code","06a5ed83":"code","36894306":"code","21dd1584":"code","bd2dbc63":"code","2bf514d6":"code","a6c53efa":"code","ad42d769":"code","9f32a8d5":"code","744dd864":"code","e668959d":"code","9be49402":"code","b3d59fcd":"code","be14f1db":"code","f97fe4aa":"code","9083e949":"code","b34bffbf":"code","7d7f3e26":"code","2002cbb5":"code","04135974":"code","ca4f9802":"code","9d67cd1b":"code","adcec817":"code","d8bf10ad":"code","c5afe6c9":"code","fb548ca7":"code","5c36a815":"code","67d1586d":"markdown","3803d554":"markdown","a9ec14c4":"markdown","fbc5095d":"markdown","7ebca375":"markdown","d30f3143":"markdown","39d8b872":"markdown","a4325b10":"markdown","e42949b1":"markdown","e78f2ba9":"markdown","1869d09b":"markdown","ab6cde33":"markdown","23eb77d6":"markdown","daeeb663":"markdown","e4bac738":"markdown","759faa3b":"markdown","a0749893":"markdown","68c465af":"markdown","7e2fcd21":"markdown","593d0e93":"markdown","d24be652":"markdown","524380bf":"markdown","9b048833":"markdown","f278e9ed":"markdown","31ac4e3b":"markdown","d3f32a18":"markdown","7a40a59f":"markdown","3c5e94a7":"markdown","869ef8b9":"markdown","8ad0e52e":"markdown","029ee0c7":"markdown","c1a073c6":"markdown","f196dabb":"markdown","a76d37aa":"markdown","926e13d0":"markdown","ad2312d9":"markdown","313f6ab9":"markdown","f7ab6c59":"markdown","f2e8a1c6":"markdown","6e3dd56d":"markdown","254824c6":"markdown","87739a0e":"markdown","e8c2a071":"markdown","6d9d9d2f":"markdown","55176204":"markdown","494f00a2":"markdown","bffaed1b":"markdown","65dbbafe":"markdown","580cc8a2":"markdown","56a3abf4":"markdown","155e5690":"markdown","ebf05c9b":"markdown","1f9abeb3":"markdown","c730e88a":"markdown","a400f739":"markdown"},"source":{"9b207a83":"import numpy as np                # linear algebra\nimport pandas as pd               # data frames\nimport seaborn as sns             # visualizations\nimport matplotlib.pyplot as plt   # visualizations\n%matplotlib inline\nimport scipy.stats                # statistics\nfrom sklearn import preprocessing\n\nimport os\nprint(os.listdir(\"..\/input\"))","72b05f4f":"# Read the data and store them in two objects\n\nred = pd.read_csv('..\/input\/winequality-red.csv')\nwhite = pd.read_csv('..\/input\/winequality-white.csv')","fc186c86":"# Explore dimension of the datasets\n\nprint(\"There are {} red wines with {} attributes in red dataset. \\n\".format(red.shape[0],red.shape[1]))\nprint(\"There are {} red wines with {} attributes in white dataset. \\n\".format(white.shape[0],white.shape[1]))","25ed77bf":"# Let\u00b4s create a new variable that let us know if the wine is red(1) or white(0)\nred['red']=1\nwhite['red']=0\n\n# Union of both datasets\nwines = pd.concat([red,white])","f7e0695e":"wines.head()","cc184960":"wines.shape","94d5075e":"# Let's see if we have duplicated records\n\ntwice = wines[wines.duplicated()]\ntwice.shape","a7a55a32":"twice.head()","de25f21e":"sns.countplot(x=\"red\", data=twice, palette=\"husl\")","f616281a":"pd.DataFrame(twice['red'].value_counts(dropna=False)).head()","5eaecba6":"pd.DataFrame(wines['red'].value_counts(dropna=False)).head()","ecf23e2e":"wine = wines.drop_duplicates(keep='first')\nwine.shape","3db6a255":"sns.countplot(x=\"red\", data=wine, palette=\"RdPu\")","1bb5b502":"wine.describe()","e1effc4f":"plt.figure(figsize=(15,10))\nsns.boxplot(data=wine.drop(columns=['red']), orient='horizontal', palette='RdPu')","5f41c415":"# Scaling the continuos variables\nwine_scale = wine.copy()\nscaler = preprocessing.StandardScaler()\ncolumns = wine.columns[0:12]\nwine_scale[columns] = scaler.fit_transform(wine_scale[columns])\nwine_scale.head()","9a829bee":"plt.figure(figsize=(15,10))\nsns.boxplot(data=wine_scale.drop(columns=['red']), orient='horizontal', palette='RdPu')","df939b0e":"g = sns.PairGrid(wine_scale.iloc[:,1:13], hue=\"red\", palette=\"RdPu\")\ng.map(plt.scatter);","752f5f43":"# Compute the correlation matrix\ncorr=wine_scale.iloc[:,1:13].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, annot=True, cmap='RdPu', center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","9b47b2dc":"sns.jointplot(x=\"total sulfur dioxide\", y=\"free sulfur dioxide\", data=wine, color='c')","cbcd507f":"sample = np.random.choice(wine_scale.index, size=int(len(wine_scale)*0.8), replace=False)\ntrain_data, test_data = wine_scale.iloc[sample], wine_scale.drop(sample)\n\nprint(\"Number of training samples is\", len(train_data))\nprint(\"Number of testing samples is\", len(test_data))\nprint(train_data[:5])\nprint(test_data[:5])","a5c78a9d":"f, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\nsns.countplot(x=\"red\", data=wine_scale, palette=\"RdPu\", ax=axes[0])\nsns.countplot(x=\"red\", data=train_data, palette=\"RdPu\", ax=axes[1])\nsns.countplot(x=\"red\", data=test_data, palette=\"RdPu\", ax=axes[2])","06a5ed83":"features = train_data.drop('red', axis=1)\ntargets = train_data['red']\nfeatures_test = test_data.drop('red', axis=1)\ntargets_test = test_data['red']","36894306":"from keras import models\nfrom keras import layers\n\n# Building the model\nNnetwork = models.Sequential()\nNnetwork.add(layers.Dense(40, activation='sigmoid', input_shape=(12,)))\nNnetwork.add(layers.Dense(1, activation='sigmoid'))","21dd1584":"# Compiling the model\nNnetwork.compile(loss = 'binary_crossentropy',\n                 optimizer='sgd',\n                 metrics=['accuracy'])\nNnetwork.summary()","bd2dbc63":"# Training the model\nNnetwork.fit(features, targets, epochs=10, batch_size=100, verbose=0)","2bf514d6":"test_loss, test_acc = Nnetwork.evaluate(features_test, targets_test)","a6c53efa":"print('test_acc:', test_acc, '\\ntest_loss:', test_loss)","ad42d769":"train_loss, train_acc = Nnetwork.evaluate(features, targets)","9f32a8d5":"print('train_acc:', train_acc, '\\ntrain_loss:', train_loss)","744dd864":"from sklearn.ensemble import RandomForestClassifier","e668959d":"Rforest = RandomForestClassifier(max_depth=4, n_estimators=10, max_features=2)","9be49402":"Rforest.fit(features, targets)","b3d59fcd":"# Accuracy\nscore = Rforest.score(features_test, targets_test)","be14f1db":"print(score)","f97fe4aa":"# Accuracy\nscore_train = Rforest.score(features, targets)","9083e949":"print(score_train)","b34bffbf":"### Predictions\ny_pred_rf = Rforest.predict(features_test)","7d7f3e26":"### Probabilities\ny_prob_rf = Rforest.predict_proba(features_test)\ny_prob_rf = y_prob_rf.T[1]","2002cbb5":"from sklearn import metrics\n# measure confusion matrix\ncm_rf = metrics.confusion_matrix(targets_test, y_pred_rf, labels=[0, 1])\ncm_rf = cm_rf.astype('float')\ncm_rf_norm = cm_rf \/ cm_rf.sum(axis=1)[:, np.newaxis]\nprint(\"True Positive (rate): \", cm_rf[1,1], \"({0:0.4f})\".format(cm_rf_norm[1,1]))\nprint(\"True Negative (rate): \", cm_rf[0,0], \"({0:0.4f})\".format(cm_rf_norm[0,0]))\nprint(\"False Positive (rate):\", cm_rf[1,0], \"({0:0.4f})\".format(cm_rf_norm[1,0]))\nprint(\"False Negative (rate):\", cm_rf[0,1], \"({0:0.4f})\".format(cm_rf_norm[0,1]))","04135974":"np.shape(y_prob_rf)","ca4f9802":"fpr, tpr, thresholds = metrics.roc_curve(targets_test, y_pred_rf)","9d67cd1b":"# measure Area Under Curve (AUC)\nauc_rf = metrics.roc_auc_score(targets_test, y_pred_rf)\nprint()\nprint(\"AUC:\", auc_rf)","adcec817":"# ------------------------------------------------------------------------------\n# Plot: Receiver-Operator Curve (ROC)\n# ------------------------------------------------------------------------------\n\nfig, axis1 = plt.subplots(figsize=(8,8))\nplt.plot(fpr, tpr, 'r-', label='ROC')\nplt.plot([0,1], [0,1], 'k--', label=\"1-1\")\nplt.title(\"Receiver Operator Characteristic (ROC)\")\nplt.xlabel(\"False positive (1 - Specificity)\")\nplt.ylabel(\"True positive (selectivity)\")\nplt.legend(loc='lower right')\nplt.tight_layout()","d8bf10ad":"# Accuracy of Random Forest\ntest_loss, test_acc_nn = Nnetwork.evaluate(features_test, targets_test)\nprint('Accuracy of NN in test data:', test_acc, '\\ntest_loss:', test_loss)","c5afe6c9":"# Accuracy of Random Forest\nscore = Rforest.score(features_test, targets_test)\nprint(score)","fb548ca7":"importances = Rforest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in Rforest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]","5c36a815":"# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in zip(features.columns, Rforest.feature_importances_):\n    print(f)\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(features.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(features.shape[1]), indices)\nplt.xlim([-1, features.shape[1]])\nplt.show()","67d1586d":"### Scatter Plot","3803d554":"The method to scale the data will be **Standardization**. Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. **Gaussian with 0 mean and unit variance**).","a9ec14c4":"## Balance of the Target Variable","fbc5095d":"This is the accuracy of our Random Forest in test data:","7ebca375":"This is the accuracy of our Neural Network in test data:","d30f3143":"Let's get rid off the duplicated ones, now we have these dimensions:","39d8b872":"# Reading the Data","a4325b10":"### Correlation Matrix","e42949b1":"# Feature importance","e78f2ba9":"Our **test set accuracy** turns out to be quite a bit higher than the **training set accuracy**. This gap between training accuracy and test accuracy is good and it seems that we are **avoiding \"overfitting\"** which is the fact that machine learning models tend to perform worse on new data than on their training data.","1869d09b":"We can rank the features according to how much each feature was used to split the dataset while training. This is a measure of their importance, i.e, of how much each feature contributes to successfully isolate pure partitions.","ab6cde33":"# Cleaning the Data","23eb77d6":"There is an interesting thing that Random Forest let us do and is to know which features are more important to predict the target variable. Let's take a look at this:","daeeb663":"# Accuracy of Neural Networks vs Random Forest","e4bac738":"Let\u00b4s append both datasets (**Red** and **White**), after that a new variable was created called **\"Red\"** that will let us know if the wine is **red(1)** or **white(0)**","759faa3b":"Some relevant hyper-parameters for the random forest:\n\n* **max_depth**: max_depth represents the depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information about the data.\n\n* **n_estimators**: the number of decision trees used.\n\n* **max_features**: The number of features (predictor variables) that the model will randomly consider when looking for the best split","a0749893":"# Explore the Data","68c465af":"# Correlation between features","7e2fcd21":"This shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.\n\nAs expected from the correlation matrix, the plot suggests that the 3 most informative features are:\n\n* **chlorides**\n* **total sulfur dioxide**\n* **volatile acidity**\n\nThose three features explains an important part of the wine's colors.","593d0e93":"## Compilation","d24be652":"# Random Forest","524380bf":"They are distributed like this:","9b048833":"We can see from the boxplot above that their range of values varies from one variable to another, so we will need to scale their values to enhance the maipulation of this data.","f278e9ed":"To make our network ready for training, we need to pick three more things, as part of the \"compilation\" step:\n\n*     A **loss function**: this is how the network will be able to measure how good a job it's doing on its training data, and thus how it will be able to steer itself in the right direction.\n\n*     An **optimizer**: this is the mechanism through which the network will update itself based on the data it sees and its loss function.\n                        sgd = stochastic gradient descent\n\n*     Some **metrics** to monitor during training and testing. Here we will only care about accuracy (the fraction of the wines that were correctly classified).","31ac4e3b":"We had better accuracy in our test set than in our training set which is very very good!","d3f32a18":"We can see here that training dataset is the more balanced of the three which is good and the test dataset is the more unbalanced one, so this will be challenging for our models.","7a40a59f":"* Predict the colors of wines (Red or White)\n* Create a Neural Network with Keras\n* Create a Random Forest\n* Compare both models\n* Obtain Feature importace","3c5e94a7":"The relationship between \"**free sulfur dioxide**\" and \"**total sulfur dioxide**\"shows heteroscedasticity and the one that is more related with our objective variable is \"**total sulfur dioxide**\"","869ef8b9":"# Libraries","8ad0e52e":"Here our network consists of a sequence (**Sequential**) of **two Dense layers**, which are densely-connected or fully-connected neural layers. The second (and last) layer is a 2-way \"sigmoid\" layer, which means it will return an array of 2 probability scores (summing to 1). Each score will be the probability that the current wine belongs to Red Wines of our two wine classes (Red and White).","029ee0c7":"Metrics of Random Forest are almost perfect!","c1a073c6":"Accuracy of the Neural Network:","f196dabb":"*Here we won't explore the Hyperparameter Tuning and we are going to keep this result*","a76d37aa":"Again, our **test set accuracy** turns out to be a bit higher than the **training set accuracy**. This gap between training accuracy and test accuracy is good and it seems that we are **avoiding \"overfitting\"**.","926e13d0":"## Remove Duplicates","ad2312d9":"We are now ready to train our network, which in Keras is done via a call to the fit method of the network: we \"fit\" the model to its training data.","313f6ab9":"# Objectives","f7ab6c59":"# Neural Networks With Keras","f2e8a1c6":"# * And the winner is... RANDOM FOREST!!!*","6e3dd56d":"There are **1177 duplicated** records:","254824c6":"Both Models have presented a superior performance but we can see that the **Accuracy of the Random Forest is almost 3% bigger** than the Accuracy of the Neural Network.","87739a0e":"# Training and Test Datasets","e8c2a071":"Our workflow will be as follow: first we will present our neural network with the training data, **features** and **targets**. The network will then learn to associate **features** and **targets**. Finally, we will ask the network to produce predictions for **features_test**, and we will verify if these predictions match the labels from **targets_test**.\n\nLet's build our network:","6d9d9d2f":"This is the accuracy of our Random Forest in training data:","55176204":"We have two separate datasets **Red Wines** and **White Wines** and these are their dimensions:","494f00a2":"These are the main statistics of the variables:","bffaed1b":"I'm going to leave both variables anyway hoping that the Neural Network and the Random Forest could be able to manage the previous findings.","65dbbafe":"## Let's see the performance in Train and Test to evaluate the possibility of Overfitting","580cc8a2":"Splitting the data into **80%** for **training** and **20%** for **test**. Here is the representation of the target variable in the **Original Dataset**, **Training Dataset** and **Test**:","56a3abf4":"# Scale the data","155e5690":"Let's take a look of their dispersion.","ebf05c9b":"## More Performance Metrics from Random Forest","1f9abeb3":"* The variables \"free sulfur dioxide\" and\t\"total sulfur dioxide\" have a strong correlation between them.\n\n* We can see that the features more related with the wine's color are **volatile acidity**, **total sulfur dioxide** and **chlorides**","c730e88a":"Accuracy of the Random Forest:","a400f739":"This is the accuracy of our Neural Network in Train data:"}}