{"cell_type":{"1ba88d0f":"code","98380f70":"code","4723414b":"code","e298251c":"code","6313f8f9":"code","7050ebec":"code","692eef40":"code","e5679438":"code","74d412e9":"code","372be9cc":"code","b253906c":"code","b685fad8":"code","d74cb9b2":"code","e30cb3b6":"code","4158b3b4":"code","55516605":"code","e900e383":"code","24b8d1b5":"code","d71f3ddf":"code","7df03809":"code","be1fe0c4":"code","0bc1ed92":"code","f4773154":"code","7de4bc28":"code","751030e7":"code","9336e0ec":"code","a48313c7":"code","bad0a1d2":"code","2425bfea":"code","874bdf15":"code","4e3fa8da":"code","b2f2b15d":"code","80fe9665":"code","c902d1f1":"code","33c125f5":"code","d13c4cfb":"code","519bfb3e":"code","9dba0a0a":"code","1911f00c":"markdown","f5eb83f4":"markdown","4882ca29":"markdown","ce63c8f1":"markdown","8fd7f8be":"markdown","31458e39":"markdown","0be8488c":"markdown","e06f592d":"markdown","c7f51b7f":"markdown","a53ac96f":"markdown","3616fb55":"markdown","3b411250":"markdown","441bea89":"markdown","728855f0":"markdown","a8b08e8e":"markdown","d033245b":"markdown","7659d378":"markdown","97b0348f":"markdown","d8a50efd":"markdown","d6eab370":"markdown","4d0a8a43":"markdown","ae5e140f":"markdown","def10681":"markdown","0b60523e":"markdown","fde45336":"markdown","507f4d7a":"markdown","f13d4f50":"markdown","06dc33cf":"markdown","22ff18d1":"markdown","a8220b7f":"markdown","9cb6b4e7":"markdown","650d1d4a":"markdown","e103453c":"markdown","ed7d3a22":"markdown","f072d1d6":"markdown","02a2740c":"markdown","0dbcf4bc":"markdown","f4aa4a1b":"markdown","1e3c8425":"markdown","7f339746":"markdown","e080770e":"markdown","eb944aae":"markdown","7750b466":"markdown","030fb290":"markdown","bc13c43b":"markdown","462b42de":"markdown","0f9b941f":"markdown","62e129b8":"markdown","26d7dc2c":"markdown"},"source":{"1ba88d0f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","98380f70":"## Read file\nraw_data = pd.read_csv(\"..\/input\/google-play-store-apps\/googleplaystore.csv\")\nprint('Shape of the google playstore data',raw_data.shape)\nraw_data.head()","4723414b":"total = raw_data.isnull().sum().sort_values(ascending=False)\npercent = 100*(raw_data.isnull().sum()\/raw_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent (%)'])\n\nprint(missing_data)","e298251c":"raw_data.dropna(inplace=True)\nprint('Shape of the google playstore data',raw_data.shape)","6313f8f9":"duplicate_sum = raw_data.duplicated().sum()\npercentage = 100 * (duplicate_sum \/ len(raw_data.index))\n\nprint('The dataset contains {} duplicate row that represent {}% of overall dataset '.format(duplicate_sum, percentage))\nraw_data=raw_data.drop_duplicates()\nprint('Shape of the google playstore data',raw_data.shape)","7050ebec":"import seaborn as sns # used for plot interactive graph.\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","692eef40":"data_visual = raw_data.copy()\ndata_visual.describe(include='all')","e5679438":"plt.figure(figsize=(10,10))\ng = sns.countplot(y=\"Category\",data=data_visual, palette = \"Set2\")\nplt.title('Total apps of each Category',size = 20)","74d412e9":"data_visual['Installs'] = data_visual['Installs'].str.replace(r'\\D','').astype(int)\n\nplt.figure(figsize=(10,10))\ng = sns.barplot(x=\"Installs\", y=\"Category\", data=data_visual, capsize=.3)\nplt.title('Installations in each Category',size = 20)","372be9cc":"data_visual[data_visual.Installs == 1000000000].head()","b253906c":"from sklearn import preprocessing\n\ndef encode_feature(data_feature):\n    le = preprocessing.LabelEncoder()\n    new_label = le.fit_transform(data_feature)\n    return new_label","b685fad8":"def preprocessor_data(data):\n    data.drop(labels = ['App', 'Current Ver','Android Ver'], axis = 1, inplace = True)\n    \n    data['Last Updated']= pd.to_datetime(data['Last Updated'], format='%B %d, %Y')\n    data['Installs'] = data['Installs'].str.replace(r'\\D','').astype(int)\n    data['Price']=data['Price'].str.replace('$','').astype(float)\n\n    data['Reviews']=data['Reviews'].astype(int)\n    data['Rating']=data['Rating'].astype(float)\n\n    data['Type'] = encode_feature(data['Type'])\n    data['Category'] = encode_feature(data['Category'])\n    data['Content Rating'] = encode_feature(data['Content Rating'])\n    \n    data['Size'].replace('Varies with device', np.nan, inplace = True ) \n    data['Size']= (data['Size'].replace(r'[kM]+$', '', regex=True).astype(float) * \\\n             data['Size'].str.extract(r'[\\d\\.]+([KM]+)', expand=False)\n            .fillna(1)\n            .replace(['k','M'], [10**3, 10**6]).astype(int))\n    data['Size'].fillna(data.groupby('Category')['Size'].transform('mean'),inplace = True)\n    data['Size'] =(data['Size']-data['Size'].min())\/(data['Size'].max()-data['Size'].min())\n\n    data2 = data['Genres'].str.get_dummies(sep=';').rename(lambda x: 'Genres_' + x, axis='columns')\n    data = pd.concat([data,data2],axis=1)\n    data.drop(labels = ['Genres'], axis = 1, inplace = True)\n    \n    data.loc[data['Installs'] < 5000, 'Installs'] = 0 \n    data.loc[(data['Installs'] >= 5000) & (data['Installs'] < 1000000), 'Installs'] = 1 \n    data.loc[data['Installs'] >= 1000000, 'Installs'] = 2\n    \n    data = data.sort_values(by=['Last Updated'])\n    data.drop(labels = ['Last Updated'], axis = 1, inplace = True)\n\n    return data","d74cb9b2":"#dropping of unrelated and unnecessary items\npreprocessed_data = preprocessor_data(raw_data.copy())\nprint('Shape after preprocessed_data data',preprocessed_data.shape)\npreprocessed_data.tail()","e30cb3b6":"preprocessed_data['Installs'].value_counts().sort_index()","4158b3b4":"from sklearn.model_selection import train_test_split \n\ntrain_label = preprocessed_data['Installs']\ntrain_data = preprocessed_data.drop('Installs', axis=1)\n\n# TODO: Split the data into training and testing sets(0.25) using the given feature as the target\nX_train, X_test, y_train, y_test = train_test_split(train_data, train_label, test_size=0.25, shuffle=False)\n\nprint('X_train Shape : {}, y_train Shape : {}'.format(X_train.shape, y_train.shape))\nprint('X_test  Shape : {}, y_test  Shape : {}'.format(X_test.shape,  y_test.shape ))","55516605":"from sklearn.metrics import f1_score\n\ndef performance_metric(y_true, y_predict):\n    score = f1_score(y_true, y_predict, average='micro')\n    \n    return score","e900e383":"y_true = [0, 1, 2, 0, 1, 2]\ny_pred = [0, 2, 2, 0, 0, 1] # 3 labels out of 6 are correct (0%)\ny_pred_bad = [1, 2, 1, 2, 0, 1] # 0 labels out of 6 are correct (50%)\n\nright_score =  performance_metric(y_true, y_true)\nscore =  performance_metric(y_true, y_pred)\nbad_score =  performance_metric(y_true, y_pred_bad)  \n\n\nprint('bad score = ({}), best score = ({}), random score = ({}) '.format(bad_score, right_score, score))","24b8d1b5":"from time import time\n\ndef evaluate_classifier(cls, X_train, X_test, y_train, y_test):\n    \n    start_train = time() # Get start time\n    cls.fit(X_train,y_train)\n    end_train = time() # Get end time\n\n    start_test = time() # Get start time\n    y_train_pred = cls.predict(X_train)\n    y_test_pred = cls.predict(X_test)\n    end_test = time() # Get end time\n\n    y_train_score =  performance_metric(y_train, y_train_pred)\n    y_test_score =  performance_metric(y_test, y_test_pred)\n\n    train_time = end_train - start_train\n    test_time = end_test - start_test\n    \n    print('- Classifier[{}]\\n- Training f1-score = ({:.4f}) in {:.4f}s.\\n- Testing f1-score = ({:.4f}) in {:.4f}s.'.format(cls.__class__.__name__,y_train_score, train_time, y_test_score, test_time))","d71f3ddf":"from sklearn.linear_model import LogisticRegression\n\nclf_lr = LogisticRegression(multi_class='ovr', solver='liblinear')\nevaluate_classifier(clf_lr, X_train, X_test, y_train, y_test)","7df03809":"from sklearn.naive_bayes import GaussianNB\n\nclf_gnb = GaussianNB()\n\nevaluate_classifier(clf_gnb, X_train, X_test, y_train, y_test)","be1fe0c4":"from sklearn.tree import DecisionTreeClassifier\n\nclf_dtc = DecisionTreeClassifier(random_state=333)\n\nevaluate_classifier(clf_dtc, X_train, X_test, y_train, y_test)","0bc1ed92":"from sklearn.svm import SVC\n\nclf_svc = SVC(random_state=333, gamma='auto', kernel='rbf')\nevaluate_classifier(clf_svc, X_train, X_test, y_train, y_test)","f4773154":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf_gbc = GradientBoostingClassifier(random_state=333)\n\nevaluate_classifier(clf_gbc, X_train, X_test, y_train, y_test)","7de4bc28":"clf_scores_plt = pd.DataFrame({'Classifier':['LogisticRegression', 'GaussianNB', 'DecisionTree', 'SVC', 'GradientBoosting', 'LogisticRegression', 'GaussianNB', 'DecisionTree', 'SVC', 'GradientBoosting'],\n'Score':['Train Score' ,'Train Score' ,'Train Score' ,'Train Score' ,'Train Score' ,'Test Score' ,'Test Score' ,'Test Score' ,'Test Score' ,'Test Score'],\n'Values':[0.4203, 0.7375, 1.0000, 0.9359, 0.9253, 0.7385, 0.8339, 0.9032, 0.8969, 0.9383]})\n\nplt.figure(figsize=(8,8))\nsns.barplot(x=\"Classifier\",y=\"Values\", hue=\"Score\",data=clf_scores_plt)\nplt.title('F1-Score For Each Classifier',size = 20)","751030e7":"clf_time_plt = pd.DataFrame({'Classifier':['LogisticRegression', 'GaussianNB', 'DecisionTree', 'SVC', 'GradientBoosting', 'LogisticRegression', 'GaussianNB', 'DecisionTree', 'SVC', 'GradientBoosting'],\n'Time':['Train Time', 'Train Time', 'Train Time', 'Train Time', 'Train Time', 'Test Time', 'Test Time','Test Time','Test Time','Test Time'],\n'Seconds':[0.0602, 0.0118, 0.0718, 6.4302, 3.3559, 0.0050, 0.0121, 0.0064, 3.9732, 0.0442]})\n\nplt.figure(figsize=(8,8))\nax = sns.barplot(x=\"Classifier\",y=\"Seconds\", hue=\"Time\",data=clf_time_plt)\nplt.title('Train\/Test Time For Each Classifier',size = 20)","9336e0ec":"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics.scorer import make_scorer\n\ndef coss_validate_clf(cls, X_train, y_train):\n    fone_scorer = make_scorer(f1_score, average='micro')\n    scores = cross_validate(cls, X_train, y_train, cv=10,scoring=fone_scorer, return_train_score=True)\n    print('--------------Classifier [%s] ----------------'  % (cls.__class__.__name__))\n    print(\"- Train score: %0.2f (+\/- %0.2f)\" % (scores['train_score'].mean(), scores['train_score'].std() * 2))\n    print(\"- Test score: %0.2f (+\/- %0.2f)\" % (scores['test_score'].mean(), scores['test_score'].std() * 2))","a48313c7":"coss_validate_clf(clf_lr, X_train, y_train)\n\ncoss_validate_clf(clf_gnb, X_train, y_train)\ncoss_validate_clf(clf_dtc, X_train, y_train)\ncoss_validate_clf(clf_svc, X_train, y_train)\ncoss_validate_clf(clf_gbc, X_train, y_train)","bad0a1d2":"clf_cv_plt = pd.DataFrame({'Classifier': ['LogisticRegression', 'GaussianNB', 'DecisionTree', 'SVC', 'GradientBoosting', 'LogisticRegression', 'GaussianNB', 'DecisionTree', 'SVC', 'GradientBoosting'],\n'Score':['Train Score' ,'Train Score' ,'Train Score' ,'Train Score' ,'Train Score' ,'Test Score' ,'Test Score' ,'Test Score' ,'Test Score' ,'Test Score'],\n'Avr. Score':[0.55, 0.74, 1.00, 0.94, 0.93, 0.54, 0.74, 0.86, 0.83, 0.90]})\n\nplt.figure(figsize=(8,8))\nax = sns.barplot(x=\"Classifier\",y=\"Avr. Score\", hue=\"Score\",data=clf_cv_plt)\nplt.title('Average F1-Score For Cross-Validation For Each Classifier',size = 20)","2425bfea":"from sklearn.decomposition import PCA\n\ndef pca_init(pca_comp):\n    pca = PCA(n_components=pca_comp)\n    # TODO: Apply PCA by fitting the good data with the same number of dimensions as features\n    pca.fit(X_train)\n\n    x_train_pca = pca.transform(X_train)\n    x_test_pca = pca.transform(X_test)\n    print('-----------------[PCA Component=',pca_comp,']-----------------', )\n    print('x_train_pca Shape : {}, y_train Shape : {}'.format(x_train_pca.shape, y_train.shape))\n    print('x_test_pca  Shape : {}, y_test  Shape : {}'.format(x_test_pca.shape,  y_test.shape ))\n    \n    return pca,x_train_pca, x_test_pca","874bdf15":"pca_componnents = [20,25,30,35,40,45,50,55]\n\nfor pca_comp in pca_componnents:\n    pca, x_train_pca, x_test_pca = pca_init(pca_comp)\n    evaluate_classifier(clf_dtc, x_train_pca, x_test_pca, y_train, y_test)\n    evaluate_classifier(clf_gbc, x_train_pca, x_test_pca, y_train, y_test)","4e3fa8da":"clf_pca_plt = pd.DataFrame({'PCA-Component': [20,20,25,25,30,30,35,35,40,40,45,45,50,50,55,55],\n'Classifier':['DecisionTree' ,'GradientBoosting' ,'DecisionTree' ,'GradientBoosting' ,'DecisionTree' ,'GradientBoosting' ,'DecisionTree' ,'GradientBoosting' ,'DecisionTree' ,'GradientBoosting' ,'DecisionTree' ,'GradientBoosting' ,'DecisionTree' ,'GradientBoosting' ,'DecisionTree' ,'GradientBoosting'],\n'Score':[0.9118, 0.9383, 0.9127, 0.9392, 0.9100, 0.9370, 0.9032, 0.9383, 0.9050, 0.9370, 0.9014, 0.9379, 0.8992, 0.9370, 0.8983, 0.9374]})\n\nplt.figure(figsize=(8,8))\nax = sns.barplot(x=\"PCA-Component\",y=\"Score\", hue=\"Classifier\",data=clf_pca_plt)\nplt.title('Prediction F1-Score For Selected Classifiers Over Number Of Components',size = 20)","b2f2b15d":"best_comp_num = 25\npca,x_train_pca, x_test_pca = pca_init(best_comp_num)\nevaluate_classifier(clf_gbc, x_train_pca, x_test_pca, y_train, y_test)","80fe9665":"import random\n\nsample_num = 8\nrow_index = []\nfor x in range(sample_num):\n  row_index.append(random.randint(1,8887)) \n\nprint(row_index)","c902d1f1":"sample_data = raw_data.iloc[row_index]\nsample_data.head(n=sample_num)","33c125f5":"row_id = list(sample_data.index) \nprint(row_id)\nsample_preprocessed = preprocessed_data.loc[row_id]\nsample_preprocessed.head(n=sample_num)","d13c4cfb":"sample_result_actual = sample_preprocessed['Installs']\nsample_preprocessed.drop(labels=['Installs'], axis=1, inplace=True)","519bfb3e":"sample_pca = pca.transform(sample_preprocessed)\nsample_result = clf_gbc.predict(sample_pca)\nprint(sample_result)","9dba0a0a":"performance_metric(sample_result,sample_result_actual)","1911f00c":"# Machine Learning Engineer Nanodegree\n\n## Capstone Project\nMahmoud Helal, May 4th, 2019\n\n## Project: Predict Mobile App Success\n","f5eb83f4":"We will calculate performance via F1-Score method","4882ca29":"The mapped indices in pre-processing data are as shown","ce63c8f1":"The data above shows random Apps fall under different installs category:\n\n* 2 failed Apps (1000 install or below): BG BRIDAL GALLERY, Au Pair Legend.\n* 3 limited success App (between 1K and 1M): BN Pro Roboto-b Neon HD Text, BlackCam Pro - B&W Camera\t, AC Air condition Troubleshoot,Repair,Maintenance.\t\n* 3 success Apps (1M and more): NBA LIVE Mobile Basketball, CallApp: Caller ID, Blocker & Phone Call Recorder, Disney Magic Kingdoms: Build Your Own Magical ...","8fd7f8be":"Again _Gradient Boosting Classifier_ and  _Decision Tree Classifier_ proves there are most reliable classifiers with mean scores 0.90 and 0.86 respectively, the graph below shows average F1-Score for each classifier in the cross-validation phase.","31458e39":"As mentioned NBC considered fastest algorithm with least (and considered not bad) result, in other hand SVM considered the slowest algorithm in both training and prediction phases the graph below demonstrates Time consumed by each classifier.","0be8488c":"We will do some data cleaning, we need to check `NAN`s percentage in our dataset, so the following code calculates NANs percentage in each columns and sort them descendingly","e06f592d":"We will implement 'evaluate_classifier' method that will be used to evaluate our benchmark model and further implemented models","c7f51b7f":"We will first import graph libraries ","a53ac96f":"We will test this random data with our chosen kernel and observe the output ","3616fb55":"As shown from table above the most NANs exist in Rating column with 13.6%, sadly we think Rating column is an important feature that affect application success as more rating can leads to more number of installs unlike other columns\/features like Current version, Android Version,  Content Rating and Type","3b411250":"### 1. Datasets and Inputs\n\nThe dataset that will used is \"[Google Play Store Apps](https:\/\/www.kaggle.com\/lava18\/google-play-store-apps)\" from [Kaggle](https:\/\/www.kaggle.com\/), the dataset structured in CSV format in one file named \u201cgoogleplaystore\u201d with 10.8k rows and 13 columns each row indicates real mobile application in play store and columns represent features as follows : Application name, Category the app belongs to, Overall user rating, Number of user reviews, Size of the app, Number of user downloads\/installs, Type (Paid or Free), Price, Rating, Age group the app is targeted at (Children, Mature 21+, Adult), Genres, Last Updated, Current Version, Min required Android version.","441bea89":"### 3.\tData Pre-processing\nIn this section should demonstrate preprocessing data by removing noise (duplicates, NANs, outliers,...) however, this part done earlier just after importing dataset, to deal with clean data in data visualization section, however in this part we will prepare data for classification to enhance classifier accuracy and our kernel performance also will work on 'Installs' column as it considered our target labels","728855f0":"The table above gives us some insights about the dataset, for example, there are 8886 apps (after removing duplications and NANs records), there are 33 different categories and the most Apps under `FAMILY` category, rating column ranges from 1 to 5 and most dataset rating values tends to lay around (4.18 +\/- 0.5), Apps have two Types Free and Paid.","a8b08e8e":"The last step is to split the dataset into training and testing sets we will use 75% of data as a training set and 25% as testing set taking in consideration the newer data are in prediction as mentioned previously","d033245b":"### 2. Data Visualization\n\nIn this section we explore and visualize dataset, the graphical representation of data will allow us to better understand the dataset, data patterns and outliers can be detected using visual elements like charts and graphs. ","7659d378":"As shown in previous graph the largest number of apps under `FAMILY` category (above 1600 app) as mentioned earlier then `GAMES` (above 1000) and `TOOLS` (above 650), then other apps distributed among other categories.","97b0348f":"**Fourth** is _Gradient Boosting_ Ensemble Classifier and it's recommended from the article mentioned in the course \"If linear regression was a Toyota Camry, then gradient boosting would be a UH-60 Blackhawk Helicopter\" [SOURCE](http:\/\/blog.kaggle.com\/2017\/01\/23\/a-kaggle-master-explains-gradient-boosting\/), also author mentioned it used to win many Kaggle competitions.","d8a50efd":"### 8. Final Kernel\n\nOur final kernel will use PCA technique with 25 principal components and Gradient Boosting Classifier to detect Mobile App success","d6eab370":"The following graph will demonstrate the number of installations in each category this will represent the most (successful) category that mobile apps belong to","4d0a8a43":"Most of the applications are very known applications (Google Play Books, WhatsApp, Chrome, Gmail) and they are under the communication category (except Google Play Books)","ae5e140f":"#### Kernel Reliability\n\nWe will use cross-validation technique to test classifiers reliability, cross-validation divides the training data into k parts (k=10 in our implementation), then using 9 parts as a training dataset and test with the remaining part (validation dataset) then repeat this operation 10-times then the final result is the average score of 10 trails.","def10681":"## I) Introduction\n\nMost new applications and startups tend to fail; 90% of startups fail and the number one for failure is the lack of market need of their product, so any startup or company intend to develop a new mobile app needs to make certain their App fulfill a market need; A model can predict application success by classifying the estimated number of downloads will occur based on previous similar applications published on the App store with similar features can help most of startups and mobile development companies to take business decisions to proceed or alter their plans from earlier stages, save money and reduce risks and increase customer satisfaction.","0b60523e":"### 7. Enhancements\n\nIn this section we will try to enhance our kernel results, for now, we will try applying PCA before classification stage many articles mentioned it improve classification performance [\\[HERE\\]](https:\/\/towardsdatascience.com\/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32) , PCA is a technique for dimensionality reduction the main goal is to project data on the minimum number of principal components that represent the maximum number of variance of data, we will try a different number of dimensionality reduction components from 25 to 55 and observe output results from two classifiers (Decision Tree and Gradient Boosting) ","fde45336":"As expected Logistic Regression classifier is very fast classifier and gives acceptable results as prediction score = 0.73 however, we need to improve performance and reach 0.85 ~ 0.9 in order to build a good kernel.","507f4d7a":"**Third** is _Support Vector Machine_ classifier, SVM is considered a robust classifier that aims to find optimal hyperplane that divides feature space by finding the maximum distance between hyperplane and the closest features for all classes (margin), also SVM can be a non-linear classifier if non-linear kernel function is used, the following implementation will use default SVM values except for gamma and kernel type as it should mention for multiclass data","f13d4f50":"#### Results and observations\n\nAs shown from results above the top classifiers are Gradient Boosting Classifier with score = 0.938 and Decision Tree Classifier with score = 0.90, SVM was very close with score = 0.89 and although NBC is the last score = 0.83, but it considered the fastest training algorithm between the other four algorithms, overall the results reached our expectations as we hope to get results from 0.85~0.9, the graph below shows F1-Score result for each mentioned classifier.","06dc33cf":"As shown in previous graph the most installed apps under `COMMUNICATION` category even the number of apps wasn't high in this category (see the first graph) then the `SOCIAL` applications lets have a look into some applications that reached 1000M downloads","22ff18d1":"## II) Project Design\n\nThis project will consist of 8 main stages in order to build our kernel\n\n1. **Datasets and Inputs:** will import our dataset and remove noise (NANs, duplications, ...).\n2. **Data Visualization:** will explore and represent our dataset graphically in order to have some insights.\n3. **Data Pre-processing:** will Prepare our dataset for classification phase (removing not useful features, make one-hot-encoding,..)\n4. **Evaluation Metrics:** will identify our score method that used to evaluate our classifiers.\n5. **Benchmark:** will build Linear regression classifier as a benchmark model and get baseline performance values.\n6. **Algorithms and Techniques:** will explore more classifiers (Naive Bayes, Decision Trees, SVM, Gradient Boosting) and compare results with base kernel result then choose the best two classifiers to the enhancement stage.\n7. **Enhancements:** will try to use PCA for feature reduction to enhance classification results.\n8. **Final Kernel:** based on previous results we will choose the final classifier and enhancement tuning to our kernel.\n\nFinally, we will try to test our kernel with some random data and check if our kernel behaves as expected to solve our business problem or not and give suggestions for improvement.","a8220b7f":"## V) Improvement","9cb6b4e7":"The following graph will represent the number of applications per category that will indicate the category most applications belongs to in this dataset","650d1d4a":"## IV) Conclusion\nWe successfully implement a kernel for predicting number of installs range the App can obtain if uploaded to Google store given set of features App category, Genres, expected rating,... which indicate if the App will be successful, limited successful, failed, our kernel accuracy is 0.9415 which is high accuracy then we select 8 random Apps observing their features and run against our model and it predicting them all successfully.","e103453c":"Best score for Decision Tree Classifier was 0.9127 when was number of PCA component = 25 and best score for Gradient Boosting Classifier was 0.9392 when was number of PCA component = 25, both results are slightly increased however, we where expected better performance and lower time for training and prediction but we understand it will be very hard to enhance the results that already high (above 0.9), the graph below demonstrate F1-Score for prediction data for our selected classifiers over number of components ranges from 20 to 55","ed7d3a22":"We will just make a small test for 'performance_metric' method to make sure it will behave as we expected.","f072d1d6":"### 4. Evaluation Metrics\n\nThe main evaluation method that will be used to measure kernel performance is f beta score with beta = 1; F1 score is considered a good evaluation metric for unbalanced datasets, the optimal value at 1 for perfect classifier and worst value at 0 for random classifier, we will implement 'performance_metric' method to evaluate classifiers performance.","02a2740c":"**Second** _Decision Tree_ classifier, DTC goal is to create a model that predicts a target based on input features using Tree models, in these model leaves represent class labels and branches represent features conjunctions that lead to those class labels.","0dbcf4bc":"**First** will test _Naive Bayes_ classifier, NBC is based on Bayes' Theorem which describes the probability of a certain event, based on prior knowledge of conditions that might be related to this event.   ","f4aa4a1b":"We will use `data_visual` for any manipulation needed to visualize data and leave `raw_data` untouched ","1e3c8425":"We will evaluate Logistic Regression classifier and analyze output results, Logistic Regression considered very fast algorithm, and easy to implement and can handle the large number of dataset.","7f339746":"We want to check the number of Apps under every Installs\/target category in order to detect if our dataset is balanced or not i.e. Apps are equally distributed under every Installs\/target.","e080770e":"As appears from results above the dataset are not balanced as 15% in category 0 (failed), 39% in category 1 (limited success), and 46% in category 2 (success).","eb944aae":"To enhance kernel accuracy we can apply the following actions:\n* Try to tune classifiers  parameters instead of using default values (Grid Search technique might be using)\n* Lower number PCA components intervals to 2 (for example) instead of 5 to try more numbers.\n* Try Neural Network for classification.\n* Can try increase number of label categories to 4 instead 3 to be (fail, limited success, success, booming)","7750b466":"Also we will check data duplicates (entire row matching), and remove redundand data from dataset before going any further ","030fb290":"Our kernel detect **ALL** eight random Apps successfully.","bc13c43b":"'Installs' columns considered our targets\/labels for our classifiers, as we need to detect our App success based on the number of downloads we should categorize our target as follows\n\n* The number of installs below 5K will consider a _**failed application**_ under category (zero).\n* The number of installs between 5K and below 1M will consider a _**limited success application**_ under category (one).\n* The number of installs above 1M will consider a _**successful application**_ under category (two).","462b42de":"### 5. Benchmark\n\nIn this section we will build the basic kernel with simple classifier (Logistic Regression) and the output performance will be used as the datum for further classifiers and enhancements applied to the kernel to compare how far our kernel progress.","0f9b941f":"### 6. Algorithms and Techniques\nIn this section, we will explore four more classifiers (Naive Bayes, decision trees, SVM, Gradient Boosting) and compare results with base kernel result then choose the best two classifiers to the enhancement stage","62e129b8":"## III) Test Kernel\n\nIn this section, we will take 8 random Apps and check their number of installs and get their actual successful rate (Fail, limited success, success), then run against our final kernel then compare predicted output with actual one and calculate the score","26d7dc2c":"In order to facilitate our work we implement 'preprocessor_data' method that takes raw_data and prepare the dataset for classification as follows:\n\n* Will cast features with their datatypes (instead of an object) so columns Reviews will cast to int, Last Updated to date-time, Rating and price to float (and remove $ from Price column).\n* Will encode features (Type, Category, Content Rating) by label encoding method as most articles state classifiers do a better job with numeric features.\n* One-hot-encoding will be applied to Genres features and handle Apps with multiple Genres. \n* Replace 'Varies with device' values Size column with an estimated average size in the same category, also unified size values by converting Megabytes and Kilobytes into Bytes, then normalizing App size values. \n\n* Sorting data by 'Last Updated' values that make data sorted from old to new, this step is necessary for the next stage (split train and test sets) to make test set contains newer data which simulate the old data used to train our kernel and the new data is used for prediction\n* Remove un-important features that we think it'll affect a lot like (App, Current Ver, Android Ver, Last Updated), as Last Updated used only for sorting and more need as a feature for classifiers"}}