{"cell_type":{"28d75528":"code","35710598":"code","ee197d4b":"code","386ce129":"code","a876c02e":"code","937d25c9":"code","187109d5":"code","42ccc258":"code","69a0088c":"code","a378f33a":"code","eb4a2e84":"code","d385c5d0":"code","7c46e843":"code","37009175":"code","093ac101":"code","aac75f69":"code","da8ff740":"code","8c60d468":"code","be8cd26b":"code","23d53d72":"code","8c33d20d":"code","7e95f40f":"code","69b6c928":"code","8ca0fdb5":"code","7e398219":"code","85e074fd":"code","2e1b152d":"code","178a940c":"code","c59bc45b":"code","85016309":"code","ada30636":"code","57c62d6f":"code","bd997151":"code","f3acc215":"code","6acb9a24":"code","53662589":"code","d43556e3":"code","36ff5bcc":"code","399564bb":"code","c5de5aec":"code","27b8630b":"code","7c9503e3":"code","6dec4301":"code","2b75af80":"code","653c7ec2":"code","e01a606c":"code","754be18d":"code","8553ff47":"code","bac78ed1":"markdown"},"source":{"28d75528":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","35710598":"GOOGLE_PATH = \"\/kaggle\/input\/google-quest-challenge\/\"\n\ntrain = pd.read_csv(GOOGLE_PATH+\"train.csv\")\ntest = pd.read_csv(GOOGLE_PATH+\"test.csv\")\nsample_submission = pd.read_csv(GOOGLE_PATH+\"sample_submission.csv\")","ee197d4b":"train.shape, test.shape","386ce129":"def url_id_ex(url):\n    try:\n        ids = int(url.split(\"\/\")[-2])\n    except:\n        ids = int(url.split(\"\/\")[-1])\n    return ids\n\n# category_type\ntrain[\"category_type\"] = train[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"\/\")[-1])\ntest[\"category_type\"] = test[\"url\"].apply(lambda x : x.split(\".\")[0].split(\"\/\")[-1])\n\ntrain[\"quser_id\"] = train[\"question_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntrain[\"auser_id\"] = train[\"answer_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntrain[\"url_id\"] = train[\"url\"].apply(url_id_ex)\n\ntest[\"quser_id\"] = test[\"question_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntest[\"auser_id\"] = test[\"answer_user_page\"].apply(lambda x: int(x.split(\"\/\")[-1]))\ntest[\"url_id\"] = test[\"url\"].apply(url_id_ex)","a876c02e":"## bert preprocess","937d25c9":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata\n\n# custom imports\nfrom gensim.utils import deaccent\nfrom collections import Counter\nfrom bs4 import BeautifulSoup\nfrom multiprocessing import Pool\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 10\npd.options.display.max_colwidth = 200","187109d5":"#################################################################################\n## Multiprocessing Run.\n# :df - DataFrame to split                      # type: pandas DataFrame\n# :func - Function to apply on each split       # type: python function\n# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\ndef df_parallelize_run(df, func):\n    num_partitions, num_cores = 16, psutil.cpu_count()  # number of partitions and cores\n    df_split = np.array_split(df, num_partitions)\n    pool = Pool(num_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df\n\n## Build of vocabulary from file - reading data line by line\n## Line splited by 'space' and we store just first argument - Word\n# :path - txt\/vec\/csv absolute file path        # type: str\ndef get_vocabulary(path):\n    with open(path) as f:\n        return [line.strip().split()[0] for line in f][0:]\n\n## Check how many words are in Vocabulary\n# :c_list - 1d array with 'comment_text'        # type: pandas Series\n# :vocabulary - words in vocabulary to check    # type: list of str\n# :response - type of response                  # type: str\ndef check_vocab(c_list, vocabulary, response='default'):\n    try:\n        words = set([w for line in c_list for w in line.split()])\n        u_list = words.difference(set(vocabulary))\n        k_list = words.difference(u_list)\n    \n        if response=='default':\n            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n        elif response=='unknown_list':\n            return list(u_list)\n        elif response=='known_list':\n            return list(k_list)\n    except:\n        return []\n        \n## Seeder\n# :seed to make all processes deterministic     # type: int\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\n    if 'torch' in sys.modules:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n \n## Simple \"Memory profilers\" to see memory usage\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]\/2.**30, 2) \n        \ndef sizeof_fmt(num, suffix='B'):\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f%s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n    \n## Export pickle\ndef make_export(tr, tt, file_name):\n    train_export = train[['id']]\n    test_export = test[['id']]\n\n    try:\n        cur_shape = tr.shape[1]>1\n        train_export = pd.concat([train_export, tr], axis=1)\n        test_export = pd.concat([test_export, tt], axis=1)        \n    except:\n        train_export['p_comment'] = tr\n        test_export['p_comment'] = tt\n    \n    train_export.to_pickle(file_name + '_x_train.pkl')\n    test_export.to_pickle(file_name + '_x_test.pkl')\n\n## Domain Search\nre_3986_enhanced = re.compile(r\"\"\"\n        # Parse and capture RFC-3986 Generic URI components.\n        ^                                    # anchor to beginning of string\n        (?:  (?P<scheme>    [^:\/?#\\s]+):\/\/ )?  # capture optional scheme\n        (?:(?P<authority>  [^\/?#\\s]*)  )?  # capture optional authority\n             (?P<path>        [^?#\\s]*)      # capture required path\n        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n        $                                    # anchor to end of string\n        \"\"\", re.MULTILINE | re.VERBOSE)\n\nre_domain =  re.compile(r\"\"\"\n        # Pick out top two levels of DNS domain from authority.\n        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n        (?::[0-9]*)?                      # Optional port number.\n        $                                 # Anchor to end of string.\n        \"\"\", \n        re.MULTILINE | re.VERBOSE)\n\ndef domain_search(text):\n    try:\n        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n    except:\n        return 'url'\n\n## Load helper helper))\ndef load_helper_file(filename):\n    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n        temp_obj = pickle.load(f)\n    return temp_obj\n        \n## Preprocess helpers\ndef place_hold(w):\n    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n\ndef check_replace(w):\n    return not bool(re.search(WPLACEHOLDER, w))\n\ndef make_cleaning(s, c_dict):\n    if check_replace(s):\n        s = s.translate(c_dict)\n    return s\n  \ndef make_dict_cleaning(s, w_dict):\n    if check_replace(s):\n        s = w_dict.get(s, s)\n    return s\n\ndef export_dict(temp_dict, serial_num):\n    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n\ndef print_dict(temp_dict, n_items=10):\n    run = 0\n    for k,v in temp_dict.items():\n        print(k,'---',v)\n        run +=1\n        if run==n_items:\n            break    \n## ----------------------------------------------------------------------------------------------------","42ccc258":"#################################################################################\nHELPER_PATH             = '..\/input\/jigsaw-general-helper-public\/'\n\nLOCAL_TEST = True       ## Local test - for test performance on part of the train set only\nSEED = 42               ## Seed for enviroment\nseed_everything(SEED)   ## Seed everything\n\nWPLACEHOLDER = 'word_placeholder'","69a0088c":"total_text = train[\"question_title\"] + \" \" + train[\"question_body\"]+ \" \" + train[\"answer\"]","a378f33a":"\n########################### Get basic helpers\n#################################################################################\nprint('1.2. Basic helpers')\nbert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\nbert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\nbert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n\nurl_extensions          = load_helper_file('helper_url_extensions')\nhtml_tags               = load_helper_file('helper_html_tags')\ngood_chars_dieter       = load_helper_file('helper_good_chars_dieter')\nbad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\nhelper_contractions     = load_helper_file('helper_contractions')\nglobal_vocabulary       = load_helper_file('helper_global_vocabulary')\nglobal_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\nnormalized_chars        = load_helper_file('helper_normalized_chars')\nwhite_list_chars        = load_helper_file('helper_white_list_chars')\nwhite_list_punct        = \" '*-.,?!\/:;_()[]{}<>=\" + '\"'\npictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\ntoxic_misspell_dict     = load_helper_file('helper_toxic_misspell_dict')\n","eb4a2e84":"data = total_text\nlocal_vocab = bert_uncased_vocabulary\nverbose = True\nglobal_lower=True\ndata = data.astype(str)\nif verbose: print('#' *20 ,'Initial State:'); check_vocab(data, local_vocab)","d385c5d0":"if global_lower:\n    data = data.apply(lambda x: x.lower())\n    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(data, local_vocab)","7c46e843":"# Normalize chars and dots - SEE HELPER FOR DETAILS\n# Global\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\ndata = data.apply(lambda x: re.sub('\\(dot\\)', '.', x))\ndata = data.apply(lambda x: deaccent(x))\nif verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(data, local_vocab)\n","37009175":"# Remove 'control' chars\n# Global    \nglobal_chars_list = list(set([c for line in data for c in line]))\nchars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(data, local_vocab)","093ac101":"# Remove hrefs\n# Global    \ndata = data.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\nif verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(data, local_vocab)","aac75f69":"# Convert or remove Bad Symbols\n# Global\nglobal_chars_list = list(set([c for line in data for c in line]))\nchars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_chars)])\nchars_dict = {}\nfor char in chars:\n    try:\n        new_char = unicodedata.name(char).split()[-1:][0].lower()\n        if len(new_char)==1:\n            chars_dict[ord(char)] = new_char\n        else:\n            chars_dict[ord(char)] = ''\n    except:\n        chars_dict[ord(char)] = ''\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(data, local_vocab)\nif verbose: print(chars)\nif verbose: print_dict(chars_dict)","da8ff740":"# Remove Bad Symbols PART 2\n# Global\nglobal_chars_list = list(set([c for line in data for c in line]))\nchars = '\u00b7' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_punct) and (ord(c)>256)])\nchars_dict = {}\nfor char in chars:\n    try:\n        new_char = unicodedata.name(char).split()[-1:][0].lower()\n        if len(new_char)==1:\n            chars_dict[ord(char)] = new_char\n        else:\n            chars_dict[ord(char)] = ''\n    except:\n        chars_dict[ord(char)] = ''\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(data, local_vocab)\nif verbose: print(chars)\nif verbose: print_dict(chars_dict)","8c60d468":"# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if ('<' in word) and ('>' in word):\n        for tag in html_tags:\n            if ('<'+tag+'>' in word) or ('<\/'+tag+'>' in word):\n                temp_dict[word] = BeautifulSoup(word, 'html5lib').text  \ndata = data.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","be8cd26b":"# Remove links (There is valuable information in links (probably you will find a way to use it)) \n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\nurl_rule = r'(?P<url>https?:\/\/[^\\s]+)'\ntemp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n    \nfor word in temp_dict:\n    new_value = temp_dict[word]\n    if word.find('http')>2:\n        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n    else:\n        temp_dict[word] = place_hold(new_value)\n            \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","23d53d72":"# Convert urls part 2\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\n\nfor word in temp_vocab:\n    url_check = False\n    if 'file:' in word:\n        url_check = True\n    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n        if 'Aww' not in word:\n            for d_zone in url_extensions:\n                if '.' + d_zone in word:\n                    url_check = True\n                    break            \n    elif ('\/' in word) and ('.' in word):\n        for d_zone in url_extensions:\n            if '.' + d_zone + '\/' in word:\n                url_check = True\n                break\n\n    if url_check:\n        temp_dict[word] =  place_hold(domain_search(word))\n        \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","8c33d20d":"# Normalize pictograms\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n        for pict in pictograms_to_emoji:\n            if (pict in word) and (len(pict)>2):\n                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n            elif pict==word:  \n                temp_dict[word] = pictograms_to_emoji[pict]\n\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","7e95f40f":"# Isolate emoji\n# Global\nglobal_chars_list = list(set([c for line in data for c in line]))\nchars = ''.join([c for c in global_chars_list if c in emoji.UNICODE_EMOJI])\nchars_dict = {ord(c):f' {c} ' for c in chars}\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(data, local_vocab)\nif verbose: print(chars)","69b6c928":"# Duplicated dots, question marks and exclamations\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n        if (Counter(word)['.']>1):\n            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n        if (Counter(word)['!']>1):\n            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n        if (Counter(word)['?']>1):\n            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n        if (Counter(word)[',']>1):\n            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n        temp_dict[word] = new_word\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(data, local_vocab);\n","8ca0fdb5":"# Remove underscore for spam words\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\\/\\']').sub('', word))\/len(word) > 0.6) and ('_' in word):\n        temp_dict[word] = re.sub('_', '', word)       \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","7e398219":"# Isolate spam chars repetition\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\\/\\']').sub('', word))\/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(3)])\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)\n","85e074fd":"# Normalize pictograms part 2\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n        for pict in pictograms_to_emoji:\n            if pict==word:  \n                temp_dict[word] = pictograms_to_emoji[pict]\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)        ","2e1b152d":"# Isolate brakets and quotes\n# Global\nchars = '()[]{}<>\"'\nchars_dict = {ord(c):f' {c} ' for c in chars}\ndata = data.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(data, local_vocab)\n","178a940c":"# Break short words\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_vocab = [k for k in temp_vocab if len(k)<=20]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    if '\/' in word:\n        temp_dict[word] = re.sub('\/', ' \/ ', word)\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)  ","c59bc45b":"# Break long words\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_vocab = [k for k in temp_vocab if len(k)>20]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    if '_' in word:\n        temp_dict[word] = re.sub('_', ' ', word)\n    elif '\/' in word:\n        temp_dict[word] = re.sub('\/', ' \/ ', word)\n    elif len(' '.join(word.split('-')).split())>2:\n        temp_dict[word] = re.sub('-', ' ', word)\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(data, local_vocab); \nif verbose: print_dict(temp_dict)","85016309":"# Remove\/Convert usernames and hashtags (add username\/hashtag word?????)\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if (len(word) > 3) and (word[1:len(word)-1].isalnum()) and (not re.compile('[#@,.:;]').sub('', word).isnumeric()):\n        if word[len(word)-1].isalnum():\n            if (word.startswith('@')) or (word.startswith('#')):\n                new_word = place_hold(new_word[0] + ' ' + new_word[1:]) \n        else:\n            if (word.startswith('@')) or (word.startswith('#')):\n                new_word = place_hold(new_word[0] + ' ' + new_word[1:len(word)-1]) + ' ' + word[len(word)-1]\n\n    temp_dict[word] = new_word\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)        ","ada30636":"# Remove ending underscore (or add quotation marks???)\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if word[len(word)-1]=='_':\n        for i in range(len(word),0,-1):\n            if word[i-1]!='_':\n                new_word = word[:i]\n                temp_dict[word] = new_word   \n                break\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","57c62d6f":"# Remove starting underscore \n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    if word[0]=='_':\n        for i in range(len(word)):\n            if word[i]!='_':\n                new_word = word[i:]\n                temp_dict[word] = new_word   \n                break\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)      ","bd997151":"# End word punctuations\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    for i in range(len(word),0,-1):\n        if word[i-1].isalnum():\n            new_word = word[:i] + ' ' + word[i:]\n            break\n    temp_dict[word] = new_word     \ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)                           ","f3acc215":"# Start word punctuations\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = word\n    for i in range(len(word)):\n        if word[i].isalnum():\n            new_word = word[:i] + ' ' + word[i:]\n            break\n    temp_dict[word] = new_word     \ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)  ","6acb9a24":"# Find and replace acronims\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if (Counter(word)['.']>1) and (check_replace(word)):\n        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['\/']>3)):\n            temp_dict[word] = place_hold('url ' + domain_search(word))\n        else: \n            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\\/\\:]').sub('', word))>0):\n                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)   ","53662589":"# Apply spellchecker for contractions\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if word in helper_contractions:\n        temp_dict[word] = place_hold(helper_contractions[word])\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(data, local_vocab)\nif verbose: print_dict(temp_dict)","d43556e3":"# Isolate obscene (or just keep the word????)\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = re.compile('[a-zA-Z0-9\\-\\.\\,\\\/\\']').sub('', word)\n    if len(Counter(new_word))>2:\n        temp_dict[word] = place_hold('fuck')\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Possible obscene:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","36ff5bcc":"# Remove 's (DO WE NEED TO REMOVE IT???)\n# Local\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)                                        ","399564bb":"\n# Convert backslash\n# Global\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]    \ntemp_dict = {k:re.sub('\\\\\\\\+', ' \/ ', k) for k in temp_vocab}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(data, local_vocab)\nif verbose: print_dict(temp_dict)","c5de5aec":"# Try remove duplicated chars (not sure about this!!!!!)\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\ntemp_vocab_dup = []\n    \nfor word in temp_vocab:\n    temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\ntemp_vocab_dup = set(temp_vocab_dup)\ntemp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n            \nfor word in temp_vocab:\n    new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n    if new_word in temp_vocab_dup:\n        temp_dict[word] = new_word\ntemp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","27b8630b":"# Isolate numbers\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\ntemp_dict = {}\nfor word in temp_vocab:\n    if re.compile('[a-zA-Z]').sub('', word) == word:\n        if re.compile('[0-9]').sub('', word) != word:\n            temp_dict[word] = word\n\nglobal_chars_list = list(set([c for line in temp_dict for c in line]))\nchars = ''.join([c for c in global_chars_list if not c.isdigit()])\nchars_dict = {ord(c):f' {c} ' for c in chars}                \ntemp_dict = {k:place_hold(make_cleaning(k,chars_dict)) for k in temp_dict}\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","7c9503e3":"# Join dashes\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    temp_dict[word] = re.sub('\\-\\-+', '-', word)\ntemp_dict = {k: v for k, v in temp_dict.items() if k != v}\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","6dec4301":"# Try join word (Sloooow)\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = ''.join(['' if c in '-' else c for c in word])\n    if (new_word in local_vocab) and (len(new_word)>3):\n        temp_dict[word] = new_word    \n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","2b75af80":"# Try Split word\n# Local (only unknown words)\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","653c7ec2":"# L33T vocabulary (SLOW)\n# https:\/\/simple.wikipedia.org\/wiki\/Leet\n# Local (only unknown words)\ndef convert_leet(word):\n    # basic conversion \n    word = re.sub('0', 'o', word)\n    word = re.sub('1', 'i', word)\n    word = re.sub('3', 'e', word)\n    word = re.sub('\\$', 's', word)\n    word = re.sub('\\@', 'a', word)\n    return word\n            \ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if check_replace(k)]\n    \ntemp_dict = {}\nfor word in temp_vocab:\n    new_word = convert_leet(word)\n    if (new_word!=word): \n        if (len(word)>2) and (new_word in local_vocab):\n            temp_dict[word] = new_word\n    \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","e01a606c":"# Open Holded words\n# Global\ntemp_vocab = list(set([c for line in data for c in line.split()]))\ntemp_vocab = [k for k in temp_vocab if (not check_replace(k))]\ntemp_dict = {}\nfor word in temp_vocab:\n    temp_dict[word] = re.sub('___', ' ', word[17:-1])\ndata = data.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\ndata = data.apply(lambda x: ' '.join([i for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(data, local_vocab)","754be18d":"# Search multiple form\n# Local | example -> flashlights \/ flashlight -> False \/ True\ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\ntemp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)                                                                      ","8553ff47":"# Convert emoji to text\n# Local \ntemp_vocab = check_vocab(data, local_vocab, response='unknown_list')\ntemp_vocab = [k for k in temp_vocab if (k in emoji.UNICODE_EMOJI)]\ntemp_dict = {}\nfor word in temp_vocab:\n    temp_dict[word] = re.compile('[:_]').sub(' ', emoji.UNICODE_EMOJI.get(word)) \ndata = data.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\nif verbose: print('#' * 10, 'Step - Convert emoji to text:'); check_vocab(data, local_vocab);\nif verbose: print_dict(temp_dict)","bac78ed1":"## This kernel from [https:\/\/www.kaggle.com\/kyakovlev\/preprocessing-bert-public\/data#data](https:\/\/www.kaggle.com\/kyakovlev\/preprocessing-bert-public\/data#data)"}}