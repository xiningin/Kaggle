{"cell_type":{"8e621287":"code","d7d28386":"code","e3abc3c6":"code","62f74c02":"code","b603ad7e":"code","9f28ad9c":"code","cc469c69":"code","118224db":"code","1c37e855":"code","1ce5f686":"code","4fd0bfe8":"code","dcbb7312":"code","e717757e":"code","49491e9d":"code","9785774c":"code","89fefbc2":"code","fb968525":"code","e321e36c":"code","474870af":"code","ee4eb451":"code","10312613":"code","1b49491e":"code","ca4c74a6":"code","602c98a2":"code","92aa0daa":"code","fadcaf78":"code","06582de3":"code","d6506b33":"code","8a4c66f7":"markdown","6de34542":"markdown","0d35e764":"markdown","a47173f4":"markdown","f65a33c9":"markdown","24d776b7":"markdown","e49165ef":"markdown","4c63c012":"markdown","e0988bca":"markdown","b6412964":"markdown","32632329":"markdown","08f62489":"markdown","30c0a906":"markdown","9d6560a8":"markdown","0f341ac6":"markdown","2eecff5d":"markdown","02406a71":"markdown","d4e0b1f4":"markdown"},"source":{"8e621287":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom sklearn import preprocessing\nfrom scipy.stats import ttest_1samp\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d7d28386":"# I will start from the raw xlsx source, luckily pandas has read_excel method which isn't very known\ndf = pd.read_excel('..\/input\/2018-consulates-schengen-visa-stats.xlsx', sheet_name='Data for consulates')\ndf.head()","e3abc3c6":"# Stripping column names\ndf.columns = [col.strip() for col in df.columns]\nprint(df.columns)","62f74c02":"df.drop([\n        'Airport transit visas (ATVs) applied for',\n        'ATVs issued (including multiple)',\n        'Multiple ATVs issued', 'ATVs not issued',\n        'Not issued rate for ATVs',\n        'Total ATVs and uniform visas applied for',\n        'Total ATVs and uniform visas issued  (including multiple ATVs, MEVs and LTVs)',\n        'Total ATVs and uniform visas not issued',\n        'Not issued rate for ATVs and uniform visas'\n        ], axis=1, inplace=True)\n\ndf.rename(columns={\n                    'Schengen State': 'sch_state',\n                    'Country where consulate is located': 'consulate_country',\n                    'Consulate': 'consulate_city',\n                    'Uniform visas applied for': 'applications',\n                    'Total  uniform visas issued (including MEV)': 'uniform_visas_issued',\n                    'Multiple entry uniform visas (MEVs) issued': 'mevs_issued',\n                    'Share of MEVs on total number of uniform visas issued': 'mevs_share',\n                    'Total LTVs issued': 'ltvs_issued',\n                    'Uniform visas not issued': 'rejected',\n                    'Not issued rate for uniform visas': 'rejection_rate'}, inplace=True)\ndf = df.dropna(how='all').reset_index(drop=True)\ndf.head()","b603ad7e":"# Exploring nulls\ndf.isnull().sum()","9f28ad9c":"# This is part is a summary in the sheet, we do not need it\ndf[df.sch_state.isnull()]","cc469c69":"df = df.dropna(axis=0, subset=['sch_state']).reset_index(drop=True)\ndf.fillna(0, inplace=True) # Remaining nulls are zeros","118224db":"# finding any miscalculations \ndf['decisions']  = df.uniform_visas_issued + df.ltvs_issued + df.rejected\nprint('More applications than descisions entries =', len(df[df.decisions < df.applications]))\nprint('More descisions than applications entries =', len(df[df.decisions > df.applications]))","1c37e855":"df['decisions'] = df.decisions - df.ltvs_issued\ndf['applications'] = df.applications - df.ltvs_issued\ndf.drop('ltvs_issued', axis=1, inplace=True)\ndf['rejection_rate'] = df.rejected\/df.decisions\ndf.head()","1ce5f686":"# I am picking Egypt\ncountry = 'EGYPT'\ndf_eg = df[df.consulate_country == country].reset_index(drop=True)\ndf_eg","4fd0bfe8":"# pie chart\ngrouped_df = df_eg.groupby('sch_state').sum().decisions.nlargest(len(df_eg))\nplt.figure(figsize=(15,15))\nplt.subplot(111)\nexplode_thr = 16\nplt.pie(grouped_df,\n        labels=grouped_df.index,\n        autopct='%.1f%%', shadow=False, pctdistance=0.85, labeldistance=1.02, startangle=25,\n        explode = [0 if i < explode_thr else (i\/len(grouped_df))-(explode_thr\/len(grouped_df)) for i in range(len(grouped_df))])\nplt.axis('equal')\nplt.show()","dcbb7312":"# bar chart\nplt.figure(figsize=(15,6))\nplt.subplot(111)\n\nchart = grouped_df.plot('bar')\nchart.set_xlabel('Schengen State')\nchart.set_ylabel('Number of Applicants')\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45)\nplt.show()","e717757e":"# Consult this rating if your application is possibly weak yet seeking a visa (any)\n# Least 15 rejection rates\ndf_eg[['sch_state', 'consulate_city', 'decisions', 'mevs_share', 'rejection_rate']]\\\n.sort_values(by=['rejection_rate'], ascending=True).iloc[:15]","49491e9d":"# Consult this rating if your application is strong AND seeking MEV\ndf_eg[['sch_state', 'consulate_city', 'decisions', 'mevs_share', 'rejection_rate']]\\\n.sort_values(by=['mevs_share'], ascending=False).iloc[:15]","9785774c":"# Consult this rating if you are unsure about documents strength, yet, possibly seeking MEVs\n# This rating is the sweet spot between acceptance rates and MEVs rates giving equal weights to both (normalized)\n# Create a normalized engineered feature = (1-rejection_rate) * mevs_share\ndf_eg['score'] = (1 - df_eg['rejection_rate']) * df_eg.mevs_share\nmin_max_scaler = preprocessing.MinMaxScaler()\ndf_eg['score'] = min_max_scaler.fit_transform(df_eg[['score']].values)\n\ndf_eg[['sch_state', 'consulate_city', 'decisions', 'score']]\\\n.sort_values(by=['score'], ascending=False).iloc[:15]","89fefbc2":"#rejection_rates\ndf_eg[['sch_state', 'consulate_city', 'decisions', 'rejection_rate']]\\\n.sort_values(by=['rejection_rate'], ascending=False).iloc[:6]","fb968525":"#mevs_rate\ndf_eg[['sch_state', 'consulate_city', 'decisions', 'mevs_share']]\\\n.sort_values(by=['mevs_share'], ascending=True).iloc[:6]","e321e36c":"# engineered score\ndf_eg[['sch_state', 'consulate_city', 'decisions', 'score']]\\\n.sort_values(by=['score'], ascending=True).iloc[:6]","474870af":"population_decisions = df_eg.decisions.sum()\npopulation_rejects = df_eg.rejected.sum()\npopulation_rejection_mean = population_rejects\/population_decisions\nprint('Average rejection rate in Egypt for Schengen Visas in 2018: {:.2f}%'.format(population_rejection_mean*100))","ee4eb451":"# Consulates having better than average acceptance rates:\ntop_generous = df_eg[['sch_state', 'consulate_city', 'decisions', 'rejected', 'rejection_rate']]\\\n.sort_values(by=['rejection_rate'], ascending=True)\ntop_generous = top_generous[top_generous.rejection_rate < population_rejection_mean]","10312613":"# To demonstrate how unlikely this could happen, I will use p = 0.001 as a threshold\ndef hypothesis_testing(df, state):\n    null_hytothesis_accepted = []\n    null_hytothesis_rejected = []\n    for idx, consulate in df.iterrows():\n        N = int(consulate.decisions)\n        K = int(consulate.rejected) # K rejects, N-K accepts\n        arr = np.array([1] * K + [0] * (N-K))\n        np.random.shuffle(arr)\n        tstat, pval = ttest_1samp(arr, population_rejection_mean)\n        if pval < 0.001:\n            null_hytothesis_rejected.append(str(consulate.sch_state) + ' in ' + str(consulate.consulate_city))\n        else:\n            null_hytothesis_accepted.append(str(consulate.sch_state) + ' in ' + str(consulate.consulate_city))\n    print('Null Hypothesis REJECTED, {} acceptance rates than average that COULDN\\'T happen by chance: \\n{}'.format(state, '\\n'.join(null_hytothesis_rejected)))\n    print('--------------------------------------------------------------------------------------------\\n')\n    print('Null Hypothesis ACCEPTED, {} acceptance rates than average that COULD have happend by chance: \\n{}'.format(state, '\\n'.join(null_hytothesis_accepted)))\nhypothesis_testing(top_generous, 'better')","1b49491e":"most_strict = df_eg[['sch_state', 'consulate_city', 'decisions', 'rejected', 'rejection_rate']]\\\n.sort_values(by=['rejection_rate'], ascending=False)\nmost_strict = most_strict[most_strict.rejection_rate > population_rejection_mean]\nhypothesis_testing(most_strict, 'lower')","ca4c74a6":"# In order to get this correctly, we need to check for inappropriate data entries\ndf[df.mevs_issued > df.uniform_visas_issued]","602c98a2":"world_rejects = df[~df.consulate_country.isin([sch_state.upper() for sch_state in df.sch_state.values])]\\\n                .groupby('consulate_country').sum()\\\n                .groupby(level=0).filter(lambda x: x.decisions > 100)\nworld_rejects['rejection_rate'] = world_rejects.rejected\/world_rejects.decisions\nhighest_rejections = world_rejects.sort_values(by=['rejection_rate'], ascending=False).drop(['mevs_issued', 'mevs_share'], axis=1)\nprint('{} is the {}th hardest country to get a schengen visa from {} total countries'.format(country, highest_rejections.index.get_loc(country), len(highest_rejections)))\n\nworld_mevs = df[(df.mevs_share <= 1) & (~df.consulate_country.isin([sch_state.upper() for sch_state in df.sch_state.values]))]\\\n                .groupby('consulate_country').sum()\\\n                .groupby(level=0).filter(lambda x: x.decisions > 100)\nworld_mevs['mevs_share'] = world_mevs.mevs_issued\/world_mevs.uniform_visas_issued\nlowest_mevs = world_mevs.sort_values(by=['mevs_share'], ascending=True)\nprint('{} is the {}th hardest country to get a MEV schengen visa from {} total countries'.format(country, lowest_mevs.index.get_loc(country), len(lowest_mevs)))","92aa0daa":"#highest_rejections.iloc[:highest_rejections.index.get_loc(country) + 1]","fadcaf78":"# Rejection Rates:\nplt.figure(figsize=(30,6))\nplt.title('{}: {:.3f}% of visa applications are rejected'.format(country, highest_rejections.loc[country].rejection_rate*100))\nplt.bar(highest_rejections.iloc[:highest_rejections.index.get_loc(country) + 1].index, highest_rejections.iloc[:highest_rejections.index.get_loc(country) + 1].rejection_rate, color='r')\nplt.xlabel('Country')\nplt.ylabel('Rejection Rate')\nplt.xticks(rotation=45, fontsize=8)\nplt.show()","06582de3":"#lowest_mevs.iloc[:lowest_mevs.index.get_loc(country) + 1]","d6506b33":"# MEVs Share:\nplt.figure(figsize=(10,30))\n#plt.subplot(111)\nplt.barh(lowest_mevs.iloc[:lowest_mevs.index.get_loc(country) + 1].index, lowest_mevs.iloc[:lowest_mevs.index.get_loc(country) + 1].mevs_share, color='g')\nplt.title('{}: {:.3f}% of issued visas are MEVs'.format(country, lowest_mevs.loc[country].mevs_share*100))\nplt.xlabel('MEVs Share')\nplt.ylabel('Country')\nplt.axis('tight')\nplt.xticks(rotation=0, fontsize=7)\nplt.show()","8a4c66f7":"This entries are incorrect, there is no explanation for these entries that I can think of.\n\nmevs_share should not under any circumstances exceed 1.0\n\nFor the first rating, using rejection_rate, we can ignore those mistakes but for ratings over mevs_share, I have no option other than dropping those rows.\n\n**I am also dropping consulates inside the Schengen Zone as this situatioin does not reflect anything really (the applicants will mostly not be citizens of this countires but rather statless\/illegal immigrants\/etc). Also, I will drop countries with extremely low decisions < 100**","6de34542":"# Picking a country & starting the analysis:","0d35e764":"# Countries Rating","a47173f4":"# Stricter Consulates","f65a33c9":"We have a lot of unequal entries \n1. The first case (in my opinion) could be a result of applications currently under processing with no final descision taken yet. This could point out to a possible mistake in the raw data rejection_rate as it is just the division of \\\\(rejected\/applicants\\\\) which is *obviously incorrect* and it should rather be \\\\(rejected\/decisions\\\\).\n2. The second case, more decisions than applications is rather weird but could possibly be late descision for case 1 (applications that happened in 2017 but descision taken in 2018), If it is the latter case, then it gets us to a another weird observation as case 1 is much bigger than case 2 (12.5x) while they should be pretty similar (applicants made in 2018 with descisions in 2019 should be somehow similar to descisions made in 2018 for applications in 2017) \n\n**In order to avoid confusion and get accurate results, I will use the descisions column instead of applications column after subtracting the LTVs, re-calculate the rejection_rate and proceed.**","24d776b7":"# What are we trying to do?\nWe are trying to figure out the best consulates to apply for to have the highest odds for receiving a uniform visa in general, or a multiple entry visa (MEV) in particular. Along the way, we would also carry out interesting analysis.\n\nThis kernel status is: Work In Progress\n\n# Important Notice:\nAlthough we would try to get a solid analytic study, there is no way we could know the actual quality of applications received by every consulate in different countries, having a higher rejection rate in a specific consulate could be the result of lower quality of application received in this consulate. But, given the sample size (applicants in consulate) and population size (applicants in all schengen consulates in country), we could carry out a hypothesis testing to figure out the possibility of such scenario to happen just to prove that it is highly unlikely that these numbers happened out of random chance.","e49165ef":"Only Malta, due to the low count of applications, we can not be very sure about it. (we can actually be sure too as p-value threshold of 0.001 is extremely low and have been used to show the difference.","4c63c012":"# Hardest Countries","e0988bca":"Since we are not interested in ATVs (airport transit visas), I will just drop those columns. Column names contain \/n and whitespaces which should to be removed first.","b6412964":"# Generous Consulates","32632329":"It is obvious that France receives the most applications in Egypt, followed by Germany, Italy and Spain.\nBut is France actually the best location to lodge the application regarding rejection rates and MEVs?\n\nWe will first consider the first parameter, rejection_rate, where we aim for the lowest one(if you have weak application).\nThen, we will consider MEVs rate, where we aim for the highest one (if you have strong application).\nLastly, We will create an engineered feature using both parameter in case one is not sure about the strength of his application. (Highest MEV odds from rather low rejection consulates)","08f62489":"# Hypothesis Testing - Could this results be just a random chance?\nIn order to answer this question correctly, we need to define the population and the sample we will use:\n1. Population: Total number of applicants\/rejects in all the country.\n2. Sample: Total number of applicants\/rejects in all the targeted consulates.\nThe null hypothesis, which is a prediction that there is no significant difference between a specific consulate over all other consulates and that these numbers could have happened out naturally due to fluctations in application qualities received.\nIn order to proceed with this, we need to make a rather **BIG** assumption, that all applications to all consulates have the same *average* quality or that they came from the same pool\/population.\n\nAlthough this could not be known for sure, one can make an educated guess that it should rather be true, given that all applications came from Egypt (same country), targeted to Europe which has mostly a good living standard. There shouldn't be any reason to consider that Netherlands receives applications from a different population than Germany. or that Portugal receives higher quality applications than Spain.\n\nWe can conduct this using univariate T-test.","30c0a906":"**It is clear that generally for the most 11 generous and most 9 strict, we can be confident that this didn't happen by random chance and there is probably different scoring criteria for applications in different consulates.**","9d6560a8":"Although the data should be pretty clean now, there are 2 things needed to be done:\n1. Check for mis-calculated columns: \\\\(applications = uniform\\_visas\\_issued + ltvs\\_issued + rejected\\\\) \n\n2. Drop the limited territory visas (LTVS) and remove their applications count as these visas are given under very specific circumstances that are not of the interest to most people or this analysis.\n\n","0f341ac6":"I will pick the consulates that have higher\/lower rejection rates than average to see if there is a possibility given the sample sizes that all this happened out of pure chance.","2eecff5d":"# Data Cleaning:","02406a71":"Feel free to fork the kernel if you want to apply the same analysis on a different country!","d4e0b1f4":"**Which consulate receives the most applications?**"}}