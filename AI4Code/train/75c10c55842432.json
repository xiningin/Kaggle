{"cell_type":{"f05451da":"code","285949db":"code","418639ac":"code","16d73700":"code","32e28591":"code","b55a4107":"code","d00f5bea":"code","515b3b99":"code","e02e08fe":"code","e4d62f3c":"code","81089e60":"code","27aa8e04":"code","16f6ae41":"code","d80b1d3d":"code","2f70f636":"code","c2f713f0":"code","006ca1f7":"code","11056c51":"code","d7f9c17b":"code","23ad9cfb":"code","771f60d0":"code","80bc8061":"code","9c7c5efa":"code","805d2dca":"code","2249189c":"code","d49e82d3":"code","2e3faf0b":"code","2d825c6c":"code","4e8718d2":"code","f60d5af5":"code","a61a8688":"code","1733ff7d":"code","bb07f748":"code","7474bc11":"code","6d4cf482":"code","3f582cc3":"code","7f5e49bb":"code","ad478dc6":"code","02cf96f6":"markdown","fdea4702":"markdown","ff44450e":"markdown","22a520e2":"markdown","c4776cea":"markdown","c67f64ea":"markdown","2848bc21":"markdown","aaee9db5":"markdown","bcaae032":"markdown","20aee2c6":"markdown","da4d3318":"markdown","635d4cdd":"markdown","02c8fa2a":"markdown","f730bb90":"markdown","c69abb9b":"markdown","3a4b2df1":"markdown","66da3515":"markdown","f4c2a82a":"markdown","4e9a927d":"markdown","65d25e58":"markdown","c79493b7":"markdown"},"source":{"f05451da":"%%capture \n!pip install pandas\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom fastai import *\nfrom fastai.tabular import *\nimport torch\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","285949db":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","418639ac":"path = Path('\/kaggle\/input\/titanic')\ntrpath = path\/'train.csv'\ncvpath = path\/'test.csv'\n\ndf_train_raw = pd.read_csv(trpath)\ndf_test_raw = pd.read_csv(cvpath)\n\ndf_train = df_train_raw.copy(deep = True)\ndf_test  = df_test_raw.copy(deep = True)\n\ndata_cleaner = [df_train_raw, df_test_raw] #to clean both simultaneously","16d73700":"df_train.head(n=10)","32e28591":"varnames = list(df_train.columns)\nfor name in varnames:\n    print(name+\": \",type(df_train.loc[1,name]))","b55a4107":"df_train.isnull().sum(axis=0)","d00f5bea":"msno.matrix(df_train)","515b3b99":"msno.bar(df_test)","e02e08fe":"plt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14)\n\nplt.figure()\nfig = df_train.groupby('Survived')['Age'].plot.hist(histtype= 'bar', alpha = 0.8)\nplt.legend(('Died','Survived'), fontsize = 12)\nplt.xlabel('Age', fontsize = 18)\nplt.show()","e4d62f3c":"df_train.corr(method='pearson')['Age'].abs()","81089e60":"plt.figure()\nfig = df_train.groupby('Survived')['Parch'].plot.hist(histtype= 'bar',alpha = 0.8)\nplt.legend(('Died','Survived'),)\nplt.xlabel('Parch')\nplt.show()","27aa8e04":"df_train['Family onboard'] = df_train['Parch'] + df_train['SibSp']\nplt.rcParams['figure.figsize'] = [20, 8]\nplt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14)\n\nfig, axes = plt.subplots(nrows=1, ncols=3)\ndf_train.groupby(['Parch'])['Survived'].value_counts(normalize=True).unstack().plot.bar(ax=axes[1],width = 0.85)\ndf_train.groupby(['SibSp'])['Survived'].value_counts(normalize=True).unstack().plot.bar(ax=axes[2],width = 0.85)\ndf_train.groupby(['Family onboard'])['Survived'].value_counts(normalize=True).unstack().plot.bar(ax=axes[0],width = 0.85)\n\naxes[0].set_xlabel('Family onboard',fontsize = 18)\naxes[1].set_xlabel('parents \/ children aboard',fontsize = 18)\naxes[2].set_xlabel(' siblings \/ spouses aboard',fontsize = 18)\n\nfor i in range(3):\n    axes[i].legend(('Died','Survived'),fontsize = 12, loc = 'upper left')\n\nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.xticks(rotation=0)\n\nplt.suptitle('Survival rates over Number of relatives onboard',fontsize =22)\nplt.show()","16f6ae41":"plt.rcParams['figure.figsize'] = [6, 5]\nplt.rc('xtick', labelsize=14) \nplt.rc('ytick', labelsize=14) \n\nplt.figure()\nfig = df_train.groupby(['Sex'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.9)\nplt.legend(('Died','Survived'),fontsize = 12, loc = 'upper left')\nplt.xlabel('Gender',fontsize =18)\nplt.xticks(rotation=0)\n\nplt.suptitle('Survival rates over Gender',fontsize =22)\nplt.show()","d80b1d3d":"plt.figure()\nfig = df_train.groupby('Survived')['Fare'].plot.hist(histtype= 'bar', alpha = 0.8)\nplt.legend(('Died','Survived'))\nplt.xlabel('Fare')\nplt.show()\n\nplt.rcParams['figure.figsize'] = [10, 5]\nplt.rc('xtick', labelsize=12) \nplt.rc('ytick', labelsize=12) ","2f70f636":"df_train['Title'] = df_train['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\nvarnames = list(df_train.columns)\nfor name in varnames:\n    print(name+\": \",type(df_train.loc[1,name]))\n    \nprint(list(df_train['Title'].unique()))    \ndf_test['Title'] = df_test['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\ndf_test['Title'].unique()","c2f713f0":"def new_titles(df):\n    new_titles = dict()\n    assert 'Title' in df.columns\n    for key in df['Title'].unique():\n        females = ['Mrs','Miss','Ms','Mlle','Mme','Dona']\n        males = ['Mr','Don']\n        notable = ['Jonkheer','the Countess','Lady','Sir','Major','Col','Capt','Dr','Rev','Notable']\n        titles = [females,males,notable,'Master']\n        newtitles = ['Mrs','Mr','Notable','Master']\n        idx = [key in sublist for sublist in titles]\n        idx = np.where(idx)[0] \n        new_titles[key] = newtitles[idx[0]]\n    return new_titles\n\n\nnew_titles_dict = new_titles(df_train)\ndf_train['Title'] = df_train['Title'].replace(new_titles_dict)","006ca1f7":"plt.rcParams['figure.figsize'] = [12, 5]\nplt.rc('xtick', labelsize=12) \nplt.rc('ytick', labelsize=12) \n\nplt.figure()\nfig = df_train.groupby(['Title'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.9)\nplt.legend(('Died','Survived'),fontsize = 12, loc = 'upper left')\nplt.xlabel('Title',fontsize =16)\nplt.xticks(rotation=0)\n\n\nplt.suptitle('Survival rates over Title',fontsize =20)\nplt.show()","11056c51":"df_train['Cabin'][df_train['Cabin'].isnull()]='Missing'\ndf_train['Cabin'] = df_train['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]","d7f9c17b":"plt.rcParams['figure.figsize'] = [12, 5]\nplt.figure()\nfig = df_train.groupby(['Cabin'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.9)\nplt.legend(('Died','Survived'),fontsize = 12, loc = 'upper left')\nplt.xlabel('Cabin Deck',fontsize =12)\nplt.suptitle('Survival rates over Cabin Deck',fontsize =18)\nplt.xticks(rotation=0)\nplt.show()","23ad9cfb":"plt.rcParams['figure.figsize'] = [10, 5]\nplt.figure()\nfig = df_train.groupby(['Embarked'])['Survived'].value_counts(normalize=True).unstack().plot.bar(width = 0.7)\nplt.legend(('Died','Survived'),fontsize = 12, loc = 'upper left')\nplt.xlabel('Embarking Port',fontsize =18)\nplt.suptitle('Survival rates over embarking port',fontsize =22)\nplt.xticks(rotation=0)\nplt.show()","771f60d0":"def df_fill(datasets, mode):\n    assert mode =='median' or mode =='sampling'\n    datasets_cp =[]\n    np.random.seed(2)\n    varnames = ['Age','Fare']\n    for d in datasets:\n        df = d.copy(deep = True)\n        for var in varnames:\n            idx = df[var].isnull()\n            if idx.sum()>0:\n                if mode =='median':\n                    medians = df.groupby('Pclass')[var].median()\n                    for i,v in enumerate(idx):\n                        if v:\n                            df[var][i] = medians[df['Pclass'][i]]\n                else:\n                    g = df[idx==False].groupby('Pclass')[var]\n                    for i,v in enumerate(idx):\n                        if v:\n                            df[var][i] = np.random.choice((g.get_group(df['Pclass'][i])).values.flatten())\n    #Embarked                 \n        idx = df['Embarked'].isnull()\n        g = df[idx==False].groupby('Pclass')['Embarked']\n        for i,v in enumerate(idx):\n            if v:\n                df['Embarked'][i] = np.random.choice((g.get_group(df['Pclass'][i])).values.flatten())                   \n    #Cabin\n        df['Cabin'][df['Cabin'].isnull()]='Missing'\n        df['Cabin'] = df['Cabin'].str.split(r'(^[A-Z])',expand = True)[1]\n        datasets_cp.append(df)\n    return datasets_cp\n\ndata_clean = df_fill(data_cleaner,'median')","80bc8061":"def prepare_data(datasets):\n        datasets_cp = []\n        for d in datasets:\n            df = d.copy(deep = True)\n            df['Family onboard'] = df['Parch'] + df['SibSp']\n            df['Title'] = df['Name'].str.split(',',expand = True)[1].str.split('.',expand = True)[0].str.strip()\n            new_titles_dict = new_titles(df)\n            df['Title'] = df['Title'].replace(new_titles_dict)\n            df.drop(columns = ['PassengerId','Name','Ticket'],axis = 1, inplace = True)\n            datasets_cp.append(df)\n        return datasets_cp\n        ","9c7c5efa":"train,test =prepare_data(df_fill(data_cleaner,mode = 'sampling'))  \nprint(\"Training data\")\nprint(train.isnull().sum())\nprint(\"Test data\")\nprint(test.isnull().sum())","805d2dca":"def corr_matrix(x,y, quant = None):\n    x_quants = x.quantile(quant) if quant else x.quantile([0, 0.25, 0.5, 0.75, 1])\n    out = np.zeros((x_quants.shape[0]-1,int(y.unique().max()+1)))\n    for i in range(x.shape[0]):\n        comp = x[i]<=x_quants\n        idx = int(next((j for j,compv in enumerate(comp) if compv),None))\n        out[idx-1,int(y[i])]+=1\n    return out.T,x_quants\n\ndef plot_corr_matrix(x,quants,fig, ax, **kwargs):\n    assert x.shape[1] == quants.shape[0]-1\n    cmap = kwargs['cmap'] if kwargs['cmap'] else 'Blues'\n    ax.set_xlabel(kwargs['xlabel'])\n    ax.set_ylabel(kwargs['ylabel'])\n    ticks = np.arange(quants.shape[0])\n    ax.set_xticks(ticks)\n    ax.set_xticklabels(list(quants))\n    if 'xlabel' and 'ylabel' in kwargs.keys():\n        ax.title.set_text(f\"{kwargs['xlabel']} vs {kwargs['ylabel']}\")\n    p = ax.pcolor(x,cmap = cmap)\n    fig.colorbar(p,ax = ax)\n    return fig,ax\n    \n    \ndef gen_corr_matrix(*args,quant = None,cmap = 'YlOrBr'):\n    totalvars = len(args)\n    assert totalvars>1\n    \n    out   = dict()\n    out_q = dict()\n    fig,axs = plt.subplots(1, totalvars-1, squeeze=False)\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n    fig.figsize=(800, 800) \n    fig.suptitle(\"Correlation Matrix\") if totalvars<3 else fig.suptitle(\"Correlation Matrices\")\n    for i in range(totalvars-1):\n        out[i],out_q[i] = corr_matrix(args[0],args[i+1],quant)\n        plot_corr_matrix(out[i], out_q[i],\n                         fig,\n                         axs[0,i],\n                         cmap = cmap ,\n                         xlabel = args[0].name,\n                         ylabel = args[i+1].name)\n    plt.show()","2249189c":"gen_corr_matrix(train['Age'],train['Parch'],train['SibSp'],train['Family onboard'])","d49e82d3":"def scatterplot(x,y):\n    fig,ax = plt.subplots()\n    ax.scatter(x,y)\n    ax.set_xlabel(x.name)\n    ax.set_ylabel(y.name)\n    ax.grid(True)\n\n    coef = np.polyfit(x,y,1)\n    poly1d_fn = np.poly1d(coef) \n    plt.plot(x,y, 'ro', x, poly1d_fn(x), '--k')\n    plt.show()\n    \nscatterplot(train['Age'],train['Fare'])","2e3faf0b":"gen_corr_matrix(df_train['Fare'],df_train['Survived'])","2d825c6c":"cont_names = ['Fare','Age']\ncat_names = ['Pclass','Sex','SibSp','Parch','Cabin','Embarked','Family onboard']\nprocs = [Categorify]\ndep_var = 'Survived'\n\ndata_test = TabularList.from_df(test, cat_names=cat_names, cont_names=cont_names, procs=procs)\n\n\ndata = (TabularList.from_df(train, path='\/kaggle\/working', cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_rand_pct(0.2)\n                           .label_from_df(cols = dep_var)\n                           .add_test(data_test, label=0)\n                           .databunch()\n       )","4e8718d2":"learn = tabular_learner(data, \n                        layers=[1000,500, 200,50, 15],\n                        metrics=accuracy,\n                        emb_drop=0.1\n                       )\n","f60d5af5":"torch.device('cuda')\nlearn.fit_one_cycle(5, 2.5e-2)","a61a8688":"learn.export('stage1')","1733ff7d":"learn.lr_find()","bb07f748":"learn.recorder.plot()","7474bc11":"learn.unfreeze()\nlearn.fit_one_cycle(10, max_lr=slice(2e-4))","6d4cf482":"# learn.model\nlearn.recorder.plot_losses()","3f582cc3":"learn.unfreeze()\nlearn.fit_one_cycle(2, max_lr=slice(5e-2))","7f5e49bb":"predictions, *_ = learn.get_preds(DatasetType.Test)\nlabels = np.argmax(predictions, 1)\nsubmission = pd.DataFrame({'PassengerId':df_test['PassengerId'],'Survived':labels})","ad478dc6":"submission.to_csv('submission-fastai.csv', index=False)","02cf96f6":"### 1)Missing values","fdea4702":"# If you like this notebook, please an Upvote! Don't forget to check out my other notebooks too!\n\n* [ConnectX Baseline](https:\/\/www.kaggle.com\/brendan45774\/connectx-baseline)\n* [Data Visuals - Matplotlib](http:\/\/www.kaggle.com\/brendan45774\/data-visuals-matplotlib)\n* [Digit Recognizer Solution](http:\/\/www.kaggle.com\/brendan45774\/digit-recognizer-solution)\n* [Dictionary and Pandas Cheat sheet](https:\/\/www.kaggle.com\/brendan45774\/dictionary-and-pandas-cheat-sheet)\n* [EDA Tutorial Hollywood Movies](https:\/\/www.kaggle.com\/brendan45774\/eda-tutorial-hollywood-movies)\n* [Getting started with Matplotlib](http:\/\/www.kaggle.com\/brendan45774\/getting-started-with-matplotlib)\n* [How to get the lowest score](https:\/\/www.kaggle.com\/brendan45774\/how-to-get-the-lowest-score)\n* [House predict solution](http:\/\/www.kaggle.com\/brendan45774\/house-predict-solution)\n* [Kuzushiji-MNIST Panda](http:\/\/www.kaggle.com\/brendan45774\/kuzushiji-mnist-panda)\n* [Plotly Coronavirus (Covid-19)](https:\/\/www.kaggle.com\/brendan45774\/plotly-coronavirus-covid-19)\n* [Titanic Top Solution](http:\/\/www.kaggle.com\/brendan45774\/titanic-top-solution)\n* [Titanic Data Solution](http:\/\/www.kaggle.com\/brendan45774\/titanic-data-solution)\n* [Word Cloud - Analyzing Names](https:\/\/www.kaggle.com\/brendan45774\/word-cloud-analyzing-names)","ff44450e":"Now we can read the data into Pandas dataframes. A copy of the original data is kept should we require it later. Both training and test datasets are put together in a list so that we can iterate over both at the same time during data cleaning. ","22a520e2":"Let's first take a look at the first couple of rows of the training data, as well as the types of variables that the dataframe posesses and their corresponding value types.","c4776cea":"Before we start cleaning up the data, it is important to see which variables are of relevance, which can be ignored  and what is the most appropriate way to fill in the missing values. As we can see in the charts above, there are 3 variables with missing values in the training set(Age,Cabin and Embarked) and only 2 in the test set (Age,Cabin). In the test set, there is also 1 fare entry missing, which we will fill later on. We shall now try and decide what we are going to do with those values.\n","c67f64ea":"# Exploratory Data Analysis\n","2848bc21":"We see that the ages distribution between those who survived and those who did not is similar.We see however that most young aged passengers were saved. Therefore, Age was, after a threshold value, probably not a major factor that determined who survived the accident. We shall now explore how to fill in the missing ages. Several strategies pinpoint to replace the missing values with the mean or median of the whole distribution, which in our eyes doesn't seem a good choice. Instead, let's look into the correlation of age with the other variables.","aaee9db5":"![image.png](attachment:image.png)","bcaae032":"We can now check the survival rates for each title to see if there is some useful information here.","20aee2c6":"data file locations:","da4d3318":"We would now to check if the title name of a person can be useful in determining whether that person survived or not. This assumption stems from the idea that people of higher status could have been given higher priority during the ship's evacuation.  Therefore, we create a new variable called 'Title'.","635d4cdd":"It is very important to understand whether and where there are missing values in the data.","02c8fa2a":"We see that the strongest correlation of the variable age is with the variable Pclass (passenger class). Therefore, it is appropriate to use this information in order to sample the missing ages according to the pclass. We can either take the median of each Pclass group or sample a random value from that group. We are going to try both and see which one yields better results. Let's now explore the impact that the amount of relatives on board had on survival. For that, we create a new feature called 'Family onboard'. ","f730bb90":"We see a clear trend that the smaller the number of relatives on board, the higher the chance of survival. Therefore, we conclude that this is an interesting feature to include in our training data. We also see that female passengers had a higher chance of survival than male ones. It was expected that females and children would be more likely to survive, as the evacuation protocol of the ship was instructing accordingly. Let us now compare the survival chances and the passengers' ticket prices.","c69abb9b":"# 1) Reading the data and setting up the environment","3a4b2df1":"# 2) Undestanding the Data","66da3515":"# Setting up training dataset","f4c2a82a":"# Data Cleanup","4e9a927d":"Some of these titles can be grouped up, since they mean the same thing. For example, \"Mrs\", \"Miss\", \"Ms\" will be grouped together under the label \"Mrs\". There are also some titles that appear to actually be a name instead of a title (Mlle, Mme) that will also be mapped to the same value. \"Don\" is probably an abbreviation to a male name and will be mapped to \"Mr\". Other title categories are \"Noble\",\"Master\",\"Dr\/Clergy\" and \"Military\".","65d25e58":"We are now going to ensure that there are no missing values in the dataset and prepare it for training our model. The 4 categories that have missing values in the train and test sets are:\n1. Age \n2. Cabin \n3. Embarked \n4. Fare\n\nIn order to ease the documents' readability, any extra variables created above will be recreated here from scratch and will be encapsulated in a function. This is done to make it easier to the reader to find all feature engineering procedures in one place.","c79493b7":"The first step to analyzing the data is to load all the libraries we are going to use. This is performed at the start so that we can know at any point which libraries are loaded in the notebook. "}}