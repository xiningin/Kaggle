{"cell_type":{"d996acc9":"code","a493fb88":"code","6d5b2c1e":"code","2bed95ab":"code","d759ee18":"code","ccfd221b":"code","2bcd93cd":"code","62917891":"code","53acaec1":"code","99a0e9b0":"code","bb2c9961":"code","aa724368":"code","4b43c454":"code","d8f962c9":"code","e733eb0d":"code","8f3da90f":"code","9cce9171":"code","bfc9c8c0":"code","9322fb24":"code","2b179bd7":"code","2bdd681f":"code","fd590637":"code","4cc63efd":"code","64c3096b":"code","7e6d7e1c":"code","1734f889":"code","5357156c":"code","a024efe0":"code","55cb102f":"code","3ffdf7c6":"code","a9a9fa42":"code","61c18552":"code","71305b3f":"code","1e0d07be":"markdown","40f13223":"markdown","20adda30":"markdown","2eb58f22":"markdown","ec69eecc":"markdown","2628d670":"markdown","d3b87cd9":"markdown","04bd34f0":"markdown","0f89c724":"markdown","de33b808":"markdown"},"source":{"d996acc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.mixture import GaussianMixture\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a493fb88":"df = pd.read_csv('\/kaggle\/input\/bank-customer-segmentation\/bank_transactions.csv')\ndf.head()","6d5b2c1e":"df.info()","2bed95ab":"df.isna().sum()","d759ee18":"df = df.dropna()\ndf['CustomerDOB'].value_counts()","ccfd221b":"df = df.loc[~(df['CustomerDOB'] == '1\/1\/1800')]\ndf['CustomerDOB'].value_counts()","2bcd93cd":"df['CustomerDOB'] = pd.to_datetime(df['CustomerDOB'], format = '%d\/%m\/%y')\ndf.head()","62917891":"df.loc[df['CustomerDOB'].dt.year >= 2021, ['CustomerDOB']] -= pd.DateOffset(years = 100)\ndf.head()","53acaec1":"df['TransactionDate'].value_counts()","99a0e9b0":"df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], format = '%d\/%m\/%y')","bb2c9961":"sns.histplot(x = df['TransactionDate'].dt.month, bins = 3, binwidth = 1)\nplt.title('Number of transactions in each month')","aa724368":"df['Age'] = (pd.to_datetime('today') - df['CustomerDOB'])\/np.timedelta64(1, 'Y')\ndf['DaysSinceTransaction'] = (pd.to_datetime('today') - df['TransactionDate'])\/np.timedelta64(1, 'D')\ndf['DaysSinceTransaction'] = df['DaysSinceTransaction'] - df['DaysSinceTransaction'].min()\ntemp = df[['CustomerID', 'TransactionID']].groupby(by = 'CustomerID', as_index = False, sort = False).count().reset_index()","4b43c454":"temp = temp.drop(columns = 'index')\ntemp.rename(columns = {'TransactionID' : 'TransactionFrequency'})\ndf = df.merge(right = temp, on = 'CustomerID')\ndf.head()","d8f962c9":"df = df.rename(columns = {'TransactionID_y' : 'TransactionFrequency',\n                         'DaysSinceTransaction' : 'Recency'})","e733eb0d":"rmf = df.drop(columns = ['CustGender', 'CustLocation', 'CustLocation',\n                         'CustAccountBalance', 'TransactionTime', 'Age']\n             ).groupby(by = 'CustomerID').agg({'Recency' : 'min',\n                                               'TransactionFrequency': 'first',\n                                               'TransactionAmount (INR)' : 'mean'})\ndf = df.rename(columns = {'TransactionAmount (INR)' : 'AverageTransactionAmount'})\nrmf = rmf.rename(columns = {'TransactionAmount (INR)' : 'AverageTransactionAmount'})\nrmf.head()","8f3da90f":"fig, axes = plt.subplots(1, 3, figsize = (15, 5))\naxes = axes.flatten()\n\nsns.countplot(x = 'Recency', data = rmf, ax = axes[0])\nsns.histplot(x = 'TransactionFrequency', data = rmf, ax = axes[1])\nsns.scatterplot(x = 'AverageTransactionAmount', y = 'Recency', data = rmf, ax = axes[2])\nplt.tight_layout()","9cce9171":"def recency_score(value, quartiles):\n    if value < quartiles[0.25]:\n        return 4\n    if value < quartiles[0.5]:\n        return 3\n    if value < quartiles[.75]:\n        return 2\n    else:\n        return 1\n\ndef monetary_score(value, quartiles):\n    if value < quartiles[0.25]:\n        return 1\n    if value < quartiles[0.5]:\n        return 2\n    if value < quartiles[0.75]:\n        return 3\n    else:\n        return 4\n    \nquartiles = rmf.quantile([0.25, 0.5, 0.75]).to_dict()\n\nrmf['recency_score'] = rmf['Recency'].apply(recency_score, quartiles = quartiles['Recency'],)\nrmf['frequency_score'] = rmf['TransactionFrequency'].astype(int)\nrmf.loc[rmf['frequency_score'] > 4, 'frequency_score'] = 4\nrmf['monetary_score'] = rmf['AverageTransactionAmount'].apply(monetary_score, quartiles = quartiles['AverageTransactionAmount'],)\nrmf['total_score'] = rmf['recency_score'] + rmf['frequency_score'] + rmf['monetary_score']","bfc9c8c0":"fig, axes = plt.subplots(1, 3, figsize = (20, 10))\naxes = axes.flatten()\n\nrecency = rmf.groupby(by = 'TransactionFrequency').mean().reset_index()\navg_amount = rmf.groupby(by = 'TransactionFrequency').mean().reset_index()\n\n\nsns.scatterplot(x = 'total_score', y = 'AverageTransactionAmount', hue = 'TransactionFrequency',\n            data = avg_amount, ax = axes[0])\naxes[0].set_title('''Transaction Amount vs Total Score \n                  \\n Averaged over Transaction Frequency ''')\nsns.scatterplot(x = 'total_score', y = 'Recency', data = recency, hue = 'TransactionFrequency',\n            ax = axes[1])\naxes[1].set_title('''Recency vs Total Score \\n\n                    Averaged over Transaction Frequency''')\nsns.countplot(x = 'total_score', data = rmf, ax = axes[2])\naxes[2].set_title('Number of Customers in each score range')\nplt.tight_layout()","9322fb24":"rmf[rmf.total_score == 12].count()","2b179bd7":"rmf.groupby(by = 'total_score').describe().T","2bdd681f":"df.head()","fd590637":"df_cluster = df.drop(columns = ['CustomerID', 'CustomerDOB', 'TransactionDate', 'TransactionTime'])\ndf_cluster['CustGender'] = df_cluster['CustGender'].map(lambda x: 1 if x == 'F' else 0)\nlocations = df_cluster.CustLocation.unique()\nlocations = {place : idx for idx, place in enumerate(locations)}\ndf_cluster['CustLocation'] = df_cluster['CustLocation'].map(locations)\ndf_cluster.head()","4cc63efd":"#df_cluster.rename(columns = {'TransactionAmount (INR)' : 'AverageTransactionAmount'})\nnumeric_to_locations = {idx : location for idx, location in enumerate(locations.keys())}\ndf_cluster['CustLocation'].value_counts().head(10)","64c3096b":"location_9 = df_cluster.loc[df_cluster.CustLocation == 9]\nlocation_9.head()","7e6d7e1c":"del(df)\ndel(rmf)","1734f889":"ss = StandardScaler()\nct = ColumnTransformer([('Standard Scaling', ss, \n                    ['CustAccountBalance', 'AverageTransactionAmount',\n                    'Age', 'Recency'])], remainder = 'passthrough')\n\nclusters = range(2, 10)\nX = location_9.drop(columns = ['TransactionID_x', 'CustLocation'])\nX = ct.fit_transform(X)","5357156c":"def plt_silhouette(clusters, clusterer):\n    #This silhouette analysis is from SKLearn : https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n    for cluster in clusters:\n        fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, len(X) + (cluster + 1) * 10])\n        \n        if clusterer == MiniBatchKMeans:\n            md = MiniBatchKMeans(n_clusters = cluster)\n            preds = md.fit_predict(X)\n            md_name = 'MiniBatchKMeans'\n        else:\n            md = GaussianMixture(n_components = cluster)\n            preds = md.fit_predict(X)\n            md_name = 'GaussianMixture'\n        \n        sil_average = silhouette_score(X, preds)\n        print(f'For {cluster} clusters,\\n the average silhouette score is {sil_average}')\n        sample_sil = silhouette_samples(X, preds)\n        y_lower = 10\n\n        for i in range(cluster):\n            cluster_sil = sample_sil[preds == i]\n            cluster_sil.sort()\n            size_cluster = cluster_sil.shape[0]\n            y_upper = y_lower + size_cluster\n            color = cm.nipy_spectral(float(i) \/ cluster)\n            ax.fill_betweenx(np.arange(y_lower, y_upper),\n                            0,\n                            cluster_sil,\n                            facecolor = color,\n                            edgecolor = color)\n            ax.text(-0.05, y_lower + 0.5 * size_cluster, str(i))\n            y_lower = y_upper + 10\n            ax.axvline(x = sil_average, color = 'red', linestyle = '--')\n            ax.set_yticks([])\n            ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n            ax.set_title(f'Silhouette analysis using {md_name} for {cluster} clusters', fontweight = 'bold')","a024efe0":"plt_silhouette(clusters, MiniBatchKMeans)","55cb102f":"plt_silhouette(clusters, GaussianMixture)","3ffdf7c6":"X = location_9.drop(columns = ['TransactionID_x', 'CustLocation', 'TransactionFrequency'])\nX = ct.fit_transform(X)\nplt_silhouette(clusters, MiniBatchKMeans)","a9a9fa42":"plt_silhouette(clusters, GaussianMixture)","61c18552":"km3 = MiniBatchKMeans(n_clusters = 3)\nkm4 = MiniBatchKMeans(n_clusters = 4)\n\nlocation_9['Three_Clusters'] = km3.fit_predict(X)\nlocation_9['Four_Clusters'] = km4.fit_predict(X)\n\nlocation_9.head()\n\n","71305b3f":"#location_9['CustGender'] = location_9['CustGender'].map(lambda x: 'F' if x == 1 else 'M')\nlocation_9['TransactionFrequency'] = location_9['TransactionFrequency'].astype(int)\nfig, axes = plt.subplots(6, 2, figsize = (15, 15))\naxes = axes.flatten()\n\ncontinuous_columns = location_9.drop(columns = ['TransactionID_x','CustLocation', 'Three_Clusters',\n                                                'Four_Clusters', 'CustGender', 'TransactionFrequency']).columns\n\ncat_columns = location_9[['CustGender', 'TransactionFrequency']]\n\nfor idx, c_name in enumerate(cat_columns):\n    sns.countplot(x = c_name, hue = 'Three_Clusters',  data = location_9, ax = axes[2 * idx])\n    sns.countplot(x = c_name, hue = 'Four_Clusters', data = location_9, ax = axes[2 * idx + 1])\n    axes[2 * idx].set_title(f'{c_name} Three Clusters')\n    axes[2 * idx + 1].set_title(f'{c_name} Four Clusters')\n\nfor idx, c_name in enumerate(continuous_columns):\n    sns.scatterplot(y = 'Three_Clusters', x = c_name,  data = location_9, ax = axes[2 * (idx + 2)])\n    sns.scatterplot(y = 'Four_Clusters', x = c_name, data = location_9, ax = axes[2 * (idx + 2) + 1])\n    axes[2 * idx].set_title(f'{c_name} Three Clusters')\n    axes[2 * idx + 1].set_title(f'{c_name} Four Clusters')\n    \nplt.tight_layout()","1e0d07be":"With Silhouette score, you'd like  for the the groups to be of roughly equal sizes, and all of the clusters should have at least the average silhouette score. With regards to this, KMeans with two clusters seem to be the best, having a signifcantly higher silhouette score, and while one cluster is significantly smaller than the other, this is probably due to the large imbalance in the frequency of transactions. Let's see what happens when we exclude the frequency","40f13223":"There are a lot of birthdates on January 1st, which seems  alittle suspicious. There's also a lot of customers birth DOB on 1\/1\/1800, having 70x the number as the next highest date. This is probably some default the bank has where this information is unknown, althought that is a little suspicous. I'll remove them.","20adda30":"Unsurpisingly, due to the low frequency of transactions, most people fall around a total score of the 5 - 7  out of 12. There are similar numbers of 3, 4 to 8, 9, and very few 10 and above. About 1% of the people have a score above 10. Moving on to clustering:","2eb58f22":"There's nothing really indicating what's different about the groups. For the four clusters, the gender ratio, and transaction frequency ratios look roughly the same across the clusters, ie the ratio of M : F in cluster 3 appears to be the same ratio as cluster 0. Cluster 0, 1, 3 have the same account balance, and average transaction amount, while cluster 2 has some values exceeding these values. Age is really the same, adn the only other major difference is the recency (DaysSinceTransaction), where cluster 3 has mostly low recencies (as in days, not score), cluster 0 has middling, cluster 1 has high recency, however cluster 2 is all over the place.\n\nA somewhat similar story exists for the three cluster situation.","ec69eecc":"The average transaction is mostly constant for the transaction frequency range, except for the sharp increase in the range of 4 - 5 transactions over the three months, and a sudden sharp decrease for the most frequent (and significantly rarer) 6 transactions over that interval. Since 4 - 6 transactions all give a frequency score of 4, one would expect that they share the same average total score, however this is clearly not the case, with the average total score increasing from 4-6 transactions.\n\nWhen we look at the recency, we see that the average recency decreases with the average frequency (which shouldn't be shocking), which will compensate for the reduced average transactional amount.","2628d670":"There are over 1000 locations, so I believe they correspond to the bank's actual location as opposed to a providence. I'm going to just take the tenth largest location, as KMeans explodes with n","d3b87cd9":"The columns with NA values account for less than 1% of our data, so I'm just going to drop them. I don't understand why CustAccountBalance would have unknown values, the only thought I have is that they have no account, or a total balance of 0, so if I kept them, I would probaby just fill with 0 instead of a median value. CustGender could matter, that's something I'd have to explore before considering if I would want to nn impute this, and the location could matter as well, with the same deal. However, we have enough data to ignore doing this.","04bd34f0":"Selecting Clusters is a little tough. Two clusters, if we include the transaction frequency could make sense since that should leave the clusters as people with only one transaction, and people with multiple transactions. KMF analysis showed that there is no practical difference between these groups, however (look at either recenvy vs total score. That leaves either three or four clusters while excluding the frequency. The issue with three clusters is that one of the clusters has a below average silhouette score, while with four clusters, we have one class that has significantly lower numbers. Let's see what both have to offer.","0f89c724":"I'm going to ignore global clustering, which may give insights to topics such as rural vs urban, and may give insights to demographics, ie is there a difference in banking between more conservative and more liberals, or with respect to religiousosity. The dataset is too large for batchedkmeans to find clusters in a reasonable time.","de33b808":"All the trnasactions took place in a roughly two month period from August to October, this could account for the low transaction frequency"}}