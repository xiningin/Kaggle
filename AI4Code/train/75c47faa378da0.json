{"cell_type":{"8b466cca":"code","1c63ece1":"code","fa71714e":"code","e2c2da4e":"code","3a65df18":"code","06a2e662":"code","5f6b1732":"code","ce993f7b":"code","af3c8311":"code","f8d5e4f9":"code","42dbf9fb":"code","c29cd8d0":"code","31b34afa":"code","28b617f0":"code","75e76aeb":"code","37df8a6f":"code","6f7ae98d":"code","09d06c80":"code","ab14379e":"code","49e11ee8":"code","da3aa2ef":"code","f01e2a6b":"code","4e6504ec":"code","99c39f03":"code","ce969550":"code","7fce3038":"code","1f9d4225":"code","b5b16e46":"code","a856ee96":"code","6e8fac1e":"code","1cd1d53a":"code","781cb49c":"code","b965b32d":"code","544b4da0":"code","61e79f5f":"code","781d7d82":"code","80e29f7d":"code","e6ce768c":"code","45b6ed62":"code","c59f13ea":"code","599961e5":"code","80129687":"code","b1dad8d9":"code","552028f5":"code","88f60f5d":"code","f1ec49ce":"code","98dcfe7b":"code","981e0178":"code","49412cd6":"code","6ce402a1":"code","5e5da6f7":"code","54123b3a":"markdown","1cb09214":"markdown","090e3810":"markdown","66acb51f":"markdown","e465d9e1":"markdown","714945de":"markdown","40499b25":"markdown","e730ddc2":"markdown","08dcc30c":"markdown","b2d57d03":"markdown","bad84d76":"markdown","ff35e50b":"markdown","a557a292":"markdown","440bcdc0":"markdown","f74b8420":"markdown","54cd8cab":"markdown","73d626f6":"markdown","995beceb":"markdown"},"source":{"8b466cca":"# \uc2dc\ud5d8\ud658\uacbd \uc138\ud305 (\ucf54\ub4dc \ubcc0\uacbd X)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef exam_data_load(df, target, id_name=\"\", null_name=\"\"):\n    if id_name == \"\":\n        df = df.reset_index().rename(columns={\"index\": \"id\"})\n        id_name = 'id'\n    else:\n        id_name = id_name\n    \n    if null_name != \"\":\n        df[df == null_name] = np.nan\n    \n    X_train, X_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=2021)\n    y_train = X_train[[id_name, target]]\n    X_train = X_train.drop(columns=[id_name, target])\n    y_test = X_test[[id_name, target]]\n    X_test = X_test.drop(columns=[id_name, target])\n    return X_train, X_test, y_train, y_test \n    \ndf = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\nX_train, X_test, y_train, y_test = exam_data_load(df, target='income', null_name='?')\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","1c63ece1":"X_train.head()","fa71714e":"X_test.head()","e2c2da4e":"y_train.head()","3a65df18":"y_test.head()","06a2e662":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier","5f6b1732":"X_train.info()","ce993f7b":"# \uacb0\uce21\uce58 \ud30c\uc545\nX_train.isnull().sum()","af3c8311":"print(X_train['workclass'].unique())\nprint(X_train['occupation'].unique())\nprint(X_train['native.country'].unique())","f8d5e4f9":"# check the number of values in each category\nprint(X_train['workclass'].value_counts())\nprint(X_test['workclass'].value_counts())","42dbf9fb":"print(X_train['occupation'].value_counts())\nprint(X_test['occupation'].value_counts())","c29cd8d0":"print(X_train['native.country'].value_counts())\nprint(X_test['native.country'].value_counts())","31b34afa":"# Since not a few numbers are missing, we cannot ignore them\n# For the workclass and the native country, almost every value is concentrated to the most frequent category, so replace it by the mode of each category\n# However, for occupation, let's treat the missing values as another category, 'unknown'\n\nX_train['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\nX_train['native.country'].fillna(X_train['native.country'].mode()[0], inplace=True)\nX_train['occupation'].fillna('unknown', inplace=True)\n\nX_test['workclass'].fillna(X_test['workclass'].mode()[0], inplace=True)\nX_test['native.country'].fillna(X_test['native.country'].mode()[0], inplace=True)\nX_test['occupation'].fillna('unknown', inplace=True)","28b617f0":"X_train.isnull().sum()","75e76aeb":"# check if the education and education.num matches\ndef make_tuple(x):\n    return (x['education'], x['education.num'])\n\nedu = X_train[['education', 'education.num']].apply(make_tuple, axis=1)\nprint(edu.unique())","37df8a6f":"# Since education.num represents the education perfectly, drop the education feature\nX_train.drop(['education'], axis=1, inplace=True)\nX_test.drop(['education'], axis=1, inplace=True)","6f7ae98d":"# Training Set\ncat_features = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\nfor cat_col in cat_features:\n    le = LabelEncoder()\n    le.fit(X_train[cat_col])\n    \n    X_train[cat_col] = le.transform(X_train[cat_col])\n    X_test[cat_col] = le.transform(X_test[cat_col])\n\nX_train.head()","09d06c80":"# Test Set - >50k: 1, <=50k: 0\nonehot = OneHotEncoder()\nonehot.fit(y_train[['income']])\ny_train['income'] = onehot.transform(y_train[['income']]).toarray()[:, 1]\ny_test['income'] = onehot.transform(y_test[['income']]).toarray()[:, 1]","ab14379e":"X_train.describe()","49e11ee8":"num_features = ['age', 'fnlwgt', 'capital.gain', 'capital.loss', 'hours.per.week']\n\n# check skewness\nfor num_col in num_features:\n    print(num_col, 'skewness: %.4f' % (X_train[num_col].skew()))","da3aa2ef":"# Transform every variable to logarithmic scale\nfor num_col in num_features[:-1]:\n    if 0 in list(X_train[num_col]):\n        scaled = np.log1p(X_train[num_col])\n    else:\n        scaled = np.log(X_train[num_col])\n    \n    print(num_col, 'skewness: %.4f' % (scaled.skew()))","f01e2a6b":"# Transform\nfor num_col in num_features[:-1]:\n    if 0 in list(X_train[num_col]):\n        X_train[num_col] = np.log1p(X_train[num_col])\n    else:\n        X_train[num_col] = np.log(X_train[num_col])\n        \n    if 0 in list(X_test[num_col]):\n        X_test[num_col] = np.log1p(X_test[num_col])\n    else:\n        X_test[num_col] = np.log(X_test[num_col])\n\nX_train.head(3)","4e6504ec":"# And standardize the numerical features\nfor num_col in num_features:\n    std = StandardScaler()\n    std.fit(X_train[[num_col]])\n    X_train[num_col] = std.transform(X_train[[num_col]]).flatten()\n    X_test[num_col] = std.transform(X_test[[num_col]]).flatten()\n\nX_train.head(3)","99c39f03":"rf = RandomForestClassifier()\ngbc = GradientBoostingClassifier()\nxgb = XGBClassifier(eval_metric='mlogloss')","ce969550":"train_x, val_x, train_y, val_y = train_test_split(X_train, y_train['income'], test_size=0.2, \n                                                  shuffle=True, random_state=42, stratify=y_train['income'])\n\nmodels = [rf, gbc, xgb]\nfor model in models:\n    name = model.__class__.__name__\n    model.fit(train_x, train_y)\n    val_pred = model.predict(val_x)\n    accuracy = np.sum(val_pred == val_y) \/ val_y.shape[0]\n    f1 = f1_score(val_y, val_pred)\n    \n    print('%s - Accuracy: %.4f, F1: %.4f' % (name, accuracy, f1))","7fce3038":"# help(GradientBoostingClassifier)","1f9d4225":"param_grid = {\n    'n_estimators': [100, 200],\n    'learning_rate': [0.05, 0.1],\n    'max_depth': [3, 5]\n}\n\nsearch = GridSearchCV(gbc, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\nsearch.fit(X_train, y_train['income'])\nprint('Best params: {0} \\nBest score: {1}'.format(search.best_params_, search.best_score_))","b5b16e46":"final_gbc = GradientBoostingClassifier(**search.best_params_)\nfinal_gbc.fit(X_train, y_train['income'])\ny_pred = final_gbc.predict(X_test)\n\nprint('Test accuracy: %.2f, F1 score: %.4f' % (accuracy_score(y_test['income'], y_pred) * 100,\n                                              f1_score(y_test['income'], y_pred)))","a856ee96":"submission = pd.DataFrame({'id': y_test['id'], 'income': y_pred.astype(np.uint8)})\nsubmission.head()","6e8fac1e":"# \uc2dc\ud5d8\ud658\uacbd\uc5d0\uc11c\ub294 \uc544\ub798\uc640 \uac19\uc774 \uc81c\uacf5\ub428\n# import pandas as pd\n# X_test = pd.read_csv(\"data\/X_test.csv\")\n# X_train = pd.read_csv(\"data\/X_train.csv\")\n# y_train = pd.read_csv(\"data\/y_train.csv\")","1cd1d53a":"# import pandas as pd\n# import numpy as np","781cb49c":"# # \ub370\uc774\ud130 \ud06c\uae30 \ud655\uc778\n# X_train.shape, X_test.shape, y_train.shape, y_test.shape","b965b32d":"# # \ub370\uc774\ud130 \ud655\uc778\n# X_train.head()","544b4da0":"# # \ud0c0\uac9f \uc218 \ud655\uc778\n# y_train['income'].value_counts()","61e79f5f":"# # type\ud655\uc778\n# X_train.info()","781d7d82":"# # \ud53c\ucc98 \uad6c\ubd84\n# # Numeric features\n# numeric_features = [\n#                     'age',\n#                     'fnlwgt', \n#                     'education.num',\n#                     'capital.gain', \n#                     'capital.loss', \n#                     'hours.per.week',                     \n#                    ]\n\n# # Categorical features\n# cat_features = [\n#                  'workclass',              \n#                  'education',            \n#                  'marital.status', \n#                  'occupation', \n#                  'relationship', \n#                  'race', \n#                  'sex',\n#                  'native.country'\n# ]","80e29f7d":"# X_train[numeric_features].describe()","e6ce768c":"# X_train[cat_features].describe()","45b6ed62":"# X_train.isnull().sum()","c59f13ea":"# X_test.isnull().sum()","599961e5":"# X_train['workclass'].value_counts()","80129687":"# X_train['occupation'].value_counts()","b1dad8d9":"# X_train['native.country'].value_counts()","552028f5":"# def data_fillna(df):\n#     df['workclass'] = df['workclass'].fillna(df['workclass'].mode()[0])\n#     df['occupation'] = df['occupation'].fillna(\"null\")\n#     df['native.country'] = df[\"native.country\"].fillna(df['native.country'].mode()[0])\n#     return df\n\n# X_train = data_fillna(X_train)\n# X_test = data_fillna(X_test)\n\n# X_train.isnull().sum()","88f60f5d":"# # \ub77c\ubca8\uc778\ucf54\ub529\n# from sklearn.preprocessing import LabelEncoder\n\n# all_df = pd.concat([X_train.assign(ind=\"train\"), X_test.assign(ind=\"test\")])\n# le = LabelEncoder()\n# all_df[cat_features] = all_df[cat_features].apply(le.fit_transform)\n\n# X_train = all_df[all_df['ind'] == 'train']\n# X_train = X_train.drop('ind',axis=1)\n# X_train\n\n# X_test = all_df[all_df['ind'] == 'test']\n# X_test = X_test.drop('ind',axis=1)\n# X_test","f1ec49ce":"# # \uc2a4\ucf00\uc77c\ub9c1\n# from sklearn.preprocessing import MinMaxScaler\n\n# scaler = MinMaxScaler()\n# X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\n# X_test[numeric_features] = scaler.transform(X_test[numeric_features])\n# X_train","98dcfe7b":"# # target\uac12 \ubcc0\uacbd\n# y = (y_train['income'] != '<=50K').astype(int)\n# y[:5]","981e0178":"# # \ud559\uc2b5\uc6a9 \ub370\uc774\ud130\uc640 \uac80\uc99d\uc6a9 \ub370\uc774\ud130\ub85c \uad6c\ubd84\n# from sklearn.model_selection import train_test_split\n# X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, test_size=0.15, random_state=2021)\n# X_tr.shape, X_val.shape, y_tr.shape, y_val.shape","49412cd6":"# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.metrics import accuracy_score\n\n# model = DecisionTreeClassifier(random_state = 2022)\n# model.fit(X_tr, y_tr)\n# pred = model.predict(X_val)\n# print('accuracy score:', (accuracy_score(y_val, pred)))","6ce402a1":"# from sklearn.ensemble import RandomForestClassifier\n\n# model = RandomForestClassifier(random_state = 2022)\n# model.fit(X_tr, y_tr)\n# pred = model.predict(X_val)\n# print('accuracy score:', (accuracy_score(y_val, pred)))","5e5da6f7":"# y_test = (y_test['income'] != '<=50K').astype(int)\n# pred = model.predict(X_test)\n\n# from sklearn.metrics import accuracy_score\n# print('accuracy score:', (accuracy_score(y_test, pred)))","54123b3a":"# Numerical Features Preprocessing","1cb09214":"# Solution","090e3810":"# Model Selection","66acb51f":"# Library Import","e465d9e1":"## \uac80\uc99d\uc6a9 \ub370\uc774\ud130 \ubd84\ub9ac","714945de":"## \ub77c\uc774\ube0c\ub7ec\ub9ac \ubd88\ub7ec\uc624\uae30","40499b25":"## EDA","e730ddc2":"# Categorical Feature Preprocessing","08dcc30c":"# \uc0ac\uc6a9\uc790 \ucf54\ub529","b2d57d03":"## \ubaa8\ub378 & \ud3c9\uac00","bad84d76":"- \uacb0\uce21\uce58\ub294 \ucd5c\ube48\uac12\uacfc \ucc28\uc774\uac00 \ud06c\uba74 \ucd5c\ube48\uac12\uc73c\ub85c \uac12\uc774 \ube44\uc2b7\ud558\uba74 \ubcc4\ub3c4\uc758 \uac12\uc73c\ub85c \ub300\uccb4\ud568","ff35e50b":"# \uc131\uc778 \uc778\uad6c\uc870\uc0ac \uc18c\ub4dd \uc608\uce21\n\n- age: \ub098\uc774\n- workclass: \uace0\uc6a9 \ud615\ud0dc\n- fnlwgt: \uc0ac\ub78c\uc758 \ub300\ud45c\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uac00\uc911\uce58(final weight)\n- education: \uad50\uc721 \uc218\uc900\n- education.num: \uad50\uc721 \uc218\uc900 \uc218\uce58\n- marital.status: \uacb0\ud63c \uc0c1\ud0dc\n- occupation: \uc5c5\uc885\n- relationship: \uac00\uc871 \uad00\uacc4\n- race: \uc778\uc885\n- sex: \uc131\ubcc4\n- capital.gain: \uc591\ub3c4 \uc18c\ub4dd\n- capital.loss: \uc591\ub3c4 \uc190\uc2e4\n- hours.per.week: \uc8fc\ub2f9 \uadfc\ubb34 \uc2dc\uac04\n- native.country: \uad6d\uc801\n- income: \uc218\uc775 (\uc608\uce21\ud574\uc57c \ud558\ub294 \uac12)","a557a292":"## \ucc44\uc810 (\uc218\ud5d8\uc790\ub294 \ud655\uc778 \ubd88\uac00)","440bcdc0":"## \uacb0\uce21\uce58 \ucc98\ub9ac","f74b8420":"## \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30(\uc0dd\ub7b5)","54cd8cab":"## \ud53c\ucc98\uc5d4\uc9c0\ub2c8\uc5b4\ub9c1","73d626f6":"# How to fill categorical missing values\n\n* Ignore the observations with missing values\n    - If the number of the missing values is small\n* Replace it with the most frequent values\n    - If almost values are concentrated to the most frequent one\n* Build a model to predict the missing values\n* Treat missing values as another category\n    - If values are distributed equivalently over the categories","995beceb":"# Label Encoding\n"}}