{"cell_type":{"d2b30600":"code","28920128":"code","417eb456":"code","045b7553":"code","1d324b8c":"code","41c5eeb9":"code","db8e1534":"code","0d4deabb":"code","3efd3bd2":"code","a090c5fa":"code","891f454c":"code","012ca994":"code","fa5a882d":"code","ca5510dd":"code","86354ffc":"code","13f09e8d":"code","c9523746":"code","c434d9b3":"code","3472a3f5":"code","ba1ac3ef":"code","8454dcac":"code","cf9229f7":"code","af181cbc":"code","deb891aa":"code","83ec254f":"code","d666909c":"code","816a8d1a":"code","0bfa62ec":"code","b2d0ac8f":"code","0b3c5d15":"code","9a8a93b8":"code","1372e51c":"code","afb1b2ed":"code","dfea9578":"code","15d7f821":"code","07efa3bc":"code","152278fb":"code","617c4a52":"code","ea8bf1a4":"code","eb9bdabd":"code","912c12f2":"code","375d2438":"code","4371d63c":"code","b83194b3":"code","a512d574":"code","5993d13a":"code","241d4d3a":"code","e2c2bfb1":"code","24d1e30c":"code","ff03f9e3":"code","0c81b75f":"code","72563bf2":"code","556da113":"code","fc75b356":"code","d57173db":"code","43d3d50e":"code","fd093c47":"code","a61fa482":"code","29377bba":"code","1cd3f212":"code","e68efb55":"code","0f3a159e":"code","611c28d7":"code","572457d0":"markdown","054a6c01":"markdown","475e0e45":"markdown","aca5dc92":"markdown","21003b9c":"markdown","530fb188":"markdown","d4a44b25":"markdown","3f2844de":"markdown","dcd7c98e":"markdown","a6670a4c":"markdown","81e83c2c":"markdown","8310aa83":"markdown","11e968ef":"markdown","e65046a2":"markdown","78d46edf":"markdown","cc46f331":"markdown","b8fa60e3":"markdown","294c0b01":"markdown","c2291ff4":"markdown","4f9c5c7a":"markdown","baaaaf52":"markdown","dda32877":"markdown","2d6b4325":"markdown","22d5af2e":"markdown","fa511dac":"markdown","30d44800":"markdown"},"source":{"d2b30600":"!pip install pyspark","28920128":"from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('classification').getOrCreate()","417eb456":"from itertools import chain\nfrom pyspark.sql.functions import count, mean, when, lit, create_map, regexp_extract","045b7553":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd","1d324b8c":"df1 = spark.read.csv('..\/input\/titanic\/train.csv', header=True, inferSchema=True)\ndf2 = spark.read.csv('..\/input\/titanic\/test.csv', header=True, inferSchema=True)","41c5eeb9":"df1.show(15)","db8e1534":"print('Number of rows: \\t', df1.count())\nprint('Number of columns: \\t', len(df1.columns))","0d4deabb":"df1.printSchema()","3efd3bd2":"df1.printSchema()","a090c5fa":"df1.select('Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare').summary().show()","891f454c":"pandas_df = df1.toPandas()\n\nplt.figure(figsize=(10,5))\nplt.title('Age distribution among all Pasengers')\nsns.distplot(pandas_df['Age']);","012ca994":"plt.figure(figsize=(5,5))\nplt.title('Pclass distribution among all Pasengers')\nsns.distplot(pandas_df['Pclass']);","fa5a882d":"plt.figure(figsize=(10,5))\nplt.title('Fare distribution among all Pasengers')\nsns.distplot(pandas_df['Fare']);","ca5510dd":"plt.figure(figsize=(10,5))\nplt.title('SibSp distribution among all Pasengers')\nsns.distplot(pandas_df['SibSp']);","86354ffc":"df1.groupBy('Survived').count().show()","13f09e8d":"df1.groupBy('Survived').mean('Fare', 'Age', 'Parch', 'SibSp' ).show()","c9523746":"df1.groupBy('Survived').pivot('Sex').count().show()","c434d9b3":"df1.groupBy('Survived').pivot('Pclass').count().show()","3472a3f5":"df1.groupBy('Survived').pivot('Embarked').count().show()","ba1ac3ef":"for col in df1.columns:\n    print(col.ljust(20), df1.filter(df1[col].isNull()).count())","8454dcac":"df1 = df1.drop('PassengerID', 'Cabin', 'Ticket')","cf9229f7":"df1.select('Age', 'Embarked').summary('mean', '50%', 'max').show()","af181cbc":"df1 = df1.fillna({'Embarked': 'S'})","deb891aa":"df1.show(5)","83ec254f":"df1 = df1.withColumn('Title', regexp_extract(df1['Name'],'([A-Za-z]+)\\.', 1))\n\ndf1.groupBy('Title').agg(count('Age'), mean('Age')).sort('count(Age)').show()","d666909c":"df1.where(\" Title == 'Master' \").show()","816a8d1a":"title_dic = {'Mr':'Mr', 'Miss':'Miss', 'Mrs':'Mrs', 'Master':'Master', \\\n             'Mlle': 'Miss', 'Major': 'Mrs', 'Col': 'Mrs', 'Sir': 'Mrs',\\\n             'Don': 'Mrs', 'Mme': 'Miss', 'Jonkheer': 'Mrs', 'Lady': 'Mrs',\\\n             'Capt': 'Mrs', 'Countess': 'Mr', 'Ms': 'Miss', 'Dr':'Mrs', \\\n             'Rev':'Mrs'}\n\nmapping = create_map([lit(x) for x in chain(*title_dic.items())])\n\ndf1 = df1.withColumn('Title', mapping[df1['Title']])\ndf1.groupBy('Title').mean('Age').show()","0bfa62ec":"def age_imputer(df, title, age):\n    return df.withColumn('Age',when((df['Age'].isNull()) \\\n                            & (df['Title']==title), age).otherwise(df['Age']))","b2d0ac8f":"dic = {'Mr':33.02, 'Mrs':35.98, 'Miss':21.86, 'Master':4.75}\n\nfor i,j in dic.items():\n    df1 = age_imputer(df1, i, j)","0b3c5d15":"df1.show(5)","9a8a93b8":"df1 = df1.drop('Name', 'Title')","1372e51c":"df1 = df1.withColumn('FamilySize', df1['Parch'] + df1['SibSp']).\\\n            drop('Parch', 'SibSp')","afb1b2ed":"df1.show(15)","dfea9578":"for col in df1.columns:\n    print(col.ljust(20), df1.filter(df1[col].isNull()).count())","15d7f821":"from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder","07efa3bc":"stringIndex = StringIndexer(inputCols=['Sex', 'Embarked'], outputCols=['SexNum', 'EmbNum'])\nstringIndex_model = stringIndex.fit(df1)\ndf1_ = stringIndex_model.transform(df1).drop('Sex', 'Embarked')\ndf1_.show(5)","152278fb":"vec_asmbl = VectorAssembler(inputCols=df1_.columns[1:], outputCol='features')\ndf1_ = vec_asmbl.transform(df1_).select('features', 'Survived')","617c4a52":"df1_.show(5, truncate=False)","ea8bf1a4":"train_df, test_df = df1_.randomSplit([0.9, 0.1])","eb9bdabd":"train_df.show(5, truncate=False)","912c12f2":"evaluator = MulticlassClassificationEvaluator(labelCol='Survived', metricName='accuracy')\nmeans = []\nstd = []\nscore = 0","375d2438":"total = 0\nlista = []\ntmp = 0.1\nfor i in range(20):\n    lista.append(tmp + i*0.05)\nfor j in lista:\n    ridge = LogisticRegression(labelCol='Survived', \n                            maxIter=100, \n                            elasticNetParam=0, # Rigde\n                            regParam=j)\n\n    model = ridge.fit(train_df)\n    pred = model.transform(test_df)\n    tmp = evaluator.evaluate(pred)\n    total += tmp\n    print(tmp)\n    if tmp > score:\n        score = tmp\n        best_model = model\n        print('\u5f97\u5206\uff1a{} regParam\uff1a{}'.format(score,j))\n\ntotal \/= 20\nprint(total)\nmeans.append(total)","4371d63c":"total = 0\nlista = []\ntmp = 0.1\nfor i in range(20):\n    lista.append(tmp + i*0.05)\nfor j in lista:\n    lasso = LogisticRegression(labelCol='Survived', \n                            maxIter=100, \n                            elasticNetParam=1, # Lasso\n                            regParam=j)\n\n    model = lasso.fit(train_df)\n    pred = model.transform(test_df)\n    tmp = evaluator.evaluate(pred)\n    total += tmp\n    print(tmp)\n    if tmp > score:\n        score = tmp\n        best_model = model\n        print('\u5f97\u5206\uff1a{} regParam\uff1a{}'.format(score,j))\n\ntotal \/= 20\nprint(total)\nmeans.append(total)","b83194b3":"total = 0\nfor i in range(50, 251, 50):\n    for j in range(1,6):\n        rf = RandomForestClassifier(labelCol='Survived', numTrees=i, maxDepth=j)\n\n        model = rf.fit(train_df)\n        pred = model.transform(test_df)\n        tmp = evaluator.evaluate(pred)\n        total += tmp\n        print(tmp)\n        if tmp > score:\n            score = tmp\n            best_model = model\n            print('\u5f97\u5206\uff1a{} numTrees\uff1a{} maxDepth\uff1a{}'.format(score,i,j))\n\ntotal \/= 25\nprint(total)\nmeans.append(total)","a512d574":"#score = 0.87951","5993d13a":"total = 0\n\nfor j in range(1,6):\n    gb = GBTClassifier(labelCol='Survived', maxIter=100, maxDepth=j)\n\n    model = gb.fit(train_df)\n    pred = model.transform(test_df)\n    tmp = evaluator.evaluate(pred)\n    total += tmp\n    print(tmp)\n    if tmp > score:\n        score = tmp\n        best_model = model\n        print('\u5f97\u5206\uff1a{} maxDepth\uff1a{}'.format(score,j))\n\ntotal \/= 5\nprint(total)\nmeans.append(total)","241d4d3a":"means = [0.7271084337349394, 0.5831325301204818, 0.8457831325301204, 0.8626506024096386]","e2c2bfb1":"\nres = pd.DataFrame({\"CrossValMeans\":means,\n                    \"Algorithm\":[\"RidgeRegression\",\n                                \"LassoRegression\",\n                                \"RandomForestClassifier\",\n                                \"GBTClassifier\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = res, palette=\"Set3\",orient = \"h\")\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","24d1e30c":"df2.show(15)","ff03f9e3":"for col in df2.columns:\n    print(col.ljust(20), df2.filter(df2[col].isNull()).count())","0c81b75f":"df1.select('Fare').summary('mean', '50%', 'max').show()","72563bf2":"df2 = df2.fillna({'Embarked': 'S', 'Fare':14.45})\ndf2 = df2.withColumn('FamilySize', df2['Parch'] + df2['SibSp']).drop('Parch', 'SibSp')","556da113":"df2 = df2.withColumn('Title', regexp_extract(df2['Name'], '([A-Za-z]+)\\.', 1))\ndf2 = df2.withColumn('Title', mapping[df2['Title']])\ndf2.groupBy('Title').agg(count('Age'), mean('Age')).sort('count(Age)').show()","fc75b356":"dic = {'Mr':33.02, 'Mrs':35.98, 'Miss':21.86, 'Master':4.75}\n\nfor i,j in dic.items():\n    df2 = age_imputer(df2, i, j)\n\ndf2 = df2.drop('Cabin', 'Name', 'Ticket', 'Title') # keep PassengerId \ndf2.show(5)","d57173db":"for col in df2.columns:\n    print(col.ljust(20), df2.filter(df2[col].isNull()).count())","43d3d50e":"stringIndex = StringIndexer(inputCols=['Sex', 'Embarked'], outputCols=['SexNum', 'EmbNum'])\nstringIndex_model = stringIndex.fit(df2)\n\ndf2_ = stringIndex_model.transform(df2).drop('Sex', 'Embarked')\ndf2_.show(5)","fd093c47":"vec_asmbl = VectorAssembler(inputCols=df2_.columns[1:], outputCol='features')\n\ndf2_ = vec_asmbl.transform(df2_).select('features', 'PassengerId')\ndf2_.show(5, truncate=False)","a61fa482":"pred_test = best_model.transform(df2_)\n\npredictions = pred_test.select('PassengerId', 'prediction')\npredictions = predictions.withColumn('Survived', predictions['prediction'].cast('integer')).drop('prediction')\npredictions.show(15)","29377bba":"# Writing csv file in Spark \npredictions.coalesce(1).write.csv('submission_file.csv', header=True)","1cd3f212":"# Reading the saved file from spark \nspark.read.csv('submission_file.csv', header=True).show(4)","e68efb55":"# Writing csv file using Pandas \npredictions.toPandas().to_csv('submission.csv', index=False)","0f3a159e":"pd.read_csv('submission.csv').head()","611c28d7":"best_model.write().save('classification.model')","572457d0":"### \u975e\u7c7b\u522b\u53d8\u91cf \u00a0'Fare', 'Age', 'Parch', 'SibSp' \u7968\u4ef7\uff0c\u5e74\u9f84\uff0c\u7236\u6bcd\u6570\u91cf\uff0c\u5b69\u5b50\u6570\u91cf","054a6c01":"### \u7c7b\u522b\u53d8\u91cf \u00a0'Sex', 'Pclass', \u2018Embarked\u2019  \u7968\u4ef7\uff0c\u5e74\u9f84\uff0c\u767b\u8239\u6e2f\u53e3","475e0e45":"### \u7279\u5f81\u63d0\u53d6 \uff0c\u6e05\u6d17\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u3001\u9519\u8bef\u503c\u548c\u5f02\u5e38\u503c\n\n","aca5dc92":"\u4e2a\u522b\u6807\u7b7e\u60c5\u51b5\u5206\u6790","21003b9c":"### 5\u3001\u4f7f\u7528pyspark\u4e2d\u4efb\u610f\u4e00\u4e2a\u6709\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\u5728\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1b\n    \n    \u524d\u671f\u7684\u51c6\u5907\u5de5\u4f5c\u90fd\u5df2\u5b8c\u6210\uff0c\u5f00\u59cb\u6784\u5efa\u6a21\u578b\u5e76\u8bad\u7ec3\n    \n### 7\u3001\u4f7f\u7528sklearn\u8ba1\u7b97\u5206\u7c7b\u51c6\u786e\u7387\uff08\u5206\u7c7b\u95ee\u9898\uff09\u6216\u8005R\u65b9\uff08\u56de\u5f52\u95ee\u9898\uff09\n    \n    \u56e0\u4e3a\u6700\u540e\u7684test\u6570\u636e\u6ca1\u6709grand_truth\uff0c\u6240\u4ee5\u6211\u4eec\u5c31\u7528train\u91cc\u5206\u5272\u51fa\u7684test_df\u6765\u8ba1\u7b97\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u8ba1\u7b97\u4e0d\u540c\u7684\u53c2\u6570\u56db\u4e2a\u6a21\u578b\u7684\u51c6\u786e\u7387\u5e73\u5747\u503c\u5e76\u6bd4\u8f83\u753b\u56fe","530fb188":"### 3\u3001\u4f7f\u7528pyspark\u63d0\u53d6\u6587\u672c\u6570\u636e\u7684\u7279\u5f81\uff1b \n\n\u628a Sex \u548c Embarked \u4ece string \u8f6c\u6362\u6210\u5411\u91cf pyspark \u4e2d\u7684 StringIndexer, VectorAssembler\u5c31\u53ef\u4ee5\u5b9e\u73b0\u5411\u91cf\u5316","d4a44b25":"### ***2\u3001\u4f7f\u7528pyspark\u5bf9\u6570\u636e\u7279\u5f81\u5206\u5e03\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u6e05\u6d17\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u3001\u9519\u8bef\u503c\u548c\u5f02\u5e38\u503c\uff1b***","3f2844de":"Age \u7684\u7f3a\u5931\u503c\u5904\u7406\u5b8c\u4e4b\u540e\u628a'Name', 'Title'\u5217\u53bb\u6389\uff0c\u5e76\u4e14\u56e0\u4e3a SibSp \u4e0e Parch \u90fd\u662f\u63cf\u8ff0\u5bb6\u5ead\u6210\u5458\u7684\uff0c\u6240\u4ee5\u6211\u5c06\u8fd9\u4e24\u5217\u52a0\u548c\u5f62\u6210\u65b0\u5217 FamilySize","dcd7c98e":"spark \u8bfbcsv\u6587\u4ef6","a6670a4c":"1. Cabin    \u56e0\u4e3a\u6709\u5f88\u9ad8\u6bd4\u4f8b\u7684\u7f3a\u5931\u503c\uff0c\u5e76\u4e14 carbine\uff08\u4ed3\u53f7\uff09 \u4e0e pclass \uff08\u7532\u677f\u7b49\u7ea7\uff09\u6709\u5f88\u5927\u7684\u5173\u8054\uff0c\u6240\u4ee5\u6211\u9009\u62e9\u53bb\u9664 carbin \u8fd8\u6709\u4e00\u4e9b\u5bf9\u9884\u6d4b\u95ee\u9898\u6ca1\u6709\u592a\u5927\u4f5c\u7528\u7684\u5c5e\u6027\u4e00\u5e76\u53bb\u9664 \uff08'PassengerID', 'Cabin', 'Name', 'Ticket'\uff0cname \u5148\u4e0d\u53bb\u9664\uff0cage \u7684\u5904\u7406\u9700\u8981\u4f7f\u7528\uff09\n2. Embarked \u7684\u7f3a\u5931\u503c\u6211\u9009\u62e9\u7528\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684 s\uff08Southampton\uff09\u6765\u586b\u5145\n3. Age      \u53ef\u4ee5\u7b80\u5355\u5730\u7528\u5e73\u5747\u503c\u6216\u8005\u4e2d\u4f4d\u6570\u6765\u4ee3\u66ff\uff0c\u4f46\u662f\u8fd9\u6837\u5bf9\u4e8e Age \u7684\u62df\u5408\u5e76\u4e0d\u597d\uff0c\u4e0b\u9762\u6211\u7528\u4e86\u522b\u7684\u65b9\u6cd5\u4e0e\u5e73\u5747\u503c\u7ed3\u5408\u6765\u5904\u7406 ","81e83c2c":"\u56e0\u4e3a\u4e4b\u540e\u5904\u7406 test \u6570\u636e\u8fd8\u9700\u8981\u76f8\u540c\u7684\u5904\u7406\uff0c\u6240\u4ee5\u8fd9\u91cc\u6211\u628a Age \u7f3a\u5931\u503c\u7684\u5904\u7406\u5305\u88c5\u6210\u51fd\u6570","8310aa83":"\u6700\u540e\u68c0\u67e5\u9884\u6d4b\u6d4b\u8bd5\u96c6\u6837\u672c\u662f\u5426\u8fd8\u6709\u7a7a\u503c","11e968ef":"### 4\u3001\u4f7f\u7528pyspark.dataframe.randomSplit\u5c06\u6570\u636e\u96c6\u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u4e0e\u6d4b\u8bd5\u96c6\uff1b","e65046a2":"\u548cdf1\u4e00\u6837\u7684\u64cd\u4f5c","78d46edf":"\u56e0\u4e3a\u53ea\u6709\u4e0a\u8868\u6700\u540e\u7684\u56db\u4e2a tittle = [Master, Mrs, Miss, Mr] \u7684\u6570\u91cf\u5927\u4e8e10\uff0c\u5176\u4ed6\u7684tittle\u6570\u91cf\u592a\u5c11\uff0c\u53c2\u8003\u7684\u4ef7\u503c\u6bd4\u8f83\u4f4e\uff0c\u6240\u4ee5\u628a\u5176\u4ed6\u7684 tittle \u6620\u5c04\u6210\u8fd9\u56db\u4e2a","cc46f331":"### 6\u3001\u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6d4b\u8bd5\u96c6\u6837\u672c\u7684\u5206\u7c7b\u6216\u56de\u5f52\u503c\uff1b \n    \u6700\u540e\u6211\u4eec\u7528\u627e\u5230\u7684\u6700\u4f73\u6a21\u578bbest_model\u8fdb\u884c\u9884\u6d4b","b8fa60e3":"\u56e0\u4e3a Master \u7684\u5e73\u5747\u5e74\u9f84\u592a\u4f4e\uff0c\u6211\u6000\u7591\u6570\u636e\u53ef\u80fd\u51fa\u9519\uff0c\u6240\u4ee5\u628a Master \u7684\u6570\u636e\u90fd\u6253\u5370\u4e86\u51fa\u6765\uff0c\u8fd8\u662f\u5f88\u6b63\u5e38\u7684","294c0b01":"\u5176\u5b9e\u5e74\u9f84\u7684\u5904\u7406\u5c31\u662f\u7528\u4ed6\u7684\u5934\u8854\uff08title\uff0c\u6bd4\u5982 Mr \uff0cMiss\uff09\u7684\u5e73\u5747\u503c\u586b\u8865\u7f3a\u5931\u503c\n\u6240\u4ee5\u6211\u5148\u8ba1\u7b97\u5404\u4e2a title \u7684\u5e73\u5747\u503c","c2291ff4":"The following is the method to read csv file in Spark. We can even read the csv file using Spark API. But there is some problem with that, especially Pandas can not read that csv and submission through kernel does not work for it. For this reason we change the submission file to pandas and make a submission.  ","4f9c5c7a":"\u5404\u4e2a\u6307\u6807\u7684\u610f\u4e49\n\n1. PassengerId\uff1a \u4e58\u5ba2 ID\n2. Pclass\uff1a \u8231\u4f4d\u7b49\u7ea7 (1 = 1st, 2 = 2nd, 3 = 3rd)\n3. Name\uff1a \u4e58\u5ba2\u59d3\u540d\n4. Sex\uff1a \u6027\u522b\n5. Age\uff1a \u5e74\u9f84\n6. SibSp\uff1a \u5728\u8239\u4e0a\u7684\u5144\u5f1f\u59d0\u59b9\uff0f\u914d\u5076\u4e2a\u6570\n7. Parch\uff1a \u5728\u8239\u4e0a\u7684\u7236\u6bcd\uff0f\u5c0f\u5b69\u4e2a\u6570\n8. Ticket\uff1a \u8239\u7968\u4fe1\u606f\n9. Fare\uff1a \u7968\u4ef7\n10. Cabin\uff1a \u5ba2\u8231\n11. Embarked\uff1a \u767b\u8239\u6e2f\u53e3 (C = Cherbourg, Q = Queenstown, S = Southampton)","baaaaf52":"\u8fd9\u91cc\u6211\u9009\u62e9\u4e86\u56db\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1a\n   1. \u5cad\u56de\u5f52(Ridge regression\uff09\n   2. \u5957\u7d22\u56de\u5f52\uff08Lasso regression\uff09\n   3. \u968f\u673a\u68ee\u6797\uff08Random forest\uff09\n   4. \u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff08GBT\uff09\n  \n  \u5e76\u4e14\u5c1d\u8bd5\u5404\u79cd\u53c2\u6570\u6765\u627e\u5230\u6700\u4f73\u6a21\u578b\uff0c\u6211\u5b9a\u4e49\u4e86best_model\u7531\u4e8e\u4fdd\u5b58\u6700\u4f73\u6a21\u578b","dda32877":"\u5728test\u6570\u636e\u4e2d\uff0cEmbarked \u4e0e Fare \u90fd\u6709\u7f3a\u5931\u503c\uff0cEmbarked\u8fd9\u91cc\u91c7\u7528\u548c\u4e4b\u524d\u4e00\u6837\u7684\u5904\u7406\u7528\u51fa\u73b0\u6700\u591a\u7684\u2018S'\u4ee3\u66ff\uff0cFare\u5c31\u7b80\u5355\u7684\u7528\u4e2d\u4f4d\u6570\u4ee3\u66ff","2d6b4325":"\u5f00\u542f SparkSession \u5e76\u521b\u5efa spark \u5b9e\u4f8b","22d5af2e":"\u6700\u540e\u68c0\u67e5\u662f\u5426\u8fd8\u6709\u7f3a\u5931\u503c","fa511dac":"\u6807\u7b7e\u603b\u4f53\u60c5\u51b5","30d44800":"### ***1\u3001\u4f7f\u7528pyspark\u5bf9\u6570\u636e\u96c6\u6807\u7b7e\u5206\u5e03\u60c5\u51b5\u8fdb\u884c\u5206\u6790\uff1b***"}}