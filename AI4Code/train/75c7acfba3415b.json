{"cell_type":{"efc611e4":"code","6395ba08":"code","33298d82":"code","909c5bc5":"code","da6f3641":"code","3dea0b5a":"code","5e9957e4":"code","a095df82":"code","bbfa77cb":"code","d640f060":"code","6bc60395":"code","a700f723":"code","3f020505":"code","1f58ee25":"code","4bd4d645":"code","86e3d275":"code","2eb9dfae":"code","0d41c43b":"code","42101101":"code","adbaaa7b":"code","b9124064":"code","c25724e9":"code","a80f92f1":"code","49d60a01":"code","808b0cd3":"code","94d568b3":"code","5101845b":"code","a0546d7a":"code","e67ea042":"code","3d031b32":"code","3dadb32e":"code","adfe07e6":"code","3937932e":"code","3bf575fe":"code","0fe34351":"code","421c3d95":"code","5fe29b9b":"code","03b6944b":"code","6c048410":"code","248ae8fa":"code","f26ebfab":"code","8a722e36":"code","2c33e194":"code","e883e151":"code","4c2b46fc":"markdown","c5481840":"markdown","034192f7":"markdown","95a22771":"markdown","8abad895":"markdown","9a8674b1":"markdown","9df3f205":"markdown","601fc392":"markdown","1b09fba8":"markdown","61a29d5a":"markdown","08ebf70b":"markdown","432e2394":"markdown","37fff9c8":"markdown","24e2a4b7":"markdown","a8dbb3d6":"markdown","478f01dc":"markdown","048214f8":"markdown","250b56dd":"markdown","e899943d":"markdown","e368aa7c":"markdown","49890a8d":"markdown","943e29fe":"markdown"},"source":{"efc611e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6395ba08":"import seaborn as sns\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nSEED=2020\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()","33298d82":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n## thanks to @Nadezda Demidova  https:\/\/www.kaggle.com\/demidova\/titanic-eda-tutorial-with-seaborn\ntrain.loc[train['PassengerId'] == 631, 'Age'] = 48\n\n# Passengers with wrong number of siblings and parch\ntrain.loc[train['PassengerId'] == 69, ['SibSp', 'Parch']] = [0,0]\ntest.loc[test['PassengerId'] == 1106, ['SibSp', 'Parch']] = [0,0]\n## to reduce the amount of code let's introduce a general frame\nfull_data = [train, test]\n","909c5bc5":"## train dataset overview\ntrain.head()","da6f3641":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.7 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n#detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain.loc[Outliers_to_drop] # Show the outliers rows\n","3dea0b5a":"train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","5e9957e4":"## train dataset overview\n\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value, %'] = round(df.isnull().sum()\/df.shape[0]*100)\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\n\nbasic_details(train)","a095df82":"basic_details(test)","bbfa77cb":"train.info()","d640f060":"full_data = [train, test]\n\nfor df in full_data:\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n    df['Has_Cabin'] = df[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n    df.drop([\"PassengerId\", 'Cabin'], axis=1, inplace=True)\n## and we have 2 NaN in train dataser for Embarked feature, so let's impute it with most frequent value 'S'\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n","6bc60395":"## the same for test dataset\ntest.info()","a700f723":"#we have 1 NaN in test dataser for Fare feature, so let's impute it...\ntest['Fare'].fillna(test[\"Fare\"].median(), inplace=True)","3f020505":"## Has_Cabin feature survival rate\ntrain[[\"Has_Cabin\", \"Survived\"]].groupby(['Has_Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)","1f58ee25":"## checking for Embarked feature distribution before we'll make it numerical\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4bd4d645":"## Making from categorial Embarked -  numerical feature\nfor df in full_data:\n    df[\"Embarked\"][df[\"Embarked\"] == \"S\"] = 1\n    df[\"Embarked\"][df[\"Embarked\"] == \"C\"] = 2\n    df[\"Embarked\"][df[\"Embarked\"] == \"Q\"] = 3\n    df[\"Embarked\"] = df[\"Embarked\"].astype(int)","86e3d275":"## new feature Name length, later we will check it's influence on target\nfor df in full_data:\n    df['Name_length'] = df['Name'].apply(len)\n\n# New Title feature\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor df in full_data:\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    df['Title'] = df['Title'].map(title_mapping)\n    df['Title'] = df['Title'].fillna(0)\n\n\n##dropping Name feature from both datasets\nfor df in full_data:\n    df.drop(['Name'], axis=1,inplace=True)\n","2eb9dfae":"# Convert 'Sex' variable to integer form\nfor df in full_data:\n    df[\"Sex\"][df[\"Sex\"] == \"male\"] = 1\n    df[\"Sex\"][df[\"Sex\"] == \"female\"] = 0\n    df[\"Sex\"] = df[\"Sex\"].astype(int)","0d41c43b":"# New 'FamilySize' feature\nfor df in full_data:\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n","42101101":"## checking for Survived dependence of FamilySize feature\ntrain[[\"FamilySize\", \"Survived\"]].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","adbaaa7b":"# FamilySize distribution\ng = sns.kdeplot(train['FamilySize'][(train[\"Survived\"] == 0) & (train['FamilySize'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['FamilySize'][(train[\"Survived\"] == 1) & (train['FamilySize'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('FamilySize')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","b9124064":"## making new FamilySize_cat feature based on above mentioned conclusion and let's see its distribution\nbins = [0,1,4,11]\nlabels=[0,1,2]\ntrain['FamilySize_cat'] = pd.cut(train['FamilySize'], bins=bins, labels=labels)\ntest['FamilySize_cat'] = pd.cut(test['FamilySize'], bins=bins, labels=labels)\ntrain[[\"FamilySize_cat\", \"Survived\"]].groupby(['FamilySize_cat'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c25724e9":"plt.xticks([0,12,29,50, 80])\n\ng = sns.kdeplot(train['Name_length'][(train[\"Survived\"] == 0) & (train['Name_length'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['Name_length'][(train[\"Survived\"] == 1) & (train['Name_length'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Name_length')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","a80f92f1":"## let's make new cat feature based on this conclusion:\n\nbins = [11,29,100]\nlabels=[0,1]\ntrain['Name_length_cat'] = pd.cut(train['Name_length'], bins=bins, labels=labels)\ntest['Name_length_cat'] = pd.cut(test['Name_length'], bins=bins, labels=labels)\ntrain[[\"Name_length_cat\", \"Survived\"]].groupby(['Name_length_cat'], as_index=False).mean().sort_values(by='Survived', ascending=False)","49d60a01":"plt.xticks([0,16,32,60,100])\n\ng = sns.kdeplot(train['Age'][(train[\"Survived\"] == 0) & (train['Age'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['Age'][(train[\"Survived\"] == 1) & (train['Age'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Age')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","808b0cd3":"## new Age_cat feature based on this conclusions\nbins = [0,16,32,60,100]\nlabels=[0,1,2,3]\ntrain['Age_cat'] = pd.cut(train['Age'], bins=bins, labels=labels)\ntest['Age_cat'] = pd.cut(test['Age'], bins=bins, labels=labels)\ntrain[[\"Age_cat\", \"Survived\"]].groupby(['Age_cat'], as_index=False).mean().sort_values(by='Survived', ascending=False)","94d568b3":"for df in full_data:\n    df['Ticket'] = df['Ticket'].apply(len)","5101845b":"g = sns.kdeplot(train['Ticket'][(train[\"Survived\"] == 0) & (train['Ticket'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['Ticket'][(train[\"Survived\"] == 1) & (train['Ticket'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Ticket')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","a0546d7a":"for df in full_data:\n    df['Ticket_5'] = train['Ticket'].map(lambda x: 1 if x == 5 else 0)\n    df['Ticket_6'] = train['Ticket'].map(lambda x: 1 if x == 6 else 0)\n\ntrain[[\"Ticket_5\", \"Survived\"]].groupby(['Ticket_5'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e67ea042":"train[[\"Ticket_6\", \"Survived\"]].groupby(['Ticket_6'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3d031b32":"sns.set(rc={'figure.figsize':(20,10)})\nplt.xticks([0,8,18,28,75,100,150,200])\n\n\n\ng = sns.kdeplot(train['Fare'][(train[\"Survived\"] == 0) & (train['Fare'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['Fare'][(train[\"Survived\"] == 1) & (train['Fare'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Fare')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","3dadb32e":"## making bins based on picture info for new feature Fare_cat\nbins = [-1,8,18,28,520]\nlabels=[0,1,2,3]\ntrain['Fare_cat'] = pd.cut(train['Fare'], bins=bins, labels=labels)\ntest['Fare_cat'] = pd.cut(test['Fare'], bins=bins, labels=labels)\ntrain[[\"Fare_cat\", \"Survived\"]].groupby(['Fare_cat'], as_index=False).mean().sort_values(by='Survived', ascending=False)","adfe07e6":"features_to_drop = ['Age','Fare','SibSp', 'Parch', 'Ticket', 'Name_length', 'FamilySize']\nfor df in full_data:\n    df[\"FamilySize_cat\"] = df[\"FamilySize_cat\"].astype(int)\n    df[\"Age_cat\"] = df[\"Age_cat\"].astype(int)\n    df[\"Fare_cat\"] = df[\"Fare_cat\"].astype(int)\n    df[\"Name_length_cat\"] = df[\"Name_length_cat\"].astype(int)\n    df['Age_scaled'] = ss.fit_transform(df['Age'].values.reshape(-1,1)) ## new feature based on Age\n    df['Fare_scaled'] = ss.fit_transform(df['Fare'].values.reshape(-1,1)) ## new feature based on Fare\n    df['Name_length_log'] = np.log1p(df['Name_length']) # new normalized feature on base of Name_lenght \n    df.drop(features_to_drop, axis=1, inplace=True) ## drop unneccessary features","3937932e":"## train dataset before modelling\ntrain.head(10)","3bf575fe":"## corrmatrix\ndef spearman(frame, features):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame['Survived'], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    plt.figure(figsize=(6, 0.25*len(features)))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n    \nfeatures = train.drop(['Survived'], axis=1).columns\nspearman(train, features)","0fe34351":"x = train.drop('Survived', axis=1)\ny = train.Survived","421c3d95":"# sm = SMOTE(random_state=SEED)\n# smk=SMOTETomek(random_state=SEED)\n# rus = RandomUnderSampler(random_state=SEED)\nros = RandomOverSampler(random_state=SEED)\n# adasyn = ADASYN(random_state=SEED)\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=SEED)\nx_train, y_train= ros.fit_resample(x_train, y_train)\n# x, y= ros.fit_resample(x, y)","5fe29b9b":"from sklearn.model_selection import RepeatedStratifiedKFold\ncv_split = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=SEED)\nlogreg = LogisticRegression(class_weight='balanced', penalty='l1', C=1, solver='liblinear')\nlogreg.fit(x_train, y_train)\nscores = cross_val_score(logreg, x, y, cv=cv_split, scoring = 'accuracy')\nprint(\"Logistic Regression based on full dataset\")\nprint(\"RF CV Accuracy Score Mean:\", scores.mean())","03b6944b":"import eli5 #for purmutation importance of features for Logistic Regression\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(logreg, random_state=1).fit(x, y)\neli5.show_weights(perm, feature_names = x.columns.tolist())","6c048410":"from sklearn.model_selection import GridSearchCV\nparams = {'n_estimators':[70,80,90,100,110],'max_depth': [18,19,20,21,22], 'min_samples_split':[2,3], 'min_samples_leaf':[1,2,3] }\nclf_rf = RandomForestClassifier(n_jobs= -1, class_weight='balanced')\ngrid = GridSearchCV(clf_rf, params, cv=5)\ngrid.fit(x_train, y_train)\n## best params of rf model\ngrid.best_params_","248ae8fa":"## best estimator score\nbest_clf = grid.best_estimator_\nscores = cross_val_score(best_clf, x, y, cv=cv_split, scoring = 'accuracy')\nprint(\"RF based on selected dataset\")\nprint(\"FR CV Accuracy Score after selection:\", scores.mean().round(3))\n","f26ebfab":"## best classifier feature importance list\nfi = best_clf.feature_importances_\nfeature_importance = pd.DataFrame({'importance':fi}, index=x_valid.columns)\nfeature_importance.sort_values('importance', ascending=False)","8a722e36":"## the same in plot\nfeature_importance.sort_values('importance').plot(kind='barh', figsize=(12, 8))","2c33e194":"import shap #for SHAP values\nexplainer = shap.TreeExplainer(best_clf)\nshap_values = explainer.shap_values(x_valid)\nshap.summary_plot(shap_values[1], x_valid, plot_type=\"bar\")","e883e151":"shap.summary_plot(shap_values[1], x_valid)","4c2b46fc":"I see high survival rate in families with 2, 3 and 4 members...Let's see plots","c5481840":"Let's load datasets","034192f7":"\n**Passengers with Name_length not more than 29 letters have low survival rate: 60% vs 30% (see below)**","95a22771":"first of all let's find and drop outliers (but with outlier step 1.7 * IQR for best perfomance)...","8abad895":"Next feature for review - Name_lenght, let's see for its distribution...","9a8674b1":"Ticket feature","9df3f205":"Note: C = Cherbourg, Q = Queenstown, S = Southampton\n\n**So Cherbourg has high survival rate**","601fc392":"Some conclusions:\n* Children till 16 years old have hight survival rate\n* Passengers from 17 till 32 and up from 60 years old have low survival rate","1b09fba8":"60% vs 30%!!!","61a29d5a":"so we've imputed all NaNs and lets start with feature engeneering....","08ebf70b":"First of all let's arrange missing values, and I decided to drop Cabin feature due to 77% of missing values.\nAs for Age feature - just make simple imputation","432e2394":"**Permutation importance** is one of the first tools for understanding a  model and involves shuffling features in the validation data and seeing the effect on accuracy","37fff9c8":"the same for test dataset...","24e2a4b7":"**Logistic Regression**","a8dbb3d6":"Missing and unique values overview...","478f01dc":"Seems tickets with lenght of 6 digits have low survival rate and with len of 5 digits vice versa","048214f8":"loading libraries","250b56dd":"**Fare feature analysis**","e899943d":"Conclusions:\n* Alone passangers have low survival rate\n* Passengers in families with more than 4 members have low survival rate\n* High survival rate have passengers in families with 2, 3 and 4 members","e368aa7c":"(outliers by Fare feature)","49890a8d":"**Random Forest Classifier**","943e29fe":"**Next feature Age review**"}}