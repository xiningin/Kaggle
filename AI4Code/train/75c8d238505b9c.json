{"cell_type":{"ff18a9c1":"code","3683d6c4":"code","ca4bfe8f":"code","d387e8ec":"code","2a41439f":"code","32c79c87":"code","d4799436":"code","2f0b31ae":"code","f2cee0ad":"code","ad4b4ebf":"code","bc1eaf89":"code","48c9a4f7":"code","a83f6d55":"code","727f9af7":"code","780075ae":"code","bcf38dcd":"code","f8c2e343":"code","af29d887":"code","726b0c86":"code","d4655216":"code","ddcd6417":"code","acbca507":"code","44192fd1":"code","ef500c5a":"code","b3c70dce":"code","b67eed0b":"code","a5f1f6b6":"code","9e2b0c6a":"code","6909a6a3":"code","a69293f4":"code","0594d173":"code","1c7c982c":"code","359cd611":"code","1af14a85":"code","813db0ae":"code","d3d89011":"code","1998fc13":"code","5b95cd89":"code","90f3c05e":"code","96c564c8":"code","8e2f624f":"code","e6192c7d":"code","b34e8d23":"code","b072ec3c":"code","8f7b7905":"code","d909d540":"markdown","3f17a909":"markdown","3b8e7692":"markdown","f2c91a5c":"markdown","1434b267":"markdown","11adf0e3":"markdown","69ea1568":"markdown","4c516b01":"markdown","b70d922b":"markdown"},"source":{"ff18a9c1":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nimport torch.utils.data\nfrom tqdm import tqdm\nimport warnings\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom gensim.models import KeyedVectors\n\nimport copy\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nimport string\nimport re\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import metrics\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\npd.set_option('max_colwidth',400)\npd.set_option('max_columns', 50)\nimport json\nimport gc\nimport os\n\nimport copy\n\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pickle","3683d6c4":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    #with open(path,'rb') as f:\n    emb_arr = KeyedVectors.load(path)\n    return emb_arr\n\ndef build_matrix(word_index, path, dim=300):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, dim))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n    \nclass SoftmaxPooling(nn.Module):\n    def __init__(self, dim=1):\n        super(self.__class__, self).__init__()\n        self.dim = dim\n        \n    def forward(self, x):\n        return (x * x.softmax(dim=self.dim)).sum(dim=self.dim)\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n        \n        self.linear_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        )\n        \n        self.linear_aux_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        )\n        \n        self.softmaxpool = SoftmaxPooling()\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        # softmax pooling\n        soft_pool = self.softmaxpool(h_lstm2)\n        \n        h_conc = torch.cat((max_pool, avg_pool, soft_pool), 1)\n        \n        hidden = h_conc\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n    \ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","ca4bfe8f":"warnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nSEED = 1234\nBATCH_SIZE = 512\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/crawl-300d-2M.gensim'\nNUMBERBATCH_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/numberbatch-en.gensim'\nPARAGRAM_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/paragram_300_sl999.gensim'\nTWITTER_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/glove.twitter.27B.200d.gensim'\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","d387e8ec":"# only here the values are like this\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 768","2a41439f":"x_train = pd.read_csv('..\/input\/jigsawbiaspreprocessed\/x_train.csv', header=None)[0].astype('str')\ny_aux_train = np.load('..\/input\/jigsawbiaspreprocessed\/y_aux_train.npy')\ny_train = np.load('..\/input\/jigsawbiaspreprocessed\/y_train.npy')\n\nloss_weight = 3.209226860170181\n\nmax_features = 400000","32c79c87":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\nannot_idx = train[train['identity_annotator_count'] > 0].sample(n=48660, random_state=13).index\nnot_annot_idx = train[train['identity_annotator_count'] == 0].sample(n=48660, random_state=13).index\nx_val_idx = list(set(annot_idx).union(set(not_annot_idx)))","d4799436":"X_val = x_train.loc[x_val_idx]\nY_val = y_train[x_val_idx]\nY_aux_val = y_aux_train[x_val_idx]","2f0b31ae":"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","f2cee0ad":"with open('..\/input\/bilstm-crawl-paragram-0\/tokenizer.pickle', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nparagram_matrix, unknown_words_paragram = build_matrix(tokenizer.word_index, PARAGRAM_EMBEDDING_PATH)\nprint('n unknown words (paragram): ', len(unknown_words_paragram))\n\nembedding_matrix = np.concatenate([crawl_matrix, paragram_matrix], axis=-1)\nprint(embedding_matrix.shape)\n\ndel crawl_matrix\ndel paragram_matrix\ngc.collect()","ad4b4ebf":"X_val_seq = tokenizer.texts_to_sequences(X_val)","bc1eaf89":"maxlen = 300\nval_lengths = torch.from_numpy(np.array([len(x) for x in X_val_seq]))\n\nX_val_padded = torch.from_numpy(sequence.pad_sequences(X_val_seq, maxlen=maxlen))","48c9a4f7":"batch_size = 512\ntest_dataset = data.TensorDataset(X_val_padded, val_lengths)\ntest_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), sequence_index=0, length_index=1)\ntest_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=test_collator)\n\ndatabunch = DataBunch(train_dl=test_loader, valid_dl=test_loader, collate_fn=test_collator)","a83f6d55":"def test_model_cp(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/bilstm-crawl-paragram-{}\/model_1_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(X_val_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","727f9af7":"val_df = train.loc[x_val_idx]","780075ae":"for i in tqdm(range(3)):\n    for j in tqdm(range(5)):\n        y_pred = test_model_cp(i, j)\n        val_df['lstm_cp_{}_{}'.format(i, j)] = y_pred[:, 0]","bcf38dcd":"# since this one - like this\nLSTM_UNITS = 256\nDENSE_HIDDEN_UNITS = 1536","f8c2e343":"def test_model_clip(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/lstm-clip-{}\/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(X_val_seq), output_dim)) \n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","af29d887":"for i in tqdm(range(3)):\n    for j in tqdm(range(5)):\n        y_pred = test_model_clip(i, j)\n        val_df['lstm_clip_{}_{}'.format(i, j)] = y_pred[:, 0]","726b0c86":"numberbatch_matrix, unknown_words_numberbatch = build_matrix(tokenizer.word_index, NUMBERBATCH_EMBEDDING_PATH)\nprint('n unknown words (numberbatch): ', len(unknown_words_numberbatch))\n\nembedding_matrix = numberbatch_matrix\nprint(embedding_matrix.shape)\n\ndel numberbatch_matrix\ngc.collect()","d4655216":"def test_model_nb(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/lstm-numberbatch-{}\/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(X_val_seq), output_dim))\n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","ddcd6417":"for i in tqdm(range(3)):\n    for j in tqdm(range(5)):\n        y_pred = test_model_nb(i, j)\n        val_df['lstm_nb_{}_{}'.format(i, j)] = y_pred[:, 0]","acbca507":"twitter_matrix, unknown_words_twitter = build_matrix(tokenizer.word_index, TWITTER_EMBEDDING_PATH, dim=200)\nprint('n unknown words (twitter): ', len(unknown_words_twitter))\n\nembedding_matrix = twitter_matrix\nprint(embedding_matrix.shape)\n\ndel twitter_matrix\ngc.collect()","44192fd1":"def test_model_tw(model_num, model_epoch,output_dim=7):\n    model = NeuralNet(embedding_matrix, 6).cuda()\n    model.load_state_dict(torch.load('..\/input\/lstm-twitter-{}\/model_{}.pth'.format(model_num, model_epoch))['model'])\n    model.eval()\n    with torch.no_grad():\n        test_preds = np.zeros((len(X_val_seq), output_dim))\n        for i, x_batch in enumerate(test_loader):\n            X = x_batch[0].cuda()\n            y_pred = sigmoid(model(X).detach().cpu().numpy())\n            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n    return test_preds","ef500c5a":"for i in tqdm(range(3)):\n    for j in tqdm(range(5)):\n        y_pred = test_model_tw(i, j)\n        val_df['lstm_tw_{}_{}'.format(i, j)] = y_pred[:, 0]","b3c70dce":"class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\nrepl = {\n    \"&lt;3\": \" good \",\n    \":d\": \" good \",\n    \":dd\": \" good \",\n    \":p\": \" good \",\n    \"8)\": \" good \",\n    \":-)\": \" good \",\n    \":)\": \" good \",\n    \";)\": \" good \",\n    \"(-:\": \" good \",\n    \"(:\": \" good \",\n    \"yay!\": \" good \",\n    \"yay\": \" good \",\n    \"yaay\": \" good \",\n    \"yaaay\": \" good \",\n    \"yaaaay\": \" good \",\n    \"yaaaaay\": \" good \",\n    \":\/\": \" bad \",\n    \":&gt;\": \" sad \",\n    \":')\": \" sad \",\n    \":-(\": \" bad \",\n    \":(\": \" bad \",\n    \":s\": \" bad \",\n    \":-s\": \" bad \",\n    \"&lt;3\": \" heart \",\n    \":d\": \" smile \",\n    \":p\": \" smile \",\n    \":dd\": \" smile \",\n    \"8)\": \" smile \",\n    \":-)\": \" smile \",\n    \":)\": \" smile \",\n    \";)\": \" smile \",\n    \"(-:\": \" smile \",\n    \"(:\": \" smile \",\n    \":\/\": \" worry \",\n    \":&gt;\": \" angry \",\n    \":')\": \" sad \",\n    \":-(\": \" sad \",\n    \":(\": \" sad \",\n    \":s\": \" sad \",\n    \":-s\": \" sad \",\n    r\"\\br\\b\": \"are\",\n    r\"\\bu\\b\": \"you\",\n    r\"\\bhaha\\b\": \"ha\",\n    r\"\\bhahaha\\b\": \"ha\",\n    r\"\\bdon't\\b\": \"do not\",\n    r\"\\bdoesn't\\b\": \"does not\",\n    r\"\\bdidn't\\b\": \"did not\",\n    r\"\\bhasn't\\b\": \"has not\",\n    r\"\\bhaven't\\b\": \"have not\",\n    r\"\\bhadn't\\b\": \"had not\",\n    r\"\\bwon't\\b\": \"will not\",\n    r\"\\bwouldn't\\b\": \"would not\",\n    r\"\\bcan't\\b\": \"can not\",\n    r\"\\bcannot\\b\": \"can not\",\n    r\"\\bi'm\\b\": \"i am\",\n    \"m\": \"am\",\n    \"r\": \"are\",\n    \"u\": \"you\",\n    \"haha\": \"ha\",\n    \"hahaha\": \"ha\",\n    \"don't\": \"do not\",\n    \"doesn't\": \"does not\",\n    \"didn't\": \"did not\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"hadn't\": \"had not\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"can't\": \"can not\",\n    \"cannot\": \"can not\",\n    \"i'm\": \"i am\",\n    \"m\": \"am\",\n    \"i'll\" : \"i will\",\n    \"its\" : \"it is\",\n    \"it's\" : \"it is\",\n    \"'s\" : \" is\",\n    \"that's\" : \"that is\",\n    \"weren't\" : \"were not\",\n}\n\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s):\n    return re_tok.sub(r' \\1 ', s).split()\n\ncont_patterns = [\n        (b'US', b'United States'),\n        (b'IT', b'Information Technology'),\n        (b'(W|w)on\\'t', b'will not'),\n        (b'(C|c)an\\'t', b'can not'),\n        (b'(I|i)\\'m', b'i am'),\n        (b'(A|a)in\\'t', b'is not'),\n        (b'(\\w+)\\'ll', b'\\g<1> will'),\n        (b'(\\w+)n\\'t', b'\\g<1> not'),\n        (b'(\\w+)\\'ve', b'\\g<1> have'),\n        (b'(\\w+)\\'s', b'\\g<1> is'),\n        (b'(\\w+)\\'re', b'\\g<1> are'),\n        (b'(\\w+)\\'d', b'\\g<1> would'),\n    ]\npatterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n\ndef prepare_for_char_n_gram(text):\n    \"\"\" Simple text clean up process\"\"\"\n    # 1. Go to lower case (only good for english)\n    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n    clean = bytes(text.lower(), encoding=\"utf-8\")\n    # 2. Drop \\n and  \\t\n    clean = clean.replace(b\"\\n\", b\" \")\n    clean = clean.replace(b\"\\t\", b\" \")\n    clean = clean.replace(b\"\\b\", b\" \")\n    clean = clean.replace(b\"\\r\", b\" \")\n    # 3. Replace english contractions\n    for (pattern, repl) in patterns:\n        clean = re.sub(pattern, repl, clean)\n    # 4. Drop puntuation\n    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n    clean = re.sub(b\"\\d+\", b\" \", clean)\n    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n    clean = re.sub(b'\\s+', b' ', clean)\n    # Remove ending space if any\n    clean = re.sub(b'\\s+$', b'', clean)\n    # 7. Now replace words by words surrounded by # signs\n    # e.g. my name is bond would become #my# #name# #is# #bond#\n    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n\n    return str(clean, 'utf-8')\n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))\n\ndef get_indicators_and_clean_comments(df):\n    \"\"\"\n    Check all sorts of content as it may help find toxic comment\n    Though I'm not sure all of them improve scores\n    \"\"\"\n    # Count number of \\n\n#     df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n    df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n    # TODO chars per row\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n    # Number of F words - f..k contains folk, fork,\n    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n    # Number of S word\n    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n    # Number of D words\n    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n    # Number of occurence of You, insulting someone usually needs someone called : you\n    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n    # Just to check you really refered to my mother ;-)\n    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n    # Just checking for toxic 19th century vocabulary\n    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n    # Some Sentences start with a <:> so it may help\n    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n    # Check for time stamp\n    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n    # Check for dates 18:44, 8 December 2010\n    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n    # Check for date short 8 December 2010\n    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n    # Check for http links\n#     df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}:\/\/\\S+\", x))\n    # check for mail\n    df[\"has_mail\"] = df[\"comment_text\"].apply(\n        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n    )\n    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n\n    # Now clean comments\n    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n\n    # Get the new length in words and characters\n    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n    # Number of different characters used in a comment\n    # Using the f word only will reduce the number of letters required in the comment\n    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) \/ df[\"clean_comment\"].apply(\n        lambda x: 1 + min(99, len(x)))\n\nfts = [\"raw_word_len\", \"raw_char_len\", \"nb_upper\", \"nb_fk\", \"nb_sk\", \"nb_dk\", \"nb_you\", \"nb_mother\", \"nb_ng\", \"start_with_columns\",\n       \"has_timestamp\", \"has_date_long\", \"has_date_short\", \"has_mail\", \"has_emphasize_equal\", \"has_emphasize_quotes\", \"clean_word_len\",\n       \"clean_char_len\", \"clean_chars\", \"clean_chars_ratio\"]\n    \ndef preprocess(df):\n    keys = [i for i in repl.keys()]\n\n    new_data = []\n    ltr = df[\"comment_text\"].tolist()\n    for i in tqdm(ltr):\n        arr = str(i).split()\n        xx = \"\"\n        for j in arr:\n            j = str(j).lower()\n            if j[:4] == 'http' or j[:3] == 'www':\n                continue\n            if j in keys:\n                # print(\"inn\")\n                j = repl[j]\n            xx += j + \" \"\n        new_data.append(xx)\n    df[\"new_comment_text\"] = new_data\n    \n    trate = df[\"new_comment_text\"].tolist()\n    for i, c in enumerate(trate):\n        trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\n    df[\"comment_text\"] = trate\n    df.drop([\"new_comment_text\"], axis=1, inplace=True)\n\n    df_text = df['comment_text']\n    \n    \n    re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\n    \n    get_indicators_and_clean_comments(df)\n    \n    return df","b67eed0b":"val_df = preprocess(val_df)","a5f1f6b6":"fts = [\"raw_word_len\", \"raw_char_len\", \"nb_upper\", \"nb_fk\", \"nb_sk\", \"nb_dk\", \"nb_you\", \"nb_mother\", \"nb_ng\", \"start_with_columns\",\n       \"has_timestamp\", \"has_date_long\", \"has_date_short\", \"has_mail\", \"has_emphasize_equal\", \"has_emphasize_quotes\", \"clean_word_len\",\n       \"clean_char_len\", \"clean_chars\", \"clean_chars_ratio\"]\n\nval_text = val_df['clean_comment'].apply(lambda x: re.sub('#', '', x)).fillna('')\n\nword_vectorizer = TfidfVectorizer(\n        sublinear_tf=True,\n        strip_accents='unicode',\n        analyzer='word',\n        min_df=5,\n        ngram_range=(1, 2),\n        max_features=60000)\n\nwith open('..\/input\/jigsaw-tfidf-models\/word_vectorizer.pickle', 'rb') as handle:\n    word_vectorizer = pickle.load(handle)\n\nval_word_features = word_vectorizer.transform(val_text)\nval_features = hstack([val_df[fts], val_word_features]).tocsr()\ndel val_word_features\n\nwith open('..\/input\/jigsaw-tfidf-models\/gbm_model.pickle', 'rb') as handle:\n    gbm = pickle.load(handle)\n    \nval_df['tfidf_gbm'] = gbm.predict(val_features)\n\ntext = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s):\n    return text.sub(r' \\1 ', s)\n\nword_vectorizer = TfidfVectorizer(ngram_range=(1,2),\n               min_df=5, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1, max_features=50000)\n\nwith open('..\/input\/simple-tfidf-models\/word_vectorizer.pickle', 'rb') as handle:\n    word_vectorizer = pickle.load(handle)\n\nval_tfidf = word_vectorizer.transform(val_df['comment_text'].fillna(''))\n\nlr = LogisticRegression(solver='lbfgs', random_state=13)\nwith open('..\/input\/simple-tfidf-models\/lr_model.pickle', 'rb') as handle:\n    lr = pickle.load(handle)\n\nval_df['tfidf_lr_simple'] = lr.predict_proba(val_tfidf)[:, 1]\n\nwith open('..\/input\/simple-tfidf-models\/gbm_model.pickle', 'rb') as handle:\n    gbm = pickle.load(handle)\n    \nval_df['tfidf_gbm_simple'] = gbm.predict(val_tfidf)\n\nclass NbSvmClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, C=1.0, dual=False, n_jobs=1):\n        self.C = C\n        self.dual = dual\n        self.n_jobs = n_jobs\n\n    def predict(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict(x.multiply(self._r))\n\n    def predict_proba(self, x):\n        # Verify that model has been fit\n        check_is_fitted(self, ['_r', '_clf'])\n        return self._clf.predict_proba(x.multiply(self._r))\n\n    def fit(self, x, y):\n        y = y\n        x, y = check_X_y(x, y, accept_sparse=True)\n\n        def pr(x, y_i, y):\n            p = x[y==y_i].sum(0)\n            return (p+1) \/ ((y==y_i).sum()+1)\n        \n        self._r = sparse.csr_matrix(np.log(pr(x,1,y) \/ pr(x,0,y)))\n        x_nb = x.multiply(self._r)\n        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n        return self\n\nNbSvm = NbSvmClassifier(C=1.5, dual=True, n_jobs=-1)\nwith open('..\/input\/simple-tfidf-nbsvm\/nbsvm_model.pickle', 'rb') as handle:\n    NbSvm = pickle.load(handle)\n    \nval_df['tfidf_nbsvm_simple'] = NbSvm.predict_proba(val_tfidf)[:, 1]","9e2b0c6a":"bert_uncased1 = pd.read_csv('..\/input\/bert-base-uncased-pretrained-ep1\/valid-predictions.csv').sort_values(by='id')['prediction']\nbert_cased1 = pd.read_csv('..\/input\/bert-base-cased-pretrained-ep1\/valid-predictions.csv').sort_values(by='id')['prediction']\nbert_uncased_fresh1 = pd.read_csv('..\/input\/bert-base-uncased-fresh-ep1\/valid-predictions.csv').sort_values(by='id')['prediction']\nbert_large1 = pd.read_csv('..\/input\/resume-lr01e5-from-bert-large-pretrained-uncased\/valid-predictions.csv').sort_values(by='id')['prediction']","6909a6a3":"val_df = val_df.sort_values(by='id').reset_index()","a69293f4":"val_df['bert_uncased1'] = bert_uncased1\nval_df['bert_cased1'] = bert_cased1\nval_df['bert_uncased_fresh1'] = bert_uncased_fresh1\nval_df['bert_large1'] = bert_large1","0594d173":"gpt22 = pd.read_csv('..\/input\/gpt2-ep2-lr8e5\/valid-predictions.csv').sort_values(by='id')['prediction']","1c7c982c":"val_df['gpt22'] = gpt22","359cd611":"val_df.to_csv('val_df.csv', index=False)","1af14a85":"identity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Convert taget and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\nval_df = convert_dataframe_to_bool(val_df)","813db0ae":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df['target']\n    predicted_labels = df[model_name]\n    return roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)","d3d89011":"def ensemble_predictions(predictions, weights, type_=\"linear\"):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=1)\n    elif type_ == \"harmonic\":\n        res = np.average([1 \/ p for p in predictions.values], weights=weights, axis=1)\n        return 1 \/ res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions.values], weights=weights, axis=1\n        )\n        res = np.exp(numerator \/ sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions.values.T], weights=weights, axis=0)\n        return res \/ (len(res) + 1)\n    return res","1998fc13":"annot_idx = val_df[val_df['identity_annotator_count'] > 0].sample(n=24330, random_state=13).index\nnot_annot_idx = val_df[val_df['identity_annotator_count'] == 0].sample(n=24330, random_state=13).index\nx_test_idx = list(set(annot_idx).union(set(not_annot_idx)))\nx_train_idx = list(set(val_df.index) - set(x_test_idx))\n\ntrain_df = val_df.loc[x_train_idx]\ntest_df = val_df.loc[x_test_idx]","5b95cd89":"model_cols = [\n    'lstm_cp_0_3',\n    'lstm_cp_1_3',\n    'lstm_cp_2_3',\n    'lstm_clip_0_2',\n    'lstm_clip_1_3',\n    'lstm_clip_2_3',\n    'lstm_nb_0_4',\n    'lstm_nb_1_4',\n    'lstm_nb_2_4',\n    'lstm_tw_0_3',\n    'lstm_tw_1_3',\n    'lstm_tw_2_3',\n    'tfidf_gbm',\n    'tfidf_lr_simple',\n    'tfidf_gbm_simple',\n    'tfidf_nbsvm_simple',\n    'bert_uncased1',\n    'bert_cased1',\n    'bert_uncased_fresh1',\n    'bert_large1',\n    'gpt22'\n]","90f3c05e":"for m_col in model_cols:\n    bias_metrics_df = compute_bias_metrics_for_model(train_df, identity_columns, m_col, 'target')\n    print(m_col)\n    print(get_final_metric(bias_metrics_df, calculate_overall_auc(train_df, m_col)))","96c564c8":"for m_col in model_cols:\n    bias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, m_col, 'target')\n    print(m_col)\n    print(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, m_col)))","8e2f624f":"init_weights = {\n    'cp': [40, 40, 40],\n    'clip': [40, 40, 40],\n    'nb': [40, 40, 40],\n    'tw': [40, 40, 40],\n    'tfidf': [40, 40, 40, 40],\n    'bert': [40, 40, 40, 40],\n    'blend': [40, 40, 40, 40, 40, 40, 40]\n}\n\ndef custom_predict(X_models, weights=init_weights, type_='geometric'):\n    model_cp = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_cp')]], weights=(np.array(weights['cp']) + 1e-15) \/ (sum(weights['cp']) + 1e-15), type_=type_)\n    model_clip = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_clip')]], weights=(np.array(weights['clip']) + 1e-15) \/ (sum(weights['clip']) + 1e-15), type_=type_)\n    model_nb = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_nb')]], weights=(np.array(weights['nb']) + 1e-15) \/ (sum(weights['nb']) + 1e-15), type_=type_)\n    model_tw = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('lstm_tw')]], weights=(np.array(weights['tw']) + 1e-15) \/ (sum(weights['tw']) + 1e-15), type_=type_)\n    model_tfidf = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('tfidf')]], weights=(np.array(weights['tfidf']) + 1e-15) \/ (sum(weights['tfidf']) + 1e-15), type_=type_)\n    model_bert = ensemble_predictions(X_models[[col for col in X_models.columns if col.startswith('bert')]], weights=(np.array(weights['bert']) + 1e-15) \/ (sum(weights['bert']) + 1e-15), type_=type_)\n    model_gpt = X_models['gpt22']\n    \n    models = [model_cp, model_clip, model_nb, model_tw, model_tfidf, model_bert, model_gpt]\n    model_blend = np.zeros_like(model_bert) + 1e-15\n    for i in range(len(models)):\n        model_blend += weights['blend'][i] * models[i]\n    model_blend \/= (sum(weights['blend']) + 1e-15)\n    return model_blend\n\n\ntrain_df['custom_model'] = custom_predict(train_df[model_cols])\nbias_metrics_df = compute_bias_metrics_for_model(train_df, identity_columns, 'custom_model', 'target')\nprint(get_final_metric(bias_metrics_df, calculate_overall_auc(train_df, 'custom_model')))\n\ntest_df['custom_model'] = custom_predict(test_df[model_cols])\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, 'custom_model', 'target')\nprint(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, 'custom_model')))","e6192c7d":"def val_result(weights):\n    y_pred = custom_predict(train_df[model_cols], weights)\n    train_df['custom_model'] = y_pred\n    bias_metrics_df = compute_bias_metrics_for_model(train_df, identity_columns, 'custom_model', 'target')\n    return get_final_metric(bias_metrics_df, calculate_overall_auc(train_df, 'custom_model'))\n\ndef go_down(best_weights, best_result, model, j):\n    curr_weights = copy.deepcopy(best_weights)\n    curr_weights[model][j] -= 1\n    curr_result = val_result(curr_weights)\n    if curr_result > best_result:\n        return curr_weights, curr_result\n    else:\n        return best_weights, best_result\n    \ndef go_up(best_weights, best_result, model, j):\n    curr_weights = copy.deepcopy(best_weights)\n    curr_weights[model][j] += 1\n    curr_result = val_result(curr_weights)\n    if curr_result > best_result:\n        return curr_weights, curr_result\n    else:\n        return best_weights, best_result\n\n    \ndef weights_tuning(\n    init_weights=init_weights,\n    max_iters=200\n):\n    models = init_weights.keys()\n    best_weights = init_weights\n    best_result = val_result(best_weights)\n    for i in range(max_iters):\n        print('Start of iteration #{}'.format(i + 1))\n        n_changes = 0\n        for model in models:\n            print('Model {}'.format(model))\n            print('Current weights: {}, result: {}'.format(best_weights[model], best_result))\n            for j in range(len(best_weights[model])):\n                changed = True\n                changed_down = False\n                while changed:\n                    curr_weights, curr_result = go_down(best_weights, best_result, model, j)\n                    if curr_result == best_result:\n                        changed = False\n                    else:\n                        print('Weights changed to: {}, result: {}'.format(curr_weights[model], curr_result))\n                        best_weights, best_result = curr_weights, curr_result\n                        n_changes += 1\n                        changed_down = True\n                if not changed_down:\n                    changed = True\n                    while changed:\n                        curr_weights, curr_result = go_up(best_weights, best_result, model, j)\n                        if curr_result == best_result:\n                            changed = False\n                        else:\n                            print('Weights changed to: {}, result: {}'.format(curr_weights[model], curr_result))\n                            best_weights, best_result = curr_weights, curr_result\n                            n_changes += 1\n            print('Best weights: {}, result: {}'.format(best_weights[model], best_result))\n         \n        print('End of iteration #{}, number of changes: {}'.format(i + 1, n_changes))\n        print()\n        if n_changes == 0:\n            break\n    return best_weights, best_result","b34e8d23":"best_weights, best_result = weights_tuning()","b072ec3c":"print(best_weights)\nprint(best_result)","8f7b7905":"test_df['custom_model'] = custom_predict(test_df[model_cols], weights=best_weights)\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, 'custom_model', 'target')\nprint(get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, 'custom_model')))","d909d540":"#### Test split","3f17a909":"## BERT","3b8e7692":"## BiLSTM Twitter","f2c91a5c":"## TF-IDF","1434b267":"## BiLSTM Crawl + Paragram","11adf0e3":"## Validation","69ea1568":"## BiLSTM clip target [0.05, 0.95]","4c516b01":"## BiLSTM Numberbatch","b70d922b":"## GPT2"}}