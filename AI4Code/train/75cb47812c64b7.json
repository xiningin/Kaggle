{"cell_type":{"9b502efa":"code","5c732789":"code","111a3f3e":"code","b58da0b0":"code","f060f326":"code","abde0c25":"code","098cee27":"code","726be0f6":"code","df50f98d":"code","1ed439c7":"code","b2d87a17":"code","6820f08d":"code","131cdc21":"code","99fa1f4e":"code","ff95d0d4":"code","85c42491":"markdown","0c68d6fe":"markdown","6f84a39c":"markdown","1d29ac9a":"markdown","b7ca0467":"markdown","02b260fb":"markdown","0f162113":"markdown","2bb830b5":"markdown","bb3a14df":"markdown","87802d97":"markdown","32eefb2d":"markdown","4ed996ff":"markdown"},"source":{"9b502efa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom keras.preprocessing.image import img_to_array\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c732789":"def data_set(dir_data):\n    data=[]\n    target=[]\n    data_map = {\n    'with_mask':1,\n    'without_mask':0\n    }\n    skipped=0\n    root=dir_data+'_annotations.csv'\n    df1 = pd.read_csv(root)\n    df1.dataframeName = '_annotations.csv'\n    nRow, nCol = df1.shape\n    for i in range(len(df1)):\n        without_mask='without_mask'\n        k=dir_data+df1['filename'][i]\n        image=cv2.imread(k)\n        xmin=int(df1['xmin'][i])\n        ymin=int(df1['ymin'][i])\n        xmax=int(df1['xmax'][i])\n        ymax=int(df1['ymax'][i])\n        image=image[ymin:ymax,  xmin:xmax]\n        try:\n                # resizing to (70 x 70)\n                image = cv2.resize(image,(70,70))\n        except Exception as E:\n                skipped += 1\n                print(E)\n                continue\n        if(df1['class'][i]=='mask'):\n            without_mask='with_mask'\n        image=img_to_array(image)\n        data.append(image)\n        target.append(data_map[without_mask])\n    data = np.array(data, dtype=\"float\") \/ 255.0\n    target = tf.keras.utils.to_categorical(np.array(target), num_classes=2)\n    return data, target\ntraining_data, training_target=data_set('\/kaggle\/input\/face-mask-detection\/train\/')\ntesting_data, testing_target=data_set('\/kaggle\/input\/face-mask-detection\/test\/')\nvalid_data, valid_target=data_set('\/kaggle\/input\/face-mask-detection\/valid\/')","111a3f3e":"plt.figure(0, figsize=(100,100))\nfor i in range(1,10):\n    plt.subplot(10,5,i)\n    plt.imshow(training_data[i])","b58da0b0":"img_shape=training_data[0].shape\ndepth, height, width=3, img_shape[0], img_shape[1]\nimg_shape=(height, width, depth)\nchanDim=-1\nif backend.image_data_format() == \"channels_first\": #Returns a string, either 'channels_first' or 'channels_last'\n        img_shape = (depth, height, width)\n        chanDim = 1","f060f326":"model=Sequential()\nmodel.add(layers.Conv2D(32,(3,3),input_shape=img_shape))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(layers.Conv2D(64,(3,3)))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(layers.Conv2D(128,(3,3)))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(layers.Conv2D(256,(3,3)))\nmodel.add(layers.Activation('relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(64,activation='relu'))\nmodel.add(layers.Dropout(0.4))\n\nmodel.add(layers.Dense(2,activation='softmax'))\n\nadam =tf.keras.optimizers.Adam(0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","abde0c25":"model.summary()","098cee27":"# augmenting dataset \naug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n                         height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n                         horizontal_flip=True, fill_mode=\"nearest\")","726be0f6":"history = model.fit(aug.flow(training_data, training_target, batch_size=10),\n                   epochs=70,\n                   validation_data=(valid_data, valid_target),\n                   verbose=2,\n                   shuffle=True) ","df50f98d":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.ylabel(['accuracy'])\nplt.xlabel(['epoch'])\nplt.legend(['accuracy', 'val_accuracy'])","1ed439c7":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(['loss'])\nplt.xlabel(['epoch'])\nplt.legend(['loss', 'val_loss'])","b2d87a17":"loss, accuracy = model.evaluate(testing_data,testing_target)\nprint('accuracy= ',loss,\" loss= \",loss)","6820f08d":"yhat = model.predict(testing_data)\ntest_pred=np.argmax(yhat,axis=1)\ntesting_target=np.argmax(testing_target,axis=1)","131cdc21":"from sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nimport itertools\nreport = classification_report(testing_target, test_pred)\nprint(report)","99fa1f4e":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.RdYlGn):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","ff95d0d4":"confusion = metrics.confusion_matrix(testing_target, test_pred)\nplt.figure()\nplot_confusion_matrix(confusion, classes=['without_mask','with_mask'], title='Confusion matrix')","85c42491":"Find loss and accuracy of our model","0c68d6fe":"let's plot a graph between loss of training and validation data set","6f84a39c":"check the summary of model","1d29ac9a":"Building Model","b7ca0467":"Most of the given data set images contain more than one person,for for our model we need area cover by face(with mask or without mask) only not the entire image. We use the csv data set to crop only the required part and than load it, in the train, test and valid data frames.","02b260fb":"let's plot a graph between accuracy of training and validation data set","0f162113":"\n\nLet's take a quick look at what the data looks like:\n","2bb830b5":"Now, we check the format of the images\nchannels_last=(row,col,channels)\nchannels_first=(channel,row,col)","bb3a14df":"Finally, we are ready to train our model.","87802d97":"let's use our model on the testing data and get the report of our model","32eefb2d":"Now we apply data augumentation for training our model with more data set which produce by modifying the same data set","4ed996ff":"confusion matrix"}}