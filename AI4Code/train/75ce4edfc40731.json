{"cell_type":{"2b17336f":"code","dee8e248":"code","44f0200e":"code","52d950ca":"code","3b013501":"code","a63601ba":"code","823966f7":"code","24a8e023":"code","2f3b437e":"code","140822d6":"code","ddb99526":"code","66a7638c":"code","c9f0a9f8":"code","960b2aa3":"code","8f3ef8c7":"code","b47bfc01":"code","ee249daf":"markdown"},"source":{"2b17336f":"import sys\n!cp ..\/input\/rapids\/rapids.0.14.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","dee8e248":"!pip install -q https:\/\/github.com\/pfnet-research\/xfeat\/archive\/master.zip","44f0200e":"import pandas as pd\nfrom sklearn.model_selection import KFold\nimport xfeat\nimport cudf\n\nimport catboost as cat\nimport lightgbm as lgb\n\n\nxfeat.utils.cudf_is_available()","52d950ca":"PATH_TRAIN_CSV = \"..\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip\"\nPATH_TEST_CSV = \"..\/input\/bnp-paribas-cardif-claims-management\/test.csv.zip\"\nPATH_SUB_CSV = \"..\/input\/bnp-paribas-cardif-claims-management\/sample_submission.csv.zip\"\n\nUSECOLS = [\n    \"v10\", \"v12\", \"v14\", \"v21\", \"v22\", \"v24\", \"v30\", \"v31\", \"v34\", \"v38\",\n    \"v40\", \"v47\", \"v50\", \"v52\", \"v56\", \"v62\", \"v66\", \"v72\", \"v75\", \"v79\",\n    \"v91\", \"v112\", \"v113\", \"v114\", \"v129\", \"target\"\n]\n\n\ndef preload():\n    # Convert dataset into feather format.\n    xfeat.utils.compress_df(pd.concat([\n        pd.read_csv(PATH_TRAIN_CSV),\n        pd.read_csv(PATH_TEST_CSV),\n    ], sort=False)).reset_index(drop=True)[USECOLS].to_feather(\n        \"..\/working\/train_test.ftr\")\n\n\npreload()","3b013501":"pd.read_feather(\"..\/working\/train_test.ftr\").head()","a63601ba":"print(\"(1) Save numerical features\")\nxfeat.SelectNumerical().fit_transform(\n    pd.read_feather(\"..\/working\/train_test.ftr\")\n).reset_index(drop=True).to_feather(\"..\/working\/feature_num_features.ftr\")","823966f7":"pd.read_feather(\"..\/working\/feature_num_features.ftr\").head()","24a8e023":"print(\"(2) Categorical encoding using label encoding: 13 features\")\nxfeat.Pipeline([\n    xfeat.SelectCategorical(), xfeat.LabelEncoder(output_suffix=\"\")]\n).fit_transform(\n    pd.read_feather(\"..\/working\/train_test.ftr\")\n).reset_index(drop=True).to_feather(\"..\/working\/feature_1way_label_encoding.ftr\")","2f3b437e":"pd.read_feather(\"..\/working\/feature_1way_label_encoding.ftr\").head()","140822d6":"def target_encoding(path_source_dataframe, path_output_dataframe):\n    df_train_test = pd.read_feather(path_source_dataframe)\n    df_train_test.loc[:, \"target\"] = pd.read_feather(\"..\/working\/train_test.ftr\")[\"target\"]\n    df_train = df_train_test.dropna(subset=[\"target\"]).copy()\n    df_test = df_train_test.loc[df_train_test[\"target\"].isnull()].copy()\n\n    fold = KFold(n_splits=5, shuffle=True, random_state=111)\n    encoder = xfeat.TargetEncoder(fold=fold, target_col=\"target\", output_suffix=\"\")\n    df_train = encoder.fit_transform(cudf.from_pandas(df_train))\n    df_test = encoder.transform(cudf.from_pandas(df_test))\n\n    pd.concat([df_train.to_pandas(), df_test.to_pandas()], sort=False).drop(\n        \"target\", axis=1).reset_index(drop=True).to_feather(path_output_dataframe)\n\n\nprint(\"(2') Target encoding of categorical variables\")\ntarget_encoding(\"..\/working\/feature_1way_label_encoding.ftr\",\n                \"..\/working\/feature_1way_label_encoding_with_te.ftr\")","ddb99526":"pd.read_feather(\"..\/working\/feature_1way_label_encoding_with_te.ftr\").head()","66a7638c":"print(\"(3) 2-order combination of categorical features: 78 features (13 * 12 \/ 2 = 78)\")\nxfeat.Pipeline([\n    xfeat.SelectCategorical(),\n    xfeat.ConcatCombination(drop_origin=True, r=2),\n    xfeat.LabelEncoder(output_suffix=\"\"),\n]).fit_transform(pd.read_feather(\"..\/working\/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"..\/working\/feature_2way_label_encoding.ftr\"\n)","c9f0a9f8":"pd.read_feather(\"..\/working\/feature_2way_label_encoding.ftr\").head()","960b2aa3":"print(\"(4) 3-order combination of categorical features\")\n# Use `include_cols=` kwargs to reduce the total count of combinations.\n# 66 features (12 * 11 \/ 2 = 66)\nxfeat.Pipeline([\n    xfeat.SelectCategorical(),\n    xfeat.ConcatCombination(drop_origin=True, include_cols=[\"v22\"], r=3),\n    xfeat.LabelEncoder(output_suffix=\"\"),\n]).fit_transform(pd.read_feather(\"..\/working\/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"..\/working\/feature_3way_including_v22_label_encoding.ftr\"\n)\n\n\nprint(\"(5) Convert numerical to categorical using round: 12 features\")\ndf_rnum = (\n    xfeat.Pipeline(\n        [\n            xfeat.SelectNumerical(),\n            xfeat.LambdaEncoder(\n                lambda x: str(x)[:-2],\n                output_suffix=\"_rnum\",\n                exclude_cols=[\"target\"],\n            ),\n        ]\n    )\n    .fit_transform(pd.read_feather(\"..\/working\/train_test.ftr\"))\n    .reset_index(drop=True)\n)\ndf_rnum.to_feather(\"..\/working\/feature_round_num.ftr\")\nrnum_cols = [col for col in df_rnum.columns if col.endswith(\"_rnum\")]\nxfeat.Pipeline([xfeat.LabelEncoder(output_suffix=\"\")]).fit_transform(\n    pd.read_feather(\"..\/working\/feature_round_num.ftr\")[rnum_cols]\n).reset_index(drop=True).to_feather(\"..\/working\/feature_round_num_label_encoding.ftr\")\n\n\nprint(\"(6) 2-order Arithmetic combinations.\")\nxfeat.Pipeline(\n    [\n        xfeat.SelectNumerical(),\n        xfeat.ArithmeticCombinations(\n            exclude_cols=[\"target\"], drop_origin=True, operator=\"+\", r=2,\n        ),\n    ]\n).fit_transform(pd.read_feather(\"..\/working\/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"..\/working\/feature_arithmetic_combi2.ftr\"\n)\n\n\nprint(\"(7) Add more combinations: 11-order concat combinations.\")\nxfeat.Pipeline(\n    [\n        xfeat.SelectCategorical(),\n        xfeat.ConcatCombination(drop_origin=True, include_cols=[\"v22\"], r=11),\n        xfeat.LabelEncoder(output_suffix=\"\"),\n    ]\n).fit_transform(pd.read_feather(\"..\/working\/train_test.ftr\")).reset_index(\n    drop=True\n).to_feather(\n    \"..\/working\/feature_11way_including_v22_label_encoding.ftr\"\n)\n\n\nprint(\"(3') Target encoding of categorical variables\")\ntarget_encoding(\"..\/working\/feature_2way_label_encoding.ftr\",\n                \"..\/working\/feature_2way_label_encoding_with_te.ftr\")\n\nprint(\"(4') Target encoding of categorical variables\")\ntarget_encoding(\"..\/working\/feature_3way_including_v22_label_encoding.ftr\",\n                \"..\/working\/feature_3way_including_v22_label_encoding_with_te.ftr\")\n\nprint(\"(5') Target encoding of categorical variables\")\ntarget_encoding(\"..\/working\/feature_round_num_label_encoding.ftr\",\n                \"..\/working\/feature_round_num_label_encoding_with_te.ftr\")\n\nprint(\"(7') Target encoding of categorical variables\")\ntarget_encoding(\"..\/working\/feature_11way_including_v22_label_encoding.ftr\",\n                \"..\/working\/feature_11way_including_v22_label_encoding_with_te.ftr\")","8f3ef8c7":"!ls -lha ..\/working\/","b47bfc01":"import catboost as cat\n\n\ndef catboost_model():\n    print(\"Load numerical features\")\n    df_num = pd.concat(\n        [\n            pd.read_feather(\"..\/working\/feature_num_features.ftr\"),\n            pd.read_feather(\"..\/working\/feature_arithmetic_combi2.ftr\"),\n        ],\n        axis=1,\n    )\n    y_train = df_num[\"target\"].dropna()\n    df_num.drop([\"target\"], axis=1, inplace=True)\n\n    print(\"Load categorical features\")\n    df = pd.concat(\n        [\n            pd.read_feather(\"..\/working\/feature_1way_label_encoding.ftr\"),\n            pd.read_feather(\"..\/working\/feature_2way_label_encoding.ftr\"),\n            pd.read_feather(\"..\/working\/feature_3way_including_v22_label_encoding.ftr\"),\n            pd.read_feather(\"..\/working\/feature_round_num_label_encoding.ftr\"),\n            pd.read_feather(\"..\/working\/feature_11way_including_v22_label_encoding.ftr\"),\n        ],\n        axis=1,\n    )\n    cat_cols = df.columns.tolist()\n    df = pd.concat([df, df_num], axis=1)\n\n    print(\"Fit\")\n    params = {\n        \"loss_function\": \"Logloss\",\n        \"eval_metric\": \"Logloss\",\n        \"learning_rate\": 0.03,\n        \"iterations\": 3000,\n        \"l2_leaf_reg\": 3,\n        \"random_seed\": 432013,\n        \"subsample\": 0.66,\n        \"od_type\": \"Iter\",\n        \"rsm\": 0.2,\n        \"depth\": 6,\n        \"border_count\": 128,\n    }\n    model = cat.CatBoostClassifier(**params)\n    train_data = cat.Pool(\n        df.iloc[: y_train.shape[0]], label=y_train, cat_features=cat_cols\n    )\n    fit_model = model.fit(train_data, verbose=30)\n\n    # Predict\n    y_pred = fit_model.predict_proba(df.iloc[y_train.shape[0] :])\n    submission = pd.read_csv(\"..\/input\/bnp-paribas-cardif-claims-management\/sample_submission.csv.zip\")\n    submission.loc[:, \"PredictedProb\"] = y_pred[:, 1]\n    submission.to_csv(\"..\/working\/solution_cat.csv\", index=False)\n\n\nLIGHTGBM_BASE_PARAMS = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"verbosity\": 1,\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 32,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 1.0,\n    \"bagging_freq\": 0,\n    \"learning_rate\": 0.03,\n    \"max_bin\": 255,\n    \"seed\": 1,\n    \"min_data_in_leaf\": 20,\n}\nLIGHTGBM_BASE_FIT_PARAMS = {\n    \"verbose_eval\": 30,\n    \"num_boost_round\": 3000,\n}\n\n\ndef lightgbm_model():\n    print(\"Load numerical features\")\n    df_num = pd.concat(\n        [\n            pd.read_feather(\"..\/working\/feature_num_features.ftr\"),\n            pd.read_feather(\"..\/working\/feature_arithmetic_combi2.ftr\"),\n        ],\n        axis=1,\n    )\n    y_train = df_num[\"target\"].dropna()\n    df_num.drop([\"target\"], axis=1, inplace=True)\n\n    print(\"Load categorical features\")\n    df = pd.concat(\n        [\n            pd.read_feather(\"..\/working\/feature_1way_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"..\/working\/feature_2way_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"..\/working\/feature_3way_including_v22_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"..\/working\/feature_round_num_label_encoding_with_te.ftr\"),\n            pd.read_feather(\"..\/working\/feature_11way_including_v22_label_encoding_with_te.ftr\"),\n        ],\n        axis=1,\n    )\n    cat_cols = df.columns.tolist()\n    df = pd.concat([df, df_num], axis=1)\n\n    X_train, X_test = df.values[:y_train.shape[0]], df.values[y_train.shape[0]:]\n    dtrain = lgb.Dataset(X_train, y_train)\n    \n    fit_params = LIGHTGBM_BASE_FIT_PARAMS.copy()\n    fit_params[\"valid_sets\"] = [dtrain]\n\n    bst = lgb.train(LIGHTGBM_BASE_PARAMS, dtrain, **fit_params)\n    y_pred = bst.predict(X_test)\n\n    # Predict\n    submission = pd.read_csv(\"..\/input\/bnp-paribas-cardif-claims-management\/sample_submission.csv.zip\")\n    submission.loc[:, \"PredictedProb\"] = y_pred\n    submission.to_csv(\"..\/working\/solution_lgb.csv\", index=False)\n\n\nlightgbm_model()   \ncatboost_model()\n\n\n# Linear blending\ny1 = pd.read_csv(\"..\/working\/solution_cat.csv\")\ny2 = pd.read_csv(\"..\/working\/solution_lgb.csv\")\n\ndf_sub = pd.read_csv(PATH_SUB_CSV)\ndf_sub.loc[:, \"PredictedProb\"] = 0.6 * y1.PredictedProb.values + 0.4 * y2.PredictedProb.values\ndf_sub[[\"ID\", \"PredictedProb\"]].to_csv(\"..\/working\/solution_blend.csv\", index=False)","ee249daf":"# xfeat: Flexible Feature Engineering & Exploration Library using GPUs and Optuna.\n\nxfeat provides sklearn-like transformation classes for feature engineering and exploration. Unlike sklearn API, xfeat provides a dataframe-in, dataframe-out interface. xfeat supports both pandas and cuDF dataframes. By using cuDF and CuPy, xfeat can generate features 10 ~ 30 times faster than a naive pandas operation.\n\nhttps:\/\/github.com\/pfnet-research\/xfeat"}}