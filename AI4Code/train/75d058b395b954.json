{"cell_type":{"fb5113e1":"code","220cdff4":"code","3009d478":"code","ed64c5d1":"code","fe6b4fa9":"code","6fea5632":"code","b6a3f06f":"code","48ba9539":"code","2d486045":"code","a61638e8":"code","ee502a83":"code","17f5ab6e":"code","fd48a1f9":"code","4662ea38":"code","0155ee4b":"code","44499d75":"code","bb20f138":"code","f04ab728":"code","8625b876":"code","0f95a4a5":"code","d49110d3":"code","ac9cba0d":"code","7155f3f6":"code","46b68c56":"code","25583942":"code","5050bfa3":"code","26925993":"code","88663e68":"code","094cda6b":"code","2de99db8":"code","b44a582c":"markdown","91df46c1":"markdown","92dbf204":"markdown","22e20246":"markdown","3dd879fc":"markdown","cca2fca7":"markdown","ac432cff":"markdown","bf2fe37a":"markdown","232142c0":"markdown"},"source":{"fb5113e1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n","220cdff4":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata=pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')","3009d478":"data[\"bmi\"]=data[\"bmi\"].astype(int)\ndata[\"charges\"]=data[\"charges\"].astype(int)","ed64c5d1":"x=data.iloc[:, :-1].values\ny=data.iloc[:,-1].values","fe6b4fa9":"x","6fea5632":"y","b6a3f06f":"## Gender\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nx[:,1]=le.fit_transform(x[:,1])\nx[:,4]=le.fit_transform(x[:,4])\nx","48ba9539":"#region\nnp.set_printoptions(threshold=np.sys.maxsize)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [5])], remainder='passthrough')\nx = np.array(ct.fit_transform(x))","2d486045":"x[:3]","a61638e8":"from sklearn.model_selection import train_test_split","ee502a83":"x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.2, random_state=43)","17f5ab6e":"from sklearn.linear_model import LinearRegression\nregressor=LinearRegression()\nregressor.fit(x_train, y_train)","fd48a1f9":"y_pred=regressor.predict(x_test) #### Vector of PREDICTED cost (x_Test)","4662ea38":"np.set_printoptions(precision=2)","0155ee4b":"print(np.concatenate((y_pred.reshape(len(y_pred), 1),y_test.reshape(len(y_test),1)),1))\n","44499d75":"x=x[:,1:] ## to avoid dummy trap \nlen(x)","bb20f138":"np.set_printoptions(threshold=np.sys.maxsize)","f04ab728":"import statsmodels.api as sm\n## Create the b0\nx=np.append(arr=np.ones((len(x),1)).astype(int),values=x,axis=1)\nx[:10]","8625b876":"x_opt=x[:,[0,1,2,3,4,5,6,7,8]]\nx_opt=x_opt.astype(np.float64)","0f95a4a5":"#Step 2: Fit the full model  with all possible predictors\nregressor_OLS=sm.OLS(endog=y,exog=x_opt).fit()\n# endog is dependent variable, exog - regressor","d49110d3":"regressor_OLS.summary()","ac9cba0d":"x[:3]","7155f3f6":"#x5 has the highest p-value: 0.693 - this is the gender, so the gender has less  significant influence on the cost\nx_opt=x[:,[0,1,2,3,4,6,7,8]]\nx_opt=x_opt.astype(np.float64)\nregressor_OLS=sm.OLS(endog=y, exog=x_opt).fit()\nregressor_OLS.summary()","46b68c56":"x[:3]","25583942":"## Again x1 has the highest p-values, 0.878\nx_opt=x[:,[0,2,3,4,6,7,8]]\nx_opt=x_opt.astype(np.float64)\nregressor_OLS=sm.OLS(endog=y,exog=x_opt).fit()\nregressor_OLS.summary()","5050bfa3":"x_opt=x[:,[0,2,4,6,7,8]]\nx_opt=x_opt.astype(np.float64)\nregressor_OLS=sm.OLS(endog=y, exog=x_opt).fit()\nregressor_OLS.summary()","26925993":"x_opt=x[:,[0,4,6,7,8]]\nx_opt=x_opt.astype(np.float64)\nregressor_OLS=sm.OLS(endog=y, exog=x_opt).fit()\nregressor_OLS.summary()","88663e68":"## Now the model is ready, with the backward elimination we have left only those variables that matter ","094cda6b":"n=x[:,[0,4,6,7,8]]\nn[:3]","2de99db8":"data","b44a582c":"### Encode Categorical Data","91df46c1":"## Variables that are statistically significant for our prediction:\n  - Constant\n  - Age\n  - BMI\n  - Children\n  - Smoker Status","92dbf204":" ### Now we need to get rid of the variables, with the highest p-values (everything that is higher 0.05)","22e20246":"## ## Predicting the Test set results using the class\n","3dd879fc":"## Test Train Split","cca2fca7":"## 1 column - predicted cost, 2 column - real cost\nIt seems that our model brought closer results to the real charges. Let me know if it makes sense","ac432cff":"## Training the Multiple Linear Regression model on the Training set","bf2fe37a":"Ordinary least-squares (OLS) models assume that the analysis is fitting a model of a relationship between one or more explanatory variables and a continuous or at least interval outcome variable that minimizes the sum of square errors, where an error is the difference between the actual and the predicted value of the","232142c0":"## Backward Elimination - Manual Elimination"}}